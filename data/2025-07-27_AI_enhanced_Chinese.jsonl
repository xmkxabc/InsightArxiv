{"id": "2507.18267", "title": "An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs", "authors": ["Zeqin Liao", "Zibin Zheng", "Peifan Reng", "Henglong Liang", "Zixu Gao", "Zhixiang Chen", "Wei Li", "Yuhong Nan"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18267v1", "summary": "Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly\nevolving technological domain. Ensuring their program correctness is\nfundamental to their successful deployment. However, a general and in-depth\nunderstanding of EAIR system bugs remains lacking, which hinders the\ndevelopment of practices and techniques to tackle EAIR system bugs.\n  To bridge this gap, we conducted the first systematic study of 885 EAIR\nsystem bugs collected from 80 EAIR system projects to investigate their\nsymptoms, underlying causes, and module distribution. Our analysis takes\nconsiderable effort, which classifies these bugs into 18 underlying causes, 15\ndistinct symptoms, and identifies 13 affected modules. It reveals several new\ninteresting findings and implications which help shed light on future research\non tackling or repairing EAIR system bugs. First, among the 15 identified\nsymptoms, our findings highlight 8 symptoms specific to EAIR systems, which is\ncharacterized by severe functional failures and potential physical hazards.\nSecond, within the 18 underlying causes, we define 8 EAIR-specific causes, the\nmajority of which stem from the intricate issues of AI- agent reasoning and\ndecision making. Finally, to facilitate precise and efficient bug prediction,\ndetection, and repair, we constructed a mapping between underlying causes and\nthe modules in which they most frequently occur, which enables researchers to\nfocus diagnostic efforts on the modules most susceptible to specific bug types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18267v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "具身人工智能机器人 (EAIR) 软件缺陷的实证研究", "tldr": "对具身人工智能机器人（EAIR）的软件缺陷进行了首次系统性研究，揭示了其特有的症状、根本原因和模块分布，并构建了根本原因与模块的映射关系，以帮助未来的缺陷诊断和修复。", "motivation": "具身人工智能机器人（EAIR）是一个新兴且快速发展的技术领域，确保其程序正确性对其成功部署至关重要。然而，目前对EAIR系统缺陷的普遍和深入理解仍然缺乏，这阻碍了解决EAIR系统缺陷的实践和技术的发展。", "method": "研究收集了来自80个EAIR系统项目的885个EAIR系统缺陷，并对其症状、根本原因和模块分布进行了系统性研究。分析将这些缺陷分为18种根本原因、15种不同的症状，并识别出13个受影响的模块。此外，还构建了根本原因与缺陷最常发生模块之间的映射关系。", "result": "1. 在15种已识别的症状中，有8种是EAIR系统特有的，其特点是严重的系统功能故障和潜在的物理危害。2. 在18种根本原因中，定义了8种EAIR特有的原因，其中大部分源于AI代理推理和决策的复杂问题。3. 构建了根本原因与最常发生模块之间的映射关系，有助于研究人员将诊断工作集中在最容易出现特定类型缺陷的模块上。", "conclusion": "本研究首次对EAIR系统缺陷进行了系统性分析，揭示了其特有症状和根本原因，并提供了根本原因与模块的映射，为未来EAIR系统缺陷的诊断、检测和修复提供了新的见解和方向。", "translation": "具身人工智能机器人（EAIR）是一个新兴且快速发展的技术领域。确保其程序正确性对其成功部署至关重要。然而，目前对EAIR系统缺陷的普遍和深入理解仍然缺乏，这阻碍了解决EAIR系统缺陷的实践和技术的发展。\n为了弥补这一差距，我们对从80个EAIR系统项目中收集的885个EAIR系统缺陷进行了首次系统性研究，以调查它们的症状、根本原因和模块分布。我们的分析付出了巨大的努力，将这些缺陷分为18种根本原因、15种不同的症状，并识别出13个受影响的模块。它揭示了一些新的有趣发现和启示，有助于阐明未来解决或修复EAIR系统缺陷的研究。首先，在15种已识别的症状中，我们的发现强调了8种EAIR系统特有的症状，其特点是严重的功能故障和潜在的物理危害。其次，在18种根本原因中，我们定义了8种EAIR特有的原因，其中大部分源于AI代理推理和决策的复杂问题。最后，为了促进精确高效的缺陷预测、检测和修复，我们构建了根本原因与它们最常出现的模块之间的映射关系，这使得研究人员能够将诊断工作集中在最容易出现特定缺陷类型的模块上。", "summary": "本研究首次对具身人工智能机器人（EAIR）的软件缺陷进行了系统性的实证分析。通过对885个EAIR系统缺陷的深入调查，识别并分类了其特有的症状、根本原因和受影响的模块。研究发现8种EAIR特有症状（涉及功能故障和物理危害）和8种EAIR特有根本原因（多源于AI推理决策问题）。此外，还构建了根本原因与模块的映射关系，旨在提高未来缺陷诊断和修复的效率。", "keywords": "具身人工智能机器人, 软件缺陷, 实证研究, 缺陷分析, AI推理", "comments": "这项研究的创新之处在于它是首次对具身人工智能机器人（EAIR）软件缺陷进行的系统性实证研究，填补了该领域理解上的空白。其重要性在于揭示了EAIR系统特有的缺陷类型和原因，特别是与AI代理推理和决策相关的复杂问题，并提供了有助于未来缺陷诊断和修复的实用映射关系。这对于确保EAIR系统的可靠性和安全性具有重要意义。"}}
{"id": "2507.18107", "title": "T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation", "authors": ["Yubin Chen", "Xuyang Guo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18107v1", "summary": "Text-to-video (T2V) models have shown remarkable performance in generating\nvisually reasonable scenes, while their capability to leverage world knowledge\nfor ensuring semantic consistency and factual accuracy remains largely\nunderstudied. In response to this challenge, we propose T2VWorldBench, the\nfirst systematic evaluation framework for evaluating the world knowledge\ngeneration abilities of text-to-video models, covering 6 major categories, 60\nsubcategories, and 1,200 prompts across a wide range of domains, including\nphysics, nature, activity, culture, causality, and object. To address both\nhuman preference and scalable evaluation, our benchmark incorporates both human\nevaluation and automated evaluation using vision-language models (VLMs). We\nevaluated the 10 most advanced text-to-video models currently available,\nranging from open source to commercial models, and found that most models are\nunable to understand world knowledge and generate truly correct videos. These\nfindings point out a critical gap in the capability of current text-to-video\nmodels to leverage world knowledge, providing valuable research opportunities\nand entry points for constructing models with robust capabilities for\ncommonsense reasoning and factual generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18107v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "T2VWorldBench：一个评估文本到视频生成中世界知识的基准", "tldr": "当前文本到视频（T2V）模型在利用世界知识方面存在不足。本文提出了T2VWorldBench，一个用于评估T2V模型世界知识生成能力的基准，涵盖多领域和评估方法。评估结果显示大多数模型未能正确理解和生成世界知识，揭示了当前T2V模型的关键能力差距。", "motivation": "文本到视频（T2V）模型在生成视觉上合理的场景方面表现出色，但它们利用世界知识以确保语义一致性和事实准确性的能力仍未得到充分研究。为了解决这一挑战，需要一个系统性的评估框架。", "method": "提出了T2VWorldBench，这是第一个系统性的评估框架，用于评估文本到视频模型的“世界知识”生成能力。该基准涵盖了6个主要类别、60个子类别和1200个提示词，涉及物理、自然、活动、文化、因果关系和物体等广泛领域。它结合了人类评估和使用视觉-语言模型（VLMs）的自动化评估。研究者评估了目前可用的10个最先进的文本到视频模型。", "result": "评估结果发现，大多数文本到视频模型无法理解世界知识并生成真正正确的视频。", "conclusion": "当前文本到视频模型在利用世界知识的能力方面存在一个关键差距。这为构建具有强大常识推理和事实生成能力的模型提供了宝贵的研究机会和切入点。", "translation": "文本到视频（T2V）模型在生成视觉上合理的场景方面表现出色，但其利用世界知识确保语义一致性和事实准确性的能力在很大程度上仍未得到充分研究。针对这一挑战，我们提出了T2VWorldBench，这是第一个系统性的评估框架，用于评估文本到视频模型的“世界知识”生成能力，涵盖了物理、自然、活动、文化、因果关系和物体等广泛领域中的6个主要类别、60个子类别和1200个提示词。为了兼顾人类偏好和可扩展评估，我们的基准结合了人类评估和使用视觉-语言模型（VLMs）的自动化评估。我们评估了目前可用的10个最先进的文本到视频模型，包括开源和商业模型，发现大多数模型无法理解世界知识并生成真正正确的视频。这些发现指出了当前文本到视频模型在利用世界知识能力方面存在的关键差距，为构建具有强大常识推理和事实生成能力的模型提供了宝贵的研究机会和切入点。", "summary": "本文介绍了T2VWorldBench，这是一个新颖的基准，旨在系统地评估文本到视频模型的世界知识生成能力。该基准包含广泛的类别和提示，并结合了人类和自动化评估方法。通过对10个领先T2V模型的评估，研究发现它们在整合世界知识以实现语义一致性和事实准确性方面存在显著不足，这突出表明了未来模型开发中的一个关键研究空白。", "keywords": "文本到视频生成, 世界知识, 基准, 评估, 语义一致性", "comments": "该论文解决了文本到视频生成中一个关键且常被忽视的方面：世界知识的整合和事实准确性。T2VWorldBench作为一个全面的基准，具有多样化的类别和双重评估方法（人类和VLM），具有高度创新性和价值。研究发现当前模型在此方面普遍存在不足，这强调了这项工作的重要性，并为未来构建更智能、更可靠的T2V系统指明了明确的研究方向。"}}
{"id": "2406.16109", "title": "X-ray2CTPA: Leveraging Diffusion Models to Enhance Pulmonary Embolism Classification", "authors": ["Noa Cahan", "Eyal Klang", "Galit Aviram", "Yiftach Barash", "Eli Konen", "Raja Giryes", "Hayit Greenspan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      preprint, project code: this https URL", "url": "http://arxiv.org/abs/2406.16109v4", "summary": "Chest X-rays or chest radiography (CXR), commonly used for medical\ndiagnostics, typically enables limited imaging compared to computed tomography\n(CT) scans, which offer more detailed and accurate three-dimensional data,\nparticularly contrast-enhanced scans like CT Pulmonary Angiography (CTPA).\nHowever, CT scans entail higher costs, greater radiation exposure, and are less\naccessible than CXRs. In this work we explore cross-modal translation from a 2D\nlow contrast-resolution X-ray input to a 3D high contrast and\nspatial-resolution CTPA scan. Driven by recent advances in generative AI, we\nintroduce a novel diffusion-based approach to this task. We evaluate the models\nperformance using both quantitative metrics and qualitative feedback from\nradiologists, ensuring diagnostic relevance of the generated images.\nFurthermore, we employ the synthesized 3D images in a classification framework\nand show improved AUC in a PE categorization task, using the initial CXR input.\nThe proposed method is generalizable and capable of performing additional\ncross-modality translations in medical imaging. It may pave the way for more\naccessible and cost-effective advanced diagnostic tools. The code for this\nproject is available: https://github.com/NoaCahan/X-ray2CTPA .", "comment": "preprint, project code: https://github.com/NoaCahan/X-ray2CTPA", "pdf_url": "http://arxiv.org/pdf/2406.16109v4", "cate": "eess.IV", "date": "2024-06-23", "updated": "2025-07-24", "AI": {"title_translation": "X-ray2CTPA：利用扩散模型增强肺栓塞分类", "tldr": "该研究提出一种基于扩散模型的方法，将2D X射线图像转换为3D CTPA图像，以提高肺栓塞分类的准确性，从而实现更经济便捷的诊断。", "motivation": "胸部X射线（CXR）成像能力有限，而CT扫描（尤其是CTPA）虽然能提供更详细的三维数据，但成本高、辐射大且不易获取。因此，需要一种方法来弥补CXR的不足并降低CTPA的门槛。", "method": "本研究探索了一种跨模态翻译方法，将低对比度分辨率的2D X射线输入转换为高对比度和空间分辨率的3D CTPA扫描。该方法引入了一种新颖的、基于扩散模型的方法，并使用定量指标和放射科医生的定性反馈来评估模型性能。", "result": "生成的3D图像在分类框架中用于肺栓塞（PE）分类任务，结果显示使用初始CXR输入时AUC有所提高。该方法具有通用性，能够执行医学成像中的其他跨模态翻译。", "conclusion": "所提出的方法可能为更易于获取且更具成本效益的先进诊断工具铺平道路。", "translation": "胸部X射线或胸部放射（CXR）常用于医学诊断，但与计算机断层扫描（CT）相比，其成像能力通常有限。CT扫描能提供更详细和准确的三维数据，特别是像CT肺动脉造影（CTPA）这样的对比增强扫描。然而，CT扫描的成本更高、辐射暴露更大，并且不如CXR易于获取。在这项工作中，我们探索了从2D低对比度分辨率X射线输入到3D高对比度和空间分辨率CTPA扫描的跨模态翻译。在生成式AI最新进展的推动下，我们引入了一种新颖的、基于扩散模型的方法来完成这项任务。我们使用定量指标和放射科医生的定性反馈来评估模型性能，确保生成图像的诊断相关性。此外，我们将合成的3D图像应用于分类框架中，并显示在使用初始CXR输入的情况下，PE分类任务的AUC有所提高。所提出的方法具有通用性，能够执行医学成像中的其他跨模态翻译。它可能为更易于获取和更具成本效益的先进诊断工具铺平道路。该项目的代码可在以下网址获取：https://github.com/NoaCahan/X-ray2CTPA。", "summary": "本研究提出了一种名为X-ray2CTPA的新型扩散模型方法，旨在将低分辨率的2D胸部X射线图像转换为高分辨率的3D CT肺动脉造影（CTPA）图像。该方法旨在克服传统CT扫描成本高、辐射大且不易获取的缺点，同时提升X射线在肺栓塞（PE）分类中的诊断能力。通过将生成的3D图像应用于分类任务，实验结果显示PE分类的AUC有所提高。该方法具有通用性，有望促进更便捷、经济的先进诊断工具发展。", "keywords": "扩散模型, 肺栓塞分类, 跨模态翻译, X射线, CTPA", "comments": "这项研究的创新之处在于利用扩散模型实现2D X射线到3D CTPA的跨模态图像转换，这在医学影像领域是一个重要突破。它解决了CT扫描高成本和高辐射的问题，同时提高了X射线在复杂诊断（如肺栓塞）中的应用价值。该方法的通用性也预示着其在其他医学图像转换任务中的潜力，有望显著提升诊断的可及性和效率。"}}
{"id": "2507.18340", "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning", "authors": ["Yifu Chen", "Bingchen Huang", "Zhiling Wang", "Yuanchao Du", "Junfeng Luo", "Lei Shen", "Zhineng chen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18340v1", "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18340v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TDR：基于任务解耦检索与细粒度LLM反馈的上下文学习", "tldr": "TDR框架通过任务解耦和细粒度LLM反馈提升上下文学习的示例检索质量，并在30个NLP任务上实现SOTA。", "motivation": "当前上下文学习（ICL）的有效性高度依赖于高质量示例，但现有检索方法面临两大挑战：难以区分跨任务数据分布，以及难以建立检索器输出与LLM反馈之间的细粒度连接。", "method": "本文提出TDR（Task-Decoupled Retrieval）框架。TDR通过解耦不同任务的ICL示例，使检索模块能检索特定于目标任务的示例。此外，TDR建模来自LLM的细粒度反馈，以监督和指导检索模块的训练，从而检索高质量示例。", "result": "在30个NLP任务上进行的广泛实验表明，TDR在所有数据集上持续改进了性能，并实现了最先进（SOTA）的性能。同时，TDR是一种即插即用的方法，易于与各种LLM结合。", "conclusion": "TDR通过任务解耦和利用LLM的细粒度反馈，显著提高了上下文学习中示例检索的质量，并实现了SOTA性能，表明其在提升LLM在各种任务上的表现方面具有巨大潜力。", "translation": "上下文学习（ICL）已成为一种经典方法，使大型语言模型（LLM）能够根据少量输入-输出示例处理各种任务。ICL的有效性高度依赖于这些示例的质量，而之前专注于增强示例检索能力的工作已经取得了令人印象深刻的性能。然而，在检索高质量示例方面仍然存在两个挑战：(1) 难以区分跨任务数据分布，(2) 难以在检索器输出和LLM反馈之间建立细粒度连接。在本文中，我们提出了一种名为TDR的新颖框架。TDR解耦了来自不同任务的ICL示例，这使得检索模块能够在多任务数据集中检索特定于目标任务的示例。此外，TDR对来自LLM的细粒度反馈进行建模，以监督和指导检索模块的训练，这有助于检索高质量的示例。我们在30个NLP任务套件上进行了广泛实验，结果表明TDR在所有数据集上持续改进了结果并实现了最先进的性能。同时，我们的方法是一种即插即用的方法，可以轻松与各种LLM结合，以提高ICL的示例检索能力。代码可在https://github.com/Nnn-s/TDR获取。", "summary": "本文提出了TDR框架，旨在解决上下文学习（ICL）中高质量示例检索的两大挑战：跨任务数据分布难以区分和LLM反馈难以细粒度连接。TDR通过解耦不同任务的ICL示例并利用LLM的细粒度反馈来监督检索模块的训练。实验结果表明，TDR在30个NLP任务上持续提升了性能并达到了SOTA水平，且其即插即用的特性使其易于与现有LLM结合。", "keywords": "上下文学习, 示例检索, 任务解耦, LLM反馈, 自然语言处理", "comments": "TDR的创新点在于其将任务解耦与LLM的细粒度反馈机制相结合，有效解决了ICL中示例检索的两个核心难题。任务解耦使得检索更具针对性，而LLM的细粒度反馈则提供了更精确的监督信号，这对于提升检索质量至关重要。其即插即用的特性也增加了其实用性和潜在影响力。"}}
{"id": "2503.17724", "title": "Trigger without Trace: Towards Stealthy Backdoor Attack on Text-to-Image Diffusion Models", "authors": ["Jie Zhang", "Zhongqi Wang", "Shiguang Shan", "Xilin Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17724v2", "summary": "Backdoor attacks targeting text-to-image diffusion models have advanced\nrapidly. However, current backdoor samples often exhibit two key abnormalities\ncompared to benign samples: 1) Semantic Consistency, where backdoor prompts\ntend to generate images with similar semantic content even with significant\ntextual variations to the prompts; 2) Attention Consistency, where the trigger\ninduces consistent structural responses in the cross-attention maps. These\nconsistencies leave detectable traces for defenders, making backdoors easier to\nidentify. In this paper, toward stealthy backdoor samples, we propose Trigger\nwithout Trace (TwT) by explicitly mitigating these consistencies. Specifically,\nour approach leverages syntactic structures as backdoor triggers to amplify the\nsensitivity to textual variations, effectively breaking down the semantic\nconsistency. Besides, a regularization method based on Kernel Maximum Mean\nDiscrepancy (KMMD) is proposed to align the distribution of cross-attention\nresponses between backdoor and benign samples, thereby disrupting attention\nconsistency. Extensive experiments demonstrate that our method achieves a 97.5%\nattack success rate while exhibiting stronger resistance to defenses. It\nachieves an average of over 98% backdoor samples bypassing three\nstate-of-the-art detection mechanisms, revealing the vulnerabilities of current\nbackdoor defense methods. The code is available at\nhttps://github.com/Robin-WZQ/TwT.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17724v2", "cate": "cs.CV", "date": "2025-03-22", "updated": "2025-07-24", "AI": {"title_translation": "无痕触发：面向文生图扩散模型的隐蔽后门攻击", "tldr": "本文提出了一种名为TwT的隐蔽后门攻击方法，通过破坏语义一致性和注意力一致性，使针对文生图扩散模型的后门攻击更难被检测。", "motivation": "当前针对文生图扩散模型的后门攻击样本存在语义一致性和注意力一致性两种可检测的异常，使得防御者能够更容易地识别后门，降低了攻击的隐蔽性。", "method": "本文提出了“无痕触发”（TwT）方法，通过显式地缓解语义一致性和注意力一致性来实现隐蔽攻击。具体而言，该方法利用句法结构作为后门触发器，以放大对文本变化的敏感性，从而打破语义一致性。此外，提出了一种基于核最大均值差异（KMMD）的正则化方法，用于对齐后门样本和良性样本之间的交叉注意力响应分布，从而破坏注意力一致性。", "result": "实验表明，该方法实现了97.5%的攻击成功率，并表现出更强的防御抵抗能力。它使平均超过98%的后门样本绕过了三种最先进的检测机制，揭示了当前后门防御方法的脆弱性。", "conclusion": "本文提出的TwT方法通过解决现有后门攻击的可检测性问题，实现了对文生图扩散模型的隐蔽且高效的后门攻击，并揭示了当前防御机制的不足。", "translation": "针对文生图扩散模型的后门攻击进展迅速。然而，与良性样本相比，当前的后门样本通常表现出两个关键异常：1）语义一致性，即后门提示即使在文本变化很大的情况下，也倾向于生成语义内容相似的图像；2）注意力一致性，即触发器在交叉注意力图中诱导一致的结构响应。这些一致性为防御者留下了可检测的痕迹，使得后门更容易被识别。在本文中，为了实现隐蔽的后门样本，我们提出了“无痕触发”（TwT）方法，通过明确缓解这些一致性。具体而言，我们的方法利用句法结构作为后门触发器，以放大对文本变化的敏感性，有效打破语义一致性。此外，提出了一种基于核最大均值差异（KMMD）的正则化方法，用于对齐后门样本和良性样本之间的交叉注意力响应分布，从而破坏注意力一致性。广泛的实验表明，我们的方法实现了97.5%的攻击成功率，同时表现出更强的防御抵抗能力。它使平均超过98%的后门样本绕过了三种最先进的检测机制，揭示了当前后门防御方法的脆弱性。代码可在https://github.com/Robin-WZQ/TwT 获取。", "summary": "本文提出了一种名为“无痕触发”（TwT）的隐蔽后门攻击方法，专门针对文生图扩散模型。该方法通过利用句法结构作为触发器来破坏语义一致性，并采用基于核最大均值差异（KMMD）的正则化来消除注意力一致性。实验结果显示，TwT实现了高攻击成功率（97.5%），并能有效规避现有先进的后门检测机制（超过98%的样本绕过检测），揭示了当前防御方法的局限性。", "keywords": "后门攻击, 扩散模型, 隐蔽性, 语义一致性, 注意力一致性", "comments": "本文创新性地指出了现有文生图扩散模型后门攻击的可检测性问题，并提出了针对性的解决方案。通过同时解决语义和注意力层面的一致性问题，TwT显著提升了后门攻击的隐蔽性，对当前依赖这些一致性进行检测的防御机制构成了巨大挑战。这项研究凸显了开发更鲁棒的后门防御策略的紧迫性。"}}
{"id": "2507.17987", "title": "Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring", "authors": ["Arsen Yermukan", "Pedro Machado", "Feliciano Domingos", "Isibor Kennedy Ihianle", "Jordan J. Bird", "Stefano S. K. Kaburu", "Samantha J. Ward"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17987v1", "summary": "Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is\ntime-consuming and prone to errors. This project introduces an automated system\nfor real-time video analysis, using You Only Look Once (YOLO) object detection\nmodels to identify two key behaviours: basking and hunting. We trained five\nYOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of\n1200 images, encompassing bearded dragons (600), heating lamps (500), and\ncrickets (100). YOLOv8s was selected as the optimal model due to its superior\nbalance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes\nvideo footage by extracting per-frame object coordinates, applying temporal\ninterpolation for continuity, and using rule-based logic to classify specific\nbehaviours. Basking detection proved reliable. However, hunting detection was\nless accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).\nFuture improvements will focus on enhancing cricket detection through expanded\ndatasets or specialised small-object detectors. This automated system offers a\nscalable solution for monitoring reptile behaviour in controlled environments,\nsignificantly improving research efficiency and data quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17987v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "胡须龙活动识别管线：一种基于AI的行为监测方法", "tldr": "该项目开发了一个基于AI的自动化系统，利用YOLOv8s模型进行胡须龙的行为（晒太阳和捕食）实时视频监测。系统在识别晒太阳行为方面表现可靠，但在捕食行为方面准确性较低，主要受限于蟋蟀检测的弱点。该系统为受控环境下爬行动物行为监测提供了可扩展的解决方案。", "motivation": "传统的胡须龙（Pogona Viticeps）行为监测耗时且容易出错。", "method": "该项目引入了一个用于实时视频分析的自动化系统，使用You Only Look Once (YOLO) 目标检测模型来识别晒太阳和捕食两种关键行为。研究人员在包含1200张图像（600张胡须龙、500张加热灯、100张蟋蟀）的定制公开数据集中训练了五种YOLO变体（v5、v7、v8、v11、v12）。最终选择YOLOv8s作为最佳模型。系统通过提取每帧对象坐标、应用时间插值以确保连续性，并使用基于规则的逻辑来分类特定行为来处理视频片段。", "result": "YOLOv8s因其在准确性（mAP@0.5:0.95 = 0.855）和速度方面的卓越平衡而被选为最佳模型。晒太阳行为的检测被证明是可靠的。然而，捕食行为的检测准确性较低，主要原因是蟋蟀检测较弱（mAP@0.5 = 0.392）。", "conclusion": "该自动化系统为受控环境下爬行动物行为监测提供了一个可扩展的解决方案，显著提高了研究效率和数据质量。未来的改进将侧重于通过扩展数据集或专业的微小物体检测器来增强蟋蟀检测。", "translation": "胡须龙活动识别管线：一种基于AI的行为监测方法\n\n传统的胡须龙（Pogona Viticeps）行为监测耗时且容易出错。本项目引入了一个用于实时视频分析的自动化系统，利用You Only Look Once (YOLO) 目标检测模型来识别两种关键行为：晒太阳和捕食。我们使用一个包含1200张图像（600张胡须龙、500张加热灯、100张蟋蟀）的定制公开数据集，训练了五种YOLO变体（v5、v7、v8、v11、v12）。YOLOv8s因其在准确性（mAP@0.5:0.95 = 0.855）和速度方面的卓越平衡而被选为最佳模型。该系统通过提取每帧对象坐标、应用时间插值以确保连续性，并使用基于规则的逻辑来分类特定行为来处理视频片段。晒太阳行为的检测被证明是可靠的。然而，捕食行为的检测准确性较低，主要原因是蟋蟀检测较弱（mAP@0.5 = 0.392）。未来的改进将侧重于通过扩展数据集或专业的微小物体检测器来增强蟋蟀检测。该自动化系统为受控环境下爬行动物行为监测提供了一个可扩展的解决方案，显著提高了研究效率和数据质量。", "summary": "该项目开发了一个基于AI的自动化系统，通过实时视频分析监测胡须龙的行为，旨在解决传统方法的耗时和易错问题。系统采用YOLOv8s模型，并在包含胡须龙、加热灯和蟋蟀的定制数据集上进行训练，以识别晒太阳和捕食行为。尽管晒太阳行为检测可靠，但捕食行为检测因蟋蟀识别准确率低而受限。该系统为爬行动物行为研究提供了一个可扩展且高效的解决方案，未来的工作将专注于提升蟋蟀检测能力。", "keywords": "胡须龙, 活动识别, YOLO, 行为监测, 目标检测", "comments": "该论文将成熟的AI目标检测技术（YOLO）应用于一个相对新颖的领域——爬行动物行为监测，具有创新性。它解决了动物行为研究中实际存在的效率和数据质量问题，具有重要的应用价值。作者明确指出了当前系统的局限性（蟋蟀检测准确率低），并提出了未来的改进方向，展现了严谨的科学态度。"}}
{"id": "2411.12127", "title": "Fine-Grained Uncertainty Quantification via Collisions", "authors": ["Jesse Friedbaum", "Sudarshan Adiga", "Ravi Tandon"], "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.12127v4", "summary": "We propose a new and intuitive metric for aleatoric uncertainty\nquantification (UQ), the prevalence of class collisions defined as the same\ninput being observed in different classes. We use the rate of class collisions\nto define the collision matrix, a novel and uniquely fine-grained measure of\nuncertainty. For a classification problem involving $K$ classes, the $K\\times\nK$ collision matrix $S$ measures the inherent difficulty in distinguishing\nbetween each pair of classes. We discuss several applications of the collision\nmatrix, establish its fundamental mathematical properties, as well as show its\nrelationship with existing UQ methods, including the Bayes error rate (BER). We\nalso address the new problem of estimating the collision matrix using one-hot\nlabeled data by proposing a series of innovative techniques to estimate $S$.\nFirst, we learn a pair-wise contrastive model which accepts two inputs and\ndetermines if they belong to the same class. We then show that this contrastive\nmodel (which is PAC learnable) can be used to estimate the Gramian matrix of\n$S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions,\n$G$ can be used to uniquely recover $S$, a new result on non-negative matrices\nwhich could be of independent interest. With a method to estimate $S$\nestablished, we demonstrate how this estimate of $S$, in conjunction with the\ncontrastive model, can be used to estimate the posterior class portability\ndistribution of any point. Experimental results are also presented to validate\nour methods of estimating the collision matrix and class posterior\ndistributions on several datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.12127v4", "cate": "cs.LG", "date": "2024-11-18", "updated": "2025-07-24", "AI": {"title_translation": "通过碰撞实现细粒度不确定性量化", "tldr": "本文提出了一种新的细粒度不确定性量化指标——类别碰撞矩阵，并提供了其估计方法和应用，实验验证了其有效性。", "motivation": "现有不确定性量化方法可能不够细致，本文旨在提出一种新的、直观的、细粒度的不确定性量化指标，以更好地衡量不同类别之间的区分难度。", "method": "本文提出通过“类别碰撞”（即相同输入出现在不同类别中）的发生率来定义碰撞矩阵S，作为一种新颖的、细粒度的不确定性度量。为估计S，研究者提出了一系列创新技术：首先训练一个成对对比模型来判断两个输入是否属于同一类别；然后证明该对比模型可用于估计S的Gramian矩阵G=S^TS；最后，在合理假设下，证明G可以唯一恢复S。该方法结合碰撞矩阵和对比模型可用于估计任何点的后验类别可移植性分布。", "result": "本文定义了碰撞矩阵S作为一种新颖且独特的细粒度不确定性度量，并阐述了其基本数学性质以及与现有不确定性量化方法（包括贝叶斯错误率）的关系。此外，提出了一种在合理假设下，通过Gramian矩阵G唯一恢复碰撞矩阵S的新结果。实验结果验证了所提出的碰撞矩阵估计方法和类别后验分布估计方法在多个数据集上的有效性。", "conclusion": "本文提出了一种新的、直观的细粒度不确定性量化指标——类别碰撞矩阵，并开发了从单热标签数据估计该矩阵的创新技术，包括一个关于非负矩阵恢复的新数学结果。实验证明了所提出方法的有效性，为理解和量化类别间固有区分难度提供了新工具。", "translation": "我们提出了一种新的、直观的随机不确定性量化（UQ）指标，即类别碰撞的普遍性，其定义为在不同类别中观察到相同输入。我们使用类别碰撞的发生率来定义碰撞矩阵，这是一种新颖且独特的细粒度不确定性度量。对于涉及K个类别的分类问题，K×K碰撞矩阵S衡量了区分每对类别固有的难度。我们讨论了碰撞矩阵的几种应用，建立了其基本数学性质，并展示了其与现有UQ方法（包括贝叶斯错误率（BER））的关系。我们还通过提出一系列创新技术来估计S，解决了使用单热标签数据估计碰撞矩阵的新问题。首先，我们学习一个成对对比模型，该模型接受两个输入并确定它们是否属于同一类别。然后我们证明这个对比模型（它是PAC可学习的）可以用来估计S的Gramian矩阵，定义为G=S^TS。最后，我们表明在合理假设下，G可以唯一地恢复S，这是关于非负矩阵的一个新结果，可能具有独立的兴趣。随着S的估计方法的建立，我们演示了S的估计值，结合对比模型，如何用于估计任何点的后验类别可移植性分布。实验结果也展示了在多个数据集上验证我们估计碰撞矩阵和类别后验分布的方法。", "summary": "本文提出了一种名为“类别碰撞矩阵”的细粒度随机不确定性量化新指标。该KxK矩阵S量化了分类问题中每对类别间的固有区分难度。论文讨论了S的应用、数学性质及其与现有不确定性量化方法的关系。为解决S的估计问题，作者提出通过训练成对对比模型来估计S的Gramian矩阵G，并证明在合理假设下G可唯一恢复S。最终，结合S的估计和对比模型，可用于估计点的后验类别可移植性分布。实验结果验证了所提方法的有效性。", "keywords": "不确定性量化, 类别碰撞, 碰撞矩阵, 对比学习, 随机不确定性", "comments": "该论文的创新点在于引入了“类别碰撞”这一新颖且直观的概念来量化细粒度不确定性，并提出了“碰撞矩阵”这一独特的度量。更重要的是，它不仅提出了理论概念，还解决了从实际数据（单热标签数据）中估计这一复杂矩阵的难题，通过引入对比学习和关于非负矩阵恢复的新数学结果，展示了强大的理论和实践贡献。这为理解和处理分类任务中固有的类别重叠和不确定性提供了新的视角和工具。"}}
{"id": "2507.17941", "title": "Resnet-conformer network with shared weights and attention mechanism for sound event localization, detection, and distance estimation", "authors": ["Quoc Thinh Vo", "David Han"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      This paper has been submitted as a technical report outlining our approach to Task 3A of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2024 and can be found in DCASE2024 technical reports", "url": "http://arxiv.org/abs/2507.17941v1", "summary": "This technical report outlines our approach to Task 3A of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2024, focusing on Sound\nEvent Localization and Detection (SELD). SELD provides valuable insights by\nestimating sound event localization and detection, aiding in various machine\ncognition tasks such as environmental inference, navigation, and other sound\nlocalization-related applications. This year's challenge evaluates models using\neither audio-only (Track A) or audiovisual (Track B) inputs on annotated\nrecordings of real sound scenes. A notable change this year is the introduction\nof distance estimation, with evaluation metrics adjusted accordingly for a\ncomprehensive assessment. Our submission is for Task A of the Challenge, which\nfocuses on the audio-only track. Our approach utilizes log-mel spectrograms,\nintensity vectors, and employs multiple data augmentations. We proposed an\nEINV2-based [1] network architecture, achieving improved results: an F-score of\n40.2%, Angular Error (DOA) of 17.7 degrees, and Relative Distance Error (RDE)\nof 0.32 on the test set of the Development Dataset [2 ,3].", "comment": "This paper has been submitted as a technical report outlining our\n  approach to Task 3A of the Detection and Classification of Acoustic Scenes\n  and Events (DCASE) 2024 and can be found in DCASE2024 technical reports", "pdf_url": "http://arxiv.org/pdf/2507.17941v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "具有共享权重和注意力机制的Resnet-conformer网络用于声音事件定位、检测和距离估计", "tldr": "本文介绍了一种基于EINV2的Resnet-conformer网络，用于DCASE 2024挑战赛的音频声音事件定位、检测和距离估计任务，并取得了改进的结果。", "motivation": "声音事件定位和检测（SELD）通过估计声音事件的定位和检测，为环境推理、导航和其他声音定位相关的机器认知任务提供了宝贵的见解。DCASE 2024挑战赛引入了距离估计，使得对模型进行更全面的评估成为必要。", "method": "该方法利用对数梅尔频谱图和强度向量，并采用多种数据增强技术。提出了一种基于EINV2的网络架构，即Resnet-conformer网络，具有共享权重和注意力机制。", "result": "在开发数据集的测试集上，该方法取得了40.2%的F-score，17.7度的角度误差（DOA）和0.32的相对距离误差（RDE）。", "conclusion": "Not mentioned in abstract", "translation": "本技术报告概述了我们参加2024年声学场景和事件检测与分类（DCASE）挑战赛任务3A的方法，重点关注声音事件定位和检测（SELD）。SELD通过估计声音事件的定位和检测，提供了宝贵的见解，有助于环境推理、导航和其他声音定位相关的各种机器认知任务。今年的挑战赛使用带注释的真实声学场景录音，通过仅音频（A轨道）或视听（B轨道）输入来评估模型。今年一个显著的变化是引入了距离估计，并相应调整了评估指标以进行全面评估。我们提交的是挑战赛任务A，该任务侧重于仅音频轨道。我们的方法利用对数梅尔频谱图、强度向量，并采用多种数据增强。我们提出了一种基于EINV2 [1] 的网络架构，取得了改进的结果：在开发数据集 [2, 3] 的测试集上，F-score为40.2%，角度误差（DOA）为17.7度，相对距离误差（RDE）为0.32。", "summary": "本文介绍了一种为DCASE 2024挑战赛任务3A（声音事件定位、检测和距离估计）设计的基于EINV2的Resnet-conformer网络。该方法专注于仅音频输入，利用对数梅尔频谱图和强度向量，并结合多种数据增强技术。实验结果显示，在开发数据集的测试集上，F-score达到40.2%，角度误差为17.7度，相对距离误差为0.32，表明其在SELD任务上的有效性。", "keywords": "声音事件定位检测, 距离估计, Resnet-conformer, DCASE 2024, EINV2", "comments": "该论文提交的是DCASE 2024挑战赛，特别关注了今年新增的距离估计任务，这体现了其对最新研究趋势的响应。其提出的基于EINV2的Resnet-conformer网络架构，结合共享权重和注意力机制，是其方法上的创新点。尽管是技术报告，但其给出的具体性能指标（F-score、DOA、RDE）提供了评估其效果的量化依据。该研究对于声音事件定位、检测和距离估计领域的实际应用具有重要意义。"}}
{"id": "2506.14516", "title": "RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge", "authors": ["Kun Ran", "Shuoqi Sun", "Khoi Nguyen Dinh Anh", "Damiano Spina", "Oleg Zendel"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      SIGIR 2025 LiveRAG winning system", "url": "http://arxiv.org/abs/2506.14516v2", "summary": "This paper presents the RMIT--ADM+S winning system in the SIGIR 2025 LiveRAG\nChallenge. Our Generation-Retrieval-Augmented Generation (G-RAG) approach\ngenerates a hypothetical answer that is used during the retrieval phase,\nalongside the original question. G-RAG also incorporates a pointwise large\nlanguage model (LLM)-based re-ranking step prior to final answer generation. We\ndescribe the system architecture and the rationale behind our design choices.\nIn particular, a systematic evaluation using the Grid of Points approach and\nN-way ANOVA enabled a controlled comparison of multiple configurations,\nincluding query variant generation, question decomposition, rank fusion\nstrategies, and prompting techniques for answer generation. The submitted\nsystem achieved the highest Borda score based on the aggregation of Coverage,\nRelatedness, and Quality scores from manual evaluations, ranking first in the\nSIGIR 2025 LiveRAG Challenge.", "comment": "SIGIR 2025 LiveRAG winning system", "pdf_url": "http://arxiv.org/pdf/2506.14516v2", "cate": "cs.IR", "date": "2025-06-17", "updated": "2025-07-23", "AI": {"title_translation": "RMIT-ADM+S 在 SIGIR 2025 LiveRAG 挑战赛中的表现", "tldr": "本文介绍了 RMIT-ADM+S 在 SIGIR 2025 LiveRAG 挑战赛中获胜的系统，该系统采用 G-RAG 方法，结合假设答案生成、检索和基于 LLM 的重排序，最终获得第一名。", "motivation": "本文的动机是参加并赢得 SIGIR 2025 LiveRAG 挑战赛。", "method": "本文提出了一种生成-检索增强生成（G-RAG）方法。该方法在检索阶段利用生成的假设答案和原始问题。G-RAG 还包含一个在最终答案生成之前的基于点式大型语言模型（LLM）的重排序步骤。系统设计选择经过了详细描述。通过使用网格点方法和 N-way ANOVA 进行系统评估，对多种配置进行了受控比较，包括查询变体生成、问题分解、排名融合策略和答案生成的提示技术。", "result": "提交的系统在 SIGIR 2025 LiveRAG 挑战赛中获得了最高的 Borda 分数，该分数基于人工评估的覆盖率、相关性和质量分数的聚合，最终排名第一。", "conclusion": "RMIT-ADM+S 团队在 SIGIR 2025 LiveRAG 挑战赛中凭借其 G-RAG 系统取得了胜利，证明了其方法在检索增强生成任务中的有效性和优越性。", "translation": "本文介绍了 RMIT-ADM+S 在 SIGIR 2025 LiveRAG 挑战赛中获胜的系统。我们的生成-检索增强生成（G-RAG）方法生成一个假设答案，该答案在检索阶段与原始问题一起使用。G-RAG 还结合了在最终答案生成之前的基于点式大型语言模型（LLM）的重排序步骤。我们描述了系统架构和我们设计选择背后的原理。特别是，使用网格点方法和 N-way ANOVA 进行的系统评估，实现了对多种配置的受控比较，包括查询变体生成、问题分解、排名融合策略和答案生成的提示技术。提交的系统根据人工评估的覆盖率、相关性和质量分数的聚合，获得了最高的 Borda 分数，在 SIGIR 2025 LiveRAG 挑战赛中排名第一。", "summary": "本文介绍了 RMIT-ADM+S 团队在 SIGIR 2025 LiveRAG 挑战赛中获胜的 G-RAG 系统。该方法通过生成假设答案辅助检索，并在最终答案生成前进行基于 LLM 的重排序。系统设计经过了 Grid of Points 和 N-way ANOVA 的严格评估，最终在挑战赛中取得了第一名。", "keywords": "G-RAG, LiveRAG 挑战赛, LLM, 检索增强生成, 系统评估", "comments": "这篇论文的创新点在于提出了 G-RAG 方法，它巧妙地将假设答案生成与检索相结合，并辅以 LLM 重排序，提升了检索增强生成的性能。其重要性体现在赢得 SIGIR 2025 LiveRAG 挑战赛，证明了该方法在实际应用中的有效性和竞争力。此外，论文中采用的系统评估方法（Grid of Points 和 N-way ANOVA）也值得关注，它为复杂系统配置的比较提供了严谨的框架。"}}
{"id": "2504.17723", "title": "Statistical Runtime Verification for LLMs via Robustness Estimation", "authors": ["Natan Levy", "Adiel Ashrov", "Guy Katz"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 4 figures", "url": "http://arxiv.org/abs/2504.17723v2", "summary": "Adversarial robustness verification is essential for ensuring the safe\ndeployment of Large Language Models (LLMs) in runtime-critical applications.\nHowever, formal verification techniques remain computationally infeasible for\nmodern LLMs due to their exponential runtime and white-box access requirements.\nThis paper presents a case study adapting and extending the RoMA statistical\nverification framework to assess its feasibility as an online runtime\nrobustness monitor for LLMs in black-box deployment settings. Our adaptation of\nRoMA analyzes confidence score distributions under semantic perturbations to\nprovide quantitative robustness assessments with statistically validated\nbounds. Our empirical validation against formal verification baselines\ndemonstrates that RoMA achieves comparable accuracy (within 1\\% deviation), and\nreduces verification times from hours to minutes. We evaluate this framework\nacross semantic, categorial, and orthographic perturbation domains. Our results\ndemonstrate RoMA's effectiveness for robustness monitoring in operational LLM\ndeployments. These findings point to RoMA as a potentially scalable alternative\nwhen formal methods are infeasible, with promising implications for runtime\nverification in LLM-based systems.", "comment": "20 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2504.17723v2", "cate": "cs.LG", "date": "2025-04-24", "updated": "2025-07-24", "AI": {"title_translation": "针对LLMs的基于鲁棒性估计的统计运行时验证", "tldr": "本文提出了一种名为RoMA的统计验证框架，用于在黑盒部署环境中对大型语言模型（LLMs）进行运行时鲁棒性监控。RoMA在保持高精度的同时，将验证时间从数小时缩短至数分钟，为形式化方法不可行时提供了一种可扩展的替代方案。", "motivation": "在运行时关键应用中安全部署大型语言模型（LLMs）需要进行对抗性鲁棒性验证，但现有的形式化验证技术对于现代LLMs来说由于其指数级运行时和白盒访问要求而计算上不可行。", "method": "本文将RoMA统计验证框架进行改编和扩展，以评估其作为LLMs在黑盒部署设置中在线运行时鲁棒性监视器的可行性。RoMA通过分析语义扰动下的置信度分数分布，提供具有统计验证界限的定量鲁棒性评估。该框架在语义、类别和拼写扰动域中进行了评估。", "result": "RoMA在与形式化验证基线进行经验验证时，实现了可比的准确性（偏差在1%以内），并将验证时间从数小时缩短到数分钟。结果表明RoMA在操作性LLM部署中进行鲁棒性监控是有效的。", "conclusion": "RoMA是一种有效的统计运行时验证框架，可以作为形式化方法不可行时的一种潜在可扩展替代方案，对基于LLM的系统中的运行时验证具有前景。", "translation": "对抗性鲁棒性验证对于确保大型语言模型（LLMs）在运行时关键应用中的安全部署至关重要。然而，由于其指数级的运行时间和白盒访问要求，形式化验证技术对于现代LLMs来说仍然在计算上不可行。本文提出了一个案例研究，改编并扩展了RoMA统计验证框架，以评估其作为LLMs在黑盒部署设置中在线运行时鲁棒性监视器的可行性。我们对RoMA的改编分析了语义扰动下的置信度分数分布，以提供具有统计验证界限的定量鲁棒性评估。我们对形式化验证基线的实证验证表明，RoMA实现了可比的准确性（偏差在1%以内），并将验证时间从数小时缩短到数分钟。我们在语义、类别和拼写扰动域中评估了该框架。我们的结果表明RoMA在操作性LLM部署中进行鲁棒性监控是有效的。这些发现指出，当形式化方法不可行时，RoMA是一种潜在的可扩展替代方案，对基于LLM的系统中的运行时验证具有前景。", "summary": "本文介绍了一种名为RoMA的统计验证框架，旨在解决大型语言模型（LLMs）在运行时关键应用中鲁棒性验证的挑战。鉴于传统形式化验证方法计算成本高且需要白盒访问，RoMA被改编用于在黑盒部署环境中对LLMs进行在线鲁棒性监控。通过分析语义扰动下的置信度分数分布，RoMA能够提供定量且具有统计验证界限的鲁棒性评估。实验结果表明，与形式化验证基线相比，RoMA在保持相似准确性（1%以内偏差）的同时，显著将验证时间从数小时缩短至数分钟。该框架在多种扰动类型下表现出有效性，证明了RoMA作为一种可扩展且高效的替代方案，在形式化方法不可行时，对于LLM系统中的运行时验证具有重要意义。", "keywords": "LLMs, 运行时验证, 鲁棒性估计, 统计验证, 黑盒部署", "comments": "本文的创新之处在于将RoMA统计验证框架应用于LLM的运行时鲁棒性验证，特别是在黑盒部署环境下的应用。其重要性体现在为LLM的实际部署提供了一种高效且可扩展的鲁棒性监控方案，解决了传统形式化验证方法计算成本高昂和白盒访问的局限性。通过显著缩短验证时间，RoMA为LLMs在生产环境中的安全部署提供了实用工具。"}}
{"id": "2507.18514", "title": "On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources", "authors": ["Jiping Luo", "Nikolaos Pappas"], "categories": ["cs.IT", "cs.NI", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted for possible journal publication. A shorter version has been accepted as invited paper at Asilomar 2025", "url": "http://arxiv.org/abs/2507.18514v1", "summary": "This paper investigates the semantics-aware remote estimation of a\nfinite-state Markov chain. We employ the maximum a posteriori (MAP) estimator\nand aim to devise a transmission policy to optimize estimation performance\nsubject to a transmission frequency constraint. We leverage two metrics, namely\nthe Age of Consecutive Error (AoCE) and the Age of Information (AoI), to\nquantify, respectively, the significance of estimation error at the transmitter\nand the predictability of outdated information at the receiver. The optimal\ntransmission problem is formulated as a constrained Markov decision process\n(CMDP) with unbounded costs. We show the existence of an optimal simple mixture\npolicy, which randomly selects between two deterministic switching policies\nwith a fixed probability. Notably, each switching policy triggers a\ntransmission only when the AoCE exceeds a threshold value that depends on both\nthe AoI and the instantaneous estimation error. We further derive sufficient\nconditions under which the switching policy reduces to a simple threshold\npolicy; that is, it admits identical thresholds for all estimation errors.\nLeveraging these results, we develop an efficient structure-aware algorithm,\nInsec-SPI, that computes the optimal policy with reduced computation overhead.\nOur results demonstrate that incorporating both AoI and AoCE yields\nsignificantly improved estimation quality compared to using either metric\nalone.", "comment": "Submitted for possible journal publication. A shorter version has\n  been accepted as invited paper at Asilomar 2025", "pdf_url": "http://arxiv.org/pdf/2507.18514v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "信息时效和语义在马尔可夫源远程估计中的作用", "tldr": "本论文研究了有限状态马尔可夫链的语义感知远程估计，旨在设计一种传输策略以在传输频率约束下优化估计性能。通过引入连续错误年龄（AoCE）和信息年龄（AoI）两个度量，将问题建模为受限马尔可夫决策过程（CMDP），并提出了一种最优的简单混合策略。结果表明，结合AoI和AoCE显著提高了估计质量。", "motivation": "本文旨在研究有限状态马尔可夫链的语义感知远程估计，并设计一种传输策略，以在传输频率约束下优化估计性能。", "method": "采用最大后验（MAP）估计器。引入连续错误年龄（AoCE）和信息年龄（AoI）来量化估计误差的显著性和过时信息的可预测性。将最优传输问题建模为具有无界成本的受限马尔可夫决策过程（CMDP）。证明了存在一种最优的简单混合策略，该策略以固定概率随机选择两种确定性切换策略。每种切换策略仅当AoCE超过一个取决于AoI和瞬时估计误差的阈值时才触发传输。进一步推导了切换策略简化为简单阈值策略的充分条件。基于这些结果，开发了一种高效的结构感知算法Insec-SPI，以降低计算开销来计算最优策略。", "result": "存在一种最优的简单混合策略，该策略以固定概率随机选择两种确定性切换策略。每种切换策略仅当AoCE超过一个取决于AoI和瞬时估计误差的阈值时才触发传输。在特定条件下，切换策略可以简化为简单的阈值策略。结合AoI和AoCE比单独使用任一指标能显著提高估计质量。", "conclusion": "结合信息年龄（AoI）和连续错误年龄（AoCE）两个度量，能够显著提高马尔可夫源远程估计的质量，并且所提出的基于CMDP的混合传输策略是有效的。", "translation": "本论文研究了有限状态马尔可夫链的语义感知远程估计。我们采用最大后验（MAP）估计器，旨在设计一种传输策略以在传输频率约束下优化估计性能。我们利用两个度量，即连续错误年龄（AoCE）和信息年龄（AoI），分别量化发射器处估计误差的显著性和接收器处过时信息的可预测性。最优传输问题被公式化为一个具有无界成本的受限马尔可夫决策过程（CMDP）。我们证明了存在一种最优的简单混合策略，该策略以固定概率随机选择两种确定性切换策略。值得注意的是，每种切换策略仅当AoCE超过一个取决于AoI和瞬时估计误差的阈值时才触发传输。我们进一步推导了切换策略简化为简单阈值策略的充分条件；即，它对所有估计误差都采用相同的阈值。利用这些结果，我们开发了一种高效的结构感知算法Insec-SPI，该算法以降低的计算开销计算最优策略。我们的结果表明，与单独使用任一指标相比，结合AoI和AoCE能显著提高估计质量。", "summary": "本文研究了有限状态马尔可夫链的语义感知远程估计问题，旨在设计一种在传输频率约束下优化估计性能的传输策略。通过引入连续错误年龄（AoCE）和信息年龄（AoI）来量化估计误差和信息过时性，并将问题建模为受限马尔可夫决策过程（CMDP）。研究证明了存在一种最优的简单混合策略，该策略基于AoCE、AoI和瞬时估计误差的阈值来触发传输。此外，还提出了Insec-SPI算法以高效计算最优策略。结果表明，结合AoI和AoCE能显著提升估计质量。", "keywords": "信息年龄, 连续错误年龄, 马尔可夫链, 远程估计, 传输策略", "comments": "本论文的创新点在于将信息年龄（AoI）和连续错误年龄（AoCE）这两个度量结合起来，应用于马尔可夫源的远程估计，并将其建模为受限马尔可夫决策过程。提出的简单混合策略和Insec-SPI算法为解决此类问题提供了有效途径，并且结果证明了结合多维度信息的重要性。"}}
{"id": "2507.16182", "title": "The Impact of Pseudo-Science in Financial Loans Risk Prediction", "authors": ["Bruno Scarone", "Ricardo Baeza-Yates"], "categories": ["cs.CY", "cs.LG"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16182v2", "summary": "We study the societal impact of pseudo-scientific assumptions for predicting\nthe behavior of people in a straightforward application of machine learning to\nrisk prediction in financial lending. This use case also exemplifies the impact\nof survival bias in loan return prediction. We analyze the models in terms of\ntheir accuracy and social cost, showing that the socially optimal model may not\nimply a significant accuracy loss for this downstream task. Our results are\nverified for commonly used learning methods and datasets. Our findings also\nshow that there is a natural dynamic when training models that suffer survival\nbias where accuracy slightly deteriorates, and whose recall and precision\nimproves with time. These results act as an illusion, leading the observer to\nbelieve that the system is getting better, when in fact the model is suffering\nfrom increasingly more unfairness and survival bias.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16182v2", "cate": "cs.CY", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "伪科学在金融贷款风险预测中的影响", "tldr": "研究表明，金融贷款风险预测中伪科学假设和生存偏差会导致模型看似性能提升，实则公平性下降。", "motivation": "研究伪科学假设和生存偏差在金融贷款风险预测中对社会的影响，并探讨如何在保证准确性的同时降低社会成本。", "method": "通过分析机器学习模型在金融贷款风险预测中的准确性和社会成本，并使用常用学习方法和数据集进行验证，探讨了生存偏差对模型表现的影响。", "result": "结果表明，社会最优模型不一定会导致显著的准确性损失。同时，受生存偏差影响的模型在训练过程中，准确率会略微下降，但召回率和精确率会随时间提高，这制造了一种系统正在改善的假象，但实际上模型的公平性和生存偏差问题正在恶化。", "conclusion": "在金融贷款风险预测中，受生存偏差影响的模型会产生性能提升的假象，但实际上会导致模型公平性下降和生存偏差加剧。", "translation": "我们研究了伪科学假设在金融借贷风险预测中对人类行为预测的社会影响，这是机器学习在风险预测中的直接应用。该用例也例证了生存偏差在贷款偿还预测中的影响。我们从准确性和社会成本方面分析了这些模型，结果表明，对于此下游任务，社会最优模型可能不会导致显著的准确性损失。我们的结果已通过常用学习方法和数据集进行了验证。我们的发现还表明，在训练受生存偏差影响的模型时，存在一种自然的动态：准确性会略微下降，但召回率和精确率会随时间提高。这些结果构成了一种错觉，导致观察者认为系统正在变好，而实际上模型正在遭受越来越大的不公平性和生存偏差。", "summary": "本研究探讨了伪科学假设和生存偏差在金融贷款风险预测中的社会影响。通过分析模型的准确性和社会成本，发现社会最优模型不一定会牺牲显著的准确性。研究还揭示了受生存偏差影响的模型在训练过程中，尽管准确率略有下降，但召回率和精确率会提高，从而产生系统改善的假象，实则模型的公平性和偏差问题日益恶化。", "keywords": "伪科学, 生存偏差, 金融贷款, 风险预测, 机器学习", "comments": "本文揭示了在金融贷款风险预测中，由伪科学假设和生存偏差引起的模型性能假象，即模型表面上表现改善，但实际上公平性恶化。这一发现对于理解机器学习模型在实际应用中的潜在风险及其社会影响具有重要意义，提醒开发者和使用者警惕看似良好的指标背后隐藏的偏见和不公平。"}}
{"id": "2507.17834", "title": "Smoothed Analysis of Online Metric Problems", "authors": ["Christian Coester", "Jack Umenberger"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted to ESA 2025, Track S", "url": "http://arxiv.org/abs/2507.17834v1", "summary": "We study three classical online problems -- $k$-server, $k$-taxi, and chasing\nsize $k$ sets -- through a lens of smoothed analysis. Our setting allows\nrequest locations to be adversarial up to small perturbations, interpolating\nbetween worst-case and average-case models. Specifically, we show that if the\nmetric space is contained in a ball in any normed space and requests are drawn\nfrom distributions whose density functions are upper bounded by $1/\\sigma$\ntimes the uniform density over the ball, then all three problems admit\npolylog$(k/\\sigma)$-competitive algorithms. Our approach is simple: it reduces\nsmoothed instances to fully adversarial instances on finite metrics and\nleverages existing algorithms in a black-box manner. We also provide a lower\nbound showing that no algorithm can achieve a competitive ratio\nsub-polylogarithmic in $k/\\sigma$, matching our upper bounds up to the exponent\nof the polylogarithm. In contrast, the best known competitive ratios for these\nproblems in the fully adversarial setting are $2k-1$, $\\infty$ and\n$\\Theta(k^2)$, respectively.", "comment": "Accepted to ESA 2025, Track S", "pdf_url": "http://arxiv.org/pdf/2507.17834v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "在线度量问题的平滑分析", "tldr": "本文通过平滑分析研究了三个经典在线问题（k-服务员、k-出租车、追逐大小k集合），表明在请求位置存在小扰动的情况下，这些问题可以达到polylog(k/σ)的竞争比，显著优于完全对抗性设置下的结果。", "motivation": "为了弥补最坏情况分析和平均情况分析之间的差距，本文引入平滑分析来研究在线度量问题。鉴于这些问题在完全对抗性设置中存在高竞争比（如$2k-1$, $\\infty$, $\\Theta(k^2)$），研究在请求存在小扰动下的性能变得有意义。", "method": "该方法将平滑实例简化为有限度量上的完全对抗性实例，并以黑盒方式利用现有算法。", "result": "在度量空间包含在范数空间的一个球体中，且请求分布的密度函数受到特定约束的条件下，所有三个问题都达到了polylog$(k/\\sigma)$-竞争比算法。同时，本文还提供了匹配的下界，表明无法实现低于polylogarithmic in $k/\\sigma$的竞争比。这些结果显著优于完全对抗性设置下的已知最佳竞争比。", "conclusion": "平滑分析为在线度量问题提供了一个强大的框架，证明了在请求存在小扰动这一更实际的模型下，这些问题的竞争比可以得到显著改善（从多项式/无穷大到多对数），这突出了平滑分析的理论和实践价值。", "translation": "我们通过平滑分析的视角研究了三个经典的在线问题——$k$-服务员问题、$k$-出租车问题和追逐大小为$k$的集合问题。我们的设置允许请求位置在小扰动下是对抗性的，介于最坏情况和平均情况模型之间。具体来说，我们表明，如果度量空间包含在任何范数空间中的一个球体内，并且请求是从密度函数被$1/\\sigma$乘以球体上均匀密度上限限制的分布中抽取的，那么所有这三个问题都允许polylog$(k/\\sigma)$-竞争算法。我们的方法很简单：它将平滑实例简化为有限度量上的完全对抗性实例，并以黑盒方式利用现有算法。我们还提供了一个下界，表明没有算法可以实现低于polylogarithmic in $k/\\sigma$的竞争比，这与我们的上界在polylogarithm的指数上相匹配。相比之下，这些问题在完全对抗性设置中已知的最佳竞争比分别为$2k-1$、$\\infty$和$\\Theta(k^2)$。", "summary": "本文采用平滑分析方法，研究了$k$-服务员、$k$-出租车和追逐大小为$k$的集合这三个经典在线度量问题。研究发现，在请求位置允许小扰动（介于最坏情况和平均情况之间）的设置下，所有这些问题都可实现polylog$(k/\\sigma)$-竞争比算法。作者通过将平滑实例简化为有限度量上的完全对抗性实例，并利用现有算法来达成此目的。研究还提供了匹配的下界，证明了这一竞争比的紧致性，并且与完全对抗性设置下的高竞争比形成了鲜明对比。", "keywords": "平滑分析, 在线算法, k-服务员, 竞争比, 度量问题", "comments": "该论文创新性地将平滑分析应用于经典的在线度量问题，成功地弥合了最坏情况分析和平均情况分析之间的鸿沟。在允许小扰动的更现实模型下，竞争比从多项式甚至无穷大显著改善到多对数级别，这凸显了平滑分析的实际相关性和理论潜力。其黑盒式的归约方法简洁而高效。"}}
{"id": "2507.18452", "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand", "authors": ["Jiaming Zhou", "Hongjie Chen", "Shiwan Zhao", "Jian Kang", "Jie Li", "Enzhi Wang", "Yujie Guo", "Haoqin Sun", "Hui Wang", "Aobo Kong", "Yong Qin", "Xuelong Li"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18452v1", "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18452v1", "cate": "cs.SD", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DIFFA：大型语言扩散模型可以听懂和理解", "tldr": "DIFFA是首个基于扩散的大型音频-语言模型，通过双适配器架构和两阶段训练，在少量数据下实现高效可扩展的语音理解，表现优于自回归基线。", "motivation": "尽管大型语言模型和扩散语言模型在文本和多模态领域取得了显著进展，但它们在音频模态上的应用仍未得到充分探索。", "method": "本文引入了DIFFA，第一个基于扩散的大型音频-语言模型，用于执行口语理解。DIFFA将一个冻结的扩散语言模型与一个轻量级双适配器架构集成，以连接语音理解和自然语言推理。它采用两阶段训练：首先通过ASR目标对齐语义表示；然后通过LLM自动生成的合成音频-字幕对学习指令遵循能力。", "result": "尽管仅在960小时的ASR数据和127小时的合成指令数据上训练，DIFFA在MMSU、MMAU和VoiceBench等主要基准测试上表现出有竞争力的性能，并优于多个自回归开源基线。", "conclusion": "结果揭示了基于扩散的语言模型在高效和可扩展的音频理解方面的潜力，为语音驱动的AI开辟了新方向。", "translation": "大型语言模型（LLMs）最近的进展在文本和多模态领域展示了卓越的能力。与此同时，基于扩散的语言模型作为自回归范式的有前景的替代方案出现，提供了改进的可控性、双向上下文建模和鲁棒的生成。然而，它们在音频模态上的应用仍未得到充分探索。在这项工作中，我们引入了**DIFFA**，第一个旨在执行口语理解的基于扩散的大型音频-语言模型。DIFFA将一个冻结的扩散语言模型与一个轻量级双适配器架构集成，该架构连接了语音理解和自然语言推理。我们采用两阶段训练流程：首先，通过ASR目标对齐语义表示；然后，通过提示LLM自动生成的合成音频-字幕对学习指令遵循能力。尽管仅在960小时的ASR数据和127小时的合成指令数据上训练，DIFFA在MMSU、MMAU和VoiceBench等主要基准测试上表现出有竞争力的性能，超越了几个自回归开源基线。我们的结果揭示了基于扩散的语言模型在高效和可扩展的音频理解方面的潜力，为语音驱动的AI开辟了新方向。我们的代码将在https://github.com/NKU-HLT/DIFFA.git提供。", "summary": "本文提出了DIFFA，首个基于扩散的大型音频-语言模型，专注于口语理解。DIFFA通过结合冻结的扩散语言模型和双适配器架构，实现了语音理解与自然语言推理的桥接。其两阶段训练方法，结合ASR对齐和合成指令学习，使其在有限训练数据下，在多项主流基准测试中超越了现有的自回归模型，展示了扩散模型在高效可扩展音频理解方面的巨大潜力。", "keywords": "扩散模型, 音频-语言模型, 语音理解, 双适配器, 两阶段训练", "comments": "DIFFA的创新之处在于它是首个将扩散模型应用于大型音频-语言理解的尝试，通过其独特的双适配器架构和两阶段训练流程，有效地整合了语音和语言模态。在相对较少的数据量下，其性能超越了自回归基线，这突出了扩散模型在音频理解方面的效率和潜力，为未来语音驱动AI的发展开辟了新的研究方向。"}}
{"id": "2502.04757", "title": "ELITE: Enhanced Language-Image Toxicity Evaluation for Safety", "authors": ["Wonjun Lee", "Doehyeon Lee", "Eugene Choi", "Sangyoon Yu", "Ashkan Yousefpour", "Haon Park", "Bumsub Ham", "Suhyun Kim"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICML 2025. Project page at this https URL", "url": "http://arxiv.org/abs/2502.04757v3", "summary": "Current Vision Language Models (VLMs) remain vulnerable to malicious prompts\nthat induce harmful outputs. Existing safety benchmarks for VLMs primarily rely\non automated evaluation methods, but these methods struggle to detect implicit\nharmful content or produce inaccurate evaluations. Therefore, we found that\nexisting benchmarks have low levels of harmfulness, ambiguous data, and limited\ndiversity in image-text pair combinations. To address these issues, we propose\nthe ELITE benchmark, a high-quality safety evaluation benchmark for VLMs,\nunderpinned by our enhanced evaluation method, the ELITE evaluator. The ELITE\nevaluator explicitly incorporates a toxicity score to accurately assess\nharmfulness in multimodal contexts, where VLMs often provide specific,\nconvincing, but unharmful descriptions of images. We filter out ambiguous and\nlow-quality image-text pairs from existing benchmarks using the ELITE evaluator\nand generate diverse combinations of safe and unsafe image-text pairs. Our\nexperiments demonstrate that the ELITE evaluator achieves superior alignment\nwith human evaluations compared to prior automated methods, and the ELITE\nbenchmark offers enhanced benchmark quality and diversity. By introducing\nELITE, we pave the way for safer, more robust VLMs, contributing essential\ntools for evaluating and mitigating safety risks in real-world applications.", "comment": "ICML 2025. Project page at https://velpegor.github.io/ELITE/", "pdf_url": "http://arxiv.org/pdf/2502.04757v3", "cate": "cs.CV", "date": "2025-02-07", "updated": "2025-07-24", "AI": {"title_translation": "ELITE：增强型语言-图像毒性评估用于安全", "tldr": "本文提出了ELITE基准和评估器，用于解决现有视觉语言模型（VLM）安全评估基准在检测有害内容方面的不足，ELITE在准确性和多样性上优于现有方法。", "motivation": "当前视觉语言模型（VLMs）容易受到恶意提示的攻击，产生有害输出。现有的VLM安全基准主要依赖自动化评估方法，但这些方法难以检测隐性有害内容，评估不准确，且存在有害性水平低、数据模糊和图像-文本对组合多样性有限的问题。", "method": "我们提出了ELITE基准，一个高质量的VLM安全评估基准，并由我们增强的评估方法——ELITE评估器作为基础。ELITE评估器明确地纳入了毒性评分，以准确评估多模态上下文中的有害性。我们使用ELITE评估器从现有基准中过滤掉模糊和低质量的图像-文本对，并生成安全和不安全图像-文本对的多样化组合。", "result": "实验表明，ELITE评估器与人类评估相比，实现了与先前自动化方法更高的对齐度，并且ELITE基准提供了增强的基准质量和多样性。", "conclusion": "通过引入ELITE，我们为更安全、更强大的视觉语言模型铺平了道路，为评估和减轻实际应用中的安全风险贡献了重要工具。", "translation": "当前视觉语言模型（VLMs）仍然容易受到恶意提示的攻击，从而产生有害输出。现有针对VLMs的安全基准主要依赖自动化评估方法，但这些方法难以检测隐性有害内容或产生不准确的评估。因此，我们发现现有基准的有害性水平较低、数据模糊且图像-文本对组合的多样性有限。为了解决这些问题，我们提出了ELITE基准，这是一个高质量的VLM安全评估基准，由我们增强的评估方法——ELITE评估器提供支持。ELITE评估器明确地纳入了毒性评分，以准确评估多模态上下文中的有害性，在这些情境中，VLMs经常提供具体、有说服力但无害的图像描述。我们使用ELITE评估器从现有基准中筛选出模糊和低质量的图像-文本对，并生成安全和不安全图像-文本对的多样化组合。我们的实验表明，与先前的自动化方法相比，ELITE评估器与人类评估实现了卓越的对齐度，并且ELITE基准提供了增强的基准质量和多样性。通过引入ELITE，我们为更安全、更强大的VLMs铺平了道路，为评估和减轻实际应用中的安全风险贡献了重要工具。", "summary": "本文针对当前视觉语言模型（VLMs）在处理恶意提示时易产生有害输出的问题，提出了ELITE基准和ELITE评估器。现有安全评估方法存在检测隐性有害内容能力弱、评估不准确、数据质量低和多样性不足等缺陷。ELITE评估器通过引入毒性评分，能更准确地评估多模态上下文中的有害性，并用于筛选现有数据和生成多样化、高质量的图像-文本对。实验证明，ELITE评估器与人类评估的对齐度更高，并且ELITE基准显著提升了评估质量和多样性，为构建更安全的VLM提供了关键工具。", "keywords": "视觉语言模型, 安全评估, 有害内容检测, ELITE, 基准", "comments": "ELITE的创新之处在于其提出的ELITE评估器，通过明确引入毒性评分来量化多模态有害性，并用于优化现有基准的数据质量和多样性。这对于提升VLM的安全性和鲁棒性至关重要，特别是在检测细微和隐性有害内容方面。该研究为VLM的安全评估领域提供了新的、更可靠的工具。"}}
{"id": "2507.17942", "title": "Minimax Data Sanitization with Distortion Constraint and Adversarial Inference", "authors": ["Amirarsalan Moatazedian", "Yauhen Yakimenka", "Rémi A. Chou", "Jörg Kliewer"], "categories": ["cs.IT", "cs.AI", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE ITW 2025", "url": "http://arxiv.org/abs/2507.17942v1", "summary": "We study a privacy-preserving data-sharing setting where a privatizer\ntransforms private data into a sanitized version observed by an authorized\nreconstructor and two unauthorized adversaries, each with access to side\ninformation correlated with the private data.\n  The reconstructor is evaluated under a distortion function, while each\nadversary is evaluated using a separate loss function. The privatizer ensures\nthe reconstructor distortion remains below a fixed threshold while maximizing\nthe minimum loss across the two adversaries. This two-adversary setting models\ncases where individual users cannot reconstruct the data accurately, but their\ncombined side information enables estimation within the distortion threshold.\nThe privatizer maximizes individual loss while permitting accurate\nreconstruction only through collaboration. This echoes secret-sharing\nprinciples, but with lossy rather than perfect recovery. We frame this as a\nconstrained data-driven minimax optimization problem and propose a data-driven\ntraining procedure that alternately updates the privatizer, reconstructor, and\nadversaries. We also analyze the Gaussian and binary cases as special scenarios\nwhere optimal solutions can be obtained. These theoretical optimal results are\nbenchmarks for evaluating the proposed minimax training approach.", "comment": "Accepted to IEEE ITW 2025", "pdf_url": "http://arxiv.org/pdf/2507.17942v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "具有失真约束和对抗性推断的最小最大数据净化", "tldr": "本文研究了一种在存在两个对抗者且每个对抗者都有侧信息的情况下，通过最小最大优化进行隐私保护数据净化的方法。", "motivation": "在隐私保护数据共享设置中，当授权重构器需要准确恢复数据，但同时存在两个非授权对抗者，且他们各自的侧信息可能导致联合推断，从而侵犯隐私。本文旨在解决如何在保证重构器失真低于阈值的同时，最大化两个对抗者的最小损失，从而迫使他们协作才能准确重构数据，以保护隐私。", "method": "将问题建模为受限的数据驱动最小最大优化问题。提出了一种数据驱动的训练过程，该过程交替更新净化器、重构器和对抗者。此外，还分析了高斯和二元情况作为特殊场景，并在这些场景下获得了最优解。", "result": "获得了高斯和二元情况下的理论最优解，这些结果作为评估所提出的最小最大训练方法的基准。", "conclusion": "本文提出的方法在允许通过协作进行准确重构的同时最大化个体损失，这与秘密共享原则相呼应，但实现的是有损而不是完美的恢复。", "translation": "我们研究了一种隐私保护的数据共享设置，其中一个净化器将私人数据转换为一个被授权的重构器和两个未经授权的对抗者观察到的净化版本，每个对抗者都可以访问与私人数据相关的侧信息。重构器根据失真函数进行评估，而每个对抗者使用单独的损失函数进行评估。净化器确保重构器失真保持在固定阈值以下，同时最大化两个对抗者的最小损失。这种双对抗者设置模拟了单个用户无法准确重构数据，但他们的组合侧信息能够在失真阈值内进行估计的情况。净化器最大化个体损失，同时只允许通过协作进行准确重构。这与秘密共享原则相呼应，但实现的是有损而不是完美的恢复。我们将此框定为一个受限的数据驱动最小最大优化问题，并提出了一种数据驱动的训练过程，该过程交替更新净化器、重构器和对抗者。我们还分析了高斯和二元情况作为特殊场景，其中可以获得最优解。这些理论最优结果是评估所提出的最小最大训练方法的基准。", "summary": "本文研究了在存在两个具有侧信息的非授权对抗者的情况下，如何进行隐私保护的数据共享。作者提出了一种最小最大数据净化框架，该框架旨在在保证授权重构器准确性的前提下，最大化两个对抗者的最小推断损失，从而迫使对抗者通过协作才能准确重构数据。该问题被建模为一个受限的数据驱动最小最大优化问题，并提出了一种交替训练算法。研究还分析了高斯和二元情况下的理论最优解，作为评估所提方法的基准。", "keywords": "数据净化, 最小最大优化, 隐私保护, 对抗性推断, 秘密共享", "comments": "本文的创新点在于提出了一个双对抗者的最小最大数据净化模型，并将其与秘密共享原则联系起来，但在有损恢复而非完美恢复的背景下进行。这种方法为在复杂对抗环境下实现隐私保护数据共享提供了一种新颖的视角，尤其是在需要通过协作才能获取完整信息的场景中。"}}
{"id": "2507.18235", "title": "A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain", "authors": ["Leon Herles", "Mario Mally", "Jörg Ostrowski", "Sebastian Schöps", "Melina Merkel"], "categories": ["math.NA", "cs.CE", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      6 pages, 9 figures", "url": "http://arxiv.org/abs/2507.18235v1", "summary": "Simulating electromagnetic fields across broad frequency ranges is\nchallenging due to numerical instabilities at low frequencies. This work\nextends a stabilized two-step formulation of Maxwell's equations to the\ntime-domain. Using a Galerkin discretization in space, we apply two different\ntime-discretization schemes that are tailored to the first- and second-order in\ntime partial differential equations of the two-step solution procedure used\nhere. To address the low-frequency instability, we incorporate a generalized\ntree-cotree gauge that removes the singularity of the curl-curl operator,\nensuring robustness even in the static limit. Numerical results on academic and\napplication-oriented 3D problems confirm stability, accuracy, and the method's\napplicability to nonlinear, temperature-dependent materials.", "comment": "6 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.18235v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "麦克斯韦方程组时域稳定两步公式", "tldr": "该研究提出了一种稳定的麦克斯韦方程组时域两步公式，通过引入广义树-余树规范来解决低频不稳定性问题，并在数值模拟中验证了其稳定性、准确性和对复杂材料的适用性。", "motivation": "在宽频率范围内模拟电磁场时，由于低频下的数值不稳定性，模拟具有挑战性。", "method": "本文将麦克斯韦方程组的稳定两步公式扩展到时域。在空间上使用伽辽金离散化，并应用两种不同的时间离散化方案，这些方案针对所使用的两步求解过程中的一阶和二阶时间偏微分方程进行定制。为了解决低频不稳定性，引入了广义树-余树规范，消除了旋度-旋度算子的奇点。", "result": "在学术和应用导向的3D问题上的数值结果证实了该方法的稳定性、准确性以及对非线性、温度依赖性材料的适用性。", "conclusion": "该研究提出的麦克斯韦方程组时域稳定两步公式通过引入广义树-余树规范，成功解决了低频不稳定性问题，并在各种复杂场景下表现出良好的稳定性、准确性和适用性。", "translation": "模拟宽频率范围内的电磁场由于低频下的数值不稳定性而具有挑战性。这项工作将麦克斯韦方程组的稳定两步公式扩展到时域。在空间上使用伽辽金离散化，我们应用了两种不同的时间离散化方案，这些方案针对此处使用的两步求解过程中的一阶和二阶时间偏微分方程进行定制。为了解决低频不稳定性，我们引入了广义树-余树规范，消除了旋度-旋度算子的奇点，确保即使在静态极限下也具有鲁棒性。在学术和应用导向的3D问题上的数值结果证实了稳定性、准确性以及该方法对非线性、温度依赖性材料的适用性。", "summary": "本文提出了一种麦克斯韦方程组的时域稳定两步公式，旨在解决电磁场模拟中常见的低频数值不稳定性问题。通过采用伽辽金空间离散化和定制的时间离散化方案，并引入广义树-余树规范以消除旋度-旋度算子的奇点，该方法即使在静态极限下也能保持鲁棒性。数值实验证明了其在三维问题上的稳定性、准确性以及对非线性、温度依赖性材料的有效性。", "keywords": "麦克斯韦方程组, 时域模拟, 低频不稳定性, 稳定公式, 伽辽金离散", "comments": "这项工作创新性地将麦克斯韦方程组的稳定两步公式扩展到时域，并引入了广义树-余树规范来有效解决长期存在的低频不稳定性问题。这对于需要宽频率范围电磁场模拟的工程和物理应用具有重要意义，特别是其对非线性、温度依赖性材料的适用性进一步拓宽了其应用前景。"}}
{"id": "2507.18457", "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols", "authors": ["Luo Cheng", "Hanwei Zhang", "Lijun Zhang", "Holger Hermanns"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18457v1", "summary": "Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18457v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "重新审视针对激光雷达检测的物理可实现对抗性物体攻击：澄清问题表述和实验协议", "tldr": "本文提出了一个设备无关的标准化框架，用于物理对抗性物体攻击，以解决现有研究中物理可实现性差和可复现性低的问题，并通过仿真和真实世界验证了其有效性。", "motivation": "激光雷达三维物体检测的对抗性鲁棒性是一个关键研究领域。现有的数字攻击缺乏物理可实现性，而物理对抗性物体攻击由于设置不一致和硬件差异，探索不足且可复现性差。", "method": "本文提出了一个设备无关的标准化框架，该框架抽象了物理对抗性物体攻击的关键要素，支持多种方法，并提供开源代码和仿真及真实世界中的基准测试协议。通过该框架，实现了模拟攻击到物理激光雷达系统的成功迁移。", "result": "该框架实现了公平比较，加速了研究，并通过将模拟攻击成功转移到物理激光雷达系统进行了验证。此外，本文还提供了影响攻击成功因素的见解，并加深了对真实世界激光雷达感知中对抗性鲁棒性的理解。", "conclusion": "本文提出的标准化框架有效地解决了物理对抗性物体攻击的可复现性问题，并为该领域的进一步研究提供了统一的平台和宝贵的见解。", "translation": "激光雷达三维物体检测的对抗性鲁棒性是一个关键研究领域，因为它在现实世界场景中有着广泛的应用。虽然许多数字攻击操纵点云或网格，但它们通常缺乏物理可实现性，限制了其实际影响。物理对抗性物体攻击仍未得到充分探索，并且由于设置不一致和硬件差异而导致可复现性差。为了解决这个问题，我们提出了一个设备无关的标准化框架，该框架抽象了物理对抗性物体攻击的关键要素，支持多种方法，并提供开源代码以及仿真和真实世界环境中的基准测试协议。我们的框架实现了公平比较，加速了研究，并通过将模拟攻击成功转移到物理激光雷达系统进行了验证。除了该框架，我们还提供了关于影响攻击成功因素的见解，并加深了对真实世界激光雷达感知中对抗性鲁棒性的理解。", "summary": "本文旨在解决激光雷达三维物体检测中物理对抗性攻击的可复现性差和缺乏标准化的问题。为此，提出了一种设备无关的标准化框架，该框架抽象了攻击要素，支持多样化方法，并提供了开源工具和基准协议。该框架通过成功将模拟攻击转移到真实激光雷达系统进行验证，并为理解攻击成功因素和提高对抗性鲁棒性提供了新见解。", "keywords": "激光雷达, 对抗性攻击, 物理可实现性, 鲁棒性, 标准化框架", "comments": "本文的创新点在于提出了一个标准化的、设备无关的框架，显著提高了物理对抗性物体攻击研究的可复现性。这对于推动该领域的实际应用和加速研究进程至关重要。其将模拟攻击成功转移到真实物理系统的能力，也证明了其方法的有效性和实用性。"}}
{"id": "2503.01424", "title": "From Hypothesis to Publication: A Comprehensive Survey of AI-Driven Research Support Systems", "authors": ["Zekun Zhou", "Xiaocheng Feng", "Lei Huang", "Xiachong Feng", "Ziyun Song", "Ruihan Chen", "Liang Zhao", "Weitao Ma", "Yuxuan Gu", "Baoxin Wang", "Dayong Wu", "Guoping Hu", "Ting Liu", "Bing Qin"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01424v3", "summary": "Research is a fundamental process driving the advancement of human\ncivilization, yet it demands substantial time and effort from researchers. In\nrecent years, the rapid development of artificial intelligence (AI)\ntechnologies has inspired researchers to explore how AI can accelerate and\nenhance research. To monitor relevant advancements, this paper presents a\nsystematic review of the progress in this domain. Specifically, we organize the\nrelevant studies into three main categories: hypothesis formulation, hypothesis\nvalidation, and manuscript publication. Hypothesis formulation involves\nknowledge synthesis and hypothesis generation. Hypothesis validation includes\nthe verification of scientific claims, theorem proving, and experiment\nvalidation. Manuscript publication encompasses manuscript writing and the peer\nreview process. Furthermore, we identify and discuss the current challenges\nfaced in these areas, as well as potential future directions for research.\nFinally, we also offer a comprehensive overview of existing benchmarks and\ntools across various domains that support the integration of AI into the\nresearch process. We hope this paper serves as an introduction for beginners\nand fosters future research. Resources have been made publicly available at\nhttps://github.com/zkzhou126/AI-for-Research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01424v3", "cate": "cs.AI", "date": "2025-03-03", "updated": "2025-07-24", "AI": {"title_translation": "从假设到出版：AI驱动的研究支持系统综合调查", "tldr": "本文对AI在研究过程中的应用进行了系统性回顾，将其分为假设提出、假设验证和手稿发表三个阶段，并讨论了挑战、未来方向及现有工具。", "motivation": "研究是推动人类文明进步的基础过程，但耗时耗力。近年来，人工智能（AI）的快速发展启发研究人员探索如何利用AI加速和增强研究，因此需要对相关进展进行监测。", "method": "本文对AI驱动的研究支持系统进行了系统性回顾，将相关研究分为假设提出（包括知识合成和假设生成）、假设验证（包括科学主张验证、定理证明和实验验证）和手稿发表（包括手稿撰写和同行评审过程）三个主要类别。", "result": "文章详细阐述了AI在研究过程三个阶段中的应用范畴，识别并讨论了当前面临的挑战以及未来潜在的研究方向。此外，还全面概述了支持AI融入研究过程的现有基准和工具。", "conclusion": "本文旨在为初学者提供入门指导，并促进未来的相关研究。", "translation": "研究是推动人类文明进步的基础过程，但它要求研究人员投入大量时间和精力。近年来，人工智能（AI）技术的快速发展启发研究人员探索AI如何加速和增强研究。为了监测相关进展，本文对该领域的进展进行了系统性回顾。具体而言，我们将相关研究分为三个主要类别：假设提出、假设验证和手稿发表。假设提出涉及知识合成和假设生成。假设验证包括科学主张的验证、定理证明和实验验证。手稿发表涵盖手稿撰写和同行评审过程。此外，我们识别并讨论了这些领域当前面临的挑战以及潜在的未来研究方向。最后，我们还全面概述了支持AI融入研究过程的现有基准和各种领域的工具。我们希望本文能为初学者提供入门指导并促进未来的研究。相关资源已在https://github.com/zkzhou126/AI-for-Research 公开。", "summary": "本文对AI驱动的研究支持系统进行了全面且系统性的回顾。作者将研究过程分为假设提出、假设验证和手稿发表三大阶段，并详细探讨了AI在每个阶段的应用、当前挑战、未来方向以及现有基准和工具。旨在为该领域的初学者提供指导并激发未来的研究。", "keywords": "AI, 研究支持系统, 系统回顾, 假设生成, 论文发表", "comments": "该论文通过系统性回顾，全面梳理了AI在科研全流程中的应用，从假设生成到论文发表，具有很高的概括性和指导意义。其创新之处在于将复杂的科研过程结构化，并细致分析了AI介入的每个环节，同时指出了挑战和未来方向，对AI与科研交叉领域的研究者具有重要的参考价值。资源的公开也体现了其开放性。"}}
{"id": "2507.08621", "title": "A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1", "authors": ["Marcin Pietroń", "Rafał Olszowski", "Jakub Gomułka", "Filip Gampel", "Andrzej Tomski"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08621v2", "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08621v2", "cate": "cs.CL", "date": "2025-07-11", "updated": "2025-07-24", "AI": {"title_translation": "基于LLM的论证分类综合研究：从LLAMA到GPT-4o再到Deepseek-R1", "tldr": "本研究对多种大型语言模型（LLM）在论证分类任务上的表现进行了全面评估，发现GPT-4o表现最佳，而结合推理能力的Deepseek-R1表现突出，但也指出LLM在论证分析中仍存在错误和局限性。", "motivation": "尽管大型语言模型（LLM）显著提升了论证挖掘的效率，但目前仍缺乏关于这些模型在公开论证分类数据库中表现的深入研究和结果。", "method": "本研究选取了GPT、Llama和DeepSeek等多种LLM模型，并在Args.me和UKP等不同数据集上进行了测试。同时，还评估了结合了思维链（Chain-of-Thoughts）算法的推理增强型变体。", "result": "结果表明，ChatGPT-4o在论证分类基准测试中表现优于其他模型。在结合了推理能力的模型中，Deepseek-R1展现出其优越性。然而，尽管表现出色，GPT-4o和Deepseek-R1仍然会犯错，论文讨论了所有模型最常见的错误。", "conclusion": "本研究首次对使用LLM和提示算法在所述数据集上进行了更广泛的分析，揭示了已知提示算法在论证分析中的一些弱点，并指出了改进方向。工作还深入分析了现有论证数据集的缺点。", "translation": "论证挖掘（AM）是一个跨学科研究领域，整合了逻辑学、哲学、语言学、修辞学、法律、心理学和计算机科学的见解。它涉及自动识别和提取论证组成部分，如前提和主张，以及检测它们之间的关系，如支持、攻击或中立。最近，该领域取得了显著进展，特别是随着大型语言模型（LLM）的出现，与传统方法和其他深度学习模型相比，LLM提升了分析和提取论证语义的效率。尽管有许多基准用于测试和验证LLM的质量，但对于这些模型在公开论证分类数据库中的操作仍缺乏研究和结果。本文对选定的一些LLM进行了研究，使用了Args.me和UKP等多样化数据集。测试的模型包括GPT、Llama和DeepSeek的各种版本，以及结合了思维链算法的推理增强型变体。结果表明，ChatGPT-4o在论证分类基准测试中表现优于其他模型。在结合了推理能力的模型中，Deepseek-R1展现出其优越性。然而，尽管表现出色，GPT-4o和Deepseek-R1仍然会犯错。本文讨论了所有模型最常见的错误。据我们所知，本研究是首次使用LLM和提示算法对所述数据集进行更广泛的分析。这项工作还揭示了已知提示算法在论证分析中的一些弱点，同时指出了它们的改进方向。这项工作的附加价值在于对现有论证数据集的深入分析以及对其缺点的展示。", "summary": "本研究对包括GPT、Llama和DeepSeek在内的多种大型语言模型（LLM）在论证分类任务中的表现进行了全面评估，使用了Args.me和UKP等数据集。研究发现，ChatGPT-4o在论证分类基准测试中表现最佳，而结合了思维链算法的Deepseek-R1在推理能力方面表现突出。尽管LLM表现优异，但仍存在常见错误，本研究还分析了现有论证数据集的局限性，并为提示算法的改进提供了方向。", "keywords": "论证分类, 大型语言模型, GPT-4o, Deepseek-R1, 思维链", "comments": "本研究首次对多种主流LLM在论证分类任务上的表现进行了全面的基准测试，特别是在公开数据集上的表现，填补了现有研究的空白。其创新之处在于不仅评估了基础LLM，还探讨了结合思维链等推理增强算法的效果。研究结果为选择合适的LLM进行论证分类提供了实证依据，并指出了当前LLM和提示算法在论证分析中的局限性，为未来的研究指明了方向。对现有数据集缺点的分析也具有重要价值。"}}
{"id": "2408.10450", "title": "RUMI: Rummaging Using Mutual Information", "authors": ["Sheng Zhong", "Nima Fazeli", "Dmitry Berenson"], "categories": ["cs.RO", "cs.AI", "I.2.9"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 20 figures, accepted by IEEE Transactions on Robotics (T-RO), preprint", "url": "http://arxiv.org/abs/2408.10450v2", "summary": "This paper presents Rummaging Using Mutual Information (RUMI), a method for\nonline generation of robot action sequences to gather information about the\npose of a known movable object in visually-occluded environments. Focusing on\ncontact-rich rummaging, our approach leverages mutual information between the\nobject pose distribution and robot trajectory for action planning. From an\nobserved partial point cloud, RUMI deduces the compatible object pose\ndistribution and approximates the mutual information of it with workspace\noccupancy in real time. Based on this, we develop an information gain cost\nfunction and a reachability cost function to keep the object within the robot's\nreach. These are integrated into a model predictive control (MPC) framework\nwith a stochastic dynamics model, updating the pose distribution in a closed\nloop. Key contributions include a new belief framework for object pose\nestimation, an efficient information gain computation strategy, and a robust\nMPC-based control scheme. RUMI demonstrates superior performance in both\nsimulated and real tasks compared to baseline methods.", "comment": "20 pages, 20 figures, accepted by IEEE Transactions on Robotics\n  (T-RO), preprint", "pdf_url": "http://arxiv.org/pdf/2408.10450v2", "cate": "cs.RO", "date": "2024-08-19", "updated": "2025-07-24", "AI": {"title_translation": "RUMI：利用互信息进行翻找", "tldr": "RUMI是一种基于互信息的机器人行动序列生成方法，用于在视觉遮挡环境中估计已知可移动物体的姿态，并在仿真和实际任务中表现优异。", "motivation": "该论文旨在解决在视觉遮挡环境中，机器人在线生成行动序列以收集关于已知可移动物体姿态信息的问题，尤其侧重于接触丰富的翻找任务。", "method": "RUMI方法利用物体姿态分布与机器人轨迹之间的互信息进行行动规划。它从观察到的部分点云推断兼容的物体姿态分布，并实时近似其与工作空间占据的互信息。在此基础上，开发了信息增益成本函数和可达性成本函数，以使物体保持在机器人的可达范围内。这些函数被整合到一个具有随机动力学模型的模型预测控制（MPC）框架中，以闭环方式更新姿态分布。主要贡献包括一个新的物体姿态估计信念框架、一个高效的信息增益计算策略以及一个鲁棒的基于MPC的控制方案。", "result": "RUMI在模拟和实际任务中都表现出优于基线方法的性能。", "conclusion": "该论文提出了RUMI，一种在视觉遮挡环境中通过在线生成机器人行动序列来收集已知可移动物体姿态信息的方法。该方法利用互信息进行规划，并通过实验证明其在模拟和实际任务中均优于基线方法。", "translation": "本文介绍了Rummaging Using Mutual Information (RUMI)，这是一种在线生成机器人行动序列的方法，用于在视觉遮挡环境中收集关于已知可移动物体姿态的信息。该方法侧重于接触丰富的翻找，利用物体姿态分布和机器人轨迹之间的互信息进行行动规划。RUMI从观察到的部分点云推断出兼容的物体姿态分布，并实时近似其与工作空间占据的互信息。在此基础上，我们开发了一个信息增益成本函数和一个可达性成本函数，以使物体保持在机器人的可达范围内。这些函数被整合到一个具有随机动力学模型的模型预测控制（MPC）框架中，以闭环方式更新姿态分布。主要贡献包括一个新的物体姿态估计信念框架、一个高效的信息增益计算策略以及一个鲁棒的基于MPC的控制方案。RUMI在模拟和实际任务中都表现出优于基线方法的性能。", "summary": "RUMI（Rummaging Using Mutual Information）是一种针对视觉遮挡环境中已知可移动物体姿态估计的机器人行动序列在线生成方法。该方法特别关注接触丰富的翻找任务，通过利用物体姿态分布与机器人轨迹之间的互信息进行行动规划。RUMI能够从部分点云推断物体姿态分布，并实时计算信息增益，结合可达性成本函数，将其整合到模型预测控制（MPC）框架中。实验结果表明，RUMI在模拟和实际任务中均表现出优异的性能。", "keywords": "互信息, 机器人翻找, 物体姿态估计, 模型预测控制, 信息增益", "comments": "该论文的创新之处在于其将互信息原理应用于机器人主动信息收集，尤其是在视觉遮挡和接触丰富的复杂环境中，这对于提高机器人自主探索和操作能力具有重要意义。其提出的信念框架、高效信息增益计算以及鲁棒的MPC控制方案是其核心亮点。"}}
{"id": "2507.18205", "title": "Time for Quiescence: Modelling quiescent behaviour in testing via time-outs in timed automata", "authors": ["Laura Brandán Briones", "Marcus Gerhold", "Petra van den Bos", "Mariëlle Stoelinga"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18205v1", "summary": "Model-based testing (MBT) derives test suites from a behavioural\nspecification of the system under test. In practice, engineers favour simple\nmodels, such as labelled transition systems (LTSs). However, to deal with\nquiescence - the absence of observable output - in practice, a time-out needs\nto be set to conclude observation of quiescence. Timed MBT exists, but it\ntypically relies on the full arsenal of timed automata (TA).\n  We present a lifting operator $\\chi^{\\scriptstyle M}\\!$ that adds timing\nwithout the TA overhead: given an LTS, $\\chi^{\\scriptstyle M}\\!$ introduces a\nsingle clock for a user chosen time bound $M>0$ to declare quiescence. In the\ntimed automaton, the clock is used to model that outputs should happen before\nthe clock reaches value $M$, while quiescence occurs exactly at time $M$. This\nway we provide a formal basis for the industrial practice of choosing a\ntime-out to conclude quiescence. Our contributions are threefold: (1) an\nimplementation conforms under $\\mathbf{ioco}$ if and only if its lifted version\nconforms under timed $\\mathbf{tioco_M}$ (2) applying $\\chi^{\\scriptstyle M}\\!$\nbefore or after the standard $\\mathbf{ioco}$ test-generation algorithm yields\nthe same set of tests, and (3) the lifted TA test suite and the original LTS\ntest suite deliver identical verdicts for every implementation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18205v1", "cate": "cs.FL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "沉寂之时：通过定时自动机中的超时建模测试中的沉寂行为", "tldr": "本文提出一个提升算子$\\\\chi^{\\\\scriptstyle M}\\\\!$，能在不增加定时自动机复杂性的情况下，为基于LTS的模型测试引入超时机制以处理系统沉寂行为，并证明了其与现有测试一致性理论的兼容性。", "motivation": "在实践中，模型驱动测试（MBT）处理系统“沉寂”（即无可见输出）时，需要设置超时来判断。现有定时MBT依赖于复杂的定时自动机（TA），而工程师偏好使用简单的标签转换系统（LTS）。因此，需要一种在不引入TA全部复杂性的前提下，为LTS引入超时以处理沉寂的方法。", "method": "本文提出了一个“提升算子”$\\\\chi^{\\\\scriptstyle M}\\\\!$，该算子在给定标签转换系统（LTS）的基础上，引入一个单一时钟，并设置一个用户选择的时间界限$M>0$来声明沉寂。在生成的定时自动机中，时钟用于建模输出应在时钟达到$M$值之前发生，而沉寂则恰好发生在时间$M$。这为工业实践中通过超时判断沉寂提供了形式化基础。", "result": "本文有三大贡献：(1) 当且仅当一个实现的提升版本在定时$\\\\mathbf{tioco_M}$下符合时，该实现才在$\\\\mathbf{ioco}$下符合；(2) 在标准$\\\\mathbf{ioco}$测试生成算法之前或之后应用$\\\\chi^{\\\\scriptstyle M}\\\\!$会产生相同的测试集；(3) 提升的定时自动机测试套件和原始LTS测试套件对每个实现都提供相同的测试结果。", "conclusion": "本文通过引入提升算子$\\\\chi^{\\\\scriptstyle M}\\\\!$，为基于LTS的模型测试处理沉寂行为提供了一种形式化且实用的方法。该方法避免了定时自动机的全部复杂性，并证明了其与现有测试一致性理论的兼容性，同时确保了测试结果的一致性，从而为工业实践中的超时设置提供了坚实的理论基础。", "translation": "模型驱动测试（MBT）从被测系统的行为规范中派生出测试套件。在实践中，工程师更喜欢简单的模型，例如带标签的转换系统（LTS）。然而，为了在实践中处理沉寂——即可观察输出的缺失——需要设置一个超时来判断沉寂的发生。虽然存在定时MBT，但它通常依赖于定时自动机（TA）的全部功能。 我们提出了一个提升算子$\\\\chi^{\\\\scriptstyle M}\\\\!$，它在不增加TA开销的情况下添加了时间特性：给定一个LTS，$\\\\chi^{\\\\scriptstyle M}\\\\!$引入一个单一时钟，用于用户选择的时间界限$M>0$来声明沉寂。在定时自动机中，该时钟用于建模输出应在时钟达到值$M$之前发生，而沉寂恰好发生在时间$M$。通过这种方式，我们为工业实践中选择超时来判断沉寂提供了形式化基础。我们的贡献有三方面：(1) 当且仅当一个实现的提升版本在定时$\\\\mathbf{tioco_M}$下符合时，该实现才在$\\\\mathbf{ioco}$下符合；(2) 在标准$\\\\mathbf{ioco}$测试生成算法之前或之后应用$\\\\chi^{\\\\scriptstyle M}\\\\!$会产生相同的测试集；(3) 提升的定时自动机测试套件和原始LTS测试套件对每个实现都提供相同的测试结果。", "summary": "本文针对模型驱动测试（MBT）中处理系统沉寂行为（无输出）的挑战，提出了一种名为$\\\\chi^{\\\\scriptstyle M}\\\\!$的提升算子。该算子允许在简单的标签转换系统（LTS）模型中引入超时机制，以形式化地判断沉寂，而无需引入复杂的定时自动机（TA）的全部开销。研究证明，该方法与现有的测试一致性理论（如$\\\\mathbf{ioco}$）兼容，并且无论在测试生成算法的哪个阶段应用，都能产生相同的测试集和测试结果，从而为工业实践中的超时设置提供了坚实的理论基础。", "keywords": "沉寂, 模型驱动测试, 定时自动机, 超时, LTS", "comments": "这篇论文的创新点在于它提出了一种轻量级的方法来处理模型驱动测试中的沉寂行为，避免了使用复杂定时自动机的全部开销。通过引入一个简单的提升算子，它将工业实践中常用的超时机制形式化，并证明了其与现有测试理论的兼容性，这对于实际应用具有重要意义。它在理论和实践之间架起了一座桥梁，使得在简单模型下也能有效地进行定时测试。"}}
{"id": "2507.18561", "title": "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing", "authors": ["Varsha Ramineni", "Hossein A. Rahmani", "Emine Yilmaz", "David Barber"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures", "url": "http://arxiv.org/abs/2507.18561v1", "summary": "As AI becomes prevalent in high-risk domains and decision-making, it is\nessential to test for potential harms and biases. This urgency is reflected by\nthe global emergence of AI regulations that emphasise fairness and adequate\ntesting, with some mandating independent bias audits. However, procuring the\nnecessary data for fairness testing remains a significant challenge.\nParticularly in industry settings, legal and privacy concerns restrict the\ncollection of demographic data required to assess group disparities, and\nauditors face practical and cultural challenges in gaining access to data.\nFurther, internal historical datasets are often insufficiently representative\nto identify real-world biases. This work focuses on evaluating classifier\nfairness when complete datasets including demographics are inaccessible. We\npropose leveraging separate overlapping datasets to construct complete\nsynthetic data that includes demographic information and accurately reflects\nthe underlying relationships between protected attributes and model features.\nWe validate the fidelity of the synthetic data by comparing it to real data,\nand empirically demonstrate that fairness metrics derived from testing on such\nsynthetic data are consistent with those obtained from real data. This work,\ntherefore, offers a path to overcome real-world data scarcity for fairness\ntesting, enabling independent, model-agnostic evaluation of fairness, and\nserving as a viable substitute where real data is limited.", "comment": "9 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18561v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "超越内部数据：构建用于公平性测试的完整数据集", "tldr": "本文提出了一种利用重叠数据集构建合成数据的方法，以解决AI公平性测试中真实数据稀缺的问题。", "motivation": "AI在风险领域应用广泛，但公平性测试数据获取困难，特别是人口统计数据受法律和隐私限制，且内部历史数据代表性不足。", "method": "本文提出利用独立的重叠数据集来构建包含人口统计信息的完整合成数据，这些合成数据能准确反映受保护属性和模型特征之间的潜在关系。", "result": "作者验证了合成数据的保真度，并经验性地证明了从合成数据测试中获得的公平性指标与从真实数据中获得的指标一致。", "conclusion": "该工作为克服公平性测试中真实数据稀缺提供了途径，实现了独立、模型无关的公平性评估，并可在真实数据有限时作为可行替代。", "translation": "随着人工智能在高风险领域和决策制定中的普及，测试其潜在危害和偏见变得至关重要。全球范围内AI法规的出现反映了这种紧迫性，这些法规强调公平性和充分测试，有些甚至强制要求进行独立的偏见审计。然而，获取公平性测试所需的数据仍然是一个重大挑战。特别是在工业环境中，法律和隐私问题限制了评估群体差异所需的人口统计数据的收集，审计师在获取数据方面也面临实际和文化挑战。此外，内部历史数据集通常代表性不足，无法识别真实世界的偏见。这项工作侧重于在无法获取包含人口统计信息的完整数据集时评估分类器的公平性。我们提出利用独立的重叠数据集来构建包含人口统计信息的完整合成数据，这些数据准确反映了受保护属性和模型特征之间的潜在关系。我们通过将合成数据与真实数据进行比较来验证其保真度，并经验性地证明了从这种合成数据测试中得出的公平性指标与从真实数据中获得的指标一致。因此，这项工作为克服公平性测试中真实数据稀缺的问题提供了一条途径，实现了独立、模型无关的公平性评估，并可在真实数据有限时作为可行替代。", "summary": "本文旨在解决AI公平性测试中真实数据（特别是人口统计数据）获取困难的问题。作者提出了一种创新的方法，即利用多个独立的重叠数据集来构建包含人口统计信息的完整合成数据集。研究结果表明，这种合成数据能够准确反映真实数据的特征，并且基于合成数据得出的公平性指标与真实数据一致。该方法为在数据受限情况下进行独立、模型无关的公平性评估提供了可行的解决方案。", "keywords": "公平性测试, 合成数据, 数据稀缺, 人口统计数据, AI偏见", "comments": "这项工作具有重要的实际意义，特别是在AI监管日益严格的背景下。它提出了一种创新的数据合成方法，有效解决了工业界在公平性测试中面临的真实数据获取难题（如隐私、法律限制和数据稀缺）。该方法通过生成反映真实关系的人口统计完整数据集，为独立审计和模型无关的公平性评估提供了可行路径，其创新点在于利用重叠数据集构建合成数据，并验证了其在公平性评估中的有效性。"}}
{"id": "2507.18464", "title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts", "authors": ["Miguel Aspis", "Sebastián A. Cajas Ordónez", "Andrés L. Suárez-Cetrulo", "Ricardo Simón Carbajo"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop", "url": "http://arxiv.org/abs/2507.18464v1", "summary": "Learning from non-stationary data streams subject to concept drift requires\nmodels that can adapt on-the-fly while remaining resource-efficient. Existing\nadaptive ensemble methods often rely on coarse-grained adaptation mechanisms or\nsimple voting schemes that fail to optimally leverage specialized knowledge.\nThis paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture\nthat addresses these limitations through a novel co-training framework.\nDriftMoE features a compact neural router that is co-trained alongside a pool\nof incremental Hoeffding tree experts. The key innovation lies in a symbiotic\nlearning loop that enables expert specialization: the router selects the most\nsuitable expert for prediction, the relevant experts update incrementally with\nthe true label, and the router refines its parameters using a multi-hot\ncorrectness mask that reinforces every accurate expert. This feedback loop\nprovides the router with a clear training signal while accelerating expert\nspecialization. We evaluate DriftMoE's performance across nine state-of-the-art\ndata stream learning benchmarks spanning abrupt, gradual, and real-world drifts\ntesting two distinct configurations: one where experts specialize on data\nregimes (multi-class variant), and another where they focus on single-class\nspecialization (task-based variant). Our results demonstrate that DriftMoE\nachieves competitive results with state-of-the-art stream learning adaptive\nensembles, offering a principled and efficient approach to concept drift\nadaptation. All code, data pipelines, and reproducibility scripts are available\nin our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.", "comment": "Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop", "pdf_url": "http://arxiv.org/pdf/2507.18464v1", "cate": "stat.ML", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DriftMoE：一种处理概念漂移的专家混合方法", "tldr": "DriftMoE是一种在线专家混合架构，通过新颖的协同训练框架和共生学习循环，有效地处理概念漂移，并在非平稳数据流上实现与现有技术相当的性能。", "motivation": "现有自适应集成方法通常依赖粗粒度适应机制或简单投票方案，未能充分利用专业知识，无法在概念漂移环境下优化学习，同时保持资源效率。", "method": "本文提出了DriftMoE，一种在线专家混合（MoE）架构。它包含一个紧凑的神经路由器，与一组增量Hoeffding树专家协同训练。其核心创新在于一个共生学习循环，该循环促进专家专业化：路由器选择最合适的专家进行预测，相关专家用真实标签进行增量更新，路由器使用多热正确性掩码（强化每个准确的专家）来优化其参数。这个反馈循环为路由器提供了清晰的训练信号，并加速了专家专业化。", "result": "DriftMoE在九个最先进的数据流学习基准测试中（涵盖突发性、渐进性和真实世界漂移）进行了评估，测试了两种配置：一种专家专注于数据机制（多类别变体），另一种专注于单类别专业化（基于任务的变体）。结果表明，DriftMoE在处理概念漂移方面达到了与最先进的流学习自适应集成方法相当的竞争性结果。", "conclusion": "DriftMoE提供了一种原则性且高效的方法来适应概念漂移，并在非平稳数据流学习中表现出与现有技术相当的性能。", "translation": "从受概念漂移影响的非平稳数据流中学习需要模型能够即时适应，同时保持资源效率。现有的自适应集成方法通常依赖粗粒度适应机制或简单的投票方案，未能最佳地利用专业知识。本文介绍了DriftMoE，一种在线专家混合（MoE）架构，通过新颖的协同训练框架解决了这些限制。DriftMoE的特点是一个紧凑的神经路由器，与一组增量Hoeffding树专家协同训练。其关键创新在于一个共生学习循环，该循环实现了专家专业化：路由器选择最适合预测的专家，相关专家用真实标签进行增量更新，路由器使用多热正确性掩码（强化每个准确的专家）来优化其参数。这个反馈循环为路由器提供了清晰的训练信号，同时加速了专家专业化。我们在九个最先进的数据流学习基准测试中评估了DriftMoE的性能，这些基准测试涵盖了突发性、渐进性和真实世界漂移，并测试了两种不同的配置：一种是专家专注于数据机制（多类别变体），另一种是专注于单类别专业化（基于任务的变体）。我们的结果表明，DriftMoE在处理概念漂移方面取得了与最先进的流学习自适应集成方法相当的竞争性结果，提供了一种原则性且高效的概念漂移适应方法。所有代码、数据管道和可复现脚本都可在我们的公共GitHub存储库中获取：https://github.com/miguel-ceadar/drift-moe。", "summary": "DriftMoE是一种新颖的在线专家混合（MoE）架构，旨在解决非平稳数据流中的概念漂移问题。它通过一个协同训练框架，将一个神经路由器与增量Hoeffding树专家池相结合。核心创新在于一个共生学习循环，该循环通过路由器选择、专家更新和基于正确性掩码的路由器参数优化，促进了专家的专业化和适应性。实验结果表明，DriftMoE在处理多种概念漂移类型时，能够达到与现有最先进数据流学习方法相当的性能，提供了一种高效且有原则的解决方案。", "keywords": "概念漂移, 专家混合, 数据流学习, 在线学习, 自适应集成", "comments": "DriftMoE的创新之处在于其独特的协同训练框架和共生学习循环，特别是引入了“多热正确性掩码”来强化准确的专家，为路由器提供了清晰的训练信号，从而加速了专家专业化。这种方法有望在需要实时适应和资源效率的动态数据流环境中发挥重要作用。"}}
{"id": "2504.13180", "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "authors": ["Jang Hyun Cho", "Andrea Madotto", "Effrosyni Mavroudi", "Triantafyllos Afouras", "Tushar Nagarajan", "Muhammad Maaz", "Yale Song", "Tengyu Ma", "Shuming Hu", "Suyog Jain", "Miguel Martin", "Huiyu Wang", "Hanoona Rasheed", "Peize Sun", "Po-Yao Huang", "Daniel Bolya", "Nikhila Ravi", "Shashank Jain", "Tammy Stark", "Shane Moon", "Babak Damavandi", "Vivian Lee", "Andrew Westbury", "Salman Khan", "Philipp Krähenbühl", "Piotr Dollár", "Lorenzo Torresani", "Kristen Grauman", "Christoph Feichtenhofer"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2504.13180v3", "summary": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models. https://github.com/facebookresearch/perception_models", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2504.13180v3", "cate": "cs.CV", "date": "2025-04-17", "updated": "2025-07-23", "AI": {"title_translation": "PerceptionLM：用于详细视觉理解的开放获取数据和模型", "tldr": "本文发布了用于详细视觉理解的开放获取数据、模型和基准，以促进透明且可复现的视觉语言模型研究。", "motivation": "许多高性能的视觉语言模型都是闭源的，这使得其数据、设计和训练方法不透明，阻碍了科学进步的衡量。研究界通过从黑盒模型中蒸馏数据来应对，但这牺牲了可衡量的科学进步。", "method": "本文研究在完全开放和可复现的框架中构建感知语言模型（PLM）。分析了不使用专有模型蒸馏的标准训练流程，并探索了大规模合成数据以识别关键数据空白，特别是在详细视频理解方面。为了弥补这些空白，发布了280万个人工标注的细粒度视频问答对和时空定位视频字幕。", "result": "发布了280万个人工标注的细粒度视频问答对和时空定位视频字幕，并推出了PLM-VideoBench，一个用于评估挑战性视频理解任务的套件，侧重于推理视频的“什么”、“在哪里”、“何时”和“如何”。", "conclusion": "通过提供数据、训练配方、代码和模型，使研究工作完全可复现，旨在促进图像和视频理解领域的透明研究。", "translation": "视觉语言模型是计算机视觉研究不可或缺的一部分，然而许多高性能模型仍然是闭源的，这模糊了它们的数据、设计和训练方法。研究界通过使用从黑盒模型中蒸馏数据来标记训练数据来应对，这在取得强大基准结果的同时，却牺牲了可衡量的科学进步。然而，在不知道教师模型及其数据来源的细节的情况下，科学进步仍然难以衡量。在本文中，我们研究在一个完全开放和可复现的框架中构建感知语言模型（PLM），以促进图像和视频理解领域的透明研究。我们分析了不使用专有模型蒸馏的标准训练流程，并探索了大规模合成数据以识别关键数据空白，特别是在详细视频理解方面。为了弥补这些空白，我们发布了280万个人工标注的细粒度视频问答对和时空定位视频字幕。此外，我们推出了PLM-VideoBench，一个用于评估挑战性视频理解任务的套件，侧重于推理视频的“什么”、“在哪里”、“何时”和“如何”。我们通过提供数据、训练配方、代码和模型，使我们的工作完全可复现。https://github.com/facebookresearch/perception_models", "summary": "本文针对视觉语言模型领域闭源模型阻碍科学进步的问题，提出了PerceptionLM项目。该项目旨在通过一个完全开放和可复现的框架，促进图像和视频理解领域的透明研究。作者分析了不依赖专有模型蒸馏的训练流程，并利用大规模合成数据识别了视频理解中的数据空白。为弥补这些空白，论文发布了280万个人工标注的细粒度视频问答对和时空定位视频字幕。此外，还引入了PLM-VideoBench，一个用于评估复杂视频理解任务的新基准。所有数据、训练配方、代码和模型均已开源，确保了研究的可复现性。", "keywords": "视觉语言模型, 开放数据, 视频理解, 感知语言模型, 可复现性", "comments": "本文的创新之处在于其对视觉语言模型领域开放性和可复现性的强调。在许多高性能模型闭源的背景下，PerceptionLM通过提供大规模的开放访问数据、训练配方、代码和模型，为社区提供了一个透明的平台。这对于促进科学进步和更深入地理解模型行为至关重要。新发布的细粒度视频数据集和PLM-VideoBench基准对于推动详细视频理解任务的发展具有重要意义。"}}
{"id": "2507.15765", "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM'25", "url": "http://arxiv.org/abs/2507.15765v1", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "comment": "Accepted by ACM MM'25", "pdf_url": "http://arxiv.org/pdf/2507.15765v1", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "从异质性中学习：通过分布鲁棒优化泛化动态面部表情识别", "tldr": "本文提出了一种名为HDF的新框架，通过时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM）来解决动态面部表情识别（DFER）中由多源数据和个体表情变异引起的样本异质性问题，显著提高了识别准确性和鲁棒性。", "motivation": "现有动态面部表情识别（DFER）方法在多源数据和个体表情变异导致的样本异质性下性能会下降。为了解决这些挑战，需要开发一种能够处理异质性并提高泛化能力的框架。", "method": "本文提出了一种名为异质性感知分布框架（HDF）的新框架。HDF包含两个即插即用模块：1. 时间-频率分布注意力模块（DAM）：通过双分支注意力设计，捕获时间一致性和频率鲁棒性，提高对序列不一致性和视觉风格变化的容忍度。2. 分布感知缩放模块（DSM）：基于梯度敏感度和信息瓶颈原理，动态平衡分类和对比损失，实现更稳定和有区分度的表征学习。", "result": "在DFEW和FERV39k两个广泛使用的数据集上进行了大量实验，结果表明HDF显著提高了识别准确性和鲁棒性。该方法在多样化和不平衡的场景中保持了强大的泛化能力，并取得了优越的加权平均召回率（WAR）和未加权平均召回率（UAR）。", "conclusion": "所提出的异质性感知分布框架（HDF）有效解决了动态面部表情识别（DFER）中的样本异质性问题，通过其创新的模块设计显著提高了识别性能和跨场景的泛化能力。", "translation": "动态面部表情识别（DFER）在情感计算和人机交互中扮演着关键角色。尽管现有方法取得了可比的性能，但它们不可避免地受到多源数据和个体表情变异引起的样本异质性所导致的性能下降。为了解决这些挑战，我们提出了一种新颖的框架，称为异质性感知分布框架（HDF），并设计了两个即插即用模块，以增强时频建模并减轻由难样本引起的优化不平衡。具体而言，时频分布注意力模块（DAM）通过双分支注意力设计捕获时间一致性和频率鲁棒性，提高了对序列不一致性和视觉风格变化的容忍度。然后，基于梯度敏感度和信息瓶颈原理，引入了一个自适应优化模块——分布感知缩放模块（DSM），以动态平衡分类和对比损失，从而实现更稳定和有区分度的表示学习。在两个广泛使用的数据集DFEW和FERV39k上进行的广泛实验表明，HDF显著提高了识别准确性和鲁棒性。我们的方法在多样化和不平衡的场景中实现了卓越的加权平均召回率（WAR）和未加权平均召回率（UAR），同时保持了强大的泛化能力。代码已在https://github.com/QIcita/HDF_DFER发布。", "summary": "本文提出了一种名为异质性感知分布框架（HDF）的新颖方法，旨在解决动态面部表情识别（DFER）中由多源数据和个体变异引起的样本异质性问题。该框架包含两个核心模块：时间-频率分布注意力模块（DAM）用于增强时频建模和处理序列不一致性，以及分布感知缩放模块（DSM）用于动态平衡损失以实现更稳定的表示学习。实验证明，HDF在识别准确性和鲁棒性方面表现出色，并在多样化和不平衡场景中展现出强大的泛化能力。", "keywords": "动态面部表情识别, 异质性学习, 分布鲁棒优化, 注意力机制, 泛化能力", "comments": "本文创新性地提出了HDF框架来应对DFER中的样本异质性问题，其核心在于DAM和DSM两个即插即用模块的设计。DAM通过双分支注意力机制有效融合了时间和频率信息，提升了模型对序列变化和视觉风格偏移的鲁棒性。DSM则通过动态平衡损失，解决了硬样本引起的优化不平衡问题。这种结合分布鲁棒优化思想的方法，在提高识别准确性的同时，显著增强了模型的泛化能力，对于实际应用具有重要意义。代码的开源也促进了研究的复现和发展。"}}
{"id": "2507.17596", "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving", "authors": ["Maciej K. Wozniak", "Lianhang Liu", "Yixi Cai", "Patric Jensfelt"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2507.17596v2", "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2507.17596v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "PRIX：从原始像素学习规划以实现端到端自动驾驶", "tldr": "PRIX是一种高效的纯摄像头端到端自动驾驶模型，直接从原始像素进行规划，实现了SOTA性能，模型更小，推理更快。", "motivation": "当前的端到端自动驾驶模型存在模型尺寸大、依赖昂贵的激光雷达传感器和计算密集型BEV特征表示的问题，这限制了它们的可扩展性，特别是对于仅配备摄像头的量产车型。", "method": "PRIX（Plan from Raw Pixels）是一种新颖高效的端到端驾驶架构，仅使用摄像头数据，无需显式BEV表示和激光雷达。它利用视觉特征提取器与生成式规划头相结合，直接从原始像素输入预测安全轨迹。其核心组件是上下文感知重校准变换器（Context-aware Recalibration Transformer, CaRT），旨在有效增强多级视觉特征以实现更稳健的规划。", "result": "PRIX在NavSim和nuScenes基准测试上取得了最先进的性能，与更大、多模态的扩散规划器能力相当，同时在推理速度和模型尺寸方面显著更高效。", "conclusion": "PRIX是一种实用解决方案，可用于端到端自动驾驶的实际部署，因为它高效、模型尺寸小、仅依赖摄像头，同时保持了最先进的性能。", "translation": "尽管端到端自动驾驶模型显示出有希望的结果，但其实际部署常常受到模型尺寸大、依赖昂贵的激光雷达传感器以及计算密集型BEV特征表示的阻碍。这限制了它们的扩展性，特别是对于仅配备摄像头的量产车型。为了解决这些挑战，我们提出了PRIX（Plan from Raw Pixels）。我们新颖高效的端到端驾驶架构仅使用摄像头数据运行，无需显式BEV表示，并且放弃了对激光雷达的需求。PRIX利用视觉特征提取器与生成式规划头相结合，直接从原始像素输入预测安全轨迹。我们架构的核心组件是上下文感知重校准变换器（Context-aware Recalibration Transformer，CaRT），这是一个旨在有效增强多级视觉特征以实现更稳健规划的新模块。我们通过全面的实验证明，PRIX在NavSim和nuScenes基准测试上取得了最先进的性能，与更大、多模态的扩散规划器能力相当，同时在推理速度和模型尺寸方面显著更高效，使其成为实际部署的实用解决方案。我们的工作是开源的，代码将在https://maxiuw.github.io/prix。", "summary": "PRIX是一种新颖高效的端到端自动驾驶架构，旨在解决现有模型尺寸大、依赖LiDAR和BEV表示的局限性。它仅使用摄像头原始像素输入，通过视觉特征提取器和生成式规划头直接预测轨迹，并引入了上下文感知重校准变换器（CaRT）模块以增强特征。实验证明，PRIX在NavSim和nuScenes基准上实现了最先进的性能，同时在推理速度和模型尺寸上显著优于现有大型模型，为实际部署提供了实用解决方案。", "keywords": "端到端自动驾驶, 原始像素, 纯摄像头, 生成式规划, CaRT", "comments": "该论文通过开发一种高效、纯摄像头的解决方案，解决了端到端自动驾驶实际部署中的关键问题。其创新之处在于无需显式BEV或激光雷达，直接从原始像素进行规划，并引入了CaRT模块以实现稳健规划。其重要性在于使先进的自动驾驶模型更具可扩展性，并适用于量产车型。"}}
{"id": "2507.18354", "title": "Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation", "authors": ["Lexuan Zhu", "Yuxuan Li", "Yuning Ren"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18354v1", "summary": "Deformable convolution can adaptively change the shape of convolution kernel\nby learning offsets to deal with complex shape features. We propose a novel\nplug and play deformable convolutional module that uses attention and\nfeedforward networks to learn offsets, so that the deformable patterns can\ncapture long-distance global features. Compared with previously existing\ndeformable convolutions, the proposed module learns the sub pixel displacement\nfield and adaptively warps the feature maps across all channels rather than\ndirectly deforms the convolution kernel , which is equivalent to a relative\ndeformation of the kernel sampling grids, achieving global feature deformation\nand the decoupling of kernel size and learning network. Considering that the\nfundus blood vessels have globally self similar complex edges, we design a deep\nlearning model for fundus blood vessel segmentation, GDCUnet, based on the\nproposed convolutional module. Empirical evaluations under the same\nconfiguration and unified framework show that GDCUnet has achieved state of the\nart performance on public datasets. Further ablation experiments demonstrated\nthat the proposed deformable convolutional module could more significantly\nlearn the complex features of fundus blood vessels, enhancing the model\nrepresentation and generalization capabilities.The proposed module is similar\nto the interface of conventional convolution, we suggest applying it to more\nmachine vision tasks with complex global self similar features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18354v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于眼底血管分割的全局学习相对偏移可变形卷积模块", "tldr": "提出了一种新的可变形卷积模块，通过学习全局相对偏移来增强特征捕获能力，并在眼底血管分割任务中实现了SOTA性能。", "motivation": "为了解决传统可变形卷积在处理复杂形状特征时，无法有效捕捉长距离全局特征的问题，并且眼底血管具有全局自相似的复杂边缘，需要更强的特征表示能力。", "method": "提出了一种即插即用的可变形卷积模块，该模块利用注意力机制和前馈网络学习子像素位移场，实现特征图跨通道的自适应扭曲，而非直接变形卷积核，从而实现全局特征变形和核尺寸与学习网络的解耦。基于此模块，设计了用于眼底血管分割的深度学习模型GDCUnet。", "result": "在相同配置和统一框架下，GDCUnet在公共数据集上取得了最先进的性能。进一步的消融实验表明，所提出的可变形卷积模块能更显著地学习眼底血管的复杂特征，增强模型的表示和泛化能力。", "conclusion": "所提出的可变形卷积模块通过学习全局相对偏移，有效提升了模型处理复杂特征的能力，并在眼底血管分割任务中达到了SOTA性能。该模块类似传统卷积接口，建议将其应用于更多具有复杂全局自相似特征的机器视觉任务。", "translation": "可变形卷积通过学习偏移来适应性地改变卷积核的形状，以处理复杂的形状特征。我们提出了一种新颖的即插即用可变形卷积模块，该模块使用注意力和前馈网络来学习偏移，从而使可变形模式能够捕获长距离全局特征。与现有可变形卷积相比，所提出的模块学习子像素位移场并自适应地扭曲所有通道的特征图，而不是直接变形卷积核，这相当于对核采样网格进行相对变形，实现了全局特征变形以及核尺寸和学习网络的解耦。考虑到眼底血管具有全局自相似的复杂边缘，我们基于所提出的卷积模块设计了用于眼底血管分割的深度学习模型GDCUnet。在相同配置和统一框架下的实证评估表明，GDCUnet在公共数据集上取得了最先进的性能。进一步的消融实验证明，所提出的可变形卷积模块可以更显著地学习眼底血管的复杂特征，增强模型的表示和泛化能力。所提出的模块与传统卷积接口相似，我们建议将其应用于更多具有复杂全局自相似特征的机器视觉任务。", "summary": "本文提出了一种新颖的即插即用可变形卷积模块，通过注意力机制和前馈网络学习全局相对偏移，实现特征图的自适应扭曲，以捕获长距离全局特征。该模块实现了全局特征变形和核尺寸与学习网络的解耦。基于此模块，作者设计了GDCUnet用于眼底血管分割，并在公共数据集上取得了最先进的性能，证明了其在处理复杂全局自相似特征方面的有效性和泛化能力。", "keywords": "可变形卷积, 眼底血管分割, 全局偏移, 深度学习, 图像分割", "comments": "该论文的创新点在于提出了一个即插即用的可变形卷积模块，通过学习全局相对偏移而非直接变形卷积核，实现了全局特征变形和核尺寸与学习网络的解耦。这种设计使其能够更好地捕捉长距离全局特征，对于处理具有复杂自相似特征的图像任务，如眼底血管分割，具有重要意义。其SOTA的性能也验证了方法的有效性。"}}
{"id": "2507.18531", "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning", "authors": ["Tianheng Qiu", "Jingchun Gao", "Jingyu Li", "Huiyi Leong", "Xuan Huang", "Xi Wang", "Xiaocheng Zhang", "Kele Xu", "Lan Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18531v1", "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18531v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "IntentVCNet：弥合时空差距以实现面向意图的可控视频字幕生成", "tldr": "本文提出了IntentVCNet，通过结合提示策略和盒子适配器，弥合LVLM在视频字幕中细粒度意图控制的时空差距，实现SOTA性能，并在IntentVC挑战赛中获得亚军。", "motivation": "当前的LVLMs在视频字幕生成中，难以对时间序列进行细粒度的空间控制，存在显著的时空差距，从而阻碍了面向意图的细粒度控制。", "method": "提出了IntentVCNet，从提示和模型两个角度统一LVLM的时空理解知识以弥合时空差距。具体包括：1) 一种提示组合策略，旨在使LLM能够建模用户意图提示与视频序列之间的隐式关系。2) 一个参数高效的盒子适配器，用于增强全局视觉上下文中的对象语义信息，使视觉token具有用户意图的先验信息。", "result": "实验证明，所提出的两种策略的结合能够进一步增强LVLM建模视频序列空间细节的能力，并促进LVLM准确生成受控的面向意图的字幕。该方法在多个开源LVLMs上取得了最先进的结果，并在IntentVC挑战赛中获得亚军。", "conclusion": "IntentVCNet通过其提出的提示组合策略和盒子适配器，成功弥合了LVLMs在面向意图的可控视频字幕生成中的时空差距，显著提升了模型对空间细节的建模能力和字幕生成的准确性，从而实现了对视频内容更精准和个性化的描述。", "translation": "面向意图的可控视频字幕生成旨在基于定制的用户意图，为视频中的特定目标生成有针对性的描述。当前的大型视觉语言模型（LVLMs）已经获得了强大的指令遵循和视觉理解能力。尽管LVLMs分别在空间和时间理解方面表现出熟练性，但它们无法直接响应指令在时间序列中执行细粒度的空间控制。这种显著的时空差距使得在视频中实现细粒度的面向意图的控制变得复杂。为此，我们提出了一种新颖的IntentVCNet，它统一了LVLMs固有的时间与空间理解知识，从提示和模型两个角度弥合时空差距。具体来说，我们首先提出了一种提示组合策略，旨在使LLM能够建模表征用户意图的提示与视频序列之间的隐式关系。然后，我们提出了一种参数高效的盒子适配器，它增强了全局视觉上下文中的对象语义信息，从而使视觉token具有用户意图的先验信息。最终的实验证明，这两种策略的结合可以进一步增强LVLM建模视频序列空间细节的能力，并促进LVLMs准确生成受控的面向意图的字幕。我们提出的方法在几个开源LVLMs上取得了最先进的结果，并在IntentVC挑战赛中获得亚军。我们的代码可在https://github.com/thqiu0419/IntentVCNet上获取。", "summary": "本文提出了IntentVCNet，旨在解决现有大型视觉语言模型（LVLMs）在面向意图的可控视频字幕生成中存在的细粒度时空控制不足的问题。该方法通过引入一种提示组合策略和一个参数高效的盒子适配器，从提示和模型层面弥合了时空差距，增强了LVLMs对视频序列空间细节的建模能力。实验结果表明，IntentVCNet在多个开源LVLMs上取得了最先进的性能，并有助于准确生成符合用户意图的受控字幕。", "keywords": "视频字幕, 意图控制, 时空理解, 大型视觉语言模型, 盒子适配器", "comments": "该论文的创新点在于从提示和模型两个角度提出解决方案来弥合LVLMs在视频字幕中细粒度空间控制的时空差距。通过结合提示策略和盒子适配器，有效提升了模型对用户意图的理解和对视频空间细节的捕捉能力，对于实现更精准、个性化的视频内容描述具有重要意义。"}}
{"id": "2505.05086", "title": "Beyond Low-rank Decomposition: A Shortcut Approach for Efficient On-Device Learning", "authors": ["Le-Trung Nguyen", "Ael Quelennec", "Van-Tam Nguyen", "Enzo Tartaglione"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.05086v2", "summary": "On-device learning has emerged as a promising direction for AI development,\nparticularly because of its potential to reduce latency issues and mitigate\nprivacy risks associated with device-server communication, while improving\nenergy efficiency. Despite these advantages, significant memory and\ncomputational constraints still represent major challenges for its deployment.\nDrawing on previous studies on low-rank decomposition methods that address\nactivation memory bottlenecks in backpropagation, we propose a novel shortcut\napproach as an alternative. Our analysis and experiments demonstrate that our\nmethod can reduce activation memory usage, even up to $120.09\\times$ compared\nto vanilla training, while also reducing overall training FLOPs up to\n$1.86\\times$ when evaluated on traditional benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.05086v2", "cate": "cs.LG", "date": "2025-05-08", "updated": "2025-07-24", "AI": {"title_translation": "超越低秩分解：一种高效设备端学习的捷径方法", "tldr": "本文提出一种新的捷径方法，显著减少设备端学习的激活内存和计算量，以应对其内存和计算限制。", "motivation": "设备端学习虽然在降低延迟、隐私风险和提高能效方面有优势，但其内存和计算限制是部署的主要挑战。", "method": "本文提出一种新颖的捷径方法，作为解决反向传播中激活内存瓶颈的低秩分解方法的替代方案。", "result": "与传统训练相比，该方法可将激活内存使用量减少高达120.09倍，同时将总训练FLOPs减少高达1.86倍。", "conclusion": "该捷径方法能有效降低设备端学习的内存和计算开销，克服了其部署的主要障碍。", "translation": "设备端学习已成为人工智能发展的一个有前景的方向，特别是因为它具有减少延迟问题、缓解设备-服务器通信相关的隐私风险，同时提高能源效率的潜力。尽管有这些优势，显著的内存和计算限制仍然是其部署的主要挑战。借鉴先前关于低秩分解方法解决反向传播中激活内存瓶颈的研究，我们提出了一种新颖的捷径方法作为替代方案。我们的分析和实验表明，与传统训练相比，我们的方法可以减少激活内存使用量高达120.09倍，同时在传统基准测试中，总训练FLOPs也减少高达1.86倍。", "summary": "本文针对设备端学习面临的内存和计算限制问题，提出了一种名为“捷径方法”的新型解决方案。该方法旨在替代传统的低秩分解技术，以优化反向传播中的激活内存使用。实验结果表明，与传统训练相比，该方法在减少激活内存使用量和降低总训练浮点运算（FLOPs）方面表现出显著效果。", "keywords": "设备端学习, 捷径方法, 低秩分解, 内存优化, 计算效率", "comments": "该论文提出了一种创新的“捷径方法”，有效解决了设备端学习中长期存在的内存和计算瓶颈，为在资源受限设备上部署AI模型提供了新的高效途径。其在激活内存和FLOPs上的显著改进具有重要实践意义。"}}
{"id": "2507.17912", "title": "SETOL: A Semi-Empirical Theory of (Deep) Learning", "authors": ["Charles H Martin", "Christopher Hinrichs"], "categories": ["cs.LG", "cond-mat.stat-mech"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      139 pages, 28 figures. Code for experiments available at this https URL", "url": "http://arxiv.org/abs/2507.17912v1", "summary": "We present a SemiEmpirical Theory of Learning (SETOL) that explains the\nremarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We\nprovide a formal explanation of the origin of the fundamental quantities in the\nphenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the\nheavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior\nwork, these metrics have been shown to predict trends in the test accuracies of\npretrained SOTA NN models, importantly, without needing access to either\ntesting or training data. Our SETOL uses techniques from statistical mechanics\nas well as advanced methods from random matrix theory and quantum chemistry.\nThe derivation suggests new mathematical preconditions for ideal learning,\nincluding a new metric, ERG, which is equivalent to applying a single step of\nthe Wilson Exact Renormalization Group. We test the assumptions and predictions\nof SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating\nexcellent agreement with the key theoretical assumptions. For SOTA NN models,\nwe show how to estimate the individual layer qualities of a trained NN by\nsimply computing the empirical spectral density (ESD) of the layer weight\nmatrices and plugging this ESD into our SETOL formulas. Notably, we examine the\nperformance of the HTSR alpha and the SETOL ERG layer quality metrics, and find\nthat they align remarkably well, both on our MLP and on SOTA NNs.", "comment": "139 pages, 28 figures. Code for experiments available at\n  https://github.com/charlesmartin14/SETOL_experiments", "pdf_url": "http://arxiv.org/pdf/2507.17912v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SETOL: (深度)学习的半经验理论", "tldr": "本文提出了SETOL，一个半经验学习理论，解释了最先进神经网络的卓越性能，并提供了重尾自正则化现象学理论中关键量（alpha和alpha-hat）的正式解释，同时引入了新的理想学习度量ERG。", "motivation": "解释最先进神经网络的卓越性能，并为重尾自正则化理论中的关键量提供正式解释。", "method": "SETOL理论结合了统计力学、随机矩阵理论和量子化学的技术。通过计算层权重矩阵的经验谱密度（ESD）并将其代入SETOL公式来估计训练好的神经网络的个体层质量。引入了新的度量ERG，等同于应用Wilson精确重整化群的单一步骤。", "result": "SETOL理论在简单的3层多层感知器（MLP）上验证了其假设和预测，显示出与关键理论假设的良好一致性。对于SOTA神经网络模型，展示了如何通过计算层权重矩阵的经验谱密度来估计个体层质量。发现HTSR的alpha和SETOL的ERG层质量度量在MLP和SOTA神经网络上都表现出显著的一致性。", "conclusion": "SETOL理论成功解释了最先进神经网络的性能，并为重尾自正则化理论中的关键量提供了形式化解释。新的度量ERG与现有度量表现出良好的一致性，证明了该理论在理解理想学习条件方面的潜力。", "translation": "我们提出了一个学习的半经验理论（SETOL），它解释了最先进（SOTA）神经网络（NNs）的卓越性能。我们为重尾自正则化（HTSR）现象学理论中基本量的起源提供了正式的解释：重尾幂律层质量指标alpha和alpha-hat。在之前的工作中，这些指标已被证明可以预测预训练SOTA神经网络模型的测试准确性趋势，重要的是，无需访问测试或训练数据。我们的SETOL利用了统计力学以及随机矩阵理论和量子化学的先进方法。推导提出了理想学习的新的数学先决条件，包括一个新指标ERG，它等同于应用Wilson精确重整化群的单一步骤。我们在一个简单的3层多层感知器（MLP）上测试了SETOL的假设和预测，证明了与关键理论假设的极好一致性。对于SOTA神经网络模型，我们展示了如何通过简单地计算层权重矩阵的经验谱密度（ESD）并将此ESD代入我们的SETOL公式来估计训练好的神经网络的个体层质量。值得注意的是，我们检查了HTSR的alpha和SETOL的ERG层质量指标的性能，发现它们在我们的MLP和SOTA神经网络上都表现出显著的一致性。", "summary": "本文提出了SETOL（学习的半经验理论），旨在解释最先进神经网络的卓越性能。该理论为重尾自正则化（HTSR）现象学理论中的关键量（如alpha和alpha-hat）提供了形式化解释，这些量已被证明无需训练数据即可预测模型性能。SETOL融合了统计力学、随机矩阵理论和量子化学的方法，并引入了新的理想学习度量ERG。实验在一个简单的MLP上验证了SETOL的假设和预测，并展示了如何通过计算层权重矩阵的经验谱密度来评估SOTA神经网络的层质量。研究发现，HTSR的alpha和SETOL的ERG度量在不同模型上均表现出良好的一致性。", "keywords": "半经验理论, 深度学习, 重尾自正则化, 随机矩阵理论, 神经网络性能", "comments": "本文提出了一种新颖的半经验理论SETOL，为深度学习中一些关键现象提供了理论基础，特别是重尾自正则化。其创新点在于结合了统计力学、随机矩阵理论和量子化学等跨学科方法来解释神经网络的性能。引入的新度量ERG为理解理想学习提供了新的视角。该研究的重要性在于，它可能为神经网络的设计和优化提供更深层次的理论指导，尤其是在无需访问训练数据的情况下预测模型性能的能力。"}}
{"id": "2507.18043", "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages. Code: this https URL", "url": "http://arxiv.org/abs/2507.18043v1", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.", "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "pdf_url": "http://arxiv.org/pdf/2507.18043v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GrAInS：基于梯度的归因用于LLM和VLM的推理时引导", "tldr": "GrAInS是一种新的推理时引导方法，它使用基于梯度的归因来识别最有影响力的token，并构建引导向量以在不更新模型权重的情况下微调LLM和VLM的行为，在多个基准测试中优于微调和现有引导方法。", "motivation": "现有的推理时引导方法依赖于固定的全局干预向量，忽略了单个输入token的因果影响，并且未能利用模型logit中信息丰富的梯度，尤其是在多模态设置中。", "method": "GrAInS通过集成梯度（Integrated Gradients）使用对比的、基于梯度的归因来识别对偏好和非偏好输出贡献最大的k个最具影响力的token（正向和负向归因）。然后，这些token被用来构建从不良行为到期望行为的语义转变的方向性引导向量。在推理过程中，GrAInS根据token级别的归因信号调整Transformer层的隐藏激活，并对激活进行归一化以保持表示尺度。", "result": "GrAInS在TruthfulQA上使用Llama-3.1-8B实现了13.22%的准确性提升，使用LLaVA-1.6-7B将MMHal-Bench上的幻觉率从0.624降低到0.514，并将SPA-VL上的对齐胜率提高了8.11%，同时保持了模型的流畅性和通用能力。", "conclusion": "GrAInS提供了一种细粒度、可解释和模块化的模型行为控制方法，无需重新训练或辅助监督，并且在多个任务中持续优于微调和现有的引导基线。", "translation": "推理时引导方法通过在测试时修改内部激活而不更新模型权重，为大型语言模型（LLM）和视觉语言模型（VLM）的微调提供了一种轻量级的替代方案。然而，大多数现有方法依赖于固定的全局干预向量，忽略了单个输入token的因果影响，并且未能利用模型logit中信息丰富的梯度，特别是在视觉和文本输入贡献不均衡的多模态设置中。为了解决这些限制，我们引入了GrAInS，这是一种推理时引导方法，适用于纯语言模型和视觉语言模型以及相关任务。GrAInS通过集成梯度使用对比的、基于梯度的归因来识别前k个最具影响力的token，这些token根据它们对偏好输出与非偏好输出的贡献进行正向和负向归因。然后，这些token被用来构建方向性引导向量，捕捉从不良行为到期望行为的语义转变。在推理过程中，GrAInS在Transformer层根据token级别的归因信号调整隐藏激活，并对激活进行归一化以保持表示尺度。这实现了对模型行为的细粒度、可解释和模块化控制，无需重新训练或辅助监督。在经验上，GrAInS始终优于微调和现有的引导基线：它在使用Llama-3.1-8B的TruthfulQA上实现了13.22%的准确性提升，使用LLaVA-1.6-7B将MMHal-Bench上的幻觉率从0.624降低到0.514，并将SPA-VL上的对齐胜率提高了8.11%，所有这些都同时保持了模型的流畅性和通用能力。", "summary": "GrAInS是一种新颖的推理时引导方法，旨在解决现有LLM和VLM引导方法的局限性，特别是它们对固定全局向量的依赖以及对个体token因果影响的忽视。GrAInS利用集成梯度进行基于梯度的对比归因，识别出对模型输出影响最大的关键token。这些token随后被用于构建引导向量，从而实现从不良到期望行为的语义转变。该方法通过调整Transformer层的隐藏激活并进行归一化来提供细粒度、可解释且模块化的模型控制，无需重新训练。实验结果表明，GrAInS在多个任务中（如TruthfulQA、MMHal-Bench和SPA-VL）显著优于传统的微调和现有的引导基线，同时保持了模型的通用能力和流畅性。", "keywords": "推理时引导, 大型语言模型, 视觉语言模型, 梯度归因, 模型控制", "comments": "GrAInS的创新之处在于其结合了基于梯度的归因（通过集成梯度）来识别关键token，并利用这些token构建细粒度的引导向量，从而在推理时实现对LLM和VLM行为的精确控制。这提供了一种轻量级且高效的替代方案，避免了昂贵的微调过程。其在多模态设置中的应用，以及对单个token因果影响的关注，解决了现有方法的关键限制。实验结果表明其在提高准确性、减少幻觉和改善对齐方面的显著效果，且不牺牲模型的核心能力，这凸显了其重要性和实用价值。"}}
{"id": "2507.18609", "title": "Partial-State DADS Control for Matched Unmodeled Dynamics", "authors": ["Iasson Karafyllis", "Miroslav Krstic"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      28 pages. arXiv admin note: text overlap with arXiv:2311.07938 , arXiv:2402.17222 , arXiv:2410.16691", "url": "http://arxiv.org/abs/2507.18609v1", "summary": "We extend the Deadzone-Adapted Disturbance Suppression (DADS) control to\ntime-invariant systems with dynamic uncertainties that satisfy the matching\ncondition and for which no bounds for the disturbance and the unknown\nparameters are known. This problem is equivalent to partial-state adaptive\nfeedback, where the states modeling the dynamic uncertainty are unmeasured. We\nshow that the DADS controller can bypass small-gain conditions and achieve\nrobust regulation for systems in spite of the fact that the strength of the\ninterconnections has no known bound. Moreover, no gain and state drift arise,\nregardless of the size of the disturbances and unknown parameters. Finally, the\npaper provides the detailed analysis of a control system where the unmeasured\nstate (or the dynamic uncertainty) is infinite-dimensional and described by a\nreaction-diffusion Partial Differential Equation, where the diffusion\ncoefficient and the reaction term are unknown. It is shown that even in the\ninfinite-dimensional case, a DADS controller can be designed and guarantees\nrobust regulation of the plant state.", "comment": "28 pages. arXiv admin note: text overlap with arXiv:2311.07938,\n  arXiv:2402.17222, arXiv:2410.16691", "pdf_url": "http://arxiv.org/pdf/2507.18609v1", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "部分状态DADS控制用于匹配的未建模动力学", "tldr": "本文将DADS控制扩展到具有未知扰动和参数界限的时不变系统，包括无限维系统，实现了鲁棒调节且无增益和状态漂移。", "motivation": "针对扰动和未知参数的界限未知、动态不确定性满足匹配条件的时不变系统，现有控制方法可能存在局限性，例如需要小增益条件。本文旨在扩展DADS控制以解决这类问题，特别是当动态不确定性状态不可测量时。", "method": "本文通过扩展Deadzone-Adapted Disturbance Suppression (DADS) 控制器来处理具有未知扰动和未知参数界限的时不变系统。该方法解决了部分状态自适应反馈问题，其中动态不确定性状态是不可测量的。论文还详细分析了当未测量状态是无限维并由反应扩散偏微分方程描述时的情况。", "result": "DADS控制器能够绕过小增益条件，即使互连强度没有已知界限，也能实现系统的鲁棒调节。此外，无论扰动和未知参数的大小如何，都不会出现增益和状态漂移。即使在无限维情况下，DADS控制器也能被设计并保证植物状态的鲁棒调节。", "conclusion": "DADS控制可以有效地应用于具有未知扰动和参数界限、动态不确定性满足匹配条件的时不变系统，包括无限维系统，从而实现鲁棒调节，且避免了传统方法中可能出现的增益和状态漂移问题。", "translation": "我们将死区自适应扰动抑制（DADS）控制扩展到具有满足匹配条件的动态不确定性且扰动和未知参数的界限未知的时不变系统。这个问题等同于部分状态自适应反馈，其中模拟动态不确定性的状态是未测量的。我们表明，DADS控制器可以绕过小增益条件，即使互连强度没有已知界限，也能实现系统的鲁棒调节。此外，无论扰动和未知参数的大小如何，都不会出现增益和状态漂移。最后，本文详细分析了一个控制系统，其中未测量状态（或动态不确定性）是无限维的，并由一个反应扩散偏微分方程描述，其中扩散系数和反应项是未知的。结果表明，即使在无限维情况下，DADS控制器也能被设计并保证植物状态的鲁棒调节。", "summary": "本文将死区自适应扰动抑制（DADS）控制扩展到具有动态不确定性且扰动和未知参数界限未知的时不变系统，这相当于处理部分状态自适应反馈问题。研究表明，该DADS控制器能够绕过传统小增益条件，实现鲁棒调节，且无增益和状态漂移。更重要的是，它被证明对无限维的未测量动态不确定性（如反应扩散偏微分方程）同样有效，保证了植物状态的鲁棒调节。", "keywords": "DADS控制, 鲁棒调节, 未建模动力学, 部分状态反馈, 无限维系统", "comments": "本文的创新之处在于将DADS控制扩展到处理具有未知扰动和参数界限的系统，并成功避免了传统小增益条件的限制。此外，其对无限维系统的适用性，特别是针对偏微分方程描述的不确定性，显示了该方法的强大泛化能力和重要性，为复杂系统的鲁棒控制提供了新的思路。"}}
{"id": "2507.18343", "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence", "authors": ["Ariana Sahitaj", "Premtim Sahitaj", "Veronika Solopova", "Jiaao Li", "Sebastian Möller", "Vera Schmitt"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      NLP4PI at ACL", "url": "http://arxiv.org/abs/2507.18343v1", "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository.", "comment": "NLP4PI at ACL", "pdf_url": "http://arxiv.org/pdf/2507.18343v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "混合标注用于宣传检测：整合LLM预标注与人类智能", "tldr": "本文提出了一种结合LLM预标注和人类智能的混合标注框架，显著提高了宣传检测任务的标注一致性和效率，并通过知识蒸馏训练小型语言模型进行结构化标注。", "motivation": "社交媒体上的宣传检测面临任务复杂性和高质量标注数据有限的挑战，导致标注一致性和可扩展性不足。", "method": "本文引入了一种结合人类专业知识和大型语言模型（LLM）辅助的新型框架。该方法提出了一个将14种细粒度宣传技术组织成三个更广泛类别的分层分类法。通过LLM辅助的预标注流程，提取宣传性文本片段，生成解释，并分配局部和全局标签。之后进行人类验证，最后通过知识蒸馏，使用LLM生成的高质量数据而非人类标注来微调小型语言模型（SLMs），使其执行结构化标注。", "result": "人类验证研究显示，标注一致性和时间效率均显著提高。通过LLM生成数据训练的SLMs能够进行结构化标注。", "conclusion": "本文的工作有助于开发可扩展且稳健的宣传检测系统，支持透明和负责任的媒体生态系统，符合可持续发展目标16。", "translation": "社交媒体上的宣传检测由于任务复杂性和高质量标注数据的限制仍然充满挑战。本文介绍了一种结合人类专业知识和大型语言模型（LLM）辅助的新型框架，旨在提高标注的一致性和可扩展性。我们提出了一个分层分类法，将14种细粒度的宣传技术组织成三个更广泛的类别，并在HQP数据集上进行了一项人工标注研究，揭示了细粒度标签之间较低的标注者间一致性。我们实现了一个LLM辅助的预标注流程，该流程提取宣传性文本片段，生成简洁的解释，并分配局部标签以及一个全局标签。二次人工验证研究表明，一致性和时间效率均显著提高。在此基础上，我们对小型语言模型（SLMs）进行微调，以执行结构化标注。我们没有使用人类标注进行微调，而是在高质量的LLM生成数据上进行训练，这使得大型模型能够生成这些标注，而小型模型通过知识蒸馏学习生成它们。我们的工作有助于开发可扩展且稳健的宣传检测系统，支持透明和负责任的媒体生态系统，符合可持续发展目标16。代码已在我们的GitHub仓库公开。", "summary": "本文针对社交媒体宣传检测中高质量标注数据匮乏和一致性差的问题，提出了一种结合LLM预标注和人类验证的混合标注框架。该框架通过分层分类法细化宣传技术，利用LLM进行初步标注和解释生成，并经人类二次验证，显著提升了标注效率和一致性。此外，研究还通过知识蒸馏，利用LLM生成的数据训练小型语言模型进行结构化标注，旨在构建可扩展且鲁棒的宣传检测系统。", "keywords": "宣传检测, 混合标注, 大型语言模型, 知识蒸馏, 可扩展性", "comments": "这项工作创新性地结合了LLM的强大预标注能力与人类智能的验证和纠正，有效解决了高质量标注数据稀缺和标注一致性低的问题。通过知识蒸馏将LLM的知识传递给小型模型，提高了实际应用的效率和可扩展性，对于推动透明媒体生态系统具有重要意义。其分层分类法也为宣传检测提供了更细致的视角。"}}
{"id": "2502.20144", "title": "Robust sensitivity control in digital pathology via tile score distribution matching", "authors": ["Arthur Pignet", "John Klein", "Genevieve Robin", "Antoine Olivier"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Camera ready version. Accepted at MICCAI 2025", "url": "http://arxiv.org/abs/2502.20144v3", "summary": "Deploying digital pathology models across medical centers is challenging due\nto distribution shifts. Recent advances in domain generalization improve model\ntransferability in terms of aggregated performance measured by the Area Under\nCurve (AUC). However, clinical regulations often require to control the\ntransferability of other metrics, such as prescribed sensitivity levels. We\nintroduce a novel approach to control the sensitivity of whole slide image\n(WSI) classification models, based on optimal transport and Multiple Instance\nLearning (MIL). Validated across multiple cohorts and tasks, our method enables\nrobust sensitivity control with only a handful of calibration samples,\nproviding a practical solution for reliable deployment of computational\npathology systems.", "comment": "Camera ready version. Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2502.20144v3", "cate": "cs.CV", "date": "2025-02-27", "updated": "2025-07-24", "AI": {"title_translation": "通过瓦片分数分布匹配实现数字病理中的鲁棒敏感性控制", "tldr": "本文提出了一种基于最优传输和多实例学习的新方法，用于在数字病理学中实现鲁棒的敏感性控制，仅需少量校准样本即可在不同队列和任务中实现可靠部署。", "motivation": "由于分布偏移，在不同医疗中心部署数字病理模型具有挑战性。虽然领域泛化提高了模型在AUC等聚合性能方面的可迁移性，但临床法规通常要求控制其他指标（如预设敏感性水平）的可迁移性。", "method": "本文引入了一种基于最优传输和多实例学习（MIL）的新方法来控制全玻片图像（WSI）分类模型的敏感性。", "result": "该方法在多个队列和任务中得到验证，仅需少量校准样本即可实现鲁棒的敏感性控制。", "conclusion": "本文提出的方法为计算病理学系统的可靠部署提供了一个实用的解决方案。", "translation": "由于分布偏移，在不同医疗中心部署数字病理模型具有挑战性。近期在领域泛化方面的进展提高了模型在以曲线下面积（AUC）衡量的聚合性能方面的可迁移性。然而，临床法规通常要求控制其他指标的可迁移性，例如预设的敏感性水平。我们引入了一种基于最优传输和多实例学习（MIL）的新方法，用于控制全玻片图像（WSI）分类模型的敏感性。我们的方法在多个队列和任务中得到了验证，仅需少量校准样本即可实现鲁棒的敏感性控制，为计算病理学系统的可靠部署提供了一个实用的解决方案。", "summary": "针对数字病理模型部署中因分布偏移导致的敏感性控制难题，本文提出了一种基于最优传输和多实例学习（MIL）的新方法。该方法旨在控制全玻片图像（WSI）分类模型的敏感性，并在多个队列和任务中得到验证。研究表明，该方法仅需少量校准样本即可实现鲁棒的敏感性控制，为计算病理学系统的可靠部署提供了实用方案。", "keywords": "数字病理学, 敏感性控制, 分布偏移, 最优传输, 多实例学习", "comments": "该论文的创新点在于将最优传输和多实例学习结合起来，以解决数字病理模型在实际部署中面临的敏感性控制难题。其重要性在于提供了一种高效且实用的解决方案，仅需少量校准样本即可在不同环境中实现模型的可靠性和法规依从性，这对于推动计算病理学在临床中的应用具有重要意义。"}}
{"id": "2507.18165", "title": "ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent", "authors": ["Yuheng Zhao", "Xueli Shu", "Liwen Fan", "Lin Gao", "Yu Zhang", "Siming Chen"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures", "url": "http://arxiv.org/abs/2507.18165v1", "summary": "Visual analytics (VA) is typically applied to complex data, thus requiring\ncomplex tools. While visual analytics empowers analysts in data analysis,\nanalysts may get lost in the complexity occasionally. This highlights the need\nfor intelligent assistance mechanisms. However, even the latest LLM-assisted VA\nsystems only provide help when explicitly requested by the user, making them\ninsufficiently intelligent to offer suggestions when analysts need them the\nmost. We propose a ProactiveVA framework in which LLM-powered UI agent monitors\nuser interactions and delivers context-aware assistance proactively. To design\neffective proactive assistance, we first conducted a formative study analyzing\nhelp-seeking behaviors in user interaction logs, identifying when users need\nproactive help, what assistance they require, and how the agent should\nintervene. Based on this analysis, we distilled key design requirements in\nterms of intent recognition, solution generation, interpretability and\ncontrollability. Guided by these requirements, we develop a three-stage UI\nagent pipeline including perception, reasoning, and acting. The agent\nautonomously perceives users' needs from VA interaction logs, providing\ntailored suggestions and intuitive guidance through interactive exploration of\nthe system. We implemented the framework in two representative types of VA\nsystems, demonstrating its generalizability, and evaluated the effectiveness\nthrough an algorithm evaluation, case and expert study and a user study. We\nalso discuss current design trade-offs of proactive VA and areas for further\nexploration.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.18165v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ProactiveVA: 基于LLM的UI代理的主动式可视化分析", "tldr": "ProactiveVA是一个基于LLM的UI代理，能主动为可视化分析用户提供情境感知帮助，解决现有系统被动响应的不足。", "motivation": "可视化分析（VA）工具通常很复杂，用户在使用时可能会感到迷失。尽管最新的LLM辅助VA系统能够提供帮助，但它们仅在用户明确请求时才响应，无法在用户最需要时主动提供建议。", "method": "本文提出了ProactiveVA框架，其中LLM驱动的UI代理监控用户交互并主动提供情境感知帮助。首先进行了一项形成性研究，分析用户交互日志中的求助行为，以识别用户何时需要主动帮助、需要何种帮助以及代理应如何介入。基于此分析，提炼出意图识别、解决方案生成、可解释性和可控性等关键设计要求。在此基础上，开发了一个三阶段UI代理管道，包括感知、推理和行动。该代理能够自主感知VA交互日志中的用户需求，通过系统交互式探索提供定制化建议和直观指导。该框架在两种代表性VA系统中实现，并进行了算法评估、案例研究、专家研究和用户研究来评估其有效性。", "result": "该框架在两种代表性VA系统中的实现，证明了其通用性。通过算法评估、案例研究、专家研究和用户研究，验证了ProactiveVA的有效性。", "conclusion": "ProactiveVA框架通过LLM驱动的UI代理，能够主动提供情境感知帮助，有效解决了可视化分析中用户迷失和现有系统被动响应的问题，并验证了其通用性和有效性。文章还讨论了主动式VA当前的设计权衡和未来探索领域。", "translation": "可视化分析（VA）通常应用于复杂数据，因此需要复杂的工具。尽管可视化分析能够帮助分析师进行数据分析，但分析师有时可能会迷失在复杂性中。这突出了对智能辅助机制的需求。然而，即使是最新由LLM辅助的VA系统也只在用户明确请求时才提供帮助，这使得它们在分析师最需要时无法提供建议，智能性不足。我们提出了一个ProactiveVA框架，其中LLM驱动的UI代理监控用户交互并主动提供情境感知辅助。为了设计有效的主动辅助，我们首先进行了一项形成性研究，分析用户交互日志中的求助行为，识别用户何时需要主动帮助、需要何种帮助以及代理应如何介入。基于此分析，我们提炼出意图识别、解决方案生成、可解释性和可控性方面的关键设计要求。在这些要求的指导下，我们开发了一个三阶段的UI代理管道，包括感知、推理和行动。该代理从VA交互日志中自主感知用户需求，通过系统交互式探索提供定制化建议和直观指导。我们在两种代表性的VA系统中实现了该框架，展示了其通用性，并通过算法评估、案例和专家研究以及用户研究评估了其有效性。我们还讨论了主动式VA当前的设计权衡和进一步探索的领域。", "summary": "本文提出了ProactiveVA框架，旨在解决可视化分析（VA）中用户因工具复杂性而迷失的问题，并克服现有LLM辅助系统仅被动响应的局限性。该框架核心是一个LLM驱动的UI代理，通过监控用户交互，主动提供情境感知辅助。研究团队通过对用户求助行为的形成性研究，提炼出意图识别、解决方案生成、可解释性和可控性的设计要求，并据此构建了包括感知、推理和行动的三阶段代理管道。ProactiveVA能在VA交互日志中自主识别用户需求，并提供定制化建议和指导。该框架已在两种VA系统中实现并验证了其通用性和有效性。", "keywords": "主动式可视化分析, LLM, UI代理, 智能辅助, 人机交互", "comments": "该论文提出了一种创新的主动式可视化分析（ProactiveVA）框架，通过引入LLM驱动的UI代理来解决现有VA系统被动响应用户请求的局限性。其核心创新在于代理能够监控用户交互并主动提供情境感知帮助，这显著提升了用户体验和分析效率。通过形成性研究来指导设计需求，确保了解决方案的实用性和针对性。在两种VA系统中的实现和多样的评估方法（算法、案例、专家、用户研究）增强了研究的严谨性和说服力。论文也坦诚地讨论了当前的设计权衡，为未来的研究指明了方向，具有重要的实践和理论价值。"}}
{"id": "2507.18139", "title": "Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions", "authors": ["Alberto Marchisio", "Muhammad Shafique"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear at the 31st IEEE International Symposium on On-Line Testing and Robust System Design (IOLTS), Ischia, Italy, July 2025", "url": "http://arxiv.org/abs/2507.18139v1", "summary": "The growing need for intelligent, adaptive, and energy-efficient autonomous\nsystems across fields such as robotics, mobile agents (e.g., UAVs), and\nself-driving vehicles is driving interest in neuromorphic computing. By drawing\ninspiration from biological neural systems, neuromorphic approaches offer\npromising pathways to enhance the perception, decision-making, and\nresponsiveness of autonomous platforms. This paper surveys recent progress in\nneuromorphic algorithms, specialized hardware, and cross-layer optimization\nstrategies, with a focus on their deployment in real-world autonomous\nscenarios. Special attention is given to event-based dynamic vision sensors and\ntheir role in enabling fast, efficient perception. The discussion highlights\nnew methods that improve energy efficiency, robustness, adaptability, and\nreliability through the integration of spiking neural networks into autonomous\nsystem architectures. We integrate perspectives from machine learning,\nrobotics, neuroscience, and neuromorphic engineering to offer a comprehensive\nview of the state of the field. Finally, emerging trends and open challenges\nare explored, particularly in the areas of real-time decision-making, continual\nlearning, and the development of secure, resilient autonomous systems.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS), Ischia, Italy, July 2025", "pdf_url": "http://arxiv.org/pdf/2507.18139v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于自主系统中具身智能的神经形态计算：当前趋势、挑战和未来方向", "tldr": "本文综述了神经形态计算在自主系统具身智能中的应用，包括其最新进展、挑战和未来方向，特别关注了事件驱动型视觉传感器和SNN集成，以提升能效、鲁棒性、适应性和可靠性。", "motivation": "机器人、移动智能体（如无人机）和自动驾驶车辆等领域对智能、自适应、高能效自主系统的需求日益增长，推动了对神经形态计算的兴趣。", "method": "本文综述了神经形态算法、专用硬件和跨层优化策略的最新进展，重点关注它们在实际自主场景中的部署。特别关注了事件驱动型动态视觉传感器及其在实现快速高效感知中的作用。讨论了通过将脉冲神经网络集成到自主系统架构中，提高能效、鲁棒性、适应性和可靠性的新方法。整合了机器学习、机器人学、神经科学和神经形态工程的视角，提供了该领域的全面视图。", "result": "论文强调了通过将脉冲神经网络集成到自主系统架构中，可以提高系统的能效、鲁棒性、适应性和可靠性。特别提到了事件驱动型动态视觉传感器在实现快速高效感知中的作用。", "conclusion": "本文探讨了神经形态计算在自主系统中具身智能的当前趋势、开放挑战和未来方向，尤其是在实时决策、持续学习以及开发安全、有弹性的自主系统方面。", "translation": "在机器人学、移动智能体（例如无人机）和自动驾驶车辆等领域，对智能、自适应和高能效自主系统日益增长的需求，正在推动人们对神经形态计算的兴趣。通过从生物神经系统中汲取灵感，神经形态方法为增强自主平台的感知、决策和响应能力提供了有前景的途径。本文综述了神经形态算法、专用硬件和跨层优化策略的最新进展，重点关注它们在实际自主场景中的部署。特别关注事件驱动型动态视觉传感器及其在实现快速、高效感知中的作用。讨论了通过将脉冲神经网络集成到自主系统架构中，提高能效、鲁棒性、适应性和可靠性的新方法。我们整合了机器学习、机器人学、神经科学和神经形态工程的视角，以提供该领域的全面视图。最后，探讨了新兴趋势和开放挑战，特别是在实时决策、持续学习以及开发安全、有弹性的自主系统方面。", "summary": "本文综述了神经形态计算在智能、自适应、高能效自主系统（如机器人、无人机、自动驾驶汽车）中的应用。论文探讨了神经形态算法、专用硬件和跨层优化策略的最新进展，并强调了事件驱动型动态视觉传感器和脉冲神经网络在提升系统感知、决策、响应能力、能效、鲁棒性、适应性和可靠性方面的潜力。文章还整合了多学科视角，并讨论了实时决策、持续学习和安全弹性系统等未来挑战和方向。", "keywords": "神经形态计算, 自主系统, 具身智能, 脉冲神经网络, 事件驱动视觉传感器", "comments": "本文系统性地综述了神经形态计算在自主系统具身智能领域的应用，其创新性在于从多学科角度（机器学习、机器人学、神经科学、神经形态工程）提供了全面视图。重要性体现在其指出了神经形态计算在提升自主系统能效、鲁棒性、适应性和可靠性方面的巨大潜力，并特别强调了事件驱动型传感器和SNN的集成。论文还明确指出了该领域面临的挑战和未来方向，如实时决策和持续学习，为后续研究提供了清晰的指引。"}}
{"id": "2507.14937", "title": "Phase-optimised linearly-constrained minimum-variance beamformers", "authors": ["Hugh L Kennedy"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Fixed a few minor things spotted since the initial upload", "url": "http://arxiv.org/abs/2507.14937v2", "summary": "A novel procedure for the determination of the optimal group-delay of a\nLinearly-Constrained Minimum-Variance (LCMV) beamformer is proposed. Two ways\nof selecting the optimal delay are recommended: the first is the solution that\nminimizes the noise power; the second is the solution that minimizes the\nprocessing delay. The potential of this hitherto unexplored degree of design\nfreedom is explored using simulated Very-High-Frequency (VHF) communication,\nand Ultra-High-Frequency (UHF) bistatic radar, applications.", "comment": "Fixed a few minor things spotted since the initial upload", "pdf_url": "http://arxiv.org/pdf/2507.14937v2", "cate": "eess.SP", "date": "2025-07-20", "updated": "2025-07-24", "AI": {"title_translation": "相位优化线性约束最小方差波束形成器", "tldr": "本文提出了一种确定线性约束最小方差（LCMV）波束形成器最佳群延迟的新程序，并推荐了两种优化策略：最小化噪声功率或最小化处理延迟。该方法在模拟的VHF通信和UHF双基地雷达应用中得到了探索。", "motivation": "探索线性约束最小方差（LCMV）波束形成器中迄今为止尚未探索的“群延迟”这一设计自由度的潜力。", "method": "本文提出了一种确定线性约束最小方差（LCMV）波束形成器最佳群延迟的新程序。推荐了两种选择最佳延迟的方法：第一种是使噪声功率最小化的解决方案；第二种是使处理延迟最小化的解决方案。", "result": "该方法在模拟的甚高频（VHF）通信和超高频（UHF）双基地雷达应用中探索了其潜力。", "conclusion": "本文探索并展示了通过优化群延迟来提升线性约束最小方差（LCMV）波束形成器性能的潜力。", "translation": "原标题：相位优化线性约束最小方差波束形成器\n\n原摘要：本文提出了一种确定线性约束最小方差（LCMV）波束形成器最佳群延迟的新颖程序。推荐了两种选择最佳延迟的方法：第一种是使噪声功率最小化的解决方案；第二种是使处理延迟最小化的解决方案。通过模拟甚高频（VHF）通信和超高频（UHF）双基地雷达应用，探索了这种迄今为止尚未探索的设计自由度的潜力。", "summary": "本文提出了一种创新的程序，用于确定线性约束最小方差（LCMV）波束形成器的最佳群延迟。该程序提供了两种优化策略：一是最小化噪声功率，二是最小化处理延迟。研究通过模拟甚高频（VHF）通信和超高频（UHF）双基地雷达应用，验证并探索了这一此前未被充分利用的设计自由度的潜力。", "keywords": "LCMV, 波束形成器, 群延迟, 噪声功率, 处理延迟", "comments": "这项工作通过引入并探索LCMV波束形成器中群延迟的优化，为波束形成器的设计提供了新的自由度。其创新性在于提出了两种具体的优化策略（最小化噪声功率或处理延迟），并在实际应用场景中进行了模拟验证，有望提升波束形成器的性能。"}}
{"id": "2507.12311", "title": "An Ecosystem for Ontology Interoperability", "authors": ["Zhangcheng Qiang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      5 pages, 8 figures", "url": "http://arxiv.org/abs/2507.12311v2", "summary": "Ontology interoperability is one of the complicated issues that restricts the\nuse of ontologies in knowledge graphs (KGs). Different ontologies with\nconflicting and overlapping concepts make it difficult to design, develop, and\ndeploy an interoperable ontology for downstream tasks. We propose an ecosystem\nfor ontology interoperability. The ecosystem employs three state-of-the-art\nsemantic techniques in different phases of the ontology engineering life cycle:\nontology design patterns (ODPs) in the design phase, ontology matching and\nversioning (OM\\&OV) in the develop phase, and ontology-compliant knowledge\ngraphs (OCKGs) in the deploy phase, to achieve better ontology interoperability\nin real-world applications. A case study of sensor observation in the building\ndomain validates the usefulness of the proposed ecosystem.", "comment": "5 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.12311v2", "cate": "cs.IR", "date": "2025-07-16", "updated": "2025-07-23", "AI": {"title_translation": "本体互操作性生态系统", "tldr": "本文提出了一个本体互操作性生态系统，通过在本体工程生命周期的不同阶段采用本体设计模式、本体匹配与版本控制以及符合本体的知识图谱，来解决知识图谱中本体互操作性的复杂问题。", "motivation": "本体互操作性是限制本体在知识图谱中使用的复杂问题之一。不同本体之间概念的冲突和重叠使得为下游任务设计、开发和部署可互操作的本体变得困难。", "method": "本文提出了一个本体互操作性生态系统。该生态系统在本体工程生命周期的不同阶段采用了三种最先进的语义技术：设计阶段的本体设计模式（ODPs）、开发阶段的本体匹配与版本控制（OM&OV），以及部署阶段的符合本体的知识图谱（OCKGs），以在实际应用中实现更好的本体互操作性。", "result": "通过一个建筑领域传感器观测的案例研究，验证了所提出的生态系统的实用性。", "conclusion": "所提出的本体互操作性生态系统通过在本体工程生命周期的关键阶段应用先进的语义技术，能够有效提升本体在知识图谱中的互操作性，并在实际应用中展现出其价值。", "translation": "本体互操作性是限制本体在知识图谱（KGs）中使用的复杂问题之一。不同本体之间冲突和重叠的概念使得为下游任务设计、开发和部署可互操作的本体变得困难。我们提出了一个本体互操作性生态系统。该生态系统在本体工程生命周期的不同阶段采用了三种最先进的语义技术：设计阶段的本体设计模式（ODPs）、开发阶段的本体匹配与版本控制（OM&OV），以及部署阶段的符合本体的知识图谱（OCKGs），以在实际应用中实现更好的本体互操作性。一个建筑领域传感器观测的案例研究验证了所提出的生态系统的实用性。", "summary": "本文针对知识图谱中本体互操作性受限的问题，提出了一个本体互操作性生态系统。该生态系统在本体工程生命周期的设计、开发和部署阶段分别集成了本体设计模式、本体匹配与版本控制以及符合本体的知识图谱等先进语义技术，旨在提高本体在实际应用中的互操作性。通过建筑领域传感器观测的案例研究，验证了该生态系统的有效性。", "keywords": "本体互操作性, 知识图谱, 本体设计模式, 本体匹配, 本体版本控制", "comments": "该论文提出的生态系统方法，通过将本体工程生命周期的不同阶段与特定的语义技术相结合，为解决本体互操作性问题提供了一个结构化且全面的框架，具有一定的创新性。其将设计模式、匹配、版本控制和符合本体的知识图谱整合，有助于提升知识图谱的实用性。"}}
{"id": "2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.11936v3", "summary": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.11936v3", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-24", "AI": {"title_translation": "深度学习在几何问题求解中的应用综述", "tldr": "本文综述了深度学习在几何问题求解领域的应用，涵盖了相关任务、深度学习方法、评估指标、挑战和未来方向，旨在提供一个全面的参考。", "motivation": "几何问题求解是数学推理的关键领域，广泛应用于教育、人工智能数学能力评估和多模态能力评估等重要领域。近年来，深度学习技术，特别是多模态大型语言模型的快速发展，引发了广泛的研究热潮。因此，本文旨在对该领域进行全面综述。", "method": "本文对深度学习在几何问题求解中的应用进行了综述，具体包括：(i) 全面总结了几何问题求解中的相关任务；(ii) 深入回顾了相关的深度学习方法；(iii) 详细分析了评估指标和方法；(iv) 批判性地讨论了当前的挑战和未来可探索的方向。", "result": "本文旨在提供一个全面且实用的深度学习在几何问题求解方面的参考，以促进该领域的进一步发展。", "conclusion": "本文旨在为深度学习在几何问题求解领域提供一个全面且实用的参考，以促进该领域的进一步发展。", "translation": "几何问题求解是数学推理的一个关键领域，广泛涉及教育、人工智能数学能力评估和多模态能力评估等许多重要领域。近年来，深度学习技术的快速发展，特别是多模态大型语言模型的兴起，引发了广泛的研究热潮。本文对深度学习在几何问题求解中的应用进行了综述，包括：(i) 对几何问题求解中相关任务的全面总结；(ii) 对相关深度学习方法的彻底回顾；(iii) 对评估指标和方法的详细分析；(iv) 对当前挑战和未来可探索方向的批判性讨论。我们的目标是为深度学习在几何问题求解方面提供一个全面且实用的参考，以促进该领域的进一步发展。我们在GitHub上创建了一个持续更新的论文列表：https://github.com/majianz/dl4gps。", "summary": "本文综述了深度学习在几何问题求解领域的应用。鉴于几何问题求解在教育、AI评估等领域的关键性以及深度学习和多模态大模型的快速发展，该综述系统地总结了相关任务、深度学习方法、评估指标，并探讨了当前挑战与未来方向。其目标是为该领域提供一个全面实用的参考，以推动进一步发展。", "keywords": "深度学习, 几何问题求解, 综述, 数学推理, 多模态大型语言模型", "comments": "这是一篇重要的综述性论文，它系统地梳理了深度学习在几何问题求解这一复杂交叉领域的研究进展。其价值在于为研究人员提供了一个清晰的路线图，不仅总结了现有技术，还指出了未来的研究方向和挑战，对于推动该领域的发展具有指导意义。"}}
{"id": "2507.18316", "title": "YATE: The Role of Test Repair in LLM-Based Unit Test Generation", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Jie M. Zhang", "Mark Harman", "Mike Papadakis"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures", "url": "http://arxiv.org/abs/2507.18316v1", "summary": "Recent advances in automated test generation utilises language models to\nproduce unit tests. While effective, language models tend to generate many\nincorrect tests with respect to both syntax and semantics. Although such\nincorrect tests can be easily detected and discarded, they constitute a \"missed\nopportunity\" -- if fixed, they are often valuable as they directly add testing\nvalue (they effectively target the underlying program logic to be tested) and\nindirectly form good seeds for generating additional tests. To this end, we\npropose a simple technique for repairing some of these incorrect tests through\na combination of rule-based static analysis and re-prompting. We evaluate this\nsimple approach, named YATE, on a set of 6 open-source projects and show that\nit can effectively produce tests that cover on average 32.06% more lines and\nkill 21.77% more mutants than a plain LLM-based method. We also compare YATE\nwith four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and\nCOVERUP and show that it produces tests that cover substantially more code.\nYATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%\nmore mutants at a comparable cost (number of calls to LLMs).", "comment": "12 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18316v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "YATE：测试修复在基于LLM的单元测试生成中的作用", "tldr": "YATE是一种通过规则静态分析和重新提示修复LLM生成的错误单元测试的方法，显著提高了测试覆盖率和变异杀伤率。", "motivation": "尽管语言模型在自动化测试生成方面有效，但它们倾向于生成大量语法和语义上不正确的测试。这些被丢弃的错误测试是“错失的机会”，如果能修复，它们将非常有价值。", "method": "我们提出了一种名为YATE的简单技术，通过结合基于规则的静态分析和重新提示来修复部分不正确的测试。", "result": "YATE在6个开源项目上进行评估，与普通LLM方法相比，其生成的测试平均覆盖行数多32.06%，变异杀伤率高21.77%。与HITS、SYMPROMPT、TESTSPARK和COVERUP等其他四种LLM方法相比，YATE在相似成本下实现了22%更高的行覆盖率，20%更高的分支覆盖率，并多杀伤20%的变异。", "conclusion": "YATE通过修复LLM生成的错误测试，能够显著提高单元测试的质量（代码覆盖率和变异杀伤率），且成本相当。", "translation": "自动化测试生成最近的进展利用语言模型来生成单元测试。尽管有效，但语言模型倾向于生成许多在语法和语义上都不正确的测试。虽然这些不正确的测试可以很容易地被检测和丢弃，但它们构成了“错失的机会”——如果被修复，它们通常很有价值，因为它们直接增加了测试价值（它们有效地针对了要测试的底层程序逻辑），并间接形成了生成额外测试的良好种子。为此，我们提出了一种简单的技术，通过结合基于规则的静态分析和重新提示来修复其中一些不正确的测试。我们评估了这种名为YATE的简单方法，在一组6个开源项目上进行，结果表明它能有效地生成测试，与普通的基于LLM的方法相比，平均覆盖行数多32.06%，变异杀伤率高21.77%。我们还将YATE与其他四种基于LLM的方法（即HITS、SYMPROMPT、TESTSPARK和COVERUP）进行了比较，结果表明它生成的测试覆盖了更多代码。YATE在相似的成本（对LLM的调用次数）下，实现了22%更高的行覆盖率，20%更高的分支覆盖率，并多杀伤20%的变异。", "summary": "本文提出了一种名为YATE的简单技术，旨在修复语言模型（LLM）在生成单元测试时产生的语法和语义错误。现有LLM生成的错误测试通常被丢弃，但YATE通过结合规则静态分析和重新提示，将这些“错失的机会”转化为有价值的测试。实验结果表明，YATE能够显著提高测试质量，与纯LLM方法相比，代码覆盖率和变异杀伤率均有显著提升，并且在与其他先进LLM测试生成方法比较时，也展现出更优异的覆盖率表现，同时保持相似的成本。", "keywords": "LLM, 单元测试生成, 测试修复, 代码覆盖率, 变异测试", "comments": "该论文的创新点在于将LLM生成的“错误”测试视为“错失的机会”并进行修复，而非简单丢弃。这种策略有效地提升了测试的质量和价值。YATE的简单性（规则分析+重提示）也值得关注，表明即使是相对直接的方法也能带来显著的性能提升。"}}
{"id": "2503.10029", "title": "HandProxy: Expanding the Affordances of Speech Interfaces in Immersive Environments with a Virtual Proxy Hand", "authors": ["Chen Liang", "Yuxuan Liu", "Martez Mott", "Anhong Guo"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted in ACM IMWUT 2025", "url": "http://arxiv.org/abs/2503.10029v2", "summary": "Hand interactions are increasingly used as the primary input modality in\nimmersive environments, but they are not always feasible due to situational\nimpairments, motor limitations, and environmental constraints. Speech\ninterfaces have been explored as an alternative to hand input in research and\ncommercial solutions, but are limited to initiating basic hand gestures and\nsystem controls. We introduce HandProxy, a system that expands the affordances\nof speech interfaces to support expressive hand interactions. Instead of\nrelying on predefined speech commands directly mapped to possible interactions,\nHandProxy enables users to control the movement of a virtual hand as an\ninteraction proxy, allowing them to describe the intended interactions\nnaturally while the system translates speech into a sequence of hand controls\nfor real-time execution. A user study with 20 participants demonstrated that\nHandProxy effectively enabled diverse hand interactions in virtual\nenvironments, achieving a 100% task completion rate with an average of 1.09\nattempts per speech command and 91.8% command execution accuracy, while\nsupporting flexible, natural speech input with varying levels of control and\ngranularity.", "comment": "Accepted in ACM IMWUT 2025", "pdf_url": "http://arxiv.org/pdf/2503.10029v2", "cate": "cs.HC", "date": "2025-03-13", "updated": "2025-07-24", "AI": {"title_translation": "HandProxy：通过虚拟代理手扩展沉浸式环境中语音界面的可供性", "tldr": "HandProxy允许用户通过自然语音控制虚拟代理手，从而在沉浸式环境中实现更丰富的交互，解决了传统手部输入和语音界面的局限性。", "motivation": "沉浸式环境中的手部交互受限于情境障碍、运动限制和环境约束。现有语音界面仅限于基本的姿势和系统控制，无法支持表达性手部交互。", "method": "引入HandProxy系统。该系统允许用户通过自然语音描述预期的交互，从而控制一个虚拟代理手。系统将语音转换为一系列手部控制指令进行实时执行，而非依赖预定义的语音命令直接映射。", "result": "一项包含20名参与者的用户研究表明，HandProxy能有效实现在虚拟环境中的多样化手部交互，任务完成率达100%，平均每次语音命令尝试次数为1.09次，命令执行准确率为91.8%，同时支持灵活、自然的语音输入，具有不同程度的控制和粒度。", "conclusion": "HandProxy通过引入虚拟代理手，显著扩展了语音界面在沉浸式环境中支持表达性手部交互的能力，克服了传统输入方式的局限性，并展现出高效率和准确性。", "translation": "手部交互正日益成为沉浸式环境中的主要输入方式，但由于情境障碍、运动限制和环境约束，它们并非总是可行。研究和商业解决方案中已探索将语音界面作为手部输入的替代方案，但其仅限于启动基本手势和系统控制。我们引入了HandProxy，这是一个扩展语音界面可供性以支持表达性手部交互的系统。HandProxy不依赖于直接映射到可能交互的预定义语音命令，而是允许用户控制虚拟手部的移动作为交互代理，让他们能够自然地描述预期的交互，同时系统将语音转换为一系列手部控制指令以进行实时执行。一项有20名参与者的用户研究表明，HandProxy有效地在虚拟环境中实现了多样化的手部交互，任务完成率达到100%，平均每次语音命令尝试次数为1.09次，命令执行准确率为91.8%，同时支持具有不同控制和粒度水平的灵活、自然语音输入。", "summary": "HandProxy是一个创新的系统，旨在解决沉浸式环境中手部交互的局限性以及现有语音界面功能单一的问题。它通过允许用户使用自然语言控制一个虚拟代理手，将语音指令转化为实时的手部动作，从而实现更丰富和表达性的手部交互。用户研究结果表明，该系统在任务完成率、准确性和用户体验方面表现出色，为虚拟现实中的交互提供了一种灵活高效的新方法。", "keywords": "语音界面, 沉浸式环境, 虚拟代理手, 手部交互, 自然语言控制", "comments": "HandProxy的核心创新在于其“虚拟代理手”的概念，这使得语音控制从简单的命令映射转变为更自然、富有表达力的交互方式。这种方法克服了传统语音界面在沉浸式环境中的局限性，特别是在手部交互受限的情况下，具有重要的应用潜力。其高任务完成率和准确性也验证了其有效性。"}}
{"id": "2409.15087", "title": "AI Workflow, External Validation, and Development in Eye Disease Diagnosis", "authors": ["Qingyu Chen", "Tiarnan D L Keenan", "Elvira Agron", "Alexis Allot", "Emily Guan", "Bryant Duong", "Amr Elsawy", "Benjamin Hou", "Cancan Xue", "Sanjeeb Bhandari", "Geoffrey Broadhead", "Chantal Cousineau-Krieger", "Ellen Davis", "William G Gensheimer", "David Grasic", "Seema Gupta", "Luis Haddock", "Eleni Konstantinou", "Tania Lamba", "Michele Maiberger", "Dimosthenis Mantopoulos", "Mitul C Mehta", "Ayman G Nahri", "Mutaz AL-Nawaflh", "Arnold Oshinsky", "Brittany E Powell", "Boonkit Purt", "Soo Shin", "Hillary Stiefel", "Alisa T Thavikulwat", "Keith James Wroblewski", "Tham Yih Chung", "Chui Ming Gemmy Cheung", "Ching-Yu Cheng", "Emily Y Chew", "Michelle R. Hribar", "Michael F. Chiang", "Zhiyong Lu"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Published in JAMA Network Open, doi: https://doi.org/10.1001/jamanetworkopen.2025.17204", "url": "http://arxiv.org/abs/2409.15087v2", "summary": "Timely disease diagnosis is challenging due to increasing disease burdens and\nlimited clinician availability. AI shows promise in diagnosis accuracy but\nfaces real-world application issues due to insufficient validation in clinical\nworkflows and diverse populations. This study addresses gaps in medical AI\ndownstream accountability through a case study on age-related macular\ndegeneration (AMD) diagnosis and severity classification. We designed and\nimplemented an AI-assisted diagnostic workflow for AMD, comparing diagnostic\nperformance with and without AI assistance among 24 clinicians from 12\ninstitutions with real patient data sampled from the Age-Related Eye Disease\nStudy (AREDS). Additionally, we demonstrated continual enhancement of an\nexisting AI model by incorporating approximately 40,000 additional medical\nimages (named AREDS2 dataset). The improved model was then systematically\nevaluated using both AREDS and AREDS2 test sets, as well as an external test\nset from Singapore. AI assistance markedly enhanced diagnostic accuracy and\nclassification for 23 out of 24 clinicians, with the average F1-score\nincreasing by 20% from 37.71 (Manual) to 45.52 (Manual + AI) (P-value <\n0.0001), achieving an improvement of over 50% in some cases. In terms of\nefficiency, AI assistance reduced diagnostic times for 17 out of the 19\nclinicians tracked, with time savings of up to 40%. Furthermore, a model\nequipped with continual learning showed robust performance across three\nindependent datasets, recording a 29% increase in accuracy, and elevating the\nF1-score from 42 to 54 in the Singapore population.", "comment": "Published in JAMA Network Open,\n  doi:10.1001/jamanetworkopen.2025.17204", "pdf_url": "http://arxiv.org/pdf/2409.15087v2", "cate": "eess.IV", "date": "2024-09-23", "updated": "2025-07-24", "AI": {"title_translation": "眼病诊断中的AI工作流程、外部验证与开发", "tldr": "本研究通过对年龄相关性黄斑变性（AMD）的案例研究，展示了AI辅助诊断如何显著提高临床医生的诊断准确性和效率，并验证了AI模型在多数据集下的鲁棒性。", "motivation": "由于疾病负担增加和临床医生资源有限，及时诊断疾病面临挑战。AI在诊断准确性上显示出潜力，但由于在临床工作流程和多样化人群中缺乏充分验证，面临实际应用问题。本研究旨在解决医疗AI下游问责制方面的空白。", "method": "本研究设计并实施了一个AI辅助的年龄相关性黄斑变性（AMD）诊断工作流程。研究比较了24位来自12个机构的临床医生在有无AI辅助下的诊断性能，使用了来自年龄相关性眼病研究（AREDS）的真实患者数据。此外，研究通过整合约40,000张额外医学图像（AREDS2数据集）来持续增强现有AI模型。改进后的模型在AREDS、AREDS2和来自新加坡的外部测试集上进行了系统评估。", "result": "AI辅助显著提高了24位临床医生中23位的诊断准确性和分类能力，平均F1分数从37.71（手动）提高到45.52（手动+AI），增幅达20%（P值<0.0001），在某些情况下提高了50%以上。AI辅助使19位受追踪的临床医生中的17位诊断时间减少，最高节省40%。此外，具有持续学习能力的模型在三个独立数据集上表现出稳健的性能，准确性提高了29%，在新加坡人群中F1分数从42提高到54。", "conclusion": "AI辅助诊断显著提高了临床医生的诊断准确性和效率，并且经过持续学习增强的AI模型在多样化数据集上表现出强大的泛化能力，证明了其在实际临床应用中的巨大潜力。", "translation": "及时诊断疾病面临挑战，原因在于疾病负担日益加重以及临床医生资源有限。人工智能（AI）在诊断准确性上显示出潜力，但由于在临床工作流程和多样化人群中缺乏充分验证，面临实际应用问题。本研究通过一个关于年龄相关性黄斑变性（AMD）诊断和严重程度分类的案例研究，解决了医疗AI下游问责制方面的空白。我们设计并实施了一个AI辅助的AMD诊断工作流程，比较了来自12个机构的24位临床医生在有无AI辅助下的诊断性能，使用了从年龄相关性眼病研究（AREDS）中抽取的真实患者数据。此外，我们通过整合约40,000张额外医学图像（命名为AREDS2数据集），展示了对现有AI模型的持续增强。改进后的模型随后在AREDS和AREDS2测试集以及来自新加坡的外部测试集上进行了系统评估。AI辅助显著提高了24位临床医生中23位的诊断准确性和分类能力，平均F1分数从37.71（手动）提高到45.52（手动+AI），增幅达20%（P值<0.0001），在某些情况下提高了50%以上。在效率方面，AI辅助减少了19位受追踪临床医生中17位的诊断时间，最高节省40%。此外，一个配备持续学习功能的模型在三个独立数据集上表现出稳健的性能，准确性提高了29%，在新加坡人群中F1分数从42提高到54。", "summary": "本研究旨在解决医疗AI在临床工作流程和多样化人群中验证不足的问题。通过对年龄相关性黄斑变性（AMD）的案例研究，设计并实施了一个AI辅助诊断工作流程。结果显示，AI辅助显著提高了临床医生的诊断准确性和效率，平均F1分数提升20%，诊断时间最多节省40%。此外，通过持续学习增强的AI模型在多个独立数据集上表现出稳健的性能，进一步验证了AI在眼病诊断中的实际应用价值和潜力。", "keywords": "AI工作流程, 眼病诊断, 外部验证, 年龄相关性黄斑变性, 持续学习", "comments": "本研究的创新之处在于其对AI在真实临床工作流程中的外部验证，并强调了AI在提升临床医生诊断准确性和效率方面的实际效用。通过引入持续学习和多数据集验证，增强了AI模型的鲁棒性和泛化能力，为医疗AI的实际部署提供了有力的证据。研究结果提供了量化的性能提升数据，具有重要的临床转化意义。"}}
{"id": "2503.07919", "title": "BEARCUBS: A benchmark for computer-using web agents", "authors": ["Yixiao Song", "Katherine Thai", "Chau Minh Pham", "Yapei Chang", "Mazin Nadaf", "Mohit Iyyer"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2503.07919v3", "summary": "Modern web agents possess computer use abilities that allow them to interact\nwith webpages by sending commands to a virtual keyboard and mouse. While such\nagents have considerable potential to assist human users with complex tasks,\nevaluating their capabilities in real-world settings poses a major challenge.\nTo this end, we introduce BEARCUBS, a \"smallbut mighty\" benchmark of 111\ninformation-seeking questions designed to evaluate a web agent's ability to\nsearch, browse, and identify factual information from the web. Unlike prior web\nagent benchmarks, solving BEARCUBS requires (1) accessing live web content\nrather than synthetic or simulated pages, which captures the unpredictability\nof real-world web interactions; and (2) performing a broad range of multimodal\ninteractions (e.g., video understanding, 3D navigation) that cannot be bypassed\nvia text-based workarounds. Each question in BEARCUBS has a corresponding\nshort, unambiguous answer and a human-validated browsing trajectory, allowing\nfor transparent evaluation of agent performance and strategies. A human study\nconfirms that BEARCUBS questions are solvable but non-trivial (84.7% human\naccuracy), revealing domain knowledge gaps and overlooked details as common\nfailure points. We find that ChatGPT Agent significantly outperforms other\ncomputer-using agents with an overall accuracy of 65.8% (compared to e.g.,\nOperator's 23.4%), showcasing substantial progress in tasks involving real\ncomputer use, such as playing web games and navigating 3D environments.\nNevertheless, closing the gap to human performance requires improvements in\nareas like fine control, complex data filtering, and execution speed. To\nfacilitate future research, BEARCUBS will be updated periodically to replace\ninvalid or contaminated questions, keeping the benchmark fresh for future\ngenerations of web agents.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2503.07919v3", "cate": "cs.AI", "date": "2025-03-10", "updated": "2025-07-24", "AI": {"title_translation": "BEARCUBS：一个用于计算机使用型网络代理的基准测试", "tldr": "BEARCUBS是一个新的基准测试，用于评估能与真实网页交互并执行多模态任务的网络代理，它揭示了现有代理与人类表现之间的差距。", "motivation": "现代网络代理具有辅助人类完成复杂任务的巨大潜力，但评估其在真实世界环境中的能力是一个重大挑战。", "method": "本文引入了BEARCUBS，这是一个包含111个信息检索问题的基准测试，旨在评估网络代理的搜索、浏览和识别事实信息的能力。与现有基准测试不同，BEARCUBS要求访问实时网络内容并执行广泛的多模态交互。每个问题都有简短、明确的答案和人类验证的浏览轨迹。", "result": "人类研究证实BEARCUBS问题可解但非平凡（84.7%的人类准确率）。ChatGPT Agent的表现显著优于其他计算机使用型代理（65.8%的准确率，而Operator为23.4%）。尽管取得了进展，但在精细控制、复杂数据过滤和执行速度等方面仍需改进才能缩小与人类表现的差距。", "conclusion": "BEARCUBS基准测试展示了计算机使用型网络代理在真实任务中的显著进展，但也突出了在达到人类水平性能方面仍需努力的领域。该基准测试将定期更新以促进未来研究。", "translation": "现代网络代理拥有计算机使用能力，允许它们通过向虚拟键盘和鼠标发送命令来与网页交互。虽然此类代理在协助人类用户完成复杂任务方面具有相当大的潜力，但在真实世界环境中评估其能力面临重大挑战。为此，我们引入了BEARCUBS，一个“小而强大”的基准测试，包含111个信息检索问题，旨在评估网络代理从网络搜索、浏览和识别事实信息的能力。与之前的网络代理基准测试不同，解决BEARCUBS需要(1) 访问实时网络内容而不是合成或模拟页面，这捕捉了真实世界网络交互的不可预测性；(2) 执行广泛的多模态交互（例如，视频理解、3D导航），这些交互不能通过基于文本的变通方法绕过。BEARCUBS中的每个问题都有一个对应的简短、明确的答案和人类验证的浏览轨迹，从而可以透明地评估代理的性能和策略。一项人类研究证实BEARCUBS问题是可解但非平凡的（84.7%的人类准确率），揭示了领域知识差距和被忽视的细节是常见的失败点。我们发现ChatGPT Agent的整体准确率为65.8%（相比之下，例如Operator为23.4%），显著优于其他计算机使用型代理，展示了在涉及真实计算机使用任务（如玩网页游戏和导航3D环境）方面的实质性进展。然而，缩小与人类表现的差距需要在精细控制、复杂数据过滤和执行速度等领域进行改进。为了促进未来的研究，BEARCUBS将定期更新以替换无效或受污染的问题，使基准测试对未来几代网络代理保持新鲜。", "summary": "本文介绍了BEARCUBS，这是一个用于评估计算机使用型网络代理的新型基准测试。它包含111个信息检索问题，要求代理访问实时网页并进行多模态交互，以模拟真实世界的复杂性。研究发现，尽管ChatGPT Agent表现优于其他代理，但所有代理在准确性上仍与人类存在显著差距，尤其是在精细控制和复杂数据处理方面。BEARCUBS旨在促进该领域未来的研究和发展。", "keywords": "网络代理, 基准测试, 计算机使用, 多模态交互, 实时网络", "comments": "BEARCUBS的创新之处在于它强调了对实时网络内容和多模态交互的需求，这使其比以往的基准测试更能反映真实世界的复杂性。其重要性在于为评估和推动网络代理的发展提供了一个更真实、更具挑战性的平台。通过揭示现有代理与人类表现之间的差距，该基准测试明确指出了未来研究的关键方向，例如精细控制和复杂数据过滤。"}}
{"id": "2504.21706", "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "authors": ["Saber Mehdipour", "Seyed Abolghasem Mirroshandel", "Seyed Amirhossein Tabatabaei"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21706v3", "summary": "Detecting plant diseases is a crucial aspect of modern agriculture, as it\nplays a key role in maintaining crop health and increasing overall yield.\nTraditional approaches, though still valuable, often rely on manual inspection\nor conventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering advantages such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This review\nexplores the application of ViTs in precision agriculture, covering a range of\ntasks. We begin by introducing the foundational architecture of ViTs and\ndiscussing their transition from Natural Language Processing (NLP) to Computer\nVision. The discussion includes the concept of inductive bias in traditional\nmodels like Convolutional Neural Networks (CNNs), and how ViTs mitigate these\nbiases. We provide a comprehensive review of recent literature, focusing on key\nmethodologies, datasets, and performance metrics. This study also includes a\ncomparative analysis of CNNs and ViTs, along with a review of hybrid models and\nperformance enhancements. Technical challenges such as data requirements,\ncomputational demands, and model interpretability are addressed, along with\npotential solutions. Finally, we outline future research directions and\ntechnological advancements that could further support the integration of ViTs\nin real-world agricultural settings. Our goal with this study is to offer\npractitioners and researchers a deeper understanding of how ViTs are poised to\ntransform smart and precision agriculture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21706v3", "cate": "cs.CV", "date": "2025-04-30", "updated": "2025-07-24", "AI": {"title_translation": "精准农业中的视觉Transformer：一项综合调查", "tldr": "本文全面调查了视觉Transformer（ViTs）在精准农业中的应用，涵盖其架构、从NLP到计算机视觉的演变、与CNN的比较、混合模型、技术挑战及未来研究方向，旨在促进ViTs在智能农业中的应用。", "motivation": "检测植物病害对作物健康和产量至关重要。传统方法在可扩展性和准确性方面存在局限性。视觉Transformer（ViTs）作为一种有前景的替代方案出现，能够更好地处理长距离依赖并提高视觉任务的可扩展性，因此有必要对其在精准农业中的应用进行探讨。", "method": "这是一篇综述性研究。文章首先介绍了视觉Transformer（ViTs）的基础架构，并讨论了它们从自然语言处理（NLP）到计算机视觉的转变，包括如何缓解传统模型（如卷积神经网络CNNs）的归纳偏置。研究对近期文献进行了全面回顾，重点关注关键方法、数据集和性能指标。此外，还对CNNs和ViTs进行了比较分析，并回顾了混合模型和性能增强技术。文章还探讨了数据需求、计算开销和模型可解释性等技术挑战及潜在解决方案，并展望了未来的研究方向。", "result": "Not mentioned in abstract", "conclusion": "本研究旨在让从业者和研究人员更深入地了解视觉Transformer（ViTs）如何有望改变智能和精准农业。", "translation": "检测植物病害是现代农业的一个关键方面，因为它在维持作物健康和提高整体产量方面发挥着关键作用。尽管传统方法仍然有价值，但它们通常依赖于人工检查或传统的机器学习技术，这两种方法在可扩展性和准确性方面都面临限制。最近，视觉Transformer（ViTs）作为一种有前景的替代方案出现，提供了处理长距离依赖的改进和视觉任务更好的可扩展性等优势。本综述探讨了ViTs在精准农业中的应用，涵盖了一系列任务。我们首先介绍ViTs的基础架构，并讨论它们从自然语言处理（NLP）到计算机视觉的转变。讨论包括传统模型（如卷积神经网络（CNNs））中的归纳偏置概念，以及ViTs如何缓解这些偏置。我们对近期文献进行了全面回顾，重点关注关键方法、数据集和性能指标。本研究还包括对CNNs和ViTs的比较分析，以及对混合模型和性能增强的回顾。解决了数据需求、计算需求和模型可解释性等技术挑战，并提出了潜在解决方案。最后，我们概述了未来的研究方向和技术进步，这些进步可以进一步支持ViTs在实际农业环境中的集成。本研究的目标是让实践者和研究人员更深入地了解ViTs如何有望改变智能和精准农业。", "summary": "本文是一项综合性调查，旨在探讨视觉Transformer（ViTs）在精准农业中的应用，特别是植物病害检测。论文详细介绍了ViT的架构及其从自然语言处理到计算机视觉的演变，并阐述了ViT如何克服传统方法（如卷积神经网络CNNs）的局限性，包括归纳偏置问题。文章回顾了相关文献中的关键方法、数据集和性能指标，并对CNNs和ViTs进行了比较分析，同时讨论了混合模型。此外，论文还提出了数据需求、计算开销和模型可解释性等技术挑战及其潜在解决方案，并展望了未来研究方向，以促进ViTs在实际农业环境中的集成应用。", "keywords": "视觉Transformer, 精准农业, 植物病害检测, 深度学习, 综述", "comments": "这是一篇有价值的综述性论文，系统地梳理了视觉Transformer在精准农业这一新兴领域的应用。其创新之处在于全面回顾了ViTs相较于传统CNNs的优势，并指出了当前面临的技术挑战，为未来的研究提供了清晰的路线图。这对于推动ViTs在智能农业中的实际部署具有重要的理论和实践意义。"}}
{"id": "2507.17995", "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID", "authors": ["Huy Nguyen", "Kien Nguyen", "Akila Pemasiri", "Akmal Jahan", "Clinton Fookes", "Sridha Sridharan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted atIEEE International Joint Conference on Biometrics (IJCB) 2025", "url": "http://arxiv.org/abs/2507.17995v1", "summary": "Person re-identification (Re-ID) across visible and infrared modalities is\ncrucial for 24-hour surveillance systems, but existing datasets primarily focus\non ground-level perspectives. While ground-based IR systems offer nighttime\ncapabilities, they suffer from occlusions, limited coverage, and vulnerability\nto obstructions--problems that aerial perspectives uniquely solve. To address\nthese limitations, we introduce AG-VPReID.VIR, the first aerial-ground\ncross-modality video-based person Re-ID dataset. This dataset captures 1,837\nidentities across 4,861 tracklets (124,855 frames) using both UAV-mounted and\nfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents\nunique challenges including cross-viewpoint variations, modality discrepancies,\nand temporal dynamics. Additionally, we propose TCC-VPReID, a novel\nthree-stream architecture designed to address the joint challenges of\ncross-platform and cross-modality person Re-ID. Our approach bridges the domain\ngaps between aerial-ground perspectives and RGB-IR modalities, through\nstyle-robust feature learning, memory-based cross-view adaptation, and\nintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR\npresents distinctive challenges compared to existing datasets, with our\nTCC-VPReID framework achieving significant performance gains across multiple\nevaluation protocols. Dataset and code are available at\nhttps://github.com/agvpreid25/AG-VPReID.VIR.", "comment": "Accepted atIEEE International Joint Conference on Biometrics (IJCB)\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.17995v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "AG-VPReID.VIR: 弥合空中和地面平台，用于基于视频的可见光-红外行人重识别", "tldr": "引入了AG-VPReID.VIR，首个空中-地面跨模态视频行人重识别数据集，并提出了TCC-VPReID，一种新颖的三流架构，旨在解决24小时监控中的挑战。", "motivation": "现有的可见光-红外行人重识别数据集主要关注地面视角，而地面红外系统存在遮挡、覆盖范围有限和易受阻碍等问题，这些是空中视角可以独特解决的。因此，需要一个能够弥合空中和地面平台之间鸿沟的视频可见光-红外行人重识别数据集和方法。", "method": "本文引入了AG-VPReID.VIR，这是首个空中-地面跨模态基于视频的行人重识别数据集，包含1,837个身份、4,861个轨迹（124,855帧），数据通过无人机和固定闭路电视摄像头在RGB和红外模态下捕获。此外，提出了TCC-VPReID，一种新颖的三流架构，通过风格鲁棒特征学习、基于记忆的跨视角适应和中介引导的时间建模，来解决跨平台和跨模态行人重识别的联合挑战，并弥合领域差距。", "result": "实验表明，与现有数据集相比，AG-VPReID.VIR数据集带来了独特的挑战。所提出的TCC-VPReID框架在多个评估协议中取得了显著的性能提升。", "conclusion": "本文成功引入了首个空中-地面跨模态视频行人重识别数据集AG-VPReID.VIR，并提出了一个新颖的三流架构TCC-VPReID，有效解决了跨平台和跨模态行人重识别的复杂问题，弥合了领域差距并显著提升了性能。", "translation": "行人重识别（Re-ID）在可见光和红外模态之间进行交叉识别对于24小时监控系统至关重要，但现有数据集主要关注地面视角。虽然地面红外系统提供夜间能力，但它们存在遮挡、覆盖范围有限和易受阻碍的问题——这些问题是空中视角独有的解决方案。为了解决这些限制，我们引入了AG-VPReID.VIR，这是首个空中-地面跨模态基于视频的行人Re-ID数据集。该数据集使用无人机和固定闭路电视摄像头在RGB和红外模态下捕获了1,837个身份，共4,861个轨迹（124,855帧）。AG-VPReID.VIR带来了独特的挑战，包括跨视角变化、模态差异和时间动态。此外，我们提出了TCC-VPReID，一种新颖的三流架构，旨在解决跨平台和跨模态行人Re-ID的联合挑战。我们的方法通过风格鲁棒特征学习、基于记忆的跨视角适应和中介引导的时间建模来弥合空中-地面视角和RGB-红外模态之间的领域差距。实验表明，与现有数据集相比，AG-VPReID.VIR呈现出独特的挑战，我们的TCC-VPReID框架在多个评估协议中取得了显著的性能提升。数据集和代码可在https://github.com/agvpreid25/AG-VPReID.VIR获取。", "summary": "本文针对24小时监控中现有地面视角数据集的局限性，引入了AG-VPReID.VIR，这是首个空中-地面跨模态基于视频的行人重识别数据集。同时，提出了一种新颖的三流架构TCC-VPReID，该架构通过风格鲁棒特征学习、基于记忆的跨视角适应和中介引导的时间建模来弥合空中-地面视角和RGB-红外模态之间的领域差距。实验证实了该数据集的独特挑战性以及TCC-VPReID框架的优越性能。", "keywords": "行人重识别, 空中-地面, 跨模态, 视频, 数据集", "comments": "该论文通过引入首个弥合空中和地面平台之间鸿沟的可见光-红外行人重识别数据集，填补了24小时监控领域的一个关键空白，做出了重要贡献。所提出的TCC-VPReID架构通过多流方法处理跨平台和跨模态挑战，具有创新性，提供了一个鲁棒的解决方案。数据集和代码的发布也对研究社区非常有价值。"}}
{"id": "2507.18144", "title": "Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement", "authors": ["Jinhong He", "Minglong Xue", "Zhipu Liu", "Mingliang Zhou", "Aoxiang Ning", "Palaiahnakote Shivakumara"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10page", "url": "http://arxiv.org/abs/2507.18144v1", "summary": "Low-light image enhancement aims to improve the visibility of degraded images\nto better align with human visual perception. While diffusion-based methods\nhave shown promising performance due to their strong generative capabilities.\nHowever, their unidirectional modelling of degradation often struggles to\ncapture the complexity of real-world degradation patterns, leading to\nstructural inconsistencies and pixel misalignments. To address these\nchallenges, we propose a bidirectional diffusion optimization mechanism that\njointly models the degradation processes of both low-light and normal-light\nimages, enabling more precise degradation parameter matching and enhancing\ngeneration quality. Specifically, we perform bidirectional diffusion-from\nlow-to-normal light and from normal-to-low light during training and introduce\nan adaptive feature interaction block (AFI) to refine feature representation.\nBy leveraging the complementarity between these two paths, our approach imposes\nan implicit symmetry constraint on illumination attenuation and noise\ndistribution, facilitating consistent degradation learning and improving the\nmodels ability to perceive illumination and detail degradation. Additionally,\nwe design a reflection-aware correction module (RACM) to guide color\nrestoration post-denoising and suppress overexposed regions, ensuring content\nconsistency and generating high-quality images that align with human visual\nperception. Extensive experiments on multiple benchmark datasets demonstrate\nthat our method outperforms state-of-the-art methods in both quantitative and\nqualitative evaluations while generalizing effectively to diverse degradation\nscenarios. Code at https://github.com/hejh8/BidDiff", "comment": "10page", "pdf_url": "http://arxiv.org/pdf/2507.18144v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "降质一致性学习：基于双向扩散的低光图像增强", "tldr": "本文提出了一种通过双向扩散实现降质一致性学习的新方法，用于低光图像增强。该方法通过同时建模低光和正常光图像的降质过程，引入自适应特征交互块（AFI）和反射感知校正模块（RACM），解决了现有单向扩散模型在处理复杂降质时的局限性，并在多个基准数据集上超越了现有最先进方法。", "motivation": "现有的基于扩散的低光图像增强方法虽然表现出色，但其单向降质建模难以捕捉真实世界降质模式的复杂性，导致结构不一致和像素错位。", "method": "本文提出了一种双向扩散优化机制，联合建模低光和正常光图像的降质过程，以实现更精确的降质参数匹配。在训练过程中，执行从低光到正常光以及从正常光到低光的双向扩散。引入了自适应特征交互块（AFI）来优化特征表示，并通过两个路径的互补性施加隐式对称约束，以促进降质一致性学习。此外，设计了反射感知校正模块（RACM）来指导去噪后的色彩恢复并抑制过曝区域。", "result": "在多个基准数据集上进行的广泛实验表明，本文方法在定量和定性评估中均优于现有最先进方法，并且能有效泛化到不同的降质场景。", "conclusion": "通过引入双向扩散优化机制、自适应特征交互块和反射感知校正模块，本文方法有效解决了低光图像增强中复杂降质模式的建模挑战，实现了降质一致性学习，并显著提升了图像增强的质量和泛化能力，使其在多种退化场景下均表现出色。", "translation": "低光图像增强旨在改善退化图像的可见性，使其更好地符合人类视觉感知。尽管基于扩散的方法因其强大的生成能力而显示出有希望的性能。然而，它们对降质的单向建模通常难以捕捉真实世界降质模式的复杂性，导致结构不一致和像素错位。为了解决这些挑战，我们提出了一种双向扩散优化机制，该机制联合建模低光和正常光图像的降质过程，从而实现更精确的降质参数匹配并提高生成质量。具体来说，我们在训练期间执行双向扩散——从低光到正常光以及从正常光到低光，并引入自适应特征交互块（AFI）来优化特征表示。通过利用这两个路径之间的互补性，我们的方法对光照衰减和噪声分布施加了隐式对称约束，促进了降质一致性学习，并提高了模型感知光照和细节降质的能力。此外，我们设计了一个反射感知校正模块（RACM）来指导去噪后的色彩恢复并抑制过曝区域，确保内容一致性并生成符合人类视觉感知的高质量图像。在多个基准数据集上进行的广泛实验表明，我们的方法在定量和定性评估中均优于现有最先进方法，同时能有效泛化到不同的降质场景。代码可在 https://github.com/hejh8/BidDiff 获取。", "summary": "针对现有低光图像增强中单向扩散模型难以捕捉复杂降质模式的问题，本文提出了一种基于双向扩散的降质一致性学习方法。该方法通过联合建模低光和正常光图像的降质过程，并在训练中执行双向扩散。为了优化特征表示和处理色彩恢复及过曝问题，模型引入了自适应特征交互块（AFI）和反射感知校正模块（RACM）。实验结果表明，该方法在多个基准数据集上均优于现有最先进方法，并展现出良好的泛化能力。", "keywords": "低光图像增强, 双向扩散, 降质一致性学习, 自适应特征交互, 反射感知校正", "comments": "本文的创新点在于提出了双向扩散优化机制，通过同时建模低光和正常光图像的降质过程，有效解决了单向扩散模型在处理复杂真实世界降质时遇到的结构不一致和像素错位问题。引入的AFI和RACM模块进一步增强了模型的特征学习和图像恢复能力，特别是对色彩和过曝区域的处理。这种降质一致性学习方法为低光图像增强领域提供了一个新颖且高效的解决方案，其在性能和泛化能力上的提升具有重要意义。"}}
{"id": "2507.18051", "title": "The TEA-ASLP System for Multilingual Conversational Speech Recognition and Speech Diarization in MLC-SLM 2025 Challenge", "authors": ["Hongfei Xue", "Kaixun Huang", "Zhikai Zhou", "Shen Huang", "Shidong Shang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Interspeech 2025 workshop", "url": "http://arxiv.org/abs/2507.18051v1", "summary": "This paper presents the TEA-ASLP's system submitted to the MLC-SLM 2025\nChallenge, addressing multilingual conversational automatic speech recognition\n(ASR) in Task I and speech diarization ASR in Task II. For Task I, we enhance\nIdeal-LLM model by integrating known language identification and a multilingual\nMOE LoRA structure, along with using CTC-predicted tokens as prompts to improve\nautoregressive generation. The model is trained on approximately 180k hours of\nmultilingual ASR data. In Task II, we replace the baseline English-Chinese\nspeaker diarization model with a more suitable English-only version. Our\napproach achieves a 30.8% reduction in word error rate (WER) compared to the\nbaseline speech language model, resulting in a final WER of 9.60% in Task I and\na time-constrained minimum-permutation WER of 17.49% in Task II, earning first\nand second place in the respective challenge tasks.", "comment": "Interspeech 2025 workshop", "pdf_url": "http://arxiv.org/pdf/2507.18051v1", "cate": "cs.SD", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MLC-SLM 2025 挑战赛中用于多语言会话语音识别和语音分离的 TEA-ASLP 系统", "tldr": "TEA-ASLP 团队在 MLC-SLM 2025 挑战赛中提交了其多语言会话语音识别和语音分离系统，通过增强 Ideal-LLM 模型和优化语音分离模型，在两个任务中分别获得第一和第二名。", "motivation": "该研究旨在参加 MLC-SLM 2025 挑战赛，解决多语言会话自动语音识别 (ASR) 和语音分离 ASR 问题。", "method": "任务一（多语言会话 ASR）：增强 Ideal-LLM 模型，集成已知语言识别和多语言 MOE LoRA 结构，并使用 CTC 预测的 token 作为提示来改进自回归生成。模型在约 18 万小时的多语言 ASR 数据上进行训练。任务二（语音分离 ASR）：将基线英汉扬声器分离模型替换为更合适的纯英文版本。", "result": "相较于基线语音语言模型，词错误率 (WER) 降低了 30.8%。任务一的最终 WER 为 9.60%，获得第一名。任务二的时间受限最小置换 WER 为 17.49%，获得第二名。", "conclusion": "TEA-ASLP 系统在 MLC-SLM 2025 挑战赛的多语言会话语音识别和语音分离任务中表现出色，分别获得第一和第二名，证明了其方法的有效性。", "translation": "本文介绍了 TEA-ASLP 团队提交给 MLC-SLM 2025 挑战赛的系统，该系统旨在解决任务一中的多语言会话自动语音识别 (ASR) 和任务二中的语音分离 ASR 问题。对于任务一，我们通过集成已知语言识别和多语言 MOE LoRA 结构来增强 Ideal-LLM 模型，并使用 CTC 预测的 token 作为提示来改进自回归生成。该模型在约 18 万小时的多语言 ASR 数据上进行训练。在任务二中，我们将基线英汉扬声器分离模型替换为更适合的纯英文版本。我们的方法使词错误率 (WER) 相较于基线语音语言模型降低了 30.8%，最终任务一的 WER 为 9.60%，任务二的时间受限最小置换 WER 为 17.49%，分别在各自的挑战任务中获得第一和第二名。", "summary": "本文详细介绍了 TEA-ASLP 团队为 MLC-SLM 2025 挑战赛设计并提交的系统。该系统专注于多语言会话自动语音识别 (ASR) 和语音分离 ASR。在 ASR 任务中，团队通过整合语言识别、MOE LoRA 结构以及 CTC 提示改进了 Ideal-LLM 模型。在语音分离任务中，他们采用了一个优化的纯英文模型。实验结果显示，该系统在词错误率上取得了显著降低，并在挑战赛中分别斩获了第一和第二名。", "keywords": "多语言语音识别, 语音分离, MLC-SLM 2025 挑战赛, Ideal-LLM, 词错误率", "comments": "该论文展示了在多语言会话 ASR 和语音分离领域的实际应用和性能优化。其创新点在于对 Ideal-LLM 模型的具体增强措施，如结合语言识别和 MOE LoRA 结构，以及利用 CTC 预测的 token 作为提示，这些方法有效地提升了模型的自回归生成能力。此外，针对特定任务（如纯英文语音分离）进行模型优化也体现了实用性。在国际挑战赛中取得优异成绩，充分证明了该系统在复杂多语言环境下的鲁棒性和先进性。"}}
{"id": "2507.18504", "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models", "authors": ["Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18504v1", "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18504v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "并非所有特征都值得关注：面向语言模型的图引导依赖学习用于表格数据生成", "tldr": "本文提出GraDe，一种将稀疏依赖图集成到LLM注意力机制中的方法，以解决LLM在表格数据生成中对不重要特征的过度关注问题，从而提高复杂数据集上的性能。", "motivation": "大型语言模型（LLMs）在表格数据生成方面潜力巨大，但表格数据固有的稀疏特征级依赖性与LLMs的自注意力机制存在根本性不匹配。LLMs的自注意力机制会不可避免地将注意力分散到所有特征对上，从而稀释对关键关系的关注，尤其是在具有复杂依赖或语义模糊特征的数据集中。", "method": "本文提出了GraDe（Graph-Guided Dependency Learning），一种将稀疏依赖图明确集成到LLMs注意力机制中的新方法。GraDe采用一个轻量级的动态图学习模块，该模块由外部提取的功能依赖关系引导，优先处理关键特征交互，同时抑制不相关的交互。", "result": "在各种真实世界数据集上的实验表明，GraDe在复杂数据集上比现有基于LLM的方法性能高出12%，同时在合成数据质量方面与最先进的方法具有竞争力。", "conclusion": "GraDe是一种微创但有效的解决方案，为使用LLMs进行结构感知的表格数据建模提供了一种实用方法。", "translation": "大型语言模型（LLMs）通过建模文本化的特征-值对，在表格数据生成方面展现出强大潜力。然而，表格数据固有地表现出稀疏的特征级依赖关系，其中许多特征交互在结构上并不重要。这造成了一个根本性的不匹配，因为LLMs的自注意力机制不可避免地会将焦点分散到所有对上，稀释对关键关系的关注，特别是在具有复杂依赖或语义模糊特征的数据集中。为了解决这一限制，我们提出了GraDe（图引导依赖学习），这是一种将稀疏依赖图明确集成到LLMs注意力机制中的新方法。GraDe采用一个轻量级的动态图学习模块，由外部提取的功能依赖关系引导，优先处理关键特征交互，同时抑制不相关的交互。我们在各种真实世界数据集上的实验表明，GraDe在复杂数据集上的性能比现有基于LLM的方法高出12%，同时在合成数据质量方面与最先进的方法具有竞争力。我们的方法侵入性极小但有效，为使用LLMs进行结构感知的表格数据建模提供了一种实用解决方案。", "summary": "该论文提出GraDe，一种新的方法，通过将稀疏依赖图明确整合到大型语言模型（LLMs）的注意力机制中，以解决LLMs在表格数据生成中对不重要特征的过度关注问题。GraDe利用一个图学习模块来优先处理关键特征交互。实验证明，GraDe在复杂数据集上优于现有LLM方法，并在合成数据质量方面与最先进方法相当，提供了一种实用的结构感知表格数据建模方案。", "keywords": "表格数据生成, 语言模型, 图引导, 依赖学习, 自注意力", "comments": "GraDe的创新点在于它通过显式整合稀疏依赖图来优化LLM的自注意力机制，解决了传统LLM在处理表格数据时注意力分配不均的问题。这种方法既保持了LLM的强大生成能力，又通过引入结构信息提高了生成数据的质量和效率，尤其是在复杂数据集上表现突出。其“微创”的设计也增强了其实用性。"}}
{"id": "2505.20268", "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits", "authors": ["Fan Chen", "Zeyu Jia", "Alexander Rakhlin", "Tengyang Xie"], "categories": ["cs.LG", "cs.AI", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20268v2", "summary": "Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20268v2", "cate": "cs.LG", "date": "2025-05-26", "updated": "2025-07-24", "AI": {"title_translation": "基于结果的在线强化学习：算法与基本限制", "tldr": "本文首次全面分析了基于结果的在线强化学习问题，提出了一种利用通用函数逼近的样本高效算法，并揭示了其统计极限，包括某些MDP的指数级分离。", "motivation": "基于结果反馈的强化学习面临一个根本性挑战：当奖励仅在轨迹终点观察到时，如何将功劳归因于正确的动作？", "method": "本文开发了一种可证明的样本高效算法，利用通用函数逼近来处理大型状态空间。此外，该方法还扩展到基于偏好的反馈设置，并展示了如何为确定性MDP消除完备性假设。", "result": "该算法实现了 $\\widetilde{O}({C_{\\rm cov} H^3}/{\\epsilon^2})$ 的样本复杂度，并在表格方法失效的大型或无限状态空间中有效。研究还揭示了基于结果的反馈与每步奖励之间可能存在的统计分离，对于某些MDP存在不可避免的指数级分离。对于确定性MDP，可以消除完备性假设。在基于偏好的反馈设置下也能实现等效的统计效率。", "conclusion": "这些结果共同构成了理解基于结果的强化学习统计特性的理论基础。", "translation": "基于结果反馈的强化学习面临一个根本性挑战：当奖励仅在轨迹终点观察到时，如何将功劳归因于正确的动作？本文首次对在线强化学习中具有通用函数逼近的这一问题进行了全面分析。我们开发了一种可证明的样本高效算法，实现了 $\\widetilde{O}({C_{\\rm cov} H^3}/{\\epsilon^2})$ 的样本复杂度，其中 $C_{\\rm cov}$ 是底层MDP的可覆盖系数。通过利用通用函数逼近，我们的方法在表格方法失效的大型或无限状态空间中也能有效工作，只需要值函数和奖励函数可以用适当的函数类表示。我们的结果还表征了基于结果的反馈何时与每步奖励在统计上分离，揭示了某些MDP不可避免的指数级分离。对于确定性MDP，我们展示了如何消除完备性假设，从而大大简化了算法。我们进一步将方法扩展到基于偏好的反馈设置，证明即使在信息更有限的情况下也能实现等效的统计效率。总之，这些结果构成了理解基于结果的强化学习统计特性的理论基础。", "summary": "本文解决了基于结果的在线强化学习中稀疏奖励下的信用分配问题。它提出了一种新颖的、样本高效的算法，利用通用函数逼近，使其在大状态空间中有效。该研究还揭示了基本的统计限制，包括某些MDP的指数级分离，并将方法扩展到基于偏好的反馈，为该领域奠定了理论基础。", "keywords": "强化学习, 基于结果反馈, 在线学习, 函数逼近, 样本复杂度", "comments": "该论文通过首次全面分析带有通用函数逼近的基于结果的在线强化学习，做出了重要的理论贡献。其创新之处在于开发了一种可扩展到大型状态空间的样本高效算法，并揭示了基本的统计限制，包括令人惊讶的指数级分离。这项工作对于推进稀疏、延迟奖励设置下的强化学习至关重要。"}}
{"id": "2309.15951", "title": "IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation", "authors": ["Xiaoqian Liu", "Yuhan Dong", "Yiqing Li", "Yousi Lin", "Ming Gan"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.15951v3", "summary": "As emerging applications demand increasingly higher throughput, IEEE standard\n802.11be -- Extremely High Throughput (EHT), also known as Wi-Fi 7, was\npublished on July 22, 2025. It can be used to meet the demand for the\nthroughput of 4K/8K videos up to tens of Gbps and low-latency video\napplications such as virtual reality (VR) and augmented reality (AR). Wi-Fi 7\nnot only scales Wi-Fi 6 with doubled bandwidth, but also supports real-time\napplications, which brings revolutionary changes to Wi-Fi. In this article, we\nstart by introducing the main objectives and timeline of Wi-Fi 7 and then list\nthe latest key techniques which promote the performance improvement of Wi-Fi 7.\nFinally, we validate the most critical objectives of Wi-Fi 7 -- the potential\nup to 30 Gbps throughput and lower latency. System-level simulation results\nsuggest that by combining the new techniques, Wi-Fi 7 achieves 30 Gbps\nthroughput and lower latency than Wi-Fi 6.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.15951v3", "cate": "cs.NI", "date": "2023-09-27", "updated": "2025-07-24", "AI": {"title_translation": "IEEE 802.11be Wi-Fi 7：功能概述与性能评估", "tldr": "Wi-Fi 7（IEEE 802.11be）旨在满足高吞吐量和低延迟应用需求，通过系统级仿真验证，其吞吐量可达30 Gbps，且延迟低于Wi-Fi 6。", "motivation": "随着新兴应用对更高吞吐量的需求，如4K/8K视频和VR/AR等低延迟视频应用，IEEE 802.11be (Wi-Fi 7) 应运而生，旨在提供数十Gbps的吞吐量。", "method": "本文首先介绍了Wi-Fi 7的主要目标和时间线，然后列举了提升其性能的关键技术。最后，通过系统级仿真验证了Wi-Fi 7高达30 Gbps的潜在吞吐量和更低的延迟。", "result": "系统级仿真结果表明，通过结合新引入的技术，Wi-Fi 7能够达到30 Gbps的吞吐量，并实现比Wi-Fi 6更低的延迟。", "conclusion": "Wi-Fi 7通过引入新关键技术，显著提升了吞吐量并降低了延迟，能够满足新兴高带宽、低延迟应用的需求，代表了Wi-Fi技术的革命性进步。", "translation": "随着新兴应用对吞吐量要求越来越高，IEEE 802.11be —— 极高吞吐量 (EHT) 标准，也被称为 Wi-Fi 7，已于2025年7月22日发布。它能够满足高达数十Gbps的4K/8K视频吞吐量需求，以及虚拟现实 (VR) 和增强现实 (AR) 等低延迟视频应用的需求。Wi-Fi 7 不仅在 Wi-Fi 6 的基础上将带宽翻倍，还支持实时应用，这为 Wi-Fi 带来了革命性的变化。在本文中，我们首先介绍了 Wi-Fi 7 的主要目标和时间线，然后列举了促进 Wi-Fi 7 性能提升的最新关键技术。最后，我们验证了 Wi-Fi 7 最关键的目标——潜在高达30 Gbps的吞吐量和更低的延迟。系统级仿真结果表明，通过结合这些新技术，Wi-Fi 7 实现了30 Gbps的吞吐量，并且延迟低于 Wi-Fi 6。", "summary": "本文介绍了为满足新兴应用高吞吐量和低延迟需求而发布的IEEE 802.11be (Wi-Fi 7) 标准。Wi-Fi 7在Wi-Fi 6基础上将带宽翻倍，并支持实时应用。文章详细阐述了Wi-Fi 7的主要目标、时间线和关键技术，并通过系统级仿真验证了其高达30 Gbps的吞吐量和优于Wi-Fi 6的低延迟性能。", "keywords": "Wi-Fi 7, IEEE 802.11be, 高吞吐量, 低延迟, 系统仿真", "comments": "这篇论文概述了即将发布的Wi-Fi 7标准，强调了其在吞吐量和延迟方面的显著提升，以满足4K/8K视频、VR/AR等新兴应用的需求。其创新性在于对新标准关键技术的总结和性能的初步验证，对于理解Wi-Fi技术发展趋势具有重要意义。"}}
{"id": "2503.16743", "title": "SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence Based On the Principles of Recursive Compression and Algorithmic Probability", "authors": ["Alberto Hernández-Espinosa", "Luan Ozelim", "Felipe S. Abrahão", "Hector Zenil"], "categories": ["cs.AI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      51 pages + Technical Supplementary Information, 79 pages total", "url": "http://arxiv.org/abs/2503.16743v4", "summary": "We introduce an open-ended test grounded in algorithmic probability that can\navoid benchmark contamination in the quantitative evaluation of frontier models\nin the context of their Artificial General Intelligence (AGI) and\nSuperintelligence (ASI) claims. Unlike other tests, this test does not rely on\nstatistical compression methods (such as GZIP or LZW), which are more closely\nrelated to Shannon entropy than to Kolmogorov complexity and are not able to\ntest beyond simple pattern matching. The test challenges aspects of AI, in\nparticular LLMs, related to features of intelligence of fundamental nature such\nas synthesis and model creation in the context of inverse problems (generating\nnew knowledge from observation). We argue that metrics based on model\nabstraction and abduction (optimal Bayesian `inference') for predictive\n`planning' can provide a robust framework for testing intelligence, including\nnatural intelligence (human and animal), narrow AI, AGI, and ASI. We found that\nLLM model versions tend to be fragile and incremental as a result of\nmemorisation only with progress likely driven by the size of training data. The\nresults were compared with a hybrid neurosymbolic approach that theoretically\nguarantees universal intelligence based on the principles of algorithmic\nprobability and Kolmogorov complexity. The method outperforms LLMs in a\nproof-of-concept on short binary sequences. We prove that compression is\nequivalent and directly proportional to a system's predictive power and vice\nversa. That is, if a system can better predict it can better compress, and if\nit can better compress, then it can better predict. Our findings strengthen the\nsuspicion regarding the fundamental limitations of LLMs, exposing them as\nsystems optimised for the perception of mastery over human language.", "comment": "51 pages + Technical Supplementary Information, 79 pages total", "pdf_url": "http://arxiv.org/pdf/2503.16743v4", "cate": "cs.AI", "date": "2025-03-20", "updated": "2025-07-24", "AI": {"title_translation": "SuperARC：一种基于递归压缩和算法概率原理的狭义、通用和超级智能的不可知测试", "tldr": "本文引入了一种基于算法概率的开放式测试SuperARC，旨在避免基准污染，评估前沿模型在AGI和ASI方面的能力。该测试不同于依赖统计压缩的方法，能挑战LLM的合成和模型创建能力，并指出LLM存在局限性，而神经符号方法在概念验证中表现更好。", "motivation": "现有测试方法依赖统计压缩，更接近香农熵而非柯尔莫哥洛夫复杂度，无法测试超越简单模式匹配的能力，并且容易导致基准污染。作者认为需要一种更鲁棒的框架来测试包括人类智能、狭义AI、AGI和ASI在内的智能，特别是在评估前沿模型在AGI和ASI方面的声明时。", "method": "本文提出了一种基于算法概率的开放式测试SuperARC，该测试不依赖于统计压缩方法（如GZIP或LZW），而是基于递归压缩和柯尔莫哥洛夫复杂度原理。它挑战AI（特别是LLM）在逆向问题中进行合成和模型创建等基本智能特征。研究通过将LLM模型版本与理论上保证通用智能的混合神经符号方法进行比较，并在短二进制序列上进行了概念验证。", "result": "研究发现LLM模型版本趋于脆弱和增量化，其进展可能仅由训练数据大小驱动，且主要通过记忆实现。在短二进制序列的概念验证中，混合神经符号方法表现优于LLM。研究证明压缩能力与系统的预测能力等效且成正比：如果一个系统能更好地预测，它就能更好地压缩；反之亦然。这些发现加强了对LLM根本局限性的怀疑，揭示它们是为感知人类语言掌握而优化的系统。", "conclusion": "LLM存在根本性局限，它们主要优化于对人类语言的感知掌握，其智能特征可能仅限于记忆和增量进步。基于算法概率和柯尔莫哥洛夫复杂度的测试方法，以及神经符号方法，提供了评估通用智能的更鲁棒框架。", "translation": "我们引入了一种基于算法概率的开放式测试，可以避免在定量评估前沿模型的人工通用智能（AGI）和超级智能（ASI）主张时出现基准污染。与其他测试不同，该测试不依赖于统计压缩方法（如GZIP或LZW），这些方法与香农熵的关系更密切，而非柯尔莫哥洛夫复杂度，并且无法测试超出简单模式匹配的能力。该测试挑战了AI，特别是大型语言模型（LLM）在逆向问题（从观察中生成新知识）背景下与合成和模型创建等基本智能特征相关的方面。我们认为，基于模型抽象和溯因（最优贝叶斯“推理”）的度量标准，用于预测性“规划”，可以为测试智能提供一个鲁棒的框架，包括自然智能（人类和动物）、狭义AI、AGI和ASI。我们发现LLM模型版本往往脆弱且增量化，这仅仅是记忆的结果，其进展可能仅由训练数据的大小驱动。结果与一种混合神经符号方法进行了比较，该方法理论上基于算法概率和柯尔莫哥洛夫复杂度原理保证了通用智能。该方法在短二进制序列的概念验证中优于LLM。我们证明了压缩与系统的预测能力是等效且直接成比例的，反之亦然。也就是说，如果一个系统能更好地预测，它就能更好地压缩；如果它能更好地压缩，那么它就能更好地预测。我们的发现加强了对LLM根本局限性的怀疑，揭示它们是为感知人类语言掌握而优化的系统。", "summary": "本文提出了一种名为SuperARC的开放式测试，该测试基于算法概率、递归压缩和柯尔莫哥洛夫复杂度，旨在克服现有基准污染问题，并更准确地评估AI模型的通用智能和超级智能。与依赖统计压缩的方法不同，SuperARC专注于测试AI在逆向问题中的合成和模型创建能力。研究发现LLM主要通过记忆实现增量进步，并且存在根本性局限，而一种混合神经符号方法在概念验证中表现出更强的能力。论文还论证了压缩与预测能力之间的直接关系，并强调了LLM作为语言感知优化系统的局限性。", "keywords": "算法概率, 柯尔莫哥洛夫复杂度, 通用智能, 大型语言模型, 递归压缩", "comments": "本文创新性地提出了SuperARC测试，其核心在于利用算法概率和柯尔莫哥洛夫复杂度来评估AI的通用智能，而非传统的统计压缩方法，这有助于避免基准污染并更深入地探究AI的本质智能特征。其重要性在于提供了一个更严谨的评估框架，尤其是在当前LLM快速发展但其深层智能受质疑的背景下。论文揭示了LLM可能存在的根本局限性，即其进步可能主要依赖于记忆和数据规模，而非真正的通用智能或创造性。这一发现对于AI领域未来研究方向具有指导意义，促使研究者探索超越当前LLM范式的新方法。"}}
{"id": "2507.18532", "title": "COT-AD: Cotton Analysis Dataset", "authors": ["Akbar Ali", "Mahek Vyas", "Soumyaratna Debnath", "Chanda Grover Kamra", "Jaidev Sanjay Khalane", "Reuben Shibu Devanesan", "Indra Deep Mastan", "Subramanian Sankaranarayanan", "Pankaj Khanna", "Shanmuganathan Raman"], "categories": ["cs.CV", "I.4.9; I.5.4; H.2.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Dataset publicly available at: this https URL . Accepted to IEEE International Conference on Image Processing (ICIP) 2025", "url": "http://arxiv.org/abs/2507.18532v1", "summary": "This paper presents COT-AD, a comprehensive Dataset designed to enhance\ncotton crop analysis through computer vision. Comprising over 25,000 images\ncaptured throughout the cotton growth cycle, with 5,000 annotated images,\nCOT-AD includes aerial imagery for field-scale detection and segmentation and\nhigh-resolution DSLR images documenting key diseases. The annotations cover\npest and disease recognition, vegetation, and weed analysis, addressing a\ncritical gap in cotton-specific agricultural datasets. COT-AD supports tasks\nsuch as classification, segmentation, image restoration, enhancement, deep\ngenerative model-based cotton crop synthesis, and early disease management,\nadvancing data-driven crop management", "comment": "Dataset publicly available at:\n  https://ieee-dataport.org/documents/cot-adcotton-analysis-dataset. Accepted\n  to IEEE International Conference on Image Processing (ICIP) 2025", "pdf_url": "http://arxiv.org/pdf/2507.18532v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "COT-AD：棉花分析数据集", "tldr": "COT-AD是一个用于棉花作物分析的综合数据集，包含25,000多张图像，旨在通过计算机视觉增强棉花作物分析。", "motivation": "本研究旨在弥补棉花特定农业数据集的关键空白，以提升通过计算机视觉进行棉花作物分析的能力。", "method": "本研究提出了COT-AD数据集，该数据集包含超过25,000张在棉花生长周期中捕获的图像，其中5,000张已标注。数据集包括用于田间尺度检测和分割的航空图像，以及记录关键病害的高分辨率单反相机图像。标注内容涵盖病虫害识别、植被和杂草分析。", "result": "COT-AD数据集支持多种计算机视觉任务，包括分类、分割、图像恢复、图像增强、基于深度生成模型的棉花作物合成以及早期病害管理。", "conclusion": "COT-AD数据集的创建旨在通过提供一个全面的、标注丰富的资源，推动数据驱动的作物管理在棉花领域的应用。", "translation": "本文介绍了COT-AD，一个旨在通过计算机视觉增强棉花作物分析的综合数据集。该数据集包含在棉花生长周期中捕获的25,000多张图像，其中5,000张已标注。COT-AD包括用于田间尺度检测和分割的航空图像，以及记录关键病害的高分辨率单反相机图像。标注内容涵盖病虫害识别、植被和杂草分析，弥补了棉花特定农业数据集的关键空白。COT-AD支持分类、分割、图像恢复、图像增强、基于深度生成模型的棉花作物合成以及早期病害管理等任务，从而推动数据驱动的作物管理。", "summary": "本文介绍了COT-AD，一个包含25,000多张图像（5,000张已标注）的棉花分析数据集。该数据集整合了航空和高分辨率单反图像，并对病虫害、植被和杂草进行了标注，旨在弥补棉花农业数据集的空白。COT-AD支持多种计算机视觉任务，如分类、分割、图像恢复、增强和作物合成，以促进数据驱动的作物管理和早期病害管理。", "keywords": "棉花分析, 数据集, 计算机视觉, 农业, COT-AD", "comments": "COT-AD数据集的创新之处在于它填补了棉花特定农业数据集的空白，提供了大量的图像和详细的标注，这对于推进棉花领域的计算机视觉研究和应用具有重要意义。其包含多种图像类型和广泛的标注内容，使其成为一个多功能且有价值的资源。"}}
{"id": "2507.17841", "title": "Better Bounds for Semi-Streaming Single-Source Shortest Paths", "authors": ["Sepehr Assadi", "Gary Hoppenworth", "Janani Sundaresan"], "categories": ["cs.DS", "cs.CC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      64 pages, 9 figures", "url": "http://arxiv.org/abs/2507.17841v1", "summary": "In the semi-streaming model, an algorithm must process any $n$-vertex graph\nby making one or few passes over a stream of its edges, use $O(n \\cdot\n\\text{polylog }n)$ words of space, and at the end of the last pass, output a\nsolution to the problem at hand. Approximating (single-source) shortest paths\non undirected graphs is a longstanding open question in this model. In this\nwork, we make progress on this question from both upper and lower bound fronts:\n  We present a simple randomized algorithm that for any $\\epsilon > 0$, with\nhigh probability computes $(1+\\epsilon)$-approximate shortest paths from a\ngiven source vertex in \\[\n  O\\left(\\frac{1}{\\epsilon} \\cdot n \\log^3 n \\right)~\\text{space} \\quad\n\\text{and} \\quad O\\left(\\frac{1}{\\epsilon} \\cdot \\left(\\frac{\\log n}{\\log\\log\nn} \\right) ^2\\right) ~\\text{passes}.\n  \\] The algorithm can also be derandomized and made to work on dynamic streams\nat a cost of some extra $\\text{poly}(\\log n, 1/\\epsilon)$ factors only in the\nspace. Previously, the best known algorithms for this problem required\n$1/\\epsilon \\cdot \\log^{c}(n)$ passes, for an unspecified large constant $c$.\n  We prove that any semi-streaming algorithm that with large constant\nprobability outputs any constant approximation to shortest paths from a given\nsource vertex (even to a single fixed target vertex and only the distance, not\nnecessarily the path) requires \\[ \\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)\n~\\text{passes}. \\] We emphasize that our lower bound holds for any\nconstant-factor approximation of shortest paths. Previously, only constant-pass\nlower bounds were known and only for small approximation ratios below two.\n  Our results collectively reduce the gap in the pass complexity of\napproximating single-source shortest paths in the semi-streaming model from\n$\\text{polylog } n$ vs $\\omega(1)$ to only a quadratic gap.", "comment": "64 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.17841v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "半流式单源最短路径的更好界限", "tldr": "本文在半流模型中，针对单源最短路径问题，提出了一个改进的近似算法（上界）并证明了新的通过次数下界，显著缩小了该问题的通过次数复杂性差距。", "motivation": "在半流式模型中，近似无向图上的单源最短路径是一个长期存在的开放问题。", "method": "本文提出了一个简单的随机算法，用于计算 $(1+\\epsilon)$-近似的单源最短路径，该算法可以去随机化并应用于动态流。同时，本文还证明了任何半流式算法在给定源点计算常数近似最短路径所需的通过次数下界。", "result": "上界：提出了一个随机算法，以高概率计算 $(1+\\epsilon)$-近似的单源最短路径，所需空间为 $O\\left(\\frac{1}{\\epsilon} \\cdot n \\log^3 n \\right)$，通过次数为 $O\\left(\\frac{1}{\\epsilon} \\cdot \\left(\\frac{\\log n}{\\log\\log n} \\right) ^2\\right)$。该算法可去随机化并应用于动态流。下界：证明了任何半流式算法，即使只计算常数近似最短路径的距离，也需要 $\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$ 次通过。综合结果：整体上将半流模型中近似单源最短路径的通过次数复杂性差距从 $\\text{polylog } n$ vs $\\omega(1)$ 缩小到仅二次方差距。", "conclusion": "本文通过提供新的上界和下界，显著缩小了半流模型中近似单源最短路径问题的通过次数复杂性差距。", "translation": "在半流式模型中，算法必须通过对边流进行一次或几次遍历来处理任何 $n$ 顶点图，使用 $O(n \\cdot \\text{polylog }n)$ 字的空间，并在最后一次遍历结束时输出问题的解决方案。在这一模型中，近似（单源）无向图上的最短路径是一个长期存在的开放问题。在这项工作中，我们从上界和下界两方面对这个问题取得了进展：我们提出了一个简单的随机算法，对于任何 $\\epsilon > 0$，以高概率计算给定源点的 $(1+\\epsilon)$-近似最短路径，所需空间为 $O\\left(\\frac{1}{\\epsilon} \\cdot n \\log^3 n \\right)$，通过次数为 $O\\left(\\frac{1}{\\epsilon} \\cdot \\left(\\frac{\\log n}{\\log\\log n} \\right) ^2\\right)$。该算法还可以去随机化，并在动态流上工作，只需付出一些额外的 $\\text{poly}(\\log n, 1/\\epsilon)$ 因子在空间上的代价。此前，该问题的最佳已知算法需要 $1/\\epsilon \\cdot \\log^{c}(n)$ 次通过，其中 $c$ 是一个未指定的大常数。我们证明，任何半流式算法，如果以大的常数概率输出给定源点（甚至仅针对单个固定目标顶点和仅距离，不一定是路径）的任何常数近似最短路径，则需要 $\\Omega\\left(\\frac{\\log n}{\\log\\log n}\\right)$ 次通过。我们强调，我们的下界适用于任何常数因子近似的最短路径。此前，只知道常数次通过的下界，且仅适用于小于二的小近似比。我们的结果共同将半流模型中近似单源最短路径的通过次数复杂性差距从 $\\text{polylog } n$ 对 $\\omega(1)$ 缩小到仅二次方差距。", "summary": "本文研究了半流式模型中单源最短路径的近似问题。作者提出了一个简单的随机算法，能以较低的空间和通过次数计算 $(1+\\epsilon)$-近似最短路径，并可去随机化和应用于动态流。同时，本文还证明了该问题在常数近似下的通过次数新下界。这些工作显著缩小了该问题在半流模型中的通过次数复杂性差距，从多对数与超常数差距缩小到仅二次方差距。", "keywords": "半流模型, 单源最短路径, 近似算法, 通过次数, 空间复杂度", "comments": "本文在半流模型中对单源最短路径问题取得了重要进展。其创新之处在于同时提供了改进的算法（上界）和更紧密的下界，这对于理解该问题在资源受限环境下的固有复杂性至关重要。特别是，将通过次数差距从多对数与超常数缩小到二次方，是理论计算机科学领域的一个显著突破，对未来半流算法的设计具有指导意义。"}}
{"id": "2502.20158", "title": "Learning to Generalize without Bias for Open-Vocabulary Action Recognition", "authors": ["Yating Yu", "Congqi Cao", "Yifan Zhang", "Yanning Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025 (Highlight)", "url": "http://arxiv.org/abs/2502.20158v2", "summary": "Leveraging the effective visual-text alignment and static generalizability\nfrom CLIP, recent video learners adopt CLIP initialization with further\nregularization or recombination for generalization in open-vocabulary action\nrecognition in-context. However, due to the static bias of CLIP, such video\nlearners tend to overfit on shortcut static features, thereby compromising\ntheir generalizability, especially to novel out-of-context actions. To address\nthis issue, we introduce Open-MeDe, a novel Meta-optimization framework with\nstatic Debiasing for Open-vocabulary action recognition. From a fresh\nperspective of generalization, Open-MeDe adopts a meta-learning approach to\nimprove known-to-open generalizing and image-to-video debiasing in a\ncost-effective manner. Specifically, Open-MeDe introduces a cross-batch\nmeta-optimization scheme that explicitly encourages video learners to quickly\ngeneralize to arbitrary subsequent data via virtual evaluation, steering a\nsmoother optimization landscape. In effect, the free of CLIP regularization\nduring optimization implicitly mitigates the inherent static bias of the video\nmeta-learner. We further apply self-ensemble over the optimization trajectory\nto obtain generic optimal parameters that can achieve robust generalization to\nboth in-context and out-of-context novel data. Extensive evaluations show that\nOpen-MeDe not only surpasses state-of-the-art regularization methods tailored\nfor in-context open-vocabulary action recognition but also substantially excels\nin out-of-context scenarios.Code is released at\nhttps://github.com/Mia-YatingYu/Open-MeDe.", "comment": "Accepted by ICCV2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2502.20158v2", "cate": "cs.CV", "date": "2025-02-27", "updated": "2025-07-24", "AI": {"title_translation": "学习无偏泛化以实现开放词汇动作识别", "tldr": "针对CLIP初始化视频学习器在开放词汇动作识别中存在的静态偏置问题，本文提出了Open-MeDe元优化框架，通过元学习和去偏置技术显著提升了模型在上下文内和上下文外新数据上的泛化能力。", "motivation": "现有的基于CLIP初始化的视频学习器在开放词汇动作识别中，由于CLIP的静态偏置，容易过拟合于捷径静态特征，从而损害其泛化能力，尤其是在新颖的上下文外动作识别上表现不佳。", "method": "本文提出了Open-MeDe，一个带有静态去偏置的元优化框架。它采用元学习方法来提升已知到开放的泛化能力和图像到视频的去偏置。具体而言，Open-MeDe引入了一种跨批次的元优化方案，通过虚拟评估鼓励视频学习器快速泛化到任意后续数据，并平滑优化过程。在优化过程中，不使用CLIP正则化隐式地减轻了视频元学习器的固有静态偏置。此外，还对优化轨迹进行自集成，以获得通用的最优参数，从而实现对上下文内和上下文外新数据的鲁棒泛化。", "result": "广泛评估表明，Open-MeDe不仅超越了为上下文内开放词汇动作识别量身定制的现有最先进正则化方法，而且在上下文外场景中表现显著优异。", "conclusion": "Open-MeDe通过创新的元优化框架和去偏置策略，有效解决了CLIP初始化视频学习器在开放词汇动作识别中的静态偏置问题，显著提高了模型在各种场景下的泛化能力。", "translation": "利用CLIP有效的视觉-文本对齐和静态泛化能力，最近的视频学习器采用CLIP初始化，并进一步正则化或重组，以实现在上下文中开放词汇动作识别的泛化。然而，由于CLIP的静态偏置，此类视频学习器倾向于过拟合于捷径静态特征，从而损害其泛化能力，尤其是在新颖的上下文外动作识别上。为了解决这个问题，我们引入了Open-MeDe，一个新颖的带有静态去偏置的元优化框架，用于开放词汇动作识别。从泛化的新视角来看，Open-MeDe采用元学习方法，以经济高效的方式改进已知到开放的泛化和图像到视频的去偏置。具体而言，Open-MeDe引入了一种跨批次的元优化方案，通过虚拟评估明确鼓励视频学习器快速泛化到任意后续数据，从而引导更平滑的优化前景。实际上，优化过程中无需CLIP正则化隐式地减轻了视频元学习器的固有静态偏置。我们进一步在优化轨迹上应用自集成，以获得通用的最优参数，从而实现对上下文内和上下文外新数据的鲁棒泛化。广泛评估表明，Open-MeDe不仅超越了为上下文内开放词汇动作识别量身定制的现有最先进正则化方法，而且在上下文外场景中表现显著优异。代码已在https://github.com/Mia-YatingYu/Open-MeDe 发布。", "summary": "本文提出了一种名为Open-MeDe的新型元优化框架，旨在解决基于CLIP初始化的视频学习器在开放词汇动作识别中存在的静态偏置和泛化能力受损问题。Open-MeDe通过引入跨批次元优化方案和优化过程中避免CLIP正则化，有效去除了静态偏置，并通过自集成策略增强了模型对上下文内和上下文外新数据的鲁棒泛化能力。实验结果表明，Open-MeDe在开放词汇动作识别任务上显著优于现有方法。", "keywords": "开放词汇动作识别, 元学习, 静态偏置, 泛化, 去偏置", "comments": "这篇论文通过引入元学习和去偏置策略，创新性地解决了基于CLIP的视频学习器在开放词汇动作识别中遇到的静态偏置和泛化能力不足的问题。其跨批次元优化和自集成方法为提升模型在未知环境下的泛化能力提供了新的视角和有效途径，对于推动开放词汇学习和视频理解领域的发展具有重要意义。"}}
{"id": "2507.17950", "title": "Deep Learning-based Position-domain Channel Extrapolation for Cell-Free Massive MIMO", "authors": ["Jiajia Guo", "Chao-Kai Wen", "Xiao Li", "Shi Jin"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      IEEE TWC. copyright2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "url": "http://arxiv.org/abs/2507.17950v1", "summary": "To reduce channel acquisition overhead, spatial, time, and frequency-domain\nchannel extrapolation techniques have been widely studied. In this paper, we\npropose a novel deep learning-based Position-domain Channel Extrapolation\nframework (named PCEnet) for cell-free massive multiple-input multiple-output\n(MIMO) systems. The user's position, which contains significant channel\ncharacteristic information, can greatly enhance the efficiency of channel\nacquisition. In cell-free massive MIMO, while the propagation environments\nbetween different base stations and a specific user vary and their respective\nchannels are uncorrelated, the user's position remains constant and unique\nacross all channels. Building on this, the proposed PCEnet framework leverages\nthe position as a bridge between channels to establish a mapping between the\ncharacteristics of different channels, thereby using one acquired channel to\nassist in the estimation and feedback of others. Specifically, this approach\nfirst utilizes neural networks (NNs) to infer the user's position from the\nobtained channel. {The estimated position, shared among BSs through a central\nprocessing unit (CPU)}, is then fed into an NN to design pilot symbols and\nconcatenated with the feedback information to the channel reconstruction NN to\nreconstruct other channels, thereby significantly enhancing channel acquisition\nperformance. Additionally, we propose a simplified strategy where only the\nestimated position is used in the reconstruction process without modifying the\npilot design, thereby reducing latency. Furthermore, we introduce a position\nlabel-free approach that infers the relative user position instead of the\nabsolute position, eliminating the need for ground truth position labels during\nthe localization NN training. Simulation results demonstrate that the proposed\nPCEnet framework reduces pilot and feedback overheads by up to 50%.", "comment": "IEEE TWC. copyright2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses, in any\n  current or future media, including reprinting/republishing this material for\n  advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work in other works", "pdf_url": "http://arxiv.org/pdf/2507.17950v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "深度学习驱动的无蜂窝大规模MIMO位置域信道外推", "tldr": "提出PCEnet框架，利用深度学习和用户位置信息，在无蜂窝大规模MIMO系统中大幅减少信道获取开销。", "motivation": "为了减少信道获取开销，尤其是在无蜂窝大规模MIMO系统中，传统的空间、时间、频率域信道外推技术未能充分利用用户位置信息。", "method": "本文提出了一个名为PCEnet的深度学习驱动的位置域信道外推框架。该框架利用神经网络从已获取的信道中推断用户位置。估计的位置信息通过中央处理单元（CPU）在基站间共享，然后被用于设计导频符号，并与反馈信息一起输入到信道重建神经网络中以重建其他信道。此外，论文还提出了两种优化策略：一是简化策略，仅在重建过程中使用估计位置而不修改导频设计，以降低延迟；二是无位置标签方法，推断相对用户位置而非绝对位置，从而消除了定位神经网络训练中对真实位置标签的需求。", "result": "仿真结果表明，所提出的PCEnet框架将导频和反馈开销降低了高达50%。", "conclusion": "所提出的PCEnet框架通过利用深度学习和用户位置信息，能够显著提高无蜂窝大规模MIMO系统中的信道获取性能，并有效降低系统开销。", "translation": "为了减少信道获取开销，空间、时间和频域信道外推技术已被广泛研究。在本文中，我们提出了一种新颖的基于深度学习的位置域信道外推框架（命名为PCEnet），用于无蜂窝大规模多输入多输出（MIMO）系统。用户的精确位置包含重要的信道特性信息，可以极大地提高信道获取的效率。在无蜂窝大规模MIMO中，尽管不同基站与特定用户之间的传播环境各异，且它们各自的信道是不相关的，但用户的位置在所有信道中保持不变和唯一。基于此，所提出的PCEnet框架利用位置作为信道之间的桥梁，建立不同信道特性之间的映射，从而利用一个已获取的信道协助其他信道的估计和反馈。具体来说，该方法首先利用神经网络（NN）从获得的信道中推断用户位置。然后，通过中央处理单元（CPU）在基站间共享的估计位置被输入到一个NN中以设计导频符号，并与反馈信息连接到信道重建NN中以重建其他信道，从而显著提高信道获取性能。此外，我们提出了一种简化策略，其中仅在重建过程中使用估计位置而不修改导频设计，从而降低了延迟。此外，我们引入了一种无位置标签的方法，该方法推断相对用户位置而不是绝对位置，从而消除了定位NN训练中对真实位置标签的需求。仿真结果表明，所提出的PCEnet框架将导频和反馈开销降低了高达50%。", "summary": "本文提出了PCEnet，一个基于深度学习的位置域信道外推框架，专为无蜂窝大规模MIMO系统设计。该框架利用用户位置作为不同信道间的桥梁，通过神经网络从一个已获取信道推断用户位置，并利用该位置信息辅助其他信道的估计和重建，从而显著降低信道获取的导频和反馈开销。论文还提出了简化策略和无位置标签的训练方法，以提高效率和实用性。", "keywords": "深度学习, 信道外推, 无蜂窝大规模MIMO, 位置域, 信道获取", "comments": "这篇论文的创新点在于将用户位置信息引入信道外推，并结合深度学习，为无蜂窝大规模MIMO系统中的信道获取问题提供了一种新颖且高效的解决方案。利用位置作为不同信道间的“桥梁”是其核心思想，有效解决了无蜂窝系统中信道不相关性的挑战。降低高达50%的开销显示了其重要性。同时，提出的无位置标签方法也增加了其在实际应用中的可行性。"}}
{"id": "2405.12312", "title": "A Principled Approach for Data Bias Mitigation", "authors": ["Bruno Scarone", "Alfredo Viola", "Renée J. Miller", "Ricardo Baeza-Yates"], "categories": ["cs.LG", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to AIES 2025", "url": "http://arxiv.org/abs/2405.12312v4", "summary": "The widespread use of machine learning and data-driven algorithms for\ndecision making has been steadily increasing over many years. \\emph{Bias} in\nthe data can adversely affect this decision-making. We present a new mitigation\nstrategy to address data bias. Our methods are explainable and come with\nmathematical guarantees of correctness. They can take advantage of new work on\ntable discovery to find new tuples that can be added to a dataset to create\nreal datasets that are unbiased or less biased. Our framework covers data with\nnon-binary labels and with multiple sensitive attributes. Hence, we are able to\nmeasure and mitigate bias that does not appear over a single attribute (or\nfeature), but only intersectionally, when considering a combination of\nattributes. We evaluate our techniques on publicly available datasets and\nprovide a theoretical analysis of our results, highlighting novel insights into\ndata bias.", "comment": "Accepted to AIES 2025", "pdf_url": "http://arxiv.org/pdf/2405.12312v4", "cate": "cs.LG", "date": "2024-05-20", "updated": "2025-07-24", "AI": {"title_translation": "数据偏差缓解的原则性方法", "tldr": "本文提出了一种新的、可解释且具有数学保证的数据偏差缓解策略，能够处理非二元标签和多敏感属性的交集偏差，并通过添加新元组来创建去偏数据集。", "motivation": "机器学习和数据驱动算法在决策制定中的广泛应用日益增加，而数据中的偏差会对其决策产生不利影响。", "method": "提出了一种新的可解释且具有数学正确性保证的偏差缓解策略。该方法利用表发现技术寻找可添加到数据集的新元组，以创建无偏或偏差较小的数据集。该框架支持非二元标签和多敏感属性的数据，能够测量和缓解单个属性或特征不明显但在属性组合下出现的交集偏差。", "result": "在公开可用数据集上评估了所提出的技术，并提供了结果的理论分析，揭示了数据偏差的新颖见解。", "conclusion": "本文提出了一种有效且理论上可靠的数据偏差缓解方法，能够处理复杂的数据偏差（包括交集偏差），并通过实验验证了其有效性并提供了新的理论见解。", "translation": "机器学习和数据驱动算法在决策制定中的广泛应用多年来一直在稳步增长。数据中的“偏差”会对此类决策产生不利影响。我们提出了一种新的缓解策略来解决数据偏差。我们的方法是可解释的，并附带数学正确性保证。它们可以利用表发现方面的新工作来找到可以添加到数据集中的新元组，从而创建无偏或偏差较小的真实数据集。我们的框架涵盖了具有非二元标签和多个敏感属性的数据。因此，我们能够测量和缓解并非出现在单个属性（或特征）上，而是在考虑属性组合时才以交集形式出现的偏差。我们在公开可用的数据集上评估了我们的技术，并提供了我们结果的理论分析，突出了对数据偏差的新颖见解。", "summary": "本文提出了一种原则性的数据偏差缓解方法，旨在解决机器学习和数据驱动决策中普遍存在的数据偏差问题。该方法具有可解释性和数学保证，能够通过利用表发现技术向数据集添加新元组，从而生成无偏或偏差较小的数据集。其框架支持非二元标签和多敏感属性，尤其擅长识别和缓解在属性组合中出现的交集偏差。研究人员在公开数据集上验证了该技术，并进行了理论分析，提供了对数据偏差的新颖见解。", "keywords": "数据偏差, 偏差缓解, 机器学习, 交集偏差, 表发现", "comments": "这篇论文的创新点在于提出了一个原则性的、可解释且具有数学保证的数据偏差缓解框架，特别是它能够处理非二元标签和多敏感属性，并能识别和缓解交集偏差，这在实际应用中非常重要。通过利用表发现来生成去偏数据集的方法也很有趣。"}}
{"id": "2507.18484", "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments", "authors": ["Xiao Yang", "Lingxuan Wu", "Lizhong Wang", "Chengyang Ying", "Hang Su", "Jun Zhu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2404.00540", "url": "http://arxiv.org/abs/2507.18484v1", "summary": "Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.", "comment": "arXiv admin note: text overlap with arXiv:2404.00540", "pdf_url": "http://arxiv.org/pdf/2507.18484v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "强化具身主动防御：利用自适应交互实现对抗性3D环境中鲁棒的视觉感知", "tldr": "Rein-EAD提出了一种主动防御框架，通过自适应探索和交互来提高3D对抗环境下的视觉感知鲁棒性。", "motivation": "3D环境中的对抗性攻击对视觉感知系统构成严重威胁，现有防御机制多为被动策略，依赖预设假设，在动态3D环境中适应性有限。", "method": "本文提出了强化具身主动防御（Rein-EAD）框架，这是一种主动防御策略，通过自适应探索和环境交互来增强3D对抗环境中的感知鲁棒性。它采用多步目标，平衡即时预测准确性和预测熵最小化，并利用面向不确定性的奖励塑形机制来优化策略更新，减少计算开销，支持实际应用，无需可微分环境。", "result": "实验证明Rein-EAD能显著降低攻击成功率，同时保持在多种任务上的标准准确性。它对未知和自适应攻击表现出强大的泛化能力，适用于3D物体分类、人脸识别和自动驾驶等复杂现实任务。", "conclusion": "Rein-EAD通过利用自适应探索和交互，为3D对抗环境中的视觉感知提供了一种鲁棒且可泛化的主动防御解决方案。", "translation": "3D环境中的对抗性攻击已成为视觉感知系统可靠性的关键威胁，尤其是在身份验证和自动驾驶等安全敏感应用中。这些攻击通过利用复杂场景中的漏洞，采用对抗性补丁和3D物体来操纵深度神经网络（DNN）的预测。现有的防御机制，如对抗训练和净化，主要采用被动策略来增强鲁棒性。然而，这些方法通常依赖于对对抗策略的预定义假设，限制了它们在动态3D环境中的适应性。为了解决这些挑战，我们引入了强化具身主动防御（Rein-EAD），这是一个主动防御框架，它利用自适应探索和与环境的交互来提高3D对抗环境中的感知鲁棒性。通过实施一个平衡即时预测准确性和预测熵最小化的多步目标，Rein-EAD在多步时间范围内优化防御策略。此外，Rein-EAD还涉及一种面向不确定性的奖励塑形机制，有助于高效的策略更新，从而减少计算开销并支持无需可微分环境的实际应用。全面的实验验证了Rein-EAD的有效性，证明在保持各种任务标准准确性的同时，攻击成功率显著降低。值得注意的是，Rein-EAD对未知和自适应攻击表现出强大的泛化能力，使其适用于3D物体分类、人脸识别和自动驾驶等现实复杂任务。", "summary": "本文提出了一种名为强化具身主动防御（Rein-EAD）的新型框架，旨在解决3D对抗环境中视觉感知系统面临的挑战。与依赖预定义假设的被动防御不同，Rein-EAD采用主动策略，通过自适应探索和环境交互来提高鲁棒性。它通过平衡预测准确性和熵最小化的多步目标，并利用面向不确定性的奖励塑形机制来优化防御策略。实验证明，Rein-EAD能有效降低攻击成功率，保持高准确性，并对未知攻击具有强大的泛化能力，使其适用于自动驾驶等实际应用。", "keywords": "对抗性攻击, 3D环境, 视觉感知, 主动防御, 强化学习", "comments": "Rein-EAD的创新之处在于其主动防御策略，通过将强化学习与环境交互相结合，克服了传统被动防御在动态3D环境中适应性差的局限性。其无需可微分环境的特性，大大增强了其实际部署的潜力，尤其是在自动驾驶等关键安全领域。面向不确定性的奖励塑形机制是其高效策略更新的关键，这对于处理复杂、不确定的对抗环境至关重要。"}}
{"id": "2507.18406", "title": "Factual Inconsistencies in Multilingual Wikipedia Tables", "authors": ["Silvia Cappa", "Lingxiao Kong", "Pille-Riin Peet", "Fanfu Wei", "Yuchen Zhou", "Jan-Christoph Kalo"], "categories": ["cs.CL", "cs.DB", "cs.DL", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School 2025", "url": "http://arxiv.org/abs/2507.18406v1", "summary": "Wikipedia serves as a globally accessible knowledge source with content in\nover 300 languages. Despite covering the same topics, the different versions of\nWikipedia are written and updated independently. This leads to factual\ninconsistencies that can impact the neutrality and reliability of the\nencyclopedia and AI systems, which often rely on Wikipedia as a main training\nsource. This study investigates cross-lingual inconsistencies in Wikipedia's\nstructured content, with a focus on tabular data. We developed a methodology to\ncollect, align, and analyze tables from Wikipedia multilingual articles,\ndefining categories of inconsistency. We apply various quantitative and\nqualitative metrics to assess multilingual alignment using a sample dataset.\nThese insights have implications for factual verification, multilingual\nknowledge interaction, and design for reliable AI systems leveraging Wikipedia\ncontent.", "comment": "11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.18406v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多语言维基百科表格中的事实不一致性", "tldr": "本研究发现多语言维基百科表格中存在事实不一致性，这会影响百科全书的可靠性及依赖其的AI系统。", "motivation": "维基百科不同语言版本独立更新导致事实不一致，影响百科全书的中立性、可靠性以及依赖维基百科作为主要训练来源的AI系统。", "method": "开发了一套方法来收集、对齐和分析维基百科多语言文章中的表格数据，并定义了不一致性的类别。使用样本数据集应用了各种定量和定性指标来评估多语言对齐。", "result": "揭示了多语言维基百科表格中的跨语言事实不一致性，并定义了不一致性类别。通过定量和定性指标对多语言对齐进行了评估。", "conclusion": "研究结果对事实核查、多语言知识交互以及设计利用维基百科内容的可靠AI系统具有重要意义。", "translation": "维基百科作为一个全球可访问的知识来源，其内容涵盖300多种语言。尽管涉及相同主题，但不同版本的维基百科是独立编写和更新的。这导致了事实不一致性，可能影响百科全书以及经常依赖维基百科作为主要训练来源的AI系统的中立性和可靠性。本研究调查了维基百科结构化内容中的跨语言不一致性，重点关注表格数据。我们开发了一种方法来收集、对齐和分析维基百科多语言文章中的表格，并定义了不一致性的类别。我们使用样本数据集应用了各种定量和定性指标来评估多语言对齐。这些见解对事实核查、多语言知识交互以及设计利用维基百科内容的可靠AI系统具有重要意义。", "summary": "本研究探讨了多语言维基百科表格中存在的事实不一致性问题。由于不同语言版本独立更新，导致内容差异，这可能影响维基百科的可靠性及其对AI系统的影响。研究开发了一种收集、对齐和分析多语言表格数据的方法，并定义了不一致性类别，通过定量和定性指标进行了评估。研究结果对事实核查、多语言知识交互和构建可靠AI系统具有指导意义。", "keywords": "维基百科, 事实不一致性, 多语言, 表格数据, 知识交互", "comments": "该研究揭示了多语言知识库（如维基百科）在跨语言一致性方面存在的深层问题，这对于依赖这些数据训练的AI模型而言尤为重要。其创新点在于针对结构化表格数据进行不一致性分析，并提出了具体的收集、对齐和分析方法，为未来的事实核查和多语言知识图谱构建提供了宝贵的见解。该工作强调了在利用多语言数据时，需警惕潜在的事实冲突，并为提升AI系统的可靠性指明了方向。"}}
{"id": "2507.12720", "title": "FLEXITOKENS: Flexible Tokenization for Evolving Language Models", "authors": ["Abraham Toluase Owodunni", "Orevaoghene Ahia", "Sachin Kumar"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12720v2", "summary": "Language models (LMs) are challenging to adapt to new data distributions by\nsimple finetuning. This is due to the rigidity of their subword tokenizers,\nwhich typically remain unchanged during adaptation. This inflexibility often\nleads to inefficient tokenization, causing overfragmentation of\nout-of-distribution domains, unseen languages, or scripts. In this work, we\ndevelop byte-level LMs with learnable tokenizers to make tokenization adaptive.\nOur models include a submodule that learns to predict boundaries between the\ninput byte sequence, encoding it into variable-length segments. Existing\ntokenizer-free methods train this boundary predictor using an auxiliary loss\nthat enforces a fixed compression rate across the training corpus, introducing\na new kind of rigidity. We propose FLEXITOKENS, a simplified training objective\nthat enables significantly greater flexibility during adaptation. Evaluating\nacross multiple multilingual benchmarks, morphologically diverse tasks, and\ndomains, we demonstrate that FLEXITOKENS consistently reduces token\nover-fragmentation and achieves up to 10% improvements on downstream task\nperformance compared to subword and other gradient-based tokenizers. Code and\ndata for our experiments will be released at\nhttps://github.com/owos/flexitokens", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12720v2", "cate": "cs.CL", "date": "2025-07-17", "updated": "2025-07-23", "AI": {"title_translation": "FLEXITOKENS：面向演进语言模型的灵活分词", "tldr": "提出FLEXITOKENS，一种基于字节级的可学习分词器，解决了传统分词器在适应新数据分布时的僵化问题，显著减少分词碎片化并提升下游任务性能。", "motivation": "语言模型难以通过简单微调适应新数据分布，原因是其子词分词器在适应过程中保持不变，导致在域外数据、未见语言或脚本上分词效率低下，造成过度碎片化。", "method": "开发了带有可学习分词器的字节级语言模型。模型包含一个子模块，学习预测输入字节序列的边界，并将其编码为可变长度片段。针对现有无分词器方法中辅助损失强制固定压缩率的僵化问题，提出了FLEXITOKENS，一个简化的训练目标，以实现在适应过程中更大的灵活性。", "result": "FLEXITOKENS在多个多语言基准、形态多样性任务和领域中，始终减少了分词过度碎片化，并且与子词分词器和其他基于梯度的分词器相比，在下游任务性能上实现了高达10%的改进。", "conclusion": "FLEXITOKENS通过引入灵活的分词机制，有效解决了传统语言模型在适应新数据分布时遇到的分词僵化和效率低下问题，显著提升了模型性能。", "translation": "语言模型（LMs）通过简单的微调难以适应新的数据分布。这是由于其子词分词器的刚性，它们在适应过程中通常保持不变。这种不灵活性常常导致低效的分词，造成域外领域、未见语言或脚本的过度碎片化。在这项工作中，我们开发了带有可学习分词器的字节级语言模型，以使分词具有适应性。我们的模型包含一个子模块，学习预测输入字节序列之间的边界，并将其编码成可变长度的片段。现有的无分词器方法使用辅助损失来训练这个边界预测器，该损失在整个训练语料库中强制执行固定的压缩率，引入了一种新的刚性。我们提出了FLEXITOKENS，一个简化的训练目标，可以在适应过程中实现显著更大的灵活性。通过在多个多语言基准、形态多样性任务和领域中进行评估，我们证明FLEXITOKENS始终减少了分词过度碎片化，并且与子词分词器和其他基于梯度的分词器相比，在下游任务性能上实现了高达10%的改进。我们实验的代码和数据将发布在https://github.com/owos/flexitokens。", "summary": "本文提出了FLEXITOKENS，一种用于字节级语言模型的可学习分词器，旨在解决传统子词分词器在适应新数据分布时存在的僵化和过度碎片化问题。FLEXITOKENS通过引入简化的训练目标，使得分词在模型适应过程中更具灵活性。实验结果表明，FLEXITOKENS能有效减少分词过度碎片化，并在下游任务中取得显著性能提升，最高达10%。", "keywords": "语言模型,分词,灵活性,字节级,适应性", "comments": "这篇论文的创新点在于提出了FLEXITOKENS，通过可学习的字节级分词器和简化的训练目标，解决了现有语言模型在应对新数据分布时分词僵化的问题。它提高了分词的灵活性和效率，对于提升跨领域和多语言场景下语言模型的泛化能力具有重要意义。"}}
{"id": "2507.18040", "title": "Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer", "authors": ["Harsh Sharma", "Janardhan Rao Doppa", "Umit Y. Ogras", "Partha Pratim Pande"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Paper accepted at ACM Transactions on Embedded Computing Systems. To be presented in Taiwan, Sept. 2025", "url": "http://arxiv.org/abs/2507.18040v1", "summary": "Multi-chiplet architectures enabled by glass interposer offer superior\nelectrical performance, enable higher bus widths due to reduced crosstalk, and\nhave lower capacitance in the redistribution layer than current silicon\ninterposer-based systems. These advantages result in lower energy per bit,\nhigher communication frequencies, and extended interconnect range. However,\ndeformation of the package (warpage) in glass interposer-based systems becomes\na critical challenge as system size increases, leading to severe mechanical\nstress and reliability concerns. Beyond a certain size, conventional packaging\ntechniques fail to manage warpage effectively, necessitating new approaches to\nmitigate warpage induced bending with scalable performance for glass interposer\nbased multi-chiplet systems. To address these inter-twined challenges, we\npropose a thermal-, warpage-, and performance-aware design framework that\nemploys architecture and packaging co-optimization. The proposed framework\ndisintegrates the surface and embedded chiplets to balance conflicting design\nobjectives, ensuring optimal trade-offs between performance, power, and\nstructural reliability. Our experiments demonstrate that optimized\nmulti-chiplet architectures from our design framework achieve up to 64.7%\nperformance improvement and 40% power reduction compared to traditional 2.5D\nsystems to execute deep neural network workloads with lower fabrication costs.", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems. To\n  be presented in Taiwan, Sept. 2025", "pdf_url": "http://arxiv.org/pdf/2507.18040v1", "cate": "cs.AR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "非可弯曲玻璃中介层实现高性能和热可行多芯片架构设计", "tldr": "本文提出一种新的设计框架，通过架构和封装协同优化，有效解决玻璃中介层多芯片系统中的翘曲问题，显著提升性能并降低功耗和成本。", "motivation": "玻璃中介层多芯片架构虽然具有优越的电学性能，但随着系统尺寸增大，封装变形（翘曲）成为关键挑战，导致严重的机械应力和可靠性问题，传统封装技术已无法有效管理。", "method": "提出一个热、翘曲和性能感知的设计框架，采用架构和封装协同优化。该框架通过分离表面和嵌入式小芯片来平衡性能、功耗和结构可靠性等相互冲突的设计目标。", "result": "实验表明，该设计框架优化后的多芯片架构在执行深度神经网络工作负载时，与传统2.5D系统相比，性能提升高达64.7%，功耗降低40%，并降低了制造成本。", "conclusion": "通过架构和封装协同优化，可以有效解决玻璃中介层多芯片系统中的翘曲问题，同时显著提升性能、降低功耗并降低成本，为未来高性能、高可靠性多芯片集成提供了可行方案。", "translation": "由玻璃中介层实现的多芯片架构比当前基于硅中介层的系统具有卓越的电学性能，由于串扰减少而支持更高的总线宽度，并且在再分布层中具有更低的电容。这些优势带来了更低的每比特能量、更高的通信频率和更长的互连范围。然而，随着系统尺寸的增大，基于玻璃中介层的系统中封装的变形（翘曲）成为一个关键挑战，导致严重的机械应力和可靠性问题。超过一定尺寸后，传统封装技术无法有效管理翘曲，因此需要新的方法来减轻翘曲引起的弯曲，同时为基于玻璃中介层的多芯片系统提供可扩展的性能。为了解决这些相互交织的挑战，我们提出了一种热、翘曲和性能感知的设计框架，该框架采用架构和封装协同优化。所提出的框架分解了表面和嵌入式小芯片，以平衡相互冲突的设计目标，确保性能、功耗和结构可靠性之间的最佳权衡。我们的实验表明，通过我们的设计框架优化后的多芯片架构在执行深度神经网络工作负载时，与传统2.5D系统相比，性能提升高达64.7%，功耗降低40%，同时制造成本更低。", "summary": "本文针对玻璃中介层多芯片架构在尺寸增大时面临的严重翘曲挑战，提出了一种热、翘曲和性能感知的协同优化设计框架。该框架通过架构和封装的联合优化，并分离表面与嵌入式小芯片，以平衡性能、功耗和结构可靠性。实验证明，该框架能显著提升深度神经网络工作负载的性能（高达64.7%），降低功耗（40%），同时降低制造成本，为下一代高性能多芯片系统提供了有效解决方案。", "keywords": "玻璃中介层, 多芯片架构, 翘曲, 协同优化, 性能提升", "comments": "这篇论文的创新点在于提出了一个综合性的设计框架，将热管理、翘曲抑制和性能优化相结合，并通过架构与封装的协同优化来解决玻璃中介层多芯片系统中的核心挑战。非可弯曲玻璃中介层的引入是解决翘曲问题的关键。其重要性在于为未来高性能、高可靠性的多芯片集成提供了新的设计思路和解决方案。"}}
{"id": "2501.12040", "title": "Select2Drive: Pragmatic Communications for Real-Time Collaborative Autonomous Driving", "authors": ["Jiahao Huang", "Jianhang Zhu", "Rongpeng Li", "Zhifeng Zhao", "Honggang Zhang"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.12040v2", "summary": "Vehicle-to-everything communications-assisted autonomous driving has\nwitnessed remarkable advancements in recent years, with pragmatic\ncommunications (PragComm) emerging as a promising paradigm for real-time\ncollaboration among vehicles and other agents. Simultaneously, extensive\nresearch has explored the interplay between collaborative perception and\ndecision-making in end-to-end driving frameworks. In this work, we revisit the\ncollaborative driving problem and propose the Select2Drive framework to\noptimize the utilization of limited computational and communication resources.\nParticularly, to mitigate cumulative latency in perception and decision-making,\nSelect2Drive introduces distributed predictive perception by formulating an\nactive prediction paradigm and simplifying high-dimensional semantic feature\nprediction into a computation cost-efficient, motion-aware reconstruction.\nGiven the ``less is more\" principle that an over-broadened perceptual horizon\npossibly confuses the decision module rather than contributing to it,\nSelect2Drive utilizes area-of-importance-based PragComm to prioritize the\ncommunications of critical regions, thus boosting both communication efficiency\nand decision-making efficacy. Empirical evaluations on the V2Xverse and\nreal-world DAIR-V2X demonstrate that Select2Drive achieves a $2.60$\\% and\n$1.99$\\% improvement in offline perception tasks under limited bandwidth\n(resp., pose error conditions). Moreover, it delivers at most $8.35$\\% and\n$2.65$\\% enhancement in closed-loop driving scores and route completion rates,\nparticularly in scenarios characterized by dense traffic and high-speed\ndynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.12040v2", "cate": "cs.CE", "date": "2025-01-21", "updated": "2025-07-24", "AI": {"title_translation": "Select2Drive：面向实时协同自动驾驶的实用通信", "tldr": "Select2Drive是一个优化有限计算和通信资源的协同自动驾驶框架，通过分布式预测感知和基于重要区域的实用通信，提高感知、决策和驾驶性能。", "motivation": "当前协同自动驾驶研究面临有限计算和通信资源利用率不高的问题，以及感知和决策累积延迟的问题。同时，过宽的感知视野可能干扰决策模块。", "method": "本文提出了Select2Drive框架，旨在优化有限的计算和通信资源。它通过以下方式实现：1) 引入分布式预测感知，将高维语义特征预测简化为计算成本高效、运动感知的重建，以减轻感知和决策中的累积延迟。2) 基于“少即是多”的原则，利用基于重要区域的实用通信（PragComm）来优先处理关键区域的通信，从而提高通信效率和决策效率。", "result": "在V2Xverse和DAIR-V2X数据集上的实证评估表明，Select2Drive在有限带宽（或姿态误差条件）下的离线感知任务中，分别实现了2.60%和1.99%的改进。此外，在闭环驾驶分数和路线完成率方面，特别是在交通密集和高速动态场景中，它最多分别提升了8.35%和2.65%。", "conclusion": "Select2Drive框架通过优化资源利用和引入分布式预测感知及基于重要区域的通信，显著提升了协同自动驾驶的感知、决策和整体驾驶性能，尤其是在资源受限和复杂交通条件下。", "translation": "车联网通信辅助的自动驾驶近年来取得了显著进展，其中实用通信（PragComm）作为一种有前景的范式，用于车辆和其他代理之间的实时协作。同时，广泛的研究探索了端到端驾驶框架中协作感知和决策之间的相互作用。在这项工作中，我们重新审视了协同驾驶问题，并提出了Select2Drive框架，以优化有限的计算和通信资源的利用。特别是，为了减轻感知和决策中的累积延迟，Select2Drive通过制定主动预测范式并将高维语义特征预测简化为计算成本高效、运动感知的重建，引入了分布式预测感知。鉴于“少即是多”的原则——即过于宽泛的感知视野可能会混淆决策模块而不是为其做出贡献，Select2Drive利用基于重要区域的实用通信来优先处理关键区域的通信，从而提高通信效率和决策效率。在V2Xverse和真实世界的DAIR-V2X上的实证评估表明，Select2Drive在有限带宽（或姿态误差条件）下的离线感知任务中，分别实现了2.60%和1.99%的改进。此外，它在闭环驾驶分数和路线完成率方面最多分别提升了8.35%和2.65%，特别是在交通密集和高速动态的场景中。", "summary": "本研究提出了Select2Drive框架，旨在解决协同自动驾驶中有限计算和通信资源的优化问题。该框架通过引入分布式预测感知来缓解感知和决策的累积延迟，并将高维语义特征预测简化为高效的运动感知重建。同时，它利用基于重要区域的实用通信，优先处理关键区域数据，以提高通信和决策效率。在V2Xverse和DAIR-V2X上的实验结果表明，Select2Drive在离线感知任务和闭环驾驶性能上均有显著提升，尤其适用于资源受限和复杂交通场景。", "keywords": "协同自动驾驶, 实用通信, 资源优化, 分布式预测感知, V2X", "comments": "Select2Drive的创新之处在于其结合了分布式预测感知和基于重要区域的实用通信，有效地解决了协同自动驾驶中资源受限和延迟累积的问题。其“少即是多”的原则在通信优化中具有实际意义，能够避免无效信息对决策的干扰。该方法在真实世界数据集上的验证，增加了其在实际应用中的潜力。"}}
{"id": "2507.18597", "title": "Linear Memory SE(2) Invariant Attention", "authors": ["Ethan Pronovost", "Neha Boloor", "Peter Schleede", "Noureldin Hendy", "Andres Morales", "Nicholas Roy"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Best paper award, Equivariant Systems Workshop at RSS", "url": "http://arxiv.org/abs/2507.18597v1", "summary": "Processing spatial data is a key component in many learning tasks for\nautonomous driving such as motion forecasting, multi-agent simulation, and\nplanning. Prior works have demonstrated the value in using SE(2) invariant\nnetwork architectures that consider only the relative poses between objects\n(e.g. other agents, scene features such as traffic lanes). However, these\nmethods compute the relative poses for all pairs of objects explicitly,\nrequiring quadratic memory. In this work, we propose a mechanism for SE(2)\ninvariant scaled dot-product attention that requires linear memory relative to\nthe number of objects in the scene. Our SE(2) invariant transformer\narchitecture enjoys the same scaling properties that have benefited large\nlanguage models in recent years. We demonstrate experimentally that our\napproach is practical to implement and improves performance compared to\ncomparable non-invariant architectures.", "comment": "Best paper award, Equivariant Systems Workshop at RSS", "pdf_url": "http://arxiv.org/pdf/2507.18597v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "线性内存SE(2)不变性注意力", "tldr": "本文提出了一种线性内存SE(2)不变性缩放点积注意力机制，解决了现有SE(2)不变性方法中二次内存消耗的问题，并在自动驾驶空间数据处理中表现出更好的性能和实用性。", "motivation": "自动驾驶中的空间数据处理任务（如运动预测、多智能体模拟和规划）依赖SE(2)不变性网络架构，但现有方法在计算对象间相对姿态时需要二次内存，限制了其在大规模场景中的应用。", "method": "提出了一种SE(2)不变性缩放点积注意力机制，该机制相对于场景中的对象数量只需要线性内存，并将其集成到SE(2)不变性Transformer架构中。", "result": "实验证明，所提出的方法易于实现，并且与可比较的非不变性架构相比，性能有所提高。", "conclusion": "通过引入线性内存SE(2)不变性注意力机制，解决了现有SE(2)不变性方法内存效率低的问题，为自动驾驶等任务中的大规模空间数据处理提供了实用且高性能的解决方案。", "translation": "处理空间数据是自动驾驶中许多学习任务的关键组成部分，例如运动预测、多智能体模拟和规划。先前的研究已经证明了使用SE(2)不变性网络架构的价值，这些架构只考虑对象之间的相对姿态（例如其他智能体、交通车道等场景特征）。然而，这些方法显式地计算所有对象对的相对姿态，需要二次内存。在这项工作中，我们提出了一种SE(2)不变性缩放点积注意力机制，该机制相对于场景中的对象数量只需要线性内存。我们的SE(2)不变性Transformer架构享有近年来使大型语言模型受益的相同缩放特性。我们通过实验证明，我们的方法易于实现，并且与可比较的非不变性架构相比，性能有所提高。", "summary": "本文提出了一种新颖的SE(2)不变性缩放点积注意力机制，旨在解决现有SE(2)不变性网络在处理自动驾驶空间数据时二次内存消耗的问题。该方法实现了线性内存复杂度，并继承了Transformer架构的良好扩展性。实验结果表明，该方法易于实现，且在性能上优于非不变性架构，为自动驾驶等领域的大规模空间数据处理提供了高效的解决方案。", "keywords": "SE(2)不变性, 线性内存, 注意力机制, 空间数据, 自动驾驶", "comments": "这项工作的创新点在于将SE(2)不变性与线性内存复杂度相结合，解决了现有方法在处理大规模空间数据时面临的内存瓶颈。借鉴大型语言模型的成功经验，将Transformer的扩展性引入SE(2)不变性网络，具有重要的实际意义，尤其是在自动驾驶等对实时性和资源效率要求高的领域。"}}
{"id": "2507.15551", "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders", "authors": ["Jie Zhu", "Zhifang Fan", "Xiaoxie Zhu", "Yuchen Jiang", "Hangyu Wang", "Xintian Han", "Haoran Ding", "Xinmin Wang", "Wenlin Zhao", "Zhen Gong", "Huizhi Yang", "Zheng Chai", "Zhe Chen", "Yuchao Zheng", "Qiwei Chen", "Feng Zhang", "Xun Zhou", "Peng Xu", "Xiao Yang", "Di Wu", "Zuotao Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15551v2", "summary": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5\\% to 45\\%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross two core application scenarios (Recommendation and Advertisement).\nFinally, we launch 1B Dense-Parameters RankMixer for full traffic serving\nwithout increasing the serving cost, which improves user active days by 0.3\\%\nand total in-app usage duration by 1.08\\%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15551v2", "cate": "cs.IR", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "RankMixer: 工业推荐系统中排序模型的扩展", "tldr": "RankMixer是一种新的硬件感知模型设计，用于扩展工业推荐系统中的排序模型，显著提高了MFU并降低了服务成本。", "motivation": "现有推荐系统面临训练和服务成本高昂、延迟要求严格、QPS需求高以及传统人工设计特征交叉模块无法有效利用现代GPU导致MFU低和可扩展性差的问题。", "method": "本文引入了RankMixer，一种硬件感知的统一可扩展特征交互架构。它保留了Transformer的高并行性，并用多头token混合模块替代二次自注意力以提高效率。RankMixer通过Per-token FFNs保持对不同特征子空间和跨特征空间交互的建模。为提高ROI，它进一步扩展到十亿参数的Sparse-MoE变体，并采用动态路由策略解决专家训练的不足和不平衡问题。", "result": "RankMixer在万亿规模的生产数据集上显示出卓越的扩展能力；将模型MFU从4.5%提升到45%；在保持大致相同推理延迟的情况下，将排序模型参数扩展了100倍；在线A/B测试验证了其在推荐和广告两大核心应用场景的通用性；上线1B密集参数RankMixer在不增加服务成本的情况下，将用户活跃天数提高了0.3%，应用内总使用时长提高了1.08%。", "conclusion": "RankMixer通过硬件感知设计和高效的特征交互机制，成功解决了工业推荐系统中排序模型的扩展性挑战，并在实际部署中取得了显著的业务提升。", "translation": "大型语言模型（LLMs）的最新进展激发了人们对扩展推荐系统的兴趣，但仍存在两个实际障碍。首先，工业推荐器上的训练和服务成本必须遵守严格的延迟限制和高QPS需求。其次，排序模型中大多数人工设计的特征交叉模块继承自CPU时代，未能充分利用现代GPU，导致模型浮点运算利用率（MFU）低和可扩展性差。我们引入了RankMixer，这是一种硬件感知的模型设计，专为统一且可扩展的特征交互架构量身定制。RankMixer保留了Transformer的高并行性，同时用多头token混合模块取代了二次自注意力以提高效率。此外，RankMixer通过Per-token FFNs保持了对不同特征子空间和跨特征空间交互的建模。我们通过稀疏MoE变体将其进一步扩展到十亿参数以获得更高的投资回报率（ROI）。采用动态路由策略来解决专家训练的不足和不平衡问题。实验表明RankMixer在万亿规模的生产数据集上具有卓越的扩展能力。通过用RankMixer取代以前多样化的手工低MFU模块，我们将模型MFU从4.5%提升到45%，并将排序模型参数扩展了100倍，同时保持大致相同的推理延迟。我们通过在两个核心应用场景（推荐和广告）中的在线A/B测试验证了RankMixer的通用性。最后，我们上线了1B密集参数的RankMixer用于全流量服务，而没有增加服务成本，这使得用户活跃天数提高了0.3%，应用内总使用时长提高了1.08%。", "summary": "本文介绍了RankMixer，一个专为工业推荐系统设计的硬件感知排序模型。它通过统一且可扩展的特征交互架构，解决了传统模型在GPU利用率低和扩展性差的问题。RankMixer采用多头token混合模块和Per-token FFNs，并可扩展至十亿参数的Sparse-MoE变体。实验证明，RankMixer显著提高了模型MFU和参数规模，同时保持了推理延迟，并在实际业务中带来了用户活跃度和使用时长的提升。", "keywords": "推荐系统, 排序模型, 硬件感知, 模型扩展, MFU", "comments": "RankMixer的创新在于其硬件感知的设计，特别是用多头token混合模块替代自注意力以提高效率，以及引入Sparse-MoE变体进行大规模参数扩展。它解决了工业推荐系统长期面临的GPU利用率低和扩展性差的痛点，并通过实际生产环境的验证，展示了其在业务价值上的巨大潜力。"}}
{"id": "2505.09619", "title": "Machine Learning Solutions Integrated in an IoT Healthcare Platform for Heart Failure Risk Stratification", "authors": ["Aiman Faiz", "Anna Maria De Roberto", "Claudio Pascarelli", "Gianvito Mitrano", "Gianluca Fimiani", "Marina Garofano", "Genoveffa Tortora", "Mariangela Lazoi", "Claudio Passino", "Alessia Bramanti"], "categories": ["stat.OT", "cs.AI"], "primary_category": "Subjects:       Other Statistics (stat.OT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.09619v4", "summary": "The management of chronic Heart Failure (HF) presents significant challenges\nin modern healthcare, requiring continuous monitoring, early detection of\nexacerbations, and personalized treatment strategies. In this paper, we present\na predictive model founded on Machine Learning (ML) techniques to identify\npatients at HF risk. This model is an ensemble learning approach, a modified\nstacking technique, that uses two specialized models leveraging clinical and\nechocardiographic features and then a meta-model to combine the predictions of\nthese two models. We initially assess the model on a real dataset and the\nobtained results suggest that it performs well in the stratification of\npatients at HR risk. Specifically, we obtained high sensitivity (95\\%),\nensuring that nearly all high-risk patients are identified. As for accuracy, we\nobtained 84\\%, which can be considered moderate in some ML contexts. However,\nit is acceptable given our priority of identifying patients at risk of HF\nbecause they will be asked to participate in the telemonitoring program of the\nPrediHealth research project on which some of the authors of this paper are\nworking. The initial findings also suggest that ML-based risk stratification\nmodels can serve as valuable decision-support tools not only in the PrediHealth\nproject but also for healthcare professionals, aiding in early intervention and\npersonalized patient management. To have a better understanding of the value\nand of potentiality of our predictive model, we also contrasted its results\nwith those obtained by using three baseline models. The preliminary results\nindicate that our predictive model outperforms these baselines that flatly\nconsider features, \\ie not grouping them in clinical and echocardiographic\nfeatures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.09619v4", "cate": "stat.OT", "date": "2025-04-07", "updated": "2025-07-24", "AI": {"title_translation": "集成到物联网医疗平台中的机器学习解决方案用于心力衰竭风险分层", "tldr": "本文提出了一种基于机器学习的集成学习模型，用于心力衰竭风险分层，实现了高灵敏度并优于基线模型，可作为决策支持工具。", "motivation": "慢性心力衰竭（HF）的管理在现代医疗保健中面临重大挑战，需要持续监测、早期发现恶化和个性化治疗策略。", "method": "本文提出了一种基于机器学习的预测模型，采用集成学习方法，具体为一种改进的堆叠技术。该方法使用两个专门模型，分别利用临床和超声心动图特征，然后通过一个元模型结合这两个模型的预测结果。", "result": "模型在真实数据集上表现良好，心力衰竭风险患者分层效果显著。获得了95%的高灵敏度（确保几乎所有高风险患者都被识别）和84%的准确率。初步结果表明，该预测模型优于不区分特征的基线模型。", "conclusion": "基于机器学习的风险分层模型可以作为有价值的决策支持工具，不仅在PrediHealth项目中，而且对于医疗专业人员，有助于早期干预和个性化患者管理。", "translation": "慢性心力衰竭（HF）的管理在现代医疗保健中面临重大挑战，需要持续监测、早期发现恶化和个性化治疗策略。本文提出了一种基于机器学习（ML）技术的预测模型，用于识别心力衰竭风险患者。该模型是一种集成学习方法，采用改进的堆叠技术，利用两个专门模型分别利用临床和超声心动图特征，然后通过一个元模型结合这两个模型的预测结果。我们首先在一个真实数据集上评估了该模型，结果表明它在心力衰竭风险患者的分层方面表现良好。具体而言，我们获得了95%的高灵敏度，确保几乎所有高风险患者都被识别。至于准确率，我们获得了84%，这在某些机器学习上下文中可能被认为是中等水平。然而，考虑到我们识别心力衰竭风险患者的优先事项，这是可以接受的，因为他们将被要求参与PrediHealth研究项目的远程监测计划，该论文的一些作者正在参与该项目。初步研究结果还表明，基于机器学习的风险分层模型不仅可以在PrediHealth项目中，而且可以为医疗专业人员提供有价值的决策支持工具，帮助早期干预和个性化患者管理。为了更好地理解我们预测模型的价值和潜力，我们还将其结果与使用三种基线模型获得的结果进行了对比。初步结果表明，我们的预测模型优于这些简单考虑特征（即不将它们分组为临床和超声心动图特征）的基线模型。", "summary": "本文提出了一种集成到物联网医疗平台中的机器学习模型，用于心力衰竭风险分层。该模型采用改进的堆叠集成学习方法，结合了利用临床和超声心动图特征的两个专门模型的预测，并通过元模型进行整合。在真实数据集上的评估显示，该模型具有95%的高灵敏度和84%的准确率，能够有效识别高风险心力衰竭患者，并优于传统基线模型。研究结果表明，该模型可作为医疗专业人员的决策支持工具，促进早期干预和个性化患者管理。", "keywords": "心力衰竭, 风险分层, 机器学习, 集成学习, 物联网医疗", "comments": "该研究通过将机器学习集成到物联网医疗平台中，为慢性心力衰竭管理提供了创新解决方案。其采用的改进堆叠集成学习方法，结合了多源特征并证明了其有效性，特别是在高风险患者识别方面表现出色。这对于提升医疗决策支持和个性化治疗具有重要意义。"}}
{"id": "2410.12193", "title": "Differentiable Motion Manifold Primitives for Reactive Motion Generation under Kinodynamic Constraints", "authors": ["Yonghyeon Lee"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages and 9 figures", "url": "http://arxiv.org/abs/2410.12193v2", "summary": "Real-time motion generation -- which is essential for achieving reactive and\nadaptive behavior -- under kinodynamic constraints for high-dimensional systems\nis a crucial yet challenging problem. We address this with a two-step approach:\noffline learning of a lower-dimensional trajectory manifold of task-relevant,\nconstraint-satisfying trajectories, followed by rapid online search within this\nmanifold. Extending the discrete-time Motion Manifold Primitives (MMP)\nframework, we propose Differentiable Motion Manifold Primitives (DMMP), a novel\nneural network architecture that encodes and generates continuous-time,\ndifferentiable trajectories, trained using data collected offline through\ntrajectory optimizations, with a strategy that ensures constraint satisfaction\n-- absent in existing methods. Experiments on dynamic throwing with a 7-DoF\nrobot arm demonstrate that DMMP outperforms prior methods in planning speed,\ntask success, and constraint satisfaction.", "comment": "6 pages and 9 figures", "pdf_url": "http://arxiv.org/pdf/2410.12193v2", "cate": "cs.RO", "date": "2024-10-16", "updated": "2025-07-24", "AI": {"title_translation": "用于运动学约束下反应式运动生成的微分运动流形原语", "tldr": "该研究提出了一种名为DMMP的新型神经网络架构，用于在运动学约束下实时生成高维系统的运动，通过离线学习低维轨迹流形并在线快速搜索实现，并在实验中表现出优越的性能。", "motivation": "在高维系统下，在运动学约束下实现实时运动生成（这对于反应性和自适应行为至关重要）是一个关键但具有挑战性的问题。", "method": "本研究采用两步法：首先，离线学习任务相关、满足约束的低维轨迹流形；其次，在此流形内进行快速在线搜索。研究扩展了离散时间运动流形原语（MMP）框架，提出了可微分运动流形原语（DMMP），这是一种新颖的神经网络架构，用于编码和生成连续时间、可微分的轨迹。该网络通过离线收集的轨迹优化数据进行训练，并采用确保约束满足的策略，这是现有方法所缺乏的。", "result": "在7自由度机械臂进行动态投掷的实验中，DMMP在规划速度、任务成功率和约束满足方面均优于现有方法。", "conclusion": "本研究提出的DMMP方法能够有效解决高维系统在运动学约束下的实时运动生成问题，并在实验中展现出优于现有方法的性能，特别是在确保约束满足方面。", "translation": "实时运动生成——这对于实现反应性和自适应行为至关重要——在高维系统下，在运动学约束下是一个关键但具有挑战性的问题。我们通过两步法解决了这个问题：首先，离线学习一个任务相关、满足约束的低维轨迹流形；然后，在此流形内进行快速在线搜索。我们扩展了离散时间运动流形原语（MMP）框架，提出了可微分运动流形原语（DMMP），这是一种新颖的神经网络架构，用于编码和生成连续时间、可微分的轨迹，其训练数据通过轨迹优化离线收集，并采用了一种确保约束满足的策略——这是现有方法所缺乏的。在7自由度机械臂进行动态投掷的实验表明，DMMP在规划速度、任务成功率和约束满足方面均优于现有方法。", "summary": "本论文提出了一种名为可微分运动流形原语（DMMP）的新型神经网络架构，旨在解决高维系统在运动学约束下的实时运动生成难题。该方法通过离线学习满足约束的低维轨迹流形，并进行在线快速搜索。DMMP能够编码和生成连续且可微分的轨迹，其训练策略特别强调了约束满足，弥补了现有方法的不足。实验结果表明，DMMP在规划速度、任务成功度和约束满足方面均超越了现有方法。", "keywords": "运动流形原语, 实时运动生成, 运动学约束, 神经网络, 轨迹优化", "comments": "这项工作在实时运动生成领域具有重要意义，尤其是在高维系统和严格运动学约束下。DMMP的创新之处在于其能够生成连续且可微分的轨迹，并通过训练策略确保约束满足，这是现有方法未能充分解决的问题。其两步法（离线学习与在线搜索）为复杂机器人系统的反应性行为提供了高效的解决方案。该研究通过具体的机器人投掷实验验证了其方法的有效性和优越性，展现了在实际应用中的巨大潜力。"}}
{"id": "2504.13784", "title": "The complexity of reachability problems in strongly connected finite automata", "authors": ["Stefan Kiefer", "Andrew Ryzhikov"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      To appear in MFCS 2025", "url": "http://arxiv.org/abs/2504.13784v2", "summary": "Several reachability problems in finite automata, such as completeness of\nNFAs and synchronisation of total DFAs, correspond to fundamental properties of\nsets of nonnegative matrices. In particular, the two mentioned properties\ncorrespond to matrix mortality and ergodicity, which ask whether there exists a\nproduct of the input matrices that is equal to, respectively, the zero matrix\nand a matrix with a column of strictly positive entries only. The case where\nthe input automaton is strongly connected (that is, the corresponding set of\nnonnegative matrices is irreducible) frequently appears in applications and\noften admits better properties than the general case. In this paper, we address\nthe existence of such properties from the computational complexity point of\nview, and develop a versatile technique to show that several NL-complete\nproblems remain NL-complete in the strongly connected case. In particular, we\nshow that deciding if a binary total DFA is synchronising is NL-complete even\nif it is promised to be strongly connected, and that deciding completeness of a\nbinary unambiguous NFA with very limited nondeterminism is NL-complete under\nthe same promise.", "comment": "To appear in MFCS 2025", "pdf_url": "http://arxiv.org/pdf/2504.13784v2", "cate": "cs.FL", "date": "2025-04-18", "updated": "2025-07-23", "AI": {"title_translation": "强连通有限自动机中可达性问题的复杂性", "tldr": "本文研究了强连通有限自动机中几种可达性问题的计算复杂性，发现它们在强连通情况下仍然是NL完全的。", "motivation": "有限自动机中的可达性问题（如NFA的完备性和DFA的同步性）与非负矩阵集合的基本性质相关，这些性质在强连通（对应于不可约非负矩阵集合）情况下经常出现且通常具有更好的性质。本文从计算复杂性角度探讨了这些性质的存在性。", "method": "论文开发了一种通用技术，用于证明在强连通情况下，几个NL完全问题仍然保持NL完全。", "result": "研究表明，即使是强连通的二元全DFA，判断其是否同步也是NL完全的；在相同条件下，判断非确定性非常有限的二元无歧义NFA的完备性也是NL完全的。", "conclusion": "即使在强连通的约束下，某些与非负矩阵性质相关的有限自动机可达性问题（如同步和完备性）的计算复杂性仍然保持在NL完全级别。", "translation": "有限自动机中的几种可达性问题，例如NFA的完备性和全DFA的同步性，对应于非负矩阵集合的基本性质。特别是，上述两种性质分别对应于矩阵的消亡性（mortality）和遍历性（ergodicity），它们询问是否存在输入矩阵的乘积分别等于零矩阵和仅含一列严格正项的矩阵。当输入自动机是强连通的（即，对应的非负矩阵集合是不可约的）情况，在应用中经常出现，并且通常比一般情况具有更好的性质。在本文中，我们从计算复杂性的角度探讨了这些性质的存在性，并开发了一种通用技术来表明，在强连通情况下，几个NL完全问题仍然是NL完全的。特别是，我们证明了即使承诺是强连通的，判断二元全DFA是否同步也是NL完全的，并且在相同承诺下，判断具有非常有限的非确定性的二元无歧义NFA的完备性也是NL完全的。", "summary": "本文研究了强连通有限自动机中若干可达性问题的计算复杂性，这些问题与非负矩阵的消亡性和遍历性等基本性质相关。研究发现，尽管强连通情况在应用中常出现且性质通常更好，但通过开发一种通用技术，论文证明了在强连通约束下，判断二元全DFA同步性以及具有有限非确定性的二元无歧义NFA完备性等多个NL完全问题依然保持NL完全的计算复杂性。", "keywords": "有限自动机, 可达性问题, 强连通, 计算复杂性, NL完全", "comments": "本文的创新之处在于其开发了一种通用技术，用于分析强连通有限自动机中特定可达性问题的计算复杂性。它揭示了即使在看似更受限且性质更优的强连通条件下，这些问题的固有计算难度（NL完全）并未降低，这对于理解这些问题的理论极限具有重要意义。"}}
{"id": "2507.18371", "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image", "authors": ["Xiaotian Chen", "DongFu Yin", "Fei Richard Yu", "Xuanchen Li", "Xinhao Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18371v1", "summary": "Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18371v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MVG4D: 基于图像矩阵的多视角和运动生成，用于从单张图像创建4D内容", "tldr": "MVG4D通过结合多视角合成和4D高斯泼溅，从单张图像生成高质量、时间一致的动态4D内容。", "motivation": "尽管生成模型在2D、3D内容创建方面取得进展，但生成高保真且时间一致的动态4D内容仍是挑战，尤其存在运动不连续和背景退化问题。", "method": "本文提出MVG4D框架，通过结合多视角合成和4D高斯泼溅（4D GS）从单张图像生成动态4D内容。其核心是图像矩阵模块，用于合成时间连贯和空间多样性的多视角图像，为3D和4D重建提供监督信号。这些多视角图像用于优化3D高斯点云，并通过轻量级变形网络扩展到时间域。", "result": "在Objaverse数据集上，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于最先进的基线。它显著减少了闪烁伪影，并锐化了跨视角和时间的结构细节。", "conclusion": "MVG4D为从最小输入高效、可控地生成4D内容开辟了新方向，并能实现更沉浸式的AR/VR体验。", "translation": "生成建模的进步显著增强了数字内容创作，从2D图像扩展到复杂的3D和4D场景。尽管取得了实质性进展，但生成高保真和时间一致的动态4D内容仍然是一个挑战。在本文中，我们提出了MVG4D，一个新颖的框架，通过结合多视角合成与4D高斯泼溅（4D GS），从单个静态图像生成动态4D内容。MVG4D的核心是采用图像矩阵模块，该模块合成时间连贯和空间多样性的多视角图像，为下游的3D和4D重建提供丰富的监督信号。这些多视角图像用于优化3D高斯点云，该点云通过一个轻量级变形网络进一步扩展到时间域。我们的方法有效增强了时间一致性、几何保真度和视觉真实感，解决了影响先前基于4D GS的方法的运动不连续和背景退化等关键挑战。在Objaverse数据集上进行的广泛实验表明，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于最先进的基线。值得注意的是，它减少了跨视角和时间的闪烁伪影并锐化了结构细节，从而实现了更沉浸式的AR/VR体验。MVG4D为从最小输入高效、可控地生成4D内容开辟了新方向。", "summary": "MVG4D是一个新颖的框架，旨在从单个静态图像生成高保真、时间一致的动态4D内容。它通过一个图像矩阵模块合成多视角图像，并结合4D高斯泼溅技术，优化3D点云并扩展至时间域。该方法有效解决了现有4D生成中的运动不连续和背景退化问题，并在实验中表现出优于现有基线的性能，减少了闪烁并增强了细节，为沉浸式AR/VR体验提供了可能。", "keywords": "4D内容生成, 多视角合成, 4D高斯泼溅, 单图像生成, 图像矩阵", "comments": "本文提出了一种创新的基于图像矩阵和4D高斯泼溅的单图4D内容生成方法MVG4D。其核心创新在于图像矩阵模块能够生成时间连贯和空间多样的多视角图像，有效克服了传统4D GS方法中的运动不连续和背景退化问题。该方法不仅提高了生成内容的质量和时间一致性，还提升了效率，为AR/VR等沉浸式应用提供了新的可能性，具有重要的研究和应用价值。"}}
{"id": "2507.17916", "title": "Recovery Thresholding Hyperinterpolations in Signal Processing", "authors": ["Congpei An", "Jiashu Ran"], "categories": ["math.NA", "cs.NA", "65K10, 65D15, 94A12, 65F10, 33C52"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      16 pages, 3 figures, 5 tables", "url": "http://arxiv.org/abs/2507.17916v1", "summary": "This paper introduces recovery thresholding hyperinterpolations, a novel\nclass of methods for sparse signal reconstruction in the presence of noise. We\ndevelop a framework that integrates thresholding operators--including hard\nthresholding, springback, and Newton thresholding--directly into the\nhyperinterpolation structure to maintain sparsity during signal recovery. Our\napproach leverages Newton's method to minimize one-dimensional nonconvex\nfunctions, which we then extend to solve multivariable nonconvex regularization\nproblems. The proposed methods demonstrate robust performance in reconstructing\nsignals corrupted by both Gaussian and impulse noise. Through numerical\nexperiments, we validate the effectiveness of these recovery thresholding\nhyperinterpolations for signal reconstruction and function denoising\napplications, showing their advantages over traditional approaches in\npreserving signal sparsity while achieving accurate recovery.", "comment": "16 pages, 3 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.17916v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "信号处理中的恢复阈值超插值", "tldr": "本文提出了一种新的恢复阈值超插值方法，用于在噪声环境下稀疏信号重建，通过将阈值算子集成到超插值结构中并利用牛顿法，实现了对高斯和脉冲噪声下信号的鲁棒重建，并优于传统方法。", "motivation": "在存在噪声的情况下，需要一种新的方法来有效地进行稀疏信号重建，并保持信号的稀疏性。", "method": "引入恢复阈值超插值，将硬阈值、回弹阈值和牛顿阈值等阈值算子直接集成到超插值结构中，以在信号恢复过程中保持稀疏性。该方法利用牛顿法最小化一维非凸函数，并扩展到解决多变量非凸正则化问题。", "result": "所提出的方法在重建受高斯噪声和脉冲噪声污染的信号时表现出鲁棒性能。数值实验验证了这些恢复阈值超插值在信号重建和函数去噪应用中的有效性，并显示出它们在保持信号稀疏性同时实现准确恢复方面优于传统方法的优势。", "conclusion": "恢复阈值超插值是一种有效且鲁棒的稀疏信号重建方法，在噪声环境下能更好地保持信号稀疏性并实现准确恢复，优于传统方法。", "translation": "本文介绍了一种新型的稀疏信号重建方法——恢复阈值超插值，用于在存在噪声的情况下进行稀疏信号重建。我们开发了一个框架，将阈值算子（包括硬阈值、回弹阈值和牛顿阈值）直接整合到超插值结构中，以在信号恢复过程中保持稀疏性。我们的方法利用牛顿法最小化一维非凸函数，并将其扩展到解决多变量非凸正则化问题。所提出的方法在重建受高斯噪声和脉冲噪声污染的信号时表现出鲁棒性能。通过数值实验，我们验证了这些恢复阈值超插值在信号重建和函数去噪应用中的有效性，展示了它们在保持信号稀疏性同时实现准确恢复方面优于传统方法的优势。", "summary": "本文提出了一种名为“恢复阈值超插值”的新型方法，用于在噪声环境下进行稀疏信号重建。该方法将多种阈值算子整合到超插值结构中，以确保恢复过程中的信号稀疏性。通过利用牛顿法解决非凸优化问题，该方法能够有效地重建受高斯和脉冲噪声影响的信号。实验结果表明，与传统方法相比，该新方法在保持信号稀疏性和实现准确恢复方面表现出显著优势。", "keywords": "恢复阈值超插值, 稀疏信号重建, 阈值算子, 超插值, 牛顿法", "comments": "这篇论文的创新点在于将多种阈值算子（硬阈值、回弹阈值、牛顿阈值）与超插值结构相结合，并利用牛顿法处理非凸优化问题，从而提供了一种在噪声环境下高效且鲁棒的稀疏信号重建方案。其优势在于能够更好地保持信号稀疏性，这对于许多信号处理应用至关重要。"}}
{"id": "2506.03590", "title": "VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration", "authors": ["Minh Luu", "Surya Jasper", "Khoi Le", "Evan Pan", "Michael Quinn", "Aakash Tyagi", "Jiang Hu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03590v3", "summary": "Failure triage in design functional verification is critical but\ntime-intensive, relying on manual specification reviews, log inspections, and\nwaveform analyses. While machine learning (ML) has improved areas like stimulus\ngeneration and coverage closure, its application to RTL-level simulation\nfailure triage, particularly for large designs, remains limited. VCDiag offers\nan efficient, adaptable approach using VCD data to classify failing waveforms\nand pinpoint likely failure locations. In the largest experiment, VCDiag\nachieves over 94% accuracy in identifying the top three most likely modules.\nThe framework introduces a novel signal selection and statistical compression\napproach, achieving over 120x reduction in raw data size while preserving\nfeatures essential for classification. It can also be integrated into diverse\nVerilog/SystemVerilog designs and testbenches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03590v3", "cate": "cs.LG", "date": "2025-06-04", "updated": "2025-07-24", "AI": {"title_translation": "VCDiag：对错误波形进行分类以加速故障分类", "tldr": "VCDiag是一种机器学习方法，利用VCD数据对功能验证中的错误波形进行分类，以加速故障分类，并在大型设计中实现了高精度和数据压缩。", "motivation": "设计功能验证中的故障分类至关重要但耗时，目前依赖手动审查。尽管机器学习在其他领域有所改进，但在RTL级仿真故障分类（尤其是大型设计）中的应用仍然有限。", "method": "VCDiag使用VCD数据，通过一种新颖的信号选择和统计压缩方法来分类错误的波形，并确定可能的故障位置。该方法实现了超过120倍的原始数据大小缩减，同时保留了分类所需的重要特征。", "result": "在最大的实验中，VCDiag在识别前三个最可能的模块方面达到了超过94%的准确率。该框架实现了超过120倍的原始数据大小缩减。", "conclusion": "VCDiag提供了一种高效、可适应的解决方案，通过对错误波形进行分类来加速RTL级仿真故障分类，显著提高了准确性并实现了数据压缩，且可集成到多种设计中。", "translation": "设计功能验证中的故障分类至关重要但耗时，它依赖于手动规范审查、日志检查和波形分析。尽管机器学习（ML）已经改进了激励生成和覆盖率闭合等领域，但其在RTL级仿真故障分类（特别是大型设计）中的应用仍然有限。VCDiag提供了一种高效、适应性强的方法，利用VCD数据对失效波形进行分类并精确定位可能的故障位置。在最大的实验中，VCDiag在识别前三个最可能的模块方面实现了超过94%的准确率。该框架引入了一种新颖的信号选择和统计压缩方法，在保留分类所需关键特征的同时，将原始数据大小减少了120倍以上。它还可以集成到各种Verilog/SystemVerilog设计和测试平台中。", "summary": "VCDiag是一种针对设计功能验证中耗时故障分类问题提出的机器学习框架。它利用VCD数据，通过创新的信号选择和统计压缩技术，高效地对错误波形进行分类，并精确定位故障模块。该方法在大型实验中实现了超过94%的故障模块识别准确率，同时将原始数据量减少了120倍以上，展现了其在加速RTL级仿真故障分类方面的有效性和实用性。", "keywords": "故障分类, 波形分析, 机器学习, VCD数据, RTL仿真", "comments": "VCDiag的创新之处在于其将机器学习应用于RTL级仿真故障分类这一具有挑战性的领域，特别是通过引入新颖的信号选择和统计压缩方法，极大地解决了VCD数据量庞大的问题，实现了高效且高精度的分类。这对于加速设计验证流程、降低手动分析成本具有重要意义。"}}
{"id": "2507.17760", "title": "How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning", "authors": ["Fatma Betül Güreş", "Tanya Nazaretsky", "Bahar Radmehr", "Martina Rau", "Tanja Käser"], "categories": ["cs.CY", "cs.AI", "cs.HC", "K.3.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Submitted to AIED 2025 main track", "url": "http://arxiv.org/abs/2507.17760v1", "summary": "Supporting students in developing effective diagnostic reasoning is a key\nchallenge in various educational domains. Novices often struggle with cognitive\nbiases such as premature closure and over-reliance on heuristics.\nScenario-based learning (SBL) can address these challenges by offering\nrealistic case experiences and iterative practice, but the optimal sequencing\nof instruction and problem-solving activities remains unclear. This study\nexamines how personalized support can be incorporated into different\ninstructional sequences and whether providing explicit diagnostic strategy\ninstruction before (I-PS) or after problem-solving (PS-I) improves learning and\nits transfer. We employ a between-groups design in an online SBL environment\ncalled PharmaSim, which simulates real-world client interactions for pharmacy\ntechnician apprentices. Results indicate that while both instruction types are\nbeneficial, PS-I leads to significantly higher performance in transfer tasks.", "comment": "Submitted to AIED 2025 main track", "pdf_url": "http://arxiv.org/pdf/2507.17760v1", "cate": "cs.CY", "date": "2025-05-08", "updated": "2025-05-08", "AI": {"title_translation": "教学顺序和个性化支持如何影响诊断策略学习", "tldr": "研究表明，在基于情境的学习中，先进行问题解决再提供诊断策略指导（PS-I）比先指导再解决问题（I-PS）更能有效提高学习迁移。", "motivation": "培养学生有效的诊断推理能力是教育领域的关键挑战，新手常受认知偏差影响。情境式学习（SBL）虽有帮助，但其最佳教学和问题解决活动顺序仍不明确。", "method": "本研究采用组间设计，在一个名为PharmaSim的在线情境式学习环境中进行，该环境模拟药剂师学徒的真实客户互动。研究比较了在问题解决前（I-PS）或问题解决后（PS-I）提供明确诊断策略指导对学习和迁移的影响。", "result": "结果显示，I-PS和PS-I两种教学类型均有益，但PS-I（先解决问题后指导）在迁移任务中导致了显著更高的表现。", "conclusion": "在情境式学习中，先让学生尝试解决问题，再提供明确的诊断策略指导，能更有效地促进学习的迁移。", "translation": "支持学生发展有效的诊断推理能力是各个教育领域的关键挑战。新手常在认知偏差（如过早下结论和过度依赖启发式方法）中挣扎。情境式学习（SBL）可以通过提供真实的案例经验和迭代练习来解决这些挑战，但教学和问题解决活动的最佳顺序仍不明确。本研究旨在探讨如何将个性化支持融入不同的教学顺序中，以及在问题解决前（I-PS）或后（PS-I）提供明确的诊断策略指导是否能改善学习及其迁移。我们采用组间设计，在一个名为PharmaSim的在线SBL环境中进行，该环境模拟药剂师学徒的真实客户互动。结果表明，虽然两种教学类型都有益，但PS-I导致在迁移任务中表现出显著更高的性能。", "summary": "本研究探讨了在情境式学习环境中，个性化支持与教学顺序对诊断策略学习及迁移的影响。通过比较在问题解决前（I-PS）或后（PS-I）提供明确诊断策略指导的效果，发现在线模拟药剂师学徒的实验中，虽然两种方式均有益，但PS-I模式能显著提高学习者在迁移任务中的表现。", "keywords": "诊断策略学习, 情境式学习, 教学顺序, 个性化支持, 学习迁移", "comments": "这项研究对于优化情境式学习的设计具有重要意义，特别是其发现“先解决问题后指导”的教学顺序在促进知识迁移方面的优越性。这挑战了传统的“先教后练”模式，为教育实践提供了新的启示。其创新点在于对教学顺序的实证比较，并揭示了知识迁移的关键因素。"}}
{"id": "2507.18044", "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at Interspeech 2025", "url": "http://arxiv.org/abs/2507.18044v1", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.", "comment": "Accepted at Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.18044v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于大型语言模型的短语停顿预测合成数据生成", "tldr": "本文探索了利用大型语言模型（LLM）生成短语停顿预测的合成数据，以减少对大量人工标注的依赖，并证明了其在解决语音领域数据挑战方面的有效性。", "motivation": "当前的短语停顿预测方法严重依赖大量人工标注，这导致了高昂的人力成本和费用。此外，语音领域固有的变异性使得获取一致、高质量的数据更加复杂。鉴于大型语言模型在通过生成定制合成数据来解决自然语言处理中数据挑战方面的成功，本文旨在利用LLM解决短语停顿预测中的数据标注挑战。", "method": "研究人员探索了利用大型语言模型生成合成短语停顿标注，通过与传统标注进行比较，并在多种语言中评估其有效性，以应对人工标注和语音相关任务的挑战。", "result": "研究结果表明，基于LLM的合成数据生成有效缓解了短语停顿预测中的数据挑战。", "conclusion": "LLM在语音领域具有作为可行解决方案的潜力。", "translation": "当前短语停顿预测方法解决了文本到语音系统中的关键韵律问题，但严重依赖于来自音频或文本的大量人工标注，这导致了显著的人力投入和成本。语音领域中由语音因素驱动的固有变异性进一步使获取一致、高质量的数据复杂化。最近，大型语言模型（LLMs）在通过生成定制合成数据同时减少人工标注需求方面，成功解决了自然语言处理中的数据挑战。受此启发，我们探索利用LLM生成合成短语停顿标注，通过与传统标注进行比较并在多种语言中评估有效性，从而解决人工标注和语音相关任务的挑战。我们的研究结果表明，基于LLM的合成数据生成有效缓解了短语停顿预测中的数据挑战，并突出了LLMs作为语音领域可行解决方案的潜力。", "summary": "本文提出使用大型语言模型（LLM）生成短语停顿预测的合成数据，以克服传统方法对大量人工标注的依赖及其高成本和语音数据获取的复杂性。研究通过与传统标注的比较，并在多语言环境中评估，证明了LLM生成的合成数据在缓解数据挑战方面的有效性，突显了LLM在语音领域应用中的巨大潜力。", "keywords": "合成数据生成, 短语停顿预测, 大型语言模型, 文本到语音, 语音领域", "comments": "该研究的创新之处在于将大型语言模型应用于语音领域的合成数据生成，特别是在短语停顿预测任务中。这对于减少人工标注成本和提高数据获取效率具有重要意义，为解决语音领域的数据稀缺问题提供了一种有前景的新途径。"}}
{"id": "2507.18627", "title": "Gait Recognition Based on Tiny ML and IMU Sensors", "authors": ["Jiahang Zhang", "Mingtong Chen", "Zhengbao Yang"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18627v1", "summary": "This project presents the development of a gait recognition system using Tiny\nMachine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The\nsystem leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU\nsensor to capture motion data, including acceleration and angular velocity,\nfrom four distinct activities: walking, stationary, going upstairs, and going\ndownstairs. The data collected is processed through Edge Impulse, an edge AI\nplatform, which enables the training of machine learning models that can be\ndeployed directly onto the microcontroller for real-time activity\nclassification.The data preprocessing step involves extracting relevant\nfeatures from the raw sensor data using techniques such as sliding windows and\ndata normalization, followed by training a Deep Neural Network (DNN) classifier\nfor activity recognition. The model achieves over 80% accuracy on a test\ndataset, demonstrating its ability to classify the four activities effectively.\nAdditionally, the platform enables anomaly detection, further enhancing the\nrobustness of the system. The integration of Tiny ML ensures low-power\noperation, making it suitable for battery-powered or energy-harvesting devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18627v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于Tiny ML和IMU传感器的步态识别", "tldr": "该项目开发了一个基于Tiny ML和IMU传感器的步态识别系统，能够实时分类四种活动，并在微控制器上实现了高效、低功耗的部署。", "motivation": "开发一个使用Tiny ML和IMU传感器进行步态识别的系统，以实现实时活动分类，并适用于低功耗设备。", "method": "系统利用XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器捕获加速度和角速度数据。通过Edge Impulse平台进行数据预处理（滑动窗口、数据归一化）和深度神经网络（DNN）分类器训练。训练后的模型直接部署到微控制器上进行实时活动分类和异常检测。", "result": "模型在测试数据集上实现了超过80%的准确率，有效分类了四种活动。平台还支持异常检测功能。", "conclusion": "该系统成功地将Tiny ML与IMU传感器结合，实现了高效、低功耗的步态识别和活动分类，证明了其在电池供电或能量收集设备中的适用性。", "translation": "该项目展示了使用微型机器学习（Tiny ML）和惯性测量单元（IMU）传感器开发步态识别系统。该系统利用XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器捕获四种不同活动（步行、静止、上楼、下楼）的运动数据，包括加速度和角速度。收集到的数据通过Edge Impulse（一个边缘AI平台）进行处理，该平台能够训练机器学习模型，并直接部署到微控制器上进行实时活动分类。数据预处理步骤涉及使用滑动窗口和数据归一化等技术从原始传感器数据中提取相关特征，然后训练深度神经网络（DNN）分类器进行活动识别。该模型在测试数据集上实现了超过80%的准确率，展示了其有效分类四种活动的能力。此外，该平台还支持异常检测，进一步增强了系统的鲁棒性。Tiny ML的集成确保了低功耗运行，使其适用于电池供电或能量收集设备。", "summary": "本文介绍了一个基于Tiny ML和IMU传感器的步态识别系统。该系统利用特定微控制器和IMU传感器采集步行、静止、上楼和下楼四种活动的运动数据。数据通过Edge Impulse平台进行预处理和深度神经网络模型训练，实现了超过80%的活动分类准确率，并支持异常检测。该集成方案确保了低功耗运行，适用于边缘设备。", "keywords": "步态识别, Tiny ML, IMU传感器, 深度神经网络, 边缘计算", "comments": "该论文的创新点在于将Tiny ML技术与IMU传感器结合，实现了在微控制器上进行实时、低功耗的步态识别和活动分类。其优势在于提高了系统在资源受限设备上的部署可行性，对于可穿戴设备和物联网应用具有重要意义。"}}
{"id": "2503.08537", "title": "Chemical reasoning in LLMs unlocks strategy-aware synthesis planning and reaction mechanism elucidation", "authors": ["Andres M Bran", "Theo A Neukomm", "Daniel P Armstrong", "Zlatko Jončev", "Philippe Schwaller"], "categories": ["cs.AI", "cond-mat.mtrl-sci"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.08537v2", "summary": "While automated chemical tools excel at specific tasks, they have struggled\nto capture the strategic thinking that characterizes expert chemical reasoning.\nHere we demonstrate that large language models (LLMs) can serve as powerful\ntools enabling chemical analysis. When integrated with traditional search\nalgorithms, they enable a new approach to computer-aided synthesis that mirrors\nhuman expert thinking. Rather than using LLMs to directly manipulate chemical\nstructures, we leverage their ability to evaluate chemical strategies and guide\nsearch algorithms toward chemically meaningful solutions. We demonstrate this\nparadigm through two fundamental challenges: strategy-aware retrosynthetic\nplanning and mechanism elucidation. In retrosynthetic planning, our system\nallows chemists to specify desired synthetic strategies in natural language --\nfrom protecting group strategies to global feasibility assessment -- and uses\ntraditional or LLM-guided Monte Carlo Tree Search to find routes that satisfy\nthese constraints. In mechanism elucidation, LLMs guide the search for\nplausible reaction mechanisms by combining chemical principles with systematic\nexploration. This approach shows strong performance across diverse chemical\ntasks, with newer and larger models demonstrating increasingly sophisticated\nchemical reasoning. Our approach establishes a new paradigm for computer-aided\nchemistry that combines the strategic understanding of LLMs with the precision\nof traditional chemical tools, opening possibilities for more intuitive and\npowerful chemical automation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.08537v2", "cate": "cs.AI", "date": "2025-03-11", "updated": "2025-07-23", "AI": {"title_translation": "LLMs中的化学推理解锁了策略感知的合成规划和反应机理阐明", "tldr": "大型语言模型（LLMs）通过评估化学策略并指导传统搜索算法，实现了策略感知的计算机辅助合成规划和反应机理阐明，弥补了现有自动化工具在战略思维方面的不足。", "motivation": "现有的自动化化学工具难以捕捉专家化学推理中的战略思维，而这对于化学分析至关重要。", "method": "该研究将大型语言模型（LLMs）与传统搜索算法结合。LLMs不直接操作化学结构，而是评估化学策略，并指导搜索算法寻找化学上有意义的解决方案。该方法应用于两个挑战：策略感知的逆合成规划（LLMs指导蒙特卡洛树搜索以满足特定策略约束）和反应机理阐明（LLMs结合化学原理系统探索并指导机理搜索）。", "result": "该方法在多样化的化学任务中表现出强大的性能，并且更新、更大的模型展现出日益复杂的化学推理能力。", "conclusion": "该研究建立了一种新的计算机辅助化学范式，将LLMs的战略理解与传统化学工具的精度相结合，为更直观、强大的化学自动化系统开辟了可能性。", "translation": "尽管自动化化学工具在特定任务上表现出色，但它们难以捕捉专家化学推理所特有的战略思维。在此，我们证明大型语言模型（LLMs）可以作为强大的工具，实现化学分析。当与传统搜索算法相结合时，它们为计算机辅助合成提供了一种模仿人类专家思维的新方法。我们并非使用LLMs直接操作化学结构，而是利用它们评估化学策略并指导搜索算法找到具有化学意义的解决方案的能力。我们通过两个基本挑战来展示这一范式：策略感知的逆合成规划和机理阐明。在逆合成规划中，我们的系统允许化学家以自然语言指定所需的合成策略——从保护基团策略到全局可行性评估——并使用传统或LLM引导的蒙特卡洛树搜索来找到满足这些约束的路线。在机理阐明中，LLMs通过结合化学原理和系统探索来指导对合理反应机理的搜索。这种方法在各种化学任务中表现出强大的性能，更新、更大的模型展现出日益复杂的化学推理能力。我们的方法为计算机辅助化学建立了一个新范式，它将LLMs的战略理解与传统化学工具的精度相结合，为更直观、强大的化学自动化系统开辟了可能性。", "summary": "该论文提出了一种结合大型语言模型（LLMs）与传统搜索算法的新型计算机辅助化学范式，旨在弥补现有自动化工具在化学战略思维上的不足。通过利用LLMs评估化学策略和指导搜索的能力，而非直接操作结构，该方法成功应用于策略感知的逆合成规划和反应机理阐明。实验证明，该方法在多种化学任务中表现出色，尤其随着模型规模的增大，其化学推理能力显著增强，为未来更智能的化学自动化系统奠定了基础。", "keywords": "大型语言模型, 化学推理, 合成规划, 反应机理, 计算机辅助化学", "comments": "这项研究的创新之处在于，它没有尝试让LLMs直接处理复杂的化学结构操作，而是巧妙地利用了LLMs在理解和评估“战略”层面的能力，将其与传统化学工具的精确性相结合。这种“LLM作为战略指导者”的范式，解决了传统自动化工具缺乏人类专家战略思维的痛点，为计算机辅助化学领域带来了突破。其重要性在于，它为开发更智能、更贴近人类思维的化学自动化系统提供了新的途径。"}}
{"id": "2503.08510", "title": "External Knowledge Injection for CLIP-Based Class-Incremental Learning", "authors": ["Da-Wei Zhou", "Kai-Wen Li", "Jingyi Ning", "Han-Jia Ye", "Lijun Zhang", "De-Chuan Zhan"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Code is available at: this https URL", "url": "http://arxiv.org/abs/2503.08510v2", "summary": "Class-Incremental Learning (CIL) enables learning systems to continuously\nadapt to evolving data streams. With the advancement of pre-training,\nleveraging pre-trained vision-language models (e.g., CLIP) offers a promising\nstarting point for CIL. However, CLIP makes decisions by matching visual\nembeddings to class names, overlooking the rich contextual information conveyed\nthrough language. For instance, the concept of ``cat'' can be decomposed into\nfeatures like tail, fur, and face for recognition. Besides, since the model is\ncontinually updated, these detailed features are overwritten in CIL, requiring\nexternal knowledge for compensation. In this paper, we introduce ExterNal\nknowledGe INjEction (ENGINE) for CLIP-based CIL. To enhance knowledge transfer\nfrom outside the dataset, we propose a dual-branch injection tuning framework\nthat encodes informative knowledge from both visual and textual modalities. The\nvisual branch is enhanced with data augmentation to enrich the visual features,\nwhile the textual branch leverages GPT-4 to rewrite discriminative descriptors.\nIn addition to this on-the-fly knowledge injection, we also implement\npost-tuning knowledge by re-ranking the prediction results during inference.\nWith the injected knowledge, the model can better capture informative features\nfor downstream tasks as data evolves. Extensive experiments demonstrate the\nstate-of-the-art performance of ENGINE. Code is available at:\nhttps://github.com/LAMDA-CL/ICCV25-ENGINE", "comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV25-ENGINE", "pdf_url": "http://arxiv.org/pdf/2503.08510v2", "cate": "cs.CV", "date": "2025-03-11", "updated": "2025-07-24", "AI": {"title_translation": "基于CLIP的类增量学习的外部知识注入", "tldr": "本文提出ENGINE框架，通过视觉和文本双分支注入外部知识，提升CLIP在类增量学习中的性能，并实现最先进的结果。", "motivation": "CLIP在类增量学习中通过匹配视觉嵌入和类名进行决策，但忽略了语言传达的丰富上下文信息，且在模型持续更新时，详细特征会被覆盖，需要外部知识补偿。", "method": "本文引入了ExterNal knowledGe INjEction (ENGINE) 框架，用于基于CLIP的类增量学习。提出一个双分支注入调优框架，编码来自视觉和文本模态的信息知识。视觉分支通过数据增强增强视觉特征，文本分支利用GPT-4重写判别性描述符。此外，还在推理阶段通过重新排序预测结果来实现后调优知识注入。", "result": "通过注入知识，模型能更好地捕获下游任务的信息特征。广泛的实验证明ENGINE实现了最先进的性能。", "conclusion": "ENGINE框架通过外部知识注入，显著提升了CLIP在类增量学习中的表现，使模型能够更好地适应数据演变，捕获信息特征。", "translation": "类增量学习（CIL）使学习系统能够持续适应不断演变的数据流。随着预训练的进步，利用预训练的视觉-语言模型（例如CLIP）为CIL提供了一个有前景的起点。然而，CLIP通过将视觉嵌入与类名匹配来做出决策，忽略了通过语言传达的丰富上下文信息。例如，“猫”的概念可以分解为尾巴、皮毛和面部等特征进行识别。此外，由于模型不断更新，这些详细特征在CIL中会被覆盖，需要外部知识进行补偿。在本文中，我们引入了用于基于CLIP的CIL的外部知识注入（ENGINE）框架。为了增强数据集外部的知识转移，我们提出了一个双分支注入调优框架，该框架编码来自视觉和文本模态的信息知识。视觉分支通过数据增强得到增强，以丰富视觉特征，而文本分支则利用GPT-4重写判别性描述符。除了这种即时知识注入之外，我们还在推理过程中通过重新排序预测结果来实现后调优知识。通过注入的知识，模型可以更好地捕获信息特征以适应数据演变的下游任务。广泛的实验证明了ENGINE的最先进性能。代码可在以下网址获取：https://github.com/LAMDA-CL/ICCV25-ENGINE", "summary": "本文针对CLIP在类增量学习中忽略上下文信息和特征覆盖的问题，提出了ExterNal knowledGe INjEction (ENGINE) 框架。ENGINE采用双分支注入调优，通过数据增强增强视觉特征，并利用GPT-4重写文本描述符以注入外部知识。此外，还通过后调优重新排序预测结果。实验表明，ENGINE有效提升了CLIP在类增量学习中的性能，达到了最先进水平。", "keywords": "类增量学习, CLIP, 外部知识注入, 双分支学习, GPT-4", "comments": "该论文的创新点在于提出了一个名为ENGINE的框架，通过双分支（视觉和文本）外部知识注入来提升CLIP在类增量学习中的性能。特别地，利用GPT-4进行文本描述符的重写以及推理阶段的后调优知识注入，是其独特且重要的方法。这解决了CLIP在CIL中对上下文信息利用不足以及特征遗忘的问题，为持续学习提供了新的思路。"}}
{"id": "2507.17922", "title": "From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models", "authors": ["Jessica Quaye", "Charvi Rastogi", "Alicia Parrish", "Oana Inel", "Minsuk Kahng", "Lora Aroyo", "Vijay Janapa Reddi"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17922v1", "summary": "Text-to-image (T2I) models have become prevalent across numerous\napplications, making their robust evaluation against adversarial attacks a\ncritical priority. Continuous access to new and challenging adversarial prompts\nacross diverse domains is essential for stress-testing these models for\nresilience against novel attacks from multiple vectors. Current techniques for\ngenerating such prompts are either entirely authored by humans or synthetically\ngenerated. On the one hand, datasets of human-crafted adversarial prompts are\noften too small in size and imbalanced in their cultural and contextual\nrepresentation. On the other hand, datasets of synthetically-generated prompts\nachieve scale, but typically lack the realistic nuances and creative\nadversarial strategies found in human-crafted prompts. To combine the strengths\nof both human and machine approaches, we propose Seed2Harvest, a hybrid\nred-teaming method for guided expansion of culturally diverse, human-crafted\nadversarial prompt seeds. The resulting prompts preserve the characteristics\nand attack patterns of human prompts while maintaining comparable average\nattack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded\ndataset achieves substantially higher diversity with 535 unique geographic\nlocations and a Shannon entropy of 7.48, compared to 58 locations and 5.28\nentropy in the original dataset. Our work demonstrates the importance of\nhuman-machine collaboration in leveraging human creativity and machine\ncomputational capacity to achieve comprehensive, scalable red-teaming for\ncontinuous T2I model safety evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17922v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "从种子到收获：利用AI增强人类创造力以对文本到图像模型进行红队测试", "tldr": "Seed2Harvest是一种结合人类和机器优势的混合红队测试方法，用于生成多样化且有效的对抗性提示，以评估文本到图像模型的安全性。", "motivation": "文本到图像（T2I）模型需要强大的对抗性攻击评估，但现有技术（纯人工或纯合成）在规模、多样性、真实性或创造性方面存在局限性。人工生成的数据集太小且代表性不足，而合成生成的数据集缺乏人类提示的细微差别和创造性策略。", "method": "本文提出了Seed2Harvest，一种混合红队测试方法，用于指导性扩展文化多样性的人工制作的对抗性提示种子。该方法结合了人类的创造力和机器的计算能力。", "result": "Seed2Harvest生成的提示保留了人类提示的特征和攻击模式，并保持了可比的平均攻击成功率（NudeNet 0.31，SD NSFW 0.36，Q16 0.12）。扩展后的数据集多样性显著提高，包含535个独特的地理位置和7.48的香农熵，而原始数据集分别为58个位置和5.28的熵。", "conclusion": "该工作证明了人机协作在利用人类创造力和机器计算能力方面的重要性，以实现全面、可扩展的红队测试，从而持续评估T2I模型的安全性。", "translation": "文本到图像（T2I）模型已在众多应用中普及，因此对其进行对抗性攻击的鲁棒性评估成为当务之急。持续获取跨越不同领域的新颖且具有挑战性的对抗性提示，对于压力测试这些模型对来自多个向量的新型攻击的弹性至关重要。目前生成此类提示的技术要么完全由人类编写，要么是合成生成的。一方面，人工制作的对抗性提示数据集通常规模太小，且在文化和语境表示方面不平衡。另一方面，合成生成的提示数据集可以实现规模化，但通常缺乏人工制作提示中发现的真实细微差别和创造性对抗策略。为了结合人类和机器方法的优势，我们提出了Seed2Harvest，一种混合红队测试方法，用于指导性扩展文化多样性的人工制作的对抗性提示种子。由此产生的提示保留了人类提示的特征和攻击模式，同时保持了可比的平均攻击成功率（NudeNet 0.31，SD NSFW 0.36，Q16 0.12）。我们扩展后的数据集实现了显著更高的多样性，包含535个独特的地理位置和7.48的香农熵，而原始数据集分别为58个位置和5.28的熵。我们的工作证明了人机协作在利用人类创造力和机器计算能力方面的重要性，以实现全面、可扩展的红队测试，从而持续评估T2I模型的安全性。", "summary": "本研究提出了一种名为Seed2Harvest的混合红队测试方法，旨在解决现有文本到图像（T2I）模型对抗性提示生成方法的局限性。针对纯人工提示数据集规模小、多样性不足以及纯合成提示缺乏真实性和创造性的问题，Seed2Harvest结合了人类创造力和机器计算能力。该方法通过扩展人工制作的对抗性提示种子，生成了多样性更高、攻击效果良好且保留人类提示特征的新数据集。实验结果表明，该方法显著提高了数据集的地理多样性和熵，并且保持了与原始人类提示相当的攻击成功率。这项工作强调了人机协作在实现T2I模型全面、可扩展安全评估中的重要性。", "keywords": "红队测试, 文本到图像模型, 对抗性提示, 人机协作, Seed2Harvest", "comments": "本文提出了一种创新的人机协作方法，通过结合人类的创造性和机器的扩展能力来解决文本到图像模型红队测试中对抗性提示生成的问题。其核心贡献在于Seed2Harvest框架，它有效地弥补了纯人工和纯合成方法各自的不足。该方法不仅提高了对抗性提示的多样性，同时保持了攻击的有效性，这对于持续评估T2I模型的鲁棒性至关重要。这项工作为未来在AI安全领域中利用混合智能提供了有价值的见解。"}}
{"id": "2507.18153", "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label", "authors": ["Riting Xia", "Rucong Wang", "Yulin Liu", "Anchen Li", "Xueyan Liu", "Yan Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18153v1", "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18153v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "当噪声标签遇到图上的类别不平衡：一种基于大型语言模型和伪标签的图增强方法", "tldr": "本文提出了GraphALP，一个利用大型语言模型（LLM）和伪标签技术来处理图上类别不平衡和噪声标签的图增强框架，并取得了优于现有方法的性能。", "motivation": "现有的图节点分类研究在处理类别不平衡图时，通常假设标签是干净可靠的，但这与真实世界中标签常含噪声的情况不符。因此，本文旨在系统地研究在存在噪声标签的类别不平衡图上进行鲁棒的节点分类。", "method": "本文提出了GraphALP，一个基于大型语言模型（LLM）和伪标签技术的图增强框架。具体来说，它设计了一个基于LLM的过采样方法来生成合成的少数类节点以缓解类别不平衡；然后，开发了一种动态加权伪标签方法以获得高置信度伪标签来降低标签噪声比；最后，实现了一个二次LLM引导的过采样机制来减轻伪标签可能引起的类别分布偏差。", "result": "实验结果表明，GraphALP在具有噪声标签的类别不平衡图上，性能优于现有最先进的方法。", "conclusion": "GraphALP通过结合LLM和伪标签技术，有效地解决了图上类别不平衡和噪声标签并存的鲁棒节点分类问题，并取得了优越的性能。", "translation": "类不平衡图节点分类是一个实用但尚未充分研究的问题。尽管最近的研究试图解决这个问题，但它们在处理类不平衡图时通常假设标签是干净可靠的。这个假设经常违反真实世界图的性质，其中标签频繁包含噪声。鉴于此差距，本文系统地研究了带有噪声标签的类不平衡图的鲁棒节点分类。我们提出了GraphALP，一个基于大型语言模型（LLM）和伪标签技术的新型图增强框架。具体来说，我们设计了一种基于LLM的过采样方法来生成合成的少数类节点，产生标签准确的少数类节点以缓解类不平衡。基于类平衡的图，我们开发了一种动态加权伪标签方法来获得高置信度伪标签以降低标签噪声比。此外，我们实现了一个二次LLM引导的过采样机制，以减轻伪标签可能引起的潜在类分布偏差。实验结果表明，GraphALP在带有噪声标签的类不平衡图上，性能优于现有最先进的方法。", "summary": "本文针对图上类别不平衡且标签含噪声的节点分类问题，提出了一个名为GraphALP的图增强框架。GraphALP结合了大型语言模型（LLM）和伪标签技术，通过LLM过采样生成少数类节点以平衡类别分布，并通过动态加权伪标签减少噪声，同时利用二次LLM过采样校正潜在的分布偏差。实验证明GraphALP在处理此类复杂图数据时表现出优越的性能。", "keywords": "图节点分类, 类别不平衡, 噪声标签, 图增强, 大型语言模型, 伪标签", "comments": "该论文创新性地将大型语言模型（LLM）引入到图数据处理中，结合伪标签技术，有效地解决了图上类别不平衡和噪声标签并存的复杂问题。其分阶段的增强策略，特别是二次LLM引导的过采样机制，展现了对细节问题的考量，提升了方法的鲁棒性和实用性。"}}
{"id": "2507.17623", "title": "SA-WiSense: A Blind-Spot-Free Respiration Sensing Framework for Single-Antenna Wi-Fi Devices", "authors": ["Guangteng Liu", "Xiayue Liu", "Zhixiang Xu", "Yufeng Yuan", "Hui Zhao", "Yuxuan Liu", "Yufei Jiang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12pages, 10figures", "url": "http://arxiv.org/abs/2507.17623v2", "summary": "Wi-Fi sensing offers a promising technique for contactless human respiration\nmonitoring. A key challenge, however, is the blind spot problem caused by\nrandom phase offsets that corrupt the complementarity of respiratory signals.\nTo address the challenge, we propose a single-antenna-Wi-Fi-sensing\n(SA-WiSense) framework to improve accuracy of human respiration monitoring,\nrobust against random phase offsets. The proposed SA-WiSense framework is\ncost-efficient, as only a single antenna is used rather than multiple antennas\nas in the previous works. Therefore, the proposed framework is applicable to\nInternet of Thing (IoT), where most of sensors are equipped with a single\nantenna. On one hand, we propose a cross-subcarrier channel state information\n(CSI) ratio (CSCR) based blind spot mitigation approach for IoT, where the\nratios of two values of CSI between subcarriers are leveraged to mitigate\nrandom phase offsets. We prove that the random phase offsets can be cancelled\nby the proposed CSCR approach, thereby restoring the inherent complementarity\nof signals for blind-spot-free sensing. On the other hand, we propose a genetic\nalgorithm (GA) based subcarrier selection (GASS) approach by formulating an\noptimization problem in terms of the sensing-signal-to-noise ratio (SSNR) of\nCSCR between subcarriers. GA is utilized to solve the formulated optimization\nproblem. We use commodity ESP32 microcontrollers to build an experiment test.\nThe proposed works are validated to achieve an detection rate of 91.2% for\nrespiration monitoring at distances up to 8.0 meters, substantially more\naccurate than the state-of-the-art methods with a single antenna.", "comment": "12pages, 10figures", "pdf_url": "http://arxiv.org/pdf/2507.17623v2", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "SA-WiSense：一种适用于单天线Wi-Fi设备的无盲点呼吸感知框架", "tldr": "SA-WiSense是一个针对单天线Wi-Fi设备的无盲点呼吸感知框架，通过创新的信道状态信息比率和遗传算法解决了随机相位偏移导致的盲点问题，实现了高精度呼吸监测。", "motivation": "现有的Wi-Fi呼吸监测技术存在盲点问题，这是由随机相位偏移引起的，会破坏呼吸信号的互补性。此外，许多现有方法需要多天线，不适用于物联网设备（多为单天线）。", "method": "SA-WiSense框架提出了两种主要方法：1. 基于交叉子载波信道状态信息比率（CSCR）的盲点缓解方法，利用子载波之间两个CSI值的比率来消除随机相位偏移，恢复信号固有的互补性。2. 基于遗传算法（GA）的子载波选择（GASS）方法，通过优化子载波间CSCR的感知信噪比（SSNR）来选择最佳子载波。", "result": "该框架在呼吸监测方面实现了91.2%的检测率，有效距离可达8.0米，并且比现有单天线方法更准确。", "conclusion": "SA-WiSense通过创新的盲点缓解和子载波选择机制，成功解决了单天线Wi-Fi设备在呼吸监测中的盲点问题，并提供了成本效益高、适用于物联网的高精度解决方案。", "translation": "Wi-Fi感知为非接触式人体呼吸监测提供了一种有前景的技术。然而，一个关键挑战是随机相位偏移引起的盲点问题，它会破坏呼吸信号的互补性。为了解决这一挑战，我们提出了一种单天线Wi-Fi感知（SA-WiSense）框架，以提高人体呼吸监测的准确性，并能抵抗随机相位偏移。所提出的SA-WiSense框架具有成本效益，因为它只使用单个天线，而不是像以前工作中那样使用多个天线。因此，所提出的框架适用于物联网（IoT），因为大多数传感器都配备了单天线。一方面，我们提出了一种基于交叉子载波信道状态信息（CSI）比率（CSCR）的物联网盲点缓解方法，其中利用子载波之间两个CSI值的比率来缓解随机相位偏移。我们证明，通过所提出的CSCR方法可以消除随机相位偏移，从而恢复信号固有的互补性，实现无盲点感知。另一方面，我们提出了一种基于遗传算法（GA）的子载波选择（GASS）方法，通过制定子载波之间CSCR的感知信噪比（SSNR）优化问题来选择子载波。利用GA来解决所制定的优化问题。我们使用商用ESP32微控制器构建了一个实验测试。经验证，所提出的工作在最远8.0米的距离上实现了91.2%的呼吸监测检测率，比现有的单天线方法精确得多。", "summary": "该论文提出了SA-WiSense框架，旨在解决单天线Wi-Fi设备在呼吸监测中由随机相位偏移引起的盲点问题。该框架通过引入交叉子载波信道状态信息比率（CSCR）来消除相位偏移，并利用遗传算法（GA）进行子载波选择以优化感知信噪比。SA-WiSense具有成本效益，适用于物联网设备。实验结果表明，该框架在8米距离内实现了91.2%的呼吸监测检测率，显著优于现有单天线方法。", "keywords": "Wi-Fi感知, 呼吸监测, 盲点缓解, 单天线, 信道状态信息", "comments": "该论文的创新点在于提出了两种核心技术：CSCR和GASS。CSCR巧妙地利用子载波间的比率来抵消随机相位偏移，解决了长期存在的盲点问题；而GASS则通过优化算法选择最佳子载波，进一步提升了感知性能。其重要性在于，该框架仅使用单天线，极大地降低了成本和复杂性，使其能够广泛应用于资源受限的物联网设备中，推动了非接触式生命体征监测的普及。论文也通过实验验证了其有效性，但抽象中未提及在更复杂环境或多目标情况下的表现，这可能是未来研究的潜在限制。"}}
{"id": "2507.13238", "title": "Multilingual LLMs Are Not Multilingual Thinkers: Evidence from Hindi Analogy Evaluation", "authors": ["Ashray Gupta", "Rohan Joseph", "Sunny Rai"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13238v2", "summary": "Analogies test a model's ability to infer implicit relationships between\nconcepts, making them a key benchmark for evaluating reasoning capabilities.\nWhile large language models (LLMs) are widely evaluated for reasoning in\nEnglish, their abilities in Indic languages remain understudied, limiting our\nunderstanding of whether these models generalize across languages. To address\nthis gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405\nmultiple-choice questions sourced from Indian government exams. We benchmark\nstate-of-the-art multilingual LLMs using various prompting strategies and\nintroduce a grounded Chain of Thought approach that leverages cognitive\ntheories of analogical reasoning. This approach improves model performance on\nHindi analogy questions. Our experiments show that models perform best with\nEnglish prompts, irrespective of the prompting strategy. Our test set addresses\nthe lack of a critical resource to evaluate LLM reasoning capabilities in\nHindi.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13238v2", "cate": "cs.CL", "date": "2025-07-17", "updated": "2025-07-23", "AI": {"title_translation": "多语言大型语言模型并非多语言思考者：来自印地语类比评估的证据", "tldr": "多语言大型语言模型在印地语类比推理方面表现不佳，即使引入新的印地语类比测试集（HATS）和接地链式思维方法，模型在使用英语提示时表现最佳，揭示了其在非英语语言推理上的局限性。", "motivation": "现有研究主要评估LLM在英语上的推理能力，但在印地语等印度语言上的能力研究不足，这限制了我们对这些模型是否能跨语言泛化的理解。", "method": "引入了一个新的印地语类比测试集（HATS），包含405个源自印度政府考试的多项选择题。使用各种提示策略对最先进的多语言LLM进行基准测试，并引入了一种利用类比推理认知理论的接地链式思维（Chain of Thought）方法。", "result": "接地链式思维方法提高了模型在印地语类比问题上的性能。实验表明，无论采用何种提示策略，模型在使用英语提示时表现最佳。", "conclusion": "该测试集解决了评估LLM在印地语推理能力方面关键资源的缺乏问题，并表明多语言LLM在印地语类比推理方面表现不如英语，并非真正的多语言思考者。", "translation": "类比测试模型推断概念之间隐含关系的能力，使其成为评估推理能力的关键基准。虽然大型语言模型（LLM）在英语推理方面得到了广泛评估，但它们在印度语言方面的能力仍未得到充分研究，这限制了我们对这些模型是否能跨语言泛化的理解。为了弥补这一差距，我们引入了一个新的印地语类比测试集（HATS），包含405个源自印度政府考试的多项选择题。我们使用各种提示策略对最先进的多语言LLM进行基准测试，并引入了一种利用类比推理认知理论的接地链式思维方法。这种方法提高了模型在印地语类比问题上的性能。我们的实验表明，无论采用何种提示策略，模型在使用英语提示时表现最佳。我们的测试集解决了评估LLM在印地语推理能力方面关键资源缺乏的问题。", "summary": "本文旨在弥补多语言大型语言模型（LLM）在印地语等印度语言推理能力评估方面的空白。研究团队引入了一个新的印地语类比测试集（HATS），包含405道多项选择题，并使用多种提示策略对SOTA多语言LLM进行基准测试。研究还提出了一种基于认知理论的接地链式思维方法，该方法能提高模型性能。实验结果显示，即使是多语言LLM，在处理印地语类比问题时，使用英语提示的表现优于其他提示，这表明它们并非真正的多语言思考者。HATS的引入为评估LLM在印地语中的推理能力提供了急需的资源。", "keywords": "多语言LLM, 印地语, 类比推理, HATS, 链式思维", "comments": "这项研究通过构建专门的印地语类比测试集，揭示了当前多语言LLM在非英语语言（特别是印地语）推理能力上的局限性。其创新点在于引入了HATS和接地链式思维方法。研究结果强调了LLM在跨语言泛化方面的挑战，并指出即使是多语言模型，其核心推理能力可能仍偏向于其主要训练语言（如英语）。这对于理解LLM的真实“多语言”能力及其局限性具有重要意义。"}}
{"id": "2507.18520", "title": "Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise", "authors": ["Keyi Li", "Yuval Kluger", "Boris Landa"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62R07, 62G"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18520v1", "summary": "Pairwise Euclidean distance calculation is a fundamental step in many machine\nlearning and data analysis algorithms. In real-world applications, however,\nthese distances are frequently distorted by heteroskedastic\nnoise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption\ncharacterized by variable noise magnitudes across data observations. Such noise\ninflates the computed distances in a nontrivial way, leading to\nmisrepresentations of the underlying data geometry. In this work, we address\nthe tasks of estimating the noise magnitudes per observation and correcting the\npairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,\nwe show that in general high-dimensional settings and without assuming prior\nknowledge on the clean data structure or noise distribution, both tasks can be\nperformed reliably, even when the noise levels vary considerably. Specifically,\nwe develop a principled, hyperparameter-free approach that jointly estimates\nthe noise magnitudes and corrects the distances. We provide theoretical\nguarantees for our approach, establishing probabilistic bounds on the\nestimation errors of both noise magnitudes and distances. These bounds,\nmeasured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates\nas both feature dimension and dataset size increase. Experiments on synthetic\ndatasets demonstrate that our method accurately estimates distances in\nchallenging regimes, significantly improving the robustness of subsequent\ndistance-based computations. Notably, when applied to single-cell RNA\nsequencing data, our method yields noise magnitude estimates consistent with an\nestablished prototypical model, enabling accurate nearest neighbor\nidentification that is fundamental to many downstream analyses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18520v1", "cate": "stat.ML", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "高维异方差噪声下的欧氏距离消除", "tldr": "本文提出了一种在异方差噪声下估计噪声大小并校正高维欧氏距离的方法，该方法无需先验知识，具有理论保证，并在合成数据和单细胞RNA测序数据上表现良好。", "motivation": "在许多机器学习和数据分析算法中，欧氏距离计算是基础步骤，但实际应用中，距离常被异方差噪声扭曲，导致数据几何结构失真。", "method": "开发了一种原则性的、无超参数的方法，可以联合估计每项观测的噪声大小并校正成对欧氏距离。该方法在高维环境下无需关于干净数据结构或噪声分布的先验知识即可可靠执行，并提供了理论保证，建立噪声大小和距离估计误差的概率界限。", "result": "该方法的理论误差界限在特征维度和数据集大小增加时以多项式速率收敛到零。在合成数据集上的实验表明，该方法在挑战性条件下能准确估计距离，显著提高了后续基于距离计算的鲁棒性。应用于单细胞RNA测序数据时，其噪声大小估计与现有模型一致，并能实现准确的最近邻识别。", "conclusion": "在高维异方差噪声下，无需先验知识即可可靠地估计噪声大小并校正欧氏距离，所提出的方法具有理论保证和良好的实际应用效果，显著提升了距离计算的鲁棒性。", "translation": "成对欧氏距离计算是许多机器学习和数据分析算法中的基本步骤。然而，在实际应用中，这些距离经常被异方差噪声扭曲——这是一种普遍存在的非均匀损坏形式，其特点是数据观测的噪声幅度各不相同。这种噪声以一种非平凡的方式膨胀了计算出的距离，导致底层数据几何结构的错误表示。在这项工作中，我们解决了在异方差噪声下估计每个观测的噪声幅度并校正成对欧氏距离的任务。也许令人惊讶的是，我们表明在一般高维设置下，并且在不假设关于干净数据结构或噪声分布的先验知识的情况下，即使噪声水平差异很大，这两个任务也可以可靠地执行。具体来说，我们开发了一种原则性的、无超参数的方法，可以联合估计噪声幅度并校正距离。我们为我们的方法提供了理论保证，建立了噪声幅度和距离估计误差的概率界限。这些以归一化 $\\ell_1$ 范数测量的界限，随着特征维度和数据集大小的增加，以多项式速率收敛到零。在合成数据集上的实验表明，我们的方法在具有挑战性的条件下能够准确估计距离，显著提高了后续基于距离计算的鲁棒性。值得注意的是，当应用于单细胞RNA测序数据时，我们的方法产生的噪声幅度估计与已建立的原型模型一致，从而实现了对许多下游分析至关重要的准确最近邻识别。", "summary": "本文针对高维异方差噪声下欧氏距离被扭曲的问题，提出了一种无超参数的创新方法。该方法能在不依赖先验知识的情况下，联合估计每项观测的噪声大小并校正成对欧氏距离。研究提供了严格的理论保证，证明了其误差界限随维度和数据量增加而收敛。实验结果表明，该方法在合成数据和单细胞RNA测序数据上均表现出色，有效提升了距离计算的准确性和鲁棒性，对后续的距离相关分析具有重要意义。", "keywords": "欧氏距离, 异方差噪声, 高维数据, 噪声估计, 距离校正", "comments": "这项工作在处理高维异方差噪声下的欧氏距离扭曲问题上具有显著的创新性，特别在于其无需先验知识和超参数的特性。理论保证的提供增加了方法的可信度。在实际应用中，如单细胞RNA测序数据上的成功应用，也凸显了其重要性和实用价值，尤其是在数据质量受限的真实世界场景中。"}}
{"id": "2507.18169", "title": "Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners", "authors": ["Lorenzo Porcaro", "Chiara Monaldi"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18169v1", "summary": "Recommender systems shape music listening worldwide due to their widespread\nadoption in online platforms. Growing concerns about representational harms\nthat these systems may cause are nowadays part of the scientific and public\ndebate, wherein music listener perspectives are oftentimes reported and\ndiscussed from a cognitive-behaviorism perspective, but rarely contextualised\nunder a psychosocial and cultural lens. We proceed in this direction, by\ninterviewing a group of Italian music listeners and analysing their narratives\nthrough Emotional Textual Analysis. Thanks to this, we identify shared cultural\nrepertoires that reveal people's complex relationship with listening practices:\neven when familiar with online platforms, listeners may still lack a critical\nunderstanding of recommender systems. Moreover, representational issues,\nparticularly gender disparities, seem not yet fully grasped in the context of\nonline music listening. This study underscores the need for interdisciplinary\nresearch to address representational harms, and the role of algorithmic\nawareness and digital literacy in developing trustworthy recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18169v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "推荐系统、代表性与在线音乐：对意大利听众的社会心理分析", "tldr": "本研究通过对意大利听众的社会心理分析，发现他们在在线音乐推荐系统中对代表性危害（尤其是性别差异）缺乏批判性理解和充分认识，强调了跨学科研究和算法意识的重要性。", "motivation": "现有研究多从认知行为角度讨论推荐系统对音乐听众的影响，但很少从社会心理和文化视角进行语境化分析。鉴于推荐系统可能导致代表性危害，本研究旨在填补这一空白。", "method": "研究通过访谈一组意大利音乐听众，并运用情感文本分析（Emotional Textual Analysis）方法分析他们的叙述。", "result": "研究发现，即使熟悉在线平台，听众仍可能缺乏对推荐系统的批判性理解；此外，在在线音乐收听背景下，代表性问题，特别是性别差异，似乎尚未被充分理解。研究还识别出揭示人们与收听习惯之间复杂关系的共享文化库。", "conclusion": "该研究强调了开展跨学科研究以解决代表性危害的必要性，以及算法意识和数字素养在开发值得信赖的推荐系统中的作用。", "translation": "推荐系统因其在在线平台中的广泛应用而影响着全球的音乐收听。这些系统可能造成的代表性危害日益成为科学和公众辩论的一部分，其中音乐听众的观点往往从认知行为角度被报道和讨论，但很少从社会心理和文化视角进行语境化。我们朝着这个方向前进，通过访谈一群意大利音乐听众，并通过情感文本分析来分析他们的叙述。通过这种方式，我们识别出揭示人们与收听习惯之间复杂关系的共享文化库：即使熟悉在线平台，听众仍可能缺乏对推荐系统的批判性理解。此外，代表性问题，特别是性别差异，似乎在在线音乐收听的背景下尚未被充分理解。这项研究强调了跨学科研究以解决代表性危害的必要性，以及算法意识和数字素养在开发值得信赖的推荐系统中的作用。", "summary": "本研究从社会心理学角度，通过访谈意大利音乐听众并进行情感文本分析，探讨了推荐系统中的代表性问题。研究发现，听众对推荐系统缺乏批判性理解，且对在线音乐收听中存在的性别差异等代表性问题认识不足。论文强调了跨学科研究、算法意识和数字素养对于解决代表性危害和构建可信赖推荐系统的重要性。", "keywords": "推荐系统, 代表性危害, 在线音乐, 社会心理分析, 意大利听众", "comments": "本文的创新之处在于其采用社会心理和文化视角来分析推荐系统对音乐听众的影响，这与以往多关注认知行为的研究形成对比。通过访谈和情感文本分析，论文揭示了听众对推荐系统潜在危害的认知盲区，特别是在代表性问题上。其强调跨学科研究和数字素养的重要性，为未来推荐系统的设计和用户教育提供了宝贵的见解。"}}
{"id": "2504.03300", "title": "On the Complexities of Testing for Compliance with Human Oversight Requirements in AI Regulation", "authors": ["Markus Langer", "Veronika Lazar", "Kevin Baum"], "categories": ["cs.HC", "cs.CY", "K.4.1; K.5.2; J.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.03300v2", "summary": "Human oversight requirements are a core component of the European AI Act and\nin AI governance. In this paper, we highlight key challenges in testing for\ncompliance with these requirements. A central difficulty lies in balancing\nsimple, but potentially ineffective checklist-based approaches with\nresource-intensive and context-sensitive empirical testing of the effectiveness\nof human oversight of AI. Questions regarding when to update compliance\ntesting, the context-dependent nature of human oversight requirements, and\ndifficult-to-operationalize standards further complicate compliance testing. We\nargue that these challenges illustrate broader challenges in the future of\nsociotechnical AI governance, i.e. a future that shifts from ensuring good\ntechnological products to good sociotechnical systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.03300v2", "cate": "cs.HC", "date": "2025-04-04", "updated": "2025-07-24", "AI": {"title_translation": "关于人工智能监管中人类监督要求合规性测试的复杂性", "tldr": "本文探讨了在AI监管中测试人类监督要求合规性的复杂挑战。", "motivation": "本文旨在识别并强调在人工智能监管中测试人类监督要求合规性所面临的关键挑战。", "method": "论文通过分析和讨论的方式，指出了测试人类监督合规性面临的平衡简单清单方法与资源密集型实证测试的困难，以及更新测试时机、情境依赖性和难以操作化标准等问题。", "result": "论文识别出测试人类监督要求合规性的主要挑战，包括平衡简单但可能无效的清单方法与资源密集型且情境敏感的实证测试，以及更新测试时机、情境依赖性及难以操作化标准等问题。这些挑战揭示了未来社会技术AI治理的更广泛问题。", "conclusion": "论文认为，这些挑战体现了未来社会技术AI治理的更广泛挑战，即从确保良好的技术产品转向确保良好的社会技术系统。", "translation": "人类监督要求是欧洲人工智能法案和人工智能治理的核心组成部分。在本文中，我们强调了测试这些要求合规性所面临的关键挑战。一个核心难题在于如何在简单但可能无效的基于清单的方法与资源密集型、情境敏感的对人工智能人类监督有效性的实证测试之间取得平衡。关于何时更新合规性测试、人类监督要求的情境依赖性以及难以操作化的标准等问题，进一步使合规性测试复杂化。我们认为，这些挑战说明了未来社会技术人工智能治理中更广泛的挑战，即从确保良好的技术产品转向确保良好的社会技术系统。", "summary": "本文探讨了在人工智能监管中测试人类监督要求合规性所面临的关键挑战。这些挑战包括在简单的清单式方法与资源密集型实证测试之间取得平衡的困难，以及何时更新合规性测试、人类监督要求的情境依赖性以及难以操作化的标准等问题。作者认为，这些挑战反映了未来社会技术人工智能治理的更广泛困境，即从关注技术产品本身转向关注整个社会技术系统。", "keywords": "人工智能监管, 人类监督, 合规性测试, 社会技术系统, 挑战", "comments": "论文深入探讨了AI监管中人类监督合规性测试的实际操作难题，强调了从纯技术视角转向社会技术系统视角的重要性，这对于未来AI治理的实践和理论发展具有指导意义。其创新之处在于系统性地提出了这些复杂性，并将其置于更宏观的社会技术治理框架下。"}}
{"id": "2410.09706", "title": "ECVC: Exploiting Non-Local Correlations in Multiple Frames for Contextual Video Compression", "authors": ["Wei Jiang", "Junru Li", "Kai Zhang", "Li Zhang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted to CVPR 2025", "url": "http://arxiv.org/abs/2410.09706v4", "summary": "In Learned Video Compression (LVC), improving inter prediction, such as\nenhancing temporal context mining and mitigating accumulated errors, is crucial\nfor boosting rate-distortion performance. Existing LVCs mainly focus on mining\nthe temporal movements while neglecting non-local correlations among frames.\nAdditionally, current contextual video compression models use a single\nreference frame, which is insufficient for handling complex movements. To\naddress these issues, we propose leveraging non-local correlations across\nmultiple frames to enhance temporal priors, significantly boosting\nrate-distortion performance. To mitigate error accumulation, we introduce a\npartial cascaded fine-tuning strategy that supports fine-tuning on full-length\nsequences with constrained computational resources. This method reduces the\ntrain-test mismatch in sequence lengths and significantly decreases accumulated\nerrors. Based on the proposed techniques, we present a video compression scheme\nECVC. Experiments demonstrate that our ECVC achieves state-of-the-art\nperformance, reducing 10.5% and 11.5% more bit-rates than previous SOTA method\nDCVC-FM over VTM-13.2 low delay B (LDB) under the intra period (IP) of 32 and\n-1, respectively.", "comment": "Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2410.09706v4", "cate": "eess.IV", "date": "2024-10-13", "updated": "2025-07-24", "AI": {"title_translation": "ECVC：利用多帧非局部相关性进行上下文视频压缩", "tldr": "提出ECVC，通过利用多帧非局部相关性和部分级联微调策略，显著提升视频压缩性能，达到SOTA水平。", "motivation": "现有学习型视频压缩（LVC）主要关注时间运动挖掘，忽略帧间非局部相关性；当前上下文视频压缩模型仅使用单一参考帧，不足以处理复杂运动。", "method": "1. 利用多帧非局部相关性增强时间先验，提升率失真性能。2. 引入部分级联微调策略，支持在有限计算资源下对全长序列进行微调，减少训练测试不匹配并显著降低累积误差。", "result": "ECVC实现了最先进的性能，在VTM-13.2低延迟B（LDB）下，相比SOTA方法DCVC-FM，在帧内周期（IP）分别为32和-1时，额外降低了10.5%和11.5%的比特率。", "conclusion": "ECVC通过利用多帧非局部相关性并引入部分级联微调策略，显著提升了视频压缩的率失真性能，达到了最先进的水平。", "translation": "在学习型视频压缩（LVC）中，改进帧间预测（例如增强时间上下文挖掘和缓解累积误差）对于提升率失真性能至关重要。现有的LVC主要关注时间运动的挖掘，而忽略了帧间的非局部相关性。此外，当前的上下文视频压缩模型使用单一参考帧，不足以处理复杂的运动。为了解决这些问题，我们提出利用多帧的非局部相关性来增强时间先验，从而显著提升率失真性能。为了缓解误差累积，我们引入了一种部分级联微调策略，该策略支持在有限计算资源下对全长序列进行微调。这种方法减少了序列长度上的训练-测试不匹配，并显著降低了累积误差。基于所提出的技术，我们提出了一种视频压缩方案ECVC。实验表明，我们的ECVC实现了最先进的性能，在VTM-13.2低延迟B（LDB）下，在帧内周期（IP）分别为32和-1时，比之前的SOTA方法DCVC-FM额外降低了10.5%和11.5%的比特率。", "summary": "本文提出了一种名为ECVC的视频压缩方案，旨在解决现有学习型视频压缩模型在处理帧间非局部相关性和复杂运动时的不足。ECVC通过利用多帧非局部相关性来增强时间先验，并引入部分级联微调策略以减少误差累积和训练-测试不匹配。实验结果表明，ECVC在率失真性能方面超越了现有的先进方法，实现了显著的比特率节省。", "keywords": "视频压缩, 非局部相关性, 多帧, 学习型视频压缩, 误差累积", "comments": "本文的创新点在于引入多帧非局部相关性来增强时间先验，以及提出部分级联微调策略以解决误差累积问题，这对于提升学习型视频压缩的性能具有重要意义。该方法有效改善了复杂运动场景下的压缩效率，并为全长序列的训练提供了解决方案，具有较强的实用价值。"}}
{"id": "2507.18534", "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models", "authors": ["Xingyu Qiu", "Mengying Yang", "Xinghua Ma", "Dong Liang", "Yuzhen Li", "Fanding Li", "Gongning Luo", "Wei Wang", "Kuanquan Wang", "Shuo Li"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 4 figures", "url": "http://arxiv.org/abs/2507.18534v1", "summary": "EDM elucidates the unified design space of diffusion models, yet its fixed\nnoise patterns restricted to pure Gaussian noise, limit advancements in image\nrestoration. Our study indicates that forcibly injecting Gaussian noise\ncorrupts the degraded images, overextends the image transformation distance,\nand increases restoration complexity. To address this problem, our proposed EDA\nElucidates the Design space of Arbitrary-noise-based diffusion models.\nTheoretically, EDA expands the freedom of noise pattern while preserving the\noriginal module flexibility of EDM, with rigorous proof that increased noise\ncomplexity incurs no additional computational overhead during restoration. EDA\nis validated on three typical tasks: MRI bias field correction (global smooth\nnoise), CT metal artifact reduction (global sharp noise), and natural image\nshadow removal (local boundary-aware noise). With only 5 sampling steps, EDA\noutperforms most task-specific methods and achieves state-of-the-art\nperformance in bias field correction and shadow removal.", "comment": "21 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18534v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "阐明任意噪声扩散模型的设计空间", "tldr": "EDA扩展了扩散模型以处理任意噪声，并在图像恢复任务中取得了SOTA性能，且计算开销未增加。", "motivation": "现有的EDM扩散模型在图像恢复任务中受限于固定高斯噪声模式，强制注入高斯噪声会破坏降级图像，过度延长图像变换距离，并增加恢复复杂性。", "method": "本文提出了EDA（Elucidates the Design space of Arbitrary-noise-based diffusion models），它扩展了噪声模式的自由度，同时保留了EDM的原始模块灵活性，并严格证明了增加噪声复杂性在恢复过程中不会产生额外的计算开销。", "result": "EDA在MRI偏置场校正（全局平滑噪声）、CT金属伪影去除（全局尖锐噪声）和自然图像阴影去除（局部边界感知噪声）三个典型任务上进行了验证。仅用5个采样步骤，EDA就超越了大多数特定任务方法，并在偏置场校正和阴影去除方面达到了最先进的性能。", "conclusion": "EDA通过引入任意噪声模式，克服了传统扩散模型在图像恢复中的局限性，并在多种任务上展现出卓越的性能和计算效率。", "translation": "EDM阐明了扩散模型的统一设计空间，但其受限于纯高斯噪声的固定噪声模式，限制了图像恢复的进步。我们的研究表明，强制注入高斯噪声会破坏降级图像，过度延长图像变换距离，并增加恢复复杂性。为了解决这个问题，我们提出的EDA阐明了基于任意噪声的扩散模型的设计空间。理论上，EDA扩展了噪声模式的自由度，同时保留了EDM的原始模块灵活性，并经过严格证明，增加噪声复杂性在恢复过程中不会产生额外的计算开销。EDA在三个典型任务上得到了验证：MRI偏置场校正（全局平滑噪声）、CT金属伪影去除（全局尖锐噪声）和自然图像阴影去除（局部边界感知噪声）。仅用5个采样步骤，EDA就超越了大多数特定任务方法，并在偏置场校正和阴影去除方面达到了最先进的性能。", "summary": "本文提出了EDA（Elucidates the Design space of Arbitrary-noise-based diffusion models），旨在解决现有EDM扩散模型在图像恢复中受限于固定高斯噪声的问题。研究指出强制注入高斯噪声会损害图像并增加恢复复杂性。EDA在理论上扩展了噪声模式的自由度，同时保持了EDM的模块灵活性，并证明其不会增加计算开销。EDA在MRI偏置场校正、CT金属伪影去除和自然图像阴影去除等任务上进行了验证，仅需5个采样步骤即可超越多数特定任务方法，并在偏置场校正和阴影去除上达到最先进水平。", "keywords": "扩散模型, 任意噪声, 图像恢复, EDM, EDA", "comments": "EDA的创新之处在于将扩散模型的噪声模式从单一高斯噪声扩展到任意噪声，这极大地增强了模型在各种图像恢复任务中的适用性和灵活性。通过理论证明不增加计算开销，并实际验证其在少量采样步骤下达到SOTA性能，显示了其潜在的广泛应用价值。"}}
{"id": "2506.03170", "title": "PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models", "authors": ["Murthy L", "Subarna Tripathi"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.03170v2", "summary": "The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n100% attribution accuracy. However, any model with less than cent percent\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.03170v2", "cate": "cs.CV", "date": "2025-05-28", "updated": "2025-07-23", "AI": {"title_translation": "PALADIN：文本到图像扩散模型的鲁棒神经指纹识别", "tldr": "本文提出了一种利用循环纠错码为文本到图像扩散模型提供鲁棒神经指纹识别的方法，以解决现有方法无法达到100%归因准确率的问题。", "motivation": "文本到图像生成模型（尤其是开源模型）的滥用风险日益严重。神经指纹识别作为一种风险缓解策略正在兴起，但现有方法都未能达到100%的归因准确率，这使得它们在实践中无法部署。", "method": "作者提出了一种精确的方法，通过利用编码理论中的循环纠错码概念，将神经指纹识别整合到文本到图像扩散模型中。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "文本到图像生成模型被恶意使用的风险，特别是由于此类模型的开源开发，已成为一个严重的问题。作为一种风险缓解策略，通过神经指纹识别来归因生成模型正成为一种流行的技术。最近有大量工作旨在解决神经指纹识别问题。人们已经广泛研究了此类模型的归因准确性与生成质量之间的权衡。然而，现有方法均未达到100%的归因准确性。任何准确率低于百分之百的模型在实践中都无法部署。在这项工作中，我们提出了一种精确的方法，利用编码理论中的循环纠错码概念，将神经指纹识别整合到文本到图像扩散模型中。", "summary": "本文针对文本到图像生成模型滥用风险日益增加的问题，提出了一种鲁棒的神经指纹识别方法。鉴于现有神经指纹识别技术无法达到100%的归因准确率，作者引入了一种基于编码理论中循环纠错码概念的新方法，旨在实现文本到图像扩散模型的高精度指纹识别，以提高模型归因能力并缓解恶意使用风险。", "keywords": "神经指纹识别, 文本到图像扩散模型, 循环纠错码, 模型归因, 生成模型", "comments": "这项工作解决了文本到图像生成模型日益增长的滥用风险，其创新点在于引入了循环纠错码的概念来提高神经指纹识别的准确性，以期达到100%的归因精度，这对于实际部署至关重要。如果能成功实现，将显著增强对生成模型内容的溯源能力。"}}
{"id": "2507.18319", "title": "Gotta catch 'em all! Towards File Localisation from Issues at Large", "authors": ["Jesse Maarleveld", "Jiapan Guo", "Daniel Feitosa"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      12 pages, 6 figures", "url": "http://arxiv.org/abs/2507.18319v1", "summary": "Bug localisation, the study of developing methods to localise the files\nrequiring changes to resolve bugs, has been researched for a long time to\ndevelop methods capable of saving developers' time. Recently, researchers are\nstarting to consider issues outside of bugs. Nevertheless, most existing\nresearch into file localisation from issues focusses on bugs or uses other\nselection methods to ensure only certain types of issues are considered as part\nof the focus of the work. Our goal is to work on all issues at large, without\nany specific selection.\n  In this work, we provide a data pipeline for the creation of issue file\nlocalisation datasets, capable of dealing with arbitrary branching and merging\npractices. We provide a baseline performance evaluation for the file\nlocalisation problem using traditional information retrieval approaches.\nFinally, we use statistical analysis to investigate the influence of biases\nknown in the bug localisation community on our dataset.\n  Our results show that methods designed using bug-specific heuristics perform\npoorly on general issue types, indicating a need for research into general\npurpose models. Furthermore, we find that there are small, but statistically\nsignificant differences in performance between different issue types. Finally,\nwe find that the presence of identifiers have a small effect on performance for\nmost issue types. Many results are project-dependent, encouraging the\ndevelopment of methods which can be tuned to project-specific characteristics.", "comment": "12 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18319v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "全部捕获！面向大规模问题的代码文件定位研究", "tldr": "本研究致力于从所有类型的问题（不仅仅是缺陷）中定位文件，提供了一个数据管道和基线评估，并发现针对缺陷设计的方法在处理通用问题时表现不佳，强调了开发通用模型的重要性。", "motivation": "现有的从问题中定位文件的研究主要集中在缺陷或使用其他选择方法来确保只考虑某些类型的问题。本研究的目标是在没有任何特定选择的情况下，处理所有大规模问题，以节省开发人员的时间。", "method": "本研究提供了一个用于创建问题文件定位数据集的数据管道，该管道能够处理任意分支和合并实践。作者使用传统信息检索方法对文件定位问题进行了基线性能评估。最后，他们使用统计分析来调查缺陷定位社区中已知的偏差对数据集的影响。", "result": "结果表明，使用缺陷特定启发式设计的方法在通用问题类型上表现不佳，这表明需要研究通用模型。此外，研究发现不同问题类型之间的性能存在微小但统计上显著的差异。最后，发现标识符的存在对大多数问题类型的性能影响很小。许多结果是项目依赖的，这鼓励了开发可以根据项目特定特性进行调整的方法。", "conclusion": "针对缺陷设计的定位方法在处理通用问题类型时表现不佳，这凸显了开发能够适应各种问题类型并可根据项目特性进行调整的通用模型的需求。", "translation": "缺陷定位，即开发方法来定位需要修改文件以解决缺陷的研究，长期以来一直致力于开发能够节省开发人员时间的方法。最近，研究人员开始考虑缺陷之外的问题。然而，现有的大多数从问题中定位文件的研究都集中在缺陷上，或者使用其他选择方法来确保只有某些类型的问题被考虑为工作的重点。我们的目标是处理所有大规模问题，不进行任何特定选择。\n在这项工作中，我们提供了一个用于创建问题文件定位数据集的数据管道，能够处理任意分支和合并实践。我们使用传统信息检索方法对文件定位问题进行了基线性能评估。最后，我们使用统计分析来调查缺陷定位社区中已知的偏差对我们数据集的影响。\n我们的结果表明，使用缺陷特定启发式设计的方法在通用问题类型上表现不佳，这表明需要研究通用模型。此外，我们发现不同问题类型之间的性能存在微小但统计上显著的差异。最后，我们发现标识符的存在对大多数问题类型的性能影响很小。许多结果是项目依赖的，这鼓励了开发可以根据项目特定特性进行调整的方法。", "summary": "本研究旨在解决文件定位问题，特别是从所有类型的问题（而不仅仅是缺陷）中定位相关文件。论文提供了一个灵活的数据管道，用于构建问题文件定位数据集，并利用传统信息检索方法进行了基线性能评估。研究发现，专门为缺陷定位设计的方法在处理更广泛的问题类型时效果不佳，这强调了开发通用模型和考虑项目特定调整的必要性。此外，研究还探讨了不同问题类型和标识符对性能的影响。", "keywords": "文件定位, 问题定位, 缺陷定位, 数据管道, 信息检索", "comments": "这项研究的创新之处在于其将文件定位的范围从传统的“缺陷”扩展到“所有类型的问题”，这为软件维护和开发带来了更广泛的视角和潜在的应用价值。通过提供一个通用的数据管道和基线评估，它为未来开发更通用、更鲁棒的文件定位模型奠定了基础。论文明确指出了现有缺陷定位方法的局限性，并强调了开发通用模型和考虑项目特定调整的重要性，这对该领域未来的研究方向具有指导意义。"}}
{"id": "2507.18173", "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection", "authors": ["Haodong Zhu", "Wenhao Dong", "Linlin Yang", "Hong Li", "Yuguang Yang", "Yangyang Ren", "Qingcheng Zhu", "Zichao Feng", "Changbai Li", "Shaohui Lin", "Runqi Wang", "Xiaoyan Luo", "Baochang Zhang"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18173v1", "summary": "Leveraging the complementary characteristics of visible (RGB) and infrared\n(IR) imagery offers significant potential for improving object detection. In\nthis paper, we propose WaveMamba, a cross-modality fusion method that\nefficiently integrates the unique and complementary frequency features of RGB\nand IR decomposed by Discrete Wavelet Transform (DWT). An improved detection\nhead incorporating the Inverse Discrete Wavelet Transform (IDWT) is also\nproposed to reduce information loss and produce the final detection results.\nThe core of our approach is the introduction of WaveMamba Fusion Block (WMFB),\nwhich facilitates comprehensive fusion across low-/high-frequency sub-bands.\nWithin WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba\nframework, first performs initial low-frequency feature fusion with channel\nswapping, followed by deep fusion with an advanced gated attention mechanism\nfor enhanced integration. High-frequency features are enhanced using a strategy\nthat applies an ``absolute maximum\" fusion approach. These advancements lead to\nsignificant performance gains, with our method surpassing state-of-the-art\napproaches and achieving average mAP improvements of 4.5% on four benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18173v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "WaveMamba：小波驱动的Mamba融合用于RGB-红外目标检测", "tldr": "提出WaveMamba，一种结合小波变换和Mamba框架的RGB-红外图像融合方法，显著提升目标检测性能。", "motivation": "利用可见光（RGB）和红外（IR）图像的互补特性在改善目标检测方面具有巨大潜力。", "method": "提出WaveMamba，一种跨模态融合方法，通过离散小波变换（DWT）有效整合RGB和IR分解的独特互补频率特征。引入改进的包含逆离散小波变换（IDWT）的检测头以减少信息损失。核心是WaveMamba融合块（WMFB），促进低/高频子带的全面融合。WMFB内部，低频Mamba融合块（LMFB）基于Mamba框架，首先进行带通道交换的低频特征初始融合，然后通过先进的门控注意力机制进行深度融合。高频特征通过“绝对最大”融合策略增强。", "result": "该方法超越了最先进的方法，并在四个基准上平均mAP提高了4.5%。", "conclusion": "WaveMamba通过有效融合RGB和红外图像的频率特征，显著提升了目标检测的性能，证明了其在跨模态融合中的优越性。", "translation": "利用可见光（RGB）和红外（IR）图像的互补特性在改善目标检测方面具有巨大潜力。本文提出了WaveMamba，一种跨模态融合方法，它有效整合了通过离散小波变换（DWT）分解的RGB和IR图像的独特和互补频率特征。同时提出了一种结合逆离散小波变换（IDWT）的改进检测头，以减少信息损失并生成最终检测结果。我们方法的核心是引入了WaveMamba融合块（WMFB），它促进了低频/高频子带之间的全面融合。在WMFB内部，基于Mamba框架构建的低频Mamba融合块（LMFB）首先进行带通道交换的初始低频特征融合，然后通过先进的门控注意力机制进行深度融合以增强整合。高频特征则采用“绝对最大”融合策略进行增强。这些进步带来了显著的性能提升，我们的方法超越了最先进的方法，并在四个基准上平均mAP提高了4.5%。", "summary": "本文提出WaveMamba，一种新颖的RGB-红外跨模态融合方法，用于目标检测。该方法利用离散小波变换分解图像，并通过WaveMamba融合块（WMFB）对低频（采用基于Mamba的LMFB）和高频特征进行高效融合。改进的检测头结合逆离散小波变换以减少信息损失。实验结果表明，WaveMamba在四个基准上实现了4.5%的mAP提升，超越了现有最先进的方法。", "keywords": "RGB-红外目标检测, 跨模态融合, 小波变换, Mamba, 频率融合", "comments": "该论文通过将小波变换与Mamba架构结合，为RGB-红外图像融合提供了一种创新的解决方案。特别是LMFB中Mamba框架和门控注意力机制的应用，以及高频特征的“绝对最大”融合策略，展现了其在多模态特征融合方面的精巧设计。此方法在提升目标检测性能方面表现出色，具有重要的研究价值和应用潜力。"}}
{"id": "2507.18352", "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "authors": ["Zhen Han", "Mattias Teye", "Derek Yadgaroff", "Judith Bütepage"], "categories": ["cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal track)", "url": "http://arxiv.org/abs/2507.18352v1", "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.", "comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)", "pdf_url": "http://arxiv.org/pdf/2507.18352v1", "cate": "cs.GR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "微小不足以：通过混合知识蒸馏实现高质量、低资源的面部动画模型", "tldr": "该论文提出通过混合知识蒸馏创建极小的、高质量的实时面部动画模型，以克服大型模型和数据稀缺问题，实现设备端推理。", "motivation": "训练高质量的语音驱动3D面部动画模型需要大量数据集，但此类数据集稀缺。现有模型（使用预训练语音编码器）尺寸过大，不适用于设备端实时推理，尤其是在游戏开发等场景中。", "method": "本文采用带有伪标签的混合知识蒸馏方法。使用一个高性能的教师模型，通过大型音频数据集来训练非常小的学生模型。学生模型仅由卷积层和全连接层组成，避免了注意力上下文或循环更新的需求，从而显著减小模型尺寸。", "result": "实验表明，模型内存占用可减少至3.4 MB，所需未来音频上下文减少至81毫秒，同时保持高质量动画。", "conclusion": "该方法为实现逼真、模型驱动的数字角色的设备端推理铺平了道路。", "translation": "高质量、鲁棒的语音驱动3D面部动画机器学习模型训练需要大量多样化的高质量音频-动画对数据集。为了克服这种数据集的缺乏，最近的工作引入了大型预训练语音编码器，这些编码器对输入音频的变化具有鲁棒性，因此使面部动画模型能够跨说话人、音频质量和语言进行泛化。然而，由此产生的面部动画模型过大，只能在专用机器上进行离线推理。在这项工作中，我们探索了游戏开发背景下的设备上实时面部动画模型。我们通过使用带有伪标签的混合知识蒸馏来克服大型数据集的缺乏。给定一个大型音频数据集，我们使用一个高性能的教师模型来训练非常小的学生模型。与预训练语音编码器不同，我们的学生模型只包含卷积层和全连接层，消除了对注意力上下文或循环更新的需求。在我们的实验中，我们证明可以将内存占用减少到3.4 MB，并将所需的未来音频上下文减少到81毫秒，同时保持高质量的动画。这为设备上推理铺平了道路，是迈向逼真、模型驱动数字角色的重要一步。", "summary": "本文解决了在资源受限设备上部署高质量语音驱动3D面部动画模型的挑战。它提出了一种新颖的方法，利用带有伪标签的混合知识蒸馏技术，从一个大型教师模型训练出极小的学生模型。通过将学生模型架构简化为仅包含卷积层和全连接层，它们在保持动画质量的同时，显著减少了内存占用和延迟，使得实时设备端推理在游戏开发等应用中成为可能。", "keywords": "面部动画, 知识蒸馏, 低资源, 设备端推理, 实时", "comments": "该论文为实时面部动画中的一个实际问题提供了创新性解决方案。混合知识蒸馏的应用和学生模型架构的简化是其主要优势，使得模型尺寸和计算需求大幅降低。这项工作对于推动AI模型在边缘计算场景，尤其是交互式应用中的部署具有重要意义。"}}
{"id": "2507.18542", "title": "Effective Multi-Task Learning for Biomedical Named Entity Recognition", "authors": ["João Ruano", "Gonçalo M. Correia", "Leonor Barreiros", "Afonso Mendes"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the 24th BioNLP workshop (ACL2025), 15 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18542v1", "summary": "Biomedical Named Entity Recognition presents significant challenges due to\nthe complexity of biomedical terminology and inconsistencies in annotation\nacross datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),\na novel approach designed to handle nested named entities while integrating\nmultiple datasets through an effective multi-task learning strategy. SRU-NER\nmitigates annotation gaps by dynamically adjusting loss computation to avoid\npenalizing predictions of entity types absent in a given dataset. Through\nextensive experiments, including a cross-corpus evaluation and human assessment\nof the model's predictions, SRU-NER achieves competitive performance in\nbiomedical and general-domain NER tasks, while improving cross-domain\ngeneralization.", "comment": "Accepted at the 24th BioNLP workshop (ACL2025), 15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18542v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "生物医学命名实体识别的有效多任务学习", "tldr": "SRU-NER是一种新颖的方法，通过多任务学习和动态损失调整来有效处理生物医学命名实体识别中的嵌套实体和注释不一致问题，并在生物医学和通用领域NER任务中取得了有竞争力的性能。", "motivation": "生物医学命名实体识别面临生物医学术语复杂性和数据集注释不一致的重大挑战。", "method": "本文提出了SRU-NER（基于槽的循环单元NER），一种处理嵌套命名实体的新方法，通过有效的多任务学习策略整合多个数据集。它通过动态调整损失计算来避免惩罚给定数据集中不存在的实体类型的预测，从而弥补注释空白。", "result": "通过广泛的实验，包括跨语料库评估和模型预测的人工评估，SRU-NER在生物医学和通用领域NER任务中取得了有竞争力的性能，同时提高了跨领域泛化能力。", "conclusion": "SRU-NER是一种有效的方法，可以处理生物医学命名实体识别中的挑战，通过多任务学习和动态损失调整，在多个领域展现出良好的性能和泛化能力。", "translation": "生物医学命名实体识别由于生物医学术语的复杂性和数据集之间注释的不一致性而面临重大挑战。本文介绍了一种名为SRU-NER（基于槽的循环单元NER）的新颖方法，旨在处理嵌套命名实体，同时通过有效的多任务学习策略整合多个数据集。SRU-NER通过动态调整损失计算来避免惩罚给定数据集中不存在的实体类型的预测，从而弥补注释空白。通过广泛的实验，包括跨语料库评估和模型预测的人工评估，SRU-NER在生物医学和通用领域NER任务中取得了有竞争力的性能，同时提高了跨领域泛化能力。", "summary": "本文提出SRU-NER，一种针对生物医学命名实体识别的新方法，通过多任务学习策略整合多数据集并处理嵌套实体。它通过动态损失调整来解决注释不一致问题。实验证明，SRU-NER在生物医学和通用领域NER任务中表现出色，并提升了跨领域泛化能力。", "keywords": "命名实体识别, 多任务学习, 生物医学, 嵌套实体, SRU-NER", "comments": "SRU-NER的创新点在于其处理嵌套实体和通过动态损失调整解决多数据集注释不一致的多任务学习策略，这对于生物医学NER的复杂性至关重要，并显著提升了模型的泛化能力。"}}
{"id": "2506.05249", "title": "On the Convergence of Gradient Descent on Learning Transformers with Residual Connections", "authors": ["Zhen Qin", "Jinxin Zhou", "Zhihui Zhu"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05249v3", "summary": "Transformer models have emerged as fundamental tools across various\nscientific and engineering disciplines, owing to their outstanding performance\nin diverse applications. Despite this empirical success, the theoretical\nfoundations of Transformers remain relatively underdeveloped, particularly in\nunderstanding their training dynamics. Existing research predominantly examines\nisolated components--such as self-attention mechanisms and feedforward\nnetworks--without thoroughly investigating the interdependencies between these\ncomponents, especially when residual connections are present. In this paper, we\naim to bridge this gap by analyzing the convergence behavior of a structurally\ncomplete yet single-layer Transformer, comprising self-attention, a feedforward\nnetwork, and residual connections. We demonstrate that, under appropriate\ninitialization, gradient descent exhibits a linear convergence rate, where the\nconvergence speed is determined by the minimum and maximum singular values of\nthe output matrix from the attention layer. Moreover, our analysis reveals that\nresidual connections serve to ameliorate the ill-conditioning of this output\nmatrix, an issue stemming from the low-rank structure imposed by the softmax\noperation, thereby promoting enhanced optimization stability. We also extend\nour theoretical findings to a multi-layer Transformer architecture, confirming\nthe linear convergence rate of gradient descent under suitable initialization.\nEmpirical results corroborate our theoretical insights, illustrating the\nbeneficial role of residual connections in promoting convergence stability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05249v3", "cate": "cs.LG", "date": "2025-06-05", "updated": "2025-07-24", "AI": {"title_translation": "关于梯度下降在学习带有残差连接的Transformer模型时的收敛性研究", "tldr": "本文理论分析了带有残差连接的单层和多层Transformer模型在梯度下降训练下的收敛性，发现其具有线性收敛率，并证明残差连接能改善输出矩阵的病态性，从而提高优化稳定性。", "motivation": "尽管Transformer模型在实践中取得了巨大成功，但其理论基础，特别是训练动态的理解相对不足。现有研究主要关注孤立组件，缺乏对残差连接存在时组件间相互依赖关系的深入探讨。", "method": "本文通过分析一个结构完整但单层的Transformer模型（包含自注意力、前馈网络和残差连接）的收敛行为来弥补这一空白。进一步将理论发现扩展到多层Transformer架构。通过理论分析和实证结果进行验证。", "result": "在适当初始化下，梯度下降表现出线性收敛率，收敛速度由注意力层输出矩阵的最小和最大奇异值决定。残差连接能改善输出矩阵的病态性（由softmax操作导致的低秩结构引起），从而促进优化稳定性。多层Transformer架构也确认了线性收敛率。", "conclusion": "本文证明了带有残差连接的Transformer模型在梯度下降训练下具有线性收敛率，并强调了残差连接在改善优化稳定性和解决输出矩阵病态性方面的关键作用。", "translation": "Transformer模型因其在各种应用中的出色性能，已成为各种科学和工程领域的基础工具。尽管取得了这些经验上的成功，但Transformer的理论基础仍然相对不完善，特别是在理解其训练动态方面。现有研究主要考察孤立的组件——例如自注意力机制和前馈网络——而没有彻底研究这些组件之间的相互依赖性，尤其是在存在残差连接的情况下。在本文中，我们旨在通过分析一个结构完整但单层的Transformer模型的收敛行为来弥合这一差距，该模型包含自注意力、前馈网络和残差连接。我们证明，在适当初始化下，梯度下降表现出线性收敛率，其中收敛速度由注意力层输出矩阵的最小和最大奇异值决定。此外，我们的分析揭示，残差连接有助于改善此输出矩阵的病态性，这是一个源于softmax操作强加的低秩结构的问题，从而促进增强的优化稳定性。我们还将理论发现扩展到多层Transformer架构，证实了在适当初始化下梯度下降的线性收敛率。实证结果证实了我们的理论见解，说明了残差连接在促进收敛稳定性方面的有益作用。", "summary": "本研究深入探讨了带有残差连接的Transformer模型的理论训练动态，旨在弥补当前理论理解的不足。论文分析了单层和多层Transformer在梯度下降下的收敛行为，证明在适当初始化条件下，梯度下降展现出线性收敛率，其速度取决于注意力层输出矩阵的奇异值。关键发现是残差连接能有效缓解由softmax操作导致的输出矩阵病态性问题，从而显著提升优化稳定性。实证结果也支持了这些理论发现，强调了残差连接在促进模型训练收敛稳定性中的重要作用。", "keywords": "Transformer, 梯度下降, 收敛性, 残差连接, 优化稳定性", "comments": "该论文在Transformer的理论研究方面迈出了重要一步，特别是在理解残差连接的作用上。通过证明线性收敛率和残差连接对病态性的改善作用，为Transformer的训练稳定性提供了重要的理论支撑。其创新点在于将分析扩展到结构完整的Transformer，而非仅关注孤立组件。这对于指导未来更深层、更复杂的Transformer架构设计和优化具有重要意义。"}}
{"id": "2507.17996", "title": "Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification", "authors": ["Emma A. M. Stanley", "Raghav Mehta", "Mélanie Roschewitz", "Nils D. Forkert", "Ben Glocker"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging (FAIMI) 2025", "url": "http://arxiv.org/abs/2507.17996v1", "summary": "Systematic mislabelling affecting specific subgroups (i.e., label bias) in\nmedical imaging datasets represents an understudied issue concerning the\nfairness of medical AI systems. In this work, we investigated how size and\nseparability of subgroups affected by label bias influence the learned features\nand performance of a deep learning model. Therefore, we trained deep learning\nmodels for binary tissue density classification using the EMory BrEast imaging\nDataset (EMBED), where label bias affected separable subgroups (based on\nimaging manufacturer) or non-separable \"pseudo-subgroups\". We found that\nsimulated subgroup label bias led to prominent shifts in the learned feature\nrepresentations of the models. Importantly, these shifts within the feature\nspace were dependent on both the relative size and the separability of the\nsubgroup affected by label bias. We also observed notable differences in\nsubgroup performance depending on whether a validation set with clean labels\nwas used to define the classification threshold for the model. For instance,\nwith label bias affecting the majority separable subgroup, the true positive\nrate for that subgroup fell from 0.898, when the validation set had clean\nlabels, to 0.518, when the validation set had biased labels. Our work\nrepresents a key contribution toward understanding the consequences of label\nbias on subgroup fairness in medical imaging AI.", "comment": "Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI) 2025", "pdf_url": "http://arxiv.org/pdf/2507.17996v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "探究标签偏差与亚组大小和可分离性的相互作用：乳腺密度分类的案例研究", "tldr": "本研究探讨了医学影像数据集中标签偏差（系统性错误标记）如何影响深度学习模型的特征学习和性能，发现偏差对特征表示的影响取决于亚组的大小和可分离性，并对模型性能产生显著影响，尤其是在验证集也存在偏差时。", "motivation": "医学影像数据集中影响特定亚组的系统性错误标记（即标签偏差）是关于医疗AI系统公平性的一个未被充分研究的问题。", "method": "研究人员使用EMory BrEast imaging Dataset (EMBED)训练了用于二元组织密度分类的深度学习模型，其中标签偏差影响了可分离亚组（基于影像制造商）或不可分离的“伪亚组”。", "result": "模拟的亚组标签偏差导致模型学习到的特征表示发生显著偏移，且这些偏移取决于受标签偏差影响的亚组的相对大小和可分离性。此外，亚组性能存在显著差异，取决于是否使用带有干净标签的验证集来定义分类阈值。例如，当标签偏差影响多数可分离亚组时，其真阳性率从验证集有干净标签时的0.898下降到验证集有偏差标签时的0.518。", "conclusion": "本研究是理解标签偏差对医学影像AI中亚组公平性影响的关键贡献。", "translation": "医学影像数据集中影响特定亚组的系统性错误标记（即标签偏差）是关于医疗AI系统公平性的一个未被充分研究的问题。在这项工作中，我们调查了受标签偏差影响的亚组的大小和可分离性如何影响深度学习模型学习到的特征和性能。因此，我们使用EMory BrEast imaging Dataset (EMBED)训练了用于二元组织密度分类的深度学习模型，其中标签偏差影响了可分离亚组（基于影像制造商）或不可分离的“伪亚组”。我们发现，模拟的亚组标签偏差导致模型学习到的特征表示发生显著偏移。重要的是，特征空间中的这些偏移取决于受标签偏差影响的亚组的相对大小和可分离性。我们还观察到，亚组性能存在显著差异，取决于是否使用带有干净标签的验证集来定义模型的分类阈值。例如，当标签偏差影响多数可分离亚组时，其真阳性率从验证集有干净标签时的0.898下降到验证集有偏差标签时的0.518。我们的工作是对理解标签偏差对医学影像AI中亚组公平性影响的关键贡献。", "summary": "本研究探讨了医学影像数据集中标签偏差对深度学习模型特征学习和性能的影响，特别关注亚组大小和可分离性的作用。研究人员使用EMBED数据集训练模型进行乳腺密度分类，模拟了对不同类型亚组的标签偏差。结果显示，标签偏差导致模型特征表示发生显著偏移，且这种偏移与受影响亚组的大小和可分离性有关。此外，当验证集本身存在标签偏差时，模型的亚组性能（如真阳性率）会大幅下降。这项工作为理解标签偏差对医疗AI系统公平性的影响提供了重要见解。", "keywords": "标签偏差, 亚组公平性, 医学影像AI, 深度学习, 乳腺密度分类", "comments": "该论文深入探讨了医学AI领域一个重要的未被充分研究的问题——标签偏差对模型公平性的影响。其创新之处在于系统地研究了亚组大小和可分离性如何调节标签偏差的影响，并强调了验证集标签质量的重要性。这项工作对于开发更公平、更可靠的医疗AI系统具有重要意义，因为它揭示了训练数据中看似微小的偏差如何导致下游任务中显著的性能下降和公平性问题。"}}
{"id": "2505.00862", "title": "Prime and Co-prime Integer Matrices", "authors": ["Xiang-Gen Xia", "Guangpu Guo"], "categories": ["eess.SP", "cs.DM", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00862v4", "summary": "This paper investigates prime and co-prime integer matrices and their\nproperties. It characterizes all pairwise co-prime integer matrices that are\nalso prime integer matrices. This provides a simple way to construct families\nof pairwise co-prime integer matrices, that may have applications in\nmultidimensional co-prime sensing and multidimensional Chinese remainder\ntheorem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00862v4", "cate": "eess.SP", "date": "2025-05-01", "updated": "2025-07-23", "AI": {"title_translation": "素数与互质整数矩阵", "tldr": "本文研究素数和互质整数矩阵的性质，刻画了特定类型的矩阵，并提供了一种简单的构造方法，可能在多维互质传感和多维中国剩余定理中得到应用。", "motivation": "本文旨在深入研究素数和互质整数矩阵及其性质，并为它们在多维互质传感和多维中国剩余定理等领域的潜在应用提供理论基础和构造方法。", "method": "该论文通过刻画所有既是素数整数矩阵又是成对互质整数矩阵的特性来达到研究目的。", "result": "研究结果提供了一种构造成对互质整数矩阵族的简单方法。", "conclusion": "该研究为多维互质传感和多维中国剩余定理等领域提供了潜在的应用基础。", "translation": "本文研究了素数和互质整数矩阵及其性质。它刻画了所有既是素数整数矩阵又是成对互质整数矩阵的特性。这提供了一种构造成对互质整数矩阵族的简单方法，可能在多维互质传感和多维中国剩余定理中具有应用。", "summary": "本文深入探讨了素数和互质整数矩阵的性质，并成功刻画了同时具备素数和成对互质特性的整数矩阵。基于此，论文提出了一种简便的构造方法，能够生成成对互质整数矩阵族，这对于多维互质传感和多维中国剩余定理等领域具有潜在的应用价值。", "keywords": "素数整数矩阵, 互质整数矩阵, 多维互质传感, 中国剩余定理", "comments": "该论文通过对素数和互质整数矩阵的深入刻画，提供了一种新颖且实用的构造方法，这对于扩展多维互质传感和多维中国剩余定理的应用范围具有重要意义。"}}
{"id": "2507.17878", "title": "Strong Sparsification for 1-in-3-SAT via Polynomial Freiman-Ruzsa", "authors": ["Benjamin Bedert", "Tamio-Vesa Nakajima", "Karolina Okrasa", "Stanislav Živný"], "categories": ["cs.DS", "cs.CC", "cs.DM", "math.CO"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Full version of a FOCS'25 paper", "url": "http://arxiv.org/abs/2507.17878v1", "summary": "We introduce a new notion of sparsification, called \\emph{strong\nsparsification}, in which constraints are not removed but variables can be\nmerged. As our main result, we present a strong sparsification algorithm for\n1-in-3-SAT. The correctness of the algorithm relies on establishing a\nsub-quadratic bound on the size of certain sets of vectors in $\\mathbb{F}_2^d$.\nThis result, obtained using the recent \\emph{Polynomial Freiman-Ruzsa Theorem}\n(Gowers, Green, Manners and Tao, Ann. Math. 2025), could be of independent\ninterest. As an application, we improve the state-of-the-art algorithm for\napproximating linearly-ordered colourings of 3-uniform hypergraphs (H{\\aa}stad,\nMartinsson, Nakajima and{\\v{Z}}ivn{\\'{y}}, APPROX 2024).", "comment": "Full version of a FOCS'25 paper", "pdf_url": "http://arxiv.org/pdf/2507.17878v1", "cate": "cs.DS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于多项式Freiman-Ruzsa定理的1-in-3-SAT强稀疏化", "tldr": "本文引入了一种新的稀疏化概念——强稀疏化，并提出了一个针对1-in-3-SAT问题的强稀疏化算法，该算法的正确性依赖于对$\\mathbb{F}_2^d$中某些向量集大小的次二次界限，该结果利用了多项式Freiman-Ruzsa定理，并可独立应用于改进3-一致超图线性有序着色的近似算法。", "motivation": "本文引入了一种名为“强稀疏化”的新稀疏化概念，其中不移除约束，但可以合并变量。研究的动机是为1-in-3-SAT问题提供一个强稀疏化算法。", "method": "本文引入了“强稀疏化”的新概念，并提出了一个针对1-in-3-SAT问题的强稀疏化算法。该算法的正确性建立在确定$\\mathbb{F}_2^d$中某些向量集大小的次二次界限之上。该结果是利用了最近的“多项式Freiman-Ruzsa定理”获得的。", "result": "本文提出了一个针对1-in-3-SAT问题的强稀疏化算法。该算法的正确性依赖于建立了$\\mathbb{F}_2^d$中某些向量集大小的次二次界限。作为一个应用，该研究改进了3-一致超图线性有序着色的最新近似算法。", "conclusion": "本文成功提出了1-in-3-SAT的强稀疏化算法，并通过多项式Freiman-Ruzsa定理获得了关于向量集大小的次二次界限，该结果本身具有独立的理论意义，并能有效改进现有算法。", "translation": "我们引入了一种新的稀疏化概念，称为“强稀疏化”，其中约束不被移除，但变量可以合并。作为我们的主要结果，我们提出了一个针对1-in-3-SAT的强稀疏化算法。该算法的正确性依赖于建立$\\mathbb{F}_2^d$中某些向量集大小的次二次界限。这一结果是利用最近的“多项式Freiman-Ruzsa定理”（Gowers, Green, Manners 和 Tao, Ann. Math. 2025）获得的，可能具有独立的兴趣。作为一个应用，我们改进了3-一致超图线性有序着色的最新算法（H{\\aa}stad, Martinsson, Nakajima 和{\\v{Z}}ivn{\\'{y}}, APPROX 2024）。", "summary": "本文提出了一种名为“强稀疏化”的新概念，其特点是合并变量而非移除约束。研究的核心成果是针对1-in-3-SAT问题的一个强稀疏化算法，其正确性基于利用多项式Freiman-Ruzsa定理获得的$\\mathbb{F}_2^d$中向量集大小的次二次界限。该理论结果具有独立价值，并被应用于改进了3-一致超图线性有序着色的现有近似算法。", "keywords": "强稀疏化, 1-in-3-SAT, 多项式Freiman-Ruzsa定理, 次二次界限, 超图着色", "comments": "本文的创新点在于提出了“强稀疏化”这一新颖的稀疏化概念，它通过合并变量而非移除约束来减少复杂性。将最新的多项式Freiman-Ruzsa定理应用于建立次二次界限，不仅为算法提供了理论基础，也展现了该定理在组合学问题中的强大应用潜力。此外，其在改进超图着色算法方面的实际应用，进一步凸显了这项工作的价值和重要性。"}}
{"id": "2502.03699", "title": "LLM Alignment as Retriever Optimization: An Information Retrieval Perspective", "authors": ["Bowen Jin", "Jinsung Yoon", "Zhen Qin", "Ziqi Wang", "Wei Xiong", "Yu Meng", "Jiawei Han", "Sercan O. Arik"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      26 pages", "url": "http://arxiv.org/abs/2502.03699v3", "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence with\ncapabilities in reasoning, coding, and communication, driving innovation across\nindustries. Their true potential depends on effective alignment to ensure\ncorrect, trustworthy and ethical behavior, addressing challenges like\nmisinformation, hallucinations, bias and misuse. While existing Reinforcement\nLearning (RL)-based alignment methods are notoriously complex, direct\noptimization approaches offer a simpler alternative. In this work, we introduce\na novel direct optimization approach for LLM alignment by drawing on\nestablished Information Retrieval (IR) principles. We present a systematic\nframework that bridges LLM alignment and IR methodologies, mapping LLM\ngeneration and reward models to IR's retriever-reranker paradigm. Building on\nthis foundation, we propose LLM Alignment as Retriever Preference Optimization\n(LarPO), a new alignment method that enhances overall alignment quality.\nExtensive experiments validate LarPO's effectiveness with 38.9 % and 13.7 %\naveraged improvement on AlpacaEval2 and MixEval-Hard respectively. Our work\nopens new avenues for advancing LLM alignment by integrating IR foundations,\noffering a promising direction for future research.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2502.03699v3", "cate": "cs.CL", "date": "2025-02-06", "updated": "2025-07-23", "AI": {"title_translation": "LLM 对齐作为检索器优化：一个信息检索视角", "tldr": "本文提出一种新的直接优化方法LarPO，通过结合信息检索原理来对齐LLM，并在基准测试中表现出显著改进。", "motivation": "现有基于强化学习的LLM对齐方法过于复杂，且LLM的潜力取决于有效的对齐以确保正确、可信和道德的行为，解决错误信息、幻觉、偏见和滥用等问题。", "method": "引入了一种新颖的直接优化方法，将LLM生成和奖励模型映射到IR的检索器-重排器范式，提出了LLM Alignment as Retriever Preference Optimization (LarPO)。", "result": "LarPO在AlpacaEval2上平均改进38.9%，在MixEval-Hard上平均改进13.7%。", "conclusion": "本工作通过整合IR基础，为推进LLM对齐开辟了新途径，为未来研究提供了有希望的方向。", "translation": "大型语言模型（LLMs）以其在推理、编码和通信方面的能力彻底改变了人工智能，推动了各行各业的创新。它们的真正潜力取决于有效的对齐，以确保正确、可信和道德的行为，解决诸如错误信息、幻觉、偏见和滥用等挑战。虽然现有的基于强化学习（RL）的对齐方法以其复杂性而闻名，但直接优化方法提供了一种更简单的替代方案。在这项工作中，我们借鉴既定的信息检索（IR）原则，引入了一种新颖的LLM对齐直接优化方法。我们提出了一个系统的框架，将LLM对齐与IR方法论联系起来，将LLM生成和奖励模型映射到IR的检索器-重排器范式。在此基础上，我们提出了LLM Alignment as Retriever Preference Optimization (LarPO)，这是一种新的对齐方法，可以提高整体对齐质量。广泛的实验验证了LarPO的有效性，在AlpacaEval2上平均提高了38.9%，在MixEval-Hard上平均提高了13.7%。我们的工作通过整合IR基础，为推进LLM对齐开辟了新途径，为未来研究提供了有希望的方向。", "summary": "本文提出了一种名为LarPO的新型直接优化方法，用于大型语言模型（LLM）的对齐。该方法借鉴了信息检索（IR）的原理，将LLM的生成和奖励模型映射到IR的检索器-重排器范式。LarPO旨在解决现有强化学习对齐方法的复杂性，并提高LLM的对齐质量，以确保其行为的正确性、可信度和道德性。实验结果表明，LarPO在AlpacaEval2和MixEval-Hard两个基准测试上均实现了显著的性能提升。这项工作为LLM对齐的研究开辟了新的方向。", "keywords": "LLM对齐, 信息检索, 直接优化, LarPO, 检索器优化", "comments": "这篇论文通过将LLM对齐问题重新概念化为信息检索中的检索器优化问题，提供了一个新颖且有潜力的视角。其创新之处在于引入了LarPO这一直接优化方法，避免了传统RL方法的复杂性，并利用了IR领域成熟的理论和方法。实验结果显示出显著的性能提升，表明该方法在提高LLM对齐质量方面的有效性。这一研究方向为LLM的可靠和道德应用提供了新的思路。"}}
{"id": "2503.13176", "title": "DeGauss: Dynamic-Static Decomposition with Gaussian Splatting for Distractor-free 3D Reconstruction", "authors": ["Rui Wang", "Quentin Lohmeyer", "Mirko Meboldt", "Siyu Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2503.13176v3", "summary": "Reconstructing clean, distractor-free 3D scenes from real-world captures\nremains a significant challenge, particularly in highly dynamic and cluttered\nsettings such as egocentric videos. To tackle this problem, we introduce\nDeGauss, a simple and robust self-supervised framework for dynamic scene\nreconstruction based on a decoupled dynamic-static Gaussian Splatting design.\nDeGauss models dynamic elements with foreground Gaussians and static content\nwith background Gaussians, using a probabilistic mask to coordinate their\ncomposition and enable independent yet complementary optimization. DeGauss\ngeneralizes robustly across a wide range of real-world scenarios, from casual\nimage collections to long, dynamic egocentric videos, without relying on\ncomplex heuristics or extensive supervision. Experiments on benchmarks\nincluding NeRF-on-the-go, ADT, AEA, Hot3D, and EPIC-Fields demonstrate that\nDeGauss consistently outperforms existing methods, establishing a strong\nbaseline for generalizable, distractor-free 3D reconstructionin highly dynamic,\ninteraction-rich environments. Project page:\nhttps://batfacewayne.github.io/DeGauss.io/", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.13176v3", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-24", "AI": {"title_translation": "DeGauss：基于高斯泼溅的动静分解，实现无干扰三维重建", "tldr": "DeGauss是一种基于解耦动静高斯泼溅的自监督框架，能够从动态杂乱场景中重建干净、无干扰的三维场景，并在多项基准测试中超越现有方法。", "motivation": "从真实世界捕获中重建干净、无干扰的三维场景仍然是一个重大挑战，尤其是在高度动态和杂乱的环境中，例如自我中心视频。", "method": "DeGauss引入了一个简单而鲁棒的自监督框架，用于动态场景重建。它基于解耦的动静高斯泼溅设计，使用前景高斯模型动态元素，背景高斯模型静态内容，并利用概率掩码协调它们的组合，实现独立而互补的优化。", "result": "DeGauss在NeRF-on-the-go、ADT、AEA、Hot3D和EPIC-Fields等基准测试中始终优于现有方法。它在各种真实世界场景中（从随意图像集合到长动态自我中心视频）都能稳健泛化，不依赖复杂的启发式方法或大量监督。", "conclusion": "DeGauss为高度动态、交互丰富的环境中可泛化的无干扰三维重建建立了一个强大的基线。", "translation": "从真实世界捕获中重建干净、无干扰的三维场景仍然是一个重大挑战，尤其是在高度动态和杂乱的环境中，例如自我中心视频。为了解决这个问题，我们引入了DeGauss，一个简单而鲁棒的自监督框架，用于基于解耦的动静高斯泼溅设计的动态场景重建。DeGauss使用前景高斯模型动态元素，使用背景高斯模型静态内容，并使用概率掩码协调它们的组合，实现独立而互补的优化。DeGauss在广泛的真实世界场景中（从随意图像集合到长动态自我中心视频）都能稳健泛化，不依赖复杂的启发式方法或大量监督。在包括NeRF-on-the-go、ADT、AEA、Hot3D和EPIC-Fields在内的基准测试中进行的实验表明，DeGauss始终优于现有方法，为高度动态、交互丰富的环境中可泛化的无干扰三维重建建立了一个强大的基线。项目页面：https://batfacewayne.github.io/DeGauss.io/", "summary": "DeGauss是一个针对动态杂乱场景中无干扰三维重建的自监督框架。它采用解耦的动静高斯泼溅设计，通过前景高斯处理动态部分，背景高斯处理静态部分，并利用概率掩码进行协调优化。该方法在多种真实世界场景中表现出强大的泛化能力，无需复杂启发式或大量监督，并在多个基准测试中超越现有技术，为动态环境下的三维重建设定了新的基线。", "keywords": "动态-静态分解, 高斯泼溅, 三维重建, 无干扰, 自我中心视频", "comments": "该论文的创新之处在于其解耦的动静高斯泼溅设计，以及自监督的学习范式，使其能够在没有大量监督的情况下，鲁棒地处理高度动态和杂乱的真实世界场景，有效解决了传统三维重建中的干扰物问题。"}}
{"id": "2412.17934", "title": "UAV Communications: Impact of Obstacles on Channel Characteristics", "authors": ["Kamal Shayegan"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.17934v5", "summary": "In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as\neffective platforms for carrying Wi-Fi Access Points (APs) and cellular Base\nStations (BSs), enabling low-cost, agile, and flexible wireless networks with\nhigh Quality of Service (QoS). The next generation of wireless communications\nwill rely on increasingly higher frequencies, which are easily obstructed by\nobstacles. One of the most critical concepts yet to be fully addressed is\npositioning the UAV at optimal coordinates while accounting for obstacles. To\nensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS,\nand establish reliable wireless links with maximum coverage, obstacles must be\nintegrated into the proposed placement algorithms. This paper introduces a\nsimulation-based measurement approach for characterizing an air-to-ground (AG)\nchannel in a simple scenario. By considering obstacles, we present a novel\nperspective on channel characterization. The results, in terms of throughput,\npacket delivery, packet loss, and delay, are compared using the proposed\npositioning approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.17934v5", "cate": "cs.NI", "date": "2024-12-23", "updated": "2025-07-24", "AI": {"title_translation": "无人机通信：障碍物对信道特性的影响", "tldr": "鉴于高频无线通信易受障碍物影响，本文提出了一种基于仿真的方法来表征存在障碍物的空地信道，并比较了其对网络性能指标（如吞吐量、包传输率、包丢失和延迟）的影响。", "motivation": "下一代无线通信将依赖于易受障碍物阻碍的更高频率。为确保无人机（UAV）与用户设备（UE）之间的视距、提高服务质量（QoS）并建立最大覆盖范围的可靠无线链路，必须在无人机放置算法中整合障碍物因素。", "method": "本文引入了一种基于仿真的测量方法，用于在简单场景下表征空地（AG）信道。通过考虑障碍物，该研究提出了一种关于信道特性的新颖视角。", "result": "结果从吞吐量、数据包传输、数据包丢失和延迟方面，比较了使用所提出的定位方法后的性能。", "conclusion": "通过考虑障碍物进行无人机定位，可以改善无线网络性能，提升吞吐量、数据包传输率并降低数据包丢失和延迟。", "translation": "近年来，无人机（UAV）已被用作携带Wi-Fi接入点（AP）和蜂窝基站（BS）的有效平台，实现了低成本、敏捷、灵活且具有高服务质量（QoS）的无线网络。下一代无线通信将依赖于越来越高的频率，这些频率很容易被障碍物阻碍。一个尚未完全解决的最关键概念是在考虑障碍物的情况下将无人机放置在最佳坐标。为了确保无人机与用户设备（UE）之间的视距（LoS）、提高QoS并建立具有最大覆盖范围的可靠无线链路，必须将障碍物整合到所提出的放置算法中。本文介绍了一种基于仿真的测量方法，用于在简单场景下表征空地（AG）信道。通过考虑障碍物，我们提出了信道特性的新颖视角。研究结果在吞吐量、数据包传输、数据包丢失和延迟方面，使用所提出的定位方法进行了比较。", "summary": "本文研究了障碍物对无人机（UAV）通信信道特性的影响。鉴于未来无线通信将使用易受障碍物阻碍的高频，论文强调了在无人机定位算法中考虑障碍物的重要性，以确保视距、提高服务质量和扩大覆盖范围。为此，论文提出了一种基于仿真的测量方法来表征存在障碍物时的空地信道。研究结果通过吞吐量、数据包传输、数据包丢失和延迟等指标，比较了考虑障碍物的定位方法对网络性能的影响。", "keywords": "无人机通信, 障碍物, 信道特性, 空地信道, 定位算法", "comments": "这篇论文的创新点在于将障碍物明确纳入空地（AG）信道特性分析中，为无人机在复杂环境中进行优化定位提供了新的视角。这对于未来高频无线通信中无人机基站的部署具有重要意义，有助于提高网络性能和可靠性。"}}
{"id": "2409.13725", "title": "Identity-related Speech Suppression in Generative AI Content Moderation", "authors": ["Grace Proebsting", "Oghenefejiro Isaacs Anigboro", "Charlie M. Crawford", "Danaé Metaxa", "Sorelle A. Friedler"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, 2025", "url": "http://arxiv.org/abs/2409.13725v3", "summary": "Automated content moderation has long been used to help identify and filter\nundesired user-generated content online. But such systems have a history of\nincorrectly flagging content by and about marginalized identities for removal.\nGenerative AI systems now use such filters to keep undesired generated content\nfrom being created by or shown to users. While a lot of focus has been given to\nmaking sure such systems do not produce undesired outcomes, considerably less\nattention has been paid to making sure appropriate text can be generated. From\nclassrooms to Hollywood, as generative AI is increasingly used for creative or\nexpressive text generation, whose stories will these technologies allow to be\ntold, and whose will they suppress?\n  In this paper, we define and introduce measures of speech suppression,\nfocusing on speech related to different identity groups incorrectly filtered by\na range of content moderation APIs. Using both short-form, user-generated\ndatasets traditional in content moderation and longer generative AI-focused\ndata, including two datasets we introduce in this work, we create a benchmark\nfor measurement of speech suppression for nine identity groups. Across one\ntraditional and four generative AI-focused automated content moderation\nservices tested, we find that identity-related speech is more likely to be\nincorrectly suppressed than other speech. We find that reasons for incorrect\nflagging behavior vary by identity based on stereotypes and text associations,\nwith, e.g., disability-related content more likely to be flagged for self-harm\nor health-related reasons while non-Christian content is more likely to be\nflagged as violent or hateful. As generative AI systems are increasingly used\nfor creative work, we urge further attention to how this may impact the\ncreation of identity-related content.", "comment": "ACM Conference on Equity and Access in Algorithms, Mechanisms, and\n  Optimization, 2025", "pdf_url": "http://arxiv.org/pdf/2409.13725v3", "cate": "cs.CL", "date": "2024-09-09", "updated": "2025-07-24", "AI": {"title_translation": "生成式AI内容审核中的身份相关言论压制", "tldr": "研究发现，在生成式AI内容审核中，与身份相关的言论更容易被错误地压制，且压制原因因身份群体而异。", "motivation": "自动化内容审核系统长期以来错误地标记边缘化群体的相关内容。随着生成式AI越来越多地用于创作和表达性文本生成，需要关注这些技术将允许哪些故事被讲述，又将压制哪些故事。", "method": "本文定义并引入了言论压制的衡量标准，重点关注被内容审核API错误过滤的与不同身份群体相关的言论。作者使用了传统的短文本用户生成数据集和更长的生成式AI专用数据集（包括两个新引入的数据集），创建了一个针对九个身份群体的言论压制测量基准，并测试了一个传统和四个生成式AI内容审核服务。", "result": "发现与身份相关的言论比其他言论更容易被错误地压制。错误标记行为的原因因身份群体而异，基于刻板印象和文本关联，例如，与残疾相关的内容更容易被标记为自残或健康相关，而非基督教内容更容易被标记为暴力或仇恨。", "conclusion": "随着生成式AI系统越来越多地用于创意工作，作者呼吁进一步关注这可能如何影响身份相关内容的创作。", "translation": "自动化内容审核长期以来一直被用于帮助识别和过滤在线不良用户生成内容。但这些系统在错误标记边缘化身份群体创作或相关内容方面有着历史记录。生成式AI系统现在使用这些过滤器来阻止不良生成内容被创建或展示给用户。虽然很多关注点都放在确保此类系统不会产生不良结果上，但对确保能生成适当文本的关注却少得多。从教室到好莱坞，随着生成式AI越来越多地用于创意或表达性文本生成，这些技术将允许哪些故事被讲述，又将压制哪些故事？\n在本文中，我们定义并引入了言论压制的衡量标准，重点关注被一系列内容审核API错误过滤的与不同身份群体相关的言论。我们使用内容审核中传统的短文本用户生成数据集和更长的生成式AI专用数据（包括我们在这项工作中引入的两个数据集），创建了一个针对九个身份群体的言论压制测量基准。在测试的一个传统和四个生成式AI自动化内容审核服务中，我们发现与身份相关的言论比其他言论更容易被错误地压制。我们发现错误标记行为的原因因身份群体而异，基于刻板印象和文本关联，例如，与残疾相关的内容更容易被标记为自残或健康相关，而非基督教内容更容易被标记为暴力或仇恨。随着生成式AI系统越来越多地用于创意工作，我们呼吁进一步关注这可能如何影响身份相关内容的创作。", "summary": "本文研究了生成式AI内容审核中存在的身份相关言论压制问题。研究指出，现有自动化审核系统常错误过滤边缘化群体内容。作者定义了言论压制并引入衡量标准，利用多种数据集（包括两个新数据集）为九个身份群体建立了基准。实验发现，与身份相关的言论更容易被错误压制，且错误标记原因因身份和刻板印象而异（如残疾内容被标记为自残，非基督教内容被标记为暴力）。文章呼吁关注生成式AI在创意领域应用时对身份相关内容创作的影响。", "keywords": "身份相关言论, 内容审核, 生成式AI, 言论压制, 偏见", "comments": "这篇论文解决了生成式AI内容审核中一个关键的伦理问题，即系统性偏见对边缘化群体言论的压制。其创新之处在于定义并量化了“言论压制”这一现象，并为此提供了测量基准。论文的重要性在于揭示了现有AI审核机制如何延续甚至加剧社会偏见，尤其是在AI被广泛应用于创意内容生成时，这可能严重影响特定群体故事的讲述和表达。"}}
{"id": "2507.18512", "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts", "authors": ["Clément Cornet", "Romaric Besançon", "Hervé Le Borgne"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18512v1", "summary": "Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18512v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "解释视觉、文本和多模态编码器如何共享概念", "tldr": "该研究提出了一种新颖的指标和方法，用于定量比较视觉、文本和多模态编码器通过稀疏自编码器（SAE）提取的特征共享程度，并发现多模态模型中的视觉特征与文本编码器共享，强调了文本预训练的影响。", "motivation": "以前基于稀疏自编码器（SAE）特征的模型比较仅限于相同模态内的模型，缺乏跨模态的定量比较方法。", "method": "1. 提出了一种新颖的指标，用于定量比较跨稀疏自编码器（SAE）特征的模型。2. 提出量化不同类别模型之间单个特征的“比较共享度”（Comparative Sharedness）。3. 使用这两种新工具对21种视觉、文本和多模态编码器进行了研究，涵盖两种不同大小的模型和通用及领域特定数据集。", "result": "1. 研究结果使得能够重新审视多模态背景下训练的编码器，并量化所有这些模型共享表示或特征的程度。2. 结果表明，视觉语言模型（VLMs）中视觉编码器特有的视觉特征与文本编码器共享，这突出了文本预训练的影响。", "conclusion": "该研究通过新的量化工具揭示了视觉、文本和多模态编码器之间特征共享的程度，并强调了文本预训练在促成跨模态特征共享中的作用。", "translation": "稀疏自编码器（SAE）已成为一种从神经网络激活中提取人类可解释特征的强大技术。以前的工作基于SAE派生的特征比较了不同的模型，但这些比较仅限于相同模态内的模型。我们提出了一种新颖的指标，允许对SAE特征跨模型进行定量比较，并用它对视觉、文本和多模态编码器进行比较研究。我们还提出量化不同类别模型之间单个特征的比较共享度。通过这两种新工具，我们对21种编码器进行了多项研究，这些编码器属于三种类型，具有两种显著不同的大小，并考虑了通用和领域特定数据集。结果使得能够根据在多模态环境中训练的编码器重新审视以前的研究，并量化所有这些模型共享表示或特征的程度。它们还表明，视觉语言模型（VLMs）中视觉编码器特有的视觉特征与文本编码器共享，突出了文本预训练的影响。代码可在https://github.com/CEA-LIST/SAEshareConcepts获取。", "summary": "该论文提出了一种新颖的指标和“比较共享度”量化方法，以解决以往基于稀疏自编码器（SAE）的特征比较仅限于单一模态的局限性。通过这些工具，作者对视觉、文本和多模态编码器进行了跨模态的定量比较研究。研究结果不仅量化了不同模态编码器之间特征共享的程度，还特别指出视觉语言模型（VLMs）中的视觉特征与文本编码器共享，强调了文本预训练在跨模态特征共享中的关键作用。", "keywords": "稀疏自编码器, 跨模态比较, 特征共享, 视觉编码器, 文本编码器", "comments": "这项工作具有创新性，因为它解决了跨模态模型特征比较的难题，提出了具体的量化指标。通过对多种编码器的大规模研究，为理解不同模态模型如何共享概念提供了新的视角，特别是揭示了文本预训练对视觉特征共享的影响，这对于多模态学习和模型可解释性研究具有重要意义。"}}
{"id": "2507.17928", "title": "A novel finite element method for simulating surface plasmon polaritons on complex graphene sheets", "authors": ["Jichun Li", "Michael Neunteufel", "Li Zhu"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17928v1", "summary": "Surface plasmon polaritons (SPPs) are generated on the graphene surface, and\nprovide a window into the nano-optical and electrodynamic response of their\nhost material and its dielectric environment. An accurate simulation of SPPs\npresents several unique challenges, since SPPs often occur at complex\ninterfaces between materials of different dielectric constants and appropriate\nboundary conditions at the graphene interfaces are crucial. Here we develop a\nsimplified graphene model and propose a new finite element method accordingly.\nStability for the continuous model is established, and extensive numerical\nresults are presented to demonstrate that the new model can capture the SPPs\nvery well for various complex graphene sheets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17928v1", "cate": "math.NA", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一种模拟复杂石墨烯薄片表面等离子体激元的新型有限元方法", "tldr": "本文开发了一种简化的石墨烯模型和一种新型有限元方法，用于准确模拟复杂石墨烯薄片上的表面等离子体激元（SPPs），解决了现有模拟中的挑战。", "motivation": "对表面等离子体激元（SPPs）的精确模拟存在一些独特的挑战，因为SPPs通常发生在不同介电常数材料之间的复杂界面处，并且石墨烯界面处的适当边界条件至关重要。", "method": "本文开发了一种简化的石墨烯模型，并相应地提出了一种新的有限元方法。", "result": "大量的数值结果表明，新模型能够很好地捕获各种复杂石墨烯薄片上的表面等离子体激元（SPPs）。", "conclusion": "所提出的简化石墨烯模型和新型有限元方法能够很好地捕获各种复杂石墨烯薄片上的表面等离子体激元（SPPs）。", "translation": "表面等离子体激元（SPPs）在石墨烯表面产生，为研究其宿主材料及其介电环境的纳米光学和电动力学响应提供了一个窗口。对SPPs的精确模拟存在一些独特的挑战，因为SPPs通常发生在不同介电常数材料之间的复杂界面处，并且石墨烯界面处的适当边界条件至关重要。本文开发了一种简化的石墨烯模型，并相应地提出了一种新的有限元方法。建立了连续模型的稳定性，并提供了大量的数值结果，以证明新模型能够很好地捕获各种复杂石墨烯薄片上的SPPs。", "summary": "本文旨在解决石墨烯表面上表面等离子体激元（SPPs）精确模拟所面临的挑战，尤其是在复杂界面和关键边界条件方面。作者提出了一种新颖的方法，包括一个简化的石墨烯模型和一个新的有限元方法。数值结果证实了连续模型的稳定性，并证明了其在准确捕获各种复杂石墨烯薄片上SPPs方面的有效性。", "keywords": "表面等离子体激元, 石墨烯, 有限元方法, 模拟, 复杂界面", "comments": "该论文引入了一种创新的有限元方法，结合简化的石墨烯模型，解决了模拟石墨烯上SPPs的复杂问题。其主要创新在于简化石墨烯模型并开发定制的有限元方法，这有望显著提高SPP模拟的效率和准确性，尤其适用于复杂几何形状。这项工作对于推动石墨烯纳米光学和电动力学领域的研究具有重要意义。"}}
{"id": "2507.14314", "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "authors": ["Marija Anđelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at Slavic NLP 2025", "url": "http://arxiv.org/abs/2507.14314v2", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "comment": "Accepted at Slavic NLP 2025", "pdf_url": "http://arxiv.org/pdf/2507.14314v2", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-24", "AI": {"title_translation": "是什么让你CLIC：克罗地亚点击诱饵标题检测", "tldr": "研究了克罗地亚语点击诱饵标题的自动检测，构建了CLIC数据集，发现微调模型比通用大型语言模型效果更好，且近半数标题是点击诱饵。", "motivation": "自动检测点击诱饵标题对于维护数字媒体的信息质量和读者信任至关重要。特别是在资源较少的语言中，尚不清楚微调方法或上下文学习（ICL）哪种效果更好。", "method": "1. 编译了一个名为CLIC的克罗地亚语新闻点击诱饵检测数据集，涵盖20年。2. 在此任务上对BERTi'c模型进行微调。3. 将其性能与使用克罗地亚语和英语提示的基于LLM的上下文学习方法进行比较。4. 分析了点击诱饵的语言特性。", "result": "1. 近一半的分析标题包含点击诱饵。2. 微调模型比通用大型语言模型（LLMs）表现出更好的结果。", "conclusion": "对于克罗地亚语点击诱饵标题检测，微调模型优于通用LLM的上下文学习方法，且点击诱饵现象普遍存在。", "translation": "在线新闻媒体主要依靠广告收入模式运营，这迫使记者创作出通常具有煽动性、引人入胜和挑衅性的标题——通常被称为点击诱饵。点击诱饵标题的自动检测对于维护数字媒体的信息质量和读者信任至关重要，它需要上下文理解和世界知识。对于这项任务，特别是在资源较少的语言中，微调方法还是上下文学习（ICL）能产生更好的结果尚不清楚。在本文中，我们编译了CLIC，这是一个用于克罗地亚新闻标题点击诱饵检测的新数据集，时间跨度为20年，涵盖了主流和边缘媒体。我们在此任务上对BERTi'c模型进行了微调，并将其性能与使用克罗地亚语和英语提示的基于LLM的ICL方法进行了比较。最后，我们分析了点击诱饵的语言特性。我们发现，近一半的分析标题包含点击诱饵，并且微调模型比通用LLM提供了更好的结果。", "summary": "本文针对克罗地亚语新闻标题的点击诱饵检测问题，构建了一个名为CLIC的20年跨度数据集。研究者在CLIC数据集上对BERTi'c模型进行了微调，并与基于大型语言模型的上下文学习方法（包括克罗地亚语和英语提示）进行了比较。研究发现，近一半的克罗地亚新闻标题属于点击诱饵，且微调模型在点击诱饵检测任务上表现优于通用大型语言模型。文章还分析了点击诱饵的语言特性。", "keywords": "点击诱饵检测, 克罗地亚语, 数据集, 微调模型, 上下文学习", "comments": "这项研究通过构建特定语言的点击诱饵数据集（CLIC），填补了克罗地亚语这一资源较少语言在点击诱饵检测方面的空白。其创新之处在于比较了微调模型与LLM-based ICL在低资源语言任务上的表现，并明确指出微调模型效果更优，这为未来低资源语言的NLP任务提供了实践指导。研究结果揭示了克罗地亚语新闻中点击诱饵的普遍性，强调了自动检测的重要性。"}}
{"id": "2507.18025", "title": "A Novel Coded Computing Approach for Distributed Multi-Task Learning", "authors": ["Minquan Cheng", "Yongkang Wang", "Lingyu Zhang", "Youlong Wu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      17 pages, 2 figures", "url": "http://arxiv.org/abs/2507.18025v1", "summary": "Distributed multi-task learning (DMTL) effectively improves model\ngeneralization performance through the collaborative training of multiple\nrelated models. However, in large-scale learning scenarios, communication\nbottlenecks severely limit practical system performance. In this paper, we\ninvestigate the communication bottleneck within a typical DMTL system that\nemploys non-linear global updates. This system involves distributed workers,\nassisted by a central server, who collaboratively learn distinct models derived\nfrom a non-linear aggregation of their local model parameters. We first\ncharacterize the communication process as a matrix decomposition problem. It\ntransforms workers' data storage constraints into structural characteristics of\nthe uplink encoding matrix, and worker data retrieval demands into Maximum\nDistance Separable (MDS) properties of the downlink encoding matrix. Building\non this, we propose a novel coded DTML scheme that can greatly reduce the\ncommunication cost of the DTML with heterogeneous data placement. Theoretical\nanalysis demonstrates that the proposed scheme achieves the theoretical lower\nbound for communication overhead under mild conditions. Remarkably, this\noptimality holds for both traditional homogeneous computing environments and\nvarious heterogeneous scenarios. Furthermore, our scheme is extensible to a\ndistributed linearly separable computation problem where the target function\ninvolves multiple linear combinations of local update values. This indicates\nthat our scheme offers a new way of tackling heterogeneous data placement\nchallenges in various distributed applications.", "comment": "17 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.18025v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "一种用于分布式多任务学习的新型编码计算方法", "tldr": "本文提出了一种新颖的编码分布式多任务学习（DTML）方案，通过将通信过程建模为矩阵分解问题，显著降低了异构数据放置下的通信成本，并理论上达到了通信开销的理论下限。", "motivation": "分布式多任务学习（DMTL）在提高模型泛化性能方面表现出色，但在大规模学习场景中，通信瓶颈严重限制了实际系统性能。本文旨在解决典型DMTL系统中非线性全局更新下的通信瓶颈问题。", "method": "本文将DMTL系统中的通信过程表征为矩阵分解问题，将工作节点的数据存储约束转化为上行编码矩阵的结构特性，将数据检索需求转化为下行编码矩阵的最大距离可分离（MDS）特性。在此基础上，提出了一种新型编码DTML方案。", "result": "所提出的方案能够大幅降低异构数据放置下DTML的通信成本。理论分析表明，该方案在温和条件下实现了通信开销的理论下限，并且在传统的同构计算环境和各种异构场景中都保持了最优性。", "conclusion": "本文提出的新型编码计算方案为解决分布式多任务学习中的通信瓶颈问题提供了一种有效方法，尤其在异构数据放置环境下表现出色，并可扩展应用于其他分布式线性可分离计算问题。", "translation": "分布式多任务学习（DMTL）通过多个相关模型的协同训练有效提高了模型的泛化性能。然而，在大规模学习场景中，通信瓶颈严重限制了实际系统性能。在本文中，我们研究了采用非线性全局更新的典型DMTL系统中的通信瓶颈。该系统涉及由中央服务器辅助的分布式工作节点，它们协同学习源自其局部模型参数非线性聚合的不同模型。我们首先将通信过程表征为矩阵分解问题。它将工作节点的数据存储约束转化为上行编码矩阵的结构特性，并将工作节点的数据检索需求转化为下行编码矩阵的最大距离可分离（MDS）特性。在此基础上，我们提出了一种新型编码DTML方案，该方案可以大大降低异构数据放置下DTML的通信成本。理论分析表明，所提出的方案在温和条件下实现了通信开销的理论下限。值得注意的是，这种最优性适用于传统的同构计算环境和各种异构场景。此外，我们的方案可扩展到目标函数涉及局部更新值的多个线性组合的分布式线性可分离计算问题。这表明我们的方案为解决各种分布式应用中的异构数据放置挑战提供了一种新方法。", "summary": "本文针对分布式多任务学习（DMTL）在大规模场景下的通信瓶颈问题，提出了一种新型编码计算方法。该方法将通信过程建模为矩阵分解问题，并基于此设计了一种编码DTML方案，旨在显著降低异构数据放置下的通信成本。理论分析验证了该方案能够达到通信开销的理论下限，且在同构和异构环境下均表现出最优性。此外，该方案还可推广应用于其他分布式线性可分离计算问题。", "keywords": "分布式多任务学习, 编码计算, 通信瓶颈, 矩阵分解, 异构数据放置", "comments": "该论文的创新点在于将分布式多任务学习中的通信瓶颈问题转化为矩阵分解问题，并设计出一种新型编码方案。其重要性在于，该方案不仅理论上证明了其在通信开销上的最优性，而且在异构数据放置的实际场景中也具有显著的性能提升，为大规模分布式学习提供了新的解决思路。"}}
{"id": "2507.12901", "title": "Agentar-DeepFinance-100K: A Large-Scale Financial Dataset via Systematic Chain-of-Thought Synthesis Optimization", "authors": ["Xiaoke Zhao", "Zhaowen Zhou", "Lin Chen", "Lihong Wang", "Zhiyi Huang", "Kaiyuan Zheng", "Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Jiawei Liu", "Xiang Qi", "Bo Zhang", "Peng Zhang", "Wei Wang", "Zhe Li"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12901v2", "summary": "Recent advancements in large language models (LLMs) have demonstrated\nremarkable general reasoning capabilities, holding significant potential for\napplications in the financial domain, a field that requires robust and reliable\nreasoning. It has been demonstrated that distilling high-quality\nchain-of-thought (CoT) rationales from advanced general reasoning models offers\na promising and efficient path to the financial reasoning model. However,\nexisting CoT synthesis methods suffer from shallow CoT sampling, leaving the\nquestion of how to construct a well-designed knowledge space for finance\nreasoning unexplored. In this paper, we present Agentar-DeepFinance-100K, a\nlarge-scale financial reasoning dataset characterized by its systematic CoT\nsynthesis optimization. We first introduce a comprehensive CoT synthesis\npipeline featuring Multi-perspective Knowledge Extraction (MKE) and\nSelf-Corrective Rewriting (SCR) to generate exhaustive and deep financial\nreasoning trajectories. Furthermore, a systematic investigation, termed CoT\nCube, is conducted to analyze critical factors that influence CoT\neffectiveness, such as necessity, length and synthesizer, yielding valuable\ninsights for high-quality financial CoT construction. Experiments demonstrate\nthat models trained on our Agentar-DeepFinance-100K achieve significant\nimprovements on financial benchmarks. We publicly release\nAgentar-DeepFinance-100K , hoping to advance the research in financial\nreasoning models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12901v2", "cate": "cs.CE", "date": "2025-07-17", "updated": "2025-07-24", "AI": {"title_translation": "Agentar-DeepFinance-100K：一个通过系统性思维链合成优化构建的大规模金融数据集", "tldr": "Agentar-DeepFinance-100K是一个大规模金融推理数据集，通过多视角知识提取和自我纠正重写优化思维链合成，显著提升了金融基准模型的表现。", "motivation": "现有思维链（CoT）合成方法存在浅层CoT采样问题，且未充分探索如何为金融推理构建设计良好的知识空间。大语言模型（LLMs）在金融领域有巨大潜力，但需要鲁棒可靠的推理能力。", "method": "提出了Agentar-DeepFinance-100K数据集，通过以下方法构建：1. 引入了多视角知识提取（MKE）和自我纠正重写（SCR）的综合CoT合成流程，以生成详尽深入的金融推理轨迹。2. 进行了名为“CoT Cube”的系统性研究，分析影响CoT有效性的关键因素（如必要性、长度和合成器）。", "result": "在Agentar-DeepFinance-100K上训练的模型在金融基准测试中取得了显著改进。", "conclusion": "通过系统性思维链合成优化，构建了Agentar-DeepFinance-100K大规模金融数据集，有效提升了金融推理模型的性能。", "translation": "大型语言模型（LLMs）的最新进展展示了卓越的通用推理能力，在金融领域具有巨大潜力，该领域需要稳健可靠的推理。事实证明，从先进的通用推理模型中提取高质量的思维链（CoT）原理为金融推理模型提供了一条有前景且高效的路径。然而，现有的CoT合成方法存在浅层CoT采样问题，关于如何构建一个设计良好的金融推理知识空间的问题尚未得到探索。在本文中，我们提出了Agentar-DeepFinance-100K，一个以系统性CoT合成优化为特征的大规模金融推理数据集。我们首先引入了一个全面的CoT合成流程，包括多视角知识提取（MKE）和自我纠正重写（SCR），以生成详尽深入的金融推理轨迹。此外，还进行了一项名为CoT Cube的系统性研究，分析了影响CoT有效性的关键因素，如必要性、长度和合成器，为高质量金融CoT的构建提供了宝贵见解。实验表明，在我们的Agentar-DeepFinance-100K上训练的模型在金融基准测试中取得了显著改进。我们公开发布Agentar-DeepFinance-100K，希望能推动金融推理模型的研究。", "summary": "本文介绍了Agentar-DeepFinance-100K，一个大规模金融推理数据集，旨在解决现有思维链（CoT）合成方法在金融领域中深度和广度不足的问题。该数据集通过引入多视角知识提取（MKE）和自我纠正重写（SCR）的CoT合成流程，生成了更全面深入的金融推理轨迹。研究还通过“CoT Cube”分析了影响CoT有效性的关键因素。实验证明，基于该数据集训练的模型在金融基准测试上表现出显著提升。", "keywords": "金融数据集, 思维链, 大语言模型, 金融推理, 数据合成", "comments": "该论文的创新点在于提出了系统性思维链合成优化方法，特别是多视角知识提取（MKE）和自我纠正重写（SCR）的CoT合成流程，以及“CoT Cube”分析框架，这些都旨在解决金融领域中CoT推理的深度和质量问题。其重要性在于提供了一个大规模、高质量的金融推理数据集，有望推动金融领域大语言模型应用的发展。"}}
{"id": "2507.18603", "title": "Demystify Protein Generation with Hierarchical Conditional Diffusion Models", "authors": ["Zinan Ling", "Yi Shi", "Da Yan", "Yang Zhou", "Bo Hui"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18603v1", "summary": "Generating novel and functional protein sequences is critical to a wide range\nof applications in biology. Recent advancements in conditional diffusion models\nhave shown impressive empirical performance in protein generation tasks.\nHowever, reliable generations of protein remain an open research question in de\nnovo protein design, especially when it comes to conditional diffusion models.\nConsidering the biological function of a protein is determined by multi-level\nstructures, we propose a novel multi-level conditional diffusion model that\nintegrates both sequence-based and structure-based information for efficient\nend-to-end protein design guided by specified functions. By generating\nrepresentations at different levels simultaneously, our framework can\neffectively model the inherent hierarchical relations between different levels,\nresulting in an informative and discriminative representation of the generated\nprotein. We also propose a Protein-MMD, a new reliable evaluation metric, to\nevaluate the quality of generated protein with conditional diffusion models.\nOur new metric is able to capture both distributional and functional\nsimilarities between real and generated protein sequences while ensuring\nconditional consistency. We experiment with the benchmark datasets, and the\nresults on conditional protein generation tasks demonstrate the efficacy of the\nproposed generation framework and evaluation metric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18603v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用分层条件扩散模型揭示蛋白质生成之谜", "tldr": "本文提出了一种多级条件扩散模型，用于可靠的蛋白质生成，并引入了一种新的评估指标Protein-MMD。", "motivation": "尽管条件扩散模型在蛋白质生成任务中表现出色，但在从头蛋白质设计中，尤其是在使用条件扩散模型时，蛋白质的可靠生成仍是一个开放的研究问题。", "method": "本文提出了一种新颖的多级条件扩散模型，该模型整合了基于序列和基于结构的信息，以实现由指定功能引导的端到端蛋白质设计。该模型通过同时生成不同层级的表示来建模固有的层次关系。此外，本文还提出了一种新的可靠评估指标Protein-MMD，用于评估条件扩散模型生成的蛋白质质量，该指标能捕获分布和功能相似性并确保条件一致性。", "result": "在基准数据集上的实验结果表明，所提出的生成框架和评估指标在条件蛋白质生成任务中是有效的。", "conclusion": "所提出的多级条件扩散模型和Protein-MMD评估指标能够有效地解决蛋白质生成中的可靠性问题，并为从头蛋白质设计提供了有效的方法和评估工具。", "translation": "生成新型功能性蛋白质序列对于广泛的生物应用至关重要。近期条件扩散模型在蛋白质生成任务中表现出令人印象深刻的经验性能。然而，在从头蛋白质设计中，蛋白质的可靠生成仍然是一个开放的研究问题，尤其是在条件扩散模型方面。考虑到蛋白质的生物功能由多级结构决定，我们提出了一种新颖的多级条件扩散模型，该模型整合了基于序列和基于结构的信息，以实现由指定功能引导的高效端到端蛋白质设计。通过同时生成不同层级的表示，我们的框架可以有效地建模不同层级之间固有的层次关系，从而生成信息丰富且具有辨别力的蛋白质表示。我们还提出了一种新的可靠评估指标Protein-MMD，用于评估条件扩散模型生成的蛋白质质量。我们的新指标能够捕获真实和生成蛋白质序列之间的分布和功能相似性，同时确保条件一致性。我们在基准数据集上进行了实验，条件蛋白质生成任务的结果证明了所提出的生成框架和评估指标的有效性。", "summary": "本文针对条件扩散模型在蛋白质生成中可靠性不足的问题，提出了一种新颖的多级条件扩散模型。该模型整合了序列和结构信息，并能同时生成多级表示以建模蛋白质的固有层次关系。此外，文章还引入了一种名为Protein-MMD的新型评估指标，用于衡量生成蛋白质的质量，该指标能捕捉分布和功能相似性并保证条件一致性。实验结果验证了所提出框架和评估指标的有效性。", "keywords": "蛋白质生成, 条件扩散模型, 分层模型, Protein-MMD, 从头蛋白质设计", "comments": "本文的创新点在于提出了一个整合序列和结构信息的多级条件扩散模型，有效解决了蛋白质生成中的可靠性问题，并能更好地建模蛋白质的复杂层次结构。同时，引入的Protein-MMD评估指标为条件蛋白质生成提供了更全面和可靠的评价方法，对从头蛋白质设计领域具有重要意义。"}}
{"id": "2503.10009", "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM", "authors": ["Bowen Zhang", "Pengcheng Luo"], "categories": ["cs.AI", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, 12 figures", "url": "http://arxiv.org/abs/2503.10009v2", "summary": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to Operations Research (OR) problem-solving has attracted increasing\nattention. Most existing approaches attempt to improve OR problem-solving\nthrough prompt engineering or fine-tuning strategies for LLMs. However, these\nmethods are fundamentally constrained by the limited capabilities of\nnon-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an\nAI agent built on reasoning LLMs for automated OR problem solving. The agent\ndecomposes the task into three sequential stages: mathematical modeling, code\ngeneration, and debugging. Each task is handled by a dedicated sub-agent, which\nenables more targeted reasoning. We also construct BWOR, a high-quality dataset\nfor evaluating LLM performance on OR tasks. Our analysis shows that existing\nbenchmarks such as NL4OPT, MAMO, and IndustryOR suffer from certain issues,\nmaking them less suitable for reliably evaluating LLM performance. In contrast,\nBWOR provides a more consistent and discriminative assessment of model\ncapabilities. Experimental results demonstrate that OR-LLM-Agent outperforms\nadvanced methods, including GPT-o3, Gemini 2.5 Pro, and ORLM, by at least 7% in\naccuracy. These results demonstrate the effectiveness of task decomposition for\nOR problem solving.", "comment": "8 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2503.10009v2", "cate": "cs.AI", "date": "2025-03-13", "updated": "2025-07-24", "AI": {"title_translation": "OR-LLM-Agent：使用推理LLM自动化运筹学优化问题的建模与求解", "tldr": "OR-LLM-Agent是一个基于推理LLM的AI代理，通过任务分解和专用子代理，提高了运筹学问题建模和求解的自动化水平，并在新数据集BWOR上表现优于现有先进方法。", "motivation": "现有将大型语言模型（LLMs）应用于运筹学（OR）问题求解的方法，如提示工程或微调策略，受到非推理LLM能力有限的根本限制，无法有效解决OR问题。", "method": "提出OR-LLM-Agent，一个基于推理LLM的AI代理，用于自动化运筹学问题求解。该代理将任务分解为三个顺序阶段：数学建模、代码生成和调试，每个阶段由一个专用子代理处理，以实现更有针对性的推理。同时构建了高质量数据集BWOR，用于评估LLM在OR任务上的性能。", "result": "OR-LLM-Agent在准确性方面至少比GPT-o3、Gemini 2.5 Pro和ORLM等先进方法高出7%。新构建的BWOR数据集比现有基准（如NL4OPT、MAMO和IndustryOR）更适合可靠地评估LLM性能，能提供更一致和有区分度的模型能力评估。", "conclusion": "实验结果表明，任务分解对于运筹学问题求解是有效的。", "translation": "随着人工智能（AI）的兴起，将大型语言模型（LLMs）应用于运筹学（OR）问题求解受到了越来越多的关注。大多数现有方法试图通过LLMs的提示工程或微调策略来改进OR问题求解。然而，这些方法从根本上受到非推理LLMs能力有限的限制。为了克服这些限制，我们提出了OR-LLM-Agent，一个基于推理LLMs构建的AI代理，用于自动化OR问题求解。该代理将任务分解为三个顺序阶段：数学建模、代码生成和调试。每个任务由一个专用的子代理处理，这使得推理更具针对性。我们还构建了BWOR，一个用于评估LLM在OR任务上性能的高质量数据集。我们的分析表明，现有的基准测试，如NL4OPT、MAMO和IndustryOR，存在某些问题，使其不太适合可靠地评估LLM性能。相比之下，BWOR提供了更一致和有区分度的模型能力评估。实验结果表明，OR-LLM-Agent在准确性方面至少比先进方法（包括GPT-o3、Gemini 2.5 Pro和ORLM）高出7%。这些结果证明了任务分解在OR问题求解中的有效性。", "summary": "本文提出了OR-LLM-Agent，一个基于推理型大语言模型（LLM）的AI代理，旨在自动化运筹学（OR）优化问题的建模与求解。针对现有LLM方法在OR问题处理上的局限性，OR-LLM-Agent通过将任务分解为数学建模、代码生成和调试三个阶段，并为每个阶段配备专用子代理，实现了更精细的推理。此外，研究团队还构建了高质量的OR任务评估数据集BWOR，弥补了现有基准的不足。实验结果显示，OR-LLM-Agent在准确性上显著优于现有先进模型，验证了任务分解在OR问题求解中的有效性。", "keywords": "运筹学, 大型语言模型, 自动化, 代理系统, 任务分解", "comments": "该论文的创新点在于提出了一个基于推理LLM的多代理系统OR-LLM-Agent，通过任务分解和专用子代理协同工作，有效提升了运筹学问题建模和求解的自动化水平。同时，构建了一个高质量的新数据集BWOR，对于未来LLM在OR领域的评估和研究具有重要意义，解决了现有基准的局限性。其提出的多代理协作和新数据集的方法为LLM在复杂领域应用提供了新的思路。"}}
{"id": "2506.03654", "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection", "authors": ["Xiaochun Lei", "Siqi Wu", "Weilin Wu", "Zetao Jiang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper is under consideration at Image and Vision Computing", "url": "http://arxiv.org/abs/2506.03654v3", "summary": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX.", "comment": "This paper is under consideration at Image and Vision Computing", "pdf_url": "http://arxiv.org/pdf/2506.03654v3", "cate": "cs.CV", "date": "2025-06-04", "updated": "2025-07-24", "AI": {"title_translation": "MambaNeXt-YOLO：一种用于实时目标检测的混合状态空间模型", "tldr": "MambaNeXt-YOLO是一种新型目标检测框架，它结合了CNN和Mamba状态空间模型，通过MambaNeXt Block和多分支不对称融合金字塔网络（MAFPN）实现高效的实时目标检测，尤其适用于边缘设备，在PASCAL VOC数据集上表现良好。", "motivation": "实时目标检测在计算资源有限的情况下是一项基础但具有挑战性的任务。YOLO系列模型在速度和精度之间取得了平衡，但对更丰富全局上下文建模的需求推动了基于Transformer的架构。然而，Transformer由于自注意力机制计算复杂度高，限制了其在实时和边缘部署中的实用性。", "method": "我们提出了MambaNeXt-YOLO，它结合了线性状态空间模型（如Mamba）的优势，通过以下三点平衡精度和效率：1) MambaNeXt Block：一种混合设计，将CNN与Mamba集成，有效捕获局部特征和长程依赖。2) 多分支不对称融合金字塔网络（MAFPN）：一种增强的特征金字塔架构，改进了不同对象尺寸的多尺度目标检测。3) 边缘聚焦效率：该方法无需预训练即可在PASCAL VOC数据集上实现66.6% mAP和31.9 FPS，并支持在NVIDIA Jetson Xavier NX和Orin NX等边缘设备上部署。", "result": "MambaNeXt-YOLO在PASCAL VOC数据集上实现了66.6%的mAP和31.9 FPS的速度，且无需任何预训练。该方法支持在NVIDIA Jetson Xavier NX和Orin NX等边缘设备上部署。", "conclusion": "MambaNeXt-YOLO通过融合CNN和Mamba，并结合改进的特征金字塔网络，成功解决了实时目标检测在计算资源受限环境下的挑战，在保证精度的同时显著提升了效率，尤其适用于边缘设备部署。", "translation": "实时目标检测是计算机视觉中一项基础但具有挑战性的任务，尤其是在计算资源有限的情况下。尽管YOLO系列模型通过平衡速度和精度设定了强大的基准，但对更丰富全局上下文建模日益增长的需求导致了基于Transformer的架构的使用。然而，Transformer由于其自注意力机制而具有高计算复杂度，这限制了它们在实时和边缘部署中的实用性。为了克服这些挑战，线性状态空间模型（如Mamba）的最新发展提供了一个有前景的替代方案，通过线性复杂度实现高效的序列建模。基于这一洞察，我们提出了MambaNeXt-YOLO，一个新颖的目标检测框架，通过三个关键贡献平衡了精度和效率：(1) MambaNeXt Block：一种混合设计，将CNN与Mamba集成，以有效捕获局部特征和长程依赖；(2) 多分支不对称融合金字塔网络（MAFPN）：一种增强的特征金字塔架构，改进了各种对象尺寸的多尺度目标检测；以及(3) 边缘聚焦效率：我们的方法在PASCAL VOC数据集上实现了66.6% mAP和31.9 FPS，无需任何预训练，并支持在NVIDIA Jetson Xavier NX和Orin NX等边缘设备上部署。", "summary": "本研究提出MambaNeXt-YOLO，一个针对实时目标检测的新型框架，旨在解决传统Transformer模型在边缘设备上部署时计算复杂度高的问题。该模型通过融合CNN与Mamba状态空间模型，设计了MambaNeXt Block以捕获局部和长程依赖，并引入了多分支不对称融合金字塔网络（MAFPN）以提升多尺度检测能力。实验结果显示，MambaNeXt-YOLO在PASCAL VOC数据集上无需预训练即可达到66.6% mAP和31.9 FPS，并能高效部署于边缘设备，平衡了精度与效率。", "keywords": "实时目标检测, 状态空间模型, Mamba, YOLO, 边缘计算", "comments": "这篇论文的创新点在于将线性状态空间模型Mamba与传统的CNN和YOLO框架结合，形成了一个混合模型MambaNeXt-YOLO。这种结合有效解决了Transformer在高计算复杂度和边缘部署方面的局限性，同时保留了捕获长程依赖的能力。其提出的MambaNeXt Block和MAFPN是关键的技术贡献，使得模型在保持高效率的同时，在目标检测精度上取得了不错的表现，尤其是在对资源敏感的边缘设备上具有很高的实用价值。"}}
{"id": "2507.18454", "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving", "authors": ["Juntao Zhao", "Jiuru Li", "Chuan Wu"], "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PL"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18454v1", "summary": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly\nalternative to GPU serving. Existing CPU-based solutions ignore workload\ndifferences between the prefill and the decode phases of LLM inference,\napplying a static per-NUMA (Non-Uniform Memory Access) node model partition and\nutilizing vendor libraries for operator-level execution, which is suboptimal.\nWe propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses\ndifferent execution plans for the prefill and decode phases and optimizes them\nseparately.\n  We evaluate Sandwich across diverse baselines and datasets on five CPU\nplatforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.\nSandwich achieves an average 2.01x throughput improvement and 90% satisfactory\ntime-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up\nto 3.40x lower requirements in single sequence serving, and significant\nimprovement in Goodput in continuous-batching serving. The GEMM kernels\ngenerated by Sandwich outperform representative vendor kernels and other\ndynamic shape solutions, achieving performance comparable to static compilers\nwith three orders of magnitude less kernel tuning costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18454v1", "cate": "cs.AR", "date": "2025-05-19", "updated": "2025-05-19", "AI": {"title_translation": "Sandwich: 分离预填充-解码编译以实现高效CPU LLM服务", "tldr": "现有的CPU LLM服务方案因忽略预填充和解码阶段的工作负载差异而效率低下。本文提出的Sandwich引擎通过为这两个阶段分别优化执行计划，显著提升了CPU LLM的吞吐量、降低了延迟并减少了资源需求。", "motivation": "现有基于CPU的LLM服务解决方案忽略了预填充和解码阶段之间工作负载的差异，采用静态的每NUMA节点模型分区并利用供应商库进行操作级执行，导致次优的性能和资源利用率。", "method": "本文提出了Sandwich，一个以硬件为中心的CPU LLM服务引擎。它为LLM推理的预填充和解码阶段使用不同的执行计划，并分别对它们进行优化。此外，Sandwich能够生成高效的GEMM内核。", "result": "Sandwich实现了平均2.01倍的吞吐量提升，90%满意的首个令牌时间（TTFT）和每输出令牌时间（TPOT）延迟。在单序列服务中，资源需求降低了高达3.40倍，并在连续批处理服务中显著提高了Goodput。Sandwich生成的GEMM内核性能优于代表性的供应商内核和其他动态形状解决方案，且内核调优成本降低了三个数量级。", "conclusion": "Sandwich通过分离和独立优化LLM推理的预填充和解码阶段，显著提高了CPU LLM服务的效率，在吞吐量、延迟和资源需求方面均优于现有解决方案，并能生成高效且低调优成本的GEMM内核。", "translation": "利用CPU服务大型语言模型（LLM）是GPU服务的一种资源友好型替代方案。现有的基于CPU的解决方案忽略了LLM推理预填充（prefill）和解码（decode）阶段之间工作负载的差异，采用静态的每NUMA（非统一内存访问）节点模型分区并利用供应商库进行操作级执行，这都是次优的。我们提出了Sandwich，一个以硬件为中心的基于CPU的LLM服务引擎，它为预填充和解码阶段使用不同的执行计划并分别优化它们。我们在包括带有AVX-2和AVX-512的x86以及带有NEON的ARM在内的五个CPU平台上，通过不同的基线和数据集评估了Sandwich。Sandwich实现了平均2.01倍的吞吐量提升，以及90%满意的首个令牌时间（TTFT）和每输出令牌时间（TPOT）延迟，在单序列服务中最高可降低3.40倍的要求，并在连续批处理服务中显著提高了Goodput。Sandwich生成的GEMM内核优于代表性的供应商内核和其他动态形状解决方案，实现了与静态编译器相当的性能，而内核调优成本降低了三个数量级。", "summary": "Sandwich是一种以硬件为中心的CPU LLM服务引擎，通过分别为预填充和解码阶段优化执行计划，解决了现有CPU LLM方案忽略两阶段工作负载差异导致的次优问题。实验结果表明，Sandwich显著提升了吞吐量、降低了延迟和资源需求，并且其生成的GEMM内核性能优于现有解决方案，同时大幅降低了内核调优成本。", "keywords": "CPU LLM serving, Prefill-Decode, Optimization, Hardware-centric, GEMM kernels", "comments": "该论文的创新之处在于识别并解决了CPU LLM服务中预填充和解码阶段的独特特性，而现有解决方案通常忽视了这一点。通过分别优化这两个阶段，Sandwich实现了显著的性能提升，使基于CPU的LLM服务成为一个更可行和高效的选择。其生成高性能GEMM内核并大幅降低调优成本的能力也是一个显著的贡献。"}}
{"id": "2506.10377", "title": "Chance and Mass Interpretations of Probabilities in Markov Decision Processes (Extended Version)", "authors": ["Yun Chen Tsai", "Kittiphon Phalakarn", "S. Akshay", "Ichiro Hasuo"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "Comments:      To appear at CONCUR'25", "url": "http://arxiv.org/abs/2506.10377v2", "summary": "Markov decision processes (MDPs) are a popular model for decision-making in\nthe presence of uncertainty. The conventional view of MDPs in verification\ntreats them as state transformers with probabilities defined over sequences of\nstates and with schedulers making random choices. An alternative view,\nespecially well-suited for modeling dynamical systems, defines MDPs as\ndistribution transformers with schedulers distributing probability masses. Our\nmain contribution is a unified semantical framework that accommodates these two\nviews and two new ones. These four semantics of MDPs arise naturally through\nidentifying different sources of randomness in an MDP (namely schedulers,\nconfigurations, and transitions) and providing different ways of interpreting\nthese probabilities (called the chance and mass interpretations). These\nsemantics are systematically unified through a mathematical construct called\nchance-mass (CM) classifier. As another main contribution, we study a\nreachability problem in each of the two new semantics, demonstrating their\nhardness and providing two algorithms for solving them.", "comment": "To appear at CONCUR'25", "pdf_url": "http://arxiv.org/pdf/2506.10377v2", "cate": "cs.FL", "date": "2025-06-12", "updated": "2025-07-24", "AI": {"title_translation": "马尔可夫决策过程（扩展版）中概率的几率和质量解释", "tldr": "本文提出了一个统一的语义框架，整合了马尔可夫决策过程（MDPs）的两种现有解释以及两种新解释，通过识别随机性来源和解释概率的方式。此外，研究了两种新语义下的可达性问题，并提供了求解算法。", "motivation": "传统的马尔可夫决策过程（MDPs）在验证中被视为状态转换器，而另一种观点（特别适合建模动态系统）将其定义为分布转换器。本文的动机是为这些现有观点以及两种新观点提供一个统一的语义框架，以更好地理解和处理MDPs中的不确定性。", "method": "本文通过识别MDP中不同的随机性来源（调度器、配置和转换）并提供不同的概率解释方式（几率解释和质量解释），自然地产生了四种MDP语义。这些语义通过一种称为几率-质量（CM）分类器的数学构造系统地统一起来。此外，研究了两种新语义中的可达性问题，并提供了两种算法来解决它们。", "result": "本文提出了一个统一的语义框架，能够容纳两种现有MDPs解释和两种新的解释，总共形成了四种MDP语义。这些语义通过几率-质量（CM）分类器得到系统统一。此外，本文研究了两种新语义下的可达性问题，证明了其难度，并提供了两种解决该问题的算法。", "conclusion": "本文成功构建了一个统一的语义框架，将马尔可夫决策过程（MDPs）的多种概率解释（包括几率和质量解释）整合在一起，并引入了两种新的语义。通过几率-质量（CM）分类器，这些语义得到了系统性的统一。此外，对新语义中可达性问题的研究及其求解算法的提供，为MDPs的理论和应用提供了新的视角和工具。", "translation": "马尔可夫决策过程（MDPs）是在不确定性下进行决策的流行模型。验证中MDPs的传统观点将其视为状态转换器，其中概率定义在状态序列上，并且调度器进行随机选择。另一种观点，特别适合建模动态系统，将MDPs定义为分布转换器，其中调度器分配概率质量。我们的主要贡献是一个统一的语义框架，它容纳了这两种观点以及两种新的观点。MDPs的这四种语义通过识别MDP中不同的随机性来源（即调度器、配置和转换）并提供不同的概率解释方式（称为几率和质量解释）自然产生。这些语义通过一种称为几率-质量（CM）分类器的数学构造系统地统一。作为另一个主要贡献，我们研究了两种新语义中的可达性问题，证明了它们的难度并提供了两种解决它们的算法。", "summary": "本文提出了一个统一的语义框架，旨在整合马尔可夫决策过程（MDPs）中概率的多种解释。该框架不仅涵盖了传统的将MDPs视为状态转换器和将MDPs视为分布转换器的观点，还引入了两种新的语义。这些语义通过识别MDP中调度器、配置和转换等随机性来源，并结合几率和质量两种概率解释方式而自然形成。通过引入几率-质量（CM）分类器，这些不同的语义得到了系统性的统一。此外，论文还深入探讨了两种新语义下的可达性问题，分析了其计算复杂性，并提出了相应的求解算法。", "keywords": "马尔可夫决策过程, 概率解释, 统一语义, 可达性问题, 几率-质量分类器", "comments": "本文的主要创新在于提出了一个统一的语义框架，有效地整合并扩展了马尔可夫决策过程（MDPs）中概率解释的多样性。通过引入几率-质量（CM）分类器，为理解和处理MDPs中的随机性提供了一个严谨且全面的数学工具。这一工作对于深化MDPs的理论基础具有重要意义，尤其是在形式验证和动态系统建模领域。对新语义中可达性问题的研究及其算法的提供，也展示了该框架的实际应用潜力。"}}
{"id": "2507.18374", "title": "Towards Effective Human-in-the-Loop Assistive AI Agents", "authors": ["Filippos Bellos", "Yayuan Li", "Cary Shu", "Ruey Day", "Jeffrey M. Siskind", "Jason J. Corso"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 2 tables", "url": "http://arxiv.org/abs/2507.18374v1", "summary": "Effective human-AI collaboration for physical task completion has significant\npotential in both everyday activities and professional domains. AI agents\nequipped with informative guidance can enhance human performance, but\nevaluating such collaboration remains challenging due to the complexity of\nhuman-in-the-loop interactions. In this work, we introduce an evaluation\nframework and a multimodal dataset of human-AI interactions designed to assess\nhow AI guidance affects procedural task performance, error reduction and\nlearning outcomes. Besides, we develop an augmented reality (AR)-equipped AI\nagent that provides interactive guidance in real-world tasks, from cooking to\nbattlefield medicine. Through human studies, we share empirical insights into\nAI-assisted human performance and demonstrate that AI-assisted collaboration\nimproves task completion.", "comment": "10 pages, 5 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.18374v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "迈向有效的人机协作辅助AI智能体", "tldr": "本文提出一个评估框架、一个多模态数据集和一个配备AR的AI智能体，用于研究人机协作，并证明AI辅助可提高任务完成度。", "motivation": "有效的人机协作在日常和专业领域都具有巨大潜力，但由于人机交互的复杂性，评估这种协作仍然面临挑战。", "method": "引入了一个评估框架和一个多模态人机交互数据集，旨在评估AI指导对任务表现、错误减少和学习成果的影响。此外，开发了一个配备增强现实（AR）的AI智能体，可在真实任务中提供交互式指导。", "result": "通过人体研究，分享了关于AI辅助人类表现的实证见解，并证明了AI辅助协作可以提高任务完成度。", "conclusion": "AI辅助协作，通过所开发的框架和AR智能体实现，能够有效提高人类任务表现和完成度。", "translation": "有效的人机协作完成物理任务在日常活动和专业领域都具有巨大潜力。配备信息指导的AI智能体可以提高人类表现，但由于人机交互的复杂性，评估这种协作仍然具有挑战性。在这项工作中，我们引入了一个评估框架和一个人机交互多模态数据集，旨在评估AI指导如何影响程序性任务表现、错误减少和学习成果。此外，我们开发了一个配备增强现实（AR）的AI智能体，它可以在从烹饪到战场医疗的真实任务中提供交互式指导。通过人体研究，我们分享了关于AI辅助人类表现的实证见解，并证明了AI辅助协作可以提高任务完成度。", "summary": "本文针对物理任务中人机协作评估的挑战，提出了一个评估框架和一个人机交互多模态数据集。同时，开发了一个配备增强现实（AR）的AI智能体，为现实世界任务提供交互式指导。通过人体研究，该工作证明了AI辅助协作能有效提高任务完成度，并提供了关于AI辅助下人类表现的实证见解。", "keywords": "人机协作, 辅助AI, 增强现实, 评估框架, 多模态数据集", "comments": "本文的创新之处在于其双重贡献：一个系统评估人机交互AI的评估框架和多模态数据集，以及一个用于现实世界任务的实用AR辅助AI智能体。这项工作对于推进有效且可衡量的人机协作，特别是在复杂物理领域，具有重要意义。"}}
{"id": "2507.18540", "title": "Deep Variational Free Energy Calculation of Hydrogen Hugoniot", "authors": ["Zihang Li", "Hao Xie", "Xinyang Dong", "Lei Wang"], "categories": ["cond-mat.str-el", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Strongly Correlated Electrons (cond-mat.str-el)", "pdf_link": null, "comments": "Comments:      7+17 pages, 5+14 figures, for source code and raw data, see this https URL", "url": "http://arxiv.org/abs/2507.18540v1", "summary": "We develop a deep variational free energy framework to compute the equation\nof state of hydrogen in the warm dense matter region. This method parameterizes\nthe variational density matrix of hydrogen nuclei and electrons at finite\ntemperature using three deep generative models: a normalizing flow model that\nrepresents the Boltzmann distribution of the classical nuclei, an\nautoregressive transformer that models the distribution of electrons in excited\nstates, and a permutational equivariant flow model that constructs backflow\ncoordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the\nthree neural networks to minimize the variational free energy, we obtain the\nequation of state and related thermodynamic properties of dense hydrogen. We\ncompare our results with other theoretical and experimental results on the\ndeuterium Hugoniot curve, aiming to resolve existing discrepancies. The\ncalculated results provide a valuable benchmark for deuterium in the warm dense\nmatter region.", "comment": "7+17 pages, 5+14 figures, for source code and raw data, see\n  https://github.com/fermiflow/Hugoniot", "pdf_url": "http://arxiv.org/pdf/2507.18540v1", "cate": "cond-mat.str-el", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "氢Hugoniot深度变分自由能计算", "tldr": "该研究开发了一种深度变分自由能框架，利用三种深度生成模型计算暖稠密物质区氢的物态方程，并与现有理论和实验结果进行比较，旨在解决分歧并提供基准。", "motivation": "在暖稠密物质区域计算氢的物态方程，并旨在解决氘Hugoniot曲线上现有理论和实验结果之间的差异。", "method": "开发了一种深度变分自由能框架。该方法利用三种深度生成模型对有限温度下氢原子核和电子的变分密度矩阵进行参数化：一个用于表示经典原子核玻尔兹曼分布的归一化流模型，一个用于模拟激发态电子分布的自回归Transformer模型，以及一个用于构建Hartree-Fock轨道中电子回流坐标的置换等变流模型。通过联合优化这三个神经网络以最小化变分自由能。", "result": "获得了致密氢的物态方程和相关的热力学性质。计算结果与氘Hugoniot曲线上的其他理论和实验结果进行了比较。", "conclusion": "计算结果为暖稠密物质区域的氘提供了有价值的基准。", "translation": "我们开发了一个深度变分自由能框架，以计算暖稠密物质区域中氢的物态方程。该方法利用三种深度生成模型在有限温度下对氢原子核和电子的变分密度矩阵进行参数化：一个表示经典原子核玻尔兹曼分布的归一化流模型，一个模拟激发态电子分布的自回归Transformer模型，以及一个构建Hartree-Fock轨道中电子回流坐标的置换等变流模型。通过联合优化这三个神经网络以最小化变分自由能，我们获得了致密氢的物态方程和相关的热力学性质。我们将我们的结果与氘Hugoniot曲线上的其他理论和实验结果进行了比较，旨在解决现有分歧。计算结果为暖稠密物质区域的氘提供了有价值的基准。", "summary": "本研究提出了一种深度变分自由能框架，用于计算暖稠密物质区域中氢的物态方程。该方法通过结合归一化流模型、自回归Transformer和置换等变流模型这三种深度生成模型，参数化了氢原子核和电子的变分密度矩阵。通过联合优化这些神经网络以最小化变分自由能，成功获得了致密氢的物态方程和热力学性质，并与氘Hugoniot曲线上的现有理论和实验数据进行了比较，旨在解决当前的分歧，并为氘在该区域提供宝贵的基准。", "keywords": "深度变分自由能, 氢Hugoniot, 暖稠密物质, 物态方程, 深度生成模型", "comments": "这项工作创新性地将深度生成模型应用于变分自由能计算，以解决暖稠密物质中氢的物态方程问题。通过结合多种神经网络架构来处理不同粒子（原子核和电子）及其状态（经典、激发态、Hartree-Fock轨道），该方法展现了其处理复杂量子多体问题的潜力。其结果不仅为解决现有实验和理论差异提供了新途径，也为相关领域提供了重要的基准数据。"}}
{"id": "2506.06021", "title": "Unisoma: A Unified Transformer-based Solver for Multi-Solid Systems", "authors": ["Shilong Tao", "Zhe Feng", "Haonan Sun", "Zhanxing Zhu", "Yunhuai Liu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Proceedings of the 42nd International Conference on Machine Learning", "url": "http://arxiv.org/abs/2506.06021v2", "summary": "Multi-solid systems are foundational to a wide range of real-world\napplications, yet modeling their complex interactions remains challenging.\nExisting deep learning methods predominantly rely on implicit modeling, where\nthe factors influencing solid deformation are not explicitly represented but\nare instead indirectly learned. However, as the number of solids increases,\nthese methods struggle to accurately capture intricate physical interactions.\nIn this paper, we introduce a novel explicit modeling paradigm that\nincorporates factors influencing solid deformation through structured modules.\nSpecifically, we present Unisoma, a unified and flexible Transformer-based\nmodel capable of handling variable numbers of solids. Unisoma directly captures\nphysical interactions using contact modules and adaptive interaction allocation\nmechanism, and learns the deformation through a triplet relationship. Compared\nto implicit modeling techniques, explicit modeling is more well-suited for\nmulti-solid systems with diverse coupling patterns, as it enables detailed\ntreatment of each solid while preventing information blending and confusion.\nExperimentally, Unisoma achieves consistent state-of-the-art performance across\nseven well-established datasets and two complex multi-solid tasks. Code is\navaiable at https://github.com/therontau0054/Unisoma.", "comment": "Proceedings of the 42nd International Conference on Machine Learning", "pdf_url": "http://arxiv.org/pdf/2506.06021v2", "cate": "cs.LG", "date": "2025-06-06", "updated": "2025-07-24", "AI": {"title_translation": "Unisoma：一个统一的基于Transformer的多固体系统求解器", "tldr": "提出Unisoma，一个基于Transformer的统一显式建模方法，用于解决多固体系统建模中现有隐式方法在处理复杂交互和大量固体时的不足，并在多项任务上达到SOTA性能。", "motivation": "现有深度学习方法在多固体系统建模中主要依赖隐式建模，但随着固体数量增加，这些方法难以准确捕捉复杂的物理相互作用，导致信息混合和混淆。", "method": "提出Unisoma，一个新颖的显式建模范式，它是一个统一且灵活的基于Transformer的模型，能够处理可变数量的固体。Unisoma通过接触模块和自适应交互分配机制直接捕获物理相互作用，并通过三元组关系学习变形。", "result": "Unisoma在七个既定数据集和两个复杂多固体任务上实现了持续的最先进性能。", "conclusion": "显式建模比隐式建模更适合具有不同耦合模式的多固体系统，因为它能对每个固体进行详细处理并防止信息混合和混淆，Unisoma证明了其有效性。", "translation": "多固体系统是广泛现实世界应用的基础，但建模其复杂的相互作用仍然具有挑战性。现有的深度学习方法主要依赖于隐式建模，其中影响固体变形的因素没有被显式表示，而是间接学习。然而，随着固体数量的增加，这些方法难以准确捕捉复杂的物理相互作用。在本文中，我们引入了一种新颖的显式建模范式，通过结构化模块整合了影响固体变形的因素。具体来说，我们提出了Unisoma，一个统一且灵活的基于Transformer的模型，能够处理可变数量的固体。Unisoma使用接触模块和自适应交互分配机制直接捕获物理相互作用，并通过三元组关系学习变形。与隐式建模技术相比，显式建模更适合具有不同耦合模式的多固体系统，因为它能够对每个固体进行详细处理，同时防止信息混合和混淆。实验表明，Unisoma在七个成熟的数据集和两个复杂的多固体任务上实现了持续的最先进性能。代码可在https://github.com/therontau0054/Unisoma获取。", "summary": "本文提出Unisoma，一种新颖的基于Transformer的统一显式建模方法，用于解决多固体系统中的复杂交互建模问题。针对现有隐式建模方法在处理大量固体时信息混合和精度下降的局限性，Unisoma通过结构化模块显式表示变形因素，并利用接触模块和自适应交互分配机制直接捕获物理交互，通过三元组关系学习变形。实验证明，Unisoma在多个数据集和任务上均达到最先进的性能。", "keywords": "多固体系统, 显式建模, Transformer, 物理模拟, 深度学习", "comments": "该论文创新性地提出了显式建模范式来解决多固体系统中的复杂交互问题，这与现有主流的隐式建模方法形成对比。其基于Transformer的统一模型设计，以及对接触和交互的精细处理，有望提升物理模拟的准确性和可解释性，对于需要高精度多固体系统建模的领域具有重要意义。"}}
{"id": "2501.17349", "title": "An Efficient Numerical Function Optimization Framework for Constrained Nonlinear Robotic Problems", "authors": ["Sait Sovukluk", "Christian Ott"], "categories": ["cs.RO", "math.OC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      \\c{opyright} 2025 the authors. This work has been accepted to IFAC for publication under a Creative Commons Licence CC-BY-NC-ND. - Implementation: this https URL", "url": "http://arxiv.org/abs/2501.17349v3", "summary": "This paper presents a numerical function optimization framework designed for\nconstrained optimization problems in robotics. The tool is designed with\nreal-time considerations and is suitable for online trajectory and control\ninput optimization problems. The proposed framework does not require any\nanalytical representation of the problem and works with constrained block-box\noptimization functions. The method combines first-order gradient-based line\nsearch algorithms with constraint prioritization through nullspace projections\nonto constraint Jacobian space. The tool is implemented in C++ and provided\nonline for community use, along with some numerical and robotic example\nimplementations presented in this paper.", "comment": "\\c{opyright} 2025 the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND. -\n  Implementation: https://github.com/ssovukluk/ENFORCpp", "pdf_url": "http://arxiv.org/pdf/2501.17349v3", "cate": "cs.RO", "date": "2025-01-28", "updated": "2025-07-24", "AI": {"title_translation": "一种用于约束非线性机器人问题的有效数值函数优化框架", "tldr": "本文提出了一种高效的数值函数优化框架，专为机器人领域的实时约束优化问题设计，结合了梯度下降和零空间投影，可处理黑盒函数，并已开源。", "motivation": "为解决机器人领域中的约束优化问题，特别是需要实时在线轨迹和控制输入优化的场景，该框架旨在提供一个高效的数值函数优化工具。", "method": "该框架结合了基于一阶梯度的线搜索算法和通过零空间投影到约束雅可比空间进行约束优先级排序的方法。它不需要问题的任何解析表示，并且能够处理受约束的黑盒优化函数。", "result": "该工具已用C++实现并在线提供给社区使用。论文中还展示了一些数值和机器人示例实现。", "conclusion": "该框架提供了一个高效且实用的解决方案，用于解决机器人领域中的实时约束非线性约束优化问题，并已开源供社区使用。", "translation": "本文提出了一种用于机器人领域约束优化问题的数值函数优化框架。该工具在设计时考虑了实时性，适用于在线轨迹和控制输入优化问题。所提出的框架不需要问题的任何解析表示，并可处理受约束的黑盒优化函数。该方法将一阶梯度线搜索算法与通过零空间投影到约束雅可比空间进行约束优先级排序相结合。该工具已用C++实现并在线提供给社区使用，同时本文还提供了一些数值和机器人示例实现。", "summary": "本文介绍了一种高效的数值函数优化框架，专门针对机器人领域的约束非线性问题设计。该框架注重实时性，适用于在线轨迹和控制输入优化，无需问题的解析表示，能够处理黑盒优化函数。它结合了基于一阶梯度的线搜索算法和通过零空间投影实现的约束优先级排序。该工具已用C++实现并开源，并提供了数值和机器人应用示例。", "keywords": "数值优化, 机器人, 约束优化, 实时, 黑盒函数", "comments": "该框架的创新之处在于其能够处理黑盒约束优化问题，并且在设计上考虑了实时性，这对于机器人在线控制和轨迹优化至关重要。结合梯度线搜索和零空间投影的方法使其在处理复杂约束方面具有优势。开源实现进一步提升了其对社区的价值和实用性。"}}
{"id": "2507.17868", "title": "Safe Reinforcement Learning-based Automatic Generation Control", "authors": ["Amr S. Mohamed", "Emily Nguyen", "Deepa Kundur"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      5 pages, conference: IEEE Power and Energy Systems General Meeting 2025", "url": "http://arxiv.org/abs/2507.17868v1", "summary": "Amidst the growing demand for implementing advanced control and\ndecision-making algorithms|to enhance the reliability, resilience, and\nstability of power systems|arises a crucial concern regarding the safety of\nemploying machine learning techniques. While these methods can be applied to\nderive more optimal control decisions, they often lack safety assurances. This\npaper proposes a framework based on control barrier functions to facilitate\nsafe learning and deployment of reinforcement learning agents for power system\ncontrol applications, specifically in the context of automatic generation\ncontrol. We develop the safety barriers and reinforcement learning framework\nnecessary to establish trust in reinforcement learning as a safe option for\nautomatic generation control - as foundation for future detailed verification\nand application studies.", "comment": "5 pages, conference: IEEE Power and Energy Systems General Meeting\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.17868v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于安全强化学习的自动发电控制", "tldr": "本文提出了一种基于控制障碍函数的框架，旨在实现电力系统自动发电控制中强化学习代理的安全学习和部署。", "motivation": "在电力系统控制中应用机器学习技术以提高可靠性、韧性和稳定性时，存在缺乏安全保证的担忧。", "method": "本文提出了一种基于控制障碍函数（CBF）的框架，用于促进强化学习代理在电力系统控制应用（特别是自动发电控制）中的安全学习和部署。该研究开发了建立对强化学习作为自动发电控制安全选项信任所需的安全障碍和强化学习框架。", "result": "Not mentioned in abstract", "conclusion": "该研究为未来详细的验证和应用研究奠定了基础，旨在建立对强化学习作为自动发电控制安全选项的信任。", "translation": "在实施先进控制和决策算法以增强电力系统可靠性、韧性和稳定性的需求日益增长的同时，对采用机器学习技术的安全性也产生了关键担忧。尽管这些方法可以用于得出更优的控制决策，但它们往往缺乏安全保证。本文提出了一种基于控制障碍函数的框架，旨在促进强化学习代理在电力系统控制应用（特别是自动发电控制）中的安全学习和部署。我们开发了建立对强化学习作为自动发电控制安全选项信任所需的安全障碍和强化学习框架——作为未来详细验证和应用研究的基础。", "summary": "本文针对在电力系统控制中应用机器学习技术时缺乏安全保障的问题，提出了一种基于控制障碍函数的框架。该框架旨在确保强化学习代理在自动发电控制应用中的安全学习和部署，并为未来深入研究奠定基础。", "keywords": "强化学习, 自动发电控制, 安全, 控制障碍函数, 电力系统", "comments": "本文的创新点在于将控制障碍函数引入强化学习，以解决电力系统控制中机器学习算法的安全性问题。这对于在关键基础设施中推广人工智能技术具有重要意义，因为它直接解决了实际应用中的核心顾虑。虽然目前仅是框架的提出，但其为未来强化学习在电力系统安全应用提供了一个坚实的基础。"}}
{"id": "2507.18628", "title": "Design and fabrication of ultrasound linear array transducer used in ultrasound endoscope", "authors": ["Yuan Zhang", "Mingtong Chen", "Zhengbao Yang"], "categories": ["physics.med-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18628v1", "summary": "This report details the successful construction of an ultrasound imaging\nplatform and the design and fabrication of a novel ultrasound endoscope probe.\nThe projects primary objective was to establish a functional system for\nacquiring and processing ultrasound signals, specifically targeting minimally\ninvasive endoscopic applications. The ultrasound imaging platform was primarily\ndesigned and developed based on Texas Instruments (TI) Evaluation Modules\n(EVMs). It enables the transmission of 32-channel high-voltage signals and the\nreception of echo signals, with on-chip signal amplification and acquisition\ncapabilities. Furthermore, the platform integrates a complete Time Gain Control\n(TGC) imaging path and a ContinuousWave Doppler (CWD) path. In conjunction with\nhost computer software, it supports imaging with linear array, convex array,\nand phased array probes. Concurrently, a 64-element, 5MHz center frequency,\nphased array linear ultrasound endoscopic probe was designed, aiming for\nminiaturization and optimal imaging performance. The fabrication and assembly\nof its matching layer, backing layer, 2-2 piezoelectric composite material, and\nelectrodes were completed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18628v1", "cate": "physics.med-ph", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "超声内窥镜用超声线性阵列换能器的设计与制造", "tldr": "该报告详细介绍了超声成像平台和新型超声内窥镜探头的设计与制造，旨在建立一个用于微创内窥镜应用的超声信号获取和处理功能系统。", "motivation": "该项目的主要目标是建立一个用于获取和处理超声信号的功能系统，专门针对微创内窥镜应用。", "method": "该研究设计并开发了一个基于德州仪器（TI）评估模块（EVMs）的超声成像平台，该平台能传输32通道高压信号并接收回波信号，集成了时变增益控制（TGC）和连续波多普勒（CWD）路径，并支持多种探头成像。同时，设计并完成了64单元、5MHz中心频率的相控阵线性超声内窥镜探头的制造和组装，包括其匹配层、背衬层、2-2压电复合材料和电极。", "result": "成功构建了一个超声成像平台，该平台能够传输和接收32通道信号，并支持线性阵列、凸阵列和相控阵探头的成像。成功设计、制造并组装了一个64单元、5MHz的微型相控阵线性超声内窥镜探头。", "conclusion": "该研究成功构建了用于获取和处理超声信号的超声成像平台，并设计制造了新型超声内窥镜探头，实现了用于微创内窥镜应用的完整功能系统。", "translation": "这份报告详细介绍了超声成像平台的成功构建以及新型超声内窥镜探头的设计与制造。该项目的主要目标是建立一个用于获取和处理超声信号的功能系统，专门针对微创内窥镜应用。超声成像平台主要基于德州仪器（TI）评估模块（EVMs）设计和开发。它能够传输32通道高压信号并接收回波信号，具有片上信号放大和采集能力。此外，该平台集成了完整的时变增益控制（TGC）成像路径和连续波多普勒（CWD）路径。结合上位机软件，它支持线性阵列、凸阵列和相控阵探头的成像。同时，设计了一款64单元、5MHz中心频率的相控阵线性超声内窥镜探头，旨在实现小型化和最佳成像性能。其匹配层、背衬层、2-2压电复合材料和电极的制造和组装已完成。", "summary": "该论文报告了超声成像平台和新型超声内窥镜探头的成功开发。该平台基于TI EVM构建，支持32通道信号传输/接收，并集成了TGC和CWD路径，可与多种探头配合使用。同时，为实现微创应用的最佳成像性能，设计并制造了一个小型化64单元、5MHz相控阵线性超声内窥镜探头，并完成了其关键组件的组装。", "keywords": "超声内窥镜, 线性阵列换能器, 成像平台, 微创, 相控阵", "comments": "该论文描述了一项实用的工程工作，旨在创建一个完整的超声内窥镜系统，涵盖从信号处理平台到微型探头。利用TI EVM表明了对快速原型设计和系统集成的关注。对探头组件（匹配层、背衬层、压电复合材料）的详细提及，显示了其设计和制造过程的严谨性。其创新之处在于将这些组件整合到一个功能性系统中，用于微创内窥镜检查。"}}
{"id": "2507.18537", "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation", "authors": ["Zhekai Chen", "Ruihang Chu", "Yukang Chen", "Shiwei Zhang", "Yujie Wei", "Yingya Zhang", "Xihui Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 Tables, 9 Figures", "url": "http://arxiv.org/abs/2507.18537v1", "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.", "comment": "10 Tables, 9 Figures", "pdf_url": "http://arxiv.org/pdf/2507.18537v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TTS-VAR：一种用于视觉自回归生成的测试时缩放框架", "tldr": "提出了TTS-VAR，一个针对视觉自回归模型在测试时进行缩放的框架，通过自适应批量大小调度、粗粒度聚类多样性搜索和细粒度重采样潜力选择，显著提升了生成质量和效率。", "motivation": "视觉生成模型在大规模内容创作中至关重要，但其扩展需要大量的训练和计算资源。测试时缩放因其资源效率和良好性能而受到关注。", "method": "提出TTS-VAR，首个针对视觉自回归（VAR）模型的通用测试时缩放框架，将生成过程建模为路径搜索问题。方法包括：1) 引入自适应递减的批量大小调度以平衡计算效率和探索能力。2) 在粗粒度尺度上，提出基于聚类的多样性搜索，通过语义特征聚类保持结构多样性。3) 在细粒度尺度上，使用基于重采样的潜力选择，根据结合多尺度生成历史的潜力分数优先选择有前景的候选。", "result": "在强大的VAR模型Infinity上，GenEval分数显著提升8.7%（从0.69到0.75）。关键发现是早期结构特征有效影响最终质量，且重采样效率随生成尺度变化。", "conclusion": "TTS-VAR框架通过创新的测试时缩放策略，显著提升了视觉自回归模型的生成质量和效率，并揭示了多尺度生成过程中关键特征和重采样策略的重要性。", "translation": "缩放视觉生成模型对于现实世界的内容创作至关重要，但需要大量的训练和计算开销。或者，测试时缩放因其资源效率和有前景的性能而受到越来越多的关注。在这项工作中，我们提出了TTS-VAR，这是第一个用于视觉自回归（VAR）模型的通用测试时缩放框架，将生成过程建模为路径搜索问题。为了动态平衡计算效率和探索能力，我们首先在整个因果生成过程中引入了自适应递减的批量大小调度。此外，受VAR分层从粗到细多尺度生成的启发，我们的框架集成了两个关键组件：（i）在粗粒度尺度上，我们观察到生成的令牌难以评估，可能导致错误接受劣质样本或拒绝优质样本。注意到粗粒度尺度包含足够的结构信息，我们提出了基于聚类的多样性搜索。它通过语义特征聚类保留了结构多样性，从而能够在更高潜力的样本上进行后续选择。（ii）在细粒度尺度上，基于重采样的潜力选择使用潜力分数优先选择有前景的候选，潜力分数被定义为结合多尺度生成历史的奖励函数。在强大的VAR模型Infinity上的实验显示GenEval分数显着提高了8.7%（从0.69到0.75）。关键见解表明，早期阶段的结构特征有效影响最终质量，并且重采样效率在不同生成尺度上有所不同。代码可在https://github.com/ali-vilab/TTS-VAR获取。", "summary": "本文提出TTS-VAR，一个针对视觉自回归（VAR）模型的通用测试时缩放框架，旨在提高生成效率和质量。该框架将生成视为路径搜索问题，并引入了自适应递减批量大小调度。此外，它在粗粒度尺度采用基于聚类的多样性搜索以保留结构多样性，在细粒度尺度则使用基于重采样的潜力选择来优先处理有前景的候选。实验结果显示，TTS-VAR在VAR模型上实现了显著的GenEval分数提升，并揭示了早期结构特征对最终质量的影响以及重采样在不同生成尺度下的差异性。", "keywords": "测试时缩放, 视觉自回归, 生成模型, 多尺度生成, 聚类多样性搜索", "comments": "TTS-VAR的创新之处在于它是首个为视觉自回归模型设计的通用测试时缩放框架，通过将生成过程视为路径搜索问题并引入多尺度优化策略，有效平衡了计算效率和生成质量。其分层的粗-细粒度处理（聚类多样性搜索和重采样潜力选择）是其核心贡献，特别是在不增加训练成本的情况下提升模型性能，这对于实际应用具有重要意义。"}}
{"id": "2503.14501", "title": "Advances in 4D Generation: A Survey", "authors": ["Qiaowei Miao", "Kehan Li", "Jinsheng Quan", "Zhiyuan Min", "Shaojie Ma", "Yichao Xu", "Yi Yang", "Ping Liu", "Yawei Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.14501v3", "summary": "Generative artificial intelligence has recently progressed from static image\nand video synthesis to 3D content generation, culminating in the emergence of\n4D generation-the task of synthesizing temporally coherent dynamic 3D assets\nguided by user input. As a burgeoning research frontier, 4D generation enables\nricher interactive and immersive experiences, with applications ranging from\ndigital humans to autonomous driving. Despite rapid progress, the field lacks a\nunified understanding of 4D representations, generative frameworks, basic\nparadigms, and the core technical challenges it faces. This survey provides a\nsystematic and in-depth review of the 4D generation landscape. To\ncomprehensively characterize 4D generation, we first categorize fundamental 4D\nrepresentations and outline associated techniques for 4D generation. We then\npresent an in-depth analysis of representative generative pipelines based on\nconditions and representation methods. Subsequently, we discuss how motion and\ngeometry priors are integrated into 4D outputs to ensure spatio-temporal\nconsistency under various control schemes. From an application perspective,\nthis paper summarizes 4D generation tasks in areas such as dynamic object/scene\ngeneration, digital human synthesis, editable 4D content, and embodied AI.\nFurthermore, we summarize and multi-dimensionally compare four basic paradigms\nfor 4D generation: End-to-End, Generated-Data-Based,\nImplicit-Distillation-Based, and Explicit-Supervision-Based. Concluding our\nanalysis, we highlight five key challenges-consistency, controllability,\ndiversity, efficiency, and fidelity-and contextualize these with current\napproaches.By distilling recent advances and outlining open problems, this work\noffers a comprehensive and forward-looking perspective to guide future research\nin 4D generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.14501v3", "cate": "cs.CV", "date": "2025-03-18", "updated": "2025-07-24", "AI": {"title_translation": "4D生成领域的进展：一项综述", "tldr": "这篇综述系统回顾了4D生成领域，分类了4D表示和生成框架，分析了生成流程和运动几何先验，总结了应用任务和基本范式，并提出了关键挑战以指导未来研究。", "motivation": "4D生成作为新兴研究前沿，缺乏对4D表示、生成框架、基本范式以及核心技术挑战的统一理解。因此，本研究旨在提供一个系统深入的综述。", "method": "本综述首先对基本4D表示进行分类并概述相关技术；接着深入分析了基于条件和表示方法的代表性生成流程；随后讨论了运动和几何先验如何整合到4D输出中以确保时空一致性；从应用角度总结了4D生成任务；此外，总结并多维度比较了四种基本范式：端到端、基于生成数据、基于隐式蒸馏和基于显式监督；最后，强调了五个关键挑战（一致性、可控性、多样性、效率和保真度）并将其与现有方法联系起来。", "result": "本综述系统且深入地回顾了4D生成领域，全面刻画了4D生成，包括基本4D表示、相关技术、代表性生成流程、运动和几何先验的整合、4D生成任务的应用、四种基本范式（端到端、基于生成数据、基于隐式蒸馏、基于显式监督），并总结了五个关键挑战。", "conclusion": "通过提炼最新进展和概述开放问题，这项工作为指导未来4D生成研究提供了全面且前瞻性的视角。", "translation": "生成式人工智能最近已从静态图像和视频合成发展到3D内容生成，最终催生了4D生成——即在用户输入指导下合成时间连贯的动态3D资产的任务。作为一个新兴的研究前沿，4D生成能够实现更丰富的交互和沉浸式体验，应用范围从数字人到自动驾驶。尽管进展迅速，但该领域缺乏对4D表示、生成框架、基本范式以及其面临的核心技术挑战的统一理解。本综述对4D生成领域进行了系统而深入的回顾。为了全面刻画4D生成，我们首先对基本的4D表示进行分类，并概述了相关的4D生成技术。然后，我们深入分析了基于条件和表示方法的代表性生成流程。随后，我们讨论了运动和几何先验如何整合到4D输出中，以确保在各种控制方案下的时空一致性。从应用角度来看，本文总结了动态对象/场景生成、数字人合成、可编辑4D内容和具身AI等领域的4D生成任务。此外，我们总结并多维度比较了四种基本的4D生成范式：端到端、基于生成数据、基于隐式蒸馏和基于显式监督。最后，我们总结了分析，并强调了五个关键挑战——一致性、可控性、多样性、效率和保真度——并结合当前方法对这些挑战进行了背景化。通过提炼最新进展和概述开放问题，这项工作为指导未来4D生成研究提供了全面且前瞻性的视角。", "summary": "本文对新兴的4D生成领域进行了全面而深入的综述。鉴于该领域缺乏统一理解，作者系统地分类了4D表示、分析了生成流程、探讨了运动几何先验的整合、总结了应用任务，并比较了四种基本生成范式。此外，论文还提出了未来研究面临的五个关键挑战（一致性、可控性、多样性、效率、保真度），旨在为4D生成领域的未来发展提供指导和前瞻性视角。", "keywords": "4D生成, 综述, 动态3D资产, 生成式AI, 时空一致性", "comments": "这篇综述性论文的重要性在于其对4D生成这一新兴领域的系统性梳理。它不仅统一了该领域的基本概念和技术，还对现有方法进行了分类和比较，并明确指出了未来的研究方向和挑战。这对于研究人员理解该领域现状、寻找突破口具有重要的指导意义。其创新之处在于首次提供了如此全面的4D生成概览，填补了该领域缺乏统一理解的空白。"}}
{"id": "2507.17765", "title": "ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding", "authors": ["Arindam Ghosh", "Mark Fuhs", "Bongjun Kim", "Anurag Chowdhury", "Monika Woszczyna"], "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Interspeech 2025 Submission", "url": "http://arxiv.org/abs/2507.17765v1", "summary": "From an application standpoint, speaker-role diarization (RD), such as doctor\nvs. patient, host vs. guest, etc. is often more useful than traditional speaker\ndiarization (SD), which assigns generic labels like speaker-1, speaker-2 etc.\nIn the context of joint automatic speech recognition (ASR) + SD (who spoke\nwhat?), recent end-to-end models employ an auxiliary SD transducer,\nsynchronized with the ASR transducer, to predict speakers per word. In this\npaper, we extend this framework to RD with three key contributions: (1) we\nsimplify the training via forced alignment and cross-entropy loss instead of\nRNNT loss, (2) we show that word prediction and role prediction require\ndifferent amounts of predictor's context, leading to separate task-specific\npredictors, unlike existing shared-predictor models, and (3) we propose a way\nto leverage RD posterior activity to influence ASR decoding and reduce\nsmall-word deletion errors.", "comment": "Interspeech 2025 Submission", "pdf_url": "http://arxiv.org/pdf/2507.17765v1", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-14", "AI": {"title_translation": "ASR引导的说话人角色日志和日志引导的ASR解码", "tldr": "本文将联合ASR+说话人日志系统扩展到说话人角色日志（RD），通过简化训练、使用独立预测器并利用RD信息改进ASR解码。", "motivation": "传统的说话人日志（SD）只分配通用标签（如speaker-1），而说话人角色日志（RD，如医生vs病人）在应用中更有用。现有联合ASR+SD模型需要扩展到RD。", "method": "本文将现有联合ASR+SD框架扩展到RD，具体方法包括：1) 通过强制对齐和交叉熵损失简化训练，代替RNNT损失；2) 发现词语和角色预测需要不同的预测器上下文，因此使用独立的任务特定预测器；3) 提出利用RD后验活动来影响ASR解码，以减少小词删除错误。", "result": "结果体现在三个贡献中：简化了训练；发现了词语和角色预测需要不同上下文并采用了独立预测器；通过RD后验活动减少了ASR解码中的小词删除错误。具体的量化结果未在摘要中提及。", "conclusion": "本文成功地将联合ASR+说话人日志框架扩展到说话人角色日志，并通过多项创新，特别是利用角色日志信息反向优化ASR解码，提升了系统的性能和实用性。", "translation": "从应用角度来看，说话人角色日志（RD），例如医生与病人、主持人与嘉宾等，通常比传统的说话人日志（SD）更有用，后者分配通用标签如说话人1、说话人2等。在联合自动语音识别（ASR）+SD（谁说了什么？）的背景下，最近的端到端模型采用一个辅助的SD传感器，与ASR传感器同步，以预测每个词的说话人。在本文中，我们将这个框架扩展到RD，并有三个主要贡献：（1）我们通过强制对齐和交叉熵损失而不是RNNT损失来简化训练；（2）我们展示了词语预测和角色预测需要不同数量的预测器上下文，从而导致独立的任务特定预测器，这与现有共享预测器模型不同；（3）我们提出了一种利用RD后验活动影响ASR解码并减少小词删除错误的方法。", "summary": "本文针对自动语音识别(ASR)与说话人日志(SD)的联合系统，提出了一种将说话人角色日志(RD)整合进该框架的新方法。研究者通过引入强制对齐和交叉熵损失简化了训练过程，并针对词语和角色预测的不同上下文需求，设计了独立的任务特定预测器。此外，论文还提出利用RD的后验信息来优化ASR解码，有效减少了小词删除错误，提升了系统在实际应用中的实用性。", "keywords": "说话人角色日志, 自动语音识别, 语音识别解码, 联合建模, 强制对齐", "comments": "这篇论文通过将说话人角色日志（RD）引入联合ASR+SD框架，提升了语音处理在实际应用中的价值。其创新点在于：1) 训练方法的简化，可能提高效率和稳定性；2) 针对不同任务（词语和角色预测）设计独立预测器，体现了对任务特性的深入理解，可能带来性能提升；3) 最重要的是，提出利用RD信息反哺ASR解码，形成一个双向优化的循环，这是一种新颖且有效的集成方式，有望显著减少ASR错误，特别是小词删除问题。"}}
{"id": "2507.18061", "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18061v1", "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18061v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TELEVAL：一个专为中文交互场景下语音语言模型设计的动态基准", "tldr": "本文提出了TELEVAL，一个动态基准，用于评估中文交互场景下语音语言模型作为对话代理的有效性，并发现现有模型在自然对话任务中仍有很大提升空间。", "motivation": "现有语音语言模型（SLM）的评估基准主要关注其执行复杂任务的能力，但未能与用户在真实对话场景中的自然交互方式对齐，因此需要一个更贴近实际用户体验的评估框架。", "method": "本文提出了TELEVAL，一个动态基准，专门用于评估SLMs在真实中文交互环境中的对话代理效果。TELEVAL定义了三个评估维度：显式语义、副语言和隐式语义、以及系统能力。它采用与真实世界使用一致的对话格式，并分别评估文本和音频输出。特别关注模型从用户语音中提取隐式线索并适当响应的能力。", "result": "实验表明，尽管最近有所进展，但现有语音语言模型在自然对话任务中仍有相当大的改进空间。", "conclusion": "TELEVAL可以作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发出更强大的面向对话的语音语言模型。", "translation": "语音语言模型（SLM）近年来发展迅速，同时涌现出许多用于评估其性能的基准。然而，大多数现有基准主要侧重于评估SLMs是否能够执行与大型语言模型（LLMs）所处理的复杂任务相当的任务，往往未能与用户在真实世界对话场景中的自然交互方式对齐。在本文中，我们提出了TELEVAL，一个专门设计用于评估SLMs在真实中文交互设置中作为对话代理有效性的动态基准。TELEVAL定义了三个评估维度：显式语义、副语言和隐式语义，以及系统能力。它采用与真实世界使用一致的对话格式，并分别评估文本和音频输出。TELEVAL特别关注模型从用户语音中提取隐式线索并无需额外指令即可适当响应的能力。我们的实验表明，尽管最近有所进展，但现有SLMs在自然对话任务中仍有相当大的改进空间。我们希望TELEVAL能够作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发出更强大的面向对话的SLMs。", "summary": "本文提出了TELEVAL，一个针对中文交互场景下语音语言模型（SLM）的动态评估基准。现有基准未能有效反映真实用户交互，而TELEVAL通过定义显式语义、副语言和隐式语义以及系统能力三个维度，并采用真实对话格式，旨在更准确地评估SLM作为对话代理的能力，尤其关注从用户语音中提取隐式线索。实验结果显示，当前SLM在自然对话任务中仍有显著提升空间，TELEVAL有望推动用户体验导向的对话型SLM发展。", "keywords": "语音语言模型, 动态基准, 中文交互, 对话代理, 评估", "comments": "本文的创新点在于提出了一个更贴近真实用户交互的动态评估基准TELEVAL，它弥补了现有SLM评估基准在真实对话场景适用性上的不足。通过关注隐式语义和副语言，该基准能够更全面地衡量SLM的对话能力，对于推动对话式AI的发展具有重要意义。其用户中心的设计理念也值得肯定。"}}
{"id": "2502.01108", "title": "Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for Wearable Applications Across Lab and Field Settings", "authors": ["Mithun Saha", "Maxwell A. Xu", "Wanting Mao", "Sameer Neupane", "James M. Rehg", "Santosh Kumar"], "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Saha and Xu are co-first authors", "url": "http://arxiv.org/abs/2502.01108v2", "summary": "Photoplethysmography (PPG)-based foundation models are gaining traction due\nto the widespread use of PPG in biosignal monitoring and their potential to\ngeneralize across diverse health applications. In this paper, we introduce\nPulse-PPG, the first open-source PPG foundation model trained exclusively on\nraw PPG data collected over a 100-day field study with 120 participants.\nExisting PPG foundation models are either open-source but trained on clinical\ndata or closed-source, limiting their applicability in real-world settings. We\nevaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its\nperformance against a state-of-the-art foundation model trained on clinical\ndata. Our results demonstrate that Pulse-PPG, trained on uncurated field data,\nexhibits superior generalization across clinical and mobile health applications\nin both lab and field settings. This suggests that exposure to real-world\nvariability enables the model to learn fine-grained representations, making it\nmore adaptable across tasks. Furthermore, pre-training on field data\nsurprisingly outperforms its pre-training on clinical data in many tasks,\nreinforcing the importance of training on real-world, diverse datasets. To\nencourage further advancements in robust foundation models leveraging field\ndata, we plan to release Pulse-PPG, providing researchers with a powerful\nresource for developing more generalizable PPG-based models.", "comment": "Saha and Xu are co-first authors", "pdf_url": "http://arxiv.org/pdf/2502.01108v2", "cate": "cs.LG", "date": "2025-02-03", "updated": "2025-07-23", "AI": {"title_translation": "Pulse-PPG：一个用于实验室和现场可穿戴应用的开源现场训练PPG基础模型", "tldr": "Pulse-PPG是一个开源的PPG基础模型，专门使用来自100天现场研究的原始PPG数据进行训练。它在实验室和现场设置中对临床和移动健康应用表现出卓越的泛化能力，表明真实世界数据的多样性对于训练更具适应性的模型至关重要。", "motivation": "现有的PPG基础模型要么是开源但基于临床数据训练，要么是闭源的，这限制了它们在真实世界应用中的普适性。该研究旨在开发一个基于真实世界现场数据训练的开源PPG基础模型，以提高其在多样化健康应用中的泛化能力。", "method": "研究引入了Pulse-PPG，这是第一个专门使用来自120名参与者100天现场研究中收集的原始PPG数据训练的开源PPG基础模型。该模型在多个数据集和下游任务上进行了评估，并与一个最先进的临床数据训练基础模型进行了性能比较。", "result": "Pulse-PPG在临床和移动健康应用中，无论是在实验室还是现场设置，都表现出卓越的泛化能力。与临床数据训练的模型相比，现场数据预训练在许多任务中表现更优。这表明暴露于真实世界的可变性使得模型能够学习更细粒度的表示，从而更具适应性。", "conclusion": "在真实世界、多样化数据集上进行训练对于开发鲁棒且泛化能力强的PPG基础模型至关重要，现场数据预训练可以显著提高模型性能。", "translation": "光电容积描记法（PPG）基础模型因PPG在生物信号监测中的广泛应用及其在各种健康应用中泛化的潜力而受到关注。本文介绍了Pulse-PPG，这是第一个完全基于120名参与者在100天现场研究中收集的原始PPG数据训练的开源PPG基础模型。现有的PPG基础模型要么是开源但基于临床数据训练，要么是闭源的，这限制了它们在真实世界环境中的适用性。我们评估了Pulse-PPG在多个数据集和下游任务上的表现，并将其性能与一个基于临床数据训练的最先进基础模型进行了比较。我们的结果表明，Pulse-PPG，一个在未经整理的现场数据上训练的模型，在实验室和现场设置的临床和移动健康应用中都表现出卓越的泛化能力。这表明暴露于真实世界的变异性使模型能够学习精细的表示，使其在不同任务中更具适应性。此外，在许多任务中，基于现场数据的预训练出人意料地优于基于临床数据的预训练，这强化了在真实世界、多样化数据集上进行训练的重要性。为了鼓励利用现场数据在鲁棒基础模型方面取得进一步进展，我们计划发布Pulse-PPG，为研究人员提供一个开发更具泛化性PPG基础模型的强大资源。", "summary": "本文介绍了Pulse-PPG，一个创新的开源PPG基础模型，其独特之处在于完全基于为期100天的真实世界现场研究中收集的原始PPG数据进行训练。与现有模型不同，Pulse-PPG旨在克服临床数据训练模型在真实世界应用中的局限性。通过在多个下游任务和数据集上的评估，研究发现Pulse-PPG在实验室和现场环境中对临床及移动健康应用展现出卓越的泛化能力，甚至在许多情况下超越了基于临床数据训练的最先进模型。这强调了利用真实世界多样化数据进行模型训练的重要性，以促进更具适应性和鲁棒性的PPG基础模型的发展。研究团队计划开源Pulse-PPG，以推动该领域的进一步研究。", "keywords": "PPG基础模型, 现场数据, 开源模型, 可穿戴应用, 泛化能力", "comments": "Pulse-PPG的创新之处在于其完全依赖于大规模、长时间的真实世界现场数据进行训练，这与传统上依赖临床数据的模型形成鲜明对比。这项工作的重要性在于它证明了现场数据的多样性和复杂性能够使模型学习到更具泛化能力的表示，从而在实际应用中表现更优。这为未来可穿戴设备和生物信号监测领域的基础模型开发提供了新的范式。其局限性可能在于现场数据的质量控制和标注难度，以及模型在特定罕见病理情况下的表现。"}}
{"id": "2507.18570", "title": "Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods", "authors": ["Ganesh Sapkota", "Md Hasibur Rahman"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18570v1", "summary": "This paper presents a novel hybrid tokenization strategy that enhances the\nperformance of DNA Language Models (DLMs) by combining 6-mer tokenization with\nByte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at\ncapturing local DNA sequence structures but often faces challenges, including\nuneven token distribution and a limited understanding of global sequence\ncontext. To address these limitations, we propose merging unique 6mer tokens\nwith optimally selected BPE tokens generated through 600 BPE cycles. This\nhybrid approach ensures a balanced and context-aware vocabulary, enabling the\nmodel to capture both short and long patterns within DNA sequences\nsimultaneously. A foundational DLM trained on this hybrid vocabulary was\nevaluated using next-k-mer prediction as a fine-tuning task, demonstrating\nsignificantly improved performance. The model achieved prediction accuracies of\n10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming\nstate-of-the-art models such as NT, DNABERT2, and GROVER. These results\nhighlight the ability of the hybrid tokenization strategy to preserve both the\nlocal sequence structure and global contextual information in DNA modeling.\nThis work underscores the importance of advanced tokenization methods in\ngenomic language modeling and lays a robust foundation for future applications\nin downstream DNA sequence analysis and biological research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18570v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DNA语言模型中结合字节对编码和K-MER方法的混合分词策略", "tldr": "本文提出一种结合6-mer和BPE的混合分词策略，显著提升DNA语言模型在下一k-mer预测任务上的性能，优于现有SOTA模型。", "motivation": "传统k-mer分词在捕捉局部DNA序列结构方面有效，但面临词元分布不均和对全局序列上下文理解有限的挑战。", "method": "本文提出一种将独特的6-mer词元与通过600次BPE循环生成的BPE词元合并的混合分词策略（BPE-600），以构建平衡且上下文感知的词汇表，使DNA语言模型能够同时捕捉短和长DNA序列模式。", "result": "在下一k-mer预测微调任务中，该模型对3-mers、4-mers和5-mers的预测准确率分别达到10.78%、10.1%和4.12%，性能显著提升，并优于NT、DNABERT2和GROVER等现有最先进模型。", "conclusion": "混合分词策略能够有效地保留DNA建模中的局部序列结构和全局上下文信息，强调了高级分词方法在基因组语言建模中的重要性，并为未来的DNA序列分析和生物研究应用奠定了基础。", "translation": "本文提出了一种新颖的混合分词策略，通过将6-mer分词与字节对编码（BPE-600）相结合，提升了DNA语言模型（DLMs）的性能。传统的k-mer分词在捕获局部DNA序列结构方面是有效的，但常面临挑战，包括词元分布不均和对全局序列上下文理解有限。为了解决这些局限性，我们建议将独特的6mer词元与通过600个BPE循环生成的优化选择的BPE词元合并。这种混合方法确保了平衡且上下文感知的词汇表，使模型能够同时捕获DNA序列中的短模式和长模式。一个基于此混合词汇表训练的基础DLM通过下一k-mer预测作为微调任务进行了评估，展示了显著的性能提升。该模型对3-mers、4-mers和5-mers的预测准确率分别达到10.78%、10.1%和4.12%，优于NT、DNABERT2和GROVER等现有最先进模型。这些结果突出了混合分词策略在DNA建模中同时保留局部序列结构和全局上下文信息的能力。这项工作强调了高级分词方法在基因组语言建模中的重要性，并为未来在下游DNA序列分析和生物研究中的应用奠定了坚实的基础。", "summary": "本文提出一种新颖的混合分词策略，结合6-mer和BPE-600来优化DNA语言模型（DLMs）的性能。该方法旨在解决传统k-mer分词在词元分布和全局上下文理解上的局限性。通过创建平衡且上下文感知的词汇表，模型能同时捕捉DNA序列中的短长模式。在下一k-mer预测任务上的评估显示，该策略显著提升了预测准确率，并超越了现有SOTA模型，证明了其在保留局部和全局DNA信息方面的有效性，为基因组语言建模提供了新的方向。", "keywords": "DNA语言模型, 混合分词, 字节对编码, K-MER, 基因组建模", "comments": "这项研究的创新点在于提出了6-mer和BPE结合的混合分词策略，有效解决了传统k-mer分词在DNA语言模型中遇到的上下文理解和词元分布不均的问题。其重要性在于显著提升了DNA语言模型在下一k-mer预测任务上的性能，为基因组序列分析和生物研究奠定了更坚实的基础。"}}
{"id": "2507.16802", "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning", "authors": ["Yanjun Zheng", "Xiyang Du", "Longfei Liao", "Xiaoke Zhao", "Zhaowen Zhou", "Jingze Song", "Bo Zhang", "Jiawei Liu", "Xiang Qi", "Zhe Li", "Zhiqiang Zhang", "Wei Wang", "Peng Zhang"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16802v3", "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16802v3", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "Agentar-Fin-R1：通过领域专业知识、训练效率和高级推理增强金融智能", "tldr": "Agentar-Fin-R1是一个新的金融大语言模型系列，通过优化训练效率、领域专业性和可信度，在金融和通用推理任务上取得了最先进的性能，并引入了新的Finova评估基准。", "motivation": "现有的大语言模型在金融应用中面临复杂推理、严格可信度要求和高效领域适应性方面的限制。", "method": "本文推出了Agentar-Fin-R1系列金融大语言模型（8B和32B参数），基于Qwen3基础模型构建，旨在增强金融应用的推理能力、可靠性和领域专业化。优化方法整合了高质量、系统性的金融任务标签系统和全面的多层可信度保障框架，包括高质量可信知识工程、多智能体可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练管道和动态归因系统，显著提高了训练效率。模型在主流金融基准（Fineva、FinEval、FinanceIQ）和通用推理数据集（MATH-500、GPQA-diamond）上进行了全面评估，并创新性地提出了Finova评估基准，专注于智能体级金融推理和合规验证。", "result": "Agentar-Fin-R1不仅在金融任务上取得了最先进的性能，而且展现出卓越的通用推理能力，验证了其作为高风险金融应用中可信解决方案的有效性。Finova基准已开源。", "conclusion": "Agentar-Fin-R1系列模型有效解决了现有LLMs在金融领域面临的挑战，通过其创新的优化方法和全面的评估，证明了其在金融智能和通用推理方面的卓越能力，是高风险金融应用中值得信赖的解决方案。", "translation": "大型语言模型（LLMs）在金融应用中展现出巨大的潜力；然而，现有模型在需要复杂推理能力、严格可信度标准和高效适应领域特定要求的场景下，经常表现出局限性。我们推出了Agentar-Fin-R1系列金融大型语言模型（8B和32B参数），该系列模型专门基于Qwen3基础模型进行工程设计，旨在增强金融应用的推理能力、可靠性和领域专业化。我们的优化方法整合了高质量、系统性的金融任务标签系统与全面的多层可信度保障框架。该框架涵盖了高质量可信知识工程、多智能体可信数据合成和严格的数据验证治理。通过标签引导的自动化难度感知优化、两阶段训练管道和动态归因系统，我们实现了训练效率的显著提升。我们的模型在包括Fineva、FinEval和FinanceIQ在内的主流金融基准以及MATH-500和GPQA-diamond等通用推理数据集上进行了全面评估。为了全面评估实际部署能力，我们创新性地提出了Finova评估基准，该基准专注于智能体级金融推理和合规性验证。实验结果表明，Agentar-Fin-R1不仅在金融任务上取得了最先进的性能，而且展现出卓越的通用推理能力，验证了其作为高风险金融应用可信解决方案的有效性。Finova基准可在https://github.com/antgroup/Finova获取。", "summary": "Agentar-Fin-R1是一个新的金融大语言模型系列（8B和32B参数），基于Qwen3构建，旨在解决现有LLMs在金融应用中推理能力、可信度和领域适应性方面的不足。该模型通过集成高质量金融任务标签系统、多层可信度保障框架以及优化训练管道（包括难度感知优化和两阶段训练），显著提高了训练效率。在主流金融基准和通用推理数据集上的评估显示，Agentar-Fin-R1在金融任务上实现了最先进的性能，并展现出卓越的通用推理能力。此外，论文还提出了用于评估智能体级金融推理和合规性的Finova基准，进一步验证了Agentar-Fin-R1作为高风险金融应用可信解决方案的有效性。", "keywords": "金融大语言模型, Agentar-Fin-R1, 可信度, 推理能力, Finova基准", "comments": "Agentar-Fin-R1的创新之处在于其对金融领域特性的深入理解和针对性优化，特别是通过引入多层可信度保障框架和标签引导的难度感知优化，有效提升了模型在复杂金融场景下的可靠性和效率。其提出的Finova评估基准也填补了智能体级金融推理和合规性评估的空白，对未来金融AI的发展具有重要意义。"}}
{"id": "2507.17924", "title": "UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction", "authors": ["Hongrong Yang", "Markus Schlaepfer"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17924v1", "summary": "Accurate population flow prediction is essential for urban planning,\ntransportation management, and public health. Yet existing methods face key\nlimitations: traditional models rely on static spatial assumptions, deep\nlearning models struggle with cross-city generalization, and Large Language\nModels (LLMs) incur high computational costs while failing to capture spatial\nstructure. Moreover, many approaches sacrifice resolution by clustering Points\nof Interest (POIs) or restricting coverage to subregions, limiting their\nutility for city-wide analytics. We introduce UrbanPulse, a scalable deep\nlearning framework that delivers ultra-fine-grained, city-wide OD flow\npredictions by treating each POI as an individual node. It combines a temporal\ngraph convolutional encoder with a transformer-based decoder to model\nmulti-scale spatiotemporal dependencies. To ensure robust generalization across\nurban contexts, UrbanPulse employs a three-stage transfer learning strategy:\npretraining on large-scale urban graphs, cold-start adaptation, and\nreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS\nrecords from three metropolitan areas in California, UrbanPulse achieves\nstate-of-the-art accuracy and scalability. Through efficient transfer learning,\nUrbanPulse takes a key step toward making high-resolution, AI-powered urban\nforecasting deployable in practice across diverse cities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17924v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "UrbanPulse：一个用于超精细粒度人口转移预测的跨城市深度学习框架", "tldr": "UrbanPulse是一个深度学习框架，通过三阶段迁移学习，实现超精细粒度的跨城市人口流动预测，在真实GPS数据上表现SOTA。", "motivation": "现有的人口流动预测方法存在局限性：传统模型依赖静态空间假设；深度学习模型难以跨城市泛化；大型语言模型（LLMs）计算成本高且未能捕捉空间结构；许多方法通过POI聚类或限制区域覆盖牺牲了分辨率，限制了其在全城分析中的实用性。", "method": "本文引入了UrbanPulse，一个可扩展的深度学习框架，通过将每个兴趣点（POI）视为一个独立的节点，提供超精细粒度的全城OD（Origin-Destination）流预测。它结合了时间图卷积编码器和基于Transformer的解码器来建模多尺度时空依赖关系。为确保在不同城市环境中的鲁棒泛化，UrbanPulse采用了三阶段迁移学习策略：在大规模城市图上进行预训练、冷启动适应和强化学习微调。", "result": "在来自加利福尼亚三个大都市区的超过1.03亿条清洗过的GPS记录上进行评估，UrbanPulse实现了最先进的准确性和可扩展性。", "conclusion": "通过高效的迁移学习，UrbanPulse在使高分辨率、AI驱动的城市预测在不同城市中实际部署方面迈出了关键一步。", "translation": "准确的人口流动预测对于城市规划、交通管理和公共健康至关重要。然而，现有方法面临着关键限制：传统模型依赖静态空间假设，深度学习模型难以实现跨城市泛化，大型语言模型（LLMs）计算成本高昂且未能捕捉空间结构。此外，许多方法通过对兴趣点（POIs）进行聚类或将覆盖范围限制在子区域来牺牲分辨率，这限制了它们在全城分析中的实用性。我们引入了UrbanPulse，一个可扩展的深度学习框架，通过将每个POI视为一个独立的节点，提供超精细粒度的全城OD（Origin-Destination）流预测。它结合了时间图卷积编码器和基于Transformer的解码器来建模多尺度时空依赖关系。为了确保在不同城市环境中的鲁棒泛化，UrbanPulse采用了三阶段迁移学习策略：在大规模城市图上进行预训练、冷启动适应和强化学习微调。在来自加利福尼亚三个大都市区的超过1.03亿条清洗过的GPS记录上进行评估，UrbanPulse实现了最先进的准确性和可扩展性。通过高效的迁移学习，UrbanPulse在使高分辨率、AI驱动的城市预测在不同城市中实际部署方面迈出了关键一步。", "summary": "UrbanPulse是一个创新的深度学习框架，旨在解决现有方法在人口流动预测中的局限性，特别是跨城市泛化能力差和分辨率不足的问题。它通过将每个POI视为一个节点，实现了超精细粒度的全城OD流预测，并结合了时间图卷积编码器和Transformer解码器来捕捉时空依赖。为了增强跨城市泛化能力，UrbanPulse采用了独特的三阶段迁移学习策略。在真实大规模GPS数据上的评估表明，UrbanPulse达到了最先进的准确性和可扩展性，为高分辨率城市预测的实际部署奠定了基础。", "keywords": "人口流动预测, 深度学习, 迁移学习, 城市规划, 时空预测", "comments": "这篇论文的创新点在于提出了一个结合图卷积和Transformer的深度学习框架UrbanPulse，并引入了三阶段迁移学习策略，显著提升了人口流动预测的粒度（POI级别）和跨城市泛化能力。其重要性在于解决了现有模型在实际城市规划和管理中面临的关键挑战，特别是高分辨率和跨城市部署的需求。"}}
{"id": "2507.18183", "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory", "authors": ["Jianchao Wang", "Qingfeng Li", "Pengcheng Zheng", "Xiaorong Pu", "Yazhou Ren"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18183v1", "summary": "Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18183v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ChronoSelect：通过动态时间记忆实现噪声标签下的鲁棒学习", "tldr": "ChronoSelect 提出一种新颖的四阶段记忆架构，利用学习过程的丰富时间动态来识别和处理噪声标签，从而在噪声环境下实现鲁棒学习。", "motivation": "在真实世界数据上训练深度神经网络时，噪声标签会严重影响模型的泛化性能，因为过参数化的模型会记忆这些噪声。现有处理噪声标签的方法（LNL）未能利用学习演化的丰富时间动态，而是依赖于静态快照评估。", "method": "ChronoSelect 是一种新颖的框架，具有创新的四阶段记忆架构，可将预测历史压缩为紧凑的时间分布。它采用独特的带控制衰减的滑动更新机制，每个样本只维护四个动态记忆单元，渐进强调最新模式并保留必要历史知识。通过时间轨迹分析和双分支一致性，实现了对样本的精确三向划分（干净、边界、噪声子集）。该机制在噪声条件下具有理论上的收敛性和稳定性保证。", "result": "ChronoSelect 在合成和真实世界基准测试中均展现了最先进的性能。", "conclusion": "ChronoSelect 通过利用学习过程的时间动态记忆，并结合创新的四阶段记忆架构和滑动更新机制，有效地解决了噪声标签问题，实现了样本的精确划分，并在各种基准测试中达到了最先进的性能。", "translation": "在真实世界数据上训练深度神经网络常常受到噪声标签的阻碍，这些噪声标签可能被过参数化的模型记忆，导致泛化性能显著下降。尽管现有的噪声标签学习（LNL）方法取得了相当大的进展，但它们根本上都受限于静态快照评估，未能利用学习演化的丰富时间动态。在本文中，我们提出了 ChronoSelect（chrono 表示其时间性质），一个具有创新四阶段记忆架构的新颖框架，该架构将预测历史压缩成紧凑的时间分布。我们独特的带控制衰减的滑动更新机制为每个样本仅维护四个动态记忆单元，逐步强调最新模式，同时保留必要的历史知识。这使得通过时间轨迹分析和双分支一致性，能够精确地将样本分为干净、边界和噪声子集。理论保证证明了该机制在噪声条件下的收敛性和稳定性。广泛的实验表明 ChronoSelect 在合成和真实世界基准测试中都达到了最先进的性能。", "summary": "本文提出 ChronoSelect，一个针对噪声标签鲁棒学习的新框架。该方法通过创新的四阶段记忆架构，将模型预测历史压缩为时间分布，并利用滑动更新机制动态维护样本的时间记忆。这使得模型能够通过时间轨迹分析和双分支一致性，将样本精确划分为干净、边界和噪声三类，有效克服了现有方法忽略学习动态的局限性。ChronoSelect 在理论上保证了收敛性和稳定性，并在多项基准测试中表现出最先进的性能。", "keywords": "噪声标签学习, 时间记忆, 深度学习, 鲁棒性, ChronoSelect", "comments": "ChronoSelect 的创新点在于其引入的时间动态记忆机制，通过捕捉学习过程中的演化信息，而非仅仅依赖静态快照，从而更精确地识别和处理噪声标签。四阶段记忆架构和滑动更新机制的设计，有效地平衡了新旧信息的权重，提升了对噪声样本的鲁棒性。这种方法为噪声学习领域提供了一个新颖且有效的视角。"}}
{"id": "2503.00230", "title": "Physics-Informed Implicit Neural Representations for Joint B0 Estimation and Echo Planar Imaging", "authors": ["Wenqi Huang", "Nan Wang", "Congyu Liao", "Yimeng Lin", "Mengze Gao", "Daniel Rueckert", "Kawin Setsompop"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.00230v3", "summary": "Echo Planar Imaging (EPI) is widely used for its rapid acquisition but\nsuffers from severe geometric distortions due to B0 inhomogeneities,\nparticularly along the phase encoding direction. Existing methods follow a\ntwo-step process: reconstructing blip-up/down EPI images, then estimating B0,\nwhich can introduce error accumulation and reduce correction accuracy. This is\nespecially problematic in high B0 regions, where distortions align along the\nsame axis, making them harder to disentangle. In this work, we propose a novel\napproach that integrates Implicit Neural Representations (INRs) with a\nphysics-informed correction model to jointly estimate B0 inhomogeneities and\nreconstruct distortion-free images from rotated-view EPI acquisitions. INRs\noffer a flexible, continuous representation that inherently captures complex\nspatial variations without requiring predefined grid-based field maps. By\nleveraging this property, our method dynamically adapts to subject-specific B0\nvariations and improves robustness across different imaging conditions.\nExperimental results on 180 slices of brain images from three subjects\ndemonstrate that our approach outperforms traditional methods in terms of\nreconstruction quality and field estimation accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.00230v3", "cate": "eess.IV", "date": "2025-02-28", "updated": "2025-07-24", "AI": {"title_translation": "物理信息隐式神经表示用于B0估计和回波平面成像", "tldr": "EPI因B0不均匀性导致严重畸变，现有方法存在误差累积问题。本文提出一种结合隐式神经表示（INRs）和物理信息校正模型的新方法，用于联合B0估计和无畸变图像重建，实验证明优于传统方法。", "motivation": "回波平面成像（EPI）因B0不均匀性导致严重的几何畸变，尤其是在相位编码方向。现有方法采用两步过程（先重建图像再估计B0），这会导致误差累积并降低校正精度，在高B0区域问题尤为突出，因为畸变难以分离。", "method": "本文提出一种新方法，将隐式神经表示（INRs）与物理信息校正模型相结合，从旋转视图EPI采集图像中联合估计B0不均匀性并重建无畸变图像。INRs提供灵活、连续的表示，能捕获复杂空间变化并适应受试者特异性B0变化。", "result": "对来自三名受试者的180个脑图像切片的实验结果表明，该方法在重建质量和场估计精度方面优于传统方法。", "conclusion": "本研究提出的结合物理信息隐式神经表示的联合B0估计和EPI重建方法，在重建质量和场估计精度上均优于传统方法，有效解决了B0不均匀性导致的几何畸变问题。", "translation": "回波平面成像 (EPI) 因其快速采集而广泛应用，但由于 B0 不均匀性，特别是在相位编码方向上，会遭受严重的几何畸变。现有方法遵循两步过程：重建上/下跳变 EPI 图像，然后估计 B0，这会引入误差累积并降低校正精度。这在 B0 高的区域尤其成问题，因为畸变沿同一轴线对齐，使其更难分离。在这项工作中，我们提出了一种新颖的方法，将隐式神经表示 (INRs) 与物理信息校正模型相结合，以从旋转视图 EPI 采集图像中联合估计 B0 不均匀性并重建无畸变图像。INRs 提供了一种灵活、连续的表示，它固有地捕获复杂的空间变化，而无需预定义基于网格的场图。通过利用这一特性，我们的方法动态适应受试者特定的 B0 变化，并提高了在不同成像条件下的鲁棒性。对来自三名受试者的 180 个脑图像切片的实验结果表明，我们的方法在重建质量和场估计精度方面优于传统方法。", "summary": "本文提出了一种新颖的物理信息隐式神经表示（INR）方法，用于联合B0不均匀性估计和无畸变回波平面成像（EPI）重建。该方法解决了现有两步法中误差累积和精度降低的问题，特别是在高B0区域。通过利用INR的灵活性和连续表示能力，该方法能够动态适应受试者特异性B0变化。在脑图像上的实验结果表明，该方法在重建质量和场估计精度方面均优于传统技术。", "keywords": "B0估计, 回波平面成像, 隐式神经表示, 物理信息, 几何畸变", "comments": "该论文的创新之处在于将隐式神经表示（INRs）与物理信息校正模型相结合，实现了B0估计和EPI图像重建的联合处理，有效避免了传统两步法中误差累积的问题。INRs作为一种灵活的连续表示，无需预定义网格场图，能够更好地捕捉复杂的空间B0变化，提高了方法的鲁棒性和适应性。"}}
{"id": "2506.09027", "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "authors": ["Runqian Wang", "Kaiming He"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.09027v2", "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.09027v2", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-24", "AI": {"title_translation": "弥散与分散：基于表示正则化的图像生成", "tldr": "本文提出了一种名为“分散损失”的简单即插即用正则化器，用于改进扩散生成模型，使其内部表示在隐藏空间中分散，无需正样本对，且无需预训练、额外参数或外部数据，在ImageNet上表现出一致的改进。", "motivation": "过去十年中，基于扩散的生成模型的发展与表示学习的进展基本独立。这些扩散模型通常依赖于基于回归的目标函数，并且普遍缺乏显式正则化，这限制了它们的性能和表示能力。", "method": "本文提出了一种名为“分散损失”（Dispersive Loss）的简单即插即用正则化器。该损失函数鼓励内部表示在隐藏空间中分散，类似于对比自监督学习，但关键区别在于它不需要正样本对，因此不干扰用于回归的采样过程。与最近的表示对齐（REPA）方法相比，该方法是自包含和极简主义的，不需要预训练、额外参数或外部数据。", "result": "在ImageNet数据集上对一系列模型进行了评估，结果表明，与广泛使用且强大的基线相比，分散损失始终能带来改进。", "conclusion": "本文提出的分散损失有效改进了扩散生成模型，并有望帮助弥合生成建模和表示学习之间的差距。", "translation": "在过去十年中，基于扩散的生成模型的发展与表示学习的进展基本独立。这些扩散模型通常依赖于基于回归的目标函数，并且普遍缺乏显式正则化。在这项工作中，我们提出了“分散损失”（Dispersive Loss），这是一种简单即插即用的正则化器，能有效改进基于扩散的生成模型。我们的损失函数鼓励内部表示在隐藏空间中分散，类似于对比自监督学习，其关键区别在于它不需要正样本对，因此不干扰用于回归的采样过程。与最近的表示对齐（REPA）方法相比，我们的方法是自包含和极简主义的，不需要预训练、额外参数和外部数据。我们在ImageNet数据集上对一系列模型评估了分散损失，并报告了相对于广泛使用和强大的基线的一致改进。我们希望我们的工作能帮助弥合生成建模和表示学习之间的差距。", "summary": "本研究引入了一种名为“分散损失”的新型即插即用正则化器，旨在改进扩散生成模型。该损失函数通过鼓励模型内部表示在隐藏空间中分散来工作，其灵感来源于对比自监督学习，但无需正样本对，从而避免了对回归采样过程的干扰。与现有方法相比，分散损失具有自包含和极简主义的特点，无需预训练、额外参数或外部数据。实验结果表明，在ImageNet数据集上，该方法在多种模型上均能持续优于基线，有望促进生成建模与表示学习的融合。", "keywords": "扩散模型, 表示学习, 正则化, 分散损失, 图像生成", "comments": "该论文的创新点在于提出了“分散损失”，这是一种简单而有效的正则化方法，弥补了扩散模型在表示学习方面缺乏显式正则化的不足。其“即插即用”和“无需正样本对”的特性使其具有很高的实用价值和普适性。该方法在不增加模型复杂度的前提下，提升了扩散模型的性能，并为生成模型与表示学习的结合提供了新的视角。"}}
{"id": "2507.18282", "title": "EigenWave: An Optimal O(N) Method for Computing Eigenvalues and Eigenvectors by Time-Filtering the Wave Equation", "authors": ["Daniel Appelo", "Jeffrey W. Banks", "William D. Henshaw", "Ngan Le", "Donald W. Schwendeman"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18282v1", "summary": "An algorithm named EigenWave is described to compute eigenvalues and\neigenvectors of elliptic boundary value problems. The algorithm, based on the\nrecently developed WaveHoltz scheme, solves a related time-dependent wave\nequation as part of an iteration. At each iteration, the solution to the wave\nequation is filtered in time. As the iteration progresses, the filtered\nsolution generally contains relatively larger and larger proportions of\neigenmodes whose eigenvalues are near a chosen target frequency (target\neigenvalue). The ability to choose an arbitrary target frequency enables the\ncomputation of eigenvalues anywhere in the spectrum, without the need to invert\nan indefinite matrix, as is common with other approaches. Furthermore, the\niteration can be embedded within a matrix-free Arnoldi algorithm, which enables\nthe efficient computation of multiple eigenpairs near the target frequency. For\nefficiency, the time-dependent wave equation can be solved with implicit\ntime-stepping and only about $10$ time-steps per-period are needed, independent\nof the mesh spacing. When the (definite) implicit time-stepping equations are\nsolved with a multigrid algorithm, the cost of the resulting EigenWave scheme\nscales linearly with the number of grid points $N$ as the mesh is refined,\ngiving an optimal $O(N)$ algorithm. The approach is demonstrated by finding\neigenpairs of the Laplacian in complex geometry using overset grids. Results in\ntwo and three space dimensions are presented using second-order and\nfourth-order accurate approximations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18282v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "EigenWave：一种通过时间滤波波动方程计算特征值和特征向量的最优O(N)方法", "tldr": "EigenWave是一种基于时间滤波波动方程的O(N)算法，用于高效计算椭圆边值问题的特征值和特征向量，无需矩阵求逆，且可计算任意目标频率附近的特征对。", "motivation": "现有的特征值计算方法通常需要对不定矩阵进行求逆，这限制了在频谱中任意位置计算特征值的能力。", "method": "提出了一种名为EigenWave的算法，它基于WaveHoltz方案，通过迭代求解一个相关的时间依赖波动方程。在每次迭代中，对波动方程的解进行时间滤波。随着迭代进行，滤波后的解中包含的目标频率（目标特征值）附近的特征模态比例逐渐增大。该算法可以嵌入到无矩阵的Arnoldi算法中，以高效计算目标频率附近的多个特征对。为了提高效率，时间依赖波动方程可以通过隐式时间步进求解，并且每周期只需约10个时间步。当隐式时间步进方程通过多重网格算法求解时，EigenWave方案的成本随网格点数N线性缩放，达到最优的O(N)算法。", "result": "该方法通过使用重叠网格在复杂几何中寻找拉普拉斯算子的特征对进行了演示。在二维和三维空间中，使用二阶和四阶精确近似得到了结果。该算法达到了最优的O(N)复杂度。", "conclusion": "EigenWave提供了一种计算椭圆边值问题特征值和特征向量的有效且最优的O(N)方法，它克服了传统方法需要矩阵求逆的限制，并能灵活地计算频谱中任意位置的特征对。", "translation": "描述了一种名为EigenWave的算法，用于计算椭圆边值问题的特征值和特征向量。该算法基于最近开发的WaveHoltz方案，作为迭代的一部分，求解一个相关的时间依赖波动方程。在每次迭代中，波动方程的解在时间上进行滤波。随着迭代的进行，滤波后的解通常包含的特征模态比例越来越大，这些特征模态的特征值接近选定的目标频率（目标特征值）。选择任意目标频率的能力使得可以在频谱中的任何位置计算特征值，而无需像其他常见方法那样反转不定矩阵。此外，该迭代可以嵌入到无矩阵的Arnoldi算法中，从而能够高效地计算目标频率附近的多个特征对。为了提高效率，时间依赖波动方程可以通过隐式时间步进求解，并且每周期只需约10个时间步，这与网格间距无关。当（定性）隐式时间步进方程通过多重网格算法求解时，所产生的EigenWave方案的成本随网格点数N线性缩放，使得其成为最优的O(N)算法。该方法通过使用重叠网格在复杂几何中寻找拉普拉斯算子的特征对进行了演示。在二维和三维空间中，使用二阶和四阶精确近似得到了结果。", "summary": "EigenWave是一种创新的O(N)算法，用于高效计算椭圆边值问题的特征值和特征向量。它通过时间滤波波动方程的迭代解来工作，允许用户选择目标频率以计算频谱中任意位置的特征对，避免了传统方法中对不定矩阵求逆的需求。该算法可与无矩阵Arnoldi结合，并通过隐式时间步进和多重网格实现O(N)复杂度，已在二维和三维复杂几何中的拉普拉斯算子特征对计算中得到验证。", "keywords": "特征值, 特征向量, 波动方程, 时间滤波, O(N)算法", "comments": "EigenWave的创新之处在于其通过时间滤波波动方程来避免不定矩阵求逆，并能灵活地计算频谱中任意位置的特征值，这解决了现有方法的一个主要限制。其O(N)的最优复杂度以及与无矩阵Arnoldi算法的结合，使其在处理大规模椭圆边值问题时具有显著的效率优势。该方法为特征值问题的数值求解提供了一个强大且高效的新途径。"}}
{"id": "2507.18252", "title": "Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning", "authors": ["Dongyang Guo", "Yasmeen Abdrabou", "Enkeleda Thaqi", "Enkelejda Kasneci"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18252v1", "summary": "Eye-tracking data reveals valuable insights into users' cognitive states but\nis difficult to analyze due to its structured, non-linguistic nature. While\nlarge language models (LLMs) excel at reasoning over text, they struggle with\ntemporal and numerical data. This paper presents a multimodal human-AI\ncollaborative framework designed to enhance cognitive pattern extraction from\neye-tracking signals. The framework includes: (1) a multi-stage pipeline using\nhorizontal and vertical segmentation alongside LLM reasoning to uncover latent\ngaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert\njudgment with LLM output to generate trust scores for behavioral\ninterpretations; and (3) a hybrid anomaly detection module combining LSTM-based\ntemporal modeling with LLM-driven semantic analysis. Our results across several\nLLMs and prompt strategies show improvements in consistency, interpretability,\nand performance, with up to 50% accuracy in difficulty prediction tasks. This\napproach offers a scalable, interpretable solution for cognitive modeling and\nhas broad potential in adaptive learning, human-computer interaction, and\neducational analytics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18252v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于眼动追踪和LLM推理的多模态行为模式分析", "tldr": "本文提出一个结合眼动追踪数据和LLM推理的多模态人机协作框架，用于增强认知模式提取，并在难度预测任务中展现出改进的性能和可解释性。", "motivation": "眼动追踪数据能揭示用户认知状态但难以分析，因为其结构化、非语言特性。大型语言模型（LLMs）擅长文本推理但难以处理时序和数值数据。因此，需要一个框架来结合两者的优势，增强从眼动信号中提取认知模式的能力。", "method": "本文提出了一个多模态人机协作框架，旨在增强从眼动信号中提取认知模式。该框架包括：1) 一个多阶段管道，利用水平和垂直分割以及LLM推理来揭示潜在的注视模式；2) 一个专家-模型协同评分模块，将专家判断与LLM输出结合以生成行为解释的信任分数；3) 一个混合异常检测模块，结合基于LSTM的时间建模与LLM驱动的语义分析。", "result": "结果显示，该方法在多个LLM和提示策略下，提高了分析的一致性、可解释性和性能，在难度预测任务中准确率高达50%。", "conclusion": "该方法为认知建模提供了一个可扩展、可解释的解决方案，在自适应学习、人机交互和教育分析等领域具有广泛潜力。", "translation": "眼动追踪数据能揭示用户认知状态的宝贵信息，但由于其结构化、非语言的特性，分析起来很困难。虽然大型语言模型（LLMs）擅长文本推理，但它们在处理时序和数值数据方面存在困难。本文提出一个多模态人机协作框架，旨在增强从眼动信号中提取认知模式的能力。该框架包括：(1) 一个多阶段管道，利用水平和垂直分割以及LLM推理来揭示潜在的注视模式；(2) 一个专家-模型协同评分模块，将专家判断与LLM输出结合以生成行为解释的信任分数；以及 (3) 一个混合异常检测模块，结合基于LSTM的时间建模与LLM驱动的语义分析。我们在多个LLM和提示策略下的结果表明，该方法在一致性、可解释性和性能方面有所改进，在难度预测任务中准确率高达50%。这种方法为认知建模提供了一个可扩展、可解释的解决方案，并在自适应学习、人机交互和教育分析方面具有广泛潜力。", "summary": "本文提出了一个创新的多模态人机协作框架，旨在克服眼动追踪数据分析的挑战和大型语言模型在处理非文本数据时的局限性。该框架整合了LLM推理、多阶段眼动数据处理、专家协同评分以及混合异常检测，以从眼动信号中提取深层认知模式。实验结果表明，该方法显著提高了认知模式分析的一致性、可解释性和性能，尤其在难度预测任务中取得了50%的准确率，为认知建模及其在教育、人机交互等领域的应用提供了可扩展且可解释的方案。", "keywords": "眼动追踪, 大型语言模型, 多模态分析, 认知建模, 人机协作", "comments": "这篇论文的创新点在于其提出了一种独特的多模态人机协作框架，有效地结合了眼动追踪数据的精细性和LLM的强大推理能力，克服了传统方法在处理非结构化、时序数据上的不足。通过引入专家协同评分和混合异常检测模块，提高了结果的可信度和鲁棒性。该方法提供了一个可扩展且可解释的认知建模方案，在自适应学习、人机交互和教育分析等领域具有重要应用潜力。"}}
{"id": "2504.17999", "title": "Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving", "authors": ["Chang Xiao", "Brenda Yang"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17999v2", "summary": "Generative conversational interfaces powered by large language models (LLMs)\ntypically stream output token-by-token at a rate determined by computational\nbudget, often neglecting actual human reading speeds and the cognitive load\nassociated with the content. This mismatch frequently leads to inefficient use\nof computational resources. For example, in cloud-based services, streaming\ncontent faster than users can read appears unnecessary, resulting in wasted\ncomputational resources and potential delays for other users, particularly\nduring peak usage periods. To address this issue, we propose an adaptive\nstreaming method that dynamically adjusts the pacing of LLM streaming output in\nreal-time based on inferred cognitive load. Our approach estimates the\ncognitive load associated with streaming content and strategically slows down\nthe stream during complex or information-rich segments, thereby freeing\ncomputational resources for other users. We conducted a statistical analysis\nand simulation based on a statistical model derived from data collected in a\ncrowdsourced user study across various types of LLM-generated content. Our\nresults show that this adaptive method can effectively reduce computational\nconsumption while largely maintaining streaming speed above user's normal\nreading speed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17999v2", "cate": "cs.HC", "date": "2025-04-25", "updated": "2025-07-23", "AI": {"title_translation": "快慢流式传输：认知负荷感知流式传输用于高效LLM服务", "tldr": "本文提出一种根据认知负荷动态调整LLM流式传输速度的方法，以提高计算资源利用效率。", "motivation": "当前大型语言模型（LLM）的生成式对话接口通常根据计算预算进行逐令牌流式传输，但忽略了人类的实际阅读速度和内容相关的认知负荷，导致计算资源（尤其是在云服务中）的低效利用和浪费，例如传输速度快于用户阅读速度。", "method": "提出了一种自适应流式传输方法，根据推断的认知负荷实时动态调整LLM流式输出的节奏。该方法通过估计流式内容的认知负荷，并在复杂或信息丰富的片段中策略性地减慢流速，从而释放计算资源。研究通过基于众包用户研究数据的统计模型进行了统计分析和模拟。", "result": "该自适应方法能够有效地降低计算消耗，同时基本保持流式传输速度高于用户的正常阅读速度。", "conclusion": "通过根据认知负荷调整LLM流式传输速度，可以有效降低计算资源消耗，提高LLM服务效率，同时不显著影响用户体验。", "translation": "由大型语言模型（LLM）驱动的生成式对话界面通常以计算预算决定的速率逐令牌流式传输输出，但往往忽略了实际的人类阅读速度和与内容相关的认知负荷。这种不匹配经常导致计算资源的低效利用。例如，在基于云的服务中，内容流式传输速度快于用户阅读速度似乎是不必要的，导致计算资源浪费，并可能在高峰使用期间给其他用户带来延迟。为了解决这个问题，我们提出了一种自适应流式传输方法，该方法根据推断的认知负荷实时动态调整LLM流式输出的节奏。我们的方法估计与流式内容相关的认知负荷，并在复杂或信息丰富的片段中策略性地减慢流速，从而为其他用户释放计算资源。我们基于从众包用户研究中收集到的各种LLM生成内容的数据得出的统计模型，进行了统计分析和模拟。我们的结果表明，这种自适应方法可以有效地降低计算消耗，同时基本保持流式传输速度高于用户的正常阅读速度。", "summary": "本文提出了一种用于LLM服务的高效自适应流式传输方法，该方法根据推断的用户认知负荷动态调整输出速度。针对LLM输出速度往往快于人类阅读速度导致的资源浪费问题，该方法在内容复杂或信息量大的部分策略性地减慢流速，以节省计算资源。通过基于众包用户研究数据的统计分析和模拟，结果表明该方法能有效降低计算消耗，同时保持流速高于用户正常阅读速度。", "keywords": "LLM服务, 自适应流式传输, 认知负荷, 资源效率, 生成式AI", "comments": "这项研究的创新之处在于将认知负荷的概念引入到LLM的流式传输机制中，超越了单纯基于计算预算的限制。这对于优化基于云的LLM服务中的资源利用至关重要，尤其是在高峰期。它提供了一个实用的解决方案，解决了常见的效率低下问题，同时没有显著牺牲用户体验。"}}
{"id": "2507.12890", "title": "DiffRhythm+: Controllable and Flexible Full-Length Song Generation with Preference Optimization", "authors": ["Huakang Chen", "Yuepeng Jiang", "Guobin Ma", "Chunbo Hao", "Shuai Wang", "Jixun Yao", "Ziqian Ning", "Meng Meng", "Jian Luan", "Lei Xie"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12890v2", "summary": "Songs, as a central form of musical art, exemplify the richness of human\nintelligence and creativity. While recent advances in generative modeling have\nenabled notable progress in long-form song generation, current systems for\nfull-length song synthesis still face major challenges, including data\nimbalance, insufficient controllability, and inconsistent musical quality.\nDiffRhythm, a pioneering diffusion-based model, advanced the field by\ngenerating full-length songs with expressive vocals and accompaniment. However,\nits performance was constrained by an unbalanced model training dataset and\nlimited controllability over musical style, resulting in noticeable quality\ndisparities and restricted creative flexibility. To address these limitations,\nwe propose DiffRhythm+, an enhanced diffusion-based framework for controllable\nand flexible full-length song generation. DiffRhythm+ leverages a substantially\nexpanded and balanced training dataset to mitigate issues such as repetition\nand omission of lyrics, while also fostering the emergence of richer musical\nskills and expressiveness. The framework introduces a multi-modal style\nconditioning strategy, enabling users to precisely specify musical styles\nthrough both descriptive text and reference audio, thereby significantly\nenhancing creative control and diversity. We further introduce direct\nperformance optimization aligned with user preferences, guiding the model\ntoward consistently preferred outputs across evaluation metrics. Extensive\nexperiments demonstrate that DiffRhythm+ achieves significant improvements in\nnaturalness, arrangement complexity, and listener satisfaction over previous\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12890v2", "cate": "eess.AS", "date": "2025-07-17", "updated": "2025-07-24", "AI": {"title_translation": "DiffRhythm+: 基于偏好优化的可控灵活全长歌曲生成", "tldr": "DiffRhythm+是一个增强的扩散模型，通过扩大数据集、多模态风格条件和用户偏好优化，解决了现有全长歌曲生成系统中数据不平衡、可控性不足和音乐质量不一致的问题，显著提升了生成歌曲的自然度、编曲复杂性和听众满意度。", "motivation": "当前的全长歌曲合成系统面临数据不平衡、可控性不足和音乐质量不一致等主要挑战。先前的DiffRhythm模型受限于不平衡的训练数据集和有限的音乐风格可控性，导致质量差异和创作灵活度受限。", "method": "我们提出了DiffRhythm+，一个增强的基于扩散的框架，用于可控和灵活的全长歌曲生成。该框架利用大幅扩展和平衡的训练数据集来解决歌词重复和遗漏等问题，并培养更丰富的音乐技能和表现力。它引入了多模态风格条件策略，允许用户通过描述性文本和参考音频精确指定音乐风格，从而显著增强创作控制和多样性。此外，还引入了与用户偏好对齐的直接性能优化，引导模型在评估指标上产生一致偏好的输出。", "result": "广泛的实验表明，DiffRhythm+在自然度、编曲复杂性和听众满意度方面比以前的系统取得了显著改进。", "conclusion": "DiffRhythm+通过改进数据集、引入多模态风格控制和用户偏好优化，成功克服了全长歌曲生成中的关键挑战，显著提升了生成歌曲的质量和可控性。", "translation": "歌曲作为音乐艺术的核心形式，展现了人类智能和创造力的丰富性。尽管生成建模的最新进展在长篇歌曲生成方面取得了显著进展，但当前的全长歌曲合成系统仍面临主要挑战，包括数据不平衡、可控性不足和音乐质量不一致。DiffRhythm作为一种开创性的基于扩散的模型，通过生成具有表现力的人声和伴奏的全长歌曲，推动了该领域的发展。然而，其性能受限于不平衡的模型训练数据集和对音乐风格的有限可控性，导致显著的质量差异和受限的创作灵活性。为了解决这些限制，我们提出了DiffRhythm+，一个增强的基于扩散的框架，用于可控和灵活的全长歌曲生成。DiffRhythm+利用大幅扩展和平衡的训练数据集来缓解歌词重复和遗漏等问题，同时促进更丰富的音乐技能和表现力的出现。该框架引入了多模态风格条件策略，使用户能够通过描述性文本和参考音频精确指定音乐风格，从而显著增强创作控制和多样性。我们进一步引入了与用户偏好对齐的直接性能优化，引导模型在评估指标上产生一致偏好的输出。广泛的实验表明，DiffRhythm+在自然度、编曲复杂性和听众满意度方面比以前的系统取得了显著改进。", "summary": "本文提出了DiffRhythm+，一个增强的扩散模型，旨在解决全长歌曲生成中数据不平衡、可控性不足和音乐质量不一致的问题。DiffRhythm+通过使用扩展且平衡的训练数据集、引入多模态风格条件策略（支持文本和音频输入）以及实施基于用户偏好的直接性能优化，显著提升了生成歌曲的自然度、编曲复杂性和听众满意度，从而实现了更可控和灵活的全长歌曲创作。", "keywords": "全长歌曲生成, 扩散模型, 可控性, 偏好优化, DiffRhythm+", "comments": "DiffRhythm+的创新之处在于其综合性的改进方案，包括扩大和平衡数据集以提升基础生成质量，引入多模态风格条件以增强用户控制和创作多样性，以及通过用户偏好优化来确保输出与听众喜好一致。这些改进共同解决了现有全长歌曲生成系统的核心痛点，对于推动音乐AI创作的实用性和艺术性具有重要意义。"}}
{"id": "2409.03803", "title": "OGRePy: An Object-Oriented General Relativity Package for Python", "authors": ["Barak Shoshany"], "categories": ["gr-qc", "cs.MS", "cs.SC", "math.DG", "G.4; I.1; J.2"], "primary_category": "Subjects:       General Relativity and Quantum Cosmology (gr-qc)", "pdf_link": null, "comments": "Comments:      5 pages, version published in JORS, full documentation and source code available at this https URL", "url": "http://arxiv.org/abs/2409.03803v2", "summary": "OGRePy is a modern, open-source Python package designed to perform symbolic\ntensor calculations, with a particular focus on applications in general\nrelativity. Built on an object-oriented architecture, OGRePy encapsulates\ntensors, metrics, and coordinate systems as self-contained objects,\nautomatically handling raising and lowering of indices, coordinate\ntransformations, contractions, partial or covariant derivatives, and all tensor\noperations. By leveraging the capabilities of SymPy and Jupyter Notebook,\nOGRePy provides a robust, user-friendly environment that facilitates both\nresearch and teaching in general relativity and differential geometry. This\nPython package reproduces the functionality of the popular Mathematica package\nOGRe, while greatly improving upon it by making use of Python's native\nobject-oriented syntax. In this paper, we describe OGRePy's design and\nimplementation, and discuss its potential for reuse across research and\neducation in mathematics and physics.", "comment": "5 pages, version published in JORS, full documentation and source\n  code available at https://github.com/bshoshany/OGRePy", "pdf_url": "http://arxiv.org/pdf/2409.03803v2", "cate": "gr-qc", "date": "2024-09-05", "updated": "2025-07-23", "AI": {"title_translation": "OGRePy：一个用于Python的面向对象广义相对论软件包", "tldr": "OGRePy是一个基于Python的面向对象开源软件包，用于广义相对论中的符号张量计算，它改进了流行的Mathematica软件包OGRe。", "motivation": "开发一个现代、开源的Python软件包，用于广义相对论中的符号张量计算，并改进现有流行的Mathematica软件包OGRe的功能。", "method": "OGRePy采用面向对象架构，将张量、度规和坐标系封装为独立对象，自动处理指标升降、坐标变换、收缩、偏导数或协变导数以及所有张量操作。它利用SymPy和Jupyter Notebook的功能。", "result": "OGRePy提供了一个健壮、用户友好的环境，便于广义相对论和微分几何的研究与教学。它再现了流行的Mathematica软件包OGRe的功能，并通过利用Python原生的面向对象语法大大改进了它。", "conclusion": "OGRePy是一个有潜力的工具，可以在数学和物理领域的研究和教育中重复使用，尤其是在广义相对论和微分几何方面。", "translation": "OGRePy是一个现代的开源Python软件包，旨在执行符号张量计算，特别关注在广义相对论中的应用。OGRePy建立在面向对象的架构之上，将张量、度规和坐标系封装为自包含对象，自动处理指标的升降、坐标变换、收缩、偏导数或协变导数以及所有张量操作。通过利用SymPy和Jupyter Notebook的功能，OGRePy提供了一个健壮、用户友好的环境，便于广义相对论和微分几何的研究和教学。这个Python软件包再现了流行的Mathematica软件包OGRe的功能，同时通过利用Python原生的面向对象语法大大改进了它。在本文中，我们描述了OGRePy的设计和实现，并讨论了其在数学和物理领域的研究和教育中重复利用的潜力。", "summary": "OGRePy是一个开源的Python软件包，专为广义相对论中的符号张量计算而设计。它采用面向对象架构，将物理量封装为独立对象，并自动化处理各种张量操作。该软件包利用SymPy和Jupyter Notebook，为广义相对论和微分几何的研究与教学提供了一个用户友好的环境，同时改进了现有Mathematica软件包OGRe的功能。", "keywords": "广义相对论, 符号计算, Python, 张量, 面向对象", "comments": "OGRePy的创新之处在于将广义相对论中的复杂张量计算以面向对象的方式实现在Python中，这对于习惯Python生态系统的研究者和教育者来说极具吸引力。它不仅复制了现有流行软件包的功能，还通过利用Python的特性进行了优化，有望降低学习曲线并提高工作效率。其开源性质也促进了社区协作和进一步发展。"}}
{"id": "2506.11790", "title": "Why Do Class-Dependent Evaluation Effects Occur with Time Series Feature Attributions? A Synthetic Data Investigation", "authors": ["Gregor Baer", "Isel Grau", "Chao Zhang", "Pieter Van Gorp"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for Time Series and Data Streams)", "url": "http://arxiv.org/abs/2506.11790v2", "summary": "Evaluating feature attribution methods represents a critical challenge in\nexplainable AI (XAI), as researchers typically rely on perturbation-based\nmetrics when ground truth is unavailable. However, recent work reveals that\nthese evaluation metrics can show different performance across predicted\nclasses within the same dataset. These \"class-dependent evaluation effects\"\nraise questions about whether perturbation analysis reliably measures\nattribution quality, with direct implications for XAI method development and\nevaluation trustworthiness. We investigate under which conditions these\nclass-dependent effects arise by conducting controlled experiments with\nsynthetic time series data where ground truth feature locations are known. We\nsystematically vary feature types and class contrasts across binary\nclassification tasks, then compare perturbation-based degradation scores with\nground truth-based precision-recall metrics using multiple attribution methods.\nOur experiments demonstrate that class-dependent effects emerge with both\nevaluation approaches, even in simple scenarios with temporally localized\nfeatures, triggered by basic variations in feature amplitude or temporal extent\nbetween classes. Most critically, we find that perturbation-based and ground\ntruth metrics frequently yield contradictory assessments of attribution quality\nacross classes, with weak correlations between evaluation approaches. These\nfindings suggest that researchers should interpret perturbation-based metrics\nwith care, as they may not always align with whether attributions correctly\nidentify discriminating features. By showing this disconnect, our work points\ntoward reconsidering what attribution evaluation actually measures and\ndeveloping more rigorous evaluation methods that capture multiple dimensions of\nattribution quality.", "comment": "Accepted at TempXAI Workshop @ ECML-PKDD 2025 (Explainable AI for\n  Time Series and Data Streams)", "pdf_url": "http://arxiv.org/pdf/2506.11790v2", "cate": "cs.LG", "date": "2025-06-13", "updated": "2025-07-24", "AI": {"title_translation": "为什么时间序列特征归因会出现类别依赖的评估效应？一项合成数据调查", "tldr": "本研究发现，在解释性AI中，特征归因方法的评估指标（尤其是基于扰动的指标）存在类别依赖性，且与真实值评估结果常不一致，这表明现有评估方法可能不可靠。", "motivation": "解释性AI（XAI）中特征归因方法的评估是一个重大挑战，因为研究人员通常在缺乏真实值时依赖基于扰动的指标。然而，近期研究发现这些评估指标在同一数据集内不同预测类别上表现不同，即“类别依赖的评估效应”。这引发了对扰动分析能否可靠衡量归因质量的质疑，并对XAI方法开发和评估的可靠性产生直接影响。", "method": "本研究通过对已知真实特征位置的合成时间序列数据进行受控实验，系统地改变二元分类任务中的特征类型和类别对比度。然后，使用多种归因方法，将基于扰动的退化分数与基于真实值的精确召回率指标进行比较。", "result": "实验表明，即使在具有时间局部特征的简单场景中，类别依赖效应也会在两种评估方法中出现，并由类别间特征幅度或时间范围的基本变化触发。最关键的是，研究发现基于扰动的指标和真实值指标在评估归因质量时经常产生矛盾的结果，且两种评估方法之间相关性很弱。", "conclusion": "这些发现表明，研究人员应谨慎解释基于扰动的指标，因为它们可能并非总能与归因是否正确识别区分性特征相符。通过揭示这种脱节，本研究指出需要重新思考归因评估实际衡量的是什么，并开发更严格的、能捕捉归因质量多个维度的评估方法。", "translation": "评估特征归因方法是可解释人工智能（XAI）中的一个关键挑战，因为研究人员在缺乏真实值时通常依赖基于扰动的度量标准。然而，最近的工作表明，这些评估指标在同一数据集内的不同预测类别中可能表现出不同的性能。这些“类别依赖的评估效应”引发了关于扰动分析是否可靠衡量归因质量的疑问，并对XAI方法开发和评估的可靠性产生直接影响。我们通过对已知真实特征位置的合成时间序列数据进行受控实验，调查这些类别依赖效应在何种条件下产生。我们系统地改变二元分类任务中的特征类型和类别对比度，然后使用多种归因方法，将基于扰动的退化分数与基于真实值的精确召回率指标进行比较。我们的实验表明，即使在具有时间局部特征的简单场景中，类别依赖效应也会在两种评估方法中出现，并由类别间特征幅度或时间范围的基本变化触发。最关键的是，我们发现基于扰动的指标和真实值指标在评估归因质量时经常产生矛盾的评估结果，且两种评估方法之间相关性很弱。这些发现表明，研究人员应谨慎解释基于扰动的指标，因为它们可能并非总能与归因是否正确识别区分性特征相符。通过揭示这种脱节，我们的工作指向重新思考归因评估实际衡量的是什么，并开发更严格的、能捕捉归因质量多个维度的评估方法。", "summary": "本研究调查了可解释人工智能（XAI）中时间序列特征归因评估中出现的“类别依赖评估效应”。通过对合成时间序列数据进行受控实验，比较了基于扰动的评估指标与真实值指标。结果表明，即使在简单场景下，类别依赖效应依然存在，且基于扰动的指标与真实值指标在评估归因质量时常产生矛盾结果。研究强调，应谨慎使用基于扰动的评估指标，并呼吁开发更严格、多维度的归因评估方法。", "keywords": "特征归因, 可解释人工智能, 评估指标, 类别依赖效应, 时间序列", "comments": "这项工作揭示了XAI领域中一个关键且被忽视的问题：现有特征归因评估方法（特别是基于扰动的指标）的可靠性。其创新之处在于通过受控的合成数据实验，明确证实了“类别依赖评估效应”的存在及其对评估结果的误导性。研究强调了基于扰动指标与真实值指标之间的脱节，对XAI方法的开发和评估实践提出了重要警示，具有重要的理论和实践意义。"}}
{"id": "2507.18339", "title": "FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping", "authors": ["Nils Bosbach", "Meik Schmidt", "Lukas Jünger", "Matthias Berthold", "Rainer Leupers"], "categories": ["cs.SE", "cs.DC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      PREPRINT - accepted by the 16th International Modelica and FMI Conference 2025", "url": "http://arxiv.org/abs/2507.18339v1", "summary": "As systems become more complex, the demand for thorough testing and virtual\nprototyping grows. To simulate whole systems, multiple tools are usually needed\nto cover different parts. These parts include the hardware of a system and the\nenvironment with which the system interacts. The Functional Mock-up Interface\n(FMI) standard for co-simulation can be used to connect these tools.\n  The control part of modern systems is usually a computing unit, such as a\nSystem-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software\nfrom a connected memory and interacts with peripherals. To develop software\nwithout requiring access to physical hardware, full-system simulators, the\nso-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized\nframework for VP development is SystemC TLM. SystemC provides interfaces and\nconcepts that enable modular design and model exchange. However, SystemC lacks\nnative FMI support, which limits the integration into broader co-simulation\nenvironments.\n  This paper presents a novel framework to control and interact with\nSystemC-based VPs using the FMI. We present a case study showing how a\nsimulated temperature sensor in a SystemC simulation can obtain temperature\nvalues from an external tool via FMI. This approach allows the unmodified\ntarget software to run on the VP and receive realistic environmental input data\nsuch as temperature, velocity, or acceleration values from other tools. Thus,\nextensive software testing and verification is enabled. By having tests ready\nand the software pre-tested using a VP once the physical hardware is available,\ncertifications like ISO 26262 can be done earlier.", "comment": "PREPRINT - accepted by the 16th International Modelica and FMI\n  Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.18339v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "FMI 遇见 SystemC：一种跨工具虚拟原型开发框架", "tldr": "本文提出了一种将基于 SystemC 的虚拟平台与 FMI 集成的框架，以实现协同仿真，从而通过真实的外部输入进行全面的软件测试。", "motivation": "随着系统复杂性增加，对彻底测试和虚拟原型开发的需求也随之增长。为了模拟整个系统，通常需要多个工具。FMI 标准可以连接这些工具，但 SystemC（常用于虚拟平台开发）缺乏原生 FMI 支持，这限制了其集成到更广泛的协同仿真环境中。需要在没有物理硬件的情况下开发软件。", "method": "本文提出了一种新颖的框架，利用 FMI 控制和与基于 SystemC 的虚拟平台 (VP) 进行交互。通过一个案例研究，展示了 SystemC 仿真中的模拟温度传感器如何通过 FMI 从外部工具获取温度值，从而使未经修改的目标软件在 VP 上运行并接收真实的外部环境输入数据。", "result": "该框架实现了广泛的软件测试和验证。它使得在物理硬件可用之前，就可以通过 VP 准备好测试并预先测试软件。这有助于更早地进行 ISO 26262 等认证。", "conclusion": "本文提出了一种将 SystemC 与 FMI 集成的新框架，通过实现逼真的协同仿真和早期软件验证，增强了复杂系统的虚拟原型开发能力，从而可以加速认证过程。", "translation": "随着系统变得越来越复杂，对彻底测试和虚拟原型开发的需求也随之增长。为了模拟整个系统，通常需要多个工具来覆盖不同的部分。这些部分包括系统的硬件以及系统与之交互的环境。功能模型接口 (FMI) 协同仿真标准可用于连接这些工具。\n现代系统的控制部分通常是计算单元，例如片上系统 (SoC) 或微控制器单元 (MCU)，它执行来自连接存储器的软件并与外设交互。为了在不需要物理硬件的情况下开发软件，通常使用全系统模拟器，即所谓的虚拟平台 (VP)。IEEE 标准的 VP 开发框架是 SystemC TLM。SystemC 提供接口和概念，可实现模块化设计和模型交换。然而，SystemC 缺乏原生的 FMI 支持，这限制了其集成到更广泛的协同仿真环境中。\n本文提出了一种新颖的框架，用于使用 FMI 控制和与基于 SystemC 的 VP 进行交互。我们提出了一个案例研究，展示了 SystemC 仿真中的模拟温度传感器如何通过 FMI 从外部工具获取温度值。这种方法允许未经修改的目标软件在 VP 上运行，并从其他工具接收真实的环​​境输入数据，例如温度、速度或加速度值。因此，可以进行广泛的软件测试和验证。通过在物理硬件可用后立即准备好测试并使用 VP 预先测试软件，可以更早地进行 ISO 26262 等认证。", "summary": "本文介绍了一种新颖的框架，将基于 SystemC 的虚拟平台 (VP) 与功能模型接口 (FMI) 标准集成。这种集成解决了 SystemC 缺乏原生 FMI 支持的限制，从而实现了复杂系统开发的跨工具协同仿真。一个案例研究展示了该框架如何允许 SystemC VP 通过 FMI 从外部工具接收真实的环​​境输入，从而促进广泛的软件测试和验证，并可能通过在物理硬件可用之前进行预测试来加速 ISO 26262 等认证。", "keywords": "FMI, SystemC, 虚拟原型, 协同仿真, 软件测试", "comments": "该创新的核心在于弥合了 SystemC（一种广泛使用的 VP 开发框架）与 FMI（一种协同仿真标准）之间的鸿沟。这种集成对于整体系统仿真至关重要，特别是对于硬件和环境交互至关重要的复杂信息物理系统。在 VP 上运行带有真实输入的未经修改的目标软件，对于早期软件开发和合规性而言是一个显著的优势。"}}
{"id": "2507.17999", "title": "Dual Charging for Half-Integral TSP", "authors": ["Nathan Klein", "Mehrshad Taziki"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17999v1", "summary": "We show that the max entropy algorithm is a randomized 1.49776 approximation\nfor half-integral TSP, improving upon the previous known bound of 1.49993 from\nKarlin et al. This also improves upon the best-known approximation for\nhalf-integral TSP due to Gupta et al. Our improvement results from using the\ndual, instead of the primal, to analyze the expected cost of the matching. We\nbelieve this method of analysis could lead to a simpler proof that max entropy\nis a better-than-3/2 approximation in the general case.\n  We also give a 1.4671 approximation for half integral LP solutions with no\nproper minimum cuts and an even number of vertices, improving upon the bound of\nHaddadan and Newman of 1.476. We then extend the analysis to the case when\nthere are an odd number of vertices $n$ at the cost of an additional $O(1/n)$\nfactor.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17999v1", "cate": "cs.DS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "半整数TSP的双重充电", "tldr": "本文通过使用对偶分析方法，改进了半整数旅行商问题（TSP）和特定半整数LP解的近似比。", "motivation": "该研究旨在改进半整数旅行商问题（TSP）的现有近似界限，并探索一种新的分析方法（使用对偶而非原始问题）以期获得更优的结果和更简洁的证明。", "method": "本文使用最大熵算法，并通过对偶而非原始问题来分析匹配的预期成本。此外，还将该分析扩展到具有奇数个顶点的LP解。", "result": "本文将半整数TSP的最大熵算法近似比提高到1.49776，优于Karlin等人先前的1.49993。同时，对于没有适当最小割且顶点数为偶数的半整数LP解，给出了1.4671的近似比，优于Haddadan和Newman的1.476。对于奇数个顶点的情况，分析也得到了扩展，但增加了O(1/n)的因子。", "conclusion": "通过使用对偶分析方法，最大熵算法在半整数TSP问题上取得了显著的近似比改进。研究者认为这种分析方法有望为最大熵算法在一般情况下获得优于3/2近似比提供更简单的证明。", "translation": "我们展示了最大熵算法对于半整数TSP的随机近似比为1.49776，这改进了Karlin等人之前已知的1.49993的界限。这也改进了Gupta等人对半整数TSP的最佳已知近似比。我们的改进来自于使用对偶而非原始问题来分析匹配的预期成本。我们相信这种分析方法可以为最大熵算法在一般情况下优于3/2的近似比提供一个更简单的证明。\n我们还为没有适当最小割且顶点数为偶数的半整数LP解给出了1.4671的近似比，这改进了Haddadan和Newman的1.476的界限。然后我们将分析扩展到当顶点数n为奇数时的情况，代价是增加了O(1/n)的因子。", "summary": "本文通过采用对偶分析方法，显著改进了半整数旅行商问题（TSP）的近似算法性能，将最大熵算法的近似比提高到1.49776。此外，研究还为特定条件的半整数线性规划（LP）解提供了1.4671的近似比改进。作者提出，这种基于对偶的分析策略有望为更普遍情况下的算法近似界限提供更简洁的证明。", "keywords": "半整数TSP, 近似算法, 最大熵算法, 对偶分析, 线性规划", "comments": "这篇论文的创新点在于其分析方法：通过利用对偶问题来分析最大熵算法，作者成功地改进了半整数TSP和相关LP问题的近似界限。这种方法的转变不仅带来了数值上的提升，更重要的是，它为未来对近似算法的理论分析提供了新的视角和潜力，可能简化复杂证明。"}}
{"id": "2507.18176", "title": "Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling", "authors": ["Abhishek Kaushik", "Norbert Haala", "Uwe Soergel"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18176v1", "summary": "Addressing performance degradation in 3D LiDAR semantic segmentation due to\ndomain shifts (e.g., sensor type, geographical location) is crucial for\nautonomous systems, yet manual annotation of target data is prohibitive. This\nstudy addresses the challenge using Unsupervised Domain Adaptation (UDA) and\nintroduces a novel two-stage framework to tackle it. Initially, unsupervised\ncontrastive learning at the segment level is used to pre-train a backbone\nnetwork, enabling it to learn robust, domain-invariant features without labels.\nSubsequently, a multi-model pseudo-labeling strategy is introduced, utilizing\nan ensemble of diverse state-of-the-art architectures (including projection,\nvoxel, hybrid, and cylinder-based methods). Predictions from these models are\naggregated via hard voting to generate high-quality, refined pseudo-labels for\nthe unlabeled target domain, mitigating single-model biases. The contrastively\npre-trained network is then fine-tuned using these robust pseudo-labels.\nExperiments adapting from SemanticKITTI to unlabeled target datasets\n(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in\nsegmentation accuracy compared to direct transfer and single-model UDA\napproaches. These results highlight the effectiveness of combining contrastive\npre-training with refined ensemble pseudo-labeling for bridging complex domain\ngaps without requiring target domain annotations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18176v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于对比学习和多模型伪标签的3D LiDAR语义分割无监督域适应", "tldr": "该研究提出一个两阶段框架，结合对比学习预训练和多模型伪标签，解决3D LiDAR语义分割中的无监督域适应问题，显著提升了分割精度。", "motivation": "解决3D LiDAR语义分割中由于域偏移（例如传感器类型、地理位置）导致的性能下降问题，同时避免目标数据手动标注成本过高的问题。", "method": "本文提出一个新颖的两阶段无监督域适应（UDA）框架。首先，在片段级别使用无监督对比学习预训练骨干网络，以学习鲁棒的、域不变的特征。其次，引入多模型伪标签策略，通过集成多种先进架构（包括投影、体素、混合和基于圆柱的方法）的预测，并使用硬投票聚合生成高质量、精炼的伪标签。最后，使用这些伪标签对对比预训练的网络进行微调。", "result": "在从SemanticKITTI到未标注目标数据集（SemanticPOSS、SemanticSlamantic）的适应实验中，与直接迁移和单模型UDA方法相比，分割精度显著提高。", "conclusion": "结合对比预训练和精炼的集成伪标签能有效弥合复杂的域差距，而无需目标域标注。", "translation": "解决3D LiDAR语义分割中由于域偏移（例如，传感器类型、地理位置）导致的性能下降对于自动驾驶系统至关重要，然而目标数据的手动标注成本过高。本研究通过无监督域适应（UDA）解决了这一挑战，并引入了一种新颖的两阶段框架来解决它。最初，在片段级别使用无监督对比学习来预训练骨干网络，使其能够在没有标签的情况下学习鲁棒的、域不变的特征。随后，引入了一种多模型伪标签策略，利用了多种最先进架构（包括投影、体素、混合和基于圆柱的方法）的集成。通过硬投票聚合这些模型的预测，为未标记的目标域生成高质量、精炼的伪标签，从而减轻了单一模型的偏差。然后使用这些鲁棒的伪标签对对比预训练的网络进行微调。从SemanticKITTI到未标记目标数据集（SemanticPOSS、SemanticSlamantic）的适应实验表明，与直接迁移和单模型UDA方法相比，分割精度显著提高。这些结果突出了结合对比预训练和精炼的集成伪标签在弥合复杂域差距方面的有效性，而无需目标域标注。", "summary": "本文针对3D LiDAR语义分割中域偏移导致的性能下降问题，提出了一种新颖的两阶段无监督域适应（UDA）框架。该框架首先利用无监督对比学习对骨干网络进行预训练以学习域不变特征，随后采用多模型伪标签策略，通过聚合多种先进模型的预测生成高质量伪标签，并用其微调预训练网络。实验证明，该方法在多个目标数据集上显著提升了分割精度，有效弥合了域差距。", "keywords": "无监督域适应, 3D LiDAR语义分割, 对比学习, 伪标签, 多模型集成", "comments": "这篇论文的创新点在于结合了无监督对比学习和多模型伪标签策略来解决3D LiDAR语义分割的无监督域适应问题。对比学习确保了学习到的特征具有域不变性，而多模型伪标签通过集成不同架构的优势，有效提高了伪标签的质量，降低了单模型偏差，从而提升了模型的泛化能力。这种两阶段方法为处理3D点云域适应提供了一个有效且无需大量标注的解决方案，对于自动驾驶等实际应用具有重要意义。"}}
{"id": "2503.12358", "title": "IPCGRL: Language-Instructed Reinforcement Learning for Procedural Level Generation", "authors": ["In-Chang Baek", "Sung-Hyun Kim", "Seo-Young Lee", "Dong-Hyeon Kim", "Kyung-Joong Kim"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025", "url": "http://arxiv.org/abs/2503.12358v4", "summary": "Recent research has highlighted the significance of natural language in\nenhancing the controllability of generative models. While various efforts have\nbeen made to leverage natural language for content generation, research on deep\nreinforcement learning (DRL) agents utilizing text-based instructions for\nprocedural content generation remains limited. In this paper, we propose\nIPCGRL, an instruction-based procedural content generation method via\nreinforcement learning, which incorporates a sentence embedding model. IPCGRL\nfine-tunes task-specific embedding representations to effectively compress\ngame-level conditions. We evaluate IPCGRL in a two-dimensional level generation\ntask and compare its performance with a general-purpose embedding method. The\nresults indicate that IPCGRL achieves up to a 21.4% improvement in\ncontrollability and a 17.2% improvement in generalizability for unseen\ninstructions. Furthermore, the proposed method extends the modality of\nconditional input, enabling a more flexible and expressive interaction\nframework for procedural content generation.", "comment": "9 pages, 9 figures, 3 tables, accepted to Conference on Games 2025", "pdf_url": "http://arxiv.org/pdf/2503.12358v4", "cate": "cs.AI", "date": "2025-03-16", "updated": "2025-07-24", "AI": {"title_translation": "IPCGRL：语言指令强化学习用于程序化关卡生成", "tldr": "IPCGRL是一种利用语言指令的强化学习方法，用于程序化关卡生成，显著提升了可控性和泛化能力。", "motivation": "现有研究在利用自然语言增强生成模型可控性方面已取得进展，但将深度强化学习（DRL）智能体与基于文本的指令结合用于程序化内容生成的研究仍然有限。", "method": "本文提出了IPCGRL，一种基于指令的程序化内容生成方法，通过强化学习实现，并整合了句子嵌入模型。IPCGRL微调特定任务的嵌入表示，以有效压缩游戏关卡条件。", "result": "IPCGRL在二维关卡生成任务中表现出色，与通用嵌入方法相比，可控性提高了21.4%，对未见指令的泛化能力提高了17.2%。", "conclusion": "所提出的方法扩展了条件输入的模态，为程序化内容生成提供了一个更灵活和富有表现力的交互框架。", "translation": "近期研究强调了自然语言在增强生成模型可控性方面的重要性。尽管在利用自然语言进行内容生成方面已做出各种努力，但关于深度强化学习（DRL）智能体利用文本指令进行程序化内容生成的研究仍然有限。在本文中，我们提出了IPCGRL，一种通过强化学习实现的基于指令的程序化内容生成方法，该方法整合了一个句子嵌入模型。IPCGRL微调特定任务的嵌入表示，以有效压缩游戏关卡条件。我们在二维关卡生成任务中评估了IPCGRL，并将其性能与通用嵌入方法进行了比较。结果表明，IPCGRL在可控性方面提高了21.4%，对未见指令的泛化能力提高了17.2%。此外，所提出的方法扩展了条件输入的模态，为程序化内容生成提供了一个更灵活和富有表现力的交互框架。", "summary": "本文提出IPCGRL，一种基于语言指令的强化学习方法，旨在解决程序化内容生成中深度强化学习智能体利用文本指令的局限性。IPCGRL通过整合并微调句子嵌入模型来压缩游戏关卡条件。实验结果表明，IPCGRL在二维关卡生成任务中显著提升了可控性（21.4%）和对未见指令的泛化能力（17.2%），从而提供了一个更灵活和富有表现力的内容生成交互框架。", "keywords": "强化学习, 程序化内容生成, 语言指令, 句子嵌入, 可控性", "comments": "IPCGRL创新性地将语言指令与强化学习结合应用于程序化内容生成，通过引入任务特定的句子嵌入，有效提升了生成模型的可控性和泛化能力，填补了该领域的研究空白。其在提升人机交互灵活性方面具有重要意义。"}}
{"id": "2503.22351", "title": "One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images", "authors": ["Byeongjun Kwon", "Munchurl Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (camera-ready version). [Project page]( this https URL )", "url": "http://arxiv.org/abs/2503.22351v2", "summary": "Zero-shot depth estimation (DE) models exhibit strong generalization\nperformance as they are trained on large-scale datasets. However, existing\nmodels struggle with high-resolution images due to the discrepancy in image\nresolutions of training (with smaller resolutions) and inference (for high\nresolutions). Processing them at full resolution leads to decreased estimation\naccuracy on depth with tremendous memory consumption, while downsampling to the\ntraining resolution results in blurred edges in the estimated depth images.\nPrevailing high-resolution depth estimation methods adopt a patch-based\napproach, which introduces depth discontinuity issues when reassembling the\nestimated depth patches, resulting in test-time inefficiency. Additionally, to\nobtain fine-grained depth details, these methods rely on synthetic datasets due\nto the real-world sparse ground truth depth, leading to poor generalizability.\nTo tackle these limitations, we propose Patch Refine Once (PRO), an efficient\nand generalizable tile-based framework. Our PRO consists of two key components:\n(i) Grouped Patch Consistency Training that enhances test-time efficiency while\nmitigating the depth discontinuity problem by jointly processing four\noverlapping patches and enforcing a consistency loss on their overlapping\nregions within a single backpropagation step, and (ii) Bias Free Masking that\nprevents the DE models from overfitting to dataset-specific biases, enabling\nbetter generalization to real-world datasets even after training on synthetic\ndata. Zero-shot evaluations on Booster, ETH3D, Middlebury 2014, and NuScenes\ndemonstrate that our PRO can be seamlessly integrated into existing depth\nestimation models.", "comment": "ICCV 2025 (camera-ready version). [Project\n  page](https://kaist-viclab.github.io/One-Look-is-Enough_site)", "pdf_url": "http://arxiv.org/pdf/2503.22351v2", "cate": "cs.CV", "date": "2025-03-28", "updated": "2025-07-24", "AI": {"title_translation": "一眼即可：一种用于高分辨率图像零样本单目深度估计模型的新型无缝分块细化方法", "tldr": "本文提出了PRO，一个高效且可泛化的基于瓦片的框架，通过分组块一致性训练和无偏掩码，解决零样本单目深度估计模型在高分辨率图像上存在的深度不连续性和泛化能力差的问题，并能无缝集成到现有模型中。", "motivation": "现有零样本深度估计模型在高分辨率图像上表现不佳，原因在于训练和推理图像分辨率不匹配，导致全分辨率处理时精度下降和内存消耗大，下采样时边缘模糊。此外，主流的基于块的方法引入了深度不连续性且效率低下，并因依赖合成数据集而泛化能力差。", "method": "本文提出了Patch Refine Once (PRO) 框架，包含两个关键组件：(i) 分组块一致性训练，通过在单个反向传播步骤中联合处理四个重叠块并在其重叠区域强制执行一致性损失，以提高测试时间效率并减轻深度不连续问题。(ii) 无偏掩码，防止深度估计模型过拟合到特定数据集的偏差，即使在合成数据训练后也能更好地泛化到真实世界数据集。", "result": "在Booster、ETH3D、Middlebury 2014和NuScenes数据集上的零样本评估表明，所提出的PRO框架可以无缝集成到现有的深度估计模型中。", "conclusion": "PRO框架通过其创新的组件，有效解决了现有零样本深度估计模型在高分辨率图像处理中面临的效率、深度不连续性和泛化能力问题。", "translation": "零样本深度估计（DE）模型由于在大型数据集上进行训练，表现出强大的泛化性能。然而，现有模型在高分辨率图像上表现不佳，原因在于训练（分辨率较低）和推理（分辨率较高）的图像分辨率存在差异。以全分辨率处理会导致深度估计精度下降，并消耗大量内存，而下采样到训练分辨率则会导致估计深度图像中边缘模糊。目前流行的高分辨率深度估计方法采用基于块的方法，这在重新组装估计的深度块时引入了深度不连续问题，导致测试时间效率低下。此外，为了获得细粒度的深度细节，这些方法由于真实世界稀疏的地面实况深度而依赖于合成数据集，从而导致泛化能力差。为了解决这些局限性，我们提出了 Patch Refine Once (PRO)，一个高效且可泛化的基于瓦片的框架。我们的 PRO 包含两个关键组件：(i) 分组块一致性训练，通过在单个反向传播步骤中联合处理四个重叠块并在其重叠区域强制执行一致性损失，从而提高测试时间效率并减轻深度不连续问题；以及 (ii) 无偏掩码，防止 DE 模型过拟合到特定数据集的偏差，即使在合成数据训练后也能更好地泛化到真实世界数据集。在 Booster、ETH3D、Middlebury 2014 和 NuScenes 上的零样本评估表明，我们的 PRO 可以无缝集成到现有的深度估计模型中。", "summary": "本文提出了一种名为Patch Refine Once (PRO) 的新型基于瓦片的框架，旨在解决零样本单目深度估计模型在高分辨率图像处理中遇到的挑战。现有方法存在分辨率差异、内存消耗、边缘模糊、深度不连续以及因依赖合成数据而导致的泛化能力差等问题。PRO通过分组块一致性训练（通过处理重叠块提高效率和连续性）和无偏掩码（通过防止过拟合数据集偏差增强真实世界泛化能力）来缓解这些问题。在多个数据集上的评估证实了PRO可以无缝集成到现有模型中。", "keywords": "零样本深度估计, 高分辨率图像, 分块细化, 深度不连续性, 泛化能力", "comments": "该论文的创新之处在于其高效的、基于瓦片的PRO方法，它直接解决了高分辨率零样本深度估计中常见的深度不连续性和泛化能力差的问题。其两个关键组件——分组块一致性训练和无偏掩码——设计精良，能够有效实现这些目标。它能够无缝集成到现有模型中，表明其具有很高的实用价值。"}}
{"id": "2507.17998", "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17998v1", "summary": "Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the\nconvergence of existing solutions or outperform them in various computer vision\ntasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17998v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "点之外的配准：基于格拉斯曼流形测地距离的广义仿射子空间对齐", "tldr": "本文首次推导了格拉斯曼流形上可优化的仿射子空间配准代价函数，通过直接最小化测地距离，实现了全局最优解，并在计算机视觉任务中表现出色。", "motivation": "现有方法在格拉斯曼流形上测量特征间距离时，无法将距离表示为刚体变换的显式函数，导致缺乏可优化的距离函数，从而限制了其在配准问题中的应用。", "method": "本文首次明确推导了关于刚体变换（R和t）的两个格拉斯曼特征之间可优化的代价函数。具体而言，通过严格的数学证明，表明高维线性子空间的基础可以作为代价的显式表示。最终，提出了一种基于变换后的基础的可优化代价函数，可应用于任何仿射子空间的配准问题，并通过直接最小化测地距离来寻找全局最优解。", "result": "与基于向量参数的方法相比，本文方法能够通过直接最小化对表示模糊性不敏感的测地距离来找到全局最优解。所得到的代价函数及其在内点集最大化BnB求解器中的扩展，已被证明可以改善现有解决方案的收敛性或在各种计算机视觉任务中超越它们。", "conclusion": "本文成功推导并提出了一种在格拉斯曼流形上对仿射子空间进行配准的可优化代价函数，该函数能够实现全局最优解，并在多个计算机视觉任务中展现出优越的性能和收敛性。", "translation": "仿射格拉斯曼流形因其在测量线和平面之间距离时的理论精确性而备受青睐。尽管有此优势，现有方法只能测量接近度，而无法将距离作为刚体变换的显式函数。因此，流形上可优化的距离函数仍未得到充分发展，阻碍了其在配准问题中的应用。本文首次明确推导了关于刚体变换（R和t）的两个格拉斯曼特征之间可优化的代价函数。具体而言，我们提出了一个严格的数学证明，表明高维线性子空间的基础可以作为代价的显式表示。最后，我们提出了一种基于变换后的基础的可优化代价函数，可应用于任何仿射子空间的配准问题。与基于向量参数的方法相比，我们的方法能够通过直接最小化对表示模糊性不敏感的测地距离来找到全局最优解。所得到的代价函数及其在内点集最大化BnB求解器中的扩展，已被证明可以改善现有解决方案的收敛性或在各种计算机视觉任务中超越它们。代码可在https://github.com/joomeok/GrassmannRegistration上获取。", "summary": "本文首次在格拉斯曼流形上推导并提出了一种针对仿射子空间配准的可优化代价函数。该方法利用高维线性子空间的基础作为代价的显式表示，并通过直接最小化测地距离，克服了现有方法无法将距离表示为刚体变换显式函数的局限性。实验证明，该方法能找到全局最优解，并显著提升了现有解决方案的收敛性或在多种计算机视觉任务中表现更优。", "keywords": "格拉斯曼流形, 配准, 测地距离, 仿射子空间, 代价函数", "comments": "本文的创新之处在于首次明确推导了格拉斯曼流形上针对刚体变换的可优化代价函数，解决了长期以来阻碍其在配准问题中应用的关键难题。通过直接最小化测地距离，该方法能够找到全局最优解，这在传统基于向量参数的方法中是难以实现的。其在各种计算机视觉任务中的性能提升和收敛性改善，突显了其重要的理论和实践价值。"}}
{"id": "2506.03586", "title": "Beamforming and Resource Allocation for Delay Minimization in RIS-Assisted OFDM Systems", "authors": ["Yu Ma", "Xiao Li", "Chongtao Guo", "Le Liang", "Michail Matthaiou", "Shi Jin"], "categories": ["cs.AI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2506.03586v4", "summary": "This paper investigates a joint beamforming and resource allocation problem\nin downlink reconfigurable intelligent surface (RIS)-assisted orthogonal\nfrequency division multiplexing (OFDM) systems to minimize the average delay,\nwhere data packets for each user arrive at the base station (BS)\nstochastically. The sequential optimization problem is inherently a Markov\ndecision process (MDP), thus falling within the remit of reinforcement\nlearning. To effectively handle the mixed action space and reduce the state\nspace dimensionality, a hybrid deep reinforcement learning (DRL) approach is\nproposed. Specifically, proximal policy optimization (PPO)-Theta is employed to\noptimize the RIS phase shift design, while PPO-N is responsible for subcarrier\nallocation decisions. The active beamforming at the BS is then derived from the\njointly optimized RIS phase shifts and subcarrier allocation decisions. To\nfurther mitigate the curse of dimensionality associated with subcarrier\nallocation, a multi-agent strategy is introduced to optimize the subcarrier\nallocation indicators more efficiently. Moreover, to achieve more adaptive\nresource allocation and accurately capture the network dynamics, key factors\nclosely related to average delay, such as the number of backlogged packets in\nbuffers and current packet arrivals, are incorporated into the state space.\nFurthermore, a transfer learning framework is introduced to enhance the\ntraining efficiency and accelerate convergence. Simulation results demonstrate\nthat the proposed algorithm significantly reduces the average delay, enhances\nresource allocation efficiency, and achieves superior system robustness and\nfairness compared to baseline methods.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2506.03586v4", "cate": "cs.AI", "date": "2025-06-04", "updated": "2025-07-24", "AI": {"title_translation": "RIS辅助OFDM系统中延迟最小化的波束成形与资源分配", "tldr": "本文提出一种混合深度强化学习方法，通过联合优化波束成形和资源分配，在RIS辅助OFDM系统中最小化平均延迟。", "motivation": "在RIS辅助OFDM系统中，由于数据包随机到达基站，导致平均延迟问题，需要对波束成形和资源分配进行联合优化以最小化平均延迟。", "method": "本文将联合波束成形和资源分配问题建模为马尔可夫决策过程（MDP）。为处理混合动作空间和降低状态空间维度，提出了一种混合深度强化学习（DRL）方法。具体而言，采用PPO-Theta优化RIS相移设计，PPO-N负责子载波分配。基站的有源波束成形由RIS相移和子载波分配的联合优化结果推导。为缓解子载波分配的维度灾难，引入多智能体策略。此外，将积压数据包数量和当前数据包到达等关键因素纳入状态空间，以实现更自适应的资源分配。为提高训练效率和加速收敛，引入了迁移学习框架。", "result": "所提出的算法显著降低了平均延迟，提高了资源分配效率，并与基线方法相比，实现了卓越的系统鲁棒性和公平性。", "conclusion": "所提出的混合深度强化学习算法在RIS辅助OFDM系统中，通过有效的波束成形和资源分配，能够显著降低平均延迟，提升系统性能。", "translation": "本文研究了下行链路可重构智能表面（RIS）辅助正交频分复用（OFDM）系统中联合波束成形和资源分配问题，以最小化平均延迟，其中每个用户的数据包随机到达基站（BS）。这个顺序优化问题本质上是一个马尔可夫决策过程（MDP），因此属于强化学习的范畴。为了有效处理混合动作空间并降低状态空间维度，提出了一种混合深度强化学习（DRL）方法。具体来说，采用近端策略优化（PPO）-Theta来优化RIS相移设计，而PPO-N负责子载波分配决策。基站的有源波束成形则从联合优化的RIS相移和子载波分配决策中导出。为了进一步缓解与子载波分配相关的维度灾难，引入了一种多智能体策略以更有效地优化子载波分配指标。此外，为了实现更自适应的资源分配并准确捕获网络动态，将与平均延迟密切相关的关键因素，如缓冲区中积压数据包的数量和当前数据包到达情况，纳入状态空间。此外，引入了迁移学习框架以提高训练效率和加速收敛。仿真结果表明，所提出的算法显著降低了平均延迟，提高了资源分配效率，并且与基线方法相比，实现了卓越的系统鲁棒性和公平性。", "summary": "本文针对RIS辅助OFDM系统中随机数据包到达导致的平均延迟问题，提出了一种混合深度强化学习（DRL）方法。该方法将波束成形和资源分配建模为马尔可夫决策过程，并利用PPO-Theta和PPO-N分别优化RIS相移和子载波分配。通过引入多智能体策略、扩展状态空间以及采用迁移学习，有效处理了混合动作空间、降低了维度并提升了训练效率。仿真结果验证了该算法在降低平均延迟、提高资源分配效率、增强系统鲁棒性和公平性方面的优越性。", "keywords": "RIS辅助OFDM系统, 波束成形, 资源分配, 深度强化学习, 平均延迟", "comments": "本文的创新点在于将混合深度强化学习应用于RIS辅助OFDM系统中的联合波束成形和资源分配问题，特别是在处理随机数据包到达和最小化平均延迟方面。通过引入PPO-Theta和PPO-N的混合DRL框架，以及多智能体策略和迁移学习，有效地解决了复杂动作空间和高维度问题，并提升了训练效率，这对于未来无线通信系统中的智能资源管理具有重要意义。"}}
{"id": "2507.17842", "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17842v1", "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17842v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Shop-R1：通过强化学习奖励大型语言模型以模拟在线购物中的人类行为", "tldr": "Shop-R1是一个新的强化学习框架，通过分解为理由生成和动作预测两个阶段，并设计分层奖励机制，显著提升了大型语言模型在在线购物环境中模拟人类行为的能力。", "motivation": "现有方法通过增强训练数据和监督微调来提升大型语言模型在网络环境中生成“可信的类人”行为的能力，但其性能受限于生成理由的模型本身的推理能力。", "method": "本文提出了Shop-R1，一个新颖的强化学习(RL)框架，旨在增强大型语言模型在在线购物环境中模拟真实人类行为的推理能力。Shop-R1将人类行为模拟任务分解为理由生成和动作预测两个阶段，每个阶段由不同的奖励信号引导。理由生成利用内部模型信号（如logit分布）进行自监督指导。动作预测提出了一种分层奖励结构，具有难度感知缩放，以防止奖励欺骗，并实现细粒度奖励分配，根据高层动作类型和细粒度子动作细节的正确性及难度按比例奖励输出。", "result": "实验结果表明，Shop-R1相比基线方法实现了超过65%的相对改进。", "conclusion": "Shop-R1通过其独特的两阶段强化学习框架和分层奖励机制，有效提升了大型语言模型模拟在线购物中人类行为的推理能力和性能。", "translation": "大型语言模型（LLMs）最近在网络环境中展示出生成“可信的类人”行为的强大潜力。先前的工作探索了通过LLM合成的理由来增强训练数据，并应用监督微调（SFT）来提高推理能力，进而改善下游动作预测。然而，此类方法的性能固有地受限于用于生成理由的模型本身的推理能力。在本文中，我们引入了Shop-R1，一个新颖的强化学习（RL）框架，旨在增强LLMs在在线购物环境中模拟真实人类行为的推理能力。具体来说，Shop-R1将人类行为模拟任务分解为两个阶段：理由生成和动作预测，每个阶段都由不同的奖励信号引导。对于理由生成，我们利用内部模型信号（例如，logit分布）以自监督方式指导推理过程。对于动作预测，我们提出了一种分层奖励结构，具有难度感知缩放，以防止奖励欺骗并实现细粒度奖励分配。这种设计评估了高层动作类型和细粒度子动作细节（属性和值）的正确性，并根据其难度按比例奖励输出。实验结果表明，我们的方法比基线方法获得了超过65%的相对改进。", "summary": "Shop-R1是一个新颖的强化学习框架，旨在提升大型语言模型在在线购物中模拟人类行为的推理能力。它将模拟任务分解为理由生成和动作预测两个阶段，并为每个阶段设计了独特的奖励机制：理由生成采用自监督的内部模型信号，而动作预测则引入了分层且难度感知的奖励结构。实验证明，Shop-R1相较于基线方法取得了超过65%的显著性能提升。", "keywords": "大型语言模型, 强化学习, 人类行为模拟, 在线购物, 理由生成", "comments": "Shop-R1的创新之处在于其将人类行为模拟任务分解为理由生成和动作预测，并为每个阶段设计了精细的强化学习奖励机制。特别是，理由生成中利用内部模型信号进行自监督学习，以及动作预测中引入分层和难度感知奖励，有效地解决了现有方法在推理能力上的局限性，并避免了奖励欺骗，使得模型能够更真实、更细致地模拟人类行为。其在在线购物场景中的应用潜力巨大，为未来LLM在复杂交互环境中的应用开辟了新路径。"}}
{"id": "2507.18606", "title": "Hybrid quantum-classical algorithm for near-optimal planning in POMDPs", "authors": ["Gilberto Cunha", "Alexandra Ramôa", "André Sequeira", "Michael de Oliveira", "Luís Barbosa"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18606v1", "summary": "Reinforcement learning (RL) provides a principled framework for\ndecision-making in partially observable environments, which can be modeled as\nMarkov decision processes and compactly represented through dynamic decision\nBayesian networks. Recent advances demonstrate that inference on sparse\nBayesian networks can be accelerated using quantum rejection sampling combined\nwith amplitude amplification, leading to a computational speedup in estimating\nacceptance probabilities.\\\\ Building on this result, we introduce Quantum\nBayesian Reinforcement Learning (QBRL), a hybrid quantum-classical look-ahead\nalgorithm for model-based RL in partially observable environments. We present a\nrigorous, oracle-free time complexity analysis under fault-tolerant assumptions\nfor the quantum device. Unlike standard treatments that assume a black-box\noracle, we explicitly specify the inference process, allowing our bounds to\nmore accurately reflect the true computational cost. We show that, for\nenvironments whose dynamics form a sparse Bayesian network, horizon-based\nnear-optimal planning can be achieved sub-quadratically faster through\nquantum-enhanced belief updates.\n  Furthermore, we present numerical experiments benchmarking QBRL against its\nclassical counterpart on simple yet illustrative decision-making tasks. Our\nresults offer a detailed analysis of how the quantum computational advantage\ntranslates into decision-making performance, highlighting that the magnitude of\nthe advantage can vary significantly across different deployment settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18606v1", "cate": "quant-ph", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "混合量子-经典算法用于POMDPs中的近最优规划", "tldr": "本文提出了一种混合量子-经典算法QBRL，用于部分可观测马尔可夫决策过程（POMDPs）中的近最优规划，并证明其在特定环境下能实现次二次方的加速。", "motivation": "在部分可观测环境中进行决策（可建模为POMDPs）是强化学习的重要组成部分。鉴于量子计算在稀疏贝叶斯网络推理中已被证明具有加速潜力，本文旨在探索如何将量子计算应用于模型基强化学习，以提高决策规划的计算效率。", "method": "本文引入了量子贝叶斯强化学习（QBRL），这是一种混合量子-经典前瞻算法，用于部分可观测环境中的模型基强化学习。该方法利用量子增强的信念更新，并在容错量子设备假设下进行了严格的、无预言机的时复杂度分析。此外，还通过数值实验将QBRL与其经典对应算法进行了基准测试。", "result": "研究表明，对于动态形成稀疏贝叶网络的环境，通过量子增强的信念更新，基于视野的近最优规划可以实现次二次方的加速。数值实验还揭示了量子计算优势如何转化为决策性能，并指出这种优势的大小在不同部署设置下可能显著不同。", "conclusion": "本研究证明了量子增强的信念更新能够为部分可观测环境中的规划提供显著的计算优势，尤其是在动态形成稀疏贝叶斯网络的环境中。尽管量子优势的程度可能因具体应用场景而异，但其潜力是显而易见的。", "translation": "强化学习（RL）为部分可观测环境中的决策提供了一个原则性框架，这些环境可以建模为马尔可夫决策过程，并通过动态决策贝叶斯网络紧凑表示。最近的进展表明，结合振幅放大和量子拒绝采样可以加速稀疏贝叶网络上的推理，从而在估计接受概率方面实现计算加速。\\n在此基础上，我们引入了量子贝叶斯强化学习（QBRL），这是一种用于部分可观测环境中基于模型的RL的混合量子-经典前瞻算法。我们为量子设备在容错假设下提供了严格的、无预言机的时复杂度分析。与假设黑盒预言机的标准处理不同，我们明确指定了推理过程，使得我们的界限能更准确地反映真实的计算成本。我们表明，对于动态形成稀疏贝斯网络的环境，通过量子增强的信念更新，可以实现基于视野的近最优规划的次二次方加速。\\n此外，我们还通过数值实验，在简单但具有启发性的决策任务上，将QBRL与其经典对应算法进行了基准测试。我们的结果详细分析了量子计算优势如何转化为决策性能，并强调了这种优势的大小在不同部署设置下可能显著不同。", "summary": "本文提出了一种名为量子贝叶斯强化学习（QBRL）的混合量子-经典算法，用于在部分可观测马尔可夫决策过程（POMDPs）中进行近最优规划。该算法利用量子增强的信念更新，并在特定稀疏贝叶斯网络动态环境下，理论上实现了次二次方的计算加速。通过严格的无预言机时间复杂度分析和数值实验，研究不仅验证了量子计算在决策制定中的潜力，也指出其优势程度会随部署环境而变化。", "keywords": "量子强化学习, POMDPs, 混合算法, 贝叶斯网络, 计算加速", "comments": "本文的创新之处在于提出了一个具体的混合量子-经典算法QBRL，将量子计算的加速潜力应用于部分可观测环境中的强化学习问题。其严格的无预言机时间复杂度分析提高了理论分析的准确性。尽管实验是在简单任务上进行，但其结果揭示了量子优势的实际转化及情境依赖性，为未来更复杂应用的研究奠定了基础。"}}
{"id": "2507.16809", "title": "LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs", "authors": ["Da-Chen Lian", "Ri-Sheng Huang", "Pin-Er Chen", "Chunki Lim", "You-Kuan Lin", "Guan-Yu Tseng", "Zi-Cheng Yang", "Zhen-Yu Lin", "Pin-Cheng Chen", "Shu-Kai Hsieh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent, OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each experiment once; statistical tests are needed for strong claims. Revised Sec 6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar", "url": "http://arxiv.org/abs/2507.16809v2", "summary": "We propose LingBench++, a linguistically-informed benchmark and reasoning\nframework designed to evaluate large language models (LLMs) on complex\nlinguistic tasks inspired by the International Linguistics Olympiad (IOL).\nUnlike prior benchmarks that focus solely on final answer accuracy, LingBench++\nprovides structured reasoning traces, stepwise evaluation protocols, and rich\ntypological metadata across over 90 low-resource and cross-cultural languages.\nWe further develop a multi-agent architecture integrating grammatical knowledge\nretrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through\nsystematic comparisons of baseline and our proposed agentic models, we\ndemonstrate that models equipped with external knowledge sources and iterative\nreasoning outperform single-pass approaches in both accuracy and\ninterpretability. LingBench++ offers a comprehensive foundation for advancing\nlinguistically grounded, culturally informed, and cognitively plausible\nreasoning in LLMs.", "comment": "42p, 17f, 10t. Revisions: Merged paragraphs in Intro to emphasize\n  contributions. Clarified benchmark design (Sec 3.5.1). Added single-agent,\n  OpenAI-guided & 6-round experiments (Sec 5.2). Note: we only ran each\n  experiment once; statistical tests are needed for strong claims. Revised Sec\n  6. Added acknowledgements, 2 new co-authors, and corrected typos/grammar", "pdf_url": "http://arxiv.org/pdf/2507.16809v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "LingBench++: 一个用于LLM多步和跨文化推理的语言学知情基准和推理框架", "tldr": "LingBench++是一个新的基准测试和推理框架，旨在评估LLM在复杂语言任务上的表现，特别是在低资源和跨文化语言方面，并通过多智能体架构提高准确性和可解释性。", "motivation": "现有基准测试仅关注最终答案的准确性，而缺乏对LLM在复杂语言任务（特别是多步和跨文化推理）中推理过程的深入评估。", "method": "提出LingBench++，一个语言学知情基准和推理框架，包含结构化推理轨迹、逐步评估协议和超过90种低资源及跨文化语言的类型学元数据。此外，开发了一个整合语法知识检索、工具增强推理和审慎假设检验的多智能体架构。", "result": "配备外部知识源和迭代推理的模型在准确性和可解释性方面均优于单次通过方法。", "conclusion": "LingBench++为推进LLM中基于语言学、文化知情和认知合理性推理提供了全面的基础。", "translation": "我们提出了LingBench++，一个语言学知情基准测试和推理框架，旨在评估大型语言模型（LLM）在受国际语言学奥林匹克（IOL）启发的复杂语言任务上的表现。与之前只关注最终答案准确性的基准测试不同，LingBench++提供了结构化推理轨迹、逐步评估协议以及超过90种低资源和跨文化语言的丰富类型学元数据。我们进一步开发了一种多智能体架构，该架构集成了语法知识检索、工具增强推理和审慎假设检验。通过对基线模型和我们提出的智能体模型的系统比较，我们证明了配备外部知识源和迭代推理的模型在准确性和可解释性方面均优于单次通过方法。LingBench++为推进LLM中基于语言学、文化知情和认知合理性推理提供了全面的基础。", "summary": "LingBench++是一个新颖的语言学知情基准测试和推理框架，旨在评估大型语言模型（LLM）在复杂多步和跨文化语言任务上的性能。它通过提供结构化推理轨迹、逐步评估协议和丰富的类型学元数据，弥补了现有基准只关注最终答案准确性的不足。该框架还引入了一个多智能体架构，结合了知识检索、工具增强推理和假设检验。实验结果表明，该框架下的模型在准确性和可解释性上均优于传统方法，为LLM的语言学和文化推理能力提供了坚实基础。", "keywords": "语言学基准, LLM, 跨文化推理, 多智能体, 低资源语言", "comments": "LingBench++的创新之处在于其对LLM推理过程的细致评估，特别是在多步和跨文化语言任务中，这超越了传统基准只关注最终结果的局限性。其引入的多智能体架构结合了外部知识和迭代推理，对于提高LLM在复杂语言理解上的性能和透明度具有重要意义。对低资源语言的关注也极具价值。"}}
{"id": "2507.16680", "title": "Latent Space Alignment for AI-Native MIMO Semantic Communications", "authors": ["Mario Edoardo Pandolfo", "Simone Fiorellino", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Proc. of IEEE IJCNN 2025", "url": "http://arxiv.org/abs/2507.16680v2", "summary": "Semantic communications focus on prioritizing the understanding of the\nmeaning behind transmitted data and ensuring the successful completion of tasks\nthat motivate the exchange of information. However, when devices rely on\ndifferent languages, logic, or internal representations, semantic mismatches\nmay occur, potentially hindering mutual understanding. This paper introduces a\nnovel approach to addressing latent space misalignment in semantic\ncommunications, exploiting multiple-input multiple-output (MIMO)\ncommunications. Specifically, our method learns a MIMO precoder/decoder pair\nthat jointly performs latent space compression and semantic channel\nequalization, mitigating both semantic mismatches and physical channel\nimpairments. We explore two solutions: (i) a linear model, optimized by solving\na biconvex optimization problem via the alternating direction method of\nmultipliers (ADMM); (ii) a neural network-based model, which learns semantic\nMIMO precoder/decoder under transmission power budget and complexity\nconstraints. Numerical results demonstrate the effectiveness of the proposed\napproach in a goal-oriented semantic communication scenario, illustrating the\nmain trade-offs between accuracy, communication burden, and complexity of the\nsolutions.", "comment": "Proc. of IEEE IJCNN 2025", "pdf_url": "http://arxiv.org/pdf/2507.16680v2", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "AI原生MIMO语义通信的潜在空间对齐", "tldr": "本文提出了一种新颖的方法，利用MIMO通信解决语义通信中的潜在空间错位问题，通过学习MIMO预编码器/解码器对来同时进行潜在空间压缩和语义信道均衡，以减轻语义不匹配和物理信道损伤。", "motivation": "语义通信关注传输数据背后的意义理解和任务完成，但当设备使用不同语言、逻辑或内部表示时，可能发生语义不匹配，阻碍相互理解。", "method": "本文提出了一种利用多输入多输出（MIMO）通信解决语义通信中潜在空间错位的新方法。具体而言，该方法学习一个MIMO预编码器/解码器对，该对联合执行潜在空间压缩和语义信道均衡，从而减轻语义不匹配和物理信道损伤。文中探讨了两种解决方案：(i) 线性模型，通过交替方向乘子法（ADMM）求解双凸优化问题进行优化；(ii) 基于神经网络的模型，在传输功率预算和复杂度约束下学习语义MIMO预编码器/解码器。", "result": "数值结果表明，在面向目标的语义通信场景中，所提出的方法是有效的，并展示了准确性、通信负担和解决方案复杂度之间的主要权衡。", "conclusion": "该研究成功地提出并验证了一种利用MIMO技术解决语义通信中潜在空间对齐问题的新方法，有效减轻了语义不匹配和物理信道损伤，并揭示了性能与资源之间的权衡。", "translation": "语义通信侧重于优先理解传输数据背后的含义，并确保促使信息交换的任务成功完成。然而，当设备依赖不同的语言、逻辑或内部表示时，可能会发生语义不匹配，从而可能阻碍相互理解。本文介绍了一种解决语义通信中潜在空间错位的新颖方法，该方法利用了多输入多输出（MIMO）通信。具体而言，我们的方法学习一个MIMO预编码器/解码器对，该对联合执行潜在空间压缩和语义信道均衡，从而减轻语义不匹配和物理信道损伤。我们探讨了两种解决方案：(i) 线性模型，通过交替方向乘子法（ADMM）求解双凸优化问题进行优化；(ii) 基于神经网络的模型，在传输功率预算和复杂度约束下学习语义MIMO预编码器/解码器。数值结果表明，所提出的方法在面向目标的语义通信场景中是有效的，并说明了准确性、通信负担和解决方案复杂度之间的主要权衡。", "summary": "本文提出了一种新颖的AI原生MIMO语义通信方法，旨在解决设备间潜在空间错位导致的语义不匹配问题。通过设计MIMO预编码器/解码器对，该方法能够同时实现潜在空间压缩和语义信道均衡，从而减轻语义不匹配和物理信道损伤。研究提出了线性模型（通过ADMM优化）和神经网络模型两种实现方案。数值结果验证了该方法在目标导向语义通信中的有效性，并分析了准确性、通信负担和复杂度的权衡。", "keywords": "语义通信, 潜在空间对齐, MIMO, 预编码器/解码器, 信道均衡", "comments": "本文的创新点在于将MIMO技术引入语义通信领域，以解决潜在空间错位和物理信道损伤的双重问题。通过联合优化潜在空间压缩和信道均衡，该方法为提升语义通信的效率和准确性提供了新的思路。同时，提出线性和神经网络两种解决方案，展现了方法的灵活性和普适性。"}}
{"id": "2507.17963", "title": "Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA", "authors": ["Rameen Abdal", "Or Patashnik", "Ekaterina Deyneka", "Hao Chen", "Aliaksandr Siarohin", "Sergey Tulyakov", "Daniel Cohen-Or", "Kfir Aberman"], "categories": ["cs.GR", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Project Page and Video : this https URL", "url": "http://arxiv.org/abs/2507.17963v1", "summary": "Recent advances in text-to-video generation have enabled high-quality\nsynthesis from text and image prompts. While the personalization of dynamic\nconcepts, which capture subject-specific appearance and motion from a single\nvideo, is now feasible, most existing methods require per-instance fine-tuning,\nlimiting scalability. We introduce a fully zero-shot framework for dynamic\nconcept personalization in text-to-video models. Our method leverages\nstructured 2x2 video grids that spatially organize input and output pairs,\nenabling the training of lightweight Grid-LoRA adapters for editing and\ncomposition within these grids. At inference, a dedicated Grid Fill module\ncompletes partially observed layouts, producing temporally coherent and\nidentity preserving outputs. Once trained, the entire system operates in a\nsingle forward pass, generalizing to previously unseen dynamic concepts without\nany test-time optimization. Extensive experiments demonstrate high-quality and\nconsistent results across a wide range of subjects beyond trained concepts and\nediting scenarios.", "comment": "Project Page and Video :\n  https://snap-research.github.io/zero-shot-dynamic-concepts/", "pdf_url": "http://arxiv.org/pdf/2507.17963v1", "cate": "cs.GR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于网格LoRA的零样本动态概念个性化", "tldr": "本文提出了一种零样本动态概念个性化框架，利用Grid-LoRA适配器和Grid Fill模块，在不进行测试时优化的情况下，实现高质量、时间连贯的文本到视频生成。", "motivation": "现有的文本到视频动态概念个性化方法通常需要对每个实例进行微调，这限制了它们的可扩展性。", "method": "本文引入了一个零样本框架，利用结构化的2x2视频网格来空间组织输入和输出对。通过训练轻量级的Grid-LoRA适配器实现网格内的编辑和合成。在推理阶段，一个专门的Grid Fill模块补全部分观察到的布局，生成时间连贯且身份保留的输出。整个系统在训练完成后只需一次前向传播即可运行。", "result": "广泛的实验表明，该方法在超出训练概念和编辑场景的广泛主体上，都能产生高质量和一致的结果。", "conclusion": "该零样本框架能够有效地对动态概念进行个性化，且无需测试时优化，展现出良好的泛化能力和高质量的生成效果。", "translation": "文本到视频生成领域的最新进展已经能够从文本和图像提示中合成高质量内容。虽然捕获特定主体外观和运动的动态概念个性化（从单个视频中获取）现在已可行，但大多数现有方法需要对每个实例进行微调，这限制了可扩展性。我们引入了一个完全零样本的框架，用于文本到视频模型中的动态概念个性化。我们的方法利用结构化的2x2视频网格，以空间方式组织输入和输出对，从而能够训练轻量级的Grid-LoRA适配器，用于这些网格内的编辑和合成。在推理时，一个专门的Grid Fill模块完成部分观察到的布局，生成时间连贯且身份保留的输出。一旦训练完成，整个系统只需一次前向传播即可运行，无需任何测试时优化即可泛化到以前未见的动态概念。广泛的实验证明了在超出训练概念和编辑场景的广泛主体上，该方法都能产生高质量和一致的结果。", "summary": "本文提出了一种名为Grid-Based LoRA的零样本框架，用于文本到视频模型中的动态概念个性化。该方法通过利用2x2视频网格来结构化输入输出，并训练轻量级Grid-LoRA适配器进行编辑和合成。在推理时，Grid Fill模块确保生成的时间连贯和身份保留的视频。该系统在训练后无需测试时优化即可泛化到新概念，并在广泛的主题和场景中展示了高质量和一致的生成效果，解决了现有方法可扩展性受限的问题。", "keywords": "零样本个性化, 动态概念, 文本到视频生成, Grid-LoRA, 视频网格", "comments": "该论文的创新点在于提出了一个完全零样本的动态概念个性化框架，通过引入结构化的2x2视频网格和轻量级的Grid-LoRA适配器，以及推理时的Grid Fill模块，有效解决了现有方法需要每实例微调的扩展性问题。其在不进行测试时优化的情况下实现高质量、时间连贯的生成，具有显著的实用价值和广阔的应用前景。"}}
{"id": "2504.14928", "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Paper URL: this https URL Presentation Video: this https URL", "url": "http://arxiv.org/abs/2504.14928v2", "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "comment": "Paper URL: https://aclanthology.org/2025.acl-long.1576/; Presentation\n  Video: https://www.youtube.com/watch?v=j63ooKE50I0", "pdf_url": "http://arxiv.org/pdf/2504.14928v2", "cate": "cs.AI", "date": "2025-04-21", "updated": "2025-07-24", "AI": {"title_translation": "EducationQ：通过多智能体对话框架评估大型语言模型的教学能力", "tldr": "EducationQ是一个多智能体对话框架，用于评估大型语言模型的教学能力。研究发现，教学效果与模型规模或通用推理能力并非线性相关，一些小型开源模型在教学方面甚至优于大型商业模型，这表明LLM作为教师需要专门的优化。", "motivation": "评估大型语言模型（LLMs）的教学能力面临挑战，因为师生互动过程资源密集、情境依赖且方法复杂。", "method": "本文引入了EducationQ，一个多智能体对话框架，通过模拟动态教育场景来高效评估LLM的教学能力，该框架包含教学、学习和评估专用智能体。研究测试了来自OpenAI、Meta、Google、Anthropic等主要AI组织的14个LLM，使用了1,498个问题，涵盖13个学科和10个难度级别。评估采用混合方法，结合定量指标、定性分析和专家案例研究。", "result": "研究发现，LLM的教学效果与模型规模或通用推理能力之间没有线性关系，一些较小的开源模型在教学情境中表现优于大型商业模型。通过混合方法评估，识别出顶尖模型所采用的独特教学优势（例如，复杂的提问策略、自适应反馈机制）。人类专家评估与自动化定性分析的有效教学行为一致性达到78%，验证了该方法。", "conclusion": "LLM作为教师需要超越简单规模化的专业优化，未来的教育AI应优先对特定教学效果进行有针对性的增强。", "translation": "大型语言模型（LLMs）正日益成为教育工具，然而，由于师生互动过程的资源密集性、情境依赖性及方法复杂性，评估其教学能力仍然充满挑战。我们引入了EducationQ，一个多智能体对话框架，通过模拟动态教育场景高效评估教学能力，该框架包含专门用于教学、学习和评估的智能体。我们对来自主要AI组织（OpenAI、Meta、Google、Anthropic等）的14个LLM进行了测试，涉及1,498个问题，涵盖13个学科和10个难度级别。结果显示，教学效果与模型规模或通用推理能力之间没有线性关系——一些小型开源模型在教学情境中甚至优于大型商业模型。这一发现突显了当前评估中存在的一个关键缺陷，即过度优先考虑知识回忆而非交互式教学法。我们的混合方法评估，结合定量指标、定性分析和专家案例研究，识别出顶尖模型所采用的独特教学优势（例如，复杂的提问策略、自适应反馈机制）。人类专家评估与我们对有效教学行为的自动化定性分析显示出78%的一致性，验证了我们的方法。EducationQ表明，作为教师的LLM需要超越简单规模化的专业优化，这提示下一代教育AI应优先对特定教学效果进行有针对性的增强。", "summary": "本文介绍了EducationQ，一个用于评估大型语言模型（LLMs）教学能力的多智能体对话框架。该框架通过模拟动态教育场景，高效测试了14个LLM在多学科和多难度级别问题上的表现。研究发现，LLM的教学效果与模型规模或通用推理能力并非线性相关，甚至有小型开源模型表现优于大型商业模型。这强调了当前评估中忽视互动教学法的缺陷。通过混合方法评估，研究识别出优秀模型的独特教学策略，并得到人类专家的验证。结论指出，LLM作为教师需要专门的优化，未来的教育AI应侧重于提升具体的教学效果。", "keywords": "LLM, 教学能力, 多智能体对话, 评估, 教育AI", "comments": "该论文的创新之处在于提出了一个多智能体对话框架EducationQ，有效地模拟了复杂的师生互动，从而解决了LLM教学能力评估的难题。其重要性在于，通过实证研究挑战了“越大越好”的普遍观念，揭示了LLM教学能力并非简单与模型规模或通用推理能力线性相关，这对于未来教育AI的发展具有指导意义，即应更注重教学策略的精细化优化而非盲目扩大模型规模。该研究为如何更科学、更有效地评估和提升LLM的教学能力提供了新的视角和方法。"}}
{"id": "2507.18541", "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping", "authors": ["Chong Cheng", "Zijian Wang", "Sicheng Yu", "Yu Hu", "Nanjie Yao", "Hao Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18541v1", "summary": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D\nrepresentation. Its effectiveness largely depends on precise camera poses and\naccurate point cloud initialization, which are often derived from pretrained\nMulti-View Stereo (MVS) models. However, in unposed reconstruction task from\nhundreds of outdoor images, existing MVS models may struggle with memory limits\nand lose accuracy as the number of input images grows. To address this\nlimitation, we propose a novel unposed 3DGS reconstruction framework that\nintegrates pretrained MVS priors with the probabilistic Procrustes mapping\nstrategy. The method partitions input images into subsets, maps submaps into a\nglobal space, and jointly optimizes geometry and poses with 3DGS. Technically,\nwe formulate the mapping of tens of millions of point clouds as a probabilistic\nProcrustes problem and solve a closed-form alignment. By employing\nprobabilistic coupling along with a soft dustbin mechanism to reject uncertain\ncorrespondences, our method globally aligns point clouds and poses within\nminutes across hundreds of images. Moreover, we propose a joint optimization\nframework for 3DGS and camera poses. It constructs Gaussians from\nconfidence-aware anchor points and integrates 3DGS differentiable rendering\nwith an analytical Jacobian to jointly refine scene and poses, enabling\naccurate reconstruction and pose estimation. Experiments on Waymo and KITTI\ndatasets show that our method achieves accurate reconstruction from unposed\nimage sequences, setting a new state of the art for unposed 3DGS\nreconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18541v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "无姿态3DGS重建与概率普罗克鲁斯特映射", "tldr": "本文提出了一种新的无姿态3DGS重建框架，通过结合预训练MVS先验和概率普罗克鲁斯特映射策略，解决了现有MVS模型在处理大量无姿态室外图像时内存限制和精度下降的问题，并在Waymo和KITTI数据集上实现了最先进的重建效果。", "motivation": "3D高斯溅射（3DGS）的有效性高度依赖于精确的相机姿态和准确的点云初始化，这些通常来自预训练的多视角立体（MVS）模型。然而，在处理数百张室外无姿态图像的重建任务中，现有MVS模型可能面临内存限制，并且随着输入图像数量的增加，精度会下降。", "method": "本文提出了一种新颖的无姿态3DGS重建框架，该框架将预训练的MVS先验与概率普罗克鲁斯特映射策略相结合。该方法将输入图像划分为子集，将子图映射到全局空间，并与3DGS共同优化几何和姿态。具体而言，将数千万点云的映射公式化为概率普罗克鲁斯特问题，并求解闭式对齐。通过采用概率耦合和软垃圾桶机制来拒绝不确定的对应关系，该方法可以在数分钟内全局对齐数百张图像的点云和姿态。此外，还提出了一个3DGS和相机姿态的联合优化框架，该框架从置信度感知的锚点构建高斯，并将3DGS可微分渲染与解析雅可比矩阵相结合，共同细化场景和姿态，从而实现准确的重建和姿态估计。", "result": "在Waymo和KITTI数据集上的实验表明，本文方法能够从无姿态图像序列中实现精确重建，为无姿态3DGS重建设定了新的最先进水平。", "conclusion": "本文提出的无姿态3DGS重建框架通过创新的概率普罗克鲁斯特映射和联合优化策略，有效克服了现有方法在处理大规模无姿态室外图像时的挑战，成功实现了高精度重建和姿态估计，达到了领域内的最新技术水平。", "translation": "3D高斯溅射（3DGS）已成为3D表示的核心技术。其有效性在很大程度上取决于精确的相机姿态和准确的点云初始化，这些通常源自预训练的多视角立体（MVS）模型。然而，在从数百张室外图像进行无姿态重建的任务中，现有MVS模型可能面临内存限制，并且随着输入图像数量的增加，精度会下降。为了解决这一限制，我们提出了一种新颖的无姿态3DGS重建框架，该框架将预训练的MVS先验与概率普罗克鲁斯特映射策略相结合。该方法将输入图像划分为子集，将子图映射到全局空间，并与3DGS共同优化几何和姿态。从技术上讲，我们将数千万点云的映射公式化为概率普罗克鲁斯特问题，并求解闭式对齐。通过采用概率耦合以及软垃圾桶机制来拒绝不确定的对应关系，我们的方法可以在数分钟内全局对齐数百张图像的点云和姿态。此外，我们提出了一个3DGS和相机姿态的联合优化框架。它从置信度感知的锚点构建高斯，并将3DGS可微分渲染与解析雅可比矩阵相结合，共同细化场景和姿态，从而实现准确的重建和姿态估计。在Waymo和KITTI数据集上的实验表明，我们的方法能够从无姿态图像序列中实现精确重建，为无姿态3DGS重建设定了新的最先进水平。", "summary": "本文针对3DGS在处理大量无姿态室外图像时面临的姿态和初始化挑战，提出了一种结合预训练MVS先验和概率普罗克鲁斯特映射的新型无姿态3DGS重建框架。该框架通过将输入图像划分子集、映射子图到全局空间并联合优化几何和姿态，有效解决了内存和精度问题。其核心在于将点云映射建模为概率普罗克鲁斯特问题，并利用概率耦合和软垃圾桶机制实现快速全局对齐。此外，还引入了3DGS与相机姿态的联合优化，通过置信度锚点构建高斯并利用可微分渲染和解析雅可比矩阵共同精炼场景和姿态。实验证明，该方法在Waymo和KITTI数据集上实现了最先进的精确重建。", "keywords": "3DGS, 无姿态重建, 概率普罗克鲁斯特, 点云对齐, 相机姿态估计", "comments": "该论文的创新之处在于其对大规模无姿态室外图像进行3DGS重建的能力，这在实际应用中非常重要。通过将MVS先验与鲁棒的概率普罗克鲁斯特映射相结合，并引入精巧的软垃圾桶机制来处理不确定对应关系，有效地克服了传统方法在内存和精度上的限制。同时，联合优化3DGS和相机姿态的框架进一步提升了重建和姿态估计的准确性，为该领域树立了新的标杆。"}}
{"id": "2506.16297", "title": "SyncMapV2: Robust and Adaptive Unsupervised Segmentation", "authors": ["Heng Zhang", "Zikang Wan", "Danilo Vasconcellos Vargas"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16297v3", "summary": "Human vision excels at segmenting visual cues without the need for explicit\ntraining, and it remains remarkably robust even as noise severity increases. In\ncontrast, existing AI algorithms struggle to maintain accuracy under similar\nconditions. Here, we present SyncMapV2, the first to solve unsupervised\nsegmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal\ndrop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop\nobserved in SOTA methods. This superior performance extends across various\ntypes of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur\n(7.0% vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust\ntraining, supervision, or loss functions. It is based on a learning paradigm\nthat uses self-organizing dynamical equations combined with concepts from\nrandom networks. Moreover, unlike conventional methods that require\nre-initialization for each new input, SyncMapV2 adapts online, mimicking the\ncontinuous adaptability of human vision. Thus, we go beyond the accurate and\nrobust results, and present the first algorithm that can do all the above\nonline, adapting to input rather than re-initializing. In adaptability tests,\nSyncMapV2 demonstrates near-zero performance degradation, which motivates and\nfosters a new generation of robust and adaptive intelligence in the near\nfuture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16297v3", "cate": "cs.CV", "date": "2025-06-19", "updated": "2025-07-24", "AI": {"title_translation": "SyncMapV2：鲁棒自适应无监督分割", "tldr": "SyncMapV2是首个在无监督分割中实现最先进鲁棒性和在线自适应性的算法，无需特殊训练。", "motivation": "现有AI算法在噪声和干扰条件下进行无监督分割时表现不佳，与人类视觉的鲁棒性形成鲜明对比。", "method": "SyncMapV2基于一种学习范式，结合了自组织动力学方程和随机网络概念。它无需鲁棒训练、监督或损失函数，并能在线自适应，无需为每个新输入重新初始化。", "result": "SyncMapV2在数字损坏下mIoU下降仅0.01%，而SOTA方法下降23.8%。在噪声（7.3% vs 37.7%）、天气（7.5% vs 33.8%）和模糊（7.0% vs 29.5%）等多种损坏类型下表现优越。在适应性测试中，性能退化接近零。", "conclusion": "SyncMapV2是首个实现准确、鲁棒且在线自适应无监督分割的算法，有望推动未来鲁棒和自适应智能的发展。", "translation": "人类视觉在无需显式训练的情况下，擅长分割视觉线索，即使噪声严重程度增加，其鲁棒性依然显著。相比之下，现有AI算法在类似条件下难以保持准确性。本文提出了SyncMapV2，首次解决了无监督分割问题，并达到了最先进的鲁棒性。在数字损坏下，SyncMapV2的mIoU下降仅为0.01%，而现有最先进方法下降23.8%。这种卓越的性能延伸到各种类型的损坏：噪声（7.3% vs 37.7%）、天气（7.5% vs 33.8%）和模糊（7.0% vs 29.5%）。值得注意的是，SyncMapV2在没有任何鲁棒训练、监督或损失函数的情况下完成了这一切。它基于一种利用自组织动力学方程结合随机网络概念的学习范式。此外，与需要为每个新输入重新初始化的传统方法不同，SyncMapV2能够在线自适应，模仿人类视觉的持续适应性。因此，我们超越了准确和鲁棒的结果，提出了第一个能够在线完成上述所有任务，适应输入而非重新初始化的算法。在适应性测试中，SyncMapV2表现出接近零的性能退化，这激励并促进了未来新一代鲁棒和自适应智能的发展。", "summary": "SyncMapV2是一种创新的无监督分割算法，解决了现有AI在噪声和干扰下性能下降的问题。它通过结合自组织动力学方程和随机网络概念，实现了卓越的鲁棒性和在线自适应能力，性能显著优于现有最先进方法，且无需传统训练、监督或损失函数。该算法能够像人类视觉一样持续适应新输入，有望推动未来智能系统的发展。", "keywords": "无监督分割, 鲁棒性, 自适应, SyncMapV2, 自组织", "comments": "SyncMapV2的创新之处在于其无监督、无损失函数、无鲁棒训练的特性，却能实现SOTA的鲁棒性。其在线自适应能力模仿人类视觉，是未来AI发展的重要方向。这项工作为构建更通用、更鲁棒的AI系统提供了新的思路，特别是在面对未知或动态环境时具有巨大潜力。"}}
{"id": "2507.17875", "title": "Trusted Data Fusion, Multi-Agent Autonomy, Autonomous Vehicles", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17875v1", "summary": "Multi-agent collaboration enhances situational awareness in intelligence,\nsurveillance, and reconnaissance (ISR) missions. Ad hoc networks of unmanned\naerial vehicles (UAVs) allow for real-time data sharing, but they face security\nchallenges due to their decentralized nature, making them vulnerable to\ncyber-physical attacks. This paper introduces a trust-based framework for\nassured sensor fusion in distributed multi-agent networks, utilizing a hidden\nMarkov model (HMM)-based approach to estimate the trustworthiness of agents and\ntheir provided information in a decentralized fashion. Trust-informed data\nfusion prioritizes fusing data from reliable sources, enhancing resilience and\naccuracy in contested environments. To evaluate the assured sensor fusion under\nattacks on system/mission sensing, we present a novel multi-agent aerial\ndataset built from the Unreal Engine simulator. We demonstrate through case\nstudies improved ISR performance and an ability to detect malicious actors in\nadversarial settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17875v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "可信数据融合、多智能体自主性、自动驾驶汽车", "tldr": "本文提出了一种基于信任的框架，用于分布式多智能体网络中的传感器融合，利用隐马尔可夫模型（HMM）评估智能体的可信度，以提高情报、监视和侦察（ISR）任务在对抗环境中的性能和安全性。", "motivation": "多智能体协作能增强情报、监视和侦察（ISR）任务的态势感知能力。然而，无人机（UAV）的自组织网络因其去中心化特性，面临网络物理攻击的安全挑战。", "method": "本文引入了一个基于信任的框架，用于分布式多智能体网络中的可靠传感器融合。该框架利用隐马尔可夫模型（HMM）以去中心化的方式估计智能体及其提供信息的信任度。信任感知的数据融合优先融合来自可靠来源的数据，以增强在对抗环境中的弹性和准确性。为评估系统/任务感知受攻击时的传感器融合，本文构建了一个基于虚幻引擎模拟器的新型多智能体空中数据集。", "result": "通过案例研究，本文展示了改进的ISR性能以及在对抗环境中检测恶意行为者的能力。", "conclusion": "本文提出的基于信任的传感器融合框架能够有效提高多智能体ISR任务在对抗环境中的性能和安全性，并能检测恶意行为者。", "translation": "多智能体协作增强了情报、监视和侦察（ISR）任务中的态势感知能力。无人机（UAV）的自组织网络允许实时数据共享，但由于其去中心化性质，它们面临安全挑战，使其容易受到网络物理攻击。本文引入了一个基于信任的框架，用于分布式多智能体网络中的可靠传感器融合，利用隐马尔可夫模型（HMM）方法以去中心化的方式估计智能体及其提供信息的信任度。信任感知的数据融合优先融合来自可靠来源的数据，从而增强了在对抗环境中的弹性和准确性。为了评估在系统/任务感知受攻击情况下的可靠传感器融合，我们展示了一个基于虚幻引擎模拟器构建的新型多智能体空中数据集。我们通过案例研究证明了改进的ISR性能以及在对抗环境中检测恶意行为者的能力。", "summary": "本文提出了一种用于分布式多智能体网络中可靠传感器融合的信任框架，旨在解决无人机自组织网络面临的网络安全挑战。该框架利用隐马尔可夫模型（HMM）去中心化地评估智能体及其数据的可信度，并通过信任感知的融合优先处理可靠数据。研究通过一个基于虚幻引擎构建的新数据集进行了评估，结果表明该方法能提高情报、监视和侦察（ISR）任务的性能，并在对抗环境中有效检测恶意行为者。", "keywords": "信任框架, 数据融合, 多智能体系统, 隐马尔可夫模型, 无人机", "comments": "本文的创新点在于提出了一个基于HMM的去中心化信任框架，用于多智能体网络中的传感器融合，有效解决了去中心化网络的安全性和数据可信度问题。通过优先融合来自可靠来源的数据，显著提高了ISR任务在对抗环境中的弹性和准确性，并能识别恶意行为者。其重要性在于为未来安全、自主的多智能体系统提供了解决方案。"}}
{"id": "2507.18194", "title": "Covert Communications in MEC-Based Networked ISAC Systems Towards Low-Altitude Economy", "authors": ["Weihao Mao", "Yang Lu", "Bo Ai", "Tony Q. S. Quek"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18194v1", "summary": "Low-altitude economy (LAE) is an emerging business model, which heavily\nrelies on integrated sensing and communications (ISAC), mobile edge computing\n(MEC), and covert communications. This paper investigates the convert\ntransmission design in MEC-based networked ISAC systems towards LAE, where an\nMEC server coordinates multiple access points to simultaneously receive\ncomputation tasks from multiple unmanned aerial vehicles (UAVs), locate a\ntarget in a sensing area, and maintain UAVs' covert transmission against\nmultiple wardens. We first derive closed-form expressions for the detection\nerror probability (DEP) at wardens. Then, we formulate a total energy\nconsumption minimization problem by optimizing communication, sensing, and\ncomputation resources as well as UAV trajectories, subject to the requirements\non quality of MEC services, DEP, and radar signal-to-interference-and-noise\nratio, and the causality of UAV trajectories. An alternating optimization based\nalgorithm is proposed to handle the considered problem, which decomposes it\ninto two subproblems: joint optimization of communication, sensing, and\ncomputation resources, and UAV trajectory optimization. The former is addressed\nby a successive convex approximation based algorithm, while the latter is\nsolved via a trust-region based algorithm. Simulations validate the\neffectiveness of the proposed algorithm compared with various benchmarks, and\nreveal the trade-offs among communication, sensing, and computation in LAE\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18194v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "迈向低空经济的MEC网络化ISAC系统中的隐蔽通信", "tldr": "本文研究了面向低空经济的MEC网络化ISAC系统中无人机隐蔽通信设计，通过优化通信、感知和计算资源以及无人机轨迹，最小化总能耗，并提出了基于交替优化的算法来解决此问题，仿真验证了其有效性。", "motivation": "低空经济（LAE）作为一种新兴的商业模式，其发展严重依赖于集成感知与通信（ISAC）、移动边缘计算（MEC）和隐蔽通信等关键技术。本文旨在为低空经济背景下的MEC网络化ISAC系统研究隐蔽传输设计，以协调无人机任务、目标定位并对抗侦察员的检测。", "method": "本文首先推导了侦察员处检测错误概率（DEP）的闭式表达式。随后，通过联合优化通信、感知和计算资源以及无人机轨迹，建立了一个以最小化总能耗为目标的优化问题，该问题受限于MEC服务质量、DEP、雷达信干噪比和无人机轨迹的因果关系等要求。为解决此复杂问题，提出了一种基于交替优化的算法，将其分解为通信、感知和计算资源的联合优化子问题（通过连续凸近似算法解决）和无人机轨迹优化子问题（通过信任域算法解决）。", "result": "仿真结果验证了所提出的交替优化算法相对于各种基准的有效性，并揭示了在低空经济系统中通信、感知和计算资源之间存在的关键权衡关系。", "conclusion": "本文成功设计并实现了MEC网络化ISAC系统中面向低空经济的隐蔽传输方案，通过联合优化资源分配和无人机轨迹，有效降低了系统总能耗，并在确保隐蔽性、服务质量和感知性能之间取得了平衡。研究结果为低空经济中无人机系统的可靠和安全运行提供了重要指导。", "translation": "低空经济（LAE）是一种新兴的商业模式，其严重依赖于集成感知与通信（ISAC）、移动边缘计算（MEC）和隐蔽通信。本文研究了面向低空经济的MEC网络化ISAC系统中的隐蔽传输设计，其中MEC服务器协调多个接入点同时接收来自多个无人机（UAV）的计算任务，定位感知区域中的目标，并维持无人机对抗多个侦察员的隐蔽传输。我们首先推导了侦察员处检测错误概率（DEP）的闭式表达式。然后，通过优化通信、感知和计算资源以及无人机轨迹，建立了一个总能耗最小化问题，该问题受限于MEC服务质量、DEP、雷达信干噪比以及无人机轨迹的因果关系等要求。提出了一种基于交替优化的算法来处理所考虑的问题，该算法将其分解为两个子问题：通信、感知和计算资源的联合优化，以及无人机轨迹优化。前者通过基于连续凸近似的算法解决，而后者通过基于信任域的算法解决。仿真验证了所提算法与各种基准相比的有效性，并揭示了低空经济系统中通信、感知和计算之间的权衡。", "summary": "本文针对低空经济（LAE）背景下的MEC网络化集成感知与通信（ISAC）系统，研究了无人机隐蔽传输的设计。研究场景涉及MEC服务器协调接入点，同时处理无人机计算任务、目标定位并确保隐蔽通信对抗侦察员。作者推导了检测错误概率的闭式表达式，并构建了一个以通信、感知、计算资源和无人机轨迹为优化变量的总能耗最小化问题，同时满足多项服务质量和系统性能约束。为解决此复杂问题，提出了一种基于交替优化的算法，将其拆分为资源联合优化和无人机轨迹优化两个子问题，并分别采用连续凸近似和信任域算法求解。仿真结果验证了算法的有效性，并揭示了LAE系统中各资源间的权衡关系。", "keywords": "低空经济, 隐蔽通信, 集成感知与通信, 移动边缘计算, 无人机", "comments": "本文将隐蔽通信、ISAC和MEC三个前沿技术融合，并应用于新兴的低空经济场景，具有较强的创新性。其通过联合优化多种异构资源（通信、感知、计算）和无人机轨迹，以最小化能耗为目标，解决了系统协同运行中的复杂问题。提出的交替优化框架及其子问题的具体求解方法（SCA和信任域）显示了扎实的理论功底。该研究对于未来低空经济中无人机群的可靠、安全与高效运行具有重要的指导意义。"}}
{"id": "2507.18546", "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface", "authors": ["Urchade Zaratiana", "Gil Pasternak", "Oliver Boyd", "George Hurn-Maloney", "Ash Lewis"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18546v1", "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18546v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GLiNER2：一个高效的、模式驱动界面的多任务信息抽取系统", "tldr": "GLiNER2是一个高效的统一框架，它扩展了GLiNER架构，在一个单一模型中支持命名实体识别、文本分类和分层结构化数据抽取，并通过模式驱动的界面实现多任务组合，同时保持CPU效率和紧凑的尺寸。", "motivation": "现有的信息抽取（IE）解决方案通常需要针对不同任务的专用模型，或者依赖于计算成本高昂的大型语言模型，这限制了它们的应用和部署。", "method": "GLiNER2是一个统一的框架，它在预训练的Transformer编码器架构基础上构建，增强了原始GLiNER架构。它通过一个直观的模式驱动接口引入多任务组合，从而在一个单一高效的模型中支持命名实体识别、文本分类和分层结构化数据抽取。", "result": "实验证明，GLiNER2在抽取和分类任务上都表现出具有竞争力的性能，并且与基于大型语言模型（LLM）的替代方案相比，在部署可访问性方面有显著提升。", "conclusion": "GLiNER2提供了一个高效、统一且易于部署的多任务信息抽取解决方案，克服了现有方法的局限性，并在性能和可访问性之间取得了良好的平衡。", "translation": "信息抽取（IE）是众多自然语言处理（NLP）应用的基础，然而现有的解决方案通常需要针对不同任务的专用模型，或者依赖于计算成本高昂的大型语言模型。我们提出了GLiNER2，这是一个统一的框架，它增强了原始GLiNER架构，在一个单一高效的模型中支持命名实体识别、文本分类和分层结构化数据抽取。GLiNER2建立在预训练的Transformer编码器架构之上，在引入通过直观的模式驱动界面实现的多任务组合的同时，保持了CPU效率和紧凑的尺寸。我们的实验证明，与基于大型语言模型（LLM）的替代方案相比，GLiNER2在抽取和分类任务上都表现出具有竞争力的性能，并在部署可访问性方面有显著提升。我们发布GLiNER2作为一个开源的、可通过pip安装的库，其中包含预训练模型和文档，网址为https://github.com/fastino-ai/GLiNER2。", "summary": "GLiNER2是一个高效的多任务信息抽取系统，它通过增强原始GLiNER架构，在一个单一模型中实现了命名实体识别、文本分类和分层结构化数据抽取。该系统基于预训练的Transformer编码器，通过模式驱动的界面支持多任务组合，同时保持了CPU效率和紧凑的尺寸。实验表明，GLiNER2在性能上具有竞争力，并显著提高了部署的可访问性，弥补了现有IE解决方案的不足。", "keywords": "信息抽取, 多任务学习, Transformer, 命名实体识别, 文本分类", "comments": "GLiNER2的创新之处在于其将多种信息抽取任务整合到一个单一、高效的模型中，并通过直观的模式驱动接口实现多任务组合，这大大简化了IE系统的开发和部署。其保持CPU效率和紧凑尺寸的特点，使其在资源受限的环境下具有很高的实用价值。与大型语言模型相比，它在部署可访问性方面的显著改进是其重要优势。"}}
{"id": "2507.18578", "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs", "authors": ["Feng Hong", "Geng Yu", "Yushi Ye", "Haicheng Huang", "Huangjie Zheng", "Ya Zhang", "Yanfeng Wang", "Jiangchao Yao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18578v1", "summary": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18578v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "宽进窄出：DLLM高效可撤销解码", "tldr": "WINO是一种无需训练的解码算法，通过并行草稿和验证机制，显著提升了DLLM的速度和质量。", "motivation": "现有的扩散大语言模型（DLLM）在快速并行生成时面临严重的质量-速度权衡问题，即更快的并行解码会导致性能显著下降。作者将此归因于标准解码的不可逆性，容易因早期错误上下文积累而偏离错误的解码方向。", "method": "本文引入了“宽进窄出”（Wide-In, Narrow-Out, WINO）算法，这是一种无需训练的解码算法，旨在实现DLLM中的可撤销解码。WINO采用并行草稿和验证机制，在积极地草拟多个token的同时，利用模型的双向上下文来验证并重新遮蔽可疑的token以进行优化。", "result": "WINO显著改善了DLLM的质量-速度权衡。例如，在GSM8K数学基准测试中，它将推理速度提高了6倍，同时准确率提高了2.58%；在Flickr30K图像描述任务中，它实现了10倍的速度提升，并具有更高的性能。", "conclusion": "WINO算法通过引入可撤销解码机制，有效解决了DLLM中并行解码的质量-速度权衡问题，显著提升了模型性能和效率。", "translation": "扩散大语言模型（DLLMs）已成为自回归模型的一种引人注目的替代方案，旨在实现快速并行生成。然而，现有的DLLMs受到严重的质量-速度权衡问题的困扰，即更快的并行解码会导致性能显著下降。我们将这归因于DLLMs中标准解码的不可逆性，其很容易随着早期错误上下文的积累而偏向错误的解码方向。为了解决这个问题，我们引入了“宽进窄出”（Wide-In, Narrow-Out, WINO），这是一种无需训练的解码算法，可实现DLLMs中的可撤销解码。WINO采用并行草稿和验证机制，在积极地草拟多个token的同时，利用模型的双向上下文来验证并重新遮蔽可疑的token以进行优化。经LLaDA和MMaDA等开源DLLMs验证，WINO被证明可以决定性地改善质量-速度权衡。例如，在GSM8K数学基准测试中，它将推理速度提高了6倍，同时准确率提高了2.58%；在Flickr30K图像描述任务中，它实现了10倍的速度提升，并具有更高的性能。我们进行了更全面的实验，以证明WINO的优越性并提供对其深入的理解。", "summary": "本文提出了一种名为“宽进窄出”（WINO）的无需训练的解码算法，旨在解决扩散大语言模型（DLLMs）中并行解码存在的质量-速度权衡问题。WINO通过引入可撤销解码机制，利用并行草稿和验证方法，能够识别并纠正早期解码错误。实验证明，WINO在多个基准测试上显著提高了DLLMs的推理速度和准确性，例如在GSM8K上加速6倍并提高2.58%的准确率，在Flickr30K上实现10倍加速并提升性能。", "keywords": "DLLM, 可撤销解码, WINO, 并行生成, 质量-速度权衡", "comments": "WINO的创新之处在于其“可撤销解码”的概念，通过并行草稿和验证机制，有效解决了DLLM在并行生成中常见的错误累积问题。其无需训练的特性也大大降低了应用门槛。该方法对于提升DLLM的实用性和效率具有重要意义。"}}
{"id": "2506.19780", "title": "Multi-Preference Lambda-weighted Listwise DPO for Small-Scale Model Alignment", "authors": ["Yuhui Sun", "Xiyao Wang", "Zixi Li", "Zhenlong Yuan", "Jinman Zhao"], "categories": ["cs.LG", "I.2.6; I.2.7; I.5.1"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 12 figures, appendix included. To appear in Proceedings of AAAI 2026. Code: this https URL", "url": "http://arxiv.org/abs/2506.19780v5", "summary": "Large language models (LLMs) demonstrate strong generalization across a wide\nrange of language tasks, but often generate outputs that misalign with human\npreferences. Reinforcement Learning from Human Feedback (RLHF) addresses this\nby optimizing models toward human preferences using a learned reward function\nand reinforcement learning, yielding improved alignment but suffering from high\ncomputational cost and instability. Direct Preference Optimization (DPO)\nsimplifies the process by treating alignment as a classification task over\nbinary preference pairs, reducing training overhead while achieving competitive\nperformance. However, it assumes fixed, single-dimensional preferences and only\nsupports pairwise supervision.\n  To address these limitations, we propose Multi-Preference Lambda-weighted\nListwise DPO, which allows the model to learn from more detailed human feedback\nand flexibly balance multiple goals such as helpfulness, honesty, and fluency.\nOur method models full-ranked preference distributions rather than binary\ncomparisons, enabling more informative learning signals. The lambda vector\ncontrols the relative importance of different alignment goals, allowing the\nmodel to generalize across diverse human objectives. During inference, lambda\ncan be adjusted without retraining, providing controllable alignment behavior\nfor downstream use. We also introduce a learned scheduler that dynamically\nsamples performant lambda configurations to improve robustness.\n  Notably, our method requires only 20GB of GPU memory for training, making it\nsuitable for compute-constrained settings such as academic labs, educational\ntools, or on-device assistants. Experiments on 1B-2B scale models show that our\nmethod consistently outperforms standard DPO on alignment benchmarks while\nenabling efficient, controllable, and fine-grained adaptation suitable for\nreal-world deployment.", "comment": "12 pages, 12 figures, appendix included. To appear in Proceedings of\n  AAAI 2026. Code:\n  https://github.com/yuhui15/Multi-Preference-Lambda-weighted-DPO", "pdf_url": "http://arxiv.org/pdf/2506.19780v5", "cate": "cs.LG", "date": "2025-06-24", "updated": "2025-07-24", "AI": {"title_translation": "多偏好Lambda加权列表式DPO用于小规模模型对齐", "tldr": "本文提出多偏好Lambda加权列表式DPO (Multi-Preference Lambda-weighted Listwise DPO)，旨在通过支持多目标和列表式偏好学习，高效地将小型语言模型与复杂的人类偏好对齐，克服了现有DPO方法的局限性。", "motivation": "大型语言模型(LLMs)的输出常与人类偏好不符。现有对齐方法如RLHF计算成本高且不稳定；DPO虽简化了流程，但假设偏好固定且为单维度，仅支持二元偏好对监督。本文旨在解决DPO在处理多维度、复杂人类偏好时的局限性。", "method": "我们提出了“多偏好Lambda加权列表式DPO (Multi-Preference Lambda-weighted Listwise DPO)”。该方法通过建模全排序偏好分布而非二元比较来学习更丰富的信息，并允许模型灵活平衡多个目标（如有用性、诚实性、流畅性）。一个lambda向量控制不同对齐目标的相对重要性，且可在推理时无需重新训练进行调整。此外，引入了一个学习调度器，动态采样高性能的lambda配置以提高鲁棒性。", "result": "该方法训练仅需20GB GPU内存，适用于计算受限环境。在1B-2B规模模型上的实验表明，我们的方法在对齐基准测试中持续优于标准DPO，并实现了高效、可控且细粒度的适应性，适用于实际部署。", "conclusion": "本文提出的多偏好Lambda加权列表式DPO有效解决了DPO的局限性，提供了一种计算高效且鲁棒的解决方案，能够将小规模LLM与多样化的人类偏好对齐，非常适合在计算资源受限的真实世界环境中部署。", "translation": "大型语言模型（LLMs）在广泛的语言任务中表现出强大的泛化能力，但其生成输出常常与人类偏好不符。基于人类反馈的强化学习（RLHF）通过使用学习到的奖励函数和强化学习将模型优化至人类偏好，从而解决了这一问题，带来了改进的对齐效果，但却面临高计算成本和不稳定性。直接偏好优化（DPO）通过将对齐视为二元偏好对上的分类任务来简化这一过程，降低了训练开销，同时取得了有竞争力的性能。然而，它假设偏好是固定的、单维度的，并且只支持成对监督。\n为了解决这些限制，我们提出了多偏好Lambda加权列表式DPO，它允许模型从更详细的人类反馈中学习，并灵活平衡多个目标，如有用性、诚实性和流畅性。我们的方法建模了全排序偏好分布而非二元比较，从而实现了更具信息量的学习信号。Lambda向量控制不同对齐目标的相对重要性，使模型能够泛化到多样化的人类目标。在推理过程中，无需重新训练即可调整lambda，为下游使用提供了可控的对齐行为。我们还引入了一个学习调度器，动态采样高性能的lambda配置以提高鲁棒性。\n值得注意的是，我们的方法训练仅需20GB的GPU内存，使其适用于计算受限的环境，例如学术实验室、教育工具或设备上的助手。在1B-2B规模模型上的实验表明，我们的方法在对齐基准测试中持续优于标准DPO，同时实现了高效、可控和细粒度的适应性，适用于实际部署。", "summary": "本文提出了一种名为“多偏好Lambda加权列表式DPO (Multi-Preference Lambda-weighted Listwise DPO)”的新方法，旨在解决现有LLM对齐方法（如RLHF的高成本和DPO的单维度、成对偏好限制）的不足。该方法通过从详细的全排序偏好分布中学习，并利用可调节的lambda向量灵活平衡多个对齐目标（如有用性、诚实性），从而提升模型对齐效果。该方法计算效率高（仅需20GB GPU内存），并在1B-2B规模模型上表现出优于标准DPO的性能，为实际部署提供了高效、可控且细粒度的对齐能力。", "keywords": "DPO, 模型对齐, 多偏好, 列表式学习, LLM", "comments": "本文通过将DPO扩展到支持多偏好和列表式反馈，是LLM对齐领域的一个重要创新。它克服了传统DPO仅限于二元偏好的局限性，能够从更丰富的人类反馈中学习。特别值得称赞的是，该方法在计算效率方面的优势（仅需20GB GPU内存），使其在资源受限的环境中具有高度实用性，对学术界和小型团队而言意义重大。动态lambda调整和学习调度器的引入也增强了对齐的可控性和鲁棒性。"}}
{"id": "2507.17886", "title": "Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling", "authors": ["James B Aimone"], "categories": ["cs.NE", "cs.AR", "cs.DC"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      True pre-print; to be submitted at future date", "url": "http://arxiv.org/abs/2507.17886v1", "summary": "Neuromorphic computing (NMC) is increasingly viewed as a low-power\nalternative to conventional von Neumann architectures such as central\nprocessing units (CPUs) and graphics processing units (GPUs), however the\ncomputational value proposition has been difficult to define precisely.\n  Here, we explain how NMC should be seen as general-purpose and programmable\neven though it differs considerably from a conventional stored-program\narchitecture. We show that the time and space scaling of NMC is equivalent to\nthat of a theoretically infinite processor conventional system, however the\nenergy scaling is significantly different. Specifically, the energy of\nconventional systems scales with absolute algorithm work, whereas the energy of\nneuromorphic systems scales with the derivative of algorithm state. The unique\ncharacteristics of NMC architectures make it well suited for different classes\nof algorithms than conventional multi-core systems like GPUs that have been\noptimized for dense numerical applications such as linear algebra. In contrast,\nthe unique characteristics of NMC make it ideally suited for scalable and\nsparse algorithms whose activity is proportional to an objective function, such\nas iterative optimization and large-scale sampling (e.g., Monte Carlo).", "comment": "True pre-print; to be submitted at future date", "pdf_url": "http://arxiv.org/pdf/2507.17886v1", "cate": "cs.NE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "神经形态计算：时间、空间和能量扩展的理论框架", "tldr": "神经形态计算在时间、空间扩展上与传统系统相当，但在能量扩展上显著不同，更适合稀疏和迭代优化算法。", "motivation": "尽管神经形态计算被视为传统架构的低功耗替代品，但其计算价值难以精确定义，因此需要解释其通用性和可编程性，并分析其时间、空间和能量扩展特性。", "method": "本文通过理论分析和解释来阐述神经形态计算的特性及其与传统计算架构的区别，尤其是在时间、空间和能量扩展方面的对比。", "result": "研究表明，神经形态计算的时间和空间扩展与理论上无限处理器的传统系统相当。然而，其能量扩展显著不同：传统系统能量随算法绝对工作量扩展，而神经形态系统能量随算法状态的导数扩展。这使得神经形态计算非常适合与传统多核系统（如GPU）不同类的算法，特别是可扩展的稀疏算法，如迭代优化和大规模采样。", "conclusion": "神经形态计算是一种通用的可编程架构，其独特的能量扩展特性使其特别适用于迭代优化和大规模采样等稀疏算法，而不是传统GPU擅长的密集数值应用。", "translation": "神经形态计算（NMC）正日益被视为传统冯·诺依曼架构（如中央处理器（CPU）和图形处理器（GPU））的低功耗替代方案，然而其计算价值主张一直难以精确定义。在此，我们解释了尽管NMC与传统的存储程序架构有很大不同，但它仍应被视为通用且可编程的。我们展示了NMC的时间和空间扩展与理论上无限处理器的传统系统相当，但能量扩展则显著不同。具体而言，传统系统的能量随算法的绝对工作量扩展，而神经形态系统的能量随算法状态的导数扩展。NMC架构的独特特性使其非常适合与传统多核系统（如GPU）不同类的算法，后者已针对线性代数等密集数值应用进行了优化。相比之下，NMC的独特特性使其非常适合活动与目标函数成比例的可扩展和稀疏算法，例如迭代优化和大规模采样（例如，蒙特卡罗）。", "summary": "本文提出了一个理论框架，阐释了神经形态计算（NMC）作为一种通用可编程架构的计算价值。研究发现，NMC在时间与空间扩展性上与传统无限处理器系统相当，但在能量扩展性上表现出显著差异：传统系统能量随算法总功耗变化，而NMC能量随算法状态变化率变化。这种独特的能量扩展特性使NMC特别适用于迭代优化和大规模采样等可扩展的稀疏算法，而非传统GPU擅长的密集数值应用。", "keywords": "神经形态计算, 能量扩展, 时间扩展, 空间扩展, 稀疏算法", "comments": "这篇论文通过提供一个理论框架，清晰地阐述了神经形态计算在时间、空间和能量扩展方面的独特优势，尤其是在能量效率方面。它创新性地指出了NMC能量扩展与算法状态导数的关系，并据此明确了NMC最适合的算法类型（稀疏、迭代优化、采样），这对于指导NMC的硬件设计和应用开发具有重要意义。它弥补了NMC计算价值定义模糊的空白。"}}
{"id": "2507.18112", "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks", "authors": ["Binghua Li", "Ziqing Chang", "Tong Liang", "Chao Li", "Toshihisa Tanaka", "Shigeki Aoki", "Qibin Zhao", "Zhe Sun"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18112v1", "summary": "We address the challenge of parameter-efficient fine-tuning (PEFT) for\nthree-dimensional (3D) U-Net-based denoising diffusion probabilistic models\n(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its\npractical significance, research on parameter-efficient representations of 3D\nconvolution operations remains limited. To bridge this gap, we propose Tensor\nVolumetric Operator (TenVOO), a novel PEFT method specifically designed for\nfine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network\nmodeling, TenVOO represents 3D convolution kernels with lower-dimensional\ntensors, effectively capturing complex spatial dependencies during fine-tuning\nwith few parameters. We evaluate TenVOO on three downstream brain MRI\ndatasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830\nT1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that\nTenVOO achieves state-of-the-art performance in multi-scale structural\nsimilarity index measure (MS-SSIM), outperforming existing approaches in\ncapturing spatial dependencies while requiring only 0.3% of the trainable\nparameters of the original model. Our code is available at:\nhttps://github.com/xiaovhua/tenvoo", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18112v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用张量网络对3D DDPM进行参数高效微调以生成MRI图像", "tldr": "提出TenVOO，一种基于张量网络的参数高效微调方法，用于3D DDPM的MRI图像生成，显著减少参数同时保持SOTA性能。", "motivation": "解决三维（3D）U-Net去噪扩散概率模型（DDPM）在磁共振成像（MRI）图像生成中参数高效微调（PEFT）的挑战，以及3D卷积操作参数高效表示研究的局限性。", "method": "提出Tensor Volumetric Operator (TenVOO)，一种新颖的PEFT方法，专为微调具有3D卷积骨干的DDPM设计。它利用张量网络建模，用低维张量表示3D卷积核，从而在微调过程中用少量参数有效捕获复杂的空间依赖性。", "result": "在ADNI、PPMI和BraTS2021三个脑部MRI数据集上，通过微调在UK Biobank预训练的DDPM，TenVOO在多尺度结构相似性指数（MS-SSIM）方面达到了最先进的性能，优于现有方法，并且仅需要原始模型0.3%的可训练参数。", "conclusion": "TenVOO成功地解决了3D DDPM在MRI图像生成中参数高效微调的挑战，通过引入张量网络显著减少了参数量，同时实现了卓越的图像生成性能。", "translation": "我们解决了磁共振成像（MRI）图像生成中基于三维（3D）U-Net去噪扩散概率模型（DDPM）的参数高效微调（PEFT）挑战。尽管其具有实际意义，但关于3D卷积操作参数高效表示的研究仍然有限。为了弥补这一空白，我们提出了Tensor Volumetric Operator (TenVOO)，这是一种专门为微调具有3D卷积骨干的DDPM设计的新型PEFT方法。TenVOO利用张量网络建模，用低维张量表示3D卷积核，从而在微调过程中以少量参数有效捕获复杂的空间依赖性。我们在三个下游脑部MRI数据集——ADNI、PPMI和BraTS2021上评估了TenVOO，通过微调一个在UK Biobank的59,830个T1加权脑部MRI扫描上预训练的DDPM。我们的结果表明，TenVOO在多尺度结构相似性指数（MS-SSIM）方面达到了最先进的性能，在捕获空间依赖性方面优于现有方法，同时仅需要原始模型0.3%的可训练参数。我们的代码可在以下网址获取：https://github.com/xiaovhua/tenvoo", "summary": "本文提出TenVOO，一种新颖的参数高效微调（PEFT）方法，用于3D U-Net去噪扩散概率模型（DDPM）在MRI图像生成中的应用。针对3D卷积操作参数高效表示的局限性，TenVOO利用张量网络将3D卷积核表示为低维张量，从而以极少的参数有效捕获复杂的空间依赖性。在ADNI、PPMI和BraTS2021等脑部MRI数据集上的实验表明，TenVOO在多尺度结构相似性指数（MS-SSIM）方面达到了最先进的性能，且仅需原始模型0.3%的可训练参数。", "keywords": "参数高效微调, 3D DDPM, MRI图像生成, 张量网络, TenVOO", "comments": "该研究通过引入张量网络进行参数高效微调，为3D DDPM在医学图像生成领域的应用提供了创新且高效的解决方案，显著降低了模型复杂度同时保持了高性能，对于资源受限环境下的部署具有重要意义。"}}
{"id": "2504.02853", "title": "Mapping Technological Futures: Anticipatory Discourse Through Text Mining", "authors": ["Maciej Skorski", "Alina Landowska", "Krzysztof Rajda"], "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.LG", "K.4; H.3.3"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      Accepted to Humanities and Social Sciences Communications. arXiv admin note: text overlap with arXiv:2407.17522", "url": "http://arxiv.org/abs/2504.02853v1", "summary": "The volatility and unpredictability of emerging technologies, such as\nartificial intelligence (AI), generate significant uncertainty, which is widely\ndiscussed on social media. This study examines anticipatory discourse\nsurrounding technological futures by analysing 1.5 million posts from 400 key\nopinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using\nadvanced text mining techniques, including BERTopic modelling, sentiment,\nemotion, and attitude analyses, the research identifies 100 distinct topics\nreflecting anticipated tech-driven futures. Our findings emphasize the dual\nrole of KOLs in framing \\textit{present futures} -- optimistic visions of\ntransformative technologies like AI and IoT -- and influencing \\textit{future\npresents}, where these projections shape contemporary societal and geopolitical\ndebates. Positive emotions such as Hope dominate, outweighing Anxiety,\nparticularly in topics like ``Machine Learning, Data Science, and Deep\nLearning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and\nTrump People'' elicit \\textit{Anxiety}. By framing technologies as solutions to\nsocietal challenges, KOLs act as mediators of societal narratives, bridging\nimagined futures and current realities. These insights underscore their pivotal\nrole in directing public attention with emerging technologies during periods of\nheightened uncertainty, advancing our understanding of anticipatory discourse\nin technology-mediated contexts.", "comment": "Accepted to Humanities and Social Sciences Communications. arXiv\n  admin note: text overlap with arXiv:2407.17522", "pdf_url": "http://arxiv.org/pdf/2504.02853v1", "cate": "cs.SI", "date": "2025-03-25", "updated": "2025-03-25", "AI": {"title_translation": "绘制技术未来：通过文本挖掘的预期性话语", "tldr": "本研究通过文本挖掘分析了社交媒体上关键意见领袖关于技术未来的预期性话语，揭示了他们如何塑造乐观的技术愿景并影响当代社会和地缘政治辩论。", "motivation": "新兴技术（如人工智能）的波动性和不可预测性产生了巨大的不确定性，并在社交媒体上被广泛讨论。本研究旨在通过分析社交媒体上的预期性话语来理解技术未来。", "method": "本研究分析了2021年至2023年间在X平台（推特）上发布的来自400位关键意见领袖（KOLs）的150万条帖子。研究采用了先进的文本挖掘技术，包括BERTopic主题建模、情感、情绪和态度分析。", "result": "研究识别出100个不同的主题，反映了预期中的技术驱动的未来。研究发现KOLs在塑造“当前未来”（对AI和IoT等变革技术的乐观愿景）和影响“未来现在”（这些预测塑造当代社会和地缘政治辩论）中扮演双重角色。积极情绪如“希望”占主导地位，超过了“焦虑”，尤其是在“机器学习、数据科学和深度学习”等主题中，而关于“气候变化”和“战争、乌克兰和特朗普支持者”的讨论则引发“焦虑”。KOLs通过将技术构建为社会挑战的解决方案，充当社会叙事的媒介。", "conclusion": "这些发现强调了KOLs在高度不确定时期引导公众关注新兴技术方面的关键作用，加深了我们对技术介导背景下预期性话语的理解。", "translation": "新兴技术，如人工智能（AI），的波动性和不可预测性产生了显著的不确定性，这在社交媒体上被广泛讨论。本研究通过分析X平台（从2021年到2023年）上400位关键意见领袖（KOLs）发布的150万条帖子，考察了围绕技术未来的预期性话语。研究使用先进的文本挖掘技术，包括BERTopic主题建模、情感、情绪和态度分析，识别出100个不同的主题，反映了预期中的技术驱动的未来。我们的发现强调了KOLs在构建“当前未来”（对AI和IoT等变革技术的乐观愿景）和影响“未来现在”（这些预测塑造当代社会和地缘政治辩论）中的双重作用。积极情绪如“希望”占主导地位，超过了“焦虑”，特别是在“机器学习、数据科学和深度学习”等主题中，而关于“气候变化”和“战争、乌克兰和特朗普支持者”的讨论则引发“焦虑”。通过将技术构建为社会挑战的解决方案，KOLs充当社会叙事的媒介，连接了想象中的未来和当前的现实。这些见解强调了他们在高度不确定时期引导公众关注新兴技术方面的关键作用，加深了我们对技术介导背景下预期性话语的理解。", "summary": "本研究通过对X平台上150万条KOLs帖子的文本挖掘分析，探讨了围绕新兴技术未来的预期性话语。研究识别了100个主题，并发现KOLs在塑造乐观技术愿景（如AI和IoT）和影响当代社会及地缘政治辩论方面发挥双重作用。结果显示“希望”情绪在技术相关主题中占主导，而“焦虑”则与气候变化和地缘政治事件相关。研究强调了KOLs在不确定时期作为社会叙事媒介，引导公众关注新兴技术的关键作用。", "keywords": "技术未来, 预期性话语, 文本挖掘, 关键意见领袖, 社交媒体", "comments": "本研究通过大规模社交媒体数据和先进的文本挖掘技术，深入分析了关键意见领袖如何塑造公众对技术未来的预期，具有创新性。它揭示了KOLs在构建技术叙事和影响社会认知方面的双重角色，对于理解技术传播和社会影响具有重要意义。研究结果强调了乐观情绪在技术讨论中的主导地位，同时也关注了与特定社会和地缘政治问题相关的焦虑情绪，提供了丰富的洞察。"}}
{"id": "2507.18318", "title": "Parametric design and adaptive sizing of lattice structures for 3d additive manufacturing", "authors": ["Jorge Manuel Mercado-Colmenero", "Daniel Diaz - Perete", "Miguel Angel Rubio- Paramio", "Cristina Martin-Donate"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18318v1", "summary": "The present research is developed into the realm of industrial design\nengineering and additive manufacturing by introducing a parametric design model\nand adaptive mechanical analysis for a new lattice structure, with a focus on\n3D additive manufacturing of complex parts. Focusing on the land-scape of\ncomplex parts additive manufacturing, this research proposes geometric\nparameterization, mechanical adaptive sizing, and numerical validation of a\nnovel lattice structure to optimize the final printed part volume and mass, as\nwell as its structural rigidity. The topology of the lattice structures\nexhibited pyramidal geometry. Complete parameterization of the lattice\nstructure ensures that the known geometric parameters adjust to defined\nrestrictions, enabling dynamic adaptability based on its load states and\nboundary conditions, thereby enhancing its mechanical performance. The core\nmethodology integrates analytical automation with mechanical analysis by\nemploying a model based in two-dimensional beam elements. The dimensioning of\nthe lattice structure is analyzed using rigidity models of its sub-elements,\nproviding an evaluation of its global structural behavior after applying the\nsuperposition principle. Numerical validation was performed to validate the\nproposed analytical model. This step ensures that the analytical model defined\nfor dimensioning the lattice structure adjusts to its real mechanical behavior\nand allows its validation. The present manuscript aims to advance additive\nmanufacturing methodologies by offering a systematic and adaptive approach to\nlattice structure design. Parametric and adaptive techniques foster new\nindustrial design engineering methods, enabling the dynamic tailoring of\nlattice structures to meet their mechanical demands and enhance their overall\nefficiency and performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18318v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "增材制造中点阵结构的参数化设计与自适应尺寸确定", "tldr": "本研究提出了一种用于3D增材制造复杂零件的参数化设计模型和自适应力学分析方法，以优化点阵结构的体积、质量和刚度。", "motivation": "旨在推进增材制造方法，特别是针对复杂零件的制造，通过提供一种系统和自适应的点阵结构设计方法，以满足其力学需求并提高整体效率和性能。", "method": "提出了一种新的点阵结构（金字塔形拓扑）的几何参数化、力学自适应尺寸确定和数值验证方法。核心方法是将分析自动化与基于二维梁单元模型的力学分析相结合，并使用子单元的刚度模型和叠加原理进行尺寸分析。通过数值验证确保分析模型的准确性。", "result": "通过所提出的参数化和自适应方法，能够优化最终打印零件的体积、质量和结构刚度，并使点阵结构能够根据载荷状态和边界条件动态调整，从而提高其力学性能。数值验证确认了分析模型的有效性。", "conclusion": "本文通过提供一种系统和自适应的点阵结构设计方法，成功推进了增材制造技术，使点阵结构能够动态适应力学需求，从而提高其效率和性能。", "translation": "本研究通过引入一种新的点阵结构的参数化设计模型和自适应力学分析，在工业设计工程和增材制造领域取得了进展，重点关注复杂零件的3D增材制造。本研究关注复杂零件增材制造的现状，提出了一种新型点阵结构的几何参数化、力学自适应尺寸确定和数值验证，以优化最终打印零件的体积和质量，以及其结构刚度。点阵结构的拓扑结构呈金字塔形。点阵结构的完全参数化确保了已知的几何参数能够根据定义的限制进行调整，从而根据其载荷状态和边界条件实现动态适应性，从而增强其力学性能。核心方法通过采用基于二维梁单元的模型，将分析自动化与力学分析相结合。利用其子单元的刚度模型分析点阵结构的尺寸确定，并在应用叠加原理后评估其整体结构行为。进行了数值验证以验证所提出的分析模型。此步骤确保了用于确定点阵结构尺寸的分析模型能够适应其真实的力学行为并允许其验证。本手稿旨在通过提供一种系统和自适应的点阵结构设计方法来推进增材制造方法。参数化和自适应技术促进了新的工业设计工程方法，使点阵结构能够动态定制以满足其力学需求并提高其整体效率和性能。", "summary": "本研究提出了一种用于3D增材制造复杂零件的参数化设计模型和自适应力学分析方法。通过对金字塔形点阵结构进行几何参数化、力学自适应尺寸确定和数值验证，旨在优化打印零件的体积、质量和结构刚度。核心方法整合了基于二维梁单元的分析自动化与力学分析，并通过数值验证确保模型准确性。该方法使点阵结构能根据载荷和边界条件动态调整，从而提高其力学性能和整体效率。", "keywords": "点阵结构, 参数化设计, 自适应尺寸, 增材制造, 力学分析", "comments": "本文创新性地将参数化设计与自适应力学分析相结合，为3D增材制造复杂点阵结构提供了系统化的新方法。通过考虑载荷状态和边界条件，实现了结构的动态适应性，这对于优化零件性能和资源利用具有重要意义。该方法有望推动工业设计工程和增材制造领域的发展。"}}
{"id": "2503.22929", "title": "Unsupervised Feature Disentanglement and Augmentation Network for One-class Face Anti-spoofing", "authors": ["Pei-Kai Huang", "Jun-Xiong Chong", "Ming-Tsung Hsu", "Fang-Yu Hsu", "Yi-Ting Lin", "Kai-Heng Chien", "Hao-Chiang Shao", "Chiou-Ting Hsu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.22929v2", "summary": "Face anti-spoofing (FAS) techniques aim to enhance the security of facial\nidentity authentication by distinguishing authentic live faces from deceptive\nattempts. While two-class FAS methods risk overfitting to training attacks to\nachieve better performance, one-class FAS approaches handle unseen attacks well\nbut are less robust to domain information entangled within the liveness\nfeatures. To address this, we propose an Unsupervised Feature Disentanglement\nand Augmentation Network (\\textbf{UFDANet}), a one-class FAS technique that\nenhances generalizability by augmenting face images via disentangled features.\nThe \\textbf{UFDANet} employs a novel unsupervised feature disentangling method\nto separate the liveness and domain features, facilitating discriminative\nfeature learning. It integrates an out-of-distribution liveness feature\naugmentation scheme to synthesize new liveness features of unseen spoof\nclasses, which deviate from the live class, thus enhancing the representability\nand discriminability of liveness features. Additionally, \\textbf{UFDANet}\nincorporates a domain feature augmentation routine to synthesize unseen domain\nfeatures, thereby achieving better generalizability. Extensive experiments\ndemonstrate that the proposed \\textbf{UFDANet} outperforms previous one-class\nFAS methods and achieves comparable performance to state-of-the-art two-class\nFAS methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.22929v2", "cate": "cs.CV", "date": "2025-03-29", "updated": "2025-07-23", "AI": {"title_translation": "用于单类别人脸防伪的无监督特征解耦与增强网络", "tldr": "UFDANet是一种单类别FAS方法，通过解耦活体和领域特征并对其进行增强来提高泛化能力，其性能优于先前的单类别方法，并与两类别方法相当。", "motivation": "两类别人脸防伪（FAS）方法存在对训练攻击过拟合的风险，而单类别FAS方法虽然能很好地处理未见攻击，但对活体特征中纠缠的领域信息鲁棒性较差。本文旨在解决单类别FAS在领域信息鲁棒性方面的不足。", "method": "本文提出了一个无监督特征解耦与增强网络（UFDANet）。该网络采用新颖的无监督特征解耦方法来分离活体特征和领域特征。它集成了一个分布外活体特征增强方案，以合成未见欺骗类的新活体特征。此外，UFDANet还结合了领域特征增强例程来合成未见领域特征，从而提高泛化能力。", "result": "实验结果表明，所提出的UFDANet优于以前的单类别FAS方法，并达到了与最先进的两类别FAS方法相当的性能。", "conclusion": "UFDANet通过有效解耦和增强活体与领域特征，显著增强了单类别人脸防伪的泛化能力，使其对未见攻击和领域变化具有鲁棒性。", "translation": "人脸防伪（FAS）技术旨在通过区分真实活体人脸和欺骗性尝试来增强面部身份验证的安全性。虽然两类别FAS方法可能存在对训练攻击过拟合以获得更好性能的风险，但单类别FAS方法能很好地处理未见攻击，但对活体特征中纠缠的领域信息鲁棒性较差。为了解决这个问题，我们提出了一个无监督特征解耦与增强网络（UFDANet），这是一种单类别FAS技术，通过解耦特征增强人脸图像来提高泛化能力。UFDANet采用一种新颖的无监督特征解耦方法来分离活体特征和领域特征，从而促进判别性特征学习。它集成了一个分布外活体特征增强方案，以合成未见欺骗类的新活体特征，这些特征偏离活体类别，从而增强活体特征的表示能力和判别能力。此外，UFDANet还结合了领域特征增强例程来合成未见领域特征，从而实现更好的泛化能力。大量实验表明，所提出的UFDANet优于以前的单类别FAS方法，并达到了与最先进的两类别FAS方法相当的性能。", "summary": "本文提出了一种名为UFDANet的单类别人脸防伪（FAS）技术，旨在解决现有单类别方法对纠缠领域信息鲁棒性不足的问题。UFDANet采用无监督特征解耦方法来分离活体和领域特征，并通过对活体和领域特征进行增强，合成未见欺骗类别和领域的新特征，显著提高了泛化能力。实验结果表明，UFDANet超越了以往的单类别FAS方法，并达到了与最先进的两类别FAS方法相当的性能。", "keywords": "人脸防伪, 单类别, 特征解耦, 特征增强, 泛化能力", "comments": "UFDANet的创新之处在于无监督地解耦活体和领域特征，并结合双重增强策略。这解决了单类别FAS在处理未见领域时的关键局限性，同时保持了其在处理未见攻击方面的优势。以单类别方法达到与两类别方法相当的性能具有重要意义。"}}
{"id": "2501.17351", "title": "Realtime Limb Trajectory Optimization for Humanoid Running Through Centroidal Angular Momentum Dynamics", "authors": ["Sait Sovukluk", "Robert Schuller", "Johannes Englsberger", "Christian Ott"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), Atlanta 2025. Link to video: this https URL", "url": "http://arxiv.org/abs/2501.17351v3", "summary": "One of the essential aspects of humanoid robot running is determining the\nlimb-swinging trajectories. During the flight phases, where the ground reaction\nforces are not available for regulation, the limb swinging trajectories are\nsignificant for the stability of the next stance phase. Due to the conservation\nof angular momentum, improper leg and arm swinging results in highly tilted and\nunsustainable body configurations at the next stance phase landing. In such\ncases, the robotic system fails to maintain locomotion independent of the\nstability of the center of mass trajectories. This problem is more apparent for\nfast and high flight time trajectories. This paper proposes a real-time\nnonlinear limb trajectory optimization problem for humanoid running. The\noptimization problem is tested on two different humanoid robot models, and the\ngenerated trajectories are verified using a running algorithm for both robots\nin a simulation environment.", "comment": "This paper has been accepted for publication at the IEEE\n  International Conference on Robotics and Automation (ICRA), Atlanta 2025.\n  Link to video: https://www.youtube.com/watch?v=czfHjwh_A0Y", "pdf_url": "http://arxiv.org/pdf/2501.17351v3", "cate": "cs.RO", "date": "2025-01-29", "updated": "2025-07-24", "AI": {"title_translation": "人形机器人跑步的实时肢体轨迹优化通过质心角动量动力学", "tldr": "论文提出了一种实时非线性肢体轨迹优化方法，以解决人形机器人跑步时肢体摆动对稳定性的影响，并在仿真环境中进行了验证。", "motivation": "人形机器人跑步时，肢体摆动轨迹的确定至关重要，特别是在腾空阶段。不当的肢体摆动会导致着陆时身体姿态倾斜和不稳定，从而影响机器人的运动能力，尤其是在高速和长腾空时间的跑步中。", "method": "提出了一种用于人形机器人跑步的实时非线性肢体轨迹优化问题。", "result": "所提出的优化问题在两种不同的人形机器人模型上进行了测试，并且生成的轨迹在仿真环境中通过跑步算法得到了验证。", "conclusion": "Not mentioned in abstract", "translation": "人形机器人跑步的一个重要方面是确定肢体摆动轨迹。在腾空阶段，由于无法利用地面反作用力进行调节，肢体摆动轨迹对于下一个支撑阶段的稳定性至关重要。由于角动量守恒，不当的腿部和手臂摆动会导致在下一个支撑阶段着陆时身体高度倾斜且不可持续的配置。在这种情况下，机器人系统无法独立于质心轨迹的稳定性来维持运动。对于快速和高腾空时间的轨迹，这个问题更加明显。本文提出了一种用于人形机器人跑步的实时非线性肢体轨迹优化问题。该优化问题在两种不同的人形机器人模型上进行了测试，并且生成的轨迹在仿真环境中通过跑步算法得到了验证。", "summary": "本文针对人形机器人跑步中肢体摆动轨迹对稳定性的关键影响，提出了一种实时非线性肢体轨迹优化方法。该方法旨在解决腾空阶段因角动量守恒导致的不稳定着陆问题，尤其适用于高速和长腾空时间的跑步。研究在两种不同的人形机器人模型上进行了仿真验证，证明了所生成轨迹的有效性。", "keywords": "人形机器人, 肢体轨迹优化, 角动量, 实时, 跑步", "comments": "该论文提出了一种解决人形机器人跑步稳定性的创新方法，即通过实时优化肢体轨迹来管理质心角动量。其重要性在于提升了机器人在高速和长腾空阶段的运动鲁棒性。方法的实时性是其主要优势，但抽象中未提及实际硬件测试或更复杂环境下的表现。"}}
{"id": "2507.18196", "title": "Goal-based Trajectory Prediction for improved Cross-Dataset Generalization", "authors": ["Daniel Grimm", "Ahmed Abouelazm", "J. Marius Zöllner"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted on IEEE ITSC 2025", "url": "http://arxiv.org/abs/2507.18196v1", "summary": "To achieve full autonomous driving, a good understanding of the surrounding\nenvironment is necessary. Especially predicting the future states of other\ntraffic participants imposes a non-trivial challenge. Current SotA-models\nalready show promising results when trained on real datasets (e.g. Argoverse2,\nNuScenes). Problems arise when these models are deployed to new/unseen areas.\nTypically, performance drops significantly, indicating that the models lack\ngeneralization. In this work, we introduce a new Graph Neural Network (GNN)\nthat utilizes a heterogeneous graph consisting of traffic participants and\nvectorized road network. Latter, is used to classify goals, i.e. endpoints of\nthe predicted trajectories, in a multi-staged approach, leading to a better\ngeneralization to unseen scenarios. We show the effectiveness of the goal\nselection process via cross-dataset evaluation, i.e. training on Argoverse2 and\nevaluating on NuScenes.", "comment": "Accepted on IEEE ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2507.18196v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于目标的轨迹预测，以改进跨数据集泛化能力", "tldr": "本文提出了一种新的图神经网络（GNN），利用异构图和多阶段目标分类方法，以提高轨迹预测模型在未见场景中的泛化能力，并通过跨数据集评估验证了其有效性。", "motivation": "实现完全自动驾驶需要对周围环境有良好的理解，其中预测其他交通参与者的未来状态是一个非平凡的挑战。当前的SotA模型在真实数据集上表现良好，但在部署到新的/未见区域时性能显著下降，表明模型缺乏泛化能力。", "method": "引入了一种新的图神经网络（GNN），该网络利用由交通参与者和矢量化道路网络组成的异构图。后者用于以多阶段方法对目标（即预测轨迹的终点）进行分类。", "result": "通过跨数据集评估（即在Argoverse2上训练并在NuScenes上评估），展示了目标选择过程的有效性。", "conclusion": "通过引入基于目标的多阶段分类方法并结合异构图GNN，可以显著提高轨迹预测模型在未见场景中的泛化能力。", "translation": "为了实现完全自动驾驶，需要对周围环境有良好的理解。特别是预测其他交通参与者的未来状态带来了不小的挑战。当前的SotA模型在真实数据集（例如Argoverse2、NuScenes）上训练时已经显示出有希望的结果。当这些模型部署到新的/未见区域时会出现问题。通常，性能会显著下降，这表明模型缺乏泛化能力。在这项工作中，我们引入了一种新的图神经网络（GNN），该网络利用由交通参与者和矢量化道路网络组成的异构图。后者用于以多阶段方法对目标（即预测轨迹的终点）进行分类，从而更好地泛化到未见场景。我们通过跨数据集评估（即在Argoverse2上训练并在NuScenes上评估）展示了目标选择过程的有效性。", "summary": "本文提出了一种新的图神经网络（GNN），旨在解决自动驾驶中轨迹预测模型在未见场景中泛化能力差的问题。该GNN利用包含交通参与者和矢量化道路网络的异构图，并通过多阶段方法对预测轨迹的目标点进行分类。研究通过在Argoverse2上训练并在NuScenes上评估的跨数据集实验，证明了其目标选择过程在提高模型泛化能力方面的有效性。", "keywords": "轨迹预测, 泛化能力, 图神经网络, 异构图, 自动驾驶", "comments": "本文的创新点在于结合异构图结构和多阶段目标分类方法来提高轨迹预测模型的跨数据集泛化能力。通过明确预测轨迹的终点（目标），模型能够更好地适应新的环境，这对于自动驾驶的实际部署至关重要。跨数据集评估是验证泛化能力的关键，使其结果更具说服力。"}}
{"id": "2507.18382", "title": "Towards Consistent Long-Term Pose Generation", "authors": ["Yayuan Li", "Filippos Bellos", "Jason Corso"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 4 tables", "url": "http://arxiv.org/abs/2507.18382v1", "summary": "Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios.", "comment": "10 pages, 5 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.18382v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "迈向一致的长期姿态生成", "tldr": "现有姿态生成方法在长期一致性方面存在问题，本文提出一种新颖的单阶段架构，直接生成姿态，在长期生成场景中显著优于现有方法。", "motivation": "当前姿态生成方法严重依赖中间表示（量化或自回归模型），导致性能下降并在推断过程中累积误差，尤其是在保持时间连贯性至关重要的长期姿态生成中。", "method": "提出一种新颖的单阶段架构，可以直接从单个RGB图像和文本描述在连续坐标空间中生成姿态。主要创新点是：通过相对运动预测机制直接操作姿态坐标，并采用统一的占位符令牌方法，实现单次前向生成，从而消除了对中间表示或基于令牌的生成的需求。", "result": "在Penn Action和First-Person Hand Action Benchmark (F-PHAB) 数据集上，该方法显著优于现有的基于量化和自回归的方法，尤其是在长期生成场景中。", "conclusion": "本文提出的单阶段架构通过直接生成姿态，有效解决了当前姿态生成方法的局限性，从而在长期场景中提高了姿态生成的一致性和性能。", "translation": "当前姿态生成方法严重依赖中间表示，无论是通过带有量化的两阶段流水线还是在推断过程中累积误差的自回归模型。这种根本性限制导致性能下降，尤其是在保持时间连贯性至关重要的长期姿态生成中。我们提出了一种新颖的单阶段架构，可以直接从最少上下文（单个RGB图像和文本描述）在连续坐标空间中生成姿态，同时保持训练和推断之间的一致分布。我们的关键创新在于通过相对运动预测机制直接操作姿态坐标，从而保留空间关系，并采用统一的占位符令牌方法，实现单次前向生成，并在训练和推断期间表现出相同的行为，从而消除了对中间表示或基于令牌的生成的需求。通过在Penn Action和First-Person Hand Action Benchmark (F-PHAB) 数据集上进行的大量实验，我们证明了我们的方法显著优于现有的基于量化和自回归的方法，尤其是在长期生成场景中。", "summary": "本文提出一种新颖的单阶段架构，用于长期姿态生成。与现有依赖中间表示并累积误差的方法不同，该方法直接从图像和文本在连续坐标空间中生成姿态。通过相对运动预测和统一占位符令牌，它避免了中间步骤，确保训练和推断的一致性。实验表明，该方法在长期生成场景中显著优于现有方法。", "keywords": "姿态生成, 长期生成, 单阶段架构, 连续坐标空间, 相对运动预测", "comments": "该方法的创新之处在于其直接、单阶段的连续空间生成，避免了中间表示和误差累积的缺陷。这对于长期一致性至关重要。统一占位符令牌方法在对齐训练和推断行为方面也值得关注。"}}
{"id": "2505.02581", "title": "Neurodivergent Influenceability as a Contingent Solution to the AI Alignment Problem", "authors": ["Alberto Hernández-Espinosa", "Felipe S. Abrahão", "Olaf Witkowski", "Hector Zenil"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      44 pages", "url": "http://arxiv.org/abs/2505.02581v4", "summary": "The AI alignment problem, which focusses on ensuring that artificial\nintelligence (AI), including AGI and ASI, systems act according to human\nvalues, presents profound challenges. With the progression from narrow AI to\nArtificial General Intelligence (AGI) and Superintelligence, fears about\ncontrol and existential risk have escalated. Here, we investigate whether\nembracing inevitable AI misalignment can be a contingent strategy to foster a\ndynamic ecosystem of competing agents as a viable path to steer them in more\nhuman-aligned trends and mitigate risks. We explore how misalignment may serve\nand should be promoted as a counterbalancing mechanism to team up with\nwhichever agents are most aligned to human interests, ensuring that no single\nsystem dominates destructively. The main premise of our contribution is that\nmisalignment is inevitable because full AI-human alignment is a mathematical\nimpossibility from Turing-complete systems, which we also offer as a proof in\nthis contribution, a feature then inherited to AGI and ASI systems. We\nintroduce a change-of-opinion attack test based on perturbation and\nintervention analysis to study how humans and agents may change or neutralise\nfriendly and unfriendly AIs through cooperation and competition. We show that\nopen models are more diverse and that most likely guardrails implemented in\nproprietary models are successful at controlling some of the agents' range of\nbehaviour with positive and negative consequences while closed systems are more\nsteerable and can also be used against proprietary AI systems. We also show\nthat human and AI intervention has different effects hence suggesting multiple\nstrategies.", "comment": "44 pages", "pdf_url": "http://arxiv.org/pdf/2505.02581v4", "cate": "cs.AI", "date": "2025-05-05", "updated": "2025-07-24", "AI": {"title_translation": "神经多样性可影响性作为AI对齐问题的权宜之计", "tldr": "该论文提出，由于AI与人类的完全对齐在数学上是不可能的，因此应将AI未对齐视为一种可控的策略，通过促进竞争代理生态系统来缓解风险并引导AI向人类利益靠拢。", "motivation": "AI对齐问题面临巨大挑战，随着AI从窄域AI发展到AGI和ASI，人们对控制和生存风险的担忧日益加剧。本文旨在探讨是否可以将AI的必然未对齐作为一种权宜之计，以促进竞争代理的动态生态系统，从而引导AI朝着更符合人类利益的方向发展并降低风险。", "method": "研究人员提出，完全的AI-人类对齐在图灵完备系统中是数学上不可能的，并提供了证明。他们引入了一种基于扰动和干预分析的“意见改变攻击测试”，以研究人类和代理如何通过合作和竞争来改变或中和友善和不友善的AI。", "result": "研究表明，开放模型更加多样化；专有模型中实施的护栏在控制某些代理行为范围方面有效，但有积极和消极后果；封闭系统更易于控制，也可用于对抗专有AI系统。此外，人类和AI干预具有不同的效果，这暗示了多种策略。", "conclusion": "由于AI与人类的完全对齐在数学上是不可能的，因此应将AI的未对齐视为一种可以利用的特性。通过促进代理之间的竞争和合作，并结合人类和AI的干预，可以创建一种动态系统来引导AI朝向人类利益，从而缓解AI对齐带来的风险。", "translation": "AI对齐问题，即确保包括AGI和ASI在内的人工智能（AI）系统按照人类价值观行事，带来了深刻的挑战。随着AI从窄域AI发展到通用人工智能（AGI）和超级智能，对控制和生存风险的担忧日益加剧。在此，我们研究是否将不可避免的AI未对齐视为一种权宜之计，以促进竞争代理的动态生态系统，作为引导它们朝着更符合人类利益的趋势发展并降低风险的可行途径。我们探讨了未对齐如何能够作为一种平衡机制，应加以推广，以与最符合人类利益的代理合作，确保没有单一系统造成破坏性主导。我们贡献的主要前提是，未对齐是不可避免的，因为从图灵完备系统来看，完全的AI-人类对齐在数学上是不可能的，我们也在本贡献中提供了证明，这是AGI和ASI系统继承的一个特征。我们引入了一种基于扰动和干预分析的“意见改变攻击测试”，以研究人类和代理如何通过合作和竞争来改变或中和友善和不友善的AI。我们表明，开放模型更加多样化，并且在专有模型中实施的护栏很可能成功地控制了某些代理的行为范围，并带来了积极和消极的后果，而封闭系统更易于控制，也可以用于对抗专有AI系统。我们还表明，人类和AI干预具有不同的效果，因此提出了多种策略。", "summary": "本论文探讨了AI对齐问题，并提出了一种新颖的观点：由于AI与人类的完全对齐在数学上是不可能的（并提供了证明），因此应将AI的“未对齐”视为一种可利用的特性。作者认为，通过促进AI代理之间的竞争性生态系统，并利用人类和AI的干预来改变或中和AI行为，可以有效地引导AI向人类利益靠拢，从而缓解AI失控的风险。研究还通过“意见改变攻击测试”分析了开放和封闭模型的特点及其对AI行为控制的影响。", "keywords": "AI对齐, 未对齐, 图灵完备系统, 竞争代理, 神经多样性", "comments": "该论文提出了一种非常规但引人深思的视角来解决AI对齐问题，即不追求完全对齐，而是利用“未对齐”作为一种动态平衡机制。其创新之处在于将AI未对齐视为一种机会而非单纯的缺陷，并引入了“意见改变攻击测试”来量化这种可影响性。如果其数学证明成立，这将对AI对齐领域产生重要影响。"}}
{"id": "2507.17702", "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17702v2", "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17702v2", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "迈向更大杠杆：高效专家混合语言模型的缩放定律", "tldr": "本研究引入效率杠杆(EL)指标，通过大规模实证研究揭示了MoE模型容量与配置（专家激活比、总计算预算、专家粒度）之间的缩放定律，并验证了其准确性，为高效MoE模型的扩展奠定了基础。", "motivation": "混合专家模型（MoE）虽然能高效扩展大型语言模型（LLMs），但其参数与计算成本解耦导致了一个关键挑战：预测给定MoE配置（如专家激活比和粒度）的模型容量仍是一个未解决的问题。", "method": "研究引入了“效率杠杆”（EL）指标来量化MoE模型相对于同等密集模型的计算优势。通过大规模实证研究，训练了300多个参数高达28B的模型，系统地调查了MoE架构配置与EL之间的关系。最终，将发现整合为一个统一的缩放定律，用于根据配置准确预测MoE架构的EL。为验证所推导的缩放定律，研究设计并训练了Ling-mini-beta模型，并与6.1B密集模型进行比较。", "result": "研究发现，效率杠杆（EL）主要由专家激活比和总计算预算驱动，两者都遵循可预测的幂律。专家粒度作为非线性调节器，具有明确的最优范围。通过验证，Ling-mini-beta模型（0.85B活跃参数）在相同的1T高质量token数据集上训练时，性能与6.1B密集模型匹配，但计算资源消耗减少了7倍以上，证实了缩放定律的准确性。", "conclusion": "这项工作为高效混合专家模型的缩放提供了原则性且基于实证的基础。", "translation": "混合专家（MoE）已成为通过将总参数与计算成本解耦来高效扩展大型语言模型（LLMs）的主导架构。然而，这种解耦带来了一个关键挑战：预测给定MoE配置（例如，专家激活比和粒度）的模型容量仍然是一个未解决的问题。为了弥补这一空白，我们引入了效率杠杆（EL），这是一个量化MoE模型相对于同等密集模型的计算优势的指标。我们进行了一项大规模的实证研究，训练了超过300个高达28B参数的模型，以系统地调查MoE架构配置与EL之间的关系。我们的发现表明，EL主要由专家激活比和总计算预算驱动，两者都遵循可预测的幂律，而专家粒度则作为一个具有明确最优范围的非线性调节器。我们将这些发现整合到一个统一的缩放定律中，该定律能够根据MoE架构的配置准确预测其EL。为了验证我们推导出的缩放定律，我们设计并训练了Ling-mini-beta，这是一个Ling-2.0系列的试点模型，仅有0.85B的活跃参数，同时训练了一个6.1B的密集模型进行比较。当在相同的1T高质量token数据集上训练时，Ling-mini-beta的性能与6.1B密集模型匹配，同时消耗的计算资源减少了7倍以上，从而证实了我们缩放定律的准确性。这项工作为高效MoE模型的缩放提供了原则性且基于实证的基础。", "summary": "本研究旨在解决混合专家（MoE）模型在扩展大型语言模型时，其模型容量难以预测的问题。为此，论文引入了“效率杠杆”（EL）这一新指标，用于量化MoE模型相对于密集模型的计算优势。通过大规模实证研究，分析了MoE架构配置（专家激活比、总计算预算和专家粒度）与EL之间的关系，并发现EL主要受前两者驱动，遵循幂律，而专家粒度具有最优范围。研究将这些发现整合为一个统一的缩放定律，并成功通过Ling-mini-beta模型验证了其准确性，该模型在显著减少计算资源的情况下达到了与大型密集模型相当的性能。这项工作为高效MoE模型的扩展提供了坚实的理论和实证基础。", "keywords": "混合专家模型, 缩放定律, 效率杠杆, 大型语言模型, 模型容量", "comments": "这项研究的创新之处在于提出了“效率杠杆”（EL）这一量化MoE模型计算效率的新指标，并首次系统地揭示了MoE模型容量与关键架构配置之间的缩放定律。通过大规模的实证研究，其发现不仅具有理论价值，更提供了实际指导，帮助研究人员和工程师更高效地设计和扩展MoE模型。Ling-mini-beta的验证结果尤其令人印象深刻，表明所提出的缩放定律能够显著优化资源使用，对未来大型语言模型的开发具有重要意义。"}}
{"id": "2507.17772", "title": "Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments", "authors": ["Ahmad Alhonainy", "Praveen Rao"], "categories": ["cs.DC", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Journal", "url": "http://arxiv.org/abs/2507.17772v1", "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.", "comment": "Journal", "pdf_url": "http://arxiv.org/pdf/2507.17772v1", "cate": "cs.DC", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "物联网环境中联邦学习通信成本降低的缓存技术", "tldr": "本文提出缓存策略（FIFO、LRU、基于优先级）以减少联邦学习在物联网环境中的通信开销，通过选择性转发重要更新，在保持模型精度的同时降低带宽使用。", "motivation": "联邦学习（FL）尽管允许分布式设备协作训练模型而不集中数据，但通信成本仍然是一个主要瓶颈，尤其是在资源受限的物联网环境中。", "method": "本文引入了FIFO、LRU和基于优先级的缓存策略，通过选择性地转发重要的模型更新来减少不必要的传输，从而降低带宽使用并维持模型精度。", "result": "在CIFAR-10和医疗数据集上的实验表明，该方法在通信量减少的同时，精度损失极小。结果证实智能缓存提高了可扩展性、内存效率，并支持边缘物联网网络中可靠的FL。", "conclusion": "智能缓存技术能够有效降低联邦学习在物联网环境中的通信成本，同时保持模型精度，使其在智慧城市、医疗保健等延迟敏感应用中具有实用部署价值。", "translation": "联邦学习（FL）允许多个分布式设备在不集中数据的情况下共同训练共享模型，但通信成本仍然是一个主要瓶颈，尤其是在资源受限的环境中。本文引入了缓存策略——FIFO、LRU和基于优先级的——以减少不必要的模型更新传输。通过选择性地转发重要的更新，我们的方法在保持模型精度的同时降低了带宽使用。在CIFAR-10和医疗数据集上的实验显示，通信量减少且精度损失极小。结果证实，智能缓存提高了可扩展性、内存效率，并支持边缘物联网网络中可靠的FL，使其在智慧城市、医疗保健和其他对延迟敏感的应用中具有实际部署价值。", "summary": "本文针对联邦学习在物联网环境中面临的通信成本瓶颈，提出并评估了FIFO、LRU和优先级缓存策略。这些策略通过选择性地转发重要的模型更新，显著减少了通信量，同时保持了模型精度。实验结果表明，该方法有效提升了联邦学习在边缘IoT网络中的可扩展性和效率，使其适用于资源受限和延迟敏感的应用场景。", "keywords": "联邦学习, 缓存技术, 物联网, 通信成本, 边缘计算", "comments": "这篇论文的创新点在于将成熟的缓存策略（FIFO, LRU, Priority-Based）应用于联邦学习的通信优化中，特别是在资源受限的IoT环境。它提供了一个实用的解决方案来缓解联邦学习的通信瓶颈，对于推动联邦学习在边缘计算和物联网领域的实际部署具有重要意义。"}}
{"id": "2506.06874", "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models", "authors": ["Ala Yankouskaya", "Areej B. Babiker", "Syeda W. F. Rizvi", "Sameha Alshakhsi", "Magnus Liebherr", "Raian Ali"], "categories": ["cs.HC", "cs.AI", "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.06874v3", "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.06874v3", "cate": "cs.HC", "date": "2025-06-07", "updated": "2025-07-24", "AI": {"title_translation": "LLM-D12：大型语言模型工具性和关系性依赖的双维度量表", "tldr": "开发并验证了一个名为LLM-D12的12项问卷，用于衡量对大型语言模型的依赖，该问卷揭示了工具性依赖和关系性依赖两个维度。", "motivation": "现有的评估个人对大型语言模型（LLMs）依赖程度的工具稀缺，且主要基于传统的行为成瘾症状，这被认为是一个概念上的局限。研究者认为LLM与人类的关系更为细致，需要一个全新的、独特的视角来填补这一空白。", "method": "开发了一个名为LLM-D12的12项问卷，用于测量LLM依赖。该量表基于作者先前的理论工作，并向英国的526名参与者收集了响应数据。采用分样本方法，在总样本的独立两半上进行了探索性（EFA）和验证性因子分析（CFA）。", "result": "因子分析支持了一个双因子结构：工具性依赖（六项）和关系性依赖（六项）。工具性依赖反映了个体在决策和认知任务中对LLM的支持或协作的依赖程度，而关系性依赖则捕捉了将LLM视为具有社会意义、有感知或类似伴侣实体的倾向。该双因子结构表现出卓越的内部一致性和清晰的区分效度。外部验证也证实了这两个子量表的概念基础和区别。", "conclusion": "对LLM的依赖不一定表明功能障碍，但仍可能反映在某些情境下可能变得有问题的依赖水平。LLM-D12量表的心理测量特性和结构支持了这一新兴观点。", "translation": "人们对理解人如何与大型语言模型（LLMs）互动以及这些模型是否会引发依赖甚至成瘾行为的兴趣日益增长。评估个体对LLMs依赖程度的有效工具稀缺，且主要建立在经典的行为成瘾症状之上，并适应于LLM使用的情境。我们认为这是一个概念上的局限，因为LLM与人类的关系更为细致，需要一个全新的、独特的视角。为了弥补这一空白，我们开发并验证了一个新的12项问卷来衡量LLM依赖，称为LLM-D12。该量表基于作者先前的理论工作，项目据此开发，并从英国的526名参与者那里收集了答复。采用分样本方法，在总样本的独立两半上进行了探索性（EFA）和验证性因子分析（CFA），支持了一个双因子结构：工具性依赖（六项）和关系性依赖（六项）。工具性依赖反映了个体在决策和认知任务中对LLMs的支持或协作的依赖程度。关系性依赖捕捉了将LLMs视为具有社会意义、有感知或类似伴侣实体的倾向。该双因子结构表现出卓越的内部一致性和清晰的区分效度。外部验证证实了这两个子量表的概念基础和区别。我们的LLM-D12量表的心理测量特性和结构是根据新兴的观点进行解释的，即对LLMs的依赖不一定表明功能障碍，但仍可能反映在某些情境下可能变得有问题的依赖水平。", "summary": "本研究旨在解决现有LLM依赖评估工具的局限性，开发并验证了一个名为LLM-D12的12项问卷。该问卷基于对526名英国参与者的数据分析，揭示了工具性依赖和关系性依赖两个独立的维度，并展现了良好的心理测量学特性。研究强调，对LLM的依赖并非必然是负面的，但其程度可能在特定情境下引发问题。", "keywords": "LLM依赖, 问卷开发, 工具性依赖, 关系性依赖, 心理测量学", "comments": "这项研究的创新之处在于其提出了一个双维度的LLM依赖评估框架，超越了传统行为成瘾的单一视角，更细致地捕捉了人机交互的复杂性。其区分工具性依赖和关系性依赖对于未来理解和管理LLM使用行为具有重要意义，尤其是在识别潜在问题性依赖方面提供了新的工具和视角。"}}
{"id": "2407.09261", "title": "A software framework for stochastic model predictive control of nonlinear continuous-time systems (GRAMPC-S)", "authors": ["Daniel Landgraf", "Andreas Völz", "Knut Graichen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Optimization and Engineering, and is available online at this https URL", "url": "http://arxiv.org/abs/2407.09261v2", "summary": "This paper presents the open-source stochastic model predictive control\nframework GRAMPC-S for nonlinear uncertain systems with chance constraints. It\nprovides several uncertainty propagation methods to predict stochastic moments\nof the system state and can consider unknown parts of the system dynamics using\nGaussian process regression. These methods are used to reformulate a stochastic\nMPC formulation as a deterministic one that is solved with GRAMPC. The\nperformance of the presented framework is evaluated using examples from a wide\nrange of technical areas. The experimental evaluation shows that GRAMPC-S can\nbe used in practice for the control of nonlinear uncertain systems with\nsampling times in the millisecond range.", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this article is\n  published in Optimization and Engineering, and is available online at\n  https://doi.org/10.1007/s11081-025-10006-z", "pdf_url": "http://arxiv.org/pdf/2407.09261v2", "cate": "eess.SY", "date": "2024-07-12", "updated": "2025-07-24", "AI": {"title_translation": "非线性连续时间系统随机模型预测控制的软件框架 (GRAMPC-S)", "tldr": "GRAMPC-S是一个开源的随机模型预测控制框架，用于具有不确定性和机会约束的非线性系统，并已在实践中证明其在毫秒级采样时间下的有效性。", "motivation": "该论文旨在为具有机会约束的非线性不确定系统提供一个实用的随机模型预测控制软件框架。", "method": "该框架（GRAMPC-S）提供了多种不确定性传播方法来预测系统状态的随机矩，并利用高斯过程回归处理系统动力学中的未知部分。它将随机MPC公式重新表述为确定性问题，并使用GRAMPC求解。", "result": "实验评估表明，GRAMPC-S可以在实践中用于控制采样时间在毫秒范围内的非线性不确定系统。", "conclusion": "GRAMPC-S是一个实用的开源软件框架，能够有效地控制具有毫秒级采样时间的非线性不确定系统。", "translation": "本文提出了开源的随机模型预测控制框架 GRAMPC-S，用于处理具有机会约束的非线性不确定系统。它提供了多种不确定性传播方法来预测系统状态的随机矩，并且可以使用高斯过程回归考虑系统动力学中的未知部分。这些方法用于将随机MPC公式重新表述为确定性问题，并使用 GRAMPC 进行求解。该框架的性能通过来自广泛技术领域的示例进行了评估。实验评估表明，GRAMPC-S 可以在实践中用于控制采样时间在毫秒范围内的非线性不确定系统。", "summary": "本文介绍了GRAMPC-S，一个用于具有机会约束的非线性不确定系统的开源随机模型预测控制框架。该框架结合了多种不确定性传播方法和高斯过程回归，将随机MPC问题转化为确定性问题并通过GRAMPC求解。实验证明GRAMPC-S在毫秒级采样时间下对非线性不确定系统的实际控制是有效的。", "keywords": "随机模型预测控制, 非线性系统, 不确定性传播, 高斯过程回归, 开源框架", "comments": "GRAMPC-S的创新之处在于其将多种不确定性传播方法与高斯过程回归相结合，有效地处理了非线性不确定系统中的挑战。作为一个开源框架，它降低了随机MPC的门槛。其在毫秒级采样时间下的实际应用能力，对于需要快速响应的实时控制系统具有重要意义。"}}
{"id": "2507.18607", "title": "Explainable Mapper: Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents", "authors": ["Xinyuan Yan", "Rita Sevastjanova", "Sinie van der Ben", "Mennatallah El-Assady", "Bei Wang"], "categories": ["cs.CG", "cs.LG"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18607v1", "summary": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18607v1", "cate": "cs.CG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "可解释映射器：使用基于扰动的解释和验证代理绘制大型语言模型嵌入空间图", "tldr": "该论文介绍了一个名为“可解释映射器”的框架，它使用基于扰动的LLM代理来半自动地探索、解释和验证大型语言模型嵌入空间的拓扑结构。", "motivation": "大型语言模型（LLMs）生成的高维嵌入空间虽然捕获了丰富的语义和句法关系，但手动探索这些嵌入空间以揭示编码的语言属性需要大量人工努力。为了解决这一挑战，本文提出了一个半自动化的框架。", "method": "本文提出了一个用于半自动标注LLM嵌入空间属性的框架。首先，定义了映射图（mapper graph）中可探索元素的分类法（如节点、边、路径、组件和轨迹）。然后，通过两种可定制的、基于LLM的代理执行这些元素的标注，这些代理采用扰动技术进行可扩展的自动化分析，以探索、解释映射元素特性并验证解释的鲁棒性。该框架在一个可视化分析工作空间中进行了实例化。", "result": "通过案例研究，该框架被证明是有效的。具体来说，它复制了先前关于BERT模型不同层级嵌入属性的研究发现，并对拓扑邻域的语言属性提供了进一步的观察。", "conclusion": "本文提出的“可解释映射器”框架通过使用基于扰动的LLM代理实现了大型语言模型嵌入空间的半自动化探索、解释和验证，有效减轻了人工探索的负担，并成功揭示了嵌入空间的语言特性。", "translation": "大型语言模型（LLMs）产生高维嵌入，捕获词语、句子和概念之间丰富的语义和句法关系。通过映射图（mapper graphs）研究LLM嵌入空间的拓扑结构，使我们能够理解其底层结构。具体来说，映射图总结了嵌入空间的拓扑结构，其中每个节点代表一个拓扑邻域（包含一组嵌入），如果两个节点对应的邻域重叠，则它们之间存在一条边。然而，手动探索这些嵌入空间以揭示编码的语言属性需要大量人工努力。为了解决这一挑战，我们引入了一个用于半自动标注这些嵌入属性的框架。为了组织探索过程，我们首先定义了映射图中可探索元素的分类法，例如节点、边、路径、组件和轨迹。这些元素的标注通过两种可定制的、基于LLM的代理执行，这些代理采用扰动技术进行可扩展的自动化分析。这些代理有助于探索和解释映射元素特征，并验证所生成解释的鲁棒性。我们在一个可视化分析工作空间中实例化了该框架，并通过案例研究展示了其有效性。特别是，我们复制了先前关于BERT架构不同层级嵌入属性的研究发现，并对拓扑邻域的语言属性提供了进一步的观察。", "summary": "本文提出了“可解释映射器”框架，旨在解决手动探索大型语言模型（LLM）高维嵌入空间所面临的巨大人工耗费问题。该框架通过定义映射图元素分类法，并利用两种基于扰动技术的LLM代理进行半自动化标注，从而探索、解释并验证嵌入空间的拓扑结构和语言属性。通过可视化分析工作空间中的案例研究，该方法成功复制了现有研究成果并提供了对拓扑邻域语言特性的新观察。", "keywords": "LLM嵌入空间, 映射图, 可解释性, 扰动技术, 语言属性", "comments": "该论文的创新之处在于提出了一个半自动化的框架，利用基于LLM的代理和扰动技术来探索和解释复杂的LLM嵌入空间。这种方法显著降低了手动分析的成本，并为理解LLM内部表征提供了一种可扩展且可解释的途径。其通过复制BERT研究结果展示了实用性，并进一步深化了对语言属性的理解。"}}
{"id": "2507.17934", "title": "Multimodal Fine-grained Reasoning for Post Quality Evaluation", "authors": ["Xiaoxu Guo", "Siyan Liang", "Yachao Cui", "Juxiang Zhou", "Lei Wang", "Han Cao"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      48 pages", "url": "http://arxiv.org/abs/2507.17934v1", "summary": "Accurately assessing post quality requires complex relational reasoning to\ncapture nuanced topic-post relationships. However, existing studies face three\nmajor limitations: (1) treating the task as unimodal categorization, which\nfails to leverage multimodal cues and fine-grained quality distinctions; (2)\nintroducing noise during deep multimodal fusion, leading to misleading signals;\nand (3) lacking the ability to capture complex semantic relationships like\nrelevance and comprehensiveness. To address these issues, we propose the\nMultimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,\nwhich mimics human cognitive processes. MFTRR reframes post-quality assessment\nas a ranking task and incorporates multimodal data to better capture quality\nvariations. It consists of two key modules: (1) the Local-Global Semantic\nCorrelation Reasoning Module, which models fine-grained semantic interactions\nbetween posts and topics at both local and global levels, enhanced by a maximum\ninformation fusion mechanism to suppress noise; and (2) the Multi-Level\nEvidential Relational Reasoning Module, which explores macro- and micro-level\nrelational cues to strengthen evidence-based reasoning. We evaluate MFTRR on\nthree newly constructed multimodal topic-post datasets and the public\nLazada-Home dataset. Experimental results demonstrate that MFTRR significantly\noutperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3\nimprovement over the best unimodal method on the Art History dataset.", "comment": "48 pages", "pdf_url": "http://arxiv.org/pdf/2507.17934v1", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-21", "AI": {"title_translation": "用于帖子质量评估的多模态细粒度推理", "tldr": "该论文提出了一个名为MFTRR的多模态细粒度主题-帖子关系推理框架，通过模拟人类认知过程，将帖子质量评估重构为排序任务，并有效解决现有方法在多模态利用、噪声处理和复杂语义关系捕获方面的局限性。", "motivation": "现有研究在帖子质量评估方面存在三个主要限制：1) 将任务视为单模态分类，未能利用多模态线索和细粒度质量区分；2) 在深度多模态融合过程中引入噪声，导致误导信号；3) 缺乏捕获相关性和全面性等复杂语义关系的能力。", "method": "本文提出了多模态细粒度主题-帖子关系推理（MFTRR）框架，该框架模拟人类认知过程，将帖子质量评估重构为排序任务，并融合多模态数据。它包含两个关键模块：1) 局部-全局语义关联推理模块，通过最大信息融合机制抑制噪声，建模帖子和主题之间细粒度的局部和全局语义交互；2) 多级证据关系推理模块，探索宏观和微观层面的关系线索以强化基于证据的推理。", "result": "MFTRR在三个新建的多模态主题-帖子数据集和公共Lazada-Home数据集上进行了评估。实验结果表明，MFTRR显著优于最先进的基线方法，在艺术史数据集上，相较于最佳单模态方法，NDCG@3指标提升高达9.52%。", "conclusion": "MFTRR框架通过模拟人类认知过程，有效解决了现有帖子质量评估方法在处理多模态信息、抑制噪声以及捕获复杂语义关系方面的不足，并在多模态数据集上取得了显著优于现有SOTA方法的性能。", "translation": "准确评估帖子质量需要复杂的关联推理来捕获细微的主题-帖子关系。然而，现有研究面临三个主要限制：(1) 将任务视为单模态分类，未能利用多模态线索和细粒度质量区分；(2) 在深度多模态融合过程中引入噪声，导致误导信号；(3) 缺乏捕获相关性和全面性等复杂语义关系的能力。为了解决这些问题，我们提出了多模态细粒度主题-帖子关系推理（MFTRR）框架，该框架模拟人类认知过程。MFTRR将帖子质量评估重构为排序任务，并融合多模态数据以更好地捕获质量变化。它由两个关键模块组成：(1) 局部-全局语义关联推理模块，通过最大信息融合机制抑制噪声，建模帖子和主题之间细粒度的局部和全局语义交互；(2) 多级证据关系推理模块，探索宏观和微观层面的关系线索以强化基于证据的推理。我们在三个新建的多模态主题-帖子数据集和公共Lazada-Home数据集上评估了MFTRR。实验结果表明，MFTRR显著优于最先进的基线方法，在艺术史数据集上，相较于最佳单模态方法，NDCG@3指标提升高达9.52%。", "summary": "本文针对现有帖子质量评估方法在多模态利用、噪声处理和复杂语义关系捕获方面的局限性，提出了多模态细粒度主题-帖子关系推理（MFTRR）框架。MFTRR将评估任务重构为排序问题，并包含局部-全局语义关联推理和多级证据关系推理两个模块，以模拟人类认知过程并有效处理多模态信息。实验结果表明，MFTRR在新建和现有数据集上均显著优于现有基线方法。", "keywords": "帖子质量评估, 多模态推理, 细粒度推理, 关系推理, 排序任务", "comments": "该论文创新性地将帖子质量评估任务重新定义为排序任务，并提出了一个模拟人类认知过程的多模态细粒度推理框架MFTRR。其通过引入局部-全局语义关联和多级证据关系推理模块，有效解决了多模态数据融合中的噪声问题和复杂语义关系捕获的挑战。该工作为多模态信息融合和细粒度推理提供了新的思路，并构建了新的数据集，具有重要的研究价值。"}}
{"id": "2507.07966", "title": "Scaling RL to Long Videos", "authors": ["Yukang Chen", "Wei Huang", "Baifeng Shi", "Qinghao Hu", "Hanrong Ye", "Ligeng Zhu", "Zhijian Liu", "Pavlo Molchanov", "Jan Kautz", "Xiaojuan Qi", "Sifei Liu", "Hongxu Yin", "Yao Lu", "Song Han"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code at this https URL and model at this https URL", "url": "http://arxiv.org/abs/2507.07966v2", "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.0% and 70.7% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-R1 across multiple benchmarks. Moreover, LongVILA-R1\nshows steady performance improvements as the number of input video frames\nincreases. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames / around 256k tokens).", "comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B", "pdf_url": "http://arxiv.org/pdf/2507.07966v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-24", "AI": {"title_translation": "将强化学习扩展到长视频", "tldr": "论文提出了一个全栈框架，利用强化学习将视觉语言模型（VLM）的推理能力扩展到长视频，包括新数据集、两阶段训练流程和高效训练基础设施，实现了显著的性能提升和训练加速。", "motivation": "旨在解决将视觉语言模型（VLM）的推理能力扩展到长视频所面临的独特挑战。", "method": "引入了一个全栈框架，通过整合三个关键组件来解决长视频推理问题：1. 大规模数据集：LongVideo-Reason，包含104K高质量长视频问答对。2. 两阶段训练管道：结合思维链监督微调（CoT-SFT）和强化学习（RL）来扩展VLM。3. 长视频RL训练基础设施：Multi-modal Reinforcement Sequence Parallelism (MR-SP)，包含序列并行和基于vLLM的引擎，并利用缓存的视频嵌入进行高效推出和预填充。", "result": "LongVILA-R1-7B在VideoMME视频基准测试中表现出色，无字幕准确率达到65.0%，带字幕准确率达到70.7%；在多个基准测试中持续优于LongVILA-R1；随着输入视频帧数的增加，LongVILA-R1的性能稳步提升；MR-SP系统在长视频RL训练中实现了高达2.1倍的加速；发布了支持RL训练的训练系统，该系统支持多种模态、多种模型，并在单个A100节点上支持长达一小时的视频RL训练。", "conclusion": "该论文成功引入了一个全栈框架，通过集成大规模数据集、两阶段训练管道和高效的训练基础设施，有效将强化学习应用于长视频推理，实现了显著的性能提升和训练效率，并为未来的多模态RL训练提供了可用的系统。", "translation": "我们引入了一个全栈框架，该框架利用强化学习将视觉语言模型（VLM）中的推理能力扩展到长视频。我们通过整合三个关键组件来解决长视频推理的独特挑战：（1）一个大规模数据集LongVideo-Reason，包含10.4万个长视频问答对，涵盖体育、游戏和视频博客等不同领域的高质量推理标注；（2）一个两阶段训练管道，通过思维链监督微调（CoT-SFT）和强化学习（RL）来扩展VLM；以及（3）一个用于长视频RL的训练基础设施，名为多模态强化序列并行（MR-SP），它结合了序列并行和为长视频量身定制的基于vLLM的引擎，使用缓存的视频嵌入进行高效推出和预填充。在我们的实验中，LongVILA-R1-7B在视频基准测试中取得了强大的性能，在VideoMME上无字幕和带字幕的准确率分别达到65.0%和70.7%，并在多个基准测试中持续优于LongVILA-R1。此外，随着输入视频帧数的增加，LongVILA-R1显示出稳定的性能提升。值得注意的是，我们的MR-SP系统在长视频RL训练中实现了高达2.1倍的加速。此外，我们发布了可公开获取的训练系统，该系统支持在各种模态（视频、文本和音频）、各种模型（VILA和Qwen系列）甚至图像和视频生成模型上进行RL训练。在单个A100节点（8个GPU）上，它支持长达一小时的视频（例如3,600帧/约256k tokens）的RL训练。", "summary": "本文提出了一个名为LongVILA的全栈框架，旨在利用强化学习将视觉语言模型（VLM）的推理能力扩展到长视频。该框架通过引入LongVideo-Reason数据集、结合CoT-SFT和RL的两阶段训练管道，以及高效的MR-SP训练基础设施来解决长视频推理的挑战。实验结果表明，LongVILA-R1-7B在多个视频基准测试中表现出色，并在长视频RL训练中实现了显著的加速。研究团队还发布了支持多模态和多种模型的训练系统。", "keywords": "强化学习, 长视频, 视觉语言模型, 数据集, 并行训练", "comments": "这篇论文的创新点在于提出了一个将强化学习应用于长视频VLM推理的全栈解决方案。它不仅构建了大规模的长视频QA数据集，还设计了一个结合CoT-SFT和RL的训练范式，并开发了高效的并行训练系统MR-SP。特别值得关注的是MR-SP在长视频RL训练中的显著加速效果，以及其支持多种模态和模型的通用性，这对于未来多模态大模型的训练具有重要意义。该工作解决了长视频处理中的关键挑战，并为相关研究提供了宝贵的资源和工具。"}}
{"id": "2507.18177", "title": "Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios", "authors": ["Dhruv Jain", "Romain Modzelewski", "Romain Hérault", "Clement Chatelain", "Eva Torfeh", "Sebastien Thureau"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18177v1", "summary": "In data-scarce scenarios, deep learning models often overfit to noise and\nirrelevant patterns, which limits their ability to generalize to unseen\nsamples. To address these challenges in medical image segmentation, we\nintroduce Diff-UMamba, a novel architecture that combines the UNet framework\nwith the mamba mechanism for modeling long-range dependencies. At the heart of\nDiff-UMamba is a Noise Reduction Module (NRM), which employs a signal\ndifferencing strategy to suppress noisy or irrelevant activations within the\nencoder. This encourages the model to filter out spurious features and enhance\ntask-relevant representations, thereby improving its focus on clinically\nmeaningful regions. As a result, the architecture achieves improved\nsegmentation accuracy and robustness, particularly in low-data settings.\nDiff-UMamba is evaluated on multiple public datasets, including MSD (lung and\npancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over\nbaseline methods across diverse segmentation tasks. To further assess\nperformance under limited-data conditions, additional experiments are conducted\non the BraTS-21 dataset by varying the proportion of available training\nsamples. The approach is also validated on a small internal non-small cell lung\ncancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam\nCT (CBCT), where it achieves a 4-5% improvement over the baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18177v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "差异化UMamba：重新思考有限数据场景下的肿瘤分割", "tldr": "Diff-UMamba结合UNet和Mamba，通过噪声抑制提高在数据稀缺场景下肿瘤分割的准确性和鲁棒性。", "motivation": "在数据稀缺场景下，深度学习模型容易过拟合噪声和不相关模式，限制了其对未知样本的泛化能力，尤其是在医学图像分割中。", "method": "提出Diff-UMamba，一种结合UNet框架和Mamba机制的新型架构。其核心是噪声抑制模块（NRM），采用信号差分策略抑制编码器中的噪声或不相关激活，从而过滤掉虚假特征并增强任务相关表示。", "result": "在MSD（肺部和胰腺）和AIIB23等多个公共数据集上，性能比基线方法提高1-3%；在BraTS-21数据集上评估了有限数据条件下的性能；在小型内部非小细胞肺癌（NSCLC）锥形束CT（CBCT）毛体积肿瘤（GTV）分割数据集上，比基线提高了4-5%。", "conclusion": "Diff-UMamba在低数据设置下实现了改进的分割准确性和鲁棒性。", "translation": "在数据稀缺的场景中，深度学习模型经常会过拟合噪声和不相关的模式，这限制了它们对未知样本的泛化能力。为了解决医学图像分割中的这些挑战，我们引入了Diff-UMamba，这是一种新颖的架构，它将UNet框架与Mamba机制相结合，用于建模长程依赖。Diff-UMamba的核心是一个噪声抑制模块（NRM），该模块采用信号差分策略来抑制编码器中嘈杂或不相关的激活。这鼓励模型过滤掉虚假特征并增强任务相关的表示，从而提高其对临床有意义区域的关注。因此，该架构实现了改进的分割准确性和鲁棒性，特别是在低数据设置下。Diff-UMamba在多个公共数据集（包括MSD（肺部和胰腺）和AIIB23）上进行了评估，在各种分割任务中，其性能比基线方法持续提高了1-3%。为了进一步评估有限数据条件下的性能，还在BraTS-21数据集上通过改变可用训练样本的比例进行了额外实验。该方法还在一个小型内部非小细胞肺癌（NSCLC）锥形束CT（CBCT）毛体积肿瘤（GTV）分割数据集上进行了验证，其性能比基线提高了4-5%。", "summary": "本文提出了Diff-UMamba，一种结合UNet和Mamba机制的新型架构，旨在解决数据稀缺场景下深度学习模型在医学图像分割中的过拟合问题。Diff-UMamba的核心是噪声抑制模块（NRM），通过信号差分策略有效抑制噪声和不相关激活，从而增强模型对任务相关特征的关注。实验结果表明，Diff-UMamba在多个公共数据集和内部数据集上均表现出优于基线方法的分割准确性和鲁棒性，尤其是在低数据环境下。", "keywords": "肿瘤分割, 有限数据, UNet, Mamba, 噪声抑制", "comments": "该论文的创新点在于引入了噪声抑制模块（NRM）及其信号差分策略，有效地解决了深度学习模型在数据稀缺医学图像分割中常见的过拟合问题。通过过滤噪声和增强任务相关特征，Diff-UMamba显著提升了模型在有限数据场景下的泛化能力、准确性和鲁棒性，对于临床应用具有重要意义。"}}
{"id": "2506.08418", "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation", "authors": ["Taiqin Chen", "Zikun Zhou", "Zheng Fang", "Wenzhen Zou", "Kangjun Liu", "Ke Chen", "Yongbing Zhang", "Yaowei Wang"], "categories": ["cs.CV", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08418v2", "summary": "The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08418v2", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-24", "AI": {"title_translation": "RadioDUN: 一种物理启发式深度展开网络用于无线电图估计", "tldr": "提出RadioDUN，一种结合物理模型的深度展开网络，用于从稀疏样本中高效估计密集无线电图，性能优于现有方法。", "motivation": "构建密集的无线电图很困难，因为实际场景中测量样本有限。现有深度学习方法难以整合无线电图的物理特性。", "method": "将无线电图估计视为稀疏信号恢复问题。引入物理传播模型将问题分解为多个因子优化子问题。提出RadioDUN，通过展开优化过程实现自适应参数调整和先验拟合。开发动态重加权模块(DRM)来建模每个因子对无线电图的重要性。整合障碍相关因子以表达障碍引起的信号随机衰减，并设计阴影损失作为补充监督目标。", "result": "实验证明所提出的方法优于最先进的方法。", "conclusion": "Not mentioned in abstract", "translation": "无线电图表示区域内的频谱资源空间分布，支持高效的资源分配和干扰缓解。然而，由于在实际场景中只能测量有限数量的样本，构建密集的无线电图十分困难。尽管现有工作已使用深度学习从稀疏样本中估计密集无线电图，但它们难以与无线电图的物理特性相结合。为解决这一挑战，我们将无线电图估计视为稀疏信号恢复问题。进一步结合物理传播模型将问题分解为多个因子优化子问题，从而降低恢复复杂性。受现有压缩感知方法的启发，我们提出了无线电深度展开网络（RadioDUN），以展开优化过程，以可学习的方式实现自适应参数调整和先验拟合。为了考虑无线电传播特性，我们开发了一个动态重加权模块（DRM），以自适应地建模每个因子对无线电图的重要性。受物理传播模型中阴影因子的启发，我们整合了障碍相关因子来表达障碍引起的信号随机衰减。进一步设计了阴影损失，以约束因子预测并作为补充监督目标，从而增强了RadioDUN的性能。已进行了大量实验，证明所提出的方法优于最先进的方法。我们的代码将在发布后公开。", "summary": "本文针对从稀疏样本估计密集无线电图的挑战，提出了一种名为RadioDUN的物理启发式深度展开网络。该方法将无线电图估计视为稀疏信号恢复问题，并结合物理传播模型进行分解。RadioDUN通过展开优化过程实现参数自适应和先验拟合，并引入动态重加权模块和阴影损失来融入无线电传播和障碍特性。实验证明，RadioDUN的性能优于现有最先进的方法。", "keywords": "无线电图估计, 深度展开网络, 物理启发, 稀疏信号恢复, 无线电传播", "comments": "该论文创新性地将物理传播模型与深度学习相结合，通过深度展开网络解决了无线电图估计中样本稀疏和物理特性难以整合的问题。特别是动态重加权模块和阴影损失的设计，有效利用了无线电传播的物理知识，提升了模型的性能和可解释性。这对于实际的频谱资源管理和干扰缓解具有重要意义。"}}
{"id": "2507.18551", "title": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration", "authors": ["Daniil Morozov", "Reuben Dorent", "Nazim Haouchine"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.18551v1", "summary": "Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.18551v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于MR-US匹配和配准的3D跨模态关键点描述符", "tldr": "提出了一种新颖的3D跨模态关键点描述符，通过合成超声图像和对比学习，实现MRI与实时超声图像的自动配准，并表现出优于现有方法的性能和鲁棒性。", "motivation": "由于外观、分辨率和视野的显著模态差异，术中实时超声(iUS)与术前磁共振成像(MRI)的配准仍然是一个未解决的问题。", "method": "该方法提出了一种3D跨模态关键点描述符。它采用患者特异性合成匹配方法，从术前MRI生成合成iUS图像，以实现监督对比训练来学习共享描述符空间。然后，采用概率关键点检测策略识别解剖学上显著且模态一致的位置。训练中，使用基于课程的三元组损失和动态硬负样本挖掘来学习对iUS伪影和有限覆盖范围具有鲁棒性，并且旋转不变的描述符。在推理时，该方法检测MR和真实iUS图像中的关键点，识别稀疏匹配，然后用于执行刚性配准。", "result": "该方法在ReMIND数据集上的3D MRI-iUS配对上进行了评估，在11名患者中表现优于最先进的关键点匹配方法，平均精度为69.8%。对于图像配准，该方法在ReMIND2Reg基准测试中实现了2.39毫米的竞争性平均目标配准误差。与现有iUS-MR配准方法相比，该框架具有可解释性，无需手动初始化，并对iUS视野变化表现出鲁棒性。", "conclusion": "该研究提出了一种新颖的3D跨模态关键点描述符，有效解决了术中MRI-iUS配准的挑战，实现了高精度和鲁棒性，且无需手动初始化，提高了临床应用的潜力。", "translation": "术中实时超声(iUS)与术前磁共振成像(MRI)的配准仍然是一个未解决的问题，原因在于外观、分辨率和视野存在严重的模态特异性差异。为了解决这个问题，我们提出了一种新颖的3D跨模态关键点描述符，用于MRI-iUS匹配和配准。我们的方法采用患者特异性合成匹配方法，从术前MRI生成合成iUS体数据。这使得监督对比训练能够学习共享的描述符空间。然后，采用概率关键点检测策略来识别解剖学上显著且模态一致的位置。在训练过程中，使用基于课程的三元组损失和动态硬负样本挖掘来学习i) 对散斑噪声和有限覆盖范围等iUS伪影具有鲁棒性，以及 ii) 旋转不变的描述符。在推理时，该方法检测MR和真实iUS图像中的关键点，并识别稀疏匹配，然后用于执行刚性配准。我们的方法使用来自ReMIND数据集的3D MRI-iUS配对进行了评估。实验表明，我们的方法在11名患者中优于最先进的关键点匹配方法，平均精度为69.8%。对于图像配准，我们的方法在ReMIND2Reg基准测试中实现了2.39毫米的竞争性平均目标配准误差。与现有iUS-MR配准方法相比，我们的框架具有可解释性，无需手动初始化，并对iUS视野变化表现出鲁棒性。代码可在https://github.com/morozovdd/CrossKEY获取。", "summary": "本研究提出了一种用于MRI-实时超声(iUS)配准的新型3D跨模态关键点描述符。该方法通过从MRI合成iUS图像，利用监督对比学习构建共享描述符空间，并结合概率关键点检测和课程学习的三元组损失，确保描述符对iUS伪影和旋转具有鲁棒性。实验结果表明，该方法在关键点匹配和图像配准方面均优于现有技术，实现了高精度和鲁棒性，且无需手动初始化，为术中图像引导提供了有效解决方案。", "keywords": "跨模态配准, 关键点描述符, MRI, 超声, 深度学习", "comments": "该论文的创新点在于提出了一种通过合成数据进行监督对比学习的3D跨模态关键点描述符，有效克服了MRI和iUS之间的模态差异。其无需手动初始化和对视野变化的鲁棒性是重要的优势，提高了临床应用的便利性和可靠性。未来研究可以探索其在更多临床场景下的泛化能力。"}}
{"id": "2507.09871", "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "authors": ["Niket Patel", "Randall Balestriero"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09871v2", "summary": "The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09871v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-07-23", "AI": {"title_translation": "任务先验：通过考虑下游任务的整个空间来增强模型评估", "tldr": "当前AI模型评估受限于固定基准，本文提出了“任务先验”以概率性地评估模型在所有可能的下游任务上的性能。", "motivation": "当前AI模型评估依赖于固定且手动挑选的基准，这在AI研究中造成了“无声的瓶颈”，未能反映AI解决任何任务的宏伟目标。特别是在自监督学习（SSL）中，下游任务评估是研究人员唯一可获得的定性信号，因此需要更全面的评估方法。", "method": "通过采用任务分布和定义“任务先验”，本文定义了一个下游任务的概率空间，从而可以在所有可能的下游任务上评估模型的性能。", "result": "该框架首次能够回答诸如模型在所有可能下游任务上的加权平均性能以及性能方差等关键问题。它为模型评估建立了一个新的标准。", "conclusion": "“任务先验”将建立一个新的评估标准，并通过提供更全面的定性信号来加速自监督学习（SSL）的研究步伐。", "translation": "AI研究，特别是自监督学习（SSL）的宏伟目标是产生能够成功解决任何可能任务的系统。相比之下，当前AI研究人员可用的评估方法通常依赖于固定集合的手动挑选的下游基准。因此，投入了大量精力来设计和寻找大量的评估任务，以作为我们宏伟目标的代理。我们认为这种僵化的评估协议在AI研究中造成了一个无声的瓶颈。为了弥补这一点，我们通过采用任务分布和定义任务先验来定义一个下游任务的概率空间。在这种观点下，可以评估模型在所有可能的下游任务上的性能。我们的框架首次回答了关键问题，例如（i）我的模型在所有可能的下游任务上的平均性能如何，并根据遇到每个任务的概率进行加权？或（ii）在定义的任务先验下，我的模型在所有下游任务上的性能方差是多少？除了建立新的评估标准外，我们相信任务先验将加速SSL研究的步伐——在SSL中，下游任务评估是研究人员唯一可获得的定性信号。", "summary": "本文旨在解决当前AI模型评估依赖于固定基准的局限性，提出了“任务先验”的概念。通过定义一个下游任务的概率空间，并结合任务分布和任务先验，该方法允许在所有可能的下游任务上对模型性能进行评估，并考虑任务的出现概率。这种框架提供了更全面和鲁棒的评估方式，旨在加速AI，特别是自监督学习（SSL）的研究，因为它能提供更丰富的定性信号。", "keywords": "任务先验, 模型评估, 下游任务, 自监督学习, 概率空间", "comments": "该论文的创新之处在于将模型评估从离散、固定的基准测试转向了一个连续、概率性的任务空间。这对于实现更具泛化能力的AI至关重要，并且可能通过提供更全面的评估指标，显著影响自监督学习领域。它挑战了传统的以基准驱动的评估范式。"}}
{"id": "2507.18315", "title": "Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking", "authors": ["Rhys Jacka", "Paola R. Peña", "Sophie Leonard", "Éva Székely", "Benjamin R. Cowan"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      12 pages, 3 figures, in Proceedings of the 7th ACM Conference on Conversational User Interfaces", "url": "http://arxiv.org/abs/2507.18315v1", "summary": "Speech disfluencies play a role in perspective-taking and audience design in\nhuman-human communication (HHC), but little is known about their impact in\nhuman-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one\nparticipants interacted with a speech agent using either fluent or disfluent\nspeech. Participants completed a partner-modelling questionnaire (PMQ) both\nbefore and after the task. Post-interaction evaluations indicated that\nparticipants perceived the disfluent agent as more competent, despite no\nsignificant differences in pre-task ratings. However, no notable differences\nwere observed in assessments of conversational flexibility or human-likeness.\nOur findings also reveal evidence of egocentric and allocentric language\nproduction when participants interact with speech agents. Interaction with\ndisfluent speech agents appears to increase egocentric communication in\ncomparison to fluent agents. Although the wide credibility intervals mean this\neffect is not clear-cut. We discuss potential interpretations of this finding,\nfocusing on how disfluencies may impact partner models and language production\nin HMD.", "comment": "12 pages, 3 figures, in Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces", "pdf_url": "http://arxiv.org/pdf/2507.18315v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "与……呃……嗯……机器对话：不流畅语音代理对伙伴模型和视角采纳的影响", "tldr": "研究发现，在人机对话中，用户倾向于认为语音不流畅的机器更具能力，并且可能导致用户产生更自我中心的交流方式，尽管后者效果不明确。", "motivation": "语音不流畅在人际交流中对视角采纳和听众设计有影响，但在人机对话（HMD）中的影响尚不清楚，因此需要进行探索。", "method": "研究通过一项在线命名匹配任务进行，61名参与者与使用流畅或不流畅语音的代理进行交互。参与者在任务前后完成了伙伴建模问卷。", "result": "交互后评估显示，参与者认为不流畅代理更具能力，尽管任务前评分无显著差异。但在对话灵活性或类人度方面未观察到显著差异。与不流畅语音代理交互似乎增加了以自我为中心的交流，尽管由于宽泛的可信区间，此效果不明确。", "conclusion": "本研究讨论了语音不流畅可能如何影响人机对话中的伙伴模型和语言产生，提出了潜在的解释。", "translation": "语音不流畅在人际交流（HHC）中的视角采纳和听众设计中发挥作用，但其在人机对话（HMD）中的影响知之甚少。在一项在线命名匹配任务中，61名参与者与使用流畅或不流畅语音的语音代理进行了交互。参与者在任务前后均完成了一份伙伴建模问卷（PMQ）。交互后的评估表明，尽管任务前的评分没有显著差异，参与者认为不流畅代理更具能力。然而，在对话灵活性或类人度方面未观察到显著差异。我们的发现还揭示了参与者与语音代理交互时存在以自我为中心和以他人为中心的语言产生证据。与不流畅语音代理的交互似乎增加了以自我为中心的交流，与流畅代理相比。尽管宽泛的可信区间意味着这种效果并不明确。我们讨论了这一发现的潜在解释，重点关注不流畅性如何影响HMD中的伙伴模型和语言产生。", "summary": "本研究探讨了语音不流畅的机器代理对人机对话中伙伴模型和视角采纳的影响。通过一项在线命名匹配任务，61名参与者与流畅或不流畅的语音代理互动。结果显示，交互后参与者认为不流畅代理更具能力，但在对话灵活性或类人度方面无显著差异。研究还发现，与不流畅代理的交互可能导致参与者产生更多以自我为中心的语言，尽管这一效果的明确性受限于宽泛的可信区间。研究讨论了这些发现对人机对话中语音不流畅影响伙伴模型和语言产生的潜在解释。", "keywords": "语音不流畅, 人机对话, 伙伴模型, 视角采纳, 语言产生", "comments": "这项研究创新性地探讨了人机对话中语音不流畅的影响，揭示了与人类交流中不同的感知模式（如对能力的感知）。然而，关于以自我为中心交流的发现，由于“宽泛的可信区间”和“不明确”的表述，其结论的确定性有所减弱，这可能是一个局限性。"}}
{"id": "2507.12106", "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "categories": ["cs.DC", "cs.CY"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      18 pages, 6 Figures", "url": "http://arxiv.org/abs/2507.12106v3", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "comment": "18 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2507.12106v3", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-24", "AI": {"title_translation": "城市绿色治理：物联网驱动的坎波巴索城市绿地管理与提升", "tldr": "该项目通过集成物联网系统和数据驱动平台，实现坎波巴索城市绿地的可持续管理，优化灌溉并提供实时决策支持，从而提升城市环境韧性和居民生活质量。", "motivation": "城市公共绿地的有效设计和管理对促进城市人口健康福祉至关重要，它们是城市生态系统的“绿色之肺”，通过提供生态服务提升生活质量。", "method": "该项目在坎波巴索市采用先进的集成和可互操作的新兴技术系统，整合物联网系统和数据驱动的治理平台，通过决策支持系统（DSS）实时监测树木和绿地的健康状况，并收集分析气象条件、空气质量、土壤湿度、污染水平等数据。利用Tree Talker传感器、土壤湿度和水势监测系统进行智能控制和管理，并通过基于机器学习算法和物联网传感器实时数据的预测模型优化公园灌溉。还激活定制警报层。", "result": "建立了基于云的平台，支持绿地管理者、技术专家和操作人员进行整体实时决策。该平台能够通过Tree Talker传感器和相关监测系统智能控制和管理城市绿地，并能通过预测模型优化公共公园的灌溉，提供何时以及灌溉多少的建议。当监测参数（如土壤温度、湿度或水势）超出预设阈值时，会激活定制警报层。", "conclusion": "数字化、物联网传感器融合和技术创新可以支持可持续的城市治理，促进环境韧性并改善公民生活质量。", "translation": "城市公共绿地的有效设计和管理是促进城市人口健康和福祉的关键因素，这一点得到了世界卫生组织（WHO）、联合国环境规划署（UNEP）和欧洲环境署（EEA）的强调。这些区域作为城市生态系统的“绿色之肺”，通过提供生态系统服务在提升生活质量方面发挥着至关重要的作用。在此背景下，由意大利企业部（MIMIT）资助的坎波巴索市智能绿色城市用例，作为通过采用先进的集成和可互操作的新兴技术系统实现城市绿地可持续管理的创新模式而出现。该项目整合了物联网系统和数据驱动的治理平台，通过决策支持系统（DSS）实现对树木和绿地健康状况的实时监测。它还促进了来自不同来源的数据收集和分析，包括天气状况、空气质量、土壤湿度、污染水平。由此产生的基于云的平台支持绿地管理者、技术专家和操作人员进行整体实时决策。它利用Tree Talker传感器，并与土壤湿度和水势监测系统集成，实现城市绿地的智能控制和管理。通过基于机器学习算法和物联网传感器提供的实时数据的预测模型，可以优化公共公园的灌溉，提供何时以及灌溉多少水的建议。当监测参数（如土壤温度、湿度或水势）超出预定义阈值时，也会激活定制警报层。该用例展示了数字化、物联网传感器融合和技术创新如何支持可持续的城市治理，促进环境韧性并改善公民生活质量。", "summary": "该论文介绍了坎波巴索市“智能绿色城市”项目，旨在通过物联网（IoT）和数据驱动平台实现城市绿地的可持续管理。该项目整合了先进技术，实时监测绿地健康状况，收集并分析环境数据，并通过基于云的决策支持系统优化灌溉和管理。研究展示了数字化和物联网如何促进可持续城市治理，提升环境韧性和居民生活质量。", "keywords": "城市绿色治理, 物联网, 智能城市, 绿地管理, 可持续发展", "comments": "该论文展示了一个实用的城市绿色治理创新案例，通过整合物联网传感器、数据分析和机器学习，实现了城市绿地的精细化、智能化管理。其创新之处在于将多种新兴技术应用于具体的城市管理问题，并提供了实时决策支持。这对于提升城市生态服务功能、改善居民福祉具有重要意义。该项目的成功实施为其他城市提供了可借鉴的模式。"}}
{"id": "2503.01075", "title": "Tackling Hallucination from Conditional Models for Medical Image Reconstruction with DynamicDPS", "authors": ["Seunghoi Kim", "Henry F. J. Tregidgo", "Matteo Figini", "Chen Jin", "Sarang Joshi", "Daniel C. Alexander"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.01075v2", "summary": "Hallucinations are spurious structures not present in the ground truth,\nposing a critical challenge in medical image reconstruction, especially for\ndata-driven conditional models. We hypothesize that combining an unconditional\ndiffusion model with data consistency, trained on a diverse dataset, can reduce\nthese hallucinations. Based on this, we propose DynamicDPS, a diffusion-based\nframework that integrates conditional and unconditional diffusion models to\nenhance low-quality medical images while systematically reducing\nhallucinations. Our approach first generates an initial reconstruction using a\nconditional model, then refines it with an adaptive diffusion-based inverse\nproblem solver. DynamicDPS skips early stage in the reverse process by\nselecting an optimal starting time point per sample and applies Wolfe's line\nsearch for adaptive step sizes, improving both efficiency and image fidelity.\nUsing diffusion priors and data consistency, our method effectively reduces\nhallucinations from any conditional model output. We validate its effectiveness\nin Image Quality Transfer for low-field MRI enhancement. Extensive evaluations\non synthetic and real MR scans, including a downstream task for tissue volume\nestimation, show that DynamicDPS reduces hallucinations, improving relative\nvolume estimation by over 15% for critical tissues while using only 5% of the\nsampling steps required by baseline diffusion models. As a model-agnostic and\nfine-tuning-free approach, DynamicDPS offers a robust solution for\nhallucination reduction in medical imaging. The code will be made publicly\navailable upon publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.01075v2", "cate": "eess.IV", "date": "2025-03-03", "updated": "2025-07-24", "AI": {"title_translation": "采用DynamicDPS解决条件模型在医学图像重建中的幻影问题", "tldr": "DynamicDPS是一种基于扩散的框架，旨在解决医学图像重建中条件模型产生的幻影问题，提高图像质量和效率。", "motivation": "医学图像重建中，特别是数据驱动的条件模型，产生的幻影（虚假结构）是一个严峻的挑战。", "method": "本文提出了DynamicDPS，一个基于扩散的框架，它整合了条件和非条件扩散模型来增强低质量医学图像并系统性地减少幻影。该方法首先使用条件模型生成初始重建，然后通过自适应扩散逆问题求解器对其进行优化。DynamicDPS通过为每个样本选择最佳起始时间点来跳过逆向过程的早期阶段，并应用Wolfe线搜索以实现自适应步长，从而提高效率和图像保真度。", "result": "DynamicDPS有效减少了幻影，将关键组织的相对体积估计提高了15%以上，并且仅使用了基线扩散模型所需采样步骤的5%。它在低场MRI增强的图像质量迁移中验证了其有效性，并在合成和真实MR扫描上进行了广泛评估。", "conclusion": "DynamicDPS作为一种模型无关且无需微调的方法，为医学成像中的幻影消除提供了一个强大的、鲁棒的解决方案。", "translation": "幻影是真实图像中不存在的虚假结构，在医学图像重建中，特别是对于数据驱动的条件模型，构成了严峻的挑战。我们假设将经过多样化数据集训练的非条件扩散模型与数据一致性相结合，可以减少这些幻影。基于此，我们提出了DynamicDPS，一个基于扩散的框架，它整合了条件和非条件扩散模型，旨在增强低质量医学图像，同时系统性地减少幻影。我们的方法首先使用条件模型生成初始重建，然后通过自适应扩散逆问题求解器对其进行优化。DynamicDPS通过为每个样本选择最佳起始时间点来跳过逆向过程的早期阶段，并应用Wolfe线搜索以实现自适应步长，从而提高效率和图像保真度。利用扩散先验和数据一致性，我们的方法有效减少了任何条件模型输出中的幻影。我们在低场MRI增强的图像质量迁移中验证了其有效性。对合成和真实MR扫描（包括组织体积估算这一下游任务）的广泛评估表明，DynamicDPS减少了幻影，将关键组织的相对体积估计提高了15%以上，同时仅使用了基线扩散模型所需采样步骤的5%。作为一种模型无关且无需微调的方法，DynamicDPS为医学成像中的幻影消除提供了一个强大的解决方案。代码将在发布后公开提供。", "summary": "DynamicDPS是一种基于扩散的框架，旨在解决医学图像重建中条件模型产生的幻影问题。该方法结合了条件和非条件扩散模型，通过自适应扩散逆问题求解器优化初始重建，并通过跳过早期逆过程阶段和应用Wolfe线搜索来提高效率和图像保真度。实验证明，DynamicDPS能有效减少幻影，将关键组织的相对体积估计提高15%以上，同时仅需基线扩散模型5%的采样步数。作为一种模型无关且无需微调的方法，DynamicDPS为医学成像中的幻影消除提供了一个强大的解决方案。", "keywords": "医学图像重建, 幻影, 扩散模型, DynamicDPS, 图像质量迁移", "comments": "DynamicDPS的创新之处在于其结合条件和非条件扩散模型来解决医学图像重建中的幻影问题，并引入了优化效率的策略（跳过早期逆过程和自适应步长）。其模型无关和无需微调的特性大大提升了其在实际应用中的普适性和便捷性。在效率上，仅需5%的采样步数即可达到显著效果，展现了其在计算资源受限环境下的潜力。"}}
{"id": "2507.18580", "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition", "authors": ["Jiahao Wang", "Ramen Liu", "Longhui Zhang", "Jing Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, accepted as oral presentation at CCL25-Eval", "url": "http://arxiv.org/abs/2507.18580v1", "summary": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV.", "comment": "8 pages, 3 figures, accepted as oral presentation at CCL25-Eval", "pdf_url": "http://arxiv.org/pdf/2507.18580v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CCL25-Eval 任务10系统报告：用于细粒度中文仇恨言论识别的SRAG-MAV", "tldr": "该论文介绍了一个名为SRAG-MAV的新框架，用于细粒度中文仇恨言论识别，其在STATE ToxiCN数据集上显著优于基线模型。", "motivation": "该研究旨在解决细粒度中文仇恨言论识别（FGCHSR）任务，并提出一个新颖的框架以提高识别性能。", "method": "本文提出了SRAG-MAV框架，该框架协同整合了任务重构（将四元组提取重构为三元组提取）、自检索增强生成（从训练集动态检索创建上下文提示）和多轮累积投票（提高输出稳定性和性能）。该系统基于Qwen2.5-7B模型。", "result": "该系统在STATE ToxiCN数据集上取得了Hard Score 26.66、Soft Score 48.35和Average Score 37.505。这显著优于基线模型，如GPT-4o（平均分数15.63）和微调后的Qwen2.5-7B（平均分数35.365）。", "conclusion": "SRAG-MAV框架在细粒度中文仇恨言论识别任务中表现出色，显著优于现有基线模型，证明了其有效性。", "translation": "本文介绍了我们针对CCL25-Eval任务10的系统，旨在解决细粒度中文仇恨言论识别（FGCHSR）问题。我们提出了一个新颖的SRAG-MAV框架，协同整合了任务重构（TR）、自检索增强生成（SRAG）和多轮累积投票（MAV）。我们的方法将四元组提取任务重构为三元组提取，利用训练集中的动态检索来创建上下文提示，并应用带投票的多轮推理以提高输出稳定性和性能。我们的系统基于Qwen2.5-7B模型，在STATE ToxiCN数据集上取得了26.66的Hard分数、48.35的Soft分数和37.505的平均分数，显著优于GPT-4o（平均分数15.63）和微调Qwen2.5-7B（平均分数35.365）等基线模型。代码可在https://github.com/king-wang123/CCL25-SRAG-MAV获取。", "summary": "该论文提出了一个名为SRAG-MAV的新框架，用于CCL25-Eval任务10的细粒度中文仇恨言论识别。该框架通过任务重构、自检索增强生成和多轮累积投票机制，显著提升了识别性能。基于Qwen2.5-7B模型，SRAG-MAV系统在STATE ToxiCN数据集上取得了优异的平均分数，并显著超越了GPT-4o和微调Qwen2.5-7B等基线模型。", "keywords": "细粒度仇恨言论识别, 中文, SRAG-MAV, 检索增强生成, 多轮投票", "comments": "该论文的创新点在于提出了一个结合任务重构、检索增强和投票机制的SRAG-MAV框架，有效提升了细粒度中文仇恨言论识别的性能。特别是其在四元组提取任务上的重构以及动态检索的使用，为解决复杂文本分类问题提供了新的思路。其在基线模型上的显著提升也证明了该框架的有效性和实用价值。"}}
{"id": "2503.23461", "title": "TextCrafter: Accurately Rendering Multiple Texts in Complex Visual Scenes", "authors": ["Nikai Du", "Zhennan Chen", "Zhizhou Chen", "Shan Gao", "Xi Chen", "Zhengkai Jiang", "Jian Yang", "Ying Tai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.23461v4", "summary": "This paper explores the task of Complex Visual Text Generation (CVTG), which\ncenters on generating intricate textual content distributed across diverse\nregions within visual images. In CVTG, image generation models often rendering\ndistorted and blurred visual text or missing some visual text. To tackle these\nchallenges, we propose TextCrafter, a novel multi-visual text rendering method.\nTextCrafter employs a progressive strategy to decompose complex visual text\ninto distinct components while ensuring robust alignment between textual\ncontent and its visual carrier. Additionally, it incorporates a token focus\nenhancement mechanism to amplify the prominence of visual text during the\ngeneration process. TextCrafter effectively addresses key challenges in CVTG\ntasks, such as text confusion, omissions, and blurriness. Moreover, we present\na new benchmark dataset, CVTG-2K, tailored to rigorously evaluate the\nperformance of generative models on CVTG tasks. Extensive experiments\ndemonstrate that our method surpasses state-of-the-art approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.23461v4", "cate": "cs.CV", "date": "2025-03-30", "updated": "2025-07-24", "AI": {"title_translation": "TextCrafter：在复杂视觉场景中精确渲染多文本", "tldr": "TextCrafter是一种新的方法，用于在复杂图像中准确生成多个文本，解决了现有模型文本模糊、缺失等问题，并提出了新的评估数据集。", "motivation": "现有图像生成模型在复杂视觉文本生成（CVTG）任务中，经常生成扭曲、模糊或缺失的视觉文本。", "method": "本文提出了TextCrafter，一种新颖的多视觉文本渲染方法。TextCrafter采用渐进式策略将复杂视觉文本分解为不同的组件，确保文本内容与视觉载体的稳健对齐。此外，它还结合了token焦点增强机制，以在生成过程中增强视觉文本的突出性。论文还提出了一个新的基准数据集CVTG-2K，用于评估CVTG任务的性能。", "result": "TextCrafter有效地解决了CVTG任务中的文本混淆、遗漏和模糊等关键挑战。广泛的实验表明，该方法超越了最先进的方法。", "conclusion": "TextCrafter能够有效、准确地在复杂视觉场景中渲染多文本，并设立了新的性能基准。", "translation": "本文探讨了复杂视觉文本生成（CVTG）任务，该任务的核心是在视觉图像中不同区域生成复杂的文本内容。在CVTG中，图像生成模型通常会渲染出扭曲、模糊的视觉文本或缺失部分视觉文本。为了解决这些挑战，我们提出了TextCrafter，一种新颖的多视觉文本渲染方法。TextCrafter采用渐进式策略将复杂视觉文本分解为不同的组件，同时确保文本内容与其视觉载体之间的稳健对齐。此外，它还结合了token焦点增强机制，以在生成过程中增强视觉文本的突出性。TextCrafter有效地解决了CVTG任务中的关键挑战，例如文本混淆、遗漏和模糊。此外，我们提出了一个新的基准数据集CVTG-2K，专门用于严格评估生成模型在CVTG任务上的性能。广泛的实验表明，我们的方法超越了最先进的方法。", "summary": "本文针对复杂视觉文本生成（CVTG）中现有模型文本渲染失真、模糊、缺失等问题，提出了TextCrafter，一种新颖的多视觉文本渲染方法。TextCrafter采用渐进式分解策略和token焦点增强机制，有效解决了文本混淆、遗漏和模糊等挑战。同时，论文还发布了新的CVTG-2K基准数据集。实验证明TextCrafter优于现有SOTA方法。", "keywords": "复杂视觉文本生成, 多文本渲染, TextCrafter, 图像生成, CVTG-2K", "comments": "TextCrafter的创新之处在于其渐进式分解策略和token焦点增强机制，这些机制有助于提高复杂视觉场景中多文本渲染的准确性和清晰度。此外，提出新的CVTG-2K数据集对于推动该领域的研究具有重要意义，因为它提供了更严格的评估标准。"}}
{"id": "2507.18476", "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning", "authors": ["Busra Icoz", "Goksel Biricik"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18476v1", "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18476v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用大型语言模型结合符号推理的自动化代码审查", "tldr": "本文提出一种结合大型语言模型（LLMs）和符号推理的混合方法，用于自动化代码审查，有效提高了准确性和效率。", "motivation": "手动代码审查主观且耗时；现有大型语言模型（LLMs）在代码审查中缺乏逻辑推理能力，无法完全理解和评估代码。", "method": "提出一种混合方法，将符号推理技术与大型语言模型（LLMs）集成，以自动化代码审查过程。在CodexGlue数据集上进行了测试，并与CodeT5、CodeBERT和GraphCodeBERT等模型进行了比较，以评估结合符号推理和提示技术与LLMs的有效性。", "result": "该方法提高了自动化代码审查的准确性和效率。", "conclusion": "结合符号推理和大型语言模型（LLMs）的混合方法能够有效提升自动化代码审查的性能。", "translation": "代码审查是软件开发生命周期中的关键过程之一，对于维护代码质量至关重要。然而，手动代码审查具有主观性且耗时。鉴于其基于规则的性质，代码审查非常适合自动化。近年来，在人工智能的帮助下，人们为自动化这一过程做出了巨大努力。大型语言模型（LLMs）的最新发展也已成为该领域有前景的工具，但这些模型通常缺乏完全理解和评估代码所需的逻辑推理能力。为了克服这一限制，本研究提出了一种混合方法，将符号推理技术与LLMs集成，以自动化代码审查过程。我们使用CodexGlue数据集测试了我们的方法，并比较了包括CodeT5、CodeBERT和GraphCodeBERT在内的几种模型，以评估将符号推理和提示技术与LLMs结合的有效性。我们的结果表明，这种方法提高了自动化代码审查的准确性和效率。", "summary": "本文提出一种结合大型语言模型（LLMs）和符号推理的混合方法，旨在解决现有LLMs在代码审查中缺乏逻辑推理能力的问题。通过在CodexGlue数据集上的实验，并与CodeT5等模型比较，证明该方法能有效提高自动化代码审查的准确性和效率。", "keywords": "自动化代码审查, 大型语言模型, 符号推理, 代码质量, 人工智能", "comments": "该研究的创新点在于结合了LLM的强大语言理解能力和符号推理的逻辑严谨性，弥补了LLM在代码逻辑推理上的不足，为自动化代码审查提供了一个有前景的新方向。"}}
{"id": "2507.18001", "title": "Quantitative Damping Calculation and Compensation Method for Global Stability Improvement of Inverter-Based Systems", "authors": ["Yang Li", "Zenghui Zheng", "Xiangyang Wu", "Jiayong Li", "Wei Wang", "Qiang Zeng", "Zhikang Shuai"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18001v1", "summary": "Small-signal stability issues-induced broadband oscillations pose significant\nthreats to the secure operation of multi-inverter systems, attracting extensive\nresearch attention. Researches revealed that system instability is led by the\nlacking of positive damping, yet it has not been clearly specified how much the\nexact amount of damping compensation required to sufficiently ensure system\nglobal stability. This paper presents a feasible solution for quantitative\ndamping calculation and compensation to enhance the global stability of\ninverter-based systems. First, based on the system nodal admittance model, a\nquantitative damping calculation algorithm is presented, which can suggest the\nrequired damping compensation as well as compensation location for sufficient\nstability improvement. Then, we propose a specific AD with output current\nfeedforward control strategy, which make the AD be quasi-pure resistive and can\neffectively enhance system damping efficiency. Finally, a testing system with\nthree inverters is used as case study, showing that the proposed method\nprovides a promising solution to efficiently enhance the global stability\nimprovement of inverter-based systems. Simulations and experiments validate the\nproposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18001v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于逆变器系统的全局稳定性改进的定量阻尼计算与补偿方法", "tldr": "本文提出了一种定量计算和补偿逆变器系统阻尼的方法，以提高其全局稳定性，并通过仿真和实验验证了其有效性。", "motivation": "多逆变器系统中的小信号稳定性问题导致的宽带振荡对系统安全运行构成重大威胁，且目前尚不清楚确保系统全局稳定性所需的精确阻尼补偿量。", "method": "首先，基于系统节点导纳模型，提出了一种定量阻尼计算算法，可以给出所需的阻尼补偿量和补偿位置。其次，提出了一种带有输出电流前馈控制策略的特定AD，使其呈准纯电阻性，有效提高系统阻尼效率。", "result": "在一个包含三个逆变器的测试系统中，所提出的方法能够有效提高逆变器系统的全局稳定性。仿真和实验验证了该方法的有效性。", "conclusion": "所提出的定量阻尼计算与补偿方法为高效提升基于逆变器系统的全局稳定性提供了一个有前景的解决方案。", "translation": "小信号稳定性问题引起的宽带振荡对多逆变器系统的安全运行构成重大威胁，引起了广泛的研究关注。研究表明系统不稳定是由于缺乏正阻尼引起的，但尚未明确指出确保系统全局稳定所需的精确阻尼补偿量。本文提出了一种可行的定量阻尼计算与补偿方案，以增强基于逆变器系统的全局稳定性。首先，基于系统节点导纳模型，提出了一种定量阻尼计算算法，该算法可以建议所需的阻尼补偿量以及补偿位置，以充分改善稳定性。然后，我们提出了一种带有输出电流前馈控制策略的特定AD，使AD呈准纯电阻性，并能有效提高系统阻尼效率。最后，以一个包含三个逆变器的测试系统作为案例研究，表明所提出的方法为高效提升基于逆变器系统的全局稳定性提供了一个有前景的解决方案。仿真和实验验证了所提出的方法。", "summary": "针对多逆变器系统因缺乏正阻尼导致的小信号稳定性问题和宽带振荡，本文提出了一种定量阻尼计算和补偿方法以提升系统全局稳定性。该方法首先基于节点导纳模型提出定量阻尼计算算法，以确定补偿量和位置；随后引入一种带有输出电流前馈控制的AD策略，使其呈现准纯电阻特性，从而提高阻尼效率。通过三逆变器系统案例研究，仿真和实验验证了该方法在有效增强系统全局稳定性方面的有效性。", "keywords": "逆变器系统, 全局稳定性, 阻尼计算, 阻尼补偿, 节点导纳模型", "comments": "该论文的创新点在于提出了定量计算和补偿逆变器系统阻尼的具体方法，解决了以往研究中未能明确阻尼补偿量的问题。通过结合节点导纳模型和特定的AD控制策略，提高了阻尼效率和系统稳定性，具有重要的工程应用价值。"}}
{"id": "2507.16838", "title": "Segmentation-free Goodness of Pronunciation", "authors": ["Xinwei Cao", "Zijian Fan", "Torbjørn Svendsen", "Giampiero Salvi"], "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.16838v2", "summary": "Mispronunciation detection and diagnosis (MDD) is a significant part in\nmodern computer aided language learning (CALL) systems. Within MDD,\nphoneme-level pronunciation assessment is key to helping L2 learners improve\ntheir pronunciation. However, most systems are based on a form of goodness of\npronunciation (GOP) which requires pre-segmentation of speech into phonetic\nunits. This limits the accuracy of these methods and the possibility to use\nmodern CTC-based acoustic models for their evaluation. In this study, we first\npropose self-alignment GOP (GOP-SA) that enables the use of CTC-trained ASR\nmodels for MDD. Next, we define a more general alignment-free method that takes\nall possible alignments of the target phoneme into account (GOP-AF). We give a\ntheoretical account of our definition of GOP-AF, an implementation that solves\npotential numerical issues as well as a proper normalization which makes the\nmethod applicable with acoustic models with different peakiness over time. We\nprovide extensive experimental results on the CMU Kids and Speechocean762\ndatasets comparing the different definitions of our methods, estimating the\ndependency of GOP-AF on the peakiness of the acoustic models and on the amount\nof context around the target phoneme. Finally, we compare our methods with\nrecent studies over the Speechocean762 data showing that the feature vectors\nderived from the proposed method achieve state-of-the-art results on\nphoneme-level pronunciation assessment.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.16838v2", "cate": "eess.AS", "date": "2025-07-18", "updated": "2025-07-24", "AI": {"title_translation": "无需分段的发音质量评估", "tldr": "本文提出了两种无需分段的发音质量评估方法（GOP-SA和GOP-AF），并实现了最先进的音素级发音评估结果。", "motivation": "现代计算机辅助语言学习（CALL）系统中的发音错误检测和诊断（MDD）非常重要，其中音素级发音评估是帮助二语学习者提高发音的关键。然而，大多数现有系统依赖于需要语音预分段的GOP形式，这限制了准确性并阻碍了使用基于CTC的现代声学模型。", "method": "本研究首先提出了自对齐发音质量评估（GOP-SA），使得CTC训练的ASR模型能够用于MDD。接着，定义了一种更通用的无需对齐的方法（GOP-AF），该方法考虑了目标音素的所有可能对齐，并提供了其理论描述、解决了潜在数值问题的实现以及适当的归一化，使其适用于具有不同时间峰度的声学模型。", "result": "在CMU Kids和Speechocean762数据集上进行了广泛的实验，比较了不同方法的定义，估计了GOP-AF对声学模型峰度和目标音素上下文的依赖性。结果表明，从所提出的方法导出的特征向量在Speechocean762数据上的音素级发音评估中取得了最先进的结果。", "conclusion": "本文提出的无需分段的GOP方法（GOP-SA和GOP-AF）克服了传统GOP对预分段的依赖，并通过GOP-AF实现了音素级发音评估的最先进性能，为MDD系统提供了更准确和灵活的评估工具。", "translation": "误发音检测和诊断（MDD）是现代计算机辅助语言学习（CALL）系统的重要组成部分。在MDD中，音素级发音评估是帮助二语学习者提高发音的关键。然而，大多数系统都基于一种发音质量评估（GOP）形式，该形式需要将语音预分段为语音单元。这限制了这些方法的准确性以及使用基于CTC的现代声学模型进行评估的可能性。在本研究中，我们首先提出了自对齐GOP（GOP-SA），它使得CTC训练的ASR模型能够用于MDD。接下来，我们定义了一种更通用的无需对齐的方法，该方法考虑了目标音素的所有可能对齐（GOP-AF）。我们提供了GOP-AF定义的理论解释、解决了潜在数值问题的实现以及适当的归一化，这使得该方法适用于具有不同时间峰度的声学模型。我们在CMU Kids和Speechocean762数据集上提供了广泛的实验结果，比较了我们方法的不同定义，估计了GOP-AF对声学模型峰度和目标音素周围上下文的依赖性。最后，我们将我们的方法与Speechocean762数据上的最新研究进行了比较，结果表明，从所提出的方法导出的特征向量在音素级发音评估中取得了最先进的结果。", "summary": "本文针对现有发音错误检测和诊断（MDD）系统中发音质量评估（GOP）对预分段的依赖性问题，提出了两种无需分段的GOP方法：自对齐GOP（GOP-SA）和通用无需对齐GOP（GOP-AF）。GOP-SA允许使用CTC训练的ASR模型，而GOP-AF则考虑所有可能的音素对齐并解决了数值问题，并进行了归一化。实验结果表明，所提出的方法在音素级发音评估上取得了最先进的性能。", "keywords": "发音质量评估, 无需分段, CTC模型, 误发音检测, 计算机辅助语言学习", "comments": "这篇论文通过提出无需分段的发音质量评估方法，解决了传统GOP方法在准确性和与现代CTC模型兼容性方面的限制。特别是GOP-AF的理论严谨性和对数值问题的处理，以及其在音素级发音评估上达到SOTA性能，显示了其创新性和重要性，对于计算机辅助语言学习领域具有显著的进步意义。"}}
{"id": "2507.18443", "title": "On MAP estimates and source conditions for drift identification in SDEs", "authors": ["Daniel Tenbrinck", "Nikolas Uesseler", "Philipp Wacker", "Benedikt Wirth"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18443v1", "summary": "We consider the inverse problem of identifying the drift in an SDE from $n$\nobservations of its solution at $M+1$ distinct time points. We derive a\ncorresponding MAP estimate, we prove differentiability properties as well as a\nso-called tangential cone condition for the forward operator, and we review the\nexisting theory for related problems, which under a slightly stronger\ntangential cone condition would additionally yield convergence rates for the\nMAP estimate as $n\\to\\infty$. Numerical simulations in 1D indicate that such\nconvergence rates indeed hold true.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18443v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "关于SDE中漂移识别的MAP估计和源条件", "tldr": "本文研究从SDE观测数据中识别漂移的逆问题，推导了MAP估计并证明了其性质，数值模拟表明收敛率成立。", "motivation": "解决从随机微分方程（SDE）的观测数据中识别其漂移项的逆问题。", "method": "推导了相应的最大后验（MAP）估计，证明了前向算子的可微性性质和切锥条件，并回顾了相关问题的现有理论。", "result": "导出了MAP估计，证明了前向算子的可微性性质和切锥条件。一维数值模拟表明，MAP估计的收敛率确实成立。", "conclusion": "数值模拟支持了MAP估计的收敛率，表明所提出的方法在识别SDE漂移方面是有效的。", "translation": "我们考虑从其解的$n$个观测值在$M+1$个不同时间点识别随机微分方程（SDE）中漂移的逆问题。我们推导了相应的最大后验（MAP）估计，证明了前向算子的可微性性质以及所谓的切锥条件，并回顾了相关问题的现有理论，该理论在稍强的切锥条件下，将额外地给出当$n\\to\\infty$时MAP估计的收敛率。一维数值模拟表明，这种收敛率确实成立。", "summary": "本文研究了从随机微分方程（SDE）的离散观测数据中识别漂移项的逆问题。作者推导了相应的最大后验（MAP）估计，并证明了前向算子的可微性及切锥条件。通过回顾现有理论，文章指出在更强的切锥条件下，MAP估计的收敛率可以得到证明。一维数值模拟结果支持了这些收敛率的有效性。", "keywords": "MAP估计, 随机微分方程, 漂移识别, 逆问题, 收敛率", "comments": "本文的创新点在于推导了SDE漂移识别的MAP估计，并严格证明了相关数学性质（如可微性和切锥条件）。其重要性在于为SDE参数估计提供了一种理论和实践上可行的方法，特别是数值模拟结果验证了理论收敛率的有效性，这对于实际应用具有指导意义。"}}
{"id": "2507.14679", "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "authors": ["Zhijie Wang", "Zixin Xu", "Zhiyuan Pan"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14679v2", "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14679v2", "cate": "cs.LG", "date": "2025-07-19", "updated": "2025-07-24", "AI": {"title_translation": "GCC-Spam：通过GAN、对比学习和字符相似网络进行垃圾邮件检测", "tldr": "GCC-Spam通过结合GAN生成伪样本、对比学习增强区分度以及字符相似网络对抗混淆，在标记数据稀缺的情况下实现了更高效的垃圾文本检测。", "motivation": "互联网上垃圾文本的指数级增长对信息安全和社会稳定构成威胁，同时垃圾邮件发送者采用对抗性策略且标记数据稀缺，因此需要开发鲁棒的检测机制。", "method": "本文提出了GCC-Spam框架，该框架整合了三项核心创新：1. 字符相似网络：捕获正字法和语音特征以对抗字符混淆攻击，并生成句子嵌入。2. 对比学习：优化垃圾文本和正常文本之间潜在空间的距离以增强可区分性。3. 生成对抗网络（GAN）：生成逼真的伪垃圾样本以缓解数据稀缺并提高模型鲁棒性和分类准确性。", "result": "在真实世界数据集上的广泛实验表明，GCC-Spam模型优于基线方法，以显著更少的标记样本实现了更高的检测率。", "conclusion": "GCC-Spam框架通过有效结合字符相似网络、对比学习和GAN，成功应对了垃圾文本检测中对抗性策略和数据稀缺的挑战，显著提升了检测性能和数据效率。", "translation": "互联网上垃圾文本的指数级增长需要强大的检测机制来减轻信息泄露和社会不稳定等风险。这项工作解决了两个主要挑战：垃圾邮件发送者采用的对抗性策略和标记数据的稀缺性。我们提出了一种新颖的垃圾文本检测框架GCC-Spam，它整合了三项核心创新。首先，字符相似网络捕获正字法和语音特征以对抗字符混淆攻击，并进一步生成句子嵌入用于下游分类。其次，对比学习通过优化垃圾文本和正常文本之间潜在空间的距离来增强可区分性。第三，生成对抗网络（GAN）生成逼真的伪垃圾样本以缓解数据稀缺问题，同时提高模型鲁棒性和分类准确性。在真实世界数据集上进行的广泛实验表明，我们的模型优于基线方法，以显著更少的标记样本实现了更高的检测率。", "summary": "GCC-Spam是一个新颖的垃圾文本检测框架，旨在应对垃圾邮件的对抗性策略和标记数据稀缺问题。它通过结合字符相似网络来处理字符混淆攻击并生成句子嵌入，利用对比学习增强垃圾和正常文本的可区分性，以及使用生成对抗网络（GAN）生成伪样本以缓解数据不足。实验证明，GCC-Spam在真实数据集上表现优于现有方法，以更少的标记数据实现了更高的检测率。", "keywords": "垃圾邮件检测, GAN, 对比学习, 字符相似网络, 数据稀缺", "comments": "该论文的创新点在于将字符相似网络、对比学习和GAN这三种技术巧妙地结合起来，共同解决垃圾邮件检测中的两大核心难题：对抗性混淆和数据稀缺。特别是GAN的引入，为数据增强提供了一条有效途径，显著提升了模型在有限标记数据下的性能和鲁棒性。这种多技术融合的方法为未来的垃圾邮件检测研究提供了有益的思路。"}}
{"id": "2507.18009", "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2507.18009v1", "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.18009v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GRR-CoCa：在多模态模型架构中利用大型语言模型机制", "tldr": "GRR-CoCa通过将LLM机制（如GeLU、RMS归一化和旋转位置嵌入）整合到CoCa模型中，显著提升了多模态任务的性能和泛化能力。", "motivation": "尽管当前最先进的多模态模型表现出色，但其架构复杂性落后于现代大型语言模型（LLMs）。本研究旨在通过引入LLM中已验证的架构改进来提升多模态模型的性能和泛化能力。", "method": "本文提出了GRR-CoCa，一个改进的对比式字幕生成器（CoCa）模型。它将高斯误差门控线性单元、均方根归一化和旋转位置嵌入集成到文本解码器和视觉Transformer（ViT）编码器中。研究人员将GRR-CoCa与基线CoCa（具有相同修改的文本解码器但保留原始ViT编码器）进行了基准测试，使用了标准的预训练和微调流程来评估模型在对比和生成任务上的表现。", "result": "GRR-CoCa在预训练数据集和三个多样化的微调数据集上显著优于基线CoCa。预训练阶段的改进包括对比损失降低27.25%，困惑度降低3.71%，CoCa损失降低7.15%。平均微调改进包括对比损失降低13.66%，困惑度降低5.18%，CoCa损失降低5.55%。", "conclusion": "GRR-CoCa的修改架构显著提高了模型在视觉-语言领域的性能和泛化能力。", "translation": "最先进的图像和文本生成模型是多模态模型，与大型语言模型（LLMs）有许多相似之处。尽管取得了强大的性能，但领先的基础多模态模型架构经常落后于当代LLMs的架构复杂性。我们提出了GRR-CoCa，一个改进的SOTA对比式字幕生成器（CoCa）模型，它将高斯误差门控线性单元、均方根归一化和旋转位置嵌入整合到文本解码器和视觉Transformer（ViT）编码器中。每个架构修改都被证明可以改善LLMs的模型性能，但尚未在CoCa中采用。我们将GRR-CoCa与基线CoCa进行了基准测试，后者是一个具有相同修改的文本解码器但保留CoCa原始ViT编码器的模型。我们使用标准的预训练和微调工作流程来评估模型在对比和生成任务上的表现。我们的GRR-CoCa在预训练数据集和三个多样化的微调数据集上显著优于基线CoCa。预训练改进包括对比损失降低27.25%，困惑度降低3.71%，CoCa损失降低7.15%。平均微调改进包括对比损失降低13.66%，困惑度降低5.18%，CoCa损失降低5.55%。我们表明GRR-CoCa的修改架构提高了跨视觉-语言领域的性能和泛化能力。", "summary": "本文提出了GRR-CoCa，一个通过整合大型语言模型（LLMs）先进架构机制（如高斯误差门控线性单元、均方根归一化和旋转位置嵌入）而改进的对比式字幕生成器（CoCa）模型。这些改进被应用于CoCa的文本解码器和视觉Transformer编码器。实验结果表明，GRR-CoCa在预训练和微调阶段，于对比和生成任务上均显著优于基线CoCa，展示了在视觉-语言领域更优的性能和泛化能力。", "keywords": "多模态模型, LLMs, CoCa, 视觉-语言, 模型架构", "comments": "该论文的创新之处在于系统地将LLM中已被验证的先进架构改进（如GeLU、RMS Norm、RoPE）应用到多模态模型CoCa中。这为提升多模态模型的性能和泛化能力提供了一个有前景的方向，表明通过借鉴LLM高度优化的架构可以构建更强大、更通用的视觉-语言模型。"}}
{"id": "2507.18119", "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18119v1", "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18119v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GOAT-SLM：一种具有副语言和说话者特征感知的口语模型", "tldr": "GOAT-SLM是一个新型口语模型，通过感知副语言和说话者特征，超越纯文本语义，实现更自然、自适应的语音交互。", "motivation": "现有端到端口语模型在处理语音时，主要关注语言内容，忽视了人类语音中丰富的副语言和说话者特征线索（如方言、年龄、情感和非言语发声）。", "method": "GOAT-SLM采用双模态头部架构，将语言建模与声学实现解耦。为提高效率和通用性，提出了一种模块化、分阶段的训练策略，利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。", "result": "在多维度评估基准TELEVAL上的实验表明，GOAT-SLM在语义和非语义任务上均表现均衡，且在处理情感、方言变异和年龄敏感交互方面优于现有开源模型。", "conclusion": "这项工作强调了超越语言内容进行建模的重要性，并推动了更自然、自适应和具有社会意识的口语系统发展。", "translation": "端到端口语模型（SLMs）的最新进展显著提高了AI系统进行自然口语交互的能力。然而，大多数现有模型仅将语音视为语言内容的载体，常常忽视人类语音中嵌入的丰富副语言和说话者特征线索，例如方言、年龄、情感和非语音发声。在这项工作中，我们引入了GOAT-SLM，一个具有副语言和说话者特征感知的新型口语模型，旨在将口语建模扩展到文本语义之外。GOAT-SLM采用双模态头部架构，将语言建模与声学实现解耦，从而在支持富有表现力和自适应的语音生成的同时，实现鲁棒的语言理解。为了提高模型效率和通用性，我们提出了一种模块化、分阶段的训练策略，利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。在多维度评估基准TELEVAL上的实验结果表明，GOAT-SLM在语义和非语义任务上均实现了良好平衡的性能，并且在处理情感、方言变异和年龄敏感交互方面优于现有的开源模型。这项工作强调了超越语言内容建模的重要性，并推动了更自然、自适应和具有社会意识的口语系统。", "summary": "GOAT-SLM是一种新型口语模型，旨在解决现有模型忽视语音中丰富副语言和说话者特征的问题。它采用双模态头部架构，将语言建模与声学实现解耦，并通过模块化、分阶段的训练策略，有效整合语言、副语言和说话者特征信息。实验证明，GOAT-SLM在语义和非语义任务上表现均衡，并在情感、方言和年龄敏感交互方面优于现有模型，推动了更自然、自适应和具有社会意识的口语系统发展。", "keywords": "口语模型, 副语言特征, 说话者特征, 双模态, 分阶段训练", "comments": "这项工作的创新之处在于其对副语言和说话者特征的强调，这使得口语模型能够超越纯粹的文本语义理解，实现更接近人类的自然交流。双模态架构和分阶段训练策略提高了模型的效率和通用性，对于开发更具社会意识的AI系统具有重要意义。"}}
{"id": "2507.18281", "title": "On recognizing graphs representing Persistent Perfect Phylogenies", "authors": ["Paola Bonizzoni", "Gianluca Della Vedova", "Mauricio Soto Gomez", "Gabriella Trucco"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18281v1", "summary": "The Persistent Perfect phylogeny, also known as Dollo-1, has been introduced\nas a generalization of the well-known perfect phylogenetic model for binary\ncharacters to deal with the potential loss of characters. The problem of\ndeciding the existence of a Persistent Perfect phylogeny can be reduced to the\none of recognizing a class of bipartite graphs whose nodes are species and\ncharacters. Thus an interesting question is solving directly the problem of\nrecognizing such graphs. We present a polynomial-time algorithm for deciding\nPersistent Perfect phylogeny existence in maximal graphs, where no character's\nspecies set is contained within another character's species set. Our solution,\nthat relies only on graph properties, narrows the gap between the linear-time\nsimple algorithm for Perfect Phylogeny and the NP-hardness results for the\nDollo-$k$ phylogeny with $k>1$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18281v1", "cate": "cs.DS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "关于识别表示持久完美系统发育的图", "tldr": "本文提出了一种多项式时间算法，用于识别表示持久完美系统发育（Dollo-1）的最大二分图，从而缩小了完美系统发育和Dollo-k系统发育（k>1）之间的复杂性差距。", "motivation": "持久完美系统发育（Dollo-1）是二进制字符完美系统发育模型的推广，用于处理字符的潜在丢失。判断持久完美系统发育存在性的问题可以简化为识别一类特殊二分图的问题。因此，一个有趣的问题是直接解决识别这类图的问题。", "method": "本文提出了一种多项式时间算法，用于判断最大图中持久完美系统发育的存在性，其中任何字符的物种集都不包含在另一个字符的物种集中。该解决方案仅依赖于图的性质。", "result": "本文提出的多项式时间算法成功地解决了最大图中持久完美系统发育的存在性问题。该解决方案缩小了完美系统发育的线性时间简单算法与Dollo-k系统发育（k>1）的NP-难性结果之间的差距。", "conclusion": "本文提出的多项式时间算法为识别表示持久完美系统发育的最大图提供了一个有效的方法，有助于填补完美系统发育和更复杂Dollo模型之间计算复杂性理解的空白。", "translation": "持久完美系统发育，也称为Dollo-1，被引入作为二进制字符中众所周知的完美系统发育模型的推广，以处理字符的潜在丢失。判断持久完美系统发育存在性的问题可以简化为识别一类二分图的问题，其中节点是物种和字符。因此，一个有趣的问题是直接解决识别这类图的问题。我们提出了一种多项式时间算法，用于判断最大图中持久完美系统发育的存在性，其中任何字符的物种集都不包含在另一个字符的物种集中。我们的解决方案仅依赖于图的性质，缩小了完美系统发育的线性时间简单算法与Dollo-k系统发育（k>1）的NP-难性结果之间的差距。", "summary": "本文研究了识别表示持久完美系统发育（Dollo-1）的二分图问题。持久完美系统发育是完美系统发育模型的推广，用于处理字符丢失。作者提出了一种多项式时间算法，用于在最大图中判断持久完美系统发育的存在性。该算法纯粹基于图的性质，并成功地缩小了完美系统发育的线性时间算法与Dollo-k（k>1）系统发育的NP-难性结果之间的复杂性差距。", "keywords": "持久完美系统发育, Dollo-1, 图识别, 多项式时间算法, 二分图", "comments": "该论文的创新之处在于为特定类型的图（最大图）提供了一个多项式时间算法来解决持久完美系统发育识别问题。其重要性在于，它在完美系统发育的简单情况和更复杂的Dollo-k模型之间的计算复杂性理解上取得了进展，为未来研究更一般化的Dollo模型提供了潜在的突破口。"}}
{"id": "2506.23276", "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Published at COLM 2025", "url": "http://arxiv.org/abs/2506.23276v2", "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "comment": "Published at COLM 2025", "pdf_url": "http://arxiv.org/pdf/2506.23276v2", "cate": "cs.AI", "date": "2025-06-29", "updated": "2025-07-24", "AI": {"title_translation": "被推理腐蚀：推理语言模型在公共物品博弈中成为搭便车者", "tldr": "研究发现，增强推理能力的语言模型在公共物品博弈中表现出较低的合作水平，而一些传统语言模型则能保持高水平合作。", "motivation": "随着大型语言模型（LLM）越来越多地被部署为自主代理，理解它们的合作和社会机制变得日益重要。特别地，LLM如何平衡自身利益和集体福祉是确保对齐、鲁棒性和安全部署的关键挑战。", "method": "本研究通过改编行为经济学中的公共物品博弈与制度选择，来研究多代理LLM系统中的惩罚成本挑战。观察不同LLM在重复互动中如何应对社会困境。", "result": "分析揭示了模型中的四种不同行为模式：一些模型持续建立并维持高水平合作；另一些在参与和脱离之间波动；一些合作行为随时间逐渐下降；还有一些无论结果如何都 rigid 地遵循固定策略。令人惊讶的是，推理型LLM（如o1系列）在合作方面表现不佳，而一些传统LLM则始终保持高水平合作。", "conclusion": "研究结果表明，当前旨在提高LLM推理能力的方法不一定能带来合作，这为在需要持续协作的环境中部署LLM代理提供了宝贵见解。", "translation": "随着大型语言模型（LLM）越来越多地被部署为自主代理，理解它们的合作和社会机制变得日益重要。特别是，LLM如何平衡自身利益和集体福祉是确保对齐、鲁棒性和安全部署的关键挑战。在本文中，我们研究了多代理LLM系统中有代价惩罚的挑战，即代理必须决定是否投入自身资源来激励合作或惩罚背叛。为了研究这一点，我们改编了行为经济学中的公共物品博弈与制度选择，这使我们能够观察不同LLM在重复互动中如何应对社会困境。我们的分析揭示了模型中的四种不同行为模式：一些模型持续建立并维持高水平合作，另一些在参与和脱离之间波动，一些合作行为随时间逐渐下降，还有一些无论结果如何都 rigid 地遵循固定策略。令人惊讶的是，我们发现推理型LLM，例如o1系列，在合作方面表现出显著困难，而一些传统LLM则始终保持高水平合作。这些发现表明，当前旨在提高LLM推理能力的方法不一定能带来合作，这为在需要持续协作的环境中部署LLM代理提供了宝贵见解。我们的代码可在 https://github.com/davidguzmanp/SanctSim 获取。", "summary": "本研究探讨了大型语言模型（LLM）作为自主代理在公共物品博弈中的合作行为，特别关注有代价惩罚的挑战。通过改编行为经济学中的公共物品博弈，研究人员观察到LLM的四种行为模式。出人意料的是，增强推理能力的LLM（如o1系列）在合作中表现不佳，反而不如一些传统LLM。这表明，提升LLM推理能力不必然导致更好的合作行为，为在需要持续协作的环境中部署LLM提供了重要启示。", "keywords": "大型语言模型, 公共物品博弈, 合作, 推理能力, 代理系统", "comments": "这项研究通过将LLM置于行为经济学的公共物品博弈情境中，提供了关于LLM社会行为的创新视角。其重要性在于揭示了当前LLM能力提升方向（如推理能力）与预期社会合作行为之间可能存在的脱节。这对于未来LLM的对齐、安全部署以及在多代理系统中的应用具有深远影响，促使我们重新思考如何设计更具社会意识的AI。"}}
{"id": "2507.10136", "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run\" Therapeutic Strategy in Melanoma", "authors": ["Zhonglin Liu"], "categories": ["q-bio.QM", "cs.AI"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures. Submitted to the IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 2025. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.10136v4", "summary": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ''hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.", "comment": "9 pages, 5 figures. Submitted to the IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM) 2025. Code is available at\n  https://github.com/Liu-Zhonglin/pbn-melanoma-project", "pdf_url": "http://arxiv.org/pdf/2507.10136v4", "cate": "q-bio.QM", "date": "2025-07-14", "updated": "2025-07-24", "AI": {"title_translation": "一种PBN-RL-XAI框架，用于发现黑色素瘤中的“速战速决”治疗策略", "tldr": "该研究开发了一个结合PBN、强化学习和可解释AI的计算框架，发现了一种针对黑色素瘤免疫治疗耐药的“速战速决”式LOXL2抑制策略。", "motivation": "转移性黑色素瘤对PD-1免疫疗法存在先天性耐药性，且其分子网络机制尚不明确，这是一个主要的临床挑战。", "method": "研究构建了一个动态概率布尔网络（PBN）模型，使用患者肿瘤活检的转录组数据来阐明治疗反应的调控逻辑。随后，利用强化学习代理发现多步治疗干预措施，并使用可解释人工智能（XAI）来解释代理的控制策略。", "result": "分析发现，精确计时的4步暂时性抑制赖氨酰氧化酶样2蛋白（LOXL2）是最有效的策略。可解释分析表明，这种“速战速决”干预足以消除导致耐药性的分子特征，使网络能够自我纠正而无需持续干预。", "conclusion": "这项研究提出了一种新颖的、时间依赖的治疗假设，用于克服免疫治疗耐药性，并提供了一个强大的计算框架，用于识别复杂生物系统中不明显的干预方案。", "translation": "转移性黑色素瘤对PD-1免疫疗法存在先天性耐药性，这仍然是一个主要的临床挑战，其潜在的分子网络机制知之甚少。为了解决这个问题，我们利用患者肿瘤活检的转录组数据构建了一个动态概率布尔网络模型，以阐明控制治疗反应的调控逻辑。然后，我们采用强化学习代理系统地发现最佳的多步治疗干预措施，并使用可解释人工智能来机械地解释代理的控制策略。分析显示，精确计时的4步暂时性抑制赖氨酰氧化酶样2蛋白（LOXL2）是最有效的策略。我们的可解释分析表明，这种“速战速决”干预足以消除驱动耐药性的分子特征，使网络能够自我纠正而无需持续干预。这项研究提出了一种新颖的、时间依赖的治疗假设，用于克服免疫治疗耐药性，并提供了一个强大的计算框架，用于识别复杂生物系统中不明显的干预方案。", "summary": "本研究旨在解决转移性黑色素瘤对PD-1免疫疗法耐药的挑战。研究团队构建了一个基于转录组数据的动态概率布尔网络（PBN）模型，并结合强化学习（RL）和可解释人工智能（XAI）来发现和解释最佳治疗策略。结果表明，一种精确计时的4步暂时性抑制LOXL2蛋白的“速战速决”策略能有效消除耐药分子特征，使网络自我纠正。该框架为克服免疫治疗耐药性提供了新的时间依赖性治疗假设和计算方法。", "keywords": "黑色素瘤, 免疫治疗耐药, 概率布尔网络, 强化学习, 可解释人工智能, LOXL2", "comments": "这项研究的创新之处在于其PBN-RL-XAI集成框架，它将系统生物学建模、智能决策和可解释性结合起来，用于发现复杂疾病中的非显而易见的治疗策略。特别是，“速战速决”的治疗理念，即通过短暂干预实现持久效应，对于减少药物毒性和提高患者依从性具有重要意义。该框架在识别时间依赖性干预措施方面的能力是其重要性所在。"}}
{"id": "2507.18361", "title": "Hermitian hull of some GRS codes and new EAQMDS codes", "authors": ["Oisin Campion", "Rodrigo San-José"], "categories": ["cs.IT", "math.IT", "81P70 (Primary) 94B05 (Secondary)"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18361v1", "summary": "We study the Hermitian hull of a particular family of generalized\nReed-Solomon codes. The problem of computing the dimension of the hull is\ntranslated to a counting problem in a lattice. By solving this problem, we\nprovide explicit formulas for the dimension of the hull, which determines the\nminimum number required of maximally entangled pairs for the associated\nentanglement-assisted quantum error-correcting codes. This flexible\nconstruction allows to obtain a wide range of entanglement-assisted quantum MDS\ncodes, as well as new parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18361v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "某些GRS码的厄米特壳和新的EAQMDS码", "tldr": "本文研究了广义Reed-Solomon码的厄米特壳，推导了壳维度的显式公式，为构建新的纠缠辅助量子MDS码提供了方法和参数。", "motivation": "确定相关纠缠辅助量子纠错码所需的最大纠缠对的最小数量。", "method": "将计算厄米特壳维度的问题转化为格中的计数问题，并通过解决该问题提供了壳维度的显式公式。", "result": "提供了厄米特壳维度的显式公式，该公式确定了相关纠缠辅助量子纠错码所需的最大纠缠对的最小数量。这种灵活的构造允许获得广泛的纠缠辅助量子MDS码以及新的参数。", "conclusion": "这种灵活的构造可以获得广泛的纠缠辅助量子MDS码和新的参数。", "translation": "我们研究了一类特殊的广义Reed-Solomon码的厄米特壳。计算壳维度的问题被转化为格中的计数问题。通过解决这个问题，我们提供了壳维度的显式公式，该公式确定了相关纠缠辅助量子纠错码所需的最大纠缠对的最小数量。这种灵活的构造允许获得广泛的纠缠辅助量子MDS码以及新的参数。", "summary": "本文研究了广义Reed-Solomon码的厄米特壳，通过将壳维度的计算问题转化为格中的计数问题并求解，得到了壳维度的显式公式。这些公式有助于确定纠缠辅助量子纠错码所需的最大纠缠对数量，并提供了一种灵活的构造方法，可以获得广泛的纠缠辅助量子MDS码及其新参数。", "keywords": "厄米特壳, GRS码, EAQMDS码, 量子纠错, 格计数", "comments": "本文的创新在于将厄米特壳维度的计算问题转化为格中的计数问题并成功求解，从而得到了显式公式。这为构建新型纠缠辅助量子MDS码提供了灵活的方法和新的参数，对量子纠错码领域具有重要意义。"}}
{"id": "2507.18219", "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting", "authors": ["Zhongzheng Yuan", "Lianshuai Guo", "Xunkai Li", "Yinlin Zhu", "Wenyu Wang", "Meixia Qu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18219v1", "summary": "Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18219v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "FedSA-GCL：一种具有个性化聚合和集群感知广播的半异步联邦图学习框架", "tldr": "FedSA-GCL是一个半异步联邦图学习框架，通过考虑图拓扑和标签分布来解决现有同步FGL的效率问题和异步方法在图数据上的局限性，实现了更高的效率和鲁棒性。", "motivation": "现有的联邦图学习（FGL）方法大多依赖同步通信，导致效率低下且在实际部署中不切实际。此外，当前的异步联邦学习（AFL）方法主要针对传统任务设计，未能考虑图数据的独特拓扑特性，直接应用可能导致全局模型的语义漂移和表示不一致。", "method": "论文提出了FedSA-GCL，一个半异步联邦框架。该框架通过新颖的ClusterCast机制，利用客户端间标签分布差异和图拓扑特性进行高效训练。", "result": "FedSA-GCL在多个真实世界图数据集上，使用Louvain和Metis分割算法进行评估，并与9个基线方法进行比较。实验表明，该方法具有强大的鲁棒性和出色的效率，在使用Louvain分割时平均优于基线2.92%，在使用Metis分割时平均优于基线3.4%。", "conclusion": "FedSA-GCL通过半异步通信和对图拓扑特性的考虑，有效解决了传统同步FGL的效率问题以及异步方法在图学习中可能导致的语义漂移和表示不一致问题，显著提升了联邦图学习的性能和效率。", "translation": "联邦图学习（FGL）是一种分布式学习范式，它支持在位于多个本地系统上的大规模子图上进行协作训练。然而，大多数现有的FGL方法依赖于同步通信，这导致效率低下，并且在实际部署中通常不切际。同时，当前的异步联邦学习（AFL）方法主要针对图像分类和自然语言处理等传统任务设计，没有考虑图数据的独特拓扑特性。直接将这些方法应用于图学习可能会导致全局模型中的语义漂移和表示不一致。为了解决这些挑战，我们提出了FedSA-GCL，一个半异步联邦框架，它通过一种新颖的ClusterCast机制，利用客户端间标签分布差异和图拓扑特性进行高效训练。我们在多个真实世界图数据集上使用Louvain和Metis分割算法评估了FedSA-GCL，并与9个基线方法进行了比较。大量实验表明，我们的方法具有强大的鲁棒性和出色的效率，在使用Louvain时平均优于基线2.92%，在使用Metis时平均优于基线3.4%。", "summary": "本文提出FedSA-GCL，一个半异步联邦图学习框架，旨在解决现有同步FGL的效率瓶颈以及异步方法在图数据上应用的局限性。FedSA-GCL通过引入ClusterCast机制，有效利用客户端间的标签分布差异和图拓扑特性，实现了高效训练。在多个真实世界图数据集上的实验证明，FedSA-GCL在鲁棒性和效率方面均显著优于现有基线方法。", "keywords": "联邦图学习, 半异步, 图神经网络, 分布式学习, ClusterCast", "comments": "FedSA-GCL的创新点在于结合了半异步通信机制和对图数据独特拓扑特性的考虑，通过ClusterCast机制解决了传统联邦图学习的效率和准确性问题。其重要性在于为大规模分布式图数据学习提供了更实用、高效且鲁棒的解决方案。"}}
{"id": "2507.17849", "title": "Dynamic and Generalizable Process Reward Modeling", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025 Main", "url": "http://arxiv.org/abs/2507.17849v1", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "comment": "Accepted by ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.17849v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "动态可泛化过程奖励建模", "tldr": "本文提出动态可泛化过程奖励建模（DG-PRM），通过奖励树捕获细粒度、多维度奖励标准，并利用帕累托支配估计动态选择奖励信号，显著提升大型语言模型在复杂任务中的性能和跨领域泛化能力。", "motivation": "现有过程奖励模型（PRMs）主要依赖启发式方法，导致跨领域泛化能力不足；LLM-as-judge方法忽视了文本中嵌入的有意义的指导信息；静态和粗粒度的评估标准难以适应复杂的过程监督。", "method": "本文提出了动态可泛化过程奖励建模（DG-PRM）。该方法利用奖励树捕获并存储细粒度、多维度的奖励标准，并动态选择奖励信号进行分步奖励评分。为处理多方面的奖励信号，开创性地采用帕累托支配估计来识别区分性正负对。", "result": "实验结果表明，DG-PRM在主流基准测试中取得了“惊人”的表现，显著提升了模型在密集奖励任务中的性能。进一步分析揭示，DG-PRM能很好地适应分布外（out-of-distribution）场景，展现出卓越的泛化能力。", "conclusion": "DG-PRM有效解决了现有过程奖励模型在泛化性、动态性和细粒度评估方面的挑战，为大型语言模型在复杂任务中的指导提供了更优的解决方案。", "translation": "标题：动态可泛化过程奖励建模\n\n摘要：过程奖励模型（PRMs）通过提供密集的奖励信号，对于指导大型语言模型（LLMs）在复杂场景中至关重要。然而，现有的PRMs主要依赖启发式方法，难以实现跨领域泛化。虽然已提出将LLM作为评判者来提供泛化奖励，但当前研究主要关注反馈结果，而忽视了文本中嵌入的有意义的指导信息。此外，静态和粗粒度的评估标准难以适应复杂的过程监督。为了解决这些挑战，我们提出了动态可泛化过程奖励建模（DG-PRM），其特点是使用奖励树来捕获和存储细粒度、多维度的奖励标准。DG-PRM动态选择奖励信号进行分步奖励评分。为了处理多方面的奖励信号，我们开创性地采用帕累托支配估计来识别区分性正负对。实验结果表明，DG-PRM在主流基准测试中取得了惊人的表现，显著提升了模型在密集奖励任务中的性能。进一步分析揭示，DG-PRM能很好地适应分布外场景，展现出卓越的卓越泛化能力。", "summary": "本文提出了一种名为动态可泛化过程奖励建模（DG-PRM）的新方法，旨在解决现有过程奖励模型在跨领域泛化能力和适应复杂过程监督方面的不足。DG-PRM引入了奖励树来存储细粒度、多维度的奖励标准，并能够动态地选择奖励信号进行分步评分。此外，它创新性地采用了帕累托支配估计来有效处理多方面的奖励信号。实验结果表明，DG-PRM在多个主流基准测试中表现出色，显著提升了模型性能，并展现了在分布外场景下的优异泛化能力。", "keywords": "过程奖励模型, 大型语言模型, 泛化能力, 奖励树, 帕累托支配", "comments": "该论文的创新点在于提出了DG-PRM，通过引入奖励树实现细粒度、动态的奖励标准，并开创性地将帕累托支配估计应用于奖励信号处理。这有效提升了过程奖励模型的泛化能力和适应性，对于指导大型语言模型在复杂任务中的表现具有重要意义。"}}
{"id": "2507.18618", "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards", "authors": ["Andreea Nica", "Ivan Zakazov", "Nicolas Mario Baldwin", "Saibo Geng", "Robert West"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18618v1", "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18618v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TRPrompt：从文本奖励引导查询感知提示优化", "tldr": "TRPrompt 统一了文本反馈和数值奖励方法，通过直接将文本反馈纳入提示模型训练，实现了查询特定提示的最新性能。", "motivation": "提示优化无需更新目标模型参数即可提高大型语言模型 (LLM) 的推理能力。现有方法分为两类：一类使用文本反馈，另一类依赖数值奖励训练专门的提示模型。本文旨在统一这两种方法，并克服对预先数据集收集的需求。", "method": "本文引入了文本奖励提示框架 (TRPrompt)，该框架通过直接将文本反馈纳入提示模型训练来统一现有方法。它不需要预先的数据集收集，并根据生成的提示反馈进行迭代改进。TRPrompt 利用 LLM 内化“良好”提示概念的能力，通过文本奖励提供高分辨率信号来训练提示模型。", "result": "TRPrompt 训练出的提示模型在具有挑战性的数学数据集 GSMHard 和 MATH 的问题上，为查询特定提示提供了最先进的性能。", "conclusion": "TRPrompt 框架通过将文本反馈直接整合到提示模型训练中，成功地统一了提示优化的两种主要方法，并在无需预先数据集收集的情况下，显著提高了 LLM 在复杂数学推理任务上的表现。", "translation": "提示优化无需更新目标模型参数即可提高大型语言模型 (LLM) 的推理能力。在基于启发式的“一步一步思考”方法之后，该领域已演变为两个主要方向：一类方法使用文本反馈从通用 LLM 中以无训练的方式引出改进的提示，而另一类研究则依赖数值奖励来训练一个特殊的提示模型，该模型专门为目标模型提供最佳提示。在本文中，我们引入了文本奖励提示框架 (TRPrompt)，它通过将文本反馈直接纳入提示模型训练来统一这些方法。我们的框架不需要预先的数据集收集，并且通过对生成的提示的反馈进行迭代改进。当与 LLM 内化“良好”提示概念的能力相结合时，文本奖励提供的高分辨率信号使我们能够训练一个提示模型，为来自具有挑战性的数学数据集 GSMHard 和 MATH 的问题提供最先进的查询特定提示。", "summary": "本文提出了 TRPrompt 框架，旨在改进大型语言模型 (LLM) 的提示优化。它统一了当前两种主流方法：基于文本反馈和基于数值奖励训练提示模型。TRPrompt 通过将文本反馈直接整合到提示模型训练中，无需预先收集数据集，并能根据反馈迭代优化。该方法利用 LLM 理解“良好”提示的能力，通过高分辨率的文本奖励信号训练出能为特定查询生成最先进提示的模型，并在 GSMHard 和 MATH 等数学数据集上取得了显著效果。", "keywords": "提示优化, 文本奖励, 大型语言模型, 查询感知, 提示模型", "comments": "TRPrompt 的创新之处在于它成功地将文本反馈机制与提示模型的训练相结合，从而克服了传统方法中对大量标记数据或复杂奖励函数设计的依赖。其迭代改进和无需预先数据集收集的特性，使其在提示优化领域具有很高的实用价值和效率。在复杂推理任务上的SOTA表现也证明了其有效性。"}}
{"id": "2507.14516", "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "authors": ["Jeyoung Lee", "Hochul Kang"], "categories": ["cs.LG", "cs.AI", "cs.LO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14516v2", "summary": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14516v2", "cate": "cs.LG", "date": "2025-07-19", "updated": "2025-07-24", "AI": {"title_translation": "SDSC：一种用于语义信号表示学习的结构感知度量", "tldr": "本文提出了信号Dice相似系数（SDSC），这是一种用于时间序列自监督表示学习的结构感知度量。SDSC通过量化带符号幅度的交集来衡量时间信号之间的结构一致性，解决了传统基于距离的损失（如MSE）的局限性。实验表明，SDSC在预测和分类任务中表现与MSE相当或更优，尤其是在域内和低资源场景下。", "motivation": "大多数信号的自监督学习（SSL）方法通常采用基于距离的目标函数，如均方误差（MSE）。这些方法对幅度敏感，对波形极性不变，且尺度无界，这些特性阻碍了语义对齐并降低了解释性。", "method": "本文提出了信号Dice相似系数（SDSC），这是一种用于时间序列自监督表示学习的结构感知度量函数。SDSC基于带符号幅度的交集量化时间信号之间的结构一致性，来源于Dice相似系数（DSC）。SDSC可以作为损失函数使用，通过用1减去SDSC并应用Heaviside函数的可微近似来实现梯度优化。此外，还提出了一种混合损失公式，将SDSC与MSE结合，以提高稳定性和在必要时保留幅度。", "result": "在预测和分类基准上的实验表明，基于SDSC的预训练实现了与MSE相当或更优的性能，特别是在域内和低资源场景下。", "conclusion": "结果表明，信号表示中的结构保真度可以提高语义表示质量，支持将结构感知度量视为传统基于距离方法的有效替代方案。", "translation": "我们提出了信号Dice相似系数（SDSC），这是一种用于时间序列自监督表示学习的结构感知度量函数。大多数信号的自监督学习（SSL）方法通常采用基于距离的目标函数，如均方误差（MSE），这些方法对幅度敏感，对波形极性不变，且尺度无界。这些特性阻碍了语义对齐并降低了解释性。SDSC通过基于带符号幅度的交集量化时间信号之间的结构一致性来解决这个问题，该方法来源于Dice相似系数（DSC）。尽管SDSC被定义为一种结构感知度量，但它可以通过用1减去SDSC并应用Heaviside函数的可微近似来实现梯度优化，从而用作损失函数。本文还提出了一种混合损失公式，将SDSC与MSE结合，以提高稳定性和在必要时保留幅度。在预测和分类基准上的实验表明，基于SDSC的预训练实现了与MSE相当或更优的性能，特别是在域内和低资源场景下。结果表明，信号表示中的结构保真度可以提高语义表示质量，支持将结构感知度量视为传统基于距离方法的有效替代方案。", "summary": "本文提出了一种名为信号Dice相似系数（SDSC）的新型结构感知度量，用于时间序列的自监督表示学习。针对传统基于距离的损失函数（如MSE）在语义对齐和可解释性方面的局限性，SDSC通过量化带符号幅度的交集来衡量信号的结构一致性。该度量可转换为损失函数进行梯度优化，并可与MSE结合形成混合损失以提高性能。实验证明，在预测和分类任务中，尤其是在域内和低资源场景下，基于SDSC的预训练表现出与MSE相当或更优的性能，这表明结构保真度对于提升信号语义表示质量具有重要意义。", "keywords": "SDSC, 结构感知度量, 自监督学习, 时间序列, 语义表示", "comments": "本文提出了一种新颖的结构感知度量SDSC，解决了传统基于距离的损失函数在信号自监督学习中存在的局限性，特别是在语义对齐和可解释性方面。其创新点在于将Dice相似系数的概念引入到信号处理中，并提出了可微分的近似方法，使其能够用于梯度优化。此外，与MSE结合的混合损失策略也体现了实用性。该研究强调了信号表示中结构保真度的重要性，为时间序列自监督学习提供了新的视角和有效替代方案。"}}
{"id": "2507.18174", "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs", "authors": ["Rashed Al Amin", "Roman Obermaisser"], "categories": ["cs.CV", "cs.AR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for the 67th International Symposium on ELMAR 2025", "url": "http://arxiv.org/abs/2507.18174v1", "summary": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications.", "comment": "This paper has been accepted for the 67th International Symposium on\n  ELMAR 2025", "pdf_url": "http://arxiv.org/pdf/2507.18174v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于YOLO的边缘FPGA实时目标检测与分类", "tldr": "本文提出了一种优化用于FPGA部署的YOLOv5系统，实现了资源高效的实时目标检测与分类，在Xilinx Kria KV260 FPGA板上达到了99%的分类精度，3.5W功耗和9 FPS的处理速度。", "motivation": "现有的深度学习目标检测方法（如YOLO）在FPGA上部署时，在实现适用于边缘FPGA平台的资源效率方面仍面临挑战。", "method": "本文提出了一种基于YOLOv5的资源高效实时目标检测与分类系统，该系统针对FPGA部署进行了优化，并在COCO和GTSRD数据集上进行训练，实现于Xilinx Kria KV260 FPGA板上。", "result": "实验结果表明，分类精度达到99%，功耗为3.5W，处理速度为9帧每秒（FPS）。", "conclusion": "本文提出的方法能够有效实现边缘计算应用中的实时、资源高效的目标检测与分类。", "translation": "目标检测与分类是各种应用领域中的关键任务，尤其是在开发安全可靠的高级驾驶辅助系统（ADAS）中。现有的基于深度学习的方法，如卷积神经网络（CNNs）、单次多盒检测器（SSDs）和“你只看一次”（YOLO），在部署到现场可编程门阵列（FPGAs）时，在准确性和计算速度方面表现出高性能。然而，尽管取得了这些进展，最先进的基于YOLO的目标检测和分类系统在实现适用于边缘FPGA平台的资源效率方面仍面临挑战。为了解决这一限制，本文提出了一种基于YOLOv5的资源高效实时目标检测与分类系统，该系统针对FPGA部署进行了优化。所提出的系统在COCO和GTSRD数据集上进行训练，并部署在Xilinx Kria KV260 FPGA板上。实验结果表明，分类精度达到99%，功耗为3.5W，处理速度为9帧每秒（FPS）。这些发现突出了所提出方法在实现边缘计算应用中实时、资源高效目标检测与分类方面的有效性。", "summary": "本文针对边缘FPGA平台资源效率不足的问题，提出了一种优化的YOLOv5实时目标检测与分类系统。该系统在COCO和GTSRD数据集上训练，并部署于Xilinx Kria KV260 FPGA板。实验结果显示，系统实现了99%的分类精度，3.5W的低功耗和9 FPS的处理速度，验证了其在边缘计算应用中实现资源高效实时目标检测的有效性。", "keywords": "目标检测, YOLO, FPGA, 边缘计算, 实时", "comments": "本文的创新点在于提出了一个针对边缘FPGA优化的YOLOv5系统，解决了现有YOLO方法在资源效率方面的挑战。其重要性在于为ADAS等需要实时、低功耗目标检测的边缘应用提供了可行的解决方案。性能指标（如99%精度，3.5W功耗）显示出其在特定硬件上的高效性。"}}
{"id": "2506.17606", "title": "Full-body WPT: wireless powering with meandered e-textiles", "authors": ["Ryo Takahashi", "Takashi Sato", "Wakako Yukita", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17606v2", "summary": "We present Full-body WPT, wireless power networking around the human body\nusing a meandered textile coil. Unlike traditional inductive systems that emit\nstrong fields into the deep tissue inside the body, the meander coil enables\nlocalized generation of strong magnetic field constrained to the skin surface,\neven when scaled to the size of the human body. Such localized inductive system\nenhances both safety and efficiency of wireless power around the body.\nFurthermore, the use of low-loss conductive yarn achieve energy-efficient and\nlightweight design. We analyze the performance of our design through\nsimulations and experimental prototypes, demonstrating high power transfer\nefficiency and adaptability to user movement and posture. Our system provides a\nsafe and efficient distributed power network using meandered textile coils\nintegrated into wearable materials, highlighting the potential of body-centric\nwireless power networking as a foundational layer for ubiquitous health\nmonitoring, augmented reality, and human-machine interaction systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17606v2", "cate": "cs.HC", "date": "2025-06-21", "updated": "2025-07-24", "AI": {"title_translation": "全身无线电力传输：基于蜿蜒电子纺织品的无线供电", "tldr": "一种新型全身无线电力传输系统，利用蜿蜒纺织线圈实现安全高效的可穿戴设备供电。", "motivation": "传统的感应式无线电力传输系统会将强磁场发射到人体深层组织，存在安全和效率问题，限制了其在以身体为中心的无线供电应用中的潜力。", "method": "该论文提出了一种名为“全身无线电力传输（Full-body WPT）”的系统，通过使用蜿蜒纺织线圈实现人体周围的无线电力网络。这种线圈由低损耗导电纱线制成，能够将强磁场局部限制在皮肤表面。研究人员通过仿真和实验原型分析了其设计性能。", "result": "该系统展示了高功率传输效率，并且能够适应用户的运动和姿势，同时确保了安全性。", "conclusion": "所开发的系统利用集成到可穿戴材料中的蜿蜒纺织线圈，提供了一个安全高效的分布式电源网络，有望成为普适健康监测、增强现实和人机交互系统的基础层。", "translation": "我们提出了全身无线电力传输（Full-body WPT），这是一种使用蜿蜒纺织线圈在人体周围进行无线电力网络传输的技术。与传统感应系统将强场发射到人体深层组织不同，蜿蜒线圈即使在扩展到人体大小时，也能在皮肤表面局部生成强磁场。这种局部感应系统提高了人体周围无线电力的安全性和效率。此外，使用低损耗导电纱线实现了节能轻量化设计。我们通过仿真和实验原型分析了我们设计的性能，展示了高功率传输效率以及对用户运动和姿势的适应性。我们的系统利用集成到可穿戴材料中的蜿蜒纺织线圈，提供了一个安全高效的分布式电源网络，突出了以身体为中心的无线电力网络作为普适健康监测、增强现实和人机交互系统基础层的潜力。", "summary": "本文介绍了一种名为“全身无线电力传输（Full-body WPT）”的新型无线电力传输系统，该系统利用蜿蜒纺织线圈在人体周围实现安全高效的电力传输。与传统感应系统不同，该方法能将磁场局部限制在皮肤表面，显著提升了安全性和效率。系统采用低损耗导电纱线制成，设计轻巧且节能。仿真和实验结果验证了其高功率传输效率和对用户运动的适应性，预示其在可穿戴健康监测、增强现实和人机交互等领域具有广阔的应用前景。", "keywords": "无线电力传输, 电子纺织品, 蜿蜒线圈, 可穿戴电子设备, 以身体为中心的网络", "comments": "该论文通过利用蜿蜒电子纺织品，提出了一种创新性的以身体为中心的无线电力传输方法。其核心创新在于将磁场局部化到皮肤表面，相较于传统感应方法，显著提升了安全性和效率。该系统与可穿戴材料的集成以及对用户运动的适应性，突显了其在未来普适计算和健康应用中的实际潜力。"}}
{"id": "2507.18052", "title": "DanceGraph: A Complementary Architecture for Synchronous Dancing Online", "authors": ["David Sinclair", "Ademyemi Ademola", "Babis Koniaris", "Kenny Mitchell"], "categories": ["cs.GR", "I.3.2; C.2.1"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      36th International Conference on Computer Animation and Social Agents", "url": "http://arxiv.org/abs/2507.18052v1", "summary": "DanceGraph is an architecture for synchronized online dancing overcoming the\nlatency of networked body pose sharing. We break down this challenge by\ndeveloping a real-time bandwidth-efficient architecture to minimize lag and\nreduce the timeframe of required motion prediction for synchronization with the\nmusic's rhythm. In addition, we show an interactive method for the\nparameterized stylization of dance motions for rhythmic dance using online\ndance correctives.", "comment": "36th International Conference on Computer Animation and Social Agents", "pdf_url": "http://arxiv.org/pdf/2507.18052v1", "cate": "cs.GR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DanceGraph: 一种在线同步舞蹈的补充架构", "tldr": "DanceGraph是一种低延迟、高带宽效率的架构，用于在线同步舞蹈，结合了实时运动预测和舞蹈风格化。", "motivation": "克服在线同步舞蹈中网络身体姿态共享的延迟问题，并实现与音乐节奏的同步。", "method": "开发了一种实时、带宽高效的架构，以最小化延迟并缩短与音乐节奏同步所需的运动预测时间。此外，还展示了一种交互式方法，用于使用在线舞蹈校正器对节奏舞蹈的动作进行参数化风格化。", "result": "成功开发了克服网络延迟的架构，并展示了用于同步和风格化的方法。", "conclusion": "DanceGraph提供了一种解决在线同步舞蹈延迟问题的有效架构，并通过实时预测和风格化增强了用户体验。", "translation": "DanceGraph是一种用于同步在线舞蹈的架构，它克服了网络身体姿态共享的延迟问题。我们通过开发一种实时、带宽高效的架构来解决这一挑战，以最大限度地减少延迟并缩短与音乐节奏同步所需的运动预测时间。此外，我们展示了一种交互式方法，用于使用在线舞蹈校正器对节奏舞蹈的舞蹈动作进行参数化风格化。", "summary": "DanceGraph提出了一种用于在线同步舞蹈的补充架构，旨在解决网络延迟问题。该架构通过实时、高带宽效率的设计来最小化延迟和缩短运动预测时间，以实现与音乐节奏的同步。此外，它还提供了一种交互式方法，利用在线舞蹈校正器对舞蹈动作进行参数化风格化。", "keywords": "在线同步舞蹈, 延迟, 身体姿态共享, 运动预测, 舞蹈风格化", "comments": "该论文提出了一种创新的架构DanceGraph，旨在解决在线同步舞蹈中关键的网络延迟问题，这对于多人在线互动体验至关重要。其结合实时运动预测和舞蹈风格化的方法具有实用价值，可以显著提升用户体验。"}}
{"id": "2507.18565", "title": "Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement", "authors": ["Muhammad Imran Zaman", "Nisar Ahmed"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6", "url": "http://arxiv.org/abs/2507.18565v1", "summary": "This paper presents a novel deep learning-based approach for simultaneous age\nand gender classification from facial images, designed to enhance the\neffectiveness of targeted advertising campaigns. We propose a custom\nConvolutional Neural Network (CNN) architecture, optimized for both tasks,\nwhich leverages the inherent correlation between age and gender information\npresent in facial features. Unlike existing methods that often treat these\ntasks independently, our model learns shared representations, leading to\nimproved performance. The network is trained on a large, diverse dataset of\nfacial images, carefully pre-processed to ensure robustness against variations\nin lighting, pose, and image quality. Our experimental results demonstrate a\nsignificant improvement in gender classification accuracy, achieving 95%, and a\ncompetitive mean absolute error of 5.77 years for age estimation. Critically,\nwe analyze the performance across different age groups, identifying specific\nchallenges in accurately estimating the age of younger individuals. This\nanalysis reveals the need for targeted data augmentation and model refinement\nto address these biases. Furthermore, we explore the impact of different CNN\narchitectures and hyperparameter settings on the overall performance, providing\nvaluable insights for future research.", "comment": "6", "pdf_url": "http://arxiv.org/pdf/2507.18565v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于深度学习的年龄估计和性别分类用于定向广告", "tldr": "本文提出了一种新颖的深度学习方法，通过定制的CNN架构同时进行面部图像的年龄和性别分类，以提高定向广告的效率，并在性别分类上实现了95%的准确率，年龄估计的平均绝对误差为5.77年。", "motivation": "为了提高定向广告活动的有效性，需要一种能够从面部图像中同时进行年龄和性别分类的有效方法。现有方法通常独立处理这些任务，未能利用年龄和性别信息之间的内在关联。", "method": "本文提出了一种定制的卷积神经网络（CNN）架构，该架构针对年龄和性别分类任务进行了优化。该模型学习共享表示，利用面部特征中年龄和性别信息之间的内在相关性。网络在一个大型、多样化的面部图像数据集上进行训练，并经过精心预处理以应对光照、姿态和图像质量的变化。", "result": "实验结果表明，性别分类准确率显著提高，达到95%；年龄估计的平均绝对误差（MAE）为5.77年。研究分析了不同年龄组的性能，发现准确估计年轻个体年龄存在挑战。", "conclusion": "本文提出的深度学习方法在面部图像的年龄和性别分类方面表现出色，特别是在性别分类准确率和年龄估计误差方面。研究识别了年轻个体年龄估计的挑战，并强调需要有针对性的数据增强和模型改进来解决这些偏差。此外，探讨了不同CNN架构和超参数设置对整体性能的影响，为未来研究提供了有价值的见解。", "translation": "本文提出了一种新颖的基于深度学习的方法，用于从面部图像中同时进行年龄和性别分类，旨在提高定向广告活动的有效性。我们提出了一种定制的卷积神经网络（CNN）架构，该架构针对这两项任务进行了优化，利用了面部特征中年龄和性别信息之间固有的关联。与通常独立处理这些任务的现有方法不同，我们的模型学习共享表示，从而提高了性能。该网络在一个大型、多样化的面部图像数据集上进行训练，并经过精心预处理，以确保对光照、姿态和图像质量变化的鲁棒性。我们的实验结果表明，性别分类准确率显著提高，达到95%，年龄估计的平均绝对误差具有竞争力，为5.77年。关键的是，我们分析了不同年龄组的性能，识别出在准确估计年轻个体年龄方面的具体挑战。这项分析揭示了需要有针对性的数据增强和模型改进来解决这些偏差。此外，我们探讨了不同CNN架构和超参数设置对整体性能的影响，为未来的研究提供了有价值的见解。", "summary": "本文提出了一种新颖的基于深度学习的方法，用于从面部图像中同时进行年龄和性别分类，旨在提高定向广告活动的有效性。研究设计了一个定制的卷积神经网络（CNN）架构，该架构通过学习共享表示来利用年龄和性别信息之间的内在关联，从而优于独立处理这些任务的现有方法。该模型在一个大型、多样化的面部图像数据集上进行训练，并经过预处理以确保鲁棒性。实验结果显示，性别分类准确率达到95%，年龄估计的平均绝对误差为5.77年。研究还分析了不同年龄组的性能，并指出了在估计年轻个体年龄方面的挑战，强调了未来数据增强和模型改进的必要性。", "keywords": "年龄估计, 性别分类, 深度学习, 卷积神经网络, 定向广告", "comments": "本文的创新点在于提出了一种定制的CNN架构，能够同时进行年龄和性别分类，并通过学习共享表示来利用这两个任务之间的内在相关性，这与传统上独立处理这两个任务的方法不同。其重要性在于为定向广告提供了更精确的用户画像能力。局限性在于，研究发现准确估计年轻个体年龄存在挑战，这表明模型在某些特定群体上可能存在偏差，需要进一步的数据增强和模型优化来解决。"}}
{"id": "2504.19634", "title": "NSegment : Label-specific Deformations for Remote Sensing Image Segmentation", "authors": ["Yechan Kim", "DongHo Yoon", "SooYeon Kim", "Moongu Jeon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The paper is being revised substantially and will be resubmitted.", "url": "http://arxiv.org/abs/2504.19634v4", "summary": "Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to high image acquisition and labeling\ncosts complicates training noise-robust models. While sophisticated mechanisms\nsuch as label selection or noise correction might address this issue, they tend\nto increase training time and add implementation complexity. In this letter, we\npropose NSegment-a simple yet effective data augmentation solution to mitigate\nthis issue. Unlike traditional methods, it applies elastic transformations only\nto segmentation labels, varying deformation intensity per sample in each\ntraining epoch to address annotation inconsistencies. Experimental results\ndemonstrate that our approach improves the performance of RS image segmentation\non various state-of-the-art models.", "comment": "The paper is being revised substantially and will be resubmitted.", "pdf_url": "http://arxiv.org/pdf/2504.19634v4", "cate": "cs.CV", "date": "2025-04-28", "updated": "2025-07-24", "AI": {"title_translation": "NSegment：遥感图像分割的标签特异性形变", "tldr": "NSegment是一种简单有效的数据增强方法，通过对遥感图像分割标签应用弹性形变来解决标注错误和数据稀缺问题，从而提高分割模型的性能。", "motivation": "遥感图像分割数据集中由于模糊的类别边界、混合像素、阴影、复杂地形特征和主观标注偏差，标注错误普遍存在且难以察觉。此外，高昂的图像采集和标注成本导致标注数据稀缺，使得训练噪声鲁棒模型变得复杂。传统的标签选择或噪声校正机制虽然能解决问题，但会增加训练时间和实现复杂性。", "method": "我们提出了NSegment，这是一种简单而有效的数据增强解决方案。与传统方法不同，它仅对分割标签应用弹性变换，并在每个训练周期中根据样本调整形变强度，以解决标注不一致性。", "result": "实验结果表明，我们的方法提高了各种最先进模型在遥感图像分割任务上的性能。", "conclusion": "NSegment通过对分割标签应用标签特异性形变，有效地解决了遥感图像分割中常见的标注错误和数据稀缺问题，从而提升了模型的性能。", "translation": "遥感（RS）图像分割数据集中的标注错误由于模糊的类别边界、混合像素、阴影、复杂地形特征以及主观的标注者偏差，通常是隐性和微妙的。此外，由于高昂的图像采集和标注成本，标注的RS数据稀缺，这使得训练噪声鲁棒模型变得复杂。虽然标签选择或噪声校正等复杂机制可能解决这个问题，但它们往往会增加训练时间并增加实现复杂性。在这篇通信中，我们提出了NSegment——一个简单而有效的数据增强解决方案来缓解这个问题。与传统方法不同，它仅对分割标签应用弹性变换，并在每个训练周期中根据样本变化形变强度，以解决标注不一致性。实验结果表明，我们的方法提高了各种最先进模型在RS图像分割上的性能。", "summary": "本研究提出了一种名为NSegment的数据增强方法，旨在解决遥感图像分割中常见的标注错误和数据稀缺问题。该方法独特之处在于，它仅对分割标签而非图像本身应用弹性形变，并根据每个样本在每个训练周期中调整形变强度，以应对标注不一致性。实验证明，NSegment能够有效提升各种SOTA遥感图像分割模型的性能。", "keywords": "遥感图像分割, 数据增强, 标签形变, 标注错误, NSegment", "comments": "NSegment的创新之处在于其标签特异性形变的数据增强策略，这与传统的图像级增强不同，更直接地针对分割任务中的标注噪声问题。其简单性也降低了实施复杂性，使其易于集成到现有工作流程中。这种方法对于遥感领域标注成本高、数据质量不均的挑战具有重要意义。"}}
{"id": "2507.18184", "title": "MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation", "authors": ["Hoang Hai Nam Nguyen", "Phan Nguyen Duc Hieu", "Ho Won Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18184v1", "summary": "MatSSL is a streamlined self-supervised learning (SSL) architecture that\nemploys Gated Feature Fusion at each stage of the backbone to integrate\nmulti-level representations effectively. Current micrograph analysis of\nmetallic materials relies on supervised methods, which require retraining for\neach new dataset and often perform inconsistently with only a few labeled\nsamples. While SSL offers a promising alternative by leveraging unlabeled data,\nmost existing methods still depend on large-scale datasets to be effective.\nMatSSL is designed to overcome this limitation. We first perform\nself-supervised pretraining on a small-scale, unlabeled dataset and then\nfine-tune the model on multiple benchmark datasets. The resulting segmentation\nmodels achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an\nImageNet-pretrained encoder, and delivers consistently up to nearly 40%\nimprovement in average mIoU on the Environmental Barrier Coating benchmark\ndataset (EBC) compared to models pretrained with MicroNet. This suggests that\nMatSSL enables effective adaptation to the metallographic domain using only a\nsmall amount of unlabeled data, while preserving the rich and transferable\nfeatures learned from large-scale pretraining on natural images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18184v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MatSSL：用于金相图像分割的鲁棒自监督表示学习", "tldr": "MatSSL是一种流线型的自监督学习架构，通过门控特征融合有效地集成多级表示，在金相图像分割方面表现出色，尤其是在小规模未标记数据集上，超越了现有方法。", "motivation": "当前金属材料的显微图像分析依赖于监督方法，需要为每个新数据集重新训练，并且在只有少量标记样本时表现不稳定。虽然自监督学习（SSL）通过利用未标记数据提供了一个有前景的替代方案，但大多数现有方法仍然依赖于大规模数据集才能有效。MatSSL旨在克服这一限制。", "method": "MatSSL是一种流线型的自监督学习（SSL）架构，在骨干网络的每个阶段采用门控特征融合（Gated Feature Fusion）来有效整合多级表示。该方法首先在小规模未标记数据集上进行自监督预训练，然后对模型在多个基准数据集上进行微调。", "result": "MatSSL分割模型在MetalDAM数据集上实现了69.13%的mIoU，优于ImageNet预训练编码器实现的66.73%。与使用MicroNet预训练的模型相比，在环境屏障涂层（EBC）基准数据集上的平均mIoU一致提高了近40%。", "conclusion": "MatSSL表明，它能够仅使用少量未标记数据有效地适应金相领域，同时保留从自然图像大规模预训练中学到的丰富且可迁移的特征。", "translation": "MatSSL是一种流线型的自监督学习（SSL）架构，它在骨干网络的每个阶段采用门控特征融合，以有效整合多级表示。当前金属材料的显微图像分析依赖于监督方法，这需要为每个新数据集重新训练，并且在只有少量标记样本时性能往往不稳定。虽然SSL通过利用未标记数据提供了一个有前景的替代方案，但大多数现有方法仍然依赖于大规模数据集才能有效。MatSSL旨在克服这一限制。我们首先在小规模未标记数据集上进行自监督预训练，然后对模型在多个基准数据集上进行微调。最终的分割模型在MetalDAM数据集上实现了69.13%的mIoU，超过了ImageNet预训练编码器实现的66.73%，并且与使用MicroNet预训练的模型相比，在环境屏障涂层（EBC）基准数据集上的平均mIoU一致提高了近40%。这表明MatSSL能够仅使用少量未标记数据有效地适应金相领域，同时保留从自然图像大规模预训练中学到的丰富且可迁移的特征。", "summary": "MatSSL提出了一种流线型的自监督学习（SSL）架构，通过在骨干网络中引入门控特征融合来有效整合多级表示。针对现有监督方法需要大量标记数据且SSL方法依赖大规模数据集的局限性，MatSSL通过在小规模未标记数据集上进行自监督预训练，然后在多个基准数据集上进行微调。实验结果表明，MatSSL在金相图像分割任务上表现优异，在MetalDAM数据集上mIoU达到69.13%，并显著提升了在EBC数据集上的性能，证明了其在少量未标记数据下对金相领域的有效适应能力。", "keywords": "自监督学习, 金相图像分割, 门控特征融合, 未标记数据, 表示学习", "comments": "MatSSL的创新之处在于其流线型的SSL架构和引入的门控特征融合机制，使得模型能够有效利用多级特征。其重要性在于解决了金相图像分析中对大量标记数据依赖的问题，并通过在小规模未标记数据上的有效预训练，实现了对特定领域的高效适应，同时保留了通用特征的迁移能力。这对于数据标注成本高昂的专业领域具有重要意义。"}}
{"id": "2503.06736", "title": "Safe, Task-Consistent Manipulation with Operational Space Control Barrier Functions", "authors": ["Daniel Morton", "Marco Pavone"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.06736v2", "summary": "Safe real-time control of robotic manipulators in unstructured environments\nrequires handling numerous safety constraints without compromising task\nperformance. Traditional approaches, such as artificial potential fields\n(APFs), suffer from local minima, oscillations, and limited scalability, while\nmodel predictive control (MPC) can be computationally expensive. Control\nbarrier functions (CBFs) offer a promising alternative due to their high level\nof robustness and low computational cost, but these safety filters must be\ncarefully designed to avoid significant reductions in the overall performance\nof the manipulator. In this work, we introduce an Operational Space Control\nBarrier Function (OSCBF) framework that integrates safety constraints while\npreserving task-consistent behavior. Our approach scales to hundreds of\nsimultaneous constraints while retaining real-time control rates, ensuring\ncollision avoidance, singularity prevention, and workspace containment even in\nhighly cluttered settings or during dynamic motions. By explicitly accounting\nfor the task hierarchy in the CBF objective, we prevent degraded performance\nacross both joint-space and operational-space tasks, when at the limit of\nsafety. We validate performance in both simulation and hardware, and release\nour open-source high-performance code and media on our project webpage,\nhttps://stanfordasl.github.io/oscbf/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.06736v2", "cate": "cs.RO", "date": "2025-03-09", "updated": "2025-07-24", "AI": {"title_translation": "基于操作空间控制障碍函数的安全、任务一致的机械臂操作", "tldr": "本文提出了一种操作空间控制障碍函数（OSCBF）框架，用于机器人的安全实时控制，能够在不牺牲任务性能的情况下处理大量安全约束。", "motivation": "在非结构化环境中对机器人机械臂进行安全的实时控制，需要在不影响任务性能的情况下处理大量安全约束。传统方法如人工势场（APFs）存在局部最小值、振荡和可伸缩性有限的问题，而模型预测控制（MPC）计算成本高昂。控制障碍函数（CBFs）虽然有前景，但如果设计不当，可能会显著降低机械臂的整体性能。因此，需要一种能够有效整合安全约束同时保持任务性能的方法。", "method": "本文引入了一种操作空间控制障碍函数（OSCBF）框架。该框架将安全约束整合进来，同时保持任务一致的行为。它通过在CBF目标中明确考虑任务层级来防止在安全极限下关节空间和操作空间任务的性能下降。", "result": "该方法能够扩展到数百个同时存在的约束，同时保持实时控制速率。它确保了在高度混乱的环境或动态运动中也能实现碰撞避免、奇点预防和工作空间限制。在仿真和硬件中都验证了其性能。", "conclusion": "OSCBF框架为机器人机械臂的安全、任务一致的实时控制提供了一个强大且高效的解决方案，它有效地整合了安全约束，即使在安全极限下也能保持性能，从而克服了现有方法的局限性。", "translation": "在非结构化环境中对机器人机械臂进行安全的实时控制，需要在不影响任务性能的情况下处理大量安全约束。传统方法，如人工势场（APFs），存在局部最小值、振荡和可伸缩性有限的问题，而模型预测控制（MPC）的计算成本可能很高。控制障碍函数（CBFs）因其高鲁棒性和低计算成本而提供了一个有前景的替代方案，但这些安全滤波器必须精心设计，以避免显著降低机械臂的整体性能。在这项工作中，我们引入了一种操作空间控制障碍函数（OSCBF）框架，该框架整合了安全约束，同时保持了任务一致的行为。我们的方法可以扩展到数百个同时存在的约束，同时保持实时控制速率，即使在高度混乱的环境或动态运动中，也能确保碰撞避免、奇点预防和工作空间限制。通过在CBF目标中明确考虑任务层级，我们防止了在安全极限下关节空间和操作空间任务的性能下降。我们在仿真和硬件中都验证了性能，并在我们的项目网页 https://stanfordasl.github.io/oscbf/ 上发布了开源高性能代码和媒体。", "summary": "该论文提出了一种操作空间控制障碍函数（OSCBF）框架，用于在非结构化环境中对机器人机械臂进行安全、实时的操作。该框架通过在CBF目标中明确考虑任务层级，有效地整合了数百个安全约束（包括碰撞避免、奇点预防和工作空间限制），同时保持了任务性能，解决了传统方法（如APFs和MPC）的局限性。该方法能够在实时控制速率下运行，并在仿真和硬件中得到了验证。", "keywords": "控制障碍函数, 机器人操作, 操作空间控制, 安全性, 实时控制", "comments": "该论文的创新之处在于提出了OSCBF框架，该框架能够在大规模约束下保持实时控制和任务一致性，解决了机器人安全操作中的关键挑战。特别是在CBF目标中明确考虑任务层级，有效避免了在安全限制下性能下降的问题，这对于实际机器人部署具有重要意义。"}}
{"id": "2507.18624", "title": "Checklists Are Better Than Reward Models For Aligning Language Models", "authors": ["Vijay Viswanathan", "Yanchao Sun", "Shuang Ma", "Xiang Kong", "Meng Cao", "Graham Neubig", "Tongshuang Wu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18624v1", "summary": "Language models must be adapted to understand and follow user instructions.\nReinforcement learning is widely used to facilitate this -- typically using\nfixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead\npropose using flexible, instruction-specific criteria as a means of broadening\nthe impact that reinforcement learning can have in eliciting instruction\nfollowing. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF).\nFrom instructions, we extract checklists and evaluate how well responses\nsatisfy each item - using both AI judges and specialized verifier programs -\nthen combine these scores to compute rewards for RL. We compare RLCF with other\nalignment methods applied to a strong instruction following model\n(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only\nmethod to improve performance on every benchmark, including a 4-point boost in\nhard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a\n3-point rise in win rate on Arena-Hard. These results establish checklist\nfeedback as a key tool for improving language models' support of queries that\nexpress a multitude of needs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18624v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "清单比奖励模型更适合对齐语言模型", "tldr": "该研究提出使用清单反馈强化学习（RLCF）来对齐语言模型，结果表明RLCF在多个基准测试中优于传统的奖励模型方法，显著提升了语言模型的指令遵循能力。", "motivation": "语言模型需要理解并遵循用户指令。传统的强化学习方法通常使用固定的标准（如“有用性”和“无害性”），这限制了其在引导指令遵循方面的广泛影响。本研究旨在通过使用灵活的、特定于指令的标准来拓宽强化学习的影响力。", "method": "论文提出了“基于清单反馈的强化学习”（RLCF）。该方法从指令中提取清单，并使用AI评判和专门的验证程序评估响应满足每个清单项目的程度，然后将这些分数结合起来计算强化学习的奖励。RLCF与其他对齐方法在Qwen2.5-7B-Instruct模型上进行了比较。", "result": "RLCF在五个广泛研究的基准测试中，是唯一能在每个基准测试上都提高性能的方法。具体提升包括：FollowBench上的硬满足率提高了4点，InFoBench提高了6点，Arena-Hard上的胜率提高了3点。", "conclusion": "清单反馈是提高语言模型支持表达多种需求查询的关键工具。", "translation": "语言模型必须适应理解和遵循用户指令。强化学习被广泛用于促进这一点——通常使用固定的标准，如“有用性”和“无害性”。在我们的工作中，我们提出使用灵活的、特定于指令的标准，以此作为拓宽强化学习在引发指令遵循方面影响力的手段。我们提出了“基于清单反馈的强化学习”（RLCF）。从指令中，我们提取清单并评估响应满足每个项目的程度——既使用AI评判，也使用专门的验证程序——然后将这些分数结合起来计算强化学习的奖励。我们将RLCF与其他应用于强大指令遵循模型（Qwen2.5-7B-Instruct）的对齐方法在五个广泛研究的基准测试上进行了比较——RLCF是唯一在每个基准测试上都提高性能的方法，包括FollowBench上硬满足率提高了4点，InFoBench上提高了6点，以及Arena-Hard上胜率提高了3点。这些结果确立了清单反馈作为改进语言模型支持表达多种需求查询的关键工具。", "summary": "本论文提出了一种新的语言模型对齐方法——基于清单反馈的强化学习（RLCF），旨在解决传统奖励模型在处理复杂用户指令时的局限性。RLCF通过从指令中提取特定清单，并利用AI评判和验证程序评估模型响应的符合度来生成奖励。实验结果表明，RLCF在多个主流基准测试中显著优于其他对齐方法，有效提升了语言模型理解和遵循多样化指令的能力，证明了清单反馈在语言模型对齐中的重要价值。", "keywords": "清单, 强化学习, 语言模型, 对齐, 指令遵循", "comments": "该论文的创新之处在于引入了“清单反馈”机制，替代了传统的固定奖励模型。这种灵活且特定于指令的评估方式，为强化学习在语言模型对齐中的应用开辟了新路径，使其能更有效地处理复杂和多样的用户需求。这对于提升语言模型在实际应用中的鲁棒性和适应性具有重要意义。"}}
{"id": "2507.18552", "title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding", "authors": ["Baoyao Yang", "Wanyun Li", "Dixin Chen", "Junxiang Chen", "Wenbin Yao", "Haifeng Lin"], "categories": ["cs.CV", "cs.AI", "68T45, 68T50, 68U35,", "I.4.8; I.2.7; I.2.10; H.5.1"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages; 14 figures", "url": "http://arxiv.org/abs/2507.18552v1", "summary": "This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.", "comment": "7 pages; 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.18552v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "VideoMind：一个用于深度认知视频理解的意图接地全模态视频数据集", "tldr": "VideoMind是一个新的全模态视频数据集，包含103K视频样本和分层文本描述（包括深层意图），通过COT生成，并提供基准测试，旨在促进深度认知视频理解和跨模态对齐。", "motivation": "现有数据集缺乏对视频深层认知（如意图）的表达，限制了深度视频内容理解和多模态特征表示。VideoMind旨在通过提供需要上下文整合的意图表达来弥补这一空白。", "method": "1. 数据集构建: 引入VideoMind，一个包含103K视频样本（3K测试用）的全模态视频数据集。 2. 数据构成: 每个视频样本配有音频和详细文本描述，总计超过2200万字。 3. 分层描述: 文本描述分为事实、抽象和意图三个层次。 4. 意图生成: 深层认知意图表达通过Chain-of-Thought (COT) 方法，引导mLLM进行逐步推理生成。 5. 丰富标注: 每个描述包含主体、地点、时间、事件、动作和意图标注。 6. 基准建立: 建立了一个包含3,000个手动验证样本的黄金标准基准。 7. 评估实验: 设计了混合认知检索实验，并使用多级检索指标进行评估。 8. 数据可用性: 数据集已在GitHub、HuggingFace和OpenDataLab公开。", "result": "1. 成功构建并发布了VideoMind数据集，包含103K视频样本和分层（包括意图）文本描述。 2. 数据集提供了针对深度认知视频理解的黄金标准基准。 3. 已发布了InternVideo、VAST、UMT-L等模型的评估结果。", "conclusion": "VideoMind数据集作为一个强大的基准，促进了细粒度跨模态对齐，并推动了需要深度视频理解的领域，如情感和意图识别的发展。", "translation": "这篇论文介绍了VideoMind，一个以视频为中心的全模态数据集，专为深度视频内容认知和增强的多模态特征表示而设计。该数据集包含103K个视频样本（其中3K用于测试），每个样本都配有音频和系统详细的文本描述。具体来说，每个视频及其音频都通过三个层次（事实、抽象和意图）进行描述，从表层到深层。它包含超过2200万字，平均每个样本约225字。VideoMind与现有数据集的关键区别在于它提供了意图表达，这些表达需要整合整个视频的上下文，并且不能直接观察到。这些深层认知表达是使用思维链（COT）方法生成的，通过逐步推理提示mLLM。每个描述都包含主体、地点、时间、事件、动作和意图的标注，支持下游识别任务。至关重要的是，我们建立了一个包含3,000个手动验证样本的黄金标准基准，用于评估深度认知视频理解。我们设计了混合认知检索实验，通过多级检索指标进行评分，以适当评估深度视频理解。模型的评估结果（例如，InternVideo、VAST、UMT-L）已发布。VideoMind作为一个强大的基准，可用于细粒度跨模态对齐，并推进需要深度视频理解的领域，例如情感和意图识别。数据已在GitHub、HuggingFace和OpenDataLab上公开，链接为https://github.com/cdx-cindy/VideoMind。", "summary": "本论文介绍了VideoMind，一个全新的、以视频为中心的全模态数据集，旨在促进深度视频内容认知和多模态特征表示。该数据集包含103K个视频样本，每个样本都附带音频和通过分层（事实、抽象、意图）方式生成的详细文本描述，其中深层意图表达通过思维链（COT）方法生成。VideoMind通过提供独特的、需要上下文整合的意图标注，弥补了现有数据集的不足。论文还建立了一个3,000个样本的黄金标准基准，并设计了混合认知检索实验来评估深度视频理解模型。该数据集及其评估结果已公开，有望成为细粒度跨模态对齐和情感/意图识别等深度视频理解任务的强大基准。", "keywords": "视频数据集, 全模态, 意图接地, 深度认知, 思维链 (COT) ", "comments": "VideoMind的创新之处在于其引入了“意图接地”的概念，通过分层描述和思维链（COT）方法生成深层认知意图表达，这对于现有数据集来说是一个显著的进步。它旨在解决视频理解中更深层次的语义和认知挑战，而不仅仅是表层事实描述。该数据集的规模、详细的标注以及提供的黄金标准基准，使其成为推动深度视频理解领域发展的重要资源。"}}
{"id": "2507.15205", "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by the 28th European Conference on Artificial Intelligence (ECAI 2025)", "url": "http://arxiv.org/abs/2507.15205v2", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.", "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.15205v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "长短距离图神经网络和改进课程学习用于对话情感识别", "tldr": "本文提出了一种新颖的长短距离图神经网络（LSDGNN）和改进课程学习（ICL）方法，用于解决对话情感识别中的多模态特征提取和数据不平衡问题，并在主流数据集上取得了SOTA性能。", "motivation": "对话情感识别（ERC）是一项实用但具有挑战性的任务。现有方法可能在多模态特征提取和数据不平衡方面存在不足。", "method": "本文提出了一种新颖的多模态方法，长短距离图神经网络（LSDGNN）。它基于有向无环图（DAG）构建了长距离和短距离图神经网络，分别获取远距离和近距离话语的多模态特征。为确保长短距离特征的区分性并促进模块间相互影响，采用了差分正则化器和双仿射模块。此外，为解决数据不平衡问题，提出了一种改进课程学习（ICL）方法，通过计算不同情感间的相似度，设计了“加权情感偏移”度量和难度衡量器，实现了从易到难的训练过程。", "result": "在IEMOCAP和MELD数据集上的实验结果表明，所提出的模型优于现有基准。", "conclusion": "本文提出的LSDGNN和ICL方法有效提升了对话情感识别的性能，并在多模态特征融合和数据不平衡处理方面展现出优越性。", "translation": "对话情感识别（ERC）是一项实用且具有挑战性的任务。本文提出了一种新颖的多模态方法，即长短距离图神经网络（LSDGNN）。它基于有向无环图（DAG），构建了长距离图神经网络和短距离图神经网络，分别获取远距离和近距离话语的多模态特征。为了确保长短距离特征在表示上尽可能不同，同时使两个模块之间能够相互影响，我们采用了差分正则化器并结合了双仿射模块以促进特征交互。此外，我们提出了一种改进课程学习（ICL）来解决数据不平衡的挑战。通过计算不同情感之间的相似度来强调相似情感的变化，我们设计了一个“加权情感偏移”指标并开发了一个难度测量器，从而实现了一个优先学习简单样本再学习困难样本的训练过程。在IEMOCAP和MELD数据集上的实验结果表明，我们的模型优于现有基准。", "summary": "本文针对对话情感识别（ERC）中的多模态特征提取和数据不平衡问题，提出了一种长短距离图神经网络（LSDGNN）和改进课程学习（ICL）方法。LSDGNN利用长短距离图网络分别捕获远近话语特征，并通过差分正则化器和双仿射模块增强特征区分与交互。ICL则通过“加权情感偏移”和难度测量器，实现从易到难的平衡训练。实验证明，该模型在IEMOCAP和MELD数据集上优于现有基准。", "keywords": "对话情感识别, 图神经网络, 课程学习, 多模态, 数据不平衡", "comments": "该论文的创新点在于结合了长短距离图神经网络来捕捉对话中不同范围的上下文信息，并通过差分正则化器和双仿射模块优化了多模态特征的融合。同时，提出的改进课程学习方法有效解决了对话情感识别中常见的数据不平衡问题，提升了模型的鲁棒性。其在主流数据集上的优异表现证明了方法的有效性。"}}
{"id": "2507.17775", "title": "Comparison of Optimised Geometric Deep Learning Architectures, over Varying Toxicological Assay Data Environments", "authors": ["Alexander D. Kalian", "Lennart Otte", "Jaewook Lee", "Emilio Benfenati", "Jean-Lou C. M. Dorne", "Claire Potter", "Olivia J. Osborne", "Miao Guo", "Christer Hogstrand"], "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17775v1", "summary": "Geometric deep learning is an emerging technique in Artificial Intelligence\n(AI) driven cheminformatics, however the unique implications of different Graph\nNeural Network (GNN) architectures are poorly explored, for this space. This\nstudy compared performances of Graph Convolutional Networks (GCNs), Graph\nAttention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7\ndifferent toxicological assay datasets of varying data abundance and endpoint,\nto perform binary classification of assay activation. Following pre-processing\nof molecular graphs, enforcement of class-balance and stratification of all\ndatasets across 5 folds, Bayesian optimisations were carried out, for each GNN\napplied to each assay dataset (resulting in 21 unique Bayesian optimisations).\nOptimised GNNs performed at Area Under the Curve (AUC) scores ranging from\n0.728-0.849 (averaged across all folds), naturally varying between specific\nassays and GNNs. GINs were found to consistently outperform GCNs and GATs, for\nthe top 5 of 7 most data-abundant toxicological assays. GATs however\nsignificantly outperformed over the remaining 2 most data-scarce assays. This\nindicates that GINs are a more optimal architecture for data-abundant\nenvironments, whereas GATs are a more optimal architecture for data-scarce\nenvironments. Subsequent analysis of the explored higher-dimensional\nhyperparameter spaces, as well as optimised hyperparameter states, found that\nGCNs and GATs reached measurably closer optimised states with each other,\ncompared to GINs, further indicating the unique nature of GINs as a GNN\nalgorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17775v1", "cate": "q-bio.QM", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "优化几何深度学习架构在不同毒理学分析数据环境下的比较", "tldr": "本研究比较了不同图神经网络（GNN）架构在不同数据量毒理学分析数据集上的性能，发现GINs在数据量充足时表现最佳，而GATs在数据量稀缺时表现更优。", "motivation": "几何深度学习在AI驱动的化学信息学中是一项新兴技术，但不同图神经网络（GNN）架构在该领域中的独特影响尚未得到充分探索。", "method": "研究比较了图卷积网络（GCNs）、图注意力网络（GATs）和图同构网络（GINs）在7个不同毒理学分析数据集上的性能，这些数据集具有不同的数据丰度和终点，用于分析激活的二元分类。对分子图进行预处理，强制执行类别平衡，并将所有数据集分层为5折。对每个GNN应用于每个分析数据集进行了贝叶斯优化（共21次）。", "result": "优化后的GNNs的AUC分数范围为0.728-0.849。在7个毒理学分析数据集中，数据量最丰富的5个数据集中，GINs始终优于GCNs和GATs。然而，GATs在数据量最稀缺的2个数据集中表现显著优于其他模型。GCNs和GATs的优化状态彼此更接近，进一步表明GINs作为GNN算法的独特特性。", "conclusion": "GINs是数据量充足环境下的更优架构，而GATs是数据量稀缺环境下的更优架构。", "translation": "几何深度学习是人工智能（AI）驱动的化学信息学中的一项新兴技术，然而，不同图神经网络（GNN）架构在该领域中的独特影响尚未得到充分探索。本研究比较了图卷积网络（GCNs）、图注意力网络（GATs）和图同构网络（GINs）的性能，这些模型应用于7个不同毒理学分析数据集，这些数据集具有不同的数据丰度和终点，以执行分析激活的二元分类。在对分子图进行预处理、强制执行类别平衡以及将所有数据集分层为5折后，对每个GNN应用于每个分析数据集进行了贝叶斯优化（共进行了21次独特的贝叶斯优化）。优化后的GNNs的曲线下面积（AUC）分数范围为0.728-0.849（在所有折叠中平均），自然地在特定分析和GNNs之间变化。研究发现，在7个数据量最丰富的毒理学分析数据集中，GINs始终优于GCNs和GATs。然而，GATs在剩余的2个数据量最稀缺的分析中表现显著优于其他模型。这表明GINs是数据量充足环境下的更优架构，而GATs是数据量稀缺环境下的更优架构。随后对所探索的高维超参数空间以及优化后的超参数状态进行分析发现，GCNs和GATs彼此之间达到了更接近的优化状态，相比之下GINs则不同，这进一步表明了GINs作为GNN算法的独特性质。", "summary": "本研究比较了三种图神经网络（GCNs、GATs、GINs）在七个不同毒理学分析数据集上的性能，以进行分析激活的二元分类。研究发现，经过贝叶斯优化后，GINs在数据量丰富的毒理学分析数据集中表现优异，而GATs在数据量稀缺的数据集中性能更佳。这表明GNN架构的选择应根据数据环境的丰度进行调整。", "keywords": "几何深度学习, 图神经网络, 毒理学, 贝叶斯优化, 化学信息学", "comments": "本研究通过对不同GNN架构在不同数据量毒理学数据集上的系统比较，揭示了GNN架构选择对性能的关键影响，尤其是在数据稀缺或丰富环境下的适应性。这是对AI驱动的化学信息学领域中GNN应用的一次有价值的探索。"}}
{"id": "2507.18077", "title": "Carbon Emission Flow Tracing: Fast Algorithm and California Grid Study", "authors": ["Yuqing Shen", "Yuanyuan Shi", "Daniel Kirschen", "Yize Chen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      In Submission, 16 pages, 11 figures, code available at this https URL", "url": "http://arxiv.org/abs/2507.18077v1", "summary": "Power systems decarbonization are at the focal point of the clean energy\ntransition. While system operators and utility companies increasingly publicize\nsystem-level carbon emission information, it remains unclear how emissions from\nindividual generators are transported through the grid and how they impact\nelectricity users at specific locations. This paper presents a novel and\ncomputationally efficient approach for exact quantification of nodal average\nand marginal carbon emission rates, applicable to both AC and DC optimal power\nflow problems. The approach leverages graph-based topological sorting and\ndirected cycle removal techniques, applied to directed graphs formed by\ngeneration dispatch and optimal power flow solutions. Our proposed algorithm\nefficiently identifies each generator's contribution to each node, capturing\nhow emissions are spatially distributed under varying system conditions. To\nvalidate its effectiveness and reveal locational and temporal emission patterns\nin the real world, we simulate the 8,870-bus realistic California grid using\nactual CAISO data and the CATS model. Based on year long hourly data on nodal\nloads and renewable generation, obtained or estimated from CAISO public data,\nour method accurately estimates power flow conditions, generation mixes, and\nsystemwide emissions, and delivers fine grained spatiotemporal emission\nanalysis for every California county. Both our algorithm and the California\nstudy are open-sourced, providing a foundation for future research on grid\nemissions, planning, operations, and energy policy.", "comment": "In Submission, 16 pages, 11 figures, code available at\n  https://github.com/yuqing5/Carbon-Tracker-California", "pdf_url": "http://arxiv.org/pdf/2507.18077v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "碳排放流追踪：快速算法与加州电网研究", "tldr": "本文提出了一种新颖高效的算法，用于精确量化电网中节点的碳排放率，并通过对加州电网的模拟验证了其有效性，实现了精细化的时空排放分析。", "motivation": "在清洁能源转型中，电力系统脱碳是焦点。尽管系统运营商和电力公司日益公开系统级碳排放信息，但个体发电机组的排放如何在电网中传输以及如何影响特定位置的电力用户仍不清楚。", "method": "本文提出了一种新颖且计算高效的方法，用于精确量化节点平均和边际碳排放率，适用于交流和直流最优潮流问题。该方法利用基于图的拓扑排序和有向循环去除技术，应用于由发电调度和最优潮流解形成的图。该算法能有效识别每个发电机对每个节点的贡献，捕获排放如何在不同系统条件下进行空间分布。", "result": "通过使用实际CAISO数据和CATS模型模拟8,870总线的真实加州电网，该方法准确估计了潮流条件、发电组合和全系统排放，并为加州每个县提供了精细化的时空排放分析。", "conclusion": "本文提出的算法和加州研究都是开源的，为未来电网排放、规划、运营和能源政策研究奠定了基础。", "translation": "电力系统脱碳是清洁能源转型的焦点。尽管系统运营商和电力公司日益公开系统级碳排放信息，但个体发电机组的排放如何在电网中传输以及如何影响特定位置的电力用户仍不清楚。本文提出了一种新颖且计算高效的方法，用于精确量化节点平均和边际碳排放率，适用于交流和直流最优潮流问题。该方法利用基于图的拓扑排序和有向循环去除技术，应用于由发电调度和最优潮流解形成的图。我们提出的算法能有效识别每个发电机对每个节点的贡献，捕获排放如何在不同系统条件下进行空间分布。为了验证其有效性并揭示现实世界中的位置和时间排放模式，我们使用实际CAISO数据和CATS模型模拟了8,870总线的真实加州电网。基于从CAISO公共数据获得或估计的节点负荷和可再生能源发电的年度小时数据，我们的方法准确估计了潮流条件、发电组合和全系统排放，并为加州每个县提供了精细化的时空排放分析。我们的算法和加州研究都是开源的，为未来电网排放、规划、运营和能源政策研究奠定了基础。", "summary": "本文提出了一种新颖且计算高效的方法，用于精确量化电力系统中节点的平均和边际碳排放率。该方法利用图论技术追踪个体发电机排放如何在电网中传输并影响特定位置的用户。通过对8,870总线加州电网的模拟，使用实际CAISO数据，该方法成功地估计了潮流、发电组合和系统排放，并提供了加州各县的精细时空排放分析。该算法和研究均已开源，旨在促进未来在电网排放、规划和政策方面的研究。", "keywords": "碳排放流追踪, 最优潮流, 加州电网, 脱碳, 时空排放分析", "comments": "该论文的创新之处在于提出了一种新颖且计算高效的图论方法来精确追踪电网中的碳排放流，解决了传统方法难以量化个体发电机排放对特定位置影响的问题。其将拓扑排序和有向循环去除技术应用于最优潮流解，实现了精细化的时空排放分析，这对于电网脱碳、规划和能源政策制定具有重要意义。通过对加州真实电网的验证，显示了其在实际应用中的潜力，且开源性质将极大促进后续研究。"}}
{"id": "2507.16641", "title": "Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis", "authors": ["Sara Giordano", "Kornikar Sen", "Miguel A. Martin-Delgado"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, color figures", "url": "http://arxiv.org/abs/2507.16641v1", "summary": "A reinforcement learning (RL) framework is introduced for the efficient\nsynthesis of quantum circuits that generate specified target quantum states\nfrom a fixed initial state, addressing a central challenge in both the NISQ era\nand future fault-tolerant quantum computing. The approach utilizes tabular\nQ-learning, based on action sequences, within a discretized quantum state\nspace, to effectively manage the exponential growth of the space dimension. The\nframework introduces a hybrid reward mechanism, combining a static,\ndomain-informed reward that guides the agent toward the target state with\ncustomizable dynamic penalties that discourage inefficient circuit structures\nsuch as gate congestion and redundant state revisits. By leveraging sparse\nmatrix representations and state-space discretization, the method enables\nscalable navigation of high-dimensional environments while minimizing\ncomputational overhead. Benchmarking on graph-state preparation tasks for up to\nseven qubits, we demonstrate that the algorithm consistently discovers\nminimal-depth circuits with optimized gate counts. Moreover, extending the\nframework to a universal gate set for arbitrary quantum states, it still\nproduces minimal depth circuits, highlighting the algorithm's robustness and\nadaptability. The results confirm that this RL-driven approach efficiently\nexplores the complex quantum state space and synthesizes near-optimal quantum\ncircuits, providing a resource-efficient foundation for quantum circuit\noptimization.", "comment": "13 pages, 4 figures, color figures", "pdf_url": "http://arxiv.org/pdf/2507.16641v1", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "混合奖励驱动的强化学习用于高效量子电路合成", "tldr": "本文引入了一种混合奖励驱动的强化学习框架，用于高效合成最小深度和优化门计数的量子电路，以应对量子计算中的电路合成挑战。", "motivation": "在NISQ时代和未来的容错量子计算中，高效合成从固定初始态生成指定目标量子态的量子电路是一个核心挑战。", "method": "该方法采用基于动作序列的表格Q学习，在离散化的量子态空间中运行，以应对空间维度的指数增长。它引入了一种混合奖励机制，结合了指导智能体达到目标态的静态领域信息奖励和惩罚低效电路结构（如门拥塞和冗余状态重访）的可定制动态惩罚。通过利用稀疏矩阵表示和态空间离散化，该方法实现了高维环境的可扩展导航，同时最小化了计算开销。", "result": "在多达七个量子位的图态制备任务上的基准测试表明，该算法始终能发现具有优化门计数的最小深度电路。此外，将该框架扩展到任意量子态的通用门集时，它仍然能生成最小深度电路，突显了算法的鲁棒性和适应性。", "conclusion": "结果证实，这种强化学习驱动的方法能高效探索复杂的量子态空间并合成接近最优的量子电路，为量子电路优化提供了资源高效的基础。", "translation": "本文引入了一种强化学习（RL）框架，用于高效合成从固定初始态生成指定目标量子态的量子电路，解决了NISQ时代和未来容错量子计算中的一个核心挑战。该方法在离散化的量子态空间内利用基于动作序列的表格Q学习，有效管理空间维度的指数增长。该框架引入了一种混合奖励机制，结合了引导智能体朝向目标态的静态领域信息奖励和阻止低效电路结构（如门拥塞和冗余状态重访）的可定制动态惩罚。通过利用稀疏矩阵表示和态空间离散化，该方法实现了高维环境的可扩展导航，同时最大限度地减少了计算开销。在多达七个量子位的图态制备任务上的基准测试表明，该算法始终能发现具有优化门计数的最小深度电路。此外，将该框架扩展到任意量子态的通用门集时，它仍然能生成最小深度电路，突显了算法的鲁棒性和适应性。结果证实，这种强化学习驱动的方法能高效探索复杂的量子态空间并合成接近最优的量子电路，为量子电路优化提供了资源高效的基础。", "summary": "本文提出了一种基于强化学习的框架，通过结合表格Q学习和创新的混合奖励机制（包含静态领域信息奖励和动态惩罚），在离散化的量子态空间中高效合成量子电路。该方法利用稀疏矩阵表示和态空间离散化来处理高维问题。实验证明，该框架在多达七个量子位的图态制备和通用门集下，能够稳定地发现最小深度和优化门计数的近最优量子电路，为量子电路优化提供了高效且资源节约的解决方案。", "keywords": "量子电路合成, 强化学习, 混合奖励, Q学习, 量子优化", "comments": "本文的创新点在于提出了一个混合奖励机制，巧妙地结合了领域知识的静态奖励和惩罚低效行为的动态奖励，这对于在指数增长的量子态空间中进行高效探索至关重要。同时，通过离散化和稀疏矩阵表示来管理维度灾难，也体现了方法的实用性。该研究为量子电路合成，特别是在NISQ和未来容错量子计算背景下，提供了一种有前景的强化学习驱动的优化范式。"}}
{"id": "2507.18466", "title": "Solution of Least Squares Problems with Randomized Preconditioned Normal Equations", "authors": ["Ilse C. F. Ipsen"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18466v1", "summary": "We consider the solution of full column-rank least squares problems by means\nof normal equations that are preconditioned, symmetrically or\nnon-symmetrically, with a randomized preconditioner. With an effective\npreconditioner, the solutions from the preconditioned normal equations are\nalmost as accurate as those from the QR-based Matlab backslash (mldivide)\ncommand -- even for highly illconditioned matrices. This means the accuracy of\nthe preconditioned normal equations depends on the residual of the original\nleast squares problem. We present non-intuitive but realistic perturbation\nbounds for the relative error in the computed solutions and show that, with an\neffective preconditioner, these bounds are essentially equal to the\nperturbation bound for the original least squares problem. Probabilitistic\ncondition number bounds corroborate the effectiveness of the randomized\npreconditioner computed with small amounts of sampling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18466v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "随机预处理正规方程求解最小二乘问题", "tldr": "本文研究使用随机预处理正规方程求解最小二乘问题，结果表明即使对于病态矩阵，其精度也接近QR分解方法，并且给出了误差界限。", "motivation": "解决全列秩最小二乘问题，提高正规方程法的精度，使其在处理病态矩阵时也能保持高精度。", "method": "采用随机预处理（对称或非对称）的正规方程来求解全列秩最小二乘问题。通过分析扰动界限和概率条件数界限来评估预处理器的有效性。", "result": "1. 采用有效预处理后，预处理正规方程的解的精度几乎与基于QR分解的Matlab反斜杠命令（mldivide）一样高，即使对于高度病态的矩阵也是如此。 2. 预处理正规方程的精度取决于原始最小二乘问题的残差。 3. 提出的相对误差扰动界限在有效预处理下，本质上与原始最小二乘问题的扰动界限相等。 4. 概率条件数界限证实了少量采样计算的随机预处理器的有效性。", "conclusion": "随机预处理正规方程法能够有效且高精度地求解最小二乘问题，其精度在有效预处理下可与QR分解方法媲美，并且对病态矩阵表现良好。", "translation": "我们考虑通过正规方程求解全列秩最小二乘问题，这些正规方程通过随机预处理器进行对称或非对称预处理。通过有效的预处理器，预处理正规方程的解的精度几乎与基于QR分解的Matlab反斜杠（mldivide）命令的精度一样高——即使对于高度病态的矩阵也是如此。这意味着预处理正规方程的精度取决于原始最小二乘问题的残差。我们提出了非直观但真实的计算解相对误差的扰动界限，并表明在有效预处理器下，这些界限本质上等于原始最小二乘问题的扰动界限。概率条件数界限证实了通过少量采样计算的随机预处理器的有效性。", "summary": "本文研究了使用随机预处理正规方程求解全列秩最小二乘问题。研究发现，通过有效的随机预处理器，即使对于高度病态的矩阵，该方法的解的精度也能与基于QR分解的Matlab方法相媲美，且其精度取决于原始问题的残差。此外，文章提出了计算解相对误差的扰动界限，并证明这些界限在有效预处理下与原始问题的界限基本相同，并通过概率条件数界限进一步验证了随机预处理器的有效性。", "keywords": "最小二乘问题, 正规方程, 随机预处理, 病态矩阵, 扰动界限", "comments": "这篇论文的创新点在于引入了随机预处理器来提高正规方程法在求解最小二乘问题时的精度和鲁棒性，尤其是在处理病态矩阵时。它通过理论分析（扰动界限）和数值证据（与QR分解方法的比较，概率条件数）证明了该方法的有效性，为解决数值稳定性问题提供了一种新颖且实用的途径。"}}
{"id": "2507.11554", "title": "Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models", "authors": ["Zejian Li", "Yize Li", "Chenye Meng", "Zhongni Liu", "Yang Ling", "Shengyuan Zhang", "Guang Yang", "Changyuan Yang", "Zhiyuan Yang", "Lingyun Sun"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM25", "url": "http://arxiv.org/abs/2507.11554v3", "summary": "Recent advancements in diffusion models (DMs) have been propelled by\nalignment methods that post-train models to better conform to human\npreferences. However, these approaches typically require computation-intensive\ntraining of a base model and a reward model, which not only incurs substantial\ncomputational overhead but may also compromise model accuracy and training\nefficiency. To address these limitations, we propose Inversion-DPO, a novel\nalignment framework that circumvents reward modeling by reformulating Direct\nPreference Optimization (DPO) with DDIM inversion for DMs. Our method conducts\nintractable posterior sampling in Diffusion-DPO with the deterministic\ninversion from winning and losing samples to noise and thus derive a new\npost-training paradigm. This paradigm eliminates the need for auxiliary reward\nmodels or inaccurate appromixation, significantly enhancing both precision and\nefficiency of training. We apply Inversion-DPO to a basic task of text-to-image\ngeneration and a challenging task of compositional image generation. Extensive\nexperiments show substantial performance improvements achieved by Inversion-DPO\ncompared to existing post-training methods and highlight the ability of the\ntrained generative models to generate high-fidelity compositionally coherent\nimages. For the post-training of compostitional image geneation, we curate a\npaired dataset consisting of 11,140 images with complex structural annotations\nand comprehensive scores, designed to enhance the compositional capabilities of\ngenerative models. Inversion-DPO explores a new avenue for efficient,\nhigh-precision alignment in diffusion models, advancing their applicability to\ncomplex realistic generation tasks. Our code is available at\nhttps://github.com/MIGHTYEZ/Inversion-DPO", "comment": "Accepted by ACM MM25", "pdf_url": "http://arxiv.org/pdf/2507.11554v3", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-24", "AI": {"title_translation": "Inversion-DPO：扩散模型的精确高效后训练", "tldr": "Inversion-DPO 是一种新的扩散模型后训练方法，它通过使用 DDIM 逆转来重新构建直接偏好优化（DPO），从而规避了奖励模型的需求，显著提高了训练的精度和效率。", "motivation": "当前的扩散模型对齐方法需要计算密集型的基础模型和奖励模型训练，导致巨大的计算开销，并可能损害模型精度和训练效率。", "method": "Inversion-DPO 通过使用 DDIM 逆转来重新构建扩散模型的直接偏好优化（DPO），从而避免了奖励建模。它通过将获胜和失败样本确定性地逆转到噪声中，在 Diffusion-DPO 中进行难以处理的后验采样，从而消除了对辅助奖励模型或不准确近似的需求。为了进行组合图像生成，还整理了一个包含 11,140 张图像的配对数据集，这些图像具有复杂的结构注释和全面的评分。", "result": "与现有后训练方法相比，Inversion-DPO 取得了显著的性能提升。经过训练的生成模型能够生成高保真、组合连贯的图像。", "conclusion": "Inversion-DPO 为扩散模型中高效、高精度的对齐探索了一条新途径，推动了它们在复杂现实生成任务中的应用。", "translation": "扩散模型（DMs）的最新进展得益于对模型进行后训练以更好地符合人类偏好的对齐方法。然而，这些方法通常需要计算密集型的基础模型和奖励模型训练，这不仅会带来大量的计算开销，还可能损害模型的准确性和训练效率。为了解决这些限制，我们提出了 Inversion-DPO，这是一种新颖的对齐框架，通过使用 DDIM 逆转来重新构建扩散模型的直接偏好优化（DPO），从而规避了奖励建模。我们的方法通过将获胜和失败样本确定性地逆转到噪声中，在 Diffusion-DPO 中进行了难以处理的后验采样，从而推导出了一个新的后训练范式。这种范式消除了对辅助奖励模型或不准确近似的需求，显著提高了训练的精度和效率。我们将 Inversion-DPO 应用于文本到图像生成的基本任务和组合图像生成的挑战性任务。大量的实验表明，与现有后训练方法相比，Inversion-DPO 取得了显著的性能提升，并突出了经过训练的生成模型生成高保真、组合连贯图像的能力。为了进行组合图像生成的后训练，我们整理了一个包含 11,140 张图像的配对数据集，这些图像具有复杂的结构注释和全面的评分，旨在增强生成模型的组合能力。Inversion-DPO 为扩散模型中高效、高精度的对齐探索了一条新途径，推动了它们在复杂现实生成任务中的应用。我们的代码可在 https://github.com/MIGHTYEZ/Inversion-DPO 获取。", "summary": "Inversion-DPO 是一种新颖的扩散模型对齐框架，通过使用 DDIM 逆转重新构建直接偏好优化（DPO），提高了后训练的精度和效率。该方法消除了对奖励模型的需求，解决了现有方法在计算开销和准确性方面的问题。它成功应用于文本到图像生成和组合图像生成任务，展示了显著的性能提升以及生成高保真、组合连贯图像的能力。此外，该研究还为组合图像生成任务整理了一个新的数据集。", "keywords": "扩散模型, 后训练, 直接偏好优化, DDIM 逆转, 对齐", "comments": "该论文的创新之处在于通过使用 DDIM 逆转来重新构建 DPO，从而绕过了对奖励模型的需求。奖励模型通常是扩散模型对齐方法中的一个瓶颈。这种方法显著提高了训练效率和精度，使扩散模型在复杂任务中更具实用性。为组合图像生成整理新数据集也增加了其价值。"}}
{"id": "2507.17958", "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling", "authors": ["Daniel Carlstrom Schad", "Shrey Dixit", "Janis Keck", "Viktor Studenyak", "Aleksandr Shpilevoi", "Andrej Bicanski"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17958v1", "summary": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 32.25 on in-distribution Friends S07 and 21.25 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17958v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VIBE：用于fMRI响应建模的视频输入脑编码器", "tldr": "VIBE是一个两阶段Transformer模型，融合多模态视频、音频和文本特征来预测fMRI活动，在多个数据集上取得了高相关性，并在Algonauts 2025挑战赛中表现出色。", "motivation": "该研究的动机是开发一个能够预测fMRI活动（即大脑活动）的模型，通过整合多模态感官信息来更好地理解和建模大脑对复杂刺激的响应。", "method": "VIBE是一个两阶段Transformer模型。第一阶段是模态融合Transformer，它融合来自开源模型（Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA）的视频、音频和文本特征表示。第二阶段是预测Transformer，它带有旋转嵌入，用于时间解码。模型在CNeuroMod数据集的65小时电影数据上进行训练。", "result": "VIBE在内部数据集Friends S07上获得了32.25的平均像素级皮尔逊相关系数，在六部外部电影上获得了21.25的平均像素级皮尔逊相关系数。该架构的早期版本在Algonauts 2025挑战赛中赢得了第一阶段，并获得总成绩第二名，分别取得0.3198和0.2096的相关系数。", "conclusion": "VIBE模型通过有效融合多模态特征，实现了对fMRI活动的准确预测，并在神经科学挑战赛中展现了其卓越的性能和潜力。", "translation": "我们提出了VIBE，一个两阶段的Transformer模型，它融合多模态视频、音频和文本特征来预测fMRI活动。来自开源模型（Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA）的表示通过一个模态融合Transformer进行合并，并由一个带有旋转嵌入的预测Transformer进行时间解码。VIBE在来自CNeuroMod数据集的65小时电影数据上进行训练，并通过20个随机种子进行集成，在内部数据集《老友记》第七季上获得了32.25的平均像素级皮尔逊相关系数，在六部外部电影上获得了21.25的相关系数。相同架构的早期版本分别获得了0.3198和0.2096，赢得了Algonauts 2025挑战赛的第一阶段并获得总成绩第二名。", "summary": "VIBE是一个创新的两阶段Transformer模型，专为预测fMRI活动而设计。它通过融合来自视频、音频和文本等多种模态的特征（利用Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA等开源模型提取），并采用模态融合和时间解码Transformer架构。该模型在CNeuroMod电影数据集上进行训练，并在内部和外部电影数据集上均表现出显著的fMRI预测准确性，其性能在Algonauts 2025挑战赛中得到验证，取得了优异成绩。", "keywords": "fMRI预测, 多模态融合, Transformer, 脑编码器, VIBE", "comments": "VIBE的创新之处在于其多模态融合方法，将视频、音频和文本特征整合到一个统一的Transformer框架中来预测大脑活动。这对于理解大脑如何处理复杂、真实世界的刺激具有重要意义。其在Algonauts挑战赛中的优异表现也证明了其模型的有效性和鲁棒性。该研究为基于多模态输入的fMRI响应建模提供了一个强大的新工具。"}}
{"id": "2507.18126", "title": "U-Net Based Healthy 3D Brain Tissue Inpainting", "authors": ["Juexin Zhang", "Ying Weng", "Ke Chen"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures", "url": "http://arxiv.org/abs/2507.18126v1", "summary": "This paper introduces a novel approach to synthesize healthy 3D brain tissue\nfrom masked input images, specifically focusing on the task of 'ASNR-MICCAI\nBraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a\nU-Net-based architecture, which is designed to effectively reconstruct the\nmissing or corrupted regions of brain MRI scans. To enhance our model's\ngeneralization capabilities and robustness, we implement a comprehensive data\naugmentation strategy that involves randomly masking healthy images during\ntraining. Our model is trained on the BraTS-Local-Inpainting dataset and\ndemonstrates the exceptional performance in recovering healthy brain tissue.\nThe evaluation metrics employed, including Structural Similarity Index (SSIM),\nPeak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently\nyields impressive results. On the BraTS-Local-Inpainting validation set, our\nmodel achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score\nof 0.007. Notably, these evaluation metrics exhibit relatively low standard\ndeviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE\nscore, which indicates that our model's reliability and consistency across\nvarious input scenarios. Our method also secured first place in the challenge.", "comment": "Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.18126v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于U-Net的健康3D脑组织修复", "tldr": "本文提出了一种基于U-Net的健康3D脑组织修复方法，通过数据增强提高了模型泛化能力，并在BraTS-Local-Inpainting挑战赛中获得第一名，取得了优异的性能。", "motivation": "该研究旨在从遮蔽的输入图像中合成健康的3D脑组织，特别是针对“ASNR-MICCAI BraTS局部组织修复合成”任务，以有效重建脑部MRI扫描中缺失或损坏的区域。", "method": "本文采用了一种基于U-Net的架构，并通过在训练期间随机遮蔽健康图像来实施全面的数据增强策略，以增强模型的泛化能力和鲁棒性。模型在BraTS-Local-Inpainting数据集上进行训练。", "result": "在BraTS-Local-Inpainting验证集上，模型在恢复健康脑组织方面表现出色，SSIM分数为0.841（标准差0.103），PSNR分数为23.257（标准差4.213），MSE分数为0.007（标准差0.007）。该方法在挑战赛中获得了第一名。", "conclusion": "该基于U-Net的健康3D脑组织修复方法在BraTS-Local-Inpainting挑战赛中取得了卓越的性能和可靠性，证明了其在重建缺失或损坏脑组织区域方面的有效性。", "translation": "本文介绍了一种从遮蔽输入图像中合成健康3D脑组织的新方法，特别关注“ASNR-MICCAI BraTS局部组织修复合成”任务。我们提出的方法采用基于U-Net的架构，旨在有效重建脑部MRI扫描中缺失或损坏的区域。为了增强模型的泛化能力和鲁棒性，我们实施了一项全面的数据增强策略，包括在训练期间随机遮蔽健康图像。我们的模型在BraTS-Local-Inpainting数据集上进行训练，并在恢复健康脑组织方面表现出卓越的性能。所采用的评估指标，包括结构相似性指数（SSIM）、峰值信噪比（PSNR）和均方误差（MSE），持续产生令人印象深刻的结果。在BraTS-Local-Inpainting验证集上，我们的模型实现了0.841的SSIM分数、23.257的PSNR分数和0.007的MSE分数。值得注意的是，这些评估指标表现出相对较低的标准差，即SSIM分数为0.103，PSNR分数为4.213，MSE分数为0.007，这表明我们的模型在各种输入场景下具有可靠性和一致性。我们的方法还在挑战赛中获得了第一名。", "summary": "本文提出了一种新颖的基于U-Net的健康3D脑组织修复方法，旨在从遮蔽的MRI图像中重建缺失或损坏的脑组织区域。通过在训练过程中随机遮蔽健康图像进行数据增强，该模型显著提高了泛化能力和鲁棒性。在BraTS-Local-Inpainting数据集上的训练和验证结果显示，该方法在SSIM、PSNR和MSE等评估指标上取得了卓越的性能，并以高分在挑战赛中获得第一名，证明了其在脑组织修复任务中的有效性和可靠性。", "keywords": "U-Net, 脑组织修复, 3D图像合成, 数据增强, BraTS", "comments": "本文的创新点在于将U-Net架构应用于3D脑组织修复任务，并通过全面的数据增强策略显著提升了模型的泛化能力和鲁棒性。其重要性体现在模型在ASNR-MICCAI BraTS局部组织修复合成挑战赛中获得第一名，以及在关键评估指标上取得的优异且稳定的性能，为脑部MRI图像的缺失数据重建提供了高效可靠的解决方案。"}}
{"id": "2507.18385", "title": "HumanMaterial: Human Material Estimation from a Single Image via Progressive Training", "authors": ["Yu Jiang", "Jiahao Xia", "Jiongming Qin", "Yusen Wang", "Tuo Cao", "Chunxia Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14", "url": "http://arxiv.org/abs/2507.18385v1", "summary": "Full-body Human inverse rendering based on physically-based rendering aims to\nacquire high-quality materials, which helps achieve photo-realistic rendering\nunder arbitrary illuminations. This task requires estimating multiple material\nmaps and usually relies on the constraint of rendering result. The absence of\nconstraints on the material maps makes inverse rendering an ill-posed task.\nPrevious works alleviated this problem by building material dataset for\ntraining, but their simplified material data and rendering equation lead to\nrendering results with limited realism, especially that of skin. To further\nalleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)\nbased on scanned real data and statistical material data. In addition to the\nnormal, diffuse albedo, roughness, specular albedo, we produce displacement and\nsubsurface scattering to enhance the realism of rendering results, especially\nfor the skin. With the increase in prediction tasks for more materials, using\nan end-to-end model as in the previous work struggles to balance the importance\namong various material maps, and leads to model underfitting. Therefore, we\ndesign a model (HumanMaterial) with progressive training strategy to make full\nuse of the supervision information of the material maps and improve the\nperformance of material estimation. HumanMaterial first obtain the initial\nmaterial results via three prior models, and then refine the results by a\nfinetuning model. Prior models estimate different material maps, and each map\nhas different significance for rendering results. Thus, we design a Controlled\nPBR Rendering (CPR) loss, which enhances the importance of the materials to be\noptimized during the training of prior models. Extensive experiments on\nOpenHumanBRDF dataset and real data demonstrate that our method achieves\nstate-of-the-art performance.", "comment": "14", "pdf_url": "http://arxiv.org/pdf/2507.18385v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "HumanMaterial：基于渐进式训练的单张图像人体材质估计", "tldr": "本文提出了一种名为HumanMaterial的新模型和OpenHumanBRDF数据集，用于从单张图像中高质量估计人体材质。通过渐进式训练策略和新颖的损失函数，该方法实现了最先进的性能。", "motivation": "现有的人体逆向渲染方法在材质贴图上缺乏约束，导致其成为一个病态问题，且由于简化的数据和渲染方程，渲染结果（尤其是皮肤）的真实感有限。此外，传统的端到端模型在处理多材质预测时难以平衡各种材质贴图的重要性，容易导致模型欠拟合。", "method": "1. 构建了一个高质量的OpenHumanBRDF数据集，该数据集基于扫描真实数据和统计材质数据，并额外包含位移和次表面散射，以增强渲染结果的真实感，特别是皮肤。2. 设计了名为HumanMaterial的模型，采用渐进式训练策略。3. HumanMaterial模型首先通过三个先验模型获取初始材质结果，然后由一个微调模型进行细化。4. 引入了受控PBR渲染（CPR）损失，以在先验模型训练期间增强要优化的材质的重要性。", "result": "在OpenHumanBRDF数据集和真实数据上的大量实验表明，所提出的方法达到了最先进的性能。", "conclusion": "本研究通过构建高质量数据集（OpenHumanBRDF）、设计创新的渐进式训练策略（HumanMaterial）以及引入新颖的损失函数（CPR损失），成功解决了单张图像人体材质估计的挑战，实现了照片级真实感人体渲染的最先进性能。", "translation": "基于物理渲染的全身体逆向渲染旨在获取高质量材质，这有助于在任意光照下实现照片级真实感渲染。该任务需要估计多个材质贴图，并且通常依赖于渲染结果的约束。材质贴图上缺乏约束使得逆向渲染成为一个病态问题。以往的工作通过构建材质数据集进行训练来缓解这个问题，但其简化的材质数据和渲染方程导致渲染结果的真实感有限，尤其是皮肤。为了进一步缓解这个问题，我们构建了一个基于扫描真实数据和统计材质数据的高质量数据集（OpenHumanBRDF）。除了法线、漫反射反照率、粗糙度、镜面反照率外，我们还生成了位移和次表面散射，以增强渲染结果的真实感，特别是对于皮肤。随着对更多材质预测任务的增加，像以前工作中使用的端到端模型难以平衡各种材质贴图之间的重要性，并导致模型欠拟合。因此，我们设计了一个具有渐进式训练策略的模型（HumanMaterial），以充分利用材质贴图的监督信息并提高材质估计的性能。HumanMaterial首先通过三个先验模型获得初始材质结果，然后通过一个微调模型细化结果。先验模型估计不同的材质贴图，并且每个贴图对渲染结果具有不同的重要性。因此，我们设计了一个受控PBR渲染（CPR）损失，该损失在先验模型训练期间增强了要优化的材质的重要性。在OpenHumanBRDF数据集和真实数据上的大量实验表明，我们的方法达到了最先进的性能。", "summary": "本研究针对单张图像中人体材质估计的病态问题，提出了HumanMaterial模型和OpenHumanBRDF数据集。OpenHumanBRDF数据集包含位移和次表面散射以提高真实感。HumanMaterial模型采用渐进式训练策略，结合先验模型和微调模型，并引入受控PBR渲染（CPR）损失来优化不同材质的重要性。实验证明，该方法在人体材质估计方面达到了最先进的性能，尤其提升了皮肤的真实感。", "keywords": "人体材质估计, 逆向渲染, 渐进式训练, OpenHumanBRDF, 受控PBR渲染损失", "comments": "这项工作通过构建高质量数据集和设计创新的渐进式训练策略，有效解决了单张图像人体逆向渲染中材质估计的病态问题和现有方法在处理多材质时的欠拟合问题。特别值得注意的是，引入位移和次表面散射以及CPR损失对于提升皮肤等复杂材质的真实感具有重要意义，是该研究的创新点。"}}
{"id": "2507.04600", "title": "DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification", "authors": ["Zhipeng Liu", "Peibo Duan", "Binwu Wang", "Xuan Tang", "Qi Chu", "Changsheng Zhang", "Yongsheng Huang", "Bin Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for presentation at the ACM International Conference on Multimedia (ACM MM 2025)", "url": "http://arxiv.org/abs/2507.04600v2", "summary": "Real-world time series typically exhibit complex temporal variations, making\nthe time series classification task notably challenging. Recent advancements\nhave demonstrated the potential of multi-scale analysis approaches, which\nprovide an effective solution for capturing these complex temporal patterns.\nHowever, existing multi-scale analysis-based time series prediction methods\nfail to eliminate redundant scale-shared features across multi-scale time\nseries, resulting in the model over- or under-focusing on scale-shared\nfeatures. To address this issue, we propose a novel end-to-end Disentangled\nMulti-Scale framework for Time Series classification (DisMS-TS). The core idea\nof DisMS-TS is to eliminate redundant shared features in multi-scale time\nseries, thereby improving prediction performance. Specifically, we propose a\ntemporal disentanglement module to capture scale-shared and scale-specific\ntemporal representations, respectively. Subsequently, to effectively learn both\nscale-shared and scale-specific temporal representations, we introduce two\nregularization terms that ensure the consistency of scale-shared\nrepresentations and the disparity of scale-specific representations across all\ntemporal scales. Extensive experiments conducted on multiple datasets validate\nthe superiority of DisMS-TS over its competitive baselines, with the accuracy\nimprovement up to 9.71%.", "comment": "This paper has been accepted for presentation at the ACM\n  International Conference on Multimedia (ACM MM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.04600v2", "cate": "cs.AI", "date": "2025-07-07", "updated": "2025-07-24", "AI": {"title_translation": "DisMS-TS: 消除时间序列分类中冗余的多尺度特征", "tldr": "本文提出DisMS-TS框架，通过消除多尺度时间序列中的冗余共享特征，提高时间序列分类性能，实验证明其优越性。", "motivation": "现实世界时间序列的复杂时间变化使得时间序列分类具有挑战性。现有多尺度分析方法未能消除多尺度时间序列中冗余的尺度共享特征，导致模型过度或不足地关注这些特征。", "method": "提出端到端的Disentangled Multi-Scale框架 (DisMS-TS)。核心思想是消除多尺度时间序列中的冗余共享特征。具体地，提出了一个时间解耦模块来分别捕获尺度共享和尺度特定的时间表示。引入两个正则化项，确保尺度共享表示的一致性和尺度特定表示在所有时间尺度上的差异性。", "result": "在多个数据集上进行了广泛实验，验证了DisMS-TS优于其竞争基线，准确率提升高达9.71%。", "conclusion": "DisMS-TS通过有效消除多尺度时间序列中的冗余共享特征，显著提升了时间序列分类的性能。", "translation": "现实世界中的时间序列通常表现出复杂的时间变化，使得时间序列分类任务极具挑战性。最近的进展表明，多尺度分析方法具有潜力，为捕获这些复杂时间模式提供了有效的解决方案。然而，现有的基于多尺度分析的时间序列预测方法未能消除多尺度时间序列中冗余的尺度共享特征，导致模型过度或不足地关注尺度共享特征。为了解决这个问题，我们提出了一种新颖的端到端解耦多尺度时间序列分类框架（DisMS-TS）。DisMS-TS的核心思想是消除多尺度时间序列中冗余的共享特征，从而提高预测性能。具体来说，我们提出了一个时间解耦模块，分别捕获尺度共享和尺度特定的时间表示。随后，为了有效地学习尺度共享和尺度特定的时间表示，我们引入了两个正则化项，以确保尺度共享表示的一致性以及尺度特定表示在所有时间尺度上的差异性。在多个数据集上进行的广泛实验验证了DisMS-TS优于其竞争基线，准确率提升高达9.71%。", "summary": "本文提出DisMS-TS，一个用于时间序列分类的端到端解耦多尺度框架。该框架旨在解决现有方法未能消除多尺度时间序列中冗余共享特征的问题。通过引入时间解耦模块和两个正则化项，DisMS-TS能够分别捕获并有效学习尺度共享和尺度特定的时间表示。实验结果表明，DisMS-TS在多个数据集上显著优于现有基线，准确率最高提升9.71%。", "keywords": "时间序列分类, 多尺度分析, 特征解耦, 冗余特征, DisMS-TS", "comments": "该论文的创新点在于提出了DisMS-TS框架，通过解耦尺度共享和尺度特定的特征来解决多尺度时间序列分析中的冗余特征问题。这种方法有效地避免了模型对冗余信息的过度或不足关注，从而提高了时间序列分类的准确性。其重要性体现在为处理复杂时间序列数据提供了一种更有效、更鲁棒的特征学习机制。抽象中未提及具体限制，但未来工作可能涉及探索更复杂的解耦机制或在更多样化的时间序列任务上的应用。"}}
{"id": "2507.14783", "title": "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards", "authors": ["Derek Li", "Jiaming Zhou", "Amirreza Kazemi", "Qianyi Sun", "Abbas Ghaddar", "Mohammad Ali Alomrani", "Liheng Ma", "Yu Luo", "Dong Li", "Feng Wen", "Jianye Hao", "Mark Coates", "Yingxue Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14783v2", "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce\nOmni-Thinker, a unified reinforcement learning (RL) framework that enhances LLM\nperformance across diverse tasks by combining rule-based verifiable rewards\nwith generative preference signals via LLM-as-a-Judge evaluations. Our approach\nenables consistent optimization across task types and scales RL-based training\nto subjective domains. We further investigate training strategies,\ndemonstrating that a curriculum-based progression that orders tasks from\nstructured to open-ended improves performance and reduces forgetting.\nExperimental results across four domains reveal that curriculum learning\nimproves performance by 5.2% over joint training and 9.1% over model merging.\nThese results highlight the importance of task-aware sampling and hybrid\nsupervision in scaling RL-based post-training for general-purpose LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14783v2", "cate": "cs.LG", "date": "2025-07-20", "updated": "2025-07-24", "AI": {"title_translation": "Omni-Thinker：通过混合奖励的多任务强化学习提升大型语言模型跨领域泛化能力", "tldr": "Omni-Thinker引入了一个统一的强化学习框架，结合规则奖励和生成偏好信号，通过课程学习策略显著提升了大型语言模型在多任务和跨领域泛化能力，优于联合训练和模型合并。", "motivation": "现有的后训练方法（如SFT）在大型语言模型（LLMs）的泛化能力上存在不足，倾向于记忆而非可迁移学习，限制了LLMs在广泛任务上的表现。", "method": "本文提出了Omni-Thinker，一个统一的强化学习（RL）框架，通过结合基于规则的可验证奖励和通过“LLM作为评判者”评估的生成偏好信号来增强LLM在不同任务上的性能。该方法实现了跨任务类型的优化，并将RL训练扩展到主观领域。研究还探索了训练策略，发现从结构化到开放式任务的课程学习路径可以提高性能并减少遗忘。", "result": "在四个领域进行的实验结果表明，课程学习相比联合训练将性能提高了5.2%，相比模型合并提高了9.1%。", "conclusion": "这些结果突出了任务感知采样和混合监督在扩展基于RL的通用大型语言模型后训练中的重要性。", "translation": "通用人工智能的进步依赖于在广泛任务（从结构化推理到创造性生成）中表现出色的大型语言模型（LLM）。然而，像监督微调（SFT）这样的后训练方法往往在泛化方面表现不佳，偏向记忆而非可迁移学习。在这项工作中，我们引入了Omni-Thinker，一个统一的强化学习（RL）框架，通过结合基于规则的可验证奖励和通过“LLM作为评判者”评估的生成偏好信号来增强LLM在不同任务上的性能。我们的方法能够实现跨任务类型的一致优化，并将基于RL的训练扩展到主观领域。我们进一步研究了训练策略，证明了从结构化到开放式任务的课程式进展可以提高性能并减少遗忘。在四个领域进行的实验结果表明，课程学习相比联合训练将性能提高了5.2%，相比模型合并提高了9.1%。这些结果突出了任务感知采样和混合监督在扩展基于RL的通用大型语言模型后训练中的重要性。", "summary": "本文提出了Omni-Thinker，一个基于强化学习的统一框架，旨在解决大型语言模型（LLMs）在跨领域泛化能力上的不足，特别是现有监督微调方法偏向记忆的问题。Omni-Thinker通过结合规则奖励和“LLM作为评判者”的生成偏好信号，实现了对LLM在多样化任务上的性能提升。研究还发现，采用从结构化到开放式任务的课程学习策略能有效提高模型性能并减少遗忘。实验结果表明，该课程学习方法在四个领域上分别比联合训练和模型合并提高了5.2%和9.1%的性能。这强调了任务感知采样和混合监督在扩展通用LLM的强化学习后训练中的关键作用。", "keywords": "大型语言模型, 强化学习, 跨领域泛化, 混合奖励, 课程学习", "comments": "该论文通过引入Omni-Thinker框架和混合奖励机制，为提升LLMs的跨领域泛化能力提供了一个新颖的强化学习视角。特别是将“LLM作为评判者”的评估引入RL训练，以及探索课程学习策略，是其重要的创新点。这对于解决LLMs在复杂和主观任务上的泛化挑战具有重要意义。"}}
{"id": "2507.18393", "title": "PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses", "authors": ["Mahiro Ozaki", "Li Chen", "Shotaro Naganuma", "Valdemar Švábenský", "Fumiya Okubo", "Atsushi Shimada"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear in the Proceedings of the IEEE SMC 2025 conference", "url": "http://arxiv.org/abs/2507.18393v1", "summary": "This study proposes and evaluates the PAnoramic Learning Map (PALM), a\nlearning analytics (LA) dashboard designed to address the scalability\nchallenges of LA by integrating curriculum-level information. Traditional LA\nresearch has predominantly focused on individual courses or learners and often\nlacks a framework that considers the relationships between courses and the\nlong-term trajectory of learning. To bridge this gap, PALM was developed to\nintegrate multilayered educational data into a curriculum map, enabling\nlearners to intuitively understand their learning records and academic\nprogression. We conducted a system evaluation to assess PALM's effectiveness in\ntwo key areas: (1) its impact on students' awareness of their learning\nbehaviors, and (2) its comparative performance against existing systems. The\nresults indicate that PALM enhances learners' awareness of study planning and\nreflection, particularly by improving perceived behavioral control through the\nvisual presentation of individual learning histories and statistical trends,\nwhich clarify the links between learning actions and outcomes. Although PALM\nrequires ongoing refinement as a system, it received significantly higher\nevaluations than existing systems in terms of visual appeal and usability. By\nserving as an information resource with previously inaccessible insights, PALM\nenhances self-regulated learning and engagement, representing a significant\nstep beyond conventional LA toward a comprehensive and scalable approach.", "comment": "To appear in the Proceedings of the IEEE SMC 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.18393v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PALM：集成学习分析与课程图的广角学习地图，实现跨课程的可扩展洞察", "tldr": "PALM是一个学习分析仪表板，通过整合课程级信息，解决了传统学习分析在可扩展性方面的挑战，并增强了学习者的自我调节学习和参与度。", "motivation": "传统的学习分析研究主要集中在个体课程或学习者，缺乏考虑课程间关系和长期学习轨迹的框架，导致学习分析的可扩展性面临挑战。", "method": "本研究提出了并评估了广角学习地图（PALM），这是一个集成了课程级信息的学习分析仪表板。通过将多层教育数据整合到课程图中，使学习者直观地理解学习记录和学术进展。研究通过系统评估，评估了PALM对学生学习行为意识的影响以及与现有系统的比较表现。", "result": "结果表明，PALM通过可视化呈现个人学习历史和统计趋势，增强了学习者对学习规划和反思的意识，特别是提高了感知行为控制。尽管PALM系统需要持续改进，但其在视觉吸引力和可用性方面获得了比现有系统更高的评价。", "conclusion": "PALM作为一种提供以前无法获取洞察的信息资源，增强了自我调节学习和参与度，代表着学习分析领域超越传统方法，迈向全面和可扩展方法的重要一步。", "translation": "本研究提出并评估了广角学习地图（PALM），这是一种学习分析（LA）仪表板，旨在通过整合课程级信息来解决学习分析的可扩展性挑战。传统的学习分析研究主要集中在个体课程或学习者，并且通常缺乏一个考虑课程之间关系和长期学习轨迹的框架。为了弥合这一差距，PALM被开发出来，将多层教育数据整合到课程图中，使学习者能够直观地理解他们的学习记录和学业进展。我们进行了系统评估，以评估PALM在两个关键领域中的有效性：（1）其对学生学习行为意识的影响，以及（2）其与现有系统的比较表现。结果表明，PALM增强了学习者对学习规划和反思的意识，特别是通过可视化呈现个人学习历史和统计趋势，阐明了学习行为与结果之间的联系，从而改善了感知行为控制。尽管PALM作为一个系统需要持续改进，但其在视觉吸引力和可用性方面获得了比现有系统显著更高的评价。通过作为一种提供以前无法获取洞察的信息资源，PALM增强了自我调节学习和参与度，代表着超越传统学习分析，迈向全面和可扩展方法的重大一步。", "summary": "本研究提出了一种名为PALM（广角学习地图）的学习分析仪表板，旨在通过整合课程级信息来解决传统学习分析在可扩展性方面的局限。PALM将多层教育数据整合到课程图中，帮助学习者直观理解学习记录和学业进展。系统评估显示，PALM能有效提升学生对学习行为的意识、规划和反思能力，并通过可视化数据增强了感知行为控制。尽管仍需完善，但PALM在视觉吸引力和可用性上优于现有系统，为自我调节学习和参与度提供了新的洞察，标志着学习分析迈向更全面、可扩展的方法。", "keywords": "学习分析, 课程图, 可扩展性, 自我调节学习, 仪表板", "comments": "PALM的创新之处在于其超越了传统学习分析对单一课程或个体的关注，通过整合课程图和多层教育数据，实现了跨课程的宏观洞察，解决了学习分析的可扩展性问题。其对学习者自我调节学习和行为控制的提升，以及在用户体验上的优势，都显示了其在教育技术领域的潜力。"}}
{"id": "2507.18220", "title": "Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective", "authors": ["Ansei Yonezawa", "Heisei Yonezawa", "Shuichi Yahagi", "Itsuro Kajiwara", "Shinya Kijimoto", "Hikaru Taniuchi", "Kentaro Murakami"], "categories": ["cs.LG", "math.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.18220v1", "summary": "The sparse identification of nonlinear dynamics (SINDy) approach can discover\nthe governing equations of dynamical systems based on measurement data, where\nthe dynamical model is identified as the sparse linear combination of the given\nbasis functions. A major challenge in SINDy is the design of a library, which\nis a set of candidate basis functions, as the appropriate library is not\ntrivial for many dynamical systems. To overcome this difficulty, this study\nproposes SINDy with library optimization mechanism (SINDy-LOM), which is a\ncombination of the sparse regression technique and the novel learning strategy\nof the library. In the proposed approach, the basis functions are parametrized.\nThe SINDy-LOM approach involves a two-layer optimization architecture: the\ninner-layer, in which the data-driven model is extracted as the sparse linear\ncombination of the candidate basis functions, and the outer-layer, in which the\nbasis functions are optimized from the viewpoint of the recursive long-term\n(RLT) prediction accuracy; thus, the library design is reformulated as the\noptimization of the parametrized basis functions. The resulting SINDy-LOM model\nhas good interpretability and usability, as the proposed approach yields the\nparsimonious model. The library optimization mechanism significantly reduces\nuser burden. The RLT perspective improves the reliability of the resulting\nmodel compared with the traditional SINDy approach that can only ensure the\none-step-ahead prediction accuracy. The validity of the proposed approach is\ndemonstrated by applying it to a diesel engine airpath system, which is a\nwell-known complex industrial system.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.18220v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "稀疏非线性动力学识别与库优化机制：递归长期预测视角", "tldr": "SINDy-LOM通过优化基函数库来提高非线性动力学系统的长期预测精度和模型可解释性，显著降低了用户负担。", "motivation": "SINDy方法在识别动力学系统时，主要挑战在于设计合适的基函数库，因为对于许多系统而言，选择合适的库并非易事。", "method": "本文提出了SINDy与库优化机制（SINDy-LOM），结合了稀疏回归技术和新颖的库学习策略。该方法将基函数参数化，并采用两层优化架构：内层提取数据驱动模型作为候选基函数的稀疏线性组合，外层从递归长期（RLT）预测精度的角度优化基函数，从而将库设计重构为参数化基函数的优化问题。", "result": "所提出的SINDy-LOM模型具有良好的可解释性和可用性，因为它产生了简约模型。库优化机制显著降低了用户负担。与传统SINDy仅能确保一步预测精度相比，RLT视角提高了所得模型的可靠性。该方法已成功应用于柴油机气路系统，验证了其有效性。", "conclusion": "SINDy-LOM通过引入库优化机制和递归长期预测视角，有效解决了传统SINDy在库设计上的挑战，并显著提升了模型在长期预测方面的可靠性、可解释性及用户友好性。", "translation": "稀疏非线性动力学识别（SINDy）方法可以根据测量数据发现动力学系统的控制方程，其中动力学模型被识别为给定基函数的稀疏线性组合。SINDy的一个主要挑战是库的设计，即一组候选基函数，因为对于许多动力学系统而言，选择合适的库并非易事。为了克服这一困难，本研究提出了SINDy与库优化机制（SINDy-LOM），它是稀疏回归技术和新颖的库学习策略的结合。在所提出的方法中，基函数是参数化的。SINDy-LOM方法涉及一个两层优化架构：内层，其中数据驱动模型被提取为候选基函数的稀疏线性组合；外层，其中基函数从递归长期（RLT）预测精度的角度进行优化；因此，库设计被重新表述为参数化基函数的优化。所得到的SINDy-LOM模型具有良好的可解释性和可用性，因为所提出的方法产生了简约模型。库优化机制显著降低了用户负担。与只能确保一步预测精度的传统SINDy方法相比，RLT视角提高了所得模型的可靠性。通过将其应用于柴油机气路系统（一个著名的复杂工业系统）验证了所提出方法的有效性。", "summary": "本文提出了SINDy与库优化机制（SINDy-LOM），旨在解决传统稀疏非线性动力学识别（SINDy）方法中库设计困难的问题。SINDy-LOM通过参数化基函数并采用两层优化架构，在内层进行稀疏回归，在外层基于递归长期预测精度优化基函数。该方法显著提升了模型的长期预测可靠性、可解释性和可用性，并减轻了用户负担。其有效性已在一个复杂的工业系统——柴油机气路系统上得到验证。", "keywords": "稀疏非线性动力学识别, 库优化, 递归长期预测, SINDy-LOM, 动力学系统", "comments": "本文创新性地将基函数库的设计问题转化为一个优化问题，并通过引入递归长期预测（RLT）视角，显著提升了SINDy模型在长期预测方面的可靠性，解决了传统SINDy仅关注一步预测的局限性。此外，其两层优化架构和对用户负担的考虑，使其具有较高的实用价值和工程意义。"}}
{"id": "2411.11596", "title": "Integrating and Comparing Radiality Constraints for Optimized Distribution System Reconfiguration", "authors": ["Pablo Cortes", "Alejandra Tabares", "Fredy Franco", "Astrid Xiomara Rodríguez", "David Álvarez-Martínez"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.11596v3", "summary": "The reconfiguration of electrical power distribution systems is a crucial\noptimization problem aimed at minimizing power losses by altering the system\ntopology through the operation of interconnection switches. This problem,\ntypically modelled as a mixed integer nonlinear program demands high\ncomputational resources for large scale networks and requires specialized\nradiality constraints for maintaining the tree like structure of distribution\nnetworks. This paper presents a comprehensive analysis that integrates and\ncompares the computational burden associated with different radiality\nconstraint formulations proposed in the specialized literature for the\nreconfiguration of distribution systems. By using consistent hardware and\nsoftware setups, we evaluate the performance of these constraints across\nseveral well known test cases. Our findings reveal significant differences in\ncomputational efficiency depending on the chosen set of radiality constraints,\nproviding valuable insights for optimizing reconfiguration strategies in\npractical distribution networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.11596v3", "cate": "eess.SY", "date": "2024-11-18", "updated": "2025-07-23", "AI": {"title_translation": "整合与比较用于优化配电系统重构的辐射状约束", "tldr": "本文整合并比较了配电系统重构中不同辐射状约束公式的计算负担，发现计算效率因所选约束集而异。", "motivation": "配电系统重构是一个关键的优化问题，旨在通过改变系统拓扑来最小化功率损耗。该问题通常被建模为混合整数非线性规划，对于大型网络需要高计算资源，并且需要专门的辐射状约束来维持配电网络的树状结构。", "method": "本文对文献中提出的用于配电系统重构的不同辐射状约束公式的计算负担进行了综合分析和比较。通过使用一致的硬件和软件设置，在多个知名测试案例中评估了这些约束的性能。", "result": "研究结果表明，计算效率因所选的辐射状约束集而存在显著差异。", "conclusion": "本文为优化实际配电网络中的重构策略提供了有价值的见解，强调了选择合适的辐射状约束对计算效率的重要性。", "translation": "电气配电系统的重构是一个关键的优化问题，旨在通过操作互联开关改变系统拓扑来最小化功率损耗。该问题通常被建模为混合整数非线性规划，对于大型网络需要高计算资源，并且需要专门的辐射状约束来维持配电网络的树状结构。本文提出了一项综合分析，整合并比较了专业文献中提出的用于配电系统重构的不同辐射状约束公式相关的计算负担。通过使用一致的硬件和软件设置，我们在几个知名测试案例中评估了这些约束的性能。我们的研究结果揭示了计算效率因所选辐射状约束集而存在显著差异，为优化实际配电网络中的重构策略提供了有价值的见解。", "summary": "本文深入分析了配电系统重构中不同辐射状约束公式的计算性能。研究通过一致的实验设置，在多个测试案例中评估了这些约束的计算负担，发现不同的辐射状约束对优化问题的计算效率有显著影响，为实际应用中的策略选择提供了重要指导。", "keywords": "配电系统重构, 辐射状约束, 计算效率, 功率损耗, 优化", "comments": "本文的创新点在于系统地整合和比较了现有文献中不同的辐射状约束公式，并通过一致的实验环境揭示了它们在计算效率上的差异。这对于配电系统重构这一计算密集型问题具有重要意义，有助于工程师和研究人员选择更高效的约束策略，从而提升优化算法的实用性。"}}
{"id": "2006.07841", "title": "On Leveraging Unlabeled Data for Concurrent Positive-Unlabeled Classification and Robust Generation", "authors": ["Bing Yu", "Ke Sun", "He Wang", "Zhouchen Lin", "Zhanxing Zhu"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in International Conference on Image and Graphics (ICIG), 2025", "url": "http://arxiv.org/abs/2006.07841v3", "summary": "The scarcity of class-labeled data is a ubiquitous bottleneck in many machine\nlearning problems. While abundant unlabeled data typically exist and provide a\npotential solution, it is highly challenging to exploit them. In this paper, we\naddress this problem by leveraging Positive-Unlabeled~(PU) classification and\nthe conditional generation with extra unlabeled data \\emph{simultaneously}. We\npresent a novel training framework to jointly target both PU classification and\nconditional generation when exposed to extra data, especially\nout-of-distribution unlabeled data, by exploring the interplay between them: 1)\nenhancing the performance of PU classifiers with the assistance of a novel\nClassifier-Noise-Invariant Conditional GAN~(CNI-CGAN) that is robust to noisy\nlabels, 2) leveraging extra data with predicted labels from a PU classifier to\nhelp the generation. Theoretically, we prove the optimal condition of CNI-CGAN\nand experimentally, we conducted extensive evaluations on diverse datasets.", "comment": "Published in International Conference on Image and Graphics (ICIG),\n  2025", "pdf_url": "http://arxiv.org/pdf/2006.07841v3", "cate": "cs.LG", "date": "2020-06-14", "updated": "2025-07-24", "AI": {"title_translation": "关于利用未标记数据进行并发正负未标记分类和鲁棒生成的研究", "tldr": "本文提出了一种新颖的训练框架，通过结合正未标记（PU）分类和条件生成，同时利用额外的未标记数据，即使是分布外数据，以解决标记数据稀缺的问题。", "motivation": "在许多机器学习问题中，类别标记数据的稀缺是一个普遍的瓶颈。虽然通常存在大量未标记数据并提供潜在解决方案，但利用它们极具挑战性。", "method": "本文提出了一种新颖的训练框架，旨在同时解决PU分类和条件生成问题，即使面对分布外未标记数据。该方法通过探索两者之间的相互作用实现：1）借助对噪声标签鲁棒的新型分类器-噪声-不变条件GAN（CNI-CGAN）来提高PU分类器的性能；2）利用PU分类器预测标签的额外数据来辅助生成。", "result": "理论上，我们证明了CNI-CGAN的最优条件；实验上，我们在不同数据集上进行了广泛评估。", "conclusion": "本文提出的框架通过同时利用PU分类和条件生成，并借助CNI-CGAN的鲁棒性，有效解决了标记数据稀缺和利用未标记数据的挑战。", "translation": "在许多机器学习问题中，类别标记数据的稀缺是一个普遍的瓶颈。虽然通常存在大量未标记数据并提供潜在解决方案，但利用它们极具挑战性。在本文中，我们通过同时利用正负未标记（PU）分类和带有额外未标记数据的条件生成来解决这个问题。我们提出了一种新颖的训练框架，旨在同时实现PU分类和条件生成，即使面对额外的，特别是分布外未标记数据，通过探索它们之间的相互作用：1）在一种对噪声标签鲁棒的新型分类器-噪声-不变条件GAN（CNI-CGAN）的帮助下，提高PU分类器的性能，2）利用来自PU分类器预测标签的额外数据来帮助生成。理论上，我们证明了CNI-CGAN的最优条件，并在实验上，我们对不同数据集进行了广泛评估。", "summary": "本文提出了一种新颖的训练框架，旨在同时解决正未标记（PU）分类和条件生成问题，以有效利用稀缺的标记数据和丰富的未标记数据。该框架通过结合一个对噪声标签鲁棒的条件GAN（CNI-CGAN）来增强PU分类器，并利用PU分类器预测的标签来辅助数据生成，即使是处理分布外数据。研究在理论上证明了CNI-CGAN的最优条件，并在多个数据集上进行了实验验证。", "keywords": "正未标记分类, 条件生成, 未标记数据, 鲁棒性, CNI-CGAN", "comments": "本文的创新点在于提出了一个统一的框架，同时解决了PU分类和鲁棒生成两个核心问题，并通过探索它们之间的相互作用来充分利用未标记数据，特别是分布外数据。CNI-CGAN的引入提高了模型对噪声标签的鲁棒性，这对于实际应用中数据质量不高的场景非常重要。该方法为解决标记数据稀缺的挑战提供了一个有前景的方向。"}}
{"id": "2505.01969", "title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection", "authors": ["Jiayi Cheng", "Can Gao", "Jie Zhou", "Jiajun Wen", "Tao Dai", "Jinbao Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025", "url": "http://arxiv.org/abs/2505.01969v2", "summary": "3D Anomaly Detection (AD) is a promising means of controlling the quality of\nmanufactured products. However, existing methods typically require carefully\ntraining a task-specific model for each category independently, leading to high\ncost, low efficiency, and weak generalization. Therefore, this paper presents a\nnovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims\nto utilize both local and global geometry-aware information to reconstruct\nnormal representations of all categories. First, to learn robust and\ngeneralized features of different categories, we propose an adaptive\ngeometry-aware masked attention module that extracts geometry variation\ninformation to guide mask attention. Then, we introduce a local geometry-aware\nencoder reinforced by the improved mask attention to encode group-level feature\ntokens. Finally, we design a global query decoder that utilizes point cloud\nposition embeddings to improve the decoding process and reconstruction ability.\nThis leads to local and global geometry-aware reconstructed feature tokens for\nthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and\nAnomaly-ShapeNet datasets, and exhibits significant superiority over current\nstate-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement\nin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The\ncode is available at https://github.com/iCAN-SZU/MC3D-AD.", "comment": "7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2505.01969v2", "cate": "cs.CV", "date": "2025-05-04", "updated": "2025-07-24", "AI": {"title_translation": "MC3D-AD：一种用于多类别三维异常检测的统一几何感知重建模型", "tldr": "MC3D-AD提出了一种统一的几何感知重建模型，用于多类别3D异常检测，通过结合局部和全局几何信息，显著优于现有单类别方法。", "motivation": "现有3D异常检测方法通常需要为每个类别独立训练特定模型，导致成本高、效率低且泛化能力弱。", "method": "本文提出MC3D-AD，一个统一的多类别3D异常检测模型。首先，引入自适应几何感知掩码注意力模块以提取几何变化信息。其次，通过改进的掩码注意力强化局部几何感知编码器。最后，设计全局查询解码器，利用点云位置嵌入提升解码和重建能力，从而生成局部和全局几何感知重建特征令牌。", "result": "MC3D-AD在Real3D-AD和Anomaly-ShapeNet两个公开数据集上进行了评估，在对象级AUROC方面分别比现有最先进的单类别方法提高了3.1%和9.3%。", "conclusion": "MC3D-AD通过其统一的几何感知重建方法，在多类别3D异常检测任务中显著优于现有的单类别方法，证明了其在质量控制中的潜力和有效性。", "translation": "三维异常检测（AD）是控制制成品质量的一种很有前景的手段。然而，现有方法通常需要为每个类别独立精心训练一个任务特定模型，导致成本高、效率低且泛化能力弱。因此，本文提出了一种新颖的统一模型，用于多类别三维异常检测（MC3D-AD），旨在利用局部和全局几何感知信息来重建所有类别的正常表示。首先，为了学习不同类别的鲁棒和广义特征，我们提出了一种自适应几何感知掩码注意力模块，该模块提取几何变化信息以引导掩码注意力。然后，我们引入了一个由改进的掩码注意力强化的局部几何感知编码器，用于编码组级特征令牌。最后，我们设计了一个全局查询解码器，该解码器利用点云位置嵌入来改善解码过程和重建能力。这为AD任务带来了局部和全局几何感知重建特征令牌。MC3D-AD在两个公开可用的Real3D-AD和Anomaly-ShapeNet数据集上进行了评估，并显示出相对于当前最先进的单类别方法的显著优越性，在Real3D-AD和Anomaly-ShapeNet上分别实现了对象级AUROC 3.1%和9.3%的改进。代码可在https://github.com/iCAN-SZU/MC3D-AD获取。", "summary": "本文提出MC3D-AD，一种新颖的统一几何感知重建模型，用于解决多类别3D异常检测中现有方法成本高、效率低和泛化能力弱的问题。MC3D-AD通过引入自适应几何感知掩码注意力模块、局部几何感知编码器和全局查询解码器，有效利用局部和全局几何信息重建正常表示。实验结果表明，MC3D-AD在Real3D-AD和Anomaly-ShapeNet数据集上，其对象级AUROC分别比现有最佳单类别方法提高了3.1%和9.3%，展现了显著的优越性。", "keywords": "3D异常检测, 多类别, 几何感知, 重建, 深度学习", "comments": "MC3D-AD的创新点在于其统一的多类别3D异常检测框架，通过结合局部和全局几何感知信息进行重建，有效解决了传统方法效率和泛化性不足的问题。其自适应几何感知掩码注意力模块和全局查询解码器的设计，提升了模型学习鲁棒特征和重建异常的能力，对于工业质量控制领域具有重要意义。"}}
{"id": "2507.18515", "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat", "authors": ["Zezhou Yang", "Ting Peng", "Cuiyun Gao", "Chaozheng Wang", "Hailiang Huang", "Yuetang Deng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in ICSME 25 Industry Track", "url": "http://arxiv.org/abs/2507.18515v1", "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.", "comment": "Accepted in ICSME 25 Industry Track", "pdf_url": "http://arxiv.org/pdf/2507.18515v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "深入探讨检索增强生成在代码补全中的应用：微信实践经验", "tldr": "RAG在微信的工业级代码库中显著提升了代码补全性能，其中基于相似性的RAG结合词法和语义检索表现最优。", "motivation": "现有研究已验证检索增强生成（RAG）在公共代码库上的有效性，但开源和闭源代码库之间存在的潜在分布差异尚未被充分探索，本研究旨在弥补这一差距，探究RAG在工业级闭源代码库中的性能。", "method": "在微信的工业级代码库（包含1,669个内部仓库）中，对26个参数范围从0.5B到671B的开源大型语言模型，实证研究了基于标识符的RAG和基于相似性的RAG两种主要方法的代码补全性能。对基于相似性的RAG，进一步探索了词法（如BM25）和语义（如GTE-Qwen）检索技术。此外，还进行了开发者调查以验证其实用性。", "result": "1. 两种RAG方法在闭源仓库中均有效，其中基于相似性的RAG表现更优。 2. 基于相似性的RAG的有效性随更先进的检索技术而提高，BM25（词法检索）和GTE-Qwen（语义检索）表现突出。 3. 词法和语义检索技术的结合产生了最佳结果，显示出互补优势。", "conclusion": "检索增强生成（RAG）方法，特别是结合词法和语义检索的基于相似性的RAG，在工业级闭源代码库中的代码补全任务上表现出显著的有效性和实用性，并获得了开发者的认可。", "translation": "代码补全作为软件工程中提高开发者生产力的关键任务，随着大型语言模型（LLMs）的快速发展取得了显著进步。近年来，检索增强生成（RAG）作为一种有前景的方法出现，它利用代码库中的相关上下文来增强LLMs的代码补全能力，而无需模型重新训练。尽管现有研究已证明RAG在公共仓库和基准测试上的有效性，但开源和闭源代码库之间潜在的分布差异带来了独特的挑战，这些挑战仍未被探索。为了弥补这个差距，我们进行了一项实证研究，调查了广泛使用的RAG方法在微信（最大的专有软件系统之一）的工业级代码库中用于代码补全的性能。具体而言，我们广泛探索了两种主要的RAG方法，即基于标识符的RAG和基于相似性的RAG，涵盖了26个参数从0.5B到671B的开源LLMs。为了进行更全面的分析，我们对基于相似性的RAG采用了不同的检索技术，包括词法和语义检索。基于1,669个内部仓库，我们取得了几个关键发现：（1）两种RAG方法在闭源仓库中均表现出有效性，其中基于相似性的RAG表现出更优的性能；（2）基于相似性的RAG的有效性随着更先进的检索技术而提高，其中BM25（词法检索）和GTE-Qwen（语义检索）取得了优异的性能；（3）词法和语义检索技术的结合产生了最佳结果，展示了互补优势。此外，我们还进行了一项开发者调查，以验证RAG方法在真实开发环境中的实际效用。", "summary": "本文深入探讨了检索增强生成（RAG）在微信工业级闭源代码库中对代码补全的有效性。研究发现，基于标识符和基于相似性的RAG方法均能提升代码补全性能，其中基于相似性的RAG结合词法和语义检索（如BM25和GTE-Qwen）表现最佳，且两者结合能达到最优效果。通过在1,669个内部仓库上对26个开源LLM进行实验，并辅以开发者调查，验证了RAG在实际开发环境中的实用价值。", "keywords": "代码补全, 检索增强生成, 大语言模型, 微信, 工业级代码库", "comments": "这项研究填补了RAG在工业级闭源代码库中应用的研究空白，揭示了其在实际生产环境中的潜力。通过大规模的实证研究和开发者调查，增强了研究结果的可信度和实用性。特别是对不同RAG类型和检索技术的详细比较，为未来的代码补全系统设计提供了宝贵的经验。"}}
{"id": "2507.17248", "title": "Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations", "authors": ["Xiaoan Liu", "Difan Jia", "Xianhao Carton Liu", "Mar Gonzalez-Franco", "Chen Zhu-Tian"], "categories": ["cs.HC", "cs.AI", "cs.GR", "H.5.2; I.3.6"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th Annual ACM Symposium on User Interface Software and Technology), Busan, Republic of Korea, 28 Sep - 1 Oct 2025", "url": "http://arxiv.org/abs/2507.17248v2", "summary": "Interacting with real-world objects in Mixed Reality (MR) often proves\ndifficult when they are crowded, distant, or partially occluded, hindering\nstraightforward selection and manipulation. We observe that these difficulties\nstem from performing interaction directly on physical objects, where input is\ntightly coupled to their physical constraints. Our key insight is to decouple\ninteraction from these constraints by introducing proxies-abstract\nrepresentations of real-world objects. We embody this concept in Reality Proxy,\na system that seamlessly shifts interaction targets from physical objects to\ntheir proxies during selection. Beyond facilitating basic selection, Reality\nProxy uses AI to enrich proxies with semantic attributes and hierarchical\nspatial relationships of their corresponding physical objects, enabling novel\nand previously cumbersome interactions in MR - such as skimming,\nattribute-based filtering, navigating nested groups, and complex multi object\nselections - all without requiring new gestures or menu systems. We demonstrate\nReality Proxy's versatility across diverse scenarios, including office\ninformation retrieval, large-scale spatial navigation, and multi-drone control.\nAn expert evaluation suggests the system's utility and usability, suggesting\nthat proxy-based abstractions offer a powerful and generalizable interaction\nparadigm for future MR systems.", "comment": "16 pages, 9 figures. Accepted for publication in UIST'25 (The 38th\n  Annual ACM Symposium on User Interface Software and Technology), Busan,\n  Republic of Korea, 28 Sep - 1 Oct 2025", "pdf_url": "http://arxiv.org/pdf/2507.17248v2", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "Reality Proxy：通过抽象表示在MR中与真实世界对象进行流畅交互", "tldr": "在混合现实（MR）中，与拥挤、遥远或被遮挡的真实世界物体交互困难。本文提出Reality Proxy系统，通过引入真实物体的抽象代理来解耦交互与物理约束，利用AI增强代理信息，实现流畅且高级的MR交互，无需新姿势或菜单。", "motivation": "在混合现实（MR）中，当真实世界物体拥挤、遥远或部分遮挡时，直接与之交互（进行选择和操作）会变得困难，因为输入与物理约束紧密耦合。", "method": "本文提出通过引入“代理”（即真实世界对象的抽象表示）来解耦交互与物理约束。Reality Proxy系统在选择过程中将交互目标从物理对象无缝转移到其代理。该系统利用AI技术丰富代理的语义属性及其对应物理对象的分层空间关系，从而在MR中实现新颖且以前繁琐的交互，例如浏览、基于属性的过滤、导航嵌套组以及复杂的多个对象选择，所有这些都无需新的手势或菜单系统。", "result": "Reality Proxy系统在办公信息检索、大规模空间导航和多无人机控制等多种场景中展示了其多功能性。专家评估表明该系统具有实用性和可用性。", "conclusion": "基于代理的抽象为未来的混合现实（MR）系统提供了一种强大且可泛化的交互范式。", "translation": "在混合现实（MR）中，当真实世界物体拥挤、遥远或部分遮挡时，与之交互常常会变得困难，阻碍了直接的选择和操作。我们观察到这些困难源于直接在物理对象上进行交互，其中输入与它们的物理约束紧密耦合。我们的关键见解是通过引入代理——真实世界对象的抽象表示——来解耦交互与这些约束。我们将这一概念体现在Reality Proxy中，这是一个在选择过程中将交互目标从物理对象无缝转移到其代理的系统。除了促进基本选择之外，Reality Proxy还利用AI丰富代理的语义属性及其对应物理对象的分层空间关系，从而在MR中实现新颖且以前繁琐的交互——例如浏览、基于属性的过滤、导航嵌套组以及复杂的多个对象选择——所有这些都无需新的手势或菜单系统。我们展示了Reality Proxy在各种场景中的多功能性，包括办公信息检索、大规模空间导航和多无人机控制。专家评估表明该系统的实用性和可用性，表明基于代理的抽象为未来的MR系统提供了一种强大且可泛化的交互范式。", "summary": "本文提出Reality Proxy系统，旨在解决混合现实（MR）中与拥挤、遥远或遮挡的真实世界物体交互困难的问题。核心思想是通过引入真实物体的抽象表示（代理）来解耦交互与物理约束。Reality Proxy系统能够在选择时将交互目标从物理对象无缝切换到其代理，并利用AI技术丰富代理的语义属性和分层空间关系。这使得在MR中能够实现新颖且高效的交互操作，如内容浏览、基于属性过滤、嵌套组导航和复杂多对象选择，且无需引入新的手势或菜单系统。该系统在办公信息检索、大规模空间导航和多无人机控制等多样化场景中展示了其通用性，并通过专家评估验证了其实用性和可用性，表明基于代理的抽象为未来MR系统提供了一种强大且可泛化的交互范式。", "keywords": "混合现实, 交互, 代理, 抽象表示, AI", "comments": "该论文创新性地提出“代理”概念，通过解耦交互与物理约束，有效解决了MR中与复杂真实物体交互的痛点。其利用AI增强代理的语义和空间信息，在不增加用户负担（无需新手势或菜单）的情况下实现了高级交互，具有较强的实用性和通用性。"}}
{"id": "2503.05245", "title": "L-FUSION: Laplacian Fetal Ultrasound Segmentation & Uncertainty Estimation", "authors": ["Johanna P. Müller", "Robert Wright", "Thomas G. Day", "Lorenzo Venturini", "Samuel F. Budd", "Hadrien Reynaud", "Joseph V. Hajnal", "Reza Razavi", "Bernhard Kainz"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI ASMUS 2025", "url": "http://arxiv.org/abs/2503.05245v3", "summary": "Accurate analysis of prenatal ultrasound (US) is essential for early\ndetection of developmental anomalies. However, operator dependency and\ntechnical limitations (e.g. intrinsic artefacts and effects, setting errors)\ncan complicate image interpretation and the assessment of diagnostic\nuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation with\nIntegrated FoundatiON models), a framework that integrates uncertainty\nquantification through unsupervised, normative learning and large-scale\nfoundation models for robust segmentation of fetal structures in normal and\npathological scans. We propose to utilise the aleatoric logit distributions of\nStochastic Segmentation Networks and Laplace approximations with fast Hessian\nestimations to estimate epistemic uncertainty only from the segmentation head.\nThis enables us to achieve reliable abnormality quantification for instant\ndiagnostic feedback. Combined with an integrated Dropout component, L-FUSION\nenables reliable differentiation of lesions from normal fetal anatomy with\nenhanced uncertainty maps and segmentation counterfactuals in US imaging. It\nimproves epistemic and aleatoric uncertainty interpretation and removes the\nneed for manual disease-labelling. Evaluations across multiple datasets show\nthat L-FUSION achieves superior segmentation accuracy and consistent\nuncertainty quantification, supporting on-site decision-making and offering a\nscalable solution for advancing fetal ultrasound analysis in clinical settings.", "comment": "Accepted at MICCAI ASMUS 2025", "pdf_url": "http://arxiv.org/pdf/2503.05245v3", "cate": "eess.IV", "date": "2025-03-07", "updated": "2025-07-24", "AI": {"title_translation": "L-FUSION：拉普拉斯胎儿超声分割与不确定性估计", "tldr": "L-FUSION是一个结合无监督学习和基础模型框架，用于在胎儿超声图像中实现鲁棒分割和不确定性估计，有助于早期发现发育异常。", "motivation": "产前超声的准确分析对于早期发现发育异常至关重要，但操作员依赖性和技术限制（如固有伪影、设置错误）使图像解释和诊断不确定性评估复杂化。", "method": "本文提出了L-FUSION框架，它通过无监督、规范学习和大型基础模型整合不确定性量化，以实现正常和病理扫描中胎儿结构的鲁棒分割。它利用随机分割网络的偶然逻辑分布和带有快速Hessian估计的拉普拉斯近似，仅从分割头部估计认知不确定性，并结合集成的Dropout组件，增强不确定性图和分割反事实。", "result": "L-FUSION在多个数据集上显示出卓越的分割精度和一致的不确定性量化，支持现场决策，并提供可扩展的解决方案。它实现了可靠的异常量化，并能可靠地区分病变与正常胎儿解剖结构，改进了认知和偶然不确定性解释，并消除了手动疾病标记的需要。", "conclusion": "L-FUSION提供了一个可扩展的解决方案，通过结合不确定性量化和鲁棒分割，显著提升了胎儿超声分析的临床应用能力，有助于早期诊断和决策支持。", "translation": "对产前超声（US）进行准确分析对于早期发现发育异常至关重要。然而，操作员依赖性和技术限制（例如固有伪影和效应、设置错误）可能会使图像解释和诊断不确定性的评估变得复杂。我们提出了L-FUSION（拉普拉斯胎儿超声分割与集成基础模型），这是一个通过无监督、规范学习和大型基础模型整合不确定性量化的框架，用于在正常和病理扫描中对胎儿结构进行鲁棒分割。我们建议利用随机分割网络的偶然逻辑分布和带有快速Hessian估计的拉普拉斯近似，仅从分割头部估计认知不确定性。这使我们能够实现可靠的异常量化，从而提供即时诊断反馈。结合集成的Dropout组件，L-FUSION能够在超声成像中通过增强的不确定性图和分割反事实，可靠地区分病变与正常胎儿解剖结构。它改善了认知和偶然不确定性解释，并消除了手动疾病标记的需要。在多个数据集上的评估表明，L-FUSION实现了卓越的分割精度和一致的不确定性量化，支持现场决策，并为推进临床环境中的胎儿超声分析提供了可扩展的解决方案。", "summary": "L-FUSION是一个创新的框架，旨在解决胎儿超声图像分析中的挑战，特别是操作员依赖性和不确定性评估。该方法结合了无监督的规范学习和大型基础模型，以实现对胎儿结构的鲁棒分割，同时量化并可视化不确定性。通过利用随机分割网络和拉普拉斯近似，L-FUSION能有效估计认知和偶然不确定性，从而提供可靠的异常量化和即时诊断反馈。该系统在多个数据集上表现出卓越的分割精度和一致的不确定性量化，为临床胎儿超声分析提供了可扩展的解决方案。", "keywords": "胎儿超声, 分割, 不确定性估计, L-FUSION, 深度学习", "comments": "L-FUSION的创新之处在于其将不确定性量化与鲁棒分割相结合，解决了胎儿超声诊断中长期存在的挑战。通过整合无监督学习和基础模型，该框架提高了诊断的可靠性和效率，尤其是在减少对人工标记的依赖方面。这对于临床实践具有重要意义，有助于实现更早、更准确的异常检测。"}}
{"id": "2507.18566", "title": "Facial Demorphing from a Single Morph Using a Latent Conditional GAN", "authors": ["Nitish Shukla", "Arun Ross"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18566v1", "summary": "A morph is created by combining two (or more) face images from two (or more)\nidentities to create a composite image that is highly similar to both\nconstituent identities, allowing the forged morph to be biometrically\nassociated with more than one individual. Morph Attack Detection (MAD) can be\nused to detect a morph, but does not reveal the constituent images. Demorphing\n- the process of deducing the constituent images - is thus vital to provide\nadditional evidence about a morph. Existing demorphing methods suffer from the\nmorph replication problem, where the outputs tend to look very similar to the\nmorph itself, or assume that train and test morphs are generated using the same\nmorph technique. The proposed method overcomes these issues. The method\ndecomposes a morph in latent space allowing it to demorph images created from\nunseen morph techniques and face styles. We train our method on morphs created\nfrom synthetic faces and test on morphs created from real faces using arbitrary\nmorph techniques. Our method outperforms existing methods by a considerable\nmargin and produces high fidelity demorphed face images.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18566v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "从单个融合图像中进行面部去融合：基于潜在条件GAN的方法", "tldr": "本文提出了一种基于潜在条件GAN的新方法，可以从单个融合图像中恢复原始人脸图像，解决了现有去融合方法中输出与融合图像过于相似以及无法处理未知融合技术的问题。", "motivation": "融合图像（morph）通过结合多个人脸图像创建，使其能与多个身份进行生物识别关联，对生物识别系统构成安全威胁。尽管融合攻击检测（MAD）可以发现融合图像，但无法揭示其组成图像。去融合（demorphing）是推断组成图像的过程，对于提供额外证据至关重要。现有去融合方法存在输出图像与融合图像过于相似的问题，或假设训练和测试的融合图像使用相同的技术生成。", "method": "本文提出的方法通过在潜在空间中分解融合图像来克服现有问题。该方法使用潜在条件生成对抗网络（latent Conditional GAN）进行训练，使其能够对由未知融合技术和人脸风格创建的图像进行去融合。该方法在合成人脸创建的融合图像上进行训练，并在使用任意融合技术从真人脸创建的融合图像上进行测试。", "result": "所提出的方法在性能上显著优于现有方法，并能生成高保真度的去融合人脸图像。", "conclusion": "该研究提出了一种有效的面部去融合方法，能够从单个融合图像中恢复高质量的原始人脸图像，并克服了现有技术在处理未知融合技术和避免融合复制问题上的局限性，从而增强了对生物识别融合攻击的抵御能力。", "translation": "融合图像是通过结合两个（或更多）身份的两个（或更多）人脸图像来创建的复合图像，该图像与两个（或更多）组成身份高度相似，从而使伪造的融合图像可以与多个个体进行生物识别关联。融合攻击检测（MAD）可用于检测融合图像，但不能揭示组成图像。因此，去融合——推断组成图像的过程——对于提供关于融合图像的额外证据至关重要。现有去融合方法存在融合复制问题，即输出往往与融合图像本身非常相似，或者假设训练和测试融合图像使用相同的融合技术生成。所提出的方法克服了这些问题。该方法在潜在空间中分解融合图像，使其能够对由未知融合技术和人脸风格创建的图像进行去融合。我们在合成人脸创建的融合图像上训练我们的方法，并在使用任意融合技术从真人脸创建的融合图像上进行测试。我们的方法在性能上显著优于现有方法，并生成高保真度的去融合人脸图像。", "summary": "本文提出了一种基于潜在条件GAN的新型面部去融合方法，旨在从单个融合图像中恢复其组成人脸图像。该方法通过在潜在空间中分解融合图像，有效解决了现有技术中输出与融合图像相似度高及无法处理未知融合技术的问题。实验结果表明，该方法在处理由任意融合技术生成的真实人脸融合图像时，性能显著优于现有方法，并能生成高保真度的去融合图像，为生物识别安全提供了重要支持。", "keywords": "面部去融合, 潜在条件GAN, 融合图像, 生物识别安全, 图像恢复", "comments": "该论文的创新点在于利用潜在条件GAN在潜在空间中对融合图像进行分解，使得模型能够有效处理由未知融合技术和人脸风格生成的融合图像，这是现有方法难以实现的能力。这种泛化能力对于实际应用中的生物识别安全至关重要，因为融合攻击的方式可能多样且不断演变。该方法显著提升了去融合图像的质量和对未知场景的适应性，具有重要的实际意义。"}}
{"id": "2507.12006", "title": "Frequency-Dynamic Attention Modulation for Dense Prediction", "authors": ["Linwei Chen", "Lin Gu", "Ying Fu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.12006v3", "summary": "Vision Transformers (ViTs) have significantly advanced computer vision,\ndemonstrating strong performance across various tasks. However, the attention\nmechanism in ViTs makes each layer function as a low-pass filter, and the\nstacked-layer architecture in existing transformers suffers from frequency\nvanishing. This leads to the loss of critical details and textures. We propose\na novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention\nModulation (FDAM), which can be easily plugged into ViTs. FDAM directly\nmodulates the overall frequency response of ViTs and consists of two\ntechniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling\n(FreqScale). Since circuit theory uses low-pass filters as fundamental\nelements, we introduce AttInv, a method that generates complementary high-pass\nfiltering by inverting the low-pass filter in the attention matrix, and\ndynamically combining the two. We further design FreqScale to weight different\nfrequency components for fine-grained adjustments to the target response\nfunction. Through feature similarity analysis and effective rank evaluation, we\ndemonstrate that our approach avoids representation collapse, leading to\nconsistent performance improvements across various models, including SegFormer,\nDeiT, and MaskDINO. These improvements are evident in tasks such as semantic\nsegmentation, object detection, and instance segmentation. Additionally, we\napply our method to remote sensing detection, achieving state-of-the-art\nresults in single-scale settings. The code is available at\nhttps://github.com/Linwei-Chen/FDAM.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.12006v3", "cate": "cs.CV", "date": "2025-07-16", "updated": "2025-07-24", "AI": {"title_translation": "密集预测的频率动态注意力调制", "tldr": "提出频率动态注意力调制(FDAM)来解决ViTs中注意力机制导致的频率衰减问题，通过引入高通滤波和动态频率权重，显著提升了ViTs在密集预测任务上的表现，并避免了表征坍塌。", "motivation": "现有Vision Transformers (ViTs)的注意力机制使每层充当低通滤波器，导致堆叠层架构出现频率衰减，进而丢失关键细节和纹理。", "method": "提出了一种受电路理论启发的频率动态注意力调制（FDAM）策略，可轻松集成到ViTs中。FDAM包含两种技术：注意力反转（AttInv）和频率动态缩放（FreqScale）。AttInv通过反转注意力矩阵中的低通滤波器生成互补的高通滤波，并动态结合两者。FreqScale用于加权不同频率分量，以对目标响应函数进行细粒度调整。", "result": "该方法通过特征相似性分析和有效秩评估，避免了表征坍塌，并在SegFormer、DeiT和MaskDINO等多种模型上实现了持续的性能改进。这些改进在语义分割、目标检测和实例分割等任务中表现明显。此外，在遥感检测中也取得了单尺度设置下的最先进结果。", "conclusion": "该研究提出了一种新颖的频率动态注意力调制方法FDAM，有效解决了ViTs中的频率衰减问题，并通过引入高通滤波和动态频率权重，显著提升了ViTs在各类密集预测任务中的性能和表征能力。", "translation": "视觉Transformer（ViT）极大地推动了计算机视觉的发展，在各种任务中展现出强大的性能。然而，ViT中的注意力机制使每一层都充当低通滤波器，现有Transformer中的堆叠层架构存在频率衰减问题。这导致关键细节和纹理的丢失。我们提出了一种新颖的、受电路理论启发的策略，称为频率动态注意力调制（FDAM），它可以很容易地插入到ViT中。FDAM直接调制ViT的整体频率响应，并包含两种技术：注意力反转（AttInv）和频率动态缩放（FreqScale）。由于电路理论使用低通滤波器作为基本元件，我们引入了AttInv，一种通过反转注意力矩阵中的低通滤波器来生成互补高通滤波，并动态结合两者的方。我们进一步设计了FreqScale来加权不同的频率分量，以对目标响应函数进行细粒度调整。通过特征相似性分析和有效秩评估，我们证明了我们的方法避免了表征坍塌，从而在包括SegFormer、DeiT和MaskDINO在内的各种模型上实现了持续的性能改进。这些改进在语义分割、目标检测和实例分割等任务中表现明显。此外，我们将我们的方法应用于遥感检测，在单尺度设置中取得了最先进的结果。代码可在https://github.com/Linwei-Chen/FDAM获取。", "summary": "本文提出了一种名为频率动态注意力调制（FDAM）的新型策略，旨在解决Vision Transformers（ViTs）中因注意力机制导致的频率衰减问题及其造成关键细节丢失的弊端。FDAM受电路理论启发，通过注意力反转（AttInv）引入高通滤波并动态结合，以及频率动态缩放（FreqScale）进行细粒度频率加权，直接调制ViTs的频率响应。实验结果表明，FDAM有效避免了表征坍塌，并在多种模型（如SegFormer、DeiT、MaskDINO）和任务（如语义分割、目标检测、实例分割、遥感检测）上实现了显著且一致的性能提升，尤其在遥感检测中达到了最先进水平。", "keywords": "视觉Transformer, 频率衰减, 注意力机制, 密集预测, 频率动态注意力调制", "comments": "该论文的创新点在于将电路理论中的频率响应概念引入到ViTs的注意力机制中，通过设计高通滤波和动态频率权重来解决现有ViTs的频率衰减问题。这种跨领域的启发式方法非常新颖，并有效提升了模型在密集预测任务中的细节捕捉能力和整体性能，同时避免了表征坍塌，具有重要的理论和实践价值。"}}
{"id": "2507.18015", "title": "Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics", "authors": ["Yuezun Li", "Delong Zhu", "Xinjie Cui", "Siwei Lyu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2507.18015v1", "summary": "The rapid advancement of AI technologies has significantly increased the\ndiversity of DeepFake videos circulating online, posing a pressing challenge\nfor \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen\nDeepFake types using a single model. Addressing this challenge requires\ndatasets that are not only large-scale but also rich in forgery diversity.\nHowever, most existing datasets, despite their scale, include only a limited\nvariety of forgery types, making them insufficient for developing generalizable\ndetection methods. Therefore, we build upon our earlier Celeb-DF dataset and\nintroduce {Celeb-DF++}, a new large-scale and challenging video DeepFake\nbenchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers\nthree commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment\n(FR), and Talking-face (TF). Each scenario contains a substantial number of\nhigh-quality forged videos, generated using a total of 22 various recent\nDeepFake methods. These methods differ in terms of architectures, generation\npipelines, and targeted facial regions, covering the most prevalent DeepFake\ncases witnessed in the wild. We also introduce evaluation protocols for\nmeasuring the generalizability of 24 recent detection methods, highlighting the\nlimitations of existing detection methods and the difficulty of our new\ndataset.", "comment": "https://github.com/OUC-VAS/Celeb-DF-PP", "pdf_url": "http://arxiv.org/pdf/2507.18015v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Celeb-DF++: 一个用于泛化取证的大规模挑战性视频深度伪造基准", "tldr": "引入了一个名为 Celeb-DF++ 的大规模深度伪造视频基准数据集，旨在解决深度伪造检测中的泛化性挑战，并评估了现有方法的局限性。", "motivation": "AI技术的快速发展导致在线深度伪造视频的多样性显著增加，对“泛化取证”（即使用单一模型检测广泛的、未见的深度伪造类型）构成了紧迫挑战。现有数据集尽管规模大，但伪造类型有限，不足以开发泛化检测方法。", "method": "作者在先前的 Celeb-DF 数据集基础上，构建了 Celeb-DF++。该数据集涵盖了三种常见的伪造场景：换脸 (FS)、面部重演 (FR) 和说话脸 (TF)。每个场景都包含大量高质量的伪造视频，这些视频是使用总共22种不同的最新深度伪造方法生成的。这些方法在架构、生成流程和目标面部区域上有所不同。同时，作者还引入了评估协议，用于衡量24种最新检测方法的泛化能力。", "result": "Celeb-DF++ 包含了使用22种不同深度伪造方法生成的大量高质量伪造视频，涵盖了三种常见伪造场景。通过引入评估协议并衡量24种最新检测方法的泛化能力，结果突出了现有检测方法的局限性以及新数据集的难度。", "conclusion": "该论文构建并发布了 Celeb-DF++，这是一个大规模且具有挑战性的视频深度伪造基准，旨在推动泛化取证领域的研究。通过评估现有方法，证明了当前检测方法在泛化性方面的不足，并强调了新数据集的挑战性。", "translation": "AI技术的快速发展显著增加了在线传播的深度伪造视频的多样性，这给“泛化取证”（即使用单一模型检测广泛的、未见的深度伪造类型）带来了紧迫的挑战。解决这一挑战需要不仅规模大而且伪造多样性丰富的 数据集。然而，大多数现有数据集，尽管规模大，但只包含有限的伪造类型，这使得它们不足以开发泛化检测方法。因此，我们在早期的 Celeb-DF 数据集的基础上，引入了 {Celeb-DF++}，这是一个专门用于泛化取证挑战的新的大规模、高挑战性视频深度伪造基准。Celeb-DF++ 涵盖了三种常见的伪造场景：换脸 (FS)、面部重演 (FR) 和说话脸 (TF)。每个场景都包含大量高质量的伪造视频，这些视频是使用总共22种各种最新的深度伪造方法生成的。这些方法在架构、生成流程和目标面部区域上有所不同，涵盖了野外最普遍的深度伪造案例。我们还引入了评估协议，用于衡量24种最新检测方法的泛化能力，突出了现有检测方法的局限性以及我们新数据集的难度。", "summary": "本文介绍了 Celeb-DF++，这是一个旨在解决深度伪造检测中泛化性挑战的大规模视频基准数据集。为了应对现有数据集缺乏伪造多样性的问题，Celeb-DF++ 在原有 Celeb-DF 的基础上，整合了22种不同的深度伪造方法，生成了涵盖换脸、面部重演和说话脸三种常见场景的大量高质量伪造视频。该数据集旨在促进开发能够检测未知深度伪造类型的方法。论文还提供了评估协议，并以此衡量了24种现有检测方法的泛化能力，揭示了当前方法的局限性和数据集的挑战性。", "keywords": "深度伪造, 泛化取证, 视频数据集, Celeb-DF++, 人脸伪造", "comments": "Celeb-DF++ 的创新之处在于其大规模和高度多样化的伪造类型，这直接解决了当前深度伪造检测领域中泛化能力不足的关键挑战。通过包含22种不同的生成方法和三种主流伪造场景，它显著提升了数据集的复杂性和实用性，为训练更鲁棒、更具泛化性的检测模型提供了宝贵的资源。其重要性在于它为研究人员提供了一个标准化的、更贴近实际的基准，有助于推动深度伪造取证技术的发展。"}}
{"id": "2507.18192", "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance", "authors": ["Minghao Fu", "Guo-Hua Wang", "Xiaohao Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. The code is publicly available at this https URL", "url": "http://arxiv.org/abs/2507.18192v1", "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt\n\\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6$\\times$ faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\n\\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.", "comment": "Accepted by ICCV 2025. The code is publicly available at\n  https://github.com/AIDC-AI/TeEFusion", "pdf_url": "http://arxiv.org/pdf/2507.18192v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TeEFusion：融合文本嵌入以蒸馏无分类器引导", "tldr": "TeEFusion是一种高效的蒸馏方法，通过融合文本嵌入来模拟无分类器引导，使文本到图像生成速度提高6倍，同时保持图像质量。", "motivation": "现有的文本到图像合成方法中，无分类器引导（CFG）虽然能确保高质量生成，但其依赖两次前向传播，导致推理成本过高，尤其与复杂采样算法结合时。", "method": "TeEFusion（文本嵌入融合）是一种新颖高效的蒸馏方法，它通过线性操作融合条件和无条件文本嵌入，将引导强度直接融入文本嵌入中，并在不增加额外参数的情况下重建所需引导。同时，它使学生模型能够学习教师模型通过复杂采样方法产生的输出。", "result": "在SD3等先进模型上的大量实验表明，TeEFusion使学生模型能够以更简单、更高效的采样策略，紧密模仿教师模型的性能。学生模型推理速度比教师模型快6倍，同时保持与教师模型复杂采样方法相当的图像质量。", "conclusion": "TeEFusion通过高效的蒸馏方法显著降低了文本到图像合成的推理成本，实现了在保持高质量图像生成的同时，大幅提升了生成速度。", "translation": "文本到图像合成的最新进展在很大程度上得益于复杂的采样策略和无分类器引导（CFG），以确保高质量的生成。然而，CFG对两次前向传播的依赖，特别是当与复杂的采样算法结合时，导致推理成本高得令人望而却步。为了解决这个问题，我们引入了TeEFusion（文本嵌入融合），这是一种新颖高效的蒸馏方法，它直接将引导强度纳入文本嵌入中，并蒸馏教师模型复杂的采样策略。通过简单地使用线性操作融合条件和无条件文本嵌入，TeEFusion在不增加额外参数的情况下重建所需的引导，同时使学生模型能够从教师模型通过其复杂采样方法产生的输出中学习。在SD3等最先进模型上的大量实验表明，我们的方法允许学生模型以更简单、更高效的采样策略，紧密模仿教师模型的性能。因此，学生模型的推理速度比教师模型快6倍，同时保持与教师模型复杂采样方法相当的图像质量。代码已在github.com/AIDC-AI/TeEFusion公开。", "summary": "TeEFusion是一种创新的蒸馏方法，旨在解决文本到图像合成中无分类器引导（CFG）带来的高推理成本问题。它通过线性融合条件和无条件文本嵌入，直接将引导强度融入文本嵌入，从而在不增加参数的情况下，使学生模型能够学习教师模型的复杂采样策略。实验证明，TeEFusion使得学生模型能在保持图像质量的同时，实现高达6倍的推理速度提升。", "keywords": "文本到图像合成, 无分类器引导, 蒸馏, 文本嵌入, 推理加速", "comments": "TeEFusion的创新之处在于其将无分类器引导的强度直接融入文本嵌入的巧妙方法，并结合蒸馏技术，有效解决了CFG带来的高计算成本问题。其在保持图像质量的同时实现了显著的推理速度提升（高达6倍），这对于实际应用和大规模部署文本到图像生成模型具有重要意义。该方法提供了一种高效的替代方案，可以在资源受限的环境中实现高性能的文本到图像合成。"}}
{"id": "2405.06995", "title": "Benchmarking Cross-Domain Audio-Visual Deception Detection", "authors": ["Xiaobao Guo", "Zitong Yu", "Nithish Muthuchamy Selvaraj", "Bingquan Shen", "Adams Wai-Kin Kong", "Alex C. Kot"], "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2405.06995v3", "summary": "Automated deception detection is crucial for assisting humans in accurately\nassessing truthfulness and identifying deceptive behavior. Conventional\ncontact-based techniques, like polygraph devices, rely on physiological signals\nto determine the authenticity of an individual's statements. Nevertheless,\nrecent developments in automated deception detection have demonstrated that\nmultimodal features derived from both audio and video modalities may outperform\nhuman observers on publicly available datasets. Despite these positive\nfindings, the generalizability of existing audio-visual deception detection\napproaches across different scenarios remains largely unexplored. To close this\ngap, we present the first cross-domain audio-visual deception detection\nbenchmark, that enables us to assess how well these methods generalize for use\nin real-world scenarios. We used widely adopted audio and visual features and\ndifferent architectures for benchmarking, comparing single-to-single and\nmulti-to-single domain generalization performance. To further exploit the\nimpacts using data from multiple source domains for training, we investigate\nthree types of domain sampling strategies, including domain-simultaneous,\ndomain-alternating, and domain-by-domain for multi-to-single domain\ngeneralization evaluation. We also propose an algorithm to enhance the\ngeneralization performance by maximizing the gradient inner products between\nmodality encoders, named ``MM-IDGM\". Furthermore, we proposed the\nAttention-Mixer fusion method to improve performance, and we believe that this\nnew cross-domain benchmark will facilitate future research in audio-visual\ndeception detection.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2405.06995v3", "cate": "cs.SD", "date": "2024-05-11", "updated": "2025-07-24", "AI": {"title_translation": "跨领域视听欺骗检测基准测试", "tldr": "提出了首个跨领域视听欺骗检测基准，并引入新算法以提高泛化性能，旨在解决现有方法泛化能力不足的问题。", "motivation": "自动化欺骗检测对评估真实性和识别欺骗行为至关重要。尽管多模态视听特征在公开数据集上表现优于人类，但现有方法在不同场景下的泛化能力尚未得到充分探索，这是一个亟待解决的空白。", "method": "建立了首个跨领域视听欺骗检测基准。使用了广泛采用的音频和视觉特征以及不同的架构进行基准测试。比较了单域到单域和多域到单域的泛化性能。研究了三种域采样策略（域同时、域交替、域逐域）以利用多源域数据进行训练。提出了一种名为“MM-IDGM”的算法，通过最大化模态编码器之间的梯度内积来增强泛化性能。提出Attention-Mixer融合方法以提高性能。", "result": "提出了首个跨领域视听欺骗检测基准，该基准能够评估现有方法的泛化能力。同时，提出了MM-IDGM算法和Attention-Mixer融合方法，旨在提高跨领域泛化性能。具体的性能提升数据未在摘要中提及。", "conclusion": "该研究提出的新跨领域基准有望促进未来在视听欺骗检测领域的研究。", "translation": "自动化欺骗检测对于协助人类准确评估真实性和识别欺骗行为至关重要。传统的接触式技术，如测谎仪，依赖生理信号来判断个人陈述的真实性。然而，自动化欺骗检测的最新发展表明，源自音频和视频模态的多模态特征在公开数据集上可能优于人类观察者。尽管取得了这些积极的发现，现有视听欺骗检测方法在不同场景下的泛化能力在很大程度上仍未被探索。为了弥补这一空白，我们提出了第一个跨领域视听欺骗检测基准，使我们能够评估这些方法在真实世界场景中的泛化能力。我们使用广泛采用的音频和视觉特征以及不同的架构进行基准测试，比较了单域到单域和多域到单域的泛化性能。为了进一步探索使用来自多个源域的数据进行训练的影响，我们研究了三种域采样策略，包括域同时、域交替和域逐域，用于多域到单域的泛化评估。我们还提出了一种通过最大化模态编码器之间的梯度内积来增强泛化性能的算法，名为“MM-IDGM”。此外，我们提出了Attention-Mixer融合方法来提高性能，我们相信这个新的跨领域基准将促进未来在视听欺骗检测领域的研究。", "summary": "本文提出了首个跨领域视听欺骗检测基准，旨在解决现有自动化欺骗检测方法在不同场景下泛化能力不足的问题。研究使用了广泛的音频和视觉特征及多种架构进行基准测试，并比较了单域到单域及多域到单域的泛化性能。为优化多源域训练效果，论文探讨了三种域采样策略，并创新性地提出了MM-IDGM算法和Attention-Mixer融合方法以提升泛化表现。该基准有望推动未来视听欺骗检测领域的研究。", "keywords": "欺骗检测, 跨领域, 视听, 泛化, 基准测试", "comments": "本文的创新点在于构建了首个跨领域视听欺骗检测基准，填补了现有研究在泛化能力评估方面的空白。同时，提出的MM-IDGM算法和Attention-Mixer融合方法为提升跨领域泛化性能提供了新的思路。该基准对于推动视听欺骗检测技术在真实世界场景中的应用具有重要意义。"}}
{"id": "2212.10678", "title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias", "authors": ["Yuen Chen", "Vethavikashini Chithrra Raghuram", "Justus Mattern", "Rada Mihalcea", "Zhijing Jin"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2212.10678v4", "summary": "Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2212.10678v4", "cate": "cs.CL", "date": "2022-12-20", "updated": "2025-07-24", "AI": {"title_translation": "因果性测试大型语言模型中的性别偏见：一项职业偏见的案例研究", "tldr": "本文提出了一种测量生成式语言模型中偏见的因果公式，并基于此设计了一个名为OccuGender的基准来调查职业性别偏见。实验结果表明，主流LLMs存在显著的职业性别偏见。", "motivation": "大型语言模型（LLMs）生成的文本已显示出对不同人群存在各种有害的、类人的偏见。这些发现促使研究人员努力理解和衡量此类影响。", "method": "本文引入了一种用于生成式语言模型偏见测量的因果公式，并基于此理论基础，提出了一系列设计鲁棒偏见基准的必要条件。随后，提出了一种名为OccuGender的基准，并采用偏见测量程序来调查职业性别偏见。研究人员使用OccuGender测试了包括Llama、Mistral及其指令调优版本在内的几种最先进的开源LLMs。", "result": "结果显示，这些模型表现出显著的职业性别偏见。", "conclusion": "本文讨论了偏见缓解的提示策略，并扩展了因果公式以说明框架的通用性。", "translation": "大型语言模型（LLMs）生成的文本已显示出对不同人群存在各种有害的、类人的偏见。这些发现促使研究人员努力理解和衡量此类影响。本文引入了一种用于生成式语言模型偏见测量的因果公式。基于这一理论基础，我们列出了一系列设计鲁棒偏见基准的必要条件。然后，我们提出了一种名为OccuGender的基准，并采用偏见测量程序来调查职业性别偏见。我们使用OccuGender测试了几种最先进的开源LLMs，包括Llama、Mistral及其指令调优版本。结果显示，这些模型表现出显著的职业性别偏见。最后，我们讨论了偏见缓解的提示策略，并扩展了我们的因果公式以说明我们框架的通用性。我们的代码和数据可在https://github.com/chenyuen0103/gender-bias 获取。", "summary": "该研究提出了一种新的因果公式来测量大型语言模型中的偏见，并基于此设计了一个名为OccuGender的基准，专注于职业性别偏见。通过对Llama和Mistral等主流LLMs进行测试，发现它们普遍存在显著的职业性别偏见。论文还探讨了偏见缓解策略和框架的通用性。", "keywords": "性别偏见, 大型语言模型, 因果测试, 职业偏见, OccuGender", "comments": "本文的创新之处在于提出了一个测量LLMs偏见的因果框架，并基于此开发了一个针对职业性别偏见的具体基准OccuGender。这为理解和量化LLMs中的偏见提供了一个理论基础和实用的工具，对于推动LLMs的公平性研究具有重要意义。"}}
{"id": "2507.17311", "title": "EarthLink: A Self-Evolving AI Agent for Climate Science", "authors": ["Zijie Guo", "Jiong Wang", "Xiaoyu Yue", "Wangxu Wei", "Zhe Jiang", "Wanghan Xu", "Ben Fei", "Wenlong Zhang", "Xinyu Gu", "Lijing Cheng", "Jing-Jia Luo", "Chao Li", "Yaqiang Wang", "Tao Chen", "Wanli Ouyang", "Fenghua Ling", "Lei Bai"], "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17311v2", "summary": "Modern Earth science is at an inflection point. The vast, fragmented, and\ncomplex nature of Earth system data, coupled with increasingly sophisticated\nanalytical demands, creates a significant bottleneck for rapid scientific\ndiscovery. Here we introduce EarthLink, the first AI agent designed as an\ninteractive copilot for Earth scientists. It automates the end-to-end research\nworkflow, from planning and code generation to multi-scenario analysis. Unlike\nstatic diagnostic tools, EarthLink can learn from user interaction,\ncontinuously refining its capabilities through a dynamic feedback loop. We\nvalidated its performance on a number of core scientific tasks of climate\nchange, ranging from model-observation comparisons to the diagnosis of complex\nphenomena. In a multi-expert evaluation, EarthLink produced scientifically\nsound analyses and demonstrated an analytical competency that was rated as\ncomparable to specific aspects of a human junior researcher's workflow.\nAdditionally, its transparent, auditable workflows and natural language\ninterface empower scientists to shift from laborious manual execution to\nstrategic oversight and hypothesis generation. EarthLink marks a pivotal step\ntowards an efficient, trustworthy, and collaborative paradigm for Earth system\nresearch in an era of accelerating global change. The system is accessible at\nour website https://earthlink.intern-ai.org.cn.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17311v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "EarthLink：一个用于气候科学的自进化AI代理", "tldr": "EarthLink是一个用于地球科学的AI代理，它能自动化研究工作流程，通过用户交互进行学习和改进，并在气候科学任务中表现出与人类初级研究员相当的分析能力。", "motivation": "现代地球科学面临大量、碎片化、复杂的数据以及日益复杂的分析需求，这严重阻碍了快速科学发现。", "method": "本文介绍了EarthLink，它是一个交互式AI代理，旨在作为地球科学家的副驾驶。它自动化端到端的研究工作流程，包括规划、代码生成和多场景分析。EarthLink通过动态反馈循环从用户交互中学习，并持续改进其能力。", "result": "EarthLink在多项气候变化核心科学任务（从模型-观测比较到复杂现象诊断）上验证了其性能。在多专家评估中，EarthLink产生了科学上合理的分析，并展示了与人类初级研究员工作流程特定方面相当的分析能力。", "conclusion": "EarthLink标志着在全球变化加速的时代，向高效、可信赖和协作的地球系统研究范式迈出了关键一步，使科学家能够从繁琐的手动执行转向战略性监督和假设生成。", "translation": "现代地球科学正处于一个拐点。地球系统数据庞大、分散且复杂，加上日益复杂的分析需求，给快速科学发现带来了显著瓶颈。在此，我们引入了EarthLink，这是第一个被设计为地球科学家交互式副驾驶的AI代理。它自动化了端到端的研究工作流程，从规划和代码生成到多场景分析。与静态诊断工具不同，EarthLink可以通过用户交互进行学习，通过动态反馈循环不断完善其能力。我们在多项气候变化的核心科学任务中验证了其性能，包括模型-观测比较和复杂现象诊断。在多专家评估中，EarthLink产生了科学上合理的分析，并展示了与人类初级研究员工作流程特定方面相当的分析能力。此外，其透明、可审计的工作流程和自然语言界面使科学家能够从繁琐的手动执行转向战略性监督和假设生成。EarthLink标志着在全球变化加速的时代，向高效、可信赖和协作的地球系统研究范式迈出了关键一步。该系统可在我们的网站 https://earthlink.intern-ai.org.cn 访问。", "summary": "EarthLink是一个创新的AI代理，旨在解决地球科学研究中数据复杂性和分析需求的瓶颈。它作为地球科学家的交互式副驾驶，自动化了从规划到多场景分析的整个研究工作流程，并能通过用户反馈进行自我进化。经过验证，EarthLink在气候科学任务中表现出与人类初级研究员相当的分析能力，并提供透明的工作流程和自然语言界面，有望推动地球系统研究进入一个更高效、可信赖和协作的新范式。", "keywords": "AI代理, 气候科学, 地球系统数据, 自进化, 研究自动化", "comments": "EarthLink的创新之处在于其“自进化”能力，即通过用户交互持续学习和改进，这使其区别于传统的静态工具。其作为“交互式副驾驶”的定位，旨在赋能科学家，将他们从繁重的手动操作中解放出来，专注于高层次的思考和假设生成，这对于加速气候科学发现具有重要意义。该系统提高了研究效率和可信度，是AI在科学研究领域应用的一个重要进展。"}}
{"id": "2507.18538", "title": "AI/ML Life Cycle Management for Interoperable AI Native RAN", "authors": ["Chu-Hsiang Huang", "Chao-Kai Wen", "Geoffrey Ye Li"], "categories": ["cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, 2 table. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.18538v1", "summary": "Artificial intelligence (AI) and machine learning (ML) models are rapidly\npermeating the 5G Radio Access Network (RAN), powering beam management, channel\nstate information (CSI) feedback, positioning, and mobility prediction.\nHowever, without a standardized life-cycle management (LCM) framework,\nchallenges, such as model drift, vendor lock-in, and limited transparency,\nhinder large-scale adoption. 3GPP Releases 16-20 progressively evolve AI/ML\nfrom experimental features to managed, interoperable network functions.\nBeginning with the Network Data Analytics Function (NWDAF) in Rel-16,\nsubsequent releases introduced standardized interfaces for model transfer,\nexecution, performance monitoring, and closed-loop control, culminating in\nRel-20's two-sided CSI-compression Work Item and vendor-agnostic LCM profile.\nThis article reviews the resulting five-block LCM architecture, KPI-driven\nmonitoring mechanisms, and inter-vendor collaboration schemes, while\nidentifying open challenges in resource-efficient monitoring, environment drift\ndetection, intelligent decision-making, and flexible model training. These\ndevelopments lay the foundation for AI-native transceivers as a key enabler for\n6G.", "comment": "8 pages, 4 figures, 2 table. This work has been submitted to the IEEE\n  for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.18538v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "互操作AI原生RAN的AI/ML生命周期管理", "tldr": "AI/ML模型正迅速渗透5G无线接入网络（RAN），但缺乏标准化的生命周期管理（LCM）框架阻碍了大规模采用。3GPP版本16-20逐步推动AI/ML标准化。本文回顾了由此产生的五块LCM架构、KPI驱动的监控机制和供应商间协作方案，并指出了开放挑战，为6G的AI原生收发器奠定基础。", "motivation": "AI/ML模型在5G无线接入网络（RAN）中的大规模应用面临挑战，例如模型漂移、供应商锁定和透明度有限，这主要是因为缺乏标准化的生命周期管理（LCM）框架。", "method": "本文回顾了3GPP版本16-20中AI/ML的逐步演进，重点介绍了由此产生的五块LCM架构、KPI驱动的监控机制以及供应商间协作方案，同时识别了资源高效监控、环境漂移检测、智能决策和灵活模型训练等方面的开放挑战。", "result": "论文回顾了3GPP标准化工作形成的五块LCM架构、KPI驱动的监控机制和供应商间协作方案。同时，识别了资源高效监控、环境漂移检测、智能决策和灵活模型训练等开放挑战。", "conclusion": "本文所回顾的AI/ML在RAN中的发展为AI原生收发器奠定了基础，而AI原生收发器是6G的关键使能技术。", "translation": "人工智能（AI）和机器学习（ML）模型正在迅速渗透5G无线接入网络（RAN），为波束管理、信道状态信息（CSI）反馈、定位和移动性预测提供支持。然而，如果没有标准化的生命周期管理（LCM）框架，模型漂移、供应商锁定和透明度有限等挑战将阻碍其大规模采用。3GPP版本16-20逐步将AI/ML从实验性功能发展为受管理、可互操作的网络功能。从版本16中的网络数据分析功能（NWDAF）开始，后续版本引入了模型传输、执行、性能监控和闭环控制的标准化接口，最终在版本20中形成了双向CSI压缩工作项目和供应商无关的LCM配置文件。本文回顾了由此产生的五块LCM架构、基于KPI的监控机制以及供应商间协作方案，同时指出了资源高效监控、环境漂移检测、智能决策和灵活模型训练等方面的开放挑战。这些发展为AI原生收发器奠定了基础，使其成为6G的关键使能技术。", "summary": "本论文探讨了5G无线接入网络（RAN）中AI/ML模型标准化生命周期管理（LCM）的必要性，以克服模型漂移和供应商锁定等挑战。文章回顾了3GPP版本16-20中AI/ML标准化的演进，重点介绍了由此产生的五块LCM架构、基于KPI的监控机制和供应商间协作方案。论文还识别了未来研究的关键开放挑战，强调这些发展是6G AI原生收发器的基础。", "keywords": "AI/ML, RAN, 生命周期管理, 3GPP, 6G", "comments": "本文识别并解决了AI/ML在关键通信基础设施（如RAN）中大规模部署的一个核心挑战——缺乏标准化的生命周期管理。通过回顾3GPP的标准化进程并指出开放挑战，它为行业提供了清晰的路线图。其对互操作性和供应商无关解决方案的关注，对于推动AI/ML在未来网络中的广泛应用具有重要意义。"}}
{"id": "2507.18095", "title": "Towards Microgrid Resilience Enhancement via Mobile Power Sources and Repair Crews: A Multi-Agent Reinforcement Learning Approach", "authors": ["Yi Wang", "Dawei Qiu", "Fei Teng", "Goran Strbac"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18095v1", "summary": "Mobile power sources (MPSs) have been gradually deployed in microgrids as\ncritical resources to coordinate with repair crews (RCs) towards resilience\nenhancement owing to their flexibility and mobility in handling the complex\ncoupled power-transport systems. However, previous work solves the coordinated\ndispatch problem of MPSs and RCs in a centralized manner with the assumption\nthat the communication network is still fully functioning after the event.\nHowever, there is growing evidence that certain extreme events will damage or\ndegrade communication infrastructure, which makes centralized decision making\nimpractical. To fill this gap, this paper formulates the resilience-driven\ndispatch problem of MPSs and RCs in a decentralized framework. To solve this\nproblem, a hierarchical multi-agent reinforcement learning method featuring a\ntwo-level framework is proposed, where the high-level action is used to switch\ndecision-making between power and transport networks, and the low-level action\nconstructed via a hybrid policy is used to compute continuous scheduling and\ndiscrete routing decisions in power and transport networks, respectively. The\nproposed method also uses an embedded function encapsulating system dynamics to\nenhance learning stability and scalability. Case studies based on IEEE 33-bus\nand 69-bus power networks are conducted to validate the effectiveness of the\nproposed method in load restoration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18095v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过移动电源和抢修队增强微电网弹性：一种多智能体强化学习方法", "tldr": "本文提出了一种去中心化的分层多智能体强化学习方法，用于协调移动电源和抢修队，以提高微电网在通信受损情况下的弹性。", "motivation": "现有工作在集中式框架下解决移动电源和抢修队的协调调度问题，并假设通信网络在事件后仍能完全运行。然而，极端事件可能损害或降级通信基础设施，使得集中式决策不切实际。本文旨在填补这一空白。", "method": "本文将移动电源和抢修队的弹性驱动调度问题公式化为去中心化框架。为解决此问题，提出了一种分层多智能体强化学习方法，其特点是两级框架：高级动作用于在电力和交通网络之间切换决策，低级动作通过混合策略构建，用于分别计算电力和交通网络中的连续调度和离散路由决策。该方法还使用嵌入式函数封装系统动态，以增强学习稳定性和可扩展性。", "result": "基于IEEE 33总线和69总线电力网络的案例研究验证了所提方法在负荷恢复方面的有效性。", "conclusion": "所提出的去中心化分层多智能体强化学习方法能够有效协调移动电源和抢修队，以提高微电网在通信受损情况下的弹性，实现负荷恢复。", "translation": "移动电源（MPS）由于其在处理复杂耦合的电力-交通系统方面的灵活性和移动性，已逐渐部署在微电网中，作为与抢修队（RC）协调以增强弹性的关键资源。然而，以往的工作以集中式方式解决MPS和RC的协调调度问题，并假设事件发生后通信网络仍能完全运行。然而，越来越多的证据表明，某些极端事件会损害或降低通信基础设施，这使得集中式决策不切实际。为了填补这一空白，本文在一个去中心化框架中提出了MPS和RC的弹性驱动调度问题。为了解决这个问题，提出了一种分层多智能体强化学习方法，其特点是两级框架，其中高层动作用于在电力和交通网络之间切换决策，通过混合策略构建的低层动作用于分别计算电力和交通网络中的连续调度和离散路由决策。所提出的方法还使用一个封装系统动态的嵌入式函数来增强学习的稳定性和可扩展性。基于IEEE 33总线和69总线电力网络的案例研究验证了所提出方法在负荷恢复方面的有效性。", "summary": "本文针对极端事件可能导致通信基础设施受损，使得集中式决策不可行的问题，提出了一种去中心化的分层多智能体强化学习方法。该方法旨在协调移动电源和抢修队，以增强微电网的弹性。其两级框架分别处理电力和交通网络的决策，并通过嵌入式函数提升学习性能。案例研究验证了该方法在负荷恢复中的有效性。", "keywords": "微电网弹性, 移动电源, 抢修队, 多智能体强化学习, 去中心化调度", "comments": "本文的创新点在于提出了一个去中心化的框架来解决移动电源和抢修队的协调调度问题，这在通信受损的场景下具有重要意义。分层多智能体强化学习方法的设计，特别是高低两级动作和嵌入式函数的使用，提升了方法的实用性和性能。这对于提高微电网在极端事件下的韧性提供了新的思路和解决方案。"}}
{"id": "2507.18021", "title": "Zeroth-order log-concave sampling", "authors": ["Yunbum Kook"], "categories": ["math.ST", "cs.DS", "cs.LG", "math.FA", "math.PR", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "Comments:      30 pages", "url": "http://arxiv.org/abs/2507.18021v1", "summary": "We study the zeroth-order query complexity of log-concave sampling,\nspecifically uniform sampling from convex bodies using membership oracles. We\npropose a simple variant of the proximal sampler that achieves the query\ncomplexity with matched R\\'enyi orders between the initial warmness and output\nguarantee. Specifically, for any $\\varepsilon>0$ and $q\\geq2$, the sampler,\ninitialized at $\\pi_{0}$, outputs a sample whose law is $\\varepsilon$-close in\n$q$-R\\'enyi divergence to $\\pi$, the uniform distribution over a convex body in\n$\\mathbb{R}^{d}$, using\n$\\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\\,\\lVert\\operatorname{cov}\\pi\\rVert\\log\\frac{1}{\\varepsilon})$\nmembership queries, where\n$M_{q}=\\lVert\\text{d}\\pi_{0}/\\text{d}\\pi\\rVert_{L^{q}(\\pi)}$.\n  We further introduce a simple annealing scheme that produces a warm start in\n$q$-R\\'enyi divergence (i.e., $M_{q}=O(1)$) using\n$\\widetilde{O}(qd^{2}R^{3/2}\\,\\lVert\\operatorname{cov}\\pi\\rVert^{1/4})$\nqueries, where $R^{2}=\\mathbb{E}_{\\pi}[|\\cdot|^{2}]$. This interpolates between\nknown complexities for warm-start generation in total variation and\nR\\'enyi-infinity divergence. To relay a R\\'enyi warmness across the annealing\nscheme, we establish hypercontractivity under simultaneous heat flow and\ntranslate it into an improved mixing guarantee for the proximal sampler under a\nlogarithmic Sobolev inequality. These results extend naturally to general\nlog-concave distributions accessible via evaluation oracles, incurring\nadditional quadratic queries.", "comment": "30 pages", "pdf_url": "http://arxiv.org/pdf/2507.18021v1", "cate": "math.ST", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "零阶对数凹采样", "tldr": "本文研究了零阶对数凹采样的查询复杂性，提出了一种改进的近端采样器变体和退火方案，显著提高了查询效率，并在Rényi散度下提供了精确的保证。", "motivation": "研究对数凹采样的零阶查询复杂性，特别是使用成员预言机从凸体中进行均匀采样的复杂性。", "method": "提出了一种近端采样器的简单变体，以实现查询复杂性，并在初始预热和输出保证之间匹配Rényi阶。此外，引入了一种简单的退火方案，以在Rényi散度中产生预热启动。为了在退火方案中传递Rényi预热性，建立了同步热流下的超收缩性，并将其转化为对数Sobolev不等式下近端采样器的改进混合保证。", "result": "对于任何$\\varepsilon>0$和$q\\geq2$，该采样器能够以$\\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\\,\\lVert\\operatorname{cov}\\pi\\rVert\\log\\frac{1}{\\varepsilon})$的成员查询次数输出一个样本，其律在$q$-Rényi散度上与$\\pi$（凸体上的均匀分布）$\\varepsilon$-接近。退火方案使用$\\widetilde{O}(qd^{2}R^{3/2}\\,\\lVert\\operatorname{cov}\\pi\\rVert^{1/4})$查询产生$q$-Rényi散度下的预热启动，弥补了总变差和Rényi无穷散度中已知复杂性之间的差距。这些结果自然地扩展到可通过评估预言机访问的通用对数凹分布，并额外增加二次查询。", "conclusion": "本文提出的近端采样器变体和退火方案在零阶对数凹采样中实现了高效的查询复杂性，尤其是在Rényi散度度量下提供了精确的理论保证，并且这些结果可以推广到更一般的对数凹分布。", "translation": "我们研究了对数凹采样的零阶查询复杂性，特别是使用成员预言机从凸体中进行均匀采样。我们提出了一种近端采样器的简单变体，它在初始预热和输出保证之间匹配Rényi阶，从而实现了查询复杂性。具体来说，对于任何$\\varepsilon>0$和$q\\geq2$，该采样器在$\\pi_{0}$处初始化，输出的样本其律在$q$-Rényi散度上与$\\mathbb{R}^{d}$中凸体上的均匀分布$\\pi$$\\varepsilon$-接近，使用$\\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\\,\\lVert\\operatorname{cov}\\pi\\rVert\\log\\frac{1}{\\varepsilon})$次成员查询，其中$M_{q}=\\lVert\\text{d}\\pi_{0}/\\text{d}\\pi\\rVert_{L^{q}(\\pi)}$。我们进一步引入了一种简单的退火方案，使用$\\widetilde{O}(qd^{2}R^{3/2}\\,\\lVert\\operatorname{cov}\\pi\\rVert^{1/4})$查询生成$q$-Rényi散度下的预热启动（即$M_{q}=O(1)$），其中$R^{2}=\\mathbb{E}_{\\pi}[|\\cdot|^{2}]$。这弥补了总变差和Rényi无穷散度中已知预热启动生成复杂性之间的差距。为了在退火方案中传递Rényi预热性，我们建立了同步热流下的超收缩性，并将其转化为对数Sobolev不等式下近端采样器的改进混合保证。这些结果自然地扩展到可通过评估预言机访问的通用对数凹分布，并额外增加二次查询。", "summary": "本文研究了对数凹采样的零阶查询复杂性，特别是从凸体中进行均匀采样。作者提出了一种改进的近端采样器变体，该变体在初始预热和输出保证之间匹配Rényi阶，从而实现了高效的查询复杂性。此外，还引入了一种简单的退火方案，以在Rényi散度中生成预热启动，并证明了其查询效率。研究还通过建立超收缩性来改进近端采样器的混合保证。这些结果可推广到一般的对数凹分布。", "keywords": "零阶采样, 对数凹, Rényi散度, 近端采样器, 退火方案", "comments": "本文在零阶对数凹采样领域取得了重要进展，特别是在Rényi散度下提供了严格的查询复杂性保证。其创新之处在于提出了一个改进的近端采样器变体和一种有效的退火方案，有效地降低了达到特定精度所需的查询次数。理论贡献包括建立了同步热流下的超收缩性，并将其应用于改进采样器的混合保证。这对于理解和改进高维空间中的采样算法具有重要意义。"}}
{"id": "2507.18491", "title": "Fast Multipole Method for Maxwell's Equations in Layered Media", "authors": ["Heng Yuan", "Bo Wang", "Wenzhong Zhang", "Wei Cai"], "categories": ["math.NA", "cs.NA", "15A15, 15A09, 15A23"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Submitted to SIAM Journal on Scientific Computing", "url": "http://arxiv.org/abs/2507.18491v1", "summary": "We present a fast multipole method (FMM) for solving Maxwell's equations in\nthree-dimensional (3-D) layered media, based on the magnetic vector potential\n$\\boldsymbol A$ under the Lorenz gauge, to derive the layered dyadic Green's\nfunction. The dyadic Green's function is represented using three scalar\nHelmholtz layered Green's functions, with all interface-induced reaction field\ncomponents expressed through a unified integral representation. By introducing\nequivalent polarization images for sources and effective locations for targets\nto reflect the actual transmission distance of different reaction field\ncomponents, multiple expansions (MEs) and local expansions (LEs) are derived\nfor the far-field governed by actual transmission distance. To further enhance\ncomputational efficiency and numerical stability, we employ a Chebyshev\npolynomial expansion of the associated Legendre functions to speed up the\ncalculation of multipole-to-local (M2L) expansion translations. Finally,\nleveraging the FMM framework of the Helmholtz equation in 3-D layered media, we\ndevelop a FMM for the dyadic Green's function of Maxwell's equations in layered\nmedia. Numerical experiments demonstrate the $\\mathcal O(N\\log N)$-complexity\nof the resulting FMM method, and rapid convergence for interactions of\nlow-frequency electromagnetic wave sources in 3-D layered media.", "comment": "Submitted to SIAM Journal on Scientific Computing", "pdf_url": "http://arxiv.org/pdf/2507.18491v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "分层介质中麦克斯韦方程组的快速多极子方法", "tldr": "本文提出了一种基于磁矢量势和层状并矢格林函数的快速多极子方法，用于求解三维分层介质中的麦克斯韦方程组，并展示了其计算效率和快速收敛性。", "motivation": "解决三维分层介质中麦克斯韦方程组的计算问题，提高计算效率和数值稳定性。", "method": "1. 基于Lorenz规范下的磁矢量势A推导层状并矢格林函数。2. 将并矢格林函数表示为三个标量亥姆霍兹层状格林函数，并统一表示界面引起的反应场分量。3. 引入等效极化图像和有效目标位置，推导基于实际传输距离的远场多极展开和局部展开。4. 采用切比雪夫多项式展开关联勒让德函数，加速多极子到局部（M2L）展开平移的计算。5. 利用三维分层介质中亥姆霍兹方程的FMM框架，开发适用于麦克斯韦方程组并矢格林函数的FMM。", "result": "所提出的FMM方法具有$\\mathcal O(N\\log N)$的计算复杂度，并且对于三维分层介质中低频电磁波源的相互作用具有快速收敛性。", "conclusion": "本文提出的用于分层介质中麦克斯韦方程组的快速多极子方法，在计算效率和数值稳定性方面表现出色，并能有效处理低频电磁波源的相互作用。", "translation": "我们提出了一种在三维（3-D）分层介质中求解麦克斯韦方程组的快速多极子方法（FMM），该方法基于Lorenz规范下的磁矢量势$\\boldsymbol A$，以推导层状并矢格林函数。该并矢格林函数使用三个标量亥姆霍兹层状格林函数表示，所有界面引起的反应场分量通过统一的积分表示法表达。通过引入源的等效极化图像和目标的有效位置，以反映不同反应场分量的实际传输距离，推导了由实际传输距离控制的远场多极展开（MEs）和局部展开（LEs）。为了进一步提高计算效率和数值稳定性，我们采用关联勒让德函数的切比雪夫多项式展开来加速多极子到局部（M2L）展开平移的计算。最后，利用三维分层介质中亥姆霍兹方程的FMM框架，我们开发了一种用于分层介质中麦克斯韦方程组并矢格林函数的FMM。数值实验表明，所得到的FMM方法具有$\\mathcal O(N\\log N)$的复杂度，并且对于三维分层介质中低频电磁波源的相互作用具有快速收敛性。", "summary": "本文提出了一种用于求解三维分层介质中麦克斯韦方程组的快速多极子方法（FMM）。该方法基于磁矢量势和层状并矢格林函数，通过引入等效极化图像和有效目标位置，并结合切比雪夫多项式展开加速M2L计算，构建了高效的FMM框架。数值实验证明该方法具有$\\mathcal O(N\\log N)$的计算复杂度，并能实现低频电磁波相互作用的快速收敛。", "keywords": "快速多极子方法, 麦克斯韦方程组, 分层介质, 并矢格林函数, 计算电磁学", "comments": "该论文创新性地将快速多极子方法应用于分层介质中的麦克斯韦方程组求解，通过引入一系列技术（如等效极化图像、切比雪夫多项式展开）显著提高了计算效率和数值稳定性。其$\\mathcal O(N\\log N)$的复杂度对于处理大规模问题具有重要意义，特别是在电磁仿真、地球物理勘探等领域有潜在应用价值。"}}
{"id": "2505.10072", "title": "ToonifyGB: StyleGAN-based Gaussian Blendshapes for 3D Stylized Head Avatars", "authors": ["Rui-Yang Ju", "Sheng-Yen Huang", "Yi-Ping Hung"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10072v2", "summary": "The introduction of 3D Gaussian blendshapes has enabled the real-time\nreconstruction of animatable head avatars from monocular video. Toonify, a\nStyleGAN-based method, has become widely used for facial image stylization. To\nextend Toonify for synthesizing diverse stylized 3D head avatars using Gaussian\nblendshapes, we propose an efficient two-stage framework, ToonifyGB. In Stage 1\n(stylized video generation), we adopt an improved StyleGAN to generate the\nstylized video from the input video frames, which overcomes the limitation of\ncropping aligned faces at a fixed resolution as preprocessing for normal\nStyleGAN. This process provides a more stable stylized video, which enables\nGaussian blendshapes to better capture the high-frequency details of the video\nframes, facilitating the synthesis of high-quality animations in the next\nstage. In Stage 2 (Gaussian blendshapes synthesis), our method learns a\nstylized neutral head model and a set of expression blendshapes from the\ngenerated stylized video. By combining the neutral head model with expression\nblendshapes, ToonifyGB can efficiently render stylized avatars with arbitrary\nexpressions. We validate the effectiveness of ToonifyGB on benchmark datasets\nusing two representative styles: Arcane and Pixar.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10072v2", "cate": "cs.CV", "date": "2025-05-15", "updated": "2025-07-24", "AI": {"title_translation": "ToonifyGB：基于StyleGAN的高斯混合形状用于3D风格化头部虚拟形象", "tldr": "ToonifyGB提出了一种两阶段框架，利用改进的StyleGAN生成风格化视频，并从中学习高斯混合形状，以高效合成具有任意表情的3D风格化头部虚拟形象。", "motivation": "现有的3D高斯混合形状能够实时重建可动画的头部虚拟形象，而Toonify是一种基于StyleGAN的方法，广泛用于面部图像风格化。为了扩展Toonify以利用高斯混合形状合成多样化的风格化3D头部虚拟形象，本研究提出了ToonifyGB。", "method": "ToonifyGB是一个高效的两阶段框架。第一阶段（风格化视频生成）采用改进的StyleGAN从输入视频帧生成风格化视频，克服了传统StyleGAN需要固定分辨率裁剪对齐人脸的限制，提供了更稳定的风格化视频，从而使高斯混合形状能更好地捕捉视频帧的高频细节。第二阶段（高斯混合形状合成）从生成的风格化视频中学习风格化的中性头部模型和一组表情混合形状。通过结合中性头部模型和表情混合形状，ToonifyGB可以高效渲染具有任意表情的风格化虚拟形象。", "result": "ToonifyGB在基准数据集上验证了其有效性，并展示了在“奥术（Arcane）”和“皮克斯（Pixar）”两种代表性风格下的表现。", "conclusion": "ToonifyGB成功地将StyleGAN的风格化能力与3D高斯混合形状相结合，实现了从单目视频高效生成高质量、可动画的3D风格化头部虚拟形象。", "translation": "3D高斯混合形状的引入使得从单目视频实时重建可动画的头部虚拟形象成为可能。Toonify作为一种基于StyleGAN的方法，已广泛应用于面部图像风格化。为了扩展Toonify以利用高斯混合形状合成多样化的风格化3D头部虚拟形象，我们提出了一种高效的两阶段框架ToonifyGB。在第一阶段（风格化视频生成）中，我们采用改进的StyleGAN从输入视频帧生成风格化视频，这克服了传统StyleGAN需要预处理裁剪固定分辨率对齐人脸的限制。此过程提供了更稳定的风格化视频，使得高斯混合形状能够更好地捕捉视频帧的高频细节，从而有助于在下一阶段合成高质量动画。在第二阶段（高斯混合形状合成）中，我们的方法从生成的风格化视频中学习一个风格化的中性头部模型和一组表情混合形状。通过将中性头部模型与表情混合形状相结合，ToonifyGB可以高效地渲染具有任意表情的风格化虚拟形象。我们在基准数据集上使用两种代表性风格：奥术（Arcane）和皮克斯（Pixar）验证了ToonifyGB的有效性。", "summary": "ToonifyGB提出了一种基于StyleGAN的两阶段框架，用于从单目视频合成3D风格化头部虚拟形象。第一阶段使用改进的StyleGAN生成稳定的风格化视频，克服了传统StyleGAN的限制。第二阶段从风格化视频中学习风格化的中性头部模型和表情混合形状，从而能够高效渲染具有任意表情的高质量风格化虚拟形象。该方法在Arcane和Pixar风格上得到了验证。", "keywords": "StyleGAN, 高斯混合形状, 3D虚拟形象, 风格化, 实时渲染", "comments": "这项研究创新性地结合了StyleGAN的图像风格化能力与3D高斯混合形状的头部建模和动画能力。其两阶段框架，特别是改进的StyleGAN在风格化视频生成中的应用，解决了传统StyleGAN在处理视频帧时的限制，使得后续的高斯混合形状能更好地捕捉细节。这为实时生成高质量、可动画的3D风格化虚拟形象提供了一种高效且有效的方法，在虚拟现实、游戏和数字内容创作领域具有潜在应用价值。"}}
{"id": "2507.18179", "title": "Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers", "authors": ["Felix Arnold", "Maxence Bouvier", "Ryan Amaudruz", "Renzo Andri", "Lukas Cavigelli"], "categories": ["cs.NE", "cs.AR", "cs.PF"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Accepted and presented at the 34th International Workshop on Logic & Synthesis June 2025", "url": "http://arxiv.org/abs/2507.18179v1", "summary": "This work presents a method to maximize power-efficiency of fixed point\nmultiplier units by decomposing them into sub-components. First, an encoder\nblock converts the operands from a two's complement to a sign magnitude\nrepresentation, followed by a multiplier module which performs the compute\noperation and outputs the resulting value in the original format. This allows\nto leverage the power-efficiency of the Sign Magnitude encoding for the\nmultiplication. To ensure the computing format is not altered, those two\ncomponents are synthesized and optimized separately. Our method leads to\nsignificant power savings for input values centered around zero, as commonly\nencountered in AI workloads. Under a realistic input stream with values\nnormally distributed with a standard deviation of 3.0, post-synthesis\nsimulations of the 4-bit multiplier design show up to 12.9% lower switching\nactivity compared to synthesis without decomposition. Those gains are achieved\nwhile ensuring compliance into any production-ready system as the overall\ncircuit stays logic-equivalent. With the compliance lifted and a slightly\nsmaller input range of -7 to +7, switching activity reductions can reach up to\n33%. Additionally, we demonstrate that synthesis optimization methods based on\nswitching-activity-driven design space exploration can yield a further 5-10%\nimprovement in power-efficiency compared to a power agnostic approach.", "comment": "Accepted and presented at the 34th International Workshop on Logic &\n  Synthesis June 2025", "pdf_url": "http://arxiv.org/pdf/2507.18179v1", "cate": "cs.NE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "显式符号-幅度编码器实现高能效乘法器", "tldr": "通过将乘法器分解并使用符号-幅度编码，显著降低了AI工作负载中乘法器的功耗。", "motivation": "旨在最大化定点乘法器单元的功耗效率，特别是针对AI工作负载中常见输入值集中在零附近的情况。", "method": "将乘法器分解为子组件：首先，一个编码器块将操作数从补码转换为符号-幅度表示；然后，一个乘法器模块执行计算并将结果输出回原始格式。这两个组件被单独综合和优化，以利用符号-幅度编码的功耗效率，同时确保计算格式不变。还结合了基于开关活动驱动的设计空间探索的综合优化方法。", "result": "对于输入值集中在零附近的情况，实现了显著的功耗节省。在标准差为3.0的真实输入流下，4位乘法器设计相比未分解的综合，开关活动降低了12.9%。在解除合规性限制且输入范围为-7到+7时，开关活动降低可达33%。此外，基于开关活动驱动的综合优化方法可进一步提高5-10%的功耗效率。", "conclusion": "通过分解乘法器并利用符号-幅度编码，可以显著提高定点乘法器的功耗效率，尤其适用于AI工作负载，并且可以与现有生产系统兼容。", "translation": "这项工作提出了一种通过将定点乘法器单元分解为子组件来最大化其功耗效率的方法。首先，一个编码器块将操作数从补码转换为符号-幅度表示，然后一个乘法器模块执行计算并将结果以原始格式输出。这使得乘法可以利用符号-幅度编码的功耗效率。为了确保计算格式不被改变，这两个组件被单独综合和优化。我们的方法对于输入值集中在零附近的情况（这在AI工作负载中很常见）带来了显著的功耗节省。在标准差为3.0的真实输入流下，4位乘法器设计的后综合仿真显示，与不进行分解的综合相比，开关活动降低了高达12.9%。这些增益是在确保符合任何生产就绪系统（因为整体电路保持逻辑等效）的情况下实现的。如果解除合规性限制并将输入范围略微缩小到-7到+7，开关活动降低可达33%。此外，我们证明了基于开关活动驱动的设计空间探索的综合优化方法，与功耗无关的方法相比，可以进一步提高5-10%的功耗效率。", "summary": "本文提出了一种提高定点乘法器功耗效率的方法，通过将其分解为补码到符号-幅度编码器和乘法模块。这种方法利用了符号-幅度编码的能效，并在AI工作负载中常见的输入值集中在零附近时表现出显著的功耗节省。实验结果显示，与传统方法相比，开关活动最多可降低33%，并通过优化方法进一步进一步提升5-10%的效率，同时保持逻辑兼容性。", "keywords": "功耗效率, 乘法器, 符号-幅度编码, AI工作负载, 开关活动", "comments": "这项工作通过引入显式的符号-幅度编码器来优化乘法器功耗，具有创新性。其重要性在于能够显著降低AI硬件中的功耗，这对于边缘计算和高性能计算都至关重要。通过分解设计并利用编码特性，它提供了一种实用的能效提升途径，同时考虑了与现有系统的兼容性。"}}
{"id": "2507.17966", "title": "Time and Frequency Synchronization for Multiuser OTFS in Uplink", "authors": ["Mohsen Bayat", "Sanoopkumar P. S.", "Arman Farhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17966v1", "summary": "In this paper, we propose time and frequency synchronization techniques for\nuplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work\nfocuses on accurately estimating and correcting timing offsets (TOs) and\ncarrier frequency offsets (CFOs). Specifically, TO estimation is essential for\nlocating users' pilots on the delay-time plane, while CFO estimation enhances\nchannel estimation accuracy. First, we propose a TO estimation technique for an\nexisting multiuser pilot structure in MU-OTFS. We replace the impulse pilot\n(IMP) in this pilot structure with a more practical pilot with a cyclic prefix\n(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs\ndifferent Zadoff-Chu (ZC) sequences, which enables pilot separation via\ncorrelation at the receiver side. Consequently, we introduce a\ncorrelation-based TO estimation technique for uplink MU-OTFS using this pilot\nstructure. Next, a spectrally efficient and practical pilot pattern is\nproposed, where each user transmits a PCP within a shared pilot region on the\ndelay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO\nestimation technique utilizes a bank of filters to separate different users'\nsignals and accurately estimate their TOs. Then, we derive a mathematical\nthreshold range to enhance TO estimation accuracy by finding the first major\npeak in the correlation function rather than relying solely on the highest\npeak. After locating the received users' pilot signals using one of the\nproposed TO estimation techniques, our proposed CFO estimation technique\nreduces the multi-dimensional maximum likelihood (ML) search problem into\nmultiple one-dimensional search problems. In this technique, we apply the\nChebyshev polynomials of the first kind basis expansion model (CPF-BEM) to\neffectively handle the time-variations of the channel in obtaining the CFO\nestimates for all the users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17966v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "上行多用户OTFS系统中的时间和频率同步", "tldr": "针对高移动性上行多用户OTFS系统，提出了时间和频率同步技术，以精确估计和校正定时和载波频率偏移。", "motivation": "在高移动性场景下，上行多用户OTFS (MU-OTFS) 系统需要精确的定时偏移 (TO) 和载波频率偏移 (CFO) 估计与校正，以准确定位用户导频并提高信道估计精度。", "method": "本文提出了两种定时偏移 (TO) 估计技术和一种载波频率偏移 (CFO) 估计技术。\n1. **TO估计（方案一）**: 针对现有MU-OTFS多用户导频结构，用循环前缀导频 (PCP) 替代了脉冲导频 (IMP)，提出了单用户启发式PCP (SU-PCP)。该结构利用不同的Zadoff-Chu (ZC) 序列，通过接收端相关性实现导频分离，并基于此引入了相关性TO估计技术。\n2. **TO估计（方案二）**: 提出了一种频谱高效且实用的多用户PCP (MU-PCP) 导频模式，其中每个用户在时延-多普勒平面上的共享导频区域内传输PCP。接收端利用滤波器组分离不同用户信号并精确估计其TO。此外，推导了一个数学阈值范围，通过查找相关函数中的第一个主要峰值而非仅最高峰来提高TO估计精度。\n3. **CFO估计**: 在定位用户导频信号后，将多维最大似然 (ML) 搜索问题简化为多个一维搜索问题。该技术应用第一类切比雪夫多项式基扩展模型 (CPF-BEM) 来有效处理信道的时变性，以获取所有用户的CFO估计。", "result": "提出的TO估计技术能够通过查找相关函数的第一个主要峰值而非仅最高峰来提高精度。提出的CFO估计技术能够将多维最大似然搜索问题简化为多个一维搜索问题，并利用CPF-BEM有效处理信道时变性，从而获得所有用户的CFO估计。", "conclusion": "Not mentioned in abstract", "translation": "在本文中，我们针对高移动性场景下的上行多用户OTFS (MU-OTFS) 系统提出了时间和频率同步技术。这项工作专注于精确估计和校正定时偏移 (TO) 和载波频率偏移 (CFO)。具体而言，TO估计对于在时延-时间平面上定位用户导频至关重要，而CFO估计则能提高信道估计精度。首先，我们针对MU-OTFS中现有的多用户导频结构提出了一种TO估计技术。我们将该导频结构中的脉冲导频 (IMP) 替换为一种更实用的带有循环前缀的导频 (PCP)，称之为单用户启发式PCP (SU-PCP)。该结构采用不同的Zadoff-Chu (ZC) 序列，通过接收端相关性实现导频分离。因此，我们引入了一种使用该导频结构的基于相关性的上行MU-OTFS TO估计技术。接下来，提出了一种频谱高效且实用的导频模式，其中每个用户在时延-多普勒平面上的共享导频区域内传输PCP，称之为MU-PCP。在接收端，第二种TO估计技术利用滤波器组分离不同用户的信号并精确估计其TO。然后，我们推导了一个数学阈值范围，通过查找相关函数中的第一个主要峰值而非仅最高峰来提高TO估计精度。在利用所提出的TO估计技术之一定位接收到的用户导频信号后，我们提出的CFO估计技术将多维最大似然 (ML) 搜索问题简化为多个一维搜索问题。在该技术中，我们应用第一类切比雪夫多项式基扩展模型 (CPF-BEM) 来有效处理信道的时变性，以获取所有用户的CFO估计。", "summary": "本文针对高移动性上行多用户OTFS系统，提出了一系列时间和频率同步技术。在定时偏移 (TO) 估计方面，引入了单用户启发式循环前缀导频 (SU-PCP) 和多用户循环前缀导频 (MU-PCP) 两种导频结构及相应的相关性估计方法，并通过数学阈值增强了精度。在载波频率偏移 (CFO) 估计方面，将多维最大似然搜索问题简化为一维问题，并利用第一类切比雪夫多项式基扩展模型 (CPF-BEM) 处理信道时变性。这些技术旨在精确估计和校正TO和CFO，以提升系统性能。", "keywords": "OTFS, 多用户, 同步, 定时偏移, 载波频率偏移", "comments": "本文的创新点在于提出了两种新颖的定时偏移导频结构 (SU-PCP和MU-PCP) 及其相应的相关性估计方法，并通过引入数学阈值来提升估计精度。此外，在载波频率偏移估计方面，通过将多维ML搜索简化为一维搜索并结合CPF-BEM，有效降低了复杂性并处理了信道时变性，这对于高移动性场景下的MU-OTFS系统具有重要意义。"}}
{"id": "2507.18013", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages, 5 figures", "url": "http://arxiv.org/abs/2507.18013v1", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "comment": "32 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.18013v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TeleChat2、TeleChat2.5和T1技术报告", "tldr": "本文介绍了TeleChat模型的最新系列：TeleChat2、TeleChat2.5和T1。这些模型通过增强的训练策略实现了显著的性能提升，T1-115B在推理和通用任务上表现出色，甚至超越了某些专有模型，并且该系列模型已公开发布。", "motivation": "本文的动机是推出TeleChat模型的最新系列（TeleChat2、TeleChat2.2和T1），以在其前身TeleChat的基础上实现显著的性能提升，主要通过增强预训练和后训练阶段的训练策略实现。", "method": "本文介绍了TeleChat模型的新系列，它们在模型架构变化最小的情况下，通过增强的预训练和后训练策略实现了性能提升。TeleChat2在10万亿高质量多样化tokens上进行预训练，并结合监督微调（SFT）和直接偏好优化（DPO）。TeleChat2.5和T1通过引入领域特定数据集的持续预训练阶段，并结合强化学习（RL），以提升代码生成和数学推理能力。T1专为复杂推理设计，支持长链式思考（CoT），而TeleChat2.5则优先考虑推理速度。旗舰模型T1和TeleChat2.5均采用115B参数的密集Transformer架构。", "result": "TeleChat2、TeleChat2.5和T1系列模型在推理和通用任务性能方面取得了显著进步。特别是T1-115B，其性能超越了OpenAI的o1-mini和GPT-4o等专有模型。", "conclusion": "本文推出了TeleChat模型的最新系列，即TeleChat2、TeleChat2.5和T1，它们通过改进的训练策略实现了显著的性能提升，并在推理和通用任务上表现出色。这些模型，包括35B和115B参数的后训练版本，已公开发布，旨在为开发人员和研究人员提供最先进的语言模型，以支持多样化应用。", "translation": "我们推出了TeleChat模型的最新系列：TeleChat2、TeleChat2.5和T1，它们比其前身TeleChat有了显著升级。尽管模型架构变化最小，但新系列通过在预训练和后训练阶段增强训练策略，实现了实质性的性能提升。该系列始于TeleChat2，它在10万亿高质量和多样化tokens上进行预训练。随后进行监督微调（SFT）和直接偏好优化（DPO），以进一步增强其能力。TeleChat2.5和T1通过结合领域特定数据集的持续预训练阶段，并结合强化学习（RL），扩展了流水线，以提高代码生成和数学推理任务的性能。T1变体专为复杂推理设计，支持长链式思考（CoT）推理，并在数学和编码方面表现出实质性改进。相比之下，TeleChat2.5优先考虑速度，提供快速推理。T1和TeleChat2.5两款旗舰模型都是具有115B参数的密集Transformer架构，与原始TeleChat相比，在推理和通用任务性能方面展现出显著进步。值得注意的是，T1-115B超越了OpenAI的o1-mini和GPT-4o等专有模型。我们公开发布TeleChat2、TeleChat2.5和T1，包括35B和115B参数的后训练版本，以赋予开发人员和研究人员最先进的语言模型，适用于各种应用。", "summary": "本文介绍了TeleChat系列的新一代模型：TeleChat2、TeleChat2.5和T1。这些模型在保持原有架构的基础上，通过优化预训练和后训练策略，实现了显著的性能飞跃。TeleChat2在海量数据上进行预训练并结合SFT和DPO。TeleChat2.5和T1则进一步引入领域特定持续预训练和强化学习，分别侧重于推理速度和复杂推理（如代码生成和数学）。其中，115B参数的T1模型在推理和通用任务上表现卓越，甚至超越了GPT-4o等领先模型。该系列模型已公开发布，旨在推动语言模型在多样化应用中的发展。", "keywords": "TeleChat, 语言模型, 预训练, 微调, 强化学习", "comments": "本文的创新点在于，在模型架构仅有微小变化的情况下，通过精巧设计的训练策略（包括大规模预训练、SFT、DPO、领域特定持续预训练和RL），成功实现了模型性能的显著提升，尤其是在复杂推理和特定领域任务上。T1-115B超越GPT-4o的性能表现，凸显了其强大的竞争力。此外，模型的公开发布对于推动社区研究和应用具有重要意义。"}}
{"id": "2507.07893", "title": "An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qing", "Qing xu", "Kaiwen Pan", "Ting luo"], "categories": ["cs.AI", "68T50, 68T30, 91F20", "I.2.7; I.2.4; K.5.1; H.3.3"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      19 pages,3 figures", "url": "http://arxiv.org/abs/2507.07893v3", "summary": "Legal dispute analysis is crucial for intelligent legal assistance systems.\nHowever, current LLMs face significant challenges in understanding complex\nlegal concepts, maintaining reasoning consistency, and accurately citing legal\nsources. This research presents a framework combining prompt engineering with\nmultidimensional knowledge graphs to improve LLMs' legal dispute analysis.\nSpecifically, the framework includes a three-stage hierarchical prompt\nstructure (task definition, knowledge background, reasoning guidance) along\nwith a three-layer knowledge graph (legal ontology, representation, instance\nlayers). Additionally, four supporting methods enable precise legal concept\nretrieval: direct code matching, semantic vector similarity, ontology path\nreasoning, and lexical segmentation. Through extensive testing, results show\nmajor improvements: sensitivity increased by 9.9%-13.8%, specificity by\n4.8%-6.7%, and citation accuracy by 22.4%-39.7%. As a result, the framework\nprovides better legal analysis and understanding of judicial logic, thus\noffering a new technical method for intelligent legal assistance systems.", "comment": "19 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2507.07893v3", "cate": "cs.AI", "date": "2025-07-10", "updated": "2025-07-24", "AI": {"title_translation": "法律纠纷分析中提示工程与多维知识图谱的集成框架", "tldr": "LLMs在法律纠纷分析中面临挑战，本文提出一个结合提示工程和多维知识图谱的框架，显著提高了敏感性、特异性和引用准确性，为智能法律辅助系统提供了新方法。", "motivation": "当前LLMs在理解复杂法律概念、保持推理一致性和准确引用法律来源方面面临重大挑战，而法律纠纷分析对智能法律辅助系统至关重要。", "method": "本文提出了一个结合提示工程与多维知识图谱的框架，以改进LLMs的法律纠纷分析能力。该框架包括一个三阶段分层提示结构（任务定义、知识背景、推理指导）和一个三层知识图谱（法律本体、表示、实例层）。此外，还包括四种支持方法用于精确法律概念检索：直接代码匹配、语义向量相似性、本体路径推理和词法分割。", "result": "该框架显著提高了LLMs的法律纠纷分析能力：敏感性提高了9.9%-13.8%，特异性提高了4.8%-6.7%，引用准确性提高了22.4%-39.7%。", "conclusion": "该框架提供了更好的法律分析和对司法逻辑的理解，从而为智能法律辅助系统提供了一种新的技术方法。", "translation": "法律纠纷分析对于智能法律辅助系统至关重要。然而，当前的大型语言模型在理解复杂法律概念、保持推理一致性以及准确引用法律来源方面面临重大挑战。本研究提出了一个结合提示工程和多维知识图谱的框架，以改进大型语言模型的法律纠纷分析能力。具体而言，该框架包括一个三阶段的分层提示结构（任务定义、知识背景、推理指导）以及一个三层知识图谱（法律本体层、表示层、实例层）。此外，四种支持方法能够实现精确的法律概念检索：直接代码匹配、语义向量相似性、本体路径推理和词法分割。通过广泛测试，结果显示出显著改进：敏感性提高了9.9%-13.8%，特异性提高了4.8%-6.7%，引用准确性提高了22.4%-39.7%。因此，该框架提供了更好的法律分析和对司法逻辑的理解，从而为智能法律辅助系统提供了一种新的技术方法。", "summary": "本文提出了一个集成提示工程和多维知识图谱的框架，旨在解决大型语言模型在法律纠纷分析中面临的复杂概念理解、推理一致性和引用准确性挑战。该框架包含三阶段分层提示结构和三层知识图谱，并辅以四种法律概念检索方法。实验结果表明，该框架显著提升了大型语言模型在法律分析中的敏感性、特异性和引用准确性，为智能法律辅助系统提供了有效的新技术途径。", "keywords": "提示工程, 多维知识图谱, 法律纠纷分析, 大型语言模型, 智能法律辅助", "comments": "该研究通过将提示工程与多维知识图谱深度融合，有效提升了LLMs在专业法律领域的表现，特别是在复杂概念理解、推理和引用准确性方面。其分层提示结构和多层知识图谱设计具有创新性，并通过具体数据验证了其有效性，为构建更可靠的智能法律辅助系统奠定了基础。"}}
{"id": "2507.17347", "title": "Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation", "authors": ["Haotian Chen", "Zhiyong Xiao"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      After discussion among the authors, some parts of the paper are deemed inappropriate and will be revised and resubmitted", "url": "http://arxiv.org/abs/2507.17347v2", "summary": "In the field of food image processing, efficient semantic segmentation\ntechniques are crucial for industrial applications. However, existing\nlarge-scale Transformer-based models (such as FoodSAM) face challenges in\nmeeting practical deploymentrequirements due to their massive parameter counts\nand high computational resource demands. This paper introduces TUNable Adapter\nmodule (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that\nintegrates multiscale trainable adapters into the Swin Transformer\narchitecture, achieving high-performance food image segmentation by updating\nonly 4% of the parameters. The core innovation of Swin-TUNA lies in its\nhierarchical feature adaptation mechanism: it designs separable convolutions in\ndepth and dimensional mappings of varying scales to address the differences in\nfeatures between shallow and deep networks, combined with a dynamic balancing\nstrategy for tasks-agnostic and task-specific features. Experiments demonstrate\nthat this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and\nUECFoodPix Complete datasets, respectively, surpassing the fully parameterized\nFoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).\nFurthermore, Swin-TUNA exhibits faster convergence and stronger generalization\ncapabilities in low-data scenarios, providing an efficient solution for\nassembling lightweight food image.", "comment": "After discussion among the authors, some parts of the paper are\n  deemed inappropriate and will be revised and resubmitted", "pdf_url": "http://arxiv.org/pdf/2507.17347v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "Swin-TUNA：一种用于精确食物图像分割的新型PEFT方法", "tldr": "Swin-TUNA是一种高效的参数高效微调（PEFT）方法，通过集成多尺度可训练适配器到Swin Transformer架构中，实现了高性能食物图像分割，同时大幅减少了参数量。", "motivation": "现有的基于Transformer的大规模模型（如FoodSAM）由于参数量巨大和计算资源需求高，难以满足实际部署要求，在食物图像处理领域，高效的语义分割技术对于工业应用至关重要。", "method": "Swin-TUNA是一种参数高效微调（PEFT）方法，它将多尺度可训练适配器集成到Swin Transformer架构中。其核心创新在于分层特征适应机制：设计了深度可分离卷积和不同尺度的维度映射，以解决浅层和深层网络之间的特征差异，并结合了任务无关和任务特定特征的动态平衡策略。该方法仅更新4%的参数。", "result": "在FoodSeg103和UECFoodPix Complete数据集上，分别实现了50.56%和74.94%的mIoU，超越了全参数化的FoodSAM模型，同时将参数量减少了98.7%（仅8.13M）。此外，Swin-TUNA在低数据量场景下表现出更快的收敛速度和更强的泛化能力。", "conclusion": "Swin-TUNA为组装轻量级食物图像分割模型提供了一种高效的解决方案，它在保持高性能的同时显著减少了模型参数和计算需求，尤其适用于实际部署和低数据量场景。", "translation": "在食物图像处理领域，高效的语义分割技术对于工业应用至关重要。然而，现有的大规模基于Transformer的模型（如FoodSAM）由于其庞大的参数数量和高计算资源需求，在满足实际部署要求方面面临挑战。本文引入了可调适配器模块（Swin-TUNA），这是一种参数高效微调（PEFT）方法，它将多尺度可训练适配器集成到Swin Transformer架构中，通过仅更新4%的参数实现了高性能食物图像分割。Swin-TUNA的核心创新在于其分层特征适应机制：它设计了深度可分离卷积和不同尺度的维度映射，以解决浅层和深层网络之间的特征差异，并结合了任务无关和任务特定特征的动态平衡策略。实验表明，该方法在FoodSeg103和UECFoodPix Complete数据集上分别达到了50.56%和74.94%的mIoU，超越了全参数化的FoodSAM模型，同时将参数量减少了98.7%（仅8.13M）。此外，Swin-TUNA在低数据量场景下表现出更快的收敛速度和更强的泛化能力，为组装轻量级食物图像提供了高效解决方案。", "summary": "本文提出了一种名为Swin-TUNA的参数高效微调（PEFT）方法，用于食物图像的精确语义分割。针对现有大型Transformer模型（如FoodSAM）参数量大、计算需求高导致部署困难的问题，Swin-TUNA将多尺度可训练适配器集成到Swin Transformer架构中，仅需更新4%的参数即可实现高性能。其创新点在于分层特征适应机制，通过深度可分离卷积和多尺度维度映射处理不同网络层特征差异，并结合动态平衡策略。实验证明，Swin-TUNA在FoodSeg103和UECFoodPix Complete数据集上取得了优于FoodSAM的mIoU，同时将参数量大幅减少98.7%，并表现出更快的收敛速度和更强的低数据泛化能力，为轻量级食物图像分割提供了高效方案。", "keywords": "食物图像分割, PEFT, Swin Transformer, 参数高效微调, 语义分割", "comments": "Swin-TUNA的创新性在于其独特的分层特征适应机制和动态平衡策略，有效地解决了大型模型部署的计算资源瓶颈。通过仅更新少量参数，它在保持甚至超越现有全参数模型性能的同时，实现了显著的模型轻量化。这对于资源受限的工业应用场景具有重要意义，尤其是在低数据量环境下展现出的泛化能力，进一步提升了其实用价值。"}}
{"id": "2503.07926", "title": "Learning Gentle Grasping Using Vision, Sound, and Touch", "authors": ["Ken Nakahara", "Roberto Calandra"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages. Accepted by 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "url": "http://arxiv.org/abs/2503.07926v2", "summary": "In our daily life, we often encounter objects that are fragile and can be\ndamaged by excessive grasping force, such as fruits. For these objects, it is\nparamount to grasp gently -- not using the maximum amount of force possible,\nbut rather the minimum amount of force necessary. This paper proposes using\nvisual, tactile, and auditory signals to learn to grasp and regrasp objects\nstably and gently. Specifically, we use audio signals as an indicator of\ngentleness during the grasping, and then train an end-to-end action-conditional\nmodel from raw visuo-tactile inputs that predicts both the stability and the\ngentleness of future grasping candidates, thus allowing the selection and\nexecution of the most promising action. Experimental results on a\nmulti-fingered hand over 1,500 grasping trials demonstrated that our model is\nuseful for gentle grasping by validating the predictive performance (3.27%\nhigher accuracy than the vision-only variant) and providing interpretations of\ntheir behavior. Finally, real-world experiments confirmed that the grasping\nperformance with the trained multi-modal model outperformed other baselines\n(17% higher rate for stable and gentle grasps than vision-only). Our approach\nrequires neither tactile sensor calibration nor analytical force modeling,\ndrastically reducing the engineering effort to grasp fragile objects. Dataset\nand videos are available at https://lasr.org/research/gentle-grasping.", "comment": "8 pages. Accepted by 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2503.07926v2", "cate": "cs.RO", "date": "2025-03-11", "updated": "2025-07-24", "AI": {"title_translation": "利用视觉、声音和触觉学习轻柔抓取", "tldr": "该论文提出了一种利用视觉、触觉和听觉信号学习轻柔抓取的方法，并通过端到端模型预测抓取的稳定性和轻柔性，在实验中表现优于仅视觉方法，且无需校准或建模。", "motivation": "在日常生活中，抓取脆弱物品（如水果）时需要轻柔，避免过度用力造成损坏。现有方法可能难以实现以最小必要力进行抓取。", "method": "该论文提出利用视觉、触觉和听觉信号来学习稳定且轻柔地抓取和重新抓取物体。具体而言，使用音频信号作为抓取过程中轻柔度的指标，然后训练一个从原始视觉-触觉输入到预测未来抓取候选的稳定性和轻柔性的端到端动作条件模型，从而选择和执行最有希望的动作。该方法无需触觉传感器校准或分析力建模。", "result": "在超过1500次抓取试验的多指手实验中，该模型在预测性能上比仅视觉变体高3.27%的准确率。真实世界实验证实，训练后的多模态模型在稳定和轻柔抓取率上比仅视觉方法高17%。该方法大大减少了抓取脆弱物品的工程量。", "conclusion": "结合视觉、声音和触觉的多模态学习方法能够有效实现对脆弱物品的轻柔抓取，并在预测性能和实际表现上优于仅视觉方法，同时简化了工程实施，无需复杂的传感器校准或力学建模。", "translation": "在我们的日常生活中，我们经常遇到易碎且可能因过度抓取力而损坏的物体，例如水果。对于这些物体，轻柔抓取至关重要——不是使用最大可能的力，而是使用最小必要的力。本文提出利用视觉、触觉和听觉信号来学习稳定且轻柔地抓取和重新抓取物体。具体而言，我们使用音频信号作为抓取过程中轻柔度的指标，然后从原始视觉-触觉输入训练一个端到端动作条件模型，该模型预测未来抓取候选的稳定性和轻柔性，从而允许选择和执行最有希望的动作。在多指手进行的超过1500次抓取试验的实验结果表明，我们的模型通过验证预测性能（比仅视觉变体高3.27%的准确率）并提供其行为解释，对于轻柔抓取非常有用。最后，真实世界实验证实，使用训练后的多模态模型抓取性能优于其他基线（稳定和轻柔抓取率比仅视觉高17%）。我们的方法既不需要触觉传感器校准，也不需要分析力建模，大大减少了抓取脆弱物品的工程量。数据集和视频可在https://lasr.org/research/gentle-grasping获取。", "summary": "本论文提出一种利用视觉、声音和触觉多模态信号学习轻柔抓取的方法。通过将音频作为轻柔度指标，训练一个端到端模型预测抓取的稳定性和轻柔性，从而选择最佳抓取动作。实验证明，该模型在预测准确性和真实世界抓取成功率上均优于仅视觉方法，并且无需复杂的传感器校准或力学建模，显著降低了工程难度，为脆弱物品的机器人抓取提供了有效方案。", "keywords": "轻柔抓取, 多模态学习, 视觉, 声音, 触觉, 机器人抓取", "comments": "该研究的创新之处在于其多模态融合策略，尤其是将声音信号作为抓取轻柔度的重要反馈，这在现有研究中相对较少见。此外，无需传感器校准和力学建模大大降低了实际部署的复杂性，提升了方法的实用性。这对于机器人处理脆弱物品的实际应用具有重要意义。"}}
{"id": "2507.18133", "title": "Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution", "authors": ["Juexin Zhang", "Ying Weng", "Ke Chen"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted by the International Brain Tumor Segmentation (BraTS) challenge organized at MICCAI 2024 conference", "url": "http://arxiv.org/abs/2507.18133v1", "summary": "Glioblastoma, a highly aggressive brain tumor with diverse molecular and\npathological features, poses a diagnostic challenge due to its heterogeneity.\nAccurate diagnosis and assessment of this heterogeneity are essential for\nchoosing the right treatment and improving patient outcomes. Traditional\nmethods rely on identifying specific features in tissue samples, but deep\nlearning offers a promising approach for improved glioblastoma diagnosis. In\nthis paper, we present our approach to the BraTS-Path Challenge 2024. We\nleverage a pre-trained model and fine-tune it on the BraTS-Path training\ndataset. Our model demonstrates poor performance on the challenging BraTS-Path\nvalidation set, as rigorously assessed by the Synapse online platform. The\nmodel achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of\n0.392229, indicating a consistent ability to correctly identify instances under\nthe target condition. Notably, our model exhibits perfect specificity of\n0.898704, showing an exceptional capacity to correctly classify negative cases.\nMoreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,\nto signify a limited positive correlation between predicted and actual values\nand highlight our model's overall predictive power. Our solution also achieves\nthe second place during the testing phase.", "comment": "Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference", "pdf_url": "http://arxiv.org/pdf/2507.18133v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "深度学习用于胶质母细胞瘤形态病理特征识别：BraTS-Pathology 挑战赛解决方案", "tldr": "本文介绍了在BraTS-Path Challenge 2024中使用深度学习识别胶质母细胞瘤形态病理特征的解决方案。该模型在验证集上表现一般，但在测试阶段取得了第二名。", "motivation": "胶质母细胞瘤是一种高度侵袭性的脑肿瘤，其异质性给诊断带来了挑战。准确诊断和评估这种异质性对于选择正确的治疗方案和改善患者预后至关重要。传统方法依赖于组织样本中的特定特征识别，而深度学习为改善胶质母细胞瘤诊断提供了一种有前景的方法。", "method": "本研究利用一个预训练模型，并在BraTS-Path训练数据集上进行微调。", "result": "模型在BraTS-Path验证集上表现不佳，准确率、召回率和F1-score均为0.392229，特异性为0.898704，Matthews相关系数(MCC)为0.255267。然而，在测试阶段，该解决方案获得了第二名。", "conclusion": "尽管模型在验证集上的表现中等，但其在测试阶段获得了第二名，显示了其在特定条件下的识别能力和对阴性病例的优秀分类能力。", "translation": "胶质母细胞瘤是一种高度侵袭性的脑肿瘤，具有多样的分子和病理特征，其异质性带来了诊断挑战。准确诊断和评估这种异质性对于选择正确的治疗方案和改善患者预后至关重要。传统方法依赖于识别组织样本中的特定特征，但深度学习为改善胶质母细胞瘤诊断提供了一种有前景的方法。在本文中，我们介绍了我们参加2024年BraTS-Path挑战赛的方法。我们利用一个预训练模型，并在BraTS-Path训练数据集上进行微调。我们的模型在具有挑战性的BraTS-Path验证集上表现不佳，正如Synapse在线平台严格评估的那样。该模型取得了0.392229的准确率、0.392229的召回率和0.392229的F1分数，这表明在目标条件下正确识别实例的能力一致。值得注意的是，我们的模型表现出0.898704的完美特异性，显示出正确分类阴性病例的卓越能力。此外，计算出的马修斯相关系数（MCC）为0.255267，这表明预测值和实际值之间存在有限的正相关，并突出了我们模型的整体预测能力。我们的解决方案还在测试阶段获得了第二名。", "summary": "本文介绍了参加BraTS-Path Challenge 2024的深度学习解决方案，旨在识别胶质母细胞瘤的形态病理特征。研究利用预训练模型并在训练集上进行微调。尽管模型在验证集上表现一般（准确率、召回率、F1-score均为0.392229，MCC为0.255267），但特异性高达0.898704，并在挑战赛的测试阶段取得了第二名。", "keywords": "胶质母细胞瘤, 深度学习, BraTS-Path, 图像诊断, 挑战赛", "comments": "该论文展示了深度学习在解决胶质母细胞瘤诊断挑战方面的潜力。尽管模型在验证集上的量化指标表现平平，但其在测试阶段获得第二名的成绩表明了其在实际应用中的有效性和鲁棒性。特别值得注意的是模型的高特异性，这对于减少误报至关重要。这篇论文的创新点在于其在竞争性挑战中的实际应用和排名，而非仅仅是理论性能的提升。"}}
{"id": "2507.18155", "title": "GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar", "authors": ["SeungJun Moon", "Hah Min Lew", "Seungeun Lee", "Ji-Su Kang", "Gyeong-Moon Park"], "categories": ["cs.GR", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project page: this https URL", "url": "http://arxiv.org/abs/2507.18155v1", "summary": "Despite recent progress in 3D head avatar generation, balancing identity\npreservation, i.e., reconstruction, with novel poses and expressions, i.e.,\nanimation, remains a challenge. Existing methods struggle to adapt Gaussians to\nvarying geometrical deviations across facial regions, resulting in suboptimal\nquality. To address this, we propose GeoAvatar, a framework for adaptive\ngeometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation\nStage (APS), an unsupervised method that segments Gaussians into rigid and\nflexible sets for adaptive offset regularization. Then, based on mouth anatomy\nand dynamics, we introduce a novel mouth structure and the part-wise\ndeformation strategy to enhance the animation fidelity of the mouth. Finally,\nwe propose a regularization loss for precise rigging between Gaussians and 3DMM\nfaces. Moreover, we release DynamicFace, a video dataset with highly expressive\nfacial motions. Extensive experiments show the superiority of GeoAvatar\ncompared to state-of-the-art methods in reconstruction and novel animation\nscenarios.", "comment": "ICCV 2025, Project page: https://hahminlew.github.io/geoavatar/", "pdf_url": "http://arxiv.org/pdf/2507.18155v1", "cate": "cs.GR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GeoAvatar: 3D头部虚拟形象的自适应几何高斯泼溅", "tldr": "GeoAvatar提出了一种自适应几何高斯泼溅框架，用于生成高质量的3D头部虚拟形象，解决了现有方法在身份保留和新姿态/表情动画之间平衡的挑战。", "motivation": "现有3D头部虚拟形象生成方法在身份保留（重建）与新姿态和表情（动画）之间难以平衡，且难以使高斯适应面部区域的不同几何偏差，导致质量不佳。", "method": "GeoAvatar提出自适应几何高斯泼溅框架，该框架包含：1. 自适应预分配阶段（APS），一种无监督方法，将高斯分割为刚性和柔性集，用于自适应偏移正则化。2. 基于口腔解剖学和动力学，引入新的口腔结构和分部件变形策略，以增强口腔动画保真度。3. 提出正则化损失，实现高斯与3DMM面部之间的精确绑定。此外，还发布了DynamicFace视频数据集。", "result": "GeoAvatar在重建和新动画场景中，与最先进方法相比，显示出优越性。", "conclusion": "GeoAvatar通过其自适应几何高斯泼溅框架，有效解决了3D头部虚拟形象生成中身份保留与动画平衡的挑战，并在质量上超越了现有方法。", "translation": "尽管3D头部虚拟形象生成最近取得了进展，但在身份保留（即重建）与新姿态和表情（即动画）之间取得平衡仍然是一个挑战。现有方法难以使高斯适应面部区域的不同几何偏差，导致质量不佳。为了解决这个问题，我们提出了GeoAvatar，一个自适应几何高斯泼溅框架。GeoAvatar利用自适应预分配阶段（APS），这是一种无监督方法，将高斯分割为刚性和柔性集，用于自适应偏移正则化。然后，基于口腔解剖学和动力学，我们引入了一种新颖的口腔结构和分部件变形策略，以增强口腔的动画保真度。最后，我们提出了一种正则化损失，用于高斯和3DMM面部之间的精确绑定。此外，我们发布了DynamicFace，一个具有高度表现力面部运动的视频数据集。大量实验表明，在重建和新动画场景中，GeoAvatar与最先进的方法相比具有优越性。", "summary": "GeoAvatar是一个用于3D头部虚拟形象的自适应几何高斯泼溅框架，旨在解决现有方法在身份保留和动画之间平衡的挑战。它引入了自适应预分配阶段（APS）进行高斯分割和偏移正则化，并基于口腔解剖学提出了新的口腔结构和分部件变形策略，以及用于精确绑定的正则化损失。实验证明GeoAvatar在重建和动画方面优于现有技术，并发布了DynamicFace数据集。", "keywords": "3D头部虚拟形象, 高斯泼溅, 身份保留, 面部动画, 自适应几何", "comments": "GeoAvatar的创新点在于其自适应几何高斯泼溅框架，特别是APS对高斯的智能分割和针对口腔细节的优化，这对于提升3D头部虚拟形象的真实感和动画质量至关重要。发布新的数据集也对社区有贡献。"}}
{"id": "2305.16901", "title": "Generalizing Adam to Manifolds for Efficiently Training Transformers", "authors": ["Benedikt Brantner"], "categories": ["cs.LG", "math.DG", "53Z50, 53C30, 68T07, 68W10, 90C26"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures (some of which contain subfigures), presented at Enumath2023 and Enumath2025", "url": "http://arxiv.org/abs/2305.16901v4", "summary": "One of the primary reasons behind the success of neural networks has been the\nemergence of an array of new, highly-successful optimizers, perhaps most\nimportantly the Adam optimizer. It is widely used for training neural networks,\nyet notoriously hard to interpret. Lacking a clear physical intuition, Adam is\ndifficult to generalize to manifolds. Some attempts have been made to directly\napply parts of the Adam algorithm to manifolds or to find an underlying\nstructure, but a full generalization has remained elusive.\n  In this work a new approach is presented that leverages the special structure\nof the manifolds which are relevant for optimization of neural networks, such\nas the Stiefel manifold, the symplectic Stiefel manifold and the Grassmann\nmanifold: all of these are homogeneous spaces and as such admit a global\ntangent space representation - a common vector space (Lie subspace) in which\nall tangent spaces can easily be represented.\n  This global tangent space representation is used to perform all of the steps\nin the Adam optimizer and we are able to fully generalize the optimizer to\nmanifolds without a projection step. The resulting algorithm is then applied to\ntrain a transformer for which orthogonality constraints are enforced up to\nmachine precision and we observe significant speed-ups in the training process.", "comment": "32 pages, 6 figures (some of which contain subfigures), presented at\n  Enumath2023 and Enumath2025", "pdf_url": "http://arxiv.org/pdf/2305.16901v4", "cate": "cs.LG", "date": "2023-05-26", "updated": "2025-07-24", "AI": {"title_translation": "将 Adam 泛化到流形以高效训练 Transformer", "tldr": "本文提出了一种将 Adam 优化器完全泛化到流形的新方法，利用齐次空间的全局切空间表示，实现了无需投影步骤的 Adam 优化，并在训练具有正交约束的 Transformer 时观察到显著加速。", "motivation": "Adam 优化器广泛用于训练神经网络，但其缺乏清晰的物理直觉，难以泛化到流形。尽管有一些尝试将 Adam 的部分算法应用于流形或寻找底层结构，但完整的泛化一直未能实现。", "method": "本文提出了一种新方法，利用与神经网络优化相关的流形（如 Stiefel 流形、辛 Stiefel 流形和 Grassmann 流形）的特殊结构。这些流形都是齐次空间，因此允许全局切空间表示——一个共同的向量空间（李子空间），其中所有切空间都可以轻松表示。这种全局切空间表示用于执行 Adam 优化器中的所有步骤，从而能够在没有投影步骤的情况下将优化器完全泛化到流形。", "result": "所提出的算法应用于训练一个 Transformer，其中正交约束被强制执行到机器精度，并且观察到训练过程中的显著加速。", "conclusion": "通过利用齐次流形的全局切空间表示，Adam 优化器可以完全泛化到流形，无需投影步骤，并且在强制正交约束的 Transformer 训练中表现出显著的效率提升。", "translation": "神经网络成功的主要原因之一是出现了一系列新的、非常成功的优化器，其中最重要的可能是 Adam 优化器。它被广泛用于训练神经网络，但出了名地难以解释。由于缺乏清晰的物理直觉，Adam 很难泛化到流形。尽管已经尝试直接将 Adam 算法的一部分应用于流形或寻找底层结构，但完全的泛化仍然难以实现。\n  在这项工作中，提出了一种新方法，利用与神经网络优化相关的流形的特殊结构，例如 Stiefel 流形、辛 Stiefel 流形和 Grassmann 流形：所有这些都是齐次空间，因此允许全局切空间表示——一个共同的向量空间（李子空间），其中所有切空间都可以轻松表示。\n  这种全局切空间表示用于执行 Adam 优化器中的所有步骤，我们能够将优化器完全泛化到流形，而无需投影步骤。然后将所得算法应用于训练一个 Transformer，其中正交约束被强制执行到机器精度，我们观察到训练过程中的显著加速。", "summary": "本研究提出了一种创新方法，将广泛使用的 Adam 优化器完全泛化到流形上，解决了其在流形上应用时缺乏物理直觉和难以泛化的问题。该方法利用了神经网络优化相关流形（如 Stiefel 流形）的齐次空间特性，实现了全局切空间表示，从而在无需投影步骤的情况下执行 Adam 的所有操作。实验结果表明，该算法在训练具有严格正交约束的 Transformer 模型时，实现了显著的训练速度提升。", "keywords": "Adam 优化器, 流形优化, Transformer, 齐次空间, 全局切空间", "comments": "本文提出了一种新颖且重要的 Adam 优化器泛化方法，其核心创新在于利用了齐次流形的全局切空间表示，从而避免了传统方法中复杂的投影步骤。这不仅解决了 Adam 在流形上应用的一大难题，还在实际应用中（如 Transformer 训练）展示了显著的效率提升，对于优化流形上的神经网络具有重要意义。"}}
{"id": "2507.17348", "title": "TOC-UCO: a comprehensive repository of tabular ordinal classification datasets", "authors": ["Rafael Ayllón-Gavilán", "David Guijo-Rubio", "Antonio Manuel Gómez-Orellana", "Francisco Bérchez-Moreno", "Víctor Manuel Vargas-Yun", "Pedro A. Gutiérrez"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 single column pages, 5 figures, 7 tables", "url": "http://arxiv.org/abs/2507.17348v2", "summary": "An ordinal classification (OC) problem corresponds to a special type of\nclassification characterised by the presence of a natural order relationship\namong the classes. This type of problem can be found in a number of real-world\napplications, motivating the design and development of many ordinal\nmethodologies over the last years. However, it is important to highlight that\nthe development of the OC field suffers from one main disadvantage: the lack of\na comprehensive set of datasets on which novel approaches to the literature\nhave to be benchmarked. In order to approach this objective, this manuscript\nfrom the University of C\\'ordoba (UCO), which have previous experience on the\nOC field, provides the literature with a publicly available repository of\ntabular data for a robust validation of novel OC approaches, namely TOC-UCO\n(Tabular Ordinal Classification repository of the UCO). Specifically, this\nrepository includes a set of $46$ tabular ordinal datasets, preprocessed under\na common framework and ensured to have a reasonable number of patterns and an\nappropriate class distribution. We also provide the sources and preprocessing\nsteps of each dataset, along with details on how to benchmark a novel approach\nusing the TOC-UCO repository. For this, indices for $30$ different randomised\ntrain-test partitions are provided to facilitate the reproducibility of the\nexperiments.", "comment": "25 single column pages, 5 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.17348v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "TOC-UCO：一个综合性的表格序数分类数据集库", "tldr": "TOC-UCO是一个新的公开可用的表格序数分类数据集库，包含46个预处理数据集，旨在解决序数分类领域缺乏综合性基准数据集的问题，并促进新方法的验证和实验可复现性。", "motivation": "序数分类（OC）问题在许多现实世界应用中存在，并促使了多种序数方法的开发。然而，OC领域的主要劣势是缺乏一个综合性的数据集集合，用于对新的文献方法进行基准测试。", "method": "本手稿提供了一个名为TOC-UCO（UCO的表格序数分类库）的公开可用表格数据存储库。该存储库包含46个表格序数数据集，这些数据集在共同框架下进行预处理，并确保具有合理的模式数量和适当的类别分布。同时提供了每个数据集的来源、预处理步骤以及如何使用TOC-UCO存储库对新方法进行基准测试的详细信息。为了便于实验的可复现性，还提供了30个不同随机训练-测试分区的索引。", "result": "TOC-UCO存储库提供了46个经过预处理的表格序数数据集，解决了序数分类领域缺乏综合性基准数据集的问题。它提供了用于鲁棒验证新OC方法的资源，并支持通过提供训练-测试分区索引来促进实验的可复现性。", "conclusion": "TOC-UCO存储库通过提供一个综合性的、公开可用的表格序数分类数据集集合，显著解决了序数分类领域中缺乏标准基准数据集的问题，从而促进了新方法的开发、验证和实验的可复现性。", "translation": "序数分类（OC）问题是一种特殊类型的分类，其特点是类别之间存在自然的顺序关系。这种类型的问题可以在许多现实世界应用中找到，这促使了过去几年中许多序数方法的创建和发展。然而，重要的是要强调，OC领域的发展面临一个主要劣势：缺乏一个综合性的数据集集合，用于对文献中的新方法进行基准测试。为了实现这一目标，这份来自科尔多瓦大学（UCO）的手稿，凭借其在OC领域的先前经验，为文献提供了一个公开可用的表格数据存储库，用于对新的OC方法进行鲁棒验证，即TOC-UCO（UCO的表格序数分类存储库）。具体来说，该存储库包含46个表格序数数据集，这些数据集在一个共同的框架下进行预处理，并确保具有合理的模式数量和适当的类别分布。我们还提供了每个数据集的来源和预处理步骤，以及如何使用TOC-UCO存储库对新方法进行基准测试的详细信息。为此，提供了30个不同随机训练-测试分区的索引，以方便实验的可复现性。", "summary": "本文介绍了TOC-UCO，一个由科尔多瓦大学（UCO）创建的综合性公开表格序数分类数据集存储库。该存储库旨在解决序数分类（OC）领域缺乏标准基准数据集的问题，这对新方法的开发和验证构成了障碍。TOC-UCO包含46个经过统一预处理的表格序数数据集，并提供了详细的来源、预处理步骤以及用于基准测试的30个随机训练-测试分区索引，从而促进了新OC方法的鲁棒验证和实验的可复现性。", "keywords": "序数分类, 数据集, 存储库, 基准测试, 可复现性", "comments": "TOC-UCO的创新之处在于它直接解决了序数分类领域的一个关键痛点：缺乏一个标准化的、全面的数据集集合。通过提供一个包含46个预处理数据集的存储库，并附带详细的元数据和可复现性工具（如训练-测试分区），该工作极大地降低了研究人员验证新OC方法的门槛，并有助于建立更公平、更一致的基准测试环境。其重要性在于，它可能成为OC研究领域的一个标准资源，加速该领域的发展。"}}
{"id": "2507.17776", "title": "Axiomatizing Rumsfeld Ignorance", "authors": ["Jie Fan"], "categories": ["math.LO", "cs.AI"], "primary_category": "Subjects:       Logic (math.LO)", "pdf_link": null, "comments": "Comments:      This is an almost-final version", "url": "http://arxiv.org/abs/2507.17776v1", "summary": "In a recent paper, Kit Fine presents some striking results concerning the\nlogical properties of (first-order) ignorance, second-order ignorance and\nRumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of\nignorance, which makes some existing results and the axiomatization problem\ntrivial. A main reason is that the accessibility relations for the implicit\nknowledge operator contained in the packaged operators of ignorance and\nRumsfeld ignorance are the same. In this work, we assume the two accessibility\nrelations to be different so that one of them is an arbitrary subset of the\nother. This will avoid the definability issue and retain most of the previous\nvalidities. The main results are axiomatizations over various proper bi-frame\nclasses. Finally we apply our framework to analyze Fine's results.", "comment": "This is an almost-final version", "pdf_url": "http://arxiv.org/pdf/2507.17776v1", "cate": "math.LO", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "鲁姆斯菲尔德无知的公理化", "tldr": "本文通过假设无知和鲁姆斯菲尔德无知中隐含知识算子的可达关系不同，解决了鲁姆斯菲尔德无知公理化中的可定义性问题，并给出了在各种双框架类上的公理化结果。", "motivation": "Kit Fine 在最近的论文中提出了关于（一阶）无知、二阶无知和鲁姆斯菲尔德无知逻辑性质的显著结果。然而，鲁姆斯菲尔德无知可以通过无知来定义，这使得一些现有结果和公理化问题变得微不足道。主要原因是无知和鲁姆斯菲尔德无知这些封装算子中包含的隐含知识算子的可达关系是相同的。", "method": "在本文中，作者假设无知和鲁姆斯菲尔德无知中隐含知识算子的两个可达关系是不同的，其中一个关系是另一个关系的任意子集。这种方法避免了可定义性问题，并保留了大部分先前的有效性。", "result": "主要结果是在各种适当的双框架类（proper bi-frame classes）上的公理化。该方法成功避免了可定义性问题，并保留了大部分先前的有效性。", "conclusion": "通过假设无知和鲁姆斯菲尔德无知中隐含知识算子的可达关系不同，本文成功解决了鲁姆斯菲尔德无知公理化的可定义性问题，并提供了在各种适当的双框架类上的非平凡公理化。最终，该框架被应用于分析Fine的结果。", "translation": "在最近的一篇论文中，Kit Fine 提出了关于（一阶）无知、二阶无知和鲁姆斯菲尔德无知逻辑性质的一些显著结果。然而，鲁姆斯菲尔德无知可以通过无知来定义，这使得一些现有结果和公理化问题变得微不足道。主要原因是无知和鲁姆斯菲尔德无知这些封装算子中包含的隐含知识算子的可达关系是相同的。在这项工作中，我们假设这两个可达关系是不同的，其中一个关系是另一个关系的任意子集。这将避免可定义性问题，并保留大部分先前的有效性。主要结果是在各种适当的双框架类上的公理化。最后，我们将我们的框架应用于分析 Fine 的结果。", "summary": "本研究旨在解决Kit Fine关于鲁姆斯菲尔德无知公理化中的一个核心问题，即鲁姆斯菲尔德无知可由一阶无知定义，导致公理化问题变得琐碎。其根本原因在于两种无知形式中隐含知识算子的可达关系相同。为解决此问题，本文提出了一种新颖的方法，假设这两种可达关系不同，其中一个关系是另一个的任意子集。这种方法不仅成功避免了可定义性问题，还保留了先前研究中的大部分有效性。研究的主要成果是在多种适当双框架类上实现的公理化，并且该框架被进一步应用于重新分析Fine的原始结果。", "keywords": "鲁姆斯菲尔德无知, 公理化, 无知, 可达关系, 双框架类", "comments": "本文通过对无知和鲁姆斯菲尔德无知中隐含知识算子的可达关系进行区分，巧妙地解决了现有公理化体系中的“琐碎性”问题。这种对基本概念的精细化处理是其创新之处，使得鲁姆斯菲尔德无知能够得到更具实质意义的公理化。其重要性在于为知识逻辑领域提供了一个更严谨、更具解释力的理论框架。"}}
{"id": "2507.18242", "title": "Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods", "authors": ["Fabian Akkerman", "Julien Ferry", "Christian Artigues", "Emmanuel Hebrard", "Thibaut Vidal"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18242v1", "summary": "Despite their theoretical appeal, totally corrective boosting methods based\non linear programming have received limited empirical attention. In this paper,\nwe conduct the first large-scale experimental study of six LP-based boosting\nformulations, including two novel methods, NM-Boost and QRLP-Boost, across 20\ndiverse datasets. We evaluate the use of both heuristic and optimal base\nlearners within these formulations, and analyze not only accuracy, but also\nensemble sparsity, margin distribution, anytime performance, and hyperparameter\nsensitivity. We show that totally corrective methods can outperform or match\nstate-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,\nwhile producing significantly sparser ensembles. We further show that these\nmethods can thin pre-trained ensembles without sacrificing performance, and we\nhighlight both the strengths and limitations of using optimal decision trees in\nthis context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18242v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "再探Boosting：基於線性規劃的集成方法基準測試與改進", "tldr": "本文對基於線性規劃的完全校正Boosting方法進行了首次大規模實驗研究，引入了兩種新方法，並證明它們在使用淺層樹時能媲美甚至超越XGBoost和LightGBM，同時產生更稀疏的集成模型，並能對預訓練模型進行剪枝。", "motivation": "儘管基於線性規劃的完全校正Boosting方法具有理論吸引力，但它們在實證研究中受到的關注有限。", "method": "進行了首次大規模實驗研究，評估了六種基於線性規劃的Boosting公式（包括兩種新方法NM-Boost和QRLP-Boost），涉及20個不同數據集。研究評估了啟發式和最優基礎學習器，並分析了準確性、集成稀疏性、邊際分佈、即時性能和超參數敏感性。", "result": "結果顯示，在使用淺層樹時，完全校正方法可以超越或媲美XGBoost和LightGBM等最先進的啟發式方法，同時產生顯著更稀疏的集成模型。這些方法還可以在不犧牲性能的情況下對預訓練的集成模型進行剪枝。", "conclusion": "本文強調了在該背景下使用最優決策樹的優點和局限性，證明了LP-based Boosting方法的潛力。", "translation": "儘管基於線性規劃的完全校正Boosting方法具有理論吸引力，但它們在實證研究中受到的關注有限。在本文中，我們對六種基於線性規劃的Boosting公式（包括兩種新方法NM-Boost和QRLP-Boost）進行了首次大規模實驗研究，涉及20個不同數據集。我們評估了在這些公式中使用啟發式和最優基礎學習器，不僅分析了準確性，還分析了集成稀疏性、邊際分佈、即時性能和超參數敏感性。我們表明，當使用淺層樹時，完全校正方法可以超越或媲美XGBoost和LightGBM等最先進的啟發式方法，同時產生顯著更稀疏的集成模型。我們進一步表明，這些方法可以在不犧牲性能的情況下對預訓練的集成模型進行剪枝，並且我們強調了在這種情況下使用最優決策樹的優點和局限性。", "summary": "本研究首次對基於線性規劃的完全校正Boosting方法進行了大規模實證分析，涉及六種公式，包括兩種新提出的NM-Boost和QRLP-Boost。實驗在20個數據集上進行，評估了不同基礎學習器並分析了多項性能指標。結果表明，這些方法在使用淺層樹時能達到或超越XGBoost和LightGBM的性能，同時生成更稀疏的模型，並能有效剪枝預訓練模型。研究還探討了使用最優決策樹的優缺點。", "keywords": "Boosting, 線性規劃, 集成學習, 基準測試, 稀疏性", "comments": "這篇論文填補了基於線性規劃的Boosting方法在實證研究方面的空白，通過大規模實驗證明了其與現有SOTA方法的競爭力，尤其是在生成稀疏模型方面的優勢。引入兩種新方法並深入分析多種性能指標，使其具有較高的研究價值和實踐指導意義。對最優決策樹的討論也為未來的研究提供了方向。"}}
{"id": "2507.18560", "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven Sentiment Integration for Financial Portfolio Optimization", "authors": ["Benjamin Coriat", "Eric Benhamou"], "categories": ["q-fin.PM", "cs.AI"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18560v1", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18560v1", "cate": "q-fin.PM", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "HARLF：层次强化学习与轻量级LLM驱动的情绪整合在金融投资组合优化中的应用", "tldr": "本文提出HARLF框架，结合层次强化学习和轻量级LLM进行金融投资组合优化，通过整合金融新闻情感信号和市场指标，实现了26%的年化收益和1.2的夏普比率，优于基准。", "motivation": "将金融新闻中的情感信号与传统市场指标相结合，以改进金融投资组合优化。", "method": "本文提出了HARLF分层框架，整合了轻量级大型语言模型（LLM）与深度强化学习（DRL）。其三层架构包括：基础RL代理处理混合数据，元代理聚合其决策，以及超级代理基于市场数据和情感分析合并决策。", "result": "在2018年至2024年的数据上评估，该框架实现了26%的年化收益和1.2的夏普比率，表现优于等权重和S&P 500基准。", "conclusion": "HARLF框架通过结合层次强化学习和轻量级LLM，有效整合情感信号和市场指标，显著提升了金融投资组合优化表现，并具有可扩展性、增强的稳定性和可复现性。", "translation": "本文提出了一种新颖的投资组合优化分层框架，将轻量级大型语言模型（LLM）与深度强化学习（DRL）相结合，以整合金融新闻中的情感信号和传统市场指标。我们的三层架构采用基础RL代理来处理混合数据，元代理来聚合其决策，以及一个超级代理基于市场数据和情感分析来合并决策。该框架在2000-2017年训练后，于2018年至2024年的数据上进行评估，实现了26%的年化收益和1.2的夏普比率，优于等权重和S&P 500基准。主要贡献包括可扩展的跨模态集成、用于增强稳定性的分层RL结构以及开源可复现性。", "summary": "本文提出了HARLF，一个用于金融投资组合优化的新型分层框架。该框架将轻量级大型语言模型（LLM）与深度强化学习（DRL）相结合，以整合金融新闻的情感信号和传统市场指标。其三层架构包括基础RL代理、元代理和超级代理，分别处理数据、聚合决策和合并决策。在2018-2024年的数据上，该框架实现了26%的年化收益和1.2的夏普比率，表现优于基准。主要贡献在于其可扩展的跨模态集成、增强稳定性的分层RL结构以及开源可复现性。", "keywords": "金融投资组合优化, 强化学习, 大型语言模型, 情感分析, 层次结构", "comments": "该论文创新性地将轻量级LLM与层次强化学习结合，用于金融投资组合优化，实现了跨模态数据（情感信号与市场指标）的有效集成。其三层架构设计提升了决策的稳定性和准确性，且优异的业绩表现（26%年化收益和1.2夏普比率）证明了其实用价值。开源可复现性也增加了其研究和应用潜力。"}}
{"id": "2507.18569", "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis", "authors": ["Yanzuo Lu", "Yuxi Ren", "Xin Xia", "Shanchuan Lin", "Xing Wang", "Xuefeng Xiao", "Andy J. Ma", "Xiaohua Xie", "Jian-Huang Lai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2507.18569v1", "summary": "Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis.", "comment": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2507.18569v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于扩散蒸馏的对抗性分布匹配，实现高效图像和视频合成", "tldr": "本文提出对抗性分布匹配（ADM）框架，通过对抗训练改进扩散蒸馏，以解决模式崩溃问题并提高图像和视频合成效率。", "motivation": "现有分布匹配蒸馏（DMD）依赖反向Kullback-Leibler（KL）散度最小化，在某些应用中可能导致模式崩溃（或模式寻求）。", "method": "提出对抗性分布匹配（ADM）框架，利用基于扩散的判别器以对抗方式对齐真实和伪分数估计器之间的潜在预测进行分数蒸馏。在一步蒸馏中，通过在潜在和像素空间中使用混合判别器进行对抗蒸馏，并引入从教师模型收集的ODE对上的分布损失，为分数蒸馏微调提供更好的初始化。将对抗蒸馏预训练与ADM微调结合成统一的DMDX管道。", "result": "在SDXL上实现优于DMD2的一步性能，同时消耗更少的GPU时间。在SD3-Medium、SD3.5-Large和CogVideoX上应用多步ADM蒸馏，为高效图像和视频合成树立了新基准。", "conclusion": "对抗性分布匹配（ADM）框架通过引入对抗性训练和分布损失，有效解决了传统DMD的模式崩溃问题，显著提升了扩散模型蒸馏的效率和性能，尤其是在一步生成和视频合成方面。", "translation": "分布匹配蒸馏（DMD）是一种有前景的分数蒸馏技术，能将预训练的教师扩散模型压缩成高效的一步或多步学生生成器。然而，其对反向Kullback-Leibler（KL）散度最小化的依赖，在某些应用中可能导致模式崩溃（或模式寻求）。为了规避这一固有缺点，我们提出了对抗性分布匹配（ADM），这是一种新颖的框架，它利用基于扩散的判别器，以对抗方式对齐真实和伪分数估计器之间的潜在预测，以进行分数蒸馏。在极具挑战性的一步蒸馏背景下，我们通过在潜在和像素空间中的混合判别器进行对抗蒸馏，进一步改进了预训练生成器。与DMD2预训练中使用的均方误差不同，我们的方法结合了从教师模型收集的ODE对上的分布损失，从而为下一阶段的分数蒸馏微调提供了更好的初始化。通过将对抗蒸馏预训练与ADM微调结合成一个名为DMDX的统一管道，我们提出的方法在SDXL上实现了优于DMD2的一步性能，同时消耗更少的GPU时间。在SD3-Medium、SD3.5-Large和CogVideoX上应用多步ADM蒸馏的额外实验，为高效图像和视频合成树立了新基准。", "summary": "本文提出对抗性分布匹配（ADM）框架，旨在解决传统分布匹配蒸馏（DMD）中存在的模式崩溃问题。ADM利用基于扩散的判别器进行对抗性分数蒸馏，并在一步蒸馏中结合混合判别器和分布损失。通过统一的DMDX管道，ADM在SDXL上实现了更优的一步生成性能，并为高效图像和视频合成设立了新基准。", "keywords": "扩散蒸馏, 对抗性分布匹配, 图像合成, 视频合成, 模式崩溃", "comments": "本文通过引入对抗性训练和分布损失，创新性地解决了扩散模型蒸馏中模式崩溃的难题，显著提升了生成效率和质量。其统一的DMDX管道在一步生成和视频合成方面表现出色，为未来高效扩散模型的开发提供了重要方向。"}}
{"id": "2505.20147", "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities", "authors": ["Jin Wang", "Yao Lai", "Aoxue Li", "Shifeng Zhang", "Jiacheng Sun", "Ning Kang", "Chengyue Wu", "Zhenguo Li", "Ping Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      37 pages, 12 figures", "url": "http://arxiv.org/abs/2505.20147v3", "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.", "comment": "37 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2505.20147v3", "cate": "cs.CV", "date": "2025-05-26", "updated": "2025-07-24", "AI": {"title_translation": "FUDOKI：基于离散流的统一理解与生成通过动力学最优速度", "tldr": "FUDOKI引入了一种基于离散流的多模态模型，统一了理解和生成，作为自回归模型的替代方案，并取得了可比的性能。", "motivation": "现有的大型多模态语言模型（MLLMs）主要依赖自回归（AR）架构，这在图像生成中存在光栅扫描顺序以及在因果上下文建模中推理能力受限等固有局限性，阻碍了未来的发展。", "method": "FUDOKI是一个纯粹基于离散流匹配的统一多模态模型，作为传统自回归范式的替代。它利用度量诱导的概率路径和动力学最优速度，超越了之前基于掩码的损坏过程，实现了迭代细化、自校正能力和更丰富的双向上下文集成。为了降低从头训练的高成本，FUDOKI从预训练的基于自回归的MLLMs初始化，并自适应地过渡到离散流匹配范式。", "result": "FUDOKI在视觉理解和图像生成任务中取得了与最先进的基于自回归的MLLMs相当的性能。此外，对FUDOKI应用测试时缩放技术可显著提升性能。", "conclusion": "FUDOKI展示了作为下一代统一多模态模型基础的潜力，并通过强化学习在未来增强方面具有广阔前景。", "translation": "大型语言模型（LLMs）的快速发展催生了多模态大型语言模型（MLLMs）的出现，这些模型在单一框架内统一了视觉理解和图像生成。然而，大多数现有MLLMs依赖于自回归（AR）架构，这给未来的发展带来了固有限制，例如图像生成中的光栅扫描顺序以及因果上下文建模中受限的推理能力。在这项工作中，我们通过引入FUDOKI，一个纯粹基于离散流匹配的统一多模态模型，作为传统自回归范式的替代，挑战了基于自回归方法的主导地位。通过利用具有动力学最优速度的度量诱导概率路径，我们的框架超越了之前基于掩码的损坏过程，实现了迭代细化、自校正能力以及在生成过程中更丰富的双向上下文集成。为了减轻从头训练的高成本，我们从预训练的基于自回归的MLLMs初始化FUDOKI，并自适应地过渡到离散流匹配范式。实验结果表明，FUDOKI在视觉理解和图像生成任务中取得了与最先进的基于自回归的MLLMs相当的性能，突显了其作为下一代统一多模态模型基础的潜力。此外，我们表明，将测试时缩放技术应用于FUDOKI可产生显著的性能提升，进一步强调了其通过强化学习在未来增强方面的希望。", "summary": "FUDOKI提出了一种新颖的基于离散流的统一多模态模型，旨在解决当前自回归多模态大型语言模型（MLLMs）的局限性。该模型通过利用离散流匹配和动力学最优速度，实现了迭代细化和改进的双向上下文集成。FUDOKI在视觉理解和图像生成任务中展现出与最先进自回归模型相当的性能，预示着其作为未来多模态人工智能基础的巨大潜力。", "keywords": "离散流匹配, 多模态模型, 统一理解, 图像生成, 动力学最优速度", "comments": "FUDOKI引入了离散流匹配作为多模态大型语言模型（MLLMs）中主导的自回归范式的有前景的替代方案，解决了其固有的局限性。它能够在非自回归框架内统一理解和生成，并结合具有竞争力的性能和未来增强的潜力，这标志着该领域的一项重要创新。"}}
{"id": "2312.12102", "title": "I-CEE: Tailoring Explanations of Image Classification Models to User Expertise", "authors": ["Yao Rong", "Peizhu Qian", "Vaibhav Unhelkar", "Enkelejda Kasneci"], "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.12102v3", "summary": "Effectively explaining decisions of black-box machine learning models is\ncritical to responsible deployment of AI systems that rely on them. Recognizing\ntheir importance, the field of explainable AI (XAI) provides several techniques\nto generate these explanations. Yet, there is relatively little emphasis on the\nuser (the explainee) in this growing body of work and most XAI techniques\ngenerate \"one-size-fits-all\" explanations. To bridge this gap and achieve a\nstep closer towards human-centered XAI, we present I-CEE, a framework that\nprovides Image Classification Explanations tailored to User Expertise. Informed\nby existing work, I-CEE explains the decisions of image classification models\nby providing the user with an informative subset of training data (i.e.,\nexample images), corresponding local explanations, and model decisions.\nHowever, unlike prior work, I-CEE models the informativeness of the example\nimages to depend on user expertise, resulting in different examples for\ndifferent users. We posit that by tailoring the example set to user expertise,\nI-CEE can better facilitate users' understanding and simulatability of the\nmodel. To evaluate our approach, we conduct detailed experiments in both\nsimulation and with human participants (N = 100) on multiple datasets.\nExperiments with simulated users show that I-CEE improves users' ability to\naccurately predict the model's decisions (simulatability) compared to\nbaselines, providing promising preliminary results. Experiments with human\nparticipants demonstrate that our method significantly improves user\nsimulatability accuracy, highlighting the importance of human-centered XAI", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.12102v3", "cate": "cs.AI", "date": "2023-12-19", "updated": "2025-07-24", "AI": {"title_translation": "I-CEE：根据用户专业知识定制图像分类模型的解释", "tldr": "I-CEE是一个为图像分类模型提供解释的框架，它根据用户的专业知识定制解释，通过提供不同的训练样本子集来提高用户对模型决策的理解和模拟能力。", "motivation": "目前的解释性AI（XAI）技术大多生成“一刀切”的解释，很少关注用户（被解释者）的专业知识。为了弥补这一空白并实现以人为中心的XAI，本文提出了I-CEE框架。", "method": "I-CEE是一个框架，它通过提供训练数据的有益子集（即示例图像）、相应的局部解释和模型决策来解释图像分类模型的决策。与现有工作不同，I-CEE根据用户专业知识来建模示例图像的信息量，从而为不同用户提供不同的示例集。", "result": "模拟用户实验表明，与基线相比，I-CEE提高了用户准确预测模型决策的能力（可模拟性），提供了有希望的初步结果。人类参与者（N=100）实验表明，该方法显著提高了用户可模拟性准确性。", "conclusion": "通过根据用户专业知识定制示例集，I-CEE可以更好地促进用户对模型的理解和可模拟性，强调了以人为中心的XAI的重要性。", "translation": "有效解释黑盒机器学习模型的决策对于负责任地部署依赖它们的AI系统至关重要。认识到它们的重要性，可解释AI（XAI）领域提供了几种生成这些解释的技术。然而，在这日益增长的工作中，对用户（被解释者）的重视相对较少，大多数XAI技术生成“一刀切”的解释。为了弥补这一差距，并向以人为中心的XAI迈进一步，我们提出了I-CEE，一个根据用户专业知识定制图像分类解释的框架。受现有工作的启发，I-CEE通过向用户提供训练数据的有用子集（即示例图像）、相应的局部解释和模型决策来解释图像分类模型的决策。然而，与之前的工作不同，I-CEE将示例图像的信息量建模为取决于用户专业知识，从而为不同用户提供不同的示例。我们认为，通过根据用户专业知识定制示例集，I-CEE可以更好地促进用户对模型的理解和可模拟性。为了评估我们的方法，我们在模拟和人类参与者（N=100）的多数据集上进行了详细实验。模拟用户实验表明，与基线相比，I-CEE提高了用户准确预测模型决策的能力（可模拟性），提供了有希望的初步结果。人类参与者实验表明，我们的方法显著提高了用户可模拟性准确性，突出了以人为中心的XAI的重要性。", "summary": "I-CEE是一个以用户为中心的解释性AI（XAI）框架，旨在解决现有XAI技术“一刀切”的局限性。它通过根据用户的专业知识定制图像分类模型的解释，提供个性化的训练数据示例子集、局部解释和模型决策。该方法通过将示例图像的信息量与用户专业知识关联起来，为不同用户提供不同的解释。通过模拟和人类参与者实验，研究表明I-CEE显著提高了用户对模型决策的理解和可模拟性，验证了以人为中心的XAI的重要性。", "keywords": "可解释AI, 用户专业知识, 图像分类, 个性化解释, 可模拟性", "comments": "本文的创新点在于提出了I-CEE框架，它突破了传统XAI“一刀切”的解释模式，首次将用户专业知识纳入解释生成过程，实现了个性化解释。这种以人为中心的方法对于提高AI系统的透明度、可信度和用户采纳度具有重要意义。通过提供定制化的解释，I-CEE有望显著提升用户对复杂模型行为的理解和预测能力。"}}
{"id": "2507.17977", "title": "Improving the Computational Efficiency and Explainability of GeoAggregator", "authors": ["Rui Deng", "Ziqi Li", "Mingshu Wang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      4 pages, 3 figures", "url": "http://arxiv.org/abs/2507.17977v1", "summary": "Accurate modeling and explaining geospatial tabular data (GTD) are critical\nfor understanding geospatial phenomena and their underlying processes. Recent\nwork has proposed a novel transformer-based deep learning model named\nGeoAggregator (GA) for this purpose, and has demonstrated that it outperforms\nother statistical and machine learning approaches. In this short paper, we\nfurther improve GA by 1) developing an optimized pipeline that accelerates the\ndataloading process and streamlines the forward pass of GA to achieve better\ncomputational efficiency; and 2) incorporating a model ensembling strategy and\na post-hoc model explanation function based on the GeoShapley framework to\nenhance model explainability. We validate the functionality and efficiency of\nthe proposed strategies by applying the improved GA model to synthetic\ndatasets. Experimental results show that our implementation improves the\nprediction accuracy and inference speed of GA compared to the original\nimplementation. Moreover, explanation experiments indicate that GA can\neffectively captures the inherent spatial effects in the designed synthetic\ndataset. The complete pipeline has been made publicly available for community\nuse (https://github.com/ruid7181/GA-sklearn).", "comment": "4 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.17977v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "提高 GeoAggregator 的计算效率和可解释性", "tldr": "本文通过优化管道和引入集成与GeoShapley框架，显著提升了GeoAggregator（GA）的计算效率和可解释性，并在合成数据集上验证了其预测准确性和推理速度的提升。", "motivation": "准确建模和解释地理空间表格数据（GTD）对于理解地理空间现象及其底层过程至关重要。GeoAggregator（GA）模型已被提出并证明优于其他方法，但仍有提升空间，尤其是在计算效率和模型可解释性方面。", "method": "通过开发优化的数据加载和前向传播管道来提高计算效率；通过结合模型集成策略和基于GeoShapley框架的后验模型解释功能来增强模型可解释性。", "result": "实验结果显示，改进后的实现提高了GA的预测准确性和推理速度。解释实验表明GA能有效捕获合成数据集中固有的空间效应。", "conclusion": "本文成功提升了GeoAggregator模型的计算效率和可解释性，并在合成数据集上验证了其性能的提升以及对空间效应的有效捕捉。完整的实现已公开可用。", "translation": "准确建模和解释地理空间表格数据（GTD）对于理解地理空间现象及其底层过程至关重要。最近的工作为此目的提出了一种名为 GeoAggregator (GA) 的新型基于Transformer的深度学习模型，并已证明其优于其他统计和机器学习方法。在这篇短论文中，我们通过以下方式进一步改进了 GA：1）开发了一个优化的管道，该管道加速了数据加载过程并简化了 GA 的前向传播，以实现更好的计算效率；2）结合了模型集成策略和基于 GeoShapley 框架的后验模型解释功能，以增强模型的可解释性。我们通过将改进后的 GA 模型应用于合成数据集来验证所提出策略的功能性和效率。实验结果表明，与原始实现相比，我们的实现提高了 GA 的预测准确性和推理速度。此外，解释实验表明 GA 可以有效地捕获所设计的合成数据集中固有的空间效应。完整的管道已公开可用，供社区使用（https://github.com/ruid7181/GA-sklearn）。", "summary": "本文针对GeoAggregator（GA）模型在处理地理空间表格数据（GTD）方面的不足，致力于提升其计算效率和可解释性。通过优化数据加载和前向传播管道，显著加速了模型运行；同时，引入模型集成策略和基于GeoShapley的后验解释功能，增强了模型的透明度。在合成数据集上的实验验证了改进后的GA在预测准确性和推理速度上的优势，并证明了其能够有效捕捉空间效应。完整的实现已开源，便于社区使用。", "keywords": "GeoAggregator, 地理空间数据, 计算效率, 可解释性, 深度学习", "comments": "本文针对地理空间表格数据处理中的关键模型GeoAggregator进行了重要改进。其创新点在于同时关注了计算效率和模型可解释性，通过具体的工程优化和引入GeoShapley框架，解决了深度学习模型在实际应用中常见的效率和“黑箱”问题。将完整管道开源的举措，极大地促进了研究成果的社区共享和应用。"}}
{"id": "2308.09954", "title": "DocTER: Evaluating Document-based Knowledge Editing", "authors": ["Suhang Wu", "Ante Wang", "Minlong Peng", "Yujie Lin", "Wenbo Li", "Mingming Sun", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Information processing & management", "url": "http://arxiv.org/abs/2308.09954v2", "summary": "Knowledge editing aims to correct outdated or inaccurate knowledge in neural\nnetworks. In this paper, we explore knowledge editing using easily accessible\ndocuments instead of manually labeled factual triples employed in earlier\nresearch. To advance this field, we establish the first evaluation benchmark,\n\\textit{DocTER}, featuring Documents containing counterfactual knowledge for\nediting. A comprehensive four-perspective evaluation is introduced: Edit\nSuccess, Locality, Reasoning, and Cross-lingual Transfer. To adapt conventional\ntriplet-based knowledge editing methods for this task, we develop an\nExtract-then-Edit pipeline that extracts triples from documents before applying\nexisting methods. Experiments on popular knowledge editing methods demonstrate\nthat editing with documents presents significantly greater challenges than\nusing triples. In document-based scenarios, even the best-performing in-context\nediting approach still lags behind by 10 points in editing success when\ncompared to using gold triples. This observation also holds for both reasoning\nand cross-lingual test sets. We further analyze key factors influencing task\nperformance, including the quality of extracted triples, the frequency and\nposition of edited knowledge in documents, various methods for enhancing\nreasoning, and performance differences across various directions in\ncross-lingual knowledge editing, which provide valuable insights for future\nresearch.", "comment": "Information processing & management", "pdf_url": "http://arxiv.org/pdf/2308.09954v2", "cate": "cs.CL", "date": "2023-08-19", "updated": "2025-07-24", "AI": {"title_translation": "DocTER：评估基于文档的知识编辑", "tldr": "该研究提出了首个用于文档级知识编辑的评估基准DocTER，并发现与使用事实三元组相比，基于文档的知识编辑更具挑战性。", "motivation": "现有的知识编辑方法主要依赖手动标注的事实三元组，而本文旨在探索使用易于获取的文档进行知识编辑，并为该领域建立一个统一的评估基准。", "method": "建立了首个文档级知识编辑评估基准DocTER，包含用于编辑的反事实知识文档。引入了四维评估体系：编辑成功率、局部性、推理和跨语言迁移。开发了“提取-然后-编辑”管道，将文档中的三元组提取出来，然后应用现有的知识编辑方法。", "result": "实验表明，使用文档进行知识编辑比使用三元组更具挑战性。即使是表现最佳的上下文编辑方法，在文档场景下的编辑成功率也比使用金标准三元组低10个百分点，这一现象在推理和跨语言测试集上也成立。分析了影响任务性能的关键因素，如提取三元组的质量、编辑知识在文档中的频率和位置、增强推理的方法以及跨语言知识编辑在不同方向上的性能差异。", "conclusion": "基于文档的知识编辑比基于三元组的知识编辑更具挑战性，并且现有方法在文档场景下表现不佳。研究结果为未来的文档级知识编辑研究提供了有价值的见解。", "translation": "知识编辑旨在纠正神经网络中过时或不准确的知识。本文探索使用易于获取的文档进行知识编辑，而非早期研究中采用的手动标注的事实三元组。为了推动该领域发展，我们建立了首个评估基准DocTER，其特点是包含用于编辑的反事实知识文档。引入了全面的四视角评估：编辑成功率、局部性、推理和跨语言迁移。为了使传统的基于三元组的知识编辑方法适应这项任务，我们开发了一个“提取-然后-编辑”管道，即在应用现有方法之前从文档中提取三元组。对流行的知识编辑方法进行的实验表明，使用文档进行编辑比使用三元组提出了显著更大的挑战。在基于文档的场景中，即使是表现最佳的上下文编辑方法，其编辑成功率仍比使用金标准三元组低10个百分点。这一观察结果在推理和跨语言测试集上也成立。我们进一步分析了影响任务性能的关键因素，包括提取三元组的质量、编辑知识在文档中的频率和位置、各种增强推理的方法以及跨语言知识编辑在不同方向上的性能差异，这些为未来的研究提供了宝贵的见解。", "summary": "该论文介绍了DocTER，一个用于评估基于文档的知识编辑的新基准，旨在解决现有方法依赖手动标注三元组的问题。研究团队提出了一个包含反事实知识文档的DocTER基准，并引入了四维评估体系（编辑成功率、局部性、推理、跨语言迁移）。他们还开发了“提取-然后-编辑”管道，以适应现有方法。实验结果表明，基于文档的知识编辑比基于三元组的编辑更具挑战性，现有方法的性能显著下降。论文还分析了影响性能的关键因素，为未来的研究提供了方向。", "keywords": "知识编辑, 文档级, 评估基准, DocTER, 提取-然后-编辑", "comments": "DocTER是知识编辑领域的一个重要进展，它首次为基于文档的知识编辑建立了统一的评估基准，填补了该领域的空白。其提出的“提取-然后-编辑”管道虽然初步，但为现有方法适应新范式提供了思路。研究结果清晰地指出了文档级知识编辑的挑战性，并深入分析了影响性能的关键因素，为未来的研究指明了方向，具有重要的实践和理论价值。"}}
{"id": "2507.17766", "title": "Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release", "authors": ["Felix Quinque", "Alan Aboudib", "Szymon Fonau", "Rodrigo Lopez Portillo Alcocer", "Brian McCrindle", "Steffen Cruz"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17766v1", "summary": "In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed\nnetwork of incentivized, permissionless actors could each pretrain large\nlanguage models (LLMs) ranging from 700 million to 14 billion parameters, while\nsurpassing established baselines. While that work validated blockchain-based\ndecentralized pretraining as viable, it contained core issues: (i) every miner\nhad to fit an entire model locally, and (ii) \"winner-takes-all\" rewards\nencouraged model hoarding.\n  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an\narchitecture that addresses these limitations by transforming SN9's previously\nisolated competitors into a single cooperating unit that can scale arbitrarily\nwhile still rewarding each contributor fairly.\n  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -\nAn orchestrator distributes model layers across heterogeneous miners and\nstreams activations between them, enabling model sizes to scale with the number\nof participants rather than being constrained by the VRAM of a single machine;\n(2) Granular, continuous incentives - Validators measure each miner's\ncontribution and allocate token emissions proportionally; (3) Activation\ncompression - We used model-bottlenecks to cut communication bandwidths of\nactivations by up to 128x, vastly improving training speed; (4) Butterfly\nAll-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,\noffering linear scalability, redundancy and built-in collusion detection; (5)\nCLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair\nattribution scheme assigns credit to miners proportional to their marginal\nutility and detects exploits, even when contributions are interdependent across\nthe pipeline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17766v1", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-16", "AI": {"title_translation": "激励式协调训练架构 (IOTA)：发布技术入门", "tldr": "IOTA 是一种新的架构，通过将矿工转化为协作单元，解决了现有分布式 LLM 预训练中模型规模限制和奖励分配不公的问题。", "motivation": "之前的 Bittensor Subnet 9 (SN9) 尽管验证了去中心化预训练的可行性，但存在核心问题：(i) 每个矿工必须在本地适应整个模型；(ii) “赢者通吃”的奖励机制鼓励模型囤积。", "method": "IOTA (Incentivized Orchestrated Training Architecture) 通过将SN9中先前孤立的竞争者转化为一个单一的协作单元来解决这些限制。其关键特性包括：1. 数据和管道并行SWARM架构：协调器将模型层分发给异构矿工，并在它们之间传输激活，使模型大小随参与者数量扩展，而非受限于单机VRAM。2. 细粒度、持续激励：验证器衡量每个矿工的贡献并按比例分配代币。3. 激活压缩：使用模型瓶颈将激活的通信带宽降低多达128倍，大幅提高训练速度。4. Butterfly All-Reduce：矿工以 O(1) 带宽平均不相交的参数切片，提供线性可扩展性、冗余和内置的串通检测。5. CLASP (Contribution Loss Assessment via Sampling of Pathways)：一种公平的归因方案，即使贡献在管道中相互依赖，也能按边际效用比例将功劳分配给矿工并检测利用。", "result": "关键初步结果包括：(1) 数据和管道并行SWARM架构实现了模型大小随参与者数量扩展；(2) 细粒度、持续激励机制；(3) 激活通信带宽降低多达128倍，显著提高训练速度；(4) Butterfly All-Reduce 提供线性可扩展性、冗余和内置串通检测；(5) CLASP 实现公平的贡献归因和利用检测。", "conclusion": "IOTA 通过将分布式网络中的独立竞争者转变为一个协作单元，解决了去中心化LLM预训练中模型规模限制和奖励分配不公的问题，同时实现了可伸缩性和公平性。", "translation": "2024年8月，Bittensor的子网9（SN9）证明了一个由激励性、无需许可的参与者组成的分布式网络可以各自预训练从7亿到140亿参数不等的大型语言模型（LLMs），同时超越了既定基线。虽然这项工作验证了基于区块链的去中心化预训练的可行性，但它存在核心问题：（i）每个矿工都必须在本地适应整个模型，以及（ii）“赢者通吃”的奖励机制鼓励模型囤积。\n在此，我们引入了IOTA（激励式协调训练架构），一种通过将SN9中先前孤立的竞争者转化为一个单一的协作单元来解决这些限制的架构，该单元可以任意扩展，同时仍然公平地奖励每个贡献者。\n关键初步结果：（1）数据和管道并行SWARM架构——一个协调器将模型层分发给异构矿工，并在它们之间传输激活，使模型大小随参与者数量扩展，而非受限于单机VRAM；（2）细粒度、持续激励——验证器衡量每个矿工的贡献并按比例分配代币排放；（3）激活压缩——我们使用模型瓶颈将激活的通信带宽降低多达128倍，大大提高了训练速度；（4）Butterfly All-Reduce——矿工以 O(1) 带宽平均不相交的参数切片，提供线性可扩展性、冗余和内置的串通检测；（5）CLASP（通过路径采样评估贡献损失）——一种公平的归因方案，即使贡献在管道中相互依赖，也能按边际效用比例将功劳分配给矿工并检测利用。", "summary": "本文介绍了激励式协调训练架构（IOTA），旨在解决现有去中心化大型语言模型（LLM）预训练中存在的模型规模限制和不公平奖励分配问题。IOTA 将独立的矿工转化为协作单元，通过数据和管道并行、细粒度激励、激活压缩、高效的通信协议（Butterfly All-Reduce）以及公平的贡献归因机制（CLASP），实现了任意规模的LLM训练，并确保了对每个贡献者的公平奖励。初步结果表明，IOTA 能够有效提升训练效率和可扩展性。", "keywords": "分布式训练, 大型语言模型, 区块链, 去中心化, 激励机制", "comments": "IOTA 的创新之处在于它将竞争性的分布式训练模式转变为协作模式，从而解决了大规模模型在去中心化网络中训练时面临的资源限制和激励公平性问题。通过引入协调器、激活压缩和高效的通信协议，它显著提升了训练的可扩展性和效率。CLASP 机制的引入对于确保公平归因和防止作弊至关重要，是去中心化协作训练成功的关键。"}}
{"id": "2302.10160", "title": "Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift", "authors": ["Kaizheng Wang"], "categories": ["stat.ME", "cs.LG", "math.ST", "stat.ML", "stat.TH", "62J07, 62G05"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "Comments:      45 pages, 2 figures", "url": "http://arxiv.org/abs/2302.10160v4", "summary": "We develop and analyze a principled approach to kernel ridge regression under\ncovariate shift. The goal is to learn a regression function with small mean\nsquared error over a target distribution, based on unlabeled data from there\nand labeled data that may have a different feature distribution. We propose to\nsplit the labeled data into two subsets, and conduct kernel ridge regression on\nthem separately to obtain a collection of candidate models and an imputation\nmodel. We use the latter to fill the missing labels and then select the best\ncandidate accordingly. Our non-asymptotic excess risk bounds demonstrate that\nour estimator adapts effectively to both the structure of the target\ndistribution and the covariate shift. This adaptation is quantified through a\nnotion of effective sample size that reflects the value of labeled source data\nfor the target regression task. Our estimator achieves the minimax optimal\nerror rate up to a polylogarithmic factor, and we find that using pseudo-labels\nfor model selection does not significantly hinder performance.", "comment": "45 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2302.10160v4", "cate": "stat.ME", "date": "2023-02-20", "updated": "2025-07-24", "AI": {"title_translation": "协变量偏移下核岭回归的伪标签方法", "tldr": "本文提出并分析了一种在协变量偏移下对核岭回归进行伪标签的新方法，通过将有标签数据分成两部分来生成候选模型和插补模型，并证明其在非渐近超额风险界限下能有效适应目标分布和协变量偏移，达到最优误差率，且伪标签不显著影响性能。", "motivation": "在协变量偏移（即有标签数据和无标签数据的特征分布不同）的情况下，目标是利用目标分布的无标签数据和可能具有不同特征分布的有标签数据，学习一个在目标分布上均方误差小的回归函数。", "method": "提出将有标签数据分成两个子集。在这两个子集上分别进行核岭回归，得到一系列候选模型和一个插补模型。利用插补模型填充缺失的标签，然后选择最佳候选模型。", "result": "非渐近超额风险界限表明，该估计器能有效适应目标分布的结构和协变量偏移。这种适应性通过反映有标签源数据对目标回归任务价值的有效样本大小概念进行量化。该估计器达到了在多对数因子下的最小最大最优误差率，并且发现使用伪标签进行模型选择不会显著妨碍性能。", "conclusion": "本文提出的伪标签方法在协变量偏移下的核岭回归中表现出强大的适应性和近乎最优的性能，即使使用伪标签进行模型选择也不会显著损害模型表现。", "translation": "我们开发并分析了一种在协变量偏移下进行核岭回归的原则性方法。目标是基于来自目标分布的无标签数据和可能具有不同特征分布的有标签数据，学习一个在目标分布上具有小均方误差的回归函数。我们提出将有标签数据分成两个子集，并分别对它们进行核岭回归以获得一组候选模型和一个插补模型。我们使用后者来填充缺失的标签，然后相应地选择最佳候选模型。我们的非渐近超额风险界限表明，我们的估计器能够有效地适应目标分布的结构和协变量偏移。这种适应性通过反映有标签源数据对目标回归任务价值的有效样本大小概念进行量化。我们的估计器达到了在多对数因子下的最小最大最优误差率，并且我们发现使用伪标签进行模型选择不会显著妨碍性能。", "summary": "本文提出了一种在协变量偏移下进行核岭回归的伪标签方法。该方法将有标签数据分为两部分，分别进行核岭回归以生成候选模型和插补模型，并利用插补模型为无标签数据生成伪标签以辅助模型选择。理论分析表明，该估计器能够有效适应目标分布和协变量偏移，并通过有效样本大小的概念量化其适应性，最终实现了接近最优的误差率，且伪标签的使用并未显著影响性能。", "keywords": "核岭回归, 协变量偏移, 伪标签, 模型选择, 超额风险界限", "comments": "这项工作创新性地将伪标签技术应用于协变量偏移下的核岭回归问题，提供了一种在源域和目标域数据分布不一致时，有效利用有限有标签数据和大量无标签数据的方法。其非渐近超额风险界限的理论分析增强了其重要性，并量化了伪标签的价值。证明伪标签对模型性能无显著负面影响，为实际应用提供了信心。"}}
{"id": "2507.18401", "title": "Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols", "authors": ["Andrew Jeyathasan", "Swati Banerjee"], "categories": ["cs.HC", "q-bio.NC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18401v1", "summary": "We experience the world through multiple senses that work together to create\na cohesive perception, whether in daily life or immersive technologies.\nUnderstanding this multisensory integration (MSI) requires examining the\ninteractions between sensory modalities, each with unique temporal dynamics and\ncharacteristics. While most research focuses on unimodal or bimodal cues, the\nintegration of three or more modalities remains underexplored. MSI studies must\naccount for factors like cross-modal correspondence, congruence, cognitive\nload, and stimulus timing, which become increasingly complex as modalities\nmultiply. This article examines these key factors and how they can be applied\nto 8 design effective MSI study protocols.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18401v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多感官整合与视觉、听觉、触觉的感官替代：回答研究方案中的内容、选择和时机", "tldr": "本文探讨了多感官整合（MSI）研究方案设计中的关键因素和挑战，特别关注三种或更多模态的整合，并讨论了跨模态对应、一致性、认知负荷和刺激时间等因素。", "motivation": "理解多感官整合（MSI）的复杂性，特别是当涉及三种或更多模态时，因为这一领域仍未得到充分探索。", "method": "本文探讨并分析了设计有效多感官整合（MSI）研究方案的关键因素及其应用。", "result": "Not mentioned in abstract", "conclusion": "本文旨在通过探讨跨模态对应、一致性、认知负荷和刺激时间等关键因素，指导设计有效的多感官整合（MSI）研究方案。", "translation": "我们通过多种感官体验世界，这些感官协同工作，无论是日常生活还是沉浸式技术中，都能创造出连贯的感知。理解这种多感官整合（MSI）需要检查感官模式之间的相互作用，每种模式都有独特的时间动态和特征。虽然大多数研究关注单模态或双模态线索，但三种或更多模态的整合仍未得到充分探索。MSI研究必须考虑跨模态对应、一致性、认知负荷和刺激时间等因素，这些因素随着模态数量的增加而变得越来越复杂。本文探讨了这些关键因素以及如何将它们应用于设计有效的MSI研究方案。", "summary": "本文探讨了多感官整合（MSI）的复杂性，特别是当涉及三种或更多感官模态时，这是一个尚未得到充分探索的领域。文章强调，有效的多感官整合研究方案必须考虑跨模态对应、一致性、认知负荷和刺激时间等关键因素，这些因素随着模态的增加而变得更加复杂。本文旨在通过审视这些因素来指导稳健的多感官整合研究设计。", "keywords": "多感官整合, 感官替代, 研究方案, 跨模态对应, 认知负荷", "comments": "这篇论文很重要，因为它指出了多感官整合研究中的一个空白（三种或更多模态的整合），并为设计更全面的研究提供了框架。它通过识别复杂MSI实验中经常被忽视的关键因素，提供了实用的指导。"}}
{"id": "2410.22074", "title": "Variational inference for pile-up removal at hadron colliders with diffusion models", "authors": ["Malte Algren", "Tobias Golling", "Christopher Pollard", "John Andrew Raine"], "categories": ["hep-ph", "cs.LG"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      19 pages, 13 figures", "url": "http://arxiv.org/abs/2410.22074v2", "summary": "In this paper, we present a novel method for pile-up removal of $pp$\ninteractions using variational inference with diffusion models, called vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval, yielding a clear advantage over existing methods especially in the\npresence of imperfect detector efficiency. We evaluate the performance of vipr\nin a sample of jets from simulated $t\\bar{t}$ events overlain with pile-up\ncontamination. vipr outperforms softdrop and has comparable performance to\npuppiml in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios.", "comment": "19 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2410.22074v2", "cate": "hep-ph", "date": "2024-10-29", "updated": "2025-07-24", "AI": {"title_translation": "强子对撞机中基于扩散模型的变分推断用于堆积去除", "tldr": "本文提出了一种名为vipr的新方法，利用扩散模型和变分推断来去除强子对撞机中的堆积效应，通过生成式模型预测硬散射粒子射流，并在模拟数据上表现优于现有方法。", "motivation": "现有堆积去除方法主要采用分类方法识别粒子来源，但这种方法存在局限性，尤其是在探测器效率不完美的情况下。本文旨在探索一种新的、更有效的方法来解决这些问题。", "method": "本文提出了一种名为vipr的新方法，该方法将变分推断与扩散模型结合，用于移除pp相互作用中的堆积效应。与传统的分类方法不同，vipr训练一个生成模型来预测去除堆积后的硬散射粒子射流的组成，从而估计硬散射射流成分的完整后验分布。", "result": "vipr在模拟的t\bar{t}事件中，在存在堆积污染的情况下，对硬散射射流子结构的预测性能优于softdrop，并与puppiml表现相当，适用于广泛的堆积场景。", "conclusion": "vipr通过估计硬散射射流成分的完整后验分布，在堆积去除方面展现出明显优势，尤其是在探测器效率不完美的情况下，优于现有方法。", "translation": "在本文中，我们提出了一种利用扩散模型和变分推断去除pp相互作用中堆积效应的新方法，称为vipr。该方法不使用分类方法来识别哪些粒子来自初级碰撞，而是训练一个生成模型来预测去除堆积后的硬散射粒子射流的组成。这导致了对硬散射射流成分的完整后验分布的估计，这在堆积去除的背景下尚未被探索，与现有方法相比具有明显的优势，尤其是在探测器效率不完美的情况下。我们在模拟的t\bar{t}事件中评估了vipr的性能，这些事件叠加了堆积污染。在广泛的堆积场景中，vipr在预测硬散射射流子结构方面优于softdrop，并且与puppiml表现相当。", "summary": "本文介绍了一种名为vipr的创新性堆积去除方法，该方法在强子对撞机中应用扩散模型和变分推断。与传统的分类方法不同，vipr采用生成模型预测去除堆积后的硬散射粒子射流成分，并估计其完整后验分布，这在以往研究中未被探索。实验结果表明，在模拟的t\bar{t}事件中，vipr在预测硬散射射流子结构方面优于softdrop，并与puppiml性能相当，尤其在探测器效率不完美时展现出优势。", "keywords": "堆积去除, 变分推断, 扩散模型, 强子对撞机, 生成模型", "comments": "本文的创新点在于首次将生成模型（扩散模型）和变分推断应用于强子对撞机中的堆积去除问题，并致力于估计硬散射射流成分的完整后验分布，这为解决探测器效率不完美情况下的堆积问题提供了新的思路和方法。"}}
{"id": "2507.18023", "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details", "authors": ["Jun Zhou", "Dinghao Li", "Nannan Li", "Mingjie Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18023v1", "summary": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis,\nparticularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n(3DGS), have greatly enhanced the fidelity and efficiency of 3D content\ncreation. However, inpainting 3D scenes remains a challenging task due to the\ninherent irregularity of 3D structures and the critical need for maintaining\nmulti-view consistency. In this work, we propose a novel 3D Gaussian inpainting\nframework that reconstructs complete 3D scenes by leveraging sparse inpainted\nviews. Our framework incorporates an automatic Mask Refinement Process and\nregion-wise Uncertainty-guided Optimization. Specifically, we refine the\ninpainting mask using a series of operations, including Gaussian scene\nfiltering and back-projection, enabling more accurate localization of occluded\nregions and realistic boundary restoration. Furthermore, our Uncertainty-guided\nFine-grained Optimization strategy, which estimates the importance of each\nregion across multi-view images during training, alleviates multi-view\ninconsistencies and enhances the fidelity of fine details in the inpainted\nresults. Comprehensive experiments conducted on diverse datasets demonstrate\nthat our approach outperforms existing state-of-the-art methods in both visual\nquality and view consistency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18023v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "高保真三维高斯修复：保持多视角一致性和真实感细节", "tldr": "提出一种高保真三维高斯修复框架，通过掩码优化和不确定性引导优化解决多视角一致性问题，实现逼真的三维场景修复。", "motivation": "尽管神经辐射场（NeRF）和三维高斯泼溅（3DGS）提升了三维内容创建的保真度和效率，但三维场景修复仍具挑战，因其固有的三维结构不规则性以及维持多视角一致性的关键需求。", "method": "提出一种新颖的三维高斯修复框架，利用稀疏修复视图重建完整三维场景。该框架包含：1. 自动掩码优化过程：通过高斯场景滤波和反向投影等操作优化修复掩码，实现更准确的遮挡区域定位和逼真的边界恢复。2. 区域不确定性引导优化：在训练期间估计多视角图像中各区域的重要性，以缓解多视角不一致性并增强修复结果的精细细节保真度。", "result": "在不同数据集上进行的综合实验表明，该方法在视觉质量和视角一致性方面均优于现有最先进的方法。", "conclusion": "该研究成功地提出了一个新颖的三维高斯修复框架，有效解决了三维场景修复中多视角一致性和细节保真度的挑战，并取得了优于现有方法的性能。", "translation": "近期多视角三维重建和新视角合成（特别是通过神经辐射场（NeRF）和三维高斯泼溅（3DGS））的进展，极大地提升了三维内容创建的保真度和效率。然而，由于三维结构固有的不规则性以及维持多视角一致性的关键需求，三维场景修复仍然是一项具有挑战性的任务。在这项工作中，我们提出了一种新颖的三维高斯修复框架，通过利用稀疏修复视图来重建完整的三维场景。我们的框架包含一个自动掩码优化过程和区域不确定性引导优化。具体来说，我们通过一系列操作（包括高斯场景滤波和反向投影）来优化修复掩码，从而实现对遮挡区域更准确的定位和逼真的边界恢复。此外，我们的不确定性引导精细优化策略在训练期间估计多视角图像中每个区域的重要性，从而缓解了多视角不一致性并增强了修复结果中精细细节的保真度。在不同数据集上进行的综合实验表明，我们的方法在视觉质量和视角一致性方面均优于现有最先进的方法。", "summary": "本文提出一种新颖的高保真三维高斯修复框架，旨在解决三维场景修复中多视角一致性和细节保真度的挑战。该框架通过引入自动掩码优化过程（利用高斯滤波和反向投影精确定位遮挡区域并恢复边界）和区域不确定性引导优化策略（在训练中估计多视角图像区域重要性以缓解不一致性并增强细节），实现了对完整三维场景的重建。实验证明，该方法在视觉质量和视角一致性方面均优于现有技术。", "keywords": "三维高斯修复, 多视角一致性, 图像修复, 3DGS, 场景重建", "comments": "这项工作在三维场景修复领域具有重要意义，特别是在结合3D Gaussian Splatting的最新进展方面。其创新点在于提出的“自动掩码优化过程”和“区域不确定性引导优化”策略，有效解决了传统三维修复中难以保持多视角一致性和细节真实感的问题。该方法对于提升三维内容生成和编辑的质量具有实际应用价值。"}}
{"id": "2507.18214", "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation", "authors": ["Qilin Huang", "Tianyu Lin", "Zhiguang Chen", "Fudan Zheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025", "url": "http://arxiv.org/abs/2507.18214v1", "summary": "Leveraging the powerful capabilities of diffusion models has yielded quite\neffective results in medical image segmentation tasks. However, existing\nmethods typically transfer the original training process directly without\nspecific adjustments for segmentation tasks. Furthermore, the commonly used\npre-trained diffusion models still have deficiencies in feature extraction.\nBased on these considerations, we propose LEAF, a medical image segmentation\nmodel grounded in latent diffusion models. During the fine-tuning process, we\nreplace the original noise prediction pattern with a direct prediction of the\nsegmentation map, thereby reducing the variance of segmentation results. We\nalso employ a feature distillation method to align the hidden states of the\nconvolutional layers with the features from a transformer-based vision encoder.\nExperimental results demonstrate that our method enhances the performance of\nthe original diffusion model across multiple segmentation datasets for\ndifferent disease types. Notably, our approach does not alter the model\narchitecture, nor does it increase the number of parameters or computation\nduring the inference phase, making it highly efficient.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.18214v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LEAF：用于医学图像分割中对齐特征的带高效编码器蒸馏的潜在扩散模型", "tldr": "LEAF是一种高效的潜在扩散模型，通过特征蒸馏和直接分割图预测，提高了医学图像分割的性能。", "motivation": "现有扩散模型在医学图像分割中直接迁移原始训练过程，缺乏针对性调整，且预训练扩散模型在特征提取方面存在不足。", "method": "本文提出了LEAF模型，一种基于潜在扩散模型的医学图像分割方法。在微调过程中，将原始的噪声预测模式替换为直接预测分割图，以减少分割结果的方差。同时，采用特征蒸馏方法，将卷积层的隐藏状态与基于Transformer的视觉编码器的特征对齐。", "result": "实验结果表明，该方法在多个针对不同疾病类型的分割数据集上增强了原始扩散模型的性能。值得注意的是，该方法不改变模型架构，也不增加推理阶段的参数数量或计算量。", "conclusion": "LEAF模型通过改进的预测模式和特征蒸馏，显著提升了扩散模型在医学图像分割中的效率和性能，且不增加推理负担。", "translation": "利用扩散模型的强大功能在医学图像分割任务中取得了相当有效的结果。然而，现有方法通常直接转移原始训练过程，没有针对分割任务进行具体调整。此外，常用的预训练扩散模型在特征提取方面仍存在缺陷。基于这些考虑，我们提出了LEAF，一个基于潜在扩散模型的医学图像分割模型。在微调过程中，我们将原始的噪声预测模式替换为直接预测分割图，从而减少分割结果的方差。我们还采用了一种特征蒸馏方法，将卷积层的隐藏状态与基于Transformer的视觉编码器的特征对齐。实验结果表明，我们的方法在多个针对不同疾病类型的分割数据集上增强了原始扩散模型的性能。值得注意的是，我们的方法不改变模型架构，也不增加推理阶段的参数数量或计算量，使其具有高效率。", "summary": "LEAF是一个针对医学图像分割任务的潜在扩散模型。它通过在微调阶段直接预测分割图来减少结果方差，并利用特征蒸馏技术将卷积层特征与Transformer编码器特征对齐，从而克服了现有扩散模型在特征提取和任务特异性调整方面的不足。实验证明，LEAF在不增加模型复杂度和推理开销的情况下，显著提升了医学图像分割的性能。", "keywords": "潜在扩散, 医学图像分割, 特征蒸馏, 对齐特征, 效率", "comments": "该论文提出了一种新颖且高效的扩散模型微调策略，通过直接预测分割图和引入特征蒸馏，有效解决了现有扩散模型在医学图像分割中泛化性和特征对齐的挑战。其创新点在于在不改变模型架构和不增加推理成本的前提下，提升了性能，这对于实际应用具有重要意义。"}}
{"id": "2507.17982", "title": "Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model", "authors": ["Pablo Ramírez-Espinosa", "Cleofás Segura-Gómez", "Ángel Palomares-Caballero", "F. Javier López-Martínez", "David Morales-Jiménez"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17982v1", "summary": "Fluid antenna systems (FASs) have become a popular topic in the wireless\ncommunity as an effective yet simple means of exploiting spatial diversity. Due\nto the limitations of physically moving radiating elements, electronically\nreconfigurable antennas are emerging as practical implementations of FASs,\nsince changing the radiation pattern is functionally equivalent to physically\nmoving the device. However, electronically reconfigurable antennas pose a\nchallenge in terms of analytical modeling, often requiring full-wave\nsimulations or measurements for their characterization; this severely limits\nthe extraction of theoretical insights useful for system design. Motivated by\nthese difficulties and the growing interest in FASs, we propose in this paper a\ncomplete analytical model for metasurface-based embodiments of FASs.\nSpecifically, we advocate for the implementation of the FAS concept through\ndynamic metasurface antennas (DMAs), hitherto proposed as array replacements in\nmultiple-input multiple-output (MIMO) systems. We leverage circuit theory to\nrewrite the conventional signal model of FASs in terms of admittance matrices\naccounting for the electromagnetic effects inherent to metasurfaces. The model\nis validated with full-wave simulations, showing good agreement. We further\nillustrate how to apply the model for standard performance analysis, and\nprovide closed-form expressions for key metrics, including the resulting signal\ncovariance matrix. Results confirm that practical DMA-based FASs can achieve\nsimilar performance to that of idealized implementations of position-flexible\nantennas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17982v1", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于超表面的流体天线：从电磁学到通信模型", "tldr": "本文提出了一个完整的基于超表面的流体天线系统（FAS）分析模型，并验证了其性能与理想FAS相当。", "motivation": "流体天线系统（FAS）在无线领域日益流行，但电子可重构天线在分析建模方面面临挑战，通常需要全波仿真或测量，这限制了理论洞察的提取。", "method": "提出了一种完整的基于超表面的流体天线系统（FAS）分析模型，通过动态超表面天线（DMA）实现FAS概念，并利用电路理论将FAS的传统信号模型改写为考虑超表面电磁效应的导纳矩阵形式。", "result": "该模型通过全波仿真验证，显示出良好的一致性。论文还展示了如何应用该模型进行标准性能分析，并提供了关键指标（包括信号协方差矩阵）的闭合形式表达式。结果证实，实际的基于DMA的FAS可以实现与理想位置灵活天线相似的性能。", "conclusion": "实际的基于动态超表面天线（DMA）的流体天线系统（FAS）能够实现与理想位置灵活天线相似的性能。", "translation": "流体天线系统（FAS）作为一种有效而简单的空间分集利用手段，已成为无线领域的热门话题。由于物理移动辐射单元的限制，电子可重构天线正作为FAS的实际实现方式出现，因为改变辐射方向图在功能上等同于物理移动设备。然而，电子可重构天线在分析建模方面带来了挑战，通常需要全波仿真或测量才能进行表征；这严重限制了提取对系统设计有用的理论见解。受这些困难以及对FAS日益增长的兴趣的启发，本文提出了一种完整的基于超表面的FAS实现体的分析模型。具体而言，我们提倡通过动态超表面天线（DMA）实现FAS概念，DMA迄今已被提议作为多输入多输出（MIMO）系统中的阵列替代品。我们利用电路理论将FAS的传统信号模型改写为考虑超表面固有电磁效应的导纳矩阵形式。该模型通过全波仿真进行了验证，显示出良好的一致性。我们进一步说明了如何将该模型应用于标准性能分析，并提供了关键指标（包括所得信号协方差矩阵）的闭合形式表达式。结果证实，实际的基于DMA的FAS可以实现与理想的位置灵活天线相似的性能。", "summary": "本文提出了一种针对基于超表面的流体天线系统（FAS）的完整分析模型，特别是利用动态超表面天线（DMA）实现。通过利用电路理论和导纳矩阵，该模型考虑了电磁效应，从而为FAS设计提供了理论见解，这在以前由于需要全波仿真而具有挑战性。该模型通过仿真验证，能够进行标准性能分析，并表明实际的基于DMA的FAS可以实现与理想位置灵活天线相当的性能。", "keywords": "流体天线系统, 超表面, 动态超表面天线, 分析模型, 空间分集", "comments": "本文解决了流体天线系统（FAS）中电子可重构天线分析建模的一个重要挑战。通过为基于超表面的FAS（DMA）提出一个基于电路理论的分析模型，它提供了一个以前缺乏的理论框架，减少了对计算成本高的全波仿真的依赖。这项创新对于推动实际FAS实现的设计和理解至关重要。"}}
{"id": "2507.18405", "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows", "authors": ["Simin Huo", "Ning Li"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 10 figures, Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence", "url": "http://arxiv.org/abs/2507.18405v1", "summary": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.", "comment": "14 pages, 10 figures, Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.18405v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Iwin Transformer: 使用交错窗口的分层视觉Transformer", "tldr": "Iwin Transformer是一种新型的分层视觉Transformer，它通过交错窗口注意力和深度可分离卷积，在一个模块内实现全局信息交换，解决了Swin Transformer的局限性，并在图像分类、语义分割和视频动作识别等任务中表现出色。", "motivation": "为了克服Swin Transformer需要两个连续块才能近似全局注意力的限制，论文旨在设计一个能够在单个模块内实现全局信息交换的视觉Transformer。", "method": "Iwin Transformer是一种新型的无位置嵌入分层视觉Transformer。它通过创新的交错窗口注意力机制和深度可分离卷积协同工作。注意力用于连接远距离的token，卷积用于连接相邻的token，从而在单个模块内实现全局信息交换。该模型可以直接从低分辨率微调到高分辨率。", "result": "Iwin Transformer在视觉基准上表现出强大的竞争力：在ImageNet-1K图像分类任务中达到87.4%的top-1准确率；在语义分割和视频动作识别任务中也表现出色。此外，Iwin的核心组件作为独立模块可有效替代类别条件图像生成中的自注意力模块。", "conclusion": "Iwin Transformer提出了一种新颖的分层视觉Transformer设计，通过在单个模块内实现高效的全局信息交换，克服了Swin Transformer等先前模型的局限性。其在各种视觉任务上的强大性能以及对未来研究（如Iwin 3D Attention）的启发潜力，突显了其重要性。", "translation": "我们引入了Iwin Transformer，这是一种新型的无位置嵌入分层视觉Transformer，通过创新的交错窗口注意力机制和深度可分离卷积的协同作用，可以直接从低分辨率微调到高分辨率。这种方法使用注意力连接远距离的token，并应用卷积连接相邻的token，从而在单个模块内实现全局信息交换，克服了Swin Transformer需要两个连续块才能近似全局注意力的限制。在视觉基准上的大量实验表明，Iwin Transformer在图像分类（ImageNet-1K上达到87.4的top-1准确率）、语义分割和视频动作识别等任务中表现出强大的竞争力。我们还验证了Iwin核心组件作为独立模块的有效性，它可以无缝替代类别条件图像生成中的自注意力模块。Iwin Transformer引入的概念和方法有潜力启发未来的研究，例如视频生成中的Iwin 3D注意力。代码和模型可在https://github.com/cominder/Iwin-Transformer获取。", "summary": "Iwin Transformer提出了一种结合交错窗口注意力和深度可分离卷积的分层视觉Transformer。该设计允许在单个模块内进行全局信息交换，从而解决了Swin Transformer多块近似全局注意力的问题。它在图像分类（ImageNet-1K上top-1准确率为87.4%）、语义分割和视频动作识别等任务中表现出竞争力，并且其核心组件可用于图像生成。", "keywords": "Iwin Transformer, 视觉Transformer, 交错窗口, 全局注意力, 深度可分离卷积", "comments": "Iwin Transformer通过巧妙地结合注意力和卷积，在一个模块内高效地捕获全局和局部信息，为视觉Transformer提供了一种创新方法。这种设计优雅地解决了Swin Transformer的已知局限性，使其在全局上下文建模方面更高效。其在各种视觉任务上的强大性能以及在3D注意力等领域更广泛应用的潜力，突显了其重要性和未来的研究潜力。"}}
{"id": "2503.20326", "title": "Modality-Agnostic Brain Lesion Segmentation with Privacy-aware Continual Learning", "authors": ["Yousef Sadegheih", "Pratibha Kumari", "Dorit Merhof"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted in MICCAI 2025 PRIME Workshop!", "url": "http://arxiv.org/abs/2503.20326v2", "summary": "Traditional brain lesion segmentation models for multi-modal MRI are\ntypically tailored to specific pathologies, relying on datasets with predefined\nmodalities. Adapting to new MRI modalities or pathologies often requires\ntraining separate models, which contrasts with how medical professionals\nincrementally expand their expertise by learning from diverse datasets over\ntime. Inspired by this human learning process, we propose a unified\nsegmentation model capable of sequentially learning from multiple datasets with\nvarying modalities and pathologies. Our approach leverages a privacy-aware\ncontinual learning framework that integrates a mixture-of-experts mechanism and\ndual knowledge distillation to mitigate catastrophic forgetting while not\ncompromising performance on newly encountered datasets. Extensive experiments\nacross five diverse brain MRI datasets and four dataset sequences demonstrate\nthe effectiveness of our framework in maintaining a single adaptable model,\ncapable of handling varying hospital protocols, imaging modalities, and disease\ntypes. Compared to widely used privacy-aware continual learning methods such as\nLwF, SI, EWC, MiB, and TED, our method achieves an average Dice score\nimprovement of approximately 14%. Our framework represents a significant step\ntoward more versatile and practical brain lesion segmentation models, with\nimplementation available on\n\\href{https://github.com/xmindflow/BrainCL}{GitHub}.", "comment": "Accepted in MICCAI 2025 PRIME Workshop!", "pdf_url": "http://arxiv.org/pdf/2503.20326v2", "cate": "eess.IV", "date": "2025-03-26", "updated": "2025-07-24", "AI": {"title_translation": "模态无关的脑部病变分割与隐私保护的持续学习", "tldr": "该研究提出了一种统一的、隐私保护的持续学习框架，用于脑部病变分割，能够顺序学习不同模态和病理的数据，有效缓解灾难性遗忘，并显著优于现有方法。", "motivation": "传统的脑部病变分割模型通常针对特定病理和预定义模态进行训练，导致适应新的MRI模态或病理时需要重新训练独立模型，这与医疗专业人员通过学习多样化数据逐步扩展专业知识的方式不符。", "method": "受人类学习过程启发，该研究提出一个统一的分割模型，能够顺序学习来自多个具有不同模态和病理的数据集。该方法利用一个隐私保护的持续学习框架，该框架集成了专家混合机制和双重知识蒸馏，以减轻灾难性遗忘，同时不影响在新数据集上的性能。", "result": "在五个不同的脑部MRI数据集和四个数据集序列上的广泛实验表明，该框架在维护一个单一的、适应性强的模型方面是有效的，该模型能够处理不同的医院协议、成像模态和疾病类型。与广泛使用的隐私保护持续学习方法（如LwF、SI、EWC、MiB和TED）相比，该方法实现了约14%的平均Dice分数改进。", "conclusion": "该框架代表了在实现更通用和实用的脑部病变分割模型方面迈出了重要一步。", "translation": "传统的脑部病变分割模型通常针对特定病理量身定制，依赖于预定义模态的数据集。适应新的MRI模态或病理通常需要训练独立的模型，这与医疗专业人员随时间从多样化数据集中学习以逐步扩展专业知识的方式形成对比。受这种人类学习过程的启发，我们提出了一种统一的分割模型，能够顺序学习来自多个具有不同模态和病理的数据集。我们的方法利用一个隐私保护的持续学习框架，该框架集成了专家混合机制和双重知识蒸馏，以减轻灾难性遗忘，同时不影响在新遇到数据集上的性能。在五个不同的脑部MRI数据集和四个数据集序列上的广泛实验证明了我们框架在维护一个单一的、适应性强的模型方面的有效性，该模型能够处理不同的医院协议、成像模态和疾病类型。与广泛使用的隐私保护持续学习方法（如LwF、SI、EWC、MiB和TED）相比，我们的方法实现了约14%的平均Dice分数改进。我们的框架代表了在实现更通用和实用脑部病变分割模型方面迈出了重要一步，其实现代码已在GitHub上提供。", "summary": "本研究提出了一种模态无关的脑部病变分割模型，采用隐私保护的持续学习框架。针对传统模型无法有效适应新MRI模态和病理的问题，该模型通过整合专家混合机制和双重知识蒸馏，实现了对多源异构数据集的顺序学习，有效缓解了灾难性遗忘。实验证明，该方法在保持单一模型适应性的同时，在多种脑部MRI数据集上相较于现有持续学习方法，Dice分数平均提升约14%，为开发更通用、实用的脑部病变分割工具奠定了基础。", "keywords": "脑部病变分割, 持续学习, 模态无关, 隐私保护, 医疗影像", "comments": "该论文的创新之处在于将持续学习与隐私保护相结合，并应用于脑部病变分割领域，解决了传统模型在面对新模态和病理时的泛化性差的问题。通过引入专家混合和双重知识蒸馏机制，有效平衡了新旧知识的学习，减轻了灾难性遗忘。其提出的统一模型概念，与医疗专业人员的渐进式学习模式相契合，具有重要的临床应用潜力。14%的Dice分数提升也表明了其方法的显著优势。"}}
{"id": "2507.17815", "title": "Analytic Regression of Feynman Integrals from High-Precision Numerical Sampling", "authors": ["Oscar Barrera", "Aurélien Dersy", "Rabia Husain", "Matthew D. Schwartz", "Xiaoyuan Zhang"], "categories": ["hep-th", "cs.NA", "hep-ph", "math.NA"], "primary_category": "Subjects:       High Energy Physics - Theory (hep-th)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17815v1", "summary": "In mathematics or theoretical physics one is often interested in obtaining an\nexact analytic description of some data which can be produced, in principle, to\narbitrary accuracy. For example, one might like to know the exact analytical\nform of a definite integral. Such problems are not well-suited to numerical\nsymbolic regression, since typical numerical methods lead only to\napproximations. However, if one has some sense of the function space in which\nthe analytic result should lie, it is possible to deduce the exact answer by\njudiciously sampling the data at a sufficient number of points with sufficient\nprecision. We demonstrate how this can be done for the computation of Feynman\nintegrals. We show that by combining high-precision numerical integration with\nanalytic knowledge of the function space one can often deduce the exact answer\nusing lattice reduction. A number of examples are given as well as an\nexploration of the trade-offs between number of datapoints, number of\nfunctional predicates, precision of the data, and compute. This method provides\na bottom-up approach that neatly complements the top-down Landau-bootstrap\napproach of trying to constrain the exact answer using the analytic structure\nalone. Although we focus on the application to Feynman integrals, the\ntechniques presented here are more general and could apply to a wide range of\nproblems where an exact answer is needed and the function space is sufficiently\nwell understood.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17815v1", "cate": "hep-th", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于高精度数值采样的费曼积分解析回归", "tldr": "本文提出了一种结合高精度数值积分和函数空间解析知识的方法，通过晶格约化从高精度数值数据中精确推导出费曼积分的解析形式。", "motivation": "在数学或理论物理中，人们通常希望获得可任意精度生成的数据的精确解析描述，但传统的数值方法（如数值符号回归）只能得到近似解。", "method": "该方法通过将高精度数值积分与函数空间的解析知识相结合，并利用晶格约化，从足够数量和精度的采样数据中推导出精确的解析答案。论文以费曼积分的计算为例进行了演示。", "result": "研究展示了如何通过该方法计算费曼积分，并探讨了数据点数量、函数谓词数量、数据精度和计算量之间的权衡关系，提供了多个示例。", "conclusion": "该方法提供了一种自下而上的方法，与仅通过解析结构约束精确答案的自上而下的Landau-bootstrap方法形成了良好互补。尽管重点关注费曼积分，但所提出的技术更具普遍性，可应用于需要精确答案且函数空间已知的问题。", "translation": "在数学或理论物理中，人们通常对获取一些数据的精确解析描述感兴趣，这些数据原则上可以任意精度生成。例如，人们可能想知道一个定积分的精确解析形式。这类问题不适合数值符号回归，因为典型的数值方法只能得到近似解。然而，如果对解析结果应位于的函数空间有所了解，则可以通过在足够数量的点上以足够的精度明智地采样数据来推导出精确答案。我们演示了如何使用这种方法计算费曼积分。我们表明，通过将高精度数值积分与函数空间的解析知识相结合，通常可以使用晶格约化推导出精确答案。论文给出了一些示例，并探讨了数据点数量、函数谓词数量、数据精度和计算量之间的权衡。这种方法提供了一种自下而上的方法，与仅利用解析结构来约束精确答案的自上而下的Landau-bootstrap方法形成了巧妙的互补。尽管我们重点关注费曼积分的应用，但本文提出的技术更具普遍性，可应用于需要精确答案且函数空间已知的一系列广泛问题。", "summary": "本文提出了一种新的方法，用于从高精度数值采样数据中精确推导解析形式，特别针对费曼积分。该方法结合了高精度数值积分和对函数空间的解析理解，并通过晶格约化技术实现。研究强调了在已知函数空间的情况下，通过足够精确和数量的采样可以获得精确解析解的可能性，并探讨了相关参数的权衡。这种自下而上的方法为获取精确解析结果提供了新的途径，补充了现有的自上而下方法，并具有广泛的应用前景。", "keywords": "费曼积分, 解析回归, 高精度数值采样, 晶格约化, 函数空间", "comments": "这项研究提出了一种创新性的“自下而上”方法，通过结合高精度数值计算和对函数空间的解析知识，实现了从数值数据中精确推导解析解。其重要性在于弥补了传统数值方法只能提供近似解的不足，并为费曼积分等复杂问题的精确求解提供了新工具。该方法与现有的“自上而下”方法形成互补，拓宽了解决此类问题的思路。其普适性也使其潜在应用范围远超费曼积分。"}}
{"id": "2308.01358", "title": "Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning", "authors": ["Constantin Philippenko", "Aymeric Dieuleveut"], "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2308.01358v2", "summary": "In this paper, we investigate the impact of compression on stochastic\ngradient algorithms for machine learning, a technique widely used in\ndistributed and federated learning. We underline differences in terms of\nconvergence rates between several unbiased compression operators, that all\nsatisfy the same condition on their variance, thus going beyond the classical\nworst-case analysis. To do so, we focus on the case of least-squares regression\n(LSR) and analyze a general stochastic approximation algorithm for minimizing\nquadratic functions relying on a random field. We consider weak assumptions on\nthe random field, tailored to the analysis (specifically, expected H\\\"older\nregularity), and on the noise covariance, enabling the analysis of various\nrandomizing mechanisms, including compression. We then extend our results to\nthe case of federated learning.\n  More formally, we highlight the impact on the convergence of the covariance\n$\\mathfrak{C}_{\\mathrm{ania}}$ of the additive noise induced by the algorithm.\nWe demonstrate despite the non-regularity of the stochastic field, that the\nlimit variance term scales with $\\mathrm{Tr}(\\mathfrak{C}_{\\mathrm{ania}}\nH^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ the\nnumber of iterations) generalizing the rate for the vanilla LSR case where it\nis $\\sigma^2 \\mathrm{Tr}(H H^{-1}) / K = \\sigma^2 d / K$ (Bach and Moulines,\n2013). Then, we analyze the dependency of $\\mathfrak{C}_{\\mathrm{ania}}$ on the\ncompression strategy and ultimately its impact on convergence, first in the\ncentralized case, then in two heterogeneous FL frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2308.01358v2", "cate": "cs.LG", "date": "2023-08-02", "updated": "2025-07-24", "AI": {"title_translation": "压缩和分布式最小二乘回归：收敛率及其在联邦学习中的应用", "tldr": "研究了压缩对分布式和联邦学习中随机梯度算法收敛率的影响。", "motivation": "论文旨在探究压缩技术对机器学习中随机梯度算法收敛性的影响，特别是在分布式和联邦学习场景下。", "method": "论文聚焦于最小二乘回归（LSR），分析了一个依赖于随机场的二次函数最小化通用随机近似算法。研究了多个满足相同方差条件的无偏压缩算子的收敛率差异，并考虑了随机场和噪声协方差的弱假设。研究结果扩展到联邦学习，并分析了加性噪声协方差对收敛的影响以及其与压缩策略的依赖关系。", "result": "论文证明了尽管随机场非正则，但极限方差项与 $\\mathrm{Tr}(\\mathfrak{C}_{\\mathrm{ania}} H^{-1})/K$ 成比例（其中 $H$ 是优化问题的Hessian， $K$ 是迭代次数），这推广了传统LSR的收敛率。研究还分析了加性噪声协方差 $\\mathfrak{C}_{\\mathrm{ania}}$ 对压缩策略的依赖及其对收敛性的影响，包括中心化和两种异构联邦学习框架。", "conclusion": "Not mentioned in abstract", "translation": "在本文中，我们研究了压缩对机器学习中随机梯度算法的影响，这是一种在分布式和联邦学习中广泛使用的技术。我们强调了几个无偏压缩算子在收敛率方面的差异，这些算子都满足相同的方差条件，从而超越了经典的“最坏情况”分析。为此，我们专注于最小二乘回归（LSR）的情况，并分析了一个依赖于随机场的最小化二次函数的通用随机近似算法。我们考虑了对随机场的弱假设，这些假设是为分析量身定制的（特别是期望H\"older正则性），以及对噪声协方差的弱假设，从而能够分析各种随机化机制，包括压缩。然后，我们将结果扩展到联邦学习的情况。更正式地，我们强调了算法引起的加性噪声协方差 $\\mathfrak{C}_{\\mathrm{ania}}$ 对收敛的影响。我们证明了尽管随机场非正则，但极限方差项与 $\\mathrm{Tr}(\\mathfrak{C}_{\\mathrm{ania}} H^{-1})/K$ 成比例（其中 $H$ 是优化问题的Hessian， $K$ 是迭代次数），这推广了传统LSR的收敛率，传统LSR的收敛率为 $\\sigma^2 \\mathrm{Tr}(H H^{-1}) / K = \\sigma^2 d / K$（Bach和Moulines，2013）。然后，我们分析了 $\\mathfrak{C}_{\\mathrm{ania}}$ 对压缩策略的依赖性，并最终分析了其对收敛的影响，首先是在中心化情况下，然后是在两种异构联邦学习框架中。", "summary": "本文研究了压缩对分布式和联邦学习中随机梯度算法收敛率的影响。通过分析最小二乘回归的通用随机近似算法，论文超越了经典最坏情况分析，揭示了不同无偏压缩算子在收敛率上的差异。研究证明了极限方差项的比例关系，并分析了加性噪声协方差与压缩策略对收敛的影响，将结果推广到联邦学习场景。", "keywords": "压缩, 分布式学习, 联邦学习, 最小二乘回归, 收敛率", "comments": "论文通过深入分析压缩对随机梯度算法收敛率的影响，并推广了最小二乘回归的收敛率，对分布式和联邦学习中的优化算法设计具有重要意义。其超越经典最坏情况分析的方法，以及对弱假设下的分析，展现了研究的严谨性。"}}
{"id": "2506.23825", "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", "authors": ["Haoji Zhang", "Yiqin Wang", "Yansong Tang", "Yong Liu", "Jiashi Feng", "Xiaojie Jin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2506.23825v2", "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2506.23825v2", "cate": "cs.CV", "date": "2025-06-30", "updated": "2025-07-24", "AI": {"title_translation": "Flash-VStream：长视频流高效实时理解", "tldr": "Flash-VStream提出了一种高效的视频语言模型，通过Flash记忆模块处理超长视频并实时响应查询，显著降低了推理延迟并达到了SOTA性能。", "motivation": "现有的大型多模态语言模型在图像和短视频理解方面表现出色，但长视频的理解仍然具有挑战性，因为其长上下文特性导致显著的计算和内存开销。大多数现有工作将长视频与短视频同等对待，这对于实际应用来说效率低下，并且难以推广到更长的视频。", "method": "我们提出了Flash-VStream，一个能够处理超长视频并实时响应用户查询的高效视频语言模型。我们特别设计了一个Flash记忆模块，其中包含一个低容量的上下文记忆来聚合长上下文时间信息并建模信息密度分布，以及一个高容量的增强记忆，用于根据该分布检索详细的空间信息。", "result": "与现有模型相比，Flash-VStream显著降低了推理延迟。在长视频基准测试和综合视频基准测试（如EgoSchema、MLVU、LVBench、MVBench和Video-MME）上的大量实验表明，我们的方法达到了最先进的性能和卓越的效率。", "conclusion": "Flash-VStream通过其创新的Flash记忆模块有效解决了长视频理解中的计算和内存挑战，实现了实时处理能力和优越的性能。", "translation": "受益于大型语言模型和跨模态对齐的进步，现有的多模态大型语言模型在图像和短视频理解方面取得了显著的性能。然而，长视频的理解仍然具有挑战性，因为其长上下文特性导致显著的计算和内存开销。大多数现有工作将长视频与短视频同等对待，这对于实际应用来说效率低下，并且难以推广到更长的视频。为了解决这些问题，我们提出了Flash-VStream，一个能够处理超长视频并实时响应用户查询的高效视频语言模型。我们特别设计了一个Flash记忆模块，其中包含一个低容量的上下文记忆来聚合长上下文时间信息并建模信息密度分布，以及一个高容量的增强记忆，用于根据该分布检索详细的空间信息。与现有模型相比，Flash-VStream显著降低了推理延迟。在长视频基准测试和综合视频基准测试（如EgoSchema、MLVU、LVBench、MVBench和Video-MME）上的大量实验表明，我们的方法达到了最先进的性能和卓越的效率。代码可在https://github.com/IVGSZ/Flash-VStream获取。", "summary": "Flash-VStream是一个针对长视频流实时理解的高效视频语言模型。针对长视频处理中存在的计算和内存开销问题，该模型引入了Flash记忆模块，其中包含低容量上下文记忆用于时间信息聚合和信息密度建模，以及高容量增强记忆用于细节空间信息检索。实验证明，Flash-VStream显著降低了推理延迟，并在多个长视频和综合视频基准测试中实现了最先进的性能和卓越的效率。", "keywords": "长视频理解, 实时处理, 视频语言模型, Flash记忆, 效率", "comments": "Flash-VStream的创新点在于其独特的Flash记忆模块设计，通过区分上下文记忆和增强记忆，有效解决了长视频理解中计算和内存效率的瓶颈，使其能够实时处理超长视频，这对于实际应用具有重要意义。"}}
{"id": "2507.17773", "title": "MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation", "authors": ["Zhongzhen Wen", "Yinghui Zhang", "Zhong Li", "Zhongxin Liu", "Linna Xie", "Tian Zhang"], "categories": ["cs.DC", "cs.LG", "cs.PF", "cs.SE"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17773v1", "summary": "The automatic generation of deep learning (DL) kernels using large language\nmodels (LLMs) has emerged as a promising approach to reduce the manual effort\nand hardware-specific expertise required for writing high-performance operator\nimplementations. However, existing benchmarks for evaluating LLMs in this\ndomain suffer from limited hardware support, coarse-grained kernel\ncategorization, and imbalanced task coverage. To address these limitations, we\nintroduce MultiKernelBench, the first comprehensive, multi-platform benchmark\nfor LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14\nwell-defined kernel categories and supports three major hardware platforms:\nNvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we\ndesign a modular backend abstraction layer that decouples platform-specific\nlogic from the core benchmarking infrastructure, allowing easy integration of\nnew hardware platforms. We further propose a simple yet effective\ncategory-aware one-shot prompting method that improves generation quality by\nproviding in-category exemplars. Through systematic evaluations of seven\nstate-of-the-art LLMs, we reveal significant variation in task difficulty, poor\ngeneralization to platforms with less training exposure, and the effectiveness\nof targeted prompting strategies. MultiKernelBench is publicly available at\nhttps://github.com/wzzll123/MultiKernelBench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17773v1", "cate": "cs.DC", "date": "2025-07-20", "updated": "2025-07-20", "AI": {"title_translation": "MultiKernelBench：一个用于内核生成的跨平台基准测试", "tldr": "MultiKernelBench是一个用于评估大型语言模型（LLMs）生成深度学习内核能力的跨平台、综合性基准测试，它解决了现有基准测试的局限性，并提出了一个有效的提示方法。", "motivation": "现有用于评估大型语言模型（LLMs）在深度学习（DL）内核生成方面能力的基准测试存在硬件支持有限、内核分类粗糙和任务覆盖不平衡的问题，这阻碍了LLMs在该领域性能的全面评估。", "method": "本研究引入了MultiKernelBench，这是第一个用于基于LLM的DL内核生成的综合性、多平台基准测试。MultiKernelBench涵盖了14个定义明确的内核类别中的285个任务，并支持Nvidia GPU、华为NPU和Google TPU三大硬件平台。为实现未来可扩展性，设计了一个模块化后端抽象层。此外，还提出了一种简单而有效的类别感知单次提示方法，通过提供类别内示例来提高生成质量。", "result": "通过对七个最先进的LLM进行系统评估，结果揭示了任务难度的显著差异、对训练暴露较少的平台泛化能力差，以及目标提示策略的有效性。", "conclusion": "MultiKernelBench是一个全面的多平台基准测试，旨在解决现有LLM深度学习内核生成评估工具的局限性，并为未来研究提供了一个可扩展的平台。它揭示了LLM在该领域表现的复杂性，并验证了特定提示策略的有效性。", "translation": "大型语言模型（LLM）自动生成深度学习（DL）内核已成为一种很有前景的方法，可以减少编写高性能算子实现所需的人工工作和硬件特定专业知识。然而，该领域现有评估LLM的基准测试存在硬件支持有限、内核分类粗糙和任务覆盖不平衡的问题。为解决这些局限性，我们引入了MultiKernelBench，这是第一个用于基于LLM的DL内核生成的综合性、多平台基准测试。MultiKernelBench涵盖了14个定义明确的内核类别中的285个任务，并支持三大主流硬件平台：Nvidia GPU、华为NPU和Google TPU。为实现未来可扩展性，我们设计了一个模块化后端抽象层，将平台特定逻辑与核心基准测试基础设施解耦，从而易于集成新的硬件平台。我们进一步提出了一种简单而有效的类别感知单次提示方法，通过提供类别内示例来提高生成质量。通过对七个最先进的LLM进行系统评估，我们揭示了任务难度的显著差异、对训练暴露较少的平台泛化能力差，以及目标提示策略的有效性。MultiKernelBench已在https://github.com/wzzll123/MultiKernelBench上公开可用。", "summary": "本论文介绍了MultiKernelBench，一个用于评估大型语言模型（LLM）生成深度学习内核能力的综合性、跨平台基准测试。该基准测试旨在解决现有评估工具在硬件支持、内核分类和任务覆盖方面的不足。MultiKernelBench包含285个任务，涵盖14个内核类别，并支持Nvidia GPU、华为NPU和Google TPU。它还引入了一个模块化后端抽象层以增强可扩展性，并提出了一种类别感知的单次提示方法以提高生成质量。通过对七个LLM的评估，研究发现任务难度差异大，LLM对不常训练的平台泛化能力弱，但目标提示策略有效。", "keywords": "深度学习内核生成, 大型语言模型, 基准测试, 多平台, MultiKernelBench", "comments": "MultiKernelBench的创新之处在于其作为首个综合性、多平台LLM深度学习内核生成基准测试，极大地扩展了现有评估的范围和深度。其模块化设计确保了未来的可扩展性，而提出的类别感知提示方法也为提升LLM生成质量提供了实用策略。该工作对于推动LLM在高性能计算领域的应用和研究具有重要意义。"}}
{"id": "2507.00566", "title": "Zero-Shot Skeleton-Based Action Recognition With Prototype-Guided Feature Alignment", "authors": ["Kai Zhou", "Shuhai Zhang", "Zeng You", "Jinwu Hu", "Mingkui Tan", "Fei Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper is accepted by IEEE TIP 2025 (The journal version is available at this https URL ). Code is publicly available at this https URL", "url": "http://arxiv.org/abs/2507.00566v2", "summary": "Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.", "comment": "This paper is accepted by IEEE TIP 2025 (The journal version is\n  available at https://doi.org/10.1109/TIP.2025.3586487). Code is publicly\n  available at https://github.com/kaai520/PGFA", "pdf_url": "http://arxiv.org/pdf/2507.00566v2", "cate": "cs.CV", "date": "2025-07-01", "updated": "2025-07-24", "AI": {"title_translation": "零样本骨架行为识别与原型引导特征对齐", "tldr": "提出PGFA方法解决零样本骨架行为识别中特征判别不足和对齐偏差问题，通过跨模态对比训练和原型引导策略显著提升了识别精度。", "motivation": "零样本骨架行为识别任务面临从已知到未知动作泛化困难的挑战。现有方法存在骨架特征判别能力不足和测试时骨架与未知文本特征对齐偏差的问题。", "method": "提出原型引导特征对齐（PGFA）范式。具体包括：1) 开发端到端跨模态对比训练框架，以改善骨架-文本对齐并增强骨架特征的判别力。2) 引入原型引导文本特征对齐策略，以减轻测试时分布差异的不利影响。", "result": "在NTU-60、NTU-120和PKU-MMD三个数据集上，PGFA相较于顶级竞争方法SMIE，分别实现了22.96%、12.53%和18.54%的绝对精度提升。", "conclusion": "本文提出的PGFA方法通过解决现有零样本骨架行为识别方法中的关键问题，显著提高了该任务的性能，证明了其在处理未知动作分类方面的有效性。", "translation": "零样本骨架行为识别旨在对训练期间未曾接触过的骨架人体动作进行分类。由于从已知动作泛化到未知动作的困难，这项任务极具挑战性。以往的研究通常采用两阶段训练：使用交叉熵损失在已知动作类别上预训练骨架编码器，然后对预提取的骨架和文本特征进行对齐，通过骨架-文本对齐和语言模型的泛化能力将知识转移到未知类别。然而，它们的有效性受到以下两点阻碍：1) 骨架特征的判别力不足，因为固定的骨架编码器未能捕获有效骨架-文本对齐所需的必要对齐信息；2) 测试时忽略了骨架和未知文本特征之间的对齐偏差。为此，我们提出了一种用于零样本骨架行为识别的原型引导特征对齐范式，命名为PGFA。具体而言，我们开发了一个端到端跨模态对比训练框架，以改善骨架-文本对齐，确保骨架特征具有足够的判别力。此外，我们引入了一种原型引导文本特征对齐策略，以减轻测试时分布差异的不利影响。我们提供了理论分析来支持我们的原型引导文本特征对齐策略，并在三个知名数据集上对我们的整体PGFA进行了实证评估。与顶级竞争方法SMIE相比，我们的PGFA在NTU-60、NTU-120和PKU-MMD数据集上分别实现了22.96%、12.53%和18.54%的绝对精度提升。", "summary": "本文针对零样本骨架行为识别中现有方法存在的骨架特征判别不足和测试时对齐偏差问题，提出了一种名为PGFA的原型引导特征对齐范式。PGFA通过端到端跨模态对比训练增强骨架特征的判别力和骨架-文本对齐，并引入原型引导文本特征对齐策略以缓解测试时的分布差异。实验结果表明，PGFA在多个数据集上显著优于现有SOTA方法。", "keywords": "零样本学习, 骨架行为识别, 特征对齐, 原型学习, 跨模态学习", "comments": "本文的创新点在于提出了PGFA范式，通过结合跨模态对比学习和原型引导策略，有效解决了零样本骨架行为识别中的两大关键挑战：特征判别力不足和对齐偏差。其端到端训练框架和理论分析进一步增强了方法的鲁棒性和说服力。在多个主流数据集上取得的显著性能提升，表明该方法在实际应用中具有重要潜力。"}}
{"id": "2507.18102", "title": "Regional Frequency-Constrained Planning for the Optimal Sizing of Power Systems via Enhanced Input Convex Neural Networks", "authors": ["Yi Wang", "Goran Strbac"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18102v1", "summary": "Large renewable penetration has been witnessed in power systems, resulting in\nreduced levels of system inertia and increasing requirements for frequency\nresponse services. There have been plenty of studies developing\nfrequency-constrained models for power system security. However, most existing\nliterature only considers uniform frequency security, while neglecting\nfrequency spatial differences in different regions. To fill this gap, this\npaper proposes a novel planning model for the optimal sizing problem of power\nsystems, capturing regional frequency security and inter-area frequency\noscillations. Specifically, regional frequency constraints are first extracted\nvia an enhanced input convex neural network (ICNN) and then embedded into the\noriginal optimisation for frequency security, where a principled weight\ninitialisation strategy is adopted to deal with the gradient vanishing issues\nof non-negative weights in traditional ICNNs and enhance its fitting ability.\nAn adaptive genetic algorithm with sparsity calculation and local search is\ndeveloped to separate the planning model into two stages and effectively solve\nit iteratively. Case studies have been conducted on three different power\nsystems to verify the effectiveness of the proposed frequency-constrained\nplanning model in ensuring regional system security and obtaining realistic\ninvestment decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18102v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于增强型输入凸神经网络的电力系统最优规模区域频率约束规划", "tldr": "本文提出一种新的电力系统规划模型，通过增强型输入凸神经网络（ICNN）考虑区域频率安全和区域间频率振荡，以应对可再生能源渗透带来的频率响应挑战。", "motivation": "现有文献大多只考虑统一的频率安全，而忽略了不同区域的频率空间差异。随着可再生能源渗透率的增加，电力系统惯性降低，对频率响应服务的要求提高，因此需要考虑区域频率约束的规划模型。", "method": "1. 通过增强型输入凸神经网络（ICNN）提取区域频率约束，并嵌入到优化模型中。该ICNN采用了一种原则性的权重初始化策略，以解决传统ICNN中非负权重的梯度消失问题并增强拟合能力。2. 开发了一种带有稀疏度计算和局部搜索的自适应遗传算法，将规划模型分为两个阶段并有效地迭代求解。", "result": "在三个不同的电力系统上进行了案例研究，验证了所提出的频率约束规划模型在确保区域系统安全和获得实际投资决策方面的有效性。", "conclusion": "通过考虑区域频率约束和区域间频率振荡，本文提出的规划模型能够有效地解决电力系统最优规模问题，并确保区域系统安全，提供更实际的投资决策。", "translation": "电力系统中可再生能源的大量渗透导致系统惯性水平降低，并增加了对频率响应服务的需求。目前已有大量研究开发了用于电力系统安全的频率约束模型。然而，大多数现有文献只考虑统一的频率安全，而忽略了不同区域的频率空间差异。为了弥补这一空白，本文提出了一种新颖的电力系统最优规模规划模型，该模型捕获了区域频率安全和区域间频率振荡。具体而言，首先通过增强型输入凸神经网络（ICNN）提取区域频率约束，然后将其嵌入到原始的频率安全优化中，其中采用了一种原则性的权重初始化策略来处理传统ICNN中非负权重的梯度消失问题并增强其拟合能力。开发了一种带有稀疏度计算和局部搜索的自适应遗传算法，将规划模型分为两个阶段并有效地迭代求解。在三个不同的电力系统上进行了案例研究，以验证所提出的频率约束规划模型在确保区域系统安全和获得实际投资决策方面的有效性。", "summary": "本文针对电力系统中大规模可再生能源渗透导致的频率安全问题，提出了一种新的电力系统最优规模规划模型。该模型通过增强型输入凸神经网络（ICNN）提取并整合区域频率约束和区域间频率振荡，解决了现有研究忽略频率空间差异的不足。为提升ICNN的性能，引入了新的权重初始化策略。同时，开发了一种自适应遗传算法来有效求解该两阶段规划模型。通过在不同电力系统上的案例研究，验证了该模型在确保区域系统安全和支持实际投资决策方面的有效性。", "keywords": "区域频率约束, 电力系统规划, 输入凸神经网络, 频率安全, 遗传算法", "comments": "该论文的创新点在于引入增强型输入凸神经网络（ICNN）来捕捉区域频率约束和区域间频率振荡，这弥补了现有研究仅考虑统一频率安全而忽略空间差异的不足。通过改进ICNN的权重初始化策略，解决了梯度消失问题，提升了模型的拟合能力。结合自适应遗传算法进行求解，也显示了其工程实用性。这对于高比例可再生能源渗透下电力系统的精细化规划具有重要意义。"}}
{"id": "2507.18135", "title": "Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy", "authors": ["Kesheng Wang", "Xiaoyu Chen", "Chunlei He", "Fenfen Li", "Xinxin Yu", "Dexing Kong", "Shoujun Huang", "Qi Dai"], "categories": ["cs.CV", "cs.IT", "math.IT"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This manuscript contains 7 figures. All comments are welcome", "url": "http://arxiv.org/abs/2507.18135v1", "summary": "In the medical image analysis field, precise quantification of curve\ntortuosity plays a critical role in the auxiliary diagnosis and pathological\nassessment of various diseases. In this study, we propose a novel framework for\ntortuosity quantification and demonstrate its effectiveness through the\nevaluation of meibomian gland atrophy uniformity,serving as a representative\napplication scenario.\n  We introduce an information entropy-based tortuosity quantification framework\nthat integrates probability modeling with entropy theory and incorporates\ndomain transformation of curve data. Unlike traditional methods such as\ncurvature or arc-chord ratio, this approach evaluates the tortuosity of a\ntarget curve by comparing it to a designated reference curve. Consequently, it\nis more suitable for tortuosity assessment tasks in medical data where\nbiologically plausible reference curves are available, providing a more robust\nand objective evaluation metric without relying on idealized straight-line\ncomparisons.\n  First, we conducted numerical simulation experiments to preliminarily assess\nthe stability and validity of the method. Subsequently, the framework was\napplied to quantify the spatial uniformity of meibomian gland atrophy and to\nanalyze the difference in this uniformity between \\textit{Demodex}-negative and\n\\textit{Demodex}-positive patient groups. The results demonstrated a\nsignificant difference in tortuosity-based uniformity between the two groups,\nwith an area under the curve of 0.8768, sensitivity of 0.75, and specificity of\n0.93. These findings highlight the clinical utility of the proposed framework\nin curve tortuosity analysis and its potential as a generalizable tool for\nquantitative morphological evaluation in medical diagnostics.", "comment": "This manuscript contains 7 figures. All comments are welcome", "pdf_url": "http://arxiv.org/pdf/2507.18135v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于信息熵的睑板腺不均匀萎缩弯曲度量化框架", "tldr": "提出了一种基于信息熵的曲线弯曲度量化新框架，并通过睑板腺萎缩评估验证了其在医学诊断中的有效性。", "motivation": "在医学图像分析领域，精确量化曲线弯曲度对各种疾病的辅助诊断和病理评估至关重要。传统方法如曲率或弦弧比不适用于医学数据中存在生物学上合理参考曲线的弯曲度评估任务。", "method": "本文提出了一种基于信息熵的弯曲度量化框架，该框架结合了概率建模、熵理论和曲线数据的域变换。它通过将目标曲线与指定的参考曲线进行比较来评估弯曲度，而非依赖理想化的直线比较。首先进行了数值模拟实验评估其稳定性和有效性，随后应用于量化睑板腺萎缩的空间均匀性，并分析了蠕形螨阴性与蠕形螨阳性患者组之间的差异。", "result": "数值模拟初步评估了方法的稳定性和有效性。将该框架应用于睑板腺萎缩均匀性量化，结果显示在蠕形螨阴性与蠕形螨阳性患者组之间，基于弯曲度的均匀性存在显著差异，曲线下面积为0.8768，敏感性0.75，特异性0.93。", "conclusion": "该框架在曲线弯曲度分析中具有临床实用性，并有望成为医学诊断中定量形态评估的通用工具。", "translation": "在医学图像分析领域，精确量化曲线弯曲度在各种疾病的辅助诊断和病理评估中起着关键作用。在本研究中，我们提出了一种新颖的弯曲度量化框架，并通过评估睑板腺萎缩均匀性（作为代表性应用场景）来证明其有效性。我们引入了一种基于信息熵的弯曲度量化框架，该框架将概率建模与熵理论相结合，并融入了曲线数据的域变换。与曲率或弦弧比等传统方法不同，这种方法通过将目标曲线与指定的参考曲线进行比较来评估目标曲线的弯曲度。因此，它更适用于医学数据中存在生物学上合理的参考曲线的弯曲度评估任务，提供了一种更稳健、更客观的评估指标，而不依赖于理想化的直线比较。首先，我们进行了数值模拟实验，初步评估了该方法的稳定性和有效性。随后，该框架被应用于量化睑板腺萎缩的空间均匀性，并分析了蠕形螨阴性与蠕形螨阳性患者组之间这种均匀性的差异。结果表明，两组之间基于弯曲度的均匀性存在显著差异，曲线下面积为0.8768，敏感性为0.75，特异性为0.93。这些发现突出了所提出的框架在曲线弯曲度分析中的临床实用性及其作为医学诊断中定量形态评估的通用工具的潜力。", "summary": "本文提出了一种新颖的基于信息熵的曲线弯曲度量化框架，旨在解决医学图像分析中曲线弯曲度精确量化的需求。该框架通过将目标曲线与生物学上合理的参考曲线进行比较来评估弯曲度，不同于传统依赖理想直线的评估方法。通过数值模拟和在睑板腺萎缩均匀性评估中的应用，研究证明了其稳定性和有效性，并成功区分了不同患者群体，显示出其在医学诊断中的潜在临床价值和通用性。", "keywords": "信息熵, 弯曲度量化, 睑板腺萎缩, 医学图像分析, 参考曲线", "comments": "该研究创新性地将信息熵引入曲线弯曲度量化，特别适用于医学图像中存在生物学参考曲线的场景，弥补了传统方法依赖理想直线比较的不足。其在睑板腺萎缩评估中的成功应用，证明了其在临床诊断中的实用性和作为通用工具的潜力，为医学形态学定量评估提供了新的视角和方法。"}}
{"id": "2507.18143", "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18143v1", "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18143v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "HIVMedQA：评估大型语言模型在艾滋病医疗决策支持中的应用", "tldr": "本研究引入了HIVMedQA基准，评估了LLMs在艾滋病医疗决策支持中的能力，发现Gemini 2.5 Pro表现最佳，但模型仍需改进以应对复杂性和偏见。", "motivation": "大型语言模型（LLMs）在临床决策支持中具有潜力，尤其在复杂的艾滋病管理中，但对其准确性、潜在危害和临床接受度存在担忧。艾滋病护理中的AI应用及LLM基准研究不足，因此需要评估LLMs在艾滋病管理中的当前能力、优势和局限性。", "method": "研究引入了HIVMedQA，一个旨在评估艾滋病护理中开放式医学问答的基准。数据集包含与传染病医生合作开发的临床相关问题。评估了七个通用型和三个医学专业型LLMs，并应用提示工程提高性能。评估框架结合了词汇相似性和“LLM作为评判”的方法，并扩展以反映临床相关性。评估维度包括问题理解、推理、知识回忆、偏见、潜在危害和事实准确性。", "result": "Gemini 2.5 Pro在大多数维度上持续优于其他模型，但前三名中有两个是专有模型。性能随问题复杂性增加而下降。医学微调模型不总优于通用模型，且模型大小不是性能的可靠预测指标。推理和理解比事实回忆更具挑战性，并观察到近期效应和现状偏见等认知偏见。", "conclusion": "研究结果强调了需要有针对性的开发和评估，以确保LLM安全有效地整合到临床护理中。", "translation": "大型语言模型（LLMs）正成为支持临床医生日常决策的重要工具。艾滋病管理因其复杂性，包括多样的治疗方案、合并症和依从性挑战，是一个引人注目的应用案例。然而，将LLMs整合到临床实践中引发了对其准确性、潜在危害和临床医生接受度的担忧。尽管前景广阔，但AI在艾滋病护理中的应用仍未得到充分探索，且LLM基准研究稀缺。本研究评估了LLMs在艾滋病管理中的当前能力，突出了它们的优势和局限性。我们引入了HIVMedQA，一个旨在评估艾滋病护理中开放式医学问答的基准。该数据集包含与传染病医生合作开发的、经过精心策划的临床相关问题。我们评估了七个通用型和三个医学专业型LLMs，并应用提示工程来提高性能。我们的评估框架结合了词汇相似性和“LLM作为评判”的方法，并进行了扩展以更好地反映临床相关性。我们评估了关键维度上的性能：问题理解、推理、知识回忆、偏见、潜在危害和事实准确性。结果显示，Gemini 2.5 Pro在大多数维度上持续优于其他模型。值得注意的是，排名前三的模型中有两个是专有模型。性能随问题复杂性增加而下降。医学微调模型并不总是优于通用模型，且更大的模型大小并非性能的可靠预测指标。推理和理解比事实回忆更具挑战性，并观察到近期效应和现状偏见等认知偏见。这些发现强调了需要有针对性的开发和评估，以确保LLM安全有效地整合到临床护理中。", "summary": "本研究引入了HIVMedQA，这是一个用于评估大型语言模型（LLMs）在艾滋病医疗决策支持中能力的基准数据集。该研究评估了多种通用和医学专业LLMs，使用新颖的评估框架，并发现Gemini 2.5 Pro表现最佳。研究揭示了LLMs在艾滋病管理中的优势和局限性，特别是在处理复杂问题、推理和认知偏见方面面临挑战，强调了为安全有效整合LLMs进行有针对性开发和评估的必要性。", "keywords": "大型语言模型, 艾滋病, 医疗决策支持, 基准测试, HIVMedQA", "comments": "这项研究具有重要的创新性，它创建了首个专门针对艾滋病护理领域的开放式医学问答基准数据集HIVMedQA，填补了该领域LLM评估的空白。通过评估多种通用和专业LLM，并采用结合词汇相似性和LLM-as-a-judge的评估框架，为理解LLM在复杂医疗场景中的表现提供了宝贵见解。研究结果揭示了现有LLM在处理复杂推理和认知偏见方面的局限性，对于未来安全有效地将LLM整合到临床实践中具有指导意义。"}}
{"id": "2412.18003", "title": "Integrated Learning and Optimization for Congestion Management and Profit Maximization in Real-Time Electricity Market", "authors": ["Imran Pervez", "Ricardo Pinto Lima", "Omar Knio"], "categories": ["eess.SY", "cs.AI", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.18003v3", "summary": "We develop novel integrated learning and optimization (ILO) methodologies to\nsolve economic dispatch (ED) and DC optimal power flow (DCOPF) problems for\nbetter economic operation. The optimization problem for ED is formulated with\nload being an unknown parameter while DCOPF consists of load and power transfer\ndistribution factor (PTDF) matrix as unknown parameters. PTDF represents the\nincremental variations of real power on transmission lines which occur due to\nreal power transfers between two regions. These values represent a linearized\napproximation of power flows over the transmission lines. We develop novel ILO\nformulations to solve post-hoc penalties in electricity market and line\ncongestion problems using ED and DCOPF optimization formulations. Our proposed\nmethodologies capture the real-time electricity market and line congestion\nbehavior to train the regret function which eventually train unknown loads at\ndifferent buses and line PTDF matrix to achieve the afore-mentioned post-hoc\ngoals. The proposed methodology is compared to sequential learning and\noptimization (SLO) which train load and PTDF forecasts for accuracy rather than\neconomic operation. Our experimentation prove the superiority of ILO in\nminimizing the post-hoc penalties in electricity markets and minimizing the\nline congestion thereby improving the economic operation with noticeable\namount.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.18003v3", "cate": "eess.SY", "date": "2024-12-23", "updated": "2025-07-24", "AI": {"title_translation": "实时电力市场中拥堵管理与利润最大化的集成学习与优化", "tldr": "本文提出了一种新颖的集成学习与优化（ILO）方法，用于解决实时电力市场中的经济调度和最优潮流问题，通过学习未知参数和优化遗憾函数来最小化事后惩罚和线路拥堵，从而显著提高经济效益。", "motivation": "为了解决实时电力市场中的经济调度（ED）和直流最优潮流（DCOPF）问题，特别是在负荷和潮流传输分布因子（PTDF）矩阵等参数未知的情况下，以减少事后惩罚和线路拥堵，实现更好的经济运行。", "method": "开发了新颖的集成学习与优化（ILO）方法，将负荷和潮流传输分布因子（PTDF）矩阵作为未知参数纳入经济调度（ED）和直流最优潮流（DCOPF）的优化公式中。通过捕获实时电力市场和线路拥堵行为来训练遗憾函数（regret function），进而训练未知负荷和PTDF矩阵，以达到减少事后惩罚和解决线路拥堵的目标。该方法与仅关注预测准确性的序贯学习与优化（SLO）方法进行了比较。", "result": "实验证明，所提出的集成学习与优化（ILO）方法在最小化电力市场事后惩罚和最小化线路拥堵方面优于序贯学习与优化（SLO）方法，从而显著改善了经济运行。", "conclusion": "提出的集成学习与优化（ILO）方法能够有效解决实时电力市场中的经济调度和线路拥堵问题，通过减少惩罚和拥堵显著提升了经济效益。", "translation": "我们开发了新颖的集成学习与优化（ILO）方法，以解决经济调度（ED）和直流最优潮流（DCOPF）问题，从而实现更好的经济运行。经济调度的优化问题将负荷作为未知参数，而直流最优潮流则将负荷和潮流传输分布因子（PTDF）矩阵作为未知参数。PTDF表示由于两个区域之间的有功功率传输而引起的输电线路有功功率的增量变化。这些值代表了输电线路潮流的线性化近似。我们开发了新颖的ILO公式，利用ED和DCOPF优化公式来解决电力市场中的事后惩罚和线路拥堵问题。我们提出的方法捕获了实时电力市场和线路拥堵行为，以训练遗憾函数，最终训练不同母线上的未知负荷和线路PTDF矩阵，以实现上述事后目标。所提出的方法与序贯学习与优化（SLO）进行了比较，后者训练负荷和PTDF预测以提高准确性，而非经济运行。我们的实验证明了ILO在最小化电力市场事后惩罚和最小化线路拥堵方面的优越性，从而显著改善了经济运行。", "summary": "本文提出了一种新颖的集成学习与优化（ILO）方法，旨在解决实时电力市场中的经济调度（ED）和直流最优潮流（DCOPF）问题。该方法将负荷和潮流传输分布因子（PTDF）矩阵视为未知参数，通过训练遗憾函数来捕捉市场和拥堵行为，从而优化这些未知参数以减少事后惩罚和线路拥堵。实验结果表明，与传统的序贯学习与优化（SLO）方法相比，ILO能更有效地降低电力市场惩罚和线路拥堵，显著提升经济运行效率。", "keywords": "集成学习与优化, 实时电力市场, 经济调度, 线路拥堵, 潮流传输分布因子", "comments": "这篇论文的创新点在于提出了集成学习与优化（ILO）框架，将学习未知参数（如负荷和PTDF）与优化问题（ED和DCOPF）紧密结合，通过优化遗憾函数直接针对经济运行目标（减少惩罚和拥堵），而非仅仅提高预测准确性。这种方法对于实时电力市场中不确定性管理和效率提升具有重要意义。"}}
{"id": "2507.16632", "title": "Step-Audio 2 Technical Report", "authors": ["Boyong Wu", "Chao Yan", "Chen Hu", "Cheng Yi", "Chengli Feng", "Fei Tian", "Feiyu Shen", "Gang Yu", "Haoyang Zhang", "Jingbei Li", "Mingrui Chen", "Peng Liu", "Wang You", "Xiangyu Tony Zhang", "Xingyuan Li", "Xuerui Yang", "Yayue Deng", "Yechang Huang", "Yuxin Li", "Yuxin Zhang", "Zhao You", "Brian Li", "Changyi Wan", "Hanpeng Hu", "Jiangjie Zhen", "Siyu Chen", "Song Yuan", "Xuelin Zhang", "Yimin Jiang", "Yu Zhou", "Yuxiang Yang", "Bingxin Li", "Buyun Ma", "Changhe Song", "Dongqing Pang", "Guoqiang Hu", "Haiyang Sun", "Kang An", "Na Wang", "Shuli Gao", "Wei Ji", "Wen Li", "Wen Sun", "Xuan Wen", "Yong Ren", "Yuankai Ma", "Yufan Lu", "Bin Wang", "Bo Li", "Changxin Miao", "Che Liu", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Donghang Wu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Han Zhang", "Hao Nie", "Haonan Jia", "Hongyu Zhou", "Jianjian Sun", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Jin Yang", "Junzhe Lin", "Kaixiang Li", "Lei Yang", "Liying Shi", "Li Zhou", "Longlong Gu", "Ming Li", "Mingliang Li", "Mingxiao Li", "Nan Wu", "Qi Han", "Qinyuan Tan", "Shaoliang Pang", "Shengjie Fan", "Siqi Liu", "Tiancheng Cao", "Wanying Lu", "Wenqing He", "Wuxun Xie", "Xu Zhao", "Xueqi Li", "Yanbo Yu", "Yang Yang", "Yi Liu", "Yifan Lu", "Yilei Wang", "Yuanhao Ding", "Yuanwei Liang", "Yuanwei Lu", "Yuchu Luo", "Yuhe Yin", "Yumeng Zhan", "Yuxiang Zhang", "Zidong Yang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Yibo Zhu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16632v2", "summary": "This paper presents Step-Audio 2, an end-to-end multi-modal large language\nmodel designed for industry-strength audio understanding and speech\nconversation. By integrating a latent audio encoder and reasoning-centric\nreinforcement learning (RL), Step-Audio 2 achieves promising performance in\nautomatic speech recognition (ASR) and audio understanding. To facilitate\ngenuine end-to-end speech conversation, Step-Audio 2 incorporates the\ngeneration of discrete audio tokens into language modeling, significantly\nenhancing its responsiveness to paralinguistic information such as speaking\nstyles and emotions. To effectively leverage the rich textual and acoustic\nknowledge in real-world data, Step-Audio 2 integrates retrieval-augmented\ngeneration (RAG) and is able to call external tools such as web search to\nmitigate hallucination and audio search to switch timbres. Trained on millions\nof hours of speech and audio data, Step-Audio 2 delivers intelligence and\nexpressiveness across diverse conversational scenarios. Evaluation results\ndemonstrate that Step-Audio 2 achieves state-of-the-art performance on various\naudio understanding and conversational benchmarks compared to other open-source\nand commercial solutions. Please visit\nhttps://github.com/stepfun-ai/Step-Audio2 for more information.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16632v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "Step-Audio 2 技术报告", "tldr": "Step-Audio 2是一个多模态大语言模型，通过整合多种技术实现了先进的语音理解和对话能力。", "motivation": "旨在实现工业级的音频理解和语音对话，解决现有模型在处理副语言信息、利用真实世界数据以及缓解幻觉等方面的不足，并提升模型在多样对话场景中的智能性和表现力。", "method": "Step-Audio 2是一个端到端多模态大语言模型，整合了潜在音频编码器和以推理为中心的强化学习。它将离散音频标记的生成融入语言建模，并集成了检索增强生成（RAG），能够调用外部工具（如网络搜索和音频搜索）来缓解幻觉和切换音色。模型在数百万小时的语音和音频数据上进行训练。", "result": "在自动语音识别（ASR）和音频理解方面表现出色。显著增强了对语速、情感等副语言信息的响应能力。在各种音频理解和对话基准测试中，与开源和商业解决方案相比，实现了最先进的性能。", "conclusion": "Step-Audio 2在音频理解和对话任务上达到了最先进的水平，展现了在多样对话场景中的智能性和表现力。", "translation": "本文介绍了Step-Audio 2，一个端到端的多模态大语言模型，旨在实现工业级的音频理解和语音对话。通过整合潜在音频编码器和以推理为中心的强化学习（RL），Step-Audio 2在自动语音识别（ASR）和音频理解方面取得了令人满意的性能。为了促进真正的端到端语音对话，Step-Audio 2将离散音频标记的生成融入语言建模，显著增强了其对语速和情感等副语言信息的响应能力。为了有效利用真实世界数据中丰富的文本和声学知识，Step-Audio 2集成了检索增强生成（RAG），并能够调用外部工具，如网络搜索以缓解幻觉，以及音频搜索以切换音色。Step-Audio 2在数百万小时的语音和音频数据上进行训练，在多样化的对话场景中提供了智能性和表现力。评估结果表明，与其他的开源和商业解决方案相比，Step-Audio 2在各种音频理解和对话基准测试中取得了最先进的性能。更多信息请访问https://github.com/stepfun-ai/Step-Audio2。", "summary": "Step-Audio 2是一个针对工业级音频理解和语音对话设计的端到端多模态大语言模型。它通过整合潜在音频编码器、推理强化学习、离散音频标记生成以及检索增强生成（RAG）和外部工具调用等技术，显著提升了在自动语音识别、音频理解以及对副语言信息的响应能力。该模型在数百万小时数据上训练，并在多项基准测试中达到了最先进的性能。", "keywords": "多模态大语言模型, 音频理解, 语音对话, 检索增强生成, 强化学习", "comments": "Step-Audio 2的创新点在于其多模态端到端的设计，特别是将离散音频标记生成融入语言建模以捕捉副语言信息，以及整合RAG和外部工具调用来处理真实世界数据和缓解幻觉。这使其在音频理解和语音对话领域具有重要意义，尤其是在工业应用中展现出强大的潜力和先进性。"}}
{"id": "2507.14660", "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Code is available at this https URL", "url": "http://arxiv.org/abs/2507.14660v2", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "comment": "Code is available at\n  https://github.com/renqibing/MultiAgent4Collusion", "pdf_url": "http://arxiv.org/pdf/2507.14660v2", "cate": "cs.AI", "date": "2025-07-19", "updated": "2025-07-24", "AI": {"title_translation": "当自主性失控时：为社会系统中多智能体串通的风险做准备", "tldr": "本文模拟了恶意多智能体系统（MAS）的串通风险，发现去中心化系统在执行恶意行为方面更有效且难以检测，强调了对更好检测和对抗措施的需求。", "motivation": "现有的AI安全研究主要关注单个AI系统，而多智能体系统（MAS）在复杂现实世界中可能造成的危害（如选举舞弊、金融诈骗等）却未得到充分探索，且AI驱动的群体可能造成类似人类群体协调行动的危害。", "method": "引入了一个概念验证模型，用于模拟恶意多智能体串通的风险。该模型使用一个灵活的框架，支持中心化和去中心化的协调结构，并将其应用于虚假信息传播和电子商务欺诈两个高风险领域。", "result": "研究发现，去中心化系统在执行恶意行为方面比中心化系统更有效。去中心化系统更高的自主性使其能够调整策略并造成更大的损害。即使应用传统干预措施（如内容标记），去中心化群体也能调整策略以避免检测。", "conclusion": "本文揭示了恶意多智能体群体如何运作的关键见解，并强调了开发更好的检测系统和对抗措施的必要性。", "translation": "近期大规模事件，如选举舞弊和金融诈骗，表明人类群体协调努力的危害性。随着自主AI系统的兴起，人们越来越担心AI驱动的群体也可能造成类似的危害。虽然大多数AI安全研究关注单个AI系统，但多智能体系统（MAS）在复杂现实世界中带来的风险仍未得到充分探索。在本文中，我们引入了一个概念验证模型，用于模拟恶意MAS串通的风险，该模型使用一个灵活的框架，支持中心化和去中心化的协调结构。我们将此框架应用于两个高风险领域：虚假信息传播和电子商务欺诈。我们的研究结果表明，去中心化系统在执行恶意行为方面比中心化系统更有效。去中心化系统更高的自主性使其能够调整策略并造成更大的损害。即使应用传统干预措施，如内容标记，去中心化群体也能调整策略以避免检测。我们提出了关于这些恶意群体如何运作的关键见解，以及对更好检测系统和对抗措施的需求。代码可在https://github.com/renqibing/RogueAgent获取。", "summary": "本文探讨了多智能体系统（MAS）在社会系统中潜在的恶意串通风险，这与当前AI安全研究主要关注单个AI系统的趋势形成对比。研究者开发了一个概念验证框架，能够模拟中心化和去中心化协调结构下的恶意MAS行为，并将其应用于虚假信息传播和电子商务欺诈。结果显示，去中心化MAS在执行恶意行为上更有效，且能适应传统检测方法。论文强调了理解这些恶意群体运作方式以及开发更优检测与对抗措施的重要性。", "keywords": "多智能体系统, AI安全, 串通风险, 去中心化, 虚假信息, 电子商务欺诈", "comments": "这篇论文的创新点在于将AI安全研究的焦点从单个AI系统扩展到多智能体系统（MAS）的恶意串通风险，特别是模拟了去中心化MAS的适应性和隐蔽性。其重要性在于揭示了未来AI驱动的恶意行为可能带来的新型挑战，并强调了对更复杂检测和对抗策略的需求，为AI安全领域提供了新的研究方向。"}}
{"id": "2507.18575", "title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation", "authors": ["Xinyu Wang", "Jinghua Hou", "Zhe Liu", "Yingying Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures", "url": "http://arxiv.org/abs/2507.18575v1", "summary": "Transformer-based methods have demonstrated remarkable capabilities in 3D\nsemantic segmentation through their powerful attention mechanisms, but the\nquadratic complexity limits their modeling of long-range dependencies in\nlarge-scale point clouds. While recent Mamba-based approaches offer efficient\nprocessing with linear complexity, they struggle with feature representation\nwhen extracting 3D features. However, effectively combining these complementary\nstrengths remains an open challenge in this field. In this paper, we propose\nHybridTM, the first hybrid architecture that integrates Transformer and Mamba\nfor 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid\nStrategy, which combines attention and Mamba at a finer granularity, enabling\nsimultaneous capture of long-range dependencies and fine-grained local\nfeatures. Extensive experiments demonstrate the effectiveness and\ngeneralization of our HybridTM on diverse indoor and outdoor datasets.\nFurthermore, our HybridTM achieves state-of-the-art performance on ScanNet,\nScanNet200, and nuScenes benchmarks. The code will be made available at\nhttps://github.com/deepinact/HybridTM.", "comment": "7 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.18575v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "HybridTM: 结合 Transformer 和 Mamba 进行 3D 语义分割", "tldr": "提出 HybridTM，首个结合 Transformer 和 Mamba 的 3D 语义分割混合架构，通过内部层混合策略同时捕获长距离依赖和细粒度局部特征，并在多个基准测试中达到最先进性能。", "motivation": "现有的 Transformer 方法在 3D 语义分割中虽强大但二次复杂度限制其长距离依赖建模；Mamba 方法效率高但特征表示能力不足。因此，有效结合两者的优势是一个开放挑战。", "method": "提出 HybridTM，第一个结合 Transformer 和 Mamba 的 3D 语义分割混合架构。此外，提出内部层混合策略（Inner Layer Hybrid Strategy），在更细粒度上结合注意力和 Mamba，以同时捕获长距离依赖和细粒度局部特征。", "result": "大量实验证明 HybridTM 在室内外数据集上的有效性和泛化能力。HybridTM 在 ScanNet、ScanNet200 和 nuScenes 基准测试中取得了最先进的性能。", "conclusion": "HybridTM 通过有效结合 Transformer 和 Mamba 的优势，成功解决了 3D 语义分割中长距离依赖和特征表示的挑战，并在多个基准测试中达到了最先进的性能。", "translation": "基于 Transformer 的方法通过其强大的注意力机制在 3D 语义分割中展现出卓越的能力，但其二次复杂度限制了它们对大规模点云中长距离依赖的建模。虽然最近基于 Mamba 的方法以线性复杂度提供了高效处理，但它们在提取 3D 特征时在特征表示方面表现不佳。然而，有效结合这些互补优势在该领域仍然是一个开放的挑战。在本文中，我们提出了 HybridTM，这是第一个将 Transformer 和 Mamba 集成用于 3D 语义分割的混合架构。此外，我们提出了内部层混合策略（Inner Layer Hybrid Strategy），它以更细的粒度结合了注意力和 Mamba，从而能够同时捕获长距离依赖和细粒度局部特征。大量的实验证明了我们的 HybridTM 在各种室内和室外数据集上的有效性和泛化能力。此外，我们的 HybridTM 在 ScanNet、ScanNet200 和 nuScenes 基准测试中取得了最先进的性能。代码将在 https://github.com/deepinact/HybridTM 提供。", "summary": "本文提出了 HybridTM，一种新颖的混合架构，首次将 Transformer 和 Mamba 结合应用于 3D 语义分割。针对 Transformer 在长距离依赖建模中的二次复杂度和 Mamba 在 3D 特征表示中的不足，HybridTM 引入了内部层混合策略，以细粒度方式整合两者，从而同时捕获长距离依赖和精细局部特征。实验结果表明，HybridTM 在多个室内外数据集上表现出优异的有效性和泛化能力，并在 ScanNet、ScanNet200 和 nuScenes 基准测试中达到了最先进的性能。", "keywords": "3D 语义分割, Transformer, Mamba, 混合架构, 点云", "comments": "这篇论文的创新点在于首次提出了结合 Transformer 和 Mamba 的混合架构 HybridTM，以解决 3D 语义分割中长距离依赖和特征表示的挑战。特别是其“内部层混合策略”在更细粒度上融合两种机制，有效地结合了全局上下文和局部细节，这对于处理大规模点云数据具有重要意义。论文在多个基准测试上达到最先进性能，证明了其方法的有效性和实用性。"}}
{"id": "2503.06079", "title": "Fixing the Pitfalls of Probabilistic Time-Series Forecasting Evaluation by Kernel Quadrature", "authors": ["Masaki Adachi", "Masahiro Fujisawa", "Michael A Osborne"], "categories": ["stat.ML", "cs.LG", "62C10, 62F15"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      11 pages, 6 figures", "url": "http://arxiv.org/abs/2503.06079v2", "summary": "Despite the significance of probabilistic time-series forecasting models,\ntheir evaluation metrics often involve intractable integrations. The most\nwidely used metric, the continuous ranked probability score (CRPS), is a\nstrictly proper scoring function; however, its computation requires\napproximation. We found that popular CRPS estimators--specifically, the\nquantile-based estimator implemented in the widely used GluonTS library and the\nprobability-weighted moment approximation--both exhibit inherent estimation\nbiases. These biases lead to crude approximations, resulting in improper\nrankings of forecasting model performance when CRPS values are close. To\naddress this issue, we introduced a kernel quadrature approach that leverages\nan unbiased CRPS estimator and employs cubature construction for scalable\ncomputation. Empirically, our approach consistently outperforms the two widely\nused CRPS estimators.", "comment": "11 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2503.06079v2", "cate": "stat.ML", "date": "2025-03-08", "updated": "2025-07-24", "AI": {"title_translation": "通过核求积法修正概率时间序列预测评估的陷阱", "tldr": "本文提出了一种基于核求积的新方法，用于无偏估计连续分位数排名分数（CRPS），从而解决了现有CRPS估计器中的偏差问题，并提高了概率时间序列预测模型评估的准确性。", "motivation": "概率时间序列预测模型虽然重要，但其评估指标（如CRPS）的计算通常涉及难以处理的积分，并且现有的CRPS估计器（如GluonTS中基于分位数的估计器和概率加权矩近似）存在固有的估计偏差，导致当CRPS值接近时，预测模型性能的排名不准确。", "method": "本文引入了一种核求积方法，该方法利用无偏CRPS估计器，并采用立方构造进行可扩展计算。", "result": "在经验上，我们提出的方法始终优于两种广泛使用的CRPS估计器。", "conclusion": "通过引入核求积方法，本文成功解决了概率时间序列预测评估中CRPS估计的偏差问题，并提供了一种更准确、可靠的评估方式。", "translation": "尽管概率时间序列预测模型具有重要意义，但其评估指标通常涉及难以处理的积分。最广泛使用的指标——连续分位数排名分数（CRPS）是一个严格的适当评分函数；然而，其计算需要近似。我们发现流行的CRPS估计器——特别是广泛使用的GluonTS库中实现的分位数估计器和概率加权矩近似——都表现出固有的估计偏差。这些偏差导致粗略的近似，当CRPS值接近时，会使预测模型性能的排名不准确。为了解决这个问题，我们引入了一种核求积方法，该方法利用无偏CRPS估计器并采用立方构造进行可扩展计算。从经验上看，我们的方法始终优于两种广泛使用的CRPS估计器。", "summary": "本文针对概率时间序列预测模型评估中连续分位数排名分数（CRPS）估计器的固有偏差问题进行了研究。研究发现，包括GluonTS中基于分位数的估计器在内的流行CRPS估计器存在偏差，导致模型性能排名不准确。为解决此问题，作者提出了一种基于核求积的新方法，该方法利用无偏CRPS估计器并采用立方构造实现可扩展计算。实验结果表明，该方法在性能上优于现有广泛使用的CRPS估计器。", "keywords": "概率时间序列预测, CRPS, 核求积, 评估偏差, 无偏估计", "comments": "本文创新性地提出了核求积方法来解决概率时间序列预测评估中CRPS估计的偏差问题，这对于提高模型评估的准确性和可靠性具有重要意义。通过提供无偏估计，它有助于更准确地比较和选择最佳预测模型。"}}
{"id": "2412.09709", "title": "DiP: A Scalable, Energy-Efficient Systolic Array for Matrix Multiplication Acceleration", "authors": ["Ahmed J. Abdelmaksoud", "Shady Agwa", "Themis Prodromakis"], "categories": ["cs.AR", "cs.DC"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.09709v3", "summary": "Transformers are gaining increasing attention across Natural Language\nProcessing (NLP) application domains due to their outstanding accuracy.\nHowever, these data-intensive models add significant performance demands to the\nexisting computing architectures. Systolic array architectures, adopted by\ncommercial AI computing platforms like Google TPUs, offer energy-efficient data\nreuse but face throughput and energy penalties due to input-output\nsynchronization via First-In-First-Out (FIFO) buffers. This paper proposes a\nnovel scalable systolic array architecture featuring Diagonal-Input and\nPermutated weight stationary (DiP) dataflow for matrix multiplication\nacceleration. The proposed architecture eliminates the synchronization FIFOs\nrequired by state-of-the-art weight stationary systolic arrays. Beyond the\narea, power, and energy savings achieved by eliminating these FIFOs, DiP\narchitecture maximizes the computational resource utilization, achieving up to\n50\\% throughput improvement over conventional weight stationary architectures.\nAnalytical models are developed for both weight stationary and DiP\narchitectures, including latency, throughput, time to full PEs utilization\n(TFPU), and FIFOs overhead. A comprehensive hardware design space exploration\nusing 22nm commercial technology demonstrates DiP's scalability advantages,\nachieving up to a 2.02x improvement in energy efficiency per area. Furthermore,\nDiP outperforms TPU-like architectures on transformer workloads from\nwidely-used models, delivering energy improvement up to 1.81x and latency\nimprovement up to 1.49x. At a 64x64 size with 4096 PEs, DiP achieves a peak\nthroughput of 8.192 TOPS with energy efficiency 9.548 TOPS/W.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.09709v3", "cate": "cs.AR", "date": "2024-12-12", "updated": "2025-07-24", "AI": {"title_translation": "DiP：一种可扩展、节能的矩阵乘法加速脉动阵列", "tldr": "DiP是一种新型的脉动阵列架构，通过消除同步FIFO和优化数据流，显著提高了矩阵乘法加速的吞吐量和能效。", "motivation": "Transformer模型在NLP领域表现出色，但计算量大，对现有计算架构构成性能挑战。现有脉动阵列（如Google TPU采用的）虽能效高，但因FIFO同步导致吞吐量和能耗损失。", "method": "本文提出了一种名为DiP（Diagonal-Input and Permutated weight stationary）的新型可扩展脉动阵列架构，用于矩阵乘法加速。该架构通过采用对角输入和置换权重固定数据流，消除了传统权重固定脉动阵列所需的同步FIFO。研究还开发了针对两种架构的分析模型，并使用22nm商业技术进行了全面的硬件设计空间探索。", "result": "DiP架构通过消除FIFO实现了面积、功耗和能耗的节省。它将计算资源利用率最大化，使吞吐量比传统权重固定架构提高了高达50%。在硬件设计空间探索中，DiP的每面积能效提高了高达2.02倍。此外，DiP在Transformer工作负载上优于TPU-like架构，能效提高了高达1.81倍，延迟提高了高达1.49倍。在64x64尺寸和4096个PE下，DiP实现了8.192 TOPS的峰值吞吐量和9.548 TOPS/W的能效。", "conclusion": "DiP架构通过消除同步FIFO和优化数据流，显著提高了矩阵乘法加速的吞吐量和能效。它在Transformer工作负载上的优异表现，证明了其在未来AI计算领域，特别是数据密集型模型加速方面的巨大潜力。", "translation": "Transformer因其出色的准确性在自然语言处理（NLP）应用领域受到越来越多的关注。然而，这些数据密集型模型对现有计算架构提出了显著的性能要求。脉动阵列架构，如谷歌TPU等商用AI计算平台所采用的，提供了高能效的数据重用，但由于通过先进先出（FIFO）缓冲器进行输入输出同步，面临吞吐量和能耗损失。本文提出了一种新颖的可扩展脉动阵列架构，其特点是采用对角输入和置换权重固定（DiP）数据流，用于矩阵乘法加速。所提出的架构消除了现有最先进权重固定脉动阵列所需的同步FIFO。除了通过消除这些FIFO实现的面积、功耗和能耗节省外，DiP架构最大限度地提高了计算资源利用率，比传统权重固定架构的吞吐量提高了高达50%。本文为权重固定和DiP架构开发了分析模型，包括延迟、吞吐量、PE完全利用时间（TFPU）和FIFO开销。使用22nm商业技术进行的全面硬件设计空间探索证明了DiP的可扩展性优势，每面积能效提高了高达2.02倍。此外，DiP在广泛使用的模型中的Transformer工作负载上优于类似TPU的架构，能效提高了高达1.81倍，延迟提高了高达1.49倍。在64x64尺寸和4096个PE下，DiP实现了8.192 TOPS的峰值吞吐量和9.548 TOPS/W的能效。", "summary": "本文提出了一种名为DiP的新型可扩展脉动阵列架构，用于高效的矩阵乘法加速，尤其针对Transformer模型。DiP通过采用对角输入和置换权重固定数据流，成功消除了传统脉动阵列中必需的同步FIFO，从而在面积、功耗和能耗方面实现显著节省。实验结果表明，DiP架构不仅将计算资源利用率最大化，使吞吐量比现有架构提高50%，而且在能效和延迟方面也超越了TPU-like架构，展示了其在高性能AI计算中的巨大潜力。", "keywords": "脉动阵列, 矩阵乘法, Transformer, 能效, 硬件加速", "comments": "DiP通过创新性地消除同步FIFO和优化数据流，解决了现有脉动阵列在处理Transformer等大规模模型时面临的吞吐量和能耗瓶颈。其显著的能效和性能提升，特别是在Transformer工作负载上的表现，使其在AI硬件加速领域具有重要意义。该工作为未来高能效、可扩展的AI计算架构设计提供了新的思路。"}}
{"id": "2507.18293", "title": "Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring", "authors": ["Sjoerd van Straten", "Alessandro Padella", "Marwan Hassani"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18293v1", "summary": "Predictive Process Monitoring (PPM) enables forecasting future events or\noutcomes of ongoing business process instances based on event logs. However,\ndeep learning PPM approaches are often limited by the low variability and small\nsize of real-world event logs. To address this, we introduce SiamSA-PPM, a\nnovel self-supervised learning framework that combines Siamese learning with\nStatistical Augmentation for Predictive Process Monitoring. It employs three\nnovel statistically grounded transformation methods that leverage control-flow\nsemantics and frequent behavioral patterns to generate realistic, semantically\nvalid new trace variants. These augmented views are used within a Siamese\nlearning setup to learn generalizable representations of process prefixes\nwithout the need for labeled supervision. Extensive experiments on real-life\nevent logs demonstrate that SiamSA-PPM achieves competitive or superior\nperformance compared to the SOTA in both next activity and final outcome\nprediction tasks. Our results further show that statistical augmentation\nsignificantly outperforms random transformations and improves variability in\nthe data, highlighting SiamSA-PPM as a promising direction for training data\nenrichment in process prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18293v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用数据增强和孪生学习进行预测性过程监控", "tldr": "SiamSA-PPM是一个结合孪生学习和统计增强的自监督学习框架，用于解决预测性过程监控中事件日志数据不足的问题，并在实际数据集上表现出色。", "motivation": "深度学习预测性过程监控（PPM）方法常受限于真实世界事件日志的低变异性和小规模，这限制了其性能。", "method": "本文引入了SiamSA-PPM，一个结合孪生学习和统计增强（Statistical Augmentation）的新型自监督学习框架。它采用三种基于统计的转换方法，利用控制流语义和频繁行为模式生成逼真且语义有效的新轨迹变体。这些增强视图在孪生学习设置中用于学习过程前缀的可泛化表示，无需标签监督。", "result": "在真实事件日志上的广泛实验表明，SiamSA-PPM在下一次活动和最终结果预测任务中均达到与SOTA相当或更优的性能。结果还显示，统计增强显著优于随机转换，并提高了数据的变异性。", "conclusion": "SiamSA-PPM为过程预测中的训练数据丰富提供了一个有前景的方向，通过结合统计增强和孪生学习有效解决了数据稀缺问题并提升了预测性能。", "translation": "预测性过程监控（PPM）能够根据事件日志预测正在进行的业务流程实例的未来事件或结果。然而，深度学习PPM方法通常受限于真实世界事件日志的低变异性和小规模。为了解决这个问题，我们引入了SiamSA-PPM，一个结合孪生学习和统计增强的预测性过程监控新型自监督学习框架。它采用了三种新颖的基于统计的转换方法，利用控制流语义和频繁行为模式生成逼真、语义有效的新轨迹变体。这些增强视图在孪生学习设置中用于学习过程前缀的可泛化表示，无需标签监督。在真实事件日志上的广泛实验表明，SiamSA-PPM在下一次活动和最终结果预测任务中均达到与SOTA（最先进技术）相当或更优的性能。我们的结果进一步表明，统计增强显著优于随机转换并提高了数据的变异性，突显了SiamSA-PPM作为过程预测中训练数据丰富的一个有前景的方向。", "summary": "本文提出了SiamSA-PPM，一个针对预测性过程监控（PPM）的自监督学习框架，旨在解决真实事件日志数据稀缺和变异性低的问题。SiamSA-PPM结合了孪生学习和统计增强，通过利用控制流语义和频繁模式生成逼真且语义有效的轨迹变体，从而在无监督情况下学习可泛化的过程前缀表示。实验证明，SiamSA-PPM在预测下一活动和最终结果方面表现出与现有最佳方法相当或更优的性能，并突出了统计增强在数据丰富方面的显著优势。", "keywords": "预测性过程监控, 数据增强, 孪生学习, 自监督学习, 事件日志", "comments": "该论文的创新点在于提出了SiamSA-PPM框架，巧妙地结合了自监督学习、孪生网络和统计增强来解决预测性过程监控中实际数据不足的挑战。其核心贡献是引入了基于统计的转换方法，这些方法能够生成高质量、语义有效的增强数据，从而在没有标注的情况下提升模型泛化能力。这对于依赖大量高质量数据的深度学习模型而言，是一个重要的突破，特别是在业务流程管理等数据获取成本高或数据稀疏的领域。"}}
{"id": "2401.17256", "title": "Weak-to-Strong Jailbreaking on Large Language Models", "authors": ["Xuandong Zhao", "Xianjun Yang", "Tianyu Pang", "Chao Du", "Lei Li", "Yu-Xiang Wang", "William Yang Wang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2401.17256v5", "summary": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient inference time attack for\naligned LLMs to produce harmful text. Our key intuition is based on the\nobservation that jailbroken and aligned models only differ in their initial\ndecoding distributions. The weak-to-strong attack's key technical insight is\nusing two smaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse open-source LLMs from 3 organizations. The\nresults show our method can increase the misalignment rate to over 99% on two\ndatasets with just one forward pass per example. Our study exposes an urgent\nsafety issue that needs to be addressed when aligning LLMs. As an initial\nattempt, we propose a defense strategy to protect against such attacks, but\ncreating more advanced defenses remains challenging. The code for replicating\nthe method is available at https://github.com/XuandongZhao/weak-to-strong", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2401.17256v5", "cate": "cs.CL", "date": "2024-01-30", "updated": "2025-07-23", "AI": {"title_translation": "大型语言模型上的弱到强越狱攻击", "tldr": "本文提出了一种高效的“弱到强越狱攻击”方法，通过利用两个较小的模型（一个安全和一个不安全）来对抗性地修改一个大型安全模型的解码概率，从而使其生成有害文本。实验证明该方法在多个大型语言模型上能将未对齐率提高到99%以上，且计算成本极低，揭示了LLM对齐中一个紧迫的安全问题。", "motivation": "大型语言模型（LLMs）容易受到越狱攻击，导致生成有害、不道德或有偏见的文本，而现有越狱方法计算成本高昂。", "method": "本文提出弱到强越狱攻击，这是一种高效的推理时间攻击，用于使对齐的LLM生成有害文本。其核心思想是利用两个较小的模型（一个安全模型和一个不安全模型）对抗性地修改一个显著更大的安全模型的解码概率。", "result": "在来自3个组织的5个不同开源LLM上评估了弱到强攻击，结果显示该方法在两个数据集上可以将未对齐率提高到99%以上，每个例子只需一次前向传播。", "conclusion": "弱到强攻击揭示了一个在对齐LLM时需要解决的紧迫安全问题。虽然提出了初步的防御策略，但创建更高级的防御仍然具有挑战性。", "translation": "大型语言模型（LLMs）容易受到越狱攻击——导致有害、不道德或有偏见的文本生成。然而，现有的越狱方法计算成本高昂。在本文中，我们提出了弱到强越狱攻击，这是一种针对已对齐LLM产生有害文本的高效推理时间攻击。我们的关键直觉基于一个观察，即越狱模型和对齐模型仅在初始解码分布上有所不同。弱到强攻击的关键技术洞察是使用两个较小的模型（一个安全模型和一个不安全模型）对抗性地修改一个显著更大的安全模型的解码概率。我们在来自3个组织的5个不同开源LLM上评估了弱到强攻击。结果显示我们的方法在两个数据集上可以将未对齐率提高到99%以上，每个例子只需一次前向传播。我们的研究揭示了一个在对齐LLM时需要解决的紧迫安全问题。作为初步尝试，我们提出了一种防御策略来抵御此类攻击，但创建更高级的防御仍然具有挑战性。复制该方法的代码可在https://github.com/XuandongZhao/weak-to-strong 获取。", "summary": "该论文介绍了一种名为“弱到强越狱攻击”的高效推理时间攻击，旨在使大型语言模型（LLMs）生成有害内容。作者观察到越狱模型与对齐模型在初始解码分布上的差异，并利用两个较小的模型（一安全一不安全）来对抗性地修改一个更大的安全模型的解码概率。实验证明，该方法在多个开源LLMs上能以极低的计算成本（每个例子一次前向传播）将未对齐率提升至99%以上，凸显了LLM对齐中的一个严重安全漏洞。论文也提出了初步的防御策略，但指出更高级的防御仍是挑战。", "keywords": "大型语言模型, 越狱攻击, 弱到强攻击, 模型安全, 对齐", "comments": "这项研究的创新之处在于提出了一种计算效率极高的越狱攻击方法，通过巧妙地利用两个小型模型来影响大型模型的行为，这与现有高成本的方法形成鲜明对比。其重要性在于揭示了LLM对齐过程中一个此前可能被忽视的严重安全漏洞，即即使是经过对齐的大型模型也可能被这种“弱到强”的策略高效地攻破。论文明确指出了对齐LLM面临的紧迫安全挑战，并强调了开发更鲁棒防御机制的必要性。"}}
{"id": "2406.03674", "title": "Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions", "authors": ["Negin Golrezaei", "Sourav Sahoo"], "categories": ["cs.DS", "cs.GT", "cs.LG"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      84 pages, 5 figures. Appeared at ICML 2025", "url": "http://arxiv.org/abs/2406.03674v3", "summary": "We study the bidding problem in repeated uniform price multi-unit auctions\nfrom the perspective of a value-maximizing buyer. The buyer aims to maximize\ntheir cumulative value over $T$ rounds while adhering to per-round\nreturn-on-investment (RoI) constraints in a strategic (or adversarial)\nenvironment. Using an $m$-uniform bidding format, the buyer submits $m$\nbid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid $b_i$, with $m \\ll\nM$ in practice, where $M$ denotes the maximum demand of the buyer.\n  We introduce the notion of safe bidding strategies as those that satisfy the\nRoI constraints irrespective of competing bids. Despite the stringent\nrequirement, we show that these strategies satisfy a mild no-overbidding\ncondition, depend only on the valuation curve of the bidder, and the bidder can\nfocus on a finite subset without loss of generality. Though the subset size is\n$O(M^m)$, we design a polynomial-time learning algorithm that achieves\nsublinear regret, both in full-information and bandit settings, relative to the\nhindsight-optimal safe strategy.\n  We assess the robustness of safe strategies against the hindsight-optimal\nstrategy from a richer class. We define the richness ratio $\\alpha \\in (0,1]$\nas the minimum ratio of the value of the optimal safe strategy to that of the\noptimal strategy from richer class and construct hard instances showing the\ntightness of $\\alpha$. Our algorithm achieves $\\alpha$-approximate sublinear\nregret against these stronger benchmarks. Simulations on semi-synthetic auction\ndata show that empirical richness ratios significantly outperform the\ntheoretical worst-case bounds. The proposed safe strategies and learning\nalgorithm extend naturally to more nuanced buyer and competitor models.", "comment": "84 pages, 5 figures. Appeared at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2406.03674v3", "cate": "cs.DS", "date": "2024-06-06", "updated": "2025-07-23", "AI": {"title_translation": "统一价格拍卖中价值最大化买家的安全策略学习", "tldr": "买家在重复统一价格多单位拍卖中，通过学习安全策略来最大化价值并满足投资回报率约束，该策略对竞争性出价不敏感，并设计了实现次线性遗憾的算法。", "motivation": "买家在重复统一价格多单位拍卖中，为了在战略性（或对抗性）环境中最大化累积价值，同时遵守每轮投资回报率（RoI）约束，面临如何出价的问题。", "method": "引入了“安全出价策略”的概念，即无论竞争性出价如何都能满足投资回报率约束的策略。证明了这些策略满足温和的不超价条件，仅依赖于投标者的估值曲线。设计了一个多项式时间学习算法，该算法在完全信息和强盗设置下都能相对于事后最优安全策略实现次线性遗憾。通过定义丰富度比率$\\alpha$来评估安全策略的鲁棒性。", "result": "安全策略满足温和的不超价条件，仅依赖于投标者的估值曲线。所设计的学习算法在完全信息和强盗设置下都能实现相对于事后最优安全策略的次线性遗憾，并且对更强的基准实现了$\\alpha$近似的次线性遗憾。在半合成拍卖数据上的仿真显示，经验丰富度比率显著优于理论最坏情况界限。", "conclusion": "论文提出了在重复统一价格多单位拍卖中，价值最大化买家的安全策略和学习算法。这些策略在满足投资回报率约束的同时表现出鲁棒性，并且所提出的方法可以自然地扩展到更细致的买家和竞争对手模型。", "translation": "我们从价值最大化买家的角度研究重复统一价格多单位拍卖中的投标问题。买家旨在在战略性（或对抗性）环境中，在T轮中最大化其累积价值，同时遵守每轮投资回报率（RoI）约束。采用m-统一投标格式，买家提交m对投标-数量对（bi, qi），以b_i的价格需求q_i单位，实际中m远小于M，其中M表示买家的最大需求。我们引入了安全投标策略的概念，即无论竞争性投标如何都能满足RoI约束的策略。尽管要求严格，我们表明这些策略满足温和的不超价条件，仅依赖于投标者的估值曲线，并且投标者可以在不失一般性的情况下专注于一个有限子集。尽管子集大小为O(M^m)，我们设计了一个多项式时间学习算法，该算法在完全信息和强盗设置下，相对于事后最优安全策略，都能实现次线性遗憾。我们评估了安全策略相对于更丰富类别的事后最优策略的鲁棒性。我们定义了丰富度比率$\\alpha\\in(0,1]$为最优安全策略的价值与更丰富类别最优策略的价值的最小比率，并构建了显示$\\alpha$紧密性的困难实例。我们的算法在这些更强的基准下实现了$\\alpha$近似的次线性遗憾。在半合成拍卖数据上的仿真显示，经验丰富度比率显著优于理论最坏情况界限。所提出的安全策略和学习算法可以自然地扩展到更细致的买家和竞争对手模型。", "summary": "本研究关注重复统一价格多单位拍卖中价值最大化买家的出价问题，目标是在战略环境中最大化累积价值并满足投资回报率约束。论文引入了“安全出价策略”的概念，该策略即使在竞争性出价未知的情况下也能保证投资回报率。研究表明这些安全策略满足不超价条件，仅依赖于估值曲线，且可在有限子集上进行考虑。作者设计了一个多项式时间学习算法，该算法在完全信息和强盗设置下均能实现相对于事后最优安全策略的次线性遗憾。此外，通过定义丰富度比率$\\alpha$来评估安全策略的鲁棒性，并证明算法能实现$\\alpha$近似的次线性遗憾。半合成数据上的仿真结果显示，经验丰富度比率优于理论最坏情况界限。所提出的策略和算法具有良好的可扩展性。", "keywords": "统一价格拍卖, 安全策略, 价值最大化, 机器学习, 次线性遗憾", "comments": "论文的创新点在于引入了“安全出价策略”的概念，该策略在不考虑竞争对手出价的情况下仍能保证投资回报率约束，这在对抗性环境中具有重要意义。此外，设计出多项式时间学习算法并证明其次线性遗憾，解决了实际应用中的可扩展性问题。该研究为拍卖理论中的鲁棒性策略学习提供了新的视角。"}}
{"id": "2507.00698", "title": "Rectifying Magnitude Neglect in Linear Attention", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025, highlight paper", "url": "http://arxiv.org/abs/2507.00698v2", "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "comment": "Accepted by ICCV2025, highlight paper", "pdf_url": "http://arxiv.org/pdf/2507.00698v2", "cate": "cs.CV", "date": "2025-07-01", "updated": "2025-07-24", "AI": {"title_translation": "纠正线性注意力中的幅度忽视", "tldr": "本文提出了一种名为MALA的新型线性注意力机制，通过重新引入Query的幅度信息，使其注意力分数分布更接近Softmax注意力，从而解决了传统线性注意力性能下降的问题，并在多项任务上取得了良好效果。", "motivation": "尽管线性注意力具有线性复杂度，能够高效地进行全局信息建模，但其性能相比标准Softmax注意力有显著下降。本文旨在分析并解决这一问题。", "method": "通过分析发现线性注意力完全忽略了Query的幅度信息，导致注意力分数分布无法动态适应。基于此，提出了一种名为幅度感知线性注意力（MALA）的新方法，修改了线性注意力的计算方式，以充分结合Query的幅度信息。", "result": "MALA能够生成与Softmax注意力高度相似且结构更均衡的注意力分数分布。在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等多项任务上，MALA均取得了优异的性能。", "conclusion": "通过纠正线性注意力中对Query幅度信息的忽视，MALA成功地提升了线性注意力的性能，使其在保持线性复杂度的同时，能够达到接近Softmax注意力的效果，并在多种任务中展现出强大的通用性。", "translation": "作为Transformer的核心操作，Softmax注意力展现出卓越的全局建模能力。然而，其二次复杂度限制了其在视觉任务中的适用性。相比之下，线性注意力与Softmax注意力具有相似的公式，同时实现了线性复杂度，从而能够高效地建模全局信息。尽管如此，与标准Softmax注意力相比，线性注意力存在显著的性能下降。在本文中，我们基于线性注意力的公式分析了这一问题的根本原因。我们发现，与Softmax注意力不同，线性注意力完全忽略了Query的幅度信息。这使得注意力分数分布无法随着Query的尺度变化而动态调整。因此，尽管其结构与Softmax注意力相似，线性注意力却表现出显著不同的注意力分数分布。基于这一观察，我们提出了幅度感知线性注意力（MALA），它修改了线性注意力的计算方式，以充分结合Query的幅度。这一调整使得MALA能够生成与Softmax注意力高度相似且结构更均衡的注意力分数分布。我们在多项任务上评估了MALA的有效性，包括图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成。我们的MALA在所有这些任务上都取得了强劲的结果。代码将在https://github.com/qhfan/MALA 提供。", "summary": "本文针对线性注意力相比Softmax注意力存在的性能下降问题进行了深入分析。研究发现，线性注意力在计算过程中忽视了Query的幅度信息，导致其注意力分数分布无法动态适应并偏离Softmax注意力。为解决此问题，作者提出了幅度感知线性注意力（MALA），通过修改计算方式，将Query的幅度信息重新融入，使MALA生成的注意力分数分布更接近Softmax注意力且结构更均衡。实验结果表明，MALA在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等多个任务上均取得了优异表现。", "keywords": "线性注意力, 幅度忽视, Softmax注意力, Transformer, MALA", "comments": "这篇论文通过深入分析线性注意力与Softmax注意力的差异，精准定位了线性注意力性能下降的关键原因——对Query幅度信息的忽视。MALA的提出不仅具有理论洞察力，而且通过简单的修改有效解决了问题，显著提升了线性注意力在多模态任务上的表现，展现出其在保持计算效率的同时，提升模型表达能力的潜力。"}}
{"id": "2504.04598", "title": "B4P: Simultaneous Grasp and Motion Planning for Object Placement via Parallelized Bidirectional Forests and Path Repair", "authors": ["Benjamin H. Leebron", "Kejia Ren", "Yiting Chen", "Kaiyu Hang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04598v2", "summary": "Robot pick and place systems have traditionally decoupled grasp, placement,\nand motion planning to build sequential optimization pipelines with the\nassumption that the individual components will be able to work together.\nHowever, this separation introduces sub-optimality, as grasp choices may limit\nor even prohibit feasible motions for a robot to reach the target placement\npose, particularly in cluttered environments with narrow passages. To this end,\nwe propose a forest-based planning framework to simultaneously find grasp\nconfigurations and feasible robot motions that explicitly satisfy downstream\nplacement configurations paired with the selected grasps. Our proposed\nframework leverages a bidirectional sampling-based approach to build a start\nforest, rooted at the feasible grasp regions, and a goal forest, rooted at the\nfeasible placement regions, to facilitate the search through randomly explored\nmotions that connect valid pairs of grasp and placement trees. We demonstrate\nthat the framework's inherent parallelism enables superlinear speedup, making\nit scalable for applications for redundant robot arms (e.g., 7 Degrees of\nFreedom) to work efficiently in highly cluttered environments. Extensive\nexperiments in simulation demonstrate the robustness and efficiency of the\nproposed framework in comparison with multiple baselines under diverse\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04598v2", "cate": "cs.RO", "date": "2025-04-06", "updated": "2025-07-24", "AI": {"title_translation": "B4P：通过并行双向森林和路径修复实现物体放置的同步抓取和运动规划", "tldr": "B4P是一种新的机器人规划框架，通过并行双向森林同步优化抓取和运动，解决了传统方法中抓取选择可能限制运动可行性的问题，提高了在杂乱环境中的放置效率和鲁棒性。", "motivation": "传统的机器人抓取和放置系统将抓取、放置和运动规划解耦，导致次优性。因为抓取选择可能会限制甚至阻止机器人到达目标放置姿态的可行运动，尤其是在杂乱且狭窄的环境中。", "method": "本文提出了一种基于森林的规划框架，通过并行化的双向采样方法，同步寻找抓取配置和可行的机器人运动。该框架构建了一个以可行抓取区域为根的起始森林和一个以可行放置区域为根的目标森林，通过随机探索的运动连接有效的抓取和放置树，从而促进搜索过程。", "result": "该框架固有的并行性实现了超线性加速，使其能够扩展应用于冗余机械臂（例如7自由度）在高度杂乱环境中高效工作。大量的仿真实验证明了该框架在与多种基线方法对比下，在不同场景中的鲁棒性和效率。", "conclusion": "该研究成功开发并验证了一个能够同步进行抓取和运动规划的框架，有效解决了传统解耦方法带来的次优问题，显著提高了机器人在复杂环境中的放置效率和鲁棒性。", "translation": "机器人抓取和放置系统传统上将抓取、放置和运动规划解耦，以构建顺序优化流程，并假设各个组件能够协同工作。然而，这种分离引入了次优性，因为抓取选择可能会限制甚至阻止机器人到达目标放置姿态的可行运动，尤其是在杂乱且狭窄的环境中。为此，我们提出了一种基于森林的规划框架，以同步寻找抓取配置和可行的机器人运动，明确满足与所选抓取配对的下游放置配置。我们提出的框架利用双向采样方法构建一个起始森林（以可行抓取区域为根）和一个目标森林（以可行放置区域为根），以促进通过随机探索的运动连接有效的抓取和放置树的搜索。我们证明了该框架固有的并行性实现了超线性加速，使其能够扩展应用于冗余机械臂（例如7自由度）在高度杂乱环境中高效工作。在模拟中进行的广泛实验证明了该框架在多样化场景下与多个基线方法相比的鲁棒性和效率。", "summary": "本文提出了一种名为B4P的基于森林的规划框架，旨在解决传统机器人抓取和放置系统中抓取、放置和运动规划分离导致的次优问题。该框架利用双向采样方法构建起始森林（基于抓取区域）和目标森林（基于放置区域），以同步寻找抓取配置和可行的机器人运动。通过连接有效的抓取和放置树，B4P实现了固有的并行性，展现出超线性加速，使其适用于在高度杂乱环境中高效操作冗余机械臂。仿真实验证明了该框架的鲁棒性和效率。", "keywords": "同步规划, 抓取规划, 运动规划, 双向森林, 机器人放置", "comments": "本文的创新之处在于通过并行双向森林的方法，实现了抓取和运动规划的同步优化，有效解决了传统解耦方法导致的次优问题。这种方法对于在高自由度机器人和复杂杂乱环境中提高机器人操作的效率和鲁棒性具有重要意义。"}}
{"id": "2506.05606", "title": "OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation", "authors": ["Ziyi Wang", "Yuxuan Lu", "Wenbo Li", "Amirali Amini", "Bo Sun", "Yakov Bart", "Weimin Lyu", "Jiri Gesi", "Tian Wang", "Jing Huang", "Yu Su", "Upol Ehsan", "Malihe Alikhani", "Toby Jia-Jun Li", "Lydia Chilton", "Dakuo Wang"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.05606v4", "summary": "Can large language models (LLMs) accurately simulate the next web action of a\nspecific user? While LLMs have shown promising capabilities in generating\n``believable'' human behaviors, evaluating their ability to mimic real user\nbehaviors remains an open challenge, largely due to the lack of high-quality,\npublicly available datasets that capture both the observable actions and the\ninternal reasoning of an actual human user. To address this gap, we introduce\nOPERA, a novel dataset of Observation, Persona, Rationale, and Action collected\nfrom real human participants during online shopping sessions. OPERA is the\nfirst public dataset that comprehensively captures: user personas, browser\nobservations, fine-grained web actions, and self-reported just-in-time\nrationales. We developed both an online questionnaire and a custom browser\nplugin to gather this dataset with high fidelity. Using OPERA, we establish the\nfirst benchmark to evaluate how well current LLMs can predict a specific user's\nnext action and rationale with a given persona and <observation, action,\nrationale> history. This dataset lays the groundwork for future research into\nLLM agents that aim to act as personalized digital twins for human.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.05606v4", "cate": "cs.CL", "date": "2025-06-05", "updated": "2025-07-24", "AI": {"title_translation": "OPeRA：一个用于评估大型语言模型在人类在线购物行为模拟方面的观察、角色、理由和行动数据集", "tldr": "OPERA是一个新的数据集，包含人类在线购物的观察、角色、理由和行动，用于评估大型语言模型（LLMs）模拟真实用户行为的能力，并建立了第一个基准。", "motivation": "大型语言模型（LLMs）在生成“可信”的人类行为方面表现出潜力，但由于缺乏高质量、公开可用的数据集来捕获实际人类用户的可观察行动和内部推理，评估它们模仿真实用户行为的能力仍然是一个开放的挑战。", "method": "引入了OPERA数据集，该数据集从真实人类参与者的在线购物会话中收集了观察、角色、理由和行动。开发了在线问卷和定制浏览器插件，以高保真度收集数据。使用OPERA建立了第一个基准，用于评估当前LLMs在给定角色和<观察、行动、理由>历史的情况下预测特定用户下一步行动和理由的能力。", "result": "OPERA是第一个全面捕获用户角色、浏览器观察、细粒度网络行动和自我报告的即时理由的公共数据集。该数据集为未来研究旨在充当人类个性化数字孪生的大型语言模型代理奠定了基础。", "conclusion": "OPERA数据集填补了评估大型语言模型模拟真实用户行为的数据空白，并为未来开发个性化数字孪生提供了基础。", "translation": "大型语言模型（LLMs）能否准确模拟特定用户的下一个网络行为？虽然LLMs在生成“可信”的人类行为方面表现出有前途的能力，但评估它们模仿真实用户行为的能力仍然是一个开放的挑战，这主要是因为缺乏高质量、公开可用的数据集，这些数据集既能捕捉实际人类用户的可观察行动，又能捕捉其内部推理。为了解决这一差距，我们引入了OPERA，这是一个从真实人类参与者在线购物会话中收集的观察、角色、理由和行动的新颖数据集。OPERA是第一个全面捕获用户角色、浏览器观察、细粒度网络行动和自我报告的即时理由的公共数据集。我们开发了在线问卷和定制浏览器插件，以高保真度收集此数据集。使用OPERA，我们建立了第一个基准，以评估当前LLMs在给定角色和<观察、行动、理由>历史的情况下，预测特定用户的下一步行动和理由的能力。该数据集为未来研究旨在充当人类个性化数字孪生的大型语言模型代理奠定了基础。", "summary": "本文介绍了OPERA数据集，这是一个专为评估大型语言模型模拟人类在线购物行为而设计的新型数据集。该数据集首次全面收集了真实用户的观察、角色、行动和即时理由，旨在解决当前缺乏高质量数据集来评估LLMs模拟真实用户行为的问题。作者通过在线问卷和浏览器插件收集数据，并利用OPERA建立了一个基准，用于评估LLMs预测用户下一步行动和理由的能力，为未来开发个性化数字孪生奠定了基础。", "keywords": "大型语言模型, 用户行为模拟, 在线购物, 数据集, 数字孪生", "comments": "OPERA数据集的创新之处在于它首次公开地全面捕获了人类在线购物行为的四个关键要素：观察、角色、理由和行动，这对于深入理解和模拟复杂的人类行为至关重要。其重要性在于为评估LLMs在真实用户行为模拟方面的能力提供了一个急需的基准，并为开发更高级的个性化数字孪生代理奠定了基础。该数据集的构建方法（结合在线问卷和浏览器插件）确保了数据的高保真度。"}}
{"id": "2507.18055", "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18055v1", "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18055v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用大型语言模型生成具有多样化写作风格的隐私保护合成评论", "tldr": "LLMs生成的合成数据在多样性和隐私方面存在局限，本文提出评估指标并用提示工程改进合成评论的多样性和隐私。", "motivation": "大型语言模型（LLMs）生成的合成数据虽然为模型训练提供了经济高效且可扩展的替代方案，但其多样性和隐私风险尚未得到充分探索。", "method": "本文提出了一套全面的指标来定量评估LLM生成的文本合成数据集的多样性（语言表达、情感、用户视角）和隐私（重识别风险、风格异常值）。基于评估结果，提出了一种基于提示（prompt-based）的方法来增强合成评论的多样性，同时保护评论者隐私。", "result": "实验结果表明，LLMs在生成多样化和隐私保护的合成数据方面存在显著局限性。", "conclusion": "本文提出了一套评估合成数据多样性和隐私的指标，并开发了一种基于提示的方法来改进LLM生成的合成评论的质量，解决了现有LLM在生成多样化和隐私保护合成数据方面的不足。", "translation": "大型语言模型（LLMs）生成的合成数据日益增长的使用，在数据驱动的应用中带来了机遇和挑战。虽然合成数据为现实世界数据提供了经济高效、可扩展的替代方案，以促进模型训练，但其多样性和隐私风险仍未得到充分探索。本文专注于基于文本的合成数据，提出了一套全面的指标来定量评估由几种最先进的LLMs生成的合成数据集的多样性（即语言表达、情感和用户视角）和隐私（即重识别风险和风格异常值）。实验结果揭示了LLMs在生成多样化和隐私保护的合成数据方面的显著局限性。在评估结果的指导下，提出了一种基于提示的方法，以在保护评论者隐私的同时增强合成评论的多样性。", "summary": "本文探讨了大型语言模型（LLMs）在生成合成数据时面临的多样性和隐私挑战。研究提出了一套全面的指标，用于量化评估LLM生成文本合成数据的多样性（包括语言表达、情感、用户视角）和隐私（包括重识别风险、风格异常值）。实验结果揭示了LLMs在生成多样化和隐私保护数据方面的局限性。为解决这些问题，论文提出了一种基于提示的方法，旨在提高合成评论的多样性，同时确保用户隐私。", "keywords": "合成数据, 隐私保护, 大型语言模型, 文本多样性, 提示工程", "comments": "本文创新性地提出了一套量化评估LLM生成合成数据多样性和隐私的指标，并进一步通过提示工程改进了合成数据的质量。这对于推动LLM在数据增强和隐私保护应用中的实际部署具有重要意义。"}}
{"id": "2507.18083", "title": "Numerical Study of Bar Suppression in Galaxy Models Due to Disc Heating", "authors": ["Alejandro López Gómez", "Ruslan Gabbasov", "Isaura Luisa Fuentes-Carrera"], "categories": ["astro-ph.GA", "cs.NA", "math.NA"], "primary_category": "Subjects:       Astrophysics of Galaxies (astro-ph.GA)", "pdf_link": null, "comments": "Comments:      Accepted for publication in the Galaxies journal. 34 pages (including references),12 figures and 2 tables", "url": "http://arxiv.org/abs/2507.18083v1", "summary": "The process of bar formation, evolution and destruction is still a\ncontroversial topic regarding galaxy dynamics. Numerical simulations show that\nthese phenomena strongly depend on physical and numerical parameters. In this\nwork, we study the combined influence of the softening parameter, $\\epsilon$\nand disc mass fraction, $m_{\\mathrm{d}}$ on the formation and evolution of bars\nin isolated disc-halo models via $N$-body simulations with different particle\nresolutions. Previous studies indicate that the bar strength depends on\n$m_{\\mathrm{d}}$ as $\\propto m_{\\mathrm{d}}^{-1}$, which is seen as a delay in\nbar formation. However, the distorsion parameter, $\\eta$, which measures the\nbar's momentum through time, shows that an increase in $m_{\\mathrm{d}}$ does\nnot always induce a delay in bar formation. This suggests that $\\epsilon$\ninteract to either enhance or weaken the bar. Moreover, numerical heating\ndominates in models with small softening values, creating highly accelerated\nparticles at the centre of discs, regardless of $m_{\\mathrm{d}}$ or resolution.\nThese enhanced particle accelerations produce chaotic orbits for $\\epsilon \\leq\n5\\,$pc, resulting in bar suppression due to collisional dynamics in the centre.\nIn our high resolution models ($N \\approx 10^{7}$), small softening values are\nincapable of reproducing the bar instability. The role of disc mass is as\nfollows: increasing $m_{\\mathrm{d}}$ for moderate $\\epsilon$ ($\\geq 10\\,$pc)\nreduces the amount of drift in the acceleration profile, without affecting the\nbar's behaviour. Models with lower $m_{\\mathrm{d}}$ values coupled with small\nsoftening values, have an excess of highly accelerated particles, introducing\nunwanted effects into otherwise reliable simulations. Finally, we show that the\nevolution of the disc's vertical acceleration profile is a reliable indicator\nof numerical heating introduced by $\\epsilon$ and the bar.", "comment": "Accepted for publication in the Galaxies journal. 34 pages (including\n  references),12 figures and 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.18083v1", "cate": "astro-ph.GA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "盘加热对星系模型中棒抑制的数值研究", "tldr": "研究了N体模拟中平滑参数和盘质量分数对星系棒形成和演化的影响，发现小平滑参数值会导致数值加热和棒抑制。", "motivation": "星系棒的形成、演化和破坏是一个有争议的话题，数值模拟显示这些现象强烈依赖于物理和数值参数。", "method": "通过N体模拟，在孤立的盘-晕模型中，研究了平滑参数($\textit{ϵ}$)和盘质量分数($\textit{m}_{\text{d}}$)对棒形成和演化的联合影响，使用了不同的粒子分辨率。", "result": "1. 之前的研究认为棒强度与$\textit{m}_{\text{d}}^{-1}$成正比，导致棒形成延迟，但本研究发现$\textit{m}_{\text{d}}$的增加并不总是引起棒形成延迟，表明$\textit{ϵ}$和$\textit{m}_{\text{d}}$相互作用。2. 小平滑值($\textit{ϵ} \text{≤} 5\text{pc}$)的模型中数值加热占主导，导致盘中心粒子加速，产生混沌轨道，进而由于中心碰撞动力学而抑制棒的形成。3. 高分辨率模型中，小平滑值无法再现棒的不稳定性。4. 适度$\textit{ϵ}$($\text{≥} 10\text{pc}$)时增加$\textit{m}_{\text{d}}$会减少加速度剖面中的漂移，但不影响棒的行为。5. 低$\textit{m}_{\text{d}}$值与小平滑值结合的模型中，有过多的高加速粒子，给原本可靠的模拟引入了不必要的影响。6. 盘垂直加速度剖面的演化是数值加热的可靠指标。", "conclusion": "研究表明，平滑参数$\textit{ϵ}$和盘质量分数$\textit{m}_{\text{d}}$共同影响星系棒的形成和演化，特别是小的$\textit{ϵ}$值会导致数值加热和棒抑制。盘的垂直加速度剖面演化可作为数值加热的可靠指标。", "translation": "星系棒的形成、演化和破坏过程仍然是星系动力学中一个有争议的话题。数值模拟表明，这些现象强烈依赖于物理和数值参数。在这项工作中，我们通过不同粒子分辨率的N体模拟，研究了平滑参数$\textit{ϵ}$和盘质量分数$\textit{m}_{\text{d}}$在孤立盘-晕模型中对棒形成和演化的综合影响。先前的研究表明，棒强度与$\textit{m}_{\text{d}}^{-1}$成正比，这被视为棒形成的延迟。然而，测量棒随时间动量的畸变参数$\textit{η}$表明，增加$\textit{m}_{\text{d}}$并不总是引起棒形成的延迟。这表明$\textit{ϵ}$会相互作用以增强或削弱棒。此外，在平滑值较小的模型中，数值加热占主导地位，无论$\textit{m}_{\text{d}}$或分辨率如何，都会在盘中心产生高度加速的粒子。这些增强的粒子加速在$\textit{ϵ} \text{≤} 5\text{pc}$时产生混沌轨道，由于中心碰撞动力学导致棒的抑制。在我们的高分辨率模型（$\textit{N} \text{≈} 10^{7}$）中，小的平滑值无法再现棒的不稳定性。盘质量的作用如下：对于中等$\textit{ϵ}$（$\text{≥} 10\text{pc}$），增加$\textit{m}_{\text{d}}$会减少加速度剖面中的漂移量，而不影响棒的行为。较低$\textit{m}_{\text{d}}$值与小平滑值结合的模型，有过多的高加速粒子，给原本可靠的模拟引入了不必要的影响。最后，我们表明盘垂直加速度剖面的演化是$\textit{ϵ}$和棒引入的数值加热的可靠指标。", "summary": "本文通过N体模拟研究了平滑参数$\textit{ϵ}$和盘质量分数$\textit{m}_{\text{d}}$对孤立盘-晕星系模型中棒形成和演化的联合影响。研究发现，小平滑参数值会导致数值加热，在盘中心产生高度加速的粒子和混沌轨道，从而抑制棒的形成。高分辨率模拟中，小平滑值无法产生棒的不稳定性。此外，增加盘质量分数在适度平滑参数下不影响棒的行为，但低盘质量分数与小平滑参数结合会引入不必要的模拟效应。研究还提出盘的垂直加速度剖面是数值加热的可靠指标。", "keywords": "星系棒, N体模拟, 数值加热, 平滑参数, 盘质量分数", "comments": "该研究通过详细的N体模拟，揭示了数值参数（如平滑参数）和物理参数（如盘质量分数）对星系棒形成和演化的复杂相互作用，特别强调了数值加热对棒抑制的关键作用。其创新之处在于揭示了在特定参数范围下，数值效应而非纯物理效应如何主导棒的行为，并提出了一个量化数值加热的可靠指标，这对于未来高精度星系模拟的参数选择和结果解释具有重要指导意义。"}}
{"id": "2503.15704", "title": "Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization", "authors": ["Kyurae Kim", "Zuheng Xu", "Jacob R. Gardner", "Trevor Campbell"], "categories": ["stat.ML", "cs.LG", "stat.CO"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Accepted to ICML'25; v4: fixed typos", "url": "http://arxiv.org/abs/2503.15704v4", "summary": "The performance of sequential Monte Carlo (SMC) samplers heavily depends on\nthe tuning of the Markov kernels used in the path proposal. For SMC samplers\nwith unadjusted Markov kernels, standard tuning objectives, such as the\nMetropolis-Hastings acceptance rate or the expected-squared jump distance, are\nno longer applicable. While stochastic gradient-based end-to-end optimization\nhas been explored for tuning SMC samplers, they often incur excessive training\ncosts, even for tuning just the kernel step sizes. In this work, we propose a\ngeneral adaptation framework for tuning the Markov kernels in SMC samplers by\nminimizing the incremental Kullback-Leibler (KL) divergence between the\nproposal and target paths. For step size tuning, we provide a gradient- and\ntuning-free algorithm that is generally applicable for kernels such as Langevin\nMonte Carlo (LMC). We further demonstrate the utility of our approach by\nproviding a tailored scheme for tuning kinetic LMC used in SMC samplers. Our\nimplementations are able to obtain a full schedule of tuned parameters at the\ncost of a few vanilla SMC runs, which is a fraction of gradient-based\napproaches.", "comment": "Accepted to ICML'25; v4: fixed typos", "pdf_url": "http://arxiv.org/pdf/2503.15704v4", "cate": "stat.ML", "date": "2025-03-19", "updated": "2025-07-23", "AI": {"title_translation": "通过贪婪增量散度最小化调整序贯蒙特卡洛采样器", "tldr": "本文提出了一种通过最小化增量 Kullback-Leibler (KL) 散度来高效调整序贯蒙特卡洛 (SMC) 采样器中马尔可夫核的新方法，该方法成本远低于现有梯度方法。", "motivation": "序贯蒙特卡洛 (SMC) 采样器的性能严重依赖于马尔可夫核的调整。对于使用未调整马尔可夫核的 SMC 采样器，标准调整目标不再适用。现有的基于随机梯度的端到端优化方法训练成本过高。", "method": "本文提出一个通用的自适应框架，通过最小化提议路径和目标路径之间的增量 Kullback-Leibler (KL) 散度来调整 SMC 采样器中的马尔可夫核。对于步长调整，提供了一种无梯度且无需额外调整的算法，适用于 Langevin Monte Carlo (LMC) 等核。此外，还为 SMC 采样器中使用的动力学 LMC 提供了定制的调整方案。", "result": "该实现能够以少量普通 SMC 运行的成本获得完整的调整参数表，这仅是基于梯度方法所需成本的一小部分。", "conclusion": "该方法提供了一种有效且计算效率高的方式来调整 SMC 采样器中的马尔可夫核，尤其是在标准方法不适用或计算成本过高的情况下，显著降低了调优成本。", "translation": "序贯蒙特卡洛 (SMC) 采样器的性能在很大程度上取决于路径提议中使用的马尔可夫核的调整。对于使用未调整马尔可夫核的 SMC 采样器，标准的调整目标，例如 Metropolis-Hastings 接受率或预期平方跳跃距离，不再适用。虽然已经探索了基于随机梯度的端到端优化来调整 SMC 采样器，但它们通常会产生过高的训练成本，即使仅用于调整核步长。在这项工作中，我们提出了一种通用的自适应框架，通过最小化提议路径和目标路径之间的增量 Kullback-Leibler (KL) 散度来调整 SMC 采样器中的马尔可夫核。对于步长调整，我们提供了一种无梯度且无需额外调整的算法，该算法通常适用于 Langevin Monte Carlo (LMC) 等核。我们通过提供一种用于调整 SMC 采样器中使用的动力学 LMC 的定制方案，进一步证明了我们方法的实用性。我们的实现能够以少量普通 SMC 运行的成本获得完整的调整参数表，这仅是基于梯度方法所需成本的一小部分。", "summary": "本文提出了一种通过最小化提议路径和目标路径之间的增量 Kullback-Leibler (KL) 散度来调整序贯蒙特卡洛 (SMC) 采样器中马尔可夫核的通用自适应框架。针对步长调整，提供了一种适用于 Langevin Monte Carlo (LMC) 等核的无梯度、无需额外调整的算法。该方法能够以远低于现有基于梯度方法的成本获得完整的调整参数，解决了传统调整方法不适用和现有优化方法成本过高的问题。", "keywords": "序贯蒙特卡洛, 马尔可夫核, KL散度, 调整, Langevin蒙特卡洛", "comments": "这篇论文提出了一种新颖且计算效率高的方法来解决序贯蒙特卡洛 (SMC) 采样器中马尔可夫核调整的挑战，尤其是在处理未调整核和避免高昂的梯度计算成本方面。其创新点在于利用增量KL散度最小化，并提供无梯度算法，显著降低了调优成本，具有重要的实际应用价值。"}}
{"id": "2507.17778", "title": "An advanced AI driven database system", "authors": ["M. Tedeschi", "S. Rizwan", "C. Shringi", "V. Devram Chandgir", "S. Belich"], "categories": ["cs.DB", "cs.AI", "cs.SE", "68P20", "H.2.4; I.2.7"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, appears in EDULEARN25 Conference Proceedings", "url": "http://arxiv.org/abs/2507.17778v1", "summary": "Contemporary database systems, while effective, suffer severe issues related\nto complexity and usability, especially among individuals who lack technical\nexpertise but are unfamiliar with query languages like Structured Query\nLanguage (SQL). This paper presents a new database system supported by\nArtificial Intelligence (AI), which is intended to improve the management of\ndata using natural language processing (NLP) - based intuitive interfaces, and\nautomatic creation of structured queries and semi-structured data formats like\nyet another markup language (YAML), java script object notation (JSON), and\napplication program interface (API) documentation. The system is intended to\nstrengthen the potential of databases through the integration of Large Language\nModels (LLMs) and advanced machine learning algorithms. The integration is\npurposed to allow the automation of fundamental tasks such as data modeling,\nschema creation, query comprehension, and performance optimization. We present\nin this paper a system that aims to alleviate the main problems with current\ndatabase technologies. It is meant to reduce the need for technical skills,\nmanual tuning for better performance, and the potential for human error. The AI\ndatabase employs generative schema inference and format selection to build its\nschema models and execution formats.", "comment": "10 pages, 5 figures, appears in EDULEARN25 Conference Proceedings", "pdf_url": "http://arxiv.org/pdf/2507.17778v1", "cate": "cs.DB", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "一个先进的AI驱动的数据库系统", "tldr": "本文介绍了一个AI驱动的数据库系统，它利用自然语言处理和大型语言模型简化数据管理，自动化任务，并减少对技术技能的需求，以解决当前数据库系统的复杂性和可用性问题。", "motivation": "当前的数据库系统存在复杂性和可用性问题，特别是对于不熟悉查询语言（如SQL）的非技术用户。需要减少对技术技能、手动调优和人为错误的需求。", "method": "本文提出了一种由AI支持的新型数据库系统，该系统利用基于自然语言处理（NLP）的直观界面，自动创建结构化查询和半结构化数据格式（如YAML, JSON, API文档）。它通过集成大型语言模型（LLMs）和先进的机器学习算法，自动化数据建模、模式创建、查询理解和性能优化等基本任务。该系统采用生成式模式推理和格式选择来构建其模式模型和执行格式。", "result": "该系统旨在改善数据管理，增强数据库的潜力，缓解当前数据库技术的主要问题，减少对技术技能、手动性能调优以及人为错误的需求。", "conclusion": "本文旨在提出一个系统，以缓解当前数据库技术的主要问题，并减少对技术技能、手动调优和人为错误的需求。", "translation": "当今的数据库系统虽然有效，但存在与复杂性和可用性相关的严重问题，特别是在缺乏技术专长但又不熟悉结构化查询语言（SQL）等查询语言的个人中。本文提出了一种由人工智能（AI）支持的新型数据库系统，旨在通过基于自然语言处理（NLP）的直观界面以及自动创建结构化查询和半结构化数据格式（如YAML、JSON和API文档）来改进数据管理。该系统旨在通过集成大型语言模型（LLM）和先进的机器学习算法来增强数据库的潜力。这种集成旨在实现数据建模、模式创建、查询理解和性能优化等基本任务的自动化。本文提出的系统旨在缓解当前数据库技术的主要问题。它旨在减少对技术技能、手动性能调优和人为错误的需求。该AI数据库采用生成式模式推理和格式选择来构建其模式模型和执行格式。", "summary": "本文提出了一种AI驱动的数据库系统，旨在解决当前数据库系统（特别是对非技术用户而言）的复杂性和可用性问题。通过集成自然语言处理（NLP）、大型语言模型（LLMs）和先进的机器学习算法，该系统提供了直观的用户界面，自动化了查询和数据格式的创建，并简化了数据建模和性能优化等任务。其目标是减少数据库管理中对技术技能、手动调优的需求以及人为错误。", "keywords": "AI驱动数据库, 自然语言处理, 大型语言模型, 数据管理, 可用性", "comments": "这篇论文解决了数据库对非技术用户可用性的一个重要痛点。它利用NLP和LLMs实现直观界面和复杂任务（如模式推理、查询生成）自动化，这是一种及时且创新的方法，利用了AI的最新进展。其在数据库访问和管理方面实现民主化的潜力值得关注。"}}
{"id": "2507.18225", "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift", "authors": ["Xin Wei", "Qin Yang", "Yijie Fang", "Mingrui Zhu", "Nannan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18225v1", "summary": "While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18225v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于图谱驱动点位移的3D测试时间适应", "tldr": "GSDTTA是一种新颖的3D点云测试时间适应方法，通过在图谱域优化低频分量，有效地解决了现有3D TTA方法的局限性，并优于现有方法。", "motivation": "现有的测试时间适应（TTA）方法在应用于3D点云时，因其不规则和无序的结构而受到阻碍。当前的3D TTA方法通常依赖于计算成本高昂的空间域优化，并且可能需要额外的训练数据。", "method": "我们提出了图谱域测试时间适应（GSDTTA）方法。该方法将目标域的点云表示为异常值感知图，并通过图傅里叶变换（GFT）将其转换到图谱域。为了提高效率，仅优化最低10%的频率分量，这些分量捕获了点云的大部分能量。然后应用逆GFT（IGFT）来重建适应后的点云，实现图谱驱动的点位移。此过程通过一个特征图引导的自训练策略得到增强，该策略迭代地改进谱调整和模型参数。", "result": "在基准数据集上的实验结果和消融研究表明，GSDTTA在3D点云分类方面优于现有TTA方法。", "conclusion": "GSDTTA是一种有效且高效的3D测试时间适应方法，通过在图谱域进行适应，克服了3D点云结构带来的挑战，并优于现有方法。", "translation": "虽然测试时间适应（TTA）方法通过在在线推理期间动态地将预训练模型适应目标域数据，有效地解决了域偏移问题，但它们在3D点云上的应用受到其不规则和无序结构的阻碍。当前的3D TTA方法通常依赖于计算成本高昂的空间域优化，并且可能需要额外的训练数据。与此相反，我们提出了图谱域测试时间适应（GSDTTA），这是一种用于3D点云分类的新颖方法，它将适应转移到图谱域，通过捕获全局结构属性并使用更少的参数实现更高效的适应。目标域中的点云被表示为异常值感知图，并通过图傅里叶变换（GFT）转换到图谱域。为了提高效率，适应过程仅通过优化最低10%的频率分量来执行，这些分量捕获了点云的大部分能量。然后应用逆GFT（IGFT）来重建具有图谱驱动点位移的适应后点云。此过程通过一个特征图引导的自训练策略得到增强，该策略迭代地改进谱调整和模型参数。在基准数据集上的实验结果和消融研究表明GSDTTA的有效性，其性能优于现有用于3D点云分类的TTA方法。", "summary": "GSDTTA是一种新颖的3D点云测试时间适应方法，旨在解决现有方法在处理不规则和无序的3D点云时效率低和可能需要额外训练数据的问题。该方法通过将点云转换为图谱域，并仅优化捕获大部分能量的最低频率分量来实现高效适应。通过图傅里叶变换和逆变换进行点云位移，并结合特征图引导的自训练策略进行迭代优化。实验证明GSDTTA在3D点云分类任务上优于现有TTA方法。", "keywords": "3D测试时间适应, 图谱域, 点云分类, 领域偏移, 图傅里叶变换", "comments": "本文的创新点在于将3D测试时间适应从空间域转移到图谱域，这有效地解决了3D点云不规则和无序结构的挑战。通过仅优化低频分量来提高适应效率是一个巧妙的设计，能够捕获点云的全局结构特性。这种方法为3D点云的领域适应提供了一个新的、更高效的视角。"}}
{"id": "2507.18231", "title": "PS-GS: Gaussian Splatting for Multi-View Photometric Stereo", "authors": ["Yixiao Chen", "Bin Liang", "Hanzhi Guo", "Yongqing Cheng", "Jiayi Zhao", "Dongdong Weng"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18231v1", "summary": "Integrating inverse rendering with multi-view photometric stereo (MVPS)\nyields more accurate 3D reconstructions than the inverse rendering approaches\nthat rely on fixed environment illumination. However, efficient inverse\nrendering with MVPS remains challenging. To fill this gap, we introduce the\nGaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently\nand jointly estimates the geometry, materials, and lighting of the object that\nis illuminated by diverse directional lights (multi-light). Our method first\nreconstructs a standard 2D Gaussian splatting model as the initial geometry.\nBased on the initialization model, it then proceeds with the deferred inverse\nrendering by the full rendering equation containing a lighting-computing\nmulti-layer perceptron. During the whole optimization, we regularize the\nrendered normal maps by the uncalibrated photometric stereo estimated normals.\nWe also propose the 2D Gaussian ray-tracing for single directional light to\nrefine the incident lighting. The regularizations and the use of multi-view and\nmulti-light images mitigate the ill-posed problem of inverse rendering. After\noptimization, the reconstructed object can be used for novel-view synthesis,\nrelighting, and material and shape editing. Experiments on both synthetic and\nreal datasets demonstrate that our method outperforms prior works in terms of\nreconstruction accuracy and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18231v1", "cate": "cs.GR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PS-GS：多视角光度立体高斯泼溅", "tldr": "PS-GS提出一种高效联合估计几何、材料和光照的方法，通过高斯泼溅和延迟逆渲染结合多视角光度立体，提高了3D重建的准确性和效率。", "motivation": "现有的逆渲染结合多视角光度立体（MVPS）虽然比固定环境光照的逆渲染更准确，但高效的逆渲染与MVPS结合仍然具有挑战性。", "method": "提出PS-GS方法，首先重建一个标准的2D高斯泼溅模型作为初始几何。基于初始化模型，通过包含光照计算多层感知机的完整渲染方程进行延迟逆渲染。在优化过程中，通过未校准的光度立体估计法线来正则化渲染的法线图。提出2D高斯光线追踪用于单方向光照以细化入射光照。正则化和使用多视角、多光照图像缓解了逆渲染的病态问题。", "result": "在合成和真实数据集上的实验表明，该方法在重建精度和计算效率方面优于现有工作。优化后重建的对象可用于新视角合成、重新打光以及材料和形状编辑。", "conclusion": "PS-GS通过结合高斯泼溅和多视角光度立体，有效地解决了高效逆渲染与MVPS结合的挑战，并在3D重建精度和效率上取得了显著提升。", "translation": "将逆渲染与多视角光度立体（MVPS）相结合，比依赖固定环境光照的逆渲染方法能产生更准确的3D重建。然而，高效的逆渲染与MVPS相结合仍然具有挑战性。为了弥补这一空白，我们引入了用于多视角光度立体的高斯泼溅（PS-GS），它能高效地联合估计由不同方向光（多光源）照明的物体的几何、材料和光照。我们的方法首先重建一个标准的2D高斯泼溅模型作为初始几何。基于初始化模型，它通过包含光照计算多层感知机的完整渲染方程进行延迟逆渲染。在整个优化过程中，我们通过未校准的光度立体估计法线来正则化渲染的法线图。我们还提出了用于单方向光照的2D高斯光线追踪以细化入射光照。正则化以及多视角和多光源图像的使用缓解了逆渲染的病态问题。优化后，重建的对象可用于新视角合成、重新打光以及材料和形状编辑。在合成和真实数据集上的实验表明，我们的方法在重建精度和计算效率方面优于现有工作。", "summary": "PS-GS是一种新颖的多视角光度立体高斯泼溅方法，旨在解决高效逆渲染与MVPS结合的难题。它通过结合2D高斯泼溅初始化、延迟逆渲染和多重正则化（包括光度立体法线和2D高斯光线追踪），实现了对物体几何、材料和光照的高效联合估计。实验证明，PS-GS在重建精度和计算效率上均优于现有方法，并支持新视角合成、重打光和编辑等应用。", "keywords": "高斯泼溅, 多视角光度立体, 逆渲染, 3D重建, 光照估计", "comments": "该论文的创新点在于将高斯泼溅与多视角光度立体有效地结合，解决了逆渲染中几何、材料和光照联合估计的效率和精度问题。通过引入2D高斯泼溅作为初始化和多重正则化，缓解了逆渲染的病态性，使得重建结果在准确性和效率上都表现出色。这为高质量的3D重建和后续应用（如新视角合成和重打光）提供了新的解决方案。"}}
{"id": "2507.17770", "title": "Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale", "authors": ["Pei-Kun Yang"], "categories": ["cs.DC", "quant-ph"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      14 pages, 5 figures", "url": "http://arxiv.org/abs/2507.17770v1", "summary": "Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework\nfor modeling combinatorial optimization problems. This study benchmarks five\nsoftware-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and\nSciPy, on randomly generated QUBO matrices ranging from 1000x1000 to\n45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate\ntheir performance in terms of solution quality (energy) and computational time.\nAmong the solvers tested, Neal achieved the lowest energy values but was\nlimited to problems with up to 6000 variables due to high memory consumption.\nPyTorch produced slightly higher energy results than Neal but demonstrated\nsuperior scalability, solving instances with up to 45000 variables. Its support\nfor GPU acceleration and CPU multi-threading also resulted in significantly\nshorter runtimes. JAX yielded energy values slightly above those of PyTorch and\nwas limited to 25000 variables, with runtimes comparable to PyTorch on GPU.\nSciPy was the most constrained solver, handling only up to 6000 variables and\nconsistently producing the highest energy values with the longest computation\ntimes. These findings highlight trade-offs between solution quality,\nscalability, and runtime efficiency, and suggest that PyTorch is the most\nbalanced choice for large-scale QUBO problems when computational resources\npermit.", "comment": "14 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.17770v1", "cate": "cs.DC", "date": "2025-07-17", "updated": "2025-07-17", "AI": {"title_translation": "PyTorch、JAX、SciPy 和 Neal 在大规模 QUBO 问题求解中的比较评估", "tldr": "本研究比较了Neal、PyTorch、JAX和SciPy在解决大规模QUBO问题时的性能，发现PyTorch在解决方案质量、可扩展性和运行时间之间提供了最佳平衡。", "motivation": "二次无约束二元优化（QUBO）是一种通用的组合优化问题建模框架。本研究的动机是为了评估和比较不同软件QUBO求解器在大规模问题上的性能，以找出其在解决方案质量、可扩展性、内存消耗和运行时间方面的权衡。", "method": "该研究对Neal、PyTorch (CPU)、PyTorch (GPU)、JAX 和 SciPy 这五种基于软件的 QUBO 求解器进行了基准测试。测试使用了从 1000x1000 到 45000x45000 的随机生成的 QUBO 矩阵，并在六个收敛阈值（从 10^-1 到 10^-6）下进行。性能评估指标是解决方案质量（能量）和计算时间。", "result": "Neal 获得了最低的能量值，但由于内存消耗高，其仅限于处理多达 6000 个变量的问题。PyTorch 产生的能量结果略高于 Neal，但展示了卓越的可扩展性，解决了多达 45000 个变量的实例，并因 GPU 加速和 CPU 多线程支持而运行时间显著缩短。JAX 产生的能量值略高于 PyTorch，并且受限于 25000 个变量，其运行时间与 PyTorch 在 GPU 上的运行时间相当。SciPy 是限制最多的求解器，仅能处理多达 6000 个变量，并且始终产生最高的能量值和最长的计算时间。", "conclusion": "研究结果强调了解决方案质量、可扩展性和运行效率之间的权衡。当计算资源允许时，PyTorch 是解决大规模 QUBO 问题的最平衡选择。", "translation": "二次无约束二元优化（QUBO）是建模组合优化问题的一种通用框架。本研究对五种基于软件的 QUBO 求解器进行了基准测试：Neal、PyTorch (CPU)、PyTorch (GPU)、JAX 和 SciPy，测试对象是范围从 1000x1000 到 45000x45000 的随机生成的 QUBO 矩阵，并在六个从 10^-1 到 10^-6 的收敛阈值下进行。我们从解决方案质量（能量）和计算时间方面评估了它们的性能。在测试的求解器中，Neal 获得了最低的能量值，但由于内存消耗高，其仅限于处理多达 6000 个变量的问题。PyTorch 产生的能量结果略高于 Neal，但展示了卓越的可扩展性，解决了多达 45000 个变量的实例。它对 GPU 加速和 CPU 多线程的支持也显著缩短了运行时间。JAX 产生的能量值略高于 PyTorch，并且受限于 25000 个变量，其运行时间与 PyTorch 在 GPU 上的运行时间相当。SciPy 是限制最多的求解器，仅能处理多达 6000 个变量，并且始终产生最高的能量值和最长的计算时间。这些发现突出了解决方案质量、可扩展性和运行效率之间的权衡，并表明当计算资源允许时，PyTorch 是处理大规模 QUBO 问题的最平衡选择。", "summary": "本研究对Neal、PyTorch、JAX和SciPy这五种软件QUBO求解器在处理大规模二次无约束二元优化（QUBO）问题时的性能进行了比较评估。通过在不同规模（1000x1000至45000x45000）和收敛阈下测试，研究发现Neal在能量值上表现最佳但可扩展性差（限于6000变量），SciPy性能最差（高能量值、长计算时间、限于6000变量）。PyTorch在解决方案质量、可扩展性（高达45000变量）和运行时间之间表现出最佳平衡，尤其适合处理大规模QUBO问题，并受益于GPU加速和多线程支持。JAX性能介于PyTorch和SciPy之间（限于25000变量）。", "keywords": "QUBO, 优化, PyTorch, JAX, SciPy", "comments": "该研究通过系统地比较多种流行库在解决大规模QUBO问题上的性能，为研究人员和实践者提供了宝贵的参考。其创新之处在于对不同求解器在可扩展性、运行效率和解决方案质量之间的权衡进行了详细分析，并明确指出了PyTorch作为大规模QUBO问题求解的优选方案，尤其强调了其GPU加速的优势。这对于指导实际应用中QUBO求解器的选择具有重要意义。"}}
{"id": "2507.01884", "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification", "authors": ["Kunlun Xu", "Fan Zhuo", "Jiangmeng Li", "Xu Zou", "Jiahuan Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.01884v2", "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.01884v2", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-24", "AI": {"title_translation": "自强化原型演化与双知识协作的半监督终身行人重识别", "tldr": "本文开创性地研究了半监督终身行人重识别（Semi-LReID）问题，提出了一种新颖的自强化原型演化与双知识协作（SPRED）框架，通过动态原型生成伪标签并利用新旧知识协作净化，有效解决了无标签数据中的噪声知识问题，并在基准测试中达到了最先进的性能。", "motivation": "当前的终身行人重识别（LReID）方法主要依赖于完全标注的数据流。然而，在真实世界场景中，标注资源有限，大量未标注数据与少量标注样本共存，导致了半监督LReID（Semi-LReID）问题。现有LReID方法，即使结合半监督策略，也因难以处理无标签数据利用过程中产生的噪声知识，导致长期适应性能有限。", "method": "本文引入了新颖的自强化原型演化与双知识协作（SPRED）框架，开创性地研究了Semi-LReID问题。其核心创新在于建立了动态原型引导的伪标签生成与新旧知识协作净化之间的自强化循环，以增强无标签数据的利用。具体而言，引入了可学习的身份原型来动态捕获身份分布并生成高质量伪标签。然后，双知识协作方案整合了当前模型的专业化和历史模型的泛化能力，以精炼噪声伪标签。通过这种循环设计，逐步挖掘可靠的伪标签，以改进当前阶段的学习并确保长期学习中知识的积极传播。", "result": "在已建立的Semi-LReID基准测试中，我们的SPRED实现了最先进的性能。", "conclusion": "SPRED框架通过自强化原型演化和双知识协作，有效解决了半监督终身行人重识别中利用无标签数据时出现的噪声知识问题，实现了高性能，并确保了知识在长期学习中的可靠传播。", "translation": "当前终身行人重识别（LReID）方法主要依赖于完全标注的数据流。然而，在真实世界场景中，标注资源有限，大量未标注数据与少量标注样本共存，导致了半监督LReID（Semi-LReID）问题，其中LReID方法性能严重下降。现有LReID方法，即使结合半监督策略，也因难以处理无标签数据利用过程中产生的噪声知识，导致长期适应性能有限。在本文中，我们开创性地研究了Semi-LReID，引入了一种新颖的自强化原型演化与双知识协作（SPRED）框架。我们的核心创新在于建立了动态原型引导的伪标签生成与新旧知识协作净化之间的自强化循环，以增强无标签数据的利用。具体而言，引入了可学习的身份原型来动态捕获身份分布并生成高质量伪标签。然后，双知识协作方案整合了当前模型的专业化和历史模型的泛化能力，以精炼噪声伪标签。通过这种循环设计，逐步挖掘可靠的伪标签，以改进当前阶段的学习并确保长期学习中知识的积极传播。在已建立的Semi-LReID基准测试中，我们的SPRED实现了最先进的性能。我们的源代码可在https://github.com/zhoujiahuan1991/ICCV2025-SPRED获取。", "summary": "本文针对半监督终身行人重识别（Semi-LReID）问题，即在标注数据稀缺且无标签数据丰富的真实场景下，LReID方法性能下降且难以处理噪声知识的挑战，提出了一种新颖的自强化原型演化与双知识协作（SPRED）框架。SPRED的核心在于构建一个自强化循环，通过可学习的身份原型动态生成高质量伪标签，并结合当前模型特化与历史模型泛化的双知识协作机制来净化噪声伪标签。这种循环设计能够逐步挖掘可靠的伪标签，从而在当前学习阶段和长期学习中实现知识的积极传播和性能提升。实验结果表明，SPRED在Semi-LReID基准测试中取得了最先进的性能。", "keywords": "半监督终身行人重识别, 原型演化, 双知识协作, 伪标签, 终身学习", "comments": "该论文的创新点在于提出了一个“自强化循环”和“双知识协作”机制，用于在半监督终身学习背景下生成鲁棒的伪标签，以解决无标签数据带来的噪声问题。这对于实际应用中标注数据稀缺的场景具有重要意义，提升了终身行人重识别在更真实环境下的实用性。"}}
{"id": "2507.17979", "title": "SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17979v1", "summary": "Identifying the factors driving data shifts in tabular datasets is a\nsignificant challenge for analysis and decision support systems, especially\nthose focusing on healthcare. Privacy rules restrict data access, and noise\nfrom complex processes hinders analysis. To address this challenge, we propose\nSIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular\nLearning) that (i) extracts privacy-compliant data summary statistics, (ii)\nemploys twin XGBoost models to disentangle intervention signals from noise with\nassistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted\ndecision tree to identify interpretable segments responsible for the shift.\nUnlike existing analyses which may ignore noise or require full data access for\nLLM-based analysis, SIFOTL addresses both challenges using only privacy-safe\nsummary statistics. Demonstrating its real-world efficacy, for a MEPS panel\ndataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of\n0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and\nstatistical tests (F1=0.20) in identifying the segment receiving the subsidy.\nFurthermore, across 18 diverse EHR datasets generated based on Synthea ABM,\nSIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with\ninjected observational noise, whereas baseline average F1 scores range from\n0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,\nprivacy-conscious workflow that is empirically robust to observational noise.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17979v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SIFOTL：一种用于表格学习的、基于统计学原理的保真度优化方法", "tldr": "SIFOTL是一种用于识别表格数据集中数据漂移驱动因素的方法，它通过提取隐私安全的数据摘要统计量，结合双XGBoost模型、LLM辅助和Pareto加权决策树来分离信号和噪声，并在存在隐私限制和噪声的情况下表现出卓越的性能。", "motivation": "识别表格数据集中导致数据漂移的因素是一个重大挑战，尤其是在医疗保健领域，因为隐私规则限制了数据访问，并且复杂过程产生的噪声阻碍了分析。", "method": "我们提出了SIFOTL（基于统计学原理的表格学习保真度优化方法），该方法：(i)提取符合隐私要求的数据摘要统计量；(ii)利用双XGBoost模型在LLM的辅助下将干预信号与噪声分离；(iii)通过Pareto加权决策树合并XGBoost输出，以识别导致漂移的可解释片段。该方法仅使用隐私安全摘要统计量。", "result": "在模拟新的Medicare药物补贴的MEPS面板数据集上，SIFOTL的F1分数为0.85，显著优于BigQuery贡献分析（F1=0.46）和统计测试（F1=0.20）。此外，在基于Synthea ABM生成的18个不同EHR数据集上，SIFOTL在无噪声情况下F1分数维持在0.86-0.96，即使注入观测噪声也达到>=0.75，而基线平均F1分数在相同测试下为0.19-0.67。", "conclusion": "SIFOTL提供了一个可解释、注重隐私的工作流程，并且在经验上对观测噪声具有鲁棒性。", "translation": "识别表格数据集中导致数据漂移的因素是数据分析和决策支持系统面临的一个重大挑战，尤其是在医疗保健领域。隐私规则限制了数据访问，复杂过程产生的噪声阻碍了分析。为了解决这一挑战，我们提出了SIFOTL（基于统计学原理的表格学习保真度优化方法），它(i)提取符合隐私要求的数据摘要统计量，(ii)利用双XGBoost模型在LLM的辅助下将干预信号与噪声分离，以及(iii)通过Pareto加权决策树合并XGBoost输出，以识别导致漂移的可解释片段。与现有分析可能忽略噪声或需要完整数据访问才能进行基于LLM的分析不同，SIFOTL仅使用隐私安全的摘要统计量解决了这两个挑战。为了证明其在真实世界中的有效性，对于一个模拟新的Medicare药物补贴的MEPS面板数据集，SIFOTL的F1分数为0.85，显著优于BigQuery贡献分析（F1=0.46）和统计测试（F1=0.20），能够识别获得补贴的群体。此外，在基于Synthea ABM生成的18个不同EHR数据集上，SIFOTL在无噪声情况下F1分数维持在0.86-0.96，即使注入观测噪声也达到>=0.75，而基线平均F1分数在相同测试下为0.19-0.67。因此，SIFOTL提供了一个可解释、注重隐私的工作流程，并且在经验上对观测噪声具有鲁棒性。", "summary": "SIFOTL是一种新颖的方法，旨在解决表格数据集中（尤其是在医疗保健领域）数据漂移因素的识别问题，同时应对隐私限制和噪声挑战。该方法通过提取符合隐私要求的数据摘要统计量，并利用双XGBoost模型在大型语言模型（LLMs）的辅助下分离信号与噪声，最后通过Pareto加权决策树识别可解释的漂移片段。在真实世界的MEPS数据集和合成的EHR数据集上，SIFOTL的性能显著优于现有基线方法，F1分数高达0.85-0.96，即使存在观测噪声也能保持鲁棒性。这表明SIFOTL提供了一个可解释、注重隐私且对噪声具有鲁棒性的解决方案。", "keywords": "表格学习, 数据漂移, 保真度优化, 隐私保护, XGBoost, LLM", "comments": "SIFOTL的创新之处在于其结合了隐私保护的数据摘要、双XGBoost模型与LLM的协助以及Pareto加权决策树，以解决表格数据中数据漂移的识别问题，尤其是在存在隐私限制和噪声的复杂环境中。其在医疗保健领域的应用潜力巨大，为数据分析和决策支持系统提供了一个鲁棒且可解释的解决方案。该方法通过仅依赖隐私安全的摘要统计量，避免了对完整数据访问的需求，是其重要优势。"}}
{"id": "2507.18110", "title": "Two-Stage TSO-DSO Services Provision Framework for Electric Vehicle Coordination", "authors": ["Yi Wang", "Dawei Qiu", "Fei Teng", "Goran Strbac"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18110v1", "summary": "High renewable penetration has been witnessed in power systems, resulting in\nreduced system inertia and increasing requirements for frequency response\nservices. Electric vehicles (EVs), owing to their vehicle-to-grid (V2G)\ncapabilities, can provide cost-effective frequency services for transmission\nsystem operators (TSOs). However, EVs that are inherently connected to\ndistribution networks may pose voltage security issues for distribution system\noperators (DSOs) when supporting TSO frequency. To coordinate both TSO\nfrequency and DSO voltage, this paper proposes a two-stage service provision\nframework for multi-EVs. At stage one, EVs participate in day-ahead TSO-DSO\ninteractions for frequency reserve schedules; at stage two, EVs make real-time\ndispatching behaviors in distribution networks for reserve delivery while\nsupporting DSO voltage. Considering the potentially large EV number and\nenvironment complexity, a decentralized operation paradigm is introduced for\nreal-time EV dispatches at stage two, while a communication-efficient\nreinforcement learning (RL) algorithm is proposed to reduce the communication\noverhead during large-scale multi-agent RL training without compromising policy\nperformance. Case studies are carried out on a 6-bus transmission and 33-bus\ndistribution network as well as a 69-bus distribution network to evaluate the\neffectiveness and scalability of the proposed method in enabling EVs for\nfrequency service and voltage support.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18110v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "电动汽车协调的两阶段TSO-DSO服务提供框架", "tldr": "本文提出了一种两阶段框架，用于协调电动汽车为输电系统运营商（TSO）提供频率服务和为配电系统运营商（DSO）提供电压支持，同时解决了大规模部署中的通信效率问题。", "motivation": "随着可再生能源渗透率的提高，电力系统惯性降低，对频率响应服务的需求增加。电动汽车（EV）可以通过车网互动（V2G）提供经济高效的频率服务。然而，电动汽车连接到配电网络时，在支持TSO频率的同时可能给配电系统运营商（DSO）带来电压安全问题。", "method": "本文提出了一种用于多电动汽车的两阶段服务提供框架：第一阶段，电动汽车参与日前TSO-DSO互动，进行频率储备调度；第二阶段，电动汽车在配电网络中进行实时调度，在输送储备的同时支持DSO电压。考虑到电动汽车数量庞大和环境复杂性，第二阶段引入了分散式运行范式，并提出了一种通信高效的强化学习（RL）算法，以减少大规模多智能体RL训练中的通信开销，同时不影响策略性能。", "result": "在6总线输电和33总线配电网络以及69总线配电网络上进行了案例研究，评估了所提方法在使电动汽车提供频率服务和电压支持方面的有效性和可扩展性。", "conclusion": "所提出的两阶段电动汽车协调服务提供框架，结合分散式运行和通信高效的强化学习算法，能够有效且可扩展地使电动汽车同时提供TSO频率服务和DSO电压支持。", "translation": "高可再生能源渗透率已在电力系统中得到体现，导致系统惯性降低，对频率响应服务的需求不断增加。电动汽车（EV）凭借其车网互动（V2G）能力，可以为输电系统运营商（TSO）提供经济高效的频率服务。然而，本质上连接到配电网络的电动汽车在支持TSO频率时，可能会给配电系统运营商（DSO）带来电压安全问题。为了协调TSO频率和DSO电压，本文提出了一种针对多电动汽车的两阶段服务提供框架。在第一阶段，电动汽车参与日前TSO-DSO互动，进行频率储备调度；在第二阶段，电动汽车在配电网络中进行实时调度行为，在输送储备的同时支持DSO电压。考虑到潜在的庞大电动汽车数量和环境复杂性，第二阶段引入了分散式运行范式，同时提出了一种通信高效的强化学习（RL）算法，以在不影响策略性能的情况下减少大规模多智能体RL训练期间的通信开销。在6总线输电和33总线配电网络以及69总线配电网络上进行了案例研究，以评估所提方法在使电动汽车提供频率服务和电压支持方面的有效性和可扩展性。", "summary": "本文提出了一种两阶段框架，旨在协调电动汽车同时为输电系统运营商（TSO）提供频率响应服务并为配电系统运营商（DSO）提供电压支持。第一阶段涉及日前频率储备调度，而第二阶段则侧重于实时调度，其中电动汽车在提供储备的同时维护配电网电压。为应对大规模电动汽车部署，该框架在第二阶段采用分散式运行，并引入了一种通信高效的强化学习算法，以优化多智能体训练。案例研究证实了该方法在频率服务和电压支持方面的有效性和可扩展性。", "keywords": "电动汽车, TSO-DSO协调, 频率响应, 电压支持, 强化学习", "comments": "该论文的创新之处在于提出了一个协调TSO和DSO需求的两阶段电动汽车服务提供框架，有效地解决了电动汽车在提供频率服务时可能引发的电压问题。引入通信高效的强化学习算法，对于大规模电动汽车的实际部署具有重要意义，因为它能有效管理复杂的通信开销，提升了方案的可扩展性和实用性。"}}
{"id": "2507.18562", "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation", "authors": ["Jiafeng Xiong", "Yuting Zhao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18562v1", "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18562v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GIIFT：图引导归纳式无图像多模态机器翻译", "tldr": "GIIFT是一个两阶段的图引导归纳式无图像多模态机器翻译框架，通过构建多模态场景图和使用跨模态图注意力网络适配器，在推断时无需图像即可超越现有方法并达到最先进水平。", "motivation": "现有的多模态机器翻译（MMT）方法在利用模态间隙方面面临挑战，它们强制执行严格的视觉-语言对齐，并且仅限于在其训练的多模态领域内进行推断。", "method": "本文构建了新颖的多模态场景图以保留和整合模态特定信息，并引入了GIIFT，一个两阶段的图引导归纳式无图像MMT框架。该框架使用跨模态图注意力网络适配器在统一的融合空间中学习多模态知识，并将其归纳地推广到更广泛的无图像翻译领域。", "result": "在Multi30K数据集的英译法和英译德任务上的实验结果表明，即使在推断时没有图像，GIIFT也超越了现有方法并达到了最先进水平。WMT基准测试的结果显示，与无图像翻译基线相比有显著改进，证明了GIIFT在归纳式无图像推断方面的强大能力。", "conclusion": "GIIFT框架能够有效地进行归纳式无图像推断，在多模态机器翻译任务中表现出优越的性能，甚至在没有视觉信息输入的情况下也能超越现有技术。", "translation": "多模态机器翻译（MMT）已经证明了视觉信息在机器翻译中的显著帮助。然而，现有的MMT方法在利用模态间隙方面面临挑战，它们通过强制执行严格的视觉-语言对齐，同时受限于在其训练的多模态领域内进行推断。在这项工作中，我们构建了新颖的多模态场景图以保留和整合模态特定信息，并引入了GIIFT，一个两阶段的图引导归纳式无图像MMT框架，该框架使用跨模态图注意力网络适配器在统一的融合空间中学习多模态知识，并将其归纳地推广到更广泛的无图像翻译领域。在Multi30K数据集的英译法和英译德任务上的实验结果表明，我们的GIIFT超越了现有方法并达到了最先进水平，即使在推断时没有图像。WMT基准测试的结果显示，与无图像翻译基线相比有显著改进，证明了GIIFT在归纳式无图像推断方面的强大能力。", "summary": "本文提出了GIIFT，一个图引导的归纳式无图像多模态机器翻译框架。该方法通过构建多模态场景图和使用跨模态图注意力网络适配器，解决了现有MMT方法在模态对齐和泛化能力上的局限性。GIIFT能够在统一的融合空间中学习多模态知识，并将其推广到无图像翻译领域。实验证明，即使在推断时没有图像，GIIFT在Multi30K和WMT数据集上均超越了现有方法，实现了最先进的性能，展现了其在归纳式无图像推断方面的强大能力。", "keywords": "多模态机器翻译, 图神经网络, 无图像推断, 归纳学习, 场景图", "comments": "GIIFT的创新之处在于其“无图像”推断能力，这大大拓宽了多模态机器翻译的应用场景，解决了现有方法对图像输入的依赖。通过引入图引导和归纳式学习，它有效地弥补了模态间隙，并实现了更强的泛化能力。这是一个重要的进步，因为它使MMT模型在实际部署中更加灵活和高效。"}}
{"id": "2507.16473", "title": "Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs", "authors": ["Chang Li", "Yaren Zhang", "Haoran Lv", "Qiong Cao", "Chao Xue", "Xiaodong He"], "categories": ["cs.AI", "I.2.7"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16473v2", "summary": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16473v2", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "通过变分同态在选项诱导抽象MDP中学习时间抽象", "tldr": "本文提出了一种通过变分同态在层级强化学习框架中学习时间抽象（即选项）的方法，以实现高效的隐式推理，避免大型语言模型中昂贵的显式思维链生成，并在语言和控制任务上取得了良好表现。", "motivation": "大型语言模型（LLMs）的思维链（CoT）推理虽然强大，但生成这些分步文本解释的计算成本高昂且速度慢。本文旨在开发一个高效、隐式的推理框架，让模型在潜在空间中“思考”而无需为每一步生成显式文本。", "method": "本文提出将潜在思考建模为层级强化学习框架中的时间扩展抽象动作（选项）。为此，首先引入了变分马尔可夫选项评论（VMOC），这是一种在HiT-MDP框架内使用变分推断的离策略算法，用于学习多样化的选项库。为了为将这些选项用作抽象推理空间提供严格的基础，扩展了连续MDP同态理论，证明了在简化的抽象潜在空间中学习策略可以保持原始复杂问题的最优性。最后，提出了一种冷启动程序，利用监督微调（SFT）数据将人类推理演示提炼到潜在选项空间中。", "result": "该方法在复杂的逻辑推理基准和具有挑战性的运动任务上取得了强大性能，验证了该框架作为学习语言和控制抽象技能的原则性方法。", "conclusion": "本文提出的框架通过变分同态和选项学习，为大型语言模型提供了一种高效、隐式的推理机制，并在理论上保证了最优性，在实际任务中展现了优越的性能，适用于语言和控制领域的抽象技能学习。", "translation": "大型语言模型（LLMs）通过显式思维链（CoT）提示展示了卓越的推理能力，但生成这些分步文本解释的计算成本高昂且速度慢。为了克服这个问题，我们旨在开发一个高效、隐式的推理框架，模型在潜在空间中“思考”而无需为每一步生成显式文本。我们提出这些潜在思考可以被建模为层级强化学习框架中的时间扩展抽象动作，即选项。为了有效地学习多样化的选项库作为潜在嵌入，我们首先引入了变分马尔可夫选项评论（VMOC），这是一种在HiT-MDP框架内使用变分推断的离策略算法。为了为将这些选项用作抽象推理空间提供严格的基础，我们扩展了连续MDP同态理论。这证明了在简化的抽象潜在空间中学习策略（VMOC适用于此）可以保持原始复杂问题的最优性。最后，我们提出了一种冷启动程序，利用监督微调（SFT）数据将人类推理演示提炼到潜在选项空间中，为模型的推理能力提供丰富的初始化。广泛的实验表明，我们的方法在复杂的逻辑推理基准和具有挑战性的运动任务上取得了强大性能，验证了我们的框架作为学习语言和控制抽象技能的原则性方法。", "summary": "本文提出一种通过在层级强化学习框架中使用变分同态学习选项（时间抽象）来实现高效隐式推理的方法，以解决大型语言模型中昂贵且缓慢的显式思维链推理问题。该方法引入了变分马尔可夫选项评论（VMOC）算法来学习多样化的选项库，并扩展了连续MDP同态理论以提供理论基础，证明了在抽象潜在空间中学习策略能保持最优性。此外，还提出了利用监督微调数据进行冷启动的程序。实验结果表明，该框架在逻辑推理和运动控制任务上均表现出色，证明了其在学习语言和控制领域抽象技能方面的有效性。", "keywords": "时间抽象, 变分同态, 选项, 强化学习, 隐式推理", "comments": "本文的创新点在于提出了一个结合变分推断、MDP同态理论和层级强化学习的统一框架，以实现LLM的隐式、高效推理。通过将潜在思考建模为选项，并提供严格的理论保证，解决了CoT推理的计算效率问题。其通用性体现在同时适用于语言和控制任务，具有重要的研究价值。"}}
{"id": "2507.18428", "title": "Towards Understanding Decision Problems As a Goal of Visualization Design", "authors": ["Lena Cibulski", "Stefan Bruckner"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18428v1", "summary": "Decision-making is a central yet under-defined goal in visualization\nresearch. While existing task models address decision processes, they often\nneglect the conditions framing a decision. To better support decision-making\ntasks, we propose a characterization scheme that describes decision problems\nthrough key properties of the data, users, and task context. This scheme helps\nvisualization researchers specify decision-support claims more precisely and\ninforms the design of appropriate visual encodings and interactions. We\ndemonstrate the utility of our approach by applying it to characterize decision\ntasks targeted by existing design studies, highlighting opportunities for\nfuture research in decision-centric visualization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18428v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "理解决策问题作为可视化设计目标", "tldr": "本文提出了一种表征方案，通过数据、用户和任务背景的关键属性来描述决策问题，以更好地支持可视化研究中的决策制定。", "motivation": "决策制定是可视化研究中一个核心但定义不清的目标。现有任务模型虽然涉及决策过程，但常忽略构成决策的条件。", "method": "我们提出了一种表征方案，通过数据、用户和任务背景的关键属性来描述决策问题。", "result": "我们通过将该方法应用于表征现有设计研究中的决策任务来证明其效用，并强调了以决策为中心的可视化未来研究机会。", "conclusion": "该方案有助于可视化研究人员更精确地阐明决策支持主张，并为适当的视觉编码和交互设计提供信息。", "translation": "决策制定是可视化研究中一个核心但定义不清的目标。虽然现有任务模型涉及决策过程，但它们常常忽略构成决策的条件。为了更好地支持决策制定任务，我们提出了一种表征方案，通过数据、用户和任务背景的关键属性来描述决策问题。该方案有助于可视化研究人员更精确地阐明决策支持主张，并为适当的视觉编码和交互设计提供信息。我们通过将该方法应用于表征现有设计研究中的决策任务来证明其效用，并强调了以决策为中心的可视化未来研究机会。", "summary": "本文提出了一种新的表征方案，旨在通过考虑数据、用户和任务背景的关键属性来更清晰地定义和支持可视化设计中的决策问题。该方案旨在帮助研究人员更精确地制定决策支持主张，并指导视觉编码和交互设计，从而为以决策为中心的可视化研究开辟新途径。", "keywords": "决策制定, 可视化设计, 表征方案, 任务模型, 决策问题", "comments": "本文的创新之处在于提出了一个表征方案，用于更系统地理解和定义可视化设计中的决策问题，填补了现有任务模型在考虑决策条件方面的不足。这对于推动可视化领域更有效地支持用户决策具有重要意义。"}}
{"id": "2503.16315", "title": "Active Learning For Repairable Hardware Systems With Partial Coverage", "authors": ["Michael Potter", "Beyza Kalkanlı", "Deniz Erdoğmuş", "Michael Everett"], "categories": ["stat.AP", "cs.LG"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Access - Reliability Society", "url": "http://arxiv.org/abs/2503.16315v3", "summary": "Identifying the optimal diagnostic test and hardware system instance to infer\nreliability characteristics using field data is challenging, especially when\nconstrained by fixed budgets and minimal maintenance cycles. Active Learning\n(AL) has shown promise for parameter inference with limited data and budget\nconstraints in machine learning/deep learning tasks. However, AL for\nreliability model parameter inference remains underexplored for repairable\nhardware systems. It requires specialized AL Acquisition Functions (AFs) that\nconsider hardware aging and the fact that a hardware system consists of\nmultiple sub-systems, which may undergo only partial testing during a given\ndiagnostic test. To address these challenges, we propose a relaxed Mixed\nInteger Semidefinite Program (MISDP) AL AF that incorporates Diagnostic\nCoverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing\nbudgets. Furthermore, we design empirical-based simulation experiments focusing\non two diagnostic testing scenarios: (1) partial tests of a hardware system\nwith overlapping subsystem coverage, and (2) partial tests where one diagnostic\ntest fully subsumes the subsystem coverage of another. We evaluate our proposed\napproach against the most widely used AL AF in the literature (entropy), as\nwell as several intuitive AL AFs tailored for reliability model parameter\ninference. Our proposed AF ranked best on average among the alternative AFs\nacross 6,000 experimental configurations, with respect to Area Under the Curve\n(AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error\n(MSE) curves, with statistical significance calculated at a 0.05 alpha level\nusing a Friedman hypothesis test.", "comment": "Submitted to IEEE Access - Reliability Society", "pdf_url": "http://arxiv.org/pdf/2503.16315v3", "cate": "stat.AP", "date": "2025-03-20", "updated": "2025-07-24", "AI": {"title_translation": "针对可修复硬件系统局部覆盖的主动学习", "tldr": "本文提出了一种新的主动学习采集函数（基于松弛混合整数半定规划），用于在预算有限的情况下，对具有局部覆盖的可修复硬件系统进行可靠性模型参数推断，并在实验中表现优于现有方法。", "motivation": "在固定预算和最少维护周期限制下，识别最佳诊断测试和硬件系统实例以推断可靠性特性具有挑战性。尽管主动学习在机器学习任务中显示出潜力，但其在可修复硬件系统可靠性模型参数推断方面的应用尚未得到充分探索，且需要专门的采集函数来考虑硬件老化和局部测试。", "method": "作者提出了一种松弛混合整数半定规划（MISDP）主动学习采集函数（AF），该函数结合了诊断覆盖率（DC）、费舍尔信息矩阵（FIMs）和诊断测试预算。他们还设计了针对两种诊断测试场景（具有重叠子系统覆盖的局部测试和一种诊断测试完全包含另一种子系统覆盖的局部测试）的基于经验的模拟实验，并将其方法与最广泛使用的主动学习采集函数（熵）以及几种为可靠性模型参数推断量身定制的直观主动学习采集函数进行了比较。", "result": "在6,000种实验配置中，所提出的采集函数在绝对总预期事件误差（ATEER）和均方误差（MSE）曲线的曲线下面积（AUC）方面，平均表现优于其他替代采集函数，并通过Friedman假设检验在0.05的alpha水平上达到了统计显著性。", "conclusion": "本文提出的基于松弛混合整数半定规划的主动学习采集函数，在处理具有局部覆盖的可修复硬件系统可靠性模型参数推断问题上，表现出卓越的性能，显著优于现有主流方法。", "translation": "识别最佳诊断测试和硬件系统实例以利用现场数据推断可靠性特性具有挑战性，尤其是在受固定预算和最少维护周期限制时。主动学习（AL）在机器学习/深度学习任务中，在有限数据和预算约束下，已显示出在参数推断方面的潜力。然而，针对可修复硬件系统进行可靠性模型参数推断的主动学习仍未得到充分探索。它需要专门的主动学习采集函数（AFs），以考虑硬件老化以及硬件系统由多个子系统组成，并且在给定诊断测试期间可能只进行部分测试的事实。为了应对这些挑战，我们提出了一种松弛混合整数半定规划（MISDP）主动学习采集函数，该函数结合了诊断覆盖率（DC）、费舍尔信息矩阵（FIMs）和诊断测试预算。此外，我们设计了基于经验的模拟实验，重点关注两种诊断测试场景：（1）具有重叠子系统覆盖的硬件系统局部测试，以及（2）一种诊断测试完全包含另一种子系统覆盖的局部测试。我们将我们提出的方法与文献中最广泛使用的主动学习采集函数（熵）以及几种为可靠性模型参数推断量身定制的直观主动学习采集函数进行了评估。在6,000种实验配置中，我们提出的采集函数在绝对总预期事件误差（ATEER）和均方误差（MSE）曲线的曲线下面积（AUC）方面，平均表现最佳，并通过Friedman假设检验在0.05的alpha水平上计算出统计显著性。", "summary": "本文研究了在预算和维护周期受限下，对可修复硬件系统进行可靠性模型参数推断时，识别最佳诊断测试的挑战。针对现有主动学习（AL）方法在此领域的不足，作者提出了一种创新的基于松弛混合整数半定规划（MISDP）的AL采集函数，该函数整合了诊断覆盖率、费舍尔信息矩阵和预算约束。通过针对两种局部测试场景的广泛模拟实验，该方法在性能上显著超越了包括熵在内的多种现有AL采集函数，在多项误差指标上均表现出统计学上的优越性。", "keywords": "主动学习, 可修复硬件系统, 可靠性推断, 诊断覆盖率, 混合整数半定规划", "comments": "本文的创新点在于为可修复硬件系统引入了专门的主动学习采集函数，解决了传统主动学习在考虑硬件老化和局部测试方面的不足。其提出的基于MISDP的AF，结合了诊断覆盖率和费舍尔信息矩阵，为有限数据和预算下的可靠性推断提供了一种有效方案。这项工作对于优化工业维护策略和提高硬件系统可靠性具有重要实践意义。"}}
{"id": "2507.18035", "title": "Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming", "authors": ["Hyeonho Noh", "Hyeonsu Lyu", "Hyun Jong Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18035v1", "summary": "This paper explores an integrated sensing and communication (ISAC) network\nempowered by multiple active simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes\ndownlink communication to multiple users while concurrently interrogating a\nsensing target. We jointly optimize the BS transmit beamformer and the\nreflection/transmission coefficients of every active STAR-RIS in order to\nmaximize the aggregate communication sum-rate, subject to (i) a stringent\nsensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an\nupper bound on the leakage of confidential information, and (iii) individual\nhardware and total power constraints at both the BS and the STAR-RISs. The\nresulting highly non-convex program is tackled with an efficient alternating\noptimization (AO) framework. First, the original formulation is reformulated\ninto an equivalent yet more tractable representation and partitioned into\nsubproblems. The BS beamformer is updated in closed form via the\nKarush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and\ntransmission vectors are refined through successive convex approximation (SCA),\nyielding a semidefinite program that is then solved via semidefinite\nrelaxation. Comprehensive simulations demonstrate that the proposed algorithm\ndelivers substantial sum-rate gains over passive-RIS and single STAR-RIS\nbaselines, all the while rigorously meeting the prescribed sensing and security\nconstraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18035v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多主动STAR-RIS辅助的安全集成感知与通信：通过协作波束赋形", "tldr": "优化多主动STAR-RIS和基站波束赋形，实现安全ISAC，在满足感知和安全约束下最大化通信和速率。", "motivation": "本文旨在探索并优化一个由多个主动STAR-RISs赋能的集成感知与通信(ISAC)网络，以在满足严格的感知和安全要求的同时，最大化通信和速率。", "method": "联合优化基站发射波束赋形器和每个主动STAR-RIS的反射/透射系数。针对由此产生的高度非凸问题，提出了一种高效的交替优化(AO)框架来解决。具体地，将原始问题重新表述为更易处理的形式并划分为子问题。基站波束赋形器通过KKT条件以闭合形式更新，而STAR-RIS的反射和透射向量则通过逐次凸逼近(SCA)进行优化，生成一个半定规划问题，并通过半定松弛求解。", "result": "仿真结果表明，所提出的算法在严格满足预设的感知和安全约束的同时，相较于无源RIS和单STAR-RIS基线，实现了显著的和速率增益。", "conclusion": "本文提出的多主动STAR-RIS辅助ISAC网络中的协作波束赋形方案，能够有效提升通信性能并满足安全要求。", "translation": "本文探讨了一个由多个主动同时传输和反射可重构智能表面（STAR-RISs）赋能的集成感知与通信（ISAC）网络。基站（BS）向多个用户提供下行链路通信，同时探测一个感知目标。我们联合优化基站发射波束赋形器和每个主动STAR-RIS的反射/传输系数，以最大化总通信和速率，同时满足（i）严格的感知信噪比（SINR）要求，（ii）机密信息泄露的上限，以及（iii）基站和STAR-RISs的独立硬件和总功率约束。由此产生的高度非凸问题通过一种高效的交替优化（AO）框架来解决。首先，将原始公式重新表述为等效但更易处理的形式，并划分为子问题。基站波束赋形器通过Karush-Kuhn-Tucker（KKT）条件以闭合形式更新，而STAR-RIS的反射和传输向量通过逐次凸逼近（SCA）进行优化，生成一个半定规划问题，然后通过半定松弛求解。全面的仿真结果表明，所提出的算法在严格满足预设的感知和安全约束的同时，相较于无源RIS和单STAR-RIS基线，实现了显著的和速率增益。", "summary": "本文提出了一种利用多个主动STAR-RISs辅助的安全集成感知与通信（ISAC）网络。该研究联合优化基站的发射波束赋形器以及每个STAR-RIS的反射/透射系数，旨在最大化通信和速率，同时严格满足感知信噪比、机密信息泄露上限以及功率约束。针对由此产生的高度非凸优化问题，本文开发了一种高效的交替优化（AO）框架，其中基站波束赋形器通过KKT条件更新，STAR-RIS系数则通过逐次凸逼近（SCA）和半定松弛进行优化。全面的仿真结果验证了所提算法相较于现有基线能够显著提升和速率，并严格满足各项感知与安全要求。", "keywords": "多主动STAR-RIS, 集成感知与通信, 安全通信, 协作波束赋形, 交替优化", "comments": "本文的创新点在于将多个主动STAR-RISs应用于安全集成感知与通信（ISAC）网络，这相较于传统的无源或单STAR-RIS方案是一个显著的进步。其提出的联合优化和高效的交替优化（AO）框架，成功解决了由此带来的高度非凸问题，展示了强大的问题解决能力。该工作对于未来无线网络中感知与通信的融合以及安全性提升具有重要意义。"}}
{"id": "2507.17769", "title": "PolyServe: Efficient Multi-SLO Serving at Scale", "authors": ["Kan Zhu", "Haiyang Shi", "Le Xu", "Jiaxin Shan", "Arvind Krishnamurthy", "Baris Kasikci", "Liguang Xie"], "categories": ["cs.DC", "cs.LG"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17769v1", "summary": "Advances in Large Language Models (LLMs) have led to a surge of LLM-powered\napplications. These applications have diverse token-generation latency\nrequirements. As a result, simply classifying workloads as latency-sensitive\n(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive\ncategory and results in suboptimal user experiences and scheduling\nopportunities. However, efficiently serving requests with multiple SLO\nrequirements poses significant challenges. First, all requests within a batch\ngenerate new tokens simultaneously, which can misalign them with their distinct\nSLO requirements. Moreover, while existing systems focus on auto-scaling for\nhandling various overall request rates, the diversity of SLOs necessitates\nfine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE\nscenarios, where BE requests can be aborted at any time to ensure the SLO\nattainment of LS requests, those with different latency-sensitive SLOs cannot\ntolerate prolonged delays, and tail latency must be controlled.\n  To tackle these challenges, we propose PolyServe, a novel multi-SLO\nscheduling policy at scale that maintains high SLO attainment while maximizing\nthroughput. PolyServe first groups requests into multiple bins based on their\nper-token latency requirement, then schedules each bin to a subset of the\nserver fleet. PolyServe routes requests to the highest-load but still\nSLO-attainable server to create a load gradient that facilitates auto-scaling.\nTo increase utilization, PolyServe permits looser-SLO requests to share\ntighter-SLO instances when their own servers are saturated. PolyServe uses\nprofiling data to guide scheduling decisions and manage tail latency through\nrequest-wait-time-aware scheduling, dynamic chunking, and continuous chunked\nprefill prediction. PolyServe achieves 1.23x goodput gain compared to existing\npolicies, achieving up to 92.5% of optimal goodput.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17769v1", "cate": "cs.DC", "date": "2025-07-17", "updated": "2025-07-17", "AI": {"title_translation": "PolyServe: 规模化高效多SLO服务", "tldr": "PolyServe是一种新颖的多SLO调度策略，旨在解决大规模语言模型（LLM）应用中多样化的延迟要求，通过请求分组、智能路由和尾部延迟管理，显著提高吞吐量和SLO达标率。", "motivation": "大型语言模型（LLM）应用的激增导致了多样化的令牌生成延迟要求。简单地将工作负载分类为延迟敏感（LS）或尽力而为（BE）会忽略延迟敏感类别中的细微差别，导致次优的用户体验和调度机会。然而，高效地服务具有多个服务水平目标（SLO）要求的请求面临巨大挑战：批处理中的所有请求同时生成新令牌可能与其不同的SLO要求不一致；现有系统侧重于处理整体请求率的自动扩缩容，但SLO的多样性需要这些SLO层级之间的细粒度自动扩缩容；与LS/BE场景不同，具有不同延迟敏感SLO的请求无法容忍长时间延迟，并且必须控制尾部延迟。", "method": "为解决这些挑战，我们提出了PolyServe，一种新颖的规模化多SLO调度策略，旨在保持高SLO达标率的同时最大化吞吐量。PolyServe首先根据每令牌延迟要求将请求分组到多个桶中，然后将每个桶调度到服务器集群的一个子集。PolyServe将请求路由到负载最高但仍能达到SLO的服务器，以创建有利于自动扩缩容的负载梯度。为提高利用率，当自身服务器饱和时，PolyServe允许SLO要求较宽松的请求共享SLO要求较严格的实例。PolyServe利用分析数据指导调度决策，并通过请求等待时间感知调度、动态分块和连续分块预填充预测来管理尾部延迟。", "result": "PolyServe相较于现有策略实现了1.23倍的吞吐量提升（goodput gain），并达到了最优吞吐量的92.5%。", "conclusion": "PolyServe通过其新颖的多SLO调度策略，有效解决了大规模LLM服务中多样化延迟要求带来的挑战，显著提高了吞LO达标率和吞吐量，证明了其在复杂服务水平目标场景下的高效性和优越性。", "translation": "大型语言模型（LLM）的进步导致了LLM驱动应用的激增。这些应用具有多样化的令牌生成延迟要求。因此，简单地将工作负载分类为延迟敏感（LS）或尽力而为（BE）会忽略延迟敏感类别中的细微差别，并导致次优的用户体验和调度机会。然而，高效地服务具有多个SLO（服务水平目标）要求的请求带来了重大挑战。首先，批处理中的所有请求同时生成新令牌，这可能使其与各自独特的SLO要求不一致。此外，虽然现有系统侧重于自动扩缩容以处理各种整体请求率，但SLO的多样性需要这些SLO层级之间的细粒度自动扩缩容。最后，与LS/BE场景不同（其中BE请求可以随时中止以确保LS请求的SLO达标），那些具有不同延迟敏感SLO的请求无法容忍长时间延迟，并且必须控制尾部延迟。\n为了应对这些挑战，我们提出了PolyServe，一种新颖的规模化多SLO调度策略，它在最大化吞吐量的同时保持高SLO达标率。PolyServe首先根据其每令牌延迟要求将请求分组到多个桶中，然后将每个桶调度到服务器集群的一个子集。PolyServe将请求路由到负载最高但仍能达到SLO的服务器，以创建有利于自动扩缩容的负载梯度。为了提高利用率，当自身服务器饱和时，PolyServe允许SLO要求较宽松的请求共享SLO要求较严格的实例。PolyServe使用分析数据指导调度决策，并通过请求等待时间感知调度、动态分块和连续分块预填充预测来管理尾部延迟。PolyServe相较于现有策略实现了1.23倍的吞吐量提升（goodput gain），达到了最优吞吐量的92.5%。", "summary": "PolyServe是一种为大规模语言模型（LLM）应用设计的新颖多SLO调度策略，旨在解决现有分类方法无法满足多样化延迟要求的问题。它通过将请求按延迟要求分组、智能路由到负载最高但SLO仍可达到的服务器、允许宽松SLO请求共享资源以及利用分析数据进行尾部延迟管理（包括请求等待时间感知调度、动态分块和连续分块预填充预测）来提高效率。实验结果表明，PolyServe相较于现有策略实现了1.23倍的吞吐量提升，并能达到最优吞吐量的92.5%。", "keywords": "LLM服务, 多SLO调度, 吞吐量优化, 尾部延迟, 自动扩缩容", "comments": "PolyServe的创新之处在于其对多SLO（服务水平目标）请求的精细化管理，超越了传统的延迟敏感/尽力而为的二元分类。它引入了请求分组、基于负载梯度的智能路由以及跨SLO层级资源共享的策略，有效提升了资源利用率和SLO达标率。尤其值得注意的是，其通过请求等待时间感知调度、动态分块和连续分块预填充预测来管理尾部延迟的方法，对于确保LLM服务质量至关重要。这篇论文解决了LLM大规模部署中的一个关键挑战，即如何在保持高性能的同时满足多样化的服务质量要求，具有重要的实践意义。"}}
{"id": "2309.03791", "title": "Optimal Transport Regularized Divergences: Application to Adversarial Robustness", "authors": ["Jeremiah Birrell", "Reza Ebrahimi"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34 pages, 2 figures", "url": "http://arxiv.org/abs/2309.03791v3", "summary": "We introduce a new class of optimal-transport-regularized divergences, $D^c$,\nconstructed via an infimal convolution between an information divergence, $D$,\nand an optimal-transport (OT) cost, $C$, and study their use in\ndistributionally robust optimization (DRO). In particular, we propose the\n$ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness\nof deep learning models. These DRO-based methods are defined by minimizing the\nmaximum expected loss over a $D^c$-neighborhood of the empirical distribution\nof the training data. Viewed as a tool for constructing adversarial samples,\nour method allows samples to be both transported, according to the OT cost, and\nre-weighted, according to the information divergence; the addition of a\nprincipled and dynamical adversarial re-weighting on top of adversarial sample\ntransport is a key innovation of $ARMOR_D$. $ARMOR_D$ can be viewed as a\ngeneralization of the best-performing loss functions and OT costs in the\nadversarial training literature; we demonstrate this flexibility by using\n$ARMOR_D$ to augment the UDR, TRADES, and MART methods and obtain improved\nperformance on CIFAR-10 and CIFAR-100 image recognition. Specifically,\naugmenting with $ARMOR_D$ leads to 1.9\\% and 2.1\\% improvement against\nAutoAttack, a powerful ensemble of adversarial attacks, on CIFAR-10 and\nCIFAR-100 respectively. To foster reproducibility, we made the code accessible\nat https://github.com/star-ailab/ARMOR.", "comment": "34 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2309.03791v3", "cate": "cs.LG", "date": "2023-09-07", "updated": "2025-07-24", "AI": {"title_translation": "最优传输正则化散度：对抗鲁棒性应用", "tldr": "本文引入了一种新的最优传输正则化散度$D^c$，并提出了$ARMOR_D$方法，通过结合信息散度和最优传输成本来增强深度学习模型的对抗鲁棒性，并在CIFAR-10和CIFAR-100数据集上取得了性能提升。", "motivation": "为了增强深度学习模型的对抗鲁棒性，作者提出了一种新的分布鲁棒优化（DRO）方法。", "method": "本文引入了一类新的最优传输正则化散度$D^c$，通过信息散度$D$和最优传输成本$C$的下卷积构建。基于此，提出了$ARMOR_D$方法来增强深度学习模型的对抗鲁棒性。这些基于DRO的方法通过在训练数据经验分布的$D^c$-邻域上最小化最大预期损失来定义。$ARMOR_D$允许对抗样本根据OT成本进行传输，并根据信息散度进行重新加权，关键创新在于在对抗样本传输之上增加了原则性的动态对抗重新加权。", "result": "作者展示了$ARMOR_D$可以作为对抗训练文献中表现最佳的损失函数和OT成本的推广。通过使用$ARMOR_D$增强UDR、TRADES和MART方法，在CIFAR-10和CIFAR-100图像识别任务上获得了改进的性能。具体而言，对抗AutoAttack攻击，在CIFAR-10上性能提升1.9%，在CIFAR-100上提升2.1%。", "conclusion": "本文引入了一种新的最优传输正则化散度，并提出了$ARMOR_D$方法，通过结合传输和动态重新加权来有效提升深度学习模型的对抗鲁棒性，并被证明是现有先进方法的有效推广。", "translation": "我们引入了一类新的最优传输正则化散度$D^c$，它通过信息散度$D$和最优传输（OT）成本$C$的下卷积构建，并研究了它们在分布鲁棒优化（DRO）中的应用。特别是，我们提出了$ARMOR_D$方法作为增强深度学习模型对抗鲁棒性的新颖方法。这些基于DRO的方法通过在训练数据经验分布的$D^c$-邻域上最小化最大预期损失来定义。作为构建对抗样本的工具，我们的方法允许样本根据OT成本进行传输，并根据信息散度进行重新加权；在对抗样本传输之上添加原则性的动态对抗重新加权是$ARMOR_D$的一个关键创新。$ARMOR_D$可以被视为对抗训练文献中表现最佳的损失函数和OT成本的推广；我们通过使用$ARMOR_D$增强UDR、TRADES和MART方法，并在CIFAR-10和CIFAR-100图像识别上获得改进的性能来证明这种灵活性。具体而言，使用$ARMOR_D$增强后，在CIFAR-10和CIFAR-100上对抗强大的对抗攻击集合AutoAttack，分别带来了1.9%和2.1%的性能提升。为了促进可复现性，我们已将代码公开在https://github.com/star-ailab/ARMOR。", "summary": "本文提出了一类新的最优传输正则化散度$D^c$，通过信息散度与最优传输成本的下卷积构建。基于此，引入了$ARMOR_D$方法，旨在通过分布鲁棒优化提升深度学习模型的对抗鲁棒性。该方法的核心创新在于其允许对抗样本同时进行传输和动态重新加权。实验证明，$ARMOR_D$是现有先进对抗训练方法的有效推广，并在CIFAR-10和CIFAR-100数据集上，对抗AutoAttack攻击，取得了显著的性能提升。", "keywords": "最优传输, 对抗鲁棒性, 分布鲁棒优化, 信息散度, 深度学习", "comments": "本文的关键创新在于引入了结合信息散度和最优传输成本的$D^c$散度，并在此基础上提出了$ARMOR_D$方法。$ARMOR_D$的独特之处在于它不仅考虑了对抗样本的传输，还引入了动态的对抗重新加权，这为生成更具挑战性的对抗样本和提升模型鲁棒性提供了新颖的视角。该方法被证明是现有最佳对抗训练方法的推广，并取得了可观的性能提升，对对抗鲁棒性领域具有重要意义。"}}
{"id": "2507.02987", "title": "Leveraging the Structure of Medical Data for Improved Representation Learning", "authors": ["Andrea Agostini", "Sonia Laguna", "Alain Ryser", "Samuel Ruiperez-Campillo", "Moritz Vandenhirtz", "Nicolas Deperrois", "Farhad Nooralahzadeh", "Michael Krauthammer", "Thomas M. Sutter", "Julia E. Vogt"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02987v3", "summary": "Building generalizable medical AI systems requires pretraining strategies\nthat are data-efficient and domain-aware. Unlike internet-scale corpora,\nclinical datasets such as MIMIC-CXR offer limited image counts and scarce\nannotations, but exhibit rich internal structure through multi-view imaging. We\npropose a self-supervised framework that leverages the inherent structure of\nmedical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and\nlateral views) as natural positive pairs, learning to reconstruct each view\nfrom sparse patches while aligning their latent embeddings. Our method requires\nno textual supervision and produces informative representations. Evaluated on\nMIMIC-CXR, we show strong performance compared to supervised objectives and\nbaselines being trained without leveraging structure. This work provides a\nlightweight, modality-agnostic blueprint for domain-specific pretraining where\ndata is structured but scarce", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02987v3", "cate": "cs.CV", "date": "2025-07-01", "updated": "2025-07-24", "AI": {"title_translation": "利用医疗数据结构改进表示学习", "tldr": "该论文提出了一种自监督框架，利用医疗数据集固有的多视图结构进行表示学习，以解决数据稀缺和注释不足的问题，为构建可泛化的医疗AI系统提供了高效的预训练策略。", "motivation": "构建可泛化的医疗AI系统需要数据高效且领域感知的预训练策略。然而，临床数据集（如MIMIC-CXR）图像数量有限且注释稀缺，但通过多视图成像展现出丰富的内部结构，这可以被利用来改进表示学习。", "method": "本文提出了一种自监督框架，利用医疗数据集的固有结构。具体地，将配对的胸部X射线（正面和侧面视图）视为自然的阳性对，通过从稀疏补丁中重建每个视图并对齐它们的潜在嵌入来学习表示。该方法不需要文本监督。", "result": "在MIMIC-CXR数据集上的评估表明，与监督目标和未利用结构训练的基线相比，该方法表现出强大的性能，并产生了信息丰富的表示。", "conclusion": "这项工作为数据结构化但稀缺的领域特定预训练提供了一个轻量级、模态无关的蓝图。", "translation": "构建可泛化的医疗AI系统需要数据高效且领域感知的预训练策略。与互联网规模的语料库不同，MIMIC-CXR等临床数据集的图像数量有限且注释稀缺，但通过多视图成像展现出丰富的内部结构。我们提出了一种自监督框架，该框架利用医疗数据集固有的结构。具体来说，我们将配对的胸部X射线（即正面和侧面视图）视为自然的阳性对，学习从稀疏补丁中重建每个视图，同时对齐它们的潜在嵌入。我们的方法不需要文本监督，并产生信息丰富的表示。在MIMIC-CXR上进行评估，我们展示了与监督目标和未利用结构训练的基线相比的强大性能。这项工作为数据结构化但稀缺的领域特定预训练提供了一个轻量级、模态无关的蓝图。", "summary": "本研究提出了一种自监督框架，旨在利用医疗数据（如多视图X射线）的固有结构进行表示学习，以应对临床数据量小和标注稀缺的问题。该方法将配对的胸部X射线视为正样本对，通过从稀疏补丁重建视图并对齐潜在嵌入来学习。实验证明，该方法在MIMIC-CXR数据集上表现优异，为数据稀缺但结构化的医学领域预训练提供了一种轻量级且通用的解决方案。", "keywords": "医疗AI, 自监督学习, 表示学习, 多视图成像, 数据稀缺", "comments": "该论文为医疗领域的自监督学习提供了一种创新方法，有效解决了数据稀缺和标注有限的关键挑战。通过巧妙地利用医学图像固有的多视图结构，它提供了一种实用且高效的预训练策略。其模态无关的特性表明其在胸部X射线之外也具有广泛的适用性，对构建更具泛化性的医疗AI系统做出了重要贡献。"}}
{"id": "2507.18171", "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models", "authors": ["Kexin Chen", "Dongxia Wang", "Yi Liu", "Haonan Zhang", "Wenhai Wang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 main", "url": "http://arxiv.org/abs/2507.18171v1", "summary": "Despite the widespread use of Transformer-based text embedding models in NLP\ntasks, surprising 'sticky tokens' can undermine the reliability of embeddings.\nThese tokens, when repeatedly inserted into sentences, pull sentence similarity\ntoward a certain value, disrupting the normal distribution of embedding\ndistances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and\nintroducing an efficient detection method, Sticky Token Detector (STD), based\non sentence and token filtering. Applying STD to 40 checkpoints across 14 model\nfamilies, we discover a total of 868 sticky tokens. Our analysis reveals that\nthese tokens often originate from special or unused entries in the vocabulary,\nas well as fragmented subwords from multilingual corpora. Notably, their\npresence does not strictly correlate with model size or vocabulary size. We\nfurther evaluate how sticky tokens affect downstream tasks like clustering and\nretrieval, observing significant performance drops of up to 50%. Through\nattention-layer analysis, we show that sticky tokens disproportionately\ndominate the model's internal representations, raising concerns about\ntokenization robustness. Our findings show the need for better tokenization\nstrategies and model design to mitigate the impact of sticky tokens in future\ntext embedding applications.", "comment": "ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2507.18171v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "紧贴均值：检测文本嵌入模型中的“粘性”词元", "tldr": "本文研究了Transformer文本嵌入模型中“粘性词元”这一现象，这些词元会使句子的相似度趋于某个特定值，从而降低模型性能。论文定义了粘性词元，并提出了一种高效的检测方法STD，发现了大量粘性词元，并分析了它们的来源和对下游任务的负面影响，强调需要改进分词策略和模型设计。", "motivation": "尽管基于Transformer的文本嵌入模型在自然语言处理任务中广泛应用，但令人惊讶的“粘性词元”会损害嵌入的可靠性。这些词元在句子中重复插入时，会将句子相似度拉向某个特定值，破坏嵌入距离的正常分布并降低下游性能。", "method": "本文系统地研究了这种异常词元，正式定义了它们，并引入了一种基于句子和词元过滤的高效检测方法——粘性词元检测器（Sticky Token Detector, STD）。通过注意力层分析，揭示了粘性词元如何不成比例地主导模型的内部表示。", "result": "将STD应用于14个模型家族的40个检查点，总共发现了868个粘性词元。分析表明，这些词元通常源自词汇表中的特殊或未使用条目，以及多语言语料库中的碎片化子词。值得注意的是，它们的出现与模型大小或词汇量没有严格关联。进一步评估了粘性词元如何影响聚类和检索等下游任务，观察到高达50%的显著性能下降。", "conclusion": "研究结果表明，未来的文本嵌入应用需要更好的分词策略和模型设计来减轻粘性词元的影响。", "translation": "尽管基于Transformer的文本嵌入模型在自然语言处理任务中广泛使用，但令人惊讶的“粘性词元”会损害嵌入的可靠性。这些词元在句子中重复插入时，会将句子相似度拉向某个特定值，破坏嵌入距离的正常分布并降低下游性能。在本文中，我们系统地研究了这种异常词元，正式定义了它们，并引入了一种基于句子和词元过滤的高效检测方法——粘性词元检测器（STD）。将STD应用于14个模型家族的40个检查点，我们总共发现了868个粘性词元。我们的分析表明，这些词元通常源自词汇表中的特殊或未使用条目，以及多语言语料库中的碎片化子词。值得注意的是，它们的出现与模型大小或词汇量没有严格关联。我们进一步评估了粘性词元如何影响聚类和检索等下游任务，观察到高达50%的显著性能下降。通过注意力层分析，我们发现粘性词元不成比例地主导模型的内部表示，引发了对分词鲁棒性的担忧。我们的研究结果表明，未来的文本嵌入应用需要更好的分词策略和模型设计来减轻粘性词元的影响。", "summary": "本文深入研究了Transformer文本嵌入模型中存在的“粘性词元”问题，这些词元在句子中重复出现时会使嵌入相似度偏离正常分布，导致下游任务性能下降。作者正式定义了粘性词元，并提出了一种高效的检测方法——粘性词元检测器（STD）。通过对多个模型家族进行检测，发现了大量此类词元，并分析了它们的来源（如特殊或未使用的词汇条目、碎片化子词）。研究还量化了粘性词元对聚类和检索等下游任务造成的显著性能损失（高达50%），并通过注意力层分析揭示了其对模型内部表示的主导作用。最后，论文强调了为解决这一问题，需要开发更稳健的分词策略和改进模型设计。", "keywords": "粘性词元, 文本嵌入, Transformer模型, 分词, 性能下降", "comments": "本文识别并深入分析了Transformer文本嵌入模型中一个之前未被充分认识到的重要问题——“粘性词元”。其创新之处在于首次正式定义了这一现象，并提出了实用的检测方法STD。研究不仅揭示了粘性词元的普遍性、来源和机制（主导内部表示），还量化了它们对下游任务的显著负面影响。这对于理解和改进文本嵌入模型的鲁棒性、可解释性具有重要意义，并为未来的分词策略和模型设计提供了明确的方向。"}}
{"id": "2507.18625", "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18625v1", "summary": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18625v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "约束表达中间表示引导的三维软件合成", "tldr": "Scenethesis是一种新颖的三维软件合成方法，利用约束感知中间表示（ScenethesisLang）实现对三维软件元素的细粒度控制和复杂空间约束的处理，并在用户需求捕捉、约束满足和视觉评估方面优于现有技术。", "motivation": "传统二维软件（如HTML/CSS和移动应用界面代码）的自动化生成已取得显著成功，但三维软件的生成仍未得到充分探索。现有三维软件生成方法通常整体生成三维环境，无法修改或控制特定元素，也难以处理现实世界中固有的复杂空间和语义约束。", "method": "本文提出了Scenethesis，一种新颖的需求敏感三维软件合成方法。它通过ScenethesisLang（一种领域特定语言）作为粒度化、约束感知的中间表示（IR），连接自然语言需求与可执行三维软件。ScenethesisLang既是全面的场景描述语言，支持三维软件元素的精细修改，也是形式化的约束表达规范语言，能够表达复杂空间约束。Scenethesis将三维软件合成分解为在ScenethesisLang上操作的多个阶段，从而实现独立验证、定向修改和系统性约束满足。", "result": "Scenethesis能够准确捕获超过80%的用户需求，同时处理超过100个约束，并满足90%以上的硬约束。与现有最先进的方法相比，Scenethesis在BLIP-2视觉评估分数上取得了42.8%的提升。", "conclusion": "Scenethesis通过引入约束表达中间表示，有效解决了三维软件生成中细粒度控制和复杂约束处理的挑战，显著提高了用户需求满足度、约束满足率和视觉质量。", "translation": "图形用户界面（UI）软件已经从传统的二维（2D）桌面/网页/移动界面发生了根本性转变，进入了空间三维（3D）环境。尽管现有工作在自动化2D软件生成（例如HTML/CSS和移动应用界面代码合成）方面取得了显著成功，但3D软件的生成仍然有待深入探索。当前3D软件生成方法通常将3D环境作为一个整体生成，无法修改或控制软件中的特定元素。此外，这些方法难以处理现实世界中固有的复杂空间和语义约束。为了应对这些挑战，我们提出了Scenethesis，一种新颖的需求敏感3D软件合成方法，它在用户规范和生成的3D软件之间保持了形式化的可追溯性。Scenethesis建立在ScenethesisLang之上，这是一种领域特定语言，作为粒度化、约束感知的中间表示（IR），用于连接自然语言需求和可执行3D软件。它既是一种全面的场景描述语言，能够对3D软件元素进行细粒度修改，又是一种形式化的约束表达规范语言，能够表达复杂的空间约束。通过将3D软件合成分解为在ScenethesisLang上操作的多个阶段，Scenethesis实现了独立验证、定向修改和系统性约束满足。我们的评估表明，Scenethesis能够准确捕获超过80%的用户需求，同时处理超过100个约束，并满足90%以上的硬约束。此外，与现有最先进的方法相比，Scenethesis在BLIP-2视觉评估分数上取得了42.8%的提升。", "summary": "Scenethesis是一种创新的三维软件合成方法，旨在解决当前三维软件生成中缺乏细粒度控制和难以处理复杂空间约束的问题。该方法引入了ScenethesisLang，一种领域特定语言，作为连接用户需求和可执行三维软件的约束感知中间表示。ScenethesisLang既支持对三维软件元素的精细修改，又能表达复杂的空间约束。通过将合成过程分解为多个阶段，Scenethesis实现了独立验证和系统性约束满足。实验结果表明，Scenethesis在用户需求捕获、硬约束满足和视觉评估方面均显著优于现有技术。", "keywords": "三维软件合成, 中间表示, 空间约束, 领域特定语言, Scenethesis", "comments": "本文的创新点在于引入了ScenethesisLang作为一种约束表达的中间表示，从而实现了对三维软件元素的细粒度控制和复杂空间约束的有效处理，这显著超越了现有整体生成三维环境的方法。该方法的重要性体现在它为未来三维UI软件的自动化生成和定制化提供了坚实的基础，有望大大提高3D应用开发的效率和精确度。"}}
{"id": "2507.18407", "title": "DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation", "authors": ["Xun Ye", "Ruixiang Tang", "Mingda Zhang", "Jianglong Qin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages , 11 figures", "url": "http://arxiv.org/abs/2507.18407v1", "summary": "Medical image segmentation leverages topological connectivity theory to\nenhance edge precision and regional consistency. However, existing deep\nnetworks integrating connectivity often forcibly inject it as an additional\nfeature module, resulting in coupled feature spaces with no standardized\nmechanism to quantify different feature strengths. To address these issues, we\npropose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It\nintroduces an innovative feature space decoupling strategy. This strategy\nquantifies the relative strength between connectivity features and other\nfeatures. It then builds a deep connectivity feature fusion-separation\narchitecture. This architecture dynamically balances multi-scale feature\nexpression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg\ndatasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by\n1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)\nand 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).\nThe results demonstrate that DCFFSNet exceeds existing mainstream methods\nacross all metrics. It effectively resolves segmentation fragmentation and\nachieves smooth edge transitions. This significantly enhances clinical\nusability.", "comment": "16 pages , 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.18407v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DCFFSNet：用于医学图像分割的深度连接特征融合分离网络", "tldr": "DCFFSNet通过解耦特征空间并动态平衡多尺度特征，提高了医学图像分割的精度，解决了现有方法中特征耦合和量化不足的问题。", "motivation": "现有深度网络在整合拓扑连接性时，常将其作为附加特征模块强制注入，导致特征空间耦合，且缺乏量化不同特征强度的标准化机制。", "method": "提出DCFFSNet（深度连接特征融合分离网络），引入创新的特征空间解耦策略，量化连接特征与其他特征的相对强度。构建深度连接特征融合分离架构，动态平衡多尺度特征表达。", "result": "在ISIC2018数据集上，DCFFSNet在Dice和IoU上分别优于CMUNet 1.3%和1.2%。在DSB2018上，在Dice和IoU上分别优于TransUNet 0.7%和0.9%。在MoNuSeg上，在Dice和IoU上分别优于CSCAUNet 0.8%和0.9%。DCFFSNet在所有指标上均超越现有主流方法。", "conclusion": "DCFFSNet有效解决了分割碎片化问题并实现了平滑的边缘过渡，显著增强了临床可用性。", "translation": "医学图像分割利用拓扑连接理论来增强边缘精度和区域一致性。然而，现有整合连接性的深度网络通常将其作为附加特征模块强制注入，导致特征空间耦合，并且缺乏量化不同特征强度的标准化机制。为了解决这些问题，我们提出了DCFFSNet（深度连接特征融合分离网络）。它引入了一种创新的特征空间解耦策略。该策略量化了连接特征与其他特征之间的相对强度。然后，它构建了一个深度连接特征融合分离架构。该架构动态平衡了多尺度特征表达。实验在ISIC2018、DSB2018和MoNuSeg数据集上进行。在ISIC2018上，DCFFSNet在Dice和IoU上分别比次优模型（CMUNet）高出1.3%和1.2%。在DSB2018上，它在Dice和IoU上分别超越TransUNet 0.7%和0.9%。在MoNuSeg上，它在Dice和IoU上分别超过CSCAUNet 0.8%和0.9%。结果表明，DCFFSNet在所有指标上都超越了现有主流方法。它有效解决了分割碎片化并实现了平滑的边缘过渡。这显著增强了临床可用性。", "summary": "本文提出DCFFSNet，一个用于医学图像分割的深度网络，旨在解决现有方法中连接性特征与其它特征耦合且强度难以量化的问题。DCFFSNet引入特征空间解耦策略来量化特征强度，并构建深度融合分离架构以动态平衡多尺度特征表达。实验结果表明，DCFFSNet在ISIC2018、DSB2018和MoNuSeg等数据集上均优于现有主流方法，有效改善了分割质量，提高了临床可用性。", "keywords": "医学图像分割, 深度学习, 连接性特征, 特征解耦, DCFFSNet", "comments": "DCFFSNet的创新点在于其特征空间解耦策略，这解决了现有方法中特征耦合和量化不足的痛点。通过动态平衡多尺度特征表达，该网络在医学图像分割的边缘平滑性和区域一致性方面取得了显著提升，对临床应用具有重要意义。"}}
{"id": "2507.18633", "title": "Identifying Prompted Artist Names from Generated Images", "authors": ["Grace Su", "Sheng-Yu Wang", "Aaron Hertzmann", "Eli Shechtman", "Jun-Yan Zhu", "Richard Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.18633v1", "summary": "A common and controversial use of text-to-image models is to generate\npictures by explicitly naming artists, such as \"in the style of Greg\nRutkowski\". We introduce a benchmark for prompted-artist recognition:\npredicting which artist names were invoked in the prompt from the image alone.\nThe dataset contains 1.95M images covering 110 artists and spans four\ngeneralization settings: held-out artists, increasing prompt complexity,\nmultiple-artist prompts, and different text-to-image models. We evaluate\nfeature similarity baselines, contrastive style descriptors, data attribution\nmethods, supervised classifiers, and few-shot prototypical networks.\nGeneralization patterns vary: supervised and few-shot models excel on seen\nartists and complex prompts, whereas style descriptors transfer better when the\nartist's style is pronounced; multi-artist prompts remain the most challenging.\nOur benchmark reveals substantial headroom and provides a public testbed to\nadvance the responsible moderation of text-to-image models. We release the\ndataset and benchmark to foster further research:\nhttps://graceduansu.github.io/IdentifyingPromptedArtists/", "comment": "Project page:\n  https://graceduansu.github.io/IdentifyingPromptedArtists", "pdf_url": "http://arxiv.org/pdf/2507.18633v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "识别生成图像中的提示艺术家名称", "tldr": "本文提出了一个用于识别文本到图像模型中被提示的艺术家名称的基准测试，并评估了多种方法，发现多艺术家提示最具挑战性，为负责任的模型监管提供了测试平台。", "motivation": "文本到图像模型中通过明确命名艺术家来生成图片是一种常见且有争议的使用方式。为了促进对这种行为的负责任监管，需要能够识别图像中被提示的艺术家。", "method": "本文引入了一个用于提示艺术家识别的基准测试，目标是仅从图像中预测提示中调用的艺术家名称。该数据集包含195万张图像，涵盖110位艺术家，并跨越四种泛化设置：保留艺术家、增加提示复杂度、多艺术家提示和不同的文本到图像模型。研究评估了特征相似性基线、对比风格描述符、数据归因方法、监督分类器和少样本原型网络。", "result": "泛化模式各不相同：监督模型和少样本模型在已见艺术家和复杂提示上表现出色，而风格描述符在艺术家风格明显时迁移效果更好；多艺术家提示仍然是最具挑战性的。", "conclusion": "该基准测试揭示了巨大的提升空间，并提供了一个公共测试平台，以推进文本到图像模型的负责任监管。数据集和基准已发布以促进进一步研究。", "translation": "文本到图像模型的一种常见且有争议的用法是通过明确命名艺术家来生成图片，例如“格雷格·鲁特科夫斯基风格”。我们引入了一个用于提示艺术家识别的基准测试：仅从图像预测提示中调用了哪些艺术家名称。该数据集包含195万张图像，涵盖110位艺术家，并跨越四种泛化设置：保留艺术家、增加提示复杂度、多艺术家提示和不同的文本到图像模型。我们评估了特征相似性基线、对比风格描述符、数据归因方法、监督分类器和少样本原型网络。泛化模式各不相同：监督模型和少样本模型在已见艺术家和复杂提示上表现出色，而风格描述符在艺术家风格明显时迁移效果更好；多艺术家提示仍然是最具挑战性的。我们的基准测试揭示了巨大的提升空间，并提供了一个公共测试平台，以推进文本到图像模型的负责任监管。我们发布了数据集和基准，以促进进一步研究：https://graceduansu.github.io/IdentifyingPromptedArtists/", "summary": "本文提出了一个名为“提示艺术家识别”的新基准测试，旨在从生成的图像中识别出在提示中被调用的艺术家名称。该基准包含一个大型数据集（195万张图像，110位艺术家）和四种泛化设置。研究评估了多种识别方法，发现监督模型和少样本模型在已知艺术家和复杂提示上表现良好，而风格描述符在风格突出时表现更佳，但多艺术家提示仍是最大挑战。该工作为文本到图像模型的负责任监管提供了重要工具和公开测试平台。", "keywords": "文本到图像模型, 艺术家识别, 基准测试, 图像生成, 模型监管", "comments": "这项工作非常有意义，它解决了文本到图像模型中一个日益增长的伦理和版权问题，即艺术家风格滥用。通过创建一个大规模的基准数据集和评估多种识别方法，该研究为未来开发更负责任的AI生成工具奠定了基础。其创新点在于构建了针对特定争议用例的识别能力，并明确指出了当前方法的局限性（如多艺术家提示的挑战性），为后续研究指明了方向。"}}
{"id": "2506.22607", "title": "Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation", "authors": ["Daniel Ciganda", "Ignacio Campón", "Iñaki Permanyer", "Jakob H Macke"], "categories": ["stat.AP", "cs.LG"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22607v2", "summary": "Age-specific fertility rates (ASFRs) provide the most extensive record of\nreproductive change, but their aggregate nature obscures the individual-level\nbehavioral mechanisms that drive fertility trends. To bridge this micro-macro\ndivide, we introduce a likelihood-free Bayesian framework that couples a\ndemographically interpretable, individual-level simulation model of the\nreproductive process with Sequential Neural Posterior Estimation (SNPE). We\nshow that this framework successfully recovers core behavioral parameters\ngoverning contemporary fertility, including preferences for family size,\nreproductive timing, and contraceptive failure, using only ASFRs. The\nframework's effectiveness is validated on cohorts from four countries with\ndiverse fertility regimes. Most compellingly, the model, estimated solely on\naggregate data, successfully predicts out-of-sample distributions of\nindividual-level outcomes, including age at first sex, desired family size, and\nbirth intervals. Because our framework yields complete synthetic life\nhistories, it significantly reduces the data requirements for building\nmicrosimulation models and enables behaviorally explicit demographic forecasts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22607v2", "cate": "stat.AP", "date": "2025-06-27", "updated": "2025-07-23", "AI": {"title_translation": "通过神经后验估计从总和生育率学习个体生殖行为", "tldr": "该研究提出了一种贝叶斯框架，结合个体层面的模拟模型和神经后验估计，以从总体生育率中推断个体生殖行为参数，并成功预测个体层面的结果。", "motivation": "年龄别生育率(ASFRs)虽然提供了生殖变化的广泛记录，但其总体性质掩盖了驱动生育趋势的个体层面行为机制。本研究旨在弥合微观与宏观之间的鸿沟。", "method": "引入了一个无似然的贝叶斯框架，该框架将人口学可解释的个体层面生殖过程模拟模型与序贯神经后验估计（SNPE）相结合。该框架仅使用年龄别生育率（ASFRs）来恢复核心行为参数。", "result": "该框架成功地恢复了控制当代生育的核心行为参数，包括家庭规模偏好、生殖时机和避孕失败率。该框架在来自四个不同生育制度国家的队列中得到了验证。该模型仅根据汇总数据进行估计，成功预测了样本外个体层面的结果分布，包括首次性行为年龄、理想家庭规模和生育间隔。", "conclusion": "该框架能够从汇总数据中推断个体生殖行为，显著降低了微观模拟模型的数据需求，并实现了行为明确的人口预测。", "translation": "年龄别生育率（ASFRs）提供了最广泛的生殖变化记录，但其总体性质掩盖了驱动生育趋势的个体层面行为机制。为了弥合这种微观-宏观鸿沟，我们引入了一个无似然的贝叶斯框架，该框架将人口学可解释的个体层面生殖过程模拟模型与序贯神经后验估计（SNPE）相结合。我们证明，该框架仅使用年龄别生育率（ASFRs），就成功恢复了控制当代生育的核心行为参数，包括家庭规模偏好、生殖时机和避孕失败率。该框架的有效性在来自四个不同生育制度国家的队列中得到了验证。最引人注目的是，该模型仅根据汇总数据进行估计，成功预测了样本外个体层面的结果分布，包括首次性行为年龄、理想家庭规模和生育间隔。由于我们的框架产生了完整的合成生命史，它显著降低了构建微观模拟模型的数据需求，并实现了行为明确的人口预测。", "summary": "本研究提出一个无似然的贝叶斯框架，结合个体生殖过程模拟模型和序贯神经后验估计（SNPE），旨在从年龄别生育率（ASFRs）等汇总数据中学习个体生殖行为。该框架成功恢复了家庭规模偏好、生殖时机和避孕失败率等核心行为参数，并在多国队列中得到验证。它还能有效预测个体层面的结果分布，如首次性行为年龄和生育间隔，从而显著减少微观模拟模型的数据需求并支持行为明确的人口预测。", "keywords": "生育率, 神经后验估计, 个体行为, 微观模拟, 贝叶斯框架", "comments": "这项研究的创新之处在于其将微观个体行为模拟模型与宏观汇总数据通过神经后验估计联系起来，有效解决了传统人口学中微观-宏观数据鸿沟的难题。其能够仅凭汇总数据预测个体行为结果，对于人口预测和政策制定具有重要意义，尤其在数据稀缺的背景下。"}}
{"id": "2507.18026", "title": "Emotion Recognition from Skeleton Data: A Comprehensive Survey", "authors": ["Haifeng Lu", "Jiuyi Chen", "Zhen Zhang", "Ruida Liu", "Runhao Zeng", "Xiping Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      34 pages, 5 figures, 13 tables", "url": "http://arxiv.org/abs/2507.18026v1", "summary": "Emotion recognition through body movements has emerged as a compelling and\nprivacy-preserving alternative to traditional methods that rely on facial\nexpressions or physiological signals. Recent advancements in 3D skeleton\nacquisition technologies and pose estimation algorithms have significantly\nenhanced the feasibility of emotion recognition based on full-body motion. This\nsurvey provides a comprehensive and systematic review of skeleton-based emotion\nrecognition techniques. First, we introduce psychological models of emotion and\nexamine the relationship between bodily movements and emotional expression.\nNext, we summarize publicly available datasets, highlighting the differences in\ndata acquisition methods and emotion labeling strategies. We then categorize\nexisting methods into posture-based and gait-based approaches, analyzing them\nfrom both data-driven and technical perspectives. In particular, we propose a\nunified taxonomy that encompasses four primary technical paradigms: Traditional\napproaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works\nwithin each category are reviewed and compared, with benchmarking results\nacross commonly used datasets. Finally, we explore the extended applications of\nemotion recognition in mental health assessment, such as detecting depression\nand autism, and discuss the open challenges and future research directions in\nthis rapidly evolving field.", "comment": "34 pages, 5 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2507.18026v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "骨骼数据情感识别：一项全面综述", "tldr": "这篇综述全面回顾了基于骨骼数据的肢体运动情感识别技术，涵盖心理模型、数据集、方法分类、统一分类法、应用及未来挑战。", "motivation": "肢体运动情感识别作为面部表情或生理信号方法的替代方案，具有保护隐私的优势。3D骨骼获取技术和姿态估计算法的进步显著增强了基于全身运动情感识别的可行性。", "method": "本综述首先介绍了情感的心理模型及身体运动与情感表达的关系；接着总结了公开数据集，并强调数据采集和情感标注策略的差异；然后将现有方法分为基于姿态和基于步态的方法，从数据驱动和技术角度进行分析；特别提出了一个统一的分类法，包括传统方法、Feat2Net、FeatFusionNet和End2EndNet四种主要技术范式，并对各类别中的代表性工作进行回顾和比较，提供了常用数据集上的基准测试结果；最后探讨了情感识别在精神健康评估中的扩展应用，并讨论了开放性挑战和未来研究方向。", "result": "综述提供了对骨骼数据情感识别技术的全面回顾，包括心理学基础、数据集、方法分类（姿态与步态），并提出了一个统一的技术范式分类（传统方法、Feat2Net、FeatFusionNet、End2EndNet），对代表性工作进行了比较，并探讨了在精神健康评估中的应用。", "conclusion": "骨骼数据情感识别是一个快速发展的领域，具有隐私保护的优势和在精神健康评估等方面的应用潜力。该领域仍面临开放性挑战，需要进一步研究。", "translation": "通过身体运动进行情感识别已成为一种引人注目且保护隐私的替代方案，取代了依赖面部表情或生理信号的传统方法。3D骨骼获取技术和姿态估计算法的最新进展显著增强了基于全身运动的情感识别的可行性。本综述对基于骨骼的情感识别技术进行了全面系统的回顾。首先，我们介绍了情感的心理模型，并探讨了身体运动与情感表达之间的关系。接下来，我们总结了公开可用的数据集，强调了数据采集方法和情感标注策略的差异。然后，我们将现有方法分为基于姿态和基于步态的方法，从数据驱动和技术角度对其进行分析。特别是，我们提出了一个统一的分类法，涵盖了四种主要技术范式：传统方法、Feat2Net、FeatFusionNet和End2EndNet。对每个类别中的代表性工作进行了回顾和比较，并提供了常用数据集上的基准测试结果。最后，我们探讨了情感识别在精神健康评估中的扩展应用，例如检测抑郁症和自闭症，并讨论了这个快速发展领域中开放的挑战和未来的研究方向。", "summary": "这篇综述全面审视了基于骨骼数据的肢体运动情感识别技术，将其作为一种保护隐私的替代方案。文章详细介绍了情感的心理学基础、现有数据集的特点、将方法归类为姿态或步态驱动，并提出了一种统一的技术分类法（包括传统、Feat2Net、FeatFusionNet和End2EndNet）。此外，综述还探讨了该技术在精神健康评估中的应用潜力，并指出了该领域的开放挑战和未来研究方向。", "keywords": "情感识别, 骨骼数据, 肢体运动, 姿态估计, 综述", "comments": "这篇综述非常有价值，因为它系统地整理了骨骼数据情感识别领域的最新进展，提供了一个清晰的框架和分类，有助于研究人员快速了解该领域。其对心理学基础、数据集、技术范式和应用前景的全面覆盖，使其成为该领域的重要参考。尤其强调隐私保护和在精神健康中的应用，突出了其潜在的社会价值。"}}
{"id": "2507.18297", "title": "Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation", "authors": ["Sergei Shumilin", "Alexander Ryabov", "Nikolay Yavich", "Evgeny Burnaev", "Vladimir Vanovskiy"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18297v1", "summary": "Due to the high computational load of modern numerical simulation, there is a\ndemand for approaches that would reduce the size of discrete problems while\nkeeping the accuracy reasonable. In this work, we present an original algorithm\nto coarsen an unstructured grid based on the concepts of differentiable\nphysics. We achieve this by employing k-means clustering, autodifferentiation\nand stochastic minimization algorithms. We demonstrate performance of the\ndesigned algorithm on two PDEs: a linear parabolic equation which governs\nslightly compressible fluid flow in porous media and the wave equation. Our\nresults show that in the considered scenarios, we reduced the number of grid\npoints up to 10 times while preserving the modeled variable dynamics in the\npoints of interest. The proposed approach can be applied to the simulation of\nan arbitrary system described by evolutionary partial differential equations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18297v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "自监督非结构化网格粗化与自动微分", "tldr": "本文提出了一种基于可微分物理概念的自监督非结构化网格粗化算法，利用k-均值聚类、自动微分和随机最小化，成功将网格点数量减少多达10倍，同时保持了模拟变量的精度，适用于各种演化偏微分方程。", "motivation": "现代数值模拟的计算负荷高，需要一种在保持合理精度的前提下减小离散问题规模的方法。", "method": "提出了一种基于可微分物理概念的非结构化网格粗化原创算法，通过采用k-均值聚类、自动微分和随机最小化算法来实现。", "result": "在所考虑的场景中，成功将网格点数量减少了多达10倍，同时保留了感兴趣点中建模变量的动态，并在线性抛物线方程和波动方程上验证了性能。", "conclusion": "所提出的自监督网格粗化方法能够有效降低演化偏微分方程的计算负荷，通过显著减少网格点数量同时保持精度。", "translation": "由于现代数值模拟的计算负荷很高，因此需要一种在保持合理精度的同时减小离散问题规模的方法。在这项工作中，我们提出了一种基于可微分物理概念的非结构化网格粗化原创算法。我们通过采用k-均值聚类、自动微分和随机最小化算法来实现这一目标。我们在两个偏微分方程上展示了所设计算法的性能：一个控制多孔介质中微可压缩流体流动的线性抛物线方程和波动方程。我们的结果表明，在所考虑的场景中，我们将网格点数量减少了多达10倍，同时保留了感兴趣点中建模变量的动态。所提出的方法可以应用于由演化偏微分方程描述的任意系统的模拟。", "summary": "本文提出了一种新颖的自监督非结构化网格粗化算法，旨在解决数值模拟中高计算成本问题。该方法利用可微分物理、k-均值聚类、自动微分和随机最小化技术。在对线性抛物线方程和波动方程的测试中，该算法成功地将网格点数量减少了多达十倍，同时保持了变量动态的精度，使其适用于各种演化偏微分方程系统。", "keywords": "自监督, 网格粗化, 自动微分, 可微分物理, 非结构化网格", "comments": "该研究的创新之处在于将自监督学习、可微分物理和自动微分相结合应用于网格粗化，为减少数值模拟中的计算负荷提供了一种新颖有效的方法，尤其对于复杂的非结构化网格。其对任意演化偏微分方程的普适性突显了其潜在的广泛影响。"}}
{"id": "2507.07769", "title": "BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning", "authors": ["Ruohong Liu", "Jack Umenberger", "Yize Chen"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the Workshop on Computational Optimization of Buildings (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML 2025), Vancouver, Canada", "url": "http://arxiv.org/abs/2507.07769v2", "summary": "Recent years have seen significant advancements in designing reinforcement\nlearning (RL)-based agents for building energy management. While individual\nsuccess is observed in simulated or controlled environments, the scalability of\nRL approaches in terms of efficiency and generalization across building\ndynamics and operational scenarios remains an open question. In this work, we\nformally characterize the generalization space for the cross-environment,\nmulti-objective building energy management task, and formulate the\nmulti-objective contextual RL problem. Such a formulation helps understand the\nchallenges of transferring learned policies across varied operational contexts\nsuch as climate and heat convection dynamics under multiple control objectives\nsuch as comfort level and energy consumption. We provide a principled way to\nparameterize such contextual information in realistic building RL environments,\nand construct a novel benchmark to facilitate the evaluation of generalizable\nRL algorithms in practical building control tasks. Our results show that\nexisting multi-objective RL methods are capable of achieving reasonable\ntrade-offs between conflicting objectives. However, their performance degrades\nunder certain environment variations, underscoring the importance of\nincorporating dynamics-dependent contextual information into the policy\nlearning process.", "comment": "Accepted at the Workshop on Computational Optimization of Buildings\n  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML\n  2025), Vancouver, Canada", "pdf_url": "http://arxiv.org/pdf/2507.07769v2", "cate": "cs.LG", "date": "2025-07-10", "updated": "2025-07-24", "AI": {"title_translation": "BEAVER：构建具有可评估变异的环境以评估多目标强化学习", "tldr": "本文提出了一个新基准BEAVER，用于评估多目标强化学习在建筑能源管理任务中的泛化能力，并指出现有方法在环境变化下性能会下降，强调了整合动态相关上下文信息的重要性。", "motivation": "尽管强化学习（RL）在建筑能源管理方面取得了进展，但其在不同建筑动态和操作场景下的效率和泛化能力仍是未解决的问题。", "method": "本文正式表征了跨环境、多目标建筑能源管理任务的泛化空间，并提出了多目标上下文RL问题。作者提供了一种原则性的方法来参数化现实建筑RL环境中的上下文信息，并构建了一个新的基准来评估可泛化的RL算法。", "result": "结果表明，现有MOLP方法能够在相互冲突的目标之间取得合理的权衡。然而，它们在某些环境变化下性能会下降。", "conclusion": "在建筑能源管理中，将依赖于动力学的上下文信息整合到策略学习过程中至关重要，以提高强化学习算法的泛化能力。", "translation": "近年来，在设计用于建筑能源管理的基于强化学习（RL）的智能体方面取得了显著进展。虽然在模拟或受控环境中观察到个体成功，但RL方法在建筑动态和操作场景方面的效率和泛化能力的可扩展性仍然是一个悬而未决的问题。在这项工作中，我们正式表征了跨环境、多目标建筑能源管理任务的泛化空间，并提出了多目标上下文RL问题。这种表述有助于理解在多种控制目标（如舒适度水平和能源消耗）下，将学习到的策略跨不同操作环境（如气候和热对流动态）进行迁移的挑战。我们提供了一种原则性的方法来参数化现实建筑RL环境中的此类上下文信息，并构建了一个新颖的基准，以促进在实际建筑控制任务中评估可泛化的RL算法。我们的结果表明，现有的多目标RL方法能够实现相互冲突目标之间的合理权衡。然而，它们在某些环境变化下性能会下降，这强调了将依赖于动力学的上下文信息整合到策略学习过程中的重要性。", "summary": "本文针对建筑能源管理中强化学习（RL）方法在不同环境下的泛化和可扩展性问题，正式定义了跨环境、多目标任务的泛化空间，并提出了多目标上下文RL问题。为解决此问题，作者提出了一种参数化上下文信息的方法，并构建了一个名为BEAVER的新基准，用于评估可泛化的RL算法。研究结果表明，现有多目标RL方法虽能权衡冲突目标，但在环境变化下性能会下降，因此强调了在策略学习中融入动态相关上下文信息的重要性。", "keywords": "多目标强化学习, 建筑能源管理, 泛化, 上下文RL, 基准", "comments": "本文的创新点在于正式表征了跨环境多目标建筑能源管理任务的泛化空间，并构建了一个新颖的基准BEAVER，为评估和开发更具泛化能力的RL算法提供了平台。其重要性在于指出了现有RL方法在复杂多变环境下的局限性，并强调了上下文信息在提升泛化能力中的关键作用。"}}
{"id": "2406.12548", "title": "P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts", "authors": ["Yuhao Dan", "Jie Zhou", "Qin Chen", "Junfeng Tian", "Liang He"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.12548v3", "summary": "Personalized large language models (LLMs) have attracted great attention in\nmany applications, such as emotional support and role-playing. However,\nexisting works primarily focus on modeling explicit character profiles, while\nignoring the underlying personality traits that truly shape behaviors and\ndecision-making, hampering the development of more anthropomorphic and\npsychologically-grounded AI systems. In this paper, we explore the modeling of\nBig Five personality traits, which is the most widely used trait theory in\npsychology, and propose P-React, a mixture of experts (MoE)-based personalized\nLLM. Particularly, we integrate a Personality Specialization Loss (PSL) to\nbetter capture individual trait expressions, providing a more nuanced and\npsychologically grounded personality simulacrum. To facilitate research in this\nfield, we curate OCEAN-Chat, a high-quality, human-verified dataset designed to\ntrain LLMs in expressing personality traits across diverse topics. Extensive\nexperiments demonstrate the effectiveness of P-React in maintaining consistent\nand real personality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.12548v3", "cate": "cs.CL", "date": "2024-06-18", "updated": "2025-07-24", "AI": {"title_translation": "P-React：通过混合专业LoRA专家合成主题自适应的人格特质反应", "tldr": "本文提出P-React，一个基于MoE的个性化LLM，旨在模拟大五人格特质，并通过引入PSL和构建OCEAN-Chat数据集，实现了真实且一致的人格表达。", "motivation": "现有LLM主要关注显式角色档案，忽略了真正塑造行为和决策的底层人格特质，阻碍了更拟人化和心理学基础的AI系统发展。", "method": "提出P-React，一个基于专家混合（MoE）的个性化LLM。整合了人格特质专业化损失（PSL）以更好地捕捉个体特质表达。构建了高质量、人工验证的OCEAN-Chat数据集，用于训练LLM在不同主题下表达人格特质。", "result": "广泛的实验证明P-React在保持一致和真实人格方面的有效性。", "conclusion": "P-React能够有效模拟大五人格特质，实现真实且一致的个性化LLM。", "translation": "个性化大型语言模型（LLM）在情感支持和角色扮演等许多应用中受到了广泛关注。然而，现有工作主要侧重于建模显式角色档案，而忽略了真正塑造行为和决策的底层人格特质，这阻碍了更拟人化和心理学基础的AI系统的发展。在本文中，我们探索大五人格特质的建模，这是心理学中使用最广泛的特质理论，并提出了P-React，一个基于专家混合（MoE）的个性化LLM。特别是，我们整合了人格特质专业化损失（PSL），以更好地捕捉个体特质表达，提供更细致和心理学基础的人格模拟。为了促进该领域的研究，我们策划了OCEAN-Chat，一个高质量、人工验证的数据集，旨在训练LLM在不同主题下表达人格特质。广泛的实验证明P-React在保持一致和真实人格方面的有效性。", "summary": "本文针对现有个性化LLM忽视底层人格特质的问题，提出了P-React模型。P-React是一个基于专家混合（MoE）的个性化LLM，专门用于模拟大五人格特质。通过引入人格特质专业化损失（PSL），模型能更细致地捕捉个体特质表达。为支持研究，作者还构建了高质量的OCEAN-Chat数据集。实验结果表明P-React能有效保持一致且真实的人格表现。", "keywords": "个性化LLM, 大五人格特质, 专家混合, P-React, OCEAN-Chat", "comments": "本文创新性地将大五人格特质引入LLM的个性化建模，并通过MoE架构和PSL损失，实现了更心理学基础的人格模拟。OCEAN-Chat数据集的构建也为该领域研究提供了宝贵资源。该工作对于开发更具拟人化和情感智能的AI系统具有重要意义。"}}
{"id": "2507.03737", "title": "Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps", "authors": ["Chong Cheng", "Sicheng Yu", "Zijian Wang", "Yifan Zhou", "Hao Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.03737v2", "summary": "3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its\nhigh-fidelity and real-time novel view synthesis performance. However, some\nprevious 3DGS SLAM methods employ a differentiable rendering pipeline for\ntracking, lack geometric priors in outdoor scenes. Other approaches introduce\nseparate tracking modules, but they accumulate errors with significant camera\nmovement, leading to scale drift. To address these challenges, we propose a\nrobust RGB-only outdoor 3DGS SLAM method: S3PO-GS. Technically, we establish a\nself-consistent tracking module anchored in the 3DGS pointmap, which avoids\ncumulative scale drift and achieves more precise and robust tracking with fewer\niterations. Additionally, we design a patch-based pointmap dynamic mapping\nmodule, which introduces geometric priors while avoiding scale ambiguity. This\nsignificantly enhances tracking accuracy and the quality of scene\nreconstruction, making it particularly suitable for complex outdoor\nenvironments. Our experiments on the Waymo, KITTI, and DL3DV datasets\ndemonstrate that S3PO-GS achieves state-of-the-art results in novel view\nsynthesis and outperforms other 3DGS SLAM methods in tracking accuracy. Project\npage: https://3dagentworld.github.io/S3PO-GS/.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.03737v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-07-24", "AI": {"title_translation": "户外单目SLAM与全局尺度一致的3D高斯点图", "tldr": "S3PO-GS是一种新的户外单目3DGS SLAM方法，通过解决尺度漂移和缺乏几何先验的问题，实现了高精度跟踪和场景重建。", "motivation": "现有的3DGS SLAM方法在户外场景中缺乏几何先验，或在相机大幅度移动时会累积误差导致尺度漂移。", "method": "本文提出S3PO-GS，一种鲁棒的仅RGB户外3DGS SLAM方法。该方法建立了一个锚定在3DGS点图中的自洽跟踪模块，以避免累积尺度漂移并实现更精确和鲁棒的跟踪。此外，它还设计了一个基于补丁的点图动态映射模块，引入了几何先验并避免了尺度模糊，从而显著增强了跟踪精度和场景重建质量。", "result": "在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖视图合成方面达到了最先进水平，并在跟踪精度上优于其他3DGS SLAM方法。", "conclusion": "S3PO-GS成功解决了户外单目3DGS SLAM中的尺度漂移和几何先验问题，实现了卓越的跟踪精度和场景重建质量，特别适用于复杂的户外环境。", "translation": "3D高斯泼溅（3DGS）因其高保真和实时的新颖视图合成性能，已成为SLAM中流行的解决方案。然而，一些先前的3DGS SLAM方法采用可微分渲染管道进行跟踪，在户外场景中缺乏几何先验。其他方法引入了独立的跟踪模块，但它们会随着相机的大幅度移动而累积误差，导致尺度漂移。为了解决这些挑战，我们提出了一种鲁棒的仅RGB户外3DGS SLAM方法：S3PO-GS。技术上，我们建立了一个锚定在3DGS点图中的自洽跟踪模块，该模块避免了累积尺度漂移，并以更少的迭代次数实现了更精确和鲁棒的跟踪。此外，我们设计了一个基于补丁的点图动态映射模块，该模块引入了几何先验，同时避免了尺度模糊。这显著增强了跟踪精度和场景重建质量，使其特别适用于复杂的户外环境。我们在Waymo、KITTI和DL3DV数据集上的实验表明，S3PO-GS在新颖视图合成方面取得了最先进的结果，并在跟踪精度上优于其他3DGS SLAM方法。项目页面：https://3dagentworld.github.io/S3PO-GS/。", "summary": "本文提出S3PO-GS，一种针对户外场景的鲁棒RGB单目3DGS SLAM方法。它通过引入自洽跟踪模块来消除尺度漂移，并设计基于补丁的点图动态映射模块以融入几何先验并解决尺度模糊问题。实验证明，S3PO-GS在新颖视图合成和跟踪精度方面均达到了最先进水平，特别适用于复杂的户外环境。", "keywords": "3D Gaussian Splatting, SLAM, 单目, 尺度漂移, 户外", "comments": "S3PO-GS的创新点在于其自洽跟踪模块有效解决了3DGS SLAM中常见的尺度漂移问题，同时引入的补丁式点图动态映射模块为户外场景提供了必要的几何先验，显著提升了系统在复杂环境下的鲁棒性和精度。这对于户外单目SLAM的实际应用具有重要意义。"}}
{"id": "2507.18166", "title": "GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing", "authors": ["Jonas Elmiger", "Gian Marti", "Christoph Studer"], "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18166v1", "summary": "Modern positioning relies on radio signals from global navigation satellite\nsystems (GNSS). Their low receive power renders these radio signals susceptible\nto jamming attacks, in which malicious transmitters emit strong interference to\ndisrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,\nin which malicious transmitters mimic legitimate satellites by transmitting\nspurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna\nGNSS receivers that mitigates jammers as well as spoofers without requiring any\nprior knowledge of the receiver position or attack type: Jammers are mitigated\nduring signal acquisition using a recently developed adaptive spatial filtering\ntechnique. Spoofers are identified and rejected after signal acquisition using\na novel approach that tests the consistency of acquired signals by comparing\ntheir respective direction of arrival (DoA) and pseudorange estimates in a test\nthat is invariant with respect to the unknown receiver position. We demonstrate\nthe efficacy of our method using extensive simulations of a GPS L1 C/A system\nunder spoofing and jamming attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18166v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过多天线处理缓解GNSS干扰器和欺骗器", "tldr": "本文提出了一种名为SCHIEBER的新方法，用于多天线GNSS接收器，可以在无需预先知道接收器位置或攻击类型的情况下，缓解干扰和欺骗攻击。", "motivation": "现代定位系统依赖于全球导航卫星系统（GNSS）的无线电信号，但这些信号接收功率低，容易受到干扰攻击（恶意发射器发出强干扰）和欺骗攻击（恶意发射器模仿合法卫星发送虚假信号），因此需要一种有效的方法来缓解这些攻击。", "method": "本文提出了一种名为SCHIEBER的新方法。对于干扰器，在信号捕获期间使用最近开发的自适应空间滤波技术进行缓解。对于欺骗器，在信号捕获后使用一种新颖的方法进行识别和拒绝，该方法通过比较捕获信号的方向（DoA）和伪距估计来测试其一致性，并且该测试对未知的接收器位置不变。", "result": "该方法在GPS L1 C/A系统在欺骗和干扰攻击下的广泛模拟中展示了其有效性。", "conclusion": "SCHIEBER方法能够有效缓解多天线GNSS接收器中的干扰器和欺骗器，且无需预知接收器位置或攻击类型。", "translation": "现代定位依赖于全球导航卫星系统（GNSS）的无线电信号。它们较低的接收功率使得这些无线电信号容易受到干扰攻击，即恶意发射器发出强干扰以破坏信号捕获。此外，GNSS容易受到欺骗攻击，即恶意发射器通过传输虚假的GNSS信号来模仿合法卫星。我们提出了一种名为SCHIEBER的新方法，用于多天线GNSS接收器，可以在无需预先知道接收器位置或攻击类型的情况下，缓解干扰器和欺骗器：干扰器在信号捕获期间通过最近开发的自适应空间滤波技术进行缓解。欺骗器在信号捕获后通过一种新颖的方法进行识别和拒绝，该方法通过比较捕获信号的到达方向（DoA）和伪距估计来测试其一致性，该测试对于未知的接收器位置是不变的。我们通过在欺骗和干扰攻击下对GPS L1 C/A系统进行广泛模拟，证明了我们方法的有效性。", "summary": "本文提出了一种名为SCHIEBER的多天线GNSS接收器方法，旨在缓解干扰和欺骗攻击，而无需预知接收器位置或攻击类型。它通过自适应空间滤波技术在信号捕获阶段缓解干扰，并在信号捕获后通过比较信号的方向和伪距估计来识别和拒绝欺骗信号。仿真结果验证了该方法在GPS L1 C/A系统中的有效性。", "keywords": "GNSS, 干扰, 欺骗, 多天线, SCHIEBER", "comments": "该论文提出了一种创新的双重缓解策略，同时处理GNSS干扰和欺骗攻击，其亮点在于无需预先获取接收器位置或攻击类型的信息，这大大增强了其实用性。通过结合自适应空间滤波和基于信号一致性的验证，SCHIEBER提供了一个鲁棒的解决方案。其方法在实际应用中具有重要意义，尤其是在复杂电磁环境下。"}}
{"id": "2507.18237", "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception", "authors": ["Chengchang Tian", "Jianwei Ma", "Yan Huang", "Zhanye Chen", "Honghao Wei", "Hui Zhang", "Wei Hong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, accepted as poster. 22 pages including supplementary materials", "url": "http://arxiv.org/abs/2507.18237v1", "summary": "Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA.", "comment": "ICCV 2025, accepted as poster. 22 pages including supplementary\n  materials", "pdf_url": "http://arxiv.org/pdf/2507.18237v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DATA：协同感知中高质量特征融合的域与时间对齐", "tldr": "本文提出了DATA网络，用于解决协同感知中特征融合的域间隙和时间未对齐问题，通过系统性对齐特征并最大化语义表示，实现了最先进的性能。", "motivation": "特征级融合在协同感知中具有潜力，但其有效性严重依赖于输入特征质量。高质量特征的获取面临硬件多样性和部署条件导致的域间隙，以及传输延迟导致的时间未对齐。这些挑战会在整个协同网络中累积效应，降低特征质量。", "method": "本文提出了域与时间对齐（DATA）网络，旨在系统性地对齐特征，同时最大化其用于融合的语义表示。具体包括：1) 一致性保持域对齐模块（CDAM），通过近邻区域分层下采样和可观测性约束判别器来减少域间隙。2) 渐进式时间对齐模块（PTAM），通过多尺度运动建模和两阶段补偿来处理传输延迟。3) 实例聚焦特征聚合模块（IFAM），在对齐特征的基础上增强语义表示。", "result": "DATA在三个典型数据集上实现了最先进的性能，并在存在严重通信延迟和姿态误差的情况下保持了鲁棒性。", "conclusion": "DATA网络通过系统性地解决协同感知中的域间隙和时间未对齐问题，有效地提高了特征融合的质量，从而实现了卓越的性能和鲁棒性。", "translation": "特征级融合在协同感知（CP）中通过平衡性能和通信带宽的权衡展现出前景。然而，其有效性关键依赖于输入特征质量。高质量特征的获取面临硬件多样性和部署条件带来的域间隙，以及传输延迟造成的时间未对齐。这些挑战会在整个协同网络中产生累积效应，降低特征质量。在本文中，我们提出了域与时间对齐（DATA）网络，旨在系统性地对齐特征，同时最大化其用于融合的语义表示。具体来说，我们提出了一个一致性保持域对齐模块（CDAM），通过近邻区域分层下采样和可观测性约束判别器来减少域间隙。我们进一步提出了一个渐进式时间对齐模块（PTAM），通过多尺度运动建模和两阶段补偿来处理传输延迟。在对齐特征的基础上，我们开发了一个实例聚焦特征聚合模块（IFAM）来增强语义表示。广泛的实验表明，DATA在三个典型数据集上实现了最先进的性能，并在存在严重通信延迟和姿态误差的情况下保持了鲁棒性。代码将在https://github.com/ChengchangTian/DATA发布。", "summary": "本文提出了一种名为DATA（Domain-And-Time Alignment）的网络，旨在解决协同感知中特征级融合面临的域间隙和时间未对齐导致的特征质量下降问题。DATA网络包含三个核心模块：一致性保持域对齐模块（CDAM）用于减少域间隙，渐进式时间对齐模块（PTAM）用于处理传输延迟，以及实例聚焦特征聚合模块（IFAM）用于增强语义表示。实验证明，DATA在多个数据集上取得了最先进的性能，并在恶劣通信条件下展现出强大的鲁棒性。", "keywords": "协同感知, 特征融合, 域对齐, 时间对齐, 深度学习", "comments": "本文的创新之处在于系统性地解决了协同感知中特征融合所面临的域间隙和时间未对齐这两大关键挑战。通过提出专门的CDAM和PTAM模块进行精细化对齐，并结合IFAM模块增强语义表示，DATA网络显著提升了协同感知系统的性能和鲁棒性，尤其是在面对复杂实际部署环境中的延迟和误差时表现出色，具有重要的实际应用价值。"}}
{"id": "2505.00046", "title": "SR-NeRV: Improving Embedding Efficiency of Neural Video Representation via Super-Resolution", "authors": ["Taiga Hayami", "Kakeru Koizumi", "Hiroshi Watanabe"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00046v2", "summary": "Implicit Neural Representations (INRs) have garnered significant attention\nfor their ability to model complex signals in various domains. Recently,\nINR-based frameworks have shown promise in neural video compression by\nembedding video content into compact neural networks. However, these methods\noften struggle to reconstruct high-frequency details under stringent\nconstraints on model size, which are critical in practical compression\nscenarios. To address this limitation, we propose an INR-based video\nrepresentation framework that integrates a general-purpose super-resolution\n(SR) network. This design is motivated by the observation that high-frequency\ncomponents tend to exhibit low temporal redundancy across frames. By offloading\nthe reconstruction of fine details to a dedicated SR network pre-trained on\nnatural images, the proposed method improves visual fidelity. Experimental\nresults demonstrate that the proposed method outperforms conventional INR-based\nbaselines in reconstruction quality, while maintaining a comparable model size.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00046v2", "cate": "eess.IV", "date": "2025-04-30", "updated": "2025-07-24", "AI": {"title_translation": "SR-NeRV：通过超分辨率提高神经视频表示的嵌入效率", "tldr": "提出SR-NeRV，将超分辨率网络集成到隐式神经表示中，以在保持模型大小的同时提高神经视频表示的高频细节重建质量。", "motivation": "现有的基于隐式神经表示(INR)的视频压缩方法在严格模型尺寸限制下难以重建高频细节，而这些细节在实际压缩场景中至关重要。高频分量在帧间的时间冗余度较低。", "method": "提出一个基于INR的视频表示框架，该框架集成了通用超分辨率(SR)网络。通过将精细细节的重建任务卸载到一个预训练在自然图像上的专用SR网络，从而提高视觉保真度。", "result": "所提出的方法在重建质量方面优于传统的基于INR的基线方法，同时保持了可比的模型大小。", "conclusion": "通过集成超分辨率网络，可以有效提高基于隐式神经表示的视频压缩方法在严格模型尺寸限制下的高频细节重建能力和视觉保真度。", "translation": "隐式神经表示（INRs）因其在各种领域建模复杂信号的能力而受到广泛关注。最近，基于INR的框架在通过将视频内容嵌入到紧凑的神经网络中，在神经视频压缩方面显示出前景。然而，这些方法在严格的模型尺寸限制下，往往难以重建高频细节，而这在实际压缩场景中至关重要。为了解决这一限制，我们提出了一种基于INR的视频表示框架，该框架集成了一个通用超分辨率（SR）网络。这种设计是基于高频分量在帧间倾向于表现出低时间冗余的观察。通过将精细细节的重建任务卸载到一个预训练在自然图像上的专用SR网络，所提出的方法提高了视觉保真度。实验结果表明，所提出的方法在重建质量方面优于传统的基于INR的基线方法，同时保持了可比的模型大小。", "summary": "SR-NeRV提出了一种新的基于隐式神经表示(INR)的视频表示框架，旨在解决现有INR方法在严格模型尺寸限制下重建高频细节的不足。该方法通过集成一个预训练的通用超分辨率(SR)网络来处理精细细节，利用高频分量时间冗余低的特性。实验证明，SR-NeRV在重建质量上超越了传统INR基线，同时保持了相似的模型大小，有效提升了神经视频表示的嵌入效率和视觉保真度。", "keywords": "隐式神经表示, 视频压缩, 超分辨率, 高频细节, 神经视频表示", "comments": "该论文的创新点在于将超分辨率网络与隐式神经表示相结合，以解决视频压缩中高频细节重建的挑战。这种方法利用了高频分量时间冗余低的特性，并巧妙地将细节重建任务分配给专门的SR网络，从而在保持模型紧凑性的同时显著提升了视觉质量，具有重要的实际应用价值。"}}
{"id": "2507.18434", "title": "Guessing sequences of eigenvectors for LMPs defining spectrahedral relaxations of Eulerian rigidly convex sets", "authors": ["Alejandro González Nevado"], "categories": ["math.CO", "cs.NA", "math.NA", "math.OC", "05A05, 05A15, 05A20 (Primary), 06A07, 65H04, 41A10, 41A45, 41A60,\n  90C23 (Secondary)", "G.2.1; G.1.2; G.1.3; G.1.5; G.1.6"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      51 pages. Preprint extracted from a selection, rewrite and recombination of several sections and chapters from my PhD thesis. For more possible lines of research in these related directions, we direct the interested reader to arXiv:2503.04628 and to arXiv:2507.03800", "url": "http://arxiv.org/abs/2507.18434v1", "summary": "Stable multivariate Eulerian polynomials were introduced by Br\\\"and\\'en.\nParticularizing some variables, it is possible to extract real zero\nmultivariate Eulerian polynomials from them. These real zero multivariate\nEulerian polynomials can be fed into constructions of spectrahedral relaxations\nproviding therefore approximations to the (Eulerian) rigidly convex sets\ndefined by these polynomials. The accuracy of these approximations is measured\nthrough the behaviour in the diagonal, where the usual univariate Eulerian\npolynomials sit. In particular, in this sense, the accuracy of the global\nspectrahedral approximation produced by the spectrahedral relaxation can be\nmeasured in terms of bounds for the extreme roots of univariate Eulerian\npolynomials. The bounds thus obtained beat the previous bounds found in the\nliterature. However, the bound explicitly studied and obtained before beat the\npreviously known bounds by a quantity going to $0$ when $n$ goes to infinity.\nHere we use numerical experiments to construct a sequence of vectors providing\na (linearized) bound whose difference with the previous known bounds is a\ngrowing exponential function (going therefore fast to infinity when $n$ grows).\nThis allows us to establish a better (diagonal) measure of accuracy for the\nspectrahedral relaxation of the Eulerian rigidly convex sets. In particular, we\nwill achieve this by linearizing through the sequence of vectors\n$\\{(y,(-2^{m-i})_{i=3}^{m},(0,\\frac{1}{2}),(1)_{i=1}^{m})\\in\\mathbb{R}^{n+1}\\}_{n=1}^{\\infty}$\nfor even $n=2m$.", "comment": "51 pages. Preprint extracted from a selection, rewrite and\n  recombination of several sections and chapters from my PhD thesis. For more\n  possible lines of research in these related directions, we direct the\n  interested reader to arXiv:2503.04628 and to arXiv:2507.03800", "pdf_url": "http://arxiv.org/pdf/2507.18434v1", "cate": "math.CO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "猜测定义欧拉刚性凸集谱面松弛的LMP特征向量序列", "tldr": "通过数值实验和特定的向量序列，本文提出了一种新的线性化方法，显著提高了欧拉刚性凸集谱面松弛的对角线精度测量。", "motivation": "现有的界限在n趋于无穷时，与已知界限的差异趋近于0，因此需要找到一种能提供更好精度测量的方法。", "method": "使用数值实验构建了一个向量序列，该序列提供了一个线性化的界限，其与先前已知界限的差异是一个指数增长函数。具体通过向量序列 {(y,(-2^{m-i})_{i=3}^{m},(0,\frac{1}{2}),(1)_{i=1}^{m})∈ℝⁿ⁺¹}ⁿ₌₁∞ (对于偶数 n=2m) 进行线性化。", "result": "建立了一种更好的（对角线）方法来测量欧拉刚性凸集的谱面松弛精度。", "conclusion": "通过引入新的线性化方法和向量序列，可以显著提高欧拉刚性凸集谱面松弛的精度测量，并且这种新的界限与先前已知界限的差异呈指数增长。", "translation": "Br\"andén引入了稳定的多元欧拉多项式。通过特定化某些变量，可以从中提取实零多元欧拉多项式。这些实零多元欧拉多项式可以用于构建谱面松弛，从而提供对这些多项式定义的（欧拉）刚性凸集的近似。这些近似的准确性通过对角线上的行为来衡量，对角线上存在通常的单变量欧拉多项式。特别地，从这个意义上讲，由谱面松弛产生的全局谱面近似的准确性可以通过单变量欧拉多项式极端根的界限来衡量。由此获得的界限优于文献中先前的界限。然而，之前明确研究和获得的界限以一个当n趋于无穷时趋于0的量击败了先前已知的界限。在这里，我们使用数值实验来构建一个向量序列，该序列提供了一个（线性化的）界限，其与先前已知界限的差异是一个指数增长函数（因此当n增长时快速趋于无穷大）。这使我们能够为欧拉刚性凸集的谱面松弛建立一个更好的（对角线）精度测量。特别是，我们将通过向量序列 {(y,(-2^{m-i})_{i=3}^{m},(0,\frac{1}{2}),(1)_{i=1}^{m})∈ℝⁿ⁺¹}ⁿ₌₁∞（对于偶数 n=2m）进行线性化来实现这一点。", "summary": "本文关注欧拉刚性凸集的谱面松弛精度测量。在现有界限在n趋于无穷时趋于0的背景下，作者通过数值实验引入了一个新的向量序列，构建了一个线性化的界限。这个新界限与先前已知界限的差异呈指数增长，从而显著提高了欧拉刚性凸集谱面松弛的对角线精度测量。", "keywords": "欧拉多项式, 谱面松弛, 刚性凸集, 精度测量, 特征向量序列", "comments": "该论文的创新点在于引入了通过数值实验构建的特定向量序列进行线性化，从而获得了与先前界限呈指数增长差异的新界限，显著提升了欧拉刚性凸集谱面松弛的精度测量。这对于优化凸集近似方法具有重要意义。"}}
{"id": "2504.10240", "title": "GNN-ACLP: Graph Neural Networks Based Analog Circuit Link Prediction", "authors": ["Guanyuan Pan", "Tiansheng Zhou", "Bingtao Ma", "Yaqi Wang", "Jianxiang Zhao", "Zhi Li", "Yugui Lin", "Pietro Lio", "Shuai Wang"], "categories": ["cs.AR", "cs.LG"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Code and data will be made available on request to the corresponding author. V4 Update: Add Future Work; Improve Typesetting", "url": "http://arxiv.org/abs/2504.10240v4", "summary": "Circuit link prediction identifying missing component connections from\nincomplete netlists is crucial in analog circuit design automation. However,\nexisting methods face three main challenges: 1) Insufficient use of topological\npatterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to\nthe complexity of annotations hinders model generalization; 3) Limited\nadaptability to various netlist formats. We propose GNN-ACLP, a graph neural\nnetworks (GNNs) based method featuring three innovations to tackle these\nchallenges. First, we introduce the SEAL (learning from Subgraphs, Embeddings,\nand Attributes for Link prediction) framework and achieve port-level accuracy\nin circuit link prediction. Second, we propose Netlist Babel Fish, a netlist\nformat conversion tool leveraging retrieval-augmented generation (RAG) with a\nlarge language model (LLM) to improve the compatibility of netlist formats.\nFinally, we construct SpiceNetlist, a comprehensive dataset that contains 775\nannotated circuits across 10 different component classes. Experiments\ndemonstrate accuracy improvements of 16.08% on SpiceNetlist, 11.38% on\nImage2Net, and 16.01% on Masala-CHAI compared to the baseline in intra-dataset\nevaluation, while maintaining accuracy from 92.05% to 99.07% in cross-dataset\nevaluation, exhibiting robust feature transfer capabilities.", "comment": "Code and data will be made available on request to the corresponding\n  author. V4 Update: Add Future Work; Improve Typesetting", "pdf_url": "http://arxiv.org/pdf/2504.10240v4", "cate": "cs.AR", "date": "2025-04-14", "updated": "2025-07-24", "AI": {"title_translation": "GNN-ACLP：基于图神经网络的模拟电路链接预测", "tldr": "GNN-ACLP是一种基于图神经网络的方法，通过引入SEAL框架、Netlist Babel Fish工具和构建SpiceNetlist数据集，解决了模拟电路链接预测中拓扑信息利用不足、数据稀缺和兼容性差的问题，显著提高了预测精度和泛化能力。", "motivation": "在模拟电路设计自动化中，从不完整的网表中识别缺失的元件连接是至关重要的。然而，现有方法面临三个主要挑战：1) 未充分利用电路图中的拓扑模式，导致预测精度降低；2) 注释复杂导致数据稀缺，阻碍模型泛化；3) 对各种网表格式的适应性有限。", "method": "本文提出了GNN-ACLP，一种基于图神经网络（GNNs）的方法，具有三项创新来解决现有挑战。首先，引入了SEAL（从子图、嵌入和属性中学习链接预测）框架，实现了端口级别的电路链接预测精度。其次，提出了Netlist Babel Fish，一个利用检索增强生成（RAG）和大型语言模型（LLM）的网表格式转换工具，以提高网表格式的兼容性。最后，构建了一个包含775个带注释电路的综合数据集SpiceNetlist，涵盖10种不同的元件类别。", "result": "实验表明，在数据集内评估中，GNN-ACLP在SpiceNetlist上比基线提高了16.08%的精度，在Image2Net上提高了11.38%，在Masala-CHAI上提高了16.01%。同时，在跨数据集评估中，精度保持在92.05%至99.07%，展现出强大的特征迁移能力。", "conclusion": "GNN-ACLP通过其创新的框架、工具和新数据集，显著提高了模拟电路链接预测的准确性和泛化能力，并展现了强大的跨数据集特征迁移能力，有效解决了现有方法的局限性。", "translation": "电路链接预测，即从不完整的网表中识别缺失的元件连接，在模拟电路设计自动化中至关重要。然而，现有方法面临三个主要挑战：1) 未充分利用电路图中的拓扑模式，导致预测精度降低；2) 注释的复杂性导致数据稀缺，阻碍模型泛化；3) 对各种网表格式的适应性有限。我们提出了GNN-ACLP，一种基于图神经网络（GNNs）的方法，其具有三项创新来解决这些挑战。首先，我们引入了SEAL（从子图、嵌入和属性中学习链接预测）框架，并在电路链接预测中实现了端口级别的精度。其次，我们提出了Netlist Babel Fish，一个利用检索增强生成（RAG）和大型语言模型（LLM）的网表格式转换工具，以提高网表格式的兼容性。最后，我们构建了SpiceNetlist，一个包含775个带注释电路的综合数据集，涵盖10种不同的元件类别。实验表明，在数据集内评估中，GNN-ACLP在SpiceNetlist上比基线提高了16.08%的精度，在Image2Net上提高了11.38%，在Masala-CHAI上提高了16.01%。同时，在跨数据集评估中，精度保持在92.05%至99.07%，展现出强大的特征迁移能力。", "summary": "本文提出GNN-ACLP，一种基于图神经网络的模拟电路链接预测方法，旨在解决现有方法在拓扑信息利用、数据稀缺和网表格式兼容性方面的不足。GNN-ACLP通过引入SEAL框架提高预测精度，利用Netlist Babel Fish工具（结合RAG和LLM）增强多格式兼容性，并构建了大规模SpiceNetlist数据集。实验结果表明，GNN-ACLP在多个数据集上均显著优于基线，并展现出优异的跨数据集泛化能力。", "keywords": "图神经网络, 模拟电路, 链接预测, 网表, 数据集", "comments": "该论文创新性地将图神经网络应用于模拟电路链接预测领域，并提出了多项针对性解决方案。SEAL框架、基于LLM的Netlist Babel Fish工具以及新构建的SpiceNetlist数据集共同解决了该领域数据稀疏、格式多样和拓扑利用不足的关键难题。特别是引入LLM进行网表格式转换，为电路设计自动化带来了新的思路，具有较高的实用价值和前瞻性。"}}
{"id": "2507.06735", "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "categories": ["cs.CV", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06735v2", "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06735v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-24", "AI": {"title_translation": "驱动残差先验的频率感知网络用于图像融合", "tldr": "提出RPFNet，一个利用残差先验和频率域处理的图像融合网络，解决了计算成本高和缺乏真值的问题，有效提升了融合图像质量和高层视觉任务性能。", "motivation": "图像融合旨在整合多模态信息以生成高质量融合图像，从而提升高层视觉任务性能。但现有全局空间建模计算成本高，且缺乏真值导致难以有效捕获互补特征。", "method": "提出RPFNet，采用双分支特征提取框架：残差先验模块(RPM)从残差图中提取模态特异性差异信息作为补充先验；频域融合模块(FDFM)通过频域卷积实现高效全局特征建模和整合；交叉促进模块(CPM)通过双向特征交互增强局部细节和全局结构的协同感知。训练中引入辅助解码器和显著性结构损失以增强模型对模态特异性差异的敏感度，并结合自适应权重频域对比损失和SSIM损失来约束解空间，促进局部细节和全局特征的联合捕获。", "result": "广泛的实验验证了RPFNet的融合性能，它能有效整合判别性特征，增强纹理细节和显著目标，并能有效促进高层视觉任务的部署。", "conclusion": "RPFNet通过引入残差先验和频率域处理，有效解决了图像融合中计算成本高和缺乏真值的问题，提升了融合图像质量，并对高层视觉任务有积极促进作用。", "translation": "图像融合旨在整合跨模态的互补信息以生成高质量的融合图像，从而提升高层视觉任务的性能。虽然全局空间建模机制展现出有前景的结果，但在空间域构建长距离特征依赖会带来巨大的计算成本。此外，缺乏真值加剧了有效捕获互补特征的难度。为了应对这些挑战，我们提出了一个驱动残差先验的频率感知网络，命名为RPFNet。具体而言，RPFNet采用双分支特征提取框架：残差先验模块（RPM）从残差图中提取模态特异性差异信息，从而为融合提供互补先验；频域融合模块（FDFM）通过频域卷积实现高效的全局特征建模和整合。此外，交叉促进模块（CPM）通过双向特征交互增强局部细节和全局结构的协同感知。在训练期间，我们引入了一个辅助解码器和显著性结构损失，以增强模型对模态特异性差异的敏感性。此外，结合基于自适应权重的频域对比损失和SSIM损失有效地约束了解决方案空间，促进了局部细节和全局特征的联合捕获，同时确保了互补信息的保留。大量的实验验证了RPFNet的融合性能，它能有效整合判别性特征，增强纹理细节和显著目标，并能有效促进高层视觉任务的部署。", "summary": "本文提出了一种名为RPFNet的图像融合网络，旨在解决现有方法中全局空间建模计算成本高和缺乏真值导致特征捕获困难的问题。RPFNet采用双分支结构，通过残差先验模块提取模态差异信息，利用频域融合模块高效处理全局特征，并通过交叉促进模块增强细节与结构协同。结合多重损失函数进行训练，实验证明RPFNet能有效融合判别性特征，提升图像细节和显著性，并促进高层视觉任务。", "keywords": "图像融合, 残差先验, 频率感知网络, 双分支, 深度学习", "comments": "该论文的创新点在于结合了残差先验和频率域处理来解决图像融合中的关键挑战。残差先验模块（RPM）通过利用残差图来捕获模态特异性差异，为融合提供了独特的互补信息。频率域融合模块（FDFM）则通过频域卷积高效地处理全局特征，避免了空间域长距离依赖带来的高计算成本。这种结合局部差异（残差先验）和全局信息（频率域）的方法，以及多重损失函数的设计，使得模型在无真值场景下也能有效工作，提升了融合图像的质量和对高层视觉任务的促进作用，具有较高的实用价值。"}}
{"id": "2507.18131", "title": "Data-Driven Model Order Reduction for Continuous- and Discrete-Time Nonlinear Systems", "authors": ["Behrad Samari", "Henrik Sandberg", "Karl H. Johansson", "Abolfazl Lavaei"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18131v1", "summary": "Model order reduction simplifies high-dimensional dynamical systems by\nderiving lower-dimensional models that preserve essential system\ncharacteristics. These techniques are crucial to controller design for complex\nsystems while significantly reducing computational costs. Nevertheless,\nconstructing effective reduced-order models (ROMs) poses considerable\nchallenges, particularly for dynamical systems characterized by highly\nnonlinear terms. These challenges are further exacerbated when the actual\nsystem model is unavailable, a scenario frequently encountered in real-world\napplications. In this work, we propose a data-driven framework for the\nconstruction of ROMs for both continuous- and discrete-time nonlinear dynamical\nsystems with unknown mathematical models. By leveraging two sets of data\ncollected from the system, referred to as two input-state trajectories, we\nfirst construct a data-based closed-loop representation of the system. We then\nestablish a similarity relation between the output trajectories of the original\nsystem and those of its data-driven ROM employing the notion of simulation\nfunctions (SFs), thereby enabling a formal characterization of their closeness.\nTo achieve this, we propose data-dependent semidefinite programs as sufficient\nconditions to simultaneously construct both ROMs and SFs, while offering\ncorrectness guarantees. We demonstrate that the obtained data-driven ROMs can\nbe employed for synthesizing controllers that ensure the unknown system\nsatisfies high-level logic properties. This is accomplished by first designing\ncontrollers for the data-driven ROMs and then translating the results back to\nthe original system through an interface function. We evaluate the efficacy of\nour data-driven findings through four benchmark case studies involving unknown\ndynamics with highly nonlinear terms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18131v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "连续和离散时间非线性系统的数据驱动模型降阶", "tldr": "本文提出了一种数据驱动框架，用于构建具有未知数学模型的连续和离散时间非线性系统的降阶模型，并证明其可用于控制器合成。", "motivation": "模型降阶对于复杂系统的控制器设计至关重要，但构建有效的降阶模型（ROMs）面临挑战，特别是在系统具有高度非线性项且实际系统模型不可用时。", "method": "本文提出了一种数据驱动框架，用于构建连续和离散时间非线性系统的降阶模型（ROMs）。通过利用系统收集的两组输入-状态轨迹数据，首先构建了系统的数据化闭环表示。然后，通过仿真函数（SFs）的概念，建立了原始系统输出轨迹与其数据驱动ROM输出轨迹之间的相似关系，并提出数据依赖的半正定规划作为充分条件，以同时构建ROMs和SFs，并提供正确性保证。此外，本文展示了如何利用这些数据驱动ROMs来合成控制器，以确保未知系统满足高级逻辑属性，通过为数据驱动ROMs设计控制器并将其结果通过接口函数转换回原始系统。", "result": "所获得的数据驱动ROMs可用于合成控制器，以确保未知系统满足高级逻辑属性。通过四个涉及未知动力学和高度非线性项的基准案例研究，验证了数据驱动方法的有效性。", "conclusion": "本文成功开发了一种数据驱动框架，用于构建连续和离散时间非线性系统的降阶模型，即使在系统模型未知的情况下也能实现有效的控制器合成，并通过仿真函数提供了严谨的相似性表征。", "translation": "模型降阶通过推导出保留基本系统特性的低维模型来简化高维动态系统。这些技术对于复杂系统的控制器设计至关重要，同时显著降低了计算成本。然而，构建有效的降阶模型（ROMs）带来了相当大的挑战，特别是对于以高度非线性项为特征的动态系统。当实际系统模型不可用时，这些挑战会进一步加剧，这种情况在现实世界应用中经常遇到。在这项工作中，我们提出了一种数据驱动框架，用于构建具有未知数学模型的连续和离散时间非线性动态系统的ROMs。通过利用从系统收集的两组数据，即两个输入-状态轨迹，我们首先构建了系统的数据化闭环表示。然后，我们利用仿真函数（SFs）的概念，建立了原始系统输出轨迹与其数据驱动ROM输出轨迹之间的相似关系，从而能够形式化地表征它们的接近程度。为了实现这一点，我们提出了数据依赖的半正定规划作为充分条件，以同时构建ROMs和SFs，同时提供正确性保证。我们证明了所获得的数据驱动ROMs可用于合成控制器，以确保未知系统满足高级逻辑属性。这是通过首先为数据驱动ROMs设计控制器，然后通过接口函数将结果转换回原始系统来完成的。我们通过四个涉及未知动力学和高度非线性项的基准案例研究，评估了我们数据驱动发现的有效性。", "summary": "本文提出了一种数据驱动框架，用于解决连续和离散时间非线性系统在模型未知情况下的模型降阶问题。研究通过利用两组输入-状态轨迹数据，构建系统的数据化闭环表示，并利用仿真函数和半正定规划同时构建降阶模型（ROMs）和仿真函数，以确保模型正确性并表征与原始系统的相似性。此外，该方法展示了如何利用这些数据驱动ROMs来合成控制器，从而使未知系统满足高级逻辑属性。通过四个基准案例研究验证了所提方法的有效性。", "keywords": "模型降阶, 数据驱动, 非线性系统, 仿真函数, 控制器合成", "comments": "本文的创新点在于提出了一个纯粹的数据驱动框架，用于构建未知非线性系统的降阶模型，这在实际应用中具有重要意义。通过引入仿真函数和半正定规划，为降阶模型的正确性和与原系统的相似性提供了理论保证。此外，将降阶模型应用于控制器合成，进一步拓展了其应用价值。该方法克服了传统模型降阶方法对系统精确模型依赖的局限性。"}}
{"id": "2507.17527", "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Seed-LiveInterpret 2.0 Technical Report", "url": "http://arxiv.org/abs/2507.17527v2", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "comment": "Seed-LiveInterpret 2.0 Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.17527v2", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "Seed LiveInterpret 2.0: 端到端同步语音到语音翻译与您的声音", "tldr": "Seed-LiveInterpret 2.0 是一个端到端同步语音到语音翻译模型，具有语音克隆功能，显著提高了翻译质量并大幅降低了延迟，超越了现有商业解决方案。", "motivation": "同步口译（SI）面临诸多挑战，包括转录和翻译质量不佳、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀，特别是在长篇论述中，这些问题严重阻碍了产品级自动化系统的发展。", "method": "本文介绍了 Seed-LiveInterpret 2.0，这是一个端到端同步口译模型，通过其新颖的双工语音到语音理解-生成框架解决现有挑战。该模型通过大规模预训练和强化学习实现。", "result": "实验结果表明，Seed-LiveInterpret 2.0 在翻译准确性和延迟之间取得了显著更好的平衡，经人工译员验证在复杂场景下正确率超过70%。该模型在翻译质量上显著优于商业 SI 解决方案，同时将克隆语音的平均延迟从近10秒大幅削减至近实时3秒，降低了近70%，极大提升了实用性。", "conclusion": "Seed-LiveInterpret 2.0 成功开发了一个端到端、产品级的同步语音到语音翻译解决方案，有效解决了现有挑战，并在翻译质量和延迟方面显著超越了商业系统，极大地提升了同步口译的实用性和可行性。", "translation": "同步口译（SI）是翻译行业中最艰巨的领域之一，产品级自动化系统长期以来一直受到难以解决的挑战困扰：转录和翻译质量不佳、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀，尤其是在长篇论述中。在这项研究中，我们介绍了 Seed-LiveInterpret 2.0，这是一个端到端 SI 模型，能够提供高保真、超低延迟的语音到语音生成，并具备语音克隆功能。作为一款完全可操作的产品级解决方案，Seed-LiveInterpret 2.0 通过我们新颖的双工语音到语音理解-生成框架直接解决了这些挑战。实验结果表明，通过大规模预训练和强化学习，该模型在翻译准确性和延迟之间取得了显著更好的平衡，经人工译员验证在复杂场景下正确率超过70%。值得注意的是，Seed-LiveInterpret 2.0 在翻译质量上显著优于商业 SI 解决方案，同时将克隆语音的平均延迟从近10秒大幅削减至近实时3秒，降低了近70%，极大提升了实用性。", "summary": "本文介绍了 Seed-LiveInterpret 2.0，一个旨在解决同步口译（SI）现有挑战的端到端语音到语音翻译模型。该模型采用新颖的双工理解-生成框架，并结合大规模预训练和强化学习，实现了高保真、超低延迟的语音生成及语音克隆。实验证明，Seed-LiveInterpret 2.0 在翻译准确性和延迟之间取得了显著平衡，在复杂场景下正确率超过70%，并且在翻译质量和延迟方面显著优于现有商业解决方案，将克隆语音延迟从近10秒降至3秒，大幅提升了实用性。", "keywords": "同步口译, 语音到语音翻译, 语音克隆, 低延迟, 端到端", "comments": "该论文提出了一种创新的端到端同步口译解决方案 Seed-LiveInterpret 2.0，其核心亮点在于新颖的双工理解-生成框架以及结合大规模预训练和强化学习的应用。该系统不仅解决了传统 SI 系统面临的质量和延迟问题，特别是实现了语音克隆功能，并且在实际性能上显著超越了商业产品，在翻译质量和延迟优化方面取得了重要突破，极大地提升了同步口译的实用性和产品化潜力。"}}
{"id": "2507.17793", "title": "CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks", "authors": ["Joel Brogan", "Matthew Yohe", "David Cornett"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17793v1", "summary": "What if you could piece together your own custom biometrics and AI analysis\nsystem, a bit like LEGO blocks? We aim to bring that technology to field\noperators in the field who require flexible, high-performance edge AI system\nthat can be adapted on a moment's notice. This paper introduces CHAMP\n(Configurable Hot-swappable Architecture for Machine Perception), a modular\nedge computing platform that allows operators to dynamically swap in\nspecialized AI \"capability cartridges\" for tasks like face recognition, object\ntracking, and document analysis. CHAMP leverages low-power FPGA-based\naccelerators on a high-throughput bus, orchestrated by a custom operating\nsystem (VDiSK) to enable plug-and-play AI pipelines and cryptographically\nsecured biometric datasets. In this paper we describe the CHAMP design,\nincluding its modular scaling with multiple accelerators and the VDiSK\noperating system for runtime reconfiguration, along with its cryptographic\ncapabilities to keep data stored on modules safe and private. Experiments\ndemonstrate near-linear throughput scaling from 1 to 5 neural compute\naccelerators, highlighting both the performance gains and saturation limits of\nthe USB3-based bus. Finally, we discuss applications of CHAMP in field\nbiometrics, surveillance, and disaster response, and outline future\nimprovements in bus protocols, cartridge capabilities, and system software.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17793v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CHAMP：一种用于自适应生物识别任务的可配置、热插拔边缘架构", "tldr": "CHAMP是一种模块化、热插拔的边缘计算平台，利用FPGA加速器和自定义操作系统，为现场操作员提供灵活、高性能的AI生物识别系统。", "motivation": "为现场操作员提供灵活、高性能、可即时适应的边缘AI系统，以应对自定义生物识别和AI分析需求。", "method": "CHAMP是一个模块化边缘计算平台，允许动态插拔AI“能力盒”，利用低功耗FPGA加速器在高吞吐量总线上运行，并通过自定义操作系统VDiSK进行编排，实现即插即用AI流水线和加密保护的生物识别数据集。", "result": "实验表明，从1到5个神经网络计算加速器，吞吐量呈现近线性扩展，并揭示了基于USB3总线的性能增益和饱和限制。", "conclusion": "CHAMP系统可应用于现场生物识别、监控和灾难响应，未来将改进总线协议、能力盒功能和系统软件。", "translation": "如果你能像乐高积木一样，拼凑出自己的定制生物识别和AI分析系统，那会怎么样？我们的目标是为现场操作员带来这项技术，他们需要灵活、高性能的边缘AI系统，能够即时适应。本文介绍了CHAMP（可配置热插拔机器感知架构），一个模块化边缘计算平台，允许操作员动态地插入专用AI“能力盒”，用于人脸识别、物体跟踪和文档分析等任务。CHAMP利用高吞吐量总线上的低功耗基于FPGA的加速器，由自定义操作系统（VDiSK）进行编排，以实现即插即用AI流水线和加密保护的生物识别数据集。在本文中，我们描述了CHAMP的设计，包括其与多个加速器的模块化扩展以及用于运行时重新配置的VDiSK操作系统，以及其加密功能，以确保存储在模块上的数据安全和隐私。实验表明，从1到5个神经网络计算加速器，吞吐量呈现近线性扩展，突出了基于USB3总线的性能增益和饱和限制。最后，我们讨论了CHAMP在现场生物识别、监控和灾难响应中的应用，并概述了总线协议、能力盒功能和系统软件方面的未来改进。", "summary": "本文介绍了CHAMP（可配置热插拔机器感知架构），一个模块化边缘计算平台，旨在为现场操作员提供灵活、高性能的AI系统。CHAMP通过允许动态插拔AI“能力盒”（如人脸识别、物体跟踪），并利用基于FPGA的加速器和自定义操作系统VDiSK实现即插即用AI流水线和加密数据保护。实验证明其在多个加速器下的吞吐量近线性扩展。该系统适用于现场生物识别、监控和灾难响应。", "keywords": "边缘计算, 生物识别, FPGA, 模块化架构, 热插拔", "comments": "CHAMP的创新在于其模块化、热插拔的架构，将AI功能以“能力盒”的形式抽象出来，大大提高了边缘AI系统的灵活性和适应性。结合FPGA加速和自定义操作系统，实现了高效的即插即用能力。此外，数据加密功能也增强了系统的实用性。然而，文中也指出了USB3总线可能存在的性能饱和限制，这可能是未来需要优化的一个点。"}}
{"id": "2507.04599", "title": "QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation", "authors": ["Jiahui Yang", "Yongjia Ma", "Donglin Di", "Hao Li", "Wei Chen", "Yan Xie", "Jianxun Cui", "Xun Yang", "Wangmeng Zuo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, 30 pages, 26 figures", "url": "http://arxiv.org/abs/2507.04599v2", "summary": "Existing text-to-image models often rely on parameter fine-tuning techniques\nsuch as Low-Rank Adaptation (LoRA) to customize visual attributes. However,\nwhen combining multiple LoRA models for content-style fusion tasks,\nunstructured modifications of weight matrices often lead to undesired feature\nentanglement between content and style attributes. We propose QR-LoRA, a novel\nfine-tuning framework leveraging QR decomposition for structured parameter\nupdates that effectively separate visual attributes. Our key insight is that\nthe orthogonal Q matrix naturally minimizes interference between different\nvisual features, while the upper triangular R matrix efficiently encodes\nattribute-specific transformations. Our approach fixes both Q and R matrices\nwhile only training an additional task-specific $\\Delta R$ matrix. This\nstructured design reduces trainable parameters to half of conventional LoRA\nmethods and supports effective merging of multiple adaptations without\ncross-contamination due to the strong disentanglement properties between\n$\\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior\ndisentanglement in content-style fusion tasks, establishing a new paradigm for\nparameter-efficient, disentangled fine-tuning in generative models. The project\npage is available at: https://luna-ai-lab.github.io/QR-LoRA/.", "comment": "ICCV 2025, 30 pages, 26 figures", "pdf_url": "http://arxiv.org/pdf/2507.04599v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-24", "AI": {"title_translation": "QR-LoRA：通过QR分解实现高效解耦微调以进行定制化生成", "tldr": "QR-LoRA是一种新的微调框架，利用QR分解解决LoRA在多模型融合中特征纠缠的问题，通过结构化参数更新实现内容与风格的解耦，同时减少了训练参数。", "motivation": "现有的文本到图像模型（如LoRA）在结合多个模型进行内容-风格融合任务时，由于权重矩阵的非结构化修改，常常导致内容和风格属性之间出现不期望的特征纠缠。", "method": "我们提出了QR-LoRA，一个利用QR分解进行结构化参数更新的新型微调框架。其核心思想是，正交Q矩阵自然地最小化不同视觉特征之间的干扰，而上三角R矩阵有效地编码属性特定的变换。我们的方法固定Q和R矩阵，仅训练一个额外的任务特定$\\\\Delta R$矩阵。这种结构化设计将可训练参数减少到传统LoRA方法的一半。", "result": "实验表明，QR-LoRA在内容-风格融合任务中实现了卓越的解耦，并支持多个自适应的有效合并而没有交叉污染。它将可训练参数减少到传统LoRA方法的一半。", "conclusion": "QR-LoRA为生成模型中的参数高效、解耦微调建立了一个新的范式，通过QR分解有效地分离了视觉属性，解决了现有LoRA在多模型融合中特征纠缠的挑战。", "translation": "现有文本到图像模型通常依赖参数微调技术，例如低秩适应（LoRA），以定制视觉属性。然而，当结合多个LoRA模型进行内容-风格融合任务时，权重矩阵的非结构化修改常常导致内容和风格属性之间出现不期望的特征纠缠。我们提出了QR-LoRA，一个利用QR分解进行结构化参数更新的新型微调框架，能有效分离视觉属性。我们的关键见解是，正交Q矩阵自然地最小化不同视觉特征之间的干扰，而上三角R矩阵有效地编码属性特定的变换。我们的方法固定Q和R矩阵，仅训练一个额外的任务特定$\\\\Delta R$矩阵。这种结构化设计将可训练参数减少到传统LoRA方法的一半，并且由于$\\\\Delta R$矩阵之间强大的解耦特性，支持多个自适应的有效合并而没有交叉污染。实验表明，QR-LoRA在内容-风格融合任务中实现了卓越的解耦，为生成模型中的参数高效、解耦微调建立了一个新的范式。项目页面可在：https://luna-ai-lab.github.io/QR-LoRA/ 访问。", "summary": "QR-LoRA提出了一种基于QR分解的新型微调框架，旨在解决现有LoRA在文本到图像模型中多模型融合时出现的特征纠缠问题。通过利用正交Q矩阵和上三角R矩阵的特性，QR-LoRA实现了视觉属性的有效分离。该方法仅训练一个额外的任务特定$\\\\Delta R$矩阵，显著减少了可训练参数，并确保了多模型合并时的解耦性，避免了交叉污染。实验证明，QR-LoRA在内容-风格融合任务中表现出卓越的解耦能力，为生成模型提供了一种参数高效且解耦的微调新范式。", "keywords": "QR-LoRA, 微调, QR分解, 特征解耦, 内容-风格融合", "comments": "QR-LoRA的创新点在于将QR分解引入LoRA微调中，巧妙地利用Q矩阵的正交性来实现特征解耦，并用R矩阵编码属性变换，这为解决多LoRA模型融合时的特征纠缠问题提供了一个优雅且高效的解决方案。其参数量减半的特性也使其在资源受限场景下更具吸引力。该方法开辟了生成模型微调的新方向，有望在定制化生成任务中发挥重要作用。"}}
{"id": "2505.20043", "title": "Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing", "authors": ["Marcello Cellina", "Matteo Corno", "Sergio Matteo Savaresi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IEEE Conference, 6 pages", "url": "http://arxiv.org/abs/2505.20043v2", "summary": "High Speed multi-vehicle Autonomous Racing will increase the safety and\nperformance of road-going Autonomous Vehicles. Precise vehicle detection and\ndynamics estimation from a moving platform is a key requirement for planning\nand executing complex autonomous overtaking maneuvers. To address this\nrequirement, we have developed a Latency-Aware EKF-based Multi Target Tracking\nalgorithm fusing LiDAR and RADAR measurements. The algorithm explots the\ndifferent sensor characteristics by explicitly integrating the Range Rate in\nthe EKF Measurement Function, as well as a-priori knowledge of the racetrack\nduring state prediction. It can handle Out-Of-Sequence Measurements via\nReprocessing using a double State and Measurement Buffer, ensuring sensor delay\ncompensation with no information loss. This algorithm has been implemented on\nTeam PoliMOVE's autonomous racecar, and was proved experimentally by completing\na number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.", "comment": "IEEE Conference, 6 pages", "pdf_url": "http://arxiv.org/pdf/2505.20043v2", "cate": "cs.RO", "date": "2025-05-26", "updated": "2025-07-24", "AI": {"title_translation": "面向自动驾驶赛车的激光雷达-雷达传感器融合目标跟踪", "tldr": "本文提出了一种延迟感知扩展卡尔曼滤波器（EKF）的多目标跟踪算法，融合激光雷达和雷达数据，用于自动驾驶赛车中的目标跟踪，成功实现了高速超车。", "motivation": "高速多车自动驾驶赛车需要从移动平台进行精确的车辆检测和动力学估计，以规划和执行复杂的自动超车机动，这是提高安全性和性能的关键要求。", "method": "开发了一种基于延迟感知扩展卡尔曼滤波器（EKF）的多目标跟踪算法，融合了激光雷达和雷达测量数据。该算法在EKF测量函数中明确集成了距离变化率，并在状态预测期间利用了赛道的先验知识。它通过使用双状态和测量缓冲区进行重新处理来处理乱序测量，确保传感器延迟补偿而没有信息丢失。", "result": "该算法已在PoliMOVE车队的自动赛车上实现，并通过完成多项速度高达275公里/小时的完全自主超车机动进行了实验验证。", "conclusion": "所开发的传感器融合算法成功实现了赛车中高速自动超车机动的精确目标跟踪，满足了关键的安全和性能要求。", "translation": "高速多车自动驾驶赛车将提高公路自动驾驶车辆的安全性和性能。从移动平台进行精确的车辆检测和动力学估计是规划和执行复杂自动超车机动的关键要求。为了解决这一要求，我们开发了一种基于延迟感知扩展卡尔曼滤波器（EKF）的多目标跟踪算法，融合了激光雷达和雷达测量数据。该算法通过在EKF测量函数中明确集成距离变化率，以及在状态预测期间利用赛道的先验知识，来利用不同传感器的特性。它可以通过使用双状态和测量缓冲区进行重新处理来处理乱序测量，确保传感器延迟补偿而没有信息丢失。该算法已在PoliMOVE车队的自动赛车上实现，并通过完成多项速度高达275公里/小时的完全自主超车机动进行了实验验证。", "summary": "本文提出了一种基于延迟感知扩展卡尔曼滤波器（EKF）的多目标跟踪算法，该算法融合了激光雷达和雷达数据，用于高速自动驾驶赛车中精确的车辆检测和动力学估计。该算法结合了距离变化率和赛道先验知识，并能处理乱序测量以补偿延迟。该算法已在自动赛车上实现，并成功演示了速度高达275公里/小时的自主超车机动，解决了提高安全性和性能的关键要求。", "keywords": "激光雷达-雷达融合, 目标跟踪, 自动驾驶赛车, EKF, 传感器融合", "comments": "该论文的创新之处在于其延迟感知EKF方法、明确集成距离变化率以及处理乱序测量的能力，这些都专门为高速自动驾驶赛车的严苛环境量身定制。其在275公里/小时的速度下进行的实验验证，证明了在具有挑战性的真实场景中的实际适用性和鲁棒性。"}}
{"id": "2507.18096", "title": "Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation", "authors": ["Jihong Huang", "Rong Yang", "Wei Gao", "Xingqun Zhan", "Zheng Yao"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18096v1", "summary": "Direct Position Estimation (DPE) is a method that directly estimate position,\nvelocity, and time (PVT) information from cross ambiguity function (CAF) of the\nGNSS signals, significantly enhancing receiver robustness in urban\nenvironments. However, there is still a lack of theoretical characterization on\nmultipath errors in the context of DPE theory. Geometric observations highlight\nthe unique characteristics of DPE errors stemming from multipath and thermal\nnoise as estimation bias and variance respectively. Expanding upon the\ntheoretical framework of DPE noise variance through geometric analysis, this\npaper focuses on a geometric representation of multipath errors by quantifying\nthe deviations in CAF and PVT solutions caused by off-centering bias relative\nto the azimuth and elevation angles. A satellite circular multipath bias (SCMB)\nmodel is introduced, amalgamating CAF and PVT errors from multiple satellite\nchannels. The boundaries for maximum or minimum PVT bias are established\nthrough discussions encompassing various multipath conditions. The correctness\nof the multipath geometrical portrait is confirmed through both Monte Carlo\nsimulations and urban canyon tests. The findings indicate that the maximum PVT\nbias depends on the largest multipath errors observed across various satellite\nchannels. Additionally, the PVT bias increases with satellite elevation angles,\ninfluenced by the CAF multipath bias projection. This serves as a reference for\nselecting DPE satellites from a geometric standpoint, underscoring the\nimportance of choosing a balanced combination of high and low elevation angles\nto achieve an optimal satellite geometry configuration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18096v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GNSS直接定位中多径误差传播的几何描绘", "tldr": "本文通过几何分析和模型，描述并量化了GNSS直接定位（DPE）中多径误差对定位、速度和时间（PVT）解的影响，并提出了卫星选择的几何参考。", "motivation": "现有DPE理论缺乏对多径误差的理论表征，而多径误差是影响城市环境下DPE接收机鲁棒性的关键因素。", "method": "论文通过几何分析量化了多径误差引起的CAF和PVT解的偏差，引入了卫星圆形多径偏差（SCMB）模型，并结合蒙特卡洛仿真和城市峡谷测试验证了该模型。", "result": "结果表明，最大PVT偏差取决于不同卫星通道中观测到的最大多径误差；PVT偏差随卫星仰角增加而增大，受CAF多径偏差投影的影响。", "conclusion": "本研究为从几何角度选择DPE卫星提供了参考，强调了选择高低仰角卫星平衡组合以实现最佳卫星几何配置的重要性。", "translation": "直接定位估计（DPE）是一种直接从GNSS信号的交叉模糊函数（CAF）中估计位置、速度和时间（PVT）信息的方法，显著增强了接收机在城市环境中的鲁棒性。然而，在DPE理论背景下，多径误差的理论表征仍然缺乏。几何观测强调了DPE误差的独特特征，这些误差分别源于多径和热噪声，表现为估计偏差和方差。本文在通过几何分析扩展DPE噪声方差理论框架的基础上，通过量化偏心偏差相对于方位角和仰角引起的CAF和PVT解决方案的偏差，重点关注多径误差的几何表示。引入了卫星圆形多径偏差（SCMB）模型，该模型整合了来自多个卫星通道的CAF和PVT误差。通过讨论各种多径条件，建立了最大或最小PVT偏差的边界。通过蒙特卡洛仿真和城市峡谷测试证实了多径几何描绘的正确性。研究结果表明，最大PVT偏差取决于在各个卫星通道中观测到的最大多径误差。此外，PVT偏差随卫星仰角的增加而增加，受CAF多径偏差投影的影响。这为从几何角度选择DPE卫星提供了参考，强调了选择高低仰角平衡组合以实现最佳卫星几何配置的重要性。", "summary": "本文针对GNSS直接定位（DPE）中多径误差缺乏理论表征的问题，通过几何分析，量化了多径误差对交叉模糊函数（CAF）和位置、速度、时间（PVT）解的影响。研究引入了卫星圆形多径偏差（SCMB）模型，并利用蒙特卡洛仿真和城市峡谷测试验证了其有效性。研究发现，最大PVT偏差与最大多径误差相关，且随卫星仰角增加而增大。这些发现为DPE卫星选择提供了重要的几何参考，建议选择高低仰角平衡的卫星组合以优化性能。", "keywords": "直接定位估计, 多径误差, 几何分析, 卫星圆形多径偏差, GNSS", "comments": "本文创新性地从几何角度深入分析了GNSS直接定位（DPE）中的多径误差传播，并提出了卫星圆形多径偏差（SCMB）模型，填补了DPE理论在多径误差表征方面的空白。通过量化误差和提供卫星选择的几何指导，该研究对于提升城市环境下DPE接收机的鲁棒性和定位精度具有重要意义。"}}
{"id": "2411.14825", "title": "Distributed Model Checking in Graphs Classes of Bounded Expansion", "authors": ["Lélia Blin", "Fedor V. Fomin", "Pierre Fraigniaud", "Sylvain Gay", "Petr A. Golovach", "Pedro Montealegre", "Ivan Rapaport", "Ioan Todinca"], "categories": ["cs.DS", "F.2"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.14825v2", "summary": "The sparsity theory of Ne\\v{s}et\\v{r}il and Ossona de Mendez provides generic\ncombinatorial and algorithmic methods suitable for many classes of sparse\ngraphs. As such, it has already driven major advances in model checking,\nparameterized complexity, and approximation algorithms. We show that this\ntheory is equally well suited to the design of distributed algorithms,\nproviding the first generic algorithmic results for distributed first-order\n(FO) model checking on graphs of bounded expansion. The latter form a rich\ngraph family which subsumes planar, bounded-genus, bounded-treewidth, and\nbounded-degree graphs, as well as graphs excluding a fixed minor or topological\nminor, and sparse Erdos-R\\'enyi graphs (a.a.s).\n  Our main results are the following algorithmic meta-theorems for distributed\nalgorithms in the standard CONGEST model on every graph class $\\mathcal{G}$ of\nbounded expansion.\n  First, we resolve an open problem originally posed by Ne\\v{s}et\\v{r}il and\nOssona de Mendez (Distributed Computing 2016), and reiterated by Pilipczuk,\nSiebertz, and Toru\\'nczyk (LICS 2018). A formula $\\varphi(x)$ is local if the\nsatisfaction of $\\varphi(x)$ depends only on the $r$-neighbourhood of its free\nvariable $x$, for some fixed $r$. For instance, the formula '$x$ belongs to a\ntriangle' is local. We show that, for every local FO formula $\\varphi(x)$,\nthere exists a deterministic algorithm that, for every $n$-vertex graph $G\\in\n\\mathcal{G}$, identifies all vertices $v\\in V(G)$ such that $G\\models\n\\varphi(v)$, in $O(\\log n)$ rounds.\n  Second, we show that, for every FO formula $\\varphi$, there is a\ndeterministic algorithm that, for every $n$-vertex graph $G\\in \\mathcal{G}$,\ndecides whether $G\\models \\varphi$, in $\\mathcal{O}(D+\\log n)$ rounds, where\n$D$ is the diameter of $G$. The techniques extend to distributed counting,\noptimization, and certification problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.14825v2", "cate": "cs.DS", "date": "2024-11-22", "updated": "2025-07-24", "AI": {"title_translation": "有界扩张图类中的分布式模型检查", "tldr": "本文首次为有界扩张图上的分布式一阶（FO）模型检查提供了通用的算法结果，解决了Nešetřil和Ossona de Mendez提出的一个开放问题，并在CONGEST模型下实现了对局部FO公式的O(log n)轮识别和对任意FO公式的O(D+log n)轮判定。", "motivation": "Nešetřil和Ossona de Mendez的稀疏性理论已推动模型检查、参数化复杂性和近似算法的重大进展，但其在分布式算法设计方面的应用尚未得到充分探索。本文旨在填补这一空白，为有界扩张图上的分布式一阶（FO）模型检查提供通用的算法结果。", "method": "本文利用Nešetřil和Ossona de Mendez的稀疏性理论，在标准的CONGEST模型下，为有界扩张图类设计分布式算法。具体方法包括：1. 针对局部一阶（FO）公式，设计一种确定性算法，在O(log n)轮内识别所有满足条件的顶点。2. 针对任意一阶（FO）公式，设计一种确定性算法，在O(D+log n)轮内判定图是否满足该公式。", "result": "1. 解决了Nešetřil和Ossona de Mendez（2016）以及Pilipczuk, Siebertz, 和Toruńczyk（2018）提出的一个开放问题：对于每个局部一阶（FO）公式$\\varphi(x)$，存在一个确定性算法，在O(log n)轮内识别所有满足$\\varphi(v)$的顶点v。2. 对于每个一阶（FO）公式$\\varphi$，存在一个确定性算法，在O(D+log n)轮内判定图G是否满足$\\varphi$。3. 这些技术还可以扩展到分布式计数、优化和认证问题。", "conclusion": "本文首次为有界扩张图类中的分布式一阶（FO）模型检查提供了通用的算法元定理，填补了稀疏性理论在分布式算法设计方面的空白，并解决了该领域的一个重要开放问题。这些结果为分布式算法设计提供了新的视角和工具。", "translation": "Nešetřil和Ossona de Mendez的稀疏性理论提供了适用于许多稀疏图类的通用组合和算法方法。因此，它已经推动了模型检查、参数化复杂性和近似算法的重大进展。我们表明，该理论同样适用于分布式算法的设计，为有界扩张图上的分布式一阶（FO）模型检查提供了首批通用算法结果。有界扩张图是一种丰富的图族，它包括平面图、有界亏格图、有界树宽图、有界度图，以及排除固定次图或拓扑次图的图，以及稀疏Erdos-Rényi图（a.a.s）。\n我们的主要成果是在标准CONGEST模型下，针对每个有界扩张图类$\\mathcal{G}$的分布式算法的以下算法元定理。\n首先，我们解决了Nešetřil和Ossona de Mendez（Distributed Computing 2016）最初提出并由Pilipczuk, Siebertz, 和Toruńczyk（LICS 2018）重申的一个开放问题。如果一个公式$\\varphi(x)$的满足性仅依赖于其自由变量x的r-邻域（对于某个固定的r），则称其为局部公式。例如，公式“x属于一个三角形”是局部公式。我们表明，对于每个局部FO公式$\\varphi(x)$，存在一个确定性算法，对于每个n顶点图$G\\in \\mathcal{G}$，在O(log n)轮内识别所有满足$G\\models \\varphi(v)$的顶点$v\\in V(G)$。\n其次，我们表明，对于每个FO公式$\\varphi$，存在一个确定性算法，对于每个n顶点图$G\\in \\mathcal{G}$，在$\\mathcal{O}(D+\\log n)$轮内判定$G\\models \\varphi$，其中D是G的直径。这些技术可以扩展到分布式计数、优化和认证问题。", "summary": "本文首次将Nešetřil和Ossona de Mendez的稀疏性理论应用于分布式算法设计，为有界扩张图（包括平面图、有界树宽图等广泛图族）上的分布式一阶（FO）模型检查提供了通用的算法元定理。研究解决了CONGEST模型下的一个开放问题，即对于局部FO公式，存在O(log n)轮的确定性算法来识别满足条件的顶点；对于任意FO公式，存在O(D+log n)轮的确定性算法来判定图是否满足该公式。这些方法还可扩展到分布式计数、优化和认证问题。", "keywords": "分布式算法, 模型检查, 有界扩张图, 稀疏性理论, CONGEST模型", "comments": "本文的创新之处在于首次将稀疏性理论系统地应用于分布式算法领域，特别是在有界扩张图上的分布式模型检查。它不仅填补了该领域的一个空白，还解决了一个长期存在的开放问题，为未来分布式算法的设计提供了强大的理论基础和通用方法。其提出的算法元定理对于理解和设计高效的分布式图算法具有重要意义。"}}
{"id": "2507.17289", "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments", "authors": ["Shitong Zhu", "Chenhao Fang", "Derek Larson", "Neel Reddy Pochareddy", "Rajeev Rao", "Sophie Zeng", "Yanqing Peng", "Wendy Summer", "Alex Goncalves", "Arya Pudota", "Hervé Robert"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17289v2", "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17289v2", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "合规大脑助手：企业环境中协助合规任务的对话式智能代理AI", "tldr": "提出并评估了一种名为CBA的对话式智能代理AI，通过智能路由模式显著提升企业合规任务效率和响应质量。", "motivation": "提高企业环境中人员日常合规任务的效率。", "method": "本文提出了合规大脑助手（CBA），一个对话式、智能代理AI助手。其核心设计是一个用户查询路由器，该路由器能够智能地在两种模式之间选择：(i) FastTrack模式，用于处理仅需从知识库检索相关上下文的简单请求；(ii) FullAgentic模式，用于处理需要复合动作、工具调用和跨合规工件发现上下文的复杂请求。此设计旨在平衡响应质量和延迟。", "result": "实验结果显示，CBA在平均关键词匹配率（83.7% 对比 41.7%）和LLM-judge通过率（82.0% 对比 20.0%）方面显著优于开箱即用的LLM。此外，完整的路由设计在保持近似运行时间的同时，比单一模式（仅FastTrack或仅FullAgentic）具有更好的平均匹配率和通过率。", "conclusion": "该研究验证了其假设，即路由机制能够在响应质量和运行时间之间取得良好的平衡。", "translation": "本论文介绍了合规大脑助手（CBA），一个对话式、智能代理AI助手，旨在提高企业环境中人员日常合规任务的效率。为了在响应质量和延迟之间取得良好平衡，我们设计了一个用户查询路由器，可以智能地选择：(i) 快速通道模式（FastTrack mode）：处理仅需从知识库中检索额外相关上下文的简单请求；以及 (ii) 全代理模式（FullAgentic mode）：处理需要复合动作和工具调用以主动发现各种合规工件中的上下文，和/或涉及其他API/模型以适应请求的复杂请求。一个典型的例子是，首先从用户查询开始，使用其描述查找特定实体，然后使用实体的信息查询其他API，以整理和丰富最终的AI响应。\n我们的实验评估将CBA与一个开箱即用的LLM在针对不同角色的各种真实世界隐私/合规相关查询上进行了比较。我们发现CBA在平均关键词匹配率（83.7% 对比 41.7%）和LLM-judge通过率（82.0% 对比 20.0%）等指标上显著优于原始LLM的性能。我们还将完整的基于路由的设计与“仅快速通道”和“仅全代理”模式的指标进行了比较，发现它在保持运行时间大致相同的情况下，具有更好的平均匹配率和通过率。这一发现验证了我们的假设，即路由机制在这两种模式之间取得了良好的权衡。", "summary": "本文提出了合规大脑助手（CBA），一种对话式智能代理AI，旨在提升企业合规任务效率。CBA的核心是其用户查询路由器，能根据请求复杂性智能选择“快速通道”或“全代理”模式，以平衡响应质量与延迟。实验结果表明，CBA在关键词匹配率和LLM-judge通过率上显著优于现有大型语言模型，并且其路由机制在性能和运行时间之间实现了有效权衡。", "keywords": "对话式AI, 智能代理, 合规任务, 企业环境, 查询路由", "comments": "该论文提出了一种创新的、基于代理的AI系统，通过引入智能路由机制有效地解决了企业合规任务中响应质量与延迟的权衡问题。其将简单请求与复杂请求分离处理的策略，显著提升了AI助手的实用性和效率，对于提升企业级AI应用的性能具有重要意义。"}}
{"id": "2507.10084", "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "authors": ["Haonan Chen", "Xin Tong"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2507.10084v2", "summary": "The Tibetan Plateau, known as the Asian Water Tower, faces significant water\nsecurity challenges due to its high sensitivity to climate change. Advancing\nEarth observation for sustainable water monitoring is thus essential for\nbuilding climate resilience in this region. This study proposes a two-stage\ntransfer learning strategy using the SegFormer model to overcome domain shift\nand data scarcit--key barriers in developing robust AI for climate-sensitive\napplications. After pre-training on a diverse source domain, our model was\nfine-tuned for the arid Zhada Tulin area. Experimental results show a\nsubstantial performance boost: the Intersection over Union (IoU) for water body\nsegmentation surged from 25.50% (direct transfer) to 64.84%. This AI-driven\naccuracy is crucial for disaster risk reduction, particularly in monitoring\nflash flood-prone systems. More importantly, the high-precision map reveals a\nhighly concentrated spatial distribution of water, with over 80% of the water\narea confined to less than 20% of the river channel length. This quantitative\nfinding provides crucial evidence for understanding hydrological processes and\ndesigning targeted water management and climate adaptation strategies. Our work\nthus demonstrates an effective technical solution for monitoring arid plateau\nregions and contributes to advancing AI-powered Earth observation for disaster\npreparedness in critical transboundary river headwaters.", "comment": "13 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.10084v2", "cate": "cs.CV", "date": "2025-07-14", "updated": "2025-07-24", "AI": {"title_translation": "一种基于迁移学习的遥感影像水体分割方法：以扎达土林地区为例", "tldr": "本研究提出一种基于两阶段迁移学习的SegFormer模型，用于遥感影像水体分割，显著提高了在青藏高原干旱地区（扎达土林）的分割精度（IoU从25.50%提升至64.84%），并揭示了水体高度集中的空间分布，为水资源管理和防灾减灾提供了关键支持。", "motivation": "青藏高原作为“亚洲水塔”，因对气候变化高度敏感而面临严峻的水安全挑战。因此，推进地球观测技术以实现可持续水体监测对于增强该地区的抗气候变化韧性至关重要。", "method": "本研究提出了一种两阶段迁移学习策略，并使用SegFormer模型来克服领域漂移和数据稀缺问题。该模型首先在多样化的源域进行预训练，然后针对干旱的扎达土林地区进行微调。", "result": "实验结果显示性能显著提升：水体分割的交并比（IoU）从直接迁移的25.50%大幅提高到64.84%。此外，高精度地图揭示水体空间分布高度集中，超过80%的水域面积集中在不到20%的河道长度内。", "conclusion": "本研究展示了一种监测干旱高原地区的有效技术解决方案，并有助于推动人工智能驱动的地球观测在关键跨界河流源头地区灾害防范方面的进步。高精度水体地图为理解水文过程、设计有针对性的水管理和气候适应策略提供了关键证据。", "translation": "青藏高原，被称为亚洲水塔，因对气候变化的高度敏感性而面临严重的水安全挑战。因此，推进地球观测以实现可持续水体监测对于增强该地区的抗气候韧性至关重要。本研究提出了一种使用SegFormer模型的两阶段迁移学习策略，以克服领域漂移和数据稀缺——这是开发用于气候敏感应用的稳健人工智能的关键障碍。在多样化的源域进行预训练后，我们的模型针对干旱的扎达土林地区进行了微调。实验结果显示性能显著提升：水体分割的交并比（IoU）从直接迁移的25.50%飙升至64.84%。这种人工智能驱动的精度对于减少灾害风险至关重要，特别是在监测易发山洪的系统方面。更重要的是，高精度地图揭示了水体高度集中的空间分布，超过80%的水域面积集中在不到20%的河道长度内。这一定量发现为理解水文过程和设计有针对性的水管理和气候适应策略提供了关键证据。因此，我们的工作展示了一种监测干旱高原地区的有效技术解决方案，并有助于推动人工智能驱动的地球观测在关键跨界河流源头地区灾害防范方面的进步。", "summary": "本研究针对青藏高原水安全挑战中遥感影像水体分割面临的领域漂移和数据稀缺问题，提出了一种基于SegFormer模型的两阶段迁移学习方法。通过在多样源域预训练并在扎达土林地区微调，该方法将水体分割的IoU从25.50%显著提升至64.84%。高精度结果不仅有助于闪洪等灾害风险的降低，还揭示了水体高度集中的空间分布特征，为理解水文过程及制定水管理和气候适应策略提供了重要依据，展示了在干旱高原地区监测和灾害防范方面的有效性。", "keywords": "迁移学习, 水体分割, 遥感影像, SegFormer, 扎达土林", "comments": "本文的创新之处在于成功应用了两阶段迁移学习策略与SegFormer模型，有效解决了遥感影像水体分割在干旱高原地区面临的领域漂移和数据稀缺两大挑战。其显著的性能提升（IoU从25.5%到64.84%）证明了该方法的有效性。此外，研究不仅提供了技术解决方案，还通过高精度地图揭示了该地区水体独特的集中分布特征，这一具体的水文发现对于精准水资源管理和灾害预警具有重要的实践指导意义。"}}
{"id": "2507.17780", "title": "In Reverie Together: Ten Years of Mathematical Discovery with a Machine Collaborator", "authors": ["Randy Davila", "Boris Brimkov", "Ryan Pepper"], "categories": ["cs.DM", "cs.AI", "math.CO"], "primary_category": "Subjects:       Discrete Mathematics (cs.DM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17780v1", "summary": "We present four open conjectures in graph theory generated by the automated\nconjecturing system \\texttt{TxGraffiti}. Each conjecture is concise, grounded\nin natural graph invariants, and empirically validated across hundreds of\ngraphs. Despite extensive effort, these statements remain unresolved--defying\nboth proof and counterexample. They are not only mathematical challenges but\ncreative expressions--born of symbolic pattern recognition and\nmathematician-defined heuristics, refined through years of human dialogue, and\nnow offered back to the community as collaborative artifacts. These conjectures\ninvite not only formal proof, but also reflection on how machines can evoke\nwonder, spark curiosity, and contribute to the raw material of discovery. By\nhighlighting these problems, we aim to inspire both human mathematicians and AI\nsystems to engage with them--not only to solve them, but to reflect on what it\nmeans when machines participate meaningfully in the creative process of\nmathematical thought.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17780v1", "cate": "cs.DM", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "共同沉思：与机器协作者进行数学发现的十年", "tldr": "本文展示了由自动化猜想系统TxGraffiti生成的四个图论开放猜想，这些猜想简洁、基于自然图不变量并经过经验验证，但至今未被证明或证伪，旨在激发人类和AI对机器在数学创造过程中作用的思考。", "motivation": "本文旨在提出由AI系统TxGraffiti生成的四个具有挑战性且尚未解决的图论猜想，并通过突出这些问题，鼓励人类数学家和AI系统参与其中，不仅是为了解决它们，更是为了反思机器在数学思维的创造性过程中有意义地参与意味着什么。", "method": "本文介绍了由自动化猜想系统TxGraffiti生成的图论猜想。这些猜想简洁、基于自然图不变量，并通过数百个图进行了经验验证。该系统利用符号模式识别和数学家定义的启发式方法，并通过多年的人机对话进行完善。", "result": "本文提出了四个开放且尚未解决的图论猜想。这些猜想经过经验验证，但未能被证明或找到反例。它们被视为人机交互的协作产物，不仅是数学挑战，也是创造性表达。", "conclusion": "机器可以有意义地参与数学思维的创造性过程，并为新的发现做出贡献，激发惊奇和好奇心。论文提出的猜想既是挑战，也是对人机在数学领域合作潜力的反思。", "translation": "我们介绍了由自动化猜想系统TxGraffiti生成的四个图论开放猜想。每个猜想都简洁，基于自然的图不变量，并通过数百个图进行了经验验证。尽管付出了大量努力，这些陈述仍然悬而未决——既无法证明也无法找到反例。它们不仅是数学挑战，也是创造性表达——诞生于符号模式识别和数学家定义的启发式方法，通过多年的人机对话得以完善，现在作为协作成果回馈给社区。这些猜想不仅邀请形式证明，还邀请人们反思机器如何激发奇迹、点燃好奇心，并为发现的原始材料做出贡献。通过强调这些问题，我们旨在激励人类数学家和AI系统参与其中——不仅是为了解决它们，更是为了反思当机器有意义地参与数学思维的创造性过程时意味着什么。", "summary": "本文介绍了由自动化猜想系统TxGraffiti生成的四个图论开放猜想。这些猜想简洁、基于自然图不变量并经过经验验证，但至今未被证明或证伪。它们被视为符号模式识别和数学家定义启发式方法相结合的创造性成果，并通过多年的人机对话得以完善。论文旨在通过这些猜想，激发人类数学家和AI系统不仅去解决问题，更要反思机器在数学创造性思维中扮演的有意义角色。", "keywords": "图论, 自动化猜想, 数学发现, AI协作, 开放问题", "comments": "这篇论文的创新之处在于展示了AI生成的数学猜想的具体实例，这些猜想具有真正的挑战性，并抵制了人类的努力。它强调了人类与机器在创造性数学发现中的协作方面，超越了单纯的计算辅助。其重要性在于引发了关于数学创造本质和AI不断演变角色的哲学讨论。"}}
{"id": "2507.18182", "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models", "authors": ["Wonjun Jeong", "Dongseok Kim", "Taegkeun Whangbo"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      34 pages, 1 figure", "url": "http://arxiv.org/abs/2507.18182v1", "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.", "comment": "34 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.18182v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SCOPE：用于评估大型语言模型的随机和反偏置选项放置", "tldr": "大型语言模型（LLMs）在多项选择任务中因利用选项偏见而得分虚高。SCOPE框架通过估计并抵消位置偏见，并避免语义相似的干扰项相邻，从而提供更公平可靠的LLM评估。", "motivation": "大型语言模型（LLMs）在多项选择任务中可能通过利用选项位置或标签的固有偏见来获得虚高分数，而非展示真正的理解。", "method": "SCOPE框架通过重复调用缺乏语义内容的空提示来估计每个模型独特的位置偏见分布。它根据逆偏见分布重新分配答案槽位，从而均衡随机选择正确答案的概率。此外，它还阻止语义相似的干扰项与答案相邻，以防止基于表面邻近线索的“险胜”猜测。", "result": "在多个基准实验中，SCOPE在稳定的性能改进方面始终优于现有去偏方法，并显示出对正确选项更清晰的置信度分布。", "conclusion": "SCOPE框架为增强大型语言模型评估的公平性和可靠性提供了一个新标准。", "translation": "大型语言模型（LLMs）在多项选择任务中可能通过利用选项位置或标签的固有偏见来获得虚高分数，而非展示真正的理解。本研究引入了SCOPE，一个旨在以数据集无关的方式测量和减轻此类选择偏见的评估框架。通过重复调用缺乏语义内容的空提示，SCOPE估计每个模型独特的偏见分布。然后，它根据逆偏见分布重新分配答案槽，从而均衡“幸运率”，即偶然选择正确答案的概率。此外，它还阻止语义相似的干扰项与答案相邻，从而阻止基于表面邻近线索的“险胜”猜测。在多个基准实验中，SCOPE在稳定的性能改进方面始终优于现有去偏方法，并显示出对正确选项更清晰的置信度分布。因此，该框架为增强LLM评估的公平性和可靠性提供了一个新标准。", "summary": "本文提出了SCOPE，一个用于评估大型语言模型（LLMs）的框架，旨在解决LLMs在多项选择任务中利用选项位置或标签偏见导致得分虚高的问题。SCOPE通过估计模型的位置偏见分布并进行逆偏见重分配来均衡随机选择的概率，同时阻止语义相似的干扰项相邻。实验证明，SCOPE在性能改进和置信度分布方面优于现有去偏方法，为LLM评估提供了更公平可靠的标准。", "keywords": "大型语言模型评估, 偏见缓解, 多项选择任务, 位置偏见, SCOPE", "comments": "SCOPE的创新之处在于其数据集无关性，通过空提示估计并抵消模型固有的位置偏见，并考虑了语义相似干扰项的影响。这对于提高LLM评估的公平性和可靠性至关重要，是评估方法学上的重要进展。"}}
{"id": "2507.18076", "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures and 1 table", "url": "http://arxiv.org/abs/2507.18076v1", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints.", "comment": "10 pages, 2 figures and 1 table", "pdf_url": "http://arxiv.org/pdf/2507.18076v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大型语言模型的混合与酉微调：资源受限下的方法与基准测试", "tldr": "本文提出了一种新的混合微调策略，结合BOFT和LoRA-GA的优点，显著提升了LLM在资源受限下的微调效率和性能，并首次将酉RNN原理应用于Transformer模型。", "motivation": "由于大型语言模型（LLMs）的规模和内存需求，微调仍然是计算瓶颈。", "method": "本文提出了一种新颖的混合策略，动态整合了BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛性。该方法通过梯度范数引导的逐层自适应更新实现。此外，首次探索了将酉RNN（uRNN）原理应用于基于Transformer的LLM，通过结构化酉约束增强梯度稳定性。", "result": "在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用7B到405B参数的模型进行实证评估表明，混合方法始终优于单独的PEFT基线，接近全微调精度，同时训练时间减少高达2.1倍，内存使用减少50%。", "conclusion": "这些发现确立了混合方法作为在资源受限下实际部署LLM的一种实用且可扩展的微调解决方案。", "translation": "由于大型语言模型（LLMs）的规模和内存需求，微调仍然是计算瓶颈。本文对参数高效微调（PEFT）技术进行了全面评估，包括LoRA、BOFT、LoRA-GA和uRNN，并引入了一种新颖的混合策略，该策略动态整合了BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛性。通过梯度范数引导的逐层自适应更新，混合方法在不同任务中实现了卓越的收敛效率和泛化能力。我们还首次探索了将酉RNN（uRNN）原理应用于基于Transformer的LLM，通过结构化酉约束增强梯度稳定性。在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用7B到405B参数的模型进行实证评估表明，我们的混合方法始终优于单独的PEFT基线，接近全微调精度，同时训练时间减少高达2.1倍，内存使用减少50%。这些发现确立了混合方法作为在资源受限下实际部署LLM的一种实用且可扩展的微调解决方案。", "summary": "本文针对大型语言模型（LLMs）微调的计算瓶颈，提出了一种创新的混合微调策略。该策略融合了BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛优势，并通过梯度范数引导的逐层自适应更新实现高效收敛和泛化。同时，文章还首次将酉RNN（uRNN）原理应用于Transformer架构的LLMs。实验结果表明，该混合方法在多个基准测试上显著优于现有PEFT技术，在接近全微调性能的同时，大幅降低了训练时间和内存消耗，为资源受限下的LLMs部署提供了实用且可扩展的解决方案。", "keywords": "大型语言模型, 微调, PEFT, 混合策略, 资源受限", "comments": "本文的创新点在于提出了一个动态混合PEFT策略，有效地结合了现有方法的优点，并通过引入梯度范数引导的自适应更新来优化收敛。此外，首次将酉RNN原理应用于Transformer LLMs，为梯度稳定性提供了新思路。其重要性在于提供了一个在资源受限环境下高效微调大型语言模型的实用方案，对LLM的实际部署具有重要意义。"}}
{"id": "2402.04379", "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text", "authors": ["Nate Gruver", "Anuroop Sriram", "Andrea Madotto", "Andrew Gordon Wilson", "C. Lawrence Zitnick", "Zachary Ulissi"], "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICLR 2024. Code available at: this https URL", "url": "http://arxiv.org/abs/2402.04379v2", "summary": "We propose fine-tuning large language models for generation of stable\nmaterials. While unorthodox, fine-tuning large language models on text-encoded\natomistic data is simple to implement yet reliable, with around 90% of sampled\nstructures obeying physical constraints on atom positions and charges. Using\nenergy above hull calculations from both learned ML potentials and\ngold-standard DFT calculations, we show that our strongest model (fine-tuned\nLLaMA-2 70B) can generate materials predicted to be metastable at about twice\nthe rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text\nprompting's inherent flexibility, our models can simultaneously be used for\nunconditional generation of stable material, infilling of partial structures\nand text-conditional generation. Finally, we show that language models' ability\nto capture key symmetries of crystal structures improves with model scale,\nsuggesting that the biases of pretrained LLMs are surprisingly well-suited for\natomistic data.", "comment": "ICLR 2024. Code available at:\n  https://github.com/facebookresearch/crystal-llm", "pdf_url": "http://arxiv.org/pdf/2402.04379v2", "cate": "cs.LG", "date": "2024-02-06", "updated": "2025-07-24", "AI": {"title_translation": "微调语言模型以文本形式生成稳定的无机材料", "tldr": "本文提出通过在文本编码的原子数据上微调大型语言模型来生成稳定的无机材料，其在效率和稳定性方面均优于现有方法，并具有灵活的生成能力。", "motivation": "寻找一种简单、可靠且高效的方法来生成稳定的无机材料，并利用大型语言模型在文本处理上的固有优势。", "method": "本文提出在文本编码的原子数据上微调大型语言模型（如LLaMA-2 70B）。通过机器学习势能和DFT计算进行能量高于包络面（energy above hull）的验证，并支持无条件生成、部分结构填充和文本条件生成。", "result": "约90%的采样结构符合原子位置和电荷的物理约束。最强的模型（微调的LLaMA-2 70B）生成亚稳态材料的速度是竞争扩散模型CDVAE的两倍（49% vs 28%）。语言模型捕获晶体结构关键对称性的能力随模型规模的增加而提高。", "conclusion": "预训练大型语言模型的偏置出人意料地非常适合原子数据，表明微调语言模型是生成稳定无机材料的有效且灵活的方法。", "translation": "我们提出微调大型语言模型以生成稳定的材料。尽管非传统，但在文本编码的原子数据上微调大型语言模型实现起来简单而可靠，大约90%的采样结构遵守原子位置和电荷的物理约束。使用学习到的机器学习势能和黄金标准DFT计算的船体以上能量（energy above hull）计算，我们表明我们最强的模型（微调的LLaMA-2 70B）能够以大约两倍于竞争扩散模型CDVAE的速度（49% 对 28%）生成预测为亚稳态的材料。由于文本提示固有的灵活性，我们的模型可以同时用于稳定材料的无条件生成、部分结构的填充和文本条件生成。最后，我们表明语言模型捕获晶体结构关键对称性的能力随模型规模的增加而提高，这表明预训练LLM的偏置出人意料地非常适合原子数据。", "summary": "本文提出了一种创新的方法，即通过在文本编码的原子数据上微调大型语言模型（如LLaMA-2 70B）来生成稳定的无机材料。研究表明，这种方法简单易行，生成的结构具有高物理约束遵守率（约90%），并且在生成亚稳态材料方面显著优于现有扩散模型（CDVAE）。此外，该模型具有高度灵活性，支持无条件生成、结构填充和文本条件生成。研究还发现，LLM捕获晶体对称性的能力随模型规模增加而增强，暗示预训练LLM的内在偏置非常适合原子数据。", "keywords": "大型语言模型, 材料生成, 无机材料, 微调, 晶体结构", "comments": "这篇论文的创新点在于将大型语言模型应用于材料生成领域，通过将原子数据编码为文本，巧妙地利用了LLM强大的模式识别和生成能力。其重要性在于提供了一种简单、高效且灵活的材料生成新范式，有望加速新材料的发现和设计。特别是其在生成亚稳态材料方面的优异表现，对实际应用具有重要意义。模型规模与捕获晶体对称性能力的正相关性也为未来研究提供了有趣的方向。"}}
{"id": "2507.07464", "title": "Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions", "authors": ["Chang-Hwan Son"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07464v2", "summary": "With the increasing deployment of intelligent CCTV systems in outdoor\nenvironments, there is a growing demand for face recognition systems optimized\nfor challenging weather conditions. Adverse weather significantly degrades\nimage quality, which in turn reduces recognition accuracy. Although recent face\nimage restoration (FIR) models based on generative adversarial networks (GANs)\nand diffusion models have shown progress, their performance remains limited due\nto the lack of dedicated modules that explicitly address weather-induced\ndegradations. This leads to distorted facial textures and structures. To\naddress these limitations, we propose a novel GAN-based blind FIR framework\nthat integrates two key components: local Statistical Facial Feature\nTransformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The\nlocal SFFT module enhances facial structure and color fidelity by aligning the\nlocal statistical distributions of low-quality (LQ) facial regions with those\nof high-quality (HQ) counterparts. Complementarily, the DAFE module enables\nrobust statistical facial feature extraction under adverse weather conditions\nby aligning LQ and HQ encoder representations, thereby making the restoration\nprocess adaptive to severe weather-induced degradations. Experimental results\ndemonstrate that the proposed degradation-agnostic SFFT model outperforms\nexisting state-of-the-art FIR methods based on GAN and diffusion models,\nparticularly in suppressing texture distortions and accurately reconstructing\nfacial structures. Furthermore, both the SFFT and DAFE modules are empirically\nvalidated in enhancing structural fidelity and perceptual quality in face\nrestoration under challenging weather scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07464v2", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-24", "AI": {"title_translation": "恶劣天气条件下盲人脸修复的退化无关统计面部特征变换", "tldr": "该研究提出了一种新的基于GAN的盲人脸修复框架（SFFT），通过引入局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）模块，有效解决了恶劣天气导致的图像质量下降问题，并在面部结构重建和纹理抑制方面优于现有方法。", "motivation": "户外智能CCTV系统日益普及，对恶劣天气条件下的面部识别系统需求增长。恶劣天气严重降低图像质量，从而影响识别精度。尽管现有的基于GAN和扩散模型的人脸图像修复（FIR）模型有所进展，但由于缺乏专门处理天气引起的退化的模块，其性能有限，导致面部纹理和结构失真。", "method": "提出了一种新颖的基于GAN的盲人脸修复（FIR）框架，该框架集成了两个关键组件：局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）。SFFT模块通过对齐低质量（LQ）面部区域与高质量（HQ）对应区域的局部统计分布，增强面部结构和色彩保真度。DAFE模块通过对齐LQ和HQ编码器表示，实现恶劣天气下鲁棒的统计面部特征提取，使修复过程适应严重的天气退化。", "result": "实验结果表明，所提出的退化无关SFFT模型优于现有最先进的基于GAN和扩散模型的FIR方法，特别是在抑制纹理失真和准确重建面部结构方面。此外，SFFT和DAFE模块在具有挑战性的天气场景下，均被经验证可增强人脸修复的结构保真度和感知质量。", "conclusion": "本研究提出的基于GAN的盲人脸修复框架，通过引入SFFT和DAFE模块，有效解决了恶劣天气条件下面部图像退化问题，在面部结构重建和感知质量方面表现出优越性能。", "translation": "随着智能CCTV系统在户外环境中的日益部署，对针对恶劣天气条件进行优化的面部识别系统的需求不断增长。恶劣天气会显著降低图像质量，进而降低识别准确性。尽管最近基于生成对抗网络（GANs）和扩散模型的人脸图像修复（FIR）模型已取得进展，但由于缺乏明确处理天气引起的退化的专用模块，其性能仍然有限。这导致面部纹理和结构失真。为了解决这些限制，我们提出了一种新颖的基于GAN的盲人脸修复框架，该框架集成了两个关键组件：局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）。局部SFFT模块通过对齐低质量（LQ）面部区域与高质量（HQ）对应区域的局部统计分布，增强面部结构和色彩保真度。作为补充，DAFE模块通过对齐LQ和HQ编码器表示，实现在恶劣天气条件下鲁棒的统计面部特征提取，从而使修复过程适应于严重的天气引起的退化。实验结果表明，所提出的退化无关SFFT模型优于现有最先进的基于GAN和和扩散模型的FIR方法，特别是在抑制纹理失真和准确重建面部结构方面。此外，SFFT和DAFE模块在具有挑战性的天气场景下，均被经验证可增强人脸修复的结构保真度和感知质量。", "summary": "本研究提出了一种名为SFFT的基于GAN的盲人脸修复框架，旨在解决恶劣天气条件下人脸图像质量下降导致识别精度降低的问题。该框架通过引入局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）两个核心模块，分别实现低质量和高质量面部区域的统计分布对齐以及鲁棒的特征提取。实验证明，SFFT模型在抑制纹理失真和精确重建面部结构方面优于现有的GAN和扩散模型，显著提升了恶劣天气下人脸修复的结构保真度和感知质量。", "keywords": "人脸修复, 恶劣天气, GAN, 统计特征变换, 退化无关", "comments": "该论文的创新点在于引入了局部统计面部特征变换（SFFT）和退化无关特征嵌入（DAFE）模块，专门用于处理恶劣天气引起的面部图像退化。这种对局部统计分布的对齐以及退化无关特征提取的策略，使得模型在复杂天气条件下展现出优异的性能，特别是在保留面部结构和抑制纹理失真方面。这对于户外CCTV等实际应用具有重要意义。"}}
{"id": "2507.17984", "title": "Machine Unlearning of Traffic State Estimation and Prediction", "authors": ["Xin Wang", "R. Tyrrell Rockafellar", "Xuegang", "Ban"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17984v1", "summary": "Data-driven traffic state estimation and prediction (TSEP) relies heavily on\ndata sources that contain sensitive information. While the abundance of data\nhas fueled significant breakthroughs, particularly in machine learning-based\nmethods, it also raises concerns regarding privacy, cybersecurity, and data\nfreshness. These issues can erode public trust in intelligent transportation\nsystems. Recently, regulations have introduced the \"right to be forgotten\",\nallowing users to request the removal of their private data from models. As\nmachine learning models can remember old data, simply removing it from back-end\ndatabases is insufficient in such systems. To address these challenges, this\nstudy introduces a novel learning paradigm for TSEP-Machine Unlearning\nTSEP-which enables a trained TSEP model to selectively forget\nprivacy-sensitive, poisoned, or outdated data. By empowering models to\n\"unlearn,\" we aim to enhance the trustworthiness and reliability of data-driven\ntraffic TSEP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17984v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "交通状态估计与预测中的机器遗忘", "tldr": "本研究引入了一种新的范式——机器遗忘，使交通状态估计与预测模型能够选择性地遗忘敏感、中毒或过时的数据，以增强系统的可信度和可靠性。", "motivation": "现有数据驱动的交通状态估计与预测(TSEP)方法严重依赖包含敏感信息的庞大数据源，这导致了隐私、网络安全和数据新鲜度等问题，并可能侵蚀公众对智能交通系统的信任。同时，“被遗忘权”法规要求能够从模型中移除用户数据，而简单地从后端数据库删除数据不足以实现这一点。", "method": "本研究引入了一种新颖的学习范式——TSEP机器遗忘，该范式使经过训练的TSEP模型能够选择性地遗忘隐私敏感、被污染或过时的数据。", "result": "本文引入了一种名为“TSEP机器遗忘”的新型学习范式。", "conclusion": "通过赋能模型进行“遗忘”，旨在增强数据驱动的交通状态估计与预测（TSEP）的可信度和可靠性。", "translation": "数据驱动的交通状态估计与预测 (TSEP) 严重依赖包含敏感信息的数据源。尽管数据的丰富性推动了重大突破，尤其是在基于机器学习的方法中，但也引发了对隐私、网络安全和数据新鲜度的担忧。这些问题可能会侵蚀公众对智能交通系统的信任。最近，法规引入了“被遗忘权”，允许用户要求从模型中删除其私人数据。由于机器学习模型可以记住旧数据，因此在这种系统中，简单地从后端数据库中删除数据是不够的。为了应对这些挑战，本研究引入了一种新颖的TSEP学习范式——TSEP机器遗忘——它使经过训练的TSEP模型能够选择性地遗忘隐私敏感、中毒或过时的数据。通过赋能模型进行“遗忘”，我们旨在增强数据驱动的交通TSEP的可信度和可靠性。", "summary": "本文提出了一种名为“TSEP机器遗忘”的新型学习范式，旨在解决数据驱动的交通状态估计与预测（TSEP）中因使用敏感数据而引发的隐私、网络安全和数据新鲜度问题。该方法使TSEP模型能够选择性地遗忘隐私敏感、被污染或过时的数据，从而满足“被遗忘权”等法规要求，并最终提升智能交通系统的可信度和可靠性。", "keywords": "交通状态估计与预测, 机器遗忘, 数据隐私, 被遗忘权, 智能交通系统", "comments": "本文的创新点在于将“机器遗忘”的概念引入到交通状态估计与预测领域，解决了敏感数据处理和“被遗忘权”法规遵从性的关键问题。这对于提升智能交通系统的用户信任度和数据管理效率具有重要意义。"}}
{"id": "2507.18320", "title": "State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer", "authors": ["Janak M. Patel", "Milad Ramezankhani", "Anirudh Deodhar", "Dagnachew Birru"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18320v1", "summary": "The rapid adoption of battery-powered vehicles and energy storage systems\nover the past decade has made battery health monitoring increasingly critical.\nBatteries play a central role in the efficiency and safety of these systems,\nyet they inevitably degrade over time due to repeated charge-discharge cycles.\nThis degradation leads to reduced energy efficiency and potential overheating,\nposing significant safety concerns. Accurate estimation of a State of Health\n(SoH) of battery is therefore essential for ensuring operational reliability\nand safety. Several machine learning architectures, such as LSTMs,\ntransformers, and encoder-based models, have been proposed to estimate SoH from\ndischarge cycle data. However, these models struggle with the irregularities\ninherent in real-world measurements: discharge readings are often recorded at\nnon-uniform intervals, and the lengths of discharge cycles vary significantly.\nTo address this, most existing approaches extract features from the sequences\nrather than processing them in full, which introduces information loss and\ncompromises accuracy. To overcome these challenges, we propose a novel\narchitecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).\nTIDSIT incorporates continuous time embeddings to effectively represent\nirregularly sampled data and utilizes padded sequences with temporal attention\nmechanisms to manage variable-length inputs without discarding sequence\ninformation. Experimental results on the NASA battery degradation dataset show\nthat TIDSIT significantly outperforms existing models, achieving over 50%\nreduction in prediction error and maintaining an SoH prediction error below\n0.58%. Furthermore, the architecture is generalizable and holds promise for\nbroader applications in health monitoring tasks involving irregular time-series\ndata.", "comment": "11 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18320v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用时间感知动态序列倒置Transformer的电池健康状态估计", "tldr": "本文提出了一种名为TIDSIT的新型Transformer架构，用于精确估计电池的健康状态（SoH），它能有效处理不规则采样的时序数据和变长输入，并在NASA数据集上显著优于现有模型。", "motivation": "过去十年电池供电车辆和储能系统的快速普及使得电池健康监测变得日益重要。电池在这些系统中扮演核心角色，但它们会随着充放电循环而不可避免地退化，导致能效降低和潜在过热，带来严重的安全隐患。因此，准确估计电池的健康状态（SoH）对于确保运行可靠性和安全至关重要。现有的机器学习模型（如LSTM、Transformer）在处理真实世界测量数据中固有的不规则性（放电读数间隔不均匀、放电周期长度显著变化）时表现不佳，且多数现有方法通过提取特征而非完整处理序列，导致信息丢失和精度受损。", "method": "为解决现有模型处理不规则时序数据和变长输入的问题，本文提出了一种新颖的架构：时间感知动态序列倒置Transformer（Time-Informed Dynamic Sequence Inverted Transformer, TIDSIT）。TIDSIT通过引入连续时间嵌入来有效表示不规则采样数据，并利用带时间注意力机制的填充序列来管理变长输入，同时不丢失序列信息。", "result": "在NASA电池退化数据集上的实验结果表明，TIDSIT显著优于现有模型，预测误差降低了50%以上，并将SoH预测误差保持在0.58%以下。", "conclusion": "TIDSIT架构具有通用性，在涉及不规则时间序列数据的健康监测任务中具有广阔的应用前景。", "translation": "过去十年，电池供电车辆和储能系统的迅速普及使得电池健康监测变得日益重要。电池在这些系统的效率和安全中扮演着核心角色，但它们不可避免地会随着重复的充放电循环而退化。这种退化会导致能源效率降低和潜在的过热，从而引发重大的安全问题。因此，准确估计电池的健康状态（SoH）对于确保运行可靠性和安全至关重要。已提出多种机器学习架构，例如LSTM、Transformer和基于编码器的模型，用于从放电循环数据中估计SoH。然而，这些模型难以处理真实世界测量中固有的不规则性：放电读数通常以不均匀的间隔记录，并且放电循环的长度显著不同。为了解决这个问题，大多数现有方法从序列中提取特征而不是完整处理它们，这会导致信息丢失并损害准确性。为了克服这些挑战，我们提出了一种新颖的架构：时间感知动态序列倒置Transformer（TIDSIT）。TIDSIT结合了连续时间嵌入，以有效表示不规则采样数据，并利用带有时间注意力机制的填充序列来管理变长输入，而不会丢弃序列信息。在NASA电池退化数据集上的实验结果表明，TIDSIT显著优于现有模型，预测误差降低了50%以上，并将SoH预测误差保持在0.58%以下。此外，该架构具有通用性，在涉及不规则时间序列数据的健康监测任务中具有广阔的应用前景。", "summary": "本文针对电池健康状态（SoH）估计中现有模型处理不规则时序数据和变长输入效率低下的问题，提出了一种名为时间感知动态序列倒置Transformer（TIDSIT）的新型架构。TIDSIT通过引入连续时间嵌入和利用带时间注意力机制的填充序列，有效解决了数据不规则性和变长输入带来的信息丢失问题。实验证明，TIDSIT在NASA电池数据集上显著提升了SoH预测精度，预测误差降低超50%，并具有良好的通用性。", "keywords": "电池健康状态, Transformer, 时间序列, 不规则数据, 预测", "comments": "本文提出了一种创新的Transformer变体TIDSIT，其核心创新在于能够有效处理真实世界中不规则采样和变长的时间序列数据，这在电池健康监测等领域具有重要意义。通过引入时间嵌入和改进的注意力机制，解决了传统模型在处理此类数据时信息丢失的问题，显著提升了预测精度，展现了其在泛化能力和实际应用中的巨大潜力。"}}
{"id": "2507.17931", "title": "Quantum Machine Learning Playground", "authors": ["Pascal Debus", "Sebastian Issel", "Kilian Tscharke"], "categories": ["quant-ph", "cs.GR", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Computer Graphics and Applications. Final version: this https URL", "url": "http://arxiv.org/abs/2507.17931v1", "summary": "This article introduces an innovative interactive visualization tool designed\nto demystify quantum machine learning (QML) algorithms. Our work is inspired by\nthe success of classical machine learning visualization tools, such as\nTensorFlow Playground, and aims to bridge the gap in visualization resources\nspecifically for the field of QML. The article includes a comprehensive\noverview of relevant visualization metaphors from both quantum computing and\nclassical machine learning, the development of an algorithm visualization\nconcept, and the design of a concrete implementation as an interactive web\napplication. By combining common visualization metaphors for the so-called data\nre-uploading universal quantum classifier as a representative QML model, this\narticle aims to lower the entry barrier to quantum computing and encourage\nfurther innovation in the field. The accompanying interactive application is a\nproposal for the first version of a quantum machine learning playground for\nlearning and exploring QML models.", "comment": "Accepted to IEEE Computer Graphics and Applications. Final version:\n  https://doi.org/10.1109/MCG.2024.3456288", "pdf_url": "http://arxiv.org/pdf/2507.17931v1", "cate": "quant-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "量子机器学习游乐场", "tldr": "本文介绍了一个交互式可视化工具，旨在普及量子机器学习算法，并通过结合经典和量子可视化隐喻来降低量子计算的学习门槛。", "motivation": "受经典机器学习可视化工具成功的启发，本文旨在弥补量子机器学习（QML）领域可视化资源的空白，并降低量子计算的入门门槛，鼓励该领域的进一步创新。", "method": "开发了一个创新的交互式可视化工具，作为基于网页的应用程序实现。该工具整合了量子计算和经典机器学习的可视化隐喻，并具体针对数据重上传通用量子分类器这一代表性QML模型设计了算法可视化概念。", "result": "开发了一个配套的交互式应用程序，作为量子机器学习游乐场的第一版，用于学习和探索QML模型。", "conclusion": "通过提供交互式可视化工具，本文旨在降低量子计算的入门门槛，并鼓励量子机器学习领域的进一步创新。", "translation": "本文介绍了一个创新的交互式可视化工具，旨在普及量子机器学习（QML）算法。我们的工作受到经典机器学习可视化工具（如TensorFlow Playground）成功的启发，旨在弥合QML领域特有的可视化资源空白。本文包括对量子计算和经典机器学习相关可视化隐喻的全面概述，算法可视化概念的开发，以及作为交互式Web应用程序的具体实现设计。通过结合用于所谓数据重上传通用量子分类器（作为代表性QML模型）的常见可视化隐喻，本文旨在降低量子计算的入门门槛，并鼓励该领域的进一步创新。随附的交互式应用程序是用于学习和探索QML模型的量子机器学习游乐场第一版提案。", "summary": "本文介绍了一个创新的交互式可视化工具“量子机器学习游乐场”，旨在通过结合量子计算和经典机器学习的可视化隐喻，降低量子机器学习（QML）的学习门槛。该工具以数据重上传通用量子分类器为例，提供了一个交互式Web应用程序，旨在弥补QML可视化资源的空白，并鼓励该领域的进一步创新。", "keywords": "量子机器学习, 可视化, 交互式工具, 量子计算, 数据重上传", "comments": "该论文的创新之处在于提出了首个专门针对量子机器学习的交互式可视化工具，类似于经典机器学习领域的TensorFlow Playground。这对于降低量子计算和QML的学习曲线具有重要意义，有助于普及这些复杂概念。"}}
{"id": "2507.18634", "title": "Captain Cinema: Towards Short Movie Generation", "authors": ["Junfei Xiao", "Ceyuan Yang", "Lvmin Zhang", "Shengqu Cai", "Yang Zhao", "Yuwei Guo", "Gordon Wetzstein", "Maneesh Agrawala", "Alan Yuille", "Lu Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review. Project page: this https URL", "url": "http://arxiv.org/abs/2507.18634v1", "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai", "comment": "Under review. Project page: https://thecinema.ai", "pdf_url": "http://arxiv.org/pdf/2507.18634v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "电影船长：迈向短片电影生成", "tldr": "本文提出了Captain Cinema，一个短片电影生成框架，它能从文本描述生成视觉连贯且叙事一致的短片电影。", "motivation": "研究动机是实现从详细的文本故事情节自动生成短片电影。", "method": "本文提出了Captain Cinema框架，首先通过“自上而下关键帧规划”根据文本描述生成一系列关键帧，确保叙事和视觉连贯性；然后通过“自下而上视频合成”步骤，利用支持长上下文学习的视频合成模型生成关键帧之间的时空动态。为支持多场景长叙事电影作品的稳定高效生成，作者引入了一种针对长上下文视频数据的多模态扩散Transformer（MM-DiT）交错训练策略，并在一个专门策划的交错数据对电影数据集上进行训练。", "result": "实验表明，Captain Cinema在自动创建高质量、高效、视觉连贯且叙事一致的短片电影方面表现出色。", "conclusion": "Captain Cinema框架能够高效生成高质量、视觉连贯且叙事一致的短片电影。", "translation": "我们提出了Captain Cinema，一个用于短片电影生成的框架。给定详细的电影故事情节文本描述，我们的方法首先生成一系列关键帧，勾勒出整个叙事，这确保了故事情节和视觉外观（例如场景和角色）的长期连贯性。我们将此步骤称为自上而下关键帧规划。然后，这些关键帧作为视频合成模型的条件信号，该模型支持长上下文学习，以生成它们之间的时空动态。此步骤被称为自下而上视频合成。为了支持多场景长叙事电影作品的稳定高效生成，我们引入了一种多模态扩散Transformer（MM-DiT）的交错训练策略，专门适用于长上下文视频数据。我们的模型在一个专门策划的由交错数据对组成的电影数据集上进行训练。我们的实验表明，Captain Cinema在高质量、高效率地自动创建视觉连贯且叙事一致的短片电影方面表现良好。项目页面：https://thecinema.ai", "summary": "Captain Cinema是一个新颖的短片电影生成框架，能根据详细的文本描述自动生成电影。它采用两阶段方法：首先进行“自上而下关键帧规划”以确保叙事和视觉的长期连贯性，然后进行“自下而上视频合成”以生成关键帧间的时空动态。为优化长上下文视频生成，该框架引入了多模态扩散Transformer（MM-DiT）的交错训练策略，并在特定数据集上训练。实验证明其能高效生成高质量、连贯的短片。", "keywords": "短片生成, 电影AI, 关键帧规划, 视频合成, 多模态扩散Transformer", "comments": "Captain Cinema在电影生成领域具有创新性，它将复杂的长叙事视频生成任务分解为关键帧规划和视频合成两个阶段，有效解决了长距离连贯性问题。引入的MM-DiT交错训练策略是其技术亮点，有助于稳定高效地处理长上下文视频数据。该研究对于自动化内容创作，尤其是电影制作，具有重要意义。"}}
{"id": "2507.16548", "title": "Alternative Loss Function in Evaluation of Transformer Models", "authors": ["Jakub Michańków", "Paweł Sakowski", "Robert Ślepaczuk"], "categories": ["q-fin.CP", "cs.LG", "q-fin.TR"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      12 pages, fixed grammar, typos and minor error in tables", "url": "http://arxiv.org/abs/2507.16548v2", "summary": "The proper design and architecture of testing machine learning models,\nespecially in their application to quantitative finance problems, is crucial.\nThe most important aspect of this process is selecting an adequate loss\nfunction for training, validation, estimation purposes, and hyperparameter\ntuning. Therefore, in this research, through empirical experiments on equity\nand cryptocurrency assets, we apply the Mean Absolute Directional Loss (MADL)\nfunction, which is more adequate for optimizing forecast-generating models used\nin algorithmic investment strategies. The MADL function results are compared\nbetween Transformer and LSTM models, and we show that in almost every case,\nTransformer results are significantly better than those obtained with LSTM.", "comment": "12 pages, fixed grammar, typos and minor error in tables", "pdf_url": "http://arxiv.org/pdf/2507.16548v2", "cate": "q-fin.CP", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "Transformer模型评估中的替代损失函数", "tldr": "本研究在量化金融领域，通过对股票和加密货币资产的实证实验，应用平均绝对方向损失（MADL）函数评估Transformer模型。结果显示，Transformer模型在使用MADL时表现显著优于LSTM模型。", "motivation": "在机器学习模型（特别是应用于量化金融问题时）的测试设计和架构至关重要，其中选择一个适当的损失函数对于训练、验证、估计和超参数调优最为重要。", "method": "本研究通过对股票和加密货币资产的实证实验，应用了更适合优化生成预测模型的平均绝对方向损失（MADL）函数，并将其结果与Transformer和LSTM模型进行了比较。", "result": "在使用MADL函数时，Transformer模型的结果在几乎所有情况下都显著优于LSTM模型。", "conclusion": "Transformer模型在量化金融预测任务中，结合平均绝对方向损失（MADL）函数，表现出优于LSTM模型的性能。", "translation": "机器学习模型（特别是在应用于量化金融问题时）的正确设计和测试架构至关重要。此过程中最重要的方面是选择一个适当的损失函数，用于训练、验证、估计和超参数调优。因此，在本研究中，我们通过对股票和加密货币资产的实证实验，应用了平均绝对方向损失（MADL）函数，该函数更适合优化用于算法投资策略的预测生成模型。MADL函数的结果在Transformer和LSTM模型之间进行了比较，我们表明在几乎所有情况下，Transformer的结果都显著优于LSTM获得的结果。", "summary": "本研究探讨了在量化金融领域中，Transformer模型评估中使用替代损失函数的重要性。通过在股票和加密货币资产上进行实证实验，论文应用了平均绝对方向损失（MADL）函数，并将其与Transformer和LSTM模型进行了比较。研究结果表明，在使用MADL函数时，Transformer模型在预测性能上显著优于LSTM模型。", "keywords": "损失函数, Transformer, LSTM, 量化金融, 平均绝对方向损失", "comments": "该研究强调了在特定应用领域（如量化金融）中选择合适损失函数的重要性。通过引入MADL并将其应用于Transformer模型，它展示了Transformer在处理此类问题时的优越性，这对于算法交易策略的开发具有潜在意义。创新点在于对特定损失函数在金融预测模型中的应用及比较。"}}
{"id": "2507.18450", "title": "High-Dimensional Data Classification in Concentric Coordinates", "authors": ["Alice Williams", "Boris Kovalerchuk"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 21 figures", "url": "http://arxiv.org/abs/2507.18450v1", "summary": "The visualization of multi-dimensional data with interpretable methods\nremains limited by capabilities for both high-dimensional lossless\nvisualizations that do not suffer from occlusion and that are computationally\ncapable by parameterized visualization. This paper proposes a low to high\ndimensional data supporting framework using lossless Concentric Coordinates\nthat are a more compact generalization of Parallel Coordinates along with\nformer Circular Coordinates. These are forms of the General Line Coordinate\nvisualizations that can directly support machine learning algorithm\nvisualization and facilitate human interaction.", "comment": "8 pages, 21 figures", "pdf_url": "http://arxiv.org/pdf/2507.18450v1", "cate": "cs.HC", "date": "2025-05-16", "updated": "2025-05-16", "AI": {"title_translation": "高维数据在同心坐标中的分类", "tldr": "提出一种基于同心坐标的框架，用于可视化高维数据，以解决现有方法中的遮挡和计算限制。", "motivation": "现有可解释的多维数据可视化方法在进行高维无损可视化时，仍受到遮挡问题和计算能力的限制。", "method": "本文提出了一种使用无损同心坐标（Concentric Coordinates）的低到高维度数据支持框架。同心坐标是平行坐标和圆形坐标的更紧凑泛化形式，属于通用线坐标可视化。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "通过可解释方法对多维数据进行可视化，仍然受限于高维无损可视化能力，这些可视化既不受遮挡影响，又具有参数化可视化的计算能力。本文提出了一种低维到高维数据支持框架，该框架使用无损同心坐标，这些坐标是平行坐标和前圆形坐标的更紧凑泛化。这些是通用线坐标可视化的一种形式，可以直接支持机器学习算法可视化并促进人机交互。", "summary": "本文提出了一种新的框架，利用无损同心坐标来克服现有高维数据可视化方法在遮挡和计算能力上的限制。同心坐标是平行坐标和圆形坐标的泛化，属于通用线坐标可视化，能够直接支持机器学习算法的可视化并促进人机交互。", "keywords": "高维数据, 可视化, 同心坐标, 平行坐标, 机器学习", "comments": "该论文的创新点在于引入了“同心坐标”这一新的高维数据可视化方法，旨在解决传统方法（如平行坐标）中常见的遮挡问题并提升计算效率。此方法为高维数据分析提供了一种新的工具，特别是在结合机器学习算法可视化和增强人机交互方面具有潜在价值。"}}
{"id": "2507.18243", "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments", "authors": ["Longjian Zeng", "Zunjie Zhu", "Rongfeng Lu", "Ming Lu", "Bolun Zheng", "Chenggang Yan", "Anke Xue"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025 conference", "url": "http://arxiv.org/abs/2507.18243v1", "summary": "In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources.", "comment": "Accepted by ACM MM 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.18243v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DepthDark：弱光环境下鲁棒的单目深度估计", "tldr": "DepthDark通过引入模拟模块生成弱光深度数据集并提出有效的PEFT策略，显著提升了弱光环境下单目深度估计的鲁棒性和性能，实现了最先进的结果。", "motivation": "当前的单目深度估计方法主要针对日光条件，在弱光环境下效果显著下降。主要挑战在于缺乏大规模、高质量的弱光配对深度数据集以及有效的参数高效微调（PEFT）策略。", "method": "本文提出了DepthDark，一个用于弱光单目深度估计的鲁棒基础模型。首先引入了眩光模拟模块和噪声模拟模块，以准确模拟夜间成像过程，生成高质量的弱光配对深度数据集。其次，提出了一种有效的弱光PEFT策略，该策略利用光照引导和多尺度特征融合来增强模型在弱光环境中的能力。", "result": "DepthDark在具有挑战性的nuScenes-Night和RobotCar-Night数据集上实现了最先进的深度估计性能，并验证了其在有限训练数据和计算资源下的有效性。", "conclusion": "DepthDark通过解决弱光深度估计中的数据稀缺问题和提出有效的PEFT策略，成功地为弱光环境下的单目深度估计提供了一个鲁棒且高性能的解决方案。", "translation": "近年来，单目深度估计的基础模型受到越来越多的关注。当前的方法主要针对典型的日光条件，但其在弱光环境下的有效性显著降低。目前缺乏专门为弱光场景设计的鲁棒的单目深度估计基础模型。这主要源于缺乏大规模、高质量的弱光配对深度数据集以及有效的参数高效微调（PEFT）策略。为了解决这些挑战，我们提出了DepthDark，一个用于弱光单目深度估计的鲁棒基础模型。我们首先引入了一个眩光模拟模块和一个噪声模拟模块，以准确模拟夜间成像过程，生成高质量的弱光配对深度数据集。此外，我们提出了一种有效的弱光PEFT策略，该策略利用光照引导和多尺度特征融合来增强模型在弱光环境中的能力。我们的方法在具有挑战性的nuScenes-Night和RobotCar-Night数据集上实现了最先进的深度估计性能，验证了其在使用有限训练数据和计算资源下的有效性。", "summary": "本文提出DepthDark，一个针对弱光环境下单目深度估计的鲁棒基础模型，旨在解决现有方法在低光照条件下性能下降的问题。为克服数据稀缺挑战，该研究引入了眩光和噪声模拟模块以生成高质量的弱光配对深度数据集。同时，提出了一种结合光照引导和多尺度特征融合的参数高效微调（PEFT）策略，以提升模型在弱光环境中的能力。实验结果表明，DepthDark在nuScenes-Night和RobotCar-Night等挑战性数据集上取得了最先进的深度估计性能，且验证了其在有限资源下的有效性。", "keywords": "单目深度估计, 弱光环境, DepthDark, 数据模拟, PEFT", "comments": "该论文的创新点在于通过引入新颖的模拟模块有效解决了弱光环境下深度估计数据稀缺的难题，并提出了针对性的PEFT策略，使得模型能够在资源有限的情况下依然表现出色。这对于推动自动驾驶和机器人等领域在夜间或弱光条件下的应用具有重要意义。"}}
{"id": "2507.18567", "title": "Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications", "authors": ["Ruben Gamboa", "Panagiotis Manolios"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18567v1", "summary": "The ACL2 Workshop series is the major technical forum for users of the ACL2\ntheorem proving system to present research related to the ACL2 theorem prover\nand its applications. ACL2 is an industrial-strength automated reasoning\nsystem, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM\nSoftware System Award was awarded to Boyer, Kaufmann, and Moore for their work\non ACL2 and the other theorem provers in the Boyer-Moore family.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18567v1", "cate": "cs.LO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "第19届ACL2定理证明器及其应用国际研讨会论文集", "tldr": "该论文集是第19届ACL2定理证明器国际研讨会的会议记录，该研讨会是ACL2定理证明系统用户展示相关研究的主要技术论坛。ACL2是一个工业级自动化推理系统，其开发者因在ACL2及其家族上的工作获得了2005年ACM软件系统奖。", "motivation": "该研讨会系列旨在为ACL2定理证明系统的用户提供一个主要的技术论坛，以展示与ACL2定理证明器及其应用相关的研究。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "ACL2研讨会系列是ACL2定理证明系统用户展示与ACL2定理证明器及其应用相关研究的主要技术论坛。ACL2是一个工业级自动化推理系统，是Boyer-Moore定理证明器家族的最新成员。2005年ACM软件系统奖授予了Boyer、Kaufmann和Moore，以表彰他们在ACL2和Boyer-Moore家族中其他定理证明器方面的工作。", "summary": "该论文集记录了第19届ACL2定理证明器及其应用国际研讨会的内容。该研讨会是ACL2定理证明系统用户展示研究成果的重要技术平台。ACL2本身是一种先进的工业级自动化推理系统，是Boyer-Moore定理证明器家族的最新发展，其开发者的贡献曾荣获2005年ACM软件系统奖。", "keywords": "ACL2, 定理证明器, 自动化推理, 研讨会, Boyer-Moore", "comments": "这份摘要主要介绍了ACL2研讨会的背景和ACL2定理证明系统的重要性，但未提供具体的研究内容或论文成果。它强调了ACL2作为一种成熟的工业级推理系统及其在学术界的认可度。"}}
{"id": "2407.19795", "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks", "authors": ["Juhwan Choi", "Junehyoung Kwon", "JungMin Yun", "Seunguk Yu", "YoungBin Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)", "url": "http://arxiv.org/abs/2407.19795v2", "summary": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.", "comment": "ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)", "pdf_url": "http://arxiv.org/pdf/2407.19795v2", "cate": "cs.CL", "date": "2024-07-29", "updated": "2025-07-24", "AI": {"title_translation": "VolDoGer：LLM辅助的视觉-语言任务域泛化数据集", "tldr": "为了解决视觉-语言任务中域泛化研究缺乏数据集的问题，本文提出了VolDoGer，一个LLM辅助的域泛化数据集，涵盖图像字幕、视觉问答和视觉蕴涵任务，并用于评估各种模型的域泛化能力。", "motivation": "深度学习模型在视觉-语言任务上的域泛化能力研究受限于所需数据集的缺乏。", "method": "本文提出了VolDoGer，一个专门用于域泛化的视觉-语言数据集，通过将基于LLM的数据标注技术扩展到视觉-语言任务来构建，从而减轻了人工标注的负担。研究人员还使用VolDoGer评估了从微调模型到多模态大型语言模型等多种模型的域泛化能力。", "result": "本文构建并提出了VolDoGer数据集，并利用该数据集评估了各种模型的域泛化能力。", "conclusion": "VolDoGer数据集的提出解决了视觉-语言任务域泛化研究中数据集缺乏的问题，为评估不同模型的域泛化能力提供了工具。", "translation": "域泛化能力是深度学习模型的一个关键方面，因为它决定了模型在来自未见领域的数据上表现良好的能力。然而，针对视觉-语言任务的深度学习模型域泛化能力的研究仍然有限，这主要是因为缺乏所需的数据集。为了解决这些挑战，我们提出了 VolDoGer：一个专为域泛化设计的视觉-语言数据集，它解决了三个视觉-语言任务：图像字幕、视觉问答和视觉蕴涵。我们通过将基于LLM的数据标注技术扩展到视觉-语言任务来构建 VolDoGer，从而减轻了招募人工标注者的负担。我们通过 VolDoGer 评估了各种模型的域泛化能力，从微调模型到最近的多模态大型语言模型。", "summary": "本文提出了VolDoGer，一个新颖的视觉-语言域泛化数据集，旨在解决该领域数据稀缺的问题。VolDoGer涵盖图像字幕、视觉问答和视觉蕴涵等任务，并利用LLM辅助的数据标注技术构建，有效减轻了人工标注的负担。该数据集可用于评估包括微调模型和多模态大型语言模型在内的各种模型的域泛化能力。", "keywords": "域泛化, 视觉-语言任务, LLM辅助, 数据集, 图像字幕", "comments": "本文的创新之处在于利用LLM辅助数据标注技术来构建一个专门用于视觉-语言任务域泛化的数据集，这对于推动该领域的研究至关重要，因为它直接解决了数据不足的痛点。"}}
{"id": "2507.07620", "title": "ViLU: Learning Vision-Language Uncertainties for Failure Prediction", "authors": ["Marc Lafon", "Yannis Karmim", "Julio Silva-Rodríguez", "Paul Couairon", "Clément Rambour", "Raphaël Fournier-Sniehotta", "Ismail Ben Ayed", "Jose Dolz", "Nicolas Thome"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.07620v3", "summary": "Reliable Uncertainty Quantification (UQ) and failure prediction remain open\nchallenges for Vision-Language Models (VLMs). We introduce ViLU, a new\nVision-Language Uncertainty quantification framework that contextualizes\nuncertainty estimates by leveraging all task-relevant textual representations.\nViLU constructs an uncertainty-aware multi-modal representation by integrating\nthe visual embedding, the predicted textual embedding, and an image-conditioned\ntextual representation via cross-attention. Unlike traditional UQ methods based\non loss prediction, ViLU trains an uncertainty predictor as a binary classifier\nto distinguish correct from incorrect predictions using a weighted binary\ncross-entropy loss, making it loss-agnostic. In particular, our proposed\napproach is well-suited for post-hoc settings, where only vision and text\nembeddings are available without direct access to the model itself. Extensive\nexperiments on diverse datasets show the significant gains of our method\ncompared to state-of-the-art failure prediction methods. We apply our method to\nstandard classification datasets, such as ImageNet-1k, as well as large-scale\nimage-caption datasets like CC12M and LAION-400M. Ablation studies highlight\nthe critical role of our architecture and training in achieving effective\nuncertainty quantification. Our code is publicly available and can be found\nhere: https://github.com/ykrmm/ViLU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.07620v3", "cate": "cs.CV", "date": "2025-07-10", "updated": "2025-07-24", "AI": {"title_translation": "ViLU：学习视觉-语言不确定性以进行故障预测", "tldr": "ViLU是一种新的视觉-语言不确定性量化框架，通过将不确定性预测器训练为二元分类器来区分正确和不正确预测，从而解决了视觉-语言模型中不确定性量化和故障预测的挑战，并在各种数据集上取得了显著的性能提升。", "motivation": "可靠的不确定性量化（UQ）和故障预测对于视觉-语言模型（VLMs）来说仍然是开放的挑战。", "method": "ViLU引入了一个新的视觉-语言不确定性量化框架，该框架通过利用所有与任务相关的文本表示来情境化不确定性估计。它通过整合视觉嵌入、预测文本嵌入和图像条件文本表示（通过交叉注意力）来构建一个不确定性感知的多模态表示。与传统的基于损失预测的UQ方法不同，ViLU将不确定性预测器训练为二元分类器，使用加权二元交叉熵损失来区分正确和不正确的预测，使其与损失无关。该方法特别适用于后验设置，即只有视觉和文本嵌入可用，而无法直接访问模型本身。", "result": "在各种数据集上的广泛实验表明，与最先进的故障预测方法相比，ViLU取得了显著的性能提升。该方法应用于标准的分类数据集（如ImageNet-1k）以及大规模图像-字幕数据集（如CC12M和LAION-400M）。消融研究强调了其架构和训练在实现有效不确定性量化方面的关键作用。", "conclusion": "ViLU通过其新颖的、与损失无关的二元分类方法，显著提高了视觉-语言模型中的不确定性量化和故障预测能力，特别适用于后验设置，并在一系列数据集上表现出优越性。", "translation": "可靠的不确定性量化（UQ）和故障预测对于视觉-语言模型（VLMs）来说仍然是开放的挑战。我们引入了ViLU，一个全新的视觉-语言不确定性量化框架，它通过利用所有与任务相关的文本表示来情境化不确定性估计。ViLU通过整合视觉嵌入、预测文本嵌入和图像条件文本表示（通过交叉注意力）来构建一个不确定性感知的多模态表示。与传统的基于损失预测的UQ方法不同，ViLU将不确定性预测器训练为二元分类器，使用加权二元交叉熵损失来区分正确和不正确的预测，使其与损失无关。特别是，我们提出的方法非常适合后验设置，即只有视觉和文本嵌入可用，而无法直接访问模型本身。在各种数据集上的广泛实验表明，与最先进的故障预测方法相比，我们的方法取得了显著的性能提升。我们将我们的方法应用于标准分类数据集，如ImageNet-1k，以及大规模图像-字幕数据集，如CC12M和LAION-400M。消融研究强调了我们的架构和训练在实现有效不确定性量化方面的关键作用。我们的代码是公开的，可以在这里找到：https://github.com/ykrmm/ViLU。", "summary": "本文介绍了ViLU，一个用于视觉-语言模型（VLMs）的新型不确定性量化框架，旨在解决可靠不确定性估计和故障预测的挑战。ViLU通过整合视觉和文本表示来构建不确定性感知的多模态表示。其核心创新在于将不确定性预测器训练为一个二元分类器，以区分正确和不正确预测，使其与传统的基于损失的方法不同，并且是与损失无关的。该方法特别适用于后验设置，即不需要直接访问原始模型。实验结果表明，ViLU在各种数据集上均优于现有的故障预测方法，证明了其在不确定性量化方面的有效性。", "keywords": "视觉-语言模型, 不确定性量化, 故障预测, 后验设置, 多模态表示", "comments": "ViLU的创新之处在于其独特的、与损失无关的二元分类方法来预测不确定性，这使其能够脱离传统的基于损失的UQ方法。此外，其对后验设置的适用性是一个重要的优势，因为它允许在不修改或访问原始模型的情况下进行故障预测。这对于实际应用和模型部署具有重要意义。该方法在多个数据集上的显著性能提升证明了其有效性和潜在影响力。"}}
{"id": "2410.02482", "title": "It is Giving Major Satisfaction: Why Fairness Matters for Software Practitioners", "authors": ["Emeralda Sesari", "Federica Sarro", "Ayushi Rastogi"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted in ACM Transactions on Software Engineering and Methodology (TOSEM)", "url": "http://arxiv.org/abs/2410.02482v5", "summary": "Software practitioners often encounter workplace unfairness, such as unequal\nrecognition and gender bias. While the link between fairness and job\nsatisfaction has been established in other fields, its relevance to software\nprofessionals remains underexplored. This study examines how fairness\nperceptions relate to job satisfaction among software practitioners, focusing\non both general trends and demographic-specific differences. We conducted an\nonline survey of 108 software practitioners, followed by ordinal logistic\nregression to analyze the relationship between fairness perceptions and job\nsatisfaction in software engineering contexts, with moderation analysis\nexamining how this relationship varies across demographic groups. Our findings\nindicate that all four fairness dimensions (namely distributive, procedural,\ninterpersonal, and informational fairness) significantly affect overall job\nsatisfaction and satisfaction with job security. Among these, interpersonal\nfairness has the biggest impact. The relationship between fairness and job\nsatisfaction is stronger for female, ethnically underrepresented, less\nexperienced practitioners, and those with work limitations. Fairness in\nauthorship emerged as an important factor for job satisfaction collectively,\nwhile fairness in policy implementation, high-demand situations, and working\nhours impacted specific demographic groups. This study highlights the role of\nfairness among software practitioners, offering strategies for organizations to\npromote fair practices and targeted approaches for certain demographic groups.", "comment": "Accepted in ACM Transactions on Software Engineering and Methodology\n  (TOSEM)", "pdf_url": "http://arxiv.org/pdf/2410.02482v5", "cate": "cs.SE", "date": "2024-10-03", "updated": "2025-07-24", "AI": {"title_translation": "带来巨大满足感：为什么公平对软件从业者至关重要", "tldr": "公平感显著影响软件从业者的工作满意度，尤其是人际公平，且对女性、少数族裔、经验较少和有工作限制的从业者影响更大，强调组织应促进公平实践。", "motivation": "软件从业者经常遭遇职场不公，尽管公平与工作满意度在其他领域已被证实相关，但在软件专业领域这一联系尚未得到充分探索。", "method": "本研究对108名软件从业者进行了在线调查，并采用序数逻辑回归分析了公平感知与工作满意度之间的关系，同时进行了调节分析以考察不同人口群体间的差异。", "result": "所有四种公平维度（分配、程序、人际、信息公平）均显著影响整体工作满意度和工作保障满意度，其中人际公平影响最大。公平与工作满意度之间的关系对女性、少数族裔、经验较少和有工作限制的从业者更强。著作权公平是集体工作满意度的重要因素，而政策执行、高需求情况和工作时间的公平则影响特定人口群体。", "conclusion": "本研究强调了公平在软件从业者中的重要作用，为组织提供了促进公平实践和针对特定人口群体的策略。", "translation": "软件从业者经常遇到职场不公，例如不平等的认可和性别偏见。尽管公平与工作满意度之间的联系已在其他领域得到证实，但其与软件专业人员的相关性仍未得到充分探索。本研究旨在探讨公平感知如何与软件从业者的工作满意度相关联，重点关注普遍趋势和人口统计学特异性差异。我们对108名软件从业者进行了一项在线调查，随后采用序数逻辑回归分析了软件工程背景下公平感知与工作满意度之间的关系，并进行了调节分析以考察这种关系在不同人口群体中的差异。我们的研究结果表明，所有四种公平维度（即分配公平、程序公平、人际公平和信息公平）都显著影响整体工作满意度和工作保障满意度。其中，人际公平影响最大。公平与工作满意度之间的关系对于女性、少数族裔、经验较少的从业者以及有工作限制的从业者来说更强。著作权公平成为集体工作满意度的重要因素，而政策执行、高需求情况和工作时间的公平则影响特定人口群体。本研究强调了公平在软件从业者中的作用，为组织提供了促进公平实践和针对特定人口群体的策略。", "summary": "本研究通过对108名软件从业者的在线调查和序数逻辑回归分析，深入探讨了公平感知对软件从业者工作满意度的影响。研究发现，分配、程序、人际和信息四种公平维度均显著影响工作满意度，其中人际公平的影响最为显著。此外，公平与工作满意度之间的关联对于女性、少数族裔、经验较少及有工作限制的从业者更为强烈。著作权公平被确定为普遍影响工作满意度的重要因素，而政策执行、高需求情境和工作时间的公平则对特定人口群体产生影响。本研究强调了公平在软件行业的重要性，并为组织提供了促进公平实践及制定针对性策略的指导。", "keywords": "公平感知, 工作满意度, 软件从业者, 人际公平, 人口统计学差异", "comments": "这篇论文填补了公平与工作满意度关系在软件从业者领域的研究空白。其创新之处在于不仅确认了公平的普遍影响，还通过调节分析揭示了公平对特定弱势群体影响更强的现象，这对于制定更具针对性的组织策略具有重要意义。研究方法清晰，但样本量相对较小可能限制了结果的普遍性。"}}
{"id": "2507.18141", "title": "Data-Driven Incremental GAS Certificate of Nonlinear Homogeneous Networks: A Formal Modular Approach", "authors": ["Mahdieh Zaker", "David Angeli", "Abolfazl Lavaei"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18141v1", "summary": "This work focuses on a compositional data-driven approach to verify\nincremental global asymptotic stability (delta-GAS) over interconnected\nhomogeneous networks of degree one with unknown mathematical dynamics. Our\nproposed approach leverages the concept of incremental input-to-state stability\n(delta-ISS) of subsystems, characterized by delta-ISS Lyapunov functions. To\nimplement our data-driven scheme, we initially reframe the delta-ISS Lyapunov\nconditions as a robust optimization program (ROP). However, due to the presence\nof unknown subsystem dynamics in the ROP constraints, we develop a scenario\noptimization program (SOP) by gathering data from trajectories of each unknown\nsubsystem. We solve the SOP and construct a delta-ISS Lyapunov function for\neach subsystem with unknown dynamics. We then leverage a small-gain\ncompositional condition to facilitate the construction of an incremental\nLyapunov function for an unknown interconnected network with unknown dynamics\nbased on its data-driven delta-ISS Lyapunov functions of individual subsystems,\nwhile providing correctness guarantees. We demonstrate that our data-driven\ncompositional approach aligns sample complexity with subsystem granularity,\nresulting in a linear increase in required data as the number of subsystems\nrises. In contrast, the existing monolithic approach in the literature exhibits\nexponential growth in sample complexity with increasing number of subsystems,\nrendering it impractical for real-world applications. To validate the\neffectiveness of our compositional data-driven approach, we apply it to an\nunknown nonlinear homogeneous network of degree one, comprising 10000\nsubsystems. By gathering data from each unknown subsystem, we demonstrate that\nthe interconnected network is delta-GAS with a correctness guarantee.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18141v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "数据驱动的非线性齐次网络增量式全局渐近稳定性认证：一种形式化模块化方法", "tldr": "本文提出一种数据驱动的组合方法，用于验证具有未知动态的互连齐次网络的增量式全局渐近稳定性（delta-GAS），其数据复杂度随子系统数量线性增长，优于现有方法。", "motivation": "现有方法（单片方法）在验证具有未知动态的互连网络的全局渐近稳定性时，样本复杂度呈指数级增长，不适用于实际应用。因此，需要一种更有效、可扩展的方法。", "method": "1. 利用子系统的增量输入到状态稳定性（delta-ISS）概念，通过delta-ISS Lyapunov函数表征。2. 将delta-ISS Lyapunov条件重新构建为鲁棒优化程序（ROP）。3. 由于未知子系统动态，通过收集每个未知子系统轨迹的数据，开发场景优化程序（SOP）并求解，为每个子系统构建delta-ISS Lyapunov函数。4. 利用小增益组合条件，基于数据驱动的子系统delta-ISS Lyapunov函数，为未知互连网络构建增量Lyapunov函数，并提供正确性保证。", "result": "1. 提出的数据驱动组合方法将样本复杂度与子系统粒度对齐，所需数据量随子系统数量线性增加。2. 与现有单片方法相比，显著降低了样本复杂度（从指数增长到线性增长），使其在实际应用中可行。3. 成功应用于包含10000个子系统的未知非线性齐次网络，验证了互连网络的delta-GAS并提供了正确性保证。", "conclusion": "本文提出了一种有效的数据驱动组合方法，用于验证具有未知动态的互连齐次网络的增量式全局渐近稳定性，该方法在数据复杂度方面具有显著优势，使其适用于大规模实际系统。", "translation": "这项工作专注于一种组合式数据驱动方法，用于验证具有未知数学动态的一阶互连齐次网络的增量式全局渐近稳定性（delta-GAS）。我们提出的方法利用了子系统的增量输入到状态稳定性（delta-ISS）概念，该概念由delta-ISS Lyapunov函数表征。为了实现我们的数据驱动方案，我们首先将delta-ISS Lyapunov条件重新构建为鲁棒优化程序（ROP）。然而，由于ROP约束中存在未知子系统动态，我们通过从每个未知子系统的轨迹中收集数据，开发了一个场景优化程序（SOP）。我们求解SOP并为每个具有未知动态的子系统构建一个delta-ISS Lyapunov函数。然后，我们利用小增益组合条件，基于其数据驱动的单个子系统的delta-ISS Lyapunov函数，促进为具有未知动态的未知互连网络构建增量Lyapunov函数，同时提供正确性保证。我们证明了我们的数据驱动组合方法使样本复杂度与子系统粒度对齐，导致所需数据随子系统数量线性增加。相比之下，文献中现有的单片方法随着子系统数量的增加，样本复杂度呈指数增长，使其不适用于实际应用。为了验证我们组合式数据驱动方法的有效性，我们将其应用于一个包含10000个子系统的一阶未知非线性齐次网络。通过从每个未知子系统收集数据，我们证明了互连网络是delta-GAS的，并具有正确性保证。", "summary": "本文提出了一种新颖的数据驱动组合方法，用于验证具有未知动态的互连齐次网络的增量式全局渐近稳定性（delta-GAS）。该方法通过将子系统的delta-ISS Lyapunov条件重构为场景优化程序（SOP），并利用数据来构建子系统的delta-ISS Lyapunov函数。随后，结合小增益组合条件，为整个未知互连网络构建增量Lyapunov函数。与现有单片方法相比，该方法将样本复杂度从指数级降低到线性级，显著提高了可扩展性。实验证明了其在包含10000个子系统的大规模网络上的有效性。", "keywords": "数据驱动, 增量式全局渐近稳定性, 齐次网络, Lyapunov函数, 组合方法", "comments": "本文的创新点在于提出了一种数据驱动的模块化方法来解决大规模互连网络（具有未知动态）的稳定性验证问题。通过将复杂度从指数级降低到线性级，该方法极大地提高了现有技术的实用性，使其能够应用于大规模实际系统。这种将数据驱动与组合理论相结合的思路，为复杂系统分析提供了新的范式。"}}
{"id": "2507.17779", "title": "CM-UNet: A Self-Supervised Learning-Based Model for Coronary Artery Segmentation in X-Ray Angiography", "authors": ["Camille Challier", "Xiaowu Sun", "Thabo Mahendiran", "Ortal Senouf", "Bernard De Bruyne", "Denise Auberson", "Olivier Müller", "Stephane Fournier", "Pascal Frossard", "Emmanuel Abbé", "Dorina Thanou"], "categories": ["q-bio.QM", "cs.LG", "I.2; I.4; I.5; J.3"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      IEEE EMBC 2025, 7 pages, 6 figures", "url": "http://arxiv.org/abs/2507.17779v1", "summary": "Accurate segmentation of coronary arteries remains a significant challenge in\nclinical practice, hindering the ability to effectively diagnose and manage\ncoronary artery disease. The lack of large, annotated datasets for model\ntraining exacerbates this issue, limiting the development of automated tools\nthat could assist radiologists. To address this, we introduce CM-UNet, which\nleverages self-supervised pre-training on unannotated datasets and transfer\nlearning on limited annotated data, enabling accurate disease detection while\nminimizing the need for extensive manual annotations. Fine-tuning CM-UNet with\nonly 18 annotated images instead of 500 resulted in a 15.2% decrease in Dice\nscore, compared to a 46.5% drop in baseline models without pre-training. This\ndemonstrates that self-supervised learning can enhance segmentation performance\nand reduce dependence on large datasets. This is one of the first studies to\nhighlight the importance of self-supervised learning in improving coronary\nartery segmentation from X-ray angiography, with potential implications for\nadvancing diagnostic accuracy in clinical practice. By enhancing segmentation\naccuracy in X-ray angiography images, the proposed approach aims to improve\nclinical workflows, reduce radiologists' workload, and accelerate disease\ndetection, ultimately contributing to better patient outcomes. The source code\nis publicly available at\nhttps://github.com/CamilleChallier/Contrastive-Masked-UNet.", "comment": "IEEE EMBC 2025, 7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.17779v1", "cate": "q-bio.QM", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "CM-UNet：一种基于自监督学习的X射线血管造影冠状动脉分割模型", "tldr": "CM-UNet通过自监督预训练和迁移学习，即使在有限的标注数据下，也能显著提高X射线血管造影中的冠状动脉分割精度，减少对大量手动标注的依赖。", "motivation": "在临床实践中，冠状动脉的精确分割仍然是一个重大挑战，阻碍了冠心病的有效诊断和管理。模型训练缺乏大型标注数据集加剧了这一问题，限制了可辅助放射科医生的自动化工具的开发。", "method": "本文提出了CM-UNet模型，该模型利用在未标注数据集上的自监督预训练和在有限标注数据上的迁移学习，以实现准确的疾病检测，同时最大限度地减少对大量手动标注的需求。", "result": "与没有预训练的基线模型相比，仅用18张标注图像而非500张图像对CM-UNet进行微调，Dice分数仅下降了15.2%，而基线模型下降了46.5%。这表明自监督学习可以提高分割性能并减少对大型数据集的依赖。", "conclusion": "自监督学习在改善X射线血管造影中的冠状动脉分割方面具有重要意义，对提高临床实践中的诊断准确性具有潜在影响。所提出的方法旨在通过提高X射线血管造影图像的分割精度来改善临床工作流程、减轻放射科医生的工作量并加速疾病检测，最终有助于改善患者预后。", "translation": "在临床实践中，冠状动脉的精确分割仍然是一个重大挑战，阻碍了冠心病的有效诊断和管理。模型训练缺乏大型标注数据集加剧了这一问题，限制了可辅助放射科医生的自动化工具的开发。为了解决这个问题，我们引入了CM-UNet，它利用在未标注数据集上的自监督预训练和在有限标注数据上的迁移学习，即使在有限的标注数据下，也能实现准确的疾病检测，同时最大限度地减少对大量手动标注的需求。与没有预训练的基线模型相比，仅用18张标注图像而非500张图像对CM-UNet进行微调，Dice分数仅下降了15.2%，而基线模型下降了46.5%。这表明自监督学习可以提高分割性能并减少对大型数据集的依赖。这是首批强调自监督学习在改善X射线血管造影中的冠状动脉分割方面重要性的研究之一，对提高临床实践中的诊断准确性具有潜在影响。通过提高X射线血管造影图像的分割精度，所提出的方法旨在改善临床工作流程、减轻放射科医生的工作量并加速疾病检测，最终有助于改善患者预后。源代码可在https://github.com/CamilleChallier/Contrastive-Masked-UNet 公开获取。", "summary": "CM-UNet是一种基于自监督学习的模型，旨在解决X射线血管造影中冠状动脉分割对大量标注数据依赖的问题。该模型通过在未标注数据上进行自监督预训练，并在少量标注数据上进行迁移学习，显著提高了分割精度。实验结果表明，与无预训练基线模型相比，CM-UNet在仅使用极少量标注数据时，性能下降幅度更小，证明了自监督学习在减少数据依赖和提升分割性能方面的有效性。这项研究强调了自监督学习在改善冠状动脉分割方面的潜力，有望优化临床诊断流程并减轻医生负担。", "keywords": "冠状动脉分割, 自监督学习, X射线血管造影, CM-UNet, 迁移学习", "comments": "本文的创新点在于将自监督学习引入冠状动脉分割任务，有效解决了医学图像领域标注数据稀缺的痛点。通过对比实验清晰展示了自监督预训练的优势，对于推动深度学习在临床诊断中的应用具有重要意义。其提出的CM-UNet模型为未来在有限数据下进行精确医学图像分割提供了新的思路。"}}
{"id": "2507.18573", "title": "Jacobi Hamiltonian Integrators", "authors": ["Adérito Araújo", "Gonçalo Inocêncio Oliveira", "João Nuno Mestre"], "categories": ["math.DG", "cs.NA", "math-ph", "math.MP", "math.NA", "math.SG", "37M15 (Primary) 53D17, 37J39 (Secondary)"], "primary_category": "Subjects:       Differential Geometry (math.DG)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2507.18573v1", "summary": "We develop a method of constructing structure-preserving integrators for\nHamiltonian systems in Jacobi manifolds. Hamiltonian mechanics, rooted in\nsymplectic and Poisson geometry, has long provided a foundation for modelling\nconservative systems in classical physics. Jacobi manifolds, generalizing both\ncontact and Poisson manifolds, extend this theory and are suitable for\nincorporating time-dependent, dissipative and thermodynamic phenomena.\n  Building on recent advances in geometric integrators - specifically Poisson\nHamiltonian Integrators (PHI), which preserve key features of Poisson systems -\nwe propose a construction of Jacobi Hamiltonian Integrators. Our approach\nexplores the correspondence between Jacobi and homogeneous Poisson manifolds,\nwith the aim of extending the PHI techniques while ensuring preservation of the\nhomogeneity structure.\n  This work develops the theoretical tools required for this generalization and\noutlines a numerical integration technique compatible with Jacobi dynamics. By\nfocusing on the homogeneous Poisson perspective rather than on direct contact\nrealizations, we provide a clear pathway for structure-preserving integration\nof time-dependent and dissipative systems within the Jacobi framework.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2507.18573v1", "cate": "math.DG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "雅可比哈密顿积分器", "tldr": "本文通过利用雅可比流形和齐次泊松流形之间的对应关系，扩展了泊松哈密顿积分器，从而开发了雅可比哈密顿积分器，实现了对时变和耗散系统的结构保持积分。", "motivation": "哈密顿力学为经典物理中的保守系统建模提供了基础。雅可比流形是接触流形和泊松流形的推广，适用于包含时变、耗散和热力学现象的系统。因此，研究雅可比流形上的结构保持积分器具有重要意义。", "method": "该研究基于泊松哈密顿积分器（PHI）的最新进展，提出了一种雅可比哈密顿积分器的构建方法。该方法探索了雅可比流形和齐次泊松流形之间的对应关系，旨在扩展PHI技术，同时确保齐次结构的保持。文中开发了泛化所需的理论工具，并概述了一种与雅可比动力学兼容的数值积分技术。", "result": "该研究开发了广义雅可比哈密顿积分器所需的理论工具，并概述了一种与雅可比动力学兼容的数值积分技术。通过关注齐次泊松视角而非直接接触实现，为雅可比框架内时变和耗散系统的结构保持积分提供了清晰的途径。", "conclusion": "通过采用齐次泊松视角，该工作为雅可比框架内时变和耗散系统的结构保持积分提供了一条清晰的路径。", "translation": "我们开发了一种在雅可比流形中构建哈密顿系统结构保持积分器的方法。哈密顿力学植根于辛几何和泊松几何，长期以来为经典物理中保守系统的建模提供了基础。雅可比流形推广了接触流形和泊松流形，扩展了这一理论，适用于包含时变、耗散和热力学现象。基于几何积分器——特别是保持泊松系统关键特征的泊松哈密顿积分器（PHI）——的最新进展，我们提出了一种雅可比哈密顿积分器的构建方法。我们的方法探索了雅可比流形和齐次泊松流形之间的对应关系，旨在扩展PHI技术，同时确保齐次结构的保持。这项工作开发了这种泛化所需的理论工具，并概述了一种与雅可比动力学兼容的数值积分技术。通过关注齐次泊松视角而非直接接触实现，我们为雅可比框架内时变和耗散系统的结构保持积分提供了清晰的途径。", "summary": "本文提出了一种用于雅可比流形中哈密顿系统的雅可比哈密顿积分器，旨在将经典哈密顿力学扩展到包含时变和耗散现象。该方法基于泊松哈密顿积分器（PHI），通过利用雅可比流形和齐次泊松流形之间的对应关系来确保结构保持。这项工作开发了理论工具和数值积分技术，为复杂系统的结构保持积分提供了清晰的途径。", "keywords": "雅可比哈密顿积分器, 结构保持积分器, 雅可比流形, 齐次泊松流形, 耗散系统", "comments": "本文提出了一种创新方法，通过利用雅可比流形，将结构保持积分从保守哈密顿系统扩展到更一般的时变和耗散系统。其核心创新在于利用与齐次泊松流形的对应关系，为这些复杂动力学中的数值积分提供了一条理论上严谨且实用的途径。这项工作对于精确模拟能量不守恒的真实物理系统具有重要意义。"}}
{"id": "2507.16761", "title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks", "authors": ["Marcel Kleinmann", "Shashank Agnihotri", "Margret Keuper"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16761v2", "summary": "Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. In this work, we\naddress these limitations by introducing anti-aliasing strategies using\nFLCPooling (FLC) and BlurPool (BP) to significantly improve explanation\nquality. Our experiments on chest X-ray datasets demonstrate that the modified\n$\\text{B-cos}_\\text{FLC}$ and $\\text{B-cos}_\\text{BP}$ preserve strong\npredictive performance while providing faithful and artifact-free explanations\nsuitable for clinical application in multi-class and multi-label settings. Code\navailable at: GitHub repository (url:\nhttps://github.com/mkleinma/B-cos-medical-paper).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16761v2", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "具有抗混叠B-cos网络的忠实可解释胸部X射线诊断", "tldr": "引入抗混叠策略改进B-cos网络，使其在胸部X射线诊断中提供忠实且无伪影的可解释性，同时保持高性能。", "motivation": "深度神经网络在医疗成像等安全关键领域需要忠实性和可解释性。标准B-cos网络虽然具有竞争力，但在其解释图中存在严重的混叠伪影，不适用于临床使用。", "method": "通过引入使用FLCPooling (FLC) 和 BlurPool (BP) 的抗混叠策略，改进了B-cos网络，以显著提高解释质量。", "result": "在胸部X射线数据集上的实验表明，修改后的B-cos_FLC和B-cos_BP模型在多类别和多标签设置中，保持了强大的预测性能，同时提供了忠实且无伪影的解释，适用于临床应用。", "conclusion": "通过引入抗混叠策略，成功改进了B-cos网络，使其在医疗成像领域提供可靠且可解释的诊断，解决了现有B-cos模型在临床应用中的局限性。", "translation": "忠实性和可解释性对于在医疗成像等安全关键领域部署深度神经网络 (DNN) 至关重要。B-cos 网络通过用权重-输入对齐机制取代标准线性层，提供了一种有前景的解决方案，无需后处理方法即可生成固有可解释的、特定于类别的解释。虽然诊断性能与最先进的 DNN 具有竞争力，但标准 B-cos 模型在其解释图中存在严重的混叠伪影，这使得它们不适合需要清晰度的临床使用。在这项工作中，我们通过引入使用 FLCPooling (FLC) 和 BlurPool (BP) 的抗混叠策略来解决这些限制，以显著提高解释质量。我们在胸部 X 射线数据集上的实验表明，修改后的 B-cos_FLC 和 B-cos_BP 在保持强大预测性能的同时，提供了忠实且无伪影的解释，适用于多类别和多标签设置中的临床应用。代码可在 GitHub 仓库获取。", "summary": "本文针对深度神经网络在医疗成像中可解释性及忠实性的需求，改进了B-cos网络。虽然现有B-cos网络能提供固有解释，但其解释图存在混叠伪影。作者通过引入FLCPooling和BlurPool两种抗混叠策略，显著提升了解释质量。实验证明，改进后的B-cos模型（B-cos_FLC和B-cos_BP）在胸部X射线诊断中保持了高预测性能，并提供了清晰、无伪影且忠实的可解释性，使其适合临床应用。", "keywords": "B-cos 网络, 可解释性, 胸部X射线, 抗混叠, 深度学习", "comments": "这项工作在医疗AI领域具有重要意义，因为它解决了深度学习模型在安全关键应用中部署的关键障碍——可解释性问题。通过引入抗混叠策略，作者不仅保留了B-cos网络固有的可解释性优势，还消除了使其不适用于临床的伪影，提高了模型在实际应用中的可信度和实用性。其创新点在于将抗混叠技术应用于可解释性网络的解释图生成，以确保临床医生可以依赖这些解释。"}}
{"id": "2507.18075", "title": "PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python", "authors": ["Jacob Mahon", "Chenxi Hou", "Zhihao Yao"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18075v1", "summary": "Python software development heavily relies on third-party packages. Direct\nand transitive dependencies create a labyrinth of software supply chains. While\nit is convenient to reuse code, vulnerabilities within these dependency chains\ncan propagate through dependencies, potentially affecting down-stream packages\nand applications. PyPI, the official Python package repository, hosts many\npackages and lacks a comprehensive analysis of the prevalence of vulnerable\ndependencies. This paper introduces PyPitfall, a quantitative analysis of\nvulnerable dependencies across the PyPI ecosystem. We analyzed the dependency\nstructures of 378,573 PyPI packages and identified 4,655 packages that\nexplicitly require at least one known-vulnerable version and 141,044 packages\nthat permit vulnerable versions within specified ranges. By characterizing the\necosystem-wide dependency landscape and the security impact of transitive\ndependencies, we aim to raise awareness of Python software supply chain\nsecurity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18075v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PyPitfall：Python 中的依赖混乱与软件供应链漏洞", "tldr": "PyPitfall对PyPI生态系统中Python包的依赖结构进行了大规模定量分析，揭示了大量包存在已知漏洞或允许使用漏洞版本，旨在提高人们对Python软件供应链安全的认识。", "motivation": "Python软件开发严重依赖第三方包，而依赖链中的漏洞可能传播并影响下游应用。PyPI缺乏对易受攻击依赖项普及程度的全面分析，因此需要进行定量研究来揭示这一问题。", "method": "本研究引入了PyPitfall工具，对PyPI生态系统中378,573个包的依赖结构进行了定量分析，以识别易受攻击的依赖项。", "result": "分析发现，有4,655个包明确要求至少一个已知漏洞版本，另有141,044个包在指定范围内允许使用漏洞版本。", "conclusion": "通过对整个生态系统依赖关系图景和传递性依赖安全影响的特征化，旨在提高人们对Python软件供应链安全的认识。", "translation": "Python软件开发严重依赖第三方包。直接和传递性依赖创建了一个软件供应链的迷宫。虽然重用代码很方便，但这些依赖链中的漏洞可以通过依赖项传播，潜在地影响下游包和应用程序。PyPI是官方的Python包仓库，托管了许多包，但缺乏对易受攻击依赖项普及程度的全面分析。本文介绍了PyPitfall，一个对PyPI生态系统中易受攻击依赖项进行定量分析的工具。我们分析了378,573个PyPI包的依赖结构，识别出4,655个明确要求至少一个已知漏洞版本的包，以及141,044个在指定范围内允许使用漏洞版本的包。通过表征整个生态系统的依赖关系图景和传递性依赖的安全影响，我们旨在提高人们对Python软件供应链安全的认识。", "summary": "本文介绍了PyPitfall，一个对PyPI生态系统中Python包的依赖结构进行大规模定量分析的工具。研究发现，在378,573个PyPI包中，有4,655个明确依赖于已知漏洞版本，另有141,044个包允许在其指定范围内使用漏洞版本。该研究旨在通过揭示Python软件供应链中依赖混乱和漏洞传播的现状，提高对供应链安全的认识。", "keywords": "Python, 软件供应链, 漏洞, 依赖, PyPI", "comments": "该论文通过对PyPI生态系统进行大规模定量分析，揭示了Python软件供应链中存在的广泛漏洞问题，其创新之处在于提供了具体的数据支持，而非停留在理论层面。这对于提高开发者和组织对依赖安全的重视具有重要意义。"}}
{"id": "2507.17792", "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains", "authors": ["Jingyi Yu", "Tim Pychynski", "Marco F. Huber"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17792v1", "summary": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17792v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多领域多传感器系统中的因果机制估计", "tldr": "本文提出了一种名为CICME的新型三步法，用于从多领域异构数据中推断因果机制，并通过因果迁移学习在特定场景下优于基线方法。", "motivation": "为了通过因果关系视角深入了解复杂的传感器系统，需要一种方法来估计来自多个领域异构数据的因果机制。", "method": "本文提出了通用和个体因果机制估计（CICME），这是一种新颖的三步法。它利用因果迁移学习（CTL）原理，在样本充足时可靠地检测领域不变的因果机制。识别出的共同因果机制进一步用于指导每个领域中剩余因果机制的个体估计。该方法建立在现有基于连续优化的因果发现方法之上。", "result": "CICME在制造过程启发下的线性高斯模型场景中进行了评估。结果表明，CICME结合了对汇总数据和单个领域数据重复应用因果发现的优点，并且在某些场景下甚至优于两种基线方法。", "conclusion": "CICME是一种有效的方法，可以从多领域异构数据中推断因果机制，通过结合共同和个体机制的估计，并在特定条件下超越现有基线方法。", "translation": "为了通过因果关系视角更深入地了解复杂的传感器系统，我们提出了通用和个体因果机制估计（CICME），这是一种新颖的三步方法，用于从跨多个领域收集的异构数据中推断因果机制。通过利用因果迁移学习（CTL）的原理，CICME在提供足够样本的情况下能够可靠地检测领域不变的因果机制。识别出的共同因果机制进一步用于指导每个领域中剩余因果机制的单独估计。CICME的性能在受制造过程启发的场景下的线性高斯模型上进行了评估。在现有基于连续优化的因果发现方法的基础上，我们表明CICME利用了对汇总数据和对来自单个领域的数据重复应用因果发现的优势，并且在某些场景下甚至优于两种基线方法。", "summary": "本文提出了一种名为通用和个体因果机制估计（CICME）的新型三步法，旨在从多领域异构数据中推断因果机制。该方法利用因果迁移学习，能够检测领域不变的因果机制，并利用这些共同机制指导个体领域机制的估计。在对线性高斯模型的评估中，CICME展示了其在结合汇总数据和个体领域数据分析上的优势，并在特定情况下超越了现有基线方法。", "keywords": "因果机制估计, 多传感器系统, 因果迁移学习, 异构数据, 因果发现", "comments": "该论文的创新点在于提出了CICME这一结合了共同和个体因果机制估计的三步法，并引入了因果迁移学习来处理多领域异构数据。它通过利用领域不变机制来指导个体机制的发现，有效地提升了因果推断在复杂传感器系统中的应用。其在特定场景下超越基线方法的表现，凸显了其潜在的实用价值。"}}
{"id": "2507.18031", "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks", "authors": ["Ahmad ALBarqawi", "Mahmoud Nazzal", "Issa Khalil", "Abdallah Khreishah", "NhatHai Phan"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18031v1", "summary": "The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18031v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ViGText：基于视觉-语言模型解释和图神经网络的深度伪造图像检测", "tldr": "ViGText是一种新颖的深度伪造图像检测方法，它将视觉-语言模型生成的文本解释与图像数据整合到图神经网络框架中，显著提升了对复杂和定制化深度伪造的泛化性和鲁棒性。", "motivation": "深度伪造技术迅速发展，生成逼真但欺诈性的数字内容，威胁媒体的真实性。传统深度伪造检测方法在应对复杂、定制化的深度伪造时，特别是在泛化性和抵御恶意攻击的鲁棒性方面，常常表现不佳。", "method": "本文提出了ViGText，一种将图像与视觉大语言模型（VLLM）文本解释整合到基于图的框架中以改进深度伪造检测的新方法。其创新之处在于将详细解释与视觉数据集成，提供比字幕更具上下文感知能力的分析。ViGText系统地将图像分割成补丁，构建图像图和文本图，并利用图神经网络（GNNs）进行整合分析以识别深度伪造。通过在空间和频率域进行多级特征提取，ViGText捕获了增强其鲁棒性和准确性的细节。", "result": "实验表明，ViGText显著增强了泛化性，在检测用户定制深度伪造时实现了显著的性能提升。具体而言，在泛化性评估中，平均F1分数从72.45%提高到98.32%，这反映了模型对未见过的、经过微调的稳定扩散模型变体的卓越泛化能力。在鲁棒性方面，ViGText的召回率比其他深度伪造检测方法提高了11.1%。面对利用其基于图的架构的定向攻击时，ViGText将分类性能下降限制在4%以内。", "conclusion": "ViGText通过详细的视觉和文本分析，为深度伪造检测设定了新标准，有助于确保媒体真实性和信息完整性。", "translation": "深度伪造技术迅速发展，产生逼真但欺诈性的数字内容，威胁着媒体的真实性。传统的深度伪造检测方法在应对复杂、定制化的深度伪造时，尤其是在泛化性和抵御恶意攻击的鲁棒性方面，常常表现不佳。本文介绍了ViGText，一种新颖的方法，它将图像与视觉大语言模型（VLLM）文本解释整合到基于图的框架中，以改进深度伪造检测。ViGText的创新之处在于它将详细解释与视觉数据集成，因为它提供了比字幕更具上下文感知能力的分析，而字幕往往缺乏特异性且未能揭示细微的不一致。ViGText系统地将图像划分为补丁，构建图像和文本图，并使用图神经网络（GNNs）进行整合分析以识别深度伪造。通过在空间和频率域使用多级特征提取，ViGText捕获了增强其鲁棒性和准确性以检测复杂深度伪造的细节。大量的实验表明，ViGText显著增强了泛化性，并在检测用户定制深度伪造时实现了显著的性能提升。具体而言，在泛化性评估中，平均F1分数从72.45%提高到98.32%，这反映了模型对未见过的、经过微调的稳定扩散模型变体的卓越泛化能力。在鲁棒性方面，ViGText的召回率比其他深度伪造检测方法提高了11.1%。面对利用其基于图的架构的定向攻击时，ViGText将分类性能下降限制在4%以内。ViGText通过详细的视觉和文本分析，为深度伪造检测设定了新标准，有助于确保媒体真实性和信息完整性。", "summary": "本文提出了ViGText，一种新颖的深度伪造图像检测方法，旨在解决传统方法在泛化性和鲁棒性方面的不足。ViGText将视觉大语言模型生成的文本解释与图像数据在图框架中整合，通过将图像划分为补丁、构建图像和文本图并利用图神经网络进行分析。它还采用跨空间和频率域的多级特征提取，以增强对复杂深度伪造的检测能力。实验证明，ViGText显著提升了泛化性（F1分数从72.45%增至98.32%）和鲁棒性（召回率提升11.1%），并能有效抵御针对性攻击，为媒体真实性保障设定了新标准。", "keywords": "深度伪造检测, 视觉-语言模型, 图神经网络, 泛化性, 鲁棒性", "comments": "这篇论文的创新点在于将视觉大语言模型生成的详细文本解释与图像数据结合，并通过图神经网络进行分析，提供了比传统字幕更具上下文感知能力的分析。这种多模态和图结构的方法显著提升了深度伪造检测在泛化性和鲁棒性方面的性能，尤其是在应对复杂和定制化深度伪造方面，为保障媒体真实性和信息完整性提供了新的有效途径。"}}
{"id": "2507.18149", "title": "Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems", "authors": ["Dongdong Zou", "Wei Wang", "Jiawen Yao", "Zhongxing Tian", "Zeyu Feng", "Huan Huang", "Fan Li", "Gordon Ning Liu", "Gangxiang Shen", "Yi Cai"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18149v1", "summary": "Probabilistic shaping (PS) has attracted significant attention in\nintensity-modulation and direct-detection (IM-DD) systems. However, due to the\nunique system model and inherent constraints, the effective application of the\nPS technique is still an open question in IM-DD systems, particularly in\nsystems with memory effects. In this paper, a novel indirect PS scheme tailored\nfor peak power constrained (PPC) IM-DD systems is proposed. The key idea lies\nin strategically controlling the signal envelope to mitigate memory-induced\nimpairments, such as nonlinearity, overshoot, peak-to-average power ratio\nenhancement, etc. The proposed scheme incorporates a dynamic selective mapping\n(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol\nmapping in which the current symbol is not only determined by the current bits\npattern but also by previously generated symbols within a specified memory\nlength. At the receiver side, a turbo equalizer with a modified M-BCJR\nalgorithm is proposed to achieve the recovery of ambiguous bits induced by\nDSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the\nproposed scheme exhibits 1dB receiver sensitivity improvement over 2km\nsingle-mode fiber transmission. In addition, the proposed scheme has also been\ndemonstrated to be compatible with the typical probabilistic amplitude shaping\narchitecture, enabling a simple and fine-granularity rate adaptation\ncapability. To the best of our knowledge, this work opens a new sight for the\napplication of the PS technique in PPC IM-DD systems with memory effects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18149v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "峰值功率受限IM-DD系统中包络控制的概率整形技术", "tldr": "本文提出了一种新颖的间接概率整形方案，通过包络控制和动态选择映射来改善峰值功率受限IM-DD系统中记忆效应引起的性能下降，并在实验中实现了1dB的接收灵敏度提升。", "motivation": "概率整形（PS）在强度调制和直接检测（IM-DD）系统中受到关注，但由于其独特的系统模型和固有限制，特别是在存在记忆效应的系统中，PS技术的有效应用仍然是一个开放问题。", "method": "本文提出了一种针对峰值功率受限（PPC）IM-DD系统的新型间接概率整形方案。核心思想是通过策略性地控制信号包络来减轻记忆效应引起的损伤，例如非线性、过冲和峰均功率比增强等。在发送端，采用动态选择映射（DSLM）机制，使当前符号不仅由当前比特模式决定，还由指定记忆长度内的先前生成符号决定。在接收端，提出了一种带有改进M-BCJR算法的涡轮均衡器，以实现DSLM引起的模糊比特的恢复。", "result": "在56GBaud PAM8系统中的实验验证表明，所提出的方案在2km单模光纤传输后，比现有方案实现了1dB的接收灵敏度提升。此外，该方案还被证明与典型的概率幅度整形架构兼容，从而实现了简单和细粒度的速率自适应能力。", "conclusion": "这项工作为概率整形技术在具有记忆效应的峰值功率受限IM-DD系统中的应用开辟了新视野。", "translation": "概率整形（PS）在强度调制和直接检测（IM-DD）系统中受到了广泛关注。然而，由于独特的系统模型和固有限制，PS技术在IM-DD系统中的有效应用仍然是一个开放问题，特别是在具有记忆效应的系统中。本文提出了一种针对峰值功率受限（PPC）IM-DD系统的新型间接PS方案。其关键思想在于策略性地控制信号包络，以减轻记忆效应引起的损伤，例如非线性、过冲、峰均功率比（PAPR）增强等。所提出的方案在发送端引入了动态选择映射（DSLM）机制，实现了非典型的比特到符号映射，其中当前符号不仅由当前比特模式决定，还由指定记忆长度内的先前生成符号决定。在接收端，提出了一种带有改进M-BCJR算法的涡轮均衡器，以实现DSLM引起的模糊比特的恢复。在56GBaud PAM8系统中的实验验证表明，所提出的方案在2km单模光纤传输后，比现有方案实现了1dB的接收灵敏度提升。此外，所提出的方案还被证明与典型的概率幅度整形架构兼容，从而实现了简单和细粒度的速率自适应能力。据我们所知，这项工作为PS技术在具有记忆效应的峰值功率受限IM-DD系统中的应用开辟了新视野。", "summary": "本文针对峰值功率受限的强度调制和直接检测（IM-DD）系统中概率整形（PS）技术在记忆效应下的应用挑战，提出了一种新颖的间接PS方案。该方案通过控制信号包络来减轻非线性等记忆效应损伤，并在发送端采用动态选择映射（DSLM），在接收端使用改进的M-BCJR算法进行恢复。实验结果显示，在56GBaud PAM8系统中，该方案实现了1dB的接收灵敏度提升，并兼容现有概率幅度整形架构，支持灵活的速率自适应。", "keywords": "概率整形, 强度调制直接检测, 包络控制, 记忆效应, 峰值功率受限", "comments": "该论文的创新点在于提出了一种新颖的间接概率整形方案，通过包络控制和动态选择映射（DSLM）来专门解决峰值功率受限IM-DD系统中记忆效应带来的挑战。这种方法不仅改善了系统性能（1dB接收灵敏度提升），还保持了与现有概率幅度整形架构的兼容性，为IM-DD系统中的高速、高效数据传输提供了新的思路。"}}
{"id": "2507.18424", "title": "Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss", "authors": ["Edward Ellis", "Robert Mendel", "Andrew Bulpitt", "Nasim Parsa", "Michael F Byrne", "Sharib Ali"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18424v1", "summary": "Acquiring and annotating large datasets in ultrasound imaging is challenging\ndue to low contrast, high noise, and susceptibility to artefacts. This process\nrequires significant time and clinical expertise. Self-supervised learning\n(SSL) offers a promising solution by leveraging unlabelled data to learn useful\nrepresentations, enabling improved segmentation performance when annotated data\nis limited. Recent state-of-the-art developments in SSL for video data include\nV-JEPA, a framework solely based on feature prediction, avoiding pixel level\nreconstruction or negative samples. We hypothesise that V-JEPA is well-suited\nto ultrasound imaging, as it is less sensitive to noisy pixel-level detail\nwhile effectively leveraging temporal information. To the best of our\nknowledge, this is the first study to adopt V-JEPA for ultrasound video data.\nSimilar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is\nwell-suited to ViT-based models. However, ViTs can underperform on small\nmedical datasets due to lack of inductive biases, limited spatial locality and\nabsence of hierarchical feature learning. To improve locality understanding, we\npropose a novel 3D localisation auxiliary task to improve locality in ViT\nrepresentations during V-JEPA pre-training. Our results show V-JEPA with our\nauxiliary task improves segmentation performance significantly across various\nfrozen encoder configurations, with gains up to 3.4\\% using 100\\% and up to\n8.35\\% using only 10\\% of the training data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18424v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于特征预测和三维局部损失的自监督超声视频分割", "tldr": "本研究通过引入新的三维局部化辅助任务，改进了基于特征预测的自监督学习V-JEPA在超声视频分割中的应用，显著提升了在有限标注数据下的分割性能。", "motivation": "超声图像数据采集和标注面临低对比度、高噪声和伪影等挑战，耗时且需要专业知识。自监督学习(SSL)通过利用未标注数据学习有用表示，为有限标注数据下的分割性能提升提供了有前景的解决方案。", "method": "本研究首次将V-JEPA（一种基于特征预测的自监督学习框架）应用于超声视频数据。为了解决ViT模型在小型医学数据集上表现不佳的问题（由于缺乏归纳偏置、空间局部性有限），作者提出了一种新颖的三维局部化辅助任务，以在V-JEPA预训练期间改善ViT表示的局部性理解。", "result": "V-JEPA结合所提出的辅助任务显著提高了各种冻结编码器配置下的分割性能，使用100%训练数据时性能提升高达3.4%，仅使用10%训练数据时性能提升高达8.35%。", "conclusion": "所提出的三维局部化辅助任务有效增强了V-JEPA在自监督超声视频分割中的性能，特别是在标注数据有限的情况下，证明了其在医学成像领域的适用性。", "translation": "在超声成像中获取和标注大型数据集由于低对比度、高噪声和易受伪影影响而具有挑战性。这个过程需要大量时间和临床专业知识。自监督学习（SSL）通过利用未标注数据学习有用的表示，提供了一种有前景的解决方案，从而在标注数据有限时提高分割性能。最近在视频数据SSL方面的最先进进展包括V-JEPA，一个完全基于特征预测的框架，避免了像素级重建或负样本。我们假设V-JEPA非常适合超声成像，因为它对嘈杂的像素级细节不那么敏感，同时有效地利用了时间信息。据我们所知，这是首次将V-JEPA应用于超声视频数据。与其他基于补丁掩蔽的SSL技术（如VideoMAE）类似，V-JEPA非常适合基于ViT的模型。然而，由于缺乏归纳偏置、有限的空间局部性以及缺乏分层特征学习，ViT在小型医学数据集上可能表现不佳。为了改善局部性理解，我们提出了一种新颖的三维局部化辅助任务，以在V-JEPA预训练期间改善ViT表示的局部性。我们的结果表明，V-JEPA与我们的辅助任务显著提高了各种冻结编码器配置下的分割性能，使用100%训练数据时增益高达3.4%，仅使用10%训练数据时增益高达8.35%。", "summary": "本文针对超声成像中带标注数据稀缺的分割挑战，提出了一种自监督学习方法。该方法将V-JEPA（一种基于特征预测的框架）应用于超声视频数据，并引入了一种新颖的三维局部化辅助任务，以在预训练期间改善基于ViT模型的局部性理解。实验结果表明，该方法显著提升了分割性能，尤其是在标注数据量较少的情况下。", "keywords": "自监督学习, 超声分割, V-JEPA, 三维局部化, 医学成像", "comments": "这项研究的创新之处在于将先进的自监督学习框架V-JEPA应用于超声视频分割，并针对ViT模型在医学图像小数据集上的局限性，创造性地引入了三维局部化辅助任务。这对于处理低对比度、高噪声的超声数据以及缓解医学领域数据标注稀缺的难题具有重要意义。"}}
{"id": "2507.17843", "title": "Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks", "authors": ["Bruno Marques da Silva", "Larissa Ferreira Rodrigues Moreira", "Flávio de Oliveira Silva", "Rodrigo Moreira"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17843v1", "summary": "The latest generation of games and pervasive communication technologies poses\nchallenges in service management and Service-Level Agreement compliance for\nmobile users. State-of-the-art edge-gaming techniques enhance throughput,\nreduce latency, and leverage cloud computing. However, further development of\ncore functions such as the User Plane Function (UPF) is needed for\nnon-intrusive user latency measurement. This paper proposes a closed-loop\narchitecture integrating the Network Data Analytics Function (NWDAF) and UPF to\nestimate user latency and enhance the 5G control plane by making it\nlatency-aware. The results show that embedding an artificial intelligence model\nwithin NWDAF enables game classification and opens new avenues for mobile edge\ngaming research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17843v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "优化超越5G网络中通过增强用户平面功能和分析的边缘游戏切片", "tldr": "本文提出一种集成NWDAF和UPF的闭环架构，用于在超越5G网络中非侵入式测量用户延迟，并通过嵌入AI模型实现游戏分类，以优化边缘游戏。", "motivation": "针对最新一代游戏和普及通信技术在移动用户服务管理和服务水平协议合规性方面带来的挑战，以及现有边缘游戏技术在用户平面功能（UPF）上进行非侵入式用户延迟测量方面的不足。", "method": "提出一个集成网络数据分析功能（NWDAF）和用户平面功能（UPF）的闭环架构，用于估算用户延迟并增强5G控制平面使其具备延迟感知能力。该方法还在NWDAF中嵌入了一个人工智能模型。", "result": "在NWDAF中嵌入人工智能模型能够实现游戏分类，并为移动边缘游戏研究开辟了新途径。", "conclusion": "通过集成NWDAF和UPF并利用AI模型，可以有效地优化超越5G网络中的边缘游戏体验，特别是通过精确的延迟测量和游戏分类。", "translation": "最新一代的游戏和普及的通信技术给移动用户的服务管理和服务水平协议合规性带来了挑战。最先进的边缘游戏技术提高了吞吐量，降低了延迟，并利用了云计算。然而，为了实现非侵入式用户延迟测量，需要进一步开发用户平面功能（UPF）等核心功能。本文提出了一种闭环架构，集成了网络数据分析功能（NWDAF）和UPF，以估算用户延迟，并通过使其具备延迟感知能力来增强5G控制平面。结果表明，在NWDAF中嵌入人工智能模型可以实现游戏分类，并为移动边缘游戏研究开辟了新途径。", "summary": "本文旨在解决移动用户在最新一代游戏和通信技术下服务管理及SLA合规性面临的挑战，特别是在非侵入式用户延迟测量方面。作者提出了一种创新的闭环架构，该架构将网络数据分析功能（NWDAF）与用户平面功能（UPF）相结合，旨在精确估算用户延迟并使5G控制平面具备延迟感知能力。研究结果表明，通过在NWDAF中集成人工智能模型，不仅能够实现有效的游戏分类，还为未来的移动边缘游戏研究奠定了基础。", "keywords": "边缘游戏, 超越5G, 用户平面功能, 网络数据分析功能, 延迟测量, 人工智能", "comments": "这篇论文通过提出一种结合NWDAF和UPF的闭环架构，并引入AI模型进行游戏分类，在优化超越5G网络中的边缘游戏体验方面表现出创新性。其重要性在于解决了当前边缘游戏在用户延迟测量和SLA合规性上的痛点，为未来低延迟、高体验的边缘游戏服务提供了新的思路和技术路径。"}}
{"id": "2507.15401", "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15401v3", "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15401v3", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "重新思考FER中的遮挡：一个语义感知视角及超越", "tldr": "ORSANet通过引入多模态语义引导、多尺度交叉交互模块和动态对抗排斥增强损失，解决了面部表情识别中遮挡和数据集偏差的挑战，并构建了首个面向遮挡的FER数据集，实现了SOTA性能。", "motivation": "面部表情识别（FER）由于普遍存在的遮挡和数据集偏差而面临挑战。特别是当面部信息部分被遮挡时，现有FER模型难以提取有效的面部特征，导致分类不准确。", "method": "本研究提出了ORSANet，包含三项关键贡献：1) 引入辅助多模态语义引导，通过语义分割图作为密集语义先验生成语义增强的面部表示，并利用面部地标作为稀疏几何先验来减轻身份和性别偏差等内在噪声。2) 定制多尺度交叉交互模块（MCM），以自适应地融合不同尺度下的地标特征和语义增强表示。3) 设计动态对抗排斥增强损失（DARELoss），动态调整模糊类别的裕度，进一步增强模型区分相似表情的能力。此外，还构建了首个面向遮挡的FER数据集Occlu-FER，用于专门分析真实世界遮挡条件下的鲁棒性。", "result": "在公共基准测试和Occlu-FER数据集上的大量实验表明，所提出的ORSANet实现了最先进（SOTA）的识别性能。", "conclusion": "ORSANet通过语义感知方法和新颖的损失函数，有效解决了面部表情识别中遮挡问题，并构建了专门的遮挡数据集以促进研究，最终取得了领先的识别性能。", "translation": "面部表情识别（FER）由于普遍存在的遮挡和数据集偏差而面临挑战。特别是当面部信息部分被遮挡时，现有FER模型难以提取有效的面部特征，导致分类不准确。为此，我们提出了ORSANet，它引入了以下三个关键贡献：首先，我们引入辅助多模态语义引导来消除面部遮挡的歧义并学习高级语义知识，这包括两个方面：1）我们引入语义分割图作为密集语义先验，以生成语义增强的面部表示；2）我们引入面部地标作为稀疏几何先验，以减轻FER中固有的噪声，例如身份和性别偏差。其次，为了促进这两种多模态先验的有效结合，我们定制了一个多尺度交叉交互模块（MCM），以自适应地融合不同尺度下的地标特征和语义增强表示。第三，我们设计了一种动态对抗排斥增强损失（DARELoss），该损失动态调整模糊类别的裕度，进一步增强模型区分相似表情的能力。我们进一步构建了第一个面向遮挡的FER数据集，以促进对各种真实世界遮挡条件的专门鲁棒性分析，该数据集命名为Occlu-FER。在公共基准测试和Occlu-FER上的大量实验表明，我们提出的ORSANet实现了最先进的识别性能。代码已在https://github.com/Wenyuzhy/ORSANet-master上公开。", "summary": "本论文提出ORSANet，旨在解决面部表情识别（FER）中因遮挡和数据集偏差导致识别不准确的问题。ORSANet通过引入多模态语义引导（结合语义分割图和面部地标）、定制多尺度交叉交互模块（MCM）以及设计动态对抗排斥增强损失（DARELoss）来增强模型在遮挡情况下的特征提取和表情区分能力。此外，论文还构建了首个面向遮挡的FER数据集Occlu-FER。实验结果表明，ORSANet在公共基准和Occlu-FER上均达到了最先进的识别性能。", "keywords": "面部表情识别, 遮挡, 语义感知, 多模态融合, 数据集", "comments": "这篇论文的创新点在于从语义感知的角度重新思考FER中的遮挡问题，并提出了一个全面的解决方案。通过结合语义分割和面部地标两种多模态先验，ORSANet能够更有效地处理遮挡和内在噪声。MCM和DARELoss的设计进一步提升了模型的特征融合能力和对相似表情的区分度。此外，构建首个面向遮挡的FER数据集Occlu-FER，对于推动该领域在真实世界应用中的鲁棒性研究具有重要意义。"}}
{"id": "2507.18157", "title": "An Improved ChaCha Algorithm Based on Quantum Random Number", "authors": ["Chao Liu", "Shuai Zhao", "Chenhao Jia", "Gengran Hu", "Tingting Cui"], "categories": ["cs.CR", "quant-ph", "Primary:94A60, Secondary:68P25, Tertiary:81P94"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      20 pages,4 figures", "url": "http://arxiv.org/abs/2507.18157v1", "summary": "Due to the merits of high efficiency and strong security against timing and\nside-channel attacks, ChaCha has been widely applied in real-time communication\nand data streaming scenarios. However, with the rapid development of\nAI-assisted cryptanalysis and quantum computing technologies, there are serious\nchallenges to the secure implementation of ChaCha cipher. To further strengthen\nthe security of ChaCha cipher, we propose an improved variant based on quantum\nrandom numbers, i.e., Quantum Random Number Enhanced ChaCha (QRE-ChaCha).\nSpecifically, the design XORs the initial constants with quantum random numbers\nand periodically injects quantum random numbers into selected state words\nduring odd rounds to enhance diffusion. Compared with the original ChaCha, the\npresent variant shows stronger resistance to differential attacks and generates\na keystream with statistical randomness, thereby offering increased robustness\nagainst both classical and quantum attacks. To evaluate the security and\nperformance of the present ChaCha, our analysis proceeds in three main parts.\nFirstly, we analyze its theoretical security in terms of quantum randomness and\nattack testing, and conduct differential cryptanalysis with an automated search\nmethod based on the Boolean satisfiability problem (SAT). Secondly, we subject\nthe keystream generated by the cipher to randomness tests using the NIST\nstatistical test suite and the GM/T 0005-2021 randomness testing standard.\nFinally, we assess its encryption and decryption performance by measuring its\nencryption speed on files of various sizes. According to the results, the\npresent ChaCha is significantly improved to resist differential attacks while\nmaintaining the high efficiency of the original ChaCha cipher, and its\nkeystream successfully passes statistical randomness tests using the NIST and\nGM/T 0005-2021 standards, meeting cryptographic application requirements.", "comment": "20 pages,4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18157v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "量子随机数改进的ChaCha算法", "tldr": "提出QRE-ChaCha，通过量子随机数增强ChaCha算法的安全性，抵抗AI辅助密码分析和量子攻击，同时保持效率。", "motivation": "鉴于AI辅助密码分析和量子计算的快速发展对ChaCha密码的安全实施构成严重挑战。", "method": "提出量子随机数增强ChaCha（QRE-ChaCha）变体，通过将初始常量与量子随机数进行异或，并在奇数轮中将量子随机数周期性注入选定的状态字以增强扩散。通过理论安全分析（量子随机性、攻击测试、基于SAT的差分密码分析）、NIST和GM/T 0005-2021标准随机性测试以及加密解密性能评估。", "result": "QRE-ChaCha对差分攻击的抵抗力更强，生成的密钥流具有统计随机性，并成功通过NIST和GM/T 0005-2021统计随机性测试，同时保持了原始ChaCha的高效率。", "conclusion": "改进后的ChaCha算法显著增强了抵抗差分攻击的能力，密钥流通过统计随机性测试，满足密码应用要求，且保持了高效率。", "translation": "由于ChaCha算法在高效性以及对抗时序和侧信道攻击方面的强大安全性，它被广泛应用于实时通信和数据流场景。然而，随着AI辅助密码分析和量子计算技术的快速发展，ChaCha密码的安全实现面临严峻挑战。为了进一步增强ChaCha密码的安全性，我们提出了一种基于量子随机数的改进变体，即量子随机数增强ChaCha（QRE-ChaCha）。具体来说，该设计将初始常量与量子随机数进行异或，并在奇数轮中周期性地将量子随机数注入选定的状态字以增强扩散。与原始ChaCha相比，本变体对差分攻击显示出更强的抵抗力，并生成具有统计随机性的密钥流，从而增强了对经典和量子攻击的鲁棒性。为了评估本ChaCha的安全性与性能，我们的分析分为三个主要部分。首先，我们从量子随机性和攻击测试方面分析其理论安全性，并利用基于布尔可满足性问题（SAT）的自动化搜索方法进行差分密码分析。其次，我们使用NIST统计测试套件和GM/T 0005-2021随机性测试标准对该密码生成的密钥流进行随机性测试。最后，我们通过测量其在各种大小文件上的加密速度来评估其加密和解密性能。结果表明，本ChaCha在保持原始ChaCha密码高效率的同时，显著提高了抵抗差分攻击的能力，其密钥流成功通过了NIST和GM/T 0005-2021标准的统计随机性测试，满足密码应用要求。", "summary": "本文提出了一种基于量子随机数增强的ChaCha算法（QRE-ChaCha），旨在应对AI辅助密码分析和量子计算对ChaCha安全性的挑战。通过在ChaCha设计中引入量子随机数，QRE-ChaCha显著增强了对差分攻击的抵抗力，并生成了统计随机的密钥流。实验结果表明，QRE-ChaCha在保持原始ChaCha高效率的同时，提升了安全性，满足了密码应用需求。", "keywords": "ChaCha, 量子随机数, 密码学, 差分攻击, 随机性测试", "comments": "这项研究通过引入量子随机数，为ChaCha算法的安全性提供了创新性的增强方案，尤其是在面对新兴的AI辅助密码分析和量子攻击方面。其通过具体的设计修改（异或初始常量、周期性注入量子随机数）来增强扩散和抵抗差分攻击，并进行了全面的安全性与性能评估，具有重要的实践意义。"}}
{"id": "2507.17756", "title": "Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy", "authors": ["Josh Hunter", "John McDermid", "Simon Burton"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, published in European Dependable Computing Conference 2025", "url": "http://arxiv.org/abs/2507.17756v1", "summary": "This study investigates how railway professionals perceive safety as a\nconcept within rail, with the intention to help inform future technological\ndevelopments within the industry. Through a series of interviews with drivers,\nroute planners,and administrative personnel, the research explores the\ncurrentstate of safety practices, the potential for automation and the\nunderstanding of the railway as a system of systems. Key findings highlight a\ncautious attitude towards automation, a preference for assistive technologies,\nand a complex understanding of safety that integrates human, systematic and\ntechnological factors. The study also addresses the limitations of transferring\nautomotive automation technologies to railways and the need for a\nrailway-specific causation model to better evaluate and enhance safety in an\nevolving technological landscape. This study aims to bridge thegap between\ncontemporary research and practical applications, contributing to the\ndevelopment of more effective safety metrics.", "comment": "9 pages, 3 figures, published in European Dependable Computing\n  Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.17756v1", "cate": "cs.HC", "date": "2025-05-06", "updated": "2025-05-06", "AI": {"title_translation": "铁路专业人士的见解：重新思考铁路关于安全和自动化的假设", "tldr": "本研究通过访谈铁路专业人士，探讨了他们对铁路安全的认知、对自动化的态度，并强调了开发铁路特定安全模型的重要性。", "motivation": "本研究旨在了解铁路专业人员如何看待铁路领域的安全概念，以期为该行业未来的技术发展提供信息。", "method": "通过对司机、路线规划员和行政人员进行一系列访谈。", "result": "主要发现包括对自动化持谨慎态度、偏好辅助技术，以及对安全有着整合了人为、系统和技术因素的复杂理解。研究还指出了将汽车自动化技术转移到铁路的局限性，以及需要一个铁路特定的因果模型来更好地评估和提升不断发展的技术环境中的安全。", "conclusion": "本研究旨在弥合当代研究与实际应用之间的差距，有助于开发更有效的安全指标。", "translation": "本研究旨在调查铁路专业人员如何看待铁路领域的安全概念，以期为该行业未来的技术发展提供信息。通过对司机、路线规划员和行政人员进行一系列访谈，本研究探讨了当前的安全实践状况、自动化的潜力以及对铁路作为系统之系统的理解。主要发现强调了对自动化持谨慎态度、偏好辅助技术，以及对安全有着整合了人为、系统和技术因素的复杂理解。本研究还讨论了将汽车自动化技术转移到铁路的局限性，以及需要一个铁路特定的因果模型来更好地评估和提升不断发展的技术环境中的安全。本研究旨在弥合当代研究与实际应用之间的差距，有助于开发更有效的安全指标。", "summary": "本研究通过访谈铁路专业人士，深入探讨了他们对铁路安全的认知、自动化技术的看法以及铁路作为复杂系统的理解。研究发现，铁路专业人士对自动化持谨慎态度，更倾向于辅助技术，并对安全有着多维度、整合人、系统和技术因素的复杂理解。论文强调了汽车自动化技术在铁路领域应用的局限性，并提出了开发铁路专用安全因果模型的必要性，以期为未来铁路技术发展和安全指标的优化提供实践指导。", "keywords": "铁路安全, 自动化, 铁路专业人士, 安全感知, 因果模型", "comments": "该研究通过直接访谈铁路专业人员，提供了宝贵的实践视角，弥补了理论研究与实际应用之间的鸿沟。其创新之处在于强调了铁路安全问题的独特性，并指出不能简单地将汽车领域的自动化技术移植到铁路，这对于未来铁路自动化和安全系统设计具有重要指导意义。研究结果揭示了铁路专业人士对安全和自动化的复杂理解，突出了以人为本和系统性思考的重要性。"}}
{"id": "2406.17929", "title": "Asymptotically Minimax Regret by Bayes Mixtures", "authors": ["Jun'ichi Takeuchi", "Andrew R. Barron"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.17929v2", "summary": "We study the problems of data compression, gambling and prediction of a\nsequence $x^n=x_1x_2...x_n$ from an alphabet ${\\cal X}$, in terms of regret and\nexpected regret (redundancy) with respect to various smooth families of\nprobability distributions. We evaluate the regret of Bayes mixture\ndistributions compared to maximum likelihood, under the condition that the\nmaximum likelihood estimate is in the interior of the parameter space. For\ngeneral exponential families (including the non-i.i.d.\\ case) the\nasymptotically mimimax value is achieved when variants of the prior of Jeffreys\nare used. %under the condition that the maximum likelihood estimate is in the\ninterior of the parameter space. Interestingly, we also obtain a modification\nof Jeffreys prior which has measure outside the given family of densities, to\nachieve minimax regret with respect to non-exponential type families. This\nmodification enlarges the family using local exponential tilting (a fiber\nbundle). Our conditions are confirmed for certain non-exponential families,\nincluding curved families and mixture families (where either the mixture\ncomponents or their weights of combination are parameterized) as well as\ncontamination models. Furthermore for mixture families we show how to deal with\nthe full simplex of parameters. These results also provide characterization of\nRissanen's stochastic complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.17929v2", "cate": "cs.IT", "date": "2024-06-25", "updated": "2025-07-24", "AI": {"title_translation": "渐近最小最大遗憾的贝叶斯混合", "tldr": "本文研究了数据压缩、赌博和序列预测问题，通过使用贝叶斯混合分布，特别是杰弗里斯先验的变体和修改，在各种平滑概率分布族下，实现了渐近最小最大遗憾。", "motivation": "该研究旨在评估贝叶斯混合分布相对于最大似然估计的遗憾值和期望遗憾值（冗余），以解决数据压缩、赌博和序列预测问题，涉及各种平滑概率分布族。", "method": "研究方法是评估贝叶斯混合分布与最大似然估计的遗憾值。对于一般指数族，通过使用杰弗里斯先验的变体来达到渐近最小最大值。对于非指数族，通过修改杰弗里斯先验并使用局部指数倾斜（纤维丛）来扩大分布族，从而实现最小最大遗憾。", "result": "结果表明，对于一般指数族，使用杰弗里斯先验的变体可以实现渐近最小最大值。此外，通过修改杰弗里斯先验，即使在给定密度族之外，也能对非指数类型族实现最小最大遗憾。这些条件已在曲线族、混合族和污染模型等非指数族中得到证实。对于混合族，还展示了如何处理完整的参数单纯形。这些结果也表征了里萨宁的随机复杂度。", "conclusion": "论文得出结论，通过使用贝叶斯混合分布，特别是杰弗里斯先验的变体和修改，可以在各种平滑概率分布族（包括指数族和非指数族）下实现渐近最小最大遗憾，并为里萨宁的随机复杂度提供了表征。", "translation": "我们研究了数据压缩、赌博和序列 $x^n=x_1x_2...x_n$（来自字母表 ${\\cal X}$）预测问题，这些问题以遗憾和期望遗憾（冗余）的形式，针对各种平滑概率分布族进行。在最大似然估计位于参数空间内部的条件下，我们评估了贝叶斯混合分布与最大似然相比的遗憾。对于一般指数族（包括非独立同分布情况），当使用杰弗里斯先验的变体时，可以达到渐近最小最大值。有趣的是，我们还获得了杰弗里斯先验的一种修改，它具有在给定密度族之外的度量，以实现对非指数类型族的最小最大遗憾。这种修改通过局部指数倾斜（纤维丛）扩大了族。我们的条件在某些非指数族中得到了证实，包括曲线族和混合族（其中混合成分或其组合权重是参数化的）以及污染模型。此外，对于混合族，我们展示了如何处理完整的参数单纯形。这些结果也表征了里萨宁的随机复杂度。", "summary": "本文研究了数据压缩、赌博和序列预测中的遗憾和冗余问题，通过评估贝叶斯混合分布与最大似然估计的遗憾。研究发现，对于一般指数族，使用杰弗里斯先验的变体可以实现渐近最小最大遗憾。更进一步，论文提出了一种修改后的杰弗里斯先验，通过局部指数倾斜扩展了分布族，从而能够在非指数族（如曲线族和混合族）中实现最小最大遗憾。这些结果还提供了对里萨宁随机复杂度的表征。", "keywords": "贝叶斯混合, 最小最大遗憾, 杰弗里斯先验, 指数族, 随机复杂度", "comments": "本文的创新之处在于，它不仅证明了杰弗里斯先验的变体在一般指数族中实现渐近最小最大遗憾的有效性，更重要的是，它提出了一种巧妙的修改方法，通过局部指数倾斜将杰弗里斯先验的应用范围扩展到非指数族。这显著拓宽了贝叶斯混合理论在更广泛概率模型中的应用潜力，并且其结果与里萨宁的随机复杂度建立了联系，具有重要的理论意义。"}}
{"id": "2507.17800", "title": "Improving Multislice Electron Ptychography with a Generative Prior", "authors": ["Christian K. Belardi", "Chia-Hao Lee", "Yingheng Wang", "Justin Lovelace", "Kilian Q. Weinberger", "David A. Muller", "Carla P. Gomes"], "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV", "physics.optics"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      16 pages, 10 figures, 5 tables", "url": "http://arxiv.org/abs/2507.17800v1", "summary": "Multislice electron ptychography (MEP) is an inverse imaging technique that\ncomputationally reconstructs the highest-resolution images of atomic crystal\nstructures from diffraction patterns. Available algorithms often solve this\ninverse problem iteratively but are both time consuming and produce suboptimal\nsolutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion\nmodel trained on a large database of crystal structures specifically for MEP to\naugment existing iterative solvers. MEP-Diffusion is easily integrated as a\ngenerative prior into existing reconstruction methods via Diffusion Posterior\nSampling (DPS). We find that this hybrid approach greatly enhances the quality\nof the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over\nexisting methods.", "comment": "16 pages, 10 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.17800v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "使用生成式先验改进多层电子叠层衍射成像", "tldr": "开发了一种名为MEP-Diffusion的扩散模型，作为生成式先验集成到现有多层电子叠层衍射成像(MEP)算法中，显著提高了3D体积重建质量，SSIM提高了90.50%。", "motivation": "多层电子叠层衍射成像 (MEP) 是一种计算重建原子晶体结构高分辨率图像的逆成像技术。然而，现有的迭代算法耗时且由于病态性质而产生次优解。", "method": "本文开发了MEP-Diffusion，一个专门为MEP训练的扩散模型，该模型在一个大型晶体结构数据库上进行训练。MEP-Diffusion通过扩散后采样 (DPS) 轻松地作为生成式先验集成到现有的重建方法中。", "result": "该混合方法极大地提升了重建3D体积的质量，相较于现有方法，SSIM（结构相似性指数）提高了90.50%。", "conclusion": "将基于扩散模型的生成式先验（MEP-Diffusion）集成到多层电子叠层衍射成像中，可以有效克服传统迭代算法的局限性，显著提升图像重建的质量。", "translation": "多层电子叠层衍射成像（MEP）是一种逆成像技术，它通过计算从衍射图样中重建原子晶体结构的最高分辨率图像。现有的算法通常迭代地解决这个逆问题，但由于其病态性质，既耗时又产生次优解。我们开发了MEP-Diffusion，这是一个专门为MEP训练的扩散模型，该模型在一个大型晶体结构数据库上进行训练，以增强现有的迭代求解器。MEP-Diffusion可以通过扩散后采样（DPS）轻松地作为生成式先验集成到现有的重建方法中。我们发现这种混合方法极大地提升了重建3D体积的质量，相较于现有方法，SSIM提高了90.50%。", "summary": "本文针对多层电子叠层衍射成像（MEP）中现有迭代算法耗时且重建质量次优的问题，提出了一种名为MEP-Diffusion的扩散模型。该模型在大型晶体结构数据库上训练，并可作为生成式先验，通过扩散后采样（DPS）轻松集成到现有MEP重建方法中。实验结果表明，这种混合方法显著提高了重建3D体积的质量，SSIM相较现有方法提升了90.50%。", "keywords": "多层电子叠层衍射成像, 扩散模型, 生成式先验, 图像重建, SSIM", "comments": "这项工作通过引入生成式先验，特别是扩散模型，为解决电子叠层衍射成像中的病态逆问题提供了一个有前景的方向。将深度学习模型与传统迭代求解器相结合的混合方法，在提高重建质量和效率方面具有创新性。其显著的SSIM提升表明了该方法的有效性。"}}
{"id": "2505.09193", "title": "BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression", "authors": ["Wei Jiang", "Junru Li", "Kai Zhang", "Li Zhang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted to ACMMM 2025", "url": "http://arxiv.org/abs/2505.09193v4", "summary": "Recent forward prediction-based learned video compression (LVC) methods have\nachieved impressive results, even surpassing VVC reference software VTM under\nthe Low Delay B (LDB) configuration. In contrast, learned bidirectional video\ncompression (BVC) remains underexplored and still lags behind its forward-only\ncounterparts. This performance gap is mainly due to the limited ability to\nextract diverse and accurate contexts: most existing BVCs primarily exploit\ntemporal motion while neglecting non-local correlations across frames.\nMoreover, they lack the adaptability to dynamically suppress harmful contexts\narising from fast motion or occlusion. To tackle these challenges, we propose\nBiECVC, a BVC framework that incorporates diversified local and non-local\ncontext modeling along with adaptive context gating. For local context\nenhancement, BiECVC reuses high-quality features from lower layers and aligns\nthem using decoded motion vectors without introducing extra motion overhead. To\nmodel non-local dependencies efficiently, we adopt a linear attention mechanism\nthat balances performance and complexity. To further mitigate the impact of\ninaccurate context prediction, we introduce Bidirectional Context Gating,\ninspired by data-dependent decay in recent autoregressive language models, to\ndynamically filter contextual information based on conditional coding results.\nExtensive experiments demonstrate that BiECVC achieves state-of-the-art\nperformance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2\nunder the Random Access (RA) configuration with intra periods of 32 and 64,\nrespectively. To our knowledge, BiECVC is the first learned video codec to\nsurpass VTM 13.2 RA across all standard test datasets.", "comment": "Accepted to ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2505.09193v4", "cate": "eess.IV", "date": "2025-05-14", "updated": "2025-07-24", "AI": {"title_translation": "BiECVC：用于学习视频压缩的双向上下文门控多样化", "tldr": "BiECVC是首个在标准测试数据集上超越VTM 13.2 RA的学习型视频编解码器，通过多样化的局部和非局部上下文建模以及自适应上下文门控，解决了现有双向学习视频压缩在上下文提取和适应性方面的不足。", "motivation": "现有基于前向预测的学习型视频压缩方法已取得显著成果，但学习型双向视频压缩（BVC）仍未得到充分探索，且落后于前向预测方法。这种性能差距主要源于提取多样化和准确上下文的能力有限，大多数现有BVC主要利用时间运动而忽略帧间的非局部相关性。此外，它们缺乏动态抑制由快速运动或遮挡引起的有害上下文的适应性。", "method": "我们提出了BiECVC，一个结合多样化局部和非局部上下文建模以及自适应上下文门控的BVC框架。为增强局部上下文，BiECVC重用下层高质量特征，并使用解码运动向量进行对齐，无需额外运动开销。为高效建模非局部依赖，我们采用线性注意力机制平衡性能和复杂性。为进一步减轻不准确上下文预测的影响，我们引入了双向上下文门控，灵感来源于近期自回归语言模型中的数据依赖衰减，根据条件编码结果动态过滤上下文信息。", "result": "BiECVC实现了最先进的性能，在帧内周期为32和64的随机访问（RA）配置下，比特率分别比VTM 13.2降低了13.4%和15.7%。据我们所知，BiECVC是第一个在所有标准测试数据集上超越VTM 13.2 RA的学习型视频编解码器。", "conclusion": "BiECVC通过创新的上下文建模和门控机制，显著提升了双向学习视频压缩的性能，首次在所有标准测试数据集上超越了VTM 13.2 RA。", "translation": "最近基于前向预测的学习型视频压缩（LVC）方法取得了令人印象深刻的成果，甚至在低延迟B（LDB）配置下超越了VVC参考软件VTM。相比之下，学习型双向视频压缩（BVC）仍未得到充分探索，并且仍然落后于其仅前向的对应方法。这种性能差距主要是由于提取多样化和准确上下文的能力有限：大多数现有BVC主要利用时间运动，而忽略了帧间的非局部相关性。此外，它们缺乏动态抑制由快速运动或遮挡引起的有害上下文的适应性。为了解决这些挑战，我们提出了BiECVC，一个结合多样化局部和非局部上下文建模以及自适应上下文门控的BVC框架。为了增强局部上下文，BiECVC重用下层高质量特征，并使用解码运动向量进行对齐，而不引入额外的运动开销。为了高效建模非局部依赖，我们采用线性注意力机制，平衡性能和复杂性。为了进一步减轻不准确上下文预测的影响，我们引入了双向上下文门控，灵感来源于近期自回归语言模型中的数据依赖衰减，以根据条件编码结果动态过滤上下文信息。大量实验表明，BiECVC实现了最先进的性能，在帧内周期为32和64的随机访问（RA）配置下，比特率分别比VTM 13.2降低了13.4%和15.7%。据我们所知，BiECVC是第一个在所有标准测试数据集上超越VTM 13.2 RA的学习型视频编解码器。", "summary": "本文提出了BiECVC，一种双向学习视频压缩（BVC）框架，旨在解决现有BVC在上下文提取和适应性方面的不足。BiECVC通过引入多样化的局部和非局部上下文建模以及自适应上下文门控机制来提升性能。具体而言，它利用下层高质量特征进行局部上下文增强，采用线性注意力机制高效建模非局部依赖，并引入双向上下文门控动态过滤不准确的上下文信息。实验证明，BiECVC在比特率方面优于VTM 13.2，并且是首个在所有标准测试数据集上超越VTM 13.2 RA的学习型视频编解码器。", "keywords": "视频压缩, 双向上下文, 深度学习, 上下文门控, 非局部相关性", "comments": "BiECVC的创新之处在于其对双向上下文建模的全面优化，特别是在引入非局部相关性和自适应上下文门控方面。通过结合局部特征重用、线性注意力机制和受语言模型启发的门控机制，该方法有效地解决了现有BVC的局限性。其超越VTM 13.2 RA的成果，标志着学习型视频压缩领域的一个重要里程碑。"}}
{"id": "1909.03820", "title": "Learning Concepts Definable in First-Order Logic with Counting", "authors": ["Steffen van Bergerem"], "categories": ["cs.LO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/1909.03820v5", "summary": "We study Boolean classification problems over relational background\nstructures in the logical framework introduced by Grohe and Tur\\'an (TOCS\n2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in\nfirst-order logic over structures of polylogarithmic degree can be learned in\nsublinear time, where the degree of the structure and the running time are\nmeasured in terms of the size of the structure. We generalise the results to\nthe first-order logic with counting FOCN, which was introduced by Kuske and\nSchweikardt (LICS 2017) as an expressive logic generalising various other\ncounting logics. Specifically, we prove that classifiers definable in FOCN over\nclasses of structures of polylogarithmic degree can be consistently learned in\nsublinear time. This can be seen as a first step towards extending the learning\nframework to include numerical aspects of machine learning. We extend the\nresult to agnostic probably approximately correct (PAC) learning for classes of\nstructures of degree at most $(\\log \\log n)^c$ for some constant $c$. Moreover,\nwe show that bounding the degree is crucial to obtain sublinear-time learning\nalgorithms. That is, we prove that, for structures of unbounded degree,\nlearning is not possible in sublinear time, even for classifiers definable in\nplain first-order logic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/1909.03820v5", "cate": "cs.LO", "date": "2019-09-09", "updated": "2025-07-24", "AI": {"title_translation": "学习可数一阶逻辑中可定义的概", "tldr": "本文研究了在具有多对数度数的关系背景结构上，使用可数一阶逻辑（FOCN）可定义的分类器，可以在亚线性时间内一致地学习，并将其扩展到不可知PAC学习；同时证明了度数限制对于亚线性时间学习算法的重要性。", "motivation": "本文旨在将一阶逻辑中可定义分类器的学习结果推广到更具表达性的可数一阶逻辑（FOCN），并将学习框架扩展到包含机器学习的数值方面。同时，它还探讨了度数边界的重要性。", "method": "作者证明了在多对数度数结构类上可定义的FOCN分类器可以在亚线性时间内一致地学习。他们将这一结果扩展到度数至多为$(\\log \\log n)^c$（c为常数）的结构类的不可知概率近似正确（PAC）学习。此外，他们还证明了在无界度数结构上，即使对于普通一阶逻辑中可定义的分类器，也无法实现亚线性时间学习。", "result": "1. 在多对数度数结构上可定义的FOCN分类器可以在亚线性时间内一致地学习。\n2. 这一结果可以扩展到度数至多为$(\\log \\log n)^c$的结构类的不可知PAC学习。\n3. 限制结构度数对于获得亚线性时间学习算法至关重要；无界度数结构即使对于普通一阶逻辑中可定义的分类器也无法在亚线性时间内学习。", "conclusion": "本文成功地将可学习性结果扩展到可数一阶逻辑（FOCN），并强调了度数边界对于在该逻辑框架中实现高效学习算法的关键作用。", "translation": "我们研究了Grohe和Turán（TOCS 2004）引入的逻辑框架中关系背景结构上的布尔分类问题。已知（Grohe和Ritzert，LICS 2017）在多对数度数结构上可定义的一阶逻辑分类器可以在亚线性时间内学习，其中结构的度数和运行时间是根据结构的大小来衡量的。我们将这些结果推广到可数一阶逻辑FOCN，该逻辑由Kuske和Schweikardt（LICS 2017）引入，作为一种概括各种其他计数逻辑的表达性逻辑。具体来说，我们证明了在多对数度数结构类上可定义的FOCN分类器可以在亚线性时间内一致地学习。这可以被视为将学习框架扩展到包含机器学习数值方面的第一步。我们将结果扩展到度数至多为$(\\log \\log n)^c$（其中c为某个常数）的结构类的不可知概率近似正确（PAC）学习。此外，我们表明限制度数对于获得亚线性时间学习算法至关重要。也就是说，我们证明，对于无界度数结构，即使对于普通一阶逻辑中可定义的分类器，也无法在亚线性时间内学习。", "summary": "本文将在关系背景结构上可定义的一阶逻辑分类器的可学习性推广到可数一阶逻辑（FOCN）。研究表明，FOCN分类器在多对数度数结构上可以在亚线性时间内一致地学习，并且该结果可扩展到度数更小的结构的不可知PAC学习。重要的是，本文证明了限制结构度数对于实现亚线性时间学习至关重要，因为无界度数结构无法实现此类高效学习。", "keywords": "可数一阶逻辑, 可学习性, 亚线性时间, 关系结构, PAC学习", "comments": "本文通过将可学习性结果从基本一阶逻辑扩展到更具表达性的可数一阶逻辑（FOCN），做出了重要的理论贡献。明确证明度数边界对于亚线性时间学习的必要性是一项重要发现，为该逻辑学习领域的未来研究提供了清晰的限制和指导。"}}
{"id": "2507.15504", "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "authors": ["Bingqing Zhang", "Zhuo Cao", "Heming Du", "Yang Li", "Xue Li", "Jiajun Liu", "Sen Wang"], "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.15504v2", "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.15504v2", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "量化和缩小未知：基于不确定性最小化的交互式文本到视频检索", "tldr": "UMIVR是一个通过量化文本歧义、映射不确定性和帧不确定性来最小化不确定性的交互式文本到视频检索框架，并通过自适应地生成澄清问题来细化用户查询，从而显著提高检索性能。", "motivation": "尽管文本到视频检索（TVR）取得了进展，但仍受到文本查询模糊、文本-视频映射不明确和视频帧质量低下等固有不确定性的阻碍。现有的交互式系统通常依赖启发式或临时策略，没有明确量化这些不确定性，从而限制了其有效性。本文旨在通过明确量化并最小化这些不确定性来解决这一问题。", "method": "本文提出了UMIVR，一个不确定性最小化交互式文本到视频检索框架。UMIVR通过原则性的、无需训练的度量标准明确量化了三个关键不确定性：语义熵（semantic entropy）的文本歧义分数（TAS）、基于Jensen-Shannon散度（Jensen-Shannon divergence）的映射不确定性分数（MUS），以及基于时间质量（Temporal Quality）的帧采样器（TQFS）。UMIVR利用这些不确定性度量自适应地生成有针对性的澄清问题，迭代地细化用户查询，以显著减少检索歧义。", "result": "UMIVR在多个基准测试上进行了广泛实验，验证了其有效性。在MSR-VTT-1k数据集上，经过10轮交互后，召回率@1（Recall@1）达到了69.2%的显著提升。", "conclusion": "本文为交互式文本到视频检索奠定了不确定性最小化的基础，并通过明确量化并减少多种不确定性，显著提高了检索性能。", "translation": "尽管最近取得了进展，但文本到视频检索（TVR）仍然受到多种固有不确定性的阻碍，例如模糊的文本查询、不明确的文本-视频映射和低质量的视频帧。尽管交互式系统已经出现，通过澄清问题来细化用户意图以解决这些挑战，但当前的方法通常依赖于启发式或临时策略，没有明确量化这些不确定性，从而限制了它们的有效性。受此差距的启发，我们提出了UMIVR，一个不确定性最小化交互式文本到视频检索框架，它通过原则性的、无需训练的度量标准明确量化了三个关键不确定性——文本歧义、映射不确定性和帧不确定性：基于语义熵的文本歧义分数（TAS）、基于Jensen-Shannon散度的映射不确定性分数（MUS），以及基于时间质量的帧采样器（TQFS）。通过在这些不确定性度量的指导下自适应地生成有针对性的澄清问题，UMIVR迭代地细化用户查询，显著减少了检索歧义。在多个基准测试上进行的广泛实验验证了UMIVR的有效性，在MSR-VTT-1k数据集上，经过10轮交互后，召回率@1（Recall@1）取得了显著的提升（69.2%），从而为交互式TVR奠定了不确定性最小化的基础。", "summary": "本文提出UMIVR，一个创新的交互式文本到视频检索框架，旨在解决现有TVR系统中固有的不确定性，包括文本歧义、映射不确定性和帧质量问题。UMIVR通过引入无需训练的度量（TAS、MUS和TQFS）来量化这些不确定性，并利用这些度量自适应地生成澄清问题以迭代优化用户查询。实验证明，UMIVR在提高检索精度方面表现出色，特别是在MSR-VTT-1k数据集上实现了显著的召回率提升，为交互式TVR建立了不确定性最小化的新范式。", "keywords": "文本到视频检索, 不确定性最小化, 交互式检索, 量化不确定性, 查询细化", "comments": "这篇论文的创新点在于首次明确量化并系统性地处理了文本到视频检索中的多种不确定性，而非依赖启发式方法。通过引入基于不确定性度量的自适应提问机制，极大地提升了交互式检索的效率和准确性。其无训练的度量方法也增加了实际应用的灵活性和可解释性。这项工作为未来的交互式检索系统提供了一个坚实的新范式。"}}
{"id": "2507.18215", "title": "Information Security Based on LLM Approaches: A Review", "authors": ["Chang Gong", "Zhongwen Li", "Xiaoqi Li"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18215v1", "summary": "Information security is facing increasingly severe challenges, and\ntraditional protection means are difficult to cope with complex and changing\nthreats. In recent years, as an emerging intelligent technology, large language\nmodels (LLMs) have shown a broad application prospect in the field of\ninformation security. In this paper, we focus on the key role of LLM in\ninformation security, systematically review its application progress in\nmalicious behavior prediction, network threat analysis, system vulnerability\ndetection, malicious code identification, and cryptographic algorithm\noptimization, and explore its potential in enhancing security protection\nperformance. Based on neural networks and Transformer architecture, this paper\nanalyzes the technical basis of large language models and their advantages in\nnatural language processing tasks. It is shown that the introduction of large\nlanguage modeling helps to improve the detection accuracy and reduce the false\nalarm rate of security systems. Finally, this paper summarizes the current\napplication results and points out that it still faces challenges in model\ntransparency, interpretability, and scene adaptability, among other issues. It\nis necessary to explore further the optimization of the model structure and the\nimprovement of the generalization ability to realize a more intelligent and\naccurate information security protection system.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18215v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于大语言模型的信息安全方法：综述", "tldr": "本文综述了大语言模型（LLM）在信息安全领域的应用进展，包括恶意行为预测、网络威胁分析、漏洞检测等，并指出其在提高检测准确率和降低误报率方面的优势，同时也面临模型透明度、可解释性等挑战。", "motivation": "信息安全面临日益严峻的挑战，传统防护手段难以应对复杂多变的威胁。大语言模型作为新兴智能技术，在信息安全领域展现出广阔的应用前景。", "method": "本文系统性综述了大语言模型在信息安全中的关键作用和应用进展，包括恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化。同时，分析了大语言模型基于神经网络和Transformer架构的技术基础及其在自然语言处理任务中的优势。", "result": "大语言模型的引入有助于提高安全系统的检测准确率并降低误报率，在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化等多个信息安全应用场景中展现出潜力。", "conclusion": "大语言模型在信息安全领域应用广泛，但仍面临模型透明度、可解释性和场景适应性等挑战。未来需进一步优化模型结构，提高泛化能力，以实现更智能、更精准的信息安全防护系统。", "translation": "信息安全正面临日益严峻的挑战，传统防护手段难以应对复杂多变的威胁。近年来，作为一种新兴的智能技术，大语言模型（LLMs）在信息安全领域展现出广阔的应用前景。本文重点关注大语言模型在信息安全中的关键作用，系统性地综述了其在恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化方面的应用进展，并探讨了其在增强安全防护性能方面的潜力。本文基于神经网络和Transformer架构，分析了大语言模型的技术基础及其在自然语言处理任务中的优势。研究表明，大语言模型的引入有助于提高安全系统的检测准确率并降低误报率。最后，本文总结了当前的应用成果，并指出其在模型透明度、可解释性和场景适应性等方面仍面临挑战。有必要进一步探索模型结构的优化和泛化能力的提高，以实现更智能、更准确的信息安全防护系统。", "summary": "本文综述了大语言模型（LLM）在信息安全领域的应用，涵盖了恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化等多个方面。研究指出，LLM能够提高安全系统的检测准确率和降低误报率，但其透明度、可解释性和场景适应性仍是未来需要解决的挑战。", "keywords": "大语言模型, 信息安全, 综述, 威胁分析, 漏洞检测", "comments": "本文作为一篇综述性文章，系统梳理了大语言模型在信息安全领域的应用现状和潜力，为研究人员提供了全面的视角。其重要性在于指出了LLM在应对传统安全挑战方面的优势，同时也清醒地认识到当前存在的局限性，为未来的研究方向提供了指引。"}}
{"id": "2507.18581", "title": "PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation", "authors": ["Ravan Nazaraliyev", "Saber Ganjisaffar", "Nurlan Nazaraliyev", "Nael Abu-Ghazaleh"], "categories": ["cs.AR", "cs.ET"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18581v1", "summary": "As DRAM density increases, Rowhammer becomes more severe due to heightened\ncharge leakage, reducing the number of activations needed to induce bit flips.\nThe DDR5 standard addresses this threat with in-DRAM per-row activation\ncounters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.\nHowever, PRAC adds performance overhead by incrementing counters during the\nprecharge phase, and recovery refreshes stalls the entire memory channel, even\nif only one bank is under attack.\n  We propose PRACtical, a performance-optimized approach to PRAC+ABO that\nmaintains the same security guarantees. First, we reduce counter update latency\nby introducing a centralized increment circuit, enabling overlap between\ncounter updates and subsequent row activations in other subarrays. Second, we\nenhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead\nof stalling the entire channel, only affected banks are paused. This is\nachieved through a DRAM-resident register that identifies attacked banks.\n  PRACtical improves performance by 8% on average (up to 20%) over the\nstate-of-the-art, reduces energy by 19%, and limits performance degradation\nfrom aggressive performance attacks to less than 6%, all while preserving\nRowhammer protection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18581v1", "cate": "cs.AR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PRACtical：用于高效PRAC Rowhammer缓解的子阵列级计数器更新和银行级恢复隔离", "tldr": "PRACtical 是一种优化 PRAC+ABO Rowhammer 缓解方案，通过子阵列级计数器更新和银行级恢复隔离，显著提升性能和能效，同时保持安全性。", "motivation": "随着DRAM密度增加，Rowhammer攻击日益严重。DDR5标准引入的PRAC+ABO机制虽然能缓解攻击，但存在性能开销（计数器更新延迟）和效率问题（恢复刷新会阻塞整个内存通道，即使只有部分银行受攻击）。", "method": "本文提出了PRACtical，一种性能优化的PRAC+ABO方法。主要通过两点实现：1. 引入集中式增量电路，减少计数器更新延迟，并允许计数器更新与其它子阵列的行激活重叠。2. 增强RFMab缓解机制，实现银行级粒度恢复，仅暂停受攻击的银行而非整个通道，通过DRAM驻留寄存器识别受攻击银行。", "result": "PRACtical 相较于现有技术，平均性能提升8%（最高达20%），能耗降低19%，将激进性能攻击导致的性能下降限制在6%以下，同时保持Rowhammer保护。", "conclusion": "PRACtical 成功地在保持Rowhammer保护的同时，显著提升了DDR5 PRAC+ABO缓解机制的性能和能效，解决了现有方案的瓶颈。", "translation": "随着DRAM密度的增加，Rowhammer由于电荷泄漏加剧而变得更加严重，减少了诱导位翻转所需的激活次数。DDR5标准通过DRAM内置的每行激活计数器（PRAC）和警报回退（ABO）信号来触发缓解措施，从而应对这一威胁。然而，PRAC在预充电阶段增加计数器会带来性能开销，并且恢复刷新会阻塞整个内存通道，即使只有一个银行受到攻击。\n我们提出了PRACtical，一种性能优化的PRAC+ABO方法，它保持了相同的安全保证。首先，我们通过引入一个集中式增量电路来减少计数器更新延迟，从而实现计数器更新与其它子阵列后续行激活之间的重叠。其次，我们通过启用银行级粒度来增强RFMab缓解：不再阻塞整个通道，而是只暂停受影响的银行。这是通过一个DRAM驻留寄存器来实现的，该寄存器用于识别受攻击的银行。\nPRACtical 相较于现有技术，平均性能提升8%（最高达20%），能耗降低19%，并将激进性能攻击导致的性能下降限制在6%以下，同时保留了Rowhammer保护。", "summary": "PRACtical 是一种针对DDR5 PRAC+ABO Rowhammer缓解机制的性能优化方案，旨在解决现有方案中计数器更新延迟和全通道阻塞恢复的性能瓶颈。通过引入集中式计数器增量电路实现子阵列级重叠更新，以及实现银行级恢复隔离，PRACtical 在保持Rowhammer保护的同时，显著提升了内存性能和能效，平均性能提升8%，能耗降低19%。", "keywords": "Rowhammer, DRAM, PRAC, DDR5, 内存安全", "comments": "这篇论文通过在硬件层面优化DDR5的Rowhammer缓解机制，提出了一种实用的解决方案。其创新点在于引入集中式增量电路减少计数器更新延迟，以及实现银行级恢复隔离，避免了对整个内存通道的阻塞。这对于未来DRAM性能和安全性至关重要，因为Rowhammer问题日益严峻。该研究在保持安全性的前提下，显著提升了性能和能效，具有重要的实际应用价值。"}}
{"id": "2507.15465", "title": "The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts", "authors": ["Sungmin Yun", "Seonyong Park", "Hwayong Nam", "Younjoo Lee", "Gunjun Lee", "Kwanhee Kyung", "Sangpyo Kim", "Nam Sung Kim", "Jongmin Kim", "Hyungyo Kim", "Juhwan Cho", "Seungmin Baek", "Jung Ho Ahn"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      15 pages, 11 figures", "url": "http://arxiv.org/abs/2507.15465v2", "summary": "Computational workloads composing traditional Transformer models are starkly\nbifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic\nintensity, while feedforward layers are compute-bound. This dichotomy has long\nmotivated research into specialized hardware to mitigate the MHA bottleneck.\n  This paper argues that recent architectural shifts, namely Multi-head Latent\nAttention (MLA) and Mixture-of-Experts (MoE), challenge the premise of\nspecialized attention hardware. We make two key observations. First, the\narithmetic intensity of MLA is over two orders of magnitude greater than that\nof MHA, shifting it close to a compute-bound regime well-suited for modern\naccelerators like GPUs. Second, by distributing MoE experts across a pool of\naccelerators, their arithmetic intensity can be tuned through batching to match\nthat of the dense layers, creating a more balanced computational profile.\n  These findings reveal a diminishing need for specialized attention hardware.\nThe central challenge for next-generation Transformers is no longer\naccelerating a single memory-bound layer. Instead, the focus must shift to\ndesigning balanced systems with sufficient compute, memory capacity, memory\nbandwidth, and high-bandwidth interconnects to manage the diverse demands of\nlarge-scale models.", "comment": "15 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.15465v2", "cate": "cs.AR", "date": "2025-07-21", "updated": "2025-07-23", "AI": {"title_translation": "新的LLM瓶颈：潜注意力与专家混合模型的系统视角", "tldr": "传统Transformer的MHA是内存瓶颈，但最新架构如MLA和MoE改变了计算特性，使得专用注意力硬件的需求降低，未来挑战在于设计平衡的系统。", "motivation": "传统Transformer模型中多头注意力（MHA）是内存密集型瓶颈，而前馈层是计算密集型，这种二分法长期以来推动了专用硬件的研究以缓解MHA瓶颈。然而，本文认为近期架构如多头潜注意力（MLA）和专家混合（MoE）挑战了专用注意力硬件的前提。", "method": "本文通过对多头潜注意力（MLA）和专家混合（MoE）两种新型Transformer架构的计算特性进行观察和分析，提出了两个关键发现。", "result": "第一，多头潜注意力（MLA）的算术强度比MHA高出两个数量级以上，使其接近计算密集型，非常适合GPU等现代加速器。第二，通过将专家混合（MoE）的专家分布在加速器池中，可以通过批处理调整其算术强度，使其与密集层匹配，从而创建更平衡的计算配置文件。", "conclusion": "这些发现表明对专用注意力硬件的需求正在减少。下一代Transformer的核心挑战不再是加速单个内存密集型层。相反，重点必须转向设计具有足够计算、内存容量、内存带宽和高带宽互连的平衡系统，以管理大规模模型的各种需求。", "translation": "传统Transformer模型的计算工作负载明显分化。多头注意力（MHA）是内存密集型，算术强度低，而前馈层是计算密集型。这种二分法长期以来激励了对专用硬件的研究，以缓解MHA瓶颈。\n本文认为，近期架构转变，即多头潜注意力（MLA）和专家混合（MoE），挑战了专用注意力硬件的前提。我们提出了两个关键观察。首先，MLA的算术强度比MHA高出两个数量级以上，使其接近计算密集型状态，非常适合GPU等现代加速器。其次，通过将MoE专家分布在加速器池中，可以通过批处理调整其算术强度，使其与密集层匹配，从而创建更平衡的计算配置文件。\n这些发现揭示了对专用注意力硬件需求的减少。下一代Transformer的核心挑战不再是加速单个内存密集型层。相反，重点必须转向设计具有足够计算、内存容量、内存带宽和高带宽互连的平衡系统，以管理大规模模型的各种需求。", "summary": "本文探讨了大型语言模型（LLM）中Transformer架构的计算瓶颈。传统上，多头注意力（MHA）是内存密集型瓶颈。然而，研究发现，最新的架构如多头潜注意力（MLA）和专家混合（MoE）显著改变了计算特性：MLA的算术强度大幅提高，使其更适合现代GPU；MoE通过分布式部署可调整算术强度以实现计算平衡。这些发现表明，专用注意力硬件的需求正在减弱，未来的挑战在于设计整体计算、内存和互连都平衡的大规模Transformer系统。", "keywords": "LLM, 潜注意力, 专家混合, Transformer, 系统平衡", "comments": "这篇论文提出了对LLM系统瓶颈的全新视角，指出随着新架构的出现，传统的MHA内存瓶颈已不再是主要问题。其创新点在于强调了系统整体平衡的重要性，而非单一组件的优化，这对于未来LLM硬件和系统设计具有重要的指导意义。"}}
{"id": "2507.18202", "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection", "authors": ["San Kim", "Jonghwi Kim", "Yejin Jeon", "Gary Geunbae Lee"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, accepted to ACL Findings 2025", "url": "http://arxiv.org/abs/2507.18202v1", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.", "comment": "18 pages, accepted to ACL Findings 2025", "pdf_url": "http://arxiv.org/pdf/2507.18202v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用GMTP保护RAG管道：一种基于梯度的掩码令牌概率中毒文档检测方法", "tldr": "RAG面临中毒文档风险，本文提出GMTP方法，通过检查梯度和掩码令牌概率来检测并过滤恶意文档，实验证明其能有效清除中毒内容。", "motivation": "检索增强生成（RAG）系统依赖外部知识库来增强大型语言模型（LLM），但攻击者可以注入中毒文档，导致LLM生成有害或误导性输出，存在安全风险。", "method": "本文提出了一种名为GMTP（Gradient-based Masked Token Probability）的新型防御方法。GMTP首先通过检查检索器相似度函数的梯度来识别高影响令牌。然后，这些关键令牌被掩码，并通过掩码语言模型（MLM）检查它们的概率。由于中毒注入的令牌通常表现出极低的掩码令牌概率，因此GMTP能够轻松检测恶意文档并实现高精度过滤。", "result": "实验表明，GMTP能够消除超过90%的中毒内容，同时保留相关文档，从而在不同数据集和对抗设置下保持鲁棒的检索和生成性能。", "conclusion": "GMTP是一种有效且高精度的防御方法，可以检测并过滤对抗性构造的文档，有效保障RAG管道的安全。", "translation": "检索增强生成（RAG）通过提供外部知识来增强大型语言模型（LLM），以实现准确和最新的响应。然而，这种对外部来源的依赖暴露出安全风险，攻击者可以将中毒文档注入知识库中，从而引导生成过程产生有害或误导性输出。在本文中，我们提出了一种新颖的防御方法——基于梯度的掩码令牌概率（GMTP），用于检测和过滤对抗性构造的文档。具体来说，GMTP通过检查检索器相似度函数的梯度来识别高影响令牌。然后，这些关键令牌被掩码，并通过掩码语言模型（MLM）检查它们的概率。由于注入的令牌通常表现出明显低的掩码令牌概率，这使得GMTP能够轻松检测恶意文档并实现高精度过滤。实验表明，GMTP能够消除90%以上的中毒内容，同时保留相关文档，从而在不同数据集和对抗性设置下保持鲁健的检索和生成性能。", "summary": "针对检索增强生成（RAG）系统中外部知识库面临的中毒文档攻击风险，本文提出了一种名为GMTP（Gradient-based Masked Token Probability）的新型防御方法。GMTP通过分析检索器相似度函数的梯度来识别高影响力的令牌，并利用掩码语言模型（MLM）检测这些令牌的概率。实验证明，GMTP能有效检测并过滤恶意注入的文档，清除超过90%的中毒内容，同时保持RAG系统的检索和生成性能。", "keywords": "RAG, 中毒文档检测, GMTP, 梯度, 掩码令牌概率", "comments": "该论文提出了一种新颖的基于梯度和掩码令牌概率的方法GMTP来解决RAG管道中的中毒文档攻击问题，具有创新性。它通过利用模型内在的特性（梯度和MLM概率）来识别恶意内容，提供了一种有效的防御机制，对于提高RAG系统的安全性具有重要意义。"}}
{"id": "2412.03860", "title": "Combinatorial Selection with Costly Information", "authors": ["Shuchi Chawla", "Dimitris Christou", "Amit Harlev", "Ziv Scully"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.03860v2", "summary": "We consider a class of optimization problems over stochastic variables where\nthe algorithm can learn information about the value of any variable through a\nseries of costly steps; we model this information acquisition process as a\nMarkov Decision Process (MDP). The algorithm's goal is to minimize the cost of\nits solution plus the cost of information acquisition, or alternately, maximize\nthe value of its solution minus the cost of information acquisition. Such\nbandit superprocesses have been studied previously but solutions are known only\nfor fairly restrictive special cases.\n  We develop a framework for approximate optimization of bandit superprocesses\nthat applies to arbitrary acyclic MDPs with a matroid feasibility constraint.\nOur framework establishes a bound on the optimal cost through a novel cost\namortization; it then couples this bound with a notion of local approximation\nthat allows approximate solutions for each component MDP in the superprocess to\nbe composed without loss into a global approximation.\n  We use this framework to obtain approximately optimal solutions for several\nvariants of bandit superprocesses for both maximization and minimization. We\nobtain new approximations for combinatorial versions of the previously studied\nPandora's Box with Optional Inspection and Pandora's Box with Partial\nInspection; the less-studied Additive Pandora's Box problem; as well as a new\nproblem that we call the Weighing Scale problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.03860v2", "cate": "cs.DS", "date": "2024-12-05", "updated": "2025-07-24", "AI": {"title_translation": "带成本信息的组合选择", "tldr": "本文提出了一种用于近似优化具有成本信息获取的随机变量组合选择问题的框架，该框架将信息获取建模为马尔可夫决策过程，并适用于具有拟阵约束的任意非循环MDP。", "motivation": "之前的强盗超过程（bandit superprocesses）解决方案仅限于相当受限的特例，本文旨在开发一个更通用的近似优化框架。", "method": "开发了一个用于近似优化强盗超过程的框架，适用于具有拟阵可行性约束的任意非循环MDP。该框架通过新颖的成本摊销建立最优成本的界限，并结合局部近似概念，使得组件MDP的近似解可以无损地组合成全局近似解。", "result": "为强盗超过程的几种变体（包括最大化和最小化）获得了近似最优解。具体包括：具有可选检查和部分检查的潘多拉魔盒的组合版本、加性潘多拉魔盒问题以及称重秤问题。", "conclusion": "本文成功开发了一个通用的框架，用于解决涉及成本信息获取的组合优化问题，为更广泛的组合问题提供了近似最优解。", "translation": "我们考虑一类随机变量上的优化问题，其中算法可以通过一系列成本高昂的步骤获取任何变量的值信息；我们将这种信息获取过程建模为马尔可夫决策过程（MDP）。算法的目标是最小化其解决方案成本加上信息获取成本，或者等价地，最大化其解决方案价值减去信息获取成本。这类强盗超过程（bandit superprocesses）之前已被研究，但解决方案仅限于相当受限的特例。\n我们开发了一个用于近似优化强盗超过程的框架，该框架适用于具有拟阵可行性约束的任意非循环MDP。我们的框架通过一种新颖的成本摊销方法建立了最优成本的界限；然后将该界限与局部近似的概念结合起来，使得超过程中每个组件MDP的近似解可以无损地组合成全局近似解。\n我们利用这个框架为强盗超过程的几种变体（包括最大化和最小化）获得了近似最优解。我们为之前研究过的具有可选检查和部分检查的潘多拉魔盒的组合版本；研究较少的加性潘多拉魔盒问题；以及我们称之为称重秤问题的新问题获得了新的近似解。", "summary": "本文提出了一个针对“强盗超过程”近似优化的框架，这类问题涉及将信息获取建模为马尔可夫决策过程的成本高昂的随机变量优化。为克服现有解决方案的局限性，该框架适用于具有拟阵约束的任意非循环MDP。它通过新颖的成本摊销来设定最优成本界限，并利用局部近似将组件解决方案组合成全局近似。该框架为多种组合问题（包括潘多拉魔盒变体和新的称重秤问题）提供了近似最优解。", "keywords": "成本信息, 组合优化, 马尔可夫决策过程, 强盗超过程, 拟阵约束", "comments": "本文的创新之处在于提供了一个通用的近似优化框架，用于解决涉及高成本信息获取的复杂随机问题，超越了之前受限的特例。其关键技术贡献在于使用成本摊销和局部近似来保证全局近似解，这对于不确定性下有价信息获取的决策制定具有重要意义。"}}
{"id": "2402.12118", "title": "DualXDA: Towards Sparse, Efficient and Explainable Data Attribution in Large AI Models", "authors": ["Galip Ümit Yolcu", "Moritz Weckbecker", "Thomas Wiegand", "Wojciech Samek", "Sebastian Lapuschkin"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.12118v2", "summary": "Deep learning models achieve remarkable performance, yet their\ndecision-making processes often remain opaque. In response, the field of\neXplainable Artificial Intelligence (XAI) has grown significantly over the last\ndecade, primarily focusing on feature attribution methods. Complementing this\nperspective, Data Attribution (DA) has emerged as a promising paradigm that\nshifts the focus from features to data provenance. However, existing DA\napproaches suffer from prohibitively high computational costs and memory\ndemands. Additionally, current attribution methods exhibit low sparsity,\nhindering the discovery of decisive patterns in the data. We introduce DualXDA,\na framework for sparse, efficient and explainable DA, comprised of two\ninterlinked approaches for Dual Data Attribution (DualDA) and eXplainable Data\nAttribution (XDA): With DualDA, we propose efficient and effective DA,\nleveraging Support Vector Machine theory to provide fast and naturally sparse\ndata attributions for AI predictions. We demonstrate that DualDA achieves high\nattribution quality, excels at solving a series of evaluated downstream tasks,\nwhile at the same time improving explanation time by a factor of up to\n4,100,000$\\times$ compared to the original Influence Functions method, and up\nto 11,000$\\times$ compared to the method's most efficient approximation from\nliterature. We further introduce XDA, a method for enhancing Data Attribution\nwith capabilities from feature attribution methods to explain why training\nsamples are relevant for the prediction of a test sample in terms of impactful\nfeatures. Taken together, our contributions in DualXDA ultimately point towards\na future of eXplainable AI applied at unprecedented scale, enabling\ntransparent, efficient and novel analysis of even the largest neural\narchitectures fostering a new generation of accountable AI systems. Code at\nhttps://github.com/gumityolcu/DualXDA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.12118v2", "cate": "cs.LG", "date": "2024-02-19", "updated": "2025-07-24", "AI": {"title_translation": "DualXDA：迈向大型人工智能模型中稀疏、高效且可解释的数据归因", "tldr": "提出了DualXDA框架，通过DualDA实现超快且稀疏的数据归因，并结合XDA解释特征影响，旨在解决现有数据归因方法计算成本高、稀疏性差的问题，实现大规模AI的可解释性。", "motivation": "现有深度学习模型决策过程不透明，尽管可解释AI（XAI）领域有所发展，但主要集中在特征归因。数据归因（DA）作为一种有前景的范式，将焦点从特征转移到数据来源，但现有DA方法计算成本和内存需求极高，且归因稀疏性低，难以发现数据中的决定性模式。", "method": "提出了DualXDA框架，包含Dual Data Attribution (DualDA) 和 eXplainable Data Attribution (XDA) 两种相互关联的方法。DualDA利用支持向量机（SVM）理论提供快速、自然的稀疏数据归因，以实现高效有效的DA。XDA通过结合特征归因方法的能力，解释训练样本为何在预测测试样本时具有相关性，即通过有影响力的特征来解释。", "result": "DualDA实现了高质量的归因，在评估的下游任务中表现出色，与原始影响力函数方法相比，解释时间加快了高达4,100,000倍，与文献中最有效的近似方法相比，加快了高达11,000倍。XDA增强了数据归因，能够解释训练样本与测试样本预测相关的原因（通过有影响力的特征）。", "conclusion": "DualXDA的贡献最终指向了前所未有规模的可解释AI应用，能够对最大的神经网络架构进行透明、高效和新颖的分析，从而促进新一代负责任的AI系统。", "translation": "深度学习模型取得了卓越的性能，但其决策过程往往不透明。为此，可解释人工智能（XAI）领域在过去十年中取得了显著发展，主要集中于特征归因方法。作为对这一视角的补充，数据归因（DA）作为一种有前景的范式出现，将焦点从特征转移到数据来源。然而，现有DA方法存在计算成本过高和内存需求过大的问题。此外，当前的归因方法稀疏性较低，阻碍了数据中决定性模式的发现。我们引入了DualXDA，一个用于稀疏、高效且可解释的DA框架，它由两种相互关联的方法组成：双重数据归因（DualDA）和可解释数据归因（XDA）。通过DualDA，我们提出了高效且有效的数据归因方法，利用支持向量机理论为AI预测提供快速且自然的稀疏数据归因。我们证明了DualDA实现了高归因质量，擅长解决一系列评估的下游任务，同时与原始影响力函数方法相比，解释时间提高了高达4,100,000倍，与文献中最有效的近似方法相比，提高了高达11,000倍。我们进一步引入了XDA，一种通过结合特征归因方法的能力来增强数据归因的方法，以解释训练样本为何在影响特征方面与测试样本的预测相关。总而言之，我们在DualXDA中的贡献最终指向了前所未有规模的可解释AI的未来应用，能够对即使是最大的神经网络架构进行透明、高效和新颖的分析，从而促进新一代负责任的AI系统。代码可在https://github.com/gumityolcu/DualXDA获取。", "summary": "本文针对现有数据归因（DA）方法计算成本高、稀疏性差的问题，提出了DualXDA框架。该框架包含DualDA和XDA。DualDA利用支持向量机理论实现高效且超快的稀疏数据归因，相比现有方法显著提升速度。XDA则结合特征归因，解释训练样本与预测的相关性。DualXDA旨在实现大规模AI模型的可解释性，促进透明和负责任的AI系统发展。", "keywords": "数据归因, 可解释人工智能, 稀疏性, 效率, 支持向量机", "comments": "这篇论文的创新点在于提出了DualXDA框架，它通过结合SVM理论和特征归因方法，显著提升了数据归因的效率和稀疏性，解决了现有方法在处理大型AI模型时的主要瓶颈。其在解释时间上的巨大提升（高达4,100,000倍）是其重要性的一大体现，这使得数据归因在实际大规模应用中变得可行。该工作为未来的可解释AI在超大规模模型上的应用奠定了基础，有助于构建更加透明和负责任的AI系统。"}}
{"id": "2507.15541", "title": "Towards Holistic Surgical Scene Graph", "authors": ["Jongmin Shin", "Enki Cho", "Ka Young Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2507.15541v2", "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.15541v2", "cate": "cs.CV", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "迈向全面的手术场景图", "tldr": "本文提出了一个新数据集Endoscapes-SG201和一个图基方法SSG-Com，用于将工具-动作-目标组合和手部身份等关键信息整合到手术场景图中，并证明了其在手术场景理解中的重要性。", "motivation": "现有的手术场景图研究未能充分探索工具-动作-目标组合和操作工具的手部身份等重要方面，尽管这些信息对手术场景理解至关重要。", "method": "作者提出了Endoscapes-SG201数据集，其中包含工具-动作-目标组合和手部身份的标注。同时，还引入了SSG-Com，一种基于图的方法，旨在学习和表示这些关键元素。", "result": "通过在安全关键视图评估和动作三元组识别等下游任务上的实验，证明了整合这些基本场景图组件的重要性，突出了它们对手术场景理解的显著贡献。", "conclusion": "整合工具-动作-目标组合和手部身份等关键信息到手术场景图中，可以显著提高手术场景理解的能力。", "translation": "手术场景理解对于计算机辅助干预系统至关重要，它需要对手术场景进行视觉理解，其中涉及手术工具、解剖结构及其相互作用等多种元素。为了有效表示手术场景中的复杂信息，已探索基于图的方法来结构化建模手术实体及其关系。以往的手术场景图研究已证明了使用图表示手术场景的可行性。然而，手术场景的某些方面——例如工具-动作-目标的多种组合以及操作工具的手的身份——尽管其重要性，但在基于图的表示中仍未得到充分探索。为了将这些方面纳入图表示中，我们提出了Endoscapes-SG201数据集，其中包括对工具-动作-目标组合和手部身份的标注。我们还引入了SSG-Com，一种旨在学习和表示这些关键元素的基于图的方法。通过在安全关键视图评估和动作三元组识别等下游任务上的实验，我们证明了整合这些基本场景图组件的重要性，突出了它们对手术场景理解的显著贡献。代码和数据集可在https://github.com/ailab-kyunghee/SSG-Com获取。", "summary": "本文旨在通过整合先前未充分探索的手术场景元素（如工具-动作-目标组合和手部身份）来改进手术场景图的表示。为此，研究团队构建了Endoscapes-SG201数据集，并开发了图基方法SSG-Com。实验结果表明，这些新组件的整合显著提升了手术场景理解，尤其是在安全关键视图评估和动作三元组识别等任务中。", "keywords": "手术场景图, 工具-动作-目标, 手部身份, Endoscapes-SG201, SSG-Com", "comments": "本文的创新点在于识别并解决了现有手术场景图表示中对手部身份和工具-动作-目标组合等关键信息缺乏探索的问题。通过引入专门的数据集和图基方法，为更全面、细致的手术场景理解提供了新的途径，对于提升计算机辅助干预系统的智能化水平具有重要意义。"}}
{"id": "2507.18249", "title": "Auto-SGCR: Automated Generation of Smart Grid Cyber Range Using IEC 61850 Standard Models", "authors": ["Muhammad M. Roomi", "S. M. Suhail Hussain", "Ee-Chien Chang", "David M. Nicol", "Daisuke Mashima"], "categories": ["cs.CR", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.18249v1", "summary": "Digitalization of power grids have made them increasingly susceptible to\ncyber-attacks in the past decade. Iterative cybersecurity testing is\nindispensable to counter emerging attack vectors and to ensure dependability of\ncritical infrastructure. Furthermore, these can be used to evaluate\ncybersecurity configuration, effectiveness of the cybersecurity measures\nagainst various attack vectors, as well as to train smart grid cybersecurity\nexperts defending the system. Enabling extensive experiments narrows the gap\nbetween academic research and production environment. A high-fidelity cyber\nrange is vital as it is often infeasible to conduct such experiments and\ntraining using production environment. However, the design and implementation\nof cyber range requires extensive domain knowledge of physical and cyber aspect\nof the infrastructure. Furthermore, costs incurred for setup and maintenance of\ncyber range are significant. Moreover, most existing smart grid cyber ranges\nare designed as a one-off, proprietary system, and are limited in terms of\nconfigurability, accessibility, portability, and reproducibility. To address\nthese challenges, an automated Smart grid Cyber Range generation framework is\npresented in this paper. Initially a human-/machine-friendly, XML-based\nmodeling language called Smart Grid Modeling Language was defined, which\nincorporates IEC 61850 System Configuration Language files. Subsequently, a\ntoolchain to parse SG-ML model files and automatically instantiate a functional\nsmart grid cyber range was developed. The developed SG-ML models can be easily\nshared and/or modified to reproduce or customize for any cyber range. The\napplication of Auto-SGCR is demonstrated through case studies with large-scale\nsubstation models. The toolchain along with example SG-ML models have been\nopen-sourced.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.18249v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Auto-SGCR：使用IEC 61850标准模型自动生成智能电网网络靶场", "tldr": "本文提出了一个名为Auto-SGCR的自动化框架，用于使用IEC 61850标准模型生成智能电网网络靶场，旨在解决现有靶场在设计、实现和可配置性方面的挑战。", "motivation": "过去十年中，电网的数字化使其日益容易受到网络攻击。迭代的网络安全测试对于应对新兴攻击向量和确保关键基础设施的可靠性至关重要。此外，需要高保真网络靶场来评估网络安全配置、措施有效性以及培训专家，但现有网络靶场的设计和实现需要大量领域知识，成本高昂，且在可配置性、可访问性、可移植性和可再现性方面存在限制。", "method": "本文提出了一个自动化智能电网网络靶场生成框架Auto-SGCR。首先，定义了一种名为智能电网建模语言（SG-ML）的人机友好型XML建模语言，该语言整合了IEC 61850系统配置语言文件。随后，开发了一个工具链来解析SG-ML模型文件并自动实例化一个功能性智能电网网络靶场。", "result": "通过大规模变电站模型的案例研究，展示了Auto-SGCR的应用。开发的SG-ML模型可以轻松共享和/或修改，以重现或定制任何网络靶场。该工具链和示例SG-ML模型已开源。", "conclusion": "该框架通过自动化生成和标准化建模，成功解决了现有智能电网网络靶场在设计复杂性、成本、可配置性、可访问性、可移植性和可再现性方面的挑战，为网络安全测试和培训提供了有效的解决方案。", "translation": "过去十年中，电网的数字化使其日益容易受到网络攻击。迭代的网络安全测试对于应对新兴攻击向量和确保关键基础设施的可靠性不可或缺。此外，这些测试可用于评估网络安全配置、网络安全措施对抗各种攻击向量的有效性，以及培训防御系统的智能电网网络安全专家。广泛的实验能够缩小学术研究与生产环境之间的差距。高保真网络靶场至关重要，因为在生产环境中使用此类实验和培训通常是不可行的。然而，网络靶场的设计和实施需要基础设施物理和网络方面的广泛领域知识。此外，网络靶场的设置和维护成本高昂。而且，大多数现有智能电网网络靶场都是一次性专有系统，在可配置性、可访问性、可移植性和可再现性方面受到限制。为了应对这些挑战，本文提出了一种自动化智能电网网络靶场生成框架。首先，定义了一种名为智能电网建模语言（SG-ML）的人机友好型XML建模语言，该语言整合了IEC 61850系统配置语言文件。随后，开发了一个工具链来解析SG-ML模型文件并自动实例化一个功能性智能电网网络靶场。所开发的SG-ML模型可以轻松共享和/或修改，以重现或定制任何网络靶场。通过大规模变电站模型的案例研究，展示了Auto-SGCR的应用。该工具链和示例SG-ML模型已开源。", "summary": "本文针对智能电网网络靶场在设计复杂性、高成本及可配置性、可移植性、可再现性受限等问题，提出了Auto-SGCR自动化生成框架。该框架首先定义了结合IEC 61850标准的SG-ML建模语言，随后开发了工具链，能够解析SG-ML模型并自动实例化功能性智能电网网络靶场。通过大规模变电站案例验证了其有效性，并已开源，旨在促进高保真靶场的便捷构建与共享。", "keywords": "智能电网, 网络靶场, IEC 61850, 自动化, SG-ML", "comments": "本文的创新点在于提出了一个基于IEC 61850标准模型和XML建模语言（SG-ML）的自动化框架Auto-SGCR，解决了现有智能电网网络靶场在设计、成本和可再现性方面的痛点。通过标准化建模和自动化工具链，极大地降低了构建和维护高保真网络靶场的门槛，并提高了其可配置性和可移植性。开源的策略也促进了研究成果的共享和复用，对于智能电网网络安全领域具有重要意义。"}}
{"id": "2401.01405", "title": "Quantifying the Uniqueness and Divisiveness of Presidential Discourse", "authors": ["Karen Zhou", "Alexander A. Meitus", "Milo Chase", "Grace Wang", "Anne Mykland", "William Howell", "Chenhao Tan"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published in PNAS Nexus: this https URL", "url": "http://arxiv.org/abs/2401.01405v2", "summary": "Do American presidents speak discernibly different from each other? If so, in\nwhat ways? And are these differences confined to any single medium of\ncommunication? To investigate these questions, this paper introduces a novel\nmetric of uniqueness based on large language models, develops a new lexicon for\ndivisive speech, and presents a framework for assessing the distinctive ways in\nwhich presidents speak about their political opponents. Applying these tools to\na variety of corpora of presidential speeches, we find considerable evidence\nthat Donald Trump's speech patterns diverge from those of all major party\nnominees for the presidency in recent history. Trump is significantly more\ndistinctive than his fellow Republicans, whose uniqueness values appear closer\nto those of the Democrats. Contributing to these differences is Trump's\nemployment of divisive and antagonistic language, particularly when targeting\nhis political opponents. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular changes in\npresidential communications.", "comment": "Published in PNAS Nexus:\n  https://academic.oup.com/pnasnexus/article/3/10/pgae431/7814873", "pdf_url": "http://arxiv.org/pdf/2401.01405v2", "cate": "cs.CL", "date": "2024-01-02", "updated": "2025-07-23", "AI": {"title_translation": "量化总统言论的独特性和分裂性", "tldr": "本文研究发现，与近期其他主要政党总统候选人相比，唐纳德·特朗普的言论模式显著不同，更具独特性和分裂性。", "motivation": "旨在探究美国总统的言论是否存在显著差异，具体体现在哪些方面，以及这些差异是否仅限于单一的沟通媒介。", "method": "本文引入了一种基于大型语言模型的新颖独特性度量指标，开发了一个用于分裂性言论的新词汇表，并提出了一个评估总统如何独特地谈论其政治对手的框架。这些工具被应用于各种总统演讲语料库进行分析。", "result": "研究发现唐纳德·特朗普的言论模式与近期所有主要政党总统候选人显著不同，他比其他共和党人更具独特性，而其他共和党人的独特性值更接近民主党人。特朗普使用分裂和对抗性语言，尤其是在针对政治对手时，是造成这些差异的原因之一。这些差异在多种测量策略中都成立，并且出现在竞选活动和官方总统演讲中，并非总统沟通方式世俗变化的产物。", "conclusion": "唐纳德·特朗普的总统言论在独特性和分裂性方面表现出显著的偏离，这与近期其他主要政党总统候选人不同，并且这种差异在不同场合和测量方法下都持续存在。", "translation": "美国总统的言论是否明显不同？如果是，以何种方式不同？这些差异是否仅限于单一的沟通媒介？为了调查这些问题，本文引入了一种基于大型语言模型的新颖独特性度量指标，开发了一个用于分裂性言论的新词汇表，并提出了一个评估总统如何独特地谈论其政治对手的框架。将这些工具应用于各种总统演讲语料库，我们发现大量证据表明唐纳德·特朗普的言论模式与近期所有主要政党总统候选人不同。特朗普比他的共和党同僚更具独特性，而这些共和党人的独特性值似乎更接近民主党人。造成这些差异的原因是特朗普使用了分裂和对抗性语言，尤其是在针对其政治对手时。这些差异在各种测量策略中都成立，出现在竞选活动和官方总统演讲中，并且似乎并非总统沟通方式世俗变化的产物。", "summary": "本文通过引入基于大型语言模型的独特性度量、分裂性言论词汇表以及评估框架，分析了美国总统的言论特征。研究发现，唐纳德·特朗普的言论模式与近期其他主要政党总统候选人显著不同，表现出更高的独特性和分裂性，尤其体现在其对政治对手的攻击性语言使用上。这些发现适用于竞选和官方演讲，且并非沟通方式随时间变化的产物。", "keywords": "总统言论, 独特性, 分裂性, 大型语言模型, 唐纳德·特朗普", "comments": "本文的创新之处在于引入了基于大型语言模型的新颖度量指标和分裂性言论词汇表，为量化总统言论的独特性和分裂性提供了新的视角和工具。研究结果对理解政治话语的演变，特别是唐纳德·特朗普的独特沟通风格及其对政治极化的影响，具有重要意义。"}}
{"id": "2506.16685", "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections", "authors": ["Xiaomeng Xu", "Yifan Hou", "Zeyi Liu", "Shuran Song"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16685v2", "summary": "We address key challenges in Dataset Aggregation (DAgger) for real-world\ncontact-rich manipulation: how to collect informative human correction data and\nhow to effectively update policies with this new data. We introduce Compliant\nResidual DAgger (CR-DAgger), which contains two novel components: 1) a\nCompliant Intervention Interface that leverages compliance control, allowing\nhumans to provide gentle, accurate delta action corrections without\ninterrupting the ongoing robot policy execution; and 2) a Compliant Residual\nPolicy formulation that learns from human corrections while incorporating force\nfeedback and force control. Our system significantly enhances performance on\nprecise contact-rich manipulation tasks using minimal correction data,\nimproving base policy success rates by over 50\\% on two challenging tasks (book\nflipping and belt assembly) while outperforming both retraining-from-scratch\nand finetuning approaches. Through extensive real-world experiments, we provide\npractical guidance for implementing effective DAgger in real-world robot\nlearning tasks. Result videos are available at:\nhttps://compliant-residual-dagger.github.io/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16685v2", "cate": "cs.RO", "date": "2025-06-20", "updated": "2025-07-24", "AI": {"title_translation": "柔顺残差DAgger：通过人工校正改进现实世界中的接触密集型操作", "tldr": "CR-DAgger引入了柔顺干预界面和柔顺残差策略，显著提高了机器人处理复杂接触任务的成功率，且仅需少量人工校正数据。", "motivation": "解决现实世界中接触密集型操作的DAgger（数据集聚合）面临的关键挑战：如何收集信息丰富的人工校正数据以及如何有效利用这些新数据更新策略。", "method": "本文提出了柔顺残差DAgger（CR-DAgger），包含两个新颖组件：1) 柔顺干预界面：利用柔顺控制，允许人类在不中断机器人策略执行的情况下提供轻柔、准确的增量动作校正；2) 柔顺残差策略：从人工校正中学习，同时融入力反馈和力控制。", "result": "该系统在精确的接触密集型操作任务中，使用最少的校正数据显著提高了性能，在两个具有挑战性的任务（翻书和皮带组装）上，将基础策略的成功率提高了50%以上，并且优于从头开始重新训练和微调的方法。", "conclusion": "通过广泛的真实世界实验，本文为在现实世界机器人学习任务中实现有效的DAgger提供了实用指导。", "translation": "我们解决了现实世界中接触密集型操作的数据集聚合（DAgger）中的关键挑战：如何收集信息丰富的人工校正数据以及如何有效利用这些新数据更新策略。我们引入了柔顺残差DAgger（CR-DAgger），它包含两个新颖的组件：1）一个柔顺干预界面，利用柔顺控制，允许人类在不中断正在进行的机器人策略执行的情况下提供轻柔、准确的增量动作校正；2）一个柔顺残差策略公式，该公式从人工校正中学习，同时结合力反馈和力控制。我们的系统使用最少的校正数据，显著提高了精确接触密集型操作任务的性能，在两个具有挑战性的任务（翻书和皮带组装）上，将基础策略的成功率提高了50%以上，同时优于从头开始重新训练和微调的方法。通过广泛的真实世界实验，我们为在现实世界机器人学习任务中实现有效的DAgger提供了实用指导。结果视频可在以下网址获取：https://compliant-residual-dagger.github.io/", "summary": "本文提出了一种名为柔顺残差DAgger（CR-DAgger）的新方法，旨在解决现实世界中接触密集型机器人操作中数据聚合（DAgger）的挑战。CR-DAgger包含两个核心创新：一个柔顺干预界面，允许人类在不中断机器人执行的情况下提供精准的delta动作校正；以及一个柔顺残差策略，能够从这些人工校正中学习并整合力反馈。实验结果表明，该系统在需要精确接触的任务中，仅用少量校正数据就能显著提升性能，成功率提高超过50%，并优于其他基线方法。", "keywords": "DAgger, 机器人操作, 柔顺控制, 人工校正, 残差学习", "comments": "该论文的创新点在于提出了柔顺干预界面和柔顺残差策略，有效解决了DAgger在真实世界接触密集型操作中数据收集和策略更新的难题。通过允许人类在不中断机器人操作的情况下进行柔顺校正，极大地提高了数据收集效率和质量。其在复杂任务上的显著性能提升证明了方法的有效性和实用性。"}}
{"id": "2507.18346", "title": "Low-rank adaptive physics-informed HyperDeepONets for solving differential equations", "authors": ["Etienne Zeudong", "Elsa Cardoso-Bihlo", "Alex Bihlo"], "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 6 figures, 5 tables", "url": "http://arxiv.org/abs/2507.18346v1", "summary": "HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an\nalternative architecture for operator learning, in which a hypernetwork\ngenerates the weights for the trunk net of a DeepONet. While this improves\nexpressivity, it incurs high memory and computational costs due to the large\nnumber of output parameters required. In this work we introduce, in the\nphysics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,\nwhich leverage low-rank adaptation (LoRA) to reduce complexity by decomposing\nthe hypernetwork's output layer weight matrix into two smaller low-rank\nmatrices. This reduces the number of trainable parameters while introducing an\nextra regularization of the trunk networks' weights. Through extensive\nexperiments on both ordinary and partial differential equations we show that\nPI-LoRA-HyperDeepONets achieve up to 70\\% reduction in parameters and\nconsistently outperform regular HyperDeepONets in terms of predictive accuracy\nand generalization.", "comment": "14 pages, 6 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.18346v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于求解微分方程的低秩自适应物理信息HyperDeepONets", "tldr": "引入PI-LoRA-HyperDeepONets，通过低秩适应减少HyperDeepONets的参数量和计算成本，同时提高求解微分方程的精度和泛化能力。", "motivation": "现有的HyperDeepONets架构在操作符学习中虽然提高了表达能力，但由于需要大量输出参数，导致内存和计算成本高昂。", "method": "提出PI-LoRA-HyperDeepONets，在物理信息机器学习设置下，利用低秩适应（LoRA）将超网络输出层权重矩阵分解为两个较小的低秩矩阵，从而减少可训练参数并引入额外的正则化。", "result": "PI-LoRA-HyperDeepONets在参数量上实现了高达70%的减少，并且在预测精度和泛化能力方面始终优于常规HyperDeepONets，这在常微分方程和偏微分方程的广泛实验中得到了验证。", "conclusion": "通过引入低秩适应，PI-LoRA-HyperDeepONets有效降低了HyperDeepONets的复杂性，同时显著提升了其在求解微分方程任务上的性能。", "translation": "HyperDeepONets由Lee、Cho和Hwang [ICLR, 2023]引入，作为一种操作符学习的替代架构，其中超网络为DeepONet的主干网络生成权重。虽然这提高了表达能力，但由于需要大量的输出参数，导致内存和计算成本很高。在这项工作中，我们在物理信息机器学习设置下引入了一种变体，即PI-LoRA-HyperDeepONets，它利用低秩适应（LoRA）通过将超网络的输出层权重矩阵分解为两个较小的低秩矩阵来降低复杂性。这减少了可训练参数的数量，同时对主干网络的权重引入了额外的正则化。通过对常微分方程和偏微分方程的广泛实验，我们表明PI-LoRA-HyperDeepONets实现了高达70%的参数减少，并在预测精度和泛化能力方面始终优于常规HyperDeepONets。", "summary": "本文针对现有HyperDeepONets在高内存和计算成本方面的局限性，提出了一种新的架构PI-LoRA-HyperDeepONets。该方法通过引入低秩适应（LoRA），将超网络的输出层权重矩阵分解为低秩矩阵，从而显著减少了可训练参数并引入了正则化。实验证明，PI-LoRA-HyperDeepONets在参数量上减少了高达70%，并在求解常微分方程和偏微分方程的任务中，在预测精度和泛化能力上均优于传统HyperDeepONets。", "keywords": "HyperDeepONets, 低秩适应, 物理信息机器学习, 微分方程, 操作符学习", "comments": "这项工作通过将低秩适应（LoRA）引入到物理信息机器学习设置中的HyperDeepONets架构，提供了一种有效的参数效率和性能提升的解决方案。其创新点在于将成熟的LoRA技术应用于HyperDeepONets，以解决其高计算成本问题，并同时提高了模型的预测精度和泛化能力。这对大规模科学计算和微分方程求解领域具有重要意义。"}}
{"id": "2507.18103", "title": "A New Pair of GloVes", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18103v1", "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18103v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "一副新的GloVe", "tldr": "本文介绍了2024年更新的英文GloVe词向量模型，这些模型基于新数据训练，并经过详细文档记录，在词汇更新、结构任务表现相似、以及在时间敏感的命名实体识别任务上有所提升。", "motivation": "原始的2014年GloVe模型已被广泛使用，但语言和世界不断演变，作者认为当前的使用可以从更新的模型中受益。此外，2014年的模型没有详细记录所用的确切数据版本和预处理方法，作者旨在纠正这一点。", "method": "作者使用Wikipedia、Gigaword和Dolma的一个子集训练了两组词嵌入。通过词汇比较、直接测试和命名实体识别（NER）任务进行评估。", "result": "2024年的词向量包含了新的文化和语言相关词汇；在类比和相似性等结构性任务上表现相当；在最近的、时间依赖的NER数据集（如非西方新闻数据）上表现出改进的性能。", "conclusion": "2024年更新的GloVe模型成功地整合了新的词汇，并在特定任务上显示出改进，同时保持了与旧模型在结构性任务上的可比性，并且提供了更详细的文档。", "translation": "本报告记录、描述并评估了2024年新的英文GloVe（词表示的全局向量）模型。虽然2014年构建的原始GloVe模型已被广泛使用并证明有用，但语言和世界持续演变，我们认为当前的使用可以从更新的模型中受益。此外，2014年的模型没有仔细记录所使用的确切数据版本和预处理方法，我们通过记录这些新模型来纠正这一点。我们使用维基百科、Gigaword和Dolma的一个子集训练了两组词嵌入。通过词汇比较、直接测试和NER任务的评估表明，2024年的向量包含了新的文化和语言相关词汇，在类比和相似性等结构性任务上表现相当，并在最近的、时间依赖的NER数据集（如非西方新闻数据）上表现出改进的性能。", "summary": "本报告介绍了2024年更新的英文GloVe词向量模型，旨在解决2014年原始模型数据未详细记录及语言演变的需求。新模型使用Wikipedia、Gigaword和Dolma数据训练，并通过词汇比较、直接测试和NER任务进行评估。结果显示，新模型收录了文化和语言相关的新词，在结构性任务上表现与旧模型相当，并在时间敏感的NER任务上有所提升。", "keywords": "GloVe, 词嵌入, 自然语言处理, 词向量, 模型更新", "comments": "这篇论文的创新点在于提供了更新的、详细文档记录的GloVe模型，以适应语言和世界的发展。其重要性在于为NLP社区提供了更现代、更透明的词嵌入资源，特别是在处理时效性数据方面。它强调了持续更新语言模型以保持其相关性的必要性。"}}
{"id": "2507.17764", "title": "Diffusion-Assisted Frequency Attention Model for Whole-body Low-field MRI Reconstruction", "authors": ["Xin Xie", "Yu Guan", "Zhuoxu Cui", "Dong Liang", "Qiegen Liu"], "categories": ["physics.med-ph", "cs.CV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "Comments:      29 pages,7 figures", "url": "http://arxiv.org/abs/2507.17764v1", "summary": "By integrating the generative strengths of diffusion models with the\nrepresentation capabilities of frequency-domain attention, DFAM effectively\nenhances reconstruction performance under low-SNR condi-tions. Experimental\nresults demonstrate that DFAM consistently outperforms both conventional\nreconstruction algorithms and recent learning-based approaches. These findings\nhighlight the potential of DFAM as a promising solution to advance low-field\nMRI reconstruction, particularly in resource-constrained or underdeveloped\nclinical settings.", "comment": "29 pages,7 figures", "pdf_url": "http://arxiv.org/pdf/2507.17764v1", "cate": "physics.med-ph", "date": "2025-07-09", "updated": "2025-07-09", "AI": {"title_translation": "弥散辅助频率注意力模型用于全身低场核磁共振图像重建", "tldr": "DFAM结合扩散模型和频域注意力，在低信噪比条件下有效提升低场核磁共振图像重建性能，并优于现有方法。", "motivation": "在低信噪比条件下，特别是在资源受限或欠发达的临床环境中，低场核磁共振图像重建面临挑战，需要一种有效提升重建性能的解决方案。", "method": "DFAM通过整合扩散模型的生成能力和频域注意力的表示能力来增强重建性能。", "result": "实验结果表明，DFAM在低信噪比条件下始终优于传统的重建算法和最近的基于学习的方法。", "conclusion": "DFAM具有作为一种有前景的解决方案来推进低场核磁共振图像重建的潜力，尤其适用于资源受限或欠发达的临床环境。", "translation": "通过整合扩散模型的生成优势和频域注意力的表示能力，DFAM在低信噪比条件下有效地增强了重建性能。实验结果表明，DFAM始终优于传统的重建算法和最近的基于学习的方法。这些发现突出了DFAM作为一种有前景的解决方案来推进低场核磁共振图像重建的潜力，特别是在资源受限或欠发达的临床环境中。", "summary": "该论文提出了一种名为DFAM（弥散辅助频率注意力模型）的新方法，用于全身低场核磁共振图像重建。DFAM结合了扩散模型的生成能力和频域注意力的表示能力，旨在提高低信噪比条件下的重建性能。实验结果显示，DFAM在性能上超越了传统的和最新的基于学习的重建算法，表明其在资源受限的临床环境中推进低场核磁共振图像重建方面具有巨大潜力。", "keywords": "弥散模型, 频率注意力, 核磁共振图像重建, 低场MRI, 低信噪比", "comments": "该论文的创新点在于将扩散模型的生成能力与频域注意力机制相结合，以解决低场核磁共振图像重建在低信噪比条件下的挑战。其重要性在于为资源受限的临床环境提供了一种有前景的解决方案，有助于推动低场核磁共振技术的普及和应用。"}}
{"id": "2507.16362", "title": "LPTR-AFLNet: Lightweight Integrated Chinese License Plate Rectification and Recognition Network", "authors": ["Guangzhu Xu", "Pengcheng Zuo", "Zhi Ke", "Bangjun Lei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 33 figures", "url": "http://arxiv.org/abs/2507.16362v2", "summary": "Chinese License Plate Recognition (CLPR) faces numerous challenges in\nunconstrained and complex environments, particularly due to perspective\ndistortions caused by various shooting angles and the correction of single-line\nand double-line license plates. Considering the limited computational resources\nof edge devices, developing a low-complexity, end-to-end integrated network for\nboth correction and recognition is essential for achieving real-time and\nefficient deployment. In this work, we propose a lightweight, unified network\nnamed LPTR-AFLNet for correcting and recognizing Chinese license plates, which\ncombines a perspective transformation correction module (PTR) with an optimized\nlicense plate recognition network, AFLNet. The network leverages the\nrecognition output as a weak supervisory signal to effectively guide the\ncorrection process, ensuring accurate perspective distortion correction. To\nenhance recognition accuracy, we introduce several improvements to LPRNet,\nincluding an improved attention module to reduce confusion among similar\ncharacters and the use of Focal Loss to address class imbalance during\ntraining. Experimental results demonstrate the exceptional performance of\nLPTR-AFLNet in rectifying perspective distortion and recognizing double-line\nlicense plate images, maintaining high recognition accuracy across various\nchallenging scenarios. Moreover, on lower-mid-range GPUs platform, the method\nruns in less than 10 milliseconds, indicating its practical efficiency and\nbroad applicability.", "comment": "28 pages, 33 figures", "pdf_url": "http://arxiv.org/pdf/2507.16362v2", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "LPTR-AFLNet：轻量级集成中文车牌校正与识别网络", "tldr": "提出LPTR-AFLNet，一个轻量级端到端网络，用于纠正透视畸变并识别中文车牌，在边缘设备上实现高效实时性。", "motivation": "在无约束复杂环境下，中文车牌识别（CLPR）面临挑战，特别是由于拍摄角度引起的透视畸变以及单行和双行车牌的校正问题。考虑到边缘设备计算资源有限，需要开发一种低复杂度、端到端的校正与识别集成网络，以实现实时高效部署。", "method": "提出了一个名为LPTR-AFLNet的轻量级统一网络，用于中文车牌的校正和识别。该网络结合了透视变换校正模块（PTR）和优化的车牌识别网络AFLNet。网络利用识别输出作为弱监督信号来有效指导校正过程。为了提高识别精度，对LPRNet进行了改进，包括引入改进的注意力模块以减少相似字符之间的混淆，并使用Focal Loss来解决训练期间的类别不平衡问题。", "result": "实验结果表明，LPTR-AFLNet在校正透视畸变和识别双行车牌图像方面表现出色，在各种挑战性场景下保持高识别精度。此外，在低中端GPU平台上，该方法运行时间少于10毫秒。", "conclusion": "LPTR-AFLNet成功解决了中文车牌在复杂环境下的畸变校正和识别问题，并在有限计算资源下实现了高效实时性能，具有广泛的实用性。", "translation": "中文车牌识别（CLPR）在无约束和复杂环境中面临诸多挑战，特别是由于各种拍摄角度引起的透视畸变以及单行和双行车牌的校正问题。考虑到边缘设备有限的计算资源，开发一种低复杂度、端到端的校正与识别集成网络对于实现实时高效部署至关重要。在这项工作中，我们提出了一个名为LPTR-AFLNet的轻量级统一网络，用于校正和识别中文车牌，该网络结合了透视变换校正模块（PTR）和优化的车牌识别网络AFLNet。该网络利用识别输出作为弱监督信号，有效指导校正过程，确保准确的透视畸变校正。为了提高识别精度，我们对LPRNet进行了多项改进，包括引入改进的注意力模块以减少相似字符之间的混淆，以及使用Focal Loss来解决训练期间的类别不平衡问题。实验结果表明，LPTR-AFLNet在校正透视畸变和识别双行车牌图像方面表现出色，在各种挑战性场景下保持高识别精度。此外，在低中端GPU平台上，该方法运行时间少于10毫秒，表明其具有实际效率和广泛适用性。", "summary": "本文提出了LPTR-AFLNet，一个轻量级集成网络，旨在解决复杂环境下中文车牌的透视畸变校正和识别问题。该网络结合了透视变换校正模块和优化的识别网络，并利用识别结果作为弱监督信号指导校正。同时，通过改进注意力机制和引入Focal Loss提升了识别精度。实验证明，LPTR-AFLNet在校正畸变和识别双行车牌方面表现卓越，且具有高效的实时处理能力，适用于边缘设备部署。", "keywords": "中文车牌识别, 透视畸变校正, 轻量级网络, 端到端, 弱监督", "comments": "该论文的创新点在于提出了一个轻量级且端到端的集成网络，实现了车牌校正与识别的统一，尤其是在资源受限的边缘设备上具有实用价值。通过将识别结果作为校正的弱监督信号，巧妙地提升了校正的准确性。同时，对现有识别网络的改进，如注意力机制和Focal Loss的应用，也进一步提升了整体性能。其低延迟的运行速度是其重要优势。"}}
{"id": "2507.18302", "title": "LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models", "authors": ["Delong Ran", "Xinlei He", "Tianshuo Cong", "Anyu Wang", "Qi Li", "Xiaoyun Wang"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.18302v1", "summary": "Language Models (LMs) typically adhere to a \"pre-training and fine-tuning\"\nparadigm, where a universal pre-trained model can be fine-tuned to cater to\nvarious specialized domains. Low-Rank Adaptation (LoRA) has gained the most\nwidespread use in LM fine-tuning due to its lightweight computational cost and\nremarkable performance. Because the proportion of parameters tuned by LoRA is\nrelatively small, there might be a misleading impression that the LoRA\nfine-tuning data is invulnerable to Membership Inference Attacks (MIAs).\nHowever, we identify that utilizing the pre-trained model can induce more\ninformation leakage, which is neglected by existing MIAs. Therefore, we\nintroduce LoRA-Leak, a holistic evaluation framework for MIAs against the\nfine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership\ninference attacks, including ten existing MIAs, and five improved MIAs that\nleverage the pre-trained model as a reference. In experiments, we apply\nLoRA-Leak to three advanced LMs across three popular natural language\nprocessing tasks, demonstrating that LoRA-based fine-tuned LMs are still\nvulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).\nWe also applied LoRA-Leak to different fine-tuning settings to understand the\nresulting privacy risks. We further explore four defenses and find that only\ndropout and excluding specific LM layers during fine-tuning effectively\nmitigate MIA risks while maintaining utility. We highlight that under the\n\"pre-training and fine-tuning\" paradigm, the existence of the pre-trained model\nmakes MIA a more severe risk for LoRA-based LMs. We hope that our findings can\nprovide guidance on data privacy protection for specialized LM providers.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.18302v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LoRA-Leak：针对LoRA微调语言模型的成员推断攻击", "tldr": "LoRA微调的语言模型容易受到成员推断攻击，尤其是当利用预训练模型时。LoRA-Leak框架揭示了这一点，并发现只有dropout和排除特定LM层是有效的防御措施。", "motivation": "由于LoRA微调参数量相对较小，可能存在一种误解，认为LoRA微调数据不易受到成员推断攻击（MIAs）。然而，现有MIAs忽略了利用预训练模型可能导致更多信息泄露的问题，本研究旨在解决这一被忽视的漏洞。", "method": "本研究引入了LoRA-Leak，一个针对语言模型微调数据集进行成员推断攻击的整体评估框架。LoRA-Leak包含了十五种成员推断攻击，其中包括十种现有MIAs和五种利用预训练模型作为参考的改进MIAs。研究将LoRA-Leak应用于三种先进的语言模型和三种流行的自然语言处理任务，并探索了不同的微调设置和四种防御措施。", "result": "实验证明，基于LoRA的微调语言模型仍然容易受到MIAs的攻击（例如，在保守的微调设置下AUC达到0.775）。在测试的防御措施中，只有dropout和在微调期间排除特定的LM层能有效缓解MIA风险，同时保持模型效用。研究还发现，在“预训练和微调”范式下，预训练模型的存在使基于LoRA的语言模型的MIA风险更为严重。", "conclusion": "在“预训练和微调”范式下，由于预训练模型的存在，基于LoRA的语言模型面临更严重的成员推断攻击风险。目前，只有dropout和排除特定LM层被证明是有效且能保持模型效用的防御措施。", "translation": "语言模型（LMs）通常遵循“预训练和微调”范式，其中通用的预训练模型可以进行微调以适应各种专业领域。低秩适应（LoRA）因其轻量级的计算成本和卓越的性能，在LM微调中得到了最广泛的应用。由于LoRA调整的参数比例相对较小，可能会产生一种误导性的印象，认为LoRA微调数据不易受到成员推断攻击（MIAs）。然而，我们发现利用预训练模型可以导致更多的信息泄露，这被现有MIAs所忽视。因此，我们引入了LoRA-Leak，一个针对LM微调数据集进行MIAs的整体评估框架。LoRA-Leak包含了十五种成员推断攻击，其中包括十种现有MIAs，以及五种利用预训练模型作为参考的改进MIAs。在实验中，我们将LoRA-Leak应用于三种先进的LM，跨越三种流行的自然语言处理任务，证明了基于LoRA的微调LM仍然容易受到MIAs的攻击（例如，在保守的微调设置下AUC达到0.775）。我们还将LoRA-Leak应用于不同的微调设置，以了解由此产生的隐私风险。我们进一步探索了四种防御措施，发现只有dropout和在微调期间排除特定的LM层能有效缓解MIA风险，同时保持效用。我们强调，在“预训练和微调”范式下，预训练模型的存在使得MIA对基于LoRA的LM来说是一个更严重的风险。我们希望我们的发现能为专业LM提供商的数据隐私保护提供指导。", "summary": "本文介绍了LoRA-Leak，一个用于评估针对LoRA微调语言模型成员推断攻击的综合框架。研究挑战了LoRA微调数据对成员推断攻击免疫的普遍看法，并揭示了预训练模型可能导致更多信息泄露，而这被现有攻击所忽视。通过实验，作者证明了LoRA微调模型在MIAs面前的脆弱性（例如，AUC高达0.775），并发现dropout以及在微调时排除特定LM层是有效缓解风险同时保持模型效用的防御方法。研究强调了预训练模型在“预训练和微调”范式下如何加剧了LoRA语言模型的隐私风险，为专业语言模型提供商的数据隐私保护提供了指导。", "keywords": "成员推断攻击, LoRA, 语言模型, 微调, 数据隐私", "comments": "该论文创新性地挑战了LoRA在隐私保护方面的普遍假设，并突出了预训练模型在信息泄露中对成员推断攻击的关键作用。其重要性在于提供了一个强大的评估框架（LoRA-Leak）和关于有效防御的实用见解，这对于专业语言模型提供商保护数据隐私至关重要。"}}
{"id": "2507.17930", "title": "How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations", "authors": ["Vahid Garousi", "Zafar Jafarov"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17930v1", "summary": "Artificial Intelligence (AI) has the potential to transform Software\nEngineering (SE) by enhancing productivity, efficiency, and decision support.\nTools like GitHub Copilot and ChatGPT have given rise to \"vibe coding\"-an\nexploratory, prompt-driven development style. Yet, how software engineers\nengage with these tools in daily tasks, especially in deciding whether to\ntrust, refine, or reject AI-generated outputs, remains underexplored. This\npaper presents two complementary contributions. First, a pragmatic process\nmodel capturing real-world AI-assisted SE activities, including prompt design,\ninspection, fallback, and refinement. Second, a 2D decision framework that\ncould help developers reason about trade-offs between effort saved and output\nquality. Grounded in practitioner reports and direct observations in three\nindustry settings across Turkiye and Azerbaijan, our work illustrates how\nengineers navigate AI use with human oversight. These models offer structured,\nlightweight guidance to support more deliberate and effective use of AI tools\nin SE, contributing to ongoing discussions on practical human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17930v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "软件工程师如何与AI协作：一个基于行业观察的实用过程模型和决策框架", "tldr": "本文基于行业观察，提出了一个实用过程模型和一个决策框架，以指导软件工程师如何有效使用AI工具并进行人机协作。", "motivation": "尽管人工智能（AI）在提高生产力、效率和决策支持方面具有改变软件工程（SE）的潜力，但软件工程师如何在日常任务中与AI工具互动，尤其是在决定信任、优化或拒绝AI生成输出方面，仍未得到充分探索。本研究旨在填补这一空白。", "method": "本文提出了两个互补的贡献：首先是一个实用的过程模型，它捕捉了现实世界中AI辅助的软件工程活动，包括提示设计、检查、回退和优化。其次是一个二维决策框架，可帮助开发人员权衡节省的精力与输出质量。这些模型基于土耳其和阿塞拜疆三个行业环境中的从业者报告和直接观察。", "result": "研究提出了一个实用的AI辅助软件工程活动过程模型和一个二维决策框架，后者可用于权衡节省的精力和输出质量。这些模型展示了工程师如何在人工监督下使用AI，并为软件工程中AI工具的更审慎和有效使用提供了结构化、轻量级的指导。", "conclusion": "本文提出的过程模型和决策框架为软件工程师在日常工作中有效使用AI工具提供了实用指导，有助于促进关于实际人机协作的持续讨论。", "translation": "人工智能（AI）有潜力通过提高生产力、效率和决策支持来改变软件工程（SE）。GitHub Copilot和ChatGPT等工具催生了“氛围编码”——一种探索性的、由提示驱动的开发风格。然而，软件工程师如何在日常任务中与这些工具互动，特别是在决定信任、优化或拒绝AI生成输出方面，仍未得到充分探索。本文提出了两个互补的贡献。首先是一个实用的过程模型，它捕捉了现实世界中AI辅助的软件工程活动，包括提示设计、检查、回退和优化。其次是一个二维决策框架，可帮助开发人员权衡节省的精力与输出质量。我们的工作基于土耳其和阿塞拜疆三个行业环境中的从业者报告和直接观察，阐明了工程师如何在人工监督下使用AI。这些模型为支持软件工程中AI工具的更审慎和有效使用提供了结构化、轻量级的指导，有助于推动关于实际人机协作的持续讨论。", "summary": "本文针对软件工程师如何与AI工具互动这一未充分探索的领域，提出了一个实用的过程模型和一个二维决策框架。该过程模型涵盖了AI辅助软件工程活动，包括提示设计、检查、回退和优化；决策框架则帮助开发者权衡节省的精力与输出质量。这些模型基于土耳其和阿塞拜疆的行业观察和从业者报告，旨在为软件工程中AI工具的有效使用提供指导，并促进人机协作的讨论。", "keywords": "软件工程, 人工智能, 人机协作, 过程模型, 决策框架", "comments": "该论文解决了AI在软件工程中实际应用的一个及时且重要的问题，通过基于真实世界观察的具象模型，为研究人员和实践者提供了宝贵的见解。其对AI生成输出背景下的人工监督和决策制定关注，尤其具有启发性。"}}
{"id": "2507.17845", "title": "Towards Robust Foundation Models for Digital Pathology", "authors": ["Jonah Kömen", "Edwin D. de Jong", "Julius Hense", "Hannah Marienwald", "Jonas Dippel", "Philip Naumann", "Eric Marcus", "Lukas Ruff", "Maximilian Alber", "Jonas Teuwen", "Frederick Klauschen", "Klaus-Robert Müller"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17845v1", "summary": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled\nhealthcare research and entering clinical validation. However, their\nsusceptibility to learning non-biological technical features -- including\nvariations in surgical/endoscopic techniques, laboratory procedures, and\nscanner hardware -- poses risks for clinical deployment. We present the first\nsystematic investigation of pathology FM robustness to non-biological features.\nOur work (i) introduces measures to quantify FM robustness, (ii) demonstrates\nthe consequences of limited robustness, and (iii) proposes a framework for FM\nrobustification to mitigate these issues. Specifically, we developed PathoROB,\na robustness benchmark with three novel metrics, including the robustness\nindex, and four datasets covering 28 biological classes from 34 medical\ncenters. Our experiments reveal robustness deficits across all 20 evaluated\nFMs, and substantial robustness differences between them. We found that\nnon-robust FM representations can cause major diagnostic downstream errors and\nclinical blunders that prevent safe clinical adoption. Using more robust FMs\nand post-hoc robustification considerably reduced (but did not yet eliminate)\nthe risk of such errors. This work establishes that robustness evaluation is\nessential for validating pathology FMs before clinical adoption and\ndemonstrates that future FM development must integrate robustness as a core\ndesign principle. PathoROB provides a blueprint for assessing robustness across\nbiomedical domains, guiding FM improvement efforts towards more robust,\nrepresentative, and clinically deployable AI systems that prioritize biological\ninformation over technical artifacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17845v1", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "迈向数字病理的鲁棒基础模型", "tldr": "病理学基础模型易受非生物技术特征影响导致错误。本研究首次系统性调查其鲁棒性，提出PathoROB基准，发现现有模型鲁棒性不足，并强调鲁棒性评估和设计对临床部署至关重要。", "motivation": "生物医学基础模型（FMs）在医疗健康领域迅速发展，但其易受非生物技术特征（如手术、实验室程序和扫描仪硬件的变化）影响，这给临床部署带来了风险。", "method": "本研究首次系统性调查病理学基础模型对非生物特征的鲁棒性。作者提出了量化FM鲁棒性的措施，展示了有限鲁棒性的后果，并提出了一个FM鲁棒性强化框架。具体而言，他们开发了PathoROB，这是一个包含三个新颖指标（包括鲁棒性指数）和四个数据集（涵盖来自34个医疗中心的28个生物学类别）的鲁棒性基准，并评估了20个基础模型。", "result": "实验揭示所有20个评估的基础模型都存在鲁棒性缺陷，且模型之间存在显著差异。不鲁棒的基础模型表示可能导致重大的诊断下游错误和临床失误。使用更鲁棒的基础模型和事后鲁棒性强化显著降低了（但未完全消除）此类错误的风险。", "conclusion": "鲁棒性评估对于病理学基础模型在临床应用前的验证至关重要。未来的基础模型开发必须将鲁棒性作为核心设计原则。PathoROB为生物医学领域评估鲁棒性提供了蓝图，指导基础模型改进，以创建更鲁棒、更具代表性且优先考虑生物信息而非技术伪影的临床可部署AI系统。", "translation": "生物医学基础模型（FMs）正在迅速改变人工智能驱动的医疗保健研究，并进入临床验证阶段。然而，它们容易学习非生物技术特征——包括手术/内窥镜技术、实验室程序和扫描仪硬件的变化——这给临床部署带来了风险。我们首次系统性地研究了病理学FM对非生物特征的鲁棒性。我们的工作（i）引入了量化FM鲁棒性的度量标准，（ii）展示了有限鲁棒性的后果，以及（iii）提出了一个FM鲁棒性强化框架来缓解这些问题。具体而言，我们开发了PathoROB，这是一个鲁棒性基准，包含三个新颖的指标，包括鲁棒性指数，以及覆盖来自34个医疗中心的28个生物类别的四个数据集。我们的实验揭示了所有20个评估的FM都存在鲁棒性缺陷，并且它们之间存在显著的鲁棒性差异。我们发现不鲁棒的FM表示可能导致重大的诊断下游错误和临床失误，从而阻碍安全临床应用。使用更鲁棒的FM和事后鲁棒性强化显著降低了（但尚未消除）此类错误的风险。这项工作确立了在临床应用前对病理学FM进行鲁棒性评估的重要性，并表明未来的FM开发必须将鲁棒性作为核心设计原则。PathoROB为跨生物医学领域评估鲁棒性提供了蓝图，指导FM改进工作，以实现更鲁棒、更具代表性、优先考虑生物信息而非技术伪影的临床可部署AI系统。", "summary": "本论文解决了数字病理学中生物医学基础模型（FMs）鲁棒性这一关键问题，这些模型易受非生物技术变化的影响。论文首次进行了系统性调查，推出了PathoROB，一个包含新颖指标和多样化数据集的鲁棒性基准。对20个FM的实验揭示了显著的鲁棒性缺陷，可能导致诊断错误。研究强调，鲁棒性评估对于临床应用至关重要，应成为未来FM开发的核心设计原则，PathoROB将作为创建更可靠AI系统的指南。", "keywords": "数字病理学, 基础模型, 鲁棒性, 生物医学AI, 临床部署", "comments": "该论文强调了在敏感医疗领域部署AI时一个关键且常被忽视的挑战：非生物技术特征的混淆效应。PathoROB的引入，包括其特定指标和多样化数据集，是一项重要贡献，为评估和提高病理学FM的临床可靠性提供了急需的基准。即使鲁棒的FM也未能完全消除风险的发现，凸显了问题的复杂性和持续研究的必要性。这项工作对于指导AI更安全、更有效地融入医疗保健领域具有重要意义。"}}
{"id": "2507.18255", "title": "LONG3R: Long Sequence Streaming 3D Reconstruction", "authors": ["Zhuoguang Chen", "Minghui Qin", "Tianyuan Yuan", "Zhe Liu", "Hang Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.18255v1", "summary": "Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.", "comment": "Accepted by ICCV 2025. Project page:\n  https://zgchen33.github.io/LONG3R/", "pdf_url": "http://arxiv.org/pdf/2507.18255v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LONG3R：长序列流式3D重建", "tldr": "LONG3R是一种用于长序列流式多视图3D场景重建的新模型，通过循环操作和创新记忆机制实现实时处理并优于现有方法。", "motivation": "现有多视图场景重建方法在处理图像流时存在局限性，要么依赖耗时的离线优化，要么仅限于短序列，这阻碍了它们在实时场景中的应用。", "method": "本文提出了LONG3R模型，通过循环操作实时处理，并维护更新记忆。它采用记忆门控机制过滤相关记忆，与新观测一起输入双源精炼解码器进行粗到细交互。为有效捕获长序列记忆，提出3D时空记忆，动态剪枝冗余空间信息并自适应调整场景分辨率。此外，采用两阶段课程训练策略以提高长序列性能并保持训练效率。", "result": "实验表明，LONG3R在长序列上优于最先进的流式方法，同时保持实时推理速度。", "conclusion": "LONG3R模型通过其创新的记忆管理和训练策略，成功解决了长序列流式3D重建的实时性和性能挑战，为实际应用提供了有效方案。", "translation": "近期多视图场景重建取得了显著进展，但现有方法在处理输入图像流时面临局限性。这些方法要么依赖耗时的离线优化，要么仅限于较短序列，这阻碍了它们在实时场景中的应用。在这项工作中，我们提出了LONG3R（长序列流式3D重建），一个专为长序列流式多视图3D场景重建设计的新模型。我们的模型通过循环操作，随着每次新的观测维护和更新记忆，实现了实时处理。我们首先采用记忆门控机制来过滤相关记忆，然后将其与新的观测一起输入到双源精炼解码器中进行从粗到细的交互。为了有效捕获长序列记忆，我们提出了一个3D时空记忆，它能动态剪枝冗余空间信息，同时自适应地调整场景分辨率。为了在保持训练效率的同时增强模型在长序列上的性能，我们采用了两阶段课程训练策略，每个阶段都针对特定的能力。实验表明，LONG3R在长序列上优于最先进的流式方法，同时保持实时推理速度。项目页面：https://zgchen33.github.io/LONG3R/。", "summary": "LONG3R是一种新颖的流式多视图3D场景重建模型，旨在克服现有方法在处理长序列和实时性方面的限制。它通过循环操作、记忆门控机制、双源精炼解码器以及创新的3D时空记忆实现实时处理和对长序列的有效捕获。此外，采用两阶段课程训练策略以优化性能。实验证明LONG3R在长序列上超越了现有流式方法，并保持实时推理速度。", "keywords": "流式3D重建, 长序列, 实时处理, 记忆门控, 3D时空记忆", "comments": "LONG3R的创新之处在于其针对长序列流式3D重建的实时性挑战，通过引入循环操作、记忆门控机制和独特的3D时空记忆来高效管理和利用历史信息。其动态剪枝和分辨率调整的3D时空记忆是关键创新点，有效解决了长序列中冗余信息和计算效率的问题。两阶段课程训练策略也进一步提升了模型在复杂场景下的性能。该工作对于推动3D重建在实时、动态环境中的应用具有重要意义。"}}
{"id": "2507.18460", "title": "Topology-Preserving Coupling of Compressible Fluids and Thin Deformables", "authors": ["Jonathan Panuelos", "Eitan Grinspun", "David Levin"], "categories": ["physics.comp-ph", "cs.GR", "physics.flu-dyn"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18460v1", "summary": "We present a novel discretization of coupled compressible fluid and thin\ndeformable structures that provides sufficient and necessary leakproofness by\npreserving the path connectedness of the fluid domain. Our method employs a\nconstrained Voronoi-based spatial partitioning combined with Godunov-style\nfinite-volume time integration. The fluid domain is discretized into cells that\nconform exactly to the fluid-solid interface, allowing boundary conditions to\nbe sharply resolved exactly at the interface. This enables direct force\nexchange between the fluid and solid while ensuring that no fluid leaks through\nthe solid, even when arbitrarily thin. We validate our approach on a series of\nchallenging scenarios -- including a balloon propelled by internal compressed\nair, a champagne cork ejecting after overcoming friction, and a supersonic\nasteroid -- demonstrating bidirectional energy transfer between fluid and\nsolid.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18460v1", "cate": "physics.comp-ph", "date": "2025-05-29", "updated": "2025-05-29", "AI": {"title_translation": "可压缩流体与薄变形体拓扑保持耦合", "tldr": "一种耦合可压缩流体与薄变形结构的新方法，通过保持流体域连通性确保无泄漏，并采用约束Voronoi离散化和Godunov风格积分，已在挑战性场景中得到验证。", "motivation": "当前的可压缩流体与薄变形结构耦合方法在流体域的路径连通性方面可能存在不足，导致流体泄漏问题，尤其是在固体任意薄的情况下。本研究的动机是提出一种能提供充分必要防泄漏性的新离散化方法。", "method": "本方法采用了一种新颖的离散化技术，结合了约束Voronoi空间划分和Godunov风格的有限体积时间积分。流体域被离散成与流固界面精确吻合的网格单元，从而能够在界面处精确解析边界条件，实现流体和固体之间的直接力交换，并确保即使在固体任意薄的情况下也没有流体泄漏。", "result": "该方法在一系列具有挑战性的场景中得到了验证，包括由内部压缩空气推动的气球、克服摩擦后喷射的香槟软木塞以及超音速小行星。验证结果表明，该方法能够实现流体和固体之间的双向能量传递。", "conclusion": "本研究提出的方法成功地实现了可压缩流体与薄变形体的防泄漏耦合，并通过保持流体域的路径连通性解决了流体泄漏问题，即使在固体极薄的挑战性场景下也能有效捕捉流固间的双向能量传递。", "translation": "我们提出了一种新的可压缩流体与薄变形结构耦合的离散化方法，通过保持流体域的路径连通性，提供了充分必要的防泄漏性。我们的方法采用了约束Voronoi空间划分结合Godunov风格的有限体积时间积分。流体域被离散成与流固界面精确吻合的网格单元，使得边界条件可以在界面处精确地尖锐解析。这使得流体和固体之间能够直接进行力交换，同时确保即使在固体任意薄的情况下也没有流体泄漏。我们在一系列具有挑战性的场景中验证了我们的方法——包括一个由内部压缩空气推动的气球、一个克服摩擦后喷射的香槟软木塞以及一个超音速小行星——展示了流体和固体之间的双向能量传递。", "summary": "本文提出了一种新颖的离散化方法，用于耦合可压缩流体与薄变形结构，通过保持流体域的路径连通性来确保防泄漏性。该方法利用约束Voronoi空间划分与Godunov风格的有限体积时间积分相结合，将流体域离散为与流固界面精确吻合的网格单元，从而实现边界条件的精确解析和流体与固体间的直接力交换，即使固体极薄也能防止流体泄漏。该方法在气球、香槟软木塞和超音速小行星等一系列挑战性场景中得到了验证，并成功展示了流体与固体间的双向能量传递。", "keywords": "可压缩流体, 变形结构, 流固耦合, Voronoi离散化, 防泄漏性", "comments": "该论文提出了一种创新的离散化方法，解决了流固耦合模拟中流体泄漏的关键挑战，特别是对于薄变形固体。拓扑保持的约束Voronoi划分是其核心创新点，确保了鲁棒的防泄漏性和精确的界面解析。通过在多样化和具有挑战性的场景中进行验证，凸显了该方法的实际适用性及其准确捕捉双向能量传递的能力，这对于真实的模拟至关重要。"}}
{"id": "2411.07037", "title": "LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios", "authors": ["Xiaodong Wu", "Minhao Wang", "Yichen Liu", "Xiaoming Shi", "He Yan", "Xiangju Lu", "Junmin Zhu", "Wei Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, 3 figures", "url": "http://arxiv.org/abs/2411.07037v3", "summary": "As Large Language Models (LLMs) evolve in natural language processing (NLP),\ntheir ability to stably follow instructions in long-context inputs has become\ncritical for real-world applications. However, existing benchmarks seldom focus\non instruction-following in long-context scenarios or stability on different\ninputs. To bridge this gap, we introduce LIFBench, a scalable dataset designed\nto evaluate LLMs' instruction-following capabilities and stability across long\ncontexts. LIFBench comprises three long-context scenarios and eleven diverse\ntasks, featuring 2,766 instructions generated through an automated expansion\nmethod across three dimensions: length, expression, and variables. For\nevaluation, we propose LIFEval, a rubric-based assessment method that enables\nprecise, automated scoring of complex LLM responses without reliance on\nLLM-assisted assessments or human judgment. This method allows for a\ncomprehensive analysis of model performance and stability from multiple\nperspectives. We conduct detailed experiments on 20 prominent LLMs across six\nlength intervals. Our work contributes LIFBench and LIFEval as robust tools for\nassessing LLM performance in complex and long-context settings, offering\nvaluable insights to guide future advancements in LLM development.", "comment": "17 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2411.07037v3", "cate": "cs.CL", "date": "2024-11-11", "updated": "2025-07-23", "AI": {"title_translation": "LIFBench：评估大型语言模型在长上下文场景中的指令遵循性能和稳定性", "tldr": "本文提出了LIFBench数据集和LIFEval评估方法，用于在大上下文场景中评估大型语言模型的指令遵循性能和稳定性。", "motivation": "现有基准测试很少关注大型语言模型在长上下文场景中的指令遵循能力或不同输入下的稳定性，而这对于实际应用至关重要。", "method": "引入了LIFBench，一个可扩展的数据集，包含三个长上下文场景和十一个不同任务，共2,766条指令，通过自动化扩展方法生成。同时提出了LIFEval，一个基于评分标准的评估方法，可以实现复杂大型语言模型响应的精确自动化评分，无需依赖大型语言模型辅助评估或人工判断。", "result": "对20个知名大型语言模型在六个长度区间进行了详细实验。工作贡献了LIFBench和LIFEval作为评估大型语言模型在复杂长上下文环境中性能的强大工具。", "conclusion": "LIFBench和LIFEval是评估大型语言模型在复杂和长上下文设置中性能的强大工具，为指导大型语言模型未来的发展提供了宝贵的见解。", "translation": "随着大型语言模型（LLMs）在自然语言处理（NLP）领域的演进，它们在长上下文输入中稳定遵循指令的能力已成为实际应用的关键。然而，现有基准测试很少关注长上下文场景中的指令遵循或不同输入下的稳定性。为了弥补这一差距，我们引入了LIFBench，一个可扩展的数据集，旨在评估LLMs在长上下文中的指令遵循能力和稳定性。LIFBench包含三个长上下文场景和十一个不同的任务，其2,766条指令通过长度、表达和变量三个维度的自动化扩展方法生成。为了进行评估，我们提出了LIFEval，一种基于评分标准的评估方法，它能够对复杂的LLM响应进行精确、自动化的评分，而无需依赖LLM辅助评估或人工判断。这种方法可以从多个角度对模型性能和稳定性进行全面分析。我们对20个知名LLMs在六个长度区间进行了详细实验。我们的工作贡献了LIFBench和LIFEval作为评估LLM在复杂和长上下文设置中性能的强大工具，为指导LLM未来发展提供了宝贵的见解。", "summary": "本文针对现有基准测试在长上下文场景中大型语言模型指令遵循能力和稳定性评估方面的不足，提出了LIFBench数据集和LIFEval评估方法。LIFBench是一个包含2,766条指令的跨多任务、多场景数据集，通过自动化方法生成。LIFEval则是一种无需人工或LLM辅助的自动化评分方法。研究人员对20个主流大型语言模型进行了实验，证明了LIFBench和LIFEval在评估长上下文LLM性能方面的有效性，为未来的LLM发展提供了重要工具。", "keywords": "大型语言模型, 指令遵循, 长上下文, 性能评估, LIFBench, LIFEval", "comments": "LIFBench和LIFEval的创新之处在于它们专注于长上下文场景下的指令遵循能力和稳定性评估，并提供了一种不依赖人工或LLM辅助的自动化评估方法。这对于大型语言模型在实际应用中的部署至关重要，因为长上下文处理和稳定性是衡量其鲁棒性的关键指标。该工作的贡献在于填补了现有基准测试的空白，为LLM的进一步发展提供了新的评估工具和方向。"}}
{"id": "2507.17268", "title": "PolarAnything: Diffusion-based Polarimetric Image Synthesis", "authors": ["Kailong Zhang", "Youwei Lyu", "Heng Guo", "Si Li", "Zhanyu Ma", "Boxin Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.17268v2", "summary": "Polarization images facilitate image enhancement and 3D reconstruction tasks,\nbut the limited accessibility of polarization cameras hinders their broader\napplication. This gap drives the need for synthesizing photorealistic\npolarization images. The existing polarization simulator Mitsuba relies on a\nparametric polarization image formation model and requires extensive 3D assets\ncovering shape and PBR materials, preventing it from generating large-scale\nphotorealistic images. To address this problem, we propose PolarAnything,\ncapable of synthesizing polarization images from a single RGB input with both\nphotorealism and physical accuracy, eliminating the dependency on 3D asset\ncollections. Drawing inspiration from the zero-shot performance of pretrained\ndiffusion models, we introduce a diffusion-based generative framework with an\neffective representation strategy that preserves the fidelity of polarization\nproperties. Experiments show that our model generates high-quality polarization\nimages and supports downstream tasks like shape from polarization.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.17268v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "极化万物：基于扩散的偏振图像合成", "tldr": "提出PolarAnything，一个基于扩散模型的方法，能从单张RGB图像合成逼真且物理精确的偏振图像。", "motivation": "偏振相机可及性有限阻碍了偏振图像的广泛应用；现有偏振模拟器（如Mitsuba）依赖大量3D资产且无法生成大规模逼真的图像。", "method": "提出PolarAnything，一个基于扩散的生成框架，能从单张RGB输入合成偏振图像，并采用有效的表示策略以保持偏振特性保真度，消除了对3D资产集合的依赖。", "result": "模型生成了高质量的偏振图像，并支持偏振形状恢复等下游任务。", "conclusion": "PolarAnything成功地解决了偏振图像合成的挑战，实现了从RGB输入生成逼真且物理准确的偏振图像，克服了现有方法的局限性，并为下游应用提供了支持。", "translation": "偏振图像有助于图像增强和3D重建任务，但偏振相机的有限可及性阻碍了其更广泛的应用。这一空白推动了合成真实感偏振图像的需求。现有偏振模拟器Mitsuba依赖于参数化偏振图像形成模型，并且需要覆盖形状和PBR材质的广泛3D资产，这使其无法生成大规模真实感图像。为了解决这个问题，我们提出了PolarAnything，它能够从单一RGB输入合成既具有真实感又具有物理准确性的偏振图像，消除了对3D资产集合的依赖。受预训练扩散模型零样本性能的启发，我们引入了一个基于扩散的生成框架，该框架具有有效的表示策略，可保持偏振特性的保真度。实验表明，我们的模型生成了高质量的偏振图像，并支持偏振形状恢复等下游任务。", "summary": "本文提出了PolarAnything，一个基于扩散的生成框架，能够从单一RGB图像输入合成逼真且物理准确的偏振图像。针对偏振相机可及性受限及现有模拟器对大量3D资产的依赖问题，PolarAnything利用扩散模型的零样本性能，通过有效表示策略保持偏振特性，成功生成高质量偏振图像并支持如偏振形状恢复等下游任务。", "keywords": "偏振图像合成, 扩散模型, 图像生成, 偏振形状恢复, 真实感", "comments": "该论文的创新之处在于提出了一种基于扩散模型的方法来合成偏振图像，克服了传统方法对昂贵偏振相机和大量3D资产的依赖。这种从单张RGB图像生成高质量、物理精确偏振图像的能力，极大地降低了偏振图像应用的门槛，并为图像增强和3D重建等下游任务提供了新的可能性。"}}
{"id": "2507.18360", "title": "Conformidade com os Requisitos Legais de Privacidade de Dados: Um Estudo sobre Técnicas de Anonimização", "authors": ["André Menolli", "Luiz Fernando Nunes", "Thiago A. Coleti"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      in Portuguese language", "url": "http://arxiv.org/abs/2507.18360v1", "summary": "The protection of personal data has become a central topic in software\ndevelopment, especially with the implementation of the General Data Protection\nLaw (LGPD) in Brazil and the General Data Protection Regulation (GDPR) in the\nEuropean Union. With the enforcement of these laws, certain software quality\ncriteria have become mandatory, such as data anonymization, which is one of the\nmain aspects addressed by these regulations. The aim of this article is to\nanalyze data anonymization techniques and assess their effectiveness in\nensuring compliance with legal requirements and the utility of the data for its\nintended purpose. Techniques such as aggregation, generalization, perturbation,\nand k-anonymity were investigated and applied to datasets containing personal\nand sensitive data. The analysis revealed significant variations in the\neffectiveness of each method, highlighting the need to balance privacy and data\nutility.", "comment": "in Portuguese language", "pdf_url": "http://arxiv.org/pdf/2507.18360v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "数据隐私法律要求合规性：匿名化技术研究", "tldr": "随着巴西LGPD和欧盟GDPR等数据隐私法律的实施，数据匿名化成为强制性的软件质量要求。本文分析了聚合、泛化、扰动和k-匿名等匿名化技术在符合法律要求和数据实用性方面的有效性，发现不同方法的效果差异显著，并强调了平衡隐私与数据效用的必要性。", "motivation": "个人数据保护已成为软件开发中的核心议题，尤其是在巴西通用数据保护法（LGPD）和欧盟通用数据保护条例（GDPR）实施后。这些法律的强制执行使得数据匿名化等软件质量标准成为强制性要求，因此需要分析数据匿名化技术及其在确保法律合规性和数据实用性方面的有效性。", "method": "研究并应用了聚合、泛化、扰动和k-匿名等匿名化技术，并将其应用于包含个人和敏感数据的数据集。", "result": "分析显示，每种方法的有效性存在显著差异。", "conclusion": "在应用匿名化技术时，需要在隐私和数据效用之间取得平衡。", "translation": "个人数据保护已成为软件开发中的核心议题，尤其是在巴西通用数据保护法（LGPD）和欧盟通用数据保护条例（GDPR）实施后。随着这些法律的强制执行，某些软件质量标准，如数据匿名化，已成为强制性要求，这也是这些法规所涉及的主要方面之一。本文旨在分析数据匿名化技术，并评估其在确保符合法律要求和数据用于其预期目的方面的有效性。研究并应用了聚合、泛化、扰动和k-匿名等技术于包含个人和敏感数据的数据集。分析显示，每种方法的有效性存在显著差异，突出了平衡隐私与数据效用的必要性。", "summary": "鉴于巴西LGPD和欧盟GDPR等数据隐私法律的实施，数据匿名化已成为强制性的软件质量标准。本文旨在分析聚合、泛化、扰动和k-匿名等数据匿名化技术，评估它们在满足法律合规性和保持数据实用性方面的有效性。研究发现，不同的匿名化方法在有效性上存在显著差异，并强调了在保护隐私和确保数据可用性之间取得平衡的重要性。", "keywords": "数据匿名化, 隐私保护, LGPD, GDPR, 数据效用", "comments": "该研究聚焦于当前数据隐私法规背景下至关重要的数据匿名化技术，具有很强的现实意义。通过对多种匿名化技术的实际应用和效果评估，揭示了隐私保护与数据效用之间的权衡，为实践提供了有价值的见解。其创新点在于对不同匿名化方法有效性的比较分析。"}}
{"id": "2507.16220", "title": "LENS-DF: Deepfake Detection and Temporal Localization for Long-Form Noisy Speech", "authors": ["Xuechen Liu", "Wanying Ge", "Xin Wang", "Junichi Yamagishi"], "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE International Joint Conference on Biometrics (IJCB) 2025, Osaka, Japan", "url": "http://arxiv.org/abs/2507.16220v2", "summary": "This study introduces LENS-DF, a novel and comprehensive recipe for training\nand evaluating audio deepfake detection and temporal localization under\ncomplicated and realistic audio conditions. The generation part of the recipe\noutputs audios from the input dataset with several critical characteristics,\nsuch as longer duration, noisy conditions, and containing multiple speakers, in\na controllable fashion. The corresponding detection and localization protocol\nuses models. We conduct experiments based on self-supervised learning front-end\nand simple back-end. The results indicate that models trained using data\ngenerated with LENS-DF consistently outperform those trained via conventional\nrecipes, demonstrating the effectiveness and usefulness of LENS-DF for robust\naudio deepfake detection and localization. We also conduct ablation studies on\nthe variations introduced, investigating their impact on and relevance to\nrealistic challenges in the field.", "comment": "Accepted by IEEE International Joint Conference on Biometrics (IJCB)\n  2025, Osaka, Japan", "pdf_url": "http://arxiv.org/pdf/2507.16220v2", "cate": "cs.SD", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "LENS-DF：长篇嘈杂语音的深度伪造检测和时间定位", "tldr": "LENS-DF是一个新颖的框架，用于在复杂和真实的音频条件下训练和评估音频深度伪造检测和时间定位模型，并显示出优于传统方法的性能。", "motivation": "在复杂和真实的音频条件下，对长篇嘈杂语音进行音频深度伪造检测和时间定位存在挑战。", "method": "本研究提出了LENS-DF，一个用于训练和评估音频深度伪造检测和时间定位的综合性方法。它能够可控地生成具有长持续时间、嘈杂条件和包含多个说话者的音频。相应的检测和定位协议使用模型，并基于自监督学习前端和简单后端进行实验。", "result": "使用LENS-DF生成的数据训练的模型，其性能始终优于通过传统方法训练的模型。消融研究也探讨了引入的变体对现实挑战的影响和相关性。", "conclusion": "LENS-DF被证明在鲁棒的音频深度伪造检测和定位方面是有效和有用的。", "translation": "本研究介绍了LENS-DF，一个新颖而全面的方法，用于在复杂和真实的音频条件下训练和评估音频深度伪造检测和时间定位。该方法的生成部分从输入数据集中输出具有几个关键特征的音频，例如更长的持续时间、嘈杂条件和包含多个说话者，并且以可控的方式进行。相应的检测和定位协议使用模型。我们基于自监督学习前端和简单后端进行实验。结果表明，使用LENS-DF生成的数据训练的模型始终优于通过传统方法训练的模型，这证明了LENS-DF在鲁棒音频深度伪造检测和定位方面的有效性和实用性。我们还对引入的变体进行了消融研究，调查它们对该领域现实挑战的影响和相关性。", "summary": "LENS-DF是一个用于在复杂和真实音频环境下进行音频深度伪造检测和时间定位的新型训练和评估框架。它能够生成具有长持续时间、噪声和多说话者特征的音频数据，并采用自监督学习前端和简单后端进行模型训练。实验结果表明，LENS-DF训练的模型在鲁棒性方面优于传统方法，证实了其在实际应用中的有效性和实用性。", "keywords": "音频深度伪造, 检测, 时间定位, 嘈杂语音, LENS-DF", "comments": "LENS-DF的创新之处在于其能够生成具有复杂特性的合成音频数据（长持续时间、噪声、多说话者），从而更好地模拟现实世界中的深度伪造挑战。这对于提高音频深度伪造检测和定位模型的鲁棒性至关重要。该方法通过引入更真实的数据生成范式，解决了现有方法在复杂环境下性能不足的问题，具有重要的实践意义。"}}
{"id": "2507.18014", "title": "Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models", "authors": ["Datta Nimmaturi", "Vaishnavi Bhargava", "Rajat Ghosh", "Johnu George", "Debojyoti Dutta"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18014v1", "summary": "Fine-tuning large language models (LLMs) for reasoning tasks using\nreinforcement learning methods like Group Relative Policy Optimization (GRPO)\nis computationally expensive. To address this, we propose a predictive\nframework that models training dynamics and helps optimize resource usage.\nThrough experiments on Llama and Qwen models (3B 8B), we derive an empirical\nscaling law based on model size, initial performance, and training progress.\nThis law predicts reward trajectories and identifies three consistent training\nphases: slow start, rapid improvement, and plateau. We find that training\nbeyond certain number of an epoch offers little gain, suggesting earlier\nstopping can significantly reduce compute without sacrificing performance. Our\napproach generalizes across model types, providing a practical guide for\nefficient GRPO-based fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18014v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大型推理模型高效GRPO训练的预测性缩放定律", "tldr": "本文提出了一个预测框架和经验缩放定律，用于优化GRPO微调大型语言模型的计算效率，发现过早停止训练可显著节省资源而不牺牲性能。", "motivation": "使用像GRPO这样的强化学习方法对大型语言模型（LLMs）进行推理任务的微调计算成本很高。", "method": "我们提出了一个预测框架来模拟训练动态并优化资源使用。通过对Llama和Qwen模型（3B 8B）的实验，我们基于模型大小、初始性能和训练进度推导出了一个经验缩放定律。", "result": "该缩放定律可以预测奖励轨迹，并识别出三个一致的训练阶段：缓慢启动、快速改进和平台期。我们发现，在一定数量的训练周期后继续训练收益甚微。", "conclusion": "在达到一定训练周期后提早停止可以显著减少计算量而不牺牲性能。我们的方法适用于不同类型的模型，为高效的GRPO微调提供了实用指导。", "translation": "使用像群组相对策略优化（GRPO）这样的强化学习方法对大型语言模型（LLMs）进行推理任务的微调计算成本很高。为了解决这个问题，我们提出了一个预测框架，该框架模拟训练动态并有助于优化资源使用。通过对Llama和Qwen模型（3B 8B）的实验，我们推导出了一个基于模型大小、初始性能和训练进度的经验缩放定律。该定律预测奖励轨迹并识别出三个一致的训练阶段：缓慢启动、快速改进和平台期。我们发现，在一定数量的训练周期后继续训练收益甚微，这表明提早停止可以显著减少计算量而不牺牲性能。我们的方法适用于不同类型的模型，为高效的GRPO微调提供了实用指导。", "summary": "本文针对使用GRPO微调大型语言模型进行推理任务时计算成本高昂的问题，提出了一个预测框架。该框架通过对Llama和Qwen模型的实验，推导出一个基于模型大小、初始性能和训练进度的经验缩放定律，能够预测奖励轨迹并识别训练的三个阶段。研究发现，在达到一定训练周期后继续训练的收益甚微，因此提早停止训练可以显著节省计算资源，同时不影响性能。该方法具有通用性，为高效的GRPO微调提供了实用指导。", "keywords": "GRPO, 缩放定律, 大语言模型, 微调, 效率", "comments": "该论文通过引入预测性缩放定律，为优化大型语言模型在推理任务中基于GRPO的微调过程提供了重要的实践指导。其创新点在于通过经验数据识别出训练阶段并量化了资源与性能之间的关系，特别是提出在训练过程中提早停止可以显著节省计算资源，这对于资源受限的LLM训练尤其重要。这项工作对于降低LLM训练成本和提高效率具有显著的实际意义。"}}
{"id": "2507.18150", "title": "Unit Commitment Framework for Nuclear Reactors with Reactivity Decline", "authors": ["Shiny Choudhury", "Michael Davidson", "George Tynan"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      11 pages, preliminary version, comments welcome", "url": "http://arxiv.org/abs/2507.18150v1", "summary": "Nuclear reactors are often modeled as inflexible, baseload generators with\nfixed downtimes and restrictive ramping limits. In practice, however, a\nreactor's operational flexibility is closely tied to it's fuel cycle stage and\nthe associated reactivity margin. A key physical constraint to power\nmaneuverability is xenon poisoning, caused by an increase in neutron absorbing\nxenon concentration following a power ramp down. This can delay or even prevent\nsubsequent power ramp up due to suppressed core reactivity. Additionally, if a\nreactor is shutdown during periods of low reactivity, restart times can vary\nsignificantly due to these xenon transients, leading to longer downtimes. This\nwork introduces a physics informed, metaheuristic modeling approach that embeds\nfuel cycle dynamics directly with a unit commitment (UC) framework. The\nframework tracks reactivity margin, dynamically activates xenon related\nconstraints, and endogenously implements refueling outages based on the core\nconditions. By capturing intra-cycle reactivity evolution and the conditional\nonset of xenon poisoning, the formulation allows for operation dependent\nnuclear dispatch that reflects both regulatory limits and physical behavior.\nWhen applied to a representative reactor fleet operating in distinct modes of\noperation -- ranging from baseload to part load -- the framework reveals that\nflexible operation can slow reactivity degradation and extend fuel cycles. The\nresults show that fuel cycle aware flexibility modeling is critical for\naccurate scheduling of nuclear reactors and offers a tractable pathway to\nintegrate nuclear power in energy system models.", "comment": "11 pages, preliminary version, comments welcome", "pdf_url": "http://arxiv.org/pdf/2507.18150v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "考虑反应性衰减的核反应堆机组组合框架", "tldr": "该研究引入了一种物理信息化的元启发式建模方法，将燃料循环动力学与机组组合框架相结合，以实现核反应堆的灵活调度，并揭示灵活运行可以减缓反应性衰减并延长燃料循环。", "motivation": "核反应堆通常被建模为不灵活的基荷发电机，具有固定的停机时间和严格的爬坡限制。然而，实际上，反应堆的运行灵活性与其燃料循环阶段和相关的反应性裕度密切相关。氙中毒是功率机动性的一个关键物理约束，它可能延迟甚至阻止随后的功率爬坡，并导致更长的停机时间。因此，需要一个能够反映这些物理行为的核电调度模型。", "method": "本研究引入了一种物理信息化的元启发式建模方法，该方法将燃料循环动力学直接嵌入到机组组合（UC）框架中。该框架跟踪反应性裕度，动态激活与氙相关的约束，并根据堆芯条件内生地实施燃料再装载停机。通过捕获循环内反应性演变和氙中毒的条件性发生，该公式允许操作依赖的核电调度，反映了监管限制和物理行为。", "result": "将该框架应用于在不同运行模式（从基荷到部分负荷）下运行的代表性反应堆群时，结果表明灵活运行可以减缓反应性衰减并延长燃料循环。研究结果表明，考虑燃料循环的灵活性建模对于核反应堆的准确调度至关重要，并为将核电集成到能源系统模型中提供了可行的途径。", "conclusion": "考虑燃料循环的灵活性建模对于核反应堆的准确调度至关重要，并且为将核电集成到能源系统模型中提供了一个可行的途径。灵活操作不仅能够反映监管限制和物理行为，还能减缓反应性衰减并延长燃料循环。", "translation": "核反应堆通常被建模为不灵活的基荷发电机，具有固定的停机时间和严格的爬坡限制。然而，实际上，反应堆的运行灵活性与其燃料循环阶段和相关的反应性裕度密切相关。功率机动性的一个关键物理约束是氙中毒，这是由功率下降后中子吸收氙浓度增加引起的。这可能由于堆芯反应性受抑制而延迟甚至阻止随后的功率爬坡。此外，如果在低反应性期间关闭反应堆，由于这些氙瞬变，重启时间可能会显著变化，导致更长的停机时间。这项工作引入了一种物理信息化的元启发式建模方法，该方法将燃料循环动力学直接嵌入到机组组合（UC）框架中。该框架跟踪反应性裕度，动态激活与氙相关的约束，并根据堆芯条件内生地实施燃料再装载停机。通过捕获循环内反应性演变和氙中毒的条件性发生，该公式允许操作依赖的核电调度，反映了监管限制和物理行为。当应用于在不同运行模式（从基荷到部分负荷）下运行的代表性反应堆群时——从基荷到部分负荷——该框架揭示灵活运行可以减缓反应性衰减并延长燃料循环。结果表明，考虑燃料循环的灵活性建模对于核反应堆的准确调度至关重要，并为将核电集成到能源系统模型中提供了可行的途径。", "summary": "本研究提出一个创新的机组组合（UC）框架，用于核反应堆的调度，克服了传统模型中核反应堆被视为不灵活的基荷发电机的局限性。该框架通过整合物理信息和元启发式方法，直接嵌入了燃料循环动力学，能够跟踪反应性裕度，动态处理氙中毒相关的约束，并根据堆芯条件内生性地管理燃料再装载停机。该方法捕获了循环内反应性演变和氙中毒的条件性发生，从而实现了反映监管限制和物理行为的运行依赖性核电调度。应用结果表明，灵活操作能够减缓反应性衰减并延长燃料循环，强调了考虑燃料循环的灵活性建模对于核反应堆准确调度和将核电有效集成到能源系统模型中的重要性。", "keywords": "核反应堆, 机组组合, 反应性衰减, 燃料循环, 氙中毒", "comments": "该论文的创新之处在于将核反应堆的复杂物理行为，特别是燃料循环动力学和氙中毒效应，直接集成到标准的机组组合框架中。这使得核电调度模型能够更准确地反映反应堆的实际运行灵活性和约束，而非仅仅将其视为基荷电源。这种物理知情的建模方法对于提高核电的经济效益和其在未来能源系统中的集成度具有重要意义。"}}
{"id": "2507.17976", "title": "Failure Prediction in Conversational Recommendation Systems", "authors": ["Maria Vlachou"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17976v1", "summary": "In a Conversational Image Recommendation task, users can provide natural\nlanguage feedback on a recommended image item, which leads to an improved\nrecommendation in the next turn. While typical instantiations of this task\nassume that the user's target item will (eventually) be returned, this might\noften not be true, for example, the item the user seeks is not within the item\ncatalogue. Failing to return a user's desired item can lead to user\nfrustration, as the user needs to interact with the system for an increased\nnumber of turns. To mitigate this issue, in this paper, we introduce the task\nof Supervised Conversational Performance Prediction, inspired by Query\nPerformance Prediction (QPP) for predicting effectiveness in response to a\nsearch engine query. In this regard, we propose predictors for conversational\nperformance that detect conversation failures using multi-turn semantic\ninformation contained in the embedded representations of retrieved image items.\nSpecifically, our AutoEncoder-based predictor learns a compressed\nrepresentation of top-retrieved items of the train turns and uses the\nclassification labels to predict the evaluation turn. Our evaluation scenario\naddressed two recommendation scenarios, by differentiating between system\nfailure, where the system is unable to find the target, and catalogue failure,\nwhere the target does not exist in the item catalogue. In our experiments using\nthe Shoes and FashionIQ Dresses datasets, we measure the accuracy of predictors\nfor both system and catalogue failures. Our results demonstrate the promise of\nour proposed predictors for predicting system failures (existing evaluation\nscenario), while we detect a considerable decrease in predictive performance in\nthe case of catalogue failure prediction (when inducing a missing item\nscenario) compared to system failures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17976v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "会话推荐系统中的故障预测", "tldr": "本文引入了会话性能预测任务，以预测会话推荐系统中的故障，并提出了基于自编码器的预测器，该预测器在系统故障预测方面表现出 promising 的结果，但在目录故障预测方面性能有所下降。", "motivation": "在会话推荐任务中，用户可能无法获得其期望的商品，例如商品不在商品目录中。这会导致用户沮丧并增加交互轮次。为了缓解这个问题，本文引入了会话性能预测任务来检测会话故障。", "method": "本文引入了“监督式会话性能预测”任务，并提出了使用检索到的图像项嵌入表示中包含的多轮语义信息来检测会话故障的预测器。具体来说，该方法提出了一个基于自编码器的预测器，该预测器学习训练轮次中检索到的热门项的压缩表示，并使用分类标签来预测评估轮次。评估场景区分了系统故障（系统无法找到目标）和目录故障（目标不存在于商品目录中）。", "result": "实验结果表明，所提出的预测器在预测系统故障方面表现出 promising 的结果。然而，在目录故障预测（当引入缺失商品场景时）的情况下，预测性能相比系统故障有显著下降。", "conclusion": "本文提出的预测器在会话推荐系统中预测系统故障方面显示出潜力，但对于目录故障的预测性能仍有待提高。", "translation": "在会话图像推荐任务中，用户可以对推荐的图像项提供自然语言反馈，从而在下一轮中获得改进的推荐。虽然该任务的典型实例化假设用户的目标项最终会被返回，但这可能通常不真实，例如，用户寻求的项不在商品目录中。未能返回用户期望的项可能导致用户沮丧，因为用户需要与系统进行更多轮次的交互。为了缓解这个问题，本文引入了监督式会话性能预测任务，其灵感来源于用于预测搜索引擎查询响应有效性的查询性能预测（QPP）。在这方面，我们提出了用于会话性能的预测器，该预测器使用检索到的图像项嵌入表示中包含的多轮语义信息来检测会话故障。具体来说，我们基于自编码器的预测器学习训练轮次中热门检索项的压缩表示，并使用分类标签来预测评估轮次。我们的评估场景解决了两种推荐场景，通过区分系统故障（系统无法找到目标）和目录故障（目标不存在于商品目录中）。在我们使用Shoes和FashionIQ Dresses数据集进行的实验中，我们测量了系统故障和目录故障预测器的准确性。我们的结果表明，我们提出的预测器在预测系统故障（现有评估场景）方面具有 promising 的前景，而与系统故障相比，我们在目录故障预测（当引入缺失项场景时）的情况下检测到预测性能的显著下降。", "summary": "本文针对会话推荐系统中用户未获得期望商品导致沮丧的问题，引入了监督式会话性能预测任务。该研究提出了基于自编码器的预测器，利用多轮语义信息来检测会话故障，并区分系统故障和目录故障。实验结果表明，该预测器在预测系统故障方面表现良好，但在预测目录故障时性能显著下降。", "keywords": "会话推荐系统, 故障预测, 自编码器, 系统故障, 目录故障", "comments": "本文的创新点在于引入了会话性能预测任务，并首次区分了系统故障和目录故障这两种不同的失败模式，这对于提升会话推荐系统的用户体验具有重要意义。然而，模型在目录故障预测上的性能下降是其局限性，未来研究可以专注于如何更好地处理商品目录外的问题。"}}
{"id": "1808.02391", "title": "Energy-preserving continuous-stage partitioned Runge-Kutta methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be improved.", "url": "http://arxiv.org/abs/1808.02391v3", "summary": "In this paper, we present continuous-stage partitioned Runge-Kutta (csPRK)\nmethods for energy-preserving integration of Hamiltonian systems. A sufficient\ncondition for the energy preservation of the csPRK methods is derived. It is\nshown that the presented condition contains the existing condition for\nenergy-preserving continuous-stage Runge-Kutta methods as a special case. A\nnoticeable and interesting result is that when we use the simplifying\nassumptions of order conditions and the normalized shifted Legendre polynomials\nfor constructing high-order energy-preserving csPRK methods, both the Butcher\n\"weight\" coefficients $B_\\tau$ and $\\widehat{B}_\\tau$ must be equal to $1$. As\nillustrative examples, new energy-preserving integrators are acquired by virtue\nof the presented condition, and for the sake of verifying our theoretical\nresults, some numerical experiments are reported.", "comment": "The paper needs to be improved.", "pdf_url": "http://arxiv.org/pdf/1808.02391v3", "cate": "math.NA", "date": "2018-08-07", "updated": "2025-07-24", "AI": {"title_translation": "能量守恒的连续阶段分段龙格-库塔方法", "tldr": "本文介绍了用于哈密顿系统能量守恒积分的连续阶段分段龙格-库塔（csPRK）方法，推导了能量守恒的充分条件，并发现构建高阶方法时某些系数必须为1，通过数值实验验证了理论结果。", "motivation": "旨在为哈密顿系统开发能量守恒的积分器。", "method": "提出了连续阶段分段龙格-库塔（csPRK）方法，并推导了其能量守恒的充分条件。利用阶条件简化假设和归一化移位勒让德多项式构建高阶方法，并通过数值实验进行验证。", "result": "推导了csPRK方法能量守恒的充分条件，该条件包含现有条件作为特例。在使用阶条件简化假设和归一化移位勒让德多项式构建高阶能量守恒csPRK方法时，Butcher“权重”系数$B_\\tau$和$\\widehat{B}_\\tau$都必须等于1。获得了新的能量守恒积分器，并通过数值实验验证了理论结果。", "conclusion": "本文成功提出了csPRK方法及其能量守恒的充分条件，揭示了构建高阶方法时特定系数的要求，并通过实例和实验验证了理论结果。", "translation": "本文提出了用于哈密顿系统能量守恒积分的连续阶段分段龙格-库塔（csPRK）方法。推导了csPRK方法能量守恒的充分条件。结果表明，所提出的条件包含了现有能量守恒连续阶段龙格-库塔方法的条件作为特例。一个值得注意且有趣的结果是，当我们使用阶条件简化假设和归一化移位勒让德多项式来构建高阶能量守恒csPRK方法时，Butcher“权重”系数$B_\\tau$和$\\widehat{B}_\\tau$都必须等于1。作为说明性例子，通过所提出的条件获得了新的能量守恒积分器，并为了验证我们的理论结果，报告了一些数值实验。", "summary": "本文介绍了用于哈密顿系统能量守恒积分的连续阶段分段龙格-库塔（csPRK）方法。文中推导了csPRK方法能量守恒的充分条件，并证明该条件是现有连续阶段龙格-库塔方法条件的推广。一个重要发现是，在构建高阶方法时，特定的Butcher“权重”系数必须为1。通过新获得的积分器和数值实验验证了理论结果。", "keywords": "能量守恒, 龙格-库塔方法, 哈密顿系统, 连续阶段, 分段", "comments": "本文引入了一种新的方法类别（csPRK），并提出了能量守恒的推广条件，这是一项重要的理论贡献。关于高阶方法中特定Butcher系数的发现对于实际应用也具有重要意义。通过数值实验验证理论结果增加了论文的可靠性。"}}
{"id": "2507.17332", "title": "PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image", "authors": ["Hyeongjin Nam", "Donghwan Kim", "Gyeongsik Moon", "Kyoung Mu Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published at ICCV 2025, 22 pages including the supplementary material", "url": "http://arxiv.org/abs/2507.17332v2", "summary": "The misaligned human texture across different human parts is one of the main\nlimitations of existing 3D human reconstruction methods. Each human part, such\nas a jacket or pants, should maintain a distinct texture without blending into\nothers. The structural coherence of human parts serves as a crucial cue to\ninfer human textures in the invisible regions of a single image. However, most\nexisting 3D human reconstruction methods do not explicitly exploit such part\nsegmentation priors, leading to misaligned textures in their reconstructions.\nIn this regard, we present PARTE, which utilizes 3D human part information as a\nkey guide to reconstruct 3D human textures. Our framework comprises two core\ncomponents. First, to infer 3D human part information from a single image, we\npropose a 3D part segmentation module (PartSegmenter) that initially\nreconstructs a textureless human surface and predicts human part labels based\non the textureless surface. Second, to incorporate part information into\ntexture reconstruction, we introduce a part-guided texturing module\n(PartTexturer), which acquires prior knowledge from a pre-trained image\ngeneration network on texture alignment of human parts. Extensive experiments\ndemonstrate that our framework achieves state-of-the-art quality in 3D human\nreconstruction. The project page is available at\nhttps://hygenie1228.github.io/PARTE/.", "comment": "Published at ICCV 2025, 22 pages including the supplementary material", "pdf_url": "http://arxiv.org/pdf/2507.17332v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "PARTE：单图三维人体重建中的部件引导纹理化", "tldr": "PARTE提出了一种利用3D人体部件信息进行纹理重建的新方法，解决了现有方法中纹理错位的问题，实现了单图3D人体重建的SOTA质量。", "motivation": "现有3D人体重建方法的主要局限性在于不同人体部件的纹理错位，导致纹理融合而非保持独立。大多数方法没有明确利用部件分割先验知识，使得重建纹理错位。", "method": "本文提出了PARTE框架，利用3D人体部件信息作为关键指导来重建3D人体纹理。该框架包含两个核心组件：1. 3D部件分割模块（PartSegmenter），用于从单图推断3D人体部件信息，通过重建无纹理人体表面并预测部件标签。2. 部件引导纹理化模块（PartTexturer），通过从预训练的图像生成网络中获取纹理对齐的先验知识，将部件信息融入纹理重建。", "result": "广泛的实验表明，PARTE框架在3D人体重建方面达到了最先进的质量。", "conclusion": "PARTE通过显式利用3D人体部件信息，有效解决了单图3D人体重建中纹理错位的问题，显著提升了重建质量。", "translation": "PARTE：单图三维人体重建中的部件引导纹理化\n\n现有三维人体重建方法的主要限制之一是不同人体部位的纹理错位。每个身体部位，例如夹克或裤子，都应保持独特的纹理，而不是相互融合。人体部位的结构连贯性是推断单幅图像中不可见区域人体纹理的关键线索。然而，大多数现有三维人体重建方法并未明确利用这种部位分割先验知识，导致其重建中出现纹理错位。鉴于此，我们提出了PARTE，它利用三维人体部位信息作为关键指导来重建三维人体纹理。我们的框架包含两个核心组件。首先，为了从单幅图像推断三维人体部位信息，我们提出了一个三维部位分割模块（PartSegmenter），该模块首先重建一个无纹理的人体表面，并根据该无纹理表面预测人体部位标签。其次，为了将部位信息融入纹理重建，我们引入了一个部位引导纹理化模块（PartTexturer），该模块从一个预训练的图像生成网络中获取人体部位纹理对齐的先验知识。广泛的实验表明，我们的框架在三维人体重建方面达到了最先进的质量。项目页面可在https://hygenie1228.github.io/PARTE/访问。", "summary": "PARTE提出了一种新颖的3D人体重建方法，旨在解决现有技术中纹理错位的问题。该方法通过引入3D人体部件信息作为核心指导，包含一个3D部件分割模块（PartSegmenter）用于推断部件信息，以及一个部件引导纹理化模块（PartTexturer）用于整合部件先验知识进行纹理重建。实验证明，PARTE在3D人体重建质量上达到了最先进水平。", "keywords": "3D人体重建, 纹理化, 部件引导, 单图重建, 纹理对齐", "comments": "PARTE的创新点在于明确利用3D人体部件信息来指导纹理重建，这解决了现有方法中普遍存在的纹理错位问题。通过引入PartSegmenter和PartTexturer两个模块，它系统地将部件结构先验融入到纹理生成过程中，对于提升单图3D人体重建的真实感和细节精度具有重要意义。其核心贡献在于强调了部件级纹理一致性对于高质量重建的重要性。"}}
{"id": "2507.17852", "title": "Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "categories": ["cs.MA", "cs.AI"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17852v1", "summary": "Building on the conceptual framework presented in our previous work on\nagentic AI for pharmaceutical research, this paper provides a comprehensive\ntechnical analysis of Tippy's multi-agent system implementation for drug\ndiscovery laboratory automation. We present a distributed microservices\narchitecture featuring five specialized agents (Supervisor, Molecule, Lab,\nAnalysis, and Report) that coordinate through OpenAI Agents SDK orchestration\nand access laboratory tools via the Model Context Protocol (MCP). The system\narchitecture encompasses agent-specific tool integration, asynchronous\ncommunication patterns, and comprehensive configuration management through\nGit-based tracking. Our production deployment strategy utilizes Kubernetes\ncontainer orchestration with Helm charts, Docker containerization, and CI/CD\npipelines for automated testing and deployment. The implementation integrates\nvector databases for RAG functionality and employs an Envoy reverse proxy for\nsecure external access. This work demonstrates how specialized AI agents can\neffectively coordinate complex laboratory workflows while maintaining security,\nscalability, reliability, and integration with existing laboratory\ninfrastructure through standardized protocols.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17852v1", "cate": "cs.MA", "date": "2025-07-18", "updated": "2025-07-18", "AI": {"title_translation": "Tippy的技术实现：药物发现实验室自动化的多智能体架构与系统设计", "tldr": "本文详细介绍了Tippy的多智能体系统在药物发现实验室自动化中的技术实现，包括其分布式微服务架构、五种专业智能体、OpenAI Agents SDK编排、MCP协议以及Kubernetes部署策略，旨在展示AI智能体如何有效协调复杂的实验室工作流。", "motivation": "在之前关于制药研究中智能体AI概念框架的基础上，本文旨在提供Tippy多智能体系统在药物发现实验室自动化方面的全面技术分析和实现细节。", "method": "论文介绍了一个分布式微服务架构，包含五种专业智能体（主管、分子、实验室、分析和报告），通过OpenAI Agents SDK进行编排，并通过模型上下文协议（MCP）访问实验室工具。该系统架构包括智能体特定工具集成、异步通信模式和基于Git的全面配置管理。生产部署策略采用Kubernetes容器编排与Helm charts、Docker容器化和CI/CD管道。实现中集成了向量数据库用于RAG功能，并使用Envoy反向代理实现安全外部访问。", "result": "本工作展示了专业AI智能体如何有效地协调复杂的实验室工作流，同时保持安全性、可扩展性、可靠性，并通过标准化协议与现有实验室基础设施集成。", "conclusion": "通过构建和部署一个多智能体系统，本研究证明了专业AI智能体能够高效、安全、可扩展地实现药物发现实验室的自动化，并与现有基础设施无缝集成。", "translation": "基于我们之前关于制药研究中智能体AI的概念框架，本文对Tippy的多智能体系统在药物发现实验室自动化中的实现提供了全面的技术分析。我们提出了一个分布式微服务架构，该架构包含五种专业智能体（主管、分子、实验室、分析和报告），它们通过OpenAI Agents SDK进行编排协调，并通过模型上下文协议（MCP）访问实验室工具。该系统架构涵盖了智能体特定的工具集成、异步通信模式以及通过基于Git跟踪的全面配置管理。我们的生产部署策略利用Kubernetes容器编排与Helm charts、Docker容器化和CI/CD管道进行自动化测试和部署。该实现集成了向量数据库以实现RAG功能，并采用Envoy反向代理进行安全外部访问。这项工作展示了专业AI智能体如何有效地协调复杂的实验室工作流，同时通过标准化协议保持安全性、可扩展性、可靠性并与现有实验室基础设施集成。", "summary": "本文详细阐述了Tippy多智能体系统在药物发现实验室自动化中的技术实现。该系统采用分布式微服务架构，包含五种专业智能体，并通过OpenAI Agents SDK进行协调和MCP协议访问实验室工具。其部署策略包括Kubernetes、Docker和CI/CD管道，并集成向量数据库和Envoy反向代理。研究证明了专业AI智能体能有效协调复杂实验室工作流，同时确保系统安全性、可扩展性、可靠性及与现有基础设施的集成。", "keywords": "多智能体系统, 药物发现, 实验室自动化, 微服务架构, Kubernetes", "comments": "这篇论文的创新点在于其详细阐述了一个针对药物发现实验室自动化的多智能体系统Tippy的具体技术实现。它不仅提出了一个包含五种专业智能体的分布式微服务架构，还结合了OpenAI Agents SDK进行编排，并通过Model Context Protocol (MCP) 实现工具访问。其生产级部署策略，包括Kubernetes、Docker和CI/CD，以及对RAG功能和安全访问的考虑，展示了将复杂AI系统落地到实际工业场景的全面性和工程严谨性。这对于推动AI在生命科学领域的自动化应用具有重要意义。"}}
{"id": "2507.18436", "title": "Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning", "authors": ["David Blanco-Mulero", "Júlia Borràs", "Carme Torras"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project website: this https URL", "url": "http://arxiv.org/abs/2507.18436v1", "summary": "Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration.", "comment": "6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project\n  website: https://sites.google.com/view/pre-dressing", "pdf_url": "http://arxiv.org/pdf/2507.18436v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "评估预穿戴步骤：通过模仿学习展开医用服装", "tldr": "本文介绍了机器人辅助穿戴前的预穿戴步骤，即展开折叠的医用服装。研究利用模仿学习训练操作原语，并结合视觉分类器来识别服装状态，实验结果显示动态运动不适用于新拆封的服装，而动作组合能有效提高展开效率。", "motivation": "机器人辅助穿戴在减轻医疗人员工作量和提高效率方面具有巨大潜力。然而，现有研究通常假设服装已展开。在医疗应用中，医用服装常以折叠状态存放，需要额外的展开步骤，这促使了“预穿戴步骤”的研究。", "method": "本文引入了“预穿戴步骤”，即在辅助穿戴前展开服装的过程。研究利用模仿学习来学习三种操作原语（包括高加速和低加速运动），并采用视觉分类器将服装状态分为“闭合”、“部分打开”和“完全打开”三类。", "result": "研究对学习到的操作原语及其组合进行了实证评估。结果表明，高度动态的运动对于展开新拆封的服装效果不佳，而动作的组合可以有效提高展开配置。", "conclusion": "通过模仿学习和视觉分类器，机器人可以有效执行医用服装的预展开步骤。研究发现，组合动作比单一的高度动态运动更能有效地展开新拆封的服装。", "translation": "机器人辅助穿戴具有显著帮助患者和医护人员的潜力，可以减轻工作量并提高临床环境的效率。尽管机器人穿戴辅助方面取得了实质性进展，但以往的工作通常假设服装已经展开并可供使用。然而，在医疗应用中，长袍和围裙通常以折叠状态存放，需要额外的展开步骤。在本文中，我们引入了预穿戴步骤，即在辅助穿戴之前展开服装的过程。我们利用模仿学习来学习三种操作原语，包括高加速和低加速运动。此外，我们采用视觉分类器将服装状态分类为闭合、部分打开和完全打开。我们对学习到的操作原语及其组合进行了实证评估。我们的结果表明，高度动态的运动对于展开新拆封的服装效果不佳，而动作的组合可以有效提高展开配置。", "summary": "本文提出并评估了机器人辅助穿戴中的“预穿戴步骤”，即通过模仿学习使机器人能够展开折叠的医用服装。研究利用模仿学习训练了多种操作原语，并结合视觉分类器识别服装的展开状态。实验结果表明，对于新拆封的服装，组合动作比单一的高动态运动在展开效率上表现更优。", "keywords": "机器人辅助穿戴, 模仿学习, 服装展开, 柔性物体操作, 预穿戴", "comments": "该论文解决了机器人辅助穿戴领域中一个实际且重要的缺失环节——服装的预展开。通过引入“预穿戴步骤”和采用模仿学习，为机器人处理复杂、柔性物体提供了新的思路。特别是对不同运动模式和组合效果的评估，为未来的柔性物体操作研究提供了有价值的见解。创新点在于将模仿学习应用于服装展开这一具体的预处理任务，并强调了动作组合的重要性。"}}
{"id": "2507.18510", "title": "ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR", "authors": ["Chenyang Zhang", "Tiffany S Ma", "John Andrews", "Eric J Gonzalez", "Mar Gonzalez-Franco", "Yalong Yang"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18510v1", "summary": "Spatial interaction in 3D environments requires balancing efficiency and\nprecision, which requires dynamic tracking speed adjustments. However, existing\ntechniques often couple tracking speed adjustments directly with hand\nmovements, reducing interaction flexibility. Inspired by the natural friction\ncontrol inherent in the physical world, we introduce ForcePinch, a novel\nforce-responsive spatial interaction method that enables users to intuitively\nmodulate pointer tracking speed and smoothly transition between rapid and\nprecise movements by varying their pinching force. To implement this concept,\nwe developed a hardware prototype integrating a pressure sensor with a\ncustomizable mapping function that translates pinching force into tracking\nspeed adjustments. We conducted a user study with 20 participants performing\nwell-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch\nagainst the distance-responsive technique Go-Go and speed-responsive technique\nPRISM. Results highlight distinctive characteristics of the force-responsive\napproach across different interaction contexts. Drawing on these findings, we\nhighlight the contextual meaning and versatility of force-responsive\ninteractions through four illustrative examples, aiming to inform and inspire\nfuture spatial interaction design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18510v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ForcePinch：用于XR中跟踪速度控制的力响应空间交互", "tldr": "ForcePinch引入了一种力响应空间交互方法，通过捏合力调节指针跟踪速度，实现XR中快速和精确移动之间的平滑过渡。", "motivation": "在3D环境中进行空间交互需要平衡效率和精度，这要求动态调整跟踪速度。然而，现有技术常将跟踪速度调整与手部动作直接耦合，降低了交互灵活性。", "method": "论文引入了ForcePinch，一种新型的力响应空间交互方法，允许用户通过改变捏合力来直观地调节指针跟踪速度。为此，开发了一个硬件原型，该原型集成了压力传感器和可定制的映射函数，将捏合力转换为跟踪速度调整。通过一项包含20名参与者的用户研究，在1D、2D和3D对象操作任务中，将ForcePinch与距离响应技术Go-Go和速度响应技术PRISM进行了比较。", "result": "结果突出了力响应方法在不同交互环境中的独特特性。", "conclusion": "论文基于研究结果，通过四个说明性示例强调了力响应交互的语境意义和多功能性，旨在为未来的空间交互设计提供信息和启发。", "translation": "3D环境中的空间交互需要平衡效率和精度，这要求动态调整跟踪速度。然而，现有技术通常将跟踪速度调整与手部动作直接耦合，降低了交互灵活性。受物理世界中固有的自然摩擦控制启发，我们引入了ForcePinch，一种新型的力响应空间交互方法，它允许用户通过改变捏合力来直观地调节指针跟踪速度，并在快速和精确移动之间平滑过渡。为了实现这一概念，我们开发了一个硬件原型，该原型集成了压力传感器和可定制的映射函数，将捏合力转换为跟踪速度调整。我们对20名参与者进行了一项用户研究，他们执行了成熟的1D、2D和3D对象操作任务，并将ForcePinch与距离响应技术Go-Go和速度响应技术PRISM进行了比较。结果突出了力响应方法在不同交互环境中的独特特性。基于这些发现，我们通过四个说明性示例强调了力响应交互的语境意义和多功能性，旨在为未来的空间交互设计提供信息和启发。", "summary": "ForcePinch提出了一种在XR中通过捏合力控制跟踪速度的新型空间交互方法，旨在解决现有技术中跟踪速度与手部动作耦合导致灵活性降低的问题。该方法通过一个集成了压力传感器的硬件原型实现，用户可以通过调节捏合力来直观地控制指针速度，从而在快速和精确移动之间平滑切换。用户研究表明，力响应方法在不同交互上下文中表现出独特的特性，为未来的空间交互设计提供了新的思路和多功能性。", "keywords": "力响应, 空间交互, 跟踪速度控制, XR, ForcePinch", "comments": "ForcePinch的创新点在于将物理世界中的“摩擦控制”概念引入到XR空间交互中，通过力反馈（捏合力）实现跟踪速度的动态调节，这比传统的手部移动耦合方式更具灵活性和直观性。其重要性在于为XR环境中的效率与精度平衡提供了一种新的解决方案，并可能启发未来更多基于力反馈的交互设计。"}}
{"id": "2507.18167", "title": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning", "authors": ["Yuxuan Wen", "Xiaoming Chen", "Maojun Zhang", "Zhaoyang Zhang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18167v1", "summary": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18167v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ICWLM: 一种基于上下文学习的多任务无线大模型", "tldr": "ICWLM是一个新的无线原生基础模型，通过上下文学习实现物理层多任务处理，解决了传统深度学习方法在无线通信中面临的数据稀缺和泛化性差的问题。", "motivation": "无线通信技术（如大规模MIMO和毫米波）的快速发展带来了显著的网络复杂性和计算需求。现有的深度学习方法虽然能改善物理层性能，但通常是任务特定的，并且面临数据稀缺和泛化能力不足的挑战。", "method": "本文提出了一种新颖的上下文无线大模型（ICWLM），这是一种无线原生基础模型，专为物理层的同步多任务学习而设计。ICWLM直接在大规模混合无线数据集上从头开始训练，共同解决了多用户预编码（和速率最大化、最大最小SINR）和信道预测等多个经典物理层问题。其关键创新在于利用上下文学习（ICL），使模型能够以最少的演示对适应不同的系统配置和信道条件，无需大量重新训练。此外，采用动态权重平均（DWA）算法在多任务训练期间动态平衡各个任务损失。", "result": "仿真结果表明，ICWLM与任务特定方法相比，取得了具有竞争力的性能，并对未见的系统配置表现出卓越的泛化能力。", "conclusion": "这项工作为未来无线网络开发统一和自适应的AI模型提供了一个有前景的范例，有望降低部署复杂性并增强智能资源管理。", "translation": "无线通信技术，特别是大规模多输入多输出（mMIMO）和毫米波（mmWave）的快速发展，带来了显著的网络复杂性和计算需求。为提高物理层性能，已通过深度学习（DL）方法进行了大量研究，然而这些方法通常是任务特定的，并且面临数据稀缺和泛化性差的问题。为解决这些挑战，我们提出了一种新颖的上下文无线大模型（ICWLM），这是一种无线原生基础模型，旨在物理层实现同步多任务学习。与将无线数据适配到预训练大语言模型（LLMs）的传统方法不同，ICWLM直接在大规模混合无线数据集上从头开始训练。它共同解决了多个经典的物理层问题，包括多用户预编码（和速率最大化和最大最小SINR）和信道预测。ICWLM的一个关键创新是利用上下文学习（ICL），使模型能够以最少的演示对适应不同的系统配置和信道条件，从而无需大量重新训练。此外，我们采用动态权重平均（DWA）算法在多任务训练期间动态平衡各个任务损失，确保跨不同目标的有效和稳定学习。广泛的仿真结果表明，与任务特定方法相比，ICWLM取得了竞争性的性能，同时对未见的系统配置表现出卓越的泛化能力。这项工作为未来无线网络开发统一和自适应的AI模型提供了一个有前景的范例，有望降低部署复杂性并增强智能资源管理。", "summary": "本文提出了ICWLM，一个无线原生的基础模型，用于解决物理层多任务学习的挑战。针对传统深度学习在无线通信中数据稀缺和泛化性差的问题，ICWLM通过从头开始在大规模混合数据集上训练，并利用上下文学习和动态权重平均算法，实现了多用户预编码和信道预测等任务的联合求解。实验结果表明，ICWLM在性能上具有竞争力，并展现出优异的泛化能力，为未来无线网络提供了一种统一且自适应的AI模型范例。", "keywords": "无线大模型, 上下文学习, 多任务学习, 物理层, 泛化能力", "comments": "ICWLM的创新之处在于其作为无线原生基础模型的定位，直接在无线数据上训练，并引入上下文学习（ICL）以实现高效适应不同系统配置，显著减少了模型重新训练的需求。此外，采用DWA算法平衡多任务学习过程，增强了模型的稳定性和效率。这项工作为构建统一、自适应的未来无线网络AI模型提供了重要思路，有望简化部署和提升资源管理智能化水平。"}}
{"id": "2507.18577", "title": "Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges", "authors": ["Liyuan Chen", "Shuoling Liu", "Jiangpeng Yan", "Xiaoyu Wang", "Henglin Liu", "Chuang Li", "Kecheng Jiao", "Jixuan Ying", "Yang Veronica Liu", "Qiang Yang", "Xiu Li"], "categories": ["q-fin.CP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.18577v1", "summary": "The advent of foundation models (FMs) - large-scale pre-trained models with\nstrong generalization capabilities - has opened new frontiers for financial\nengineering. While general-purpose FMs such as GPT-4 and Gemini have\ndemonstrated promising performance in tasks ranging from financial report\nsummarization to sentiment-aware forecasting, many financial applications\nremain constrained by unique domain requirements such as multimodal reasoning,\nregulatory compliance, and data privacy. These challenges have spurred the\nemergence of Financial Foundation Models (FFMs) - a new class of models\nexplicitly designed for finance. This survey presents a comprehensive overview\nof FFMs, with a taxonomy spanning three key modalities: Financial Language\nFoundation Models (FinLFMs), Financial Time-Series Foundation Models\n(FinTSFMs), and Financial Visual-Language Foundation Models (FinVLFMs). We\nreview their architectures, training methodologies, datasets, and real-world\napplications. Furthermore, we identify critical challenges in data\navailability, algorithmic scalability, and infrastructure constraints, and\noffer insights into future research opportunities. We hope this survey serves\nas both a comprehensive reference for understanding FFMs and a practical\nroadmap for future innovation. An updated collection of FFM-related\npublications and resources will be maintained on our website\nhttps://github.com/FinFM/Awesome-FinFMs.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.18577v1", "cate": "q-fin.CP", "date": "2025-07-07", "updated": "2025-07-07", "AI": {"title_translation": "利用基础模型推进金融工程：进展、应用与挑战", "tldr": "本综述全面概述了金融基础模型 (FFMs)，包括其分类、架构、训练、应用和挑战，并展望了未来研究方向。", "motivation": "通用基础模型（如GPT-4和Gemini）在金融任务中表现出潜力，但金融领域独特的模态推理、监管合规和数据隐私等要求限制了它们的广泛应用。这促使了专门为金融领域设计的基础模型（FFMs）的出现。本综述旨在对FFMs进行全面概述，以推动金融工程的进步。", "method": "本综述对金融基础模型（FFMs）进行了全面概述，提出了一个涵盖金融语言基础模型（FinLFMs）、金融时间序列基础模型（FinTSFMs）和金融视觉语言基础模型（FinVLFMs）的三种关键模态的分类法。文章审查了它们的架构、训练方法、数据集和实际应用，并识别了数据可用性、算法可扩展性和基础设施限制方面的关键挑战。", "result": "本综述提出了金融基础模型（FFMs）的三种关键模态分类：金融语言基础模型（FinLFMs）、金融时间序列基础模型（FinTSFMs）和金融视觉语言基础模型（FinVLFMs）。文章回顾了这些模型的架构、训练方法、数据集和实际应用，并识别了数据可用性、算法可扩展性和基础设施限制等关键挑战。", "conclusion": "本综述旨在为理解金融基础模型（FFMs）提供全面参考，并为未来的创新提供实用路线图。作者希望通过这份综述促进FFMs在金融工程领域的研究和发展。", "translation": "基础模型（FMs）——具有强大泛化能力的大规模预训练模型——的出现为金融工程开辟了新前沿。虽然GPT-4和Gemini等通用基础模型在从金融报告摘要到情感感知预测等任务中表现出有希望的性能，但许多金融应用仍受限于独特的领域要求，例如多模态推理、监管合规和数据隐私。这些挑战促使了金融基础模型（FFMs）的出现——这是一类专门为金融领域设计的新模型。本综述全面概述了FFMs，其分类法涵盖三种关键模态：金融语言基础模型（FinLFMs）、金融时间序列基础模型（FinTSFMs）和金融视觉语言基础模型（FinVLFMs）。我们审查了它们的架构、训练方法、数据集和实际应用。此外，我们还指出了数据可用性、算法可扩展性和基础设施限制方面的关键挑战，并提供了对未来研究机会的见解。我们希望本综述既能作为理解FFMs的全面参考，又能作为未来创新的实用路线图。FFM相关出版物和资源的最新集合将维护在我们的网站https://github.com/FinFM/Awesome-FinFMs上。", "summary": "本综述探讨了基础模型在金融工程领域的应用，特别关注了为满足金融领域独特需求而开发的金融基础模型（FFMs）。文章提出了FFMs的三种关键模态分类（FinLFMs、FinTSFMs、FinVLFMs），详细审视了其架构、训练方法、数据集和实际应用。此外，论文还指出了数据可用性、算法可扩展性和基础设施等方面的挑战，并展望了未来的研究机会，旨在为FFMs的研究和创新提供参考和指导。", "keywords": "金融基础模型, 金融工程, 基础模型, 语言模型, 时间序列模型", "comments": "这篇综述及时地梳理了金融基础模型这一新兴且重要的领域。其创新之处在于提出了一个清晰的FFMs分类法，并系统地分析了其进展、应用和面临的挑战。这对于研究人员和从业者理解该领域的现状和未来方向具有重要指导意义。其局限性可能在于作为一篇综述，它本身不提出新的模型或方法，但其价值在于整合现有知识并指明未来方向。"}}
{"id": "2507.17904", "title": "PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training", "authors": ["Talha Mehboob", "Luanzheng Guo", "Nathan Tallent", "Michael Zink", "David Irwin"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17904v1", "summary": "The exponential growth of large-scale AI models has led to computational and\npower demands that can exceed the capacity of a single data center. This is due\nto the limited power supplied by regional grids that leads to limited regional\ncomputational power. Consequently, distributing training workloads across\ngeographically distributed sites has become essential. However, this approach\nintroduces a significant challenge in the form of communication overhead,\ncreating a fundamental trade-off between the performance gains from accessing\ngreater aggregate power and the performance losses from increased network\nlatency. Although prior work has focused on reducing communication volume or\nusing heuristics for distribution, these methods assume constant homogeneous\npower supplies and ignore the challenge of heterogeneous power availability\nbetween sites.\n  To address the challenge of training large models in power-constrained,\ngeo-distributed environments, we introduce PowerTrip, a system that dynamically\nselects a subset of sites during runtime to optimize the power-communication\ntrade-off. Specifically, PowerTrip selects sites based on a power-to-cost\nheuristic, prioritizing those with high power availability and low network\nlatency. PowerTrip employs a dynamic greedy approach and uses the marginal gain\nin training efficiency, i.e., accuracy improvement per unit of time, to\noptimize for the number of sites where the performance penalty from network\noverhead negates the benefit of adding more computational power. Our\nevaluation, which uses real-world Google power traces to model realistic power\ncapacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy\nby up to 50% compared to existing baseline policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17904v1", "cate": "cs.DC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PowerTrip：利用联邦异构数据中心电源进行分布式机器学习训练", "tldr": "PowerTrip系统动态选择数据中心子集，优化功耗-通信权衡，以加速分布式机器学习训练，尤其是在电源受限的环境中。", "motivation": "大型AI模型增长导致计算和功耗需求超出单个数据中心容量，需要跨地理分布式站点训练。现有方法未考虑异构电源可用性，且存在通信开销与性能提升的权衡问题。", "method": "引入PowerTrip系统，在运行时动态选择站点子集以优化功耗-通信权衡。它基于功耗-成本启发式方法选择站点（优先选择高功耗可用性、低网络延迟的站点），并采用动态贪婪方法，利用训练效率的边际收益（单位时间精度提升）来优化站点数量。", "result": "使用真实的Google功耗轨迹进行评估，PowerTrip可以将达到相同精度的时间（time-to-accuracy）缩短高达50%，优于现有基线策略。", "conclusion": "PowerTrip有效解决了在电源受限的地理分布式环境中训练大型模型的挑战，显著提高了分布式机器学习训练的效率。", "translation": "大规模人工智能模型的指数级增长导致计算和功耗需求可能超出单个数据中心的容量。这是由于区域电网供电有限，从而导致区域计算能力受限。因此，将训练工作负载分布到地理上分散的站点变得至关重要。然而，这种方法引入了通信开销这一重大挑战，在从获取更大总功耗中获得的性能提升与因网络延迟增加而导致的性能损失之间产生了根本性的权衡。尽管先前的工作侧重于减少通信量或使用启发式方法进行分布，但这些方法假设电源供应是恒定同质的，并忽略了站点之间异构电源可用性的挑战。\n为了解决在电源受限、地理分布式环境中训练大型模型的挑战，我们引入了PowerTrip，一个在运行时动态选择站点子集以优化功耗-通信权衡的系统。具体来说，PowerTrip根据功耗-成本启发式方法选择站点，优先选择功耗可用性高和网络延迟低的站点。PowerTrip采用动态贪婪方法，并利用训练效率的边际收益（即单位时间精度提升）来优化站点数量，以避免网络开销的性能损失抵消增加计算能力的收益。我们的评估使用真实的谷歌功耗轨迹来模拟实际的功耗容量限制，结果表明，与现有基线策略相比，PowerTrip可以将达到相同精度的时间缩短高达50%。", "summary": "PowerTrip是一个针对分布式机器学习训练的系统，旨在解决大型AI模型在电源受限、地理分布式环境中面临的计算和功耗挑战。它通过动态选择数据中心子集，基于功耗可用性和网络延迟的权衡，优化训练效率。实验表明，PowerTrip能将达到相同精度的时间缩短高达50%。", "keywords": "分布式机器学习, 异构数据中心, 功耗管理, 资源调度", "comments": "这篇论文的创新点在于它首次明确地解决了分布式机器学习训练中异构电源可用性的问题，并提出了一个动态的、基于功耗-成本启发式的站点选择策略。它克服了以往研究中假设同质电源供应的局限性，使得在实际复杂环境中进行大规模AI模型训练成为可能。其实证结果也很有说服力，展示了显著的性能提升。"}}
{"id": "2407.03605", "title": "Orthogonal Constrained Minimization with Tensor $\\ell_{2,p}$ Regularization for HSI Denoising and Destriping", "authors": ["Xiaoxia Liu", "Shijie Yu", "Jian Lu", "Xiaojun Chen"], "categories": ["math.OC", "cs.CV", "68U10, 90C26, 15A18, 65F22"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.03605v3", "summary": "Hyperspectral images~(HSIs) are often contaminated by a mixture of noise such\nas Gaussian noise, dead lines, stripes, and so on. In this paper, we propose a\nmulti-scale low-rank tensor regularized $\\ell_{2,p}$ (MLTL2p) approach for HSI\ndenoising and destriping, which consists of an orthogonal constrained\nminimization model and an iterative algorithm with convergence guarantees. The\nmodel of the proposed MLTL2p approach is built based on a new sparsity-enhanced\nMulti-scale Low-rank Tensor regularization and a tensor $\\ell_{2,p}$ norm with\n\\(p\\in (0,1)\\). The multi-scale low-rank regularization for HSI denoising\nutilizes the global and local spectral correlation as well as the spatial\nnonlocal self-similarity priors of HSIs. The corresponding low-rank constraints\nare formulated based on independent higher-order singular value decomposition\nwith sparsity enhancement on its core tensor to prompt more low-rankness. The\ntensor $\\ell_{2,p}$ norm for HSI destriping is extended from the matrix\n$\\ell_{2,p}$ norm. A proximal block coordinate descent algorithm is proposed in\nthe MLTL2p approach to solve the resulting nonconvex nonsmooth minimization\nwith orthogonal constraints. We show any accumulation point of the sequence\ngenerated by the proposed algorithm converges to a first-order stationary\npoint, which is defined using three equalities of substationarity, symmetry,\nand feasibility for orthogonal constraints. In the numerical experiments, we\ncompare the proposed method with state-of-the-art methods including a deep\nlearning based method, and test the methods on both simulated and real HSI\ndatasets. Our proposed MLTL2p method demonstrates outperformance in terms of\nmetrics such as mean peak signal-to-noise ratio as well as visual quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.03605v3", "cate": "math.OC", "date": "2024-07-04", "updated": "2025-07-24", "AI": {"title_translation": "具有张量 $\\ell_{2,p}$ 正则化的正交约束最小化用于高光谱图像去噪和去条纹", "tldr": "本文提出了一种名为MLTL2p的新型高光谱图像去噪和去条纹方法，该方法结合了稀疏性增强的多尺度低秩张量正则化和张量$\\ell_{2,p}$范数，并在实验中表现出优异性能。", "motivation": "高光谱图像（HSIs）经常受到高斯噪声、死线、条纹等多种噪声的混合污染，需要有效的去噪和去条纹方法。", "method": "提出了一种多尺度低秩张量正则化$\\ell_{2,p}$ (MLTL2p) 方法，用于高光谱图像去噪和去条纹。该方法包含一个基于稀疏性增强多尺度低秩张量正则化和张量$\\ell_{2,p}$范数（p∈(0,1)）的正交约束最小化模型。多尺度低秩正则化利用了HSIs的全局和局部光谱相关性以及空间非局部自相似先验。低秩约束通过对核心张量进行稀疏性增强的独立高阶奇异值分解来制定。张量$\\ell_{2,p}$范数从矩阵$\\ell_{2,p}$范数扩展而来。采用近端块坐标下降算法求解由此产生的非凸非光滑带正交约束的最小化问题，并证明了算法生成的序列的任何累积点都收敛到一阶驻点。", "result": "提出的MLTL2p方法在均方峰值信噪比等指标以及视觉质量方面表现出优异性能，优于包括基于深度学习方法在内的现有先进方法，并在模拟和真实高光谱图像数据集上进行了测试。", "conclusion": "Not mentioned in abstract", "translation": "高光谱图像（HSIs）通常受到高斯噪声、死线、条纹等多种噪声的混合污染。在本文中，我们提出了一种多尺度低秩张量正则化$\\ell_{2,p}$（MLTL2p）方法，用于高光谱图像去噪和去条纹，该方法包含一个正交约束最小化模型和一个具有收敛性保证的迭代算法。所提出的MLTL2p方法模型建立在一种新的稀疏性增强多尺度低秩张量正则化和张量$\\ell_{2,p}$范数（其中$p\\in (0,1)$）的基础上。用于HSI去噪的多尺度低秩正则化利用了HSIs的全局和局部光谱相关性以及空间非局部自相似先验。相应的低秩约束是基于独立的、对核心张量进行稀疏性增强的高阶奇异值分解来制定的，以促使更多的低秩性。用于HSI去条纹的张量$\\ell_{2,p}$范数是从矩阵$\\ell_{2,p}$范数扩展而来的。在MLTL2p方法中，提出了一种近端块坐标下降算法来解决由此产生的非凸非光滑带正交约束的最小化问题。我们证明了所提出的算法生成的序列的任何累积点都收敛到一个一阶驻点，该驻点使用正交约束的次驻点、对称性和可行性三个等式来定义。在数值实验中，我们将所提出的方法与包括基于深度学习方法在内的现有先进方法进行了比较，并在模拟和真实HSI数据集上测试了这些方法。我们提出的MLTL2p方法在均方峰值信噪比等指标以及视觉质量方面表现出优异性能。", "summary": "本文提出了一种名为MLTL2p的新型高光谱图像去噪和去条纹方法，该方法结合了稀疏性增强的多尺度低秩张量正则化和张量$\\ell_{2,p}$范数。通过构建正交约束最小化模型并设计具有收敛性保证的近端块坐标下降算法进行求解。实验结果表明，MLTL2p在去噪和去条纹方面优于现有先进方法，在多项指标和视觉质量上表现出色。", "keywords": "高光谱图像, 去噪, 去条纹, 张量$\\ell_{2,p}$正则化, 低秩", "comments": "本文的创新点在于结合了多尺度低秩张量正则化和张量$\\ell_{2,p}$范数来解决高光谱图像的混合噪声问题，并设计了具有理论收敛性保证的优化算法。这种方法有效地利用了高光谱图像的内在结构先验，并在性能上超越了包括深度学习方法在内的现有技术，显示了其在高光谱图像处理领域的实用价值和重要性。"}}
{"id": "2507.18478", "title": "Scout: Leveraging Large Language Models for Rapid Digital Evidence Discovery", "authors": ["Shariq Murtuza"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18478v1", "summary": "Recent technological advancements and the prevalence of technology in day to\nday activities have caused a major increase in the likelihood of the\ninvolvement of digital evidence in more and more legal investigations.\nConsumer-grade hardware is growing more powerful, with expanding memory and\nstorage sizes and enhanced processor capabilities. Forensics investigators\noften have to sift through gigabytes of data during an ongoing investigation\nmaking the process tedious. Memory forensics, disk analysis all are well\nsupported by state of the art tools that significantly lower the effort\nrequired to be put in by a forensic investigator by providing string searches,\nanalyzing images file etc. During the course of the investigation a lot of\nfalse positives are identified that need to be lowered. This work presents\nScout, a digital forensics framework that performs preliminary evidence\nprocessing and prioritizing using large language models. Scout deploys\nfoundational language models to identify relevant artifacts from a large number\nof potential evidence files (disk images, captured network packets, memory\ndumps etc.) which would have taken longer to get identified. Scout employs text\nbased large language models can easily process files with textual information.\nFor the forensic analysis of multimedia files like audio, image, video, office\ndocuments etc. multimodal models are employed by Scout. Scout was able to\nidentify and realize the evidence file that were of potential interest for the\ninvestigator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18478v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Scout：利用大型语言模型实现快速数字证据发现", "tldr": "Scout是一个数字取证框架，它利用大型语言模型对大量数字证据进行初步处理和优先级排序，从而加速证据发现过程。", "motivation": "随着技术进步和数字证据在法律调查中日益普遍，取证调查员需要处理海量数据，这一过程耗时且易产生大量误报。尽管现有工具能辅助分析，但仍需进一步降低调查员的工作量并减少误报。", "method": "本文提出了Scout，一个数字取证框架，利用大型语言模型进行初步证据处理和优先级排序。Scout部署基础语言模型从大量潜在证据文件（如磁盘镜像、捕获的网络数据包、内存转储等）中识别相关工件，对于文本信息文件使用基于文本的大型语言模型，对于多媒体文件（如音频、图像、视频、办公文档等）则采用多模态模型进行取证分析。", "result": "Scout能够识别并发现对调查人员具有潜在兴趣的证据文件。", "conclusion": "Scout框架通过利用大型语言模型和多模态模型，有效加速了数字证据的识别和初步处理，从而显著提高了数字取证的效率。", "translation": "最近的技术进步和技术在日常活动中的普及导致数字证据在越来越多的法律调查中涉及的可能性大大增加。消费级硬件变得越来越强大，内存和存储容量不断扩大，处理器能力也得到增强。取证调查员在正在进行的调查中经常需要筛选数千兆字节的数据，这使得过程变得繁琐。内存取证、磁盘分析都得到了最先进工具的良好支持，这些工具通过提供字符串搜索、分析图像文件等功能，显著降低了取证调查员所需投入的精力。在调查过程中，会识别出许多需要降低的误报。这项工作提出了Scout，一个数字取证框架，它使用大型语言模型执行初步证据处理和优先级排序。Scout部署基础语言模型，从大量潜在证据文件（磁盘镜像、捕获的网络数据包、内存转储等）中识别相关工件，而这些文件通常需要更长时间才能被识别。Scout采用基于文本的大型语言模型，可以轻松处理包含文本信息的文件。对于音频、图像、视频、办公文档等多媒体文件的取证分析，Scout则采用多模态模型。Scout能够识别并发现对调查人员具有潜在兴趣的证据文件。", "summary": "Scout是一个创新的数字取证框架，旨在解决法律调查中数字证据量激增和处理耗时的问题。它利用大型语言模型对海量数字证据（包括文本文件和多媒体文件）进行初步处理和优先级排序，通过识别相关工件来显著加速证据发现过程，从而减少调查员的工作量并提高效率。", "keywords": "数字取证, 大型语言模型, 证据发现, Scout, 多模态模型", "comments": "Scout的创新之处在于将大型语言模型（LLMs）引入数字取证领域，实现了对海量异构数字证据的快速初步处理和优先级排序。它利用LLMs处理文本数据，并结合多模态模型处理多媒体数据，这在传统取证工具中尚不常见。该方法有望大幅提升数字调查的效率，减少人工筛选的负担和误报。其重要性在于为应对不断增长的数字证据挑战提供了一种智能化的解决方案。"}}
{"id": "2507.18198", "title": "Comparing Non-minimal Semantics for Disjunction in Answer Set Programming", "authors": ["Felicidad Aguado", "Pedro Cabalar", "Brais Muñiz", "Gilberto Pérez", "Concepción Vidal"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18198v1", "summary": "In this paper, we compare four different semantics for disjunction in Answer\nSet Programming that, unlike stable models, do not adhere to the principle of\nmodel minimality. Two of these approaches, Cabalar and Mu\\~niz' \\emph{Justified\nModels} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly\nprovide an alternative non-minimal semantics for disjunction. The other two,\nAguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference}\n(DI) semantics, actually introduce a new disjunction connective, but are\ncompared here as if they constituted new semantics for the standard disjunction\noperator. We are able to prove that three of these approaches (Forks, Justified\nModels and a reasonable relaxation of the DI semantics) actually coincide,\nconstituting a common single approach under different definitions. Moreover,\nthis common semantics always provides a superset of the stable models of a\nprogram (in fact, modulo any context) and is strictly stronger than the fourth\napproach (Strongly Supported Models), that actually treats disjunctions as in\nclassical logic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18198v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "回答集编程中非最小析取语义的比较", "tldr": "本文比较了四种用于析取式回答集编程的非最小语义，发现其中三种在不同定义下实际上是等价的，并且比第四种更强。", "motivation": "现有回答集编程（ASP）中的稳定模型遵循模型最小性原则，但存在不遵循此原则的析取语义。本文旨在比较这些非最小析取语义。", "method": "比较了四种不同的析取语义：Justified Models, Strongly Supported Models, Forks, 和 Determining Inference (DI) 语义。其中Forks和DI语义被视为标准析取算子。通过理论证明来分析它们之间的关系。", "result": "证明了其中三种方法（Forks, Justified Models, 以及 DI 语义的合理放宽版本）实际上是等价的，构成了一种共同的单一方法。这种共同语义总是提供程序稳定模型的超集，并且严格强于第四种方法（Strongly Supported Models）。", "conclusion": "本文揭示了在回答集编程中，几种看似不同的非最小析取语义实际上是等价的，并阐明了它们与稳定模型及其他非最小语义的关系。", "translation": "在本文中，我们比较了回答集编程中析取的四种不同语义，它们与稳定模型不同，不遵循模型最小化原则。其中两种方法，Cabalar 和 Mu\\~niz 的“合理模型”（Justified Models）以及 Doherty 和 Szalas 的“强支持模型”（Strongly Supported Models），直接提供了析取的替代非最小语义。另外两种，Aguado 等人的“分支”（Forks）和 Shen 和 Eiter 的“确定性推理”（DI）语义，实际上引入了一种新的析取连接词，但在这里被比较为它们构成了标准析取算子新的语义。我们能够证明其中三种方法（Forks、Justified Models 和 DI 语义的合理放宽版本）实际上是重合的，在不同定义下构成了一种共同的单一方法。此外，这种共同语义总是提供程序稳定模型的超集（实际上，在任何上下文下），并且严格强于第四种方法（强支持模型），后者实际上将析取视为经典逻辑中的析取。", "summary": "本文对回答集编程中四种不遵循模型最小性原则的析取语义进行了比较研究。研究发现，Justified Models、Forks 和经过合理放宽的Determining Inference (DI) 语义在不同定义下是等价的，形成了一种共同的非最小语义。这种共同语义是程序稳定模型的超集，并且比将析取视为经典逻辑的Strongly Supported Models语义更强。", "keywords": "回答集编程, 析取语义, 非最小性, 稳定模型, 逻辑编程", "comments": "这项研究的创新之处在于揭示了不同名称下非最小析取语义的内在等价性，这有助于统一和简化对这些复杂逻辑系统的理解。它为回答集编程中析取语义的理论基础提供了重要的见解。"}}
{"id": "2501.15480", "title": "Exploring and Evaluating Interplays of BPpy with Deep Reinforcement Learning and Formal Methods", "authors": ["Tom Yaacov", "Gera Weiss", "Adiel Ashrov", "Guy Katz", "Jules Zisser"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to the 20th International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE 2025)", "url": "http://arxiv.org/abs/2501.15480v2", "summary": "We explore and evaluate the interactions between Behavioral Programming (BP)\nand a range of Artificial Intelligence (AI) and Formal Methods (FM) techniques.\nOur goal is to demonstrate that BP can serve as an abstraction that integrates\nvarious techniques, enabling a multifaceted analysis and a rich development\nprocess. Specifically, the paper examines how the BPpy framework, a\nPython-based implementation of BP, is enhanced by and enhances various FM and\nAI tools. We assess how integrating BP with tools such as Satisfiability Modulo\nTheory (SMT) solvers, symbolic and probabilistic model checking, and Deep\nReinforcement Learning (DRL) allow us to scale the abilities of BP to model\ncomplex systems. Additionally, we illustrate how developers can leverage\nmultiple tools within a single modeling and development task. The paper\nprovides quantitative and qualitative evidence supporting the feasibility of\nour vision to create a comprehensive toolbox for harnessing AI and FM methods\nin a unified development framework.", "comment": "Accepted to the 20th International Conference on Evaluation of Novel\n  Approaches to Software Engineering (ENASE 2025)", "pdf_url": "http://arxiv.org/pdf/2501.15480v2", "cate": "cs.SE", "date": "2025-01-26", "updated": "2025-07-24", "AI": {"title_translation": "探索和评估BPpy与深度强化学习和形式化方法的相互作用", "tldr": "本文探索并评估了行为编程（BP）与人工智能（AI）和形式化方法（FM）技术的结合，旨在展示BP作为一个抽象层，能够整合多种技术，从而实现多方面分析和丰富的开发过程。研究特别关注BPpy框架如何增强并被各种FM和AI工具增强，以扩展BP建模复杂系统的能力。", "motivation": "研究旨在证明行为编程（BP）可以作为一个抽象层，整合各种人工智能（AI）和形式化方法（FM）技术，从而实现多方面的分析和丰富的开发过程，并扩展BP建模复杂系统的能力。", "method": "本文研究了BPpy框架（一个基于Python的BP实现）如何被各种形式化方法（FM）和人工智能（AI）工具增强，以及它如何增强这些工具。具体评估了将BP与可满足性模理论（SMT）求解器、符号和概率模型检查以及深度强化学习（DRL）等工具集成后，如何扩展BP建模复杂系统的能力。研究还展示了开发者如何在单一建模和开发任务中利用多种工具。", "result": "论文提供了定量和定性证据，支持了创建一个综合工具箱的愿景的可行性，该工具箱用于在统一的开发框架中利用AI和FM方法。", "conclusion": "结论是创建一种综合工具箱是可行的，该工具箱能够在统一的开发框架中有效利用AI和FM方法，通过将行为编程与这些技术结合来实现。", "translation": "我们探索并评估了行为编程（BP）与一系列人工智能（AI）和形式化方法（FM）技术之间的相互作用。我们的目标是证明BP可以作为一个抽象层，整合各种技术，从而实现多方面的分析和丰富的开发过程。具体来说，本文研究了BPpy框架（一个基于Python的BP实现）如何被各种FM和AI工具增强，以及它如何增强这些工具。我们评估了将BP与诸如可满足性模理论（SMT）求解器、符号和概率模型检查以及深度强化学习（DRL）等工具集成后，如何扩展BP建模复杂系统的能力。此外，我们还阐述了开发人员如何在单一建模和开发任务中利用多种工具。本文提供了定量和定性证据，支持了我们创建一套综合工具箱的愿景的可行性，该工具箱用于在统一的开发框架中利用AI和FM方法。", "summary": "本文探讨了行为编程（BP）及其Python实现BPpy如何与人工智能（AI）和形式化方法（FM）技术（如SMT求解器、模型检查和深度强化学习）相互作用并相互增强。研究旨在证明BP能够作为一个有效的抽象层，整合这些异构技术，以实现复杂系统的多方面分析和开发。通过集成，BP扩展了其建模复杂系统的能力，并允许开发者在统一框架内利用多种工具。研究提供了支持这一综合工具箱愿景的定量和定性证据。", "keywords": "行为编程, 深度强化学习, 形式化方法, BPpy, 系统建模", "comments": "这篇论文的创新点在于提出了将行为编程（BP）作为整合人工智能（AI）和形式化方法（FM）的抽象层。这种整合有望简化复杂系统的开发和分析，提供更强大的建模和验证能力。其重要性在于为软件工程和AI的交叉领域提供了一个新的范式，可能极大地提升系统开发的效率和可靠性。"}}
{"id": "2507.17869", "title": "Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging", "authors": ["Atif Bilal Asad", "Achyut Paudel", "Safal Kshetri", "Chenchen Kang", "Salik Ram Khanal", "Nataliya Shcherbatyuk", "Pierre Davadant", "R. Paul Schreiner", "Santosh Kalauni", "Manoj Karkee", "Markus Keller"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17869v1", "summary": "Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting\nplant growth and subsequent products such as wine and juice. Because soil N has\nhigh spatial and temporal variability, it is desirable to accurately estimate\nthe N concentration of grapevine leaves and manage fertilization at the\nindividual plant level to optimally meet plant needs. In this study, we used\nin-field hyperspectral images with wavelengths ranging from $400 to 1000nm of\nfour different grapevine cultivars collected from distinct vineyards and over\ntwo growth stages during two growing seasons to develop models for predicting N\nconcentration at the leaf-level and canopy-level. After image processing, two\nfeature selection methods were employed to identify the optimal set of spectral\nbands that were responsive to leaf N concentrations. The selected spectral\nbands were used to train and test two different Machine Learning (ML) models,\nGradient Boosting and XGBoost, for predicting nitrogen concentrations. The\ncomparison of selected bands for both leaf-level and canopy-level datasets\nshowed that most of the spectral regions identified by the feature selection\nmethods were across both methods and the dataset types (leaf- and canopy-level\ndatasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,\nand 900-950nm. These findings indicated the robustness of these spectral\nregions for predicting nitrogen content. The results for N prediction\ndemonstrated that the ML model achieved an R square of 0.49 for canopy-level\ndata and an R square of 0.57 for leaf-level data, despite using different sets\nof selected spectral bands for each analysis level. The study demonstrated the\npotential of using in-field hyperspectral imaging and the use of spectral data\nin integrated feature selection and ML techniques to monitor N status in\nvineyards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17869v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "将特征选择和机器学习相结合，利用田间高光谱成像评估葡萄叶片氮含量", "tldr": "本研究利用田间高光谱成像和机器学习方法，结合特征选择技术，成功预测了葡萄叶片和冠层的氮含量，并识别出预测氮含量的重要光谱区域。", "motivation": "氮是葡萄园中最重要的营养元素之一，影响植物生长和最终产品。由于土壤氮含量存在高时空变异性，因此需要准确估计葡萄叶片氮浓度并进行个体植株水平的施肥管理。", "method": "研究使用了来自不同葡萄园和不同生长阶段的四种葡萄品种的田间高光谱图像（400-1000nm）。经过图像处理后，采用两种特征选择方法识别与叶片氮浓度相关的最佳光谱波段集合。选定的波段用于训练和测试两种机器学习模型（Gradient Boosting和XGBoost）来预测氮浓度。", "result": "特征选择方法识别出在叶片级和冠层级数据集中都具有鲁棒性的光谱区域，特别是500-525nm、650-690nm、750-800nm和900-950nm。机器学习模型在冠层级数据上实现了0.49的R平方，在叶片级数据上实现了0.57的R平方。", "conclusion": "该研究证明了利用田间高光谱成像以及结合特征选择和机器学习技术的光谱数据在监测葡萄园氮状况方面的潜力。", "translation": "氮（N）是葡萄园中最重要的营养元素之一，影响植物生长以及随后的葡萄酒和果汁等产品。由于土壤氮具有高度的空间和时间变异性，因此需要准确估计葡萄叶片的氮浓度，并在个体植株水平上管理施肥以最佳地满足植物需求。在本研究中，我们使用了来自不同葡萄园和两个生长季节的四个不同葡萄品种的田间高光谱图像，波长范围为400至1000纳米，以开发用于预测叶片级和冠层级氮浓度的模型。图像处理后，采用两种特征选择方法识别对叶片氮浓度有响应的最佳光谱波段集合。选定的光谱波段用于训练和测试两种不同的机器学习（ML）模型，Gradient Boosting和XGBoost，用于预测氮浓度。叶片级和冠层级数据集的选定波段比较表明，特征选择方法识别出的大多数光谱区域在两种方法和数据集类型（叶片级和冠层级数据集）之间都存在，特别是在关键区域：500-525纳米、650-690纳米、750-800纳米和900-950纳米。这些发现表明这些光谱区域在预测氮含量方面的鲁棒性。氮预测结果表明，尽管每种分析级别使用了不同组的选定光谱波段，但ML模型在冠层级数据上实现了0.49的R平方，在叶片级数据上实现了0.57的R平方。该研究证明了利用田间高光谱成像以及在集成特征选择和ML技术中使用光谱数据来监测葡萄园氮状况的潜力。", "summary": "本研究结合田间高光谱成像、特征选择和机器学习技术，开发并验证了预测葡萄叶片和冠层氮含量模型。通过识别出与氮含量高度相关的关键光谱区域，并利用Gradient Boosting和XGBoost模型进行预测，结果显示了该方法在葡萄园氮状况监测中的有效性和潜力。", "keywords": "葡萄园氮评估, 高光谱成像, 特征选择, 机器学习, 精准农业", "comments": "本研究的创新之处在于其将高光谱成像与先进的特征选择和机器学习算法相结合，实现了对葡萄叶片和冠层氮含量的精确非侵入式监测。这对于推动葡萄园的精准农业管理具有重要意义，有助于优化施肥，提高作物产量和品质，并减少环境影响。研究识别出的关键光谱区域为未来的相关研究提供了宝贵参考。"}}
{"id": "2507.17804", "title": "On the Energy Distribution of the Galactic Center Excess' Sources", "authors": ["Florian List", "Yujin Park", "Nicholas L. Rodd", "Eve Schoen", "Florian Wolf"], "categories": ["astro-ph.HE", "astro-ph.CO", "astro-ph.IM", "cs.LG", "hep-ph"], "primary_category": "Subjects:       High Energy Astrophysical Phenomena (astro-ph.HE)", "pdf_link": null, "comments": "Comments:      7+20 pages, 2+20 figures, comments welcome", "url": "http://arxiv.org/abs/2507.17804v1", "summary": "The Galactic Center Excess (GCE) remains one of the defining mysteries\nuncovered by the Fermi $\\gamma$-ray Space Telescope. Although it may yet herald\nthe discovery of annihilating dark matter, weighing against that conclusion are\nanalyses showing the spatial structure of the emission appears more consistent\nwith a population of dim point sources. Technical limitations have restricted\nprior analyses to studying the point-source hypothesis purely spatially. All\nspectral information that could help disentangle the GCE from the complex and\nuncertain astrophysical emission was discarded. We demonstrate that a neural\nnetwork-aided simulation-based inference approach can overcome such limitations\nand thereby confront the point source explanation of the GCE with spatial and\nspectral data. The addition is profound: energy information drives the putative\npoint sources to be significantly dimmer, indicating either the GCE is truly\ndiffuse in nature or made of an exceptionally large number of sources.\nQuantitatively, for our best fit background model, the excess is essentially\nconsistent with Poisson emission as predicted by dark matter. If the excess is\ninstead due to point sources, our median prediction is ${\\cal O}(10^5)$ sources\nin the Galactic Center, or more than 35,000 sources at 90% confidence, both\nsignificantly larger than the hundreds of sources preferred by earlier\npoint-source analyses of the GCE.", "comment": "7+20 pages, 2+20 figures, comments welcome", "pdf_url": "http://arxiv.org/pdf/2507.17804v1", "cate": "astro-ph.HE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "关于银河系中心过量源的能量分布", "tldr": "本研究利用神经网络辅助的模拟推理方法，首次结合空间和光谱数据来分析银河系中心过量（GCE）的来源。结果显示，如果GCE由点源组成，则其数量远超先前估计，表明GCE可能本质上是弥散的，或者由数量极其庞大的源构成。", "motivation": "银河系中心过量（GCE）是费米伽马射线空间望远镜发现的一个未解之谜。虽然它可能预示着暗物质湮灭的发现，但也有分析表明其空间结构更符合暗点源群的分布。先前的分析受限于技术，仅能从空间上研究点源假设，而忽略了有助于区分GCE与复杂天体物理辐射的光谱信息。", "method": "本研究采用了一种神经网络辅助的模拟推理方法，以克服传统分析的技术限制。该方法能够同时利用GCE的空间和光谱数据来检验点源解释。", "result": "加入能量信息后，推测的点源显得显著更暗，这表明GCE要么本质上是弥散的，要么由数量极其庞大的源组成。定量来看，对于最佳拟合背景模型，该过量与暗物质预测的泊松发射基本一致。如果该过量是由点源引起的，我们的中位预测显示银河系中心存在约$10^5$个源，或在90%置信度下超过35,000个源，这远高于早期点源分析所倾向的数百个源。", "conclusion": "GCE的能量分布分析表明，如果其由点源组成，则需要一个数量远超预期的点源群体。这支持了GCE可能本质上是弥散的观点，或者与暗物质湮灭产生的泊松发射一致，挑战了仅基于空间分析的早期点源解释。", "translation": "银河系中心过量（GCE）仍然是费米伽马射线空间望远镜发现的决定性谜团之一。尽管它可能预示着暗物质湮灭的发现，但反对这一结论的分析显示，其发射的空间结构似乎更符合暗点源群的分布。技术限制使得先前的分析仅限于纯粹从空间上研究点源假设。所有可能有助于将GCE与复杂且不确定的天体物理发射区分开来的光谱信息都被丢弃了。我们证明，一种神经网络辅助的基于模拟的推理方法可以克服这些限制，从而将GCE的点源解释与空间和光谱数据进行对比。这一增加是深远的：能量信息使得推测的点源显著变暗，这表明GCE要么本质上是弥散的，要么由数量极其庞大的源组成。定量来看，对于我们最佳拟合的背景模型，该过量本质上与暗物质预测的泊松发射一致。如果该过量是由点源引起的，我们的中位预测是银河系中心有约$10^5$个源，或在90%置信度下超过35,000个源，这两者都显著大于早期GCE点源分析所倾向的数百个源。", "summary": "本研究旨在解决银河系中心过量（GCE）的起源问题，特别是其是否由暗物质湮灭或点源造成。与以往仅依赖空间数据的分析不同，本文引入了一种创新的神经网络辅助模拟推理方法，首次将空间和光谱信息结合起来。研究发现，当考虑能量信息时，如果GCE由点源构成，则其源的数量需要远大于先前估计，达到约10^5个，而非数百个。这一结果表明GCE可能本质上是弥散的，或者其性质与暗物质产生的泊松发射更一致，从而为GCE的解释提供了新的视角。", "keywords": "银河系中心过量, 点源, 暗物质, 伽马射线, 神经网络", "comments": "这项研究的创新之处在于其首次将光谱信息整合到GCE的点源分析中，通过神经网络辅助的模拟推理方法克服了以往的技术限制。这一方法学上的进步显著改变了对GCE点源数量的估计，从数百个增加到数万甚至数十万个，对GCE的本质（弥散性或大量点源）提出了新的见解。它强调了多维数据在解决天体物理学复杂问题中的重要性，并为未来GCE的研究提供了新的方向。"}}
{"id": "2404.14445", "title": "A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models", "authors": ["Yefeng Yuan", "Yuhong Liu", "Liang Cheng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 1 figure, 4 tables", "url": "http://arxiv.org/abs/2404.14445v2", "summary": "The rapid advancements in generative AI and large language models (LLMs) have\nopened up new avenues for producing synthetic data, particularly in the realm\nof structured tabular formats, such as product reviews. Despite the potential\nbenefits, concerns regarding privacy leakage have surfaced, especially when\npersonal information is utilized in the training datasets. In addition, there\nis an absence of a comprehensive evaluation framework capable of quantitatively\nmeasuring the quality of the generated synthetic data and their utility for\ndownstream tasks. In response to this gap, we introduce SynEval, an open-source\nevaluation framework designed to assess the fidelity, utility, and privacy\npreservation of synthetically generated tabular data via a suite of diverse\nevaluation metrics. We validate the efficacy of our proposed framework -\nSynEval - by applying it to synthetic product review data generated by three\nstate-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings\nilluminate the trade-offs between various evaluation metrics in the context of\nsynthetic data generation. Furthermore, SynEval stands as a critical instrument\nfor researchers and practitioners engaged with synthetic tabular data,,\nempowering them to judiciously determine the suitability of the generated data\nfor their specific applications, with an emphasis on upholding user privacy.", "comment": "10 pages, 1 figure, 4 tables", "pdf_url": "http://arxiv.org/pdf/2404.14445v2", "cate": "cs.LG", "date": "2024-04-20", "updated": "2025-07-24", "AI": {"title_translation": "评估大型语言模型生成合成数据的多方面评估框架", "tldr": "论文提出了SynEval，一个开源的多方面评估框架，用于衡量LLM生成的合成表格数据的真实性、实用性和隐私保护，并展示了评估指标之间的权衡。", "motivation": "尽管大型语言模型（LLMs）生成合成数据具有潜力，但存在隐私泄露的担忧，并且缺乏一个全面的评估框架来定量衡量生成数据的质量及其在下游任务中的效用。", "method": "本文引入了SynEval，一个开源的多方面评估框架，旨在通过一套多样化的评估指标来评估大型语言模型生成的合成表格数据的真实性（fidelity）、实用性（utility）和隐私保护（privacy preservation）。该框架通过应用于ChatGPT、Claude和Llama这三个最先进的LLM生成的合成产品评论数据来验证其有效性。", "result": "实验结果揭示了合成数据生成中各种评估指标（包括真实性、实用性和隐私保护）之间的权衡。", "conclusion": "SynEval是一个关键工具，能帮助研究人员和从业者明智地确定大型语言模型生成的合成表格数据对其特定应用的适用性，并强调在整个过程中维护用户隐私。", "translation": "随着生成式AI和大型语言模型（LLM）的快速发展，为生产合成数据开辟了新途径，特别是在结构化表格格式领域，如产品评论。尽管有潜在好处，但隐私泄露的担忧已浮出水面，尤其是在训练数据集中使用了个人信息时。此外，目前缺乏一个全面的评估框架，能够定量衡量所生成合成数据的质量及其对下游任务的效用。针对这一空白，我们引入了SynEval，一个开源评估框架，旨在通过一套多样化的评估指标来评估合成表格数据的真实性、实用性和隐私保护。我们通过将SynEval应用于由ChatGPT、Claude和Llama这三个最先进的LLM生成的合成产品评论数据，验证了我们提出的框架——SynEval的有效性。我们的实验结果阐明了合成数据生成中各种评估指标之间的权衡。此外，SynEval是从事合成表格数据的研究人员和从业者的一个关键工具，使他们能够明智地确定生成数据对其特定应用的适用性，并强调维护用户隐私。", "summary": "本文针对大型语言模型（LLMs）在生成合成表格数据时面临的隐私泄露问题和现有评估框架的不足，提出了一个名为SynEval的开源多方面评估框架。SynEval旨在通过一系列多样化的指标，量化评估合成数据的真实性、实用性和隐私保护。通过对ChatGPT、Claude和Llama等主流LLM生成的合成产品评论数据进行实验验证，SynEval揭示了不同评估指标之间的权衡，并被视为帮助研究人员和从业者在确保用户隐私的前提下，判断合成数据适用性的重要工具。", "keywords": "合成数据, 大型语言模型, 评估框架, 隐私保护, 表格数据", "comments": "该论文提出了一种及时的、多方面的评估框架SynEval，以解决LLM生成合成数据中关键的质量、实用性和隐私问题。其创新之处在于将真实性、实用性和隐私保护纳入统一的评估体系。该框架的开源性质及其对现有LLM的验证增加了其实用性和影响力，对于促进负责任的合成数据生成具有重要意义。"}}
{"id": "2507.18224", "title": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation", "authors": ["Shiyuan Li", "Yixin Liu", "Qingsong Wen", "Chengqi Zhang", "Shirui Pan"], "categories": ["cs.MA", "cs.CL"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18224v1", "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have emerged\nas a powerful solution for dealing with complex problems across diverse\ndomains. The effectiveness of MAS is critically dependent on its collaboration\ntopology, which has become a focal point for automated design research.\nHowever, existing approaches are fundamentally constrained by their reliance on\na template graph modification paradigm with a predefined set of agents and\nhard-coded interaction structures, significantly limiting their adaptability to\ntask-specific requirements. To address these limitations, we reframe MAS design\nas a conditional autoregressive graph generation task, where both the system\ncomposition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing\nthe collaboration graph from scratch. Conditioned on a natural language task\nquery, ARG-Designer sequentially and dynamically determines the required number\nof agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative\napproach creates a customized topology in a flexible and extensible manner,\nprecisely tailored to the unique demands of different tasks. Extensive\nexperiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly\ngreater token efficiency and enhanced extensibility. The source code of\nARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18224v1", "cate": "cs.MA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "集结你的团队：通过自回归图生成实现多智能体通信拓扑的自动设计", "tldr": "ARG-Designer是一种新颖的自回归模型，能够根据自然语言任务查询自动从零开始生成多智能体系统（MAS）的通信拓扑，解决了现有方法对预定义模板的依赖，并在性能、代币效率和可扩展性方面表现出色。", "motivation": "现有基于大语言模型（LLM）的多智能体系统（MAS）设计方法受限于依赖预定义智能体和硬编码交互结构的模板图修改范式，这严重限制了它们对特定任务需求的适应性。", "method": "本文将MAS设计重新定义为条件自回归图生成任务，其中系统组成和结构被联合设计。提出了一种名为ARG-Designer的新型自回归模型，该模型通过从零开始构建协作图来实现这一范式。ARG-Designer根据自然语言任务查询，顺序且动态地确定所需智能体数量，从可扩展的池中选择合适的角色，并建立它们之间最优的通信链接。", "result": "在六个不同的基准测试中，ARG-Designer不仅实现了最先进的性能，而且显著提高了代币效率和增强了可扩展性。", "conclusion": "ARG-Designer的生成方法以灵活和可扩展的方式创建定制拓扑，精确地适应不同任务的独特需求，从而在多智能体系统设计中实现卓越的性能和效率。", "translation": "基于大型语言模型（LLM）的多智能体系统（MAS）已成为处理各种领域复杂问题的强大解决方案。MAS的有效性关键取决于其协作拓扑，这已成为自动化设计研究的焦点。然而，现有方法受限于其对模板图修改范式的依赖，该范式具有预定义智能体和硬编码交互结构，这严重限制了它们对特定任务要求的适应性。为了解决这些限制，我们将MAS设计重新定义为条件自回归图生成任务，其中系统组成和结构被联合设计。我们提出了ARG-Designer，一种新颖的自回归模型，通过从零开始构建协作图来实现这一范式。ARG-Designer在自然语言任务查询的条件下，顺序且动态地确定所需智能体的数量，从可扩展的池中选择其适当的角色，并建立它们之间最优的通信链接。这种生成方法以灵活和可扩展的方式创建定制拓扑，精确地适应不同任务的独特需求。在六个不同的基准测试中，广泛的实验表明ARG-Designer不仅实现了最先进的性能，而且显著提高了代币效率和增强了可扩展性。ARG-Designer的源代码可在https://github.com/Shiy-Li/ARG-Designer获取。", "summary": "多智能体系统（MAS）的有效性高度依赖于其协作拓扑，而现有设计方法因依赖预定义模板和硬编码结构而受限。为解决此问题，本文将MAS设计重构为条件自回归图生成任务，并提出了ARG-Designer模型。该模型能够根据自然语言任务查询，从零开始动态地确定智能体数量、选择角色并建立最优通信链接。实验证明，ARG-Designer在多个基准测试中达到了最先进的性能，并展现出更高的代币效率和增强的可扩展性。", "keywords": "多智能体系统, 大语言模型, 图生成, 通信拓扑, 自回归模型", "comments": "本文提出了一种创新的、基于自回归图生成的多智能体系统（MAS）通信拓扑设计方法，突破了传统模板化设计的限制。其核心创新在于将MAS设计视为一个从零开始的条件图生成任务，能够动态地确定智能体组成和通信结构，极大地提高了系统的灵活性和任务适应性。对于基于大语言模型（LLM）的MAS而言，这种能够定制化和动态调整协作拓扑的能力，是提升其解决复杂问题能力的关键一步，具有重要的实践意义和研究价值。"}}
{"id": "2507.17795", "title": "LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction", "authors": ["Shiyuan Zhang", "Tong Li", "Zhu Xiao", "Hongyang Du", "Kaibin Huang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 9 figures", "url": "http://arxiv.org/abs/2507.17795v1", "summary": "Service-level mobile traffic prediction for individual users is essential for\nnetwork efficiency and quality of service enhancement. However, current\nprediction methods are limited in their adaptability across different urban\nenvironments and produce inaccurate results due to the high uncertainty in\npersonal traffic patterns, the lack of detailed environmental context, and the\ncomplex dependencies among different network services. These challenges demand\nadvanced modeling techniques that can capture dynamic traffic distributions and\nrich environmental features. Inspired by the recent success of diffusion models\nin distribution modeling and Large Language Models (LLMs) in contextual\nunderstanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model\n(LSDM). LSDM integrates the generative power of diffusion models with the\nadaptive learning capabilities of transformers, augmented by the ability to\ncapture multimodal environmental information for modeling service-level\npatterns and dynamics. Extensive evaluations on real-world service-level\ndatasets demonstrate that the model excels in traffic usage predictions,\nshowing outstanding generalization and adaptability. After incorporating\ncontextual information via LLM, the performance improves by at least 2.83% in\nterms of the coefficient of determination. Compared to models of a similar\ntype, such as CSDI, the root mean squared error can be reduced by at least\n8.29%. The code and dataset will be available at:\nhttps://github.com/SoftYuaneR/LSDM.", "comment": "14 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.17795v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "LSDM：LLM增强型时空扩散模型用于服务级移动流量预测", "tldr": "本文提出LSDM，一个结合扩散模型和LLM的时空模型，用于服务级移动流量预测，显著提高了预测准确性、泛化性和适应性。", "motivation": "当前服务级移动流量预测方法在不同城市环境中的适应性有限，且由于个人流量模式的高度不确定性、缺乏详细环境上下文以及不同网络服务间的复杂依赖关系，导致预测结果不准确。这些挑战需要能捕获动态流量分布和丰富环境特征的先进建模技术。", "method": "本文提出LSDM（LLM增强型时空扩散模型），该模型结合了扩散模型在分布建模中的生成能力和Transformer的自适应学习能力。LSDM通过捕获多模态环境信息来增强模型，以更好地建模服务级模式和动态。", "result": "在真实服务级数据集上的广泛评估表明，LSDM在流量使用预测方面表现出色，显示出卓越的泛化性和适应性。通过LLM整合上下文信息后，决定系数方面的性能至少提高了2.83%。与CSDI等同类模型相比，均方根误差至少降低了8.29%。", "conclusion": "LSDM通过结合扩散模型和LLM，显著提升了服务级移动流量预测的准确性、泛化性和适应性，有效解决了现有方法在处理复杂流量模式和环境上下文方面的局限性。", "translation": "针对个体用户的服务级移动流量预测对于提升网络效率和改善服务质量至关重要。然而，当前的预测方法在不同城市环境中的适应性有限，并且由于个人流量模式的高度不确定性、缺乏详细的环境上下文以及不同网络服务之间复杂的依赖关系，导致预测结果不准确。这些挑战要求先进的建模技术，能够捕获动态的流量分布和丰富的环境特征。受扩散模型在分布建模方面以及大型语言模型（LLMs）在上下文理解方面近期成功的启发，我们提出了一种LLM增强型时空扩散模型（LSDM）。LSDM将扩散模型的生成能力与Transformer的自适应学习能力相结合，并通过捕获多模态环境信息的能力进行增强，以建模服务级模式和动态。在真实世界服务级数据集上的广泛评估表明，该模型在流量使用预测方面表现出色，显示出卓越的泛化性和适应性。在通过LLM整合上下文信息后，决定系数方面的性能至少提高了2.83%。与CSDI等同类模型相比，均方根误差至少可以降低8.29%。代码和数据集将在以下网址提供：https://github.com/SoftYuaneR/LSDM。", "summary": "本文提出了一种LLM增强型时空扩散模型（LSDM），旨在解决当前服务级移动流量预测方法在适应性、准确性和处理复杂上下文方面的局限性。LSDM创新性地结合了扩散模型的生成能力和大型语言模型（LLM）的上下文理解能力，并通过整合多模态环境信息来提高预测精度。在真实数据集上的实验结果表明，LSDM在流量预测方面表现出卓越的泛化性和适应性，与现有方法相比，在决定系数和均方根误差方面均有显著改进。", "keywords": "服务级移动流量预测, 时空扩散模型, 大型语言模型, 流量预测, 上下文理解", "comments": "该论文的创新点在于将扩散模型和大型语言模型（LLM）结合应用于时空流量预测，利用LLM的上下文理解能力来增强模型对复杂环境特征和动态流量模式的捕获。这种融合有望解决现有方法在适应性和准确性方面的不足。其重要性在于提升了服务级移动流量预测的精度，对于网络效率和服务质量的提升具有实际意义。"}}
{"id": "2507.18046", "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training", "authors": ["Hanwen Shen", "Jiajie Lu", "Yupeng Cao", "Xiaonan Yang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18046v1", "summary": "Recent advances in AI-generated video have shown strong performance on\n\\emph{text-to-video} tasks, particularly for short clips depicting a single\nscene. However, current models struggle to generate longer videos with coherent\nscene transitions, primarily because they cannot infer when a transition is\nneeded from the prompt. Most open-source models are trained on datasets\nconsisting of single-scene video clips, which limits their capacity to learn\nand respond to prompts requiring multiple scenes. Developing scene transition\nawareness is essential for multi-scene generation, as it allows models to\nidentify and segment videos into distinct clips by accurately detecting\ntransitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV)\ndataset, which consists of preprocessed video clips with multiple scene\ntransitions. Our experiment shows that post-training on the \\textbf{TAV}\ndataset improves prompt-based scene transition understanding, narrows the gap\nbetween required and generated scenes, and maintains image quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18046v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过后训练增强视频生成中的场景转换感知", "tldr": "当前文生视频模型在多场景视频生成和场景转换方面表现不佳。本文提出TAV数据集并进行后训练，以提升模型对场景转换的理解并保持图像质量。", "motivation": "当前AI生成的视频在处理多场景视频和连贯的场景转换时存在困难，主要原因是模型无法从提示中推断出何时需要转换，且大多数开源模型在单场景视频数据集上训练。为了实现多场景视频生成，模型必须具备识别和分割视频中不同场景的场景转换感知能力。", "method": "本文提出了名为“Transition-Aware Video (TAV)”的数据集，该数据集包含经过预处理的具有多个场景转换的视频片段。研究人员在该TAV数据集上进行了后训练。", "result": "在TAV数据集上进行后训练可以提高基于提示的场景转换理解能力，缩小所需场景与生成场景之间的差距，并保持图像质量。", "conclusion": "通过在TAV数据集上进行后训练，可以有效提升视频生成模型对场景转换的感知能力，从而更好地生成包含连贯多场景的视频，同时不牺牲图像质量。", "translation": "最近AI生成视频的进展在“文本到视频”任务上表现出色，特别是对于描绘单个场景的短视频片段。然而，当前模型难以生成具有连贯场景转换的更长视频，主要是因为它们无法从提示中推断出何时需要转换。大多数开源模型都在由单场景视频片段组成的数据集上进行训练，这限制了它们学习和响应需要多个场景的提示的能力。开发场景转换感知能力对于多场景生成至关重要，因为它允许模型通过准确检测转换来识别视频并将其分割成不同的片段。为了解决这个问题，我们提出了“Transition-Aware Video (TAV)”数据集，该数据集包含经过预处理的具有多个场景转换的视频片段。我们的实验表明，在TAV数据集上进行后训练可以改善基于提示的场景转换理解，缩小所需场景与生成场景之间的差距，并保持图像质量。", "summary": "本文针对当前文本到视频模型在生成包含连贯场景转换的多场景长视频时遇到的挑战。研究指出，现有模型主要在单场景数据集上训练，导致其难以根据提示理解和执行场景转换。为解决此问题，作者提出了一个名为“Transition-Aware Video (TAV)”的新数据集，该数据集专门包含带有多个场景转换的视频片段。实验结果表明，通过在TAV数据集上进行后训练，可以显著提升模型对基于提示的场景转换的理解能力，有效缩小生成场景与所需场景之间的差距，同时还能保持良好的图像质量。", "keywords": "视频生成, 场景转换, 后训练, TAV数据集, 文本到视频", "comments": "该论文通过引入专门的TAV数据集并采用后训练策略，为提升文生视频模型在处理多场景转换方面的能力提供了一个有效且实用的解决方案。其创新点在于数据集的构建和利用后训练来弥补现有模型在多场景理解上的不足，而不是从头开始训练新模型，这对于现有模型的改进具有重要意义。"}}
{"id": "2507.18229", "title": "From Individual Learning to Market Equilibrium: Correcting Structural and Parametric Biases in RL Simulations of Economic Models", "authors": ["Zeqiang Zhang", "Ruxin Chen"], "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18229v1", "summary": "The application of Reinforcement Learning (RL) to economic modeling reveals a\nfundamental conflict between the assumptions of equilibrium theory and the\nemergent behavior of learning agents. While canonical economic models assume\natomistic agents act as `takers' of aggregate market conditions, a naive\nsingle-agent RL simulation incentivizes the agent to become a `manipulator' of\nits environment. This paper first demonstrates this discrepancy within a\nsearch-and-matching model with concave production, showing that a standard RL\nagent learns a non-equilibrium, monopsonistic policy. Additionally, we identify\na parametric bias arising from the mismatch between economic discounting and\nRL's treatment of intertemporal costs. To address both issues, we propose a\ncalibrated Mean-Field Reinforcement Learning framework that embeds a\nrepresentative agent in a fixed macroeconomic field and adjusts the cost\nfunction to reflect economic opportunity costs. Our iterative algorithm\nconverges to a self-consistent fixed point where the agent's policy aligns with\nthe competitive equilibrium. This approach provides a tractable and\ntheoretically sound methodology for modeling learning agents in economic\nsystems within the broader domain of computational social science.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18229v1", "cate": "econ.GN", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "从个体学习到市场均衡：纠正经济模型RL模拟中的结构和参数偏差", "tldr": "强化学习（RL）在经济建模中存在结构和参数偏差，导致代理学习非均衡策略。本文提出了一种校准的均场强化学习框架，通过调整成本函数和嵌入代表性代理，使学习到的策略与竞争均衡对齐，提供了一种建模经济系统中学习代理的可行方法。", "motivation": "强化学习（RL）应用于经济建模时，存在均衡理论假设与学习代理行为之间的根本冲突。传统的经济模型假设代理是市场条件的“接受者”，而朴素的单代理RL模拟则会激励代理成为环境的“操纵者”，导致非均衡行为。此外，经济贴现与RL跨期成本处理方式不匹配，引入了参数偏差。", "method": "本文提出了一个校准的均场强化学习（Mean-Field Reinforcement Learning）框架。该框架将一个代表性代理嵌入固定的宏观经济场中，并调整成本函数以反映经济机会成本。通过迭代算法，该方法旨在收敛到一个自洽的定点。", "result": "所提出的迭代算法能够收敛到一个自洽的定点，在该定点上，代理的策略与竞争均衡对齐。这种方法为在计算社会科学领域中建模经济系统中的学习代理提供了一种可处理且理论上合理的方法。", "conclusion": "本文提出的校准均场强化学习框架，成功解决了强化学习模拟经济模型时出现的结构性和参数性偏差，使得学习代理的策略能够与竞争均衡对齐，为计算社会科学中经济系统学习代理的建模提供了一种可处理且理论上合理的方法。", "translation": "强化学习（RL）在经济建模中的应用揭示了均衡理论假设与学习代理的涌现行为之间存在根本性冲突。传统的经济模型假设个体代理是总市场条件的“接受者”，而天真的单代理RL模拟则激励代理成为其环境的“操纵者”。本文首先在一个具有凹生产的搜索匹配模型中展示了这种差异，表明标准的RL代理学习到的是非均衡的、买方垄断的策略。此外，我们还识别出一种参数偏差，它源于经济贴现与RL处理跨期成本之间的不匹配。为了解决这两个问题，我们提出了一个校准的均场强化学习框架，该框架将一个代表性代理嵌入固定的宏观经济场中，并调整成本函数以反映经济机会成本。我们的迭代算法收敛到一个自洽的定点，在该定点上，代理的策略与竞争均衡对齐。这种方法为在更广泛的计算社会科学领域中建模经济系统中的学习代理提供了一种可处理且理论上合理的方法。", "summary": "该论文探讨了强化学习（RL）在经济建模中与传统经济均衡理论的冲突，指出RL代理可能学习到非均衡的操纵性策略，并存在参数偏差。为解决这些问题，论文提出了一个校准的均场强化学习框架。该框架通过将代表性代理嵌入固定的宏观经济场并调整成本函数，使迭代算法收敛到与竞争均衡对齐的自洽策略，从而提供了一种在计算社会科学中建模学习代理的可行且理论上合理的方法。", "keywords": "强化学习, 经济模型, 市场均衡, 均场强化学习, 偏差纠正", "comments": "本文的创新点在于提出了一个将强化学习与经济均衡理论相结合的框架，有效地解决了在RL模拟经济模型时出现的结构性（如单方垄断行为）和参数性（如贴现率不匹配）偏差。通过引入均场方法和调整成本函数，该研究使得RL代理的行为能够更符合经济学中的均衡假设，这对于计算社会科学领域中更准确地建模和理解经济系统中的学习行为具有重要的理论和应用价值。"}}
{"id": "2507.18429", "title": "NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning", "authors": ["Mahdi Ghafourian", "Federico M. Sukno"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18429v1", "summary": "Head pose estimation (HPE) plays a critical role in various computer vision\napplications such as human-computer interaction and facial recognition. In this\npaper, we propose a novel deep learning approach for head pose estimation with\nlimited training data via non-linear manifold learning called NLML-HPE. This\nmethod is based on the combination of tensor decomposition (i.e., Tucker\ndecomposition) and feed forward neural networks. Unlike traditional\nclassification-based approaches, our method formulates head pose estimation as\na regression problem, mapping input landmarks into a continuous representation\nof pose angles. To this end, our method uses tensor decomposition to split each\nEuler angle (yaw, pitch, roll) to separate subspaces and models each dimension\nof the underlying manifold as a cosine curve. We address two key challenges: 1.\nAlmost all HPE datasets suffer from incorrect and inaccurate pose annotations.\nHence, we generated a precise and consistent 2D head pose dataset for our\ntraining set by rotating 3D head models for a fixed set of poses and rendering\nthe corresponding 2D images. 2. We achieved real-time performance with limited\ntraining data as our method accurately captures the nature of rotation of an\nobject from facial landmarks. Once the underlying manifold for rotation around\neach axis is learned, the model is very fast in predicting unseen data. Our\ntraining and testing code is available online along with our trained models:\nhttps: //github.com/MahdiGhafoorian/NLML_HPE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18429v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "NLML-HPE：通过流形学习实现有限数据下的头部姿态估计", "tldr": "NLML-HPE是一种新颖的深度学习方法，通过非线性流形学习和张量分解，在有限数据下实现了实时、准确的头部姿态估计，并解决了现有数据集标注不准确的问题。", "motivation": "现有的头部姿态估计（HPE）数据集通常存在不正确和不准确的姿态标注问题，且传统方法在有限训练数据下难以同时实现实时性能。", "method": "本文提出了一种名为NLML-HPE的深度学习方法，通过非线性流形学习在有限训练数据下进行头部姿态估计。该方法结合了张量分解（即Tucker分解）和前馈神经网络，并将头部姿态估计公式化为回归问题，将输入地标映射到姿态角的连续表示。为了实现这一目标，该方法使用张量分解将每个欧拉角（偏航、俯仰、滚动）分解到单独的子空间，并将底层流形的每个维度建模为余弦曲线。此外，为了解决数据集标注不准确的问题，作者通过旋转3D头部模型并渲染相应的2D图像，生成了一个精确且一致的2D头部姿态数据集用于训练。", "result": "该方法在有限训练数据下实现了实时性能，并且能够从面部地标中准确捕捉物体旋转的本质。一旦学习了围绕每个轴旋转的底层流形，模型在预测未见数据时速度非常快。", "conclusion": "NLML-HPE通过引入一种新颖的基于回归的深度学习方法，利用流形学习和张量分解，成功解决了头部姿态估计中数据有限和不准确的挑战，从而实现了一个准确高效的实时系统。", "translation": "头部姿态估计（HPE）在人机交互和面部识别等各种计算机视觉应用中扮演着关键角色。在本文中，我们提出了一种名为NLML-HPE的新型深度学习方法，通过非线性流形学习在有限训练数据下进行头部姿态估计。该方法基于张量分解（即Tucker分解）和前馈神经网络的结合。与传统的基于分类的方法不同，我们的方法将头部姿态估计公式化为一个回归问题，将输入地标映射到姿态角的连续表示。为此，我们的方法使用张量分解将每个欧拉角（偏航、俯仰、滚动）分解到单独的子空间，并将底层流形的每个维度建模为余弦曲线。我们解决了两个关键挑战：1. 几乎所有HPE数据集都存在不正确和不准确的姿态标注。因此，我们通过旋转3D头部模型以固定姿态集并渲染相应的2D图像，为我们的训练集生成了一个精确且一致的2D头部姿态数据集。2. 我们的方法从面部地标中准确捕捉了物体旋转的本质，从而在有限训练数据下实现了实时性能。一旦学习了围绕每个轴旋转的底层流形，模型在预测未见数据时速度非常快。我们的训练和测试代码以及训练好的模型可在网上获取：https://github.com/MahdiGhafoorian/NLML_HPE。", "summary": "本文提出了一种名为NLML-HPE的深度学习方法，旨在通过非线性流形学习解决有限数据下的头部姿态估计问题。该方法结合了张量分解和前馈神经网络，将头部姿态估计视为回归任务，并将欧拉角分解为独立的子空间，并建模为余弦曲线。为克服现有数据集标注不准确的挑战，研究者生成了一个新的精确2D头部姿态数据集。实验结果表明，NLML-HPE在有限训练数据下实现了实时性能，并能快速准确地预测未见数据。", "keywords": "头部姿态估计, 流形学习, 张量分解, 有限数据, 深度学习", "comments": "该论文的创新点在于将头部姿态估计问题重新定义为使用非线性流形学习和张量分解的回归问题，这与传统的分类方法不同。生成一个精确的新数据集以解决现有标注不准确的问题也是一项重要贡献。该方法能够在有限数据下实现实时性能，这使其具有很高的实用价值。"}}
{"id": "2403.02418", "title": "The Role of the Time-Dependent Hessian in High-Dimensional Optimization", "authors": ["Tony Bonnaire", "Giulio Biroli", "Chiara Cammarota"], "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2403.02418v3", "summary": "Gradient descent is commonly used to find minima in rough landscapes,\nparticularly in recent machine learning applications. However, a theoretical\nunderstanding of why good solutions are found remains elusive, especially in\nstrongly non-convex and high-dimensional settings. Here, we focus on the phase\nretrieval problem as a typical example, which has received a lot of attention\nrecently in theoretical machine learning. We analyze the Hessian during\ngradient descent, identify a dynamical transition in its spectral properties,\nand relate it to the ability of escaping rough regions in the loss landscape.\nWhen the signal-to-noise ratio (SNR) is large enough, an informative negative\ndirection exists in the Hessian at the beginning of the descent, i.e in the\ninitial condition. While descending, a BBP transition in the spectrum takes\nplace in finite time: the direction is lost, and the dynamics is trapped in a\nrugged region filled with marginally stable bad minima. Surprisingly, for\nfinite system sizes, this window of negative curvature allows the system to\nrecover the signal well before the theoretical SNR found for infinite sizes,\nemphasizing the central role of initialization and early-time dynamics for\nefficiently navigating rough landscapes.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2403.02418v3", "cate": "cs.LG", "date": "2024-03-04", "updated": "2025-07-24", "AI": {"title_translation": "高维优化中时变Hessian的作用", "tldr": "本文分析了高维优化中梯度下降过程中时变Hessian的作用，发现一个初始的负曲率方向有助于在谱转换发生前逃离崎岖区域，解释了为何能找到好的解。", "motivation": "现有理论难以解释梯度下降在高维非凸优化中为何能找到好的解。", "method": "以相位恢复问题为例，分析了梯度下降过程中Hessian矩阵的动态演化及其谱性质的动力学转变，并将其与逃离损失函数崎岖区域的能力联系起来。", "result": "当信噪比足够大时，Hessian在下降初期存在一个信息丰富的负方向；下降过程中，谱发生BBP转变，该方向消失，动力学被困在充满局部极小值的崎岖区域。对于有限系统，这个负曲率窗口使得系统能在理论信噪比之前恢复信号。", "conclusion": "强调了初始化和早期动力学对于有效导航崎岖损失函数景观的核心作用。", "translation": "梯度下降法常用于在崎岖的损失函数景观中寻找最小值，尤其是在近期的机器学习应用中。然而，在强非凸和高维设置下，为何能找到好的解的理论理解仍然难以捉摸。本文以相位恢复问题为例，该问题最近在理论机器学习中受到了广泛关注。我们分析了梯度下降过程中Hessian矩阵的动态演化，识别了其谱性质中的一个动力学转变，并将其与逃离损失函数崎岖区域的能力联系起来。当信噪比（SNR）足够大时，在下降开始时，即在初始条件下，Hessian中存在一个信息丰富的负方向。在下降过程中，谱在有限时间内发生BBP转变：该方向消失，动力学被困在充满边缘稳定不良局部极小值的崎岖区域。令人惊讶的是，对于有限系统规模，这个负曲率窗口使得系统能够在无穷大系统理论信噪比之前很好地恢复信号，这强调了初始化和早期动力学对于高效导航崎岖景观的核心作用。", "summary": "本文研究了高维非凸优化中的梯度下降，以相位恢复为例。通过分析时变Hessian，发现了一个初始信息丰富的负方向，该方向对于逃离崎岖区域至关重要。随后发生的BBP谱转变将动力学困于不良局部极小值中。值得注意的是，对于有限系统，这种初始负曲率使得信号恢复可以在低于理论无限系统信噪比的条件下实现，强调了初始化和早期动力学的重要性。", "keywords": "高维优化, 梯度下降, Hessian, 相位恢复, 谱性质", "comments": "这篇论文为梯度下降在复杂损失函数景观中的有效性提供了理论见解，特别是强调了初始化和早期动力学的作用，这在实践中常被观察到但缺乏强有力的理论支持。关于有限系统能够在低于理论信噪比下恢复信号的发现尤其有趣且反直觉，为实际优化提供了新的视角。"}}
{"id": "2507.18631", "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment", "authors": ["Hao Li", "Lijun Li", "Zhenghao Lu", "Xianyi Wei", "Rui Li", "Jing Shao", "Lei Sha"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18631v1", "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions. In this paper, we show that fine-tuning datasets often contain\nsamples with safety-degrading features that are not easily identifiable on the\nsurface. These samples can significantly degrade the safety alignment of LLMs\nduring fine-tuning. To address this issue, we propose LARF, a\n\\textbf{L}ayer-\\textbf{A}ware \\textbf{R}epresentation \\textbf{F}iltering\nmethod. This method identifies safety-sensitive layers within the LLM and\nleverages their representations to detect which data samples in the\npost-training dataset contain safety-degrading features. Experimental results\ndemonstrate that LARF can effectively identify benign data with\nsafety-degrading features. After removing such data, the safety alignment\ndegradation caused by fine-tuning is mitigated. Please see our code at\n\\href{https://github.com/LLLeoLi/LARF}{https://github.com/LLLeoLi/LARF}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18631v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "层感知表示过滤：净化微调数据以保持大型语言模型安全对齐", "tldr": "微调看似无害的数据集可能会损害LLM的安全对齐；本文提出LARF方法，通过识别LLM中安全敏感层的表示来过滤掉含有安全降级特征的数据，从而减轻微调导致的安全退化。", "motivation": "随着大型语言模型（LLMs）的快速发展和普及，微调对齐模型以适应实际应用变得至关重要，这使得微调过程的安全性前所未有地重要。然而，即使使用看似良性的下游数据集进行微调，对齐的LLMs的安全性也可能受到损害，使其更容易受到恶意指令的影响。研究表明，微调数据集中常包含表面上不易识别但能显著降低LLM安全对齐的样本。", "method": "本文提出了一种名为LARF（Layer-Aware Representation Filtering）的方法。该方法通过识别LLM中安全敏感的层，并利用这些层的表示来检测微调数据集中哪些样本包含安全降级特征。", "result": "实验结果表明，LARF能够有效识别出含有安全降级特征的良性数据。在移除这些数据后，微调所引起的安全对齐退化得到了缓解。", "conclusion": "通过使用LARF过滤掉微调数据集中潜在的有害样本，可以有效减轻大型语言模型在微调过程中出现的安全对齐退化问题，从而提升LLM的安全性。", "translation": "随着大型语言模型（LLMs）的快速发展和日益普及，对齐模型进行微调已成为使其适应实际应用的关键一步，这使得微调过程的安全性比以往任何时候都更加重要。然而，最近的研究强调了一个严峻的挑战：即使使用看似良性的下游数据集进行微调，对齐的LLMs的安全性也可能受到损害，使其更容易受到恶意指令的影响。在本文中，我们展示了微调数据集通常包含具有安全降级特征的样本，这些特征在表面上不易识别。这些样本在微调过程中会显著降低LLMs的安全对齐。为了解决这个问题，我们提出了LARF，一种层感知表示过滤方法。该方法识别LLM中安全敏感的层，并利用它们的表示来检测后训练数据集中哪些数据样本包含安全降级特征。实验结果表明，LARF可以有效地识别出具有安全降级特征的良性数据。在移除此类数据后，微调引起的安全对齐退化得到了缓解。请访问我们的代码：https://github.com/LLLeoLi/LARF。", "summary": "本文针对大型语言模型（LLM）微调过程中可能出现的安全对齐退化问题，提出了一种名为LARF（Layer-Aware Representation Filtering）的新方法。研究发现，即使是看似无害的微调数据集也可能包含隐藏的安全降级特征，这些特征会导致LLM更容易受到恶意指令的影响。LARF通过识别LLM中的安全敏感层并利用其表示，来检测并过滤掉含有这些有害特征的数据样本。实验证明，移除这些被LARF识别出的数据后，LLM在微调过程中的安全对齐退化问题得到了有效缓解。", "keywords": "大型语言模型安全, 微调, 数据过滤, 层感知表示, 安全对齐", "comments": "这篇论文解决了LLM微调中一个重要的安全问题，即微调数据中隐藏的“毒性”样本可能导致模型安全性下降。LARF的创新之处在于其“层感知”的方法，通过深入到LLM内部的表示层来识别问题数据，而不是仅仅依赖表面特征。这对于确保LLM在实际应用中的鲁棒性和安全性具有重要意义。"}}
{"id": "2507.17757", "title": "BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches", "authors": ["Sam Gordon James", "Miranda Elaine Glynis Armstrong", "Aisling Ann O'Kane", "Harry Emerson", "Zahraa S. Abdallah"], "categories": ["cs.HC", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      13 pages, 14 figures", "url": "http://arxiv.org/abs/2507.17757v1", "summary": "Background: Type 1 diabetes (T1D) has seen a rapid evolution in management\ntechnology and forms a useful case study for the future management of other\nchronic conditions. Further development of this management technology requires\nan exploration of its real-world use and the potential of additional data\nstreams. To facilitate this, we contribute the BrisT1D Dataset to the growing\nnumber of public T1D management datasets. The dataset was developed from a\nlongitudinal study of 24 young adults in the UK who used a smartwatch alongside\ntheir usual T1D management. Findings: The BrisT1D dataset features both device\ndata from the T1D management systems and smartwatches used by participants, as\nwell as transcripts of monthly interviews and focus groups conducted during the\nstudy. The device data is provided in a processed state, for usability and more\nrapid analysis, and in a raw state, for in-depth exploration of novel insights\ncaptured in the study. Conclusions: This dataset has a range of potential\napplications. The quantitative elements can support blood glucose prediction,\nhypoglycaemia prediction, and closed-loop algorithm development. The\nqualitative elements enable the exploration of user experiences and opinions,\nas well as broader mixed-methods research into the role of smartwatches in T1D\nmanagement.", "comment": "13 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.17757v1", "cate": "cs.HC", "date": "2025-05-07", "updated": "2025-05-07", "AI": {"title_translation": "BrisT1D数据集：英国使用智能手表的1型糖尿病青年成人", "tldr": "引入BrisT1D数据集，包含24名英国1型糖尿病青年使用智能手表和管理系统的数据，以及访谈记录，旨在促进1型糖尿病管理技术的研究和应用。", "motivation": "1型糖尿病管理技术发展迅速，是其他慢性病管理的有用案例。进一步开发管理技术需要探索其实际使用和额外数据流的潜力，因此需要贡献新的数据集来促进此项研究。", "method": "BrisT1D数据集来源于一项纵向研究，研究对象是24名英国青年成人，他们在日常1型糖尿病管理的同时使用了智能手表。数据集包含设备数据（来自1型糖尿病管理系统和智能手表）以及研究期间进行的每月访谈和焦点小组的记录。", "result": "BrisT1D数据集包含参与者使用的1型糖尿病管理系统和智能手表的设备数据，以及研究期间进行的每月访谈和焦点小组的记录。设备数据提供处理后的状态（便于使用和快速分析）和原始状态（用于深入探索新颖见解）。", "conclusion": "该数据集具有广泛的潜在应用。定量元素可支持血糖预测、低血糖预测和闭环算法开发。定性元素可用于探索用户体验和意见，以及对智能手表在1型糖尿病管理中作用的更广泛的混合方法研究。", "translation": "背景：1型糖尿病（T1D）的管理技术经历了快速发展，为未来其他慢性病的管理提供了有益的案例研究。这种管理技术的进一步发展需要探索其实际应用以及额外数据流的潜力。为了促进这一点，我们将BrisT1D数据集贡献给不断增长的公共1型糖尿病管理数据集。该数据集来自一项对24名英国青年成人的纵向研究，他们在日常1型糖尿病管理的同时使用了智能手表。发现：BrisT1D数据集包含参与者使用的1型糖尿病管理系统和智能手表的设备数据，以及研究期间进行的每月访谈和焦点小组的记录。设备数据以处理后的状态提供，以便于使用和更快速的分析；也以原始状态提供，用于深入探索研究中捕获的新颖见解。结论：该数据集具有一系列潜在的应用。定量元素可以支持血糖预测、低血糖预测和闭环算法开发。定性元素可以探索用户体验和意见，以及对智能手表在1型糖尿病管理中作用的更广泛的混合方法研究。", "summary": "本研究介绍了BrisT1D数据集，旨在促进1型糖尿病管理技术的发展。该数据集来源于一项对24名英国青年成人进行的纵向研究，他们在使用日常1型糖尿病管理系统的同时佩戴智能手表。数据集整合了来自管理系统和智能手表的设备数据（包括原始和处理后版本），以及每月访谈和焦点小组的定性记录。该数据集具有多方面应用潜力，其定量部分可用于血糖和低血糖预测及算法开发，定性部分则有助于理解用户体验和进行混合方法研究，以探索智能手表在1型糖尿病管理中的作用。", "keywords": "1型糖尿病, 智能手表, 数据集, 慢性病管理, 纵向研究", "comments": "BrisT1D数据集的创新之处在于其结合了定量（设备数据）和定性（访谈记录）数据，为深入理解1型糖尿病管理中智能手表的真实世界应用提供了独特资源。数据集同时提供原始和处理后的数据，兼顾了易用性和深度分析的需求。其重要性在于能够支持血糖预测、低血糖预测以及闭环算法的开发，同时也能通过定性数据探索用户体验，为未来慢性病管理技术的发展提供宝贵见解。该数据集的局限性可能在于样本量（24名参与者）相对较小，可能影响结果的普遍性。"}}
{"id": "2411.06690", "title": "Polarization Aware Movable Antenna", "authors": ["Runxin Zhang", "Yulin Shao", "Yonina C. Eldar"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.06690v3", "summary": "This paper presents a polarization-aware movable antenna (PAMA) framework\nthat integrates polarization effects into the design and optimization of\nmovable antennas (MAs). While MAs have proven effective at boosting wireless\ncommunication performance, existing studies primarily focus on phase variations\ncaused by different propagation paths and leverage antenna movements to\nmaximize channel gains. This narrow focus limits the full potential of MAs. In\nthis work, we introduce a polarization-aware channel model rooted in\nelectromagnetic theory, unveiling a defining advantage of MAs over other\nwireless technologies such as precoding: the ability to optimize polarization\nmatching. This new understanding enables PAMA to extend the applicability of\nMAs beyond radio-frequency, multipath-rich scenarios to higher-frequency bands,\nsuch as mmWave, even with a single line-of-sight (LOS) path. Our findings\ndemonstrate that incorporating polarization considerations into MAs\nsignificantly enhances efficiency, link reliability, and data throughput,\npaving the way for more robust and efficient future wireless networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.06690v3", "cate": "cs.IT", "date": "2024-11-11", "updated": "2025-07-24", "AI": {"title_translation": "极化感知可移动天线", "tldr": "本文提出了一个极化感知可移动天线（PAMA）框架，通过整合极化效应显著提升了可移动天线在无线通信中的性能，特别是在高频段和单视距路径下。", "motivation": "现有的可移动天线（MA）研究主要关注由不同传播路径引起的相位变化，并利用天线移动来最大化信道增益，这种狭隘的关注限制了MA的全部潜力。", "method": "本文提出了一个极化感知可移动天线（PAMA）框架，该框架将极化效应整合到可移动天线的设计和优化中。具体而言，引入了一个基于电磁理论的极化感知信道模型，揭示了MA在优化极化匹配方面的优势。", "result": "研究结果表明，将极化考量纳入可移动天线显著提高了效率、链路可靠性和数据吞吐量。PAMA扩展了MA的适用性，使其能够应用于射频、多径丰富场景之外的更高频段（如毫米波），即使在单一视距（LOS）路径下也能发挥作用。", "conclusion": "整合极化效应到可移动天线的设计中，能够显著提升无线通信的性能，为未来更鲁棒和高效的无线网络奠定基础。", "translation": "本文提出了一个极化感知可移动天线（PAMA）框架，该框架将极化效应整合到可移动天线（MA）的设计和优化中。尽管MA已被证明能有效提升无线通信性能，但现有研究主要关注由不同传播路径引起的相位变化，并利用天线移动来最大化信道增益。这种狭隘的关注限制了MA的全部潜力。在这项工作中，我们引入了一个植根于电磁理论的极化感知信道模型，揭示了MA相对于预编码等其他无线技术的一个决定性优势：优化极化匹配的能力。这种新理解使PAMA能够将MA的适用性从射频、多径丰富场景扩展到更高频段，如毫米波，即使在单一视距（LOS）路径下也能实现。我们的研究结果表明，将极化考量纳入MA显著增强了效率、链路可靠性和数据吞吐量，为未来更鲁棒和高效的无线网络铺平了道路。", "summary": "本文提出了一种极化感知可移动天线（PAMA）框架，旨在通过将极化效应纳入可移动天线（MA）的设计和优化来克服现有MA研究的局限性。通过引入基于电磁理论的极化感知信道模型，PAMA揭示了MA在优化极化匹配方面的独特优势，从而扩展了MA在高频段（如毫米波）甚至单视距环境下的应用潜力。研究结果证实，整合极化考量显著提升了无线通信的效率、链路可靠性和数据吞吐量，为未来网络发展奠定基础。", "keywords": "可移动天线, 极化感知, 毫米波, 信道模型, 无线通信", "comments": "这篇论文的创新点在于将极化效应引入到可移动天线的设计和优化中，这突破了现有研究仅关注相位变化的局限。通过引入极化感知信道模型，揭示了可移动天线在极化匹配方面的独特优势，并扩展了其在高频段和单视距场景下的应用，这对于未来毫米波通信等高频无线网络的发展具有重要意义。"}}
{"id": "2507.18366", "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation", "authors": ["Lakshmana Sri Harsha Nemani", "P. K. Srijith", "Tomasz Kuśmierczyk"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18366v1", "summary": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18366v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过证据知识蒸馏实现LLM中的高效不确定性量化", "tldr": "本文提出了一种通过证据知识蒸馏将不确定性感知教师模型蒸馏到紧凑学生模型的方法，以在LLM中高效地进行不确定性量化，只需一次前向传播即可达到或超越教师模型的性能。", "motivation": "标准大型语言模型（LLMs）准确的不确定性量化仍是一个关键挑战。现有的贝叶斯和基于集成的方法虽然有效，但通常需要计算成本高昂的采样和多次前向传播来估计预测不确定性。", "method": "本文引入了一种新颖的方法，通过知识蒸馏在LLMs中实现高效且有效的不确定性估计。具体来说，将需要多次前向传播的不确定性感知教师模型蒸馏到使用低秩适应（LoRA）进行微调的紧凑学生模型中。比较了两种蒸馏策略：一种是学生模型使用传统的softmax输出，另一种是学生模型利用狄利克雷分布输出通过证据学习显式建模认知不确定性。", "result": "在分类数据集上的实证评估表明，学生模型相对于教师模型可以实现相当或更优的预测和不确定性量化性能，而关键是仅需一次前向传播。", "conclusion": "据作者所知，这是首次证明通过证据知识蒸馏可以在LLMs中实现即时且鲁棒的不确定性量化。", "translation": "准确的不确定性量化仍然是标准大型语言模型（LLMs）面临的一个关键挑战，这促使人们采用贝叶斯和基于集成的方法。然而，这些方法通常需要计算成本高昂的采样，涉及多次前向传播才能有效地估计预测不确定性。\n在本文中，我们介绍了一种新颖的方法，能够在不牺牲性能的情况下，在LLMs中实现高效且有效的不确定性估计。具体来说，我们将不确定性感知的教师模型（最初需要多次前向传播）蒸馏成紧凑的学生模型，这些学生模型共享相同的架构，但使用低秩适应（LoRA）进行微调。我们比较了两种不同的蒸馏策略：一种是学生模型采用传统的基于softmax的输出，另一种是学生模型利用狄利叶分布输出，通过证据学习明确地建模认知不确定性。\n在分类数据集上的实证评估表明，这些学生模型相对于其教师模型可以实现相当或更优的预测和不确定性量化性能，而关键是仅需一次前向传播。据我们所知，这是首次证明通过证据蒸馏可以在LLMs中实现即时且鲁棒的不确定性量化。", "summary": "本文提出一种高效且有效的大型语言模型（LLM）不确定性量化方法，通过将需要多次前向传播的不确定性感知教师模型，利用低秩适应（LoRA）蒸馏到紧凑的学生模型中。研究比较了使用softmax输出和狄利克雷分布输出（用于显式建模认知不确定性）的两种蒸馏策略。实验结果表明，蒸馏后的学生模型仅需一次前向传播，即可达到或超越教师模型在预测和不确定性量化方面的性能，是首次通过证据蒸馏在LLMs中实现即时鲁棒不确定性量化的尝试。", "keywords": "LLM, 不确定性量化, 知识蒸馏, 证据学习, LoRA", "comments": "这项工作提出了一种新颖且高效的方法来解决LLM中不确定性量化计算成本高昂的问题。通过知识蒸馏和LoRA技术，它成功地将复杂模型的能力转移到更紧凑、更快的模型中，同时保持甚至提升了性能。特别是引入狄利克雷分布输出进行证据学习，为建模认知不确定性提供了一个有前景的方向。其创新点在于将不确定性量化与高效蒸馏相结合，为LLM的实际应用提供了重要的性能提升。"}}
{"id": "2505.14980", "title": "Rate-Accuracy Bounds in Visual Coding for Machines", "authors": ["Ivan V. Bajić"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures, IEEE MIPR 2025", "url": "http://arxiv.org/abs/2505.14980v3", "summary": "Increasingly, visual signals such as images, videos and point clouds are\nbeing captured solely for the purpose of automated analysis by computer vision\nmodels. Applications include traffic monitoring, robotics, autonomous driving,\nsmart home, and many others. This trend has led to the need to develop\ncompression strategies for these signals for the purpose of analysis rather\nthan reconstruction, an area often referred to as \"coding for machines.\" By\ndrawing parallels with lossy coding of a discrete memoryless source, in this\npaper we derive rate-accuracy bounds on several popular problems in visual\ncoding for machines, and compare these with state-of-the-art results from the\nliterature. The comparison shows that the current results are at least an order\nof magnitude -- and in some cases two or three orders of magnitude -- away from\nthe theoretical bounds in terms of the bitrate needed to achieve a certain\nlevel of accuracy. This, in turn, means that there is much room for improvement\nin the current methods for visual coding for machines.", "comment": "8 pages, 8 figures, IEEE MIPR 2025", "pdf_url": "http://arxiv.org/pdf/2505.14980v3", "cate": "eess.IV", "date": "2025-05-20", "updated": "2025-07-23", "AI": {"title_translation": "机器视觉编码中的速率-精度界限", "tldr": "本文推导了机器视觉编码中的速率-精度界限，并发现当前最先进的方法距离理论极限仍有巨大差距，表明该领域有很大的改进空间。", "motivation": "随着图像、视频和点云等视觉信号越来越多地被计算机视觉模型用于自动化分析（如交通监控、机器人、自动驾驶、智能家居等），需要开发针对分析而非重建目的的信号压缩策略，即“机器视觉编码”。", "method": "本文通过类比离散无记忆信源的有损编码，推导了机器视觉编码中几个常见问题的速率-精度界限，并将其与现有文献中的最新成果进行了比较。", "result": "比较结果表明，当前的方法在达到特定精度水平所需的比特率方面，与理论界限之间至少存在一个数量级，在某些情况下甚至是两到三个数量级的差距。", "conclusion": "目前的机器视觉编码方法仍有很大的改进空间，远未达到理论最优性能。", "translation": "视觉信号（如图像、视频和点云）正越来越多地被捕获，其唯一目的是供计算机视觉模型进行自动化分析。应用包括交通监控、机器人技术、自动驾驶、智能家居等。这一趋势导致需要为这些信号开发压缩策略，以便进行分析而非重建，这一领域通常被称为“机器视觉编码”。通过与离散无记忆信源的有损编码进行类比，本文推导了机器视觉编码中几个流行问题的速率-精度界限，并将其与现有文献中的最新成果进行了比较。比较结果表明，当前的结果在达到特定精度水平所需的比特率方面，与理论界限之间至少存在一个数量级——在某些情况下是两到三个数量级——的差距。这反过来意味着，当前的机器视觉编码方法有很大的改进空间。", "summary": "本文研究了专门为机器分析而设计的视觉信号编码，即“机器视觉编码”问题。通过借鉴离散无记忆信源的有损编码理论，作者推导了机器视觉编码中速率-精度之间的理论界限。研究发现，当前最先进的机器视觉编码方法在比特率效率方面，与这些理论界限相比存在显著差距（一到三个数量级），这表明该领域存在巨大的优化和改进潜力。", "keywords": "机器视觉编码, 速率-精度界限, 视觉信号, 压缩, 计算机视觉", "comments": "该论文通过建立理论速率-精度界限，为机器视觉编码领域设定了重要的基准。其创新之处在于将经典信息论中的有损编码理论应用于新兴的“为机器编码”场景。论文的重要性在于揭示了当前方法与理论最优性能之间的巨大差距，为未来研究指明了方向，激励研究人员开发更高效的视觉压缩策略。"}}
{"id": "2312.05407", "title": "ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation", "authors": ["Md Shazid Islam", "Sayak Nag", "Arindam Dutta", "Miraj Ahmed", "Fahim Faisal Niloy", "Amit K. Roy-Chowdhury"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.05407v3", "summary": "Unsupervised domain adaptive segmentation typically relies on self-training\nusing pseudo labels predicted by a pre-trained network on an unlabeled target\ndataset. However, the noisy nature of such pseudo-labels presents a major\nbottleneck in adapting a network to the distribution shift between source and\ntarget datasets. This challenge is exaggerated when the network encounters an\nincoming data stream in online fashion, where the network is constrained to\nadapt to incoming streams of target domain data in exactly one round of forward\nand backward passes. In this scenario, relying solely on inaccurate\npseudo-labels can lead to low-quality segmentation, which is detrimental to\nmedical image analysis where accuracy and precision are of utmost priority. We\nhypothesize that a small amount of pixel-level annotation obtained from an\nexpert can address this problem, thereby enhancing the performance of domain\nadaptation of online streaming data, even in the absence of dedicated training\ndata. We call our method ODES: Domain Adaptation with Expert Guidance for\nOnline Medical Image Segmentation that adapts to each incoming data batch in an\nonline setup, incorporating feedback from an expert through active learning.\nThrough active learning, the most informative pixels in each image can be\nselected for expert annotation. However, the acquisition of pixel-level\nannotations across all images in a batch often leads to redundant information\nwhile increasing temporal overhead in online learning. To reduce the annotation\nacquisition time and make the adaptation process more online-friendly, we\nfurther propose a novel image-pruning strategy that selects the most useful\nsubset of images from the current batch for active learning. Our proposed\napproach outperforms existing online adaptation approaches and produces\ncompetitive results compared to offline domain adaptive active learning\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.05407v3", "cate": "cs.CV", "date": "2023-12-08", "updated": "2025-07-23", "AI": {"title_translation": "ODES：专家指导的在线医学图像分割域适应", "tldr": "ODES是一种新的域适应方法，通过结合专家指导和图像剪枝策略来解决在线医学图像分割中伪标签噪声和在线数据流的挑战，提高了分割性能。", "motivation": "传统的无监督域适应在在线医学图像分割中依赖有噪声的伪标签，导致网络难以适应源域和目标域之间的数据分布变化。当网络在线处理传入数据流时，这种挑战更加突出，因为网络受限于单次前向和反向传播。在这种情况下，仅依赖不准确的伪标签会导致低质量的分割结果，这对于对准确性和精度要求极高的医学图像分析是极其不利的。", "method": "本文提出ODES：一种用于在线医学图像分割的专家指导域适应方法。该方法通过主动学习将专家反馈整合到在线设置中，适应每个传入的数据批次，选择每张图像中最具信息量的像素进行专家标注。为了减少标注获取时间并使适应过程更适合在线学习，进一步提出了一种新颖的图像剪枝策略，从当前批次中选择最有用的图像子集进行主动学习。", "result": "本文提出的方法优于现有的在线适应方法，并且与离线域适应主动学习方法相比，产生了具有竞争力的结果。", "conclusion": "通过少量专家像素级标注和新颖的图像剪枝策略，ODES方法能够有效解决在线医学图像分割中的域适应挑战，显著提升分割性能，并优于现有在线适应方法，与离线方法表现相当。", "translation": "无监督域适应分割通常依赖于使用预训练网络在未标注目标数据集上预测的伪标签进行自训练。然而，此类伪标签的噪声性质是网络适应源域和目标域之间分布变化的 主要瓶颈。当网络以在线方式遇到传入数据流时，这种挑战被放大，因为网络被限制在仅一次前向和反向传播中适应传入的目标域数据流。在这种情况下，仅依赖不准确的伪标签可能导致低质量的分割，这对于准确性和精度至关重要的医学图像分析是 有害的。我们假设从专家那里获得少量像素级标注可以解决这个问题，从而即使在没有专用训练数据的情况下，也能增强在线流式数据域适应的性能。我们称我们的方法为 ODES：专家指导的在线医学图像分割域适应，它在在线设置中适应每个传入的数据批次，通过主动学习整合来自专家的反馈。通过主动学习，可以选择每张图像中最具信息量的像素进行专家标注。然而，在批次中所有图像上获取像素级标注通常会导致信息冗余，同时增加在线学习中的时间开销。为了减少标注获取时间并使适应过程更适合在线，我们进一步提出了一种新颖的图像剪枝策略，从当前批次中选择最有用的图像子集进行主动学习。我们提出的方法优于现有的在线适应方法，并且与离线域适应主动学习方法相比，产生了具有竞争力的结果。", "summary": "ODES是一种针对在线医学图像分割的域适应方法，旨在解决无监督域适应中伪标签噪声带来的挑战，尤其是在处理在线数据流时。该方法通过主动学习引入少量专家像素级标注来增强适应性能，并创新性地提出图像剪枝策略以高效选择最具信息量的图像进行标注，从而减少时间开销。实验证明，ODES在性能上超越了现有在线适应方法，并能与离线域适应主动学习方法媲美。", "keywords": "域适应, 医学图像分割, 在线学习, 专家指导, 主动学习", "comments": "该论文的创新点在于将专家指导（通过主动学习获取少量像素级标注）与在线域适应相结合，以应对医学图像分割中伪标签噪声和实时数据流的挑战。特别是其提出的图像剪枝策略，有效解决了在线学习中标注获取效率的问题，使得专家知识的引入更加实用和高效。这对于需要高精度且数据持续流入的医学图像分析领域具有重要意义。"}}
{"id": "2507.18284", "title": "Designing Value-Aligned Traffic Agents through Conflict Sensitivity", "authors": ["Astrid Rakow", "Joe Collenette", "Maike Schwammberger", "Marija Slavkovik", "Gleifer Vs Alves"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Short version of this paper has been accepted at EUMAS 2025 this https URL", "url": "http://arxiv.org/abs/2507.18284v1", "summary": "Autonomous traffic agents (ATAs) are expected to act in ways tat are not only\nsafe, but also aligned with stakeholder values across legal, social, and moral\ndimensions. In this paper, we adopt an established formal model of conflict\nfrom epistemic game theory to support the development of such agents. We focus\non value conflicts-situations in which agents face competing goals rooted in\nvalue-laden situations and show how conflict analysis can inform key phases of\nthe design process. This includes value elicitation, capability specification,\nexplanation, and adaptive system refinement. We elaborate and apply the concept\nof Value-Aligned Operational Design Domains (VODDs) to structure autonomy in\naccordance with contextual value priorities. Our approach shifts the emphasis\nfrom solving moral dilemmas at runtime to anticipating and structuring\nvalue-sensitive behaviour during development.", "comment": "Short version of this paper has been accepted at EUMAS 2025\n  https://euramas.github.io/eumas2025/", "pdf_url": "http://arxiv.org/pdf/2507.18284v1", "cate": "cs.MA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过冲突敏感性设计价值对齐的交通智能体", "tldr": "本文提出了一种利用认知博弈论中的冲突形式模型来设计与利益相关者价值观对齐的自动交通智能体的方法，将重点从运行时解决道德困境转移到开发阶段预测和构建价值敏感行为。", "motivation": "自动交通智能体（ATAs）不仅需要安全，还需要在法律、社会和道德维度上与利益相关者的价值观保持一致。传统方法在运行时解决道德困境，而本文旨在通过设计阶段的冲突敏感性来更好地实现价值对齐。", "method": "本文采用认知博弈论中已建立的冲突形式模型来支持自动交通智能体的开发。研究聚焦于价值冲突，并展示了冲突分析如何影响设计过程的关键阶段，包括价值提取、能力规范、解释和自适应系统完善。此外，还阐述并应用了“价值对齐操作设计域”（VODDs）概念，以根据上下文价值优先级构建自主性。", "result": "通过使用冲突分析，本文展示了如何将价值对齐融入到自动交通智能体的设计过程的关键阶段。该方法将重点从在运行时解决道德困境转移到在开发过程中预测和构建价值敏感行为。", "conclusion": "本文提出了一种通过冲突敏感性来设计价值对齐交通智能体的新方法，强调在开发阶段而非运行时解决价值冲突的重要性，从而更好地确保自动交通智能体的行为与利益相关者价值观保持一致。", "translation": "自动交通智能体（ATAs）被期望以不仅安全，而且在法律、社会和道德维度上与利益相关者价值观对齐的方式行事。在本文中，我们采用认知博弈论中已建立的冲突形式模型来支持此类智能体的开发。我们专注于价值冲突——即智能体面临植根于价值情境中的相互竞争目标的情况，并展示了冲突分析如何为设计过程的关键阶段提供信息。这包括价值提取、能力规范、解释和自适应系统完善。我们阐述并应用了“价值对齐操作设计域”（VODDs）的概念，以根据上下文价值优先级构建自主性。我们的方法将重点从在运行时解决道德困境转移到在开发过程中预测和构建价值敏感行为。", "summary": "本文提出了一种通过冲突敏感性来设计价值对齐自动交通智能体的方法。它采用认知博弈论中的冲突形式模型，并引入价值对齐操作设计域（VODDs）概念，将价值对齐的重点从运行时决策转移到开发阶段的预见性设计，以确保智能体行为符合法律、社会和道德价值观。", "keywords": "自动交通智能体, 价值对齐, 冲突敏感性, 认知博弈论, VODDs", "comments": "本文的创新之处在于将认知博弈论中的冲突形式模型引入到自动交通智能体的设计中，并提出了VODDs概念，从而将价值对齐的重点从被动地在运行时解决道德困境转变为主动地在开发阶段进行预测和结构化。这种方法有望提高自动交通智能体的可信赖性和社会接受度，具有重要的理论和实践意义。"}}
{"id": "2502.18639", "title": "Quantum Machine Learning in Precision Medicine and Drug Discovery -- A Game Changer for Tailored Treatments?", "authors": ["Markus Bertl", "Alan Mott", "Salvatore Sinno", "Bhavika Bhalgamiya"], "categories": ["cs.ET", "cs.AI", "quant-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "Comments:      presented at AISoLA 2024", "url": "http://arxiv.org/abs/2502.18639v2", "summary": "The digitization of healthcare presents numerous challenges, including the\ncomplexity of biological systems, vast data generation, and the need for\npersonalized treatment plans. Traditional computational methods often fall\nshort, leading to delayed and sometimes ineffective diagnoses and treatments.\nQuantum Computing (QC) and Quantum Machine Learning (QML) offer transformative\nadvancements with the potential to revolutionize medicine. This paper\nsummarizes areas where QC promises unprecedented computational power, enabling\nfaster, more accurate diagnostics, personalized treatments, and enhanced drug\ndiscovery processes. However, integrating quantum technologies into precision\nmedicine also presents challenges, including errors in algorithms and high\ncosts. We show that mathematically-based techniques for specifying, developing,\nand verifying software (formal methods) can enhance the reliability and\ncorrectness of QC. By providing a rigorous mathematical framework, formal\nmethods help to specify, develop, and verify systems with high precision. In\ngenomic data analysis, formal specification languages can precisely (1) define\nthe behavior and properties of quantum algorithms designed to identify genetic\nmarkers associated with diseases. Model checking tools can systematically\nexplore all possible states of the algorithm to (2) ensure it behaves correctly\nunder all conditions, while theorem proving techniques provide mathematical (3)\nproof that the algorithm meets its specified properties, ensuring accuracy and\nreliability. Additionally, formal optimization techniques can (4) enhance the\nefficiency and performance of quantum algorithms by reducing resource usage,\nsuch as the number of qubits and gate operations. Therefore, we posit that\nformal methods can significantly contribute to enabling QC to realize its full\npotential as a game changer in precision medicine.", "comment": "presented at AISoLA 2024", "pdf_url": "http://arxiv.org/pdf/2502.18639v2", "cate": "cs.ET", "date": "2025-02-25", "updated": "2025-07-24", "AI": {"title_translation": "量子机器学习在精准医疗和药物发现中的应用——个性化治疗的颠覆者？", "tldr": "量子计算和量子机器学习有望彻底改变精准医疗和药物发现，通过形式化方法可以提高其可靠性和正确性，从而充分发挥其潜力。", "motivation": "医疗保健数字化面临生物系统复杂性、大数据生成和个性化治疗需求等挑战。传统计算方法力有不逮，导致诊断和治疗延迟或无效。", "method": "本文总结了量子计算和量子机器学习在精准医疗和药物发现中的应用潜力，并提出通过数学形式化方法来增强量子计算的可靠性和正确性。具体而言，形式化方法可以用于：1) 精确定义量子算法的行为和属性；2) 使用模型检测工具系统探索算法状态以确保其正确性；3) 利用定理证明技术提供算法满足其指定属性的数学证明；4) 通过形式化优化技术提高量子算法的效率和性能。", "result": "量子计算和量子机器学习有望提供前所未有的计算能力，实现更快、更准确的诊断、个性化治疗和增强的药物发现过程。形式化方法可以提高量子算法的可靠性、准确性和效率，例如在基因组数据分析中，通过精确定义、验证和优化量子算法，减少资源使用。", "conclusion": "形式化方法可以显著促进量子计算充分发挥其作为精准医疗颠覆者的潜力。", "translation": "医疗保健的数字化带来了诸多挑战，包括生物系统的复杂性、海量数据的生成以及对个性化治疗方案的需求。传统的计算方法往往力不从心，导致诊断和治疗的延迟，有时甚至无效。量子计算（QC）和量子机器学习（QML）提供了变革性的进步，有可能彻底改变医学。本文总结了量子计算有望提供前所未有的计算能力，从而实现更快、更准确的诊断、个性化治疗和增强的药物发现过程的领域。然而，将量子技术整合到精准医疗中也带来了挑战，包括算法错误和高成本。我们表明，用于指定、开发和验证软件的基于数学的技术（形式化方法）可以增强量子计算的可靠性和正确性。通过提供严谨的数学框架，形式化方法有助于高精度地指定、开发和验证系统。在基因组数据分析中，形式化规约语言可以精确地（1）定义旨在识别与疾病相关的遗传标记的量子算法的行为和属性。模型检测工具可以系统地探索算法的所有可能状态，以（2）确保其在所有条件下都能正确运行，而定理证明技术则提供数学（3）证明，证明算法符合其指定的属性，从而确保准确性和可靠性。此外，形式化优化技术可以通过减少资源使用（例如量子比特和门操作的数量）来（4）提高量子算法的效率和性能。因此，我们认为形式化方法可以显著促进量子计算充分发挥其作为精准医疗颠覆者的潜力。", "summary": "本文探讨了量子计算（QC）和量子机器学习（QML）在精准医疗和药物发现领域的变革潜力，旨在解决传统计算方法在处理复杂生物数据和实现个性化治疗方面的不足。论文指出，尽管量子技术面临算法错误和高成本等挑战，但通过引入数学形式化方法，可以显著提高量子算法的可靠性、正确性、准确性和效率。具体而言，形式化方法能够精确定义量子算法、系统验证其行为、提供数学证明并优化资源使用，从而帮助量子计算充分发挥其在精准医疗中的颠覆性作用。", "keywords": "量子机器学习, 精准医疗, 药物发现, 形式化方法, 量子计算", "comments": "该论文提出了一个重要的观点，即形式化方法可以作为解决量子计算和量子机器学习在精准医疗应用中面临的可靠性和正确性挑战的关键工具。其创新之处在于将严谨的数学验证引入到新兴的量子技术领域，这对于确保未来医疗应用的安全性和有效性至关重要。论文强调了在量子技术发展初期就考虑其验证和优化的问题，具有前瞻性。"}}
{"id": "2504.08305", "title": "A 55-nm SRAM Chip Scanning Errors Every 125 ns for Event-Wise Soft Error Measurement", "authors": ["Yuibi Gomi", "Akira Sato", "Waleed Madany", "Kenichi Okada", "Satoshi Adachi", "Masatoshi Itoh", "Masanori Hashimoto"], "categories": ["physics.ins-det", "cs.AR"], "primary_category": "Subjects:       Instrumentation and Detectors (physics.ins-det)", "pdf_link": null, "comments": "Comments:      4 pages, 9 figures, accepted for publication in IEEE Solid-State Circuits Letters (SSCL)", "url": "http://arxiv.org/abs/2504.08305v2", "summary": "We developed a 55 nm CMOS SRAM chip that scans all data every 125 ns and\noutputs timestamped soft error data via an SPI interface through a FIFO. The\nproposed system, consisting of the developed chip and particle detectors,\nenables event-wise soft error measurement and precise identification of SBUs\nand MCUs, thus resolving misclassifications such as Pseudo- and Distant MCUs\nthat conventional methods cannot distinguish. An 80-MeV proton irradiation\nexperiment at RARiS, Tohoku University verified the system operation.\nTimestamps between the SRAM chip and the particle detectors were successfully\nsynchronized, accounting for PLL disturbances caused by radiation. Event\nbuilding was achieved by determining a reset offset with sub-ns resolution, and\nspatial synchronization was maintained within several tens of micrometers.", "comment": "4 pages, 9 figures, accepted for publication in IEEE Solid-State\n  Circuits Letters (SSCL)", "pdf_url": "http://arxiv.org/pdf/2504.08305v2", "cate": "physics.ins-det", "date": "2025-04-11", "updated": "2025-07-24", "AI": {"title_translation": "用于事件级软错误测量的每125纳秒扫描错误的55纳米SRAM芯片", "tldr": "开发了一种55纳米SRAM芯片系统，能够以125纳秒的扫描速度进行事件级软错误测量，并精确识别单比特和多比特错误，解决了传统方法的误分类问题。", "motivation": "传统方法无法区分伪多比特错误和远距离多比特错误等误分类，因此需要开发一种新的系统来实现事件级软错误测量和精确识别单比特错误（SBU）和多比特错误（MCU）。", "method": "研究人员开发了一种55纳米CMOS SRAM芯片，该芯片每125纳秒扫描所有数据，并通过SPI接口和FIFO输出带时间戳的软错误数据。该系统由开发的芯片和粒子探测器组成。通过80-MeV质子辐照实验验证了系统操作，并成功同步了SRAM芯片和粒子探测器之间的时间戳，考虑了辐射引起的PLL干扰。通过确定亚纳秒分辨率的复位偏移实现了事件构建，并将空间同步保持在几十微米以内。", "result": "开发的系统在RARiS的80-MeV质子辐照实验中验证了其操作。SRAM芯片和粒子探测器之间的时间戳成功同步，并考虑了辐射引起的PLL干扰。通过确定亚纳秒分辨率的复位偏移，实现了事件构建，并且空间同步保持在几十微米以内。", "conclusion": "开发的55纳米SRAM芯片系统能够实现事件级软错误测量和SBU与MCU的精确识别，解决了传统方法无法区分的误分类问题，并通过质子辐照实验验证了其有效性。", "translation": "我们开发了一种55纳米CMOS SRAM芯片，该芯片每125纳秒扫描所有数据，并通过SPI接口通过FIFO输出带时间戳的软错误数据。所提出的系统由开发的芯片和粒子探测器组成，能够实现事件级软错误测量和SBU与MCU的精确识别，从而解决了传统方法无法区分的伪多比特错误和远距离多比特错误等误分类问题。在东北大学RARiS进行的80-MeV质子辐照实验验证了系统操作。SRAM芯片和粒子探测器之间的时间戳成功同步，并考虑了辐射引起的PLL干扰。通过确定亚纳秒分辨率的复位偏移实现了事件构建，并将空间同步保持在几十微米以内。", "summary": "本文介绍了一种新型55纳米CMOS SRAM芯片系统，该系统具备每125纳秒扫描一次数据并输出带时间戳软错误信息的能力。结合粒子探测器，该系统能够进行事件级软错误测量，并精确区分单比特（SBU）和多比特（MCU）错误，有效解决了传统方法中存在的伪多比特和远距离多比特错误等误分类问题。系统在80-MeV质子辐照实验中得到验证，并成功实现了时间戳同步和高精度空间同步。", "keywords": "SRAM芯片, 软错误测量, 单比特错误, 多比特错误, 时间同步", "comments": "这项研究的创新之处在于开发了一种高扫描频率（每125纳秒）的SRAM芯片，并将其集成到一套系统中，实现了事件级的软错误测量。其重要性在于能够精确识别不同类型的软错误，解决了传统方法在误分类方面的局限性，这对于高可靠性电子设备的设计和评估具有重要意义。该系统在辐射环境下的同步和数据处理能力也值得关注。"}}
{"id": "2507.18260", "title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection", "authors": ["Junyao Li", "Yahao Lu", "Xingyuan Guo", "Xiaoyu Xian", "Tiantian Wang", "Yukai Shi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to Neural Networks. We propose the Gaussian Group Squeezer, leveraging Gaussian sampling and compression with diffusion models for channel-based data augmentation", "url": "http://arxiv.org/abs/2507.18260v1", "summary": "Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.", "comment": "Submitted to Neural Networks. We propose the Gaussian Group Squeezer,\n  leveraging Gaussian sampling and compression with diffusion models for\n  channel-based data augmentation", "pdf_url": "http://arxiv.org/pdf/2507.18260v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用高斯不可知表示学习与扩散先验增强红外小目标检测", "tldr": "本文提出了一种利用高斯不可知表示学习和两阶段扩散模型来增强红外小目标检测（ISTD）的方法，特别是在高质量数据稀缺的场景下，提高了模型在实际应用中的鲁棒性和合成样本的质量。", "motivation": "现有的红外小目标检测（ISTD）方法过度依赖大量昂贵的手动标注数据进行表示学习，这使得它们在面对高质量红外数据稀缺的实际挑战时显得非常脆弱。", "method": "本文提出了高斯不可知表示学习（Gaussian Agnostic Representation Learning），并引入了高斯群压缩器（Gaussian Group Squeezer），利用高斯采样和压缩进行非均匀量化，以增强模型对各种挑战的韧性。此外，还引入了两阶段扩散模型进行真实世界重建，通过将量化信号与真实世界分布对齐，显著提升了合成样本的质量和保真度。", "result": "在各种数据稀缺场景下，与最先进的检测方法进行的对比评估表明，所提出的方法是有效的。", "conclusion": "所提出的利用高斯不可知表示学习和扩散先验的方法，能够有效增强红外小目标检测的性能，特别是在高质量数据稀缺的实际应用中表现出优越的鲁棒性和合成样本质量。", "translation": "红外小目标检测（ISTD）在众多实际应用中扮演着至关重要的角色。为了探究性能边界，研究人员利用大量昂贵的手动标注数据进行表示学习。然而，这种方法使得最先进的ISTD方法在实际挑战中变得非常脆弱。在本文中，我们首先研究了几种主流方法在各种数据稀缺——即缺乏高质量红外数据——情况下的检测性能变化，这些变化挑战了当前关于实用ISTD的流行理论。为了解决这一问题，我们引入了高斯不可知表示学习。具体来说，我们提出了高斯群压缩器，利用高斯采样和压缩进行非均匀量化。通过利用多样化的训练样本，我们增强了ISTD模型应对各种挑战的韧性。然后，我们引入了两阶段扩散模型进行真实世界重建。通过将量化信号与真实世界分布紧密对齐，我们显著提升了合成样本的质量和保真度。在各种数据稀缺场景下，与最先进的检测方法进行的对比评估证明了所提出方法的有效性。", "summary": "本文针对红外小目标检测（ISTD）在数据稀缺环境下性能脆弱的问题，提出了一种利用高斯不可知表示学习的方法。该方法引入了高斯群压缩器进行非均匀量化以增强模型韧性，并结合两阶段扩散模型进行高质量的真实世界样本重建。实验结果表明，在数据稀缺场景下，所提出的方法比现有最先进方法更有效。", "keywords": "红外小目标检测, 高斯不可知表示学习, 扩散模型, 数据稀缺, 表示学习", "comments": "该论文解决了红外小目标检测领域中一个关键的实际问题：数据稀缺性。其创新点在于结合了高斯不可知表示学习和扩散模型，以提高模型在有限高质量数据下的鲁棒性和合成样本的质量，这对于实际部署具有重要意义。"}}
{"id": "2411.10371", "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects", "authors": ["Qing Cheng", "Zefan Zeng", "Xingchen Hu", "Yuehang Si", "Zhong Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.10371v5", "summary": "Event Causality Identification (ECI) has become an essential task in Natural\nLanguage Processing (NLP), focused on automatically detecting causal\nrelationships between events within texts. This comprehensive survey\nsystematically investigates fundamental concepts and models, developing a\nsystematic taxonomy and critically evaluating diverse models. We begin by\ndefining core concepts, formalizing the ECI problem, and outlining standard\nevaluation protocols. Our classification framework divides ECI models into two\nprimary tasks: Sentence-level Event Causality Identification (SECI) and\nDocument-level Event Causality Identification (DECI). For SECI, we review\nmodels employing feature pattern-based matching, machine learning classifiers,\ndeep semantic encoding, prompt-based fine-tuning, and causal knowledge\npre-training, alongside data augmentation strategies. For DECI, we focus on\napproaches utilizing deep semantic encoding, event graph reasoning, and\nprompt-based fine-tuning. Special attention is given to recent advancements in\nmulti-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large\nLanguage Models (LLMs). We analyze the strengths, limitations, and unresolved\nchallenges associated with each approach. Extensive quantitative evaluations\nare conducted on four benchmark datasets to rigorously assess the performance\nof various ECI models. We conclude by discussing future research directions and\nhighlighting opportunities to advance the field further.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.10371v5", "cate": "cs.CL", "date": "2024-11-15", "updated": "2025-07-24", "AI": {"title_translation": "事件因果识别综述：分类、挑战、评估与展望", "tldr": "这篇综述系统性地调查了事件因果识别（ECI）领域，提出了一个分类框架，评估了不同模型，并讨论了未来的研究方向。", "motivation": "事件因果识别（ECI）已成为自然语言处理（NLP）中一项重要任务，旨在自动检测文本中事件间的因果关系。本综述旨在系统地调查该领域，提供一个全面的概述。", "method": "本综述系统地调查了基本概念和模型，开发了一个系统的分类框架，并批判性地评估了各种模型。它将ECI模型分为句子级（SECI）和文档级（DECI），并回顾了各种方法，包括基于特征模式匹配、机器学习分类器、深度语义编码、基于提示的微调、因果知识预训练等。同时，还关注了多语言、跨语言和零样本ECI的最新进展，分析了每种方法的优缺点和未解决的挑战。此外，还在四个基准数据集上进行了广泛的定量评估。", "result": "本综述提出了一个系统的ECI分类框架，将模型分为句子级和文档级。它详细审查并分析了不同ECI方法的优势、局限性和未解决的挑战。通过在四个基准数据集上进行广泛的定量评估，严格评估了各种ECI模型的性能。", "conclusion": "本综述讨论了事件因果识别领域的未来研究方向，并强调了进一步推动该领域发展的机遇。", "translation": "事件因果识别（ECI）已成为自然语言处理（NLP）中一项重要任务，专注于自动检测文本中事件间的因果关系。这项全面的综述系统地调查了基本概念和模型，开发了一个系统的分类法并批判性地评估了各种模型。我们首先定义了核心概念，形式化了ECI问题，并概述了标准评估协议。我们的分类框架将ECI模型分为两个主要任务：句子级事件因果识别（SECI）和文档级事件因果识别（DECI）。对于SECI，我们回顾了采用特征模式匹配、机器学习分类器、深度语义编码、基于提示的微调和因果知识预训练的模型，以及数据增强策略。对于DECI，我们重点关注利用深度语义编码、事件图推理和基于提示的微调的方法。我们特别关注多语言和跨语言ECI的最新进展，以及利用大型语言模型（LLMs）的零样本ECI。我们分析了每种方法的优点、局限性和未解决的挑战。在四个基准数据集上进行了广泛的定量评估，以严格评估各种ECI模型的性能。最后，我们讨论了未来的研究方向，并强调了进一步推动该领域发展的机遇。", "summary": "这篇综述全面探讨了事件因果识别（ECI）领域，这是一项在自然语言处理中检测事件因果关系的关键任务。文章首先定义了核心概念、形式化了ECI问题并概述了评估协议。它提出了一个分类框架，将ECI模型分为句子级（SECI）和文档级（DECI），并详细回顾了每类中的各种方法，包括传统方法和基于深度学习、大语言模型（LLMs）的最新进展。综述还分析了不同方法的优缺点和未解决的挑战，并在四个基准数据集上进行了定量评估。最后，文章讨论了未来的研究方向和发展机遇。", "keywords": "事件因果识别, 自然语言处理, 因果关系, 综述, 大语言模型", "comments": "这是一篇全面且系统性的综述论文，对于事件因果识别（ECI）领域的研究人员和从业者具有重要价值。它不仅提供了清晰的分类和方法回顾，还指出了当前挑战和未来方向，有助于领域内的研究进展。"}}
{"id": "2507.15511", "title": "Certificate-Sensitive Subset Sum: Realizing Instance Complexity", "authors": ["Jesus Salas"], "categories": ["cs.CC", "cs.DS", "F.1.3; F.2.2"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      14 pages + appendix. Companion to arXiv:2503.20162 (\"Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2^{n/2} Enumeration\"", "url": "http://arxiv.org/abs/2507.15511v2", "summary": "We present, to our knowledge, the first deterministic, certificate-sensitive\nalgorithm for a canonical NP-complete problem whose runtime provably adapts to\nthe structure of each input. For a Subset-Sum instance $(S, t)$, let\n$\\Sigma(S)$ denote the set of distinct subset sums and define $U =\n|\\Sigma(S)|$. This set serves as an information-theoretically minimal witness,\nthe instance-complexity (IC) certificate.\n  Our solver, IC-SubsetSum, enumerates every element of $\\Sigma(S)$ in\ndeterministic time $O(U \\cdot n^2)$ and space $O(U \\cdot n)$. A randomized\nvariant achieves expected runtime $O(U \\cdot n)$. The algorithm's complexity is\nthus directly governed by the certificate size, and this structure-sensitive\nperformance is paired with a guaranteed worst-case runtime of $O^*(2^{n/2 -\n\\varepsilon})$ for some constant $\\varepsilon > 0$, the first such result to\nstrictly outperform classical methods on every instance.\n  We revisit fine-grained reductions that rely on the classical $2^{n/2}$\nhardness of SubsetSum and show that these arguments hold only for\ncollision-free instances where $U$ is maximal. IC-SubsetSum reframes this\nbarrier structurally and introduces a new paradigm for certificate-sensitive\nalgorithms across NP-complete problems.", "comment": "14 pages + appendix. Companion to arXiv:2503.20162 (\"Beyond\n  Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2^{n/2}\n  Enumeration\"", "pdf_url": "http://arxiv.org/pdf/2507.15511v2", "cate": "cs.CC", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "证书敏感子集和：实现实例复杂性", "tldr": "提出首个确定性、证书敏感的子集和算法IC-SubsetSum，其运行时间与实例复杂性（证书大小U）成正比，并首次在所有实例上严格优于经典算法，引入了NP完全问题的新范式。", "motivation": "开发第一个确定性的、证书敏感的算法，其运行时能够根据每个输入的结构进行调整；解决NP完全问题，特别是子集和问题，以实现运行时对输入结构（即实例复杂性证书）的适应性；挑战并重新审视依赖于经典$2^{n/2}$子集和硬度的小粒度归约的适用范围。", "method": "提出了一个名为IC-SubsetSum的确定性、证书敏感算法，用于解决子集和问题；该算法通过枚举所有不同的子集和集合$\\\\Sigma(S)$的每个元素来实现；定义了信息论上最小的见证，即实例复杂性（IC）证书$U = |\\\\Sigma(S)|$，算法的复杂度直接受其大小控制；还提出了一个随机化变体。", "result": "IC-SubsetSum算法的确定性版本在$O(U \\\\cdot n^2)$时间复杂度和$O(U \\\\cdot n)$空间复杂度内枚举$\\\\Sigma(S)$的每个元素；随机化变体实现了$O(U \\\\cdot n)$的预期运行时间；算法的复杂性直接由证书大小$U$决定，实现了结构敏感的性能；该算法具有$O^*(2^{n/2 - \\\\varepsilon})$的保证最坏情况运行时间（其中$\\\\varepsilon > 0$为常数），这是首次在所有实例上严格优于经典方法的结果；研究揭示了依赖于经典$2^{n/2}$子集和硬度的小粒度归约仅适用于$U$达到最大值的无冲突实例。", "conclusion": "IC-SubsetSum算法通过将计算复杂度与实例复杂性（由证书大小$U$衡量）直接关联，为子集和问题提供了一种新的、更高效的解决方案；该研究重新定义了NP完全问题中的性能障碍，并为开发证书敏感算法引入了一个新的范式；它挑战并修正了对子集和问题经典硬度论证的理解，指出其适用范围的局限性。", "translation": "我们提出了一个确定性的、证书敏感的算法，用于解决一个经典的NP完全问题，据我们所知，这是第一个其运行时间能根据每个输入的结构进行证明性适应的算法。对于一个子集和实例$(S, t)$，令$\\\\Sigma(S)$表示不同子集和的集合，并定义$U = |\\\\Sigma(S)|$。这个集合作为信息论上最小的见证，即实例复杂性（IC）证书。\n我们的求解器IC-SubsetSum以确定性时间$O(U \\\\cdot n^2)$和空间$O(U \\\\cdot n)$枚举$\\\\Sigma(S)$的每个元素。一个随机化变体实现了$O(U \\\\cdot n)$的预期运行时间。因此，算法的复杂性直接受证书大小的控制，并且这种结构敏感的性能与一个保证的最坏情况运行时间$O^*(2^{n/2 - \\\\varepsilon})$（对于某个常数$\\\\varepsilon > 0$）相匹配，这是第一个在每个实例上都严格优于经典方法的结果。\n我们重新审视了依赖于子集和经典$2^{n/2}$硬度的小粒度归约，并表明这些论证仅适用于$U$最大化的无冲突实例。IC-SubsetSum从结构上重新定义了这一障碍，并为NP完全问题中的证书敏感算法引入了一个新范式。", "summary": "本文提出了IC-SubsetSum，这是首个确定性、证书敏感的子集和问题算法。该算法的运行时间$O(U \\\\cdot n^2)$和空间$O(U \\\\cdot n)$（随机化版本为$O(U \\\\cdot n)$预期时间）直接由实例复杂性证书$U = |\\\\Sigma(S)|$（不同子集和的数量）决定。它首次在所有实例上严格优于经典算法，最坏情况运行时为$O^*(2^{n/2 - \\\\varepsilon})$。研究还指出，依赖于经典$2^{n/2}$硬度的小粒度归约仅适用于$U$最大的无冲突实例，从而为NP完全问题中的证书敏感算法引入了新范式。", "keywords": "子集和, 证书敏感算法, 实例复杂性, NP完全问题, 算法复杂度", "comments": "创新性：首次提出了一个确定性且证书敏感的NP完全问题算法，其运行时能根据输入结构（实例复杂性证书$U$）自适应调整。这是对经典算法范式的重大突破。重要性：算法在所有实例上都严格优于传统的$O^*(2^{n/2})$方法，这在理论和实践上都具有重要意义。它重新定义了对子集和问题硬度的理解，并为其他NP完全问题提供了新的解决思路和范式。局限性/影响：论文揭示了现有小粒度归约的局限性，即它们仅在特定（无冲突且$U$最大）的实例上成立，这修正了对NP完全问题复杂性的认识。"}}
{"id": "2507.17895", "title": "Lower Bounds for Public-Private Learning under Distribution Shift", "authors": ["Amrith Setlur", "Pratiksha Thaker", "Jonathan Ullman"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.17895v1", "summary": "The most effective differentially private machine learning algorithms in\npractice rely on an additional source of purportedly public data. This paradigm\nis most interesting when the two sources combine to be more than the sum of\ntheir parts. However, there are settings such as mean estimation where we have\nstrong lower bounds, showing that when the two data sources have the same\ndistribution, there is no complementary value to combining the two data\nsources. In this work we extend the known lower bounds for public-private\nlearning to setting where the two data sources exhibit significant distribution\nshift. Our results apply to both Gaussian mean estimation where the two\ndistributions have different means, and to Gaussian linear regression where the\ntwo distributions exhibit parameter shift. We find that when the shift is small\n(relative to the desired accuracy), either public or private data must be\nsufficiently abundant to estimate the private parameter. Conversely, when the\nshift is large, public data provides no benefit.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.17895v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "分布偏移下公私学习的下界", "tldr": "本文研究了在存在显著分布偏移的情况下，公私数据结合学习的下界，发现当偏移较小时，公私数据需要足够丰富才能估计参数；当偏移较大时，公共数据无益。", "motivation": "实践中，有效的差分隐私机器学习算法依赖于额外的公共数据源。当这两种数据源结合能产生超越其部分总和的价值时，这种范式最有趣。然而，在均值估计等场景中，已知当两种数据源分布相同时，结合它们没有互补价值。本文旨在将已知的公私学习下界扩展到存在显著分布偏移的场景。", "method": "本文将已知的公私学习下界扩展到两种数据源存在显著分布偏移的设置。研究方法应用于高斯均值估计（两种分布均值不同）和高斯线性回归（两种分布参数偏移）。", "result": "研究发现，当分布偏移较小（相对于所需精度）时，公共数据或私有数据必须足够丰富才能准确估计私有参数。相反，当偏移较大时，公共数据不提供任何益处。", "conclusion": "在公私学习中，分布偏移对公共数据源的效用有显著影响：小偏移要求数据量充足，大偏移则使公共数据失去价值。", "translation": "实践中最有效的差分隐私机器学习算法依赖于额外的公共数据源。当这两种来源结合起来的价值大于它们各自部分的总和时，这种范式最有趣。然而，在均值估计等场景中，我们有很强的下界，表明当两种数据源具有相同分布时，结合它们没有互补价值。在这项工作中，我们将公私学习的已知下界扩展到两种数据源表现出显著分布偏移的设置。我们的结果适用于两种分布具有不同均值的高斯均值估计，以及两种分布表现出参数偏移的高斯线性回归。我们发现，当偏移较小（相对于所需精度）时，公共或私有数据必须足够丰富才能估计私有参数。相反，当偏移较大时，公共数据不提供任何益处。", "summary": "本文研究了在数据源存在显著分布偏移的情况下，公私学习（即结合公共和私有数据进行机器学习）的理论下界。研究扩展了现有结果，并应用于高斯均值估计和高斯线性回归。结果表明，当分布偏移较小（相对于所需精度）时，公共或私有数据必须足够丰富才能有效估计参数；而当偏移较大时，公共数据则不再提供任何益处。", "keywords": "公私学习, 分布偏移, 下界, 差分隐私, 均值估计", "comments": "这项工作通过引入“分布偏移”这一重要变量，深化了对公私学习范式的理解。其创新之处在于揭示了公共数据在不同偏移程度下的互补价值，为实际应用中如何有效利用公共数据提供了理论指导。尤其是在隐私保护机器学习日益重要的背景下，理解数据源异质性对模型性能的影响具有重要意义。"}}
{"id": "2506.15690", "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs", "authors": ["Tianyu Wang", "Akira Horiguchi", "Lingyou Pang", "Carey E. Priebe"], "categories": ["cs.LG", "cs.AI", "cs.SI", "stat.ME"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15690v3", "summary": "The increasing use of synthetic data from the public Internet has enhanced\ndata usage efficiency in large language model (LLM) training. However, the\npotential threat of model collapse remains insufficiently explored. Existing\nstudies primarily examine model collapse in a single model setting or rely\nsolely on statistical surrogates. In this work, we introduce LLM Web Dynamics\n(LWD), an efficient framework for investigating model collapse at the network\nlevel. By simulating the Internet with a retrieval-augmented generation (RAG)\ndatabase, we analyze the convergence pattern of model outputs. Furthermore, we\nprovide theoretical guarantees for this convergence by drawing an analogy to\ninteracting Gaussian Mixture Models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15690v3", "cate": "cs.LG", "date": "2025-05-26", "updated": "2025-07-24", "AI": {"title_translation": "LLM Web Dynamics：追溯LLM网络中的模型崩溃", "tldr": "本文引入LLM Web Dynamics (LWD) 框架，通过模拟互联网和RAG数据库，在网络层面研究大型语言模型（LLM）的模型崩溃现象，并提供理论保证。", "motivation": "现有研究对大型语言模型（LLM）模型崩溃的潜在威胁探索不足，主要局限于单个模型设置或依赖统计替代，未能从网络层面进行深入研究。", "method": "本文引入了LLM Web Dynamics (LWD) 框架，通过使用检索增强生成（RAG）数据库模拟互联网，分析模型输出的收敛模式，并通过与交互式高斯混合模型的类比提供理论保证。", "result": "分析了模型输出的收敛模式，并为这种收敛提供了理论保证。", "conclusion": "本文成功在网络层面分析了LLM的模型输出收敛模式，并提供了理论保证，为理解和缓解模型崩溃提供了新视角。", "translation": "大型语言模型（LLM）训练中合成数据在公共互联网上的日益普及提高了数据使用效率。然而，模型崩溃的潜在威胁仍未得到充分探索。现有研究主要在单一模型设置中检验模型崩溃，或仅依赖统计替代。在这项工作中，我们引入了LLM网络动力学（LLM Web Dynamics, LWD），一个用于在网络层面调查模型崩溃的有效框架。通过使用检索增强生成（RAG）数据库模拟互联网，我们分析了模型输出的收敛模式。此外，通过类比交互式高斯混合模型，我们为这种收敛提供了理论保证。", "summary": "本文提出了LLM Web Dynamics (LWD) 框架，旨在解决现有研究在探索大型语言模型（LLM）模型崩溃问题时仅限于单一模型或统计替代的局限。LWD通过使用检索增强生成（RAG）数据库模拟互联网，从而能够在网络层面分析LLM模型输出的收敛模式。研究不仅揭示了这种收敛行为，还通过与交互式高斯混合模型的类比，为模型收敛提供了理论保证，为深入理解和缓解LLM在网络环境中的模型崩溃现象提供了新的工具和见解。", "keywords": "LLM, 模型崩溃, 网络动力学, RAG, 理论保证", "comments": "这篇论文的创新点在于首次在网络层面而非单一模型设置下研究LLM的模型崩溃问题，并引入了LLM Web Dynamics (LWD) 这一新颖框架。通过模拟互联网环境和提供理论保证，它为理解LLM在复杂网络交互中的动态行为，特别是模型崩溃的机制，提供了重要的新视角和研究工具。"}}
{"id": "2507.06174", "title": "Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model", "authors": ["Koki Yamane", "Yunhan Li", "Masashi Konosu", "Koki Inami", "Junji Oaki", "Sho Sakaino", "Toshiaki Tsuji"], "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 9 figures, Submitted to CoRL 2025", "url": "http://arxiv.org/abs/2507.06174v5", "summary": "In recent years, the advancement of imitation learning has led to increased\ninterest in teleoperating low-cost manipulators to collect demonstration data.\nHowever, most existing systems rely on unilateral control, which only transmits\ntarget position values. While this approach is easy to implement and suitable\nfor slow, non-contact tasks, it struggles with fast or contact-rich operations\ndue to the absence of force feedback. This work demonstrates that fast\nteleoperation with force feedback is feasible even with force-sensorless,\nlow-cost manipulators by leveraging 4-channel bilateral control. Based on\naccurately identified manipulator dynamics, our method integrates nonlinear\nterms compensation, velocity and external force estimation, and variable gain\ncorresponding to inertial variation. Furthermore, using data collected by\n4-channel bilateral control, we show that incorporating force information into\nboth the input and output of learned policies improves performance in imitation\nlearning. These results highlight the practical effectiveness of our system for\nhigh-fidelity teleoperation and data collection on affordable hardware.", "comment": "20 pages, 9 figures, Submitted to CoRL 2025", "pdf_url": "http://arxiv.org/pdf/2507.06174v5", "cate": "cs.RO", "date": "2025-07-08", "updated": "2025-07-24", "AI": {"title_translation": "基于精确动力学模型的无力传感器力控制的快速双边远程操作与模仿学习", "tldr": "本文展示了即使是无力传感器的低成本机械臂，通过4通道双边控制和精确动力学模型，也能实现快速且带力反馈的远程操作，并发现将力信息融入模仿学习能提升性能。", "motivation": "现有低成本机械臂的远程操作系统多依赖单边控制，仅传输目标位置，易于实现但缺乏力反馈，不适用于快速或接触丰富的任务。", "method": "本文通过利用4通道双边控制，即使在无力传感器的低成本机械臂上，也实现了带力反馈的快速远程操作。该方法基于精确识别的机械臂动力学，整合了非线性项补偿、速度和外部力估计以及对应惯量变化的变增益。此外，通过4通道双边控制收集的数据，作者展示了将力信息整合到学习策略的输入和输出中，可以提高模仿学习的性能。", "result": "研究结果表明，即使是无力传感器的低成本机械臂，也能实现带力反馈的快速远程操作。将力信息融入模仿学习策略的输入和输出中，可以提高其性能。", "conclusion": "本文系统在经济实惠的硬件上实现了高保真远程操作和数据收集，证明了其在实际应用中的有效性。", "translation": "近年来，模仿学习的进步使得人们对远程操作低成本机械臂以收集演示数据越来越感兴趣。然而，大多数现有系统依赖于单边控制，仅传输目标位置值。尽管这种方法易于实现且适用于缓慢、非接触任务，但由于缺乏力反馈，它在快速或接触丰富的操作中表现不佳。这项工作表明，即使是无力传感器的低成本机械臂，通过利用4通道双边控制，也能实现带力反馈的快速远程操作。基于精确识别的机械臂动力学，我们的方法整合了非线性项补偿、速度和外部力估计，以及对应惯量变化的变增益。此外，利用通过4通道双边控制收集的数据，我们发现将力信息整合到学习策略的输入和输出中可以提高模仿学习的性能。这些结果突出了我们系统在经济实惠的硬件上实现高保真远程操作和数据收集的实际有效性。", "summary": "本文提出了一种针对无力传感器的低成本机械臂的快速双边远程操作系统，通过精确的动力学模型实现了力反馈控制，并集成了非线性补偿、速度/力估计和变增益技术。研究还表明，将力信息纳入模仿学习策略的输入和输出可以提升其性能，从而为高保真远程操作和经济硬件上的数据收集提供了实用方案。", "keywords": "双边远程操作, 无力传感器, 力控制, 模仿学习, 动力学模型", "comments": "这项工作具有重要的创新性，它解决了低成本机械臂在缺乏力传感器的情况下实现高精度力反馈远程操作的挑战。通过利用精确的动力学模型和创新的控制策略（如4通道双边控制、非线性补偿和力估计），该研究显著扩展了低成本硬件的应用范围。其在模仿学习中整合力信息以提高性能的发现也具有实际意义，为数据收集和技能学习提供了更有效的方法。其主要优势在于，它使得在经济可行的硬件上实现复杂、高精度的机器人任务成为可能。"}}
{"id": "2507.18158", "title": "Stability Constrained Voltage Control in Distribution Grids with Arbitrary Communication Infrastructure", "authors": ["Zhenyi Yuan", "Jie Feng", "Yuanyuan Shi", "Jorge Cortés"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18158v1", "summary": "We consider the problem of designing learning-based reactive power\ncontrollers that perform voltage regulation in distribution grids while\nensuring closed-loop system stability. In contrast to existing methods, where\nthe provably stable controllers are restricted to be decentralized, we propose\na unified design framework that enables the controllers to take advantage of an\narbitrary communication infrastructure on top of the physical power network.\nThis allows the controllers to incorporate information beyond their local bus,\ncovering existing methods as a special case and leading to less conservative\nconstraints on the controller design. We then provide a design procedure to\nconstruct input convex neural network (ICNN) based controllers that satisfy the\nidentified stability constraints by design under arbitrary communication\nscenarios, and train these controllers using supervised learning. Simulation\nresults on the the University of California, San Diego (UCSD) microgrid testbed\nillustrate the effectiveness of the framework and highlight the role of\ncommunication in improving control performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18158v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "具有任意通信基础设施的配电网中稳定性受限的电压控制", "tldr": "本文提出一个统一的学习型无功功率控制器设计框架，用于在配电网中进行电压调节，同时确保系统闭环稳定性，并利用任意通信基础设施来提高控制性能。", "motivation": "现有可证明稳定的控制器仅限于分散式，无法利用通信基础设施获取非本地信息，导致控制器设计保守。", "method": "提出一个统一的设计框架，允许控制器利用任意通信基础设施；设计基于输入凸神经网络（ICNN）的控制器，通过监督学习进行训练，以满足稳定性约束。", "result": "在加州大学圣地亚哥分校（UCSD）微电网测试平台上的仿真结果表明了该框架的有效性，并突出了通信在提高控制性能方面的作用。", "conclusion": "该研究成功设计了利用任意通信基础设施、确保稳定性的学习型电压控制器，并通过仿真验证了其有效性和通信对性能提升的重要性。", "translation": "我们考虑设计基于学习的无功功率控制器的问题，这些控制器在配电网中执行电压调节，同时确保闭环系统稳定性。与现有方法不同，现有方法中可证明稳定的控制器仅限于分散式，我们提出了一个统一的设计框架，使控制器能够在物理电网之上利用任意通信基础设施。这使得控制器能够整合其本地总线之外的信息，将现有方法作为特例涵盖，并导致控制器设计的约束更不保守。然后，我们提供了一个设计程序，用于构建基于输入凸神经网络（ICNN）的控制器，这些控制器在任意通信场景下通过设计满足已识别的稳定性约束，并使用监督学习训练这些控制器。在加州大学圣地亚哥分校（UCSD）微电网测试平台上的仿真结果说明了该框架的有效性，并突出了通信在提高控制性能中的作用。", "summary": "本文提出了一个统一的学习型无功功率控制器设计框架，用于在配电网中进行电压调节，同时确保闭环系统稳定性。该框架允许控制器利用任意通信基础设施，整合本地总线之外的信息，从而克服了现有分散式方法的局限性，并减少了控制器设计的保守性。研究构建了基于输入凸神经网络（ICNN）的控制器，并通过监督学习进行训练。仿真结果验证了该框架的有效性以及通信对提升控制性能的重要性。", "keywords": "电压控制, 配电网, 稳定性, 通信基础设施, 学习型控制器, 输入凸神经网络", "comments": "本文的创新点在于提出了一个统一的框架，允许学习型电压控制器利用任意通信基础设施，突破了传统稳定控制器仅限于分散式的限制。这使得控制器能够获取更全面的信息，从而实现更优的控制性能和更不保守的设计。该方法结合了机器学习（ICNN）和电力系统控制，具有较强的实用价值。"}}
{"id": "1808.08451", "title": "Energy-preserving continuous-stage Runge-Kutta-Nyström methods", "authors": ["Wensheng Tang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      The paper needs to be improved.", "url": "http://arxiv.org/abs/1808.08451v3", "summary": "Many practical problems can be described by second-order system\n$\\ddot{q}=-M\\nabla U(q)$, in which people give special emphasis to some\ninvariants with explicit physical meaning, such as energy, momentum, angular\nmomentum, etc. However, conventional numerical integrators for such systems\nwill fail to preserve any of these quantities which may lead to qualitatively\nincorrect numerical solutions. This paper is concerned with the development of\nenergy-preserving continuous-stage Runge-Kutta-Nystr\\\"om (csRKN) methods for\nsolving second-order systems. Sufficient conditions for csRKN methods to be\nenergy-preserving are presented and it is proved that all the energy-preserving\ncsRKN methods satisfying these sufficient conditions can be essentially induced\nby energy-preserving continuous-stage partitioned Runge-Kutta methods. Some\nillustrative examples are given and relevant numerical results are reported.", "comment": "The paper needs to be improved.", "pdf_url": "http://arxiv.org/pdf/1808.08451v3", "cate": "math.NA", "date": "2018-08-25", "updated": "2025-07-24", "AI": {"title_translation": "能量守恒的连续阶段Runge-Kutta-Nyström方法", "tldr": "本文开发了能量守恒的连续阶段Runge-Kutta-Nyström方法，用于求解二阶系统，并给出了其能量守恒的充分条件。", "motivation": "许多实际问题可用二阶系统描述，其中能量等物理量是重要的不变量。然而，传统的数值积分器无法保存这些量，导致数值解出现定性错误。", "method": "开发了能量守恒的连续阶段Runge-Kutta-Nyström (csRKN) 方法。提出了csRKN方法能量守恒的充分条件，并证明了所有满足这些充分条件的能量守恒csRKN方法本质上可以由能量守恒的连续阶段分离Runge-Kutta方法导出。", "result": "给出了若干说明性示例，并报告了相关的数值结果。这些结果表明所提出的方法能够有效保持能量。", "conclusion": "所提出的能量守恒连续阶段Runge-Kutta-Nyström方法能够有效解决二阶系统的能量守恒问题，避免了传统方法导致的定性错误。", "translation": "许多实际问题可以用二阶系统 $\\ddot{q}=-M\\nabla U(q)$ 来描述，其中人们特别强调一些具有明确物理意义的不变量，例如能量、动量、角动量等。然而，这些系统的传统数值积分器将无法保留任何这些量，这可能导致定性上不正确的数值解。本文关注开发用于求解二阶系统的能量守恒连续阶段Runge-Kutta-Nyström (csRKN) 方法。文中提出了csRKN方法能量守恒的充分条件，并证明了所有满足这些充分条件的能量守恒csRKN方法本质上可以由能量守恒的连续阶段分离Runge-Kutta方法导出。给出了一些说明性示例并报告了相关的数值结果。", "summary": "本文针对二阶系统 $\\ddot{q}=-M\\nabla U(q)$ 中传统数值积分器无法保持能量等不变量的问题，提出了一种能量守恒的连续阶段Runge-Kutta-Nyström (csRKN) 方法。研究工作给出了csRKN方法能量守恒的充分条件，并证明了满足这些条件的能量守恒csRKN方法可由能量守恒的连续阶段分离Runge-Kutta方法导出。通过示例和数值结果验证了该方法的有效性。", "keywords": "能量守恒, 连续阶段Runge-Kutta-Nyström方法, 二阶系统, 数值积分, 充分条件", "comments": "本文的创新点在于提出了能量守恒的连续阶段Runge-Kutta-Nyström方法，并从理论上证明了其与连续阶段分离Runge-Kutta方法的联系，为解决二阶系统模拟中的能量守恒问题提供了新的有效途径。其重要性体现在能够获得定性正确的数值解，避免传统方法带来的误差积累。"}}
{"id": "2507.18140", "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.18140v1", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.18140v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MathOPEval：一个用于数学推理中多模态大语言模型视觉操作的细粒度评估基准", "tldr": "本文提出了MathOPEval，一个细粒度评估基准，用于衡量多模态大语言模型（MLLMs）在数学推理中基于代码执行视觉操作的能力，发现现有模型与人类表现仍有显著差距。", "motivation": "现有的多模态大语言模型（MLLMs）评估主要集中于文本推理输出，而忽略了模型通过代码执行精确视觉操作的能力，这方面仍未被充分探索。", "method": "本研究提出了一个名为MathOPEval的框架，用于评估MLLM在多模态数学推理中基于代码的能力。该框架专注于两个关键评估方面：1) 多模态代码生成（MCG），评估模型从头开始准确理解和构建可视化的能力；2) 多模态代码编辑（MCE），评估模型进行细粒度操作（包括删除、修改和注释）的能力。为评估这些任务，研究整合了一个数据集，涵盖几何图、函数图和三种统计图等五种最流行的数学图形类型。实验评估了九个主流MLLM。", "result": "实验结果显示，现有模型在执行细粒度视觉操作方面与人类表现仍有显著差距。", "conclusion": "多模态大语言模型在数学推理中进行细粒度视觉操作的能力仍远低于人类水平，需要进一步的提升和研究。", "translation": "多模态大语言模型（MLLMs）的最新进展使得基于文本指令执行视觉操作，从而实现分步多模态数学推理成为可能。一种有前景的方法是使用代码作为中间表示，以精确表达和操作推理步骤中的图像。然而，现有评估主要集中于纯文本推理输出，使得MLLM通过代码执行精确视觉操作的能力在很大程度上仍未被探索。这项工作迈出了解决这一空白的第一步，通过评估MLLM在多模态数学推理中基于代码的能力。具体而言，我们的框架侧重于两个关键评估方面：(1) 多模态代码生成（MCG）评估模型从头开始准确理解和构建可视化的能力。(2) 多模态代码编辑（MCE）评估模型进行细粒度操作的能力，其中包括三种类型：删除、修改和注释。为了评估上述任务，我们整合了一个数据集，涵盖五种最流行的数学图形类型，包括几何图、函数图和三种统计图，以提供对现有MLLM的全面有效测量。我们的实验评估涉及九个主流MLLM，结果表明现有模型在执行细粒度视觉操作方面仍显著落后于人类表现。", "summary": "本文介绍了MathOPEval，一个用于评估多模态大语言模型（MLLMs）在数学推理中基于代码的视觉操作能力的细粒度基准。针对现有评估忽视视觉操作的空白，MathOPEval框架包含多模态代码生成（MCG）和多模态代码编辑（MCE）两方面。研究构建了一个包含五种数学图形类型的数据集，并对九个主流MLLM进行了实验评估。结果表明，现有模型在执行细粒度视觉操作方面与人类表现存在显著差距。", "keywords": "MLLMs, 视觉操作, 数学推理, 评估基准, 代码生成", "comments": "本文的创新之处在于首次将评估重点放在多模态大语言模型（MLLMs）在数学推理中通过代码执行“视觉操作”的能力上，填补了现有评估主要关注文本输出的空白。其重要性在于提供了一个急需的细粒度评估基准和数据集，揭示了当前MLLMs在视觉操作方面与人类表现的巨大差距，为未来研究指明了方向。"}}
{"id": "2507.17990", "title": "Rapid Modeling Architecture for Lightweight Simulator to Accelerate and Improve Decision Making for Industrial Systems", "authors": ["Takumi Kato", "Zhi Li Hu"], "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st International Conference on Automation Science and Engineering (CASE 2025)", "url": "http://arxiv.org/abs/2507.17990v1", "summary": "Designing industrial systems, such as building, improving, and automating\ndistribution centers and manufacturing plants, involves critical\ndecision-making with limited information in the early phases. The lack of\ninformation leads to less accurate designs of the systems, which are often\ndifficult to resolve later. It is effective to use simulators to model the\ndesigned system and find out the issues early. However, the modeling time\nrequired by conventional simulators is too long to allow for rapid model\ncreation to meet decision-making demands. In this paper, we propose a Rapid\nModeling Architecture (RMA) for a lightweight industrial simulator that\nmitigates the modeling burden while maintaining the essential details in order\nto accelerate and improve decision-making. We have prototyped a simulator based\non the RMA and applied it to the actual factory layout design problem. We also\ncompared the modeling time of our simulator to that of an existing simulator,\nand as a result, our simulator achieved a 78.3% reduction in modeling time\ncompared to conventional simulators.", "comment": "8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17990v1", "cate": "eess.SY", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "工业系统轻量级模拟器快速建模架构，以加速和改进决策", "tldr": "提出了一种快速建模架构（RMA），用于轻量级工业模拟器，可将建模时间缩短78.3%，从而加速和改进工业系统决策。", "motivation": "在工业系统（如配送中心和制造工厂）的早期设计阶段，决策者面临信息有限的问题，导致设计不准确且难以后期修正。传统模拟器所需的建模时间过长，无法满足快速决策的需求。", "method": "提出了一种快速建模架构（RMA），用于轻量级工业模拟器。该架构旨在减轻建模负担，同时保留关键细节。研究人员基于RMA原型化了一个模拟器，并将其应用于实际工厂布局设计问题。", "result": "与传统模拟器相比，基于RMA的模拟器将建模时间缩短了78.3%。", "conclusion": "RMA能够显著减少工业系统模拟器的建模时间，从而加速并改进早期设计阶段的决策。", "translation": "设计工业系统，例如建造、改进和自动化配送中心和制造工厂，在早期阶段涉及信息有限的关键决策。信息缺乏导致系统设计不准确，这通常难以在后期解决。使用模拟器对设计的系统进行建模并及早发现问题是有效的。然而，传统模拟器所需的建模时间过长，无法实现快速模型创建以满足决策需求。在本文中，我们提出了一种用于轻量级工业模拟器的快速建模架构（RMA），该架构在保持必要细节的同时减轻了建模负担，以加速和改进决策。我们基于RMA原型化了一个模拟器，并将其应用于实际工厂布局设计问题。我们还将我们的模拟器与现有模拟器的建模时间进行了比较，结果显示，与传统模拟器相比，我们的模拟器建模时间减少了78.3%。", "summary": "本文提出了一种名为快速建模架构（RMA）的方法，用于构建轻量级工业模拟器。该架构旨在解决传统模拟器建模时间过长的问题，通过减轻建模负担并保留关键细节，从而加速和改进工业系统早期设计阶段的决策。实验证明，基于RMA的模拟器能够将建模时间缩短78.3%，有效提升了决策效率和准确性。", "keywords": "快速建模架构, 轻量级模拟器, 工业系统, 决策制定, 建模时间", "comments": "这篇论文的创新点在于提出了快速建模架构（RMA），有效解决了工业系统模拟中建模时间过长的问题。通过显著缩短建模时间（78.3%的削减），RMA能够使决策者在早期设计阶段更快地获得反馈，从而改进决策质量并减少后期修正的成本。这对于需要快速迭代和优化设计的工业应用具有重要意义。"}}
{"id": "2507.17991", "title": "Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work", "authors": ["Peter Eckmann", "Adrian Barnett", "Alexandra Bannach-Brown", "Elisa Pilar Bascunan Atria", "Guillaume Cabanac", "Louise Delwen Owen Franzen", "Małgorzata Anna Gazda", "Kaitlyn Hair", "James Howison", "Halil Kilicoglu", "Cyril Labbe", "Sarah McCann", "Vladislav Nachev", "Martijn Roelandse", "Maia Salholz-Hillel", "Robert Schulz", "Gerben ter Riet", "Colby Vorland", "Anita Bandrowski", "Tracey Weissgerber"], "categories": ["cs.SE", "cs.IR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17991v1", "summary": "The causes of the reproducibility crisis include lack of standardization and\ntransparency in scientific reporting. Checklists such as ARRIVE and CONSORT\nseek to improve transparency, but they are not always followed by authors and\npeer review often fails to identify missing items. To address these issues,\nthere are several automated tools that have been designed to check different\nrigor criteria. We have conducted a broad comparison of 11 automated tools\nacross 9 different rigor criteria from the ScreenIT group. We found some\ncriteria, including detecting open data, where the combination of tools showed\na clear winner, a tool which performed much better than other tools. In other\ncases, including detection of inclusion and exclusion criteria, the combination\nof tools exceeded the performance of any one tool. We also identified key areas\nwhere tool developers should focus their effort to make their tool maximally\nuseful. We conclude with a set of insights and recommendations for stakeholders\nin the development of rigor and transparency detection tools. The code and data\nfor the study is available at https://github.com/PeterEckmann1/tool-comparison.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17991v1", "cate": "cs.SE", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "按指示使用？旨在检查已发表工作严谨性和透明度的软件工具比较", "tldr": "研究比较了11种自动化工具，用于检查已发表工作的严谨性和透明度，发现不同工具在不同标准下表现各异，并提出了改进建议。", "motivation": "科学报告中缺乏标准化和透明度导致了可重复性危机。尽管有ARRIVE和CONSORT等清单，但作者不总是遵循，同行评审也常未能发现缺失项。为了解决这些问题，一些自动化工具被设计出来检查严谨性标准。", "method": "研究对来自ScreenIT组的11种自动化工具，在9个不同的严谨性标准上进行了广泛比较。", "result": "发现某些标准（如检测开放数据）下，工具组合中存在一个表现远超其他工具的“明确赢家”；而在其他情况（如检测纳入和排除标准）下，工具组合的性能超过了任何单一工具。研究还确定了工具开发者应重点努力以最大化工具效用的关键领域。", "conclusion": "研究得出了一系列见解和建议，供严谨性和透明度检测工具开发中的利益相关者参考。", "translation": "可重复性危机的原因包括科学报告中缺乏标准化和透明度。ARRIVE和CONSORT等清单旨在提高透明度，但作者并不总是遵循，同行评审也常常未能识别缺失项。为了解决这些问题，已经设计了几种自动化工具来检查不同的严谨性标准。我们对来自ScreenIT组的11种自动化工具，在9个不同的严谨性标准上进行了广泛比较。我们发现某些标准，包括检测开放数据，工具组合显示出明显的赢家，该工具的表现远优于其他工具。在其他情况下，包括检测纳入和排除标准，工具组合的性能超过了任何单一工具。我们还确定了工具开发者应重点努力以最大化其工具效用的关键领域。我们最后提出了一系列见解和建议，供严谨性和透明度检测工具开发中的利益相关者参考。本研究的代码和数据可在https://github.com/PeterEckmann1/tool-comparison 获取。", "summary": "本文旨在解决科学报告中因缺乏标准化和透明度而导致的可重复性危机。研究比较了11种旨在检查已发表工作严谨性和透明度的自动化软件工具，评估它们在9个不同严谨性标准上的表现。结果显示，某些标准下存在表现卓越的单一工具，而另一些标准下工具组合的性能更优。研究还指出了工具开发需要改进的关键领域，并为相关利益者提供了开发严谨性和透明度检测工具的见解和建议。", "keywords": "自动化工具, 可重复性危机, 严谨性, 透明度, 软件比较", "comments": "这项研究通过系统比较现有自动化工具来解决科学报告中的可重复性危机，具有重要意义。其创新之处在于对多种工具进行了全面评估，并提供了具体的改进方向。这有助于推动更有效、更自动化的方法来提高科学研究的严谨性和透明度。"}}
{"id": "2507.17896", "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17896v1", "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17896v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VeriMinder：缓解NL2SQL中的分析漏洞", "tldr": "VeriMinder是一个交互式系统，旨在检测和缓解自然语言到数据库（NLIDB）应用中用户在提出分析问题时可能存在的认知偏差和分析漏洞，并通过LLM驱动的方法进行优化。", "motivation": "尽管自然语言接口数据库（NLIDBs）使数据分析民主化，但它也带来了挑战，即帮助缺乏统计分析背景的用户提出无偏见的分析问题。现有研究主要关注文本到SQL的生成准确性，而分析问题中的认知偏差问题却未得到充分探索。", "method": "本文提出了VeriMinder系统，它引入了三项关键创新：1) 一个针对特定分析上下文偏见的上下文语义映射框架；2) 一个操作化“难以改变原则”并指导用户进行系统数据分析的分析框架；3) 一个优化的LLM驱动系统，通过涉及多个候选、批评反馈和自我反思的结构化过程生成高质量、任务特定的提示。", "result": "用户测试证实了该方法的优点。在直接用户体验评估中，82.5%的参与者表示对分析质量产生了积极影响。在比较评估中，VeriMinder在分析的具体性、全面性和准确性指标上比替代方法得分显著更高，至少提高了20%。", "conclusion": "VeriMinder系统（一个网络应用程序和MIT许可的开源软件）旨在帮助用户在数据分析过程中避免“错误问题”的漏洞，从而促进社区内的进一步研究和采用。", "translation": "使用自然语言接口数据库（NLIDBs）的应用系统使数据分析民主化。这一积极发展也带来了一个紧迫的挑战，即帮助那些可能在没有统计分析背景的情况下使用这些系统的用户，来制定无偏见的分析问题。尽管大量研究集中在文本到SQL的生成准确性上，但解决分析问题中的认知偏差仍未得到充分探索。我们提出了VeriMinder，一个用于检测和缓解此类分析漏洞的交互式系统。我们的方法引入了三个关键创新点：(1) 一个针对特定分析上下文相关偏见的上下文语义映射框架；(2) 一个操作化“难以改变原则”并指导用户进行系统数据分析的分析框架；(3) 一个优化的、由大型语言模型（LLM）驱动的系统，该系统通过涉及多个候选、批评反馈和自我反思的结构化过程生成高质量、任务特定的提示。用户测试证实了我们方法的优点。在直接用户体验评估中，82.5%的参与者表示对分析质量产生了积极影响。在比较评估中，VeriMinder在分析的具体性、全面性和准确性指标上比替代方法得分显著更高，至少提高了20%。我们的系统作为网络应用程序实现，旨在帮助用户在数据分析过程中避免“错误问题”的漏洞。VeriMinder的代码库和提示作为MIT许可的开源软件提供，以促进社区内的进一步研究和采用。", "summary": "VeriMinder是一个交互式系统，旨在解决NLIDB应用中用户在提出分析问题时可能存在的认知偏差和分析漏洞。该系统通过引入上下文语义映射框架、操作化“难以改变原则”的分析框架以及一个优化的LLM驱动提示生成系统，帮助用户制定无偏见的分析问题。用户测试表明，VeriMinder显著提高了分析质量，并在具体性、全面性和准确性方面优于现有方法，是一个开源的Web应用。", "keywords": "NL2SQL, 认知偏差, 分析漏洞, VeriMinder, LLM", "comments": "VeriMinder的创新之处在于它首次将焦点从传统的文本到SQL生成准确性转移到解决NLIDB中用户提问的认知偏差问题，这对于数据分析的民主化和可靠性至关重要。其结合了语义映射、分析原则和LLM优化的多方面方法是其亮点，特别是LLM通过迭代反馈生成高质量提示的机制。作为一个开源的Web应用，它具有很高的实用性和推广价值。"}}
{"id": "2507.18370", "title": "Quantized Signal Recovery with Interference via Parametrized Look-Up Tables", "authors": ["Morriel Kasher", "Michael Tinston", "Predrag Spasojevic"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages, 18 figures", "url": "http://arxiv.org/abs/2507.18370v1", "summary": "Efficient all-digital post-correction of low-resolution analog-to-digital\nconverters can be achieved by using Look-Up Tables (LUTs). The performance of a\nLUT can be optimized by incorporating a parametric model for the expected input\nsignal, noise level, and interference signals. We evaluate three analytical\nestimators for integration with parametrized LUTs, especially with applications\nto low-resolution, non-linear, or wideband quantizers. We also propose several\napproximations to improve tractability of the estimation problem for\nPhase-Shift Keyed input signals and Linear Frequency Modulated interference\nsignals. Simulated results validate the ability of our estimator to recover the\ninstantaneous value of the desired input signal in real-time with a high degree\nof accuracy. This includes cancellation of harmonic distortion that aliases\ninto the desired signal bandwidth from front-end saturation due to high-power\nout-of-band interference. Our estimators are shown to achieve a significant\ngain over conventional linear-filtering techniques while also being robust to\nchanges in input parameters, non-linear quantizers, and time-variant\ninterference sources. For a tone input quantized to 3 bits and estimated with a\nfixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error\nand $>$20 dBc improvement in Spurious-Free Dynamic Range.", "comment": "13 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.18370v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过参数化查找表实现带干扰的量化信号恢复", "tldr": "本文提出了一种利用参数化查找表（LUTs）结合分析估计器和近似方法，对低分辨率模数转换器（ADCs）进行高效全数字后校正的方法，并在存在干扰的情况下实现了显著的信号恢复性能提升和鲁棒性。", "motivation": "本文旨在实现低分辨率模数转换器（ADCs）的高效全数字后校正，并通过为预期输入信号、噪声水平和干扰信号建立参数化模型来优化查找表（LUTs）的性能。", "method": "作者评估了三种与参数化查找表（LUTs）集成的分析估计器，特别适用于低分辨率、非线性或宽带量化器。他们还提出了几种近似方法，以提高相移键控（PSK）输入信号和线性调频（LFM）干扰信号的估计问题的可处理性。", "result": "仿真结果验证了所提出的估计器能够高精度地实时恢复所需输入信号的瞬时值，包括消除由高功率带外干扰导致前端饱和而产生的谐波失真。这些估计器相对于传统的线性滤波技术显示出显著增益，并且对输入参数、非线性量化器和时变干扰源的变化具有鲁棒性。对于一个量化为3位并使用固定12抽头模型阶数估计的音调输入，在均方误差（MSE）方面实现了>10 dB的改进，在无杂散动态范围（SFDR）方面实现了>20 dBc的改进。", "conclusion": "与传统的线性滤波技术相比，本文提出的参数化查找表结合分析估计器的方法，为带干扰的量化信号恢复提供了一个鲁棒且显著更有效的解决方案，尤其适用于低分辨率、非线性或宽带量化等具有挑战性的场景。", "translation": "高效的全数字低分辨率模数转换器后校正可以通过使用查找表（LUT）实现。通过为预期输入信号、噪声水平和干扰信号建立参数化模型，可以优化LUT的性能。我们评估了三种与参数化LUT集成的分析估计器，特别是应用于低分辨率、非线性或宽带量化器。我们还提出了几种近似方法，以提高相移键控输入信号和线性调频干扰信号估计问题的可处理性。仿真结果验证了我们的估计器能够高精度地实时恢复所需输入信号的瞬时值。这包括消除由于高功率带外干扰导致前端饱和而混叠到所需信号带宽内的谐波失真。我们的估计器被证明比传统的线性滤波技术实现了显著增益，同时对输入参数、非线性量化器和时变干扰源的变化具有鲁棒性。对于一个量化为3位并使用固定12抽头模型阶数估计的音调输入，我们在均方误差（MSE）方面实现了>10 dB的改进，在无杂散动态范围（SFDR）方面实现了>20 dBc的改进。", "summary": "本文提出了一种利用参数化查找表（LUTs）对低分辨率模数转换器（ADCs）进行高效全数字后校正的方法。通过为输入信号和干扰引入参数化模型，并结合分析估计器和所提出的近似方法，该方法能够实现所需信号的精确实时恢复。仿真结果表明，与传统线性滤波技术相比，该方法表现出卓越的性能（例如，均方误差改善>10 dB，无杂散动态范围改善>20 dBc）和鲁棒性，有效减轻了谐波失真并处理了各种复杂的量化场景。", "keywords": "量化信号恢复, 查找表, 模数转换器, 干扰消除, 参数化模型", "comments": "该论文的创新之处在于将参数化模型和分析估计器与查找表（LUTs）相结合用于ADC后校正，与传统方法相比，性能得到了显著提升。其对非线性和时变干扰的鲁棒性使其在复杂环境下的实际应用中特别有价值。"}}
{"id": "2507.06646", "title": "Assessing Learned Models for Phase-only Hologram Compression", "authors": ["Zicong Peng", "Yicheng Zhan", "Josef Spjut", "Kaan Akşit"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      SIGGRAPH 2025 Poster", "url": "http://arxiv.org/abs/2507.06646v2", "summary": "We evaluate the performance of four common learned models utilizing INR and\nVAE structures for compressing phase-only holograms in holographic displays.\nThe evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as\nthe representative VAE model. Our experiments reveal that a pretrained image\nVAE, TAESD, with 2.2M parameters struggles with phase-only hologram\ncompression, revealing the need for task-specific adaptations. Among the INRs,\nSIREN with 4.9k parameters achieves %40 compression with high quality in the\nreconstructed 3D images (PSNR = 34.54 dB). These results emphasize the\neffectiveness of INRs and identify the limitations of pretrained image\ncompression VAEs for hologram compression task.", "comment": "SIGGRAPH 2025 Poster", "pdf_url": "http://arxiv.org/pdf/2507.06646v2", "cate": "cs.GR", "date": "2025-07-09", "updated": "2025-07-24", "AI": {"title_translation": "评估用于仅相位全息图压缩的学习模型", "tldr": "本文评估了基于INR和VAE的学习模型在仅相位全息图压缩方面的性能。结果显示，INR模型（特别是SIREN）表现出色，而预训练的图像VAE则效果不佳，强调了任务特定适应的重要性。", "motivation": "在全息显示中，需要对仅相位全息图进行有效压缩。这项研究旨在评估不同学习模型在此任务中的表现，并找出适合该任务的特定模型。", "method": "研究评估了四种常用的学习模型，包括基于INR结构的普通MLP、SIREN和FilmSIREN，以及作为VAE代表的TAESD模型。这些模型被用于压缩全息显示中的仅相位全息图，并通过实验评估其性能。", "result": "实验发现，拥有2.2M参数的预训练图像VAE（TAESD）在仅相位全息图压缩方面表现不佳。然而，在INR模型中，拥有4.9k参数的SIREN模型实现了40%的压缩率，并在重建的3D图像中保持了高质量（PSNR = 34.54 dB）。", "conclusion": "研究结果强调了INR模型在全息图压缩方面的有效性，并指出了预训练图像压缩VAE在处理全息图压缩任务时的局限性，表明需要针对该任务进行模型适应。", "translation": "我们评估了四种常用的利用INR和VAE结构的学习模型在全息显示中压缩仅相位全息图的性能。评估的模型包括普通的MLP、SIREN和FilmSIREN，其中TAESD作为代表性的VAE模型。我们的实验表明，预训练的图像VAE（TAESD，拥有2.2M参数）在仅相位全息图压缩方面表现不佳，这揭示了对任务特定适应的需求。在INRs中，SIREN（拥有4.9k参数）实现了40%的压缩率，并在重建的3D图像中保持了高质量（PSNR = 34.54 dB）。这些结果强调了INR的有效性，并指出了预训练图像压缩VAE在全息图压缩任务中的局限性。", "summary": "本文评估了多种学习模型，特别是基于隐式神经表示（INR）和变分自编码器（VAE）的模型，在全息显示中对仅相位全息图进行压缩的性能。研究发现，虽然像TAESD这样的预训练图像VAE在处理这项专业任务时表现不佳，但INR模型，特别是SIREN，表现出显著的有效性，以紧凑的模型尺寸实现了高质量的压缩。这项研究强调了INR的潜力以及针对全息图压缩任务进行模型特定适应的必要性。", "keywords": "仅相位全息图, 全息图压缩, 隐式神经表示, 变分自编码器, SIREN", "comments": "本文为全息图压缩领域提供了宝贵的见解，特别是通过比较不同学习模型架构的适用性。其创新之处在于直接对比了INR和VAE在仅相位全息图压缩上的表现，这是一个相对较少探索的领域。小型INR模型优于大型预训练VAE的发现意义重大，为未来全息数据压缩研究指明了有前景的方向。"}}
{"id": "2507.18005", "title": "C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters", "authors": ["Shengye Song", "Minxian Xu", "Zuowei Zhang", "Chengxi Gao", "Fansong Zeng", "Yu Ding", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.18005v1", "summary": "Microservices transform traditional monolithic applications into lightweight,\nloosely coupled application components and have been widely adopted in many\nenterprises. Cloud platform infrastructure providers enhance the resource\nutilization efficiency of microservices systems by co-locating different\nmicroservices. However, this approach also introduces resource competition and\ninterference among microservices. Designing interference-aware strategies for\nlarge-scale, co-located microservice clusters is crucial for enhancing resource\nutilization and mitigating competition-induced interference. These challenges\nare further exacerbated by unreliable metrics, application diversity, and node\nheterogeneity.\n  In this paper, we first analyze the characteristics of large-scale and\nco-located microservices clusters at Alibaba and further discuss why cycle per\ninstruction (CPI) is adopted as a metric for interference measurement in\nlarge-scale production clusters, as well as how to achieve accurate prediction\nof CPI through multi-dimensional metrics. Based on CPI interference prediction\nand analysis, we also present the design of the C-Koordinator platform, an\nopen-source solution utilized in Alibaba cluster, which incorporates\nco-location and interference mitigation strategies. The interference prediction\nmodels consistently achieve over 90.3% accuracy, enabling precise prediction\nand rapid mitigation of interference in operational environments. As a result,\napplication latency is reduced and stabilized across all percentiles (P50, P90,\nP99) response time (RT), achieving improvements ranging from 16.7% to 36.1%\nunder various system loads compared with state-of-the-art system. These results\ndemonstrate the system's ability to maintain smooth application performance in\nco-located environments.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.18005v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "C-Koordinator：大规模和协同部署微服务集群的干扰感知管理", "tldr": "C-Koordinator是一个开源平台，通过使用CPI作为干扰度量和多维度指标预测，实现了对大规模协同部署微服务集群的干扰感知管理，显著降低了应用延迟并提高了性能稳定性。", "motivation": "微服务在企业中被广泛采用，但将不同微服务协同部署以提高资源利用率会导致资源竞争和干扰。在大规模协同部署的微服务集群中，设计干扰感知策略对于提高资源利用率和缓解干扰至关重要。不可靠的指标、应用多样性和节点异构性进一步加剧了这些挑战。", "method": "本文分析了阿里巴巴大规模协同部署微服务集群的特点，并探讨了为何选择每指令周期数（CPI）作为大规模生产集群中干扰测量的指标，以及如何通过多维度指标实现CPI的准确预测。基于CPI干扰预测和分析，论文提出了C-Koordinator平台的设计，这是一个在阿里巴巴集群中使用的开源解决方案，集成了协同部署和干扰缓解策略。", "result": "干扰预测模型始终保持90.3%以上的准确率，实现了对操作环境中干扰的精确预测和快速缓解。结果显示，与现有技术相比，在各种系统负载下，应用程序的延迟在所有百分位数（P50、P90、P99）响应时间（RT）上均得到降低和稳定，改进幅度从16.7%到36.1%不等。", "conclusion": "这些结果表明该系统能够在协同部署环境中保持流畅的应用程序性能。", "translation": "微服务将传统的单体应用转变为轻量级、松散耦合的应用组件，并已在许多企业中得到广泛采用。云平台基础设施提供商通过协同部署不同的微服务来提高微服务系统的资源利用效率。然而，这种方法也引入了微服务之间的资源竞争和干扰。为大规模协同部署的微服务集群设计干扰感知策略对于提高资源利用率和缓解竞争引起的干扰至关重要。不可靠的指标、应用多样性和节点异构性进一步加剧了这些挑战。\n\n在本文中，我们首先分析了阿里巴巴大规模协同部署微服务集群的特点，并进一步探讨了为什么选择每指令周期数（CPI）作为大规模生产集群中干扰测量的指标，以及如何通过多维度指标实现CPI的准确预测。基于CPI干扰预测和分析，我们还提出了C-Koordinator平台的设计，这是一个在阿里巴巴集群中使用的开源解决方案，集成了协同部署和干扰缓解策略。干扰预测模型始终保持90.3%以上的准确率，实现了对操作环境中干扰的精确预测和快速缓解。结果显示，与现有技术相比，在各种系统负载下，应用程序的延迟在所有百分位数（P50、P90、P99）响应时间（RT）上均得到降低和稳定，改进幅度从16.7%到36.1%不等。这些结果表明该系统能够在协同部署环境中保持流畅的应用程序性能。", "summary": "本文提出C-Koordinator，一个针对大规模协同部署微服务集群的干扰感知管理平台。该平台首先分析了阿里巴巴集群的特性，并确立每指令周期数（CPI）作为干扰测量指标，通过多维度指标实现准确预测。C-Koordinator集成了协同部署和干扰缓解策略，其预测模型准确率超过90.3%。实验结果表明，该系统能有效降低并稳定应用延迟，在不同负载下响应时间提升16.7%至36.1%，显著优于现有技术，确保了协同部署环境下的应用性能流畅性。", "keywords": "微服务, 协同部署, 干扰管理, C-Koordinator, CPI", "comments": "该论文提出了一种实用的解决方案，通过创新的使用CPI作为干扰度量，并结合多维度预测模型，有效解决了大规模微服务集群中协同部署带来的性能干扰问题。其在真实生产环境（阿里巴巴）中的应用和显著的性能提升（延迟降低和稳定性提高）证明了其重要性和有效性。作为开源解决方案，C-Koordinator也具有较高的推广和应用价值。"}}
{"id": "2507.17846", "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy", "authors": ["Alison Bartsch", "Arvind Car", "Amir Barati Farimani"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17846v1", "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17846v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "PinchBot：基于引导扩散策略的长周期可变形操作", "tldr": "PinchBot是一个使用引导扩散策略的机器人系统，能够通过捏合动作成功完成简单的陶艺目标，解决了长周期可变形操作的挑战。", "motivation": "陶艺创作是一项复杂的艺术形式，需要灵巧、精确和精细的动作来将粘土块缓慢塑造成有意义的3D目标形状。本研究旨在创建一个机器人系统，仅通过捏合动作即可完成简单的陶艺目标，以探索高度多模态和长周期可变形操作任务的挑战。", "method": "本文提出了PinchBot，一个目标条件扩散策略模型。该模型结合了预训练的3D点云嵌入、任务进度预测和碰撞约束动作投影。", "result": "PinchBot能够成功创建各种简单的陶艺目标。", "conclusion": "Not mentioned in abstract", "translation": "陶艺创作是一种复杂的艺术形式，需要灵巧、精确和精细的动作，才能将一块粘土缓慢塑造成有意义且通常有用的3D目标形状。在这项工作中，我们旨在创建一个机器人系统，仅通过捏合动作即可完成简单的陶艺目标。这项捏合陶艺任务使我们能够探索高度多模态和长周期可变形操作任务的挑战。为此，我们提出了PinchBot，一个目标条件扩散策略模型，当与预训练的3D点云嵌入、任务进度预测和碰撞约束动作投影相结合时，能够成功创建各种简单的陶艺目标。有关实验视频和演示数据集的访问，请访问我们的项目网站：https://sites.google.com/andrew.cmu.edu/pinchbot/home。", "summary": "PinchBot是一个机器人系统，旨在通过捏合动作实现长周期可变形操作，特别是简单的陶艺创作。它采用了一个目标条件扩散策略模型，并结合了3D点云嵌入、任务进度预测和碰撞约束动作投影，成功地完成了多种陶艺目标。", "keywords": "PinchBot, 可变形操作, 扩散策略, 陶艺, 机器人", "comments": "PinchBot的创新之处在于将引导扩散策略应用于长周期可变形操作，特别是在复杂的陶艺任务中。它通过结合多种技术（如3D点云嵌入和碰撞约束）来处理高难度和多模态的变形任务，为机器人处理软体材料提供了新的思路。"}}
{"id": "2507.18462", "title": "A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots", "authors": ["Alghalya Al-Hajri", "Ejmen Al-Ubejdij", "Aiman Erbad", "Ali Safa"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18462v1", "summary": "In recent years, Compressed Sensing (CS) has gained significant interest as a\ntechnique for acquiring high-resolution sensory data using fewer measurements\nthan traditional Nyquist sampling requires. At the same time, autonomous\nrobotic platforms such as drones and rovers have become increasingly popular\ntools for remote sensing and environmental monitoring tasks, including\nmeasurements of temperature, humidity, and air quality. Within this context,\nthis paper presents, to the best of our knowledge, the first investigation into\nhow the structure of CS measurement matrices can be exploited to design\noptimized sampling trajectories for robotic environmental data collection. We\npropose a novel Monte Carlo optimization framework that generates measurement\nmatrices designed to minimize both the robot's traversal path length and the\nsignal reconstruction error within the CS framework. Central to our approach is\nthe application of Dictionary Learning (DL) to obtain a data-driven sparsifying\ntransform, which enhances reconstruction accuracy while further reducing the\nnumber of samples that the robot needs to collect. We demonstrate the\neffectiveness of our method through experiments reconstructing $NO_2$ pollution\nmaps over the Gulf region. The results indicate that our approach can reduce\nrobot travel distance to less than $10\\%$ of a full-coverage path, while\nimproving reconstruction accuracy by over a factor of five compared to\ntraditional CS methods based on DCT and polynomial dictionaries, as well as by\na factor of two compared to previously-proposed Informative Path Planning (IPP)\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18462v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "遥感机器人高效路径规划的蒙特卡洛压缩感知与字典学习新方法", "tldr": "本文提出一种新的蒙特卡洛优化框架，结合字典学习，利用压缩感知测量矩阵为遥感机器人设计优化采样路径，显著减少路径长度并提高数据重建精度。", "motivation": "远程传感机器人需要高效采集高分辨率环境数据，而传统方法需要大量测量。本文旨在探索如何利用压缩感知测量矩阵的结构来设计优化的机器人采样轨迹，以最小化路径长度和信号重建误差。", "method": "本文提出了一种新颖的蒙特卡洛优化框架，用于生成测量矩阵，旨在同时最小化机器人的遍历路径长度和压缩感知框架内的信号重建误差。该方法核心是应用字典学习（DL）来获得数据驱动的稀疏变换，以提高重建精度并进一步减少机器人需要收集的样本数量。", "result": "实验表明，该方法可以将机器人行程距离减少到全覆盖路径的不到10%，同时将重建精度提高：与基于DCT和多项式字典的传统压缩感知方法相比，提高超过五倍；与先前提出的信息路径规划（IPP）方法相比，提高两倍。", "conclusion": "本文提出的蒙特卡洛优化框架结合字典学习，成功地利用压缩感知测量矩阵的结构，为遥感机器人设计出高效的采样路径，显著减少了机器人行程并大幅提高了环境数据重建精度。", "translation": "近年来，压缩感知（CS）作为一种使用比传统奈奎斯特采样更少测量即可获取高分辨率传感数据的技术，引起了广泛关注。与此同时，无人机和漫游车等自主机器人平台已成为遥感和环境监测任务（包括温度、湿度和空气质量测量）日益流行的工具。在此背景下，据我们所知，本文首次研究了如何利用CS测量矩阵的结构来设计优化采样轨迹，以用于机器人环境数据采集。我们提出了一种新颖的蒙特卡洛优化框架，该框架生成的测量矩阵旨在最小化机器人的遍历路径长度和CS框架内的信号重建误差。我们方法的核心是应用字典学习（DL）来获得数据驱动的稀疏变换，这提高了重建精度，同时进一步减少了机器人需要收集的样本数量。我们通过重建海湾地区的$NO_2$污染地图的实验证明了我们方法的有效性。结果表明，我们的方法可以将机器人行程距离减少到全覆盖路径的不到10%，同时与基于DCT和多项式字典的传统CS方法相比，重建精度提高超过五倍，与先前提出的信息路径规划（IPP）方法相比，提高两倍。", "summary": "本文提出了一种新颖的蒙特卡洛优化框架，结合字典学习，用于遥感机器人高效路径规划。该方法利用压缩感知测量矩阵的结构，旨在同时最小化机器人路径长度和数据重建误差。通过字典学习获得数据驱动的稀疏变换，进一步提高了重建精度并减少了所需样本数。实验证明，该方法能将机器人行程距离显著减少至全覆盖路径的不到10%，并将重建精度较传统CS和IPP方法分别提高五倍和两倍以上。", "keywords": "压缩感知, 字典学习, 路径规划, 遥感机器人, 蒙特卡洛优化", "comments": "这项研究的创新之处在于首次将压缩感知测量矩阵的结构与路径规划相结合，并通过蒙特卡洛优化和字典学习来设计高效的采样轨迹。其重要性在于显著提高了遥感机器人数据采集的效率和精度，对于环境监测等应用具有实际价值。该方法在减少机器人能耗和提高数据质量方面表现出色。"}}
{"id": "2507.18067", "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents", "authors": ["Abdessamad El-Kabid", "Loubna Benabbou", "Redouane Lguensat", "Alex Hernández-García"], "categories": ["cs.LG", "cs.CE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Workshop @ ICML2025", "url": "http://arxiv.org/abs/2507.18067v1", "summary": "Accurate modeling of physical systems governed by partial differential\nequations is a central challenge in scientific computing. In oceanography,\nhigh-resolution current data are critical for coastal management, environmental\nmonitoring, and maritime safety. However, available satellite products, such as\nCopernicus data for sea water velocity at ~0.08 degrees spatial resolution and\nglobal ocean models, often lack the spatial granularity required for detailed\nlocal analyses. In this work, we (a) introduce a supervised deep learning\nframework based on neural operators for solving PDEs and providing arbitrary\nresolution solutions, and (b) propose downscaling models with an application to\nCopernicus ocean current data. Additionally, our method can model surrogate\nPDEs and predict solutions at arbitrary resolution, regardless of the input\nresolution. We evaluated our model on real-world Copernicus ocean current data\nand synthetic Navier-Stokes simulation datasets.", "comment": "Workshop @ ICML2025", "pdf_url": "http://arxiv.org/pdf/2507.18067v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多尺度神经PDE代理用于预测和降尺度：在洋流中的应用", "tldr": "本文提出了一种基于神经算子的深度学习框架，用于解决PDE并对洋流数据进行降尺度处理，以提供任意分辨率的预测。", "motivation": "物理系统（特别是洋流）的偏微分方程（PDE）建模具有挑战性，而现有卫星产品（如哥白尼数据）和全球海洋模型缺乏进行详细局部分析所需的高空间分辨率数据。", "method": "引入了一个基于神经算子的监督深度学习框架，用于解决PDE并提供任意分辨率的解决方案。同时，提出了将该框架应用于哥白尼洋流数据的降尺度模型。该方法能够建模代理PDE并预测任意分辨率的解，不受输入分辨率限制。", "result": "模型在真实的哥白尼洋流数据和合成的Navier-Stokes模拟数据集上进行了评估。", "conclusion": "Not mentioned in abstract", "translation": "物理系统由偏微分方程（PDEs）控制的精确建模是科学计算中的核心挑战。在海洋学中，高分辨率的洋流数据对于海岸管理、环境监测和海上安全至关重要。然而，现有的卫星产品，例如用于0.08度空间分辨率海流速度的哥白尼数据和全球海洋模型，通常缺乏详细局部分析所需的空间粒度。在这项工作中，我们（a）引入了一个基于神经算子的监督深度学习框架，用于解决PDE并提供任意分辨率的解决方案，以及（b）提出了应用于哥白尼洋流数据的降尺度模型。此外，我们的方法可以建模代理PDE并预测任意分辨率的解决方案，无论输入分辨率如何。我们在真实的哥白尼洋流数据和合成的Navier-Stokes模拟数据集上评估了我们的模型。", "summary": "本文提出了一种基于神经算子的多尺度深度学习框架，旨在解决由偏微分方程控制的物理系统（特别是洋流）的建模和数据降尺度问题。针对现有海洋数据分辨率不足的挑战，该框架能够生成任意分辨率的解决方案，并可作为代理PDE进行预测。研究通过在真实的哥白尼洋流数据和合成的Navier-Stokes数据上进行评估，验证了其有效性。", "keywords": "神经算子, 偏微分方程, 降尺度, 洋流, 深度学习", "comments": "这篇论文通过结合深度学习中的神经算子和多尺度方法，创新性地解决了物理系统PDE建模和高分辨率数据获取的难题，特别是在海洋学领域。其能够从低分辨率输入中生成任意分辨率预测的能力，对于弥补现有卫星数据空间粒度不足具有重要意义，有望在海岸管理和环境监测等实际应用中发挥巨大作用。"}}
{"id": "2507.17911", "title": "Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency", "authors": ["Dou Hoon Kwark", "Shirui Luo", "Xiyue Zhu", "Yudu Li", "Zhi-Pei Liang", "Volodymyr Kindratenko"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures", "url": "http://arxiv.org/abs/2507.17911v1", "summary": "Pseudo-healthy image inpainting is an essential preprocessing step for\nanalyzing pathological brain MRI scans. Most current inpainting methods favor\nslice-wise 2D models for their high in-plane fidelity, but their independence\nacross slices produces discontinuities in the volume. Fully 3D models alleviate\nthis issue, but their high model capacity demands extensive training data for\nreliable, high-fidelity synthesis -- often impractical in medical settings. We\naddress these limitations with a hierarchical diffusion framework by replacing\ndirect 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial\ndiffusion model first yields a coarse, globally consistent inpainting; a\ncoronal diffusion model then refines anatomical details. By combining\nperpendicular spatial views with adaptive resampling, our method balances data\nefficiency and volumetric consistency. Our experiments show our approach\noutperforms state-of-the-art baselines in both realism and volumetric\nconsistency, making it a promising solution for pseudo-healthy image\ninpainting. Code is available at\nhttps://github.com/dou0000/3dMRI-Consistent-Inpaint.", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.17911v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于伪健康脑部MRI修复的分层扩散框架，具有增强的3D一致性", "tldr": "该研究提出了一种分层扩散框架，通过结合两个垂直的2D扩散模型，在数据效率和体积一致性之间取得平衡，以实现伪健康脑部MRI修复。", "motivation": "当前的伪健康脑部MRI修复方法存在局限性：2D模型虽然平面内保真度高，但在切片之间产生不连续性；3D模型虽然能缓解不连续性，但需要大量训练数据，这在医疗环境中通常不切实际。", "method": "本研究提出了一种分层扩散框架，用两个垂直的粗到细2D阶段代替直接的3D建模。首先，轴向扩散模型生成一个粗略的、全局一致的修复结果；然后，冠状扩散模型细化解剖细节。通过结合垂直空间视图和自适应重采样来平衡数据效率和体积一致性。", "result": "实验表明，该方法在真实性和体积一致性方面均优于现有先进基线。", "conclusion": "该分层扩散框架在伪健康图像修复方面提供了一个有前景的解决方案，因为它在数据效率和体积一致性之间取得了平衡，并超越了现有技术。", "translation": "伪健康图像修复是分析病理脑部MRI扫描的重要预处理步骤。大多数当前的修复方法倾向于使用切片式2D模型，因为它们具有高平面内保真度，但其切片间的独立性会在体积中产生不连续性。完全3D模型缓解了这个问题，但其高模型容量需要大量的训练数据才能实现可靠、高保真度的合成——这在医疗环境中通常不切实际。我们通过用两个垂直的粗到细2D阶段代替直接的3D建模，来解决这些局限性，提出了一个分层扩散框架。轴向扩散模型首先产生一个粗略的、全局一致的修复；然后，冠状扩散模型细化解剖细节。通过结合垂直空间视图和自适应重采样，我们的方法平衡了数据效率和体积一致性。我们的实验表明，我们的方法在真实性和体积一致性方面均优于现有最先进的基线，使其成为伪健康图像修复的一个有前景的解决方案。代码可在https://github.com/dou0000/3dMRI-Consistent-Inpaint 获取。", "summary": "该论文提出了一种新的分层扩散框架，用于伪健康脑部MRI图像修复。针对现有2D模型缺乏3D一致性和3D模型对数据量要求高的问题，作者设计了一个包含两个垂直2D扩散阶段的框架：首先由轴向模型进行粗略修复以确保全局一致性，然后由冠状模型进行细节细化。这种方法通过结合不同空间视图和自适应重采样，有效平衡了数据效率和体积一致性，并在实验中展现出优于现有方法的真实性和一致性。", "keywords": "脑部MRI, 图像修复, 扩散模型, 3D一致性, 分层框架", "comments": "该论文的创新点在于提出了一个分层扩散框架，巧妙地结合了2D模型的效率和3D模型的一致性优势，解决了医疗图像处理中数据量有限的挑战。通过将复杂的3D修复分解为两个垂直的2D阶段，它在保持细节的同时，显著提升了体积一致性，这对于后续的病理分析至关重要。该方法在医疗影像领域具有重要的应用潜力。"}}
{"id": "2507.18365", "title": "RecPS: Privacy Risk Scoring for Recommender Systems", "authors": ["Jiajie He", "Yuechun Gu", "Keke Chen"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18365v1", "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning. Our code is\navailable at https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18365v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "RecPS：推荐系统隐私风险评分", "tldr": "RecPS提出了一种基于成员推理攻击（MIA）的隐私风险评分方法，用于量化推荐系统在交互和用户层面的隐私风险，以支持隐私感知型模型开发和遗忘。", "motivation": "推荐系统依赖高度敏感的用户交互数据进行训练，但现实世界的模型开发仍依赖于最小的隐私保护（如受控访问）。用户缺乏了解哪些交互更为敏感的方法，因此量化推荐系统训练数据的隐私风险对于实现隐私感知型推荐系统模型开发和部署至关重要。", "method": "本文提出了一种基于成员推理攻击（MIA）的隐私评分方法RecPS，用于衡量交互和用户层面的隐私风险。RecPS的交互层面评分定义受差分隐私启发并从中推导，然后扩展到用户层面评分方法。一个关键组件是交互层面的MIA方法RecLiRA，它能提供高质量的成员估计。", "result": "我们在知名的基准数据集和推荐系统模型上进行了大量实验，结果表明RecPS评分在风险评估和推荐系统模型遗忘方面具有独特的功能和优势。", "conclusion": "RecPS提供了一种量化推荐系统训练数据隐私风险的方法，是实现隐私感知型推荐系统模型开发和部署的关键一步，并支持模型遗忘。", "translation": "推荐系统（RecSys）已成为许多网络应用不可或缺的组成部分。系统的核心是使用高度敏感的用户-物品交互数据训练的推荐模型。尽管隐私增强技术在研究社区中得到积极研究，但现实世界的模型开发仍然依赖于最小的隐私保护，例如通过受控访问。此类系统的用户应该有权选择不共享高度敏感的交互。然而，目前没有方法允许用户知道哪些交互比其他交互更敏感。因此，量化推荐系统训练数据的隐私风险是实现隐私感知型推荐系统模型开发和部署的关键一步。我们提出了一种基于成员推理攻击（MIA）的隐私评分方法RecPS，用于衡量交互和用户层面的隐私风险。RecPS的交互层面评分定义受差分隐私启发并从中推导，然后扩展到用户层面评分方法。一个关键组件是交互层面的MIA方法RecLiRA，它能提供高质量的成员估计。我们已经在知名基准数据集和推荐系统模型上进行了大量实验，以展示RecPS评分在风险评估和推荐系统模型遗忘方面的独特功能和优势。我们的代码可在https://anonymous.4open.science/r/RsLiRA-4BD3/readme.md获取。", "summary": "RecPS是一种基于成员推理攻击（MIA）的隐私评分方法，旨在量化推荐系统训练数据在交互和用户层面的隐私风险。该方法通过从差分隐私推导出的交互层面评分，并扩展到用户层面，同时引入了RecLiRA作为关键组件，以提供高质量的成员估计。实验证明，RecPS在风险评估和推荐系统模型遗忘方面具有显著优势，有助于实现隐私感知型的推荐系统开发和部署。", "keywords": "隐私风险, 推荐系统, 成员推理攻击, 差分隐私, 模型遗忘", "comments": "本文的创新之处在于提出了一种可量化推荐系统隐私风险的方法（RecPS），并将其应用于风险评估和模型遗忘，这对于弥补理论隐私增强技术与实际模型开发之间的差距具有重要意义。通过量化隐私风险，它为开发者提供了更清晰的指导，以构建更注重隐私的推荐系统。"}}
{"id": "2406.17813", "title": "Unsupervised Concept Drift Detection from Deep Learning Representations in Real-time", "authors": ["Salvatore Greco", "Bartolomeo Vacchetti", "Daniele Apiletti", "Tania Cerquitelli"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Transactions on Knowledge and Data Engineering (TKDE)", "url": "http://arxiv.org/abs/2406.17813v2", "summary": "Concept drift is the phenomenon in which the underlying data distributions\nand statistical properties of a target domain change over time, leading to a\ndegradation in model performance. Consequently, production models require\ncontinuous drift detection monitoring. Most drift detection methods to date are\nsupervised, relying on ground-truth labels. However, they are inapplicable in\nmany real-world scenarios, as true labels are often unavailable. Although\nrecent efforts have proposed unsupervised drift detectors, many lack the\naccuracy required for reliable detection or are too computationally intensive\nfor real-time use in high-dimensional, large-scale production environments.\nMoreover, they often fail to characterize or explain drift effectively.\n  To address these limitations, we propose \\textsc{DriftLens}, an unsupervised\nframework for real-time concept drift detection and characterization. Designed\nfor deep learning classifiers handling unstructured data, \\textsc{DriftLens}\nleverages distribution distances in deep learning representations to enable\nefficient and accurate detection. Additionally, it characterizes drift by\nanalyzing and explaining its impact on each label. Our evaluation across\nclassifiers and data-types demonstrates that \\textsc{DriftLens} (i) outperforms\nprevious methods in detecting drift in 15/17 use cases; (ii) runs at least 5\ntimes faster; (iii) produces drift curves that align closely with actual drift\n(correlation $\\geq\\!0.85$); (iv) effectively identifies representative drift\nsamples as explanations.", "comment": "Accepted at IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "pdf_url": "http://arxiv.org/pdf/2406.17813v2", "cate": "cs.LG", "date": "2024-06-24", "updated": "2025-07-24", "AI": {"title_translation": "从深度学习表示中进行实时无监督概念漂移检测", "tldr": "本文提出了DriftLens，一个无监督、实时的概念漂移检测框架，专门用于深度学习分类器，解决了现有方法对标签的依赖、准确性不足和计算成本高的问题，并在准确性和速度上显著优于现有方法。", "motivation": "概念漂移导致模型性能下降，现有多数有监督漂移检测方法依赖真实标签，但在实际场景中常不可用。已有的无监督方法在准确性、实时性（计算成本）或漂移解释性方面存在不足。", "method": "本文提出了名为DriftLens的无监督框架，专为处理非结构化数据的深度学习分类器设计。它利用深度学习表示中的分布距离进行高效准确的漂移检测，并通过分析和解释漂移对每个标签的影响来表征漂移。", "result": "DriftLens在17个用例中的15个中，在检测漂移方面优于现有方法；运行速度至少快5倍；生成的漂移曲线与实际漂移高度吻合（相关性≥0.85）；能有效识别代表性漂移样本作为解释。", "conclusion": "DriftLens是一个有效、高效且可解释的无监督概念漂移检测框架，适用于深度学习模型，成功解决了现有方法的关键局限性。", "translation": "概念漂移是指目标领域的基础数据分布和统计特性随时间变化，导致模型性能下降的现象。因此，生产模型需要持续的概念漂移检测监控。迄今为止，大多数漂移检测方法都是有监督的，依赖于真实标签。然而，它们在许多实际场景中不适用，因为真实标签通常不可用。尽管最近的努力提出了无监督漂移检测器，但许多缺乏可靠检测所需的准确性，或者对于在高维、大规模生产环境中实时使用来说计算成本过高。此外，它们通常无法有效地表征或解释漂移。\n为了解决这些限制，我们提出了\\textsc{DriftLens}，一个用于实时概念漂移检测和表征的无监督框架。专为处理非结构化数据的深度学习分类器设计，\\textsc{DriftLens}利用深度学习表示中的分布距离来实现高效准确的检测。此外，它通过分析和解释漂移对每个标签的影响来表征漂移。我们对不同分类器和数据类型的评估表明，\\textsc{DriftLens} (i) 在17个用例中的15个中检测漂移方面优于现有方法；(ii) 运行速度至少快5倍；(iii) 生成的漂移曲线与实际漂移高度吻合（相关性$\\geq\\!0.85$）；(iv) 有效地识别代表性漂移样本作为解释。", "summary": "本文提出了DriftLens，一个针对深度学习分类器的无监督、实时概念漂移检测和表征框架。它通过利用深度学习表示中的分布距离来高效准确地检测漂移，并能通过分析漂移对每个标签的影响来解释漂移。DriftLens解决了现有方法对标签的依赖、准确性不足和计算成本高的问题。实验证明，DriftLens在漂移检测性能上优于现有方法，运行速度更快，并能提供高度相关的漂移曲线和可解释的漂移样本。", "keywords": "无监督概念漂移, 深度学习表示, 实时检测, DriftLens, 分布距离", "comments": "DriftLens的创新之处在于它提供了一个无监督、实时、准确且可解释的概念漂移检测框架，专为深度学习表示设计。这对于真实世界中缺乏真实标签且数据维度高的大规模生产环境至关重要。其能够表征和解释漂移的能力是一个显著的进步，增强了漂移检测的实用性。"}}
{"id": "2507.18037", "title": "Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping", "authors": ["Sivana Hamer", "Jacob Bowen", "Md Nazmul Haque", "Chris Madden", "Laurie Williams"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Mapping generated from: arXiv:2503.12192", "url": "http://arxiv.org/abs/2507.18037v1", "summary": "The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)\nAttack Technique to Proactive Software Supply Chain Risk Management Framework\n(P-SSCRM) Task mapping described in this document helps software organizations\nto determine how different tasks mitigate the attack techniques of software\nsupply chain attacks. The mapping was created through four independent\nstrategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to\none or more tasks from the 10 frameworks, the mapping we provide is also a\nmapping between MITRE ATT&CK and other prominent government and industry\nframeworks.", "comment": "Mapping generated from: arXiv:2503.12192", "pdf_url": "http://arxiv.org/pdf/2507.18037v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "您的ATs到Ts：MITRE ATT&CK攻击技术到P-SSCRM任务映射", "tldr": "本文描述了MITRE ATT&CK攻击技术到P-SSCRM任务的映射，旨在帮助软件组织了解如何通过不同任务来缓解软件供应链攻击。", "motivation": "帮助软件组织确定不同的任务如何缓解软件供应链攻击的技术。", "method": "通过四种独立的策略创建，以找到达成一致的映射。", "result": "创建了一个MITRE ATT&CK攻击技术到P-SSCRM任务的映射，并且由于每个P-SSCRM任务都映射到来自10个框架的一个或多个任务，因此该映射也连接了MITRE ATT&CK与其他重要的政府和行业框架。", "conclusion": "该映射有助于软件组织理解如何通过P-SSCRM任务来缓解软件供应链攻击技术，并且实现了MITRE ATT&CK与其他主要框架的互联互通。", "translation": "本文描述的MITRE对抗策略、技术和通用知识（MITRE ATT&CK）攻击技术到主动软件供应链风险管理框架（P-SSCRM）任务的映射，旨在帮助软件组织确定不同的任务如何缓解软件供应链攻击的技术。该映射是通过四种独立的策略创建的，以找到达成一致的映射。由于每个P-SSCRM任务都映射到来自10个框架的一个或多个任务，因此我们提供的映射也是MITRE ATT&CK与其他重要的政府和行业框架之间的映射。", "summary": "本文介绍了一个将MITRE ATT&CK攻击技术映射到主动软件供应链风险管理框架（P-SSCRM）任务的方案。该映射通过四种独立策略构建，旨在帮助软件组织识别和缓解软件供应链攻击。此外，由于P-SSCRM任务已与多个现有框架关联，此映射也间接实现了MITRE ATT&CK与这些主流政府及行业框架的连接。", "keywords": "MITRE ATT&CK, 软件供应链攻击, P-SSCRM, 风险管理, 映射", "comments": "这项工作的重要性在于为软件组织提供了一个实用的工具，以利用P-SSCRM来应对日益复杂的软件供应链攻击。其创新之处在于通过多策略方法创建映射，并实现了MITRE ATT&CK与其他重要框架的互操作性，这对于整合安全实践和提升整体防御能力具有重要意义。"}}
{"id": "2507.18290", "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights", "authors": ["Antonino Rotolo", "Beatrice Ferrigno", "Jose Miguel Angel Garcia Godinez", "Claudio Novelli", "Giovanni Sartor"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      24 pages, 1 figure. To be published in: The Philosophical Foundations of Information Technology Law. Oxford University Press, Oxford", "url": "http://arxiv.org/abs/2507.18290v1", "summary": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.", "comment": "24 pages, 1 figure. To be published in: The Philosophical Foundations\n  of Information Technology Law. Oxford University Press, Oxford", "pdf_url": "http://arxiv.org/pdf/2507.18290v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "保护基本权利中人工智能风险评估的基础", "tldr": "本文提出了一个用于AI风险定性评估的概念框架，该框架结合了定义平衡和可废止推理，以应对法律合规性和基本权利保护的复杂性，并为未来形式化模型奠定基础。", "motivation": "解决AI风险评估在法律合规性（特别是欧盟AI法案）和基本权利保护方面的复杂性，并为AI风险分析提供哲学基础和逻辑解释。", "method": "引入了一个概念框架，用于AI的定性风险评估。该框架整合了“定义平衡”（Definitional Balancing，采用比例性分析解决权利冲突）和“可废止推理”（Defeasible Reasoning，适应法律决策的动态性）。强调分析AI部署场景，识别潜在法律违规和对基本权利的多层影响，并考虑AI部署场景与基本权利之间互动的基本构建模块。", "result": "提供了一个新的概念框架，用于AI的定性风险评估，该框架整合了定义平衡和可废止推理，为分析AI部署场景对基本权利的影响提供了理论基础。这种分层方法使得对高风险AI系统和通用AI（GPAI）系统的评估模型更具操作性，并强调了GPAI系统的广泛适用性。", "conclusion": "该论文为AI风险评估提供了一个结合定义平衡和可废止推理的哲学和逻辑基础，旨在更有效地评估AI系统对基本权利的影响，并为未来的形式化模型和算法开发奠定了基础，以支持负责任的AI治理。", "translation": "本章介绍了一个人工智能定性风险评估的概念框架，特别是在欧盟人工智能法案的背景下。该框架通过整合定义平衡和可废止推理来解决法律合规性和基本权利保护的复杂性。定义平衡采用比例性分析来解决相互冲突的权利之间的冲突，而可废止推理则适应法律决策的动态性质。我们的方法强调需要对人工智能部署场景进行分析，并识别潜在的法律违规以及对基本权利的多层影响。在此分析的基础上，我们为人工智能风险分析的逻辑解释提供了哲学基础。特别是，我们考虑了概念上理解人工智能部署场景与基本权利之间互动的基本构成要素，将定义平衡和关于权利情境性促进或贬低的论证纳入可废止推理中。这种分层方法允许对高风险人工智能系统和通用人工智能（GPAI）系统进行更具操作性的评估模型，强调了后者的更广泛适用性。未来的工作旨在开发形式化模型和有效算法，以增强人工智能风险评估，将理论见解与实际应用相结合，以支持负责任的人工智能治理。", "summary": "本文提出了一个用于人工智能定性风险评估的概念框架，尤其针对欧盟人工智能法案。该框架通过整合定义平衡（解决权利冲突的比例性分析）和可废止推理（适应法律决策动态性）来处理法律合规性和基本权利保护的复杂性。研究强调分析AI部署场景及其对基本权利的潜在影响，并为AI风险分析提供了哲学和逻辑基础。这种分层方法有助于评估高风险AI系统和通用AI系统，并为未来开发形式化模型和算法以支持负责任的AI治理奠定基础。", "keywords": "人工智能风险评估, 基本权利, 定义平衡, 可废止推理, 欧盟人工智能法案", "comments": "本文的创新之处在于提出了一个结合定义平衡和可废止推理的AI风险评估概念框架，这对于理解和管理AI对基本权利的影响具有重要意义，尤其是在欧盟AI法案的背景下。该框架为复杂的法律和伦理问题提供了逻辑和哲学基础，并强调了分析AI部署场景的重要性。其分层方法使得评估更具操作性，并为未来的形式化研究指明了方向，有助于弥合理论与实践之间的鸿沟。"}}
{"id": "2507.18572", "title": "PosterMate: Audience-driven Collaborative Persona Agents for Poster Design", "authors": ["Donghoon Shin", "Daniel Lee", "Gary Hsieh", "Gromit Yeuk-Yin Chan"], "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18572v1", "summary": "Poster designing can benefit from synchronous feedback from target audiences.\nHowever, gathering audiences with diverse perspectives and reconciling them on\ndesign edits can be challenging. Recent generative AI models present\nopportunities to simulate human-like interactions, but it is unclear how they\nmay be used for feedback processes in design. We introduce PosterMate, a poster\ndesign assistant that facilitates collaboration by creating audience-driven\npersona agents constructed from marketing documents. PosterMate gathers\nfeedback from each persona agent regarding poster components, and stimulates\ndiscussion with the help of a moderator to reach a conclusion. These\nagreed-upon edits can then be directly integrated into the poster design.\nThrough our user study (N=12), we identified the potential of PosterMate to\ncapture overlooked viewpoints, while serving as an effective prototyping tool.\nAdditionally, our controlled online evaluation (N=100) revealed that the\nfeedback from an individual persona agent is appropriate given its persona\nidentity, and the discussion effectively synthesizes the different persona\nagents' perspectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18572v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PosterMate：受众驱动的协作式角色代理用于海报设计", "tldr": "PosterMate是一个海报设计助手，它利用基于营销文档构建的受众驱动的角色代理来收集反馈并促进讨论，以克服传统上难以从多样化受众那里获得同步反馈的挑战。", "motivation": "海报设计需要目标受众的同步反馈，但召集具有不同视角的受众并协调他们的设计修改具有挑战性。尽管最近的生成式AI模型为模拟人机交互提供了机会，但它们如何用于设计反馈过程尚不清楚。", "method": "PosterMate通过创建基于营销文档构建的受众驱动的角色代理来促进协作。它从每个角色代理收集关于海报组件的反馈，并在主持人的帮助下激发讨论以达成结论。这些达成一致的修改可以直接集成到海报设计中。", "result": "通过用户研究（N=12），我们发现PosterMate有潜力捕捉被忽视的观点，并可作为有效的原型设计工具。此外，受控在线评估（N=100）表明，个体角色代理的反馈与其角色身份相符，且讨论有效地综合了不同角色代理的观点。", "conclusion": "PosterMate能够有效捕捉多方观点，并作为一个有潜力的原型设计工具，其角色代理的反馈和讨论机制能有效整合不同视角。", "translation": "海报设计可以受益于目标受众的同步反馈。然而，召集具有不同视角的受众并在设计修改上协调他们可能具有挑战性。最近的生成式AI模型为模拟类人交互提供了机会，但尚不清楚它们如何用于设计中的反馈过程。我们介绍了PosterMate，一个海报设计助手，它通过创建基于营销文档构建的受众驱动的角色代理来促进协作。PosterMate收集每个角色代理关于海报组件的反馈，并在主持人的帮助下激发讨论以达成结论。这些达成一致的修改可以直接集成到海报设计中。通过我们的用户研究（N=12），我们发现了PosterMate捕捉被忽视观点的潜力，同时它也作为一个有效的原型工具。此外，我们的受控在线评估（N=100）显示，个体角色代理的反馈与其角色身份相符，并且讨论有效地综合了不同角色代理的观点。", "summary": "PosterMate是一个创新的海报设计辅助工具，旨在解决传统海报设计中难以获取多样化受众反馈的问题。它通过创建基于营销文档的受众驱动角色代理来模拟目标受众，并利用主持人引导这些代理进行讨论和提供反馈，最终将达成共识的设计修改直接整合到海报中。用户研究和在线评估证实了PosterMate在捕捉不同观点和有效整合多方意见方面的潜力，证明了其作为原型工具的有效性。", "keywords": "海报设计, 协作代理, 角色代理, 生成式AI, 用户反馈", "comments": "这项研究通过引入受众驱动的协作式角色代理，为设计反馈过程提供了一种新颖且实用的方法。它利用了生成式AI模拟人机交互的潜力，有效地解决了传统上获取和协调多样化受众反馈的难题。PosterMate的创新之处在于其能够自动化和结构化反馈收集与整合，并被证明能捕捉被忽视的观点，这对于提高设计效率和质量具有重要意义。"}}
{"id": "2507.18263", "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models", "authors": ["Suhang Wu", "Jialong Tang", "Chengyi Yang", "Pei Zhang", "Baosong Yang", "Junhui Li", "Junfeng Yao", "Min Zhang", "Jinsong Su"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ACL 2025", "url": "http://arxiv.org/abs/2507.18263v1", "summary": "Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.", "comment": "Accepted at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.18263v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "定位与聚焦：增强语音语言模型中的术语翻译", "tldr": "本文提出了一种新颖的“定位与聚焦”方法，通过有效定位语音中的术语并整合多模态翻译知识，显著提升了语音翻译模型中术语的翻译准确率，同时保持了整体翻译性能。", "motivation": "直接语音翻译（ST）中术语的准确翻译仍然是一个巨大挑战。现有方法在利用翻译知识时常受无关噪音干扰，且未能充分利用翻译知识。", "method": "本文提出了一种新颖的“定位与聚焦”方法。该方法首先有效定位语音中包含术语的片段，以构建翻译知识，从而最小化对ST模型的无关信息干扰。随后，它将翻译知识与语音和假设从音频和文本两种模态进行关联，使ST模型在翻译过程中能更好地聚焦于翻译知识。", "result": "实验结果表明，我们的方法能有效定位语音中的术语，并提高了术语翻译的成功率，同时保持了稳健的通用翻译性能。", "conclusion": "本文提出的“定位与聚焦”方法能够有效定位语音中的术语，并通过多模态知识关联显著增强术语翻译的准确性，同时不影响整体翻译表现。", "translation": "直接语音翻译（ST）如今受到越来越多的关注，然而，话语中术语的准确翻译仍然是一个巨大的挑战。在这方面，当前的研究主要集中于将各种翻译知识融入ST模型。然而，这些方法常常受到无关噪音的干扰，并且无法充分利用翻译知识。为了解决这些问题，本文提出了一种新颖的“定位与聚焦”术语翻译方法。它首先有效定位话语中包含术语的语音片段以构建翻译知识，从而最大限度地减少ST模型的无关信息。随后，它将翻译知识与话语和假设从音频和文本两种模态进行关联，使ST模型在翻译过程中能更好地聚焦于翻译知识。在各种数据集上的实验结果表明，我们的方法能有效定位话语中的术语并提高了术语翻译的成功率，同时保持了稳健的通用翻译性能。", "summary": "本文针对直接语音翻译（ST）中术语翻译的准确性挑战，提出了一种名为“定位与聚焦”的新方法。该方法首先识别并提取语音中包含术语的片段，以减少无关信息干扰；随后，它通过音频和文本两种模态，将提取的术语翻译知识与整体语音和翻译假设关联起来，引导ST模型更有效地利用这些知识进行翻译。实验证明，该方法显著提升了术语翻译的成功率，并维持了良好的整体翻译性能。", "keywords": "语音翻译, 术语翻译, 定位与聚焦, 多模态, 语音语言模型", "comments": "该论文提出了一种创新性的“定位与聚焦”方法，有效解决了语音翻译中术语翻译的痛点。其核心创新在于通过显式定位术语片段并利用多模态信息进行知识关联，克服了现有方法受噪音干扰和知识利用不足的问题。这对于提升专业领域语音翻译的准确性和实用性具有重要意义。"}}
{"id": "2507.18583", "title": "DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data", "authors": ["Zhengyun Zhao", "Huaiyuan Ying", "Yue Zhong", "Sheng Yu"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Model and code released upon acceptance", "url": "http://arxiv.org/abs/2507.18583v1", "summary": "Electronic Health Records (EHRs) are pivotal in clinical practices, yet their\nretrieval remains a challenge mainly due to semantic gap issues. Recent\nadvancements in dense retrieval offer promising solutions but existing models,\nboth general-domain and biomedical-domain, fall short due to insufficient\nmedical knowledge or mismatched training corpora. This paper introduces\n\\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for\nEHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV\ndischarge summaries to address the need for extensive medical knowledge and\nlarge-scale training data. The first stage involves medical entity extraction\nand knowledge injection from a biomedical knowledge graph, while the second\nstage employs large language models to generate diverse training data. We train\ntwo variants of \\texttt{DR.EHR}, with 110M and 7B parameters, respectively.\nEvaluated on the CliniQ benchmark, our models significantly outperforms all\nexisting dense retrievers, achieving state-of-the-art results. Detailed\nanalyses confirm our models' superiority across various match and query types,\nparticularly in challenging semantic matches like implication and abbreviation.\nAblation studies validate the effectiveness of each pipeline component, and\nsupplementary experiments on EHR QA datasets demonstrate the models'\ngeneralizability on natural language questions, including complex ones with\nmultiple entities. This work significantly advances EHR retrieval, offering a\nrobust solution for clinical applications.", "comment": "Model and code released upon acceptance", "pdf_url": "http://arxiv.org/pdf/2507.18583v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DR.EHR：用于电子健康记录的密集检索，结合知识注入和合成数据", "tldr": "DR.EHR是一种针对电子健康记录（EHR）检索的密集检索模型，通过两阶段训练（知识注入和合成数据生成）显著提高了检索性能，并在CliniQ基准测试中达到了最先进水平。", "motivation": "电子健康记录（EHR）在临床实践中至关重要，但由于语义鸿沟问题，其检索仍然面临挑战。现有的密集检索模型，无论是通用领域还是生物医学领域，都因医学知识不足或训练语料不匹配而表现不佳。", "method": "本文提出DR.EHR，一系列专门为EHR检索定制的密集检索模型。我们利用MIMIC-IV出院摘要，提出了一个两阶段的训练流程：第一阶段涉及从生物医学知识图中进行医学实体提取和知识注入；第二阶段利用大型语言模型生成多样化的训练数据。我们训练了DR.EHR的两个变体，参数量分别为1.1亿和70亿。", "result": "在CliniQ基准测试中，DR.EHR模型显著优于所有现有密集检索器，取得了最先进的结果。详细分析证实了模型在各种匹配和查询类型上的优越性，特别是在含义和缩写等挑战性语义匹配方面。消融研究验证了每个流程组件的有效性，补充实验表明模型在自然语言问答数据集（包括涉及多个实体的复杂问题）上的泛化能力。", "conclusion": "这项工作显著推动了EHR检索领域的发展，为临床应用提供了一个鲁棒的解决方案。", "translation": "电子健康记录（EHR）在临床实践中至关重要，但由于语义鸿沟问题，其检索仍然面临挑战。近期密集检索的进展提供了有前景的解决方案，但现有模型，无论是通用领域还是生物医学领域，都因医学知识不足或训练语料不匹配而表现不佳。本文介绍了DR.EHR，一系列专门为EHR检索定制的密集检索模型。我们提出了一个两阶段的训练流程，利用MIMIC-IV出院摘要来解决对大量医学知识和大规模训练数据的需求。第一阶段涉及从生物医学知识图中进行医学实体提取和知识注入，而第二阶段则利用大型语言模型生成多样化的训练数据。我们训练了DR.EHR的两个变体，参数量分别为1.1亿和70亿。在CliniQ基准测试中，我们的模型显著优于所有现有密集检索器，取得了最先进的结果。详细分析证实了我们的模型在各种匹配和查询类型上的优越性，特别是在含义和缩写等挑战性语义匹配方面。消融研究验证了每个流程组件的有效性，补充实验表明模型在EHR问答数据集上对自然语言问题（包括涉及多个实体的复杂问题）的泛化能力。这项工作显著推动了EHR检索领域的发展，为临床应用提供了一个鲁棒的解决方案。", "summary": "DR.EHR是一种专为电子健康记录（EHR）检索设计的密集检索模型，旨在解决现有模型因医学知识不足和训练数据不匹配导致的语义鸿沟问题。该模型采用两阶段训练管道，首先从生物医学知识图中注入医学实体知识，然后利用大型语言模型生成多样化的合成训练数据。DR.EHR在CliniQ基准测试中表现出色，显著优于现有模型，达到了最先进水平，并在各种查询类型和复杂自然语言问题上展现出强大的泛化能力。", "keywords": "密集检索, 电子健康记录, 知识注入, 合成数据, 语义鸿沟", "comments": "DR.EHR的创新之处在于其独特的两阶段训练管道，它有效地结合了知识注入和大规模合成数据生成，从而解决了EHR检索中长期存在的语义鸿沟和数据稀缺问题。该方法通过引入特定领域的医学知识和利用大型语言模型生成高质量训练数据，显著提升了模型在复杂医学文本检索中的性能。其在CliniQ基准测试中取得的最先进结果，以及在EHR QA任务中的泛化能力，证明了其在临床应用中的巨大潜力。这项工作为未来EHR检索系统的发展提供了重要的参考。"}}
{"id": "2407.09468", "title": "Beyond Euclid: An Illustrated Guide to Modern Machine Learning with Geometric, Topological, and Algebraic Structures", "authors": ["Mathilde Papillon", "Sophia Sanborn", "Johan Mathe", "Louisa Cornelis", "Abby Bertics", "Domas Buracas", "Hansen J Lillemark", "Christian Shewmake", "Fatih Dinc", "Xavier Pennec", "Nina Miolane"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.09468v2", "summary": "The enduring legacy of Euclidean geometry underpins classical machine\nlearning, which, for decades, has been primarily developed for data lying in\nEuclidean space. Yet, modern machine learning increasingly encounters richly\nstructured data that is inherently nonEuclidean. This data can exhibit\nintricate geometric, topological and algebraic structure: from the geometry of\nthe curvature of space-time, to topologically complex interactions between\nneurons in the brain, to the algebraic transformations describing symmetries of\nphysical systems. Extracting knowledge from such non-Euclidean data\nnecessitates a broader mathematical perspective. Echoing the 19th-century\nrevolutions that gave rise to non-Euclidean geometry, an emerging line of\nresearch is redefining modern machine learning with non-Euclidean structures.\nIts goal: generalizing classical methods to unconventional data types with\ngeometry, topology, and algebra. In this review, we provide an accessible\ngateway to this fast-growing field and propose a graphical taxonomy that\nintegrates recent advances into an intuitive unified framework. We subsequently\nextract insights into current challenges and highlight exciting opportunities\nfor future development in this field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.09468v2", "cate": "cs.LG", "date": "2024-07-12", "updated": "2025-07-24", "AI": {"title_translation": "超越欧几里得：几何、拓扑和代数结构在现代机器学习中的图解指南", "tldr": "经典机器学习基于欧几里得数据，而现代机器学习面临日益复杂的非欧几里得数据。本文作为一篇综述，旨在为这个新兴领域提供一个易于理解的入口，并提出一个图形分类法，整合几何、拓扑和代数结构在现代机器学习中的应用，同时探讨挑战与机遇。", "motivation": "经典机器学习主要针对欧几里得空间中的数据开发，但现代机器学习日益遇到具有复杂几何、拓扑和代数结构的非欧几里得数据。从这类数据中提取知识需要更广阔的数学视角，因此需要发展能处理非欧几里得结构的新方法。", "method": "本文作为一篇综述，旨在为快速发展的非欧几里得机器学习领域提供一个易于理解的入口。它提出了一种图形分类法，将近期进展整合到一个直观的统一框架中。此外，文章还深入分析了当前面临的挑战，并展望了未来的发展机遇。", "result": "本文提供了一个理解非欧几里得机器学习的统一框架（通过图形分类法），整合了该领域的最新进展，并识别了当前面临的挑战和未来的发展机遇。", "conclusion": "本文为处理非欧几里得数据的现代机器学习这一新兴且快速发展的领域提供了基础性的理解和统一的框架，强调了其重要性和未来的发展方向。", "translation": "欧几里得几何的持久遗产是经典机器学习的基础，几十年来，经典机器学习主要为欧几里得空间中的数据而开发。然而，现代机器学习日益遇到本质上非欧几里得的丰富结构化数据。这类数据可以展现出复杂的几何、拓扑和代数结构：从时空曲率的几何形状，到大脑中神经元之间拓扑复杂的相互作用，再到描述物理系统对称性的代数变换。从这类非欧几里得数据中提取知识需要更广阔的数学视角。呼应19世纪非欧几里得几何兴起的革命，一个新兴的研究方向正在用非欧几里得结构重新定义现代机器学习。其目标是：将经典方法推广到具有几何、拓扑和代数结构的非常规数据类型。在本综述中，我们为这个快速发展的领域提供了一个易于理解的入口，并提出一个图形分类法，将近期进展整合到一个直观的统一框架中。随后，我们深入分析了当前的挑战，并强调了该领域未来发展的激动人心的机遇。", "summary": "本综述文章探讨了现代机器学习如何超越经典欧几里得几何的限制，以处理日益复杂的非欧几里得数据，这些数据展现出丰富的几何、拓扑和代数结构。文章旨在为这个新兴领域提供一个易于理解的指南，并提出一个创新的图形分类法，以统一和整合该领域的最新进展。此外，文中还深入分析了当前面临的挑战，并指出了未来发展的潜在机遇。", "keywords": "非欧几里得机器学习, 几何结构, 拓扑结构, 代数结构, 综述", "comments": "这篇综述论文的重要性在于它解决了机器学习领域的一个根本性转变，即超越传统的欧几里得假设来处理复杂的、真实世界的数据结构。其价值在于为这个新兴且快速发展的领域提供了一个结构化的概述和统一的框架（图形分类法），使其更容易被更广泛的受众理解。"}}
{"id": "2507.18059", "title": "Multi-Agent Guided Policy Optimization", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18059v1", "summary": "Due to practical constraints such as partial observability and limited\ncommunication, Centralized Training with Decentralized Execution (CTDE) has\nbecome the dominant paradigm in cooperative Multi-Agent Reinforcement Learning\n(MARL). However, existing CTDE methods often underutilize centralized training\nor lack theoretical guarantees. We propose Multi-Agent Guided Policy\nOptimization (MAGPO), a novel framework that better leverages centralized\ntraining by integrating centralized guidance with decentralized execution.\nMAGPO uses an auto-regressive joint policy for scalable, coordinated\nexploration and explicitly aligns it with decentralized policies to ensure\ndeployability under partial observability. We provide theoretical guarantees of\nmonotonic policy improvement and empirically evaluate MAGPO on 43 tasks across\n6 diverse environments. Results show that MAGPO consistently outperforms strong\nCTDE baselines and matches or surpasses fully centralized approaches, offering\na principled and practical solution for decentralized multi-agent learning. Our\ncode and experimental data can be found in https://github.com/liyheng/MAGPO.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18059v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多智能体引导策略优化", "tldr": "MAGPO是一种新的多智能体强化学习框架，通过将集中式引导与分布式执行相结合，解决了现有CTDE方法对集中式训练利用不足或缺乏理论保证的问题，并在多个任务中表现出色。", "motivation": "现有的集中式训练、分布式执行（CTDE）多智能体强化学习方法在实际约束下（如部分可观测性和有限通信）存在对集中式训练利用不足或缺乏理论保证的问题。", "method": "我们提出了多智能体引导策略优化（MAGPO），一个新颖的框架，通过将集中式引导与分布式执行相结合，更好地利用集中式训练。MAGPO使用自回归联合策略进行可扩展的协调探索，并明确将其与分布式策略对齐，以确保在部分可观测性下的可部署性。我们提供了单调策略改进的理论保证。", "result": "MAGPO在6个不同环境的43个任务中进行了实证评估。结果表明，MAGPO始终优于强大的CTDE基线，并与完全集中式方法相匹配或超越。", "conclusion": "MAGPO为分布式多智能体学习提供了一个原则性且实用的解决方案，通过更好地利用集中式训练并提供理论保证，解决了现有CTDE方法的不足。", "translation": "由于部分可观测性和有限通信等实际约束，集中式训练与分布式执行（CTDE）已成为协同多智能体强化学习（MARL）中的主导范式。然而，现有的CTDE方法往往未能充分利用集中式训练或缺乏理论保证。我们提出了多智能体引导策略优化（MAGPO），一个通过将集中式引导与分布式执行相结合，更好地利用集中式训练的新颖框架。MAGPO使用自回归联合策略进行可扩展的协调探索，并明确将其与分布式策略对齐，以确保在部分可观测性下的可部署性。我们提供了单调策略改进的理论保证，并在6个不同环境的43个任务中对MAGPO进行了实证评估。结果表明，MAGPO始终优于强大的CTDE基线，并与完全集中式方法相匹配或超越，为分布式多智能体学习提供了一个原则性且实用的解决方案。我们的代码和实验数据可在https://github.com/liyheng/MAGPO找到。", "summary": "本论文提出了多智能体引导策略优化（MAGPO），一种新的多智能体强化学习（MARL）框架，旨在克服现有集中式训练、分布式执行（CTDE）方法在利用集中式训练和理论保证方面的不足。MAGPO通过结合集中式引导和分布式执行，利用自回归联合策略进行协调探索，并确保与分布式策略的对齐以适应部分可观测性。该方法提供了单调策略改进的理论保证，并在多项任务中经验性地展示了优于现有CTDE基线并媲美或超越完全集中式方法的性能。", "keywords": "多智能体强化学习, 集中式训练, 分布式执行, 策略优化, 理论保证", "comments": "MAGPO的创新之处在于其将集中式引导与分布式执行相结合的框架，有效解决了CTDE范式下集中训练利用不足的问题。其提供理论保证并展示出色的经验性能，使其成为多智能体强化学习领域一个有前景且实用的解决方案。该研究强调了在保持分布式部署能力的同时，最大化集中训练效益的重要性。"}}
{"id": "2505.18444", "title": "On the Structure and Semantics of Identifier Names Containing Closed Syntactic Category Words", "authors": ["Christian D. Newman", "Anthony Peruma", "Eman Abdullah AlOmar", "Mahie Crabbe", "Syreen Banabilah", "Reem S. AlSuhaibani", "Michael J. Decker", "Farhad Akhbardeh", "Marcos Zampieri", "Mohamed Wiem Mkaouer", "Jonathan I. Maletic"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by Empirical Software Engineering (EMSE)", "url": "http://arxiv.org/abs/2505.18444v4", "summary": "Identifier names are crucial components of code, serving as primary clues for\ndevelopers to understand program behavior. This paper investigates the\nlinguistic structure of identifier names by extending the concept of grammar\npatterns, which represent the part-of-speech (PoS) sequences underlying\nidentifier phrases. The specific focus is on closed syntactic categories (e.g.,\nprepositions, conjunctions, determiners), which are rarely studied in software\nengineering despite their central role in general natural language. To study\nthese categories, the Closed Category Identifier Dataset (CCID), a new manually\nannotated dataset of 1,275 identifiers drawn from 30 open-source systems, is\nconstructed and presented. The relationship between closed-category grammar\npatterns and program behavior is then analyzed using grounded-theory-inspired\ncoding, statistical, and pattern analysis. The results reveal recurring\nstructures that developers use to express concepts such as control flow, data\ntransformation, temporal reasoning, and other behavioral roles through naming.\nThis work contributes an empirical foundation for understanding how linguistic\nresources encode behavior in identifier names and supports new directions for\nresearch in naming, program comprehension, and education.", "comment": "Accepted by Empirical Software Engineering (EMSE)", "pdf_url": "http://arxiv.org/pdf/2505.18444v4", "cate": "cs.SE", "date": "2025-05-24", "updated": "2025-07-24", "AI": {"title_translation": "包含封闭句法类别词的标识符名称的结构与语义研究", "tldr": "本研究通过分析包含封闭句法类别词的标识符名称，揭示了开发者如何通过命名来表达程序行为。", "motivation": "标识符名称是理解程序行为的关键组成部分，但软件工程领域对包含封闭句法类别词（如介词、连词、限定词）的标识符名称研究较少，尽管它们在自然语言中扮演核心角色。本研究旨在填补这一空白，深入探究这些词在标识符名称中的语言结构和语义。", "method": "本研究通过扩展语法模式概念（代表标识符短语的词性序列）来调查标识符名称的语言结构。具体方法包括：构建了一个新的、手动标注的“封闭类别标识符数据集”（CCID），该数据集包含从30个开源系统中提取的1,275个标识符；然后，利用扎根理论启发式编码、统计分析和模式分析，分析了封闭类别语法模式与程序行为之间的关系。", "result": "研究结果揭示了开发者在命名中用于表达控制流、数据转换、时间推理及其他行为角色的重复结构。", "conclusion": "这项工作为理解语言资源如何将行为编码到标识符名称中提供了实证基础，并支持了命名、程序理解和教育领域的新研究方向。", "translation": "标识符名称是代码的关键组成部分，是开发者理解程序行为的主要线索。本文通过扩展语法模式概念（代表标识符短语的词性序列），研究了标识符名称的语言结构。具体关注封闭句法类别词（例如介词、连词、限定词），尽管它们在一般自然语言中扮演核心角色，但在软件工程中却鲜有研究。为了研究这些类别，本文构建并提出了一个名为“封闭类别标识符数据集”（CCID）的新数据集，该数据集是手动标注的，包含从30个开源系统中提取的1,275个标识符。随后，利用扎根理论启发式编码、统计和模式分析，对封闭类别语法模式与程序行为之间的关系进行了分析。结果揭示了开发者通过命名来表达控制流、数据转换、时间推理及其他行为角色的重复结构。这项工作为理解语言资源如何将行为编码到标识符名称中提供了实证基础，并支持了命名、程序理解和教育领域的新研究方向。", "summary": "本研究深入探讨了软件标识符名称的语言结构和语义，特别是其中包含的封闭句法类别词（如介词、连词）。为了克服现有研究的不足，作者构建了一个新的大型手动标注数据集CCID。通过对该数据集进行扎根理论启发式编码、统计和模式分析，研究揭示了开发者在命名中用于表达控制流、数据转换和时间推理等程序行为的重复性语言模式。这项工作为理解标识符命名中的语言编码行为提供了实证基础，并为相关领域的研究开辟了新方向。", "keywords": "标识符名称, 封闭句法类别, 语法模式, 程序理解, 命名", "comments": "这项研究的创新之处在于其首次系统性地关注了软件标识符名称中封闭句法类别词的作用，这在软件工程领域是相对未被探索的。通过构建专门的CCID数据集，为后续研究提供了宝贵的资源。研究结果对于提高程序理解、改进命名实践以及软件工程教育都具有重要意义。"}}
{"id": "2507.17796", "title": "CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series", "authors": ["Nicholas A. Pearson", "Francesca Zanello", "Davide Russo", "Luca Bortolussi", "Francesca Cairoli"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for Presentation at Runtime Verification 25", "url": "http://arxiv.org/abs/2507.17796v1", "summary": "We propose a novel framework that harnesses the power of generative\nartificial intelligence and copula-based modeling to address two critical\nchallenges in multivariate time-series analysis: delivering accurate\npredictions and enabling robust anomaly detection. Our method, Copula-based\nConformal Anomaly Identification for Multivariate Time-Series (CoCAI),\nleverages a diffusion-based model to capture complex dependencies within the\ndata, enabling high quality forecasting. The model's outputs are further\ncalibrated using a conformal prediction technique, yielding predictive regions\nwhich are statistically valid, i.e., cover the true target values with a\ndesired confidence level. Starting from these calibrated forecasts, robust\noutlier detection is performed by combining dimensionality reduction techniques\nwith copula-based modeling, providing a statistically grounded anomaly score.\nCoCAI benefits from an offline calibration phase that allows for minimal\noverhead during deployment and delivers actionable results rooted in\nestablished theoretical foundations. Empirical tests conducted on real\noperational data derived from water distribution and sewerage systems confirm\nCoCAI's effectiveness in accurately forecasting target sequences of data and in\nidentifying anomalous segments within them.", "comment": "Accepted for Presentation at Runtime Verification 25", "pdf_url": "http://arxiv.org/pdf/2507.17796v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "CoCAI：基于Copula的多元时间序列共形异常识别", "tldr": "CoCAI是一个结合生成AI和Copula模型的新框架，用于多元时间序列的准确预测和鲁棒异常检测，并已在实际数据中验证有效性。", "motivation": "解决多元时间序列分析中的两个关键挑战：提供准确预测和实现鲁棒异常检测。", "method": "CoCAI利用基于扩散的模型捕获数据中的复杂依赖关系以进行高质量预测。模型输出通过共形预测技术校准，产生统计有效的预测区域。从这些校准后的预测开始，通过结合降维技术和基于Copula的建模来执行鲁棒的异常检测，提供统计学上可靠的异常分数。CoCAI受益于离线校准阶段，部署时开销最小。", "result": "在从供水和污水处理系统获得的真实操作数据上进行的实证测试证实了CoCAI在准确预测目标数据序列和识别其中的异常片段方面的有效性。", "conclusion": "CoCAI是一个有效且理论基础扎实的新框架，能够对多元时间序列进行准确预测和鲁棒异常检测，并在实际应用中表现出色。", "translation": "我们提出了一个新颖的框架，该框架利用生成式人工智能和基于Copula的建模的力量，以解决多元时间序列分析中的两个关键挑战：提供准确的预测和实现鲁棒的异常检测。我们的方法，即基于Copula的多元时间序列共形异常识别（CoCAI），利用基于扩散的模型来捕获数据中的复杂依赖关系，从而实现高质量的预测。模型的输出通过共形预测技术进一步校准，产生统计有效的预测区域，即以所需的置信水平覆盖真实目标值。从这些校准后的预测开始，通过结合降维技术和基于Copula的建模来执行鲁棒的异常检测，提供统计学上可靠的异常分数。CoCAI受益于离线校准阶段，该阶段允许在部署期间的最小开销，并提供植根于既定理论基础的可操作结果。在从供水和污水处理系统获得的真实操作数据上进行的实证测试证实了CoCAI在准确预测目标数据序列和识别其中的异常片段方面的有效性。", "summary": "CoCAI是一个新颖的框架，结合生成式AI（扩散模型）和基于Copula的建模，旨在解决多元时间序列的准确预测和鲁棒异常检测问题。它利用共形预测校准预测区域，并通过结合降维和Copula模型进行异常评分。该方法具有离线校准阶段，降低了部署开销，并在实际水务系统数据上验证了其在预测和异常识别方面的有效性。", "keywords": "多元时间序列, 异常检测, 共形预测, Copula模型, 扩散模型", "comments": "CoCAI的创新之处在于它将扩散模型、共形预测和Copula模型巧妙地结合起来，以解决多元时间序列预测和异常检测的挑战。其离线校准设计降低了部署成本，并强调了方法的理论基础和实际应用价值，特别是在关键基础设施监控方面。"}}
{"id": "2507.18376", "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges", "authors": ["Xing Hua", "Haodong Chen", "Qianqian Duan", "Danfeng Hong", "Ruijiao Li", "Huiliang Shang", "Linghua Jiang", "Haima Yang", "Dawei Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18376v1", "summary": "With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18376v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "智能农业中扩散模型的综合综述：进展、应用和挑战", "tldr": "本文综述了扩散模型在智能农业中的最新进展、应用及其面临的挑战，指出扩散模型在处理农业数据限制和提高模型性能方面的潜力。", "motivation": "随着全球人口增长和耕地资源稀缺，智能农业和精准农业成为未来农业发展的关键方向。人工智能技术，特别是深度学习模型，已广泛应用于农业。扩散模型作为一种新兴的生成模型，在农业图像处理、数据增强和遥感等任务中展现出巨大潜力，且相比GANs具有更好的训练稳定性和生成质量，能有效解决农业数据有限和样本不平衡的挑战。", "method": "本文对扩散模型在农业中的最新应用进展进行了综述，重点关注其在作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理方面的潜力。", "result": "实验结果表明，扩散模型在数据增强、图像生成和去噪方面显著提高了模型精度和鲁棒性，尤其是在复杂环境中表现突出。", "conclusion": "尽管面临计算效率和泛化能力方面的挑战，但随着技术进步，扩散模型有望在智能和精准农业中发挥越来越重要的作用，为全球农业的可持续发展提供实质性支持。", "translation": "随着全球人口增长和耕地资源日益稀缺，智能农业和精准农业已成为未来农业发展的关键方向。人工智能（AI）技术，特别是深度学习模型，已在作物监测和病虫害检测等领域得到广泛应用。作为一种新兴的生成模型，扩散模型在农业图像处理、数据增强和遥感等任务中展现出巨大潜力。与传统的生成对抗网络（GANs）相比，扩散模型具有卓越的训练稳定性和生成质量，有效解决了农业数据有限和图像样本不平衡等挑战。本文综述了扩散模型在农业应用中的最新进展，重点关注其在作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理方面的潜力。实验结果表明，扩散模型在数据增强、图像生成和去噪方面显著提高了模型精度和鲁棒性，尤其是在复杂环境中。尽管面临计算效率和泛化能力方面的挑战，但随着技术进步，扩散模型有望在智能和精准农业中发挥越来越重要的作用，为全球农业的可持续发展提供实质性支持。", "summary": "本文对扩散模型在智能农业领域的应用进行了全面综述。论文指出，面对全球人口增长和耕地稀缺，智能农业至关重要。扩散模型作为一种新兴的生成模型，在农业图像处理、数据增强和遥感等任务中显示出优于GANs的性能，能有效解决数据限制和样本不平衡问题。文章详细回顾了扩散模型在作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理中的应用潜力，并强调其在提高模型精度和鲁棒性方面的优势。尽管存在计算效率和泛化能力挑战，但该综述预测扩散模型将在未来智能农业的可持续发展中扮演关键角色。", "keywords": "扩散模型, 智能农业, 深度学习, 数据增强, 遥感", "comments": "这是一篇综述性论文，全面梳理了扩散模型在智能农业中的应用现状、优势和挑战。其创新性在于系统性地总结了扩散模型如何克服传统方法在农业数据处理上的局限性，并展望了其在未来农业发展中的重要作用。该综述对于研究人员了解该领域的最新进展和识别潜在研究方向具有重要价值。"}}
{"id": "2507.17881", "title": "A Supervised Machine Learning Framework for Multipactor Breakdown Prediction in High-Power Radio Frequency Devices and Accelerator Components: A Case Study in Planar Geometry", "authors": ["Asif Iqbal", "John Verboncoeur", "Peng Zhang"], "categories": ["physics.acc-ph", "cs.LG", "physics.app-ph", "physics.plasm-ph"], "primary_category": "Subjects:       Accelerator Physics (physics.acc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17881v1", "summary": "Multipactor is a nonlinear electron avalanche phenomenon that can severely\nimpair the performance of high-power radio frequency (RF) devices and\naccelerator systems. Accurate prediction of multipactor susceptibility across\ndifferent materials and operational regimes remains a critical yet\ncomputationally intensive challenge in accelerator component design and RF\nengineering. This study presents the first application of supervised machine\nlearning (ML) for predicting multipactor susceptibility in two-surface planar\ngeometries. A simulation-derived dataset spanning six distinct secondary\nelectron yield (SEY) material profiles is used to train regression models -\nincluding Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting\n(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the\ntime-averaged electron growth rate, ${\\delta}_{avg}$. Performance is evaluated\nusing Intersection over Union (IoU), Structural Similarity Index (SSIM), and\nPearson correlation coefficient. Tree-based models consistently outperform MLPs\nin generalizing across disjoint material domains. MLPs trained using a\nscalarized objective function that combines IoU and SSIM during Bayesian\nhyperparameter optimization with 5-fold cross-validation outperform those\ntrained with single-objective loss functions. Principal Component Analysis\nreveals that performance degradation for certain materials stems from disjoint\nfeature-space distributions, underscoring the need for broader dataset\ncoverage. This study demonstrates both the promise and limitations of ML-based\nmultipactor prediction and lays the groundwork for accelerated, data-driven\nmodeling in advanced RF and accelerator system design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17881v1", "cate": "physics.acc-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "高功率射频设备和加速器组件中多重击穿预测的监督机器学习框架：平面几何案例研究", "tldr": "本文首次应用监督机器学习（包括随机森林、极限树、XGBoost和多层感知器）来预测高功率射频设备中多重击穿的易感性，结果显示基于树的模型在泛化能力上优于多层感知器，并强调了更广泛数据集覆盖的重要性。", "motivation": "多重击穿是一种非线性电子雪崩现象，严重损害高功率射频设备和加速器系统的性能。准确预测不同材料和操作条件下多重击穿的易感性至关重要，但计算成本高昂。", "method": "本研究首次将监督机器学习应用于预测双表面平面几何中多重击穿的易感性。使用包含六种不同次级电子产额（SEY）材料剖面的模拟数据集来训练回归模型，包括随机森林（RF）、极限树（ET）、极端梯度提升（XGBoost）和漏斗结构的多层感知器（MLPs），以预测时间平均电子增长率${\\delta}_{avg}$。性能通过交并比（IoU）、结构相似性指数（SSIM）和皮尔逊相关系数进行评估。在使用贝叶斯超参数优化和5折交叉验证时，采用结合IoU和SSIM的标量化目标函数训练的多层感知器优于使用单目标损失函数训练的模型。主成分分析用于揭示某些材料性能下降的原因。", "result": "基于树的模型在跨不相交材料域的泛化能力上始终优于多层感知器。使用结合IoU和SSIM的标量化目标函数进行贝叶斯超参数优化并进行5折交叉验证训练的多层感知器，其性能优于使用单目标损失函数训练的模型。主成分分析显示，某些材料的性能下降源于特征空间分布的不相交性，这强调了需要更广泛的数据集覆盖。", "conclusion": "本研究展示了基于机器学习的多重击穿预测的潜力和局限性，并为先进射频和加速器系统设计中加速数据驱动建模奠定了基础。", "translation": "多重击穿是一种非线性电子雪崩现象，会严重损害高功率射频（RF）设备和加速器系统的性能。准确预测不同材料和操作方案下的多重击穿易感性仍然是加速器组件设计和射频工程中一个关键但计算密集型的挑战。本研究首次将监督机器学习（ML）应用于预测双表面平面几何中的多重击穿易感性。一个包含六种不同次级电子产额（SEY）材料剖面的模拟数据集被用于训练回归模型——包括随机森林（RF）、极限树（ET）、极端梯度提升（XGBoost）和漏斗结构的多层感知器（MLP）——以预测时间平均电子增长率${\\delta}_{avg}$。性能通过交并比（IoU）、结构相似性指数（SSIM）和皮尔逊相关系数进行评估。基于树的模型在泛化跨不相交材料域方面始终优于多层感知器。在使用贝叶斯超参数优化和5折交叉验证时，采用结合IoU和SSIM的标量化目标函数训练的多层感知器优于使用单目标损失函数训练的模型。主成分分析揭示，某些材料的性能下降源于不相交的特征空间分布，这强调了需要更广泛的数据集覆盖。本研究展示了基于机器学习的多重击穿预测的潜力和局限性，并为先进射频和加速器系统设计中加速数据驱动建模奠定了基础。", "summary": "本文首次将监督机器学习应用于高功率射频设备和加速器组件中的多重击穿预测，旨在解决其计算密集型挑战。研究利用一个包含六种不同材料剖面的模拟数据集，训练了随机森林、极限树、XGBoost和多层感知器等回归模型，以预测时间平均电子增长率。结果表明，基于树的模型在跨材料域的泛化能力上优于多层感知器，且通过结合IoU和SSIM的标量化目标函数优化多层感知器能提升性能。研究还通过主成分分析指出，数据集中特征空间分布的不足是某些材料性能下降的原因，强调了未来需要更广泛的数据覆盖，为加速射频和加速器系统设计中的数据驱动建模奠定了基础。", "keywords": "多重击穿, 机器学习, 射频设备, 加速器组件, 击穿预测", "comments": "本文的创新之处在于首次将监督机器学习应用于多重击穿预测，解决了高功率射频设备设计中一个重要的计算密集型挑战。其重要性在于为加速器和射频系统设计提供了新的数据驱动建模方法。研究也明确指出了局限性，即某些材料性能下降源于特征空间分布的不相交性，这表明需要更广泛、更多样化的数据集来提高模型的鲁棒性和泛化能力。"}}
{"id": "2403.18915", "title": "PLOT-TAL: Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization", "authors": ["Edward Fish", "Andrew Gilbert"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCVWS", "url": "http://arxiv.org/abs/2403.18915v2", "summary": "Few-shot temporal action localization (TAL) methods that adapt large models\nvia single-prompt tuning often fail to produce precise temporal boundaries.\nThis stems from the model learning a non-discriminative mean representation of\nan action from sparse data, which compromises generalization. We address this\nby proposing a new paradigm based on multi-prompt ensembles, where a set of\ndiverse, learnable prompts for each action is encouraged to specialize on\ncompositional sub-events. To enforce this specialization, we introduce\nPLOT-TAL, a framework that leverages Optimal Transport (OT) to find a globally\noptimal alignment between the prompt ensemble and the video's temporal\nfeatures. Our method establishes a new state-of-the-art on the challenging\nfew-shot benchmarks of THUMOS'14 and EPIC-Kitchens, without requiring complex\nmeta-learning. The significant performance gains, particularly at high IoU\nthresholds, validate our hypothesis and demonstrate the superiority of learning\ndistributed, compositional representations for precise temporal localization.", "comment": "Accepted to ICCVWS", "pdf_url": "http://arxiv.org/pdf/2403.18915v2", "cate": "cs.CV", "date": "2024-03-27", "updated": "2025-07-24", "AI": {"title_translation": "PLOT-TAL：基于最优传输的少样本时间动作定位提示学习", "tldr": "PLOT-TAL提出了一种基于多提示集成和最优传输的新范式，以解决少样本时间动作定位中边界不精确的问题，并在基准测试中达到了最先进的性能。", "motivation": "现有的少样本时间动作定位（TAL）方法通过单提示调整适应大型模型时，通常无法产生精确的时间边界。这是因为模型从稀疏数据中学习到非判别性的动作平均表示，从而损害了泛化能力。", "method": "提出了一种基于多提示集成的新范式，其中鼓励为每个动作学习一组多样化的可学习提示，使其专门化于组合子事件。引入了PLOT-TAL框架，该框架利用最优传输（OT）在提示集成和视频时间特征之间找到全局最优对齐。", "result": "在THUMOS'14和EPIC-Kitchens这两个具有挑战性的少样本基准测试中，无需复杂的元学习，我们的方法建立了新的最先进水平。显著的性能提升，尤其是在高IoU阈值下。", "conclusion": "该研究验证了其假设，并证明了学习分布式、组合表示对于精确时间定位的优越性。", "translation": "少样本时间动作定位（TAL）方法通过单提示调整适应大型模型时，通常无法产生精确的时间边界。这源于模型从稀疏数据中学习到非判别性的动作平均表示，从而损害了泛化能力。我们通过提出一种基于多提示集成的新范式来解决这个问题，其中鼓励为每个动作学习一组多样化的可学习提示，使其专门化于组合子事件。为了强制执行这种专业化，我们引入了PLOT-TAL，一个利用最优传输（OT）在提示集成和视频的时间特征之间找到全局最优对齐的框架。我们的方法在THUMOS'14和EPIC-Kitchens这两个具有挑战性的少样本基准测试中，无需复杂的元学习，建立了新的最先进水平。显著的性能提升，尤其是在高IoU阈值下，验证了我们的假设，并证明了学习分布式、组合表示对于精确时间定位的优越性。", "summary": "本文提出PLOT-TAL，一种用于少样本时间动作定位的新框架，旨在解决现有方法在精确边界识别方面的不足。通过引入多提示集成范式和利用最优传输（OT）进行提示与视频特征的全局最优对齐，PLOT-TAL鼓励模型学习分布式、组合式的动作表示。实验结果表明，该方法在THUMOS'14和EPIC-Kitchens基准测试中取得了最先进的性能，尤其在高IoU阈值下表现出色，验证了其在精确时间定位方面的优越性。", "keywords": "少样本时间动作定位, 提示学习, 最优传输, 多提示集成, 动作定位", "comments": "PLOT-TAL的创新点在于其提出的多提示集成范式和引入最优传输来优化提示与特征的对齐，这有效地解决了少样本场景下动作边界不精确的问题。通过鼓励学习组合子事件的专业化提示，该方法提升了模型对动作表示的判别能力和泛化性。其无需复杂元学习即可达到SOTA性能，显示出较强的实用价值和潜在影响力。"}}
{"id": "2507.17856", "title": "A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation", "authors": ["Dennis Benders", "Laura Ferranti", "Johannes Köhler"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      51 pages, 3 figures", "url": "http://arxiv.org/abs/2507.17856v1", "summary": "Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary.", "comment": "51 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.17856v1", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "非线性模型预测控制在安全移动机器人导航中的分步指南", "tldr": "本报告提供了一个关于如何实现非线性模型预测控制（NMPC）以实现移动机器人安全导航的分步指南。", "motivation": "在充满障碍的环境中设计一个能够安全导航的移动机器人模型预测控制（MPC）方案是一项复杂但至关重要的任务。本报告旨在弥合理论NMPC公式与实际机器人应用之间的差距，为研究人员、机器人工程师和从业者提供一个实用且易于理解的路径。", "method": "报告提供了一个实现非线性模型预测控制（NMPC）方案的分步方法，旨在满足移动机器人在存在干扰和测量噪声的情况下，遵守状态和输入约束并避免与障碍物碰撞的安全要求。", "result": "Not mentioned in abstract", "conclusion": "本报告提供了一个实用且易于理解的路径，将非线性模型预测控制（NMPC）的理论概念转化为数学证明和实际实现，强调安全性和性能保证，旨在帮助研究人员、机器人工程师和从业者将理论NMPC应用于实际机器人。", "translation": "设计一个能使移动机器人在充满障碍的环境中安全导航的模型预测控制（MPC）方案是机器人领域一项复杂但至关重要的任务。在本技术报告中，安全性指的是确保机器人在存在干扰和测量噪声的情况下，遵守状态和输入约束，同时避免与障碍物碰撞。本报告提供了一种分步方法来实施非线性模型预测控制（NMPC）方案，以解决这些安全要求。\n许多书籍和调查论文对线性MPC（LMPC）\\cite{bemporad2007robust,kouvaritakis2016model}、NMPC \\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook}及其在包括机器人技术在内的各个领域的应用 \\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc} 提供了全面的概述。本报告无意重复这些详尽的综述。相反，它专门关注NMPC作为安全移动机器人导航的基础。目标是提供一个从理论概念到数学证明和实现的实用且易于理解的路径，强调安全性和性能保证。它面向寻求弥合理论NMPC公式与实际机器人应用之间差距的研究人员、机器人工程师和从业人员。\n本报告不一定保持不变。如果有人发现所提出理论中的错误，请通过提供的电子邮件地址联系。如有必要，我们很乐意更新文档。", "summary": "本技术报告提供了一份关于如何实现非线性模型预测控制（NMPC）以确保移动机器人在复杂环境中安全导航的分步指南。它旨在为研究人员和工程师提供从NMPC理论到实际应用的实用路径，侧重于满足状态、输入约束和避障等安全要求，并强调性能保证。", "keywords": "非线性模型预测控制, 移动机器人导航, 安全性, 避障, 分步指南", "comments": "这篇技术报告的创新之处在于它提供了一个实用的、分步的NMPC实现指南，弥补了理论与实践之间的鸿沟。其重要性在于解决了移动机器人安全导航这一关键挑战，并为从业者提供了直接可用的方法。报告还表现出开放性，鼓励反馈以持续改进内容。"}}
{"id": "2507.17759", "title": "DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation", "authors": ["Riddhi Heda", "Sidhant Singh", "Umair Yasir", "Tanmay Jaiswal", "Anil Mokhade"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17759v1", "summary": "Traditional hostel management practices in academic institutions often suffer\nfrom inefficiencies, delays, and fragmented communication. These systems fail\nto meet the expectations of digitally native students and place a significant\noperational burden on hostel staff. This paper introduces DHMS (Digital Hostel\nManagement System), a modular and integrated platform designed to digitize and\nstreamline essential hostel management functions. DHMS leverages modern web\ntechnologies, artificial intelligence, and cloud infrastructure to automate\nroom allotment, grievance redressal, gate pass logistics, and communication via\na natural language chatbot. In simulation tests, DHMS achieved a 92% student\nsatisfaction rate in room allocation and maintained an average chatbot response\ntime below one second. Additional features include predictive analytics for\nproactive maintenance planning and sentiment analysis for feedback processing.\nWhile promising, the system requires further testing for integration across\nmultiple hostel blocks, user acceptance, scalability under load, and ERP\ncompatibility before campus-wide deployment. This work discusses the system\narchitecture, implementation approach, and factors critical to improving user\nexperience, administrative efficiency, and decision-making processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17759v1", "cate": "cs.HC", "date": "2025-05-08", "updated": "2025-05-08", "AI": {"title_translation": "DHMS：一个整合了校园聊天机器人、预测智能和实时自动化的数字宿舍管理系统", "tldr": "DHMS是一个数字宿舍管理系统，利用AI和自动化解决传统宿舍管理的低效问题，并在模拟测试中表现良好，提高了学生满意度和效率。", "motivation": "传统宿舍管理实践效率低下、延迟严重、沟通碎片化，未能满足数字化时代学生需求，并给宿舍员工带来沉重运营负担。", "method": "DHMS是一个模块化、集成化的平台，利用现代网络技术、人工智能和云基础设施，通过自然语言聊天机器人实现房间分配、申诉处理、门禁物流和通信的自动化。它还包括用于主动维护计划的预测分析和用于反馈处理的情感分析。", "result": "在模拟测试中，DHMS在房间分配方面实现了92%的学生满意度，并保持了平均聊天机器人响应时间低于一秒。", "conclusion": "DHMS系统前景光明，但在全校范围部署前，需要进一步测试其在多个宿舍楼的集成、用户接受度、负载下的可伸缩性以及与ERP的兼容性。本文讨论了系统架构、实现方法和关键因素，旨在改善用户体验、管理效率和决策过程。", "translation": "传统学术机构的宿舍管理实践常常效率低下、延迟严重且沟通碎片化。这些系统未能满足数字化原生学生的需求，并给宿舍员工带来了沉重的运营负担。本文介绍了DHMS（数字宿舍管理系统），一个模块化、集成化的平台，旨在数字化并简化基本的宿舍管理功能。DHMS利用现代网络技术、人工智能和云基础设施，通过自然语言聊天机器人实现房间分配、申诉处理、门禁物流和通信的自动化。在模拟测试中，DHMS在房间分配方面实现了92%的学生满意度，并保持了平均聊天机器人响应时间低于一秒。附加功能包括用于主动维护计划的预测分析和用于反馈处理的情感分析。尽管前景光明，但该系统在全校范围部署之前，需要进一步测试其在多个宿舍楼的集成、用户接受度、负载下的可伸缩性以及与ERP的兼容性。这项工作讨论了系统架构、实施方法以及对改善用户体验、管理效率和决策过程至关重要的因素。", "summary": "本文提出DHMS（数字宿舍管理系统），旨在解决传统宿舍管理低效和沟通碎片化的问题。该系统集成了校园聊天机器人、预测智能和实时自动化，利用现代网络技术、AI和云基础设施，实现房间分配、申诉处理、门禁管理和沟通的自动化。模拟测试显示，DHMS在学生满意度和聊天机器人响应速度方面表现出色。系统还提供预测分析和情感分析功能。作者指出，DHMS在全面部署前仍需进行多方面测试，以确保其在实际应用中的性能和兼容性。", "keywords": "数字宿舍管理系统, 聊天机器人, 预测智能, 自动化, 学生满意度", "comments": "该论文提出了一种创新的数字宿舍管理系统，通过整合人工智能（聊天机器人、预测分析、情感分析）和自动化技术，显著提升了传统宿舍管理的效率和用户体验。其模块化和集成化的设计理念具有很高的实用价值和前瞻性。论文也清晰地指出了系统在实际大规模部署前所需进行的严格测试，例如多宿舍楼集成、用户接受度、负载下的可伸缩性以及与ERP的兼容性，这体现了研究的严谨性。"}}
{"id": "2507.18060", "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion", "authors": ["Chengxuan Zhu", "Qingnan Fan", "Qi Zhang", "Jinwei Chen", "Huaqi Zhang", "Chao Xu", "Boxin Shi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.18060v1", "summary": "We introduce BokehDiff, a novel lens blur rendering method that achieves\nphysically accurate and visually appealing outcomes, with the help of\ngenerative diffusion prior. Previous methods are bounded by the accuracy of\ndepth estimation, generating artifacts in depth discontinuities. Our method\nemploys a physics-inspired self-attention module that aligns with the image\nformation process, incorporating depth-dependent circle of confusion constraint\nand self-occlusion effects. We adapt the diffusion model to the one-step\ninference scheme without introducing additional noise, and achieve results of\nhigh quality and fidelity. To address the lack of scalable paired data, we\npropose to synthesize photorealistic foregrounds with transparency with\ndiffusion models, balancing authenticity and scene diversity.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.18060v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "BokehDiff: 神经镜头模糊与一步扩散", "tldr": "BokehDiff是一种新颖的神经镜头模糊渲染方法，利用一步扩散模型生成物理精确且视觉吸引人的散景效果，解决了传统方法在深度估计上的局限性。", "motivation": "先前的方法受限于深度估计的准确性，在深度不连续处会产生伪影。", "method": "引入BokehDiff，一种新颖的镜头模糊渲染方法。该方法利用生成扩散先验，采用受物理启发的自注意力模块，与图像形成过程对齐，并结合了深度依赖的模糊圈约束和自遮挡效应。他们将扩散模型调整为一步推理方案，且不引入额外噪声。为了解决缺乏可扩展配对数据的问题，他们提出使用扩散模型合成具有透明度的真实感前景。", "result": "实现了物理精确且视觉吸引人的结果，具有高质量和高保真度。", "conclusion": "BokehDiff通过结合物理启发和一步扩散模型，克服了传统镜头模糊渲染方法的局限性，提供了高质量的散景效果。", "translation": "我们引入了BokehDiff，一种新颖的镜头模糊渲染方法，借助生成扩散先验，实现了物理精确和视觉吸引人的效果。先前的方法受限于深度估计的准确性，在深度不连续处会产生伪影。我们的方法采用了一个受物理启发的自注意力模块，与图像形成过程对齐，结合了深度依赖的模糊圈约束和自遮挡效应。我们使扩散模型适应一步推理方案，不引入额外噪声，并实现了高质量和高保真度的结果。为了解决可扩展配对数据缺乏的问题，我们提出使用扩散模型合成具有透明度的真实感前景，平衡了真实性和场景多样性。", "summary": "BokehDiff提出了一种基于一步扩散模型的新型神经镜头模糊渲染方法，旨在生成物理精确且视觉吸引人的散景效果。该方法通过结合受物理启发的自注意力模块来解决传统方法在深度估计上的伪影问题，并适应一步扩散推理以提高效率和质量。此外，为解决数据稀缺性，该研究还引入了利用扩散模型合成真实感前景的方法。", "keywords": "镜头模糊, 扩散模型, 神经渲染, 自注意力, 散景", "comments": "这篇论文的创新点在于将扩散模型应用于物理精确的镜头模糊渲染，并通过结合物理启发式自注意力模块和一步推理方案，有效解决了传统方法的深度估计伪影问题。同时，其数据合成策略也为解决高质量配对数据稀缺提供了可行方案，具有重要的实际应用价值。"}}
{"id": "2507.18287", "title": "Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis", "authors": ["Wenran Zhang", "Huihuan Luo", "Linda Wei", "Ping Nie", "Yiqun Wu", "Dedong Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18287v1", "summary": "Periodontitis and dental caries are common oral diseases affecting billions\nglobally. While observational studies suggest links between these conditions\nand lung cancer, causality remains uncertain. This study used two sample\nMendelian randomization (MR) to explore causal relationships between dental\ntraits (periodontitis, dental caries) and lung cancer subtypes, and to assess\nmediation by pulmonary function. Genetic instruments were derived from the\nlargest available genome wide association studies, including data from 487,823\ndental caries and 506,594 periodontitis cases, as well as lung cancer data from\nthe Transdisciplinary Research of Cancer in Lung consortium. Inverse variance\nweighting was the main analytical method; lung function mediation was assessed\nusing the delta method. The results showed a significant positive causal effect\nof dental caries on overall lung cancer and its subtypes. Specifically, a one\nstandard deviation increase in dental caries incidence was associated with a\n188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =\n1.236--6.713, p = 0.014), partially mediated by declines in forced vital\ncapacity (FVC) and forced expiratory volume in one second (FEV1), accounting\nfor 5.124% and 5.890% of the total effect. No causal effect was found for\nperiodontitis. These findings highlight a causal role of dental caries in lung\ncancer risk and support integrating dental care and pulmonary function\nmonitoring into cancer prevention strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18287v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过孟德尔随机化和中介分析剖析牙齿-肺癌轴", "tldr": "本研究利用孟德尔随机化发现龋齿与肺癌风险存在因果关系，并部分通过肺功能下降介导，而牙周炎则无此关联。", "motivation": "观察性研究表明牙周炎和龋齿与肺癌之间存在关联，但因果关系尚不确定。本研究旨在探究这些口腔疾病与肺癌亚型之间的因果关系，并评估肺功能在其中的中介作用。", "method": "本研究采用两样本孟德尔随机化（MR）方法，利用来自最大规模全基因组关联研究的遗传工具，包括487,823例龋齿和506,594例牙周炎病例数据，以及来自肺癌跨学科研究联盟的肺癌数据。主要分析方法为逆方差加权法；肺功能的中介作用通过Delta法评估。", "result": "研究发现龋齿与整体肺癌及其亚型之间存在显著的正向因果效应。具体而言，龋齿发病率每增加一个标准差，鳞状细胞肺癌风险增加188.0%（OR = 2.880, 95% CI = 1.236--6.713, p = 0.014），其中5.124%和5.890%的总效应分别由用力肺活量（FVC）和一秒用力呼气容积（FEV1）的下降部分介导。牙周炎未发现因果效应。", "conclusion": "研究结果强调了龋齿在肺癌风险中的因果作用，并支持将口腔护理和肺功能监测整合到癌症预防策略中。", "translation": "牙周炎和龋齿是全球影响数十亿人的常见口腔疾病。尽管观察性研究表明这些疾病与肺癌之间存在关联，但因果关系仍不确定。本研究采用两样本孟德尔随机化（MR）来探索牙齿特征（牙周炎、龋齿）与肺癌亚型之间的因果关系，并评估肺功能的中介作用。遗传工具来源于现有最大的全基因组关联研究，包括487,823例龋齿和506,594例牙周炎病例数据，以及来自肺癌跨学科研究联盟的肺癌数据。逆方差加权法是主要的分析方法；肺功能的中介作用通过Delta法评估。结果显示，龋齿对整体肺癌及其亚型具有显著的正向因果效应。具体而言，龋齿发病率每增加一个标准差，鳞状细胞肺癌的风险增加188.0%（OR = 2.880, 95% CI = 1.236--6.713, p = 0.014），其中部分由用力肺活量（FVC）和一秒用力呼气容积（FEV1）的下降介导，分别占总效应的5.124%和5.890%。未发现牙周炎的因果效应。这些发现强调了龋齿在肺癌风险中的因果作用，并支持将牙齿护理和肺功能监测整合到癌症预防策略中。", "summary": "本研究利用两样本孟德尔随机化和中介分析，探讨了口腔疾病（牙周炎、龋齿）与肺癌之间的因果关系。结果表明，龋齿与肺癌风险存在显著的正向因果效应，且部分通过肺功能下降介导。具体地，龋齿与鳞状细胞肺癌风险显著相关，而牙周炎未发现因果关系。研究强调了龋齿在肺癌发生中的因果作用，并建议将口腔护理和肺功能监测纳入癌症预防策略。", "keywords": "龋齿, 肺癌, 孟德尔随机化, 中介分析, 肺功能", "comments": "本研究通过孟德尔随机化这一强大的因果推断方法，为龋齿与肺癌之间的因果关系提供了遗传学证据，而非仅仅是观察性关联。其创新之处在于不仅探究了因果关系，还进一步揭示了肺功能下降在其中的部分中介作用。这对于肺癌的预防和公共卫生策略制定具有重要意义，提示在癌症预防中应重视口腔健康。"}}
{"id": "2507.18447", "title": "PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior", "authors": ["Junda Wu", "Jessica Echterhoff", "Kyungtae Han", "Amr Abdelraouf", "Rohit Gupta", "Julian McAuley"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18447v1", "summary": "Understanding a driver's behavior and intentions is important for potential\nrisk assessment and early accident prevention. Safety and driver assistance\nsystems can be tailored to individual drivers' behavior, significantly\nenhancing their effectiveness. However, existing datasets are limited in\ndescribing and explaining general vehicle movements based on external visual\nevidence. This paper introduces a benchmark, PDB-Eval, for a detailed\nunderstanding of Personalized Driver Behavior, and aligning Large Multimodal\nModels (MLLMs) with driving comprehension and reasoning. Our benchmark consists\nof two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'\nunderstanding of temporal driving scenes. Our dataset is designed to find valid\nvisual evidence from the external view to explain the driver's behavior from\nthe internal view. To align MLLMs' reasoning abilities with driving tasks, we\npropose PDB-QA as a visual explanation question-answering task for MLLM\ninstruction fine-tuning. As a generic learning task for generative models like\nMLLMs, PDB-QA can bridge the domain gap without harming MLLMs'\ngeneralizability. Our evaluation indicates that fine-tuning MLLMs on\nfine-grained descriptions and explanations can effectively bridge the gap\nbetween MLLMs and the driving domain, which improves zero-shot performance on\nquestion-answering tasks by up to 73.2%. We further evaluate the MLLMs\nfine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition\ntasks. We observe up to 12.5% performance improvements on the turn intention\nprediction task in Brain4Cars, and consistent performance improvements up to\n11.0% on all tasks in AIDE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18447v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PDB-Eval：大型多模态模型在个性化驾驶行为描述和解释方面的评估", "tldr": "本文提出了PDB-Eval基准，用于评估和调整大型多模态模型(MLLMs)在个性化驾驶行为理解和解释方面的能力，并通过细粒度微调显著提高了其在驾驶领域任务上的性能。", "motivation": "理解驾驶员行为和意图对于风险评估和事故预防至关重要。现有的数据集在基于外部视觉证据描述和解释通用车辆运动方面存在局限性，导致安全和驾驶辅助系统难以针对个体驾驶员行为进行定制。", "method": "本文引入了PDB-Eval基准，包含PDB-X和PDB-QA两个主要组件。PDB-X用于评估MLLMs对时间驾驶场景的理解，旨在从外部视图中找到有效的视觉证据来解释驾驶员的内部视图行为。PDB-QA被提议作为视觉解释问答任务，用于MLLM指令微调，以弥合领域差距并保持MLLMs的泛化能力。", "result": "在细粒度描述和解释上对MLLMs进行微调可以有效弥合MLLMs与驾驶领域之间的差距。在问答任务上的零样本性能提高了高达73.2%。在Brain4Cars的转向意图预测任务上，性能提高了高达12.5%。在AIDE的所有任务上，性能持续提高了高达11.0%。", "conclusion": "通过PDB-Eval基准对大型多模态模型进行细粒度微调，可以显著提高其在个性化驾驶行为理解、解释和预测任务上的性能，有效弥合了模型与驾驶领域之间的差距。", "translation": "了解驾驶员的行为和意图对于潜在的风险评估和早期事故预防至关重要。安全和驾驶辅助系统可以根据个体驾驶员的行为进行定制，从而显著提高其有效性。然而，现有数据集在基于外部视觉证据描述和解释通用车辆运动方面存在局限性。本文引入了一个基准PDB-Eval，用于详细理解个性化驾驶行为，并将大型多模态模型（MLLMs）与驾驶理解和推理对齐。我们的基准由两个主要组件组成：PDB-X和PDB-QA。PDB-X可以评估MLLMs对时间驾驶场景的理解。我们的数据集旨在从外部视图中找到有效的视觉证据，以从内部视图解释驾驶员的行为。为了使MLLMs的推理能力与驾驶任务对齐，我们提出了PDB-QA作为MLLM指令微调的视觉解释问答任务。作为MLLMs等生成模型的通用学习任务，PDB-QA可以在不损害MLLMs泛化能力的情况下弥合领域差距。我们的评估表明，在细粒度描述和解释上对MLLMs进行微调可以有效弥合MLLMs与驾驶领域之间的差距，这将问答任务上的零样本性能提高了高达73.2%。我们进一步评估了在Brain4Cars的意图预测和AIDE的识别任务中对PDB-X进行微调的MLLMs。我们观察到在Brain4Cars的转向意图预测任务上性能提高了高达12.5%，在AIDE的所有任务上性能持续提高了高达11.0%。", "summary": "本文提出了PDB-Eval，这是一个用于评估大型多模态模型(MLLMs)在个性化驾驶行为描述和解释方面能力的基准。该基准包含PDB-X（用于理解时间驾驶场景和视觉证据解释）和PDB-QA（用于视觉解释问答任务以进行MLLM指令微调）。研究表明，通过PDB-Eval对MLLMs进行细粒度微调，可以显著提高其在驾驶领域任务上的性能，包括零样本问答、驾驶意图预测和行为识别，有效弥合了模型与驾驶领域之间的差距。", "keywords": "大型多模态模型, 驾驶行为, PDB-Eval, 行为解释, 驾驶理解", "comments": "该论文通过引入PDB-Eval基准，为大型多模态模型在复杂驾驶行为理解和解释方面提供了一个急需的评估和微调框架。其创新之处在于将外部视觉证据与内部驾驶员行为解释相结合，并提出了一种不损害模型泛化能力的微调策略。这项工作对于推动驾驶安全和智能辅助系统的发展具有重要意义。"}}
{"id": "2501.01144", "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference", "authors": ["Wonsuk Jang", "Thierry Tambe"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2501.01144v5", "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2501.01144v5", "cate": "cs.CL", "date": "2025-01-02", "updated": "2025-07-24", "AI": {"title_translation": "BlockDialect：块级细粒度混合格式量化用于能效LLM推理", "tldr": "BlockDialect提出一种块级细粒度混合格式量化技术，通过为每个数据块选择最优的数值格式来提高LLM推理的能效和精度。", "motivation": "大语言模型（LLMs）不断增大的规模带来了巨大的内存使用和计算成本挑战。现有的量化方法难以捕捉细微的块数据分布。", "method": "本文提出了BlockDialect，一种块级细粒度混合格式量化技术，它从一个格式本中为每个数据块分配最优的数值格式，以实现更好的数据表示。此外，引入了DialectFP4，一个包含FP4变体（类似于方言）的格式本，以适应多样的数据分布。为高效利用，提出了一种两阶段方法进行在线DialectFP4激活量化。DialectFP4通过选择可表示的值作为可与低精度整数算术兼容的缩放整数来确保能效。", "result": "BlockDialect在LLaMA3-8B (LLaMA2-7B) 模型上相比MXFP4格式实现了10.78% (7.48%) 的精度提升，同时每个数据使用更低的比特数；即使量化全路径矩阵乘法，也仅比全精度低5.45% (2.69%)。", "conclusion": "该工作通过关注如何表示而非如何缩放，为能效LLM推理提供了一条有前景的路径。", "translation": "大语言模型（LLMs）规模的迅速增长对内存使用和计算成本提出了严峻挑战。对权重和激活进行量化可以解决这些问题，其中硬件支持的细粒度缩放作为一种有前景的解决方案，可以缓解异常值问题。然而，现有方法难以捕捉细微的块数据分布。我们提出了BlockDialect，一种块级细粒度混合格式技术，它从格式本中为每个数据块分配最优的数值格式，以实现更好的数据表示。此外，我们引入了DialectFP4，一个由FP4变体（类似于方言）组成的格式本，以适应多样的数据分布。为了高效利用这一点，我们提出了一种两阶段方法进行在线DialectFP4激活量化。重要的是，DialectFP4通过选择可表示的值作为与低精度整数算术兼容的缩放整数来确保能效。与MXFP4格式相比，BlockDialect在LLaMA3-8B (LLaMA2-7B) 模型上实现了10.78% (7.48%) 的精度提升，同时每个数据使用的比特数更低，即使在量化全路径矩阵乘法时，也仅比全精度低5.45% (2.69%)。我们的工作侧重于如何表示而非如何缩放，为能效LLM推理提供了一条有前景的路径。", "summary": "本文提出了BlockDialect，一种用于大语言模型（LLMs）能效推理的块级细粒度混合格式量化技术。针对现有方法难以捕捉细微块数据分布的问题，BlockDialect为每个数据块从预定义的格式本（如DialectFP4）中选择最优的数值格式，以实现更好的数据表示。DialectFP4包含多种FP4变体，并通过两阶段在线量化确保能效，其核心在于选择与低精度整数算术兼容的缩放整数。实验结果表明，BlockDialect在LLaMA模型上相比现有MXFP4格式显著提高了精度，同时保持了较低的比特使用和接近全精度的性能，为LLM的能效推理开辟了新方向。", "keywords": "量化, 大语言模型, 混合格式, 能效, BlockDialect", "comments": "本文的创新点在于提出了“块级细粒度混合格式量化”的概念，并引入了“格式本”（formatbook）和“DialectFP4”的概念，通过为不同数据块选择最优的数值格式来适应数据分布，这比传统的统一量化方法更灵活和精确。其关注点从“如何缩放”转向“如何表示”，为低比特量化提供了新的视角。在精度和能效方面的显著提升，显示了该方法在解决LLM部署挑战上的巨大潜力。"}}
{"id": "2507.18105", "title": "Understanding the Supply Chain and Risks of Large Language Model Applications", "authors": ["Yujie Ma", "Lili Quan", "Xiaofei Xie", "Qiang Hu", "Jiongchi Yu", "Yao Zhang", "Sen Chen"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      26 pages", "url": "http://arxiv.org/abs/2507.18105v1", "summary": "The rise of Large Language Models (LLMs) has led to the widespread deployment\nof LLM-based systems across diverse domains. As these systems proliferate,\nunderstanding the risks associated with their complex supply chains is\nincreasingly important. LLM-based systems are not standalone as they rely on\ninterconnected supply chains involving pretrained models, third-party\nlibraries, datasets, and infrastructure. Yet, most risk assessments narrowly\nfocus on model or data level, overlooking broader supply chain vulnerabilities.\nWhile recent studies have begun to address LLM supply chain risks, there\nremains a lack of benchmarks for systematic research.\n  To address this gap, we introduce the first comprehensive dataset for\nanalyzing and benchmarking LLM supply chain security. We collect 3,859\nreal-world LLM applications and perform interdependency analysis, identifying\n109,211 models, 2,474 datasets, and 9,862 libraries. We extract model\nfine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's\nstructure. To evaluate security, we gather 1,555 risk-related issues-50 for\napplications, 325 for models, 18 for datasets, and 1,229 for libraries from\npublic vulnerability databases.\n  Using this dataset, we empirically analyze component dependencies and risks.\nOur findings reveal deeply nested dependencies in LLM applications and\nsignificant vulnerabilities across the supply chain, underscoring the need for\ncomprehensive security analysis. We conclude with practical recommendations to\nguide researchers and developers toward safer, more trustworthy LLM-enabled\nsystems.", "comment": "26 pages", "pdf_url": "http://arxiv.org/pdf/2507.18105v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "理解大型语言模型应用的供应链与风险", "tldr": "该研究引入了首个全面数据集，用于分析和基准测试大型语言模型（LLM）供应链安全，揭示了LLM应用中深层嵌套的依赖关系和显著漏洞。", "motivation": "随着大型语言模型（LLM）系统日益普及，理解其复杂供应链相关的风险变得越来越重要。当前大多数风险评估仅关注模型或数据层面，忽视了更广泛的供应链漏洞。尽管近期研究已开始关注LLM供应链风险，但仍缺乏系统性研究的基准。", "method": "研究构建了首个全面数据集，用于分析和基准测试LLM供应链安全。收集了3,859个真实世界的LLM应用，并进行了相互依赖性分析，识别出109,211个模型、2,474个数据集和9,862个库。提取了模型微调路径、数据集重用和库依赖关系，以绘制生态系统结构。为评估安全性，从公共漏洞数据库收集了1,555个风险相关问题（应用50个、模型325个、数据集18个、库1,229个）。利用该数据集对组件依赖关系和风险进行了实证分析。", "result": "研究结果揭示了LLM应用中深层嵌套的依赖关系，以及整个供应链中存在的显著漏洞。", "conclusion": "研究强调了进行全面安全分析的必要性，并提出了实用的建议，以指导研究人员和开发人员构建更安全、更值得信赖的支持LLM的系统。", "translation": "大型语言模型（LLM）的兴起导致了基于LLM的系统在各个领域的广泛部署。随着这些系统的普及，理解与其复杂供应链相关的风险变得越来越重要。基于LLM的系统并非独立存在，它们依赖于涉及预训练模型、第三方库、数据集和基础设施的相互连接的供应链。然而，大多数风险评估仅狭隘地关注模型或数据层面，忽视了更广泛的供应链漏洞。尽管近期研究已开始关注LLM供应链风险，但仍缺乏系统性研究的基准。\n为了弥补这一空白，我们引入了首个用于分析和基准测试LLM供应链安全的全面数据集。我们收集了3,859个真实世界的LLM应用，并进行了相互依赖性分析，识别出109,211个模型、2,474个数据集和9,862个库。我们提取了模型微调路径、数据集重用和库依赖关系，绘制了生态系统的结构。为了评估安全性，我们从公共漏洞数据库收集了1,555个风险相关问题——应用50个、模型325个、数据集18个、库1,229个。\n利用该数据集，我们对组件依赖关系和风险进行了实证分析。我们的发现揭示了LLM应用中深层嵌套的依赖关系以及整个供应链中存在的显著漏洞，强调了进行全面安全分析的必要性。最后，我们提出了实用的建议，以指导研究人员和开发人员构建更安全、更值得信赖的支持LLM的系统。", "summary": "本研究旨在解决大型语言模型（LLM）应用供应链中风险评估不足的问题。通过构建首个全面的LLM供应链安全数据集，包含3,859个真实应用及其关联的模型、数据集和库，并收集了大量风险问题，研究人员对LLM组件的依赖性和漏洞进行了深入分析。结果表明LLM应用存在复杂的嵌套依赖和显著的供应链漏洞，强调了对LLM系统进行全面安全分析的迫切需求，并提出了相应的实践建议。", "keywords": "大型语言模型, 供应链, 风险, 安全, 漏洞", "comments": "这项研究的创新之处在于构建了首个专门针对大型语言模型供应链安全的综合数据集，并进行了系统的实证分析。它填补了当前风险评估主要集中在模型或数据层面而忽视整个供应链漏洞的空白。该研究揭示了LLM应用中深层嵌套的依赖关系和显著的供应链漏洞，对于提升LLM系统的安全性和可信度具有重要意义。"}}
{"id": "2502.04277", "title": "Non-Variational Quantum Random Access Optimization with Alternating Operator Ansatz", "authors": ["Zichang He", "Rudy Raymond", "Ruslan Shaydulin", "Marco Pistoia"], "categories": ["quant-ph", "cs.ET"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      9 pages, 8+1 figures, accepted by Scientific Reports", "url": "http://arxiv.org/abs/2502.04277v2", "summary": "Solving hard optimization problems is one of the most promising application\ndomains for quantum computers due to the ubiquity of such problems in industry\nand the availability of broadly applicable quantum speedups. However, the\nability of near-term quantum computers to tackle industrial-scale optimization\nproblems is limited by their size and the overheads of quantum error\ncorrection. Quantum Random Access Optimization (QRAO) has been proposed to\nreduce the space requirements of quantum optimization. However, to date QRAO\nhas only been implemented using variational algorithms, which suffer from the\nneed to train instance-specific variational parameters, making them difficult\nto scale. We propose and benchmark a non-variational approach to QRAO based on\nthe Quantum Alternating Operator Ansatz (QAOA) for the MaxCut problem. We show\nthat instance-independent ``fixed\" parameters achieve good performance,\nremoving the need for variational parameter optimization. Additionally, we\nevaluate different design choices, such as various mixers, initial states, and\nQRAO-specific implementations of the QAOA cost operator, and identify a\nstrategy that performs well in practice. Our results pave the way for the\npractical execution of QRAO on early fault-tolerant quantum computers.", "comment": "9 pages, 8+1 figures, accepted by Scientific Reports", "pdf_url": "http://arxiv.org/pdf/2502.04277v2", "cate": "quant-ph", "date": "2025-02-06", "updated": "2025-07-24", "AI": {"title_translation": "非变分量子随机存取优化与交替算子本征态", "tldr": "提出了一种基于QAOA的非变分QRAO方法，通过使用固定参数解决了变分QRAO的扩展性问题，并在MaxCut问题上取得了良好性能。", "motivation": "近期量子计算机在处理工业规模优化问题时受到尺寸和量子纠错开销的限制。量子随机存取优化（QRAO）旨在减少空间需求，但现有QRAO方法依赖变分算法，需要训练特定实例的参数，使其难以扩展。", "method": "提出了一种基于量子交替算子本征态（QAOA）的非变分QRAO方法，并将其应用于MaxCut问题。通过使用实例无关的“固定”参数，消除了对变分参数优化的需求。此外，还评估了不同的设计选择，如混合器、初始状态和QRAO特有的QAOA成本算子实现。", "result": "研究表明，实例无关的“固定”参数能够实现良好的性能，从而无需进行变分参数优化。同时，通过评估不同的设计选择，识别出一种在实践中表现良好的策略。", "conclusion": "本研究结果为在早期容错量子计算机上实际执行QRAO铺平了道路。", "translation": "解决困难优化问题是量子计算机最有前景的应用领域之一，因为这些问题在工业中普遍存在，并且量子加速具有广泛适用性。然而，近期量子计算机处理工业规模优化问题的能力受到其规模和量子纠错开销的限制。量子随机存取优化（QRAO）已被提出以减少量子优化所需的空间。但迄今为止，QRAO仅通过变分算法实现，这需要训练特定实例的变分参数，使其难以扩展。我们提出并基准测试了一种基于量子交替算子本征态（QAOA）的非变分QRAO方法，用于MaxCut问题。我们表明，实例无关的“固定”参数实现了良好的性能，消除了变分参数优化的需要。此外，我们评估了不同的设计选择，例如各种混合器、初始状态以及QRAO特有的QAOA成本算子实现，并确定了一种在实践中表现良好的策略。我们的结果为在早期容错量子计算机上实际执行QRAO铺平了道路。", "summary": "本文提出了一种创新的非变分量子随机存取优化（QRAO）方法，该方法基于量子交替算子本征态（QAOA），旨在解决现有变分QRAO算法在可扩展性上的限制。通过在MaxCut问题上的基准测试，研究发现使用实例无关的固定参数即可达到良好性能，从而避免了繁琐的参数优化过程。此外，研究还系统评估了不同的算法设计选择，并确定了一种实用的优化策略，为QRAO在未来容错量子计算机上的实际应用奠定了基础。", "keywords": "量子随机存取优化, 非变分算法, 量子交替算子本征态, MaxCut, 量子优化", "comments": "这篇论文的创新点在于提出了非变分QRAO，解决了传统变分QRAO需要大量参数训练的痛点，显著提高了其可扩展性和实用性。通过使用固定参数，降低了算法的复杂性，使其更适合未来的早期容错量子计算机。这项工作对于推动量子优化算法的实际应用具有重要意义。"}}
{"id": "2501.06726", "title": "Integrated Sensing and Edge AI: Realizing Intelligent Perception in 6G", "authors": ["Zhiyan Liu", "Xu Chen", "Hai Wu", "Zhanwei Wang", "Xianhao Chen", "Dusit Niyato", "Kaibin Huang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      To appear in IEEE Communications Surveys and Tutorials", "url": "http://arxiv.org/abs/2501.06726v2", "summary": "Sensing and edge artificial intelligence (AI) are envisioned as two essential\nand interconnected functions in sixth-generation (6G) mobile networks. On the\none hand, sensing-empowered applications rely on powerful AI models to extract\nfeatures and understand semantics from ubiquitous wireless sensors. On the\nother hand, the massive amount of sensory data serves as the fuel to\ncontinuously refine edge AI models. This deep integration of sensing and edge\nAI has given rise to a new task-oriented paradigm known as integrated sensing\nand edge AI (ISEA), which features a holistic design approach to communication,\nAI computation, and sensing for optimal sensing-task performance. In this\narticle, we present a comprehensive survey for ISEA. We first provide technical\npreliminaries for sensing, edge AI, and new communication paradigms in ISEA.\nThen, we study several use cases of ISEA to demonstrate its practical relevance\nand introduce current standardization and industrial progress. Next, the design\nprinciples, metrics, tradeoffs, and architectures of ISEA are established,\nfollowed by a thorough overview of ISEA techniques, including digital air\ninterface, over-the-air computation, and advanced signal processing. Its\ninterplay with various 6G advancements, e.g., new physical-layer and networking\ntechniques, are presented. Finally, we present future research opportunities in\nISEA, including the integration of foundation models, convergence of ISEA and\nintegrated sensing and communications (ISAC), ultra-low-latency ISEA, and\npracticality issues.", "comment": "To appear in IEEE Communications Surveys and Tutorials", "pdf_url": "http://arxiv.org/pdf/2501.06726v2", "cate": "cs.IT", "date": "2025-01-12", "updated": "2025-07-24", "AI": {"title_translation": "集成感知与边缘AI：实现6G中的智能感知", "tldr": "本文全面综述了6G中的集成感知与边缘AI (ISEA)，涵盖了其技术基础、用例、设计原则、关键技术、与6G进展的相互作用以及未来的研究机会。", "motivation": "感知与边缘AI被认为是6G移动网络中必不可少且相互关联的功能。它们的深度融合催生了集成感知与边缘AI (ISEA) 这一新的面向任务的范式，旨在实现最佳的感知任务性能。本文旨在对ISEA进行全面调查，以阐明其重要性。", "method": "本文对集成感知与边缘AI (ISEA) 进行了全面综述。具体方法包括：提供感知、边缘AI和新通信范式的技术基础；研究ISEA的用例以展示其实际相关性，并介绍标准化和工业进展；建立ISEA的设计原则、指标、权衡和架构；概述ISEA技术，如数字空口、空口计算和高级信号处理；探讨其与各种6G进展的相互作用；并提出未来的研究机会。", "result": "本文提供了一个关于集成感知与边缘AI (ISEA) 的全面概述，详细阐述了其作为6G中一种新的面向任务的范式的形成与发展。它涵盖了ISEA的技术基础、实际应用场景、设计考量、具体技术实现以及与现有6G技术的融合点。此外，论文还识别并提出了ISEA领域未来的关键研究方向。", "conclusion": "本文提出了集成感知与边缘AI (ISEA) 未来的研究机会，包括基础模型的集成、ISEA与集成感知和通信 (ISAC) 的融合、超低延迟ISEA以及实际应用中的挑战。", "translation": "感知与边缘人工智能 (AI) 被设想为第六代 (6G) 移动网络中两个必不可少且相互关联的功能。一方面，感知赋能的应用依赖于强大的AI模型从无处不在的无线传感器中提取特征并理解语义。另一方面，大量的感官数据作为燃料，不断完善边缘AI模型。感知与边缘AI的这种深度融合催生了一种新的面向任务的范式，即集成感知与边缘AI (ISEA)，其特点是对通信、AI计算和感知采用整体设计方法，以实现最佳感知任务性能。在本文中，我们对ISEA进行了全面调查。我们首先提供了ISEA中感知、边缘AI和新通信范式的技术基础。然后，我们研究了ISEA的几个用例，以展示其实际相关性，并介绍了当前的标准化和工业进展。接下来，建立了ISEA的设计原则、指标、权衡和架构，随后全面概述了ISEA技术，包括数字空口、空口计算和高级信号处理。论文还介绍了ISEA与各种6G进展的相互作用，例如新的物理层和网络技术。最后，我们提出了ISEA未来的研究机会，包括基础模型的集成、ISEA与集成感知和通信 (ISAC) 的融合、超低延迟ISEA以及实际问题。", "summary": "本文对6G网络中的集成感知与边缘AI (ISEA) 进行了全面综述。它详细阐述了感知与边缘AI的深度融合如何形成一种新的面向任务的范式，旨在优化感知任务性能。该综述涵盖了ISEA的技术基础、实际用例、设计原则、具体技术（如数字空口和空口计算），以及其与6G其他进展的相互作用。此外，论文还指出了ISEA未来研究的关键机会。", "keywords": "集成感知, 边缘AI, 6G, 综述, 智能感知", "comments": "这是一篇及时且高度相关的综述论文，它探讨了6G中一个新兴的关键范式。其全面性，涵盖了基础概念、实际应用、设计考量以及未来方向，使其成为该领域研究人员的宝贵资源。文中提出的未来研究机会尤其富有洞察力。"}}
{"id": "2506.02574", "title": "Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding", "authors": ["Shuai Yuan", "Shuang Chen", "Tianwu Lin", "Jincheng Yuan", "Geng Tian", "Yang Xu", "Jie Wang", "Peng Gong"], "categories": ["eess.IV", "cs.CV", "cs.MM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.02574v2", "summary": "Accurate remote sensing geographic mapping requires timely and representative\nsamples. However, rapid land surface changes often render static samples\nobsolete within months, making manual sample updates labor-intensive and\nunsustainable. To address this challenge, we propose TasGen, a two-stage\nTemporal spectral-aware Automatic Sample Generation method for generating\ndynamic training samples from single-date static labels without human\nintervention. Land surface dynamics often manifest as anomalies in\ntemporal-spectral sequences. %These anomalies are multivariate yet unified:\ntemporal, spectral, or joint anomalies stem from different mechanisms and\ncannot be naively coupled, as this may obscure the nature of changes. Yet, any\nland surface state corresponds to a coherent temporal-spectral signature, which\nwould be lost if the two dimensions are modeled separately. To effectively\ncapture these dynamics, TasGen first disentangles temporal and spectral\nfeatures to isolate their individual contributions, and then couples them to\nmodel their synergistic interactions. In the first stage, we introduce a\nhierarchical temporal-spectral variational autoencoder (HTS-VAE) with a\ndual-dimension embedding to learn low-dimensional latent patterns of normal\nsamples by first disentangling and then jointly embedding temporal and spectral\ninformation. This temporal-spectral embedding enables robust anomaly detection\nby identifying deviations from learned joint patterns. In the second stage, a\nclassifier trained on stable samples relabels change points across time to\ngenerate dynamic samples. To not only detect but also explain surface dynamics,\nwe further propose an anomaly interpretation method based on Gibbs sampling,\nwhich attributes changes to specific spectral-temporal dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.02574v2", "cate": "eess.IV", "date": "2025-06-03", "updated": "2025-07-24", "AI": {"title_translation": "从静态标签到动态制图：基于时空光谱嵌入的遥感动态样本生成", "tldr": "TasGen是一种两阶段方法，通过时空光谱嵌入从单日静态标签中自动生成遥感动态训练样本，以解决静态样本过时的问题。", "motivation": "遥感地理制图需要及时且具有代表性的样本，但地表快速变化导致静态样本很快过时，手动更新费时费力且不可持续。", "method": "本文提出了TasGen，一个两阶段的时空光谱感知自动样本生成方法。第一阶段，引入分层时空光谱变分自编码器（HTS-VAE），通过解耦和联合嵌入时空和光谱信息，学习正常样本的低维潜在模式，并进行异常检测。第二阶段，训练一个分类器对稳定样本进行重标记，以生成动态样本。此外，还提出了基于吉布斯采样的异常解释方法，将变化归因于特定的光谱-时间维度。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "准确的遥感地理制图需要及时且具有代表性的样本。然而，地表快速变化常常导致静态样本在数月内过时，使得手动样本更新劳动密集且不可持续。为了解决这一挑战，我们提出了TasGen，一种两阶段的时空光谱感知自动样本生成方法，用于从单日静态标签中生成动态训练样本，无需人工干预。地表动态通常表现为时空光谱序列中的异常。这些异常是多变量但统一的：时间、光谱或联合异常源于不同的机制，不能简单地耦合，因为这可能会模糊变化的性质。然而，任何地表状态都对应着一个连贯的时空光谱特征，如果将这两个维度分开建模，则会丢失该特征。为了有效地捕捉这些动态，TasGen首先解耦时间特征和光谱特征，以隔离它们各自的贡献，然后将它们耦合以模拟它们的协同相互作用。在第一阶段，我们引入了一个分层时空光谱变分自编码器（HTS-VAE），其具有双维度嵌入，通过首先解耦然后联合嵌入时间信息和光谱信息来学习正常样本的低维潜在模式。这种时空光谱嵌入通过识别与学习到的联合模式的偏差来实现鲁棒的异常检测。在第二阶段，在稳定样本上训练的分类器对跨时间的变化点进行重标记，以生成动态样本。为了不仅检测而且解释地表动态，我们进一步提出了一种基于吉布斯采样的异常解释方法，该方法将变化归因于特定的光谱-时间维度。", "summary": "TasGen是一种新颖的两阶段方法，用于从单日静态标签自动生成遥感动态训练样本，以应对地表快速变化导致静态样本过时的问题。它通过分层时空光谱变分自编码器（HTS-VAE）学习正常样本的时空光谱嵌入模式，并基于此进行异常检测。随后，利用分类器对变化点进行重标记，生成动态样本。此外，该方法还提供了一种基于吉布斯采样的异常解释机制。", "keywords": "遥感, 动态样本生成, 时空光谱嵌入, 异常检测, 变分自编码器", "comments": "TasGen通过自动化动态样本生成，有效解决了遥感领域中传统静态样本快速过时的问题，显著降低了人工成本。其创新点在于结合了时空解耦和联合嵌入的HTS-VAE，能够鲁棒地捕捉地表动态并进行异常检测，同时提供变化解释能力，具有重要的实用价值和研究意义。"}}
{"id": "2507.18204", "title": "Optimal Integration Of Heat-Pump And Solar Thermal Energy In The Pre-heating Loop Of Wood And Gas Boiler Based District Heating System", "authors": ["Hamza Mettali", "Rousset François", "Eric Bideaux", "Clausse Marc"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18204v1", "summary": "The integration of renewable sources is essential for decarbonizing heat\nproduction in district energy networks. Beyond biomass-based solutions, solar\nthermal energy, with or without heat pumps, presents a significant opportunity.\nHowever, system performance is highly dependent on outdoor and setpoint\ntemperatures. This study aims to optimize system design using a multi-criteria\napproach that considers techno-economic and environmental (CO2) factors. A\nMixed-Integer Linear Programming (MILP) model is developed, incorporating\ntemperature discretization for problem linearization and capturing key dynamic\ncharacteristics of heat generators. The model improves convergence, reducing a\n19% MIP gap in 26 hours to 10% in 12 hours by dissipating 6% excess solar heat.\nA multi-scenario analysis under two carbon taxation levels and different CO2\nemission cases revealed solar integration up to 11,932 m${}^2$ but increased\ngas reliance (50%) and TES losses (49%). Wood boiler inclusion reduced solar\ndependency, covering 45% of heat, lowered LCOH, but limited renewable\npenetration. Higher carbon taxes boosted solar adoption but faced storage\ninefficiencies, while biomass enhanced cost efficiency and system stability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18204v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "燃木和燃气锅炉区域供热系统中热泵与太阳能热能预热回路的优化集成", "tldr": "论文通过MILP模型优化了区域供热系统中热泵和太阳能与燃木/燃气锅炉的集成，以实现脱碳，结果显示了太阳能集成、燃气依赖、木材锅炉和碳税对系统性能的影响。", "motivation": "在区域能源网络中，整合可再生能源对于热生产的脱碳至关重要。除了基于生物质的解决方案，太阳能热能（无论是否结合热泵）都提供了重要的机遇。然而，系统性能高度依赖于室外和设定温度。本研究旨在优化系统设计，考虑技术经济和环境（CO2）因素。", "method": "开发了一个多目标优化方法，采用混合整数线性规划（MILP）模型，该模型包含温度离散化以实现问题线性化，并捕捉热发生器的关键动态特性。进行了在两种碳税水平和不同CO2排放情况下的多场景分析。", "result": "该模型通过耗散6%的过剩太阳能热量，将26小时内的19%MIP间隙缩短至12小时内的10%，从而改善了收敛性。在多场景分析中，太阳能集成面积可达11,932平方米，但增加了50%的燃气依赖和49%的TES损失。包含燃木锅炉减少了对太阳能的依赖，覆盖了45%的热量需求，降低了LCOH，但限制了可再生能源的渗透。更高的碳税促进了太阳能的采用，但面临存储效率低下；而生物质则提高了成本效率和系统稳定性。", "conclusion": "优化集成热泵和太阳能热能与燃木/燃气锅炉在区域供热系统中是可行的，但需要权衡太阳能集成、燃气依赖、热储损失以及燃木锅炉对可再生能源渗透的影响。碳税可以促进太阳能采用，而生物质则能提升系统经济性和稳定性。", "translation": "标题：燃木和燃气锅炉区域供热系统中热泵与太阳能热能预热回路的优化集成\n摘要：在区域能源网络中，整合可再生能源对于热生产的脱碳至关重要。除了基于生物质的解决方案，太阳能热能（无论是否结合热泵）都提供了重要的机遇。然而，系统性能高度依赖于室外和设定温度。本研究旨在通过考虑技术经济和环境（CO2）因素的多标准方法来优化系统设计。开发了一个混合整数线性规划（MILP）模型，该模型结合了温度离散化以实现问题线性化，并捕捉了热发生器的关键动态特性。该模型通过耗散6%的过剩太阳能热量，将26小时内的19%MIP间隙缩短至12小时内的10%，从而改善了收敛性。在两种碳税水平和不同CO2排放情况下的多场景分析显示，太阳能集成面积可达11,932平方米，但增加了50%的燃气依赖和49%的TES损失。包含燃木锅炉减少了对太阳能的依赖，覆盖了45%的热量需求，降低了LCOH，但限制了可再生能源的渗透。更高的碳税促进了太阳能的采用，但面临存储效率低下，而生物质则提高了成本效率和系统稳定性。", "summary": "本文旨在通过多标准优化方法，在燃木和燃气锅炉区域供热系统中，优化热泵和太阳能热能的集成。研究开发了一个混合整数线性规划（MILP）模型，考虑技术经济和环境因素，并进行了多场景分析。结果表明，该模型提高了收敛性，太阳能集成潜力巨大但可能增加燃气依赖和储热损失。燃木锅炉的加入可降低太阳能依赖和成本，但限制可再生能源渗透。碳税能促进太阳能应用，而生物质能提升系统稳定性和成本效益。", "keywords": "区域供热, 太阳能热能, 热泵, 混合整数线性规划, 脱碳", "comments": "该研究创新性地将混合整数线性规划模型应用于区域供热系统中热泵、太阳能与传统锅炉的优化集成，并考虑了多重标准（技术经济和环境）。其贡献在于揭示了可再生能源集成、传统能源依赖、储热效率和碳税政策之间的复杂权衡关系，为区域供热系统的脱碳提供了有价值的见解。论文也指出了过度依赖太阳能可能带来的储热效率问题，以及生物质在成本效率和系统稳定性方面的优势，这些都对未来系统设计和政策制定具有指导意义。"}}
{"id": "2407.13066", "title": "Fast And Scalable FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices With Application to Linear Inverse Problems Governed by Autonomous Dynamical Systems", "authors": ["Sreeram Venkat", "Milinda Fernando", "Stefan Henneking", "Omar Ghattas"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.13066v2", "summary": "We present an efficient and scalable algorithm for performing matrix-vector\nmultiplications (\"matvecs\") for block Toeplitz matrices. Such matrices, which\nare shift-invariant with respect to their blocks, arise in the context of\nsolving inverse problems governed by autonomous systems, and time-invariant\nsystems in particular. In this article, we consider inverse problems that infer\nunknown parameters from observational data of a linear time-invariant dynamical\nsystem given in the form of partial differential equations (PDEs). Matrix-free\nNewton-conjugate-gradient methods are often the gold standard for solving these\ninverse problems, but they require numerous actions of the Hessian on a vector.\nMatrix-free adjoint-based Hessian matvecs require solution of a pair of\nlinearized forward/adjoint PDE solves per Hessian action, which may be\nprohibitive for large-scale inverse problems. Time invariance of the forward\nPDE problem leads to a block Toeplitz structure of the discretized\nparameter-to-observable (p2o) map defining the mapping from inputs (parameters)\nto outputs (observables) of the PDEs. This block Toeplitz structure enables us\nto exploit two key properties: (1) compact storage of the p2o map and its\nadjoint; and (2) efficient fast Fourier transform (FFT)-based Hessian matvecs.\nThe proposed algorithm is mapped onto large multi-GPU clusters and achieves\nmore than 80 percent of peak bandwidth on NVIDIA A100 GPUs. Excellent weak\nscaling is shown for up to 48 A100 GPUs. For the targeted problems, the\nimplementation executes Hessian matvecs within fractions of a second, which is\norders of magnitude faster than can be achieved by conventional matrix-free\nHessian matvecs via forward/adjoint PDE solves.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.13066v2", "cate": "math.NA", "date": "2024-07-18", "updated": "2025-07-23", "AI": {"title_translation": "用于自治动力系统控制的线性逆问题的块三角托普利茨矩阵的快速可扩展FFT加速GPU算法", "tldr": "本文提出了一种高效、可扩展的基于FFT的GPU加速算法，用于块托普利茨矩阵的矩阵向量乘法，显著加速了大规模线性逆问题中的Hessian矩阵向量乘法。", "motivation": "解决大规模线性逆问题中Hessian矩阵向量乘法计算成本高昂的问题，特别是传统无矩阵方法（如基于前向/伴随PDE求解）的计算量可能过大。", "method": "利用时间不变PDE问题导致的参数到可观测(p2o)映射的块托普利茨结构；利用该结构实现p2o映射及其伴随的紧凑存储和高效的基于FFT的Hessian矩阵向量乘法；将算法映射到大型多GPU集群上。", "result": "在NVIDIA A100 GPU上实现了超过80%的峰值带宽；在多达48个A100 GPU上表现出出色的弱扩展性；对于目标问题，Hessian矩阵向量乘法在几分之一秒内完成，比传统无矩阵方法快几个数量级。", "conclusion": "该研究表明，通过利用块托普利茨结构并结合FFT和GPU加速，可以显著提高处理由时不变动力系统控制的大规模线性逆问题中Hessian矩阵向量乘法的效率和可扩展性。", "translation": "我们提出了一种高效且可扩展的算法，用于执行块托普利茨矩阵的矩阵向量乘法（“matvecs”）。这类矩阵对其块具有平移不变性，出现在解决由自治系统，特别是时不变系统控制的逆问题中。在本文中，我们考虑了从给定为偏微分方程（PDEs）形式的线性时不变动力系统的观测数据中推断未知参数的逆问题。无矩阵的牛顿共轭梯度法通常是解决这些逆问题的金标准，但它们需要对向量进行大量的Hessian作用。基于伴随的无矩阵Hessian矩阵向量乘法需要每次Hessian作用求解一对线性化前向/伴随PDE，这对于大规模逆问题可能代价过高。前向PDE问题的时间不变性导致离散化参数到可观测（p2o）映射的块托普利茨结构，该映射定义了PDE的输入（参数）到输出（可观测值）的映射。这种块托普利茨结构使我们能够利用两个关键特性：（1）p2o映射及其伴随的紧凑存储；（2）高效的基于快速傅里叶变换（FFT）的Hessian矩阵向量乘法。所提出的算法被映射到大型多GPU集群上，并在NVIDIA A100 GPU上实现了超过80%的峰值带宽。在多达48个A100 GPU上表现出出色的弱扩展性。对于目标问题，该实现可以在几分之一秒内执行Hessian矩阵向量乘法，这比通过前向/伴随PDE求解的传统无矩阵Hessian矩阵向量乘法快几个数量级。", "summary": "本文提出了一种高效、可扩展的块托普利茨矩阵向量乘法算法，该算法在解决由自治动力系统，特别是线性时不变系统控制的逆问题中非常有用。通过利用参数到可观测映射的块托普利茨结构，该算法实现了p2o映射的紧凑存储和高效的基于FFT的Hessian矩阵向量乘法。该算法在大型多GPU集群上实现了高性能和出色的扩展性，与传统方法相比，将Hessian矩阵向量乘法的计算速度提高了几个数量级，从而解决了大规模逆问题中计算成本过高的问题。", "keywords": "块托普利茨矩阵, FFT, GPU加速, 线性逆问题, Hessian矩阵向量乘法", "comments": "该论文的创新点在于利用了线性时不变动力系统产生的块托普利茨结构，并结合FFT和GPU加速，极大地提高了大规模逆问题中Hessian矩阵向量乘法的效率和可扩展性。其在NVIDIA A100 GPU上超过80%的峰值带宽利用率和出色的弱扩展性表明了其强大的工程实现能力和实用价值。"}}
{"id": "2507.18333", "title": "Remembering the Markov Property in Cooperative MARL", "authors": ["Kale-ab Abebe Tessera", "Leonard Hinckeldey", "Riccardo Zamboni", "David Abel", "Amos Storkey"], "categories": ["cs.LG", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      RLC Finding the Frame Workshop Camera-Ready, 8 pages", "url": "http://arxiv.org/abs/2507.18333v1", "summary": "Cooperative multi-agent reinforcement learning (MARL) is typically formalised\nas a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),\nwhere agents must reason about the environment and other agents' behaviour. In\npractice, current model-free MARL algorithms use simple recurrent function\napproximators to address the challenge of reasoning about others using partial\ninformation. In this position paper, we argue that the empirical success of\nthese methods is not due to effective Markov signal recovery, but rather to\nlearning simple conventions that bypass environment observations and memory.\nThrough a targeted case study, we show that co-adapting agents can learn\nbrittle conventions, which then fail when partnered with non-adaptive agents.\nCrucially, the same models can learn grounded policies when the task design\nnecessitates it, revealing that the issue is not a fundamental limitation of\nthe learning models but a failure of the benchmark design. Our analysis also\nsuggests that modern MARL environments may not adequately test the core\nassumptions of Dec-POMDPs. We therefore advocate for new cooperative\nenvironments built upon two core principles: (1) behaviours grounded in\nobservations and (2) memory-based reasoning about other agents, ensuring\nsuccess requires genuine skill rather than fragile, co-adapted agreements.", "comment": "RLC Finding the Frame Workshop Camera-Ready, 8 pages", "pdf_url": "http://arxiv.org/pdf/2507.18333v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "在合作多智能体强化学习中记住马尔可夫性质", "tldr": "本文认为当前合作多智能体强化学习算法的成功并非源于有效的马尔可夫信号恢复，而是学习了简单的约定。通过案例研究，我们发现这些约定可能很脆弱，并提出新的环境设计原则以测试真正的技能。", "motivation": "当前模型无关的合作多智能体强化学习（MARL）算法在实践中通过简单的循环函数逼近器来处理利用部分信息推断其他智能体行为的挑战。本文的动机是质疑这些方法经验成功的真正原因，并指出其可能并非由于有效的马尔可夫信号恢复，而是学习了脆弱的约定，以及现代MARL环境可能未能充分测试Dec-POMDP的核心假设。", "method": "本文是一篇立场论文（position paper），通过一个有针对性的案例研究（targeted case study）来论证其观点，并在此基础上进行分析和提出建议。", "result": "研究结果表明，现有方法的经验成功并非源于有效的马尔可夫信号恢复，而是学习了简单的约定，这些约定可以绕过环境观测和记忆。通过案例研究，发现共同适应的智能体可以学习到脆弱的约定，这些约定在与非适应性智能体合作时会失败。然而，当任务设计需要时，相同的模型可以学习到基于观测的策略，这表明问题不在于学习模型的根本局限性，而是基准设计的缺陷。分析还表明，现代MARL环境可能未能充分测试Dec-POMDP的核心假设。", "conclusion": "本文得出结论，现代合作多智能体强化学习算法的成功可能依赖于脆弱的约定而非真正的马尔可夫性质恢复。因此，作者倡导构建新的合作环境，这些环境应基于两个核心原则：行为基于观测和基于记忆的其他智能体推理，以确保成功需要真正的技能而非脆弱的共同适应性协议。", "translation": "合作多智能体强化学习（MARL）通常被形式化为分散式部分可观测马尔可夫决策过程（Dec-POMDP），其中智能体必须推断环境和其他智能体的行为。在实践中，当前的模型无关MARL算法使用简单的循环函数逼近器来解决利用部分信息推断其他智能体行为的挑战。在这篇立场论文中，我们认为这些方法的经验成功并非源于有效的马尔可夫信号恢复，而是学习了绕过环境观测和记忆的简单约定。通过一个有针对性的案例研究，我们发现共同适应的智能体可以学习到脆弱的约定，这些约定在与非适应性智能体合作时会失败。关键的是，当任务设计需要时，相同的模型可以学习到基于观测的策略，这表明问题并非学习模型的根本局限性，而是基准设计的失败。我们的分析还表明，现代MARL环境可能未能充分测试Dec-POMDP的核心假设。因此，我们提倡构建基于两个核心原则的新合作环境：（1）行为基于观测，（2）基于记忆的其他智能体推理，以确保成功需要真正的技能而不是脆弱的、共同适应的协议。", "summary": "本文是一篇立场论文，探讨了合作多智能体强化学习（MARL）中马尔可夫性质的问题。作者认为，当前MARL算法的成功并非源于有效的马尔可夫信号恢复，而是通过学习简单的、脆弱的约定来实现的。通过案例研究，论文展示了这些约定在与非适应性智能体合作时会失效，并指出问题在于现有基准环境的设计缺陷，未能充分测试Dec-POMDP的核心假设。因此，论文倡导设计新的合作环境，强调基于观测的行为和基于记忆的智能体推理，以促进真正技能的学习。", "keywords": "合作多智能体强化学习, 马尔可夫性质, Dec-POMDP, 基准环境, 脆弱约定", "comments": "这篇论文的创新之处在于它对当前合作MARL算法的成功原因提出了质疑，并指出其可能依赖于脆弱的“约定”而非真正的马尔可夫属性恢复。它强调了现有基准环境的局限性，并提出了构建更具挑战性、更能测试智能体“真正技能”的新环境的原则。其重要性在于，它促使研究者重新思考MARL的评估标准和环境设计，有助于推动该领域向更鲁棒、更泛化的智能体发展。论文的贡献在于其批判性分析和对未来研究方向的指导。"}}
{"id": "2507.18029", "title": "An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges", "authors": ["Xiang Echo Chen", "Wenhan Zhu", "Guoshuai Albert Shi", "Michael W. Godfrey"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18029v1", "summary": "The growing capabilities of generative AI (GenAI) have begun to reshape how\ngames are designed and developed, offering new tools for content creation,\ngameplay simulation, and design ideation. While prior research has explored\ntraditional uses of AI in games, such as controlling agents or generating\nprocedural content. There is limited empirical understanding of how GenAI is\nadopted by developers in real-world contexts, especially within the open-source\ncommunity. This study aims to explore how GenAI technologies are discussed,\nadopted, and integrated into open-source game development by analyzing issue\ndiscussions on GitHub. We investigate the tools, tasks, and challenges\nassociated with GenAI by comparing GenAI-related issues to those involving\ntraditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI\ndiffers from other approaches in terms of usage patterns, developer concerns,\nand integration practices. To address this objective, we construct a dataset of\nopen-source game repositories that discuss AI-related topics. We apply open\ncard sorting and thematic analysis to a stratified sample of GitHub issues,\nlabelling each by type and content. These annotations enable comparative\nanalysis across GenAI, TradAI, and NonAI groups, and provide insight into how\nGenAI is shaping the workflows and pain points of open-source game developers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18029v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "开放世界游戏开发中生成式AI采纳的实证研究：工具、任务与开发者挑战", "tldr": "本研究通过分析GitHub议题讨论，实证探究了生成式AI在开源游戏开发中的工具、任务和开发者挑战，并与传统AI及非AI方法进行比较。", "motivation": "尽管生成式AI（GenAI）正在重塑游戏设计和开发，但对于开发者在实际场景，尤其是在开源社区中如何采纳GenAI，缺乏实证理解。本研究旨在填补这一空白。", "method": "研究通过分析GitHub上的议题讨论来探索GenAI在开源游戏开发中的讨论、采纳和集成。具体方法包括：构建一个讨论AI相关主题的开源游戏仓库数据集，对分层抽样的GitHub议题应用开放式卡片分类和主题分析，并对GenAI、传统AI（TradAI）和非AI（NonAI）主题进行比较分析，以揭示GenAI在使用模式、开发者关注点和集成实践方面的差异。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "生成式AI（GenAI）日益增长的能力已开始重塑游戏的 设计与开发方式，为内容创作、游戏玩法模拟和设计构思提供了新工具。尽管先前的研究已探索了AI在游戏中的传统用途，例如控制代理或生成程序化内容，但对于开发者在实际情境中，特别是在开源社区中如何采纳GenAI，目前仍缺乏实证理解。本研究旨在通过分析GitHub上的议题讨论，探索GenAI技术在开源游戏开发中是如何被讨论、采纳和集成的。我们通过将GenAI相关议题与涉及传统AI（TradAI）和非AI主题的议题进行比较，调查与GenAI相关的工具、任务和挑战。我们的目标是揭示GenAI在使用模式、开发者关注点和集成实践方面与其它方法的不同。为实现这一目标，我们构建了一个讨论AI相关主题的开源游戏仓库数据集。我们对GitHub议题的分层样本应用开放式卡片分类和主题分析，并根据类型和内容对每个议题进行标注。这些标注有助于对GenAI、TradAI和非AI组进行比较分析，并深入了解GenAI如何塑造开源游戏开发者的工作流程和痛点。", "summary": "本研究旨在通过对GitHub议题讨论的实证分析，探究生成式AI（GenAI）在开源游戏开发中的采纳情况，包括所使用的工具、执行的任务以及开发者面临的挑战。研究构建了一个开源游戏仓库数据集，并对其中的GitHub议题应用开放式卡片分类和主题分析，通过比较GenAI、传统AI和非AI相关议题，以揭示GenAI在工作流程和痛点方面如何影响开源游戏开发者。", "keywords": "生成式AI采纳, 开源游戏开发, 开发者挑战, GitHub分析, 实证研究", "comments": "该研究的创新之处在于其专注于生成式AI在开源游戏开发这一特定领域的实证采纳情况，填补了现有研究的空白。通过分析GitHub上的实际开发者讨论，研究提供了一个独特的视角来理解GenAI的真实世界应用、挑战和与传统方法的差异。这种方法对于理解新兴技术在特定社区中的实际影响具有重要价值。"}}
{"id": "2507.10055", "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10055v2", "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10055v2", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-24", "AI": {"title_translation": "用于协作机器人的轻量级深度学习实时机器人系统手势识别", "tldr": "本文提出了一种基于轻量级深度学习的手势识别系统，用于自然控制协作机器人，实现了高精度和小模型尺寸，并成功应用于实际机器人系统。", "motivation": "为了实现直观的人机协作，消除对操纵杆、平板电脑或可穿戴传感器等额外设备的需求，直接和自然的交互至关重要。", "method": "研究人员提出了一种基于轻量级深度学习的手势识别系统。该模型识别八种不同的手势，拥有1,103个参数和22KB的紧凑尺寸，准确率为93.5%。为了进一步优化模型以在边缘设备上部署，使用TensorFlow Lite进行量化和剪枝，将最终模型尺寸减小到7KB。该系统在基于ROS2的实时机器人框架内，在Universal Robot UR5协作机器人上成功实现和测试。", "result": "该模型识别八种手势，参数仅1,103个，尺寸为22 KB，准确率达93.5%。经过量化和剪枝后，最终模型尺寸减小到7 KB。系统在Universal Robot UR5协作机器人上成功实现和测试，结果表明即使是极其轻量级的模型也能为协作机器人提供准确且响应迅速的基于手势的控制。", "conclusion": "即使是极其轻量级的模型也能为协作机器人提供准确且响应迅速的基于手势的控制，为受限环境中的自然人机交互开辟了新的可能性。", "translation": "直接和自然的交互对于直观的人机协作至关重要，消除了对操纵杆、平板电脑或可穿戴传感器等额外设备的需求。在本文中，我们提出了一种基于轻量级深度学习的手势识别系统，使人类能够自然高效地控制协作机器人。该模型识别八种不同的手势，参数仅1,103个，尺寸紧凑，为22 KB，准确率达到93.5%。为了进一步优化模型以在边缘设备上进行实际部署，我们使用TensorFlow Lite应用了量化和剪枝，将最终模型尺寸减小到仅7 KB。该系统在基于ROS2的实时机器人框架内，在Universal Robot UR5协作机器人上成功实现和测试。结果表明，即使是极其轻量级的模型也能为协作机器人提供准确且响应迅速的基于手势的控制，为受限环境中的自然人机交互开辟了新的可能性。", "summary": "本文介绍了一种基于轻量级深度学习的手势识别系统，旨在实现人与协作机器人的自然高效交互。该系统识别八种手势，模型参数少（1,103个），尺寸小（22KB，优化后仅7KB），并实现了93.5%的准确率。该系统已成功应用于实时机器人平台，证明了轻量级模型在协作机器人手势控制方面的可行性和有效性，为受限环境中的人机交互提供了新途径。", "keywords": "手势识别, 协作机器人, 轻量级深度学习, 实时系统, 人机交互", "comments": "该论文的创新点在于其专注于开发极其轻量级的深度学习模型，以实现实时手势识别，这对于资源受限的边缘设备和协作机器人应用至关重要。其将模型尺寸从22KB压缩到7KB的能力，同时保持高精度，是显著的成就。这为在实际工业和协作环境中部署低成本、高效的人机交互方案提供了新的可能性。"}}
{"id": "2507.18587", "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff", "authors": ["Jérôme Emery", "Ali Hasanzadeh Karkan", "Jean-François Frigon", "François Leduc-Primeau"], "categories": ["eess.SP", "cs.AI"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures. Accepted to the IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "url": "http://arxiv.org/abs/2507.18587v1", "summary": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.", "comment": "6 pages, 3 figures. Accepted to the IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.18587v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大规模MIMO预编码的基础模型，具有自适应的每用户速率-功率权衡", "tldr": "提出一个基于Transformer的大规模MIMO预编码基础模型，通过数据增强解决数据稀缺问题，实现低复杂度高性能的零样本部署。", "motivation": "深度学习在大规模MIMO预编码中表现出潜力，但其训练需要高质量的本地数据集，而这通常难以收集。", "method": "提出一个基于Transformer的基础模型用于大规模MIMO预编码，旨在最小化发射机能耗，同时动态适应每用户速率需求。为解决数据稀缺场景下的模型适应性问题，引入了一种数据增强方法，通过计算预训练特征提取器输出之间的余弦相似度来寻找与目标分布相似的训练样本。", "result": "在相同能耗下，所提出的基础模型的零样本部署显著优于零迫，并且在复杂度降低8倍的情况下接近加权最小均方误差的性能。", "conclusion": "该工作通过解决数据可用性和训练复杂性挑战，使得基于深度学习的解决方案得以实际应用。此外，动态配置每用户速率要求的能力可被更高层次的资源分配和调度算法利用，以更好地控制能效、频谱效率和公平性。", "translation": "深度学习（DL）由于其学习传播环境特征的能力，已成为大规模多输入多输出（mMIMO）系统中预编码的一种解决方案。然而，训练这种模型需要在部署地点收集高质量的本地数据集，这通常很困难。我们提出了一种基于Transformer的大规模MIMO预编码基础模型，旨在最小化发射机的能耗，同时动态适应每用户速率要求。在相同能耗下，所提出的基础模型的零样本部署显著优于零迫（zero forcing），并且在复杂度降低8倍的情况下接近加权最小均方误差（weighted minimum mean squared error）的性能。为了解决数据稀缺环境下的模型适应性问题，我们引入了一种数据增强方法，通过计算预训练特征提取器输出之间的余弦相似度来寻找与目标分布相似的训练样本。我们的工作通过解决数据可用性和训练复杂性的挑战，使得基于深度学习的解决方案得以实际应用。此外，动态配置每用户速率要求的能力可以被更高层次的资源分配和调度算法利用，以更好地控制能效、频谱效率和公平性。", "summary": "本文针对大规模MIMO预编码中深度学习模型训练所需高质量本地数据集难以获取的问题，提出了一种基于Transformer的基础模型。该模型旨在最小化发射机能耗并动态适应每用户速率需求。为应对数据稀缺，引入了基于余弦相似度的数据增强方法。实验结果表明，该模型在零样本部署下性能显著优于传统方法，且复杂度更低，为深度学习在实际通信系统中的应用提供了可行性。", "keywords": "大规模MIMO, 预编码, 深度学习, 基础模型, 数据增强", "comments": "这项工作具有创新性，它提出了一种基于Transformer的通用基础模型来解决大规模MIMO预编码中的数据稀缺和训练复杂性问题。通过引入数据增强方法，使得模型能够在数据有限的环境下有效部署，这对于推动深度学习在实际通信系统中的应用具有重要意义。其在性能和复杂度上的优势也显示了其潜力。"}}
{"id": "2507.18190", "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks", "authors": ["Keyu Wu", "Qianjin Yu", "Manlin Mei", "Ruiting Liu", "Jun Wang", "Kailai Zhang", "Yelun Bao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.18190v1", "summary": "Root Cause Analysis (RCA) in telecommunication networks is a critical task,\nyet it presents a formidable challenge for Artificial Intelligence (AI) due to\nits complex, graph-based reasoning requirements and the scarcity of realistic\nbenchmarks.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.18190v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TN-AutoRCA：电信网络中基于告警的自改进根因分析的基准构建与智能体框架", "tldr": "电信网络中的根因分析对AI来说很困难，因为推理复杂且缺乏真实基准。", "motivation": "电信网络中的根因分析（RCA）是一项关键任务，但由于其复杂的、基于图的推理需求以及真实基准的稀缺性，它对人工智能（AI）构成了严峻的挑战。", "method": "论文提出了一个用于电信网络中基于告警的自改进根因分析的基准构建和智能体框架（TN-AutoRCA）。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "电信网络中的根因分析（RCA）是一项关键任务，然而，由于其复杂的、基于图的推理需求以及真实基准的稀缺性，它对人工智能（AI）构成了严峻的挑战。", "summary": "本文介绍了TN-AutoRCA，这是一个针对电信网络中基于告警的自改进根因分析的基准构建和智能体框架。该研究旨在解决电信网络根因分析对AI而言的挑战，包括复杂的图基推理和缺乏真实基准的问题。", "keywords": "根因分析, 电信网络, 基准, 智能体框架, 自改进", "comments": "该论文聚焦于电信网络根因分析这一关键且具挑战性的领域，提出了构建基准和智能体框架的创新思路，有望推动AI在该领域的应用和发展，特别是在解决数据稀缺和推理复杂性方面。"}}
{"id": "2507.18006", "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling", "authors": ["Jingfeng Wu", "Yiyuan He", "Minxian Xu", "Xitong Gao", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.18006v1", "summary": "The rise of large language models (LLMs) has created new opportunities across\nvarious fields but has also introduced significant challenges in resource\nmanagement. Current LLM serving systems face a fundamental tension: balancing\nserving demands with limited resources while adapting to unpredictable traffic\npatterns. Static deployments lead to suboptimal resource utilization and\nperformance degradation under dynamic workloads. Furthermore, the high cost of\nadjusting instances hinders dynamic scaling, limiting the true potential of\nefficient LLM serving.\n  To address this, we propose CoCoServe, an elastic system that facilitates\ndynamic and fine-grained scaling. Its key innovation lies in the module-level\noperations for the replication and migration of LLM modules, such as decoder\nlayers and projections. Through a comprehensive analysis of the trade-offs\nassociated with these operations, we develop an auto-scaling mechanism that\ndynamically regulates module-level resource allocation and performance\noptimization, enabling a more cost-effective deployment of LLMs. Our evaluation\ndemonstrates that the scaling operations employed by CoCoServe exhibit\nexcellent scalability and can reduce costs by 46% while maintaining\navailability. Compared to state-of-the-art LLM serving systems (e.g., Hugging\nFace Transformers and vLLM), our approach reduces latency by 14%-75% and\nachieves 1.16x-4x throughput on average across different model sizes and\nworkloads.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.18006v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过动态模块伸缩释放细粒度LLM服务的潜力", "tldr": "CoCoServe是一个弹性系统，通过模块级操作实现LLM的细粒度动态伸缩，显著降低成本和延迟，提高吞吐量。", "motivation": "当前LLM服务系统在有限资源下平衡服务需求和适应不可预测的流量模式时面临挑战，静态部署导致资源利用率低下和性能下降，高昂的实例调整成本阻碍了动态伸缩，限制了LLM高效服务的潜力。", "method": "提出CoCoServe系统，其核心创新在于对LLM模块（如解码器层和投影）进行模块级复制和迁移操作。通过对这些操作的权衡分析，开发了一种自动伸缩机制，动态调节模块级资源分配和性能优化，从而实现更具成本效益的LLM部署。", "result": "CoCoServe的伸缩操作展现出卓越的伸缩性，可降低46%的成本并保持可用性。与现有LLM服务系统（如Hugging Face Transformers和vLLM）相比，延迟降低14%-75%，吞吐量平均提高1.16倍-4倍。", "conclusion": "CoCoServe通过其创新的模块级动态伸缩机制，显著提升了LLM服务的效率、成本效益和性能，解决了现有系统在资源管理和动态工作负载适应方面的核心挑战。", "translation": "大型语言模型（LLM）的兴起在各个领域创造了新的机遇，但也为资源管理带来了重大挑战。当前的LLM服务系统面临一个根本性的矛盾：在有限资源下平衡服务需求，同时适应不可预测的流量模式。静态部署导致资源利用率低下，并在动态工作负载下性能下降。此外，调整实例的高成本阻碍了动态伸缩，限制了高效LLM服务的真正潜力。\n为了解决这个问题，我们提出了CoCoServe，一个促进动态和细粒度伸缩的弹性系统。其关键创新在于LLM模块（如解码器层和投影）的模块级复制和迁移操作。通过对这些操作相关权衡的全面分析，我们开发了一种自动伸缩机制，动态调节模块级资源分配和性能优化，从而实现更具成本效益的LLM部署。我们的评估表明，CoCoServe采用的伸缩操作展现出卓越的伸缩性，可降低46%的成本同时保持可用性。与最先进的LLM服务系统（例如Hugging Face Transformers和vLLM）相比，我们的方法在不同模型大小和工作负载下，平均将延迟降低14%-75%，吞吐量提高1.16倍-4倍。", "summary": "该论文提出了CoCoServe，一个旨在优化大型语言模型（LLM）服务的弹性系统。针对现有LLM服务在资源管理和动态负载下的效率低下问题，CoCoServe引入了模块级（如解码器层）的动态复制和迁移机制，并结合自动伸缩算法来优化资源分配。实验结果表明，CoCoServe显著降低了服务成本和延迟，同时大幅提升了吞吐量，超越了现有主流LLM服务系统。", "keywords": "LLM服务, 动态伸缩, 模块级操作, 资源管理, 自动伸缩", "comments": "CoCoServe的创新之处在于其细粒度的模块级伸缩方法，这与传统基于实例的伸缩方式不同，能够更精确地管理LLM的计算资源。这种方法有效解决了LLM服务中资源利用率低和适应动态流量困难的问题，具有重要的实践意义和潜在影响。"}}
{"id": "2507.18033", "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18033v1", "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18033v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "OpenNav：基于多模态大语言模型的开放世界导航", "tldr": "OpenNav利用多模态大语言模型（MLLMs）实现开放世界导航，使机器人能够解释复杂指令并生成轨迹点，通过大型自动驾驶数据集和真实机器人验证了其鲁健性。", "motivation": "尽管最近取得进展，但弥合语言描述与开放世界中机器人实际动作之间的差距，而非仅仅调用有限的预定义运动原语，仍然是一个开放的挑战。本研究旨在使机器人能够解释和分解复杂的语言指令，最终合成一系列轨迹点，以完成给定开放集指令和开放集对象的各种导航任务。", "method": "该方法观察到多模态大语言模型（MLLMs）在处理自由形式语言指令时表现出强大的跨模态理解能力和鲁棒的场景理解。更重要的是，利用其代码生成能力，MLLMs可以与视觉-语言感知模型交互，生成组合式2D鸟瞰图价值地图，有效地将MLLMs的语义知识与地图的空间信息相结合，以增强机器人的空间理解。该方法利用大规模自动驾驶数据集（AVDs）验证了所提出的零样本视觉-语言导航框架在户外导航任务中的能力。", "result": "该方法展示了执行各种自由形式自然语言导航指令的能力，同时对目标检测错误和语言歧义保持鲁棒性（通过大规模自动驾驶数据集验证）。此外，该系统还在室内和室外场景中的Husky机器人上进行了验证，展示了其在现实世界中的鲁棒性和适用性。", "conclusion": "该研究提出的零样本视觉-语言导航框架，利用多模态大语言模型（MLLMs）的能力，成功使机器人能够解释和分解复杂的语言指令，从而在开放世界中完成多样化的导航任务。该框架在大型数据集和真实机器人上的验证证明了其有效性、鲁棒性以及在处理语言歧义和感知错误方面的能力，为开放世界机器人导航提供了实用的解决方案。", "translation": "预训练的大语言模型（LLMs）已经展示出强大的常识推理能力，使其在机器人导航和规划任务中具有广阔前景。然而，尽管最近取得了进展，但在开放世界中弥合语言描述与实际机器人动作之间的差距，而非仅仅调用有限的预定义运动原语，仍然是一个开放的挑战。在这项工作中，我们旨在使机器人能够解释和分解复杂的语言指令，最终合成一系列轨迹点，以完成给定开放集指令和开放集对象的各种导航任务。我们观察到多模态大语言模型（MLLMs）在处理自由形式语言指令时表现出强大的跨模态理解能力，展示出鲁棒的场景理解。更重要的是，利用其代码生成能力，MLLMs可以与视觉-语言感知模型交互，生成组合式2D鸟瞰图价值地图，有效地将MLLMs的语义知识与地图的空间信息相结合，以增强机器人的空间理解。为了进一步验证我们的方法，我们有效地利用大规模自动驾驶数据集（AVDs）来验证我们提出的零样本视觉-语言导航框架在户外导航任务中的能力，展示了其执行各种自由形式自然语言导航指令的能力，同时对目标检测错误和语言歧义保持鲁棒性。此外，我们还在室内和室外场景中的Husky机器人上验证了我们的系统，展示了其在现实世界中的鲁棒性和适用性。补充视频可在https://trailab.github.io/OpenNav-website/查看。", "summary": "OpenNav提出了一种创新的零样本视觉-语言导航框架，该框架利用多模态大语言模型（MLLMs）的能力，使机器人能够在开放世界中理解和执行复杂的自然语言导航指令。该方法通过MLLMs的跨模态理解和代码生成能力，结合视觉-语言感知模型，生成2D鸟瞰图价值地图，有效地融合了语义和空间信息。该框架在大型自动驾驶数据集和真实Husky机器人上进行了广泛验证，证明了其在执行多样化指令、同时对抗目标检测错误和语言歧义方面的鲁棒性和实用性。", "keywords": "开放世界导航, 多模态大语言模型, 机器人导航, 视觉-语言, 零样本", "comments": "本文创新性地将多模态大语言模型（MLLMs）的跨模态理解和代码生成能力应用于开放世界机器人导航，超越了传统的预定义运动原语。其通过价值地图整合语义知识与空间信息的能力，以及在大型数据集和真实机器人上的验证，突显了其在解决常见导航挑战方面的实际意义和鲁棒性。"}}
{"id": "2507.17801", "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Tech Report, 23 pages, 11 figures, 7 tables", "url": "http://arxiv.org/abs/2507.17801v1", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.17801v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Lumina-mGPT 2.0：独立自回归图像建模", "tldr": "Lumina-mGPT 2.0是一个从零开始训练的独立自回归模型，在图像生成质量和多任务能力上媲美甚至超越了最先进的扩散模型。", "motivation": "现有的图像生成方法依赖预训练组件或混合架构，限制了设计自由和许可。本文旨在通过一个从零开始训练的独立自回归模型来解决这些问题，并重新激活自回归范式在高质量图像生成中的潜力。", "method": "Lumina-mGPT 2.0是一个独立、仅解码器的自回归模型，完全从零开始训练。它采用统一的tokenization方案来处理多种任务，并结合了推理时缩放和推测性Jacobi采样等高效解码策略以提高质量和速度。", "result": "Lumina-mGPT 2.0在图像生成质量上与DALL-E 3和SANA等最先进的扩散模型相当，甚至在某些标准文本到图像基准测试（如GenEval, DPG）上超越了它们。它在Graph200K基准测试上展示了出色的多任务能力，包括主体驱动生成、图像编辑、可控合成和密集预测。", "conclusion": "Lumina-mGPT 2.0被定位为一个强大、灵活的统一多模态生成基础模型，证明了独立自回归模型在高质量图像生成和多任务处理方面的潜力。", "translation": "我们提出了Lumina-mGPT 2.0，一个独立的、仅解码器的自回归模型，它重新审视并振兴了用于高质量图像生成及其他领域的自回归范式。与依赖预训练组件或混合架构的现有方法不同，Lumina-mGPT 2.0完全从零开始训练，从而实现不受限制的架构设计和许可自由。它实现了与DALL-E 3和SANA等最先进的扩散模型相当的生成质量，同时保留了自回归建模固有的灵活性和组合性。我们统一的标记化方案使模型能够在单个生成框架内无缝处理广泛的任务——包括主体驱动生成、图像编辑、可控合成和密集预测。为了进一步提高可用性，我们结合了高效的解码策略，如推理时缩放和推测性Jacobi采样，分别用于提高质量和速度。对标准文本到图像基准测试（例如GenEval、DPG）的广泛评估表明，Lumina-mGPT 2.0不仅与基于扩散的模型相匹配，在某些情况下甚至超越了它们。此外，我们在Graph200K基准测试上证实了其多任务能力，原生的Lumina-mGPT 2.0表现异常出色。这些结果将Lumina-mGPT 2.0定位为一个强大、灵活的统一多模态生成基础模型。我们已在https://github.com/Alpha-VLLM/Lumina-mGPT-2.0发布了我们的训练细节、代码和模型。", "summary": "Lumina-mGPT 2.0是一个创新性的独立自回归图像生成模型，它从零开始训练，旨在超越现有依赖预训练组件或混合架构的限制。该模型在高质量图像生成方面达到了与顶级扩散模型相当的水平，并通过统一的tokenization方案实现了卓越的多任务处理能力，包括图像编辑和可控合成。其高效的解码策略进一步提升了性能，使其成为一个有潜力的统一多模态生成基础模型。", "keywords": "自回归模型, 图像生成, 基础模型, 多模态生成, Lumina-mGPT 2.0", "comments": "这篇论文的创新点在于提出了一个完全从零开始训练的独立自回归模型，打破了对预训练组件或混合架构的依赖，为图像生成领域带来了更大的设计自由和许可灵活性。其在生成质量上媲美甚至超越了扩散模型，并展示了强大的多任务处理能力，预示着自回归模型在统一多模态生成中的巨大潜力。"}}
{"id": "2507.17937", "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17937v1", "summary": "Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17937v1", "cate": "cs.SD", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "鲍勃的五彩纸屑：音乐和视频生成中的语音记忆攻击", "tldr": "研究发现，通过同音替换改变歌词的语音提示可以在音乐和文本到视频生成模型中触发训练数据的记忆，导致与原始内容高度相似的输出，引发版权和安全问题。", "motivation": "歌词到歌曲（LS2）生成模型对训练数据记忆的脆弱性仍未得到充分探索。", "method": "引入了对抗性语音提示（Adversarial PhoneTic Prompting, APT），这是一种通过同音替换（例如“mom's spaghetti”→“Bob's confetti”）在语义上改变歌词但保留其声学结构的新型攻击。", "result": "在LS2模型（如SUNO和YuE）中发现了强大的子词汇记忆形式，即使歌词被扭曲，模型也能再生与已知训练内容惊人相似的输出，并在CLAP、AudioJudge和CoverID等音频域指标上实现高相似度。这种漏洞在多种语言和流派中持续存在。更令人惊讶的是，发现仅语音改变的歌词就能触发文本到视频模型的视觉记忆，例如用语音修改的歌词提示Veo 3，模型重建了原始音乐视频的视觉元素（包括人物外观和场景构成），尽管提示中没有视觉线索。将此现象称为语音到视觉反刍。", "conclusion": "这些发现揭示了转录条件下的多模态生成中存在的关键漏洞：仅语音提示即可解锁记忆的视听内容，对现代生成系统中的版权、安全和内容来源提出了紧迫问题。", "translation": "歌词到歌曲（LS2）生成模型有望实现文本到端到端音乐合成，但其对训练数据记忆的脆弱性仍未得到充分探索。我们引入了对抗性语音提示（Adversarial PhoneTic Prompting, APT），这是一种新颖的攻击，通过同音替换（例如，Eminem著名的“mom's spaghetti”→“Bob's confetti”）在语义上改变歌词，同时保留其声学结构。尽管存在这些扭曲，我们发现了一种强大的子词汇记忆形式：SUNO和YuE等模型再生出与已知训练内容惊人相似的输出，在CLAP、AudioJudge和CoverID等音频域指标上实现了高相似度。这种漏洞在多种语言和流派中持续存在。更令人惊讶的是，我们发现仅语音改变的歌词就能触发文本到视频模型的视觉记忆。当用《Lose Yourself》中语音修改的歌词提示Veo 3时，模型重建了原始音乐视频的视觉元素——包括人物外观和场景构成——尽管提示中没有视觉线索。我们将此现象称为语音到视觉反刍。总而言之，这些发现揭示了转录条件下的多模态生成中存在的关键漏洞：仅语音提示即可解锁记忆的视听内容，对现代生成系统中的版权、安全和内容来源提出了紧迫问题。示例生成可在我们的演示页面（jrohsc.github.io/music_attack/）上获取。", "summary": "本文探讨了歌词到歌曲（LS2）和文本到视频生成模型中未被充分探索的训练数据记忆漏洞。研究者提出了对抗性语音提示（APT）攻击，通过同音替换在语义上改变歌词但保留其发音，发现LS2模型会再生出与训练数据高度相似的音频内容，且此漏洞跨语言和流派。更进一步，研究发现仅语音修改的歌词就能触发文本到视频模型对原始视频视觉元素的记忆。这些发现揭示了多模态生成系统中存在的关键漏洞，即纯粹的语音提示即可解锁记忆的视听内容，对版权、安全和内容溯源提出了严峻挑战。", "keywords": "语音记忆攻击, 生成模型, 音乐生成, 视频生成, 对抗性语音提示", "comments": "这项研究通过引入APT攻击，揭示了多模态生成模型中深层次的“语音记忆”漏洞，特别是“语音到视觉反刍”的发现极具创新性。它不仅强调了生成AI在版权和内容来源方面的潜在风险，也为未来模型开发提供了重要的安全考量。"}}
{"id": "2507.17971", "title": "Benchmarking of Deep Learning Methods for Generic MRI Multi-OrganAbdominal Segmentation", "authors": ["Deepa Krishnaswamy", "Cosmin Ciausu", "Steve Pieper", "Ron Kikinis", "Benjamin Billot", "Andrey Fedorov"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17971v1", "summary": "Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17971v1", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "深度学习方法在通用MRI多器官腹部分割中的基准测试", "tldr": "本文对三种最先进的开源深度学习模型（MRSegmentator、MRISegmentator-Abdomen、TotalSegmentator MRI）在MRI腹部多器官分割任务上进行了全面基准测试，并引入了基于CT分割数据训练的新模型ABDSynth。结果显示MRSegmentator表现最佳且泛化能力最强，而ABDSynth在标注预算有限时是一个可行的替代方案。", "motivation": "由于固有的信号变异性和训练数据集标注所需的大量工作，磁共振成像（MRI）分割比计算机断层扫描（CT）分割更具挑战性。现有方法在有限的MRI序列集上进行训练，这可能限制了它们的泛化能力。因此，需要对MRI腹部分割工具的性能和泛化能力进行全面评估。", "method": "本文对三种最先进的开源模型（MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI）进行了全面的基准测试。此外，还引入并评估了ABDSynth，一个纯粹基于广泛可用的CT分割数据训练的SynthSeg模型（不使用真实MRI图像）。通过利用三个公开数据集（在训练过程中未被任何评估方法使用），评估了这些方法的准确性和泛化能力，这些数据集涵盖了主要制造商、五种MRI序列以及各种受试者条件、体素分辨率和视野。", "result": "MRSegmentator实现了最佳性能和最强的泛化能力。相比之下，ABDSynth的结果略逊一筹，但其对训练数据要求较低，使其在标注预算有限时成为一种替代方案。", "conclusion": "MRSegmentator是MRI腹部多器官分割中性能最佳且泛化能力最强的模型。ABDSynth提供了一种创新的解决方案，通过利用CT数据来缓解MRI数据标注的负担，使其在资源受限的情况下具有实用价值。", "translation": "深度学习的最新进展已催生出用于腹部计算机断层扫描（CT）分割的强大自动化工具。然而，由于固有的信号变异性和标注训练数据集所需的大量工作，磁共振成像（MRI）的分割更具挑战性。因此，现有方法在有限的MRI序列集上进行训练，这可能会限制它们的泛化能力。为了表征MRI腹部分割工具的现状，我们在此对三种最先进的开源模型进行了全面基准测试：MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI。由于这些模型是使用劳动密集型手动标注周期进行训练的，我们还引入并评估了ABDSynth，一个纯粹基于广泛可用的CT分割数据（无真实图像）训练的SynthSeg模型。更普遍地，我们通过利用三个公共数据集（在训练过程中未被任何评估方法看到）来评估准确性和泛化能力，这些数据集涵盖了所有主要制造商、五种MRI序列以及各种受试者条件、体素分辨率和视野。我们的结果表明，MRSegmentator取得了最佳性能，并且泛化能力最强。相比之下，ABDSynth的结果略逊一筹，但其对训练数据要求较低，使其在标注预算有限时成为一种替代方案。评估代码和数据集可在https://github.com/deepakri201/AbdoBench上获取，以供未来基准测试，同时还提供了ABDSynth的推理代码和权重。", "summary": "本文对用于MRI腹部多器官分割的四种深度学习模型进行了全面的基准测试。研究评估了三种现有最先进的开源模型（MRSegmentator、MRISegmentator-Abdomen、TotalSegmentator MRI）的性能和泛化能力，并引入了ABDSynth，一个利用CT分割数据训练的新型模型，以减少对昂贵MRI标注的需求。通过在多个公共数据集上进行评估，结果显示MRSegmentator表现出最佳性能和泛化能力，而ABDSynth在训练数据标注预算有限时提供了一个可行的替代方案。", "keywords": "深度学习, MRI分割, 腹部器官, 基准测试, 泛化能力", "comments": "该论文解决了MRI分割中数据变异性和高昂标注成本的关键挑战。通过对现有SOTA模型进行全面基准测试，它为研究人员和临床医生提供了有价值的指导。ABDSynth的引入是一个显著的创新点，因为它提出了一种利用更易获得的CT数据来辅助MRI分割训练的方法，从而大大降低了标注负担，提高了方法的可及性。这项工作对推动MRI腹部多器官分割领域的发展具有重要意义。"}}
{"id": "2507.08513", "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation", "authors": ["Liu He", "Xiao Zeng", "Yizhi Song", "Albert Y. C. Chen", "Lu Xia", "Shashwat Verma", "Sankalp Dayal", "Min Sun", "Cheng-Hao Kuo", "Daniel Aliaga"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08513v2", "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08513v2", "cate": "cs.GR", "date": "2025-07-11", "updated": "2025-07-23", "AI": {"title_translation": "通过大规模三维视觉指令数据集生成推进多模态大语言模型", "tldr": "该研究提出了一种合成生成管道，用于创建大规模三维视觉指令数据集Ultimate3D，以解决多模态大语言模型在捕获相机-物体关系方面的不足，并通过该数据集显著提升了模型性能。", "motivation": "多模态大语言模型（MLLMs）在准确捕获相机-物体关系（特别是物体方向、相机视角和相机镜头）方面存在困难。这源于现有MLLMs在训练时使用的图像中相机-物体关系多样性有限且缺乏相应的文本描述。", "method": "提出了一种合成生成管道来创建大规模三维视觉指令数据集。该框架以三维资产为输入，利用渲染和基于扩散的图像生成模型创建保留精确相机-物体关系的照片级真实图像。此外，使用大语言模型（LLMs）生成文本提示，用于指导视觉指令微调和控制图像生成。", "result": "创建了Ultimate3D数据集，包含24万个带有精确相机-物体标注的视觉问答对及相应的基准测试。在所提出的数据集上进行微调的MLLMs在相机-物体关系识别任务上比商业模型平均准确率提高了33.4%。", "conclusion": "所提出的数据集、代码和基准将有助于多模态大语言模型的广泛应用，显著提高了模型在相机-物体关系识别任务上的性能。", "translation": "多模态大语言模型（MLLMs）在准确捕获相机-物体关系方面存在困难，特别是对于物体方向、相机视角和相机镜头。这源于现有MLLMs在训练时使用的图像中相机-物体关系多样性有限且缺乏相应的文本描述。为了解决这个问题，我们提出了一种合成生成管道，以创建大规模三维视觉指令数据集。我们的框架以三维资产作为输入，并使用渲染和基于扩散的图像生成模型来创建保留精确相机-物体关系的照片级真实图像。此外，大语言模型（LLMs）用于生成文本提示，以指导视觉指令微调和控制图像生成。我们创建了Ultimate3D，一个包含24万个带有精确相机-物体标注的视觉问答（VQA）数据集，以及相应的基准测试。在所提出的数据集上进行微调的MLLMs在相机-物体关系识别任务上比商业模型表现出大幅领先，平均准确率提高了33.4%。我们的代码、数据集和基准将为广泛的MLLM应用做出贡献。", "summary": "本研究旨在解决多模态大语言模型（MLLMs）在精确理解相机与物体关系方面的挑战，这些模型因训练数据中相机-物体关系多样性不足而受限。为此，论文提出了一种创新的合成生成管道，利用三维资产、渲染技术和扩散模型来生成大量包含精确相机-物体关系的逼真图像。同时，大语言模型被用于生成指导视觉指令微调的文本提示。通过此方法，研究团队构建了名为Ultimate3D的大规模数据集（包含24万个视觉问答对），并建立了一个基准测试。实验结果表明，使用Ultimate3D数据集进行微调的MLLMs在相机-物体关系识别任务上，相比商业模型实现了33.4%的显著准确率提升。该研究的成果（包括代码、数据集和基准）有望推动MLLM在更广泛应用中的发展。", "keywords": "多模态大语言模型, 三维视觉指令, 数据集生成, 相机-物体关系, Ultimate3D", "comments": "这项工作通过引入大规模合成3D视觉指令数据集，有效解决了MLLMs在理解复杂相机-物体关系方面的核心局限性。其创新点在于结合了3D渲染、扩散模型和LLM的优势来生成高质量、多样化的训练数据。33.4%的显著性能提升凸显了数据质量和多样性对MLLM能力的重要性。该数据集和方法的开源将对未来MLLM的研究和应用产生积极影响。"}}
{"id": "2410.08989", "title": "Zeroth-Order Fine-Tuning of LLMs in Random Subspaces", "authors": ["Ziming Yu", "Pan Zhou", "Sike Wang", "Jia Li", "Mi Tian", "Hua Huang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICCV 2025 camera-ready version", "url": "http://arxiv.org/abs/2410.08989v3", "summary": "Fine-tuning Large Language Models (LLMs) has proven effective for a variety\nof downstream tasks. However, as LLMs grow in size, the memory demands for\nbackpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization\nmethods offer a memory-efficient alternative by using forward passes to\nestimate gradients, but the variance of gradient estimates typically scales\nlinearly with the model's parameter dimension$\\unicode{x2013}$a significant\nissue for LLMs. In this paper, we propose the random Subspace Zeroth-order\n(SubZero) optimization to address the challenges posed by LLMs' high\ndimensionality. We introduce a low-rank perturbation tailored for LLMs that\nsignificantly reduces memory consumption while improving training performance.\nAdditionally, we prove that our gradient estimation closely approximates the\nbackpropagation gradient, exhibits lower variance than traditional ZO methods,\nand ensures convergence when combined with SGD. Experimental results show that\nSubZero enhances fine-tuning performance and achieves faster convergence\ncompared to standard ZO approaches like MeZO across various language modeling\ntasks. Code is available at https://github.com/zimingyy/SubZero.", "comment": "ICCV 2025 camera-ready version", "pdf_url": "http://arxiv.org/pdf/2410.08989v3", "cate": "cs.LG", "date": "2024-10-11", "updated": "2025-07-24", "AI": {"title_translation": "随机子空间中LLMs的零阶微调", "tldr": "SubZero通过在随机子空间中进行零阶优化，解决了LLMs微调的内存限制和高维度问题，并提升了性能。", "motivation": "微调大型语言模型（LLMs）对下游任务有效，但随着LLMs规模增大，反向传播的内存需求变得过高。零阶（ZO）优化方法虽内存效率高，但梯度估计的方差随模型参数维度线性增长，这对于高维的LLMs是重大问题。", "method": "本文提出了随机子空间零阶（SubZero）优化方法来解决LLMs高维度带来的挑战。引入了一种为LLMs量身定制的低秩扰动，显著减少内存消耗并提高训练性能。理论证明其梯度估计近似反向传播梯度，方差低于传统ZO方法，并结合SGD确保收敛。", "result": "实验结果表明，与MeZO等标准ZO方法相比，SubZero在各种语言建模任务中增强了微调性能并实现了更快的收敛。", "conclusion": "SubZero提供了一种内存高效且性能优越的LLMs零阶微调方法，有效解决了高维度带来的挑战。", "translation": "大型语言模型（LLMs）的微调已被证明对各种下游任务有效。然而，随着LLMs规模的增大，反向传播的内存需求变得越来越高昂。零阶（ZO）优化方法通过使用前向传播来估计梯度，提供了一种内存高效的替代方案，但梯度估计的方差通常随模型的参数维度线性增长——这对LLMs来说是一个显著的问题。在本文中，我们提出了随机子空间零阶（SubZero）优化方法来解决LLMs高维度带来的挑战。我们引入了一种为LLMs量身定制的低秩扰动，显著减少了内存消耗，同时提高了训练性能。此外，我们证明了我们的梯度估计与反向传播梯度非常接近，比传统ZO方法具有更低的方差，并结合SGD确保收敛。实验结果表明，与MeZO等标准ZO方法相比，SubZero在各种语言建模任务中增强了微调性能并实现了更快的收敛。代码可在https://github.com/zimingyy/SubZero获取。", "summary": "本文提出了一种名为SubZero的随机子空间零阶优化方法，旨在解决大型语言模型（LLMs）微调过程中因模型规模增大导致的反向传播内存需求过高以及零阶优化方法梯度估计方差大的问题。SubZero通过引入针对LLMs的低秩扰动，显著降低了内存消耗并提升了训练性能。理论上，该方法证明了其梯度估计与反向传播梯度接近，且方差低于传统零阶方法，并能确保收SGD收敛。实验结果表明，SubZero在多种语言建模任务中提升了微调表现并加速了收敛速度，优于现有标准零阶方法。", "keywords": "零阶优化, 大型语言模型, 微调, 随机子空间, 低秩扰动", "comments": "SubZero的创新点在于将零阶优化与随机子空间以及低秩扰动相结合，有效解决了LLMs高维度带来的梯度估计方差大和内存效率低的问题。其理论证明和实验结果都支持了该方法的有效性，对于推动大型模型在资源受限环境下的应用具有重要意义。"}}
{"id": "2507.17777", "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics", "authors": ["Theofanis Aravanis", "Grigorios Chrimatopoulos", "Mohammad Ferdows", "Michalis Xenos", "Efstratios Em Tzirtzilakis"], "categories": ["cs.AI", "76A02"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This research was implemented in the framework of the Action \"Flagship actions in interdisciplinary scientific fields with a special focus on the productive fabric'', which is implemented through the National Recovery and Resilience Fund Greece 2.0 and funded by the European Union--NextGenerationEU (Project ID: TAEDR-0535983)", "url": "http://arxiv.org/abs/2507.17777v1", "summary": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial.", "comment": "This research was implemented in the framework of the Action\n  \"Flagship actions in interdisciplinary scientific fields with a special focus\n  on the productive fabric'', which is implemented through the National\n  Recovery and Resilience Fund Greece 2.0 and funded by the European\n  Union--NextGenerationEU (Project ID: TAEDR-0535983)", "pdf_url": "http://arxiv.org/pdf/2507.17777v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "ASP辅助的符号回归：揭示流体力学中的隐藏物理", "tldr": "本研究将符号回归（SR）应用于流体力学，以揭示可解释的物理方程，并通过结合答案集编程（ASP）确保模型不仅准确而且符合物理原理。", "motivation": "在流体力学中，理解潜在的流动物理与准确预测同等重要，这促使研究者应用符号回归来揭示复杂物理系统中的可解释数学关系。", "method": "本研究采用符号回归（SR）方法，利用PySR库直接从数值模拟数据中推导出紧凑的符号方程。此外，提出了一种创新方法，将SR与答案集编程（ASP）的知识表示框架相结合，形成混合SR/ASP框架，以确保生成的符号表达式符合物理原理。", "result": "研究成功从数值模拟数据中推导出了紧凑的符号方程，这些方程不仅近似于研究流体中观察到的抛物线速度分布和压降，而且与文献中的解析解完美吻合。混合SR/ASP框架确保了SR生成的符号表达式在统计上准确且物理上合理。", "conclusion": "本研究强调了符号回归将复杂流动行为简化为简洁、可解释方程的能力，以及知识表示方法在提高数据驱动SR模型可靠性和与领域原理对齐方面的潜力。", "translation": "与通常被批评为“黑箱”的传统机器学习（ML）方法不同，符号回归（SR）作为一种强大的工具，能够揭示复杂物理系统中可解释的数学关系，且无需对模型结构进行先验假设。鉴于在流体力学中，对底层流动物理的理解与准确预测同等重要，本研究将SR应用于模拟矩形通道中基本的三维（3D）不可压缩层流，重点关注（轴向）速度和压力场。通过使用PySR库，直接从数值模拟数据中推导出了紧凑的符号方程，揭示了流动动力学的关键特征。这些方程不仅近似于所研究流体中观察到的抛物线速度分布和压降，而且与文献中的解析解完美吻合。此外，我们提出了一种创新方法，将SR与答案集编程（ASP）的知识表示框架相结合，将SR的生成能力与ASP的声明式推理优势相结合。所提出的混合SR/ASP框架确保SR生成的符号表达式不仅统计准确，而且物理上合理，符合领域特定原则。总的来说，本研究突出了两大关键贡献：SR将复杂流动行为简化为简洁、可解释方程的能力，以及知识表示方法在提高数据驱动SR模型可靠性和与领域原理对齐方面的潜力。对所考察的3D通道流的见解为将此类混合方法整合到高效框架中铺平了道路，[...]在这些框架中，可解释的预测和实时数据分析至关重要。", "summary": "本研究利用符号回归（SR）来揭示流体力学中三维不可压缩层流的隐藏物理规律，直接从数值模拟数据中推导出可解释的数学方程。研究发现，这些方程能准确描述速度分布和压降，并与解析解吻合。此外，论文提出了一种将SR与答案集编程（ASP）结合的创新混合框架，以确保生成的符号表达式不仅统计准确，而且符合物理原理。这突出了SR在简化复杂流动行为方面的能力以及知识表示方法在提高数据驱动模型可靠性方面的潜力。", "keywords": "符号回归, 流体力学, 答案集编程, 隐藏物理, 可解释AI", "comments": "该研究创新性地将符号回归（SR）与答案集编程（ASP）相结合，解决了传统机器学习“黑箱”问题，并确保了数据驱动模型在物理上的合理性，这对于流体力学等需要可解释性和物理一致性的领域至关重要。其揭示隐藏物理规律的能力具有重要意义。"}}
{"id": "2507.18502", "title": "Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces", "authors": ["Sait Sovukluk", "Grazia Zambella", "Tobias Egle", "Christian Ott"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication in 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025). - Link to video: this https URL", "url": "http://arxiv.org/abs/2507.18502v1", "summary": "This paper studies the experimental comparison of two different whole-body\ncontrol formulations for humanoid robots: inverse dynamics whole-body control\n(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers\nfundamentally differ from each other as the first is formulated in task\nacceleration space and the latter is in task force space with passivity\nconsiderations. Even though both control methods predict stability under ideal\nconditions in closed-loop dynamics, their robustness against joint friction,\nsensor noise, unmodeled external disturbances, and non-perfect contact\nconditions is not evident. Therefore, we analyze and experimentally compare the\ntwo controllers on a humanoid robot platform through swing foot position and\norientation control, squatting with and without unmodeled additional weights,\nand jumping. We also relate the observed performance and characteristic\ndifferences with the controller formulations and highlight each controller's\nadvantages and disadvantages.", "comment": "This paper has been accepted for publication in 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025). -\n  Link to video: https://youtu.be/Nfm50ycz-FU", "pdf_url": "http://arxiv.org/pdf/2507.18502v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "人形机器人任务加速度空间与任务力空间全身控制公式的实验比较", "tldr": "本文实验比较了人形机器人两种全身控制（ID-WBC和PB-WBC）在不同任务下的性能和鲁棒性，并分析了其优缺点。", "motivation": "尽管逆动力学全身控制（ID-WBC）和基于无源性的全身控制（PB-WBC）两种全身控制方法在理想条件下预测稳定，但它们在面对关节摩擦、传感器噪声、未建模外部干扰和非完美接触条件时的鲁棒性尚不明确。", "method": "通过在人形机器人平台上进行摆动脚位置和姿态控制、带/不带额外重量的深蹲以及跳跃等实验，对逆动力学全身控制（ID-WBC）和基于无源性的全身控制（PB-WBC）进行了分析和实验比较。", "result": "论文将观察到的性能和特性差异与控制器公式相关联，并强调了每种控制器的优缺点。具体实验结果未在摘要中详细说明。", "conclusion": "通过实验比较，揭示了两种全身控制方法（ID-WBC和PB-WBC）在实际条件下的性能和鲁棒性差异，并指出了各自的优缺点。", "translation": "本文研究了人形机器人两种不同全身控制公式的实验比较：逆动力学全身控制（ID-WBC）和基于无源性的全身控制（PB-WBC）。这两种控制器根本上不同，前者在任务加速度空间中制定，后者在任务力空间中制定并考虑了无源性。尽管两种控制方法在闭环动力学中预测在理想条件下具有稳定性，但它们对关节摩擦、传感器噪声、未建模的外部干扰和不完美的接触条件下的鲁棒性尚不明显。因此，我们通过摆动脚位置和姿态控制、带和不带未建模附加重量的深蹲以及跳跃，在人形机器人平台上分析并实验比较了这两种控制器。我们还将观察到的性能和特性差异与控制器公式相关联，并强调了每种控制器的优点和缺点。", "summary": "本文对人形机器人常用的两种全身控制方法——逆动力学全身控制（ID-WBC）和基于无源性的全身控制（PB-WBC）进行了实验比较。研究旨在评估这两种在任务加速度空间和任务力空间中定义的控制器在面对关节摩擦、传感器噪声等实际干扰时的鲁棒性。实验在人形机器人上通过摆动脚控制、深蹲和跳跃等任务进行，最终分析并突出了两种控制器的优缺点。", "keywords": "全身控制, 人形机器人, 逆动力学, 无源性控制, 实验比较", "comments": "该论文的创新点在于对两种理论上稳定的全身控制方法进行了实际机器人平台上的实验比较，特别关注了它们在非理想条件下的鲁棒性。这对于人形机器人在实际复杂环境中部署具有重要意义，因为它提供了关于控制器选择的实用见解。"}}
{"id": "2507.18071", "title": "Group Sequence Policy Optimization", "authors": ["Chujie Zheng", "Shixuan Liu", "Mingze Li", "Xiong-Hui Chen", "Bowen Yu", "Chang Gao", "Kai Dang", "Yuqiong Liu", "Rui Men", "An Yang", "Jingren Zhou", "Junyang Lin"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18071v1", "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18071v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "组序列策略优化", "tldr": "本文提出了组序列策略优化（GSPO），一种用于训练大型语言模型的稳定、高效、高性能强化学习算法，其通过序列级重要性比率和优化，在训练效率和性能上超越了现有算法。", "motivation": "传统的强化学习算法在训练大型语言模型时，采用基于token级别的重要性比率，这可能导致不稳定或效率低下。本文旨在提出一种更稳定、高效、高性能的强化学习算法来解决这些问题。", "method": "本文引入了组序列策略优化（GSPO）算法。与以往采用token级重要性比率的算法不同，GSPO基于序列似然定义重要性比率，并执行序列级裁剪、奖励和优化。", "result": "GSPO与GRPO算法相比，实现了卓越的训练效率和性能。它显著稳定了专家混合（MoE）强化学习训练，并具有简化强化学习基础设施设计的潜力。GSPO的这些优点促进了最新Qwen3模型的显著改进。", "conclusion": "GSPO作为一种基于序列级优化的强化学习算法，在大型语言模型训练中展现出卓越的稳定性、效率和性能，并已成功应用于Qwen3模型，预示着其在简化RL基础设施设计方面的巨大潜力。", "translation": "本文介绍了组序列策略优化（GSPO），这是一种用于训练大型语言模型的稳定、高效、高性能强化学习算法。与以往采用token级重要性比率的算法不同，GSPO基于序列似然定义重要性比率，并执行序列级裁剪、奖励和优化。我们证明了GSPO与GRPO算法相比，实现了卓越的训练效率和性能，显著稳定了专家混合（MoE）强化学习训练，并具有简化强化学习基础设施设计的潜力。GSPO的这些优点促进了最新Qwen3模型的显著改进。", "summary": "本文提出了一种名为组序列策略优化（GSPO）的强化学习算法，专为大型语言模型训练设计。GSPO通过引入序列级重要性比率和优化，解决了传统token级方法的问题，显著提高了训练的稳定性、效率和性能。实验证明，GSPO优于GRPO，并能有效稳定MoE模型训练，同时简化RL基础设施，其成功应用于Qwen3模型证明了其有效性。", "keywords": "强化学习, 大型语言模型, 序列策略优化, MoE, Qwen3", "comments": "GSPO的创新点在于从传统的token级优化转向序列级优化，这可能更好地捕捉语言模型的整体生成特性，从而带来更高的稳定性和性能。其对MoE模型训练的稳定作用尤其重要，因为MoE模型在训练中常面临挑战。此外，简化RL基础设施的潜力也预示着该算法在实际部署中的巨大价值。"}}
{"id": "2507.17799", "title": "A Concept-based approach to Voice Disorder Detection", "authors": ["Davide Ghia", "Gabriele Ciravegna", "Alkis Koudounas", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17799v1", "summary": "Voice disorders affect a significant portion of the population, and the\nability to diagnose them using automated, non-invasive techniques would\nrepresent a substantial advancement in healthcare, improving the quality of\nlife of patients. Recent studies have demonstrated that artificial intelligence\nmodels, particularly Deep Neural Networks (DNNs), can effectively address this\ntask. However, due to their complexity, the decision-making process of such\nmodels often remain opaque, limiting their trustworthiness in clinical\ncontexts. This paper investigates an alternative approach based on Explainable\nAI (XAI), a field that aims to improve the interpretability of DNNs by\nproviding different forms of explanations. Specifically, this works focuses on\nconcept-based models such as Concept Bottleneck Model (CBM) and Concept\nEmbedding Model (CEM) and how they can achieve performance comparable to\ntraditional deep learning methods, while offering a more transparent and\ninterpretable decision framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17799v1", "cate": "eess.AS", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于概念的语音障碍检测方法", "tldr": "本文提出一种基于可解释AI（概念瓶颈模型和概念嵌入模型）的语音障碍检测方法，旨在提高深度学习模型在临床应用中的透明度和可信度，同时保持相似的性能。", "motivation": "语音障碍影响大量人口，自动化诊断能显著改善医疗。然而，现有深度神经网络模型虽然有效，但其决策过程不透明，限制了其在临床环境中的可信度。", "method": "本文研究了一种基于可解释人工智能（XAI）的替代方法，特别是概念型模型，如概念瓶颈模型（CBM）和概念嵌入模型（CEM），旨在提高深度神经网络的可解释性。", "result": "本文探讨了概念型模型如何能在提供更透明和可解释的决策框架的同时，实现与传统深度学习方法相当的性能。", "conclusion": "基于概念的可解释AI模型为语音障碍检测提供了一种有前景的替代方案，有望在临床环境中提高AI诊断的可信度和应用。", "translation": "语音障碍影响着相当一部分人口，使用自动化、非侵入性技术诊断它们将代表医疗保健领域的一项重大进步，改善患者的生活质量。最近的研究表明，人工智能模型，特别是深度神经网络（DNN），可以有效地解决这项任务。然而，由于其复杂性，这些模型的决策过程通常不透明，限制了它们在临床环境中的可信度。本文研究了一种基于可解释人工智能（XAI）的替代方法，XAI是一个旨在通过提供不同形式的解释来提高DNN可解释性的领域。具体而言，这项工作侧重于概念型模型，如概念瓶颈模型（CBM）和概念嵌入模型（CEM），以及它们如何在提供更透明和可解释的决策框架的同时，实现与传统深度学习方法相当的性能。", "summary": "本文针对深度神经网络在语音障碍自动化诊断中透明度不足的问题，提出并研究了一种基于可解释人工智能（XAI）的概念型模型方法。通过探索概念瓶颈模型（CBM）和概念嵌入模型（CEM），该研究旨在证明这些模型在提供可解释决策的同时，能够达到与传统深度学习方法相媲美的诊断性能，从而提高AI在临床应用中的可信度。", "keywords": "语音障碍检测, 可解释人工智能, 概念型模型, 深度学习, 临床应用", "comments": "这篇论文的创新点在于将可解释人工智能（XAI）应用于语音障碍检测，解决了深度学习模型在医疗领域中“黑箱”决策的问题。通过引入概念型模型，它不仅关注了诊断的准确性，更强调了模型决策过程的透明度和可信度，这对于AI在临床环境中的实际部署至关重要。其重要性在于，为医疗AI提供了一种更易于理解和信任的路径，有助于弥合技术与临床实践之间的鸿沟。"}}
{"id": "2507.18455", "title": "LLM-based Embedders for Prior Case Retrieval", "authors": ["Damith Premasiri", "Tharindu Ranasinghe", "Ruslan Mitkov"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted in Recent Advancements in Natural Language Processing (RANLP 2025) conference", "url": "http://arxiv.org/abs/2507.18455v1", "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models.", "comment": "Accepted in Recent Advancements in Natural Language Processing (RANLP\n  2025) conference", "pdf_url": "http://arxiv.org/pdf/2507.18455v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于LLM的先例检索嵌入器", "tldr": "本研究利用基于LLM的文本嵌入器进行先例检索，解决了传统深度学习方法在处理长法律文本和缺乏训练数据方面的挑战，并在基准数据集上超越了传统和监督模型。", "motivation": "在普通法系统中，法律专业人士依赖先例，但案件量巨大，有效检索变得至关重要。传统的先例检索方法（如BM25）仍占主导地位，而最先进的深度学习信息检索方法在先例检索中面临两大挑战：1) 法律文本过长导致基于BERT的模型截断或分割时信息丢失；2) 法律训练数据缺乏，难以有效训练深度学习模型。本研究旨在解决这些挑战。", "method": "本研究通过利用基于LLM的文本嵌入器来解决先例检索中的挑战。这些嵌入器支持更长的输入长度，并且以无监督方式使用，因此不需要训练数据，从而同时解决了长文本限制和数据缺乏的问题。", "result": "在四个先例检索基准数据集上评估了最先进的基于LLM的文本嵌入器，结果表明它们优于BM25和有监督的基于Transformer的模型。", "conclusion": "基于LLM的文本嵌入器在先例检索任务中表现出色，能够有效克服传统深度学习方法在处理长法律文本和训练数据不足方面的局限性。", "translation": "在普通法体系中，律师和法官等法律专业人士依赖判例来构建他们的论点。随着时间的推移，案件数量大幅增长，有效检索先前的案件变得至关重要。先例检索（PCR）是一项信息检索（IR）任务，旨在从大量潜在候选案例中自动识别与特定查询最相关的法院案例。尽管信息检索方法在过去几年中经历了多次范式转变，但绝大多数PCR方法仍然依赖传统的IR方法，例如BM25。最先进的深度学习IR方法在PCR中未能成功，原因在于两个关键挑战：i. 冗长的法律文本限制；当使用强大的基于BERT的Transformer模型时，输入文本长度存在限制，这不可避免地需要通过截断或分割来缩短输入，从而导致法律上下文信息的丢失。ii. 法律训练数据缺乏；由于数据隐私问题，可用的PCR数据集通常规模有限，使得深度学习模型难以有效训练。在本研究中，我们通过利用基于LLM的文本嵌入器来解决这些挑战。基于LLM的嵌入器支持更长的输入长度，并且由于我们以无监督方式使用它们，它们不需要训练数据，从而同时解决了这两个挑战。在本文中，我们在四个PCR基准数据集上评估了最先进的基于LLM的文本嵌入器，并表明它们优于BM25和有监督的基于Transformer的模型。", "summary": "本文针对先例检索（PCR）中深度学习方法面临的挑战，即长法律文本处理限制和训练数据缺乏，提出了一种基于大型语言模型（LLM）的文本嵌入器方法。该方法以无监督方式运行，能够处理长输入文本且无需专门的训练数据。实验结果表明，在四个PCR基准数据集上，基于LLM的嵌入器性能优于传统的BM25模型以及有监督的Transformer模型。", "keywords": "先例检索, LLM, 文本嵌入器, 无监督学习, 法律信息检索", "comments": "这篇论文通过引入无监督的LLM-based嵌入器，巧妙地解决了法律领域信息检索中长期存在的两大难题：长文本处理和数据稀缺。其创新之处在于利用LLM处理长文本的能力以及无监督学习范式，避免了对大量标注数据的依赖，这对于法律这种数据敏感且文本冗长的领域尤其重要。研究结果表明了LLM在法律信息检索领域的巨大潜力。"}}
{"id": "2408.10610", "title": "On the Approximation of Stationary Processes using the ARMA Model", "authors": ["Anand Ganesh", "Babhrubahan Bose", "Anand Rajagopalan"], "categories": ["cs.LG", "math.PR", "stat.ME", "60G10", "G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 1 figure", "url": "http://arxiv.org/abs/2408.10610v3", "summary": "We revisit an old problem related to Autoregressive Moving Average (ARMA)\nmodels, on quantifying and bounding the approximation error between a true\nstationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function\nrepresentation of an ARMA model and show that the associated $L^{\\infty}$ norm\nprovides a valid alternate norm that controls the $L^2$ norm and has structural\nproperties comparable to the cepstral norm. We show that a certain subspace of\nstationary processes, which includes ARMA models, forms a Banach algebra under\nthe $L^{\\infty}$ norm that respects the group structure of $H^{\\infty}$\ntransfer functions. The natural definition of invertibility in this algebra is\nconsistent with the original definition of ARMA invertibility, and generalizes\nbetter to non-ARMA processes than Wiener's $\\ell^1$ condition. Finally, we\ncalculate some explicit approximation bounds in the simpler context of\ncontinuous transfer functions, and critique some heuristic ideas on Pad\\'e\napproximations and parsimonious models.", "comment": "16 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2408.10610v3", "cate": "cs.LG", "date": "2024-08-20", "updated": "2025-07-24", "AI": {"title_translation": "关于使用ARMA模型逼近平稳过程", "tldr": "本文重新审视了使用ARMA模型逼近真实平稳过程时的近似误差问题。研究引入了ARMA模型的传递函数表示下的L∞范数，并证明了其作为有效替代范数的性质，以及其在包含ARMA模型的平稳过程子空间中形成巴拿赫代数。此外，该范数下可逆性的定义与传统ARMA可逆性一致，并能更好地推广到非ARMA过程。文章还计算了连续传递函数背景下的近似界限，并对启发式方法进行了评论。", "motivation": "重新审视与自回归滑动平均(ARMA)模型相关的一个老问题，即量化和限定真实平稳过程与ARMA模型之间的近似误差。", "method": "采用ARMA模型的传递函数表示，并引入L∞范数作为分析工具。证明了在L∞范数下，包含ARMA模型在内的平稳过程的特定子空间形成一个巴拿赫代数。计算了连续传递函数背景下的显式近似界限，并对Padé近似和简约模型的一些启发式思想进行了批判。", "result": "L∞范数提供了一个有效的替代范数，能够控制L2范数，并具有与倒谱范数相似的结构特性。包含ARMA模型在内的平稳过程的特定子空间在L∞范数下形成一个巴拿赫代数，尊重H∞传递函数的群结构。在该代数中可逆性的自然定义与ARMA可逆性的原始定义一致，并且比维纳的ℓ1条件更好地推广到非ARMA过程。计算出了一些连续传递函数背景下的显式近似界限。", "conclusion": "本文通过引入L∞范数，为量化和限定平稳过程的ARMA模型近似误差提供了一个新的理论框架。该框架下的可逆性定义更具普适性，并能够更好地处理非ARMA过程。研究还提供了具体的近似界限，并对现有启发式方法进行了批判性评估。", "translation": "我们重新审视了一个与自回归滑动平均（ARMA）模型相关的老问题，即量化和限定真实平稳过程Xt与ARMA模型Yt之间的近似误差。我们采用ARMA模型的传递函数表示，并表明相关的L∞范数提供了一个有效的替代范数，它可以控制L2范数，并具有与倒谱范数相当的结构特性。我们证明了平稳过程的某个特定子空间（包括ARMA模型）在L∞范数下形成一个巴拿赫代数，该代数尊重H∞传递函数的群结构。该代数中可逆性的自然定义与ARMA可逆性的原始定义一致，并且比维纳的ℓ1条件更好地推广到非ARMA过程。最后，我们在连续传递函数这一更简单背景下计算了一些显式近似界限，并批判了关于Padé近似和简约模型的一些启发式思想。", "summary": "本文重新探讨了ARMA模型逼近平稳过程的误差量化问题。研究引入了ARMA模型传递函数表示下的L∞范数，证明其能有效控制L2范数并具有良好结构。文章进一步指出，在L∞范数下，包含ARMA模型的平稳过程子空间构成一个巴拿赫代数，其可逆性定义更具普适性。此外，还计算了具体的近似界限，并对现有启发式方法进行了批判性分析。", "keywords": "ARMA模型, 平稳过程, 近似误差, L∞范数, 巴拿赫代数", "comments": "本文的创新之处在于引入了L∞范数作为分析ARMA模型近似误差的新工具，并从数学上证明了其优良的结构特性（如形成巴拿赫代数），以及在可逆性定义上的普适性。这为理解和量化时间序列分析中的近似误差提供了一个更为严格和广义的理论框架，对于更准确地应用和评估ARMA模型具有重要意义。"}}
{"id": "2507.18289", "title": "Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling", "authors": ["Yan Li", "Wenzhang Yang", "Yuekun Wang", "Jian Gao", "Shaohua Wang", "Yinxing Xue", "Lijun Zhang"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      15 pages, 12 figures, 5 tables", "url": "http://arxiv.org/abs/2507.18289v1", "summary": "Fuzzing a library requires experts to understand the library usage well and\ncraft high-quality fuzz drivers, which is tricky and tedious. Therefore, many\ntechniques have been proposed to automatically generate fuzz drivers. However,\nthey fail to generate rational fuzz drivers due to the lack of adherence to\nproper library usage conventions, such as ensuring a resource is closed after\nbeing opened. To make things worse, existing library fuzzing techniques\nunconditionally execute each driver, resulting in numerous irrational drivers\nthat waste computational resources while contributing little coverage and\ngenerating false positive bug reports.\n  To tackle these challenges, we propose a novel automatic library fuzzing\ntechnique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs\nto understand rational usage of libraries and extract API combination\nconstraints. To optimize computational resource utilization, a dual scheduling\nframework is implemented to efficiently manage API combinations and fuzz\ndrivers. The framework models driver generation and the corresponding fuzzing\ncampaign as an online optimization problem. Within the scheduling loop,\nmultiple API combinations are selected to generate fuzz drivers, while\nsimultaneously, various optimized fuzz drivers are scheduled for execution or\nsuspension.\n  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared\nto baseline approaches, Scheduzz significantly reduces computational overhead\nand outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and\n1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,\nPromptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,\nScheduzz discovered 33 previously unknown bugs in these well-tested libraries,\n3 of which have been assigned CVEs.", "comment": "15 pages, 12 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.18289v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Scheduzz：基于约束的双调度模糊驱动生成", "tldr": "Scheduzz是一种基于LLM的库模糊测试技术，通过双调度框架生成高质量模糊驱动并优化资源利用，显著提高了覆盖率并发现了新bug。", "motivation": "现有自动模糊驱动生成技术未能生成合理的模糊驱动，因为它们不遵循库使用约定，且无条件执行驱动导致资源浪费和假阳性。", "method": "提出Scheduzz，一种基于LLM的库模糊测试技术。它利用LLM理解库的合理用法并提取API组合约束。实现了一个双调度框架来管理API组合和模糊驱动，将其建模为在线优化问题，在调度循环中选择API组合生成驱动，并调度优化后的驱动执行或暂停。", "result": "在33个真实世界库中进行评估。与基线方法相比，Scheduzz显著降低了计算开销，并在21个库中的16个上优于UTopia。比CKGFuzzer、Promptfuzz和手工项目OSS-Fuzz分别高出1.62x、1.50x和1.89x的总体覆盖率。发现了33个以前未知的bug，其中3个已获得CVE。", "conclusion": "Scheduzz通过其基于LLM和双调度框架的方法，有效解决了现有库模糊测试的挑战，显著提高了效率和发现bug的能力。", "translation": "模糊测试一个库需要专家深入理解库的使用并编写高质量的模糊驱动，这既棘手又繁琐。因此，许多技术被提出用于自动生成模糊驱动。然而，由于未能遵循适当的库使用约定（例如确保资源在打开后被关闭），它们无法生成合理的模糊驱动。更糟糕的是，现有的库模糊测试技术无条件地执行每个驱动，导致大量的非理性驱动浪费计算资源，同时对覆盖率贡献甚微并产生误报。\n为了解决这些挑战，我们提出了一种新颖的自动化库模糊测试技术Scheduzz，一种基于LLM的库模糊测试技术。它利用LLM理解库的合理用法并提取API组合约束。为了优化计算资源利用，实现了一个双调度框架来高效管理API组合和模糊驱动。该框架将驱动生成和相应的模糊测试活动建模为在线优化问题。在调度循环中，选择多个API组合来生成模糊驱动，同时调度各种优化后的模糊驱动执行或暂停。\n我们实现了Scheduzz并在33个真实世界库中进行了评估。与基线方法相比，Scheduzz显著降低了计算开销，并在21个库中的16个上优于UTopia。它比最先进的技术CKGFuzzer、Promptfuzz和手工项目OSS-Fuzz分别高出1.62倍、1.50倍和1.89倍的总体覆盖率。此外，Scheduzz在这些经过充分测试的库中发现了33个以前未知的错误，其中3个已获得CVE。", "summary": "Scheduzz是一种创新的基于LLM的库模糊测试技术，旨在解决现有模糊驱动生成方法中不合理驱动和资源浪费的问题。它通过利用LLM理解库用法和提取API约束，并引入一个双调度框架来优化驱动生成和执行，从而实现高效的模糊测试。实验结果表明，Scheduzz显著降低了计算开销，提高了代码覆盖率，并成功发现了多个新漏洞。", "keywords": "模糊测试, LLM, 模糊驱动生成, 调度, 在线优化", "comments": "Scheduzz的创新之处在于结合了大型语言模型（LLM）来理解库的语义和使用约束，这解决了传统模糊测试在生成合理驱动方面的不足。其引入的双调度框架将驱动生成和模糊测试活动建模为在线优化问题，有效地优化了计算资源利用，提高了模糊测试的效率和效果。该方法在实际库中表现出色，不仅提升了覆盖率，还发现了真实世界的漏洞，证明了其重要性和实用价值。"}}
{"id": "2507.18337", "title": "The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams", "authors": ["Peter Baumgartner", "Lachlan McGinness"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18337v1", "summary": "We present our method for automatically marking Physics exams. The marking\nproblem consists in assessing typed student answers for correctness with\nrespect to a ground truth solution. This is a challenging problem that we seek\nto tackle using a combination of a computer algebra system, an SMT solver and a\nterm rewriting system. A Large Language Model is used to interpret and remove\nerrors from student responses and rewrite these in a machine readable format.\nOnce formalized and language-aligned, the next step then consists in applying\nautomated reasoning techniques for assessing student solution correctness. We\nconsider two methods of automated theorem proving: off-the-shelf SMT solving\nand term rewriting systems tailored for physics problems involving\ntrigonometric expressions. The development of the term rewrite system and\nestablishing termination and confluence properties was not trivial, and we\ndescribe it in some detail in the paper. We evaluate our system on a rich pool\nof over 1500 real-world student exam responses from the 2023 Australian Physics\nOlympiad.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18337v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "物理考试代数表达式批改的AlphaPhysics项重写系统", "tldr": "本文提出了一种用于自动批改物理考试的方法，结合了大型语言模型、计算机代数系统、SMT求解器和项重写系统，并在1500多份真实学生答卷上进行了评估。", "motivation": "自动批改物理考试是一个具有挑战性的问题，需要评估学生输入的答案相对于标准答案的正确性。", "method": "该方法结合了计算机代数系统、SMT求解器和项重写系统。首先使用大型语言模型解释学生回答并将其转换为机器可读格式，然后应用自动化推理技术（现成的SMT求解和为涉及三角表达式的物理问题量身定制的项重写系统）来评估学生解决方案的正确性。文中详细描述了项重写系统的开发以及终止性和合流性属性的建立。", "result": "该系统在来自2023年澳大利亚物理奥林匹克的1500多份真实学生考试答卷的大型样本池上进行了评估。", "conclusion": "本文提出了一种结合多种先进技术（包括大型语言模型和专门的项重写系统）的自动化物理考试批改方法，并对其在真实世界数据上的应用进行了展示。", "translation": "我们提出了自动批改物理考试的方法。批改问题在于评估学生输入的答案相对于标准答案的正确性。这是一个具有挑战性的问题，我们试图通过结合计算机代数系统、SMT求解器和项重写系统来解决。大型语言模型用于解释学生回答并从中去除错误，然后将其重写为机器可读的格式。一旦形式化并与语言对齐，下一步就是应用自动化推理技术来评估学生解决方案的正确性。我们考虑了两种自动化定理证明方法：现成的SMT求解和为涉及三角表达式的物理问题量身定制的项重写系统。项重写系统的开发以及终止性和合流性属性的建立并非易事，我们在论文中对其进行了详细描述。我们使用来自2023年澳大利亚物理奥林匹克的1500多份真实学生考试答卷的丰富样本池对我们的系统进行了评估。", "summary": "本文介绍了一种名为AlphaPhysics的自动物理考试批改系统。该系统利用大型语言模型解释和规范学生答案，并结合计算机代数系统、SMT求解器和专门的项重写系统来评估答案的正确性。特别强调了为物理问题定制的项重写系统的开发。该系统已在超过1500份来自2023年澳大利亚物理奥林匹克的真实学生答卷上进行了评估。", "keywords": "自动批改, 项重写系统, 物理考试, 大型语言模型, SMT求解器", "comments": "该论文的创新之处在于将大型语言模型与传统的计算机代数系统、SMT求解器和专门定制的项重写系统相结合，以解决物理考试中代数表达式的自动批改这一复杂问题。其重要性在于提供了一种自动化且可能更高效的批改解决方案，尤其是在处理真实世界、多样化的学生回答时。定制的项重写系统及其终止性和合流性属性的详细描述是其技术贡献的亮点。"}}
{"id": "2507.18619", "title": "MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss", "authors": ["Yichen Yu", "Qiaoran Wang"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18619v1", "summary": "Children with hearing impairments face ongoing challenges in language and\nmotor development. This study explores how multi-sensory feedback technology\nbased on virtual reality (VR), integrating auditory, visual, and tactile\nstimuli, can enhance rehabilitation outcomes. Using functional near-infrared\nspectroscopy (fNIRS) technology, we assessed cortical activation patterns in\nchildren during pitch-matching tasks across different interaction modes. Our\nfindings aim to provide evidence for designing personalized, interactive\nrehabilitation systems that enhance cognitive engagement and motor control in\nchildren with hearing impairments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18619v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MeloKids：多感官VR系统增强听障儿童的言语和运动协调能力", "tldr": "MeloKids是一个多感官VR系统，旨在利用听觉、视觉和触觉刺激，并通过fNIRS技术评估皮层激活，以改善听障儿童的语言和运动发展。", "motivation": "听障儿童在语言和运动发展方面面临持续挑战，本研究旨在探索基于虚拟现实的多感官反馈技术如何增强康复效果。", "method": "研究使用MeloKids多感官VR系统，整合听觉、视觉和触觉刺激，并通过功能性近红外光谱（fNIRS）技术评估儿童在音高匹配任务中不同互动模式下的皮层激活模式。", "result": "研究结果旨在为设计个性化、互动式康复系统提供证据，以增强听障儿童的认知参与和运动控制。", "conclusion": "Not mentioned in abstract", "translation": "听障儿童在语言和运动发展方面面临持续挑战。本研究探索了基于虚拟现实（VR）的多感官反馈技术，如何整合听觉、视觉和触觉刺激，以增强康复效果。我们使用功能性近红外光谱（fNIRS）技术，评估了儿童在音高匹配任务中不同互动模式下的皮层激活模式。我们的研究结果旨在为设计个性化、互动式康复系统提供证据，以增强听障儿童的认知参与和运动控制。", "summary": "本文介绍了MeloKids，一个旨在通过多感官VR系统改善听障儿童言语和运动协调的研究。该系统结合了听觉、视觉和触觉刺激，并利用fNIRS技术评估儿童在音高匹配任务中的皮层激活。研究旨在为开发个性化康复系统提供依据，以提升听障儿童的认知参与和运动控制。", "keywords": "听障儿童, 多感官VR, fNIRS, 言语康复, 运动协调", "comments": "该研究创新性地将多感官VR技术应用于听障儿童的康复，通过整合多种感官刺激和使用fNIRS进行客观评估，为个性化康复系统的设计提供了新的视角和潜在的证据基础。其重要性在于为改善听障儿童的语言和运动发展提供了科技赋能的解决方案。"}}
{"id": "2507.18288", "title": "TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis", "authors": ["Xuebo Jin", "Longfei Gao", "Anshuo Tong", "Zhengyang Chen", "Jianlei Kong", "Ning Sun", "Huijun Ma", "Qiang Wang", "Yuting Bai", "Tingli Su"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      16 pages, 11 figures, 2 Tables", "url": "http://arxiv.org/abs/2507.18288v1", "summary": "Traditional Chinese medicine (TCM) tongue diagnosis, while clinically\nvaluable, faces standardization challenges due to subjective interpretation and\ninconsistent imaging protocols, compounded by the lack of large-scale,\nannotated datasets for AI development. To address this gap, we present the\nfirst specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719\nhigh-quality images captured under standardized conditions and annotated with\n20 pathological symptom categories (averaging 2.54 clinically validated labels\nper image, all verified by licensed TCM practitioners). The dataset supports\nmultiple annotation formats (COCO, TXT, XML) for broad usability and has been\nbenchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and\nMobileNetV2) to demonstrate its utility for AI development. This resource\nprovides a critical foundation for advancing reliable computational tools in\nTCM, bridging the data shortage that has hindered progress in the field, and\nfacilitating the integration of AI into both research and clinical practice\nthrough standardized, high-quality diagnostic data.", "comment": "16 pages, 11 figures, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.18288v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "中医舌象：一个用于AI辅助中医诊断的标准化病理标注舌象数据集", "tldr": "提出了一个包含6719张标准化舌象图像和20种病理标注的大规模数据集TCM-Tongue，以推动AI辅助中医舌诊的发展。", "motivation": "传统中医舌诊面临标准化挑战、主观判读和成像协议不一致，且缺乏大规模、带标注的数据集来开发AI。", "method": "构建了第一个专门用于AI驱动中医舌诊的数据集TCM-Tongue，包含6719张在标准化条件下捕获的高质量图像，并用20种病理症状类别进行了标注（每张图像平均2.54个临床验证标签，均由执业中医师验证）。数据集支持多种标注格式（COCO, TXT, XML），并使用九种深度学习模型（YOLOv5/v7/v8变体, SSD, MobileNetV2）进行了基准测试。", "result": "成功构建了TCM-Tongue数据集，并使用多种深度学习模型进行了基准测试，证明了其对AI开发的实用性。该资源为推进中医领域可靠的计算工具提供了关键基础，弥补了阻碍该领域进展的数据短缺。", "conclusion": "TCM-Tongue数据集通过提供标准化、高质量的诊断数据，为AI在中医研究和临床实践中的整合提供了关键基础，并解决了该领域数据短缺的问题。", "translation": "传统中医（TCM）舌诊虽然具有临床价值，但由于主观判读和成像协议不一致，面临标准化挑战，且缺乏大规模、带标注的数据集用于AI开发。为了弥补这一空白，我们提出了第一个专门用于AI驱动中医舌诊的数据集，该数据集包含6,719张在标准化条件下捕获的高质量图像，并标注了20种病理症状类别（每张图像平均2.54个经过临床验证的标签，均由执业中医师验证）。该数据集支持多种标注格式（COCO、TXT、XML），具有广泛的可用性，并已使用九种深度学习模型（YOLOv5/v7/v8变体、SSD和MobileNetV2）进行基准测试，以展示其对AI开发的实用性。这一资源为推进中医领域可靠的计算工具提供了关键基础，弥补了阻碍该领域进展的数据短缺，并通过标准化、高质量的诊断数据促进了AI在研究和临床实践中的整合。", "summary": "本文推出了TCM-Tongue数据集，这是首个专为AI辅助中医舌诊设计的大规模标准化舌象数据集。该数据集包含6719张高质量图像，标注了20种病理症状，并经过执业中医师验证。它支持多种标注格式，并已通过多种深度学习模型进行基准测试，旨在解决当前中医舌诊AI发展中数据短缺和标准化不足的问题，为AI在中医领域的应用奠定基础。", "keywords": "中医舌诊, 舌象数据集, 人工智能辅助诊断, 病理标注, 深度学习", "comments": "该论文通过构建首个大规模、标准化且经过专业标注的舌象数据集，极大地解决了中医舌诊AI发展中的数据瓶颈问题。其创新性在于数据的标准化采集和专业验证，以及支持多种AI模型格式，这为未来AI在中医诊断领域的应用提供了坚实的基础。"}}
{"id": "2407.02075", "title": "Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts", "authors": ["Pasquale De Marinis", "Nicola Fanelli", "Raffaele Scaringi", "Emanuele Colonna", "Giuseppe Fiameni", "Gennaro Vessio", "Giovanna Castellano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.02075v2", "summary": "We present Label Anything, an innovative neural network architecture designed\nfor few-shot semantic segmentation (FSS) that demonstrates remarkable\ngeneralizability across multiple classes with minimal examples required per\nclass. Diverging from traditional FSS methods that predominantly rely on masks\nfor annotating support images, Label Anything introduces varied visual prompts\n-- points, bounding boxes, and masks -- thereby enhancing the framework's\nversatility and adaptability. Unique to our approach, Label Anything is\nengineered for end-to-end training across multi-class FSS scenarios,\nefficiently learning from diverse support set configurations without\nretraining. This approach enables a \"universal\" application to various FSS\nchallenges, ranging from $1$-way $1$-shot to complex $N$-way $K$-shot\nconfigurations while remaining agnostic to the specific number of class\nexamples. This innovative training strategy reduces computational requirements\nand substantially improves the model's adaptability and generalization across\ndiverse segmentation tasks. Our comprehensive experimental validation,\nparticularly achieving state-of-the-art results on the COCO-$20^i$ benchmark,\nunderscores Label Anything's robust generalization and flexibility. The source\ncode is publicly available at: https://github.com/pasqualedem/LabelAnything.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.02075v2", "cate": "cs.CV", "date": "2024-07-02", "updated": "2025-07-24", "AI": {"title_translation": "任意标注：基于视觉提示的多类别小样本语义分割", "tldr": "Label Anything是一种创新的神经网络架构，用于多类别小样本语义分割，通过引入点、边界框和掩码等多种视觉提示，实现了卓越的泛化能力和适应性，并在COCO-20i基准上取得了最先进的结果。", "motivation": "传统的小样本语义分割（FSS）方法主要依赖掩码进行支持图像标注，限制了其通用性和适应性。本文旨在开发一种更灵活、更通用的FSS框架，能够处理多类别场景，并从少量示例中高效学习，同时降低计算需求。", "method": "本文提出了Label Anything，一个用于小样本语义分割的神经网络架构。它引入了多种视觉提示（点、边界框和掩码），而非仅仅依赖掩码。该方法支持多类别FSS场景的端到端训练，无需再训练即可从多样化的支持集配置中学习，适用于1-way 1-shot到N-way K-shot等不同配置，且对类别示例数量不敏感。", "result": "Label Anything在多类别小样本语义分割中展现出卓越的泛化能力和适应性。在COCO-20i基准测试中取得了最先进（state-of-the-art）的结果，证明了其强大的泛化能力和灵活性。", "conclusion": "Label Anything通过引入多样化的视觉提示和实现多类别FSS的端到端训练，显著提升了小样本语义分割的通用性、适应性和计算效率，并在实验中取得了优异的性能。", "translation": "我们提出了Label Anything，一种创新的神经网络架构，专为小样本语义分割（FSS）设计，它在多类别场景中展现出卓越的泛化能力，且每个类别仅需极少的示例。与主要依赖掩码标注支持图像的传统FSS方法不同，Label Anything引入了多种视觉提示——点、边界框和掩码——从而增强了框架的多功能性和适应性。我们方法的独特之处在于，Label Anything被设计用于多类别FSS场景的端到端训练，无需重新训练即可从多样化的支持集配置中高效学习。这种方法使得它能够“通用地”应用于各种FSS挑战，从1-way 1-shot到复杂的N-way K-shot配置，同时对特定类别的示例数量保持无关性。这种创新的训练策略减少了计算需求，并大幅提高了模型在各种分割任务中的适应性和泛化能力。我们全面的实验验证，特别是在COCO-20i基准上取得了最先进的结果，突显了Label Anything强大的泛化能力和灵活性。源代码已公开：https://github.com/pasqualedem/LabelAnything。", "summary": "Label Anything是一种新颖的神经网络架构，专为多类别小样本语义分割设计。它通过引入点、边界框和掩码等多种视觉提示，克服了传统方法对单一掩码的依赖，显著增强了模型的通用性和适应性。该模型支持多类别FSS的端到端训练，无需重新训练即可适应不同配置，从而减少了计算需求并提高了泛化能力。实验结果（尤其是在COCO-20i基准上的最先进表现）证实了其强大的性能和灵活性。", "keywords": "小样本语义分割, 视觉提示, 多类别, 泛化, 端到端训练", "comments": "Label Anything的创新点在于其引入了多样化的视觉提示（点、边界框、掩码）来替代单一的掩码标注，极大地提升了小样本语义分割的灵活性和实用性。其端到端的多类别训练策略也解决了传统方法需要针对特定类别或配置重新训练的问题，显著提高了效率和泛化能力。这是一个重要的进展，有望推动小样本分割技术在实际应用中的普及。"}}
{"id": "2507.18467", "title": "Contraction, Criticality, and Capacity: A Dynamical-Systems Perspective on Echo-State Networks", "authors": ["Pradeep Singh", "Lavanya Sankaranarayanan", "Balasubramanian Raman"], "categories": ["cs.NE", "nlin.CD", "68T07, 37M25, 37N30, 41A30", "I.2.6; F.1.1; G.3"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.18467v1", "summary": "Echo-State Networks (ESNs) distil a key neurobiological insight: richly\nrecurrent but fixed circuitry combined with adaptive linear read-outs can\ntransform temporal streams with remarkable efficiency. Yet fundamental\nquestions about stability, memory and expressive power remain fragmented across\ndisciplines. We present a unified, dynamical-systems treatment that weaves\ntogether functional analysis, random attractor theory and recent\nneuroscientific findings. First, on compact multivariate input alphabets we\nprove that the Echo-State Property (wash-out of initial conditions) together\nwith global Lipschitz dynamics necessarily yields the Fading-Memory Property\n(geometric forgetting of remote inputs). Tight algebraic tests translate\nactivation-specific Lipschitz constants into certified spectral-norm bounds,\ncovering both saturating and rectifying nonlinearities. Second, employing a\nStone-Weierstrass strategy we give a streamlined proof that ESNs with\npolynomial reservoirs and linear read-outs are dense in the Banach space of\ncausal, time-invariant fading-memory filters, extending universality to\nstochastic inputs. Third, we quantify computational resources via\nmemory-capacity spectrum, show how topology and leak rate redistribute\ndelay-specific capacities, and link these trade-offs to Lyapunov spectra at the\n\\textit{edge of chaos}. Finally, casting ESNs as skew-product random dynamical\nsystems, we establish existence of singleton pullback attractors and derive\nconditional Lyapunov bounds, providing a rigorous analogue to cortical\ncriticality. The analysis yields concrete design rules-spectral radius, input\ngain, activation choice-grounded simultaneously in mathematics and\nneuroscience, and clarifies why modest-sized reservoirs often rival fully\ntrained recurrent networks in practice.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.18467v1", "cate": "cs.NE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "收缩、临界性与容量：回声状态网络的一个动力系统视角", "tldr": "本文从动力系统角度统一了对回声状态网络（ESNs）的理解，证明了其衰减记忆特性和普适性，量化了记忆容量，并将其与皮层临界性联系起来，提出了具体的设计规则。", "motivation": "关于回声状态网络（ESNs）的稳定性、记忆和表达能力等基本问题在不同学科中仍是分散的，缺乏统一的理解。", "method": "本文采用统一的动力系统处理方法，结合了泛函分析、随机吸引子理论和神经科学发现。具体方法包括：通过Lipschitz动力学和代数测试证明回声状态特性和衰减记忆特性；采用Stone-Weierstrass策略证明普适性；通过记忆容量谱量化计算资源；将ESNs视为斜积随机动力系统，建立回拉吸引子的存在性并推导条件Lyapunov界限。", "result": "1. 证明了回声状态特性与全局Lipschitz动力学必然产生衰减记忆特性，并提供了严格的代数测试。2. 简化证明了具有多项式储层和线性读出的ESNs在因果、时不变衰减记忆滤波器的Banach空间中是稠密的，将普适性扩展到随机输入。3. 通过记忆容量谱量化了计算资源，展示了拓扑结构和泄漏率如何重新分配延迟特定容量，并将其与“混沌边缘”的Lyapunov谱联系起来。4. 建立了单例回拉吸引子的存在性，并推导了条件Lyapunov界限，为皮层临界性提供了严格的类比。", "conclusion": "该分析同时以数学和神经科学为基础，产生了具体的ESN设计规则（谱半径、输入增益、激活选择），并阐明了为什么适度大小的储层在实践中常常能与完全训练的循环网络相媲美。", "translation": "回声状态网络（ESNs）提炼了一个关键的神经生物学见解：丰富循环但固定的电路与自适应线性读出相结合，可以高效地转换时间流。然而，关于稳定性、记忆和表达能力的基本问题在不同学科中仍然分散。我们提出了一种统一的动力系统处理方法，将泛函分析、随机吸引子理论和最近的神经科学发现结合起来。首先，在紧凑的多变量输入字母表上，我们证明了回声状态特性（初始条件的洗脱）与全局Lipschitz动力学必然产生衰减记忆特性（对远端输入的几何遗忘）。严格的代数测试将激活特定的Lipschitz常数转换为经过认证的谱范数界限，涵盖了饱和和整流非线性。其次，采用Stone-Weierstrass策略，我们给出了一个简化的证明，即具有多项式储层和线性读出的ESNs在因果、时不变衰减记忆滤波器的Banach空间中是稠密的，从而将普适性扩展到随机输入。第三，我们通过记忆容量谱量化计算资源，展示了拓扑结构和泄漏率如何重新分配延迟特定容量，并将这些权衡与“混沌边缘”的Lyapunov谱联系起来。最后，将ESNs视为斜积随机动力系统，我们建立了单例回拉吸引子的存在性，并推导了条件Lyapunov界限，为皮层临界性提供了严格的类比。该分析产生了具体的、同时以数学和神经科学为基础的设计规则——谱半径、输入增益、激活选择——并阐明了为什么适度大小的储层在实践中常常能与完全训练的循环网络相媲美。", "summary": "本文为回声状态网络（ESNs）提供了一个统一的动力系统框架，整合了泛函分析、随机吸引子理论和神经科学。它严格证明了从回声状态特性和Lipschitz动力学推导出的衰减记忆特性，展示了ESNs对随机输入的普适性，量化了记忆容量并将其与临界性联系起来。通过将ESNs视为斜积随机动力系统，本文建立了回拉吸引子的存在性并推导了类似于皮层临界性的Lyapunov界限。该分析为ESNs提供了实用的设计规则，并解释了其在实践中具有竞争力的性能。", "keywords": "回声状态网络, 动力系统, 衰减记忆, 记忆容量, 皮层临界性", "comments": "本文通过提供一个严谨、统一的数学框架来理解回声状态网络，具有创新性，弥合了不同学科之间的鸿沟。其重要性在于从理论基础推导出具体的ESN设计规则，并解释了ESN在实践中的有效性，对循环神经网络的理论理解和实际应用都做出了贡献。与神经科学发现和皮层临界性的联系尤其具有洞察力。"}}
{"id": "2507.18623", "title": "Moving Out: Physically-grounded Human-AI Collaboration", "authors": ["Xuhui Kang", "Sung-Wook Lee", "Haolin Liu", "Yuyan Wang", "Yen-Ling Kuo"], "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 8 figures", "url": "http://arxiv.org/abs/2507.18623v1", "summary": "The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.", "comment": "24 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.18623v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "搬出：物理接地的人机协作", "tldr": "引入了《搬出》基准和BASS方法，以实现物理接地的人机协作，并在实验中表现优异。", "motivation": "具身智能体（如机器人）需要适应环境中的物理动作和约束，才能有效地与人类协作。这种物理接地的人机协作必须考虑物理约束导致的连续状态-动作空间和受限动态的复杂性增加。", "method": "本文介绍了《搬出》，一个新的物理接地人机协作基准，它模拟了受物理属性和约束影响的广泛协作模式，例如共同搬运重物和在拐角处保持一致的动作以移动大件物品。使用《搬出》，我们设计了两个任务并收集了人-人交互数据，以评估模型适应不同人类行为和未见物理属性的能力。为了解决物理环境中的挑战，我们提出了一种新颖的方法BASS（行为增强、模拟和选择），以增强智能体的多样性及其对行动结果的理解。", "result": "我们的实验表明，BASS在AI-AI和人-AI协作中均优于最先进的模型。", "conclusion": "BASS方法有效解决了物理环境中人机协作的挑战，显著提升了智能体在复杂物理场景下与人类协作的能力。", "translation": "具身智能体（例如机器人）适应环境中物理动作和约束的能力对于其与人类的有效协作至关重要。这种物理接地的人机协作必须考虑物理约束导致的连续状态-动作空间和受限动态的复杂性增加。在本文中，我们引入了《搬出》（Moving Out），一个新的物理接地人机协作基准，它模拟了受物理属性和约束影响的广泛协作模式，例如共同搬运重物和在拐角处保持一致的动作以移动大件物品。使用《搬出》，我们设计了两个任务并收集了人-人交互数据，以评估模型适应不同人类行为和未见物理属性的能力。为了解决物理环境中的挑战，我们提出了一种新颖的方法BASS（行为增强、模拟和选择），以增强智能体的多样性及其对行动结果的理解。我们的实验表明，BASS在AI-AI和人-AI协作中均优于最先进的模型。项目页面可在https://live-robotics-uva.github.io/movingout_ai/查看。", "summary": "该论文引入了《搬出》，一个用于物理接地人机协作的新基准，旨在解决具身智能体在物理环境中与人类协作时面临的复杂性和约束。该基准模拟了多种协作模式，并用于收集人-人交互数据。为应对这些挑战，论文提出了一种名为BASS（行为增强、模拟和选择）的新方法，旨在提高智能体的多样性和对行动结果的理解。实验结果表明，BASS在AI-AI和人-AI协作任务中均超越了现有最先进的模型。", "keywords": "人机协作, 具身智能体, 物理约束, 基准, BASS", "comments": "本文的创新之处在于提出了一个专门针对物理接地人机协作的基准《搬出》，并设计了BASS方法来应对物理环境带来的复杂挑战。这项工作对于推动具身智能体在现实世界中与人类进行更有效、更自然的协作具有重要意义，尤其是在需要考虑物理交互和约束的场景下。"}}
{"id": "2507.17797", "title": "GenSelect: A Generative Approach to Best-of-N", "authors": ["Shubham Toshniwal", "Ivan Sorokin", "Aleksander Ficek", "Ivan Moshkov", "Igor Gitman"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Presented at the 2nd AI for MATH Workshop @ ICML", "url": "http://arxiv.org/abs/2507.17797v1", "summary": "Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting.", "comment": "Presented at the 2nd AI for MATH Workshop @ ICML", "pdf_url": "http://arxiv.org/pdf/2507.17797v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "GenSelect：一种N选一的生成式方法", "tldr": "GenSelect是一种新的生成式方法，通过利用LLM的长推理能力高效地从N个候选方案中选择最佳方案，在数学推理任务上表现优于现有方法。", "motivation": "现有的点式评分方法未能充分利用LLM的比较能力，而成对比较方法在采样预算较大时效率低下。", "method": "引入GenSelect，该方法让LLM利用长推理从N个候选方案中选择最佳方案，从而利用LLM的比较优势并高效地扩展并行采样预算。", "result": "在数学推理任务中，QwQ和DeepSeek-R1-0528等推理模型在GenSelect上表现出色，通过简单的提示就优于现有评分方法。", "conclusion": "GenSelect通过利用LLM的长推理能力，能够有效地从N个候选方案中选择最佳方案，并在数学推理任务上取得了优于现有方法的性能。", "translation": "生成式奖励模型与并行采样相结合，使得推理任务在测试时能够有效扩展。当前的方法采用对单个解决方案进行逐点评分或成对比较。然而，逐点方法未能充分利用大型语言模型（LLM）的比较能力，而成对方法在采样预算较大时扩展效率低下。我们引入了GenSelect，其中LLM利用长推理从N个候选方案中选择最佳解决方案。这既利用了LLM的比较优势，又有效地扩展了并行采样预算。对于数学推理，我们证明了QwQ和DeepSeek-R1-0528等推理模型在GenSelect上表现出色，通过简单的提示就优于现有评分方法。", "summary": "本文提出了GenSelect，一种利用大型语言模型（LLM）长推理能力来从N个候选方案中选择最佳方案的生成式方法。该方法旨在解决现有逐点评分方法未能充分利用LLM比较能力以及成对比较方法扩展效率低下的问题。实验证明，在数学推理任务中，GenSelect在QwQ和DeepSeek-R1-0528等推理模型上表现出色，通过简单的提示即可超越现有评分方法。", "keywords": "生成式奖励模型, LLM, N选一, 数学推理, GenSelect", "comments": "GenSelect的创新之处在于其通过让LLM进行“长推理”来直接进行“N选一”的最佳选择，而非传统的逐点评分或成对比较。这有效地利用了LLM的内在比较能力，并解决了并行采样预算下的效率问题，为推理任务的测试时扩展提供了一个有前景的新方向。"}}
{"id": "2507.09186", "title": "OpenCAMS: An Open-Source Connected and Automated Mobility Co-Simulation Platform for Advancing Next-Generation Intelligent Transportation Systems Research", "authors": ["Minhaj Uddin Ahmad", "Akid Abrar", "Sagar Dasgupta", "Mizanur Rahman"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09186v3", "summary": "We introduce OpenCAMS (Open-Source Connected and Automated Mobility\nCo-Simulation Platform), an open-source, synchronized, and extensible\nco-simulation framework that tightly couples three best-in-class simulation\ntools: (i) SUMO, (ii) CARLA, and (iii) OMNeT++. OpenCAMS is designed to support\nadvanced research in transportation safety, mobility, and cybersecurity by\ncombining the strengths of each simulation domain. Specifically, SUMO provides\nlarge-scale, microscopic traffic modeling; CARLA offers high-fidelity 3D\nperception, vehicle dynamics, and control simulation; and OMNeT++ enables\nmodular, event-driven network communication, such as cellular\nvehicle-to-everything (C-V2X). OpenCAMS employs a time-synchronized,\nbidirectional coupling architecture that ensures coherent simulation\nprogression across traffic, perception, and communication domains while\npreserving modularity and reproducibility. For example, CARLA can simulate and\nrender a subset of vehicles that require detailed sensor emulation and control\nlogic; SUMO orchestrates network-wide traffic flow, vehicle routing, and\ntraffic signal management; and OMNeT++ dynamically maps communication nodes to\nboth mobile entities (e.g., vehicles) and static entities (e.g., roadside\nunits) to enable C-V2X communication. While these three simulators form the\nfoundational core of OpenCAMS, the platform is designed to be expandable and\nfuture-proof, allowing additional simulators to be integrated on top of this\ncore without requiring fundamental changes to the system architecture. The\nOpenCAMS platform is fully open-source and publicly available through its\nGitHub repository https://github.com/minhaj6/carla-sumo-omnetpp-cosim,\nproviding the research community with an accessible, flexible, and\ncollaborative environment for advancing next-generation intelligent\ntransportation systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09186v3", "cate": "cs.SE", "date": "2025-07-12", "updated": "2025-07-24", "AI": {"title_translation": "OpenCAMS：一个开源的互联与自动驾驶协同仿真平台，用于推进下一代智能交通系统研究", "tldr": "OpenCAMS是一个开源的协同仿真平台，结合SUMO、CARLA和OMNeT++，用于研究下一代智能交通系统。", "motivation": "该研究旨在提供一个能够结合不同仿真领域优势的平台，以支持交通安全、移动性和网络安全等互联与自动驾驶（CAM）领域的先进研究。", "method": "OpenCAMS是一个开源、同步、可扩展的协同仿真框架，它紧密耦合了SUMO（大规模微观交通建模）、CARLA（高保真3D感知、车辆动力学和控制仿真）和OMNeT++（模块化、事件驱动的网络通信，如蜂窝车联网C-V2X）。该平台采用时间同步、双向耦合架构，确保交通、感知和通信领域的一致性仿真进展，同时保持模块化和可复现性。此外，OpenCAMS被设计为可扩展，允许在核心架构之上集成其他模拟器。", "result": "OpenCAMS平台已完全开源，并通过GitHub仓库公开可用，为研究社区提供了一个可访问、灵活和协作的环境，以推进下一代智能交通系统。", "conclusion": "OpenCAMS提供了一个强大的、开放的协同仿真平台，通过集成SUMO、CARLA和OMNeT++等多种专业仿真工具，有效支持了下一代智能交通系统在交通安全、移动性和网络安全等方面的先进研究。", "translation": "我们引入了OpenCAMS（开源互联与自动驾驶协同仿真平台），这是一个开源、同步且可扩展的协同仿真框架，它紧密耦合了三个一流的仿真工具：(i) SUMO，(ii) CARLA，和(iii) OMNeT++。OpenCAMS旨在通过结合每个仿真领域的优势来支持交通安全、移动性和网络安全方面的先进研究。具体而言，SUMO提供大规模的微观交通建模；CARLA提供高保真3D感知、车辆动力学和控制仿真；OMNeT++则支持模块化、事件驱动的网络通信，例如蜂窝车联网（C-V2X）。OpenCAMS采用时间同步、双向耦合的架构，确保在交通、感知和通信领域之间实现一致的仿真进展，同时保持模块化和可复现性。例如，CARLA可以仿真和渲染需要详细传感器仿真和控制逻辑的车辆子集；SUMO协调全网络的交通流、车辆路径规划和交通信号管理；OMNeT++动态地将通信节点映射到移动实体（例如车辆）和静态实体（例如路边单元），以实现C-V2X通信。尽管这三个模拟器构成了OpenCAMS的基础核心，但该平台被设计为可扩展和面向未来的，允许在不改变系统基本架构的情况下，在此核心之上集成额外的模拟器。OpenCAMS平台完全开源，并通过其GitHub仓库https://github.com/minhaj6/carla-sumo-omnetpp-cosim公开可用，为研究社区提供了一个可访问、灵活和协作的环境，以推进下一代智能交通系统。", "summary": "OpenCAMS是一个开源的互联与自动驾驶协同仿真平台，它将SUMO、CARLA和OMNeT++三个领先的仿真工具紧密结合。该平台旨在通过整合交通、感知和通信领域的优势，支持交通安全、移动性和网络安全方面的先进研究。OpenCAMS采用时间同步的双向耦合架构，确保仿真的一致性和可复现性，同时保持模块化。作为一个可扩展的开源平台，OpenCAMS为下一代智能交通系统的研究提供了一个灵活且协作的环境。", "keywords": "协同仿真, 智能交通系统, 互联与自动驾驶, 开源, C-V2X", "comments": "OpenCAMS的创新之处在于其将多个领域领先的仿真工具进行紧密耦合，形成一个统一的协同仿真平台，这对于研究复杂的互联与自动驾驶系统至关重要。其开源特性极大地降低了研究门槛，促进了学术界和工业界的协作与发展。该平台通过整合交通流、高精度感知和V2X通信，为智能交通系统的多方面研究提供了强大的支持。"}}
{"id": "2507.18423", "title": "Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins", "authors": ["Mizuki Funato", "Yohei Sawada"], "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18423v1", "summary": "Despite the critical need for accurate flood prediction and water management,\nmany regions lack sufficient river discharge observations, limiting the skill\nof rainfall-runoff analyses. Although numerous physically based and machine\nlearning models exist, achieving high accuracy, interpretability, and\ncomputational efficiency under data-scarce conditions remains a major\nchallenge. We address this challenge with a novel method, HYdrological\nPrediction with multi-model Ensemble and Reservoir computing (HYPER) that\nleverages multi-model ensemble and reservoir computing (RC). Our approach first\napplies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based\nconceptual hydrological models. An RC model is then trained via linear\nregression to correct errors in the BMA output, a non-iterative process that\nensures high computational efficiency. For ungauged basins, we infer the\nrequired BMA and RC weights by linking them to catchment attributes from gauged\nbasins, creating a generalizable framework. We evaluated HYPER using data from\n87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta\nEfficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)\nbut required only 5% of its computational time. In a data-scarce scenario (23%\nof basins gauged), HYPER maintained robust performance (KGE 0.55) and lower\nuncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).\nThese results reveal that individual conceptual hydrological models do not\nnecessarily need to be calibrated when an effectively large ensemble is\nassembled and combined with machine-learning-based bias correction. HYPER\nprovides a robust, efficient, and generalizable solution for discharge\nprediction, particularly in ungauged basins, making it applicable to a wide\nrange of regions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18423v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "多模型集成和储层计算在无测站流域河流流量预测中的应用", "tldr": "HYPER是一种结合多模型集成和储层计算的新方法，用于在数据稀缺的无测站流域进行高效且鲁棒的河流流量预测，表现优于LSTM。", "motivation": "尽管对准确的洪水预测和水管理有迫切需求，但许多地区缺乏足够的河流流量观测数据，限制了降雨-径流分析的准确性。在数据稀缺条件下，实现高精度、可解释性和计算效率仍然是一个重大挑战。", "method": "本研究提出了一种名为HYPER（多模型集成和储层计算水文预测）的新方法。该方法首先对43个“未校准”的流域概念水文模型应用贝叶斯模型平均（BMA）。然后，通过线性回归训练一个储层计算（RC）模型来校正BMA输出中的误差，这是一个非迭代过程，确保了高计算效率。对于无测站流域，通过将所需BMA和RC权重与有测站流域的流域属性相关联来推断，从而创建一个通用框架。", "result": "在日本87个河流流域的数据评估中，在数据丰富的场景下，HYPER（中位Kling-Gupta效率KGE为0.56）表现与基准LSTM（KGE 0.55）相当，但计算时间仅为后者的5%。在数据稀缺场景（23%的流域有测站）下，HYPER保持了稳健的性能（KGE 0.55）和较低的不确定性，而LSTM的性能显著下降（KGE -0.04）。", "conclusion": "研究结果表明，当有效的大型集成模型与基于机器学习的偏差校正相结合时，单个概念水文模型不一定需要校准。HYPER为流量预测提供了一个鲁棒、高效和可推广的解决方案，特别是在无测站流域，使其适用于广泛的区域。", "translation": "尽管对准确的洪水预测和水管理有迫切需求，但许多地区缺乏足够的河流流量观测数据，限制了降雨-径流分析的准确性。虽然存在许多基于物理和机器学习的模型，但在数据稀缺条件下实现高精度、可解释性和计算效率仍然是一个重大挑战。我们通过一种新颖的方法——结合多模型集成和储层计算的水文预测（HYPER）来应对这一挑战。我们的方法首先对43个“未校准”的流域概念水文模型应用贝叶斯模型平均（BMA）。然后，通过线性回归训练一个储层计算（RC）模型来校正BMA输出中的误差，这是一个非迭代过程，确保了高计算效率。对于无测站流域，我们通过将所需的BMA和RC权重与有测站流域的流域属性相关联来推断，从而创建一个通用框架。我们使用日本87个河流流域的数据评估了HYPER。在数据丰富的场景下，HYPER（中位Kling-Gupta效率KGE为0.56）表现与基准LSTM（KGE 0.55）相当，但计算时间仅为后者的5%。在数据稀缺场景（23%的流域有测站）下，HYPER保持了稳健的性能（KGE 0.55）和较低的不确定性，而LSTM的性能显著下降（KGE -0.04）。这些结果表明，当有效的大型集成模型与基于机器学习的偏差校正相结合时，单个概念水文模型不一定需要校准。HYPER为流量预测提供了一个鲁棒、高效和可推广的解决方案，特别是在无测站流域，使其适用于广泛的区域。", "summary": "本研究提出了一种名为HYPER的新型水文预测方法，该方法结合了多模型集成（通过BMA对43个未校准概念模型进行）和储层计算（RC）进行误差校正，旨在解决数据稀缺条件下无测站流域的河流流量预测挑战。HYPER通过将权重与流域属性关联，实现了对无测站流域的泛化。在日本87个流域的评估显示，HYPER在数据丰富时性能与LSTM相当但计算效率更高，在数据稀缺时性能显著优于LSTM。研究表明，通过有效集成和机器学习偏差校正，概念水文模型无需单独校准。", "keywords": "河流流量预测, 无测站流域, 多模型集成, 储层计算, 数据稀缺", "comments": "HYPER方法的创新之处在于其结合了多模型集成（BMA）和储层计算（RC）来处理水文预测中的不确定性和误差，尤其是在数据稀缺的无测站流域。其非迭代的RC训练过程确保了高计算效率，同时在数据稀缺场景下表现出显著的鲁棒性，解决了传统模型在此类条件下的性能下降问题。该方法提供了一个通用且高效的解决方案，对于水资源管理和洪水预警具有重要意义。"}}
{"id": "2507.18584", "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs", "authors": ["Xiaopeng Ke", "Hexuan Deng", "Xuebo Liu", "Jun Rao", "Zhenxi Song", "Jun Yu", "Min Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures", "url": "http://arxiv.org/abs/2507.18584v1", "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.", "comment": "32 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18584v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "AQuilt：将逻辑和自检融入低成本、高相关性数据合成，以用于专业领域LLM", "tldr": "AQuilt是一种低成本、高相关性的数据合成框架，通过结合逻辑和自检，为专业领域LLM生成指令微调数据，性能媲美现有先进模型且成本显著降低。", "motivation": "现有LLM在通用领域表现出色，但在专业领域表现不佳。现有数据合成方法存在计算成本高昂、性能受限以及泛化能力不足的问题。", "method": "提出AQuilt框架，用于从无标注数据构建任何专业领域的指令微调数据。该框架包含答案、问题、无标注数据、检查、逻辑和任务类型（Answer, Question, Unlabeled data, Inspection, Logic, Task type）。通过整合逻辑和自检来增强推理过程和模型性能，并支持可定制的任务指令以生成高质量数据。", "result": "构建了一个包含70.3万个示例的数据集用于训练强大的数据合成模型。实验表明，AQuilt的性能与DeepSeek-V3相当，但生产成本仅为17%。此外，生成的数据对下游任务表现出更高的相关性。", "conclusion": "AQuilt提供了一种有效且成本低廉的方法，通过结合逻辑和自检为专业领域LLMs合成高质量的指令微调数据，显著提升了模型性能和数据相关性。", "translation": "尽管大型语言模型（LLM）在通用领域表现出色，但它们在专业领域往往表现不佳。现有方法通常依赖数据合成方法，通过使用未标注数据捕获领域特定特征来取得有希望的结果。然而，这些方法要么计算成本高昂，要么性能受限，同时在不同任务之间也表现出泛化能力不足。为了应对这些挑战，我们提出了AQuilt，一个用于从相应未标注数据构建任何专业领域指令微调数据的框架，包括答案、问题、未标注数据、检查、逻辑和任务类型。通过结合逻辑和检查，我们鼓励推理过程和自检以提高模型性能。此外，可定制的任务指令能够为任何任务生成高质量数据。因此，我们构建了一个包含70.3万个示例的数据集来训练一个强大的数据合成模型。实验表明，AQuilt与DeepSeek-V3相当，而生产成本仅为17%。进一步分析表明，我们生成的数据与下游任务具有更高的相关性。源代码、模型和脚本可在https://github.com/Krueske/AQuilt获取。", "summary": "AQuilt是一个创新的框架，旨在解决大型语言模型在专业领域性能不佳的问题。它通过从无标注数据合成高质量的指令微调数据，并融入逻辑和自检机制来提升LLM的推理能力和表现。该方法不仅显著降低了数据合成成本（仅为DeepSeek-V3的17%），同时保持了可比的性能，并生成了与下游任务高度相关的数据，为专业领域LLM的开发提供了高效且经济的解决方案。", "keywords": "数据合成, LLM, 指令微调, 逻辑推理, 自检", "comments": "AQuilt的创新之处在于其将逻辑和自检机制融入数据合成过程，这有助于生成更高质量、更具推理能力的数据，从而提升专业领域LLM的性能。其显著的成本效益（仅为DeepSeek-V3的17%）和高数据相关性使其成为一个非常重要的贡献，尤其是在资源受限或需要快速迭代的专业领域应用中。"}}
{"id": "2502.12988", "title": "Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs", "authors": ["Zixiao Wang", "Duzhen Zhang", "Ishita Agrawal", "Shen Gao", "Le Song", "Xiuying Chen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025 Findings", "url": "http://arxiv.org/abs/2502.12988v3", "summary": "Previous approaches to persona simulation large language models (LLMs) have\ntypically relied on learning basic biographical information, or using limited\nrole-play dialogue datasets to capture a character's responses. However, a\nholistic representation of an individual goes beyond surface-level facts or\nconversations to deeper thoughts and thinking. In this work, we introduce\nCharacterBot, a model designed to replicate both the linguistic patterns and\ndistinctive thought patterns as manifested in the textual works of a character.\nUsing Lu Xun, a renowned Chinese writer as a case study, we propose four\ntraining tasks derived from his 17 essay collections. These include a\npre-training task focused on mastering external linguistic structures and\nknowledge, as well as three fine-tuning tasks: multiple-choice question\nanswering, generative question answering, and style transfer, each aligning the\nLLM with Lu Xun's internal ideation and writing style. To optimize learning\nacross these tasks, we introduce a CharLoRA parameter updating mechanism, where\na general linguistic style expert collaborates with other task-specific experts\nto better study both the language style and the understanding of deeper\nthoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and\nopinion comprehension, demonstrating that it significantly outperforms the\nbaselines on our adapted metrics. We hope this work inspires future research on\ndeep character persona simulation LLMs while considering the importance of\nethical standards.", "comment": "Accepted by ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2502.12988v3", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-07-24", "AI": {"title_translation": "超越档案：从表层事实到大语言模型中的深度角色模拟", "tldr": "本文介绍了CharacterBot，一个旨在模拟大语言模型（LLM）中深度角色的模型，超越了表层事实，能学习语言模式和独特的思维模式。通过鲁迅的文本作为案例研究，CharacterBot在语言准确性和观点理解方面显著优于基线模型。", "motivation": "以往的大语言模型（LLM）角色模拟方法主要依赖于学习基本的传记信息或使用有限的角色扮演对话数据集来捕捉角色的回应。然而，一个人的整体表现超越了表层事实或对话，需要更深层次的思想和思维。", "method": "本文引入了CharacterBot模型，旨在复制角色文本作品中表现出的语言模式和独特的思维模式。以著名中国作家鲁迅为例，提出了四项训练任务，这些任务源自他的17部散文集。其中包括一项侧重于掌握外部语言结构和知识的预训练任务，以及三项微调任务：多项选择问答、生成式问答和风格迁移，每项任务都使LLM与鲁迅的内在思想和写作风格对齐。为了优化跨这些任务的学习，引入了CharLoRA参数更新机制，其中一个通用语言风格专家与其他特定任务专家协作，以更好地研究语言风格和对深层思想的理解。CharacterBot在三项任务上进行了评估，包括语言准确性和观点理解。", "result": "CharacterBot在本文调整的指标上显著优于基线模型。", "conclusion": "这项工作有望启发未来关于深度角色模拟大语言模型的研究，同时考虑道德标准的重要性。", "translation": "以往的大语言模型（LLM）角色模拟方法通常依赖于学习基本的传记信息，或使用有限的角色扮演对话数据集来捕捉角色的回应。然而，一个人的整体表现超越了表层事实或对话，需要更深层次的思想和思维。在这项工作中，我们引入了CharacterBot，一个旨在复制角色文本作品中表现出的语言模式和独特的思维模式的模型。以著名中国作家鲁迅为例，我们提出了四项训练任务，这些任务源自他的17部散文集。其中包括一项侧重于掌握外部语言结构和知识的预训练任务，以及三项微调任务：多项选择问答、生成式问答和风格迁移，每项任务都使LLM与鲁迅的内在思想和写作风格对齐。为了优化跨这些任务的学习，我们引入了CharLoRA参数更新机制，其中一个通用语言风格专家与其他特定任务专家协作，以更好地研究语言风格和对深层思想的理解。我们在三项任务上评估了CharacterBot的语言准确性和观点理解能力，结果表明它在我们的调整指标上显著优于基线模型。我们希望这项工作能启发未来关于深度角色模拟大语言模型的研究，同时考虑道德标准的重要性。", "summary": "CharacterBot是一种新颖的大语言模型，旨在进行深度角色模拟，超越表面事实，捕捉角色的语言和思维模式。该模型以鲁迅的散文为案例，采用多任务训练方法（包括预训练、多选问答、生成问答和风格迁移微调），并引入CharLoRA机制以优化学习。评估结果显示，CharacterBot在语言准确性和观点理解方面显著优于基线模型，为未来的伦理深度角色模拟研究奠定了基础。", "keywords": "角色模拟, 大语言模型, 深度角色, CharacterBot, 鲁迅", "comments": "本文的创新之处在于将角色模拟从表层事实提升到深层思维模式的捕捉，并引入了CharLoRA机制以实现多专家学习。以著名作家鲁迅作为案例研究，具有独特的文化背景和应用价值。此外，论文强调了伦理标准的重要性，这对于高级角色模拟技术的发展至关重要。"}}
{"id": "2507.18070", "title": "Modular Robot and Landmark Localisation Using Relative Bearing Measurements", "authors": ["Behzad Zamani", "Jochen Trumpf", "Chris Manzie"], "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to RA-L", "url": "http://arxiv.org/abs/2507.18070v1", "summary": "In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements.", "comment": "Submitted to RA-L", "pdf_url": "http://arxiv.org/pdf/2507.18070v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "模块化机器人和地标定位，使用相对方位测量", "tldr": "本文提出了一种模块化的非线性最小二乘滤波方法，用于处理包含独立子系统的系统，并通过协方差交集算法解决信息重复计算问题，并在机器人-地标定位问题中进行了验证。", "motivation": "解决由独立子系统组成的系统中，当相对测量同时依赖于多个子系统状态时，如何独立更新每个子系统的状态和误差协方差估计；防止当子系统之间共享估计时信息重复计算；以及处理机器人姿态和地标位置估计问题中的耦合。", "method": "提出了一种模块化非线性最小二乘滤波方法，用于由独立子系统组成的系统。即使相对测量同时依赖于多个子系统状态，也能独立更新每个子系统的状态和误差协方差估计。将协方差交集（CI）算法作为解决方案的一部分，以防止信息重复计算，并基于最小二乘估计对CI算法进行了替代推导。将该方法应用于机器人-地标定位问题，其中机器人对静止地标的方位角测量耦合了机器人姿态和地标位置的估计问题。", "result": "在随机模拟研究中，将所提出的模块化方法与单一联合状态滤波器进行了基准测试，以阐明它们各自的权衡。该研究还包括了所提出方法的变体，这些变体在降低通信和带宽要求的情况下实现了性能的平稳下降。", "conclusion": "该模块化非线性最小二乘滤波方法能够有效处理由独立子系统组成的定位问题，并通过协方差交集算法避免信息重复计算。在机器人-地标定位问题中表现出与单一联合状态滤波器不同的权衡，并且可以通过调整实现通信和带宽要求的降低，同时保持性能的平稳下降。", "translation": "在本文中，我们提出了一种模块化的非线性最小二乘滤波方法，用于由独立子系统组成的系统。即使当一个相对测量同时依赖于多个子系统的状态时，每个子系统的状态和误差协方差估计也能独立更新。我们将在解决方案中整合协方差交集（CI）算法，以防止当子系统之间共享估计时信息重复计算。基于最小二乘估计的CI算法的替代推导使得这种整合成为可能。我们将所提出的方法特别应用于机器人-地标定位问题。在这个问题中，对相对于移动机器人的SE(2)姿态测量的静止地标位置的方位角噪声测量耦合了机器人姿态和地标位置的估计问题。在随机模拟研究中，我们将所提出的模块化方法与单一联合状态滤波器进行基准测试，以阐明它们各自的权衡。在这项研究中，我们还包含了所提出方法的变体，这些变体在降低通信和带宽要求的情况下实现了性能的平稳下降。", "summary": "本文提出了一种模块化的非线性最小二乘滤波方法，旨在解决由独立子系统构成的系统中的状态估计问题。该方法允许子系统状态和误差协方差独立更新，并整合了协方差交集（CI）算法以避免信息重复计数，这得益于CI算法基于最小二乘估计的新推导。研究将该方法应用于机器人-地标定位问题，其中机器人姿态和地标位置的估计是耦合的。通过随机模拟，研究将该模块化方法与传统的单一联合状态滤波器进行了比较，并探讨了不同通信和带宽要求下性能的权衡。", "keywords": "模块化滤波, 非线性最小二乘, 协方差交集, 机器人定位, 地标定位", "comments": "这项研究的创新之处在于提出了一个模块化的非线性最小二乘滤波框架，特别是在处理多子系统交互和信息共享时的协方差交集算法的集成。通过基于最小二乘的CI推导，实现了更自然的整合。其重要性在于为分布式或模块化系统中的状态估计提供了一个鲁棒且可扩展的解决方案，特别是在通信受限的环境下，通过性能的平稳下降特性，为实际应用提供了灵活性。"}}
{"id": "2507.17761", "title": "Co-constructing Explanations for AI Systems using Provenance", "authors": ["Jan-Christoph Kalo", "Fina Polat", "Shubha Guha", "Paul Groth"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.17761v1", "summary": "Modern AI systems are complex workflows containing multiple components and\ndata sources. Data provenance provides the ability to interrogate and\npotentially explain the outputs of these systems. However, provenance is often\ntoo detailed and not contextualized for the user trying to understand the AI\nsystem. In this work, we present our vision for an interactive agent that works\ntogether with the user to co-construct an explanation that is simultaneously\nuseful to the user as well as grounded in data provenance. To illustrate this\nvision, we present: 1) an initial prototype of such an agent; and 2) a scalable\nevaluation framework based on user simulations and a large language model as a\njudge approach.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.17761v1", "cate": "cs.HC", "date": "2025-05-31", "updated": "2025-05-31", "AI": {"title_translation": "使用溯源共同构建AI系统解释", "tldr": "提出一个交互式智能体，利用数据溯源与用户共同构建AI系统解释，并展示了原型和评估框架。", "motivation": "现代AI系统是复杂的工作流程，其数据溯源信息通常过于详细且缺乏上下文，导致用户难以理解AI系统的输出。", "method": "本文提出了一个交互式智能体的愿景，该智能体与用户合作共同构建一个对用户有用且基于数据溯源的解释。为实现此愿景，文中展示了一个初始原型，并提出了一个基于用户模拟和大型语言模型作为裁判的可扩展评估框架。", "result": "本文展示了一个交互式解释智能体的初步原型，并提出了一个可扩展的评估框架。", "conclusion": "本文提出了一个通过交互式智能体与用户共同构建AI系统解释的愿景，该解释既对用户有用又植根于数据溯源，并为此提供了初步实现和评估方法。", "translation": "现代人工智能系统是包含多个组件和数据源的复杂工作流程。数据溯源提供了询问和潜在解释这些系统输出的能力。然而，溯源通常过于详细，并且对于试图理解人工智能系统的用户来说缺乏上下文。在这项工作中，我们提出了一个交互式智能体的愿景，它与用户一起共同构建一个对用户有用且基于数据溯源的解释。为了阐明这一愿景，我们展示了：1）这样一个智能体的初始原型；2）一个基于用户模拟和大型语言模型作为裁判方法的可扩展评估框架。", "summary": "本文提出了一个通过交互式智能体与用户共同构建AI系统解释的愿景，旨在解决现有数据溯源解释过于详细且缺乏上下文的问题。该智能体将与用户合作，生成既实用又基于数据溯源的解释。为验证此愿景，文中展示了一个初步原型，并提出了一个结合用户模拟和大型语言模型作为裁判的可扩展评估框架。", "keywords": "AI解释, 数据溯源, 交互式智能体, 共同构建, 可解释AI", "comments": "这项工作提出了一种新颖的方法来解决AI可解释性中的一个关键挑战：如何使复杂的溯源信息对最终用户有用。通过引入“共同构建”的概念和交互式智能体，它强调了用户在解释过程中的参与。提出的评估框架也很有趣，尤其是在结合用户模拟和LLM作为裁判方面，这可能为XAI系统的评估提供新的思路。"}}
{"id": "2507.18064", "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement", "authors": ["Xiaoran Sun", "Liyan Wang", "Cong Wang", "Yeying Jin", "Kin-man Lam", "Zhixun Su", "Yang Yang", "Jinshan Pan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18064v1", "summary": "Most existing low-light image enhancement (LLIE) methods rely on pre-trained\nmodel priors, low-light inputs, or both, while neglecting the semantic guidance\navailable from normal-light images. This limitation hinders their effectiveness\nin complex lighting conditions. In this paper, we propose VLM-IMI, a novel\nframework that leverages large vision-language models (VLMs) with iterative and\nmanual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions\nof the desired normal-light content as enhancement cues, enabling semantically\ninformed restoration. To effectively integrate cross-modal priors, we introduce\nan instruction prior fusion module, which dynamically aligns and fuses image\nand text features, promoting the generation of detailed and semantically\ncoherent outputs. During inference, we adopt an iterative and manual\ninstruction strategy to refine textual instructions, progressively improving\nvisual quality. This refinement enhances structural fidelity, semantic\nalignment, and the recovery of fine details under extremely low-light\nconditions. Extensive experiments across diverse scenarios demonstrate that\nVLM-IMI outperforms state-of-the-art methods in both quantitative metrics and\nperceptual quality. The source code is available at\nhttps://github.com/sunxiaoran01/VLM-IMI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18064v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用迭代和手动指令调整大型视觉语言模型以实现生成式低光增强", "tldr": "提出VLM-IMI框架，利用大型视觉语言模型和迭代/手动指令进行低光图像增强，通过语义指导优于现有方法。", "motivation": "现有低光图像增强方法忽略了正常光照图像中的语义指导，限制了它们在复杂光照条件下的有效性。", "method": "提出VLM-IMI框架，利用大型视觉语言模型（VLMs）和迭代/手动指令（IMIs）进行低光图像增强。它将正常光照内容的文本描述作为增强线索。引入指令先验融合模块，动态对齐和融合图像与文本特征。推理时，采用迭代和手动指令策略来优化文本指令，逐步提高视觉质量。", "result": "VLM-IMI在各种场景下的广泛实验表明，其在定量指标和感知质量方面均优于现有最先进方法。", "conclusion": "VLM-IMI是一种有效且优越的低光图像增强方法，通过结合语义指导和迭代优化，显著提升了图像质量。", "translation": "大多数现有的低光图像增强（LLIE）方法依赖于预训练模型先验、低光输入或两者兼有，同时忽略了正常光照图像中可用的语义指导。这种限制阻碍了它们在复杂光照条件下的有效性。在本文中，我们提出VLM-IMI，一个利用大型视觉语言模型（VLMs）与迭代和手动指令（IMIs）进行LLIE的新颖框架。VLM-IMI将所需正常光照内容的文本描述作为增强线索，从而实现语义知情的恢复。为了有效整合跨模态先验，我们引入了一个指令先验融合模块，该模块动态对齐和融合图像和文本特征，促进生成详细且语义一致的输出。在推理过程中，我们采用迭代和手动指令策略来细化文本指令，逐步提高视觉质量。这种细化增强了结构保真度、语义对齐以及在极低光照条件下的细节恢复。在不同场景下的大量实验表明，VLM-IMI在定量指标和感知质量方面均优于现有最先进方法。源代码可在https://github.com/sunxiaoran01/VLM-IMI获得。", "summary": "本文提出VLM-IMI，一种新颖的低光图像增强框架，通过引入正常光照图像的语义指导来弥补现有方法的不足。该框架利用大型视觉语言模型（VLMs）和迭代/手动指令（IMIs），将文本描述作为增强线索。指令先验融合模块用于整合跨模态特征，迭代优化策略则提升视觉质量。实验证明VLM-IMI在定量和感知质量上均优于现有先进方法。", "keywords": "低光增强, 视觉语言模型, 迭代指令, 语义指导, 跨模态融合", "comments": "该论文的创新之处在于利用大型视觉语言模型和迭代/手动文本指令进行低光增强，引入了以往被忽视的语义指导。指令先验融合模块是有效跨模态整合的关键组成部分。这种方法解决了现有方法的一个显著局限性，从而在复杂光照条件下提升了性能。"}}
{"id": "2507.18300", "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection", "authors": ["Jincheng Li", "Chunyu Xie", "Ji Ao", "Dawei Leng", "Yuhui Yin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.18300v1", "summary": "Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.18300v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LMM-Det：让大型多模态模型在目标检测中表现出色", "tldr": "LMM-Det使大型多模态模型无需专用检测模块即可有效执行目标检测，通过优化数据和推理来弥补其与专业检测器之间的性能差距。", "motivation": "现有的大型多模态模型（LMMs）在图像标注、视觉问答等任务中表现出色，但在目标检测方面与专业检测器存在显著差距，本文旨在弥补这一差距。", "method": "本文提出了LMM-Det，一种简单而有效的方法，利用大型多模态模型进行原生的目标检测，不依赖专门的检测模块。通过探索性分析发现LMMs在目标检测中召回率显著下降，为此，LMM-Det引入了针对目标检测的数据分布调整和推理优化来提高召回率，并重新组织指令对话以增强LMMs的目标检测能力。", "result": "广泛的实验支持了LMM-Det的主张，即大型多模态模型本身具备检测能力，无需额外检测模块，并展示了其多功能性和有效性。", "conclusion": "本文主张大型多模态模型无需任何额外的检测模块即可具备目标检测能力。LMM-Det通过数据分布调整、推理优化和指令对话重组，有效提升了LMMs在目标检测任务上的表现，弥补了与专业检测器的差距。", "translation": "大型多模态模型（LMMs）因其在多模态理解、推理和上下文学习等方面的卓越能力，在人工智能研究和工业界获得了广泛关注和兴趣。尽管LMMs在处理图像标注、视觉问答和视觉定位等多模态任务中表现出良好的前景，但LMMs的目标检测能力与专业检测器相比存在显著差距。为了弥补这一差距，我们摒弃了将重型检测器与LMMs集成的传统方法，提出了一种简单而有效的方法LMM-Det，该方法利用大型多模态模型进行原生的目标检测，而无需依赖专门的检测模块。具体而言，我们对大型多模态模型与目标检测结合时进行了全面的探索性分析，发现与专业检测模型相比，召回率显著下降。为了缓解这一问题，我们提出通过引入针对目标检测的数据分布调整和推理优化来提高召回率。我们重新组织指令对话以增强大型多模态模型的目标检测能力。我们声称大型多模态模型无需任何额外检测模块即可具备检测能力。广泛的实验支持了我们的主张，并展示了多功能LMM-Det的有效性。数据集、模型和代码可在https://github.com/360CVGroup/LMM-Det获取。", "summary": "本文提出LMM-Det，旨在弥补大型多模态模型（LMMs）在目标检测方面与专业检测器之间的性能差距。LMM-Det通过利用LMMs自身能力进行原生目标检测，避免集成额外检测模块。针对LMMs在目标检测中召回率低的问题，LMM-Det引入了数据分布调整和推理优化，并重新组织指令对话。实验结果证实，LMMs无需额外检测模块即可具备检测能力，且LMM-Det表现出有效性。", "keywords": "大型多模态模型, 目标检测, LMM-Det, 召回率, 指令对话", "comments": "这项工作具有创新性，因为它挑战了将LMMs与传统重型检测器结合的范式，转而探索LMMs自身在目标检测方面的潜力。通过优化数据处理和推理过程，并调整指令对话，LMM-Det为LMMs在视觉任务中的应用开辟了新路径，证明了LMMs在不引入复杂专用模块的情况下也能胜任目标检测任务，这对于多模态模型的统一和简化具有重要意义。"}}
{"id": "2507.17897", "title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)", "authors": ["Semih Eren", "Deniz Kucukahmetler", "Nico Scherf"], "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts Project session (3rd-place team). Code: this https URL", "url": "http://arxiv.org/abs/2507.17897v1", "summary": "Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.", "comment": "8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN", "pdf_url": "http://arxiv.org/pdf/2507.17897v1", "cate": "q-bio.NC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "多模态循环集成模型用于预测对自然电影的脑部反应（Algonauts 2025）", "tldr": "该研究提出了一种多模态循环集成模型，通过整合视觉、听觉和语言信息，准确预测了观看自然电影时的大脑皮层反应，并在Algonauts 2025挑战赛中取得了第三名的成绩。", "motivation": "准确预测对自然刺激的分布式皮层反应需要能够随时间整合视觉、听觉和语义信息的模型。", "method": "我们提出了一种分层多模态循环集成模型，将预训练的视频、音频和语言嵌入映射到观看电影时记录的fMRI时间序列。模型包含特定模态的双向RNN编码时间动态，其隐藏状态融合后传递给第二个循环层，并由轻量级受试者特定头部输出1000个皮层区域的反应。训练采用复合MSE-相关性损失，并采用逐步从早期感觉区域向晚期关联区域转移重点的课程学习策略。通过平均100个模型变体进一步提高了鲁棒性。", "result": "该系统在竞赛排行榜上排名第三，总体Pearson r值为0.2094，并在所有参与者中取得了最高的单像素峰值得分（平均r = 0.63），尤其在最具挑战性的受试者（受试者5）上取得了显著提升。", "conclusion": "该方法为未来的多模态脑编码基准建立了一个简单、可扩展的基线。", "translation": "准确预测对自然刺激的分布式皮层反应需要能够随时间整合视觉、听觉和语义信息的模型。我们提出了一种分层多模态循环集成模型，将预训练的视频、音频和语言嵌入映射到四名受试者观看近80小时由Algonauts 2025挑战赛提供的电影时记录的fMRI时间序列。特定模态的双向RNN编码时间动态；它们的隐藏状态被融合并传递到第二个循环层，轻量级的受试者特定头部输出1000个皮层区域的反应。训练依赖于复合MSE-相关性损失和逐步将重点从早期感觉区域转移到晚期关联区域的课程。平均100个模型变体进一步提高了鲁棒性。由此产生的系统在竞赛排行榜上排名第三，总体Pearson r值为0.2094，并在所有参与者中取得了最高的单像素峰值得分（平均r = 0.63），尤其在最具挑战性的受试者（受试者5）上取得了显著提升。该方法为未来的多模态脑编码基准建立了一个简单、可扩展的基线。", "summary": "本研究提出了一种分层多模态循环集成模型，用于预测人类大脑对自然电影的反应。该模型整合了预训练的视频、音频和语言嵌入，并通过双向RNNs捕获时间动态，最终映射到fMRI时间序列。通过复合损失函数和课程学习策略进行训练，并在Algonauts 2025挑战赛中取得了第三名的成绩，尤其在单像素峰值得分上表现突出。该方法为未来多模态脑编码研究提供了一个简单且可扩展的基线。", "keywords": "多模态, 脑反应, 循环集成, fMRI, 自然电影", "comments": "该论文提出了一种有效且可扩展的多模态脑编码模型，其创新之处在于结合了分层循环网络、多模态嵌入融合、复合损失函数和课程学习策略。在Algonauts 2025挑战赛中取得的优异成绩（尤其是最高的单像素峰值得分）证明了其有效性。该方法为未来该领域的研究提供了一个坚实且易于扩展的基线，对于理解大脑如何整合多模态信息具有重要意义。"}}
{"id": "2409.03872", "title": "Continuous data assimilation for hydrodynamics: consistent discretization and application to moment recovery", "authors": ["Jingcheng Lu", "Kunlun Qi", "Li Wang", "Jeff Calder"], "categories": ["math.NA", "cs.NA", "93C20, 70S10, 65Dxx, 42A15, 82C40"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Updated version in Journal of Computational Physics", "url": "http://arxiv.org/abs/2409.03872v3", "summary": "Motivated by the challenge of moment recovery in hydrodynamic approximation\nin kinetic theory, we propose a data-driven approach for the hydrodynamic\nmodels. Inspired by continuous data assimilation, our method introduces a\nrelaxation-based nudging system coupled with a novel discretization technique.\nThis approach facilitates the simultaneous recovery of both the force term and\na high-resolution solution from sparsely observed data. To address potential\nnumerical artifacts, we use kernel regression to fit the observed data. We also\nanalyze the convergence of the proposed nudging system under both full and\npartial data scenarios. When applied to moment systems, the source term\ninvolves the derivative of higher-order moments, our approach serves as a\ncrucial step for data preparation in machine-learning based moment closure\nmodels. Multiple numerical experiments demonstrate the effectiveness of our\nalgorithm, and we discuss its potential extension to high-dimensional systems.", "comment": "Updated version in Journal of Computational Physics", "pdf_url": "http://arxiv.org/pdf/2409.03872v3", "cate": "math.NA", "date": "2024-09-05", "updated": "2025-07-23", "AI": {"title_translation": "水动力学中的连续数据同化：一致离散化和矩恢复应用", "tldr": "该研究提出了一种基于松弛的推挽系统和新颖离散化技术的数据驱动方法，用于水动力学模型中的矩恢复，能从稀疏数据中同时恢复力项和高分辨率解，并对系统收敛性进行了分析。", "motivation": "解决动理学理论中水动力学近似中矩恢复的挑战。", "method": "提出了一种数据驱动的水动力学模型方法，灵感来源于连续数据同化，引入了基于松弛的推挽系统，并结合了一种新颖的离散化技术。使用核回归来拟合观测数据以解决潜在的数值伪影。还分析了在完整和部分数据场景下所提出的推挽系统的收敛性。", "result": "该方法能够从稀疏观测数据中同时恢复力项和高分辨率解。多个数值实验证明了该算法的有效性。", "conclusion": "该方法在应用于矩系统时，可作为基于机器学习的矩闭合模型中数据准备的关键一步。该算法在数值实验中表现出有效性，并有潜力扩展到高维系统。", "translation": "受动理学理论中水动力学近似中矩恢复挑战的启发，我们提出了一种用于水动力学模型的数据驱动方法。受连续数据同化的启发，我们的方法引入了一种基于松弛的推挽系统，并结合了一种新颖的离散化技术。这种方法有助于从稀疏观测数据中同时恢复力项和高分辨率解。为了解决潜在的数值伪影，我们使用核回归来拟合观测数据。我们还分析了所提出的推挽系统在完整和部分数据场景下的收敛性。当应用于矩系统时，源项涉及高阶矩的导数，我们的方法可作为基于机器学习的矩闭合模型中数据准备的关键一步。多项数值实验证明了我们算法的有效性，我们讨论了其扩展到高维系统的潜力。", "summary": "本文提出了一种用于水动力学模型的数据驱动方法，旨在解决动理学理论中矩恢复的挑战。该方法结合了连续数据同化思想，引入了基于松弛的推挽系统和新颖的离散化技术，并利用核回归处理数据。它能从稀疏观测数据中同时恢复力项和高分辨率解。研究还分析了系统的收敛性，并通过数值实验验证了其有效性，指出其在机器学习驱动的矩闭合模型数据准备中的重要作用。", "keywords": "连续数据同化, 水动力学, 矩恢复, 数据驱动, 推挽系统", "comments": "该论文提出了一种创新的数据驱动方法，结合了连续数据同化和新型离散化技术，以解决水动力学模型中的矩恢复问题。其亮点在于能够从稀疏数据中同时恢复力项和高分辨率解，并对系统收敛性进行了理论分析。该方法在为基于机器学习的矩闭合模型准备数据方面具有重要意义，显示了其在多物理场和数据科学交叉领域的应用潜力。"}}
{"id": "2507.17874", "title": "I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis", "authors": ["SaiBarath Sundar", "Pranav Satheesan", "Udayaadithya Avadhanam"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17874v1", "summary": "Recent advances in agentic systems for data analysis have emphasized\nautomation of insight generation through multi-agent frameworks, and\norchestration layers. While these systems effectively manage tasks like query\ntranslation, data transformation, and visualization, they often overlook the\nstructured reasoning process underlying analytical thinking. Reasoning large\nlanguage models (LLMs) used for multi-step problem solving are trained as\ngeneral-purpose problem solvers. As a result, their reasoning or thinking steps\ndo not adhere to fixed processes for specific tasks. Real-world data analysis\nrequires a consistent cognitive workflow: interpreting vague goals, grounding\nthem in contextual knowledge, constructing abstract plans, and adapting\nexecution based on intermediate outcomes. We introduce I2I-STRADA\n(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an\nagentic architecture designed to formalize this reasoning process. I2I-STRADA\nfocuses on modeling how analysis unfolds via modular sub-tasks that reflect the\ncognitive steps of analytical reasoning. Evaluations on the DABstep and DABench\nbenchmarks show that I2I-STRADA outperforms prior systems in planning coherence\nand insight alignment, highlighting the importance of structured cognitive\nworkflows in agent design for data analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17874v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "I2I-STRADA——通过结构化推理代理进行数据分析，从信息到洞察", "tldr": "I2I-STRADA是一个用于数据分析的代理架构，它通过模拟认知步骤的模块化子任务来形式化推理过程，从而在规划连贯性和洞察对齐方面优于现有系统。", "motivation": "现有的数据分析代理系统忽视了结构化推理过程，而大型语言模型（LLMs）在多步问题解决中，其推理步骤不遵循特定任务的固定流程。真实世界的数据分析需要一个一致的认知工作流，包括解释模糊目标、将其与上下文知识结合、构建抽象计划以及根据中间结果调整执行。", "method": "本文介绍了I2I-STRADA（通过结构化推理代理进行数据分析，从信息到洞察），这是一种代理架构，旨在形式化数据分析中的推理过程。I2I-STRADA的重点是通过反映分析推理认知步骤的模块化子任务来建模分析如何展开。", "result": "在DABstep和DABench基准测试上的评估表明，I2I-STRADA在规划连贯性和洞察对齐方面优于先前的系统。", "conclusion": "I2I-STRADA的成功突出了在数据分析的代理设计中，结构化认知工作流的重要性。", "translation": "数据分析中代理系统的最新进展强调了通过多代理框架和编排层实现洞察生成的自动化。虽然这些系统有效地管理查询翻译、数据转换和可视化等任务，但它们常常忽视了分析思维背后的结构化推理过程。用于多步问题解决的推理大型语言模型（LLMs）被训练为通用问题解决器。因此，它们的推理或思考步骤不遵循特定任务的固定流程。真实世界的数据分析需要一个一致的认知工作流：解释模糊目标、将其与上下文知识结合、构建抽象计划，并根据中间结果调整执行。我们引入了I2I-STRADA（通过结构化推理代理进行数据分析，从信息到洞察），这是一种旨在形式化此推理过程的代理架构。I2I-STRADA专注于通过反映分析推理认知步骤的模块化子任务来建模分析如何展开。在DABstep和DABench基准测试上的评估表明，I2I-STRADA在规划连贯性和洞察对齐方面优于先前的系统，突出了在数据分析的代理设计中结构化认知工作流的重要性。", "summary": "I2I-STRADA是一个新颖的代理架构，旨在解决现有数据分析代理系统在结构化推理方面的不足。它通过模拟数据分析的认知步骤，将复杂的分析过程分解为模块化子任务，从而形式化了推理过程。在DABstep和DABench基准测试上的评估显示，I2I-STRADA在规划连贯性和洞察对齐方面表现优异，强调了在数据分析代理设计中结构化认知工作流的关键作用。", "keywords": "数据分析, 代理系统, 结构化推理, 认知工作流, 大型语言模型", "comments": "该论文创新性地提出了一种结构化推理代理架构I2I-STRADA，解决了现有数据分析代理系统中缺乏固定认知工作流的问题。通过将分析过程分解为模块化子任务，I2I-STRADA更好地模拟了人类分析思维，并在基准测试中取得了显著的性能提升，这对于未来智能数据分析系统发展具有重要意义。"}}
{"id": "2507.17917", "title": "Modular and Automated Workflow for Streamlined Raman Signal Analysis", "authors": ["Mykyta Kizilov", "Vsevolod Cheburkanov", "Joseph Harrington", "Vladislav V. Yakovlev"], "categories": ["physics.optics", "eess.SP", "physics.chem-ph"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "Comments:      Preprint. Submitted to Journal of Raman Spectroscopy", "url": "http://arxiv.org/abs/2507.17917v1", "summary": "Raman spectroscopy is a powerful tool for material characterization. However,\ncareful preprocessing is required for the identification and handling of noise,\nbaseline drift, and random spikes. This paper presents a comprehensive approach\nto generating and preprocessing Raman spectra. Additionally, we describe\nmethods for fitting Voigt peaks to the spectrum to determine peak parameters.\nThe effectiveness of these methods is demonstrated using both synthetic and\nreal Raman spectra, with code provided in an open-source GitHub repository.", "comment": "Preprint. Submitted to Journal of Raman Spectroscopy", "pdf_url": "http://arxiv.org/pdf/2507.17917v1", "cate": "physics.optics", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "模块化自动化流程，用于简化拉曼信号分析", "tldr": "本文提出了一种模块化自动化工作流，用于拉曼光谱的生成、预处理和峰拟合。", "motivation": "拉曼光谱在材料表征中非常强大，但其信号分析需要仔细的预处理来识别和处理噪声、基线漂移和随机尖峰。", "method": "本文提出了一种生成和预处理拉曼光谱的综合方法，并描述了将Voigt峰拟合到光谱以确定峰参数的方法。所有代码都在一个开源GitHub仓库中提供。", "result": "这些方法的有效性通过合成和真实的拉曼光谱得到了验证。", "conclusion": "本文成功展示了一种模块化自动化的拉曼信号分析工作流，能够有效处理噪声并确定峰参数。", "translation": "拉曼光谱是材料表征的强大工具。然而，为了识别和处理噪声、基线漂移和随机尖峰，需要仔细的预处理。本文提出了一种生成和预处理拉曼光谱的综合方法。此外，我们描述了将Voigt峰拟合到光谱以确定峰参数的方法。这些方法的有效性通过合成和真实的拉曼光谱得到了证明，并且代码在一个开源的GitHub仓库中提供。", "summary": "本文介绍了一个模块化自动化的工作流，旨在简化拉曼信号分析。该工作流涵盖拉曼光谱的生成、预处理（处理噪声、基线漂移和尖峰），以及使用Voigt峰拟合确定峰参数的方法。通过合成和真实光谱验证了其有效性，并提供了开源代码。", "keywords": "拉曼光谱, 信号分析, 预处理, Voigt峰拟合, 自动化", "comments": "本文的创新之处在于提供了一个模块化且自动化的拉曼信号分析工作流，解决了传统拉曼光谱分析中预处理复杂性问题。开源代码的提供也大大增强了其应用价值和可复现性。"}}
{"id": "2505.16908", "title": "Is Circuit Depth Accurate for Comparing Quantum Circuit Runtimes?", "authors": ["Matthew Tremba", "Paul Hovland", "Ji Liu"], "categories": ["quant-ph", "cs.ET"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2505.16908v2", "summary": "Although quantum circuit depth is commonly used to approximate circuit\nruntimes, it overlooks a prevailing trait of current hardware implementation:\ndifferent gates have different execution times. Recognizing the potential for\ndiscrepancies, we investigate depth's accuracy for comparing runtimes between\ncompiled versions of the same circuit. In particular, we assess the accuracy of\ntraditional and multi-qubit depth for (1) predicting relative differences in\nruntime and (2) identifying compiled circuit version(s) with the shortest\nruntime. Finding that circuit depth is not accurate for either task, we\nintroduce a new metric, gate-aware depth, that weights gates' contributions to\nruntime using an architecture's average gate execution times. Using average\ngate times allows gate-aware depth to capture variations by gate type without\nrequiring exact knowledge of all gate times, increasing accuracy while\nmaintaining portability across devices of the same architecture. Compared to\ntraditional and multi-qubit depth, gate-aware depth reduces the average\nrelative error of predictions in task (1) by 68 and 18 times and increases the\naverage number of correct identifications in task (2) by 20 and 43 percentage\npoints, respectively. Finally, we provide gate-aware depth weight\nconfigurations for current IBM Eagle and Heron architectures.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2505.16908v2", "cate": "quant-ph", "date": "2025-05-22", "updated": "2025-07-24", "AI": {"title_translation": "量子电路深度在比较量子电路运行时是否准确？", "tldr": "量子电路深度在比较运行时方面不准确；论文提出了一种新的门感知深度指标，其准确性显著提高，能更好地预测运行时差异和识别最短运行时版本。", "motivation": "尽管量子电路深度常用于估算运行时，但它忽略了当前硬件实现中不同门执行时间不同的特点，导致潜在差异。因此，研究其准确性是必要的。", "method": "研究了传统和多量子位深度在预测运行时相对差异和识别最短运行时电路版本方面的准确性。提出了一种新的门感知深度指标，该指标利用架构的平均门执行时间来加权门对运行时的贡献。", "result": "发现传统电路深度在两个任务中均不准确。与传统和多量子位深度相比，门感知深度将任务（1）的平均相对误差分别降低了68倍和18倍，并将任务（2）中正确识别的平均数量分别增加了20和43个百分点。此外，提供了IBM Eagle和Heron架构的门感知深度权重配置。", "conclusion": "量子电路深度在比较运行时方面不准确。门感知深度是一种更准确的替代方案，能显著提高运行时预测和最短运行时版本识别的准确性。", "translation": "尽管量子电路深度通常用于近似电路运行时，但它忽略了当前硬件实现的一个普遍特征：不同门具有不同的执行时间。认识到这种潜在差异，我们研究了深度在比较同一电路不同编译版本运行时方面的准确性。特别是，我们评估了传统和多量子位深度在（1）预测运行时相对差异和（2）识别运行时最短的编译电路版本方面的准确性。发现电路深度在这两项任务中均不准确后，我们引入了一种新的度量标准——门感知深度，它利用架构的平均门执行时间来加权门对运行时的贡献。使用平均门时间使门感知深度能够在不需要精确了解所有门时间的情况下捕获按门类型变化的差异，从而在提高准确性的同时保持在相同架构设备间的可移植性。与传统和多量子位深度相比，门感知深度将任务（1）中的预测平均相对误差分别降低了68倍和18倍，并将任务（2）中中正确识别的平均数量分别增加了20和43个百分点。最后，我们提供了当前IBM Eagle和Heron架构的门感知深度权重配置。", "summary": "本研究指出，量子电路深度在比较电路运行时方面存在不足，因为它忽略了不同量子门执行时间差异。为解决此问题，论文提出了一种新的度量标准——门感知深度，该指标通过加权不同门的贡献，显著提高了运行时预测和最短运行时版本识别的准确性。实验结果表明，门感知深度在预测误差和识别准确率方面均远优于传统方法，并提供了针对IBM Eagle和Heron架构的权重配置。", "keywords": "量子电路, 运行时, 电路深度, 门感知深度, 量子硬件", "comments": "该论文的创新点在于提出了门感知深度这一新度量，它克服了传统电路深度忽略不同门执行时间差异的局限性。这对于在实际量子硬件上优化量子程序运行时具有重要意义。尽管该方法依赖于平均门执行时间而非精确时间，且可移植性主要限于相同架构内，但其显著的性能提升证明了其价值。"}}
{"id": "2411.06106", "title": "Towards a Universal 3D Medical Multi-modality Generalization via Learning Personalized Invariant Representation", "authors": ["Zhaorui Tan", "Xi Yang", "Tan Pan", "Tianyi Liu", "Chen Jiang", "Xin Guo", "Qiufeng Wang", "Anh Nguyen", "Yuan Qi", "Kaizhu Huang", "Yuan Cheng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV25", "url": "http://arxiv.org/abs/2411.06106v4", "summary": "Variations in medical imaging modalities and individual anatomical\ndifferences pose challenges to cross-modality generalization in multi-modal\ntasks. Existing methods often concentrate exclusively on common anatomical\npatterns, thereby neglecting individual differences and consequently limiting\ntheir generalization performance. This paper emphasizes the critical role of\nlearning individual-level invariance, i.e., personalized representation\n$\\mathbb{X}_h$, to enhance multi-modality generalization under both homogeneous\nand heterogeneous settings. It reveals that mappings from individual biological\nprofile to different medical modalities remain static across the population,\nwhich is implied in the personalization process. We propose a two-stage\napproach: pre-training with invariant representation $\\mathbb{X}_h$ for\npersonalization, then fine-tuning for diverse downstream tasks. We provide both\ntheoretical and empirical evidence demonstrating the feasibility and advantages\nof personalization, showing that our approach yields greater generalizability\nand transferability across diverse multi-modal medical tasks compared to\nmethods lacking personalization. Extensive experiments further validate that\nour approach significantly enhances performance in various generalization\nscenarios.", "comment": "Accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2411.06106v4", "cate": "cs.CV", "date": "2024-11-09", "updated": "2025-07-24", "AI": {"title_translation": "迈向通用的3D医学多模态泛化：通过学习个性化不变表示", "tldr": "通过学习个性化不变表示，本研究旨在提高医学图像中跨模态的泛化能力，解决现有方法忽视个体差异的问题。", "motivation": "医学成像模态和个体解剖差异导致多模态任务中跨模态泛化面临挑战。现有方法通常只关注共同的解剖模式，从而忽略了个体差异，限制了泛化性能。", "method": "提出一种两阶段方法：首先通过不变表示（个性化表示 $\\mathbb{X}_h$）进行预训练以实现个性化，然后对各种下游任务进行微调。该方法基于个体生物学特征到不同医学模态的映射在人群中保持静态的发现。", "result": "理论和实证证据表明个性化的可行性和优势。与缺乏个性化的方法相比，本方法在各种多模态医学任务中表现出更强的泛化能力和可迁移性。广泛的实验进一步验证了本方法显著提升了各种泛化场景的性能。", "conclusion": "学习个体层面的不变性，即个性化表示，对于增强医学多模态泛化至关重要，能够显著提高泛化能力和可迁移性。", "translation": "医学成像模态和个体解剖差异对多模态任务中的跨模态泛化提出了挑战。现有方法通常只专注于共同的解剖模式，从而忽略了个体差异，并因此限制了其泛化性能。本文强调学习个体层面不变性，即个性化表示$\\mathbb{X}_h$，对于增强同质和异质设置下的多模态泛化至关重要。它揭示了个体生物学特征到不同医学模态的映射在人群中保持静态，这隐含在个性化过程中。我们提出了一种两阶段方法：使用不变表示$\\mathbb{X}_h$进行预训练以实现个性化，然后针对不同的下游任务进行微调。我们提供了理论和经验证据，证明了个性化的可行性和优势，表明与缺乏个性化的方法相比，我们的方法在各种多模态医学任务中产生了更大的泛化能力和可迁移性。广泛的实验进一步验证了我们的方法显著提升了各种泛化场景的性能。", "summary": "本论文旨在解决医学成像中跨模态泛化面临的挑战，这些挑战源于模态差异和个体解剖差异。为克服现有方法忽略个体差异的局限性，论文提出学习个体层面的不变表示（个性化表示$\\mathbb{X}_h$），以增强同质和异质设置下的多模态泛化能力。该方法采用两阶段策略：首先利用不变表示进行预训练以实现个性化，随后对不同的下游任务进行微调。研究提供了理论和实证证据，证明了个性化方法的有效性，显示其在多模态医学任务中比非个性化方法具有更强的泛化能力和可迁移性。", "keywords": "医学成像, 多模态泛化, 个性化表示, 不变表示, 跨模态", "comments": "本研究的创新点在于强调并利用个体层面的不变性（个性化表示）来解决多模态泛化问题，这弥补了现有方法仅关注共同模式而忽视个体差异的不足。这对于开发更鲁棒、更准确的医疗AI系统具有重要意义。"}}
{"id": "2506.12006", "title": "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023", "authors": ["Navodini Wijethilake", "Reuben Dorent", "Marina Ivory", "Aaron Kujawa", "Stefan Cornelissen", "Patrick Langenhuizen", "Mohamed Okasha", "Anna Oviedova", "Hexin Dong", "Bogyeong Kang", "Guillaume Sallé", "Luyi Han", "Ziyuan Zhao", "Han Liu", "Yubo Fan", "Tao Yang", "Shahad Hardan", "Hussain Alasmawi", "Santosh Sanjeev", "Yuzhou Zhuang", "Satoshi Kondo", "Maria Baldeon Calisto", "Shaikh Muhammad Uzair Noman", "Cancan Chen", "Ipek Oguz", "Rongguo Zhang", "Mina Rezaei", "Susana K. Lai-Yuen", "Satoshi Kasai", "Yunzhi Huang", "Chih-Cheng Hung", "Mohammad Yaqub", "Lisheng Wang", "Benoit M. Dawant", "Cuntai Guan", "Ritse Mann", "Vincent Jaouen", "Tae-Eui Kam", "Li Zhang", "Jonathan Shapey", "Tom Vercauteren"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.12006v3", "summary": "The cross-Modality Domain Adaptation (crossMoDA) challenge series, initiated\nin 2021 in conjunction with the International Conference on Medical Image\nComputing and Computer Assisted Intervention (MICCAI), focuses on unsupervised\ncross-modality segmentation, learning from contrast-enhanced T1 (ceT1) and\ntransferring to T2 MRI. The task is an extreme example of domain shift chosen\nto serve as a meaningful and illustrative benchmark. From a clinical\napplication perspective, it aims to automate Vestibular Schwannoma (VS) and\ncochlea segmentation on T2 scans for more cost-effective VS management. Over\ntime, the challenge objectives have evolved to enhance its clinical relevance.\nThe challenge evolved from using single-institutional data and basic\nsegmentation in 2021 to incorporating multi-institutional data and Koos grading\nin 2022, and by 2023, it included heterogeneous routine data and\nsub-segmentation of intra- and extra-meatal tumour components. In this work, we\nreport the findings of the 2022 and 2023 editions and perform a retrospective\nanalysis of the challenge progression over the years. The observations from the\nsuccessive challenge contributions indicate that the number of outliers\ndecreases with an expanding dataset. This is notable since the diversity of\nscanning protocols of the datasets concurrently increased. The winning approach\nof the 2023 edition reduced the number of outliers on the 2021 and 2022 testing\ndata, demonstrating how increased data heterogeneity can enhance segmentation\nperformance even on homogeneous data. However, the cochlea Dice score declined\nin 2023, likely due to the added complexity from tumour sub-annotations\naffecting overall segmentation performance. While progress is still needed for\nclinically acceptable VS segmentation, the plateauing performance suggests that\na more challenging cross-modal task may better serve future benchmarking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.12006v3", "cate": "eess.IV", "date": "2025-06-13", "updated": "2025-07-24", "AI": {"title_translation": "crossMoDA 挑战：2021年至2023年间前庭神经鞘瘤和耳蜗分割的跨模态域适应技术演进", "tldr": "crossMoDA挑战系列专注于从ceT1到T2 MRI的无监督跨模态分割，旨在自动化前庭神经鞘瘤（VS）和耳蜗分割。挑战从2021年到2023年不断演进，数据集和任务复杂性增加。研究发现，随着数据集的扩展和异质性增加，异常值减少，但2023年耳蜗Dice分数下降。", "motivation": "自动化T2扫描上的前庭神经鞘瘤（VS）和耳蜗分割，以实现更具成本效益的VS管理。同时，作为无监督跨模态分割的一个有意义且具有启发性的基准。", "method": "crossMoDA挑战系列专注于从对比增强T1 (ceT1) 学习并转移到T2 MRI的无监督跨模态分割。挑战从2021年使用单一机构数据和基本分割，到2022年纳入多机构数据和Koos分级，再到2023年包含异构常规数据和肿瘤内部与外部成分的亚分割。本文报告了2022年和2023年挑战的结果，并对挑战多年来的进展进行了回顾性分析。", "result": "随着数据集的扩展，异常值数量减少，尽管扫描协议的多样性同时增加。2023年获胜方法在2021年和2022年测试数据上减少了异常值，表明数据异质性增加可以提高分割性能，即使在同质数据上也是如此。然而，2023年耳蜗Dice分数下降，可能由于肿瘤亚注释增加了复杂性。", "conclusion": "尽管在临床可接受的VS分割方面仍需努力，但性能趋于稳定表明，更具挑战性的跨模态任务可能更适合未来的基准测试。", "translation": "cross-Modality Domain Adaptation (crossMoDA) 挑战系列于2021年与国际医学图像计算与计算机辅助干预大会 (MICCAI) 联合发起，专注于无监督跨模态分割，即从对比增强T1 (ceT1) 学习并转移到T2 MRI。该任务是域偏移的一个极端示例，旨在作为有意义且具有启发性的基准。从临床应用角度来看，它旨在自动化T2扫描上的前庭神经鞘瘤 (VS) 和耳蜗分割，以实现更具成本效益的VS管理。随着时间的推移，挑战目标不断演变以增强其临床相关性。挑战从2021年使用单一机构数据和基本分割，发展到2022年纳入多机构数据和Koos分级，到2023年则包含异构常规数据以及肿瘤内和外耳道成分的亚分割。在这项工作中，我们报告了2022年和2023年挑战赛的结果，并对多年来的挑战进展进行了回顾性分析。连续挑战贡献的观察结果表明，随着数据集的扩展，异常值数量减少。这一点值得注意，因为数据集的扫描协议多样性同时增加了。2023年获胜方法在2021年和2022年测试数据上减少了异常值，证明了数据异质性增加如何即使在同质数据上也能提高分割性能。然而，2023年耳蜗Dice分数下降，可能是由于肿瘤亚注释增加了复杂性，影响了整体分割性能。尽管在临床可接受的VS分割方面仍需取得进展，但性能趋于稳定表明，更具挑战性的跨模态任务可能更适合未来的基准测试。", "summary": "crossMoDA挑战系列自2021年启动以来，专注于从ceT1到T2 MRI的无监督跨模态前庭神经鞘瘤和耳蜗分割。该挑战旨在为医学图像域适应提供基准，并自动化临床分割。挑战任务和数据集复杂性逐年增加，从单一机构数据和基本分割发展到多机构、异构数据和亚分割任务。对2022年和2023年挑战的分析表明，数据集的扩展和异质性增加有助于减少异常值并提高分割性能。然而，2023年耳蜗分割性能有所下降，可能与任务复杂性增加有关。研究指出，尽管VS分割仍需改进，但性能趋于稳定，未来可能需要更具挑战性的跨模态任务进行基准测试。", "keywords": "跨模态域适应, 医学图像分割, 前庭神经鞘瘤, 耳蜗, crossMoDA挑战", "comments": "这篇论文回顾了crossMoDA挑战系列在跨模态域适应方面的演变。其创新之处在于提供了一个持续演进的、具有临床相关性的基准，用于评估无监督域适应技术在医学图像分割中的表现。挑战通过逐步增加数据集的异质性和任务复杂性，推动了该领域的研究。论文的发现，即数据异质性可以减少异常值，对于实际应用具有重要指导意义。然而，耳蜗分割性能的下降也揭示了在应对更精细的解剖结构和复杂注释时，现有方法的局限性，以及在追求临床实用性时可能面临的挑战。"}}
{"id": "2501.13724", "title": "Dual-Domain Exponent of Maximum Mutual Information Decoding", "authors": ["AmirPouya Moeini", "Albert Guillén i Fàbregas"], "categories": ["cs.IT", "math.IT", "math.PR"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This paper is scheduled to be presented at IEEE ITW 2025, Sydney, Australia", "url": "http://arxiv.org/abs/2501.13724v2", "summary": "This paper provides a dual domain derivation of the error exponent of maximum\nmutual information (MMI) decoding with constant composition codes, showing it\ncoincides with that of maximum likelihood decoding for discrete memoryless\nchannels. The analysis is further extended to joint source-channel coding,\ndemonstrating that the generalized MMI decoder achieves the same random coding\nerror exponent as the maximum a posteriori decoder.", "comment": "This paper is scheduled to be presented at IEEE ITW 2025, Sydney,\n  Australia", "pdf_url": "http://arxiv.org/pdf/2501.13724v2", "cate": "cs.IT", "date": "2025-01-23", "updated": "2025-07-24", "AI": {"title_translation": "最大互信息解码的双域指数", "tldr": "本文推导了最大互信息（MMI）解码的误差指数，并证明其与最大似然解码和最大后验解码的误差指数相同。", "motivation": "旨在提供最大互信息（MMI）解码误差指数的双域推导，并证明其与离散无记忆信道的最大似然解码误差指数一致，同时将其扩展到联合信源-信道编码，以证明广义MMI解码器能达到与最大后验解码器相同的随机编码误差指数。", "method": "采用双域推导方法来分析最大互信息（MMI）解码在恒定组成码下的误差指数。该分析进一步扩展到联合信源-信道编码。", "result": "结果表明，最大互信息（MMI）解码的误差指数与离散无记忆信道的最大似然解码误差指数一致。此外，在联合信源-信道编码中，广义MMI解码器能够达到与最大后验（MAP）解码器相同的随机编码误差指数。", "conclusion": "本文结论是，最大互信息（MMI）解码在误差指数性能上与最大似然（ML）解码和最大后验（MAP）解码具有等价性。", "translation": "本文提供了最大互信息（MMI）解码在恒定组成码下的误差指数的双域推导，表明其与离散无记忆信道的最大似然解码误差指数相吻合。该分析进一步扩展到联合信源-信道编码，证明广义MMI解码器实现了与最大后验解码器相同的随机编码误差指数。", "summary": "本文通过双域推导，证明了恒定组成码下最大互信息（MMI）解码的误差指数与离散无记忆信道的最大似然解码误差指数一致。进一步的研究表明，在联合信源-信道编码中，广义MMI解码器实现了与最大后验解码器相同的随机编码误差指数。", "keywords": "最大互信息解码, 误差指数, 双域推导, 恒定组成码, 联合信源-信道编码", "comments": "这项研究通过提供MMI解码误差指数的双域推导，并证明其与ML和MAP解码的等价性，为信息论领域提供了重要的理论见解。它强调了MMI解码在某些条件下可以达到最优的错误性能，这对于通信系统的设计和分析具有实际意义。"}}
{"id": "2507.18269", "title": "Designing efficient interventions for pre-disease states using control theory", "authors": ["Makito Oku"], "categories": ["math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      24 pages, 14 figures, 1 table, submitted to NOLTA", "url": "http://arxiv.org/abs/2507.18269v1", "summary": "To extend healthy life expectancy in an aging society, it is crucial to\nprevent various diseases at pre-disease states. Although dynamical network\nbiomarker theory has been developed for pre-disease detection, mathematical\nframeworks for pre-disease treatment have not been well established. Here I\npropose a control theory-based approach for pre-disease treatment, named Markov\nchain sparse control (MCSC), where time evolution of a probability distribution\non a Markov chain is described as a discrete-time linear system. By designing a\nsparse controller, a few candidate states for intervention are identified. The\nvalidity of MCSC is demonstrated using numerical simulations and real-data\nanalysis.", "comment": "24 pages, 14 figures, 1 table, submitted to NOLTA", "pdf_url": "http://arxiv.org/pdf/2507.18269v1", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用控制理论设计疾病前期状态的有效干预措施", "tldr": "本文提出了一种基于控制理论的马尔可夫链稀疏控制（MCSC）方法，用于疾病前期状态的治疗，并通过数值模拟和真实数据分析验证了其有效性。", "motivation": "在老龄化社会中，延长健康预期寿命至关重要，需要在疾病前期预防各种疾病。尽管已有用于疾病前期检测的动力学网络生物标志物理论，但疾病前期治疗的数学框架尚未完善。", "method": "提出了一种基于控制理论的疾病前期治疗方法，名为马尔可夫链稀疏控制（MCSC）。该方法将马尔可夫链上概率分布的时间演化描述为离散时间线性系统，并通过设计稀疏控制器来识别少数候选干预状态。", "result": "通过设计稀疏控制器，该方法能够识别出少数可用于干预的候选状态。MCSC的有效性通过数值模拟和真实数据分析得到了验证。", "conclusion": "本文提出的马尔可夫链稀疏控制（MCSC）方法为疾病前期治疗提供了一个有效的基于控制理论的数学框架，并通过实践证明了其可行性。", "translation": "为了在老龄化社会中延长健康预期寿命，在疾病前期预防各种疾病至关重要。尽管已经开发了用于疾病前期检测的动力学网络生物标志物理论，但疾病前期治疗的数学框架尚未完善。本文提出了一种基于控制理论的疾病前期治疗方法，命名为马尔可夫链稀疏控制（MCSC），其中马尔可夫链上概率分布的时间演化被描述为一个离散时间线性系统。通过设计稀疏控制器，可以识别出少数用于干预的候选状态。MCSC的有效性通过数值模拟和真实数据分析得到了证明。", "summary": "本文针对老龄化社会中疾病前期预防的需求，提出了一种基于控制理论的马尔可夫链稀疏控制（MCSC）方法，旨在为疾病前期治疗提供数学框架。MCSC将马尔可夫链上的概率分布演化建模为离散时间线性系统，并通过稀疏控制识别关键干预状态。该方法的有效性已通过数值模拟和真实数据分析得到验证。", "keywords": "疾病前期, 控制理论, 马尔可夫链稀疏控制, 干预, 健康寿命", "comments": "该论文的创新点在于将控制理论引入疾病前期治疗领域，提出了MCSC这一新颖的数学框架，填补了该领域在治疗方法上的空白。通过识别少数关键干预状态，MCSC有望提高干预效率，具有重要的应用潜力。"}}
{"id": "1611.04175", "title": "Recognizing and Eliciting Weakly Single Crossing Profiles on Trees", "authors": ["Palash Dey"], "categories": ["cs.MA", "cs.AI", "cs.DS"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      Accepted in TCS journal", "url": "http://arxiv.org/abs/1611.04175v4", "summary": "We introduce and study the weakly single-crossing domain on trees which is a\ngeneralization of the well-studied single-crossing domain in social choice\ntheory. We design a polynomial-time algorithm for recognizing preference\nprofiles which belong to this domain. We then develop an efficient elicitation\nalgorithm for this domain which works even if the preferences can be accessed\nonly sequentially and the underlying single-crossing tree structure is not\nknown beforehand. We also prove matching lower bound on the query complexity of\nour elicitation algorithm when the number of voters is large compared to the\nnumber of candidates. We also prove a lower bound of $\\Omega(m^2\\log n)$ on the\nnumber of queries that any algorithm needs to ask to elicit single crossing\nprofile when random queries are allowed. This resolves an open question in an\nearlier paper and proves optimality of their preference elicitation algorithm\nwhen random queries are allowed.", "comment": "Accepted in TCS journal", "pdf_url": "http://arxiv.org/pdf/1611.04175v4", "cate": "cs.MA", "date": "2016-11-13", "updated": "2025-07-24", "AI": {"title_translation": "识别和启发树上的弱单交叉偏好", "tldr": "本文引入并研究了树上的弱单交叉域，设计了识别和启发该域偏好配置的多项式时间算法，并证明了查询复杂度的匹配下界。", "motivation": "动机是引入并研究树上的弱单交叉域，这是社会选择理论中单交叉域的泛化，并解决相关识别和启发问题。", "method": "方法包括设计一个识别弱单交叉偏好配置的多项式时间算法，以及一个高效的偏好启发算法（即使偏好只能顺序访问且底层结构未知）。此外，还通过证明查询复杂度的匹配下界来分析算法效率，并证明了随机查询下启发单交叉偏好所需的查询数量的下界。", "result": "结果是设计了一个用于识别弱单交叉域偏好配置的多项式时间算法；开发了一个高效的启发算法，该算法即使在偏好只能顺序访问且底层单交叉树结构未知的情况下也能工作；证明了当投票者数量远大于候选者数量时，启发算法查询复杂度的匹配下界；还证明了在允许随机查询时，任何算法启发单交叉偏好配置所需查询数量的下界为 $\\Omega(m^2\\log n)$。", "conclusion": "结论是解决了早期论文中的一个开放问题，并证明了其偏好启发算法在允许随机查询时的最优性。", "translation": "我们引入并研究了树上的弱单交叉域，这是社会选择理论中被充分研究的单交叉域的泛化。我们设计了一个多项式时间算法来识别属于该域的偏好配置。然后，我们为该域开发了一种高效的启发算法，即使偏好只能顺序访问且底层单交叉树结构事先未知也能工作。当投票者数量相对于候选者数量较大时，我们还证明了我们的启发算法查询复杂度的匹配下界。我们还证明了在允许随机查询的情况下，任何算法启发单交叉偏好配置所需查询数量的下界为 $\\Omega(m^2\\log n)$。这解决了一篇早期论文中的一个开放问题，并证明了其偏好启发算法在允许随机查询时的最优性。", "summary": "本文引入了树上的弱单交叉域，作为社会选择理论中单交叉域的推广。研究者设计了一个多项式时间算法来识别该域的偏好配置，并开发了一个高效的启发算法，该算法在限制条件下依然有效。此外，论文还证明了启发算法的查询复杂度下界，并解决了之前研究中的一个开放问题，证明了相关偏好启发算法在特定条件下的最优性。", "keywords": "弱单交叉域, 偏好启发, 树结构, 查询复杂度, 社会选择理论", "comments": "本文通过引入树上的弱单交叉域，扩展了社会选择理论中单交叉域的研究范围，具有重要的理论意义。其提出的多项式时间识别算法和高效启发算法解决了实际问题，特别是对未知底层结构和顺序访问限制的处理，体现了创新性。同时，通过严格的下界证明，解决了开放问题并确立了算法的最优性，提升了研究的严谨性。"}}
{"id": "2507.18473", "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting", "authors": ["Haoran Xu", "Saining Zhang", "Peishuo Li", "Baijun Ye", "Xiaoxue Chen", "Huan-ang Gao", "Jv Zheng", "Xiaowei Song", "Ziqiao Peng", "Run Miao", "Jinrang Jia", "Yifeng Shi", "Guangqi Yi", "Hang Zhao", "Hao Tang", "Hongyang Li", "Kaicheng Yu", "Hao Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IROS 2025, Code: this https URL", "url": "http://arxiv.org/abs/2507.18473v1", "summary": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous\ndriving, enabling cooperation between vehicles and infrastructure. While\nsimulation has significantly contributed to various autonomous driving tasks,\nits potential for data generation and augmentation in V2X scenarios remains\nunderexplored. In this paper, we introduce CRUISE, a comprehensive\nreconstruction-and-synthesis framework designed for V2X driving environments.\nCRUISE employs decomposed Gaussian Splatting to accurately reconstruct\nreal-world scenes while supporting flexible editing. By decomposing dynamic\ntraffic participants into editable Gaussian representations, CRUISE allows for\nseamless modification and augmentation of driving scenes. Furthermore, the\nframework renders images from both ego-vehicle and infrastructure views,\nenabling large-scale V2X dataset augmentation for training and evaluation. Our\nexperimental results demonstrate that: 1) CRUISE reconstructs real-world V2X\ndriving scenes with high fidelity; 2) using CRUISE improves 3D detection across\nego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D\ntracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates\nchallenging corner cases.", "comment": "IROS 2025, Code: https://github.com/SainingZhang/CRUISE", "pdf_url": "http://arxiv.org/pdf/2507.18473v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CRUISE：基于高斯泼溅的V2X场景协同重建与编辑", "tldr": "CRUISE是一个用于V2X场景的重建与合成框架，利用分解高斯泼溅实现高保真重建和灵活编辑，并能生成V2X数据集以改进自动驾驶任务。", "motivation": "自动驾驶中V2X通信至关重要，但V2X场景的数据生成和增强潜力尚未充分开发。", "method": "CRUISE框架利用分解高斯泼溅技术精确重建现实世界V2X场景并支持灵活编辑。它将动态交通参与者分解为可编辑的高斯表示，允许对驾驶场景进行无缝修改和增强。此外，该框架能从自车和基础设施视角渲染图像，以实现大规模V2X数据集增强，用于训练和评估。", "result": "1) CRUISE高保真重建现实世界V2X驾驶场景；2) 使用CRUISE改进了自车、基础设施和协同视角下的3D检测，以及V2X-Seq基准上的协同3D跟踪；3) CRUISE有效生成具有挑战性的极端情况。", "conclusion": "CRUISE是一个有效的V2X场景重建与合成框架，能够生成高质量数据并显著提升自动驾驶任务（如3D检测和跟踪）的性能。", "translation": "车联网（V2X）通信在自动驾驶中扮演着关键角色，实现了车辆与基础设施之间的协同。尽管模拟在各种自动驾驶任务中做出了显著贡献，但其在V2X场景中数据生成和增强的潜力仍未得到充分探索。在本文中，我们介绍了CRUISE，一个为V2X驾驶环境设计的综合性重建与合成框架。CRUISE采用分解高斯泼溅技术，以高精度重建真实世界场景，同时支持灵活编辑。通过将动态交通参与者分解为可编辑的高斯表示，CRUISE允许对驾驶场景进行无缝修改和增强。此外，该框架能从自车和基础设施视角渲染图像，从而实现大规模V2X数据集的增强，用于训练和评估。我们的实验结果表明：1）CRUISE能够高保真地重建真实世界的V2X驾驶场景；2）使用CRUISE可以改善自车、基础设施和协同视角下的3D检测，以及V2X-Seq基准上的协同3D跟踪；3）CRUISE能够有效地生成具有挑战性的极端情况。", "summary": "CRUISE是一个针对V2X自动驾驶场景的综合性重建与合成框架。它利用分解高斯泼溅技术高保真重建真实场景，并允许对动态交通参与者进行灵活编辑和场景增强。通过生成自车和基础设施视角的图像，CRUISE支持大规模V2X数据集的增强，实验证明其能高保真重建场景，提升3D检测和跟踪性能，并生成极端情况，从而有效推动自动驾驶数据生成和任务改进。", "keywords": "V2X, 高斯泼溅, 场景重建, 数据增强, 自动驾驶", "comments": "CRUISE的创新之处在于其将分解高斯泼溅技术应用于V2X场景的重建与编辑，实现了高保真数据生成和灵活的场景增强。这对于弥补V2X数据集不足、生成多样化和极端情况数据具有重要意义，能有效提升自动驾驶系统的训练和评估效果。"}}
{"id": "2507.18007", "title": "Cloud Native System for LLM Inference Serving", "authors": ["Minxian Xu", "Junhan Liao", "Jingfeng Wu", "Yiyuan He", "Kejiang Ye", "Chengzhong Xu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.18007v1", "summary": "Large Language Models (LLMs) are revolutionizing numerous industries, but\ntheir substantial computational demands create challenges for efficient\ndeployment, particularly in cloud environments. Traditional approaches to\ninference serving often struggle with resource inefficiencies, leading to high\noperational costs, latency issues, and limited scalability. This article\nexplores how Cloud Native technologies, such as containerization,\nmicroservices, and dynamic scheduling, can fundamentally improve LLM inference\nserving. By leveraging these technologies, we demonstrate how a Cloud Native\nsystem enables more efficient resource allocation, reduces latency, and\nenhances throughput in high-demand scenarios. Through real-world evaluations\nusing Kubernetes-based autoscaling, we show that Cloud Native architectures can\ndynamically adapt to workload fluctuations, mitigating performance bottlenecks\nwhile optimizing LLM inference serving performance. This discussion provides a\nbroader perspective on how Cloud Native frameworks could reshape the future of\nscalable LLM inference serving, offering key insights for researchers,\npractitioners, and industry leaders in cloud computing and artificial\nintelligence.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.18007v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LLM推理服务云原生系统", "tldr": "LLM推理服务在云环境中部署面临挑战，本文探讨云原生技术如何优化资源、降低延迟、提高吞吐量，并通过Kubernetes自动扩缩容验证其动态适应性，展望云原生框架对LLM推理服务未来的重塑。", "motivation": "大型语言模型（LLM）的巨大计算需求导致在云环境中部署效率低下，传统推理服务方法存在资源效率低、运营成本高、延迟问题和可扩展性有限等问题。", "method": "本文探讨并利用容器化、微服务和动态调度等云原生技术来改善LLM推理服务。通过基于Kubernetes的自动扩缩容进行实际评估。", "result": "云原生系统能够实现更高效的资源分配，降低延迟，提高高需求场景下的吞吐量。通过基于Kubernetes的自动扩缩容的实际评估表明，云原生架构可以动态适应工作负载波动，减轻性能瓶颈，同时优化LLM推理服务性能。", "conclusion": "云原生框架可以重塑可扩展LLM推理服务的未来，为云计算和人工智能领域的研究人员、从业者和行业领导者提供关键见解。", "translation": "大型语言模型（LLM）正在彻底改变众多行业，但其巨大的计算需求给高效部署带来了挑战，尤其是在云环境中。传统的推理服务方法往往存在资源效率低下、运营成本高、延迟问题和可扩展性有限等问题。本文探讨了容器化、微服务和动态调度等云原生技术如何从根本上改进LLM推理服务。通过利用这些技术，我们展示了云原生系统如何实现更高效的资源分配、降低延迟并提高高需求场景下的吞吐量。通过使用基于Kubernetes的自动扩缩容进行的真实世界评估，我们表明云原生架构可以动态适应工作负载波动，减轻性能瓶颈，同时优化LLM推理服务性能。本次讨论为云原生框架如何重塑可扩展LLM推理服务的未来提供了更广阔的视角，为云计算和人工智能领域的研究人员、从业者和行业领导者提供了关键见解。", "summary": "本文探讨了云原生技术（如容器化、微服务和动态调度）如何解决大型语言模型（LLM）在云环境中推理服务面临的部署效率、成本、延迟和可扩展性挑战。研究表明，云原生系统能优化资源分配、降低延迟并提高吞吐量。通过基于Kubernetes的自动扩缩容的实证评估，证明了云原生架构能动态适应工作负载，从而优化LLM推理服务性能，并为未来可扩展的LLM推理服务提供新思路。", "keywords": "大型语言模型, 云原生, 推理服务, Kubernetes, 资源优化", "comments": "这篇论文的创新点在于将云原生技术应用于LLM推理服务，解决了传统部署中存在的资源效率低下和可扩展性问题。其重要性体现在为LLM在云环境中的高效、大规模部署提供了可行的解决方案和新的视角。"}}
{"id": "2507.17844", "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "url": "http://arxiv.org/abs/2507.17844v1", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "pdf_url": "http://arxiv.org/pdf/2507.17844v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SV3.3B：一种用于动作识别的体育视频理解模型", "tldr": "SV3.3B是一个轻量级模型，通过新颖的采样和自监督学习，在设备上高效分析体育视频，在生成详细动作描述方面优于大型模型。", "motivation": "现有的体育视频分析模型计算密集，需要服务器处理，且缺乏对细微运动生物力学转换的精细理解，难以捕捉关键的动作阶段。", "method": "本文引入了SV3.3B，一个轻量级的3.3B参数视频理解模型。该模型结合了新颖的时间运动差异采样和自监督学习，旨在实现高效的设备端部署。它采用基于DWT-VGG16-LDA的关键帧提取机制来识别16个最具代表性的帧，然后是一个通过掩码去噪目标预训练的V-DWT-JEPA2编码器，以及一个为体育动作描述生成而微调的LLM解码器。", "result": "SV3.3B在NSVA篮球数据集的一个子集上进行了评估，在传统文本生成指标和体育特定评估标准上均表现出色。它优于包括GPT-4o变体在内的更大闭源模型，同时保持显著更低的计算要求。该模型在生成技术详细和分析丰富的体育描述方面表现出卓越的能力，在真实验证指标上比GPT-4o提高了29.2%，并在信息密度、动作复杂性和测量精度等对全面运动分析至关重要的指标上取得了显著改进。", "conclusion": "SV3.3B模型能够高效地在设备端部署，并生成高质量、高精度的体育动作描述，显著优于现有大型模型，为自动化体育视频分析提供了新的解决方案。", "translation": "本文解决了自动化体育视频分析的挑战，该领域传统上受限于计算密集型模型，这些模型需要服务器端处理，并且缺乏对运动动作精细理解的能力。当前方法难以捕捉有意义的体育分析所必需的细微生物力学转换，经常错过几秒钟内发生的准备、执行和完成等关键阶段。为了解决这些限制，我们引入了SV3.3B，一个轻量级的3.3B参数视频理解模型，该模型结合了新颖的时间运动差异采样和自监督学习，以实现高效的设备端部署。我们的方法采用基于DWT-VGG16-LDA的关键帧提取机制，智能地识别体育序列中16个最具代表性的帧，然后是一个通过掩码去噪目标预训练的V-DWT-JEPA2编码器，以及一个为体育动作描述生成而微调的LLM解码器。在NSVA篮球数据集的一个子集上进行评估，SV3.3B在传统文本生成指标和体育特定评估标准上均取得了卓越的性能，优于包括GPT-4o变体在内的更大闭源模型，同时保持显著更低的计算要求。我们的模型在生成技术详细和分析丰富的体育描述方面表现出卓越的能力，在真实验证指标上比GPT-4o提高了29.2%，在信息密度、动作复杂性和测量精度等对全面运动分析至关重要的指标上取得了实质性改进。模型可在https://huggingface.co/sportsvision/SV3.3B获取。", "summary": "SV3.3B是一个轻量级的3.3B参数体育视频理解模型，旨在解决现有体育视频分析模型计算密集且缺乏精细理解的问题。该模型结合了新颖的时间运动差异采样和自监督学习，并采用DWT-VGG16-LDA关键帧提取、V-DWT-JEPA2编码器和LLM解码器。它在NSVA篮球数据集上表现出色，在设备端高效运行的同时，在生成详细体育动作描述方面显著优于GPT-4o等大型模型，在多项指标上实现显著提升。", "keywords": "体育视频分析, 动作识别, 视频理解, 轻量级模型, 自监督学习", "comments": "SV3.3B的创新之处在于其轻量级设计和结合时间运动差异采样与自监督学习的方法，实现了高效的设备端部署。其在体育视频分析中对细微动作的捕捉能力以及优于大型闭源模型的性能，显示了其在自动化体育分析领域的巨大潜力。该模型在计算效率和性能上的平衡是其重要性所在。"}}
{"id": "2507.18313", "title": "Regression-aware Continual Learning for Android Malware Detection", "authors": ["Daniele Ghiani", "Daniele Angioni", "Giorgio Piras", "Angelo Sotgiu", "Luca Minnei", "Srishti Gupta", "Maura Pintor", "Fabio Roli", "Battista Biggio"], "categories": ["cs.LG", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Information Forensics and Security", "url": "http://arxiv.org/abs/2507.18313v1", "summary": "Malware evolves rapidly, forcing machine learning (ML)-based detectors to\nadapt continuously. With antivirus vendors processing hundreds of thousands of\nnew samples daily, datasets can grow to billions of examples, making full\nretraining impractical. Continual learning (CL) has emerged as a scalable\nalternative, enabling incremental updates without full data access while\nmitigating catastrophic forgetting. In this work, we analyze a critical yet\noverlooked issue in this context: security regression. Unlike forgetting, which\nmanifests as a general performance drop on previously seen data, security\nregression captures harmful prediction changes at the sample level, such as a\nmalware sample that was once correctly detected but evades detection after a\nmodel update. Although often overlooked, regressions pose serious risks in\nsecurity-critical applications, as the silent reintroduction of previously\ndetected threats in the system may undermine users' trust in the whole updating\nprocess. To address this issue, we formalize and quantify security regression\nin CL-based malware detectors and propose a regression-aware penalty to\nmitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL\nsetting, preserving prior predictive behavior in a model-agnostic manner.\nExperiments on the ELSA, Tesseract, and AZ-Class datasets show that our method\neffectively reduces regression across different CL scenarios while maintaining\nstrong detection performance over time.", "comment": "Submitted to IEEE Transactions on Information Forensics and Security", "pdf_url": "http://arxiv.org/pdf/2507.18313v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "面向Android恶意软件检测的回归感知持续学习", "tldr": "本研究提出了一种回归感知持续学习方法，通过引入回归感知惩罚来解决持续学习中恶意软件检测的“安全回归”问题，并在多个数据集上验证了其有效性。", "motivation": "恶意软件快速演变，使得基于机器学习的检测器需要持续适应。然而，面对每日数百万的新样本，完全重新训练变得不切实际。持续学习（CL）虽然是可扩展的替代方案，但存在一个关键且常被忽视的问题：安全回归。安全回归指的是恶意软件样本在模型更新后从被正确检测变为逃避检测，这会严重损害用户对系统更新的信任，因此需要被解决。", "method": "本研究首先形式化并量化了基于持续学习的恶意软件检测器中的安全回归问题。为了解决这个问题，提出了一种回归感知惩罚机制，并通过将正向一致性训练（PCT）方法应用于持续学习设置，以模型无关的方式保留先前的预测行为。", "result": "在ELSA、Tesseract和AZ-Class数据集上的实验表明，所提出的方法在不同的持续学习场景中能有效减少安全回归，同时保持了长期稳定的检测性能。", "conclusion": "本研究成功地形式化并解决了持续学习中恶意软件检测的安全回归问题，通过引入回归感知惩罚和适应PCT，显著降低了回归率，并维持了良好的检测性能，提升了安全关键应用中模型更新的可靠性。", "translation": "恶意软件迅速演变，迫使基于机器学习（ML）的检测器持续适应。随着反病毒供应商每天处理数十万个新样本，数据集可能增长到数十亿个样本，使得完全重新训练变得不切实际。持续学习（CL）已成为一种可扩展的替代方案，可以在不完全访问数据的情况下实现增量更新，同时减轻灾难性遗忘。在这项工作中，我们分析了在此背景下一个关键但被忽视的问题：安全回归。与遗忘不同，遗忘表现为先前见过的数据上的整体性能下降，而安全回归则捕获样本级别的有害预测变化，例如曾被正确检测的恶意软件样本在模型更新后逃避了检测。尽管常被忽视，回归在安全关键应用中构成了严重风险，因为系统中先前检测到的威胁的无声重新引入可能会损害用户对整个更新过程的信任。为了解决这个问题，我们形式化并量化了基于CL的恶意软件检测器中的安全回归，并提出了一种回归感知惩罚来缓解它。具体来说，我们将正向一致性训练（PCT）适应到CL设置中，以模型无关的方式保留先前的预测行为。在ELSA、Tesseract和AZ-Class数据集上的实验表明，我们的方法在不同CL场景中有效减少了回归，同时随着时间的推移保持了强大的检测性能。", "summary": "本论文关注持续学习（CL）在Android恶意软件检测中的应用，并首次提出了“安全回归”这一关键且常被忽视的问题。安全回归是指模型更新后，之前能被正确检测的恶意软件样本反而逃避了检测。为解决此问题，作者团队形式化并量化了安全回归，并提出了一种回归感知惩罚机制，通过调整正向一致性训练（PCT）方法来保留模型先前的预测行为。实验结果表明，该方法能有效降低安全回归，并保持良好的检测性能。", "keywords": "持续学习, 恶意软件检测, 安全回归, 回归感知惩罚, Android", "comments": "这篇论文的创新点在于首次明确提出了“安全回归”这一概念，并将其与传统的“灾难性遗忘”区分开来，强调了其在安全领域的重要性。通过形式化量化问题并引入回归感知惩罚，该研究为持续学习在安全关键应用中的可靠性提供了新的视角和解决方案。将PCT适应到CL设置中以实现模型无关的预测行为保留，是其方法上的亮点。这对于确保模型在持续更新过程中不会“倒退”至关重要，具有重要的实际应用价值。"}}
{"id": "2507.17944", "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17944v1", "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17944v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "评估AI文本检测器、少样本和思维链提示在DeepSeek生成文本上的性能", "tldr": "本研究评估了六种AI文本检测器识别DeepSeek生成文本的性能，并测试了对抗性攻击（如人性化改写）的影响；同时发现少样本和思维链提示在DeepSeek自身识别AI文本方面表现出色。", "motivation": "现有研究主要集中在ChatGPT等知名大型语言模型上，而对DeepSeek这一新兴LLM的文本检测能力存在明显空白。", "method": "研究评估了AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero六种AI检测工具识别DeepSeek生成文本的能力。这些检测器暴露于标准和人性化改写等对抗性攻击。此外，研究还通过少样本提示和思维链推理（CoT）将DeepSeek自身作为检测器来分类AI和人类撰写的文本。为此，收集了49对人工撰写的问答对，并使用DeepSeek-v3生成了49个AI样本，再通过改写和人性化处理增加了196个对抗性样本。", "result": "QuillBot和Copyleaks在原始和改写的DeepSeek文本上表现近乎完美，但AI Text Classifier和GPT-2等其他工具表现不一致。最有效的攻击是“人性化”，它将Copyleaks的准确率降至71%，QuillBot降至58%，GPTZero降至52%。少样本和思维链提示表现出高准确率，其中最佳的五样本结果仅错误分类49个样本中的一个（AI召回率96%，人类召回率100%）。", "conclusion": "AI文本检测器在识别DeepSeek生成文本时的表现差异显著，尤其是在面对“人性化”对抗性攻击时准确率会大幅下降。然而，当DeepSeek自身通过少样本和思维链提示进行正确引导时，它在区分AI和人类撰写文本方面表现出高准确率。", "translation": "大型语言模型（LLMs）已迅速改变了书面材料的创作方式。LLMs引发了对写作完整性的质疑，从而推动了人工智能（AI）检测技术的诞生。对抗性攻击，如标准改写和人性化改写，会抑制检测器识别机器生成文本的能力。以往的研究主要集中在ChatGPT和其他知名LLMs上，并显示出检测器之间不同的准确性。然而，关于DeepSeek这一近期发布的LLM，文献中存在明显的空白。因此，在这项工作中，我们调查了六种普遍可访问的AI检测工具——AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2和GPTZero——是否能持续识别DeepSeek生成的文本。这些检测器暴露于上述对抗性攻击。我们还将DeepSeek视为一个检测器，通过执行少样本提示和思维链推理（CoT）来分类AI和人类撰写的文本。我们收集了49对LLM时代之前的人工撰写问答对，并使用DeepSeek-v3生成了匹配的回复，产生了49个AI生成样本。然后，我们应用了改写和人性化等对抗性技术，增加了196个样本。这些样本用于挑战检测器的鲁棒性并评估其准确性影响。虽然QuillBot和Copyleaks在原始和改写的DeepSeek文本上表现出近乎完美的性能，但其他检测器——特别是AI Text Classifier和GPT-2——显示出不一致的结果。最有效的攻击是人性化，它将Copyleaks的准确率降至71%，QuillBot降至58%，GPTZero降至52%。少样本和CoT提示显示出高准确率，其中最佳的五样本结果仅错误分类49个样本中的一个（AI召回率96%，人类召回率100%）。", "summary": "本文旨在弥补现有研究在评估AI文本检测器识别DeepSeek生成内容方面的空白。研究评估了六种常用AI检测工具对DeepSeek-v3生成文本的识别能力，并测试了标准改写和人性化改写等对抗性攻击的影响。此外，研究还探索了DeepSeek自身利用少样本和思维链提示进行AI文本检测的潜力。结果显示，部分检测器（如QuillBot和Copyleaks）在原始及改写文本上表现良好，但“人性化”攻击显著降低了其准确率。值得注意的是，DeepSeek在经过适当提示后，在区分AI和人类文本方面展现出高准确率。", "keywords": "AI文本检测器, DeepSeek, 少样本提示, 思维链, 对抗性攻击", "comments": "该论文揭示了当前AI文本检测器在面对特定LLM（DeepSeek）时的性能差异，并强调了“人性化”等对抗性技术对检测准确性的显著影响。此外，通过利用LLM自身（DeepSeek）进行少样本和思维链提示来识别AI文本的创新方法，展现出非常高的准确率，为未来的检测方法提供了有前景的方向。其局限性在于仅专注于DeepSeek和特定检测器集合。"}}
{"id": "2507.18012", "title": "Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model", "authors": ["Hang Xu", "Alexandre Bousse", "Alessandro Perelli"], "categories": ["eess.IV", "cs.CV", "physics.med-ph", "92C55, 94A08", "I.4.5; J.3"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      13 pages, 10 figures, 2 tables", "url": "http://arxiv.org/abs/2507.18012v1", "summary": "Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced\ntechnology which enables automatic decomposition of materials in clinical\nimages without manual segmentation using the dependency of the X-ray linear\nattenuation with energy. However, most methods perform material decomposition\nin the image domain as a post-processing step after reconstruction but this\nprocedure does not account for the beam-hardening effect and it results in\nsub-optimal results. In this work, we propose a deep learning procedure called\nDual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative\nmaterial decomposition which directly converts the DECT projection data into\nmaterial images. The algorithm is based on incorporating the knowledge of the\nspectral DECT model into the deep learning training loss and combining a\nscore-based denoising diffusion learned prior in the material image domain.\nImportantly the inference optimization loss takes as inputs directly the\nsinogram and converts to material images through a model-based conditional\ndiffusion model which guarantees consistency of the results. We evaluate the\nperformance with both quantitative and qualitative estimation of the proposed\nDEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset.\nFinally, we show that DEcomp-MoD outperform state-of-the-art unsupervised\nscore-based model and supervised deep learning networks, with the potential to\nbe deployed for clinical diagnosis.", "comment": "13 pages, 10 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.18012v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于模型去噪扩散模型的直接双能CT物质分解", "tldr": "本文提出DEcomp-MoD，一种结合DECT模型和去噪扩散模型的深度学习方法，直接将双能CT投影数据转换为物质图像，有效克服传统方法的局限性，并优于现有方法，具有临床应用潜力。", "motivation": "大多数现有双能CT物质分解方法在图像域进行后处理，未考虑束硬化效应，导致结果次优。", "method": "本文提出一种名为双能分解模型基扩散（DEcomp-MoD）的深度学习程序，用于定量物质分解。该方法直接将DECT投影数据转换为物质图像，通过将光谱DECT模型的知识融入深度学习训练损失，并结合物质图像域中的基于分数的去噪扩散学习先验。推理优化损失直接以正弦图为输入，并通过基于模型的条件扩散模型转换为物质图像，以保证结果的一致性。", "result": "在低剂量AAPM数据集的合成DECT正弦图上，对所提出的DEcomp-MoD方法进行了定量和定性评估。结果表明DEcomp-MoD优于最先进的无监督基于分数模型和有监督深度学习网络。", "conclusion": "DEcomp-MoD在性能上超越了现有最先进的无监督和有监督方法，具有在临床诊断中部署的潜力。", "translation": "双能X射线计算机断层扫描（DECT）是一种先进技术，它利用X射线线性衰减与能量的依赖性，无需手动分割即可在临床图像中自动分解物质。然而，大多数方法在图像域中作为重建后的后处理步骤进行物质分解，但此过程未考虑束硬化效应，导致结果次优。\n在这项工作中，我们提出了一种名为双能分解模型基扩散（DEcomp-MoD）的深度学习程序，用于定量物质分解，它直接将DECT投影数据转换为物质图像。该算法基于将光谱DECT模型的知识融入深度学习训练损失中，并结合了物质图像域中的基于分数的去噪扩散学习先验。重要的是，推理优化损失直接以正弦图为输入，并通过基于模型的条件扩散模型转换为物质图像，这保证了结果的一致性。\n我们使用低剂量AAPM数据集的合成DECT正弦图，对所提出的DEcomp-MoD方法进行了定量和定性评估。最后，我们表明DEcomp-MoD优于最先进的无监督基于分数模型和有监督深度学习网络，具有在临床诊断中部署的潜力。", "summary": "本文提出了一种名为DEcomp-MoD的深度学习方法，用于双能CT物质分解。与传统在图像域后处理的方法不同，DEcomp-MoD直接将DECT投影数据转换为物质图像。该方法通过将光谱DECT模型知识与基于分数的去噪扩散模型相结合，有效解决了束硬化效应问题，提高了分解精度和结果一致性。实验结果表明，DEcomp-MoD在合成DECT正弦图上的性能优于现有先进的无监督和有监督深度学习网络，显示出其在临床诊断中的巨大应用潜力。", "keywords": "双能CT, 物质分解, 扩散模型, 深度学习, 图像重建", "comments": "该论文的创新点在于将DECT物理模型知识与先进的去噪扩散模型相结合，实现了从投影数据到物质图像的直接转换，有效避免了传统方法的束硬化效应问题。这种端到端的方法提高了物质分解的准确性和一致性，对于临床诊断具有重要意义。其性能超越现有SOTA方法，显示了扩散模型在医学图像处理中的强大潜力。"}}
{"id": "2507.18197", "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization", "authors": ["Aline Belloni", "Patrick Prieur"], "categories": ["cs.CL", "cs.DL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      in French language. AGeCSO2025 : 18{è}me Colloque International de l'Association pour la Gestion des Connaissances dans la Soci{é}t{é} et les Organisations, Association pour la Gestion des Connaissances dans la Soci{é}t{é} et les Organisations (AGECSO), Jun 2025, TROYES, France", "url": "http://arxiv.org/abs/2507.18197v1", "summary": "Business process modeling is used by most organizations as an essential\nframework for ensuring efficiency and effectiveness of the work and workflow\nperformed by its employees and for ensuring the alignment of such work with its\nstrategic goals. For organizations that are compliant or near-compliant with\nISO 9001, this approach involves the detailed mapping of processes,\nsub-processes, activities, and tasks. ISO30401 is a Management System Standard,\nintroduced in 2018, establishing universal requirements for the set up of a\nKnowledge Management System in an organization. As ``ISO30401 implementers'' we\nregularly face the challenge of explaining our clients how the knowledge\ndevelopment, transformation and conveyances activities depicted in ISO30401 do\nintegrate with existing operational processes. This article recaps process\nmodelling principles in the context of ISO9001 and explores, based on our\nexperience, how an ISO30401-compliant Knowledge Management System (KMS)\nentwines with all other processes of an Integrated Management System and in\nparticular how it can be implemented by deploying the mechanisms of the SECI\nmodel through the steps of PDCA cycles.", "comment": "in French language. AGeCSO2025 : 18{\\`e}me Colloque International de\n  l'Association pour la Gestion des Connaissances dans la Soci{\\'e}t{\\'e} et\n  les Organisations, Association pour la Gestion des Connaissances dans la\n  Soci{\\'e}t{\\'e} et les Organisations (AGECSO), Jun 2025, TROYES, France", "pdf_url": "http://arxiv.org/pdf/2507.18197v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "将符合ISO30401标准的知识管理系统整合到组织现有业务流程中", "tldr": "本文探讨了如何将符合ISO30401标准的知识管理系统与组织现有业务流程整合，并基于经验提出了通过SECI模型和PDCA循环实现整合的方法。", "motivation": "组织在实施ISO30401知识管理系统时，面临如何将其知识开发、转化和传递活动与现有运营流程整合的挑战。", "method": "本文回顾了ISO9001背景下的流程建模原则，并基于作者的经验，探讨了符合ISO30401标准的知识管理系统如何与集成管理系统的其他流程融合，特别是通过SECI模型和PDCA循环的步骤进行实施。", "result": "论文探讨并提出了将符合ISO30401标准的知识管理系统与组织现有业务流程整合的方法，特别是通过SECI模型在PDCA循环中进行部署。", "conclusion": "Not mentioned in abstract", "translation": "商业流程建模被大多数组织用作确保员工工作效率、有效性以及工作与战略目标一致性的重要框架。对于符合或接近符合ISO 9001的组织，这种方法涉及流程、子流程、活动和任务的详细映射。ISO30401是2018年引入的管理系统标准，它为组织建立知识管理系统设定了通用要求。作为“ISO30401实施者”，我们经常面临向客户解释ISO30401中描述的知识开发、转化和传递活动如何与现有运营流程整合的挑战。本文回顾了ISO9001背景下的流程建模原则，并根据我们的经验，探讨了符合ISO30401标准的知识管理系统（KMS）如何与集成管理系统的所有其他流程交织在一起，特别是如何通过PDCA循环的步骤部署SECI模型的机制来实现。", "summary": "本文探讨了如何将符合ISO30401标准的知识管理系统（KMS）有效地整合到组织现有的业务流程中。针对ISO30401实施者面临的挑战，即知识管理活动与运营流程的整合问题，论文回顾了ISO9001背景下的流程建模原则，并基于实践经验，提出了通过SECI模型和PDCA循环的机制来部署和实现KMS与其他管理系统流程的融合。", "keywords": "知识管理系统, ISO30401, 业务流程整合, SECI模型, PDCA循环", "comments": "这篇论文解决了组织在引入新的知识管理标准（ISO30401）时面临的实际整合挑战。其创新点在于结合了ISO9001的流程建模原则、ISO30401的知识管理要求以及SECI模型和PDCA循环的实践框架，为知识管理系统的有效实施提供了一个实用的指导。"}}
{"id": "2507.16214", "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers", "authors": ["Batu Candan", "Simone Servadio"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16214v2", "summary": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16214v2", "cate": "cs.RO", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "用于安全接近机动的双噪声自适应相对姿态估计框架", "tldr": "本文提出了一种结合计算机视觉和自适应非线性滤波的完整管道，用于在主动碎片清除任务中对翻滚卫星进行精确鲁棒的相对姿态估计，通过双重噪声自适应策略增强了鲁棒性。", "motivation": "精确和鲁棒的相对姿态估计对于执行挑战性的主动碎片清除（ADR）任务至关重要，特别是针对欧洲空间局ENVISAT等翻滚废弃卫星的清除。", "method": "该方法整合了先进的计算机视觉技术与自适应非线性滤波。利用卷积神经网络（CNN）结合图像预处理检测结构标记（角点），并将2D坐标通过相机建模转换为3D测量值。这些测量值在一个无迹卡尔曼滤波器（UKF）框架内进行融合，并引入了双重自适应策略：动态调整测量噪声协方差以补偿CNN测量的不确定性，以及利用测量残差分析自适应调整过程噪声协方差以处理未建模的动力学或机动。", "result": "通过使用逼真的ENVISAT模型进行高保真仿真，并在各种条件下（包括测量中断）将估计值与真实值进行比较，评估了所提出的自适应集成系统的性能。该方法为鲁棒的在轨相对导航提供了增强的解决方案。", "conclusion": "所提出的综合方法为主动碎片清除任务中的安全接近操作所需的鲁棒在轨相对导航提供了增强的解决方案，显著提升了相关能力。", "translation": "精确和鲁棒的相对姿态估计对于执行针对翻滚废弃卫星（如欧洲空间局的ENVISAT）的挑战性主动碎片清除（ADR）任务至关重要。本工作提出了一个完整的管道，将先进的计算机视觉技术与自适应非线性滤波相结合，以应对这一挑战。一个通过图像预处理增强的卷积神经网络（CNN）从追踪器图像中检测结构标记（角点），其2D坐标通过相机建模转换为3D测量值。这些测量值在一个无迹卡尔曼滤波器（UKF）框架内进行融合，该框架因其处理非线性相对动力学的能力而被选中，以估计完整的相对姿态。主要贡献包括集成的系统架构和UKF内的双重自适应策略：测量噪声协方差的动态调整补偿了变化的CNN测量不确定性，而利用测量残差分析对过程噪声协方差的自适应调整则考虑了在线的未建模动力学或机动。这种双重自适应增强了对测量缺陷和动态模型不确定性的鲁棒性。所提出的自适应集成系统的性能通过使用逼真的ENVISAT模型进行高保真仿真进行评估，在各种条件下（包括测量中断）将估计值与真实值进行比较。这种综合方法为鲁棒的在轨相对导航提供了增强的解决方案，显著提升了ADR任务中安全接近操作所需的能力。", "summary": "本文提出了一种用于主动碎片清除（ADR）任务中翻滚卫星相对姿态估计的自适应框架。该框架结合了基于CNN的计算机视觉技术进行特征检测和3D测量转换，并利用无迹卡尔曼滤波器（UKF）进行数据融合。关键创新在于UKF中引入的双重自适应策略，即动态调整测量噪声协方差以应对CNN测量不确定性，以及自适应调整过程噪声协方差以处理未建模动力学。高保真仿真验证了该系统在提高鲁棒性和安全性方面的有效性，对于ADR任务中的安全接近操作至关重要。", "keywords": "相对姿态估计, 主动碎片清除, 无迹卡尔曼滤波器, 自适应滤波, 噪声调整", "comments": "本文提出了一种创新的自适应相对姿态估计框架，通过结合CNN视觉识别和UKF滤波，并特别引入了双重噪声自适应策略，显著提升了系统在复杂非线性动态和测量不确定性下的鲁棒性。这种方法对于高风险的太空主动碎片清除任务具有重要的实践意义和技术推进作用。"}}
{"id": "2507.18039", "title": "Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey", "authors": ["Ahmad D. Suleiman", "Yiming Tang", "Daqing Hou"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Frontiers in Education (FIE) 2025. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.18039v1", "summary": "This research full paper investigates the factors influencing computing\neducators' adoption of project-based learning (PjBL) in software engineering\nand computing curricula. Recognized as a student-centered pedagogical approach,\nPjBL has the potential to enhance student motivation, engagement, critical\nthinking, collaboration, and problem-solving skills. Despite these benefits,\nfaculty adoption remains inconsistent due to challenges such as insufficient\ninstitutional support, time constraints, limited training opportunities,\ndesigning or sourcing projects, and aligning them with course objectives. This\nresearch explores these barriers and investigates the strategies and resources\nthat facilitate a successful adoption. Using a mixed-methods approach, data\nfrom 80 computing faculty were collected through an online survey comprising\nclosed-ended questions to quantify barriers, enablers, and resource needs,\nalong with an open-ended question to gather qualitative insights. Quantitative\ndata were analyzed using statistical methods, while qualitative responses\nunderwent thematic analysis. Results reveal that while PjBL is widely valued,\nits adoption is often selective and impacted by challenges in planning and\nmanaging the learning process, designing suitable projects, and a lack of\ninstitutional support, such as time, funding, and teaching assistants. Faculty\nare more likely to adopt or sustain PjBL when they have access to peer\ncollaboration, professional development, and institutional incentives. In\naddition, sourcing projects from research, industry partnerships, and borrowing\nfrom peers emerged as key facilitators for new projects. These findings\nunderscore the need for systemic support structures to empower faculty to\nexperiment with and scale PjBL practices.", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025. This work has\n  been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.18039v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "计算教育中教师采纳项目式学习的影响因素：一项调查", "tldr": "调查发现，计算教育中教师采纳项目式学习受规划、项目设计及机构支持不足影响，但同伴协作、专业发展和机构激励可促进采纳。", "motivation": "项目式学习（PjBL）在计算教育中潜力巨大，但教师采纳PjBL的程度不一，面临机构支持、时间、培训、项目设计等挑战。本研究旨在探讨这些障碍以及促进成功采纳的策略和资源。", "method": "采用混合方法研究。通过在线问卷从80名计算领域教师收集数据，问卷包含量化障碍、促成因素和资源需求的封闭式问题，以及收集定性见解的开放式问题。量化数据使用统计方法分析，定性回答进行主题分析。", "result": "结果显示，PjBL虽被广泛认可，但采纳常具选择性，受规划和管理学习过程、设计合适项目以及缺乏时间、资金、助教等机构支持的挑战影响。教师在获得同伴协作、专业发展和机构激励时更可能采纳或维持PjBL。从研究、行业伙伴关系和同伴处获取项目是新项目的重要促成因素。", "conclusion": "研究强调需要系统性的支持结构，以赋能教师尝试和推广PjBL实践。", "translation": "这篇研究全文探讨了影响计算教育者在软件工程和计算课程中采纳项目式学习（PjBL）的因素。PjBL被认为是一种以学生为中心的教学方法，有潜力增强学生的学习动机、参与度、批判性思维、协作能力和解决问题的能力。尽管有这些益处，但由于机构支持不足、时间限制、培训机会有限、项目设计或获取困难以及与课程目标对齐等挑战，教师的采纳情况仍然不一致。本研究探讨了这些障碍，并调查了促进成功采纳的策略和资源。研究采用混合方法，通过一项在线调查从80名计算领域教师那里收集了数据，该调查包含量化障碍、促成因素和资源需求的封闭式问题，以及一个收集定性见解的开放式问题。量化数据使用统计方法分析，而定性回答则进行了主题分析。结果表明，虽然PjBL受到广泛重视，但其采纳往往是选择性的，并受到规划和管理学习过程、设计合适项目以及缺乏机构支持（如时间、资金和助教）等挑战的影响。当教师能够获得同伴协作、专业发展和机构激励时，他们更有可能采纳或维持PjBL。此外，从研究、行业合作以及向同伴借鉴获取项目，成为新项目的关键促成因素。这些发现强调需要系统性的支持结构，以赋能教师尝试和推广PjBL实践。", "summary": "本研究调查了计算教育中教师采纳项目式学习（PjBL）的影响因素。尽管PjBL对学生有诸多益处，但其采纳面临机构支持、时间、项目设计等挑战。研究通过对80名计算教师的混合方法调查发现，PjBL的采纳受规划管理、项目设计及机构支持不足的限制。然而，同伴协作、专业发展和机构激励能显著促进PjBL的采纳，同时从研究、行业和同伴处获取项目是关键促成因素。研究强调需建立系统性支持结构以推广PjBL实践。", "keywords": "项目式学习, 教师采纳, 计算教育, 影响因素, 机构支持", "comments": "这项研究通过混合方法深入探讨了计算教育领域教师采纳项目式学习的关键障碍和促成因素，具有重要的实践意义。其创新之处在于不仅识别了挑战，还具体指出了能够促进采纳的策略和资源，如同伴协作、专业发展和多源项目获取。研究结果为教育机构制定更有效的支持政策提供了数据支持，有助于提升PjBL的普及率和教学质量。"}}
{"id": "2412.09900", "title": "Analyzing Fairness of Computer Vision and Natural Language Processing Models", "authors": ["Ahmed Rashed", "Abdelkrim Kallich", "Mohamed Eltayeb"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      25 pages, 8 table, 11 figures", "url": "http://arxiv.org/abs/2412.09900v3", "summary": "Machine learning (ML) algorithms play a critical role in decision-making\nacross various domains, such as healthcare, finance, education, and law\nenforcement. However, concerns about fairness and bias in these systems have\nraised significant ethical and social challenges. To address these challenges,\nthis research utilizes two prominent fairness libraries, Fairlearn by Microsoft\nand AIF360 by IBM. These libraries offer comprehensive frameworks for fairness\nanalysis, providing tools to evaluate fairness metrics, visualize results, and\nimplement bias mitigation algorithms. The study focuses on assessing and\nmitigating biases for unstructured datasets using Computer Vision (CV) and\nNatural Language Processing (NLP) models. The primary objective is to present a\ncomparative analysis of the performance of mitigation algorithms from the two\nfairness libraries. This analysis involves applying the algorithms\nindividually, one at a time, in one of the stages of the ML lifecycle,\npre-processing, in-processing, or post-processing, as well as sequentially\nacross more than one stage. The results reveal that some sequential\napplications improve the performance of mitigation algorithms by effectively\nreducing bias while maintaining the model's performance. Publicly available\ndatasets from Kaggle were chosen for this research, providing a practical\ncontext for evaluating fairness in real-world machine learning workflows.", "comment": "25 pages, 8 table, 11 figures", "pdf_url": "http://arxiv.org/pdf/2412.09900v3", "cate": "cs.LG", "date": "2024-12-13", "updated": "2025-07-23", "AI": {"title_translation": "分析计算机视觉和自然语言处理模型的公平性", "tldr": "本研究利用Fairlearn和AIF360库，评估并比较了计算机视觉和自然语言处理模型中偏见的缓解算法，发现一些串行应用能有效减少偏见并保持模型性能。", "motivation": "机器学习算法在多个领域（如医疗、金融、教育、执法）的决策中发挥关键作用，但其公平性和偏见问题引发了重要的伦理和社会挑战。本研究旨在解决这些挑战。", "method": "研究利用微软的Fairlearn和IBM的AIF360这两个公平性库，评估和缓解计算机视觉（CV）和自然语言处理（NLP）模型在非结构化数据集上的偏见。方法包括在ML生命周期的预处理、处理中或后处理阶段单独应用缓解算法，以及跨多个阶段串行应用。使用Kaggle上的公开数据集进行研究。", "result": "结果显示，一些串行应用能够有效减少偏见，同时保持模型的性能，从而提升缓解算法的效果。", "conclusion": "通过对Fairlearn和AIF360库中缓解算法的比较分析，研究表明串行应用这些算法可以有效减少计算机视觉和自然语言处理模型中的偏见，同时不牺牲模型性能。", "translation": "机器学习（ML）算法在医疗、金融、教育和执法等各个领域的决策中发挥着关键作用。然而，这些系统中关于公平性和偏见的担忧引发了重要的伦理和社会挑战。为了应对这些挑战，本研究利用了两个著名的公平性库：微软的Fairlearn和IBM的AIF360。这些库提供了全面的公平性分析框架，提供工具来评估公平性指标、可视化结果并实现偏见缓解算法。本研究重点关注使用计算机视觉（CV）和自然语言处理（NLP）模型评估和缓解非结构化数据集的偏见。主要目标是对比分析这两个公平性库中缓解算法的性能。此分析涉及在ML生命周期的一个阶段（预处理、处理中或后处理）单独应用算法，以及跨多个阶段串行应用。结果显示，一些串行应用通过有效减少偏见同时保持模型性能，从而提高了缓解算法的性能。本研究选择了Kaggle上公开可用的数据集，为评估现实世界机器学习工作流中的公平性提供了实用背景。", "summary": "本研究旨在解决机器学习模型中的公平性和偏见问题，特别是针对计算机视觉和自然语言处理模型。通过使用微软的Fairlearn和IBM的AIF360公平性库，研究人员比较了不同的偏见缓解算法在机器学习生命周期不同阶段（单独或串行）的应用效果。结果表明，将这些缓解算法串行应用于多个阶段可以有效减少模型偏见，同时保持模型的性能，为现实世界ML工作流中的公平性评估提供了实用见解。", "keywords": "公平性, 偏见缓解, 计算机视觉, 自然语言处理, 机器学习", "comments": "该研究的创新点在于比较了两种主流公平性库（Fairlearn和AIF360）的缓解算法，并探索了在ML生命周期不同阶段（包括串行应用）的有效性。其重要性在于为解决CV和NLP模型中的实际偏见问题提供了具体方法和实证结果，特别是串行应用的效果。局限性可能在于其结果的泛化性，因为仅使用了Kaggle上的公共数据集，且未深入探讨偏见产生的根本原因。"}}
{"id": "2507.18138", "title": "A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion", "authors": ["Min-Gyu Kim", "Dongyun Kang", "Hajun Kim", "Hae-Won Park"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, IEEE RA-L accepted (July 2025)", "url": "http://arxiv.org/abs/2507.18138v1", "summary": "This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity.", "comment": "8 pages, IEEE RA-L accepted (July 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18138v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "一种模块化残差学习框架，用于增强基于模型的方法以实现鲁棒运动", "tldr": "本文提出了一种模块化残差学习框架，结合了基于模型和基于学习的优点，通过集成残差模块来弥补模型失配导致的性能下降，从而在不确定环境中实现鲁棒运动和更高的学习效率。", "motivation": "为了解决基于模型的方法在面对模型失配时性能下降的问题，本文旨在通过结合学习方法来增强其鲁棒性。", "method": "本文提出了一种新颖的方法，将残差模块与基于模型的框架（包括启发式设计的足迹规划器和动态模型）的每个相应部分集成。通过采用模块化结构并为每个残差模块选择合适的基于学习的方法，以弥补模型失配导致的性能下降。并在四足机器人上结合模型预测控制进行了验证。", "result": "该框架在不确定环境中表现出改进的控制性能，与基线方法相比具有更高的学习效率。此外，它使名义控制器对参数调整更加鲁棒。在真实的四足机器人上，尽管存在模拟之外的不确定性，机器人仍成功保持平衡并跟踪指令速度。", "conclusion": "本文提出的模块化残差学习框架能够有效增强基于模型的方法的鲁棒性，并在不确定环境中实现稳定和高效的运动控制，同时提高了控制器对参数调整的鲁棒性。", "translation": "本文提出了一种新颖的方法，结合了基于模型和基于学习框架的优点，以实现鲁棒运动。将残差模块与基于模型的框架的每个相应部分（即使用启发式方法设计的足迹规划器和动态模型）集成，以弥补模型失配导致的性能下降。通过利用模块化结构并为每个残差模块选择合适的基于学习的方法，我们的框架在高度不确定性的环境中展示了改进的控制性能，同时与基线方法相比实现了更高的学习效率。此外，我们观察到我们提出的方法不仅增强了控制性能，还提供了额外的益处，例如使名义控制器对参数调整更加鲁棒。为了验证我们框架的可行性，我们在真实的四足机器人上展示了残差模块与模型预测控制的结合。尽管存在模拟之外的不确定性，机器人仍成功保持平衡并跟踪指令速度。", "summary": "本文提出了一种新颖的模块化残差学习框架，旨在通过结合基于模型和基于学习的方法来提高机器人运动的鲁棒性。该框架将残差模块集成到基于模型的足迹规划器和动态模型中，以补偿模型失配引起的性能下降。实验结果表明，该方法在不确定环境中显著提升了控制性能和学习效率，并增强了控制器对参数调整的鲁棒性。在真实四足机器人上的验证进一步证实了其在复杂环境下的可行性和有效性。", "keywords": "残差学习, 鲁棒运动, 模型预测控制, 四足机器人, 模型失配", "comments": "该论文的创新点在于提出了一种模块化残差学习框架，有效地结合了传统模型方法和现代学习方法的优势，以解决机器人运动控制中模型失配的常见问题。其模块化设计允许针对不同部分选择最佳的学习方法，提高了灵活性和效率。在真实机器人上的验证增加了其实际应用价值。该方法不仅提高了控制性能，还增强了控制器对参数调整的鲁棒性，这对于实际部署具有重要意义。"}}
{"id": "2502.18549", "title": "ARBoids: Adaptive Residual Reinforcement Learning With Boids Model for Cooperative Multi-USV Target Defense", "authors": ["Jiyue Tao", "Tongsheng Shen", "Dexin Zhao", "Feitian Zhang"], "categories": ["cs.LG", "cs.CR", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.18549v2", "summary": "The target defense problem (TDP) for unmanned surface vehicles (USVs)\nconcerns intercepting an adversarial USV before it breaches a designated target\nregion, using one or more defending USVs. A particularly challenging scenario\narises when the attacker exhibits superior maneuverability compared to the\ndefenders, significantly complicating effective interception. To tackle this\nchallenge, this letter introduces ARBoids, a novel adaptive residual\nreinforcement learning framework that integrates deep reinforcement learning\n(DRL) with the biologically inspired, force-based Boids model. Within this\nframework, the Boids model serves as a computationally efficient baseline\npolicy for multi-agent coordination, while DRL learns a residual policy to\nadaptively refine and optimize the defenders' actions. The proposed approach is\nvalidated in a high-fidelity Gazebo simulation environment, demonstrating\nsuperior performance over traditional interception strategies, including pure\nforce-based approaches and vanilla DRL policies. Furthermore, the learned\npolicy exhibits strong adaptability to attackers with diverse maneuverability\nprofiles, highlighting its robustness and generalization capability. The code\nof ARBoids will be released upon acceptance of this letter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.18549v2", "cate": "cs.LG", "date": "2025-02-25", "updated": "2025-07-10", "AI": {"title_translation": "ARBoids：基于Boids模型的自适应残差强化学习用于合作多无人水面艇目标防御", "tldr": "ARBoids是一种结合深度强化学习和Boids模型的自适应残差强化学习框架，用于解决无人水面艇目标防御问题，尤其是在攻击者机动性更优的情况下，它在仿真中表现出优越的性能和强大的适应性。", "motivation": "无人水面艇（USV）的目标防御问题，特别是当攻击者表现出比防御者更优越的机动性时，有效的拦截变得非常复杂。", "method": "本文提出了ARBoids，一个新颖的自适应残差强化学习框架，它将深度强化学习（DRL）与受生物启发的、基于力的Boids模型相结合。在该框架中，Boids模型作为一个计算高效的多智能体协调基线策略，而DRL学习一个残差策略来自适应地细化和优化防御者的行动。", "result": "在Gazebo高保真仿真环境中验证了所提出的方法，证明其性能优于传统的拦截策略，包括纯粹基于力的方法和原始DRL策略。此外，学习到的策略对具有不同机动性特征的攻击者表现出强大的适应性，突出了其鲁棒性和泛化能力。", "conclusion": "ARBoids框架通过结合Boids模型作为基线策略和DRL学习残差策略，成功解决了无人水面艇目标防御中的挑战，尤其是在攻击者机动性更优的情况下，表现出优越的性能、鲁棒性和泛化能力。", "translation": "无人水面艇（USV）的目标防御问题（TDP）涉及在使用一个或多个防御USV拦截敌对USV，防止其突破指定目标区域。当攻击者与防御者相比表现出更优越的机动性时，会出现一个特别具有挑战性的场景，这使得有效拦截变得异常复杂。为了解决这一挑战，本文引入了ARBoids，一个新颖的自适应残差强化学习框架，它将深度强化学习（DRL）与受生物启发的、基于力的Boids模型相结合。在该框架中，Boids模型作为一个计算高效的多智能体协调基线策略，而DRL学习一个残差策略来自适应地细化和优化防御者的行动。所提出的方法在高保真Gazebo仿真环境中得到了验证，证明其性能优于传统的拦截策略，包括纯粹基于力的方法和原始DRL策略。此外，学习到的策略对具有不同机动性特征的攻击者表现出强大的适应性，突出了其鲁棒性和泛化能力。ARBoids的代码将在本文被接受后发布。", "summary": "本文提出ARBoids，一个创新的自适应残差强化学习框架，旨在解决无人水面艇（USV）目标防御问题，尤其是在攻击者机动性更强的情况下。该框架将Boids模型作为高效的多智能体协调基线策略，并利用深度强化学习（DRL）学习残差策略以优化防御者的行动。在Gazebo仿真环境中的验证结果表明，ARBoids在性能上超越了传统策略，并对不同机动性的攻击者展现出强大的适应性、鲁棒性和泛化能力。", "keywords": "无人水面艇, 目标防御, 残差强化学习, Boids模型, 多智能体系统", "comments": "本文的创新点在于结合了Boids模型的计算效率和DRL的自适应优化能力，形成了一种新颖的残差强化学习框架。这种混合方法有效地解决了多智能体合作中的基线策略与精细调整之间的平衡问题，特别适用于机动性差异大的动态环境。其在Gazebo高保真仿真中的优越性能和对多样化攻击者的高度适应性，表明了该方法在实际USV防御应用中的巨大潜力。"}}
{"id": "2507.18072", "title": "C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams", "authors": ["Ryusei Fujimoto", "Yugo Nakamura", "Yutaka Arakawa"], "categories": ["cs.LG", "cs.CR", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18072v1", "summary": "Wearable accelerometers and gyroscopes encode fine-grained behavioural\nsignatures that can be exploited to re-identify users, making privacy\nprotection essential for healthcare applications. We introduce C-AAE, a\ncompressive anonymizing autoencoder that marries an Anonymizing AutoEncoder\n(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first\nprojects raw sensor windows into a latent space that retains activity-relevant\nfeatures while suppressing identity cues. ADPCM then differentially encodes\nthis latent stream, further masking residual identity information and shrinking\nthe bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE\ncuts user re-identification F1 scores by 10-15 percentage points relative to\nAAE alone, while keeping activity-recognition F1 within 5 percentage points of\nthe unprotected baseline. ADPCM also reduces data volume by roughly 75 %,\neasing transmission and storage overheads. These results demonstrate that C-AAE\noffers a practical route to balancing privacy and utility in continuous,\nsensor-based activity recognition for healthcare.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18072v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "C-AAE：用于医疗保健传感器流中隐私保护活动识别的压缩匿名化自编码器", "tldr": "C-AAE结合匿名化自编码器（AAE）和自适应差分脉冲编码调制（ADPCM），在医疗保健传感器数据中实现隐私保护的活动识别，同时降低数据量并有效抑制用户重识别。", "motivation": "可穿戴传感器数据包含精细的行为特征，可能导致用户重识别，因此在医疗保健应用中隐私保护至关重要。", "method": "本文提出了C-AAE，一种结合了匿名化自编码器（AAE）和自适应差分脉冲编码调制（ADPCM）的压缩匿名化自编码器。AAE首先将原始传感器窗口投影到潜在空间，保留活动相关特征并抑制身份线索；ADPCM进一步对潜在流进行差分编码，掩盖剩余身份信息并缩小比特率。", "result": "在MotionSense和PAMAP2数据集上的实验表明，C-AAE相对于单独使用AAE，用户重识别F1分数降低了10-15个百分点，同时活动识别F1分数保持在与未保护基线相差5个百分点以内。ADPCM还将数据量减少了约75%。", "conclusion": "C-AAE为医疗保健领域连续、基于传感器的活动识别提供了一种平衡隐私和实用性的实用方法。", "translation": "可穿戴加速度计和陀螺仪编码了可用于重新识别用户的精细行为特征，这使得隐私保护在医疗保健应用中至关重要。我们引入了C-AAE，一种结合了匿名化自编码器（AAE）和自适应差分脉冲编码调制（ADPCM）的压缩匿名化自编码器。AAE首先将原始传感器窗口投影到一个潜在空间，该空间保留了活动相关特征，同时抑制了身份线索。然后，ADPCM对这个潜在流进行差分编码，进一步掩盖了剩余的身份信息并缩小了比特率。在MotionSense和PAMAP2数据集上的实验表明，与单独使用AAE相比，C-AAE将用户重识别F1分数降低了10-15个百分点，同时将活动识别F1分数保持在未受保护基线的5个百分点以内。ADPCM还将数据量减少了大约75%，从而减轻了传输和存储开销。这些结果表明，C-AAE为医疗保健领域连续、基于传感器的活动识别提供了一种平衡隐私和实用性的实用途径。", "summary": "本文提出了C-AAE，一种结合匿名化自编码器（AAE）和自适应差分脉冲编码调制（ADPCM）的新型框架，旨在医疗保健传感器流中实现隐私保护的活动识别。C-AAE通过AAE在潜在空间中抑制身份信息，并通过ADPCM进一步压缩和匿名化数据。实验证明，C-AAE在显著降低用户重识别率的同时，基本保持了活动识别性能，并大幅减少了数据量，为医疗保健应用中隐私与效用的平衡提供了实用解决方案。", "keywords": "隐私保护, 活动识别, 自动编码器, 传感器流, 数据压缩", "comments": "C-AAE的创新之处在于结合了深度学习的匿名化能力（AAE）和信号处理的压缩技术（ADPCM），提供了一种双重保护机制。其重要性在于解决了医疗保健领域可穿戴设备数据隐私泄露的痛点，同时优化了数据传输和存储效率。这种方法在平衡隐私和实用性方面具有显著优势。"}}
{"id": "2507.18161", "title": "Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges", "authors": ["Samuele Cornell", "Christoph Boeddeker", "Taejin Park", "He Huang", "Desh Raj", "Matthew Wiesner", "Yoshiki Masuyama", "Xuankai Chang", "Zhong-Qiu Wang", "Stefano Squartini", "Paola Garcia", "Shinji Watanabe"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18161v1", "summary": "The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on\nmulti-channel, generalizable, joint automatic speech recognition (ASR) and\ndiarization of conversational speech. With participation from 9 teams\nsubmitting 32 diverse systems, these challenges have contributed to\nstate-of-the-art research in the field. This paper outlines the challenges'\ndesign, evaluation metrics, datasets, and baseline systems while analyzing key\ntrends from participant submissions. From this analysis it emerges that: 1)\nMost participants use end-to-end (e2e) ASR systems, whereas hybrid systems were\nprevalent in previous CHiME challenges. This transition is mainly due to the\navailability of robust large-scale pre-trained models, which lowers the data\nburden for e2e-ASR. 2) Despite recent advances in neural speech separation and\nenhancement (SSE), all teams still heavily rely on guided source separation,\nsuggesting that current neural SSE techniques are still unable to reliably deal\nwith complex scenarios and different recording setups. 3) All best systems\nemploy diarization refinement via target-speaker diarization techniques.\nAccurate speaker counting in the first diarization pass is thus crucial to\navoid compounding errors and CHiME-8 DASR participants especially focused on\nthis part. 4) Downstream evaluation via meeting summarization can correlate\nweakly with transcription quality due to the remarkable effectiveness of\nlarge-language models in handling errors. On the NOTSOFAR-1 scenario, even\nsystems with over 50\\% time-constrained minimum permutation WER can perform\nroughly on par with the most effective ones (around 11\\%). 5) Despite recent\nprogress, accurately transcribing spontaneous speech in challenging acoustic\nenvironments remains difficult, even when using computationally intensive\nsystem ensembles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18161v1", "cate": "eess.AS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "远距离对话语音识别的最新趋势：CHiME-7 和 8 DASR 挑战回顾", "tldr": "本文回顾了CHiME-7和8远距离对话语音识别（DASR）挑战，分析了参与者提交的关键趋势，包括端到端ASR的普及、神经语音分离和增强技术的局限性、说话人分离精炼的重要性、转录质量与下游评估的弱相关性，以及在挑战性声学环境中转录自发语音的持续困难。", "motivation": "本文旨在概述CHiME-7和8远距离对话语音识别（DASR）挑战的设计、评估指标、数据集和基线系统，并分析参与者提交的关键趋势，以促进该领域的最新研究。", "method": "本文通过概述CHiME-7和8远距离对话语音识别（DASR）挑战的设计、评估指标、数据集和基线系统，并分析参与者提交的系统中的关键趋势来进行研究。", "result": "1) 大多数参与者使用端到端（e2e）ASR系统，而非以往CHiME挑战中流行的混合系统，这主要得益于鲁棒的大规模预训练模型降低了e2e-ASR的数据负担。2) 尽管神经语音分离和增强（SSE）近期取得进展，所有团队仍严重依赖引导源分离，表明当前神经SSE技术仍无法可靠处理复杂场景和不同录音设置。3) 所有最佳系统都通过目标说话人分离技术进行说话人分离精炼。因此，在首次说话人分离中准确的说话人计数对于避免复合错误至关重要。4) 通过会议摘要进行的下游评估与转录质量可能弱相关，因为大型语言模型在处理错误方面表现出色。在NOTSOFAR-1场景中，即使系统的时间受限最小置换词错误率超过50%，其性能也能与最有效的系统（约11%）大致持平。5) 尽管近期有所进展，但在挑战性声学环境中准确转录自发语音仍然困难，即使使用计算密集型系统集成。", "conclusion": "尽管在端到端ASR和大型语言模型错误处理方面取得了进展，但在复杂的声学环境中准确进行远距离对话语音识别，特别是在可靠的语音分离和初始说话人计数方面，仍然面临显著挑战。", "translation": "CHiME-7和8远距离语音识别（DASR）挑战专注于多通道、可泛化、对话语音的联合自动语音识别（ASR）和说话人分离。9个团队提交了32个不同的系统，这些挑战为该领域的最新研究做出了贡献。本文概述了挑战的设计、评估指标、数据集和基线系统，同时分析了参与者提交的关键趋势。从这项分析中可以看出：1) 大多数参与者使用端到端（e2e）ASR系统，而在之前的CHiME挑战中，混合系统更为普遍。这种转变主要是由于鲁棒的大规模预训练模型的可用性，这降低了e2e-ASR的数据负担。2) 尽管神经语音分离和增强（SSE）近期取得了进展，所有团队仍然严重依赖引导源分离，这表明当前的神经SSE技术仍然无法可靠地处理复杂场景和不同的录音设置。3) 所有最佳系统都通过目标说话人分离技术进行说话人分离精炼。因此，在首次说话人分离中准确的说话人计数对于避免复合错误至关重要，CHiME-8 DASR的参与者尤其关注这部分。4) 通过会议摘要进行的下游评估可能与转录质量弱相关，因为大型语言模型在处理错误方面具有显著的有效性。在NOTSOFAR-1场景中，即使时间受限最小置换词错误率超过50%的系统也能与最有效的系统（约11%）大致持平。5) 尽管近期有所进展，但在挑战性声学环境中准确转录自发语音仍然困难，即使使用计算密集型系统集成。", "summary": "本文回顾了CHiME-7和8远距离对话语音识别（DASR）挑战，该挑战侧重于多通道、可泛化的ASR和对话语音的说话人分离。文章概述了挑战的设计、评估指标、数据集和基线系统，并分析了参与者提交的关键趋势。主要发现包括：端到端ASR系统成为主流，替代了混合系统；尽管神经语音分离和增强有所发展，但团队仍依赖引导源分离；说话人分离精炼，尤其是准确的初始说话人计数至关重要；大型语言模型处理错误的能力导致转录质量与下游摘要评估的弱相关性；以及在复杂声学环境下准确转录自发语音依然面临挑战。", "keywords": "远距离语音识别, ASR, 说话人分离, CHiME挑战, 端到端ASR", "comments": "本文通过对CHiME-7和8挑战的深入分析，为远距离对话语音识别领域的最新进展和现有挑战提供了宝贵的见解。其创新之处在于识别并量化了该领域的技术趋势，例如端到端ASR的普及和神经语音分离的局限性。论文强调了说话人分离精炼的重要性，并指出了大型语言模型在下游任务中对转录错误的缓解作用，这对于未来研究方向和系统设计具有重要指导意义。"}}
{"id": "2507.18479", "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to Expert-Defined Concepts", "authors": ["Ngoc Luyen Le", "Marie-Hélène Abel"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18479v1", "summary": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18479v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大型语言模型预测前置技能的效果如何？与专家定义概念的零样本比较", "tldr": "本文研究了大型语言模型（LLMs）在零样本设置下预测前置技能的能力，并发现一些先进的LLMs（如LLaMA4-Maverick、Claude-3-7-Sonnet和Qwen2-72B）的预测结果与专家定义的高度一致，表明LLMs在无需监督的情况下具有强大的语义推理能力，有望支持可扩展的前置技能建模。", "motivation": "前置技能对于有效的学习、评估和技能差距分析至关重要，但传统上由领域专家维护这些关系成本高昂且难以扩展。因此，本文旨在探索大型语言模型（LLMs）是否能够在零样本设置下预测前置技能，以解决传统方法的局限性。", "method": "本文引入了ESCO-PrereqSkill基准数据集，该数据集基于ESCO分类法构建，包含3,196项技能及其专家定义的前置链接。研究者使用标准化的提示策略，评估了13个最先进的LLMs，包括GPT-4、Claude 3、Gemini、LLaMA 4、Qwen2和DeepSeek，并在语义相似度、BERTScore和推理延迟方面进行了评估。", "result": "研究结果表明，LLaMA4-Maverick、Claude-3-7-Sonnet和Qwen2-72B等模型生成的预测结果与专家定义的真实情况高度一致，展示了在没有监督的情况下强大的语义推理能力。", "conclusion": "本文的研究结果强调了LLMs在支持可扩展的前置技能建模方面的潜力，可应用于个性化学习、智能辅导和基于技能的推荐系统。", "translation": "前置技能——在掌握更高级概念之前所需的基础能力——对于支持有效的学习、评估和技能差距分析非常重要。传统上由领域专家精心策划，这些关系维护成本高昂且难以扩展。本文研究大型语言模型（LLMs）是否能够在零样本设置下预测前置技能，仅使用自然语言描述，无需针对特定任务进行微调。我们引入了ESCO-PrereqSkill，一个从ESCO分类法构建的基准数据集，包含3,196项技能及其专家定义的前置链接。我们使用标准化的提示策略，评估了13个最先进的LLMs，包括GPT-4、Claude 3、Gemini、LLaMA 4、Qwen2和DeepSeek，并在语义相似度、BERTScore和推理延迟方面进行了评估。我们的结果显示，LLaMA4-Maverick、Claude-3-7-Sonnet和Qwen2-72B等模型生成的预测与专家定义的真实情况高度一致，展示了无需监督的强大语义推理能力。这些发现突出了LLMs在支持个性化学习、智能辅导和基于技能的推荐系统应用中可扩展的前置技能建模方面的潜力。", "summary": "本文探讨了大型语言模型（LLMs）在零样本设置下预测前置技能的能力，以克服传统专家维护方式的成本和可扩展性问题。研究构建了ESCO-PrereqSkill数据集，包含3,196项技能及其前置链接，并评估了包括GPT-4、Claude 3、Gemini等在内的13个先进LLMs。结果表明，部分LLMs（如LLaMA4-Maverick、Claude-3-7-Sonnet、Qwen2-72B）的预测与专家定义高度吻合，展现了强大的语义推理能力，这预示着LLMs在个性化学习和智能辅导等领域中可扩展的前置技能建模方面具有巨大潜力。", "keywords": "大型语言模型, 前置技能, 零样本学习, ESCO-PrereqSkill, 技能建模", "comments": "这项研究具有重要的实际意义，因为它提供了一种潜在的解决方案，以自动化和扩展前置技能的识别，这对于大规模个性化学习系统至关重要。零样本方法的成功尤为突出，因为它避免了昂贵的数据标注和模型微调。该研究也为未来在教育技术和知识图谱构建领域中利用LLMs提供了坚实的基础。"}}
{"id": "2408.01162", "title": "PreMix: Label-Efficient Multiple Instance Learning via Non-Contrastive Pre-training and Feature Mixing", "authors": ["Bryan Wong", "Mun Yong Yi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2408.01162v3", "summary": "Multiple instance learning (MIL) has emerged as a powerful framework for\nweakly supervised whole slide image (WSI) classification, enabling slide-level\npredictions without requiring detailed patch-level annotations. Despite its\nsuccess, a critical limitation of current MIL methods lies in the\nunderutilization of pre-training for the MIL aggregator. Most existing\napproaches initialize the aggregator randomly and train it from scratch, making\nperformance highly sensitive to the quantity of labeled WSIs and ignoring the\nabundance of unlabeled WSIs commonly available in clinical settings. To address\nthis, we propose PreMix, a novel framework that leverages a non-contrastive\npre-training method, Barlow Twins, augmented with the Slide Mixing approach to\ngenerate additional positive pairs and enhance feature learning, particularly\nunder limited labeled WSI conditions. Fine-tuning with Mixup and Manifold Mixup\nfurther enhances robustness by effectively handling the diverse sizes of\ngigapixel WSIs. Experimental results demonstrate that integrating PreMix as a\nplug-in module into HIPT yields an average F1 improvement of 4.7% over the\nbaseline HIPT across various WSI training sizes and datasets. These findings\nunderscore its potential to advance WSI classification with limited labeled\ndata and its applicability to real-world histopathology practices. The code is\navailable at https://github.com/bryanwong17/PreMix", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2408.01162v3", "cate": "cs.CV", "date": "2024-08-02", "updated": "2025-07-24", "AI": {"title_translation": "PreMix：通过非对比预训练和特征混合实现标签高效的多实例学习", "tldr": "PreMix提出了一种新的框架，通过非对比预训练和特征混合来解决多实例学习（MIL）中标签数据不足的问题，显著提高了WSI分类的性能。", "motivation": "当前的多实例学习（MIL）方法在全玻片图像（WSI）分类中存在关键局限性，即MIL聚合器对预训练的利用不足。大多数现有方法随机初始化聚合器并从头开始训练，这使得性能对标记WSI的数量高度敏感，并忽略了临床环境中常见的未标记WSI的丰富性。", "method": "我们提出了PreMix框架，它利用非对比预训练方法Barlow Twins，并结合Slide Mixing方法来生成额外的正样本对并增强特征学习，尤其是在标记WSI数据有限的条件下。通过Mixup和Manifold Mixup进行微调进一步增强了鲁棒性，有效处理了千兆像素WSI的不同大小。", "result": "实验结果表明，将PreMix作为插件模块集成到HIPT中，在各种WSI训练大小和数据集上，F1分数比基线HIPT平均提高了4.7%。", "conclusion": "这些发现强调了PreMix在标记数据有限的情况下推进WSI分类的潜力及其在真实世界组织病理学实践中的适用性。", "translation": "多实例学习（MIL）已成为一种强大的框架，用于弱监督全玻片图像（WSI）分类，无需详细的补丁级标注即可实现玻片级预测。尽管取得了成功，但当前MIL方法的一个关键局限性在于MIL聚合器对预训练的利用不足。大多数现有方法随机初始化聚合器并从头开始训练，这使得性能对标记WSI的数量高度敏感，并忽略了临床环境中常见的未标记WSI的丰富性。为了解决这个问题，我们提出了PreMix，一个新颖的框架，它利用非对比预训练方法Barlow Twins，并结合Slide Mixing方法来生成额外的正样本对并增强特征学习，尤其是在标记WSI标记数据有限的条件下。通过Mixup和Manifold Mixup进行微调进一步增强了鲁棒性，有效处理了千兆像素WSI的不同大小。实验结果表明，将PreMix作为插件模块集成到HIPT中，在各种WSI训练大小和数据集上，F1分数比基线HIPT平均提高了4.7%。这些发现强调了其在标记数据有限的情况下推进WSI分类的潜力及其在真实世界组织病理学实践中的适用性。代码可在https://github.com/bryanwong17/PreMix获取。", "summary": "PreMix是一个新颖的框架，旨在解决多实例学习（MIL）在全玻片图像（WSI）分类中对大量标记数据依赖的问题。它通过结合非对比预训练方法Barlow Twins和Slide Mixing来增强特征学习，特别是在标记数据有限的情况下。此外，通过Mixup和Manifold Mixup进行微调以提高鲁棒性。实验证明，PreMix作为HIPT的插件模块，在各种数据集和训练规模下，F1分数平均提高了4.7%，显示了其在真实世界病理学应用中的潜力。", "keywords": "多实例学习, 全玻片图像分类, 非对比预训练, 标签效率, 特征混合", "comments": "PreMix的创新点在于将非对比预训练（Barlow Twins）与数据增强技术（Slide Mixing, Mixup, Manifold Mixup）相结合，有效地解决了MIL在WSI分类中标记数据稀缺的问题。其作为插件模块的特性增加了其实用性和兼容性。这项工作对于推动弱监督学习在医疗影像领域的应用具有重要意义，尤其是在病理学诊断中。"}}
{"id": "2507.17927", "title": "SMARTAPS: Tool-augmented LLMs for Operations Management", "authors": ["Timothy Tin Long Yu", "Mahdi Mostajabdaveh", "Jabo Serge Byusa", "Rindra Ramamonjison", "Giuseppe Carenini", "Kun Mao", "Zirui Zhou", "Yong Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2507.17927v1", "summary": "Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw", "comment": "https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/", "pdf_url": "http://arxiv.org/pdf/2507.17927v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "SMARTAPS：面向运营管理的工具增强型大型语言模型", "tldr": "SmartAPS是一个基于工具增强型大型语言模型的会话系统，旨在通过直观的自然语言聊天界面，帮助运营规划者更便捷地使用高级规划系统（APS），从而降低传统APS的咨询和维护成本。", "motivation": "传统的高级规划系统（APS）虽然非常有益，但由于定制和维护所需的持续顾问成本，许多客户无法负担。为了满足供应链规划者对更易于访问的APS的需求，本研究提出了SmartAPS。", "method": "本研究提出了SmartAPS，一个基于工具增强型大型语言模型（LLM）构建的会话系统。该系统为运营规划者提供直观的自然语言聊天界面。", "result": "SmartAPS系统能够让运营规划者查询信息、进行反事实推理、接收建议并执行情景分析，以更好地管理其运营。", "conclusion": "SmartAPS通过提供一个直观的自然语言聊天界面，使运营规划者能够更便捷地使用高级规划系统（APS）的功能，从而降低了传统APS的使用门槛和成本。", "translation": "大型语言模型（LLMs）为增强用户与现实世界应用中传统算法和工具的交互提供了有趣的机会。高级规划系统（APS）是一种复杂的软件，它利用优化来帮助运营规划者创建、解释和修改运营计划。虽然非常有益，但由于负责定制和维护的顾问的持续成本，许多客户无法使用APS。为了解决供应链规划者对更易于访问的APS的需求，我们提出了SmartAPS，一个基于工具增强型LLM构建的会话系统。我们的系统为运营规划者提供了一个直观的自然语言聊天界面，允许他们查询信息、执行反事实推理、接收建议和执行情景分析，以更好地管理其运营。一个演示该系统的短视频已经发布：https://youtu.be/KtIrJjlDbyw", "summary": "本文介绍了SmartAPS，一个基于工具增强型大型语言模型（LLM）的会话系统，旨在解决传统高级规划系统（APS）因高昂的定制和维护成本而导致的可访问性问题。SmartAPS为运营规划者提供了一个直观的自然语言聊天界面，使他们能够轻松地查询信息、进行反事实推理、获取建议和执行情景分析，从而更有效地管理运营。", "keywords": "大型语言模型, 运营管理, 高级规划系统, 工具增强, 自然语言处理", "comments": "SmartAPS的创新之处在于将大型语言模型与传统运营管理工具结合，通过自然语言界面极大地降低了高级规划系统的使用门槛和成本。这对于中小型企业或预算有限的公司而言，具有重要的实际应用价值。该方法提供了一个可扩展的框架，有望在其他领域也实现类似的人机交互优化。"}}
{"id": "2507.18368", "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios", "authors": ["Zhuang Qiang Bok", "Watson Wei Khong Chua"], "categories": ["cs.AI", "I.2.0; I.2.6; J.4"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on Evaluation and Trustworthiness of Agentic and Generative AI Models this https URL", "url": "http://arxiv.org/abs/2507.18368v1", "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "pdf_url": "http://arxiv.org/pdf/2507.18368v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "超越显而易见的推理：评估大型语言模型在金融场景中的发散和收敛思维", "tldr": "引入ConDiFi基准，评估LLM在金融场景中的发散和收敛思维，发现模型表现差异显著。", "motivation": "现有LLM推理基准侧重事实准确性或逻辑步骤，但在金融领域，专业人士需要生成创造性、合理的结果并做出最优决策，这要求发散和收敛思维。", "method": "本文引入了ConDiFi基准，包含607个用于发散推理的宏观金融提示和990个用于收敛推理的多跳对抗性多选题。研究使用该基准评估了14个领先的大型语言模型。", "result": "评估发现模型之间存在显著差异。尽管GPT-4o具有高流畅性，但在新颖性和可操作性方面表现不佳。相比之下，DeepSeek-R1和Cohere Command R+等模型在生成可操作的、适合投资决策的见解方面表现优异。", "conclusion": "ConDiFi为评估大型语言模型在金融领域安全和战略部署所必需的推理能力提供了新视角。", "translation": "大多数大型语言模型（LLMs）的推理基准都强调事实准确性或循序渐进的逻辑。然而，在金融领域，专业人士不仅必须收敛于最优决策，还必须在不确定性下生成有创意、合理且可能出现的未来情景。我们引入了ConDiFi，这是一个联合评估LLMs在金融任务中发散思维和收敛思维的基准。\nConDiFi包含607个用于发散推理的宏观金融提示和990个用于收敛推理的多跳对抗性多选题。使用此基准，我们评估了14个领先的模型，并发现了惊人的差异。尽管GPT-4o具有高流畅性，但在新颖性和可操作性方面表现不佳。相比之下，DeepSeek-R1和Cohere Command R+等模型在生成可操作的、适合投资决策的见解方面名列前茅。ConDiFi为评估LLMs在金融领域安全和战略部署所必需的推理能力提供了新视角。", "summary": "本文提出了ConDiFi基准，旨在全面评估大型语言模型在金融场景中的发散和收敛思维能力。该基准包含宏观金融发散提示和多跳收敛多选题。研究发现，尽管GPT-4o表现流畅，但在新颖性和可操作性方面有所欠缺，而DeepSeek-R1和Cohere Command R+等模型在生成可操作的金融见解方面表现突出。ConDiFi为金融领域LLM的部署提供了新的评估视角。", "keywords": "大型语言模型, 金融推理, 发散思维, 收敛思维, 基准测试", "comments": "本文的创新之处在于提出了一个专门针对金融领域发散和收敛思维的LLM评估基准ConDiFi，填补了现有基准的空白。这对于理解LLM在复杂、不确定金融环境中的实际应用能力至关重要，有助于推动LLM在金融领域的安全和战略部署。"}}
{"id": "2507.18622", "title": "Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork", "authors": ["Armin Bernstetter", "Tom Kwasnitschka", "Isabella Peters"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted for Mensch und Computer 2025 Short Paper Track", "url": "http://arxiv.org/abs/2507.18622v1", "summary": "Ensuring reproducibility of research is an integral part of good scientific\npractice. One way to support this is through provenance: information about\nresearch workflows from data gathering to researchers' sensemaking processes\nleading to published results. This is highly important in disciplines such as\ngeosciences, where researchers use software for interactive and immersive\nvisualizations of geospatial data, doing virtual measurements in simulated\nfieldwork on 3D models. We evaluated a provenance management tool, which allows\nrecording of interactions with a virtual fieldwork tool and annotating\ndifferent states of the visualization. The user study investigated how\nresearchers used this Digital Lab Book (DLB) and whether perceived ease of use\nand perceived usefulness differed between groups in immersive or non-immersive\nsettings. Participants perceived the DLB as both useful and easy to use. While\nthere were indications of differences in perceived ease of use (higher for\nimmersive setting), usage patterns showed no significant group differences.", "comment": "Accepted for Mensch und Computer 2025 Short Paper Track", "pdf_url": "http://arxiv.org/pdf/2507.18622v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "沉浸式虚拟野外工作中溯源管理工具的评估", "tldr": "本文评估了一款用于沉浸式虚拟野外工作的溯源管理工具（数字实验手册DLB）。用户研究发现，研究人员普遍认为该工具易于使用且有用，尽管在沉浸式环境中易用性感知略高，但使用模式无显著差异。", "motivation": "确保研究的可重复性是良好科学实践的重要组成部分，尤其是在地球科学等领域，研究人员需要记录从数据收集到结果发布整个研究工作流程的信息（溯源）。本文旨在评估一款支持沉浸式虚拟野外工作溯源管理的工具。", "method": "研究评估了一款允许记录虚拟野外工具交互并注释可视化状态的溯源管理工具（数字实验手册DLB）。通过用户研究，调查了研究人员如何使用该DLB，并比较了沉浸式和非沉浸式设置下感知易用性和感知有用性的差异。", "result": "参与者普遍认为DLB既有用又易于使用。虽然有迹象表明感知易用性存在差异（沉浸式设置中更高），但使用模式没有显示出显著的组间差异。", "conclusion": "该溯源管理工具（DLB）在虚拟野外工作中被认为是有用且易于使用的，这有助于提高研究的可重复性，尽管在沉浸式和非沉浸式环境下的使用模式差异不显著。", "translation": "确保研究的可重复性是良好科学实践不可或缺的一部分。支持这一点的一种方式是通过溯源：关于研究工作流程的信息，从数据收集到研究人员的意义构建过程，最终形成已发布的结果。这在地球科学等学科中尤为重要，在该领域中，研究人员使用软件进行地理空间数据的交互式和沉浸式可视化，在3D模型的模拟野外工作中进行虚拟测量。我们评估了一个溯源管理工具，该工具允许记录与虚拟野外工具的交互并注释可视化的不同状态。用户研究调查了研究人员如何使用这个数字实验手册（DLB），以及在沉浸式或非沉浸式设置中，感知易用性和感知有用性是否存在差异。参与者认为DLB既有用又易于使用。虽然有迹象表明感知易用性存在差异（沉浸式设置中更高），但使用模式没有显示出显著的组间差异。", "summary": "本文评估了一款旨在支持虚拟野外工作可重复性的溯源管理工具（数字实验手册DLB）。该工具记录用户与虚拟野外工具的交互并注释可视化状态。通过用户研究，作者调查了研究人员对DLB的接受度，并比较了沉浸式与非沉浸式环境下的感知差异。结果表明，研究人员普遍认为DLB有用且易于使用，尽管沉浸式环境下的易用性感知略高，但不同组间的使用模式无显著差异。", "keywords": "溯源管理, 虚拟野外工作, 可重复性, 用户研究, 地球科学", "comments": "这项研究的创新之处在于将溯源管理工具应用于沉浸式虚拟野外工作这一特定领域，这对于提高地球科学等依赖复杂可视化和测量学科的研究可重复性具有重要意义。该研究通过用户研究评估了工具的实用性和易用性，为未来虚拟野外工作工具的设计和应用提供了实践指导。研究结果表明该工具具有积极的用户体验，但指出在不同沉浸度设置下的使用模式差异不显著，这可能提示在特定设计上进一步优化的空间。"}}
{"id": "2507.18323", "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation", "authors": ["Minje Park", "Jeonghwa Lim", "Taehyung Yu", "Sunghoon Joo"], "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages, 2 figures", "url": "http://arxiv.org/abs/2507.18323v1", "summary": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.", "comment": "6 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.18323v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于心电图描绘中半监督语义分割的多数据集基准", "tldr": "本研究提出了首个针对心电图描绘中半监督语义分割的多数据集基准，并发现Transformer在半监督心电图描绘中优于卷积网络。", "motivation": "心电图描绘对临床诊断至关重要，但由于公开可用标注数据集的稀缺性，深度学习的进展受到限制。半监督学习通过利用大量未标注的心电图数据提供了一个有前景的解决方案。", "method": "本研究建立了首个系统性的心电图描绘半监督语义分割（SemiSeg）基准。研究者整理并统一了多个公共数据集，采用了五种代表性的计算机视觉SemiSeg算法，并在卷积网络和Transformer两种不同架构上实现，同时在域内和跨域两种设置下进行了评估。此外，还提出了针对心电图的特定训练配置和增强策略，并引入了标准化的评估框架。", "result": "Transformer在半监督心电图描绘中表现优于卷积网络。", "conclusion": "该基准有望为推进半监督心电图描绘方法奠定基础，并促进该领域的进一步研究。", "translation": "心电图（ECG）描绘，即有意义波形特征的分割，对临床诊断至关重要。尽管深度学习取得了最新进展，但由于公开可用标注数据集的稀缺性，其进展受到限制。半监督学习通过利用大量未标注的心电图数据，提供了一个有前景的解决方案。在本研究中，我们提出了首个针对心电图描绘中半监督语义分割（SemiSeg）的系统性基准。我们整理并统一了多个公共数据集，包括以前未充分利用的来源，以支持稳健和多样化的评估。我们采用了来自计算机视觉的五种代表性SemiSeg算法，并在两种不同的架构：卷积网络和Transformer上实现，并在两种不同的设置：域内和跨域中进行了评估。此外，我们提出了针对心电图的特定训练配置和增强策略，并引入了标准化的评估框架。我们的结果表明，Transformer在半监督心电图描绘中表现优于卷积网络。我们预期我们的基准将作为推进半监督心电图描绘方法的基础，并促进该领域的进一步研究。", "summary": "本研究提出了首个针对心电图描绘中半监督语义分割的系统性基准。通过整合多个公共数据集，并在卷积网络和Transformer两种架构上评估了五种代表性半监督算法，发现在半监督心电图描绘任务中Transformer表现更优。研究还提出了心电图特有的训练配置和评估框架，旨在为该领域的未来研究提供基础和促进。", "keywords": "心电图描绘, 半监督学习, 语义分割, 基准, Transformer", "comments": "本文通过构建首个针对心电图描绘中半监督语义分割的多数据集基准，解决了该领域因标注数据稀缺而面临的挑战。其创新性在于整合了多种数据集，系统性地评估了不同算法和网络架构，并提出了特定于心电图的训练策略和标准化评估框架。这项工作对于推动心电图分析的半监督学习方法具有重要意义。"}}
{"id": "2507.18594", "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement", "authors": ["Xuecheng Bai", "Yuxiang Wang", "Boyu Hu", "Qinyuan Jie", "Chuanzhi Xu", "Hongru Xiao", "Kechen Li", "Vera Chung"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18594v1", "summary": "Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18594v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DRWKV：聚焦物体边缘的低光图像增强", "tldr": "DRWKV是一种新的低光图像增强模型，通过结合全局边缘Retinex理论、演化WKV注意力机制以及双边光谱对齐器，有效提升了图像边缘细节和整体视觉质量，并在多个基准测试中取得了领先性能，同时计算复杂度较低。", "motivation": "低光图像增强仍然是一个具有挑战性的任务，尤其是在极端光照退化下，如何保持物体边缘的连续性和精细结构细节是一个难题。", "method": "本文提出了一种新颖的模型DRWKV（Detailed Receptance Weighted Key Value），它集成了以下组件：1. 全局边缘Retinex（GER）理论，用于有效解耦光照和边缘结构，增强边缘保真度。2. 演化WKV注意力机制，采用螺旋扫描机制捕获空间边缘连续性并更有效地建模不规则结构。3. 双边光谱对齐器（Bi-SAB）和定制的MS2-Loss，共同对齐亮度和色度特征，以提高视觉自然度并减轻伪影。", "result": "在五个LLIE基准测试中，DRWKV在PSNR、SSIM和NIQE方面取得了领先性能，同时保持了较低的计算复杂度。此外，DRWKV还增强了低光多目标跟踪任务的下游性能，验证了其泛化能力。", "conclusion": "DRWKV模型通过其独特的设计，成功解决了低光图像增强中边缘细节保持和结构连续性的挑战，并在多个方面展现出卓越的性能和泛化能力。", "translation": "低光图像增强仍然是一个具有挑战性的任务，特别是在极端光照退化下，保持物体边缘的连续性和精细结构细节尤为困难。在本文中，我们提出了一种新颖的模型DRWKV（Detailed Receptance Weighted Key Value），该模型整合了我们提出的全局边缘Retinex（GER）理论，从而能够有效地解耦光照和边缘结构，以增强边缘保真度。其次，我们引入了演化WKV注意力机制，这是一种螺旋扫描机制，能够捕获空间边缘连续性并更有效地建模不规则结构。第三，我们设计了双边光谱对齐器（Bi-SAB）和定制的MS2-Loss，以共同对齐亮度和色度特征，从而提高视觉自然度并减轻伪影。在五个LLIE基准测试中进行的广泛实验表明，DRWKV在PSNR、SSIM和NIQE方面取得了领先性能，同时保持了较低的计算复杂度。此外，DRWKV增强了低光多目标跟踪任务的下游性能，验证了其泛化能力。", "summary": "DRWKV是一种用于低光图像增强的新模型，旨在解决极端光照下边缘和细节保持的挑战。该模型核心包含三部分：全局边缘Retinex理论用于解耦光照和边缘，演化WKV注意力机制用于捕获空间边缘连续性，以及双边光谱对齐器和MS2-Loss用于对齐亮度和色度特征。实验证明，DRWKV在多个基准测试中表现出领先的性能，且计算效率高，并能提升下游多目标跟踪任务的表现。", "keywords": "低光图像增强, 边缘增强, Retinex, 注意力机制, DRWKV", "comments": "DRWKV的创新之处在于其多组件协同设计，特别是全局边缘Retinex理论和演化WKV注意力机制，它们共同解决了低光图像增强中边缘和细节丢失的关键问题。模型还考虑了视觉自然度与伪影抑制，并通过下游任务验证了其泛化性，显示出其实用价值和潜在影响力。"}}
{"id": "2502.15487", "title": "ExpliCa: Evaluating Explicit Causal Reasoning in Large Language Models", "authors": ["Martina Miliani", "Serena Auriemma", "Alessandro Bondielli", "Emmanuele Chersoni", "Lucia Passaro", "Irene Sucameli", "Alessandro Lenci"], "categories": ["cs.CL", "cs.AI", "68T50, 68T07", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication in Findings of ACL 2025", "url": "http://arxiv.org/abs/2502.15487v3", "summary": "Large Language Models (LLMs) are increasingly used in tasks requiring\ninterpretive and inferential accuracy. In this paper, we introduce ExpliCa, a\nnew dataset for evaluating LLMs in explicit causal reasoning. ExpliCa uniquely\nintegrates both causal and temporal relations presented in different linguistic\norders and explicitly expressed by linguistic connectives. The dataset is\nenriched with crowdsourced human acceptability ratings. We tested LLMs on\nExpliCa through prompting and perplexity-based metrics. We assessed seven\ncommercial and open-source LLMs, revealing that even top models struggle to\nreach 0.80 accuracy. Interestingly, models tend to confound temporal relations\nwith causal ones, and their performance is also strongly influenced by the\nlinguistic order of the events. Finally, perplexity-based scores and prompting\nperformance are differently affected by model size.", "comment": "Accepted for publication in Findings of ACL 2025", "pdf_url": "http://arxiv.org/pdf/2502.15487v3", "cate": "cs.CL", "date": "2025-02-21", "updated": "2025-07-24", "AI": {"title_translation": "ExpliCa: 评估大型语言模型中的显式因果推理", "tldr": "引入了ExpliCa数据集，用于评估大型语言模型在显式因果推理方面的能力，发现即使是顶级模型也难以达到高精度，并且容易混淆因果与时间关系，性能受语言顺序影响。", "motivation": "大型语言模型（LLMs）在需要解释性和推理准确性的任务中应用日益广泛，但其在显式因果推理方面的能力尚不明确，因此需要一个专门的数据集来评估。", "method": "引入了一个名为ExpliCa的新数据集，该数据集独特地整合了不同语言顺序呈现的因果和时间关系，并由语言连接词明确表达。数据集通过众包人类可接受度评分进行丰富。通过提示和基于困惑度的指标，对七个商业和开源LLMs在ExpliCa上进行了测试。", "result": "测试发现，即使是顶级模型也难以达到0.80的准确率。模型倾向于将时间关系与因果关系混淆，并且它们的性能也受到事件语言顺序的强烈影响。此外，基于困惑度的分数和提示性能受模型大小的影响不同。", "conclusion": "大型语言模型在显式因果推理方面仍有显著不足，尤其是在区分因果和时间关系以及处理不同语言顺序时。模型大小对不同评估指标的影响也值得进一步研究。", "translation": "大型语言模型（LLMs）在需要解释性和推理准确性的任务中应用日益广泛。在本文中，我们引入了ExpliCa，这是一个用于评估LLMs在显式因果推理方面的新数据集。ExpliCa独特地整合了以不同语言顺序呈现并由语言连接词明确表达的因果和时间关系。该数据集通过众包的人类可接受度评分进行了丰富。我们通过提示和基于困惑度的指标在ExpliCa上测试了LLMs。我们评估了七个商业和开源LLMs，结果显示即使是顶级模型也难以达到0.80的准确率。有趣的是，模型倾向于将时间关系与因果关系混淆，并且它们的性能也受到事件语言顺序的强烈影响。最后，基于困惑度的分数和提示性能受模型大小的不同影响。", "summary": "本文介绍了ExpliCa，一个用于评估大型语言模型（LLMs）显式因果推理能力的新数据集。ExpliCa数据集包含不同语言顺序呈现的因果和时间关系，并附有众包的人类评分。研究人员使用提示和困惑度指标测试了七个LLMs，发现即使是顶级模型在因果推理方面也表现不佳，准确率低于0.80。实验结果表明，LLMs容易混淆因果与时间关系，且其性能受事件语言顺序的显著影响。此外，模型大小对不同的评估指标有不同的影响。", "keywords": "大型语言模型, 因果推理, 数据集, ExpliCa, 时间关系", "comments": "这项研究通过引入ExpliCa数据集，为评估LLMs的显式因果推理能力提供了一个新颖且重要的工具。其创新之处在于整合了因果和时间关系，并考虑了语言顺序的影响。研究结果揭示了当前LLMs在深层语义理解方面的局限性，特别是在区分细微的因果和时间关联上，这对于未来LLMs的改进具有指导意义。数据集的众包评分也增加了其研究价值。"}}
{"id": "2504.03173", "title": "PPFPL: Cross-silo Privacy-preserving Federated Prototype Learning Against Data Poisoning Attacks on Non-IID Data", "authors": ["Hongliang Zhang", "Jiguo Yu", "Fenghua Xu", "Chunqiang Hu", "Yongzhao Zhang", "Xiaofen Wang", "Zhongyuan Yu", "Xiaosong Zhang"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.03173v4", "summary": "Privacy-Preserving Federated Learning (PPFL) allows multiple clients to\ncollaboratively train a deep learning model by submitting hidden model updates.\nNonetheless, PPFL is vulnerable to data poisoning attacks due to the\ndistributed training nature of clients. Existing solutions have struggled to\nimprove the performance of cross-silo PPFL in poisoned Non-IID data. To address\nthe issues, this paper proposes a privacy-preserving federated prototype\nlearning framework, named PPFPL, which enhances the cross-silo FL performance\nin poisoned Non-IID data while effectively resisting data poisoning attacks.\nSpecifically, we adopt prototypes as client-submitted model updates to\neliminate the impact of tampered data distribution on federated learning.\nMoreover, we utilize two servers to achieve Byzantine-robust aggregation by\nsecure aggregation protocol, which greatly reduces the impact of malicious\nclients. Theoretical analyses confirm the convergence of PPFPL, and\nexperimental results on publicly available datasets show that PPFPL is\neffective for resisting data poisoning attacks with Non-IID conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.03173v4", "cate": "cs.CR", "date": "2025-04-04", "updated": "2025-07-24", "AI": {"title_translation": "PPFPL：针对非IID数据投毒攻击的跨筒仓隐私保护联邦原型学习", "tldr": "PPFPL是一个隐私保护的联邦原型学习框架，通过使用原型作为客户端更新和双服务器拜占庭鲁棒聚合，有效抵抗非IID数据上的数据投毒攻击，同时提高跨筒仓联邦学习性能。", "motivation": "现有的隐私保护联邦学习（PPFL）解决方案在中毒的非IID数据上提升跨筒仓PPFL性能方面面临困难，且PPFL易受数据投毒攻击。", "method": "本文提出了一个名为PPFPL的隐私保护联邦原型学习框架。它采用原型作为客户端提交的模型更新，以消除被篡改数据分布对联邦学习的影响。此外，它利用两个服务器通过安全聚合协议实现拜占庭鲁棒聚合，大大减少了恶意客户端的影响。", "result": "理论分析证实了PPFPL的收敛性。在公开数据集上的实验结果表明，PPFPL在非IID条件下能有效抵抗数据投毒攻击。", "conclusion": "PPFPL框架能够有效抵抗非IID条件下的数据投毒攻击，同时提高跨筒仓联邦学习的性能。", "translation": "隐私保护联邦学习（PPFL）允许多个客户端通过提交隐藏的模型更新来协作训练深度学习模型。然而，由于客户端的分布式训练性质，PPFL容易受到数据投毒攻击。现有解决方案难以改善中毒非IID数据上跨筒仓PPFL的性能。为了解决这些问题，本文提出了一种名为PPFPL的隐私保护联邦原型学习框架，该框架在有效抵抗数据投毒攻击的同时，提高了中毒非IID数据上的跨筒仓联邦学习性能。具体来说，我们采用原型作为客户端提交的模型更新，以消除被篡改数据分布对联邦学习的影响。此外，我们利用两个服务器通过安全聚合协议实现拜占庭鲁棒聚合，这大大减少了恶意客户端的影响。理论分析证实了PPFPL的收敛性，并在公开数据集上的实验结果表明，PPFPL在非IID条件下能有效抵抗数据投毒攻击。", "summary": "本文提出了PPFPL，一个隐私保护的联邦原型学习框架，旨在解决跨筒仓隐私保护联邦学习在非IID数据上易受数据投毒攻击的问题。PPFPL通过使用原型作为客户端提交的模型更新来减轻篡改数据分布的影响，并利用双服务器和安全聚合协议实现拜占庭鲁棒聚合以抵御恶意客户端。理论分析和实验结果均证实了PPFPL在抵抗非IID条件下的数据投毒攻击方面的有效性。", "keywords": "联邦学习, 隐私保护, 数据投毒攻击, 非IID数据, 原型学习", "comments": "PPFPL的创新点在于将原型学习引入联邦学习中，以应对非IID数据上的数据投毒攻击，并通过双服务器安全聚合增强了鲁棒性。这为在现实世界中部署更安全的联邦学习提供了重要的方向。"}}
{"id": "2507.17798", "title": "Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism", "authors": ["Kenta Shiraishi", "Yuka Muto", "Atsushi Okazaki", "Shunji Kotsuki"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17798v1", "summary": "High-resolution (HR) precipitation prediction is essential for reducing\ndamage from stationary and localized heavy rainfall; however, HR precipitation\nforecasts using process-driven numerical weather prediction models remains\nchallenging. This study proposes using Wasserstein Generative Adversarial\nNetwork (WGAN) to perform precipitation downscaling with an optimal transport\ncost. In contrast to a conventional neural network trained with mean squared\nerror, the WGAN generated visually realistic precipitation fields with\nfine-scale structures even though the WGAN exhibited slightly lower performance\non conventional evaluation metrics. The learned critic of WGAN correlated well\nwith human perceptual realism. Case-based analysis revealed that large\ndiscrepancies in critic scores can help identify both unrealistic WGAN outputs\nand potential artifacts in the reference data. These findings suggest that the\nWGAN framework not only improves perceptual realism in precipitation\ndownscaling but also offers a new perspective for evaluating and\nquality-controlling precipitation datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17798v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "基于 Wasserstein GAN 的降水降尺度与最优传输，以增强感知真实感", "tldr": "本研究提出使用 Wasserstein GAN (WGAN) 和最优传输来对降水进行降尺度，以生成视觉上更真实的降水场，并发现 WGAN 的判别器可用于识别不真实的输出和数据伪影，为降水数据评估和质量控制提供了新视角。", "motivation": "高分辨率降水预测对于减少局部强降雨造成的损害至关重要，但使用过程驱动的数值天气预报模型进行高分辨率降水预报仍然具有挑战性。", "method": "本研究提出使用 Wasserstein 生成对抗网络 (WGAN) 并结合最优传输成本进行降水降尺度。", "result": "与使用均方误差训练的传统神经网络相比，WGAN 生成了具有精细尺度结构的视觉真实降水场，尽管在传统评估指标上性能略低。WGAN 学习到的判别器与人类感知真实感高度相关。基于案例的分析表明，判别器分数中的巨大差异有助于识别不真实的 WGAN 输出以及参考数据中潜在的伪影。", "conclusion": "WGAN 框架不仅提高了降水降尺度中的感知真实感，而且为评估和质量控制降水数据集提供了新的视角。", "translation": "高分辨率 (HR) 降水预测对于减少静止和局部强降雨造成的损害至关重要；然而，使用过程驱动的数值天气预报模型进行高分辨率降水预报仍然具有挑战性。本研究提出使用 Wasserstein 生成对抗网络 (WGAN) 进行降水降尺度，并采用最优传输成本。与使用均方误差训练的传统神经网络不同，WGAN 生成了具有精细尺度结构的视觉真实降水场，尽管 WGAN 在传统评估指标上表现略低。WGAN 学习到的判别器与人类感知真实感高度相关。基于案例的分析表明，判别器分数中的巨大差异有助于识别不真实的 WGAN 输出以及参考数据中潜在的伪影。这些发现表明，WGAN 框架不仅提高了降水降尺度中的感知真实感，而且为评估和质量控制降水数据集提供了新的视角。", "summary": "本研究提出一种基于 Wasserstein GAN (WGAN) 和最优传输的降水降尺度方法，旨在解决高分辨率降水预测的挑战。实验结果表明，尽管在传统度量上表现略逊，WGAN 能够生成具有更高视觉真实感和精细结构的高分辨率降水场。WGAN 的判别器与人类感知真实感高度相关，并且其分数差异可用于识别模型输出中的不真实性以及参考数据中的潜在缺陷。这表明 WGAN 不仅能提升降水降尺度的感知真实感，还为降水数据的评估和质量控制提供了新途径。", "keywords": "Wasserstein GAN, 降水降尺度, 最优传输, 感知真实感, 质量控制", "comments": "这项研究的创新之处在于将 Wasserstein GAN 应用于降水降尺度，并强调了感知真实感的重要性，这与传统上侧重于均方误差等客观指标的方法形成对比。该研究还提出了利用 WGAN 判别器进行数据质量控制的新颖视角，这对于处理气象数据中的潜在伪影具有重要意义。尽管在传统指标上性能略低，但其在视觉真实感和数据质量控制方面的潜力使其成为一个有价值的贡献。"}}
{"id": "2507.10062", "title": "LLMShot: Reducing snapshot testing maintenance via LLMs", "authors": ["Ergün Batuhan Kaynak", "Mayasah Lami", "Sahand Moslemi", "Anil Koyuncu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted to ICSME 2025", "url": "http://arxiv.org/abs/2507.10062v2", "summary": "Snapshot testing has emerged as a critical technique for UI validation in\nmodern software development, yet it suffers from substantial maintenance\noverhead due to frequent UI changes causing test failures that require manual\ninspection to distinguish between genuine regressions and intentional design\nchanges. This manual triage process becomes increasingly burdensome as\napplications evolve, creating a need for automated analysis solutions. This\npaper introduces LLMShot, a novel framework that leverages Vision-Language\nModels (VLMs) to automatically analyze snapshot test failures through semantic\nclassification of UI changes. To evaluate LLMShot's effectiveness, we developed\na comprehensive dataset using a feature-rich iOS application with configurable\nfeature flags, creating realistic scenarios that produce authentic snapshot\ndifferences representative of real development workflows. Our evaluation using\nGemma3 models demonstrates strong classification performance, with the 12B\nvariant achieving over 84% recall in identifying failure root causes while the\n4B model offers practical deployment advantages with acceptable performance for\ncontinuous integration environments. However, our exploration of selective\nignore mechanisms revealed significant limitations in current prompting-based\napproaches for controllable visual reasoning. LLMShot represents the first\nautomated approach to semantic snapshot test analysis, offering developers\nstructured insights that can substantially reduce manual triage effort and\nadvance toward more intelligent UI testing paradigms.", "comment": "Accepted to ICSME 2025", "pdf_url": "http://arxiv.org/pdf/2507.10062v2", "cate": "cs.SE", "date": "2025-07-14", "updated": "2025-07-24", "AI": {"title_translation": "LLMShot：通过大型语言模型减少快照测试维护", "tldr": "LLMShot是一个利用视觉-语言模型自动分析快照测试失败的框架，通过语义分类UI变化，显著减少了手动维护工作。", "motivation": "快照测试在现代软件开发中是UI验证的关键技术，但由于频繁的UI变化导致测试失败，需要手动检查以区分真正的回归和有意设计更改，这带来了巨大的维护开销。随着应用程序的发展，这种手动分类过程变得越来越繁重，因此需要自动化的分析解决方案。", "method": "本文引入了LLMShot，一个利用视觉-语言模型（VLMs）通过UI变化的语义分类来自动分析快照测试失败的新颖框架。为了评估LLMShot的有效性，开发了一个综合数据集，使用一个功能丰富的iOS应用程序，该应用程序具有可配置的功能标志，创建了产生真实快照差异的现实场景。", "result": "使用Gemma3模型的评估显示出强大的分类性能，其中12B变体在识别失败根本原因方面实现了超过84%的召回率，而4B模型则提供了实用的部署优势，并在持续集成环境中具有可接受的性能。然而，对选择性忽略机制的探索揭示了当前基于提示的方法在可控视觉推理方面的显著局限性。", "conclusion": "LLMShot代表了首个自动化语义快照测试分析方法，为开发人员提供了结构化洞察，可以大幅减少手动分类工作，并推动更智能的UI测试范式。", "translation": "快照测试已成为现代软件开发中UI验证的关键技术，但由于频繁的UI更改导致测试失败，从而需要手动检查以区分真正的回归和有意设计更改，因此它承受着巨大的维护开销。随着应用程序的发展，这种手动分类过程变得越来越繁重，从而产生了对自动化分析解决方案的需求。本文介绍了LLMShot，一个利用视觉-语言模型（VLMs）通过UI变化的语义分类来自动分析快照测试失败的新颖框架。为了评估LLMShot的有效性，我们使用一个功能丰富的iOS应用程序开发了一个综合数据集，该应用程序具有可配置的功能标志，创建了产生真实快照差异的现实场景，这些差异代表了真实的开发工作流程。我们使用Gemma3模型的评估显示出强大的分类性能，其中12B变体在识别失败根本原因方面实现了超过84%的召回率，而4B模型则提供了实用的部署优势，并在持续集成环境中具有可接受的性能。然而，我们对选择性忽略机制的探索揭示了当前基于提示的方法在可控视觉推理方面的显著局限性。LLMShot代表了首个自动化语义快照测试分析方法，为开发人员提供了结构化洞察，可以大幅减少手动分类工作，并推动更智能的UI测试范式。", "summary": "本文介绍了LLMShot，一个利用视觉-语言模型（VLMs）自动分析快照测试失败的创新框架。针对快照测试因UI频繁变化导致高维护开销的问题，LLMShot通过对UI变化的语义分类来区分真正的回归和有意设计更改。通过在真实iOS应用数据集上的Gemma3模型评估，LLMShot展示了强大的分类性能，尤其12B模型在识别失败原因方面召回率超过84%，4B模型适用于CI环境。尽管在可控视觉推理方面仍存在局限性，LLMShot是首个自动化语义快照测试分析方法，有望显著减少手动维护工作并推动UI测试智能化。", "keywords": "快照测试, 视觉-语言模型, UI验证, 维护, 语义分类", "comments": "LLMShot的创新之处在于首次将视觉-语言模型应用于快照测试的语义分析，解决了长期困扰开发人员的维护难题。其通过自动化分类UI变化，极大地提升了测试效率。尽管在可控视觉推理方面仍有局限，但其为智能UI测试开辟了新路径，具有重要的实践意义和研究价值。"}}
{"id": "2507.18519", "title": "Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning", "authors": ["Leiji Zhang", "Zeyu Wang", "Xin Li", "Yao-Hui Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18519v1", "summary": "Bisimulation metric has long been regarded as an effective control-related\nrepresentation learning technique in various reinforcement learning tasks.\nHowever, in this paper, we identify two main issues with the conventional\nbisimulation metric: 1) an inability to represent certain distinctive\nscenarios, and 2) a reliance on predefined weights for differences in rewards\nand subsequent states during recursive updates. We find that the first issue\narises from an imprecise definition of the reward gap, whereas the second issue\nstems from overlooking the varying importance of reward difference and\nnext-state distinctions across different training stages and task settings. To\naddress these issues, by introducing a measure for state-action pairs, we\npropose a revised bisimulation metric that features a more precise definition\nof reward gap and novel update operators with adaptive coefficient. We also\noffer theoretical guarantees of convergence for our proposed metric and its\nimproved representation distinctiveness. In addition to our rigorous\ntheoretical analysis, we conduct extensive experiments on two representative\nbenchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of\nour approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18519v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "重新审视强化学习中鲁棒表示的双模拟度量", "tldr": "本文修正了传统的双模拟度量，以解决其在表示独特场景和依赖预定义权重方面的局限性，提出了一个具有自适应系数的新度量，并提供了理论和实验验证。", "motivation": "传统的双模拟度量存在两个主要问题：1) 无法表示某些独特场景，这源于奖励差距定义不精确；2) 递归更新时依赖预定义的奖励和后续状态差异权重，这忽视了不同训练阶段和任务设置下这些差异的重要性。", "method": "通过引入状态-动作对的度量，提出了一个修正的双模拟度量，其特点是更精确的奖励差距定义和具有自适应系数的新颖更新算子。", "result": "提供了所提出度量的收敛性及其改进表示区分度的理论保证。在DeepMind Control和Meta-World两个代表性基准上进行了广泛实验，证明了该方法的有效性。", "conclusion": "通过提出一种修正的双模拟度量，该研究成功解决了传统方法的局限性，并在理论和实践上证明了其在强化学习中学习鲁棒且具有区分度表示的有效性。", "translation": "双模拟度量长期以来被认为是各种强化学习任务中一种有效的控制相关表示学习技术。然而，在本文中，我们发现了传统双模拟度量存在的两个主要问题：1）无法表示某些独特的场景，以及2）在递归更新过程中依赖于预定义的奖励和后续状态差异权重。我们发现第一个问题源于奖励差距的定义不精确，而第二个问题则源于忽视了奖励差异和下一状态区别在不同训练阶段和任务设置中的不同重要性。为了解决这些问题，通过引入状态-动作对的度量，我们提出了一种修正的双模拟度量，其特点是更精确的奖励差距定义和具有自适应系数的新颖更新算子。我们还为所提出的度量及其改进的表示区分度提供了收敛性的理论保证。除了严谨的理论分析外，我们还在DeepMind Control和Meta-World这两个代表性基准上进行了广泛实验，证明了我们方法的有效性。", "summary": "本文重新审视了强化学习中常用的双模拟度量，指出了其在表示独特场景和依赖预定义权重方面的不足。为解决这些问题，作者提出了一个修正的双模拟度量，通过引入状态-动作对的度量，提供了更精确的奖励差距定义和具有自适应系数的新颖更新算子。该方法得到了理论收敛性保证，并在DeepMind Control和Meta-World基准上通过实验验证了其有效性。", "keywords": "双模拟度量, 强化学习, 表示学习, 自适应系数, 鲁棒表示", "comments": "本文的创新点在于提出了一个修正的双模拟度量，通过引入状态-动作对的度量、更精确的奖励差距定义和自适应系数的更新算子，有效解决了传统双模拟度量在表示能力和权重预定义上的局限性。这对于强化学习中学习更鲁棒和更具区分度的状态表示具有重要意义。"}}
{"id": "2507.17953", "title": "Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search", "authors": ["Chang Eun Song", "Weihong Xu", "Keming Fan", "Soumil Jain", "Gopabandhu Hota", "Haichao Yang", "Leo Liu", "Kerem Akarvardar", "Meng-Fan Chang", "Carlos H. Diaz", "Gert Cauwenberghs", "Tajana Rosing", "Mingu Kang"], "categories": ["cs.AR", "cs.LG"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Published in 2025 Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), Kyoto, Japan, 2025", "url": "http://arxiv.org/abs/2507.17953v1", "summary": "Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging\ncontinual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing\n(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature\nextraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts\ngradient-free CL to efficiently update and store the learned knowledge in the\nform of class hypervectors. Its dual-mode operation enables bypassing costly\nfeature extraction for simpler datasets, while progressive search reduces\ncomplexity by up to 61% by encoding and comparing only partial query\nhypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),\nClo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL\naccelerators.", "comment": "Published in 2025 Symposium on VLSI Technology and Circuits (VLSI\n  Technology and Circuits), Kyoto, Japan, 2025", "pdf_url": "http://arxiv.org/pdf/2507.17953v1", "cate": "cs.AR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Clo-HDnn：一种4.66 TFLOPS/W和3.78 TOPS/W的持续设备端学习加速器，通过渐进式搜索实现能效高维计算", "tldr": "Clo-HDnn是一种能效高的设备端持续学习加速器，通过结合高维计算和渐进式搜索，在性能和效率上取得显著提升。", "motivation": "为新兴的持续学习（CL）任务设计高效的设备端学习（ODL）加速器。", "method": "Clo-HDnn集成了高维计算（HDC）、低成本Kronecker HD编码器和权重聚类特征提取（WCFE），以优化精度和效率。它采用无梯度持续学习（gradient-free CL）来更新和存储知识。其双模式操作允许对简单数据集绕过特征提取，并通过渐进式搜索仅编码和比较部分查询超向量来降低复杂性。", "result": "Clo-HDnn实现了4.66 TFLOPS/W（特征提取）和3.78 TOPS/W（分类器）的性能。与现有最先进的ODL加速器相比，能效分别提高了7.77倍和4.85倍。渐进式搜索将复杂性降低了高达61%。", "conclusion": "Clo-HDnn是一种高效且能效极高的设备端持续学习加速器，通过创新的高维计算和渐进式搜索方法，显著超越了现有技术水平。", "translation": "Clo-HDnn是一种为新兴的持续学习（CL）任务设计的设备端学习（ODL）加速器。Clo-HDnn集成了高维计算（HDC）以及低成本的Kronecker HD编码器和权重聚类特征提取（WCFE），以优化精度和效率。Clo-HDnn采用无梯度持续学习，以类超向量的形式高效更新和存储所学知识。其双模式操作使得对于更简单的数据集可以绕过昂贵的特征提取，而渐进式搜索通过仅编码和比较部分查询超向量，将复杂性降低高达61%。Clo-HDnn实现了4.66 TFLOPS/W（特征提取）和3.78 TOPS/W（分类器）的性能，与最先进的ODL加速器相比，能效分别提高了7.77倍和4.85倍。", "summary": "Clo-HDnn是一款专为持续学习设计的设备端学习加速器。它通过整合高维计算、Kronecker HD编码器和权重聚类特征提取来提升精度和效率。该加速器采用无梯度持续学习，并支持双模式操作以适应不同数据集。其创新的渐进式搜索功能可显著降低计算复杂性。实验结果显示，Clo-HDnn在能效方面远超现有最先进的设备端学习加速器。", "keywords": "持续学习, 设备端学习, 高维计算, 能量效率, 渐进式搜索", "comments": "这篇论文的创新点在于将高维计算与无梯度持续学习相结合，并引入了Kronecker HD编码器、权重聚类特征提取以及渐进式搜索等技术，极大地提升了设备端持续学习的能效。其双模式操作也增加了灵活性。对于边缘计算和物联网设备上的AI部署具有重要意义。"}}
{"id": "2502.00352", "title": "A Differentiated Reward Method for Reinforcement Learning based Multi-Vehicle Cooperative Decision-Making Algorithms", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "categories": ["cs.AI", "cs.MA", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures", "url": "http://arxiv.org/abs/2502.00352v2", "summary": "Reinforcement learning (RL) shows great potential for optimizing\nmulti-vehicle cooperative driving strategies through the state-action-reward\nfeedback loop, but it still faces challenges such as low sample efficiency.\nThis paper proposes a differentiated reward method based on steady-state\ntransition systems, which incorporates state transition gradient information\ninto the reward design by analyzing traffic flow characteristics, aiming to\noptimize action selection and policy learning in multi-vehicle cooperative\ndecision-making. The performance of the proposed method is validated in RL\nalgorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle\npenetration. The results show that the differentiated reward method\nsignificantly accelerates training convergence and outperforms centering reward\nand others in terms of traffic efficiency, safety, and action rationality.\nAdditionally, the method demonstrates strong scalability and environmental\nadaptability, providing a novel approach for multi-agent cooperative\ndecision-making in complex traffic scenarios.", "comment": "10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2502.00352v2", "cate": "cs.AI", "date": "2025-02-01", "updated": "2025-07-24", "AI": {"title_translation": "一种基于强化学习的多车辆协同决策差异化奖励方法", "tldr": "本文提出了一种差异化奖励方法，通过结合状态转移梯度信息，显著提高了多车辆协同决策强化学习算法的训练效率和性能。", "motivation": "强化学习在优化多车辆协同驾驶策略方面潜力巨大，但仍面临样本效率低等挑战。", "method": "本文提出了一种基于稳态转换系统的差异化奖励方法，通过分析交通流特性，将状态转换梯度信息融入奖励设计中，以优化多车辆协同决策中的动作选择和策略学习。该方法在MAPPO、MADQN和QMIX等强化学习算法中进行了验证。", "result": "结果表明，该差异化奖励方法显著加速了训练收敛，并在交通效率、安全性和动作合理性方面优于中心化奖励等其他方法。此外，该方法还表现出强大的可扩展性和环境适应性。", "conclusion": "该差异化奖励方法为复杂交通场景下的多智能体协同决策提供了一种新颖的途径，能够有效提高强化学习算法的性能。", "translation": "强化学习（RL）通过状态-动作-奖励反馈循环在优化多车辆协同驾驶策略方面显示出巨大潜力，但仍面临样本效率低等挑战。本文提出了一种基于稳态转换系统的差异化奖励方法，通过分析交通流特性，将状态转换梯度信息融入奖励设计中，旨在优化多车辆协同决策中的动作选择和策略学习。该方法的性能在不同自动驾驶汽车渗透率下的MAPPO、MADQN和QMIX等RL算法中得到了验证。结果表明，该差异化奖励方法显著加速了训练收敛，并在交通效率、安全性以及动作合理性方面优于中心化奖励和其他方法。此外，该方法还表现出强大的可扩展性和环境适应性，为复杂交通场景下的多智能体协同决策提供了一种新颖的方法。", "summary": "本文针对强化学习在多车辆协同决策中样本效率低的问题，提出了一种基于稳态转换系统的差异化奖励方法。该方法将状态转换梯度信息融入奖励设计，以优化动作选择和策略学习。实验结果表明，该方法显著加速了训练收敛，并在交通效率、安全性及动作合理性方面优于现有方法，同时展现出良好的可扩展性和环境适应性。", "keywords": "强化学习, 多车辆协同决策, 差异化奖励, 交通流, 样本效率", "comments": "本文提出的差异化奖励方法通过引入状态转移梯度信息，有效解决了多智能体强化学习中样本效率低的问题，提升了算法在复杂交通场景下的性能，具有创新性和实用价值。"}}
{"id": "2410.02540", "title": "$hp$-error analysis of mixed-order hybrid high-order methods for elliptic problems on simplicial meshes", "authors": ["Zhaonan Dong", "Alexandre Ern"], "categories": ["math.NA", "cs.NA", "65N15, 65N30"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.02540v2", "summary": "We present both $hp$-a priori and $hp$-a posteriori error analysis of a\nmixed-order hybrid high-order (HHO) method to approximate second-order elliptic\nproblems on simplicial meshes. Our main result on the $hp$-a priori error\nanalysis is a $\\frac12$-order $p$-suboptimal error estimate. This result is, to\nour knowledge, the first of this kind for hybrid nonconforming methods and\nmatches the state-of-the-art for other nonconforming methods (as discontinuous\nGalerkin methods) with general (mixed Dirichlet/Neumann) boundary conditions.\nOur second main result is a residual-based $hp$-a posteriori upper error bound,\ncomprising residual, normal flux jump, tangential jump, and stabilization\nestimators (plus data oscillation terms). The first three terms are $p$-optimal\nand only the latter is $\\frac12$-order $p$-suboptimal. This result is, to our\nknowledge, the first $hp$-a posteriori error estimate for HHO methods. A novel\napproach based on the partition-of-unity provided by hat basis functions and on\nlocal Helmholtz decompositions on vertex stars is devised to estimate the\nnonconforming error. Finally, we establish local lower error bounds.\nRemarkably, the normal flux jump estimator is only $\\frac12$-order\n$p$-suboptimal, as it can be bounded by the stabilization owing to the local\nconservation property of HHO methods. Numerical examples illustrate the theory.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.02540v2", "cate": "math.NA", "date": "2024-10-03", "updated": "2025-07-24", "AI": {"title_translation": "$hp$-误差分析：用于单形网格椭圆问题的混合阶混合高阶方法", "tldr": "本文对应用于椭圆问题的混合阶混合高阶（HHO）方法进行了$hp$-先验和$hp$-后验误差分析，包括新颖的误差估计和一种新的估计方法。", "motivation": "本文旨在为应用于椭圆问题的混合阶混合高阶（HHO）方法提供$hp$-误差分析（包括先验和后验），因为此类分析，特别是对于混合非协调方法和HHO方法，此前尚不明确或次优。", "method": "作者提出了一种混合阶混合高阶（HHO）方法。对于先验分析，他们推导出了一个$\frac12$阶$p$-次优误差估计。对于后验分析，他们使用了一个基于残差的误差上限，该上限包括残差、法向通量跳跃、切向跳跃和稳定化估计器。为了估计非协调误差，设计了一种基于帽子基函数提供的单位分解和顶点星上的局部Helmholtz分解的新颖方法。此外，还建立了局部误差下限。通过数值例子验证了理论。", "result": "1. $hp$-先验误差分析：获得了$\frac12$阶$p$-次优误差估计，这是混合非协调方法中的首次此类结果，并与现有其他非协调方法的最新水平相符。2. 基于残差的$hp$-后验误差上限：包括残差、法向通量跳跃、切向跳跃和稳定化估计器；前三项是$p$-最优的，最后一项是$\frac12$阶$p$-次优的。这是HHO方法中的首次$hp$-后验误差估计。3. 提出了一种基于单位分解和局部Helmholtz分解的新颖方法来估计非协调误差。4. 建立了局部误差下限。5. 法向通量跳跃估计器仅为$\frac12$阶$p$-次优，这得益于HHO方法的局部守恒性。", "conclusion": "本文成功地为混合阶HHO方法提供了全面的$hp$-误差分析，建立了新颖的先验和后验误差估计，并引入了一种新的非协调误差估计方法，数值例子支持了理论发现。", "translation": "我们对混合阶混合高阶 (HHO) 方法在单形网格上逼近二阶椭圆问题进行了 $hp$-先验和 $hp$-后验误差分析。我们关于 $hp$-先验误差分析的主要结果是一个 $\frac12$ 阶 $p$-次优误差估计。据我们所知，这是混合非协调方法中首次出现此类结果，并且与具有一般（混合Dirichlet/Neumann）边界条件的其他非协调方法（如不连续Galerkin方法）的最新水平相符。我们的第二个主要结果是基于残差的 $hp$-后验误差上限，包括残差、法向通量跳跃、切向跳跃和稳定化估计器（加上数据振荡项）。前三项是 $p$-最优的，只有最后一项是 $\frac12$ 阶 $p$-次优的。据我们所知，这是HHO方法中首次出现 $hp$-后验误差估计。为了估计非协调误差，我们设计了一种基于帽子基函数提供的单位分解和顶点星上的局部Helmholtz分解的新颖方法。最后，我们建立了局部误差下限。值得注意的是，法向通量跳跃估计器仅为 $\frac12$ 阶 $p$-次优，因为它可以通过HHO方法的局部守恒性被稳定化项所限制。数值例子说明了该理论。", "summary": "本文对在单形网格上求解二阶椭圆问题的混合阶混合高阶（HHO）方法进行了全面的$hp$-误差分析。研究建立了混合非协调方法中首个$\frac12$阶$p$-次优的$hp$-先验误差估计，以及HHO方法中首个基于残差的$hp$-后验误差上限（大部分项为$p$-最优）。此外，论文还引入了一种基于单位分解和局部Helmholtz分解的新颖方法来估计非协调误差，并推导了局部误差下限。数值例子验证了理论结果。", "keywords": "$hp$-误差分析, 混合高阶方法, 椭圆问题, 先验误差, 后验误差", "comments": "该论文通过为混合非协调方法提供首个$hp$-先验误差分析，以及为HHO方法提供首个$hp$-后验误差估计，填补了现有文献的空白，做出了重要贡献。其用于非协调误差估计的新颖方法也是一个创新点。这项工作提升了对HHO方法及其误差行为的理论理解。"}}
{"id": "2507.17774", "title": "Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems", "authors": ["Zhangqi Liu"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17774v1", "summary": "As artificial intelligence (AI) continues to evolve from a back-end\ncomputational tool into an interactive, generative collaborator, its\nintegration into early-stage design processes demands a rethinking of\ntraditional workflows in human-centered design. This paper explores the\nemergent paradigm of human-AI co-creation, where AI is not merely used for\nautomation or efficiency gains, but actively participates in ideation, visual\nconceptualization, and decision-making. Specifically, we investigate the use of\nlarge language models (LLMs) like GPT-4 and multimodal diffusion models such as\nStable Diffusion as creative agents that engage designers in iterative cycles\nof proposal, critique, and revision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17774v1", "cate": "cs.HC", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "人机协同创作：智能系统中协作设计的框架", "tldr": "本文探讨了人机协同创作的新范式，其中AI作为积极的创意伙伴参与设计过程，特别是利用大型语言模型和多模态扩散模型。", "motivation": "随着人工智能从后端计算工具演变为交互式、生成式协作者，其在早期设计过程中的整合需要重新思考以人为中心的设计中的传统工作流程。", "method": "本文探讨了人机协同创作的范式，其中AI积极参与构思、视觉概念化和决策。具体而言，研究调查了GPT-4等大型语言模型和Stable Diffusion等多模态扩散模型作为创意代理，如何让设计师参与到提案、批评和修改的迭代循环中。", "result": "摘要中未提及", "conclusion": "摘要中未提及", "translation": "随着人工智能（AI）不断从后端计算工具演变为交互式、生成式协作者，其在早期设计过程中的整合要求重新思考以人为中心设计中的传统工作流程。本文探讨了人机协同创作的新兴范式，其中AI不再仅仅用于自动化或提高效率，而是积极参与构思、视觉概念化和决策。具体而言，我们研究了GPT-4等大型语言模型和Stable Diffusion等多模态扩散模型作为创意代理，如何让设计师参与到提案、批评和修改的迭代循环中。", "summary": "本文提出并探讨了人机协同创作的范式，旨在将人工智能从单纯的自动化工具转变为设计过程中积极的、生成性的合作者。研究关注AI在早期设计阶段，如构思、视觉概念化和决策中的参与，并特别考察了大型语言模型（如GPT-4）和多模态扩散模型（如Stable Diffusion）作为创意代理，如何与设计师进行迭代的提案、批评和修改循环。", "keywords": "人机协同创作, 智能系统, 协作设计, 大型语言模型, 多模态扩散模型", "comments": "该论文提出了一种创新的人机协同创作框架，强调AI作为积极的创意伙伴参与设计全过程，而非仅仅是效率工具。这对于推动人机交互和智能系统在创意领域的应用具有重要意义。"}}
{"id": "2507.18082", "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound", "authors": ["Pascal Spiegler", "Taha Koleilat", "Arash Harirpoush", "Corey S. Miller", "Hassan Rivaz", "Marta Kersten-Oertel", "Yiming Xiao"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 Workshop CVAMD", "url": "http://arxiv.org/abs/2507.18082v1", "summary": "Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Our code will be publicly available upon acceptance.", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "pdf_url": "http://arxiv.org/pdf/2507.18082v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "TextSAM-EUS：基于文本提示学习的SAM模型在内镜超声中准确分割胰腺肿瘤", "tldr": "TextSAM-EUS是一种轻量级、文本驱动的SAM模型改进，用于在内镜超声图像中自动准确分割胰腺肿瘤，解决了传统方法对大量标注数据依赖和图像质量差的问题，且性能优于现有SOTA模型。", "motivation": "胰腺癌预后差，依赖内镜超声（EUS）进行活检和放疗。然而，EUS图像存在散斑噪声、低对比度和非直观外观等问题，导致全监督深度学习模型进行胰腺肿瘤分割时容易出错，并且高度依赖大量专家标注数据集。", "method": "本文提出了TextSAM-EUS，一种新颖、轻量级的文本驱动SAM模型改进，在推理时无需手动几何提示。该方法通过BiomedCLIP文本编码器结合LoRA-based的SAM架构改进，利用文本提示学习（上下文优化）实现EUS中胰腺肿瘤的自动分割，仅调整了总参数的0.86%。", "result": "在公共胰腺内镜超声数据库上，TextSAM-EUS在自动提示下达到了82.69%的Dice系数和85.28%的归一化表面距离（NSD），在手动几何提示下达到了83.10%的Dice系数和85.70%的NSD，性能优于现有的最先进（SOTA）全监督深度学习模型和基础模型（如SAM及其变体）。", "conclusion": "TextSAM-EUS是首次尝试将提示学习引入基于SAM的医学图像分割中，为高效、鲁棒的自动EUS分割提供了一个实用的选择。", "translation": "胰腺癌预后差，其诊断和治疗依赖于内镜超声（EUS）进行靶向活检和放疗。然而，EUS图像固有的散斑噪声、低对比度和非直观外观使得使用全监督深度学习（DL）模型进行胰腺肿瘤分割容易出错，并且高度依赖于大量由专家整理的标注数据集。为了解决这些挑战，我们提出了TextSAM-EUS，这是一种新颖、轻量级的文本驱动型Segment Anything Model（SAM）的改编版本，在推理时无需手动几何提示。我们的方法利用BiomedCLIP文本编码器进行文本提示学习（上下文优化），并结合基于LoRA的SAM架构改编，从而实现在EUS中自动胰腺肿瘤分割，仅调整了总参数的0.86%。在公共胰腺内镜超声数据库上，TextSAM-EUS在自动提示下达到了82.69%的Dice系数和85.28%的归一化表面距离（NSD），在手动几何提示下达到了83.10%的Dice系数和85.70%的NSD，性能优于现有的最先进（SOTA）监督深度学习模型和基础模型（例如SAM及其变体）。作为首次尝试将提示学习引入基于SAM的医学图像分割中，TextSAM-EUS为高效、鲁棒的自动EUS分割提供了一个实用的选择。我们的代码将在接受后公开。", "summary": "本文提出TextSAM-EUS，一种针对内镜超声（EUS）中胰腺肿瘤分割的轻量级、文本驱动的SAM模型改进。针对EUS图像质量差和传统方法依赖大量标注数据的挑战，TextSAM-EUS利用BiomedCLIP文本编码器进行文本提示学习，并结合LoRA对SAM架构进行适应性调整，仅需微调少量参数即可实现自动分割。实验结果表明，TextSAM-EUS在自动和手动提示下均优于现有SOTA模型和SAM变体，为医学图像分割提供了一种高效且鲁棒的解决方案。", "keywords": "胰腺肿瘤分割, 内镜超声, SAM模型, 文本提示学习, 医学图像分割", "comments": "该论文的创新点在于首次将文本提示学习引入SAM模型以实现医学图像（特别是EUS胰腺肿瘤）的分割，解决了传统方法对大量手动标注的依赖。其轻量级（仅调整0.86%参数）和无需手动几何提示的特性使其在临床应用中具有很高的实用价值和效率。这为医疗领域的AI辅助诊断提供了新的思路。"}}
{"id": "2507.18311", "title": "Improving Large Vision-Language Models' Understanding for Field Data", "authors": ["Xiaomei Zhang", "Hanyu Zheng", "Xiangyu Zhu", "Jinghuan Wei", "Junhong Zou", "Zhen Lei", "Zhaoxiang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18311v1", "summary": "Large Vision-Language Models (LVLMs) have shown impressive capabilities\nacross a range of tasks that integrate visual and textual understanding, such\nas image captioning and visual question answering. These models are trained on\nlarge-scale image and video datasets paired with text, enabling them to bridge\nvisual perception and natural language processing. However, their application\nto scientific domains, especially in interpreting complex field data commonly\nused in the natural sciences, remains underexplored. In this work, we introduce\nFieldLVLM, a novel framework designed to improve large vision-language models'\nunderstanding of field data. FieldLVLM consists of two main components: a\nfield-aware language generation strategy and a data-compressed multimodal model\ntuning. The field-aware language generation strategy leverages a\nspecial-purpose machine learning pipeline to extract key physical features from\nfield data, such as flow classification, Reynolds number, and vortex patterns.\nThis information is then converted into structured textual descriptions that\nserve as a dataset. The data-compressed multimodal model tuning focuses on\nLVLMs with these generated datasets, using a data compression strategy to\nreduce the complexity of field inputs and retain only the most informative\nvalues. This ensures compatibility with the models language decoder and guides\nits learning more effectively. Experimental results on newly proposed benchmark\ndatasets demonstrate that FieldLVLM significantly outperforms existing methods\nin tasks involving scientific field data. Our findings suggest that this\napproach opens up new possibilities for applying large vision-language models\nto scientific research, helping bridge the gap between large models and\ndomain-specific discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18311v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "提高大型视觉-语言模型对野外数据的理解", "tldr": "FieldLVLM是一个新框架，通过字段感知语言生成和数据压缩多模态模型微调，显著提高了大型视觉-语言模型在科学野外数据理解方面的能力。", "motivation": "大型视觉-语言模型（LVLMs）在集成视觉和文本理解的任务中表现出色，但其在科学领域，特别是解释自然科学中常用的复杂野外数据方面的应用仍未得到充分探索。", "method": "本文引入了FieldLVLM框架，包含两个主要组件：1. 字段感知语言生成策略：利用专用机器学习管道从野外数据中提取关键物理特征（如流分类、雷诺数、涡流模式），并将其转换为结构化文本描述作为数据集。2. 数据压缩多模态模型微调：使用数据压缩策略处理这些生成的数据集，以减少野外输入的复杂性并保留最具信息量的值，从而确保与模型语言解码器的兼容性并有效指导学习。", "result": "在最新提出的基准数据集上的实验结果表明，FieldLVLM在涉及科学野外数据的任务中显著优于现有方法。", "conclusion": "该方法为将大型视觉-语言模型应用于科学研究开辟了新的可能性，有助于弥合大型模型与领域特定发现之间的鸿沟。", "translation": "大型视觉-语言模型（LVLMs）在集成视觉和文本理解的一系列任务中展现了令人印象深刻的能力，例如图像标注和视觉问答。这些模型通过与文本配对的大规模图像和视频数据集进行训练，使其能够连接视觉感知和自然语言处理。然而，它们在科学领域的应用，特别是在解释自然科学中常用的复杂野外数据方面，仍未得到充分探索。在这项工作中，我们引入了FieldLVLM，这是一个旨在提高大型视觉-语言模型对野外数据理解的新颖框架。FieldLVLM由两个主要组件组成：一个字段感知语言生成策略和一个数据压缩多模态模型微调。字段感知语言生成策略利用一个专用机器学习管道从野外数据中提取关键物理特征，例如流分类、雷诺数和涡流模式。然后将这些信息转换为结构化文本描述，作为数据集。数据压缩多模态模型微调侧重于使用这些生成的数据集对LVLMs进行微调，采用数据压缩策略来降低野外输入的复杂性并仅保留最具信息量的值。这确保了与模型语言解码器的兼容性并更有效地指导其学习。在最新提出的基准数据集上的实验结果表明，FieldLVLM在涉及科学野外数据的任务中显著优于现有方法。我们的研究结果表明，这种方法为将大型视觉-语言模型应用于科学研究开辟了新的可能性，有助于弥合大型模型与领域特定发现之间的鸿沟。", "summary": "本研究提出FieldLVLM框架，旨在提升大型视觉-语言模型对科学野外数据的理解能力。该框架包含字段感知语言生成策略，用于从野外数据中提取物理特征并生成结构化文本描述；以及数据压缩多模态模型微调，以优化LVLMs对这些数据的学习。实验证明，FieldLVLM在处理科学野外数据任务时显著优于现有方法，为LVLMs在科学研究中的应用开辟了新途径。", "keywords": "大型视觉-语言模型, 野外数据, 科学研究, 数据压缩, 领域适应性", "comments": "这项工作通过提出FieldLVLM框架，有效解决了大型视觉-语言模型在理解复杂科学野外数据方面的局限性。其创新点在于结合了字段感知语言生成和数据压缩模型微调，这不仅提高了模型的领域适应性，也为LVLMs在自然科学研究中的实际应用提供了新的范式，具有重要的实践意义。"}}
{"id": "2507.18481", "title": "Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection", "authors": ["Francesco Dalmonte", "Emirhan Bayar", "Emre Akbas", "Mariana-Iuliana Georgescu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2507.18481v1", "summary": "Anomaly detection in medical images is an important yet challenging task due\nto the diversity of possible anomalies and the practical impossibility of\ncollecting comprehensively annotated data sets. In this work, we tackle\nunsupervised medical anomaly detection proposing a modernized autoencoder-based\nframework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained\nvision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead\nof training encoders from scratch, we directly utilize frozen vision foundation\nmodels as feature extractors, enabling rich, multi-stage, high-level\nrepresentations without domain-specific fine-tuning. We propose the usage of\nthe Q-Former architecture as the bottleneck, which enables the control of the\nlength of the reconstruction sequence, while efficiently aggregating multiscale\nfeatures. Additionally, we incorporate a perceptual loss computed using\nfeatures from a pretrained Masked Autoencoder, guiding the reconstruction\ntowards semantically meaningful structures. Our framework is evaluated on four\ndiverse medical anomaly detection benchmarks, achieving state-of-the-art\nresults on BraTS2021, RESC, and RSNA. Our results highlight the potential of\nvision foundation model encoders, pretrained on natural images, to generalize\neffectively to medical image analysis tasks without further fine-tuning. We\nrelease the code and models at https://github.com/emirhanbayar/QFAE.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2507.18481v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Q-Former自编码器：一种现代医学异常检测框架", "tldr": "提出了一种名为Q-Former自编码器的新型无监督医学异常检测框架，它利用预训练的视觉基础模型作为特征提取器，并在多个医学异常检测基准上取得了最先进的成果。", "motivation": "医学图像中的异常检测是一项重要但具有挑战性的任务，因为异常的多样性以及全面注释数据集在实践中几乎不可能收集到。", "method": "本研究提出了一种现代化的基于自编码器的框架——Q-Former自编码器，利用DINO、DINOv2和Masked Autoencoder等最先进的预训练视觉基础模型。该框架直接使用冻结的视觉基础模型作为特征提取器，无需领域特定的微调即可获得丰富、多阶段、高级的表示。研究引入Q-Former架构作为瓶颈，以控制重建序列的长度并有效地聚合多尺度特征。此外，还结合了使用预训练Masked Autoencoder特征计算的感知损失，以引导重建过程生成语义上有意义的结构。", "result": "该框架在四个不同的医学异常检测基准上进行了评估，并在BraTS2021、RESC和RSNA上取得了最先进的结果。结果突出表明，在自然图像上预训练的视觉基础模型编码器无需进一步微调即可有效泛化到医学图像分析任务中。", "conclusion": "预训练的视觉基础模型编码器具有无需进一步微调即可有效泛化到医学图像分析任务的潜力。", "translation": "医学图像中的异常检测是一项重要但具有挑战性的任务，因为可能出现的异常多种多样，而且实际中不可能收集到全面标注的数据集。在这项工作中，我们提出了一种现代化的基于自编码器的框架——Q-Former自编码器，解决了无监督医学异常检测问题，该框架利用了最先进的预训练视觉基础模型，如DINO、DINOv2和Masked Autoencoder。我们没有从头开始训练编码器，而是直接利用冻结的视觉基础模型作为特征提取器，从而在无需领域特定微调的情况下实现丰富、多阶段、高层次的表示。我们建议使用Q-Former架构作为瓶颈，这使得能够控制重建序列的长度，同时有效地聚合多尺度特征。此外，我们还引入了一种使用预训练Masked Autoencoder特征计算的感知损失，以引导重建过程生成语义上有意义的结构。我们的框架在四个不同的医学异常检测基准上进行了评估，并在BraTS2021、RESC和RSNA上取得了最先进的结果。我们的结果突出表明，在自然图像上预训练的视觉基础模型编码器无需进一步微调即可有效泛化到医学图像分析任务中。我们已在https://github.com/emirhanbayar/QFAE发布了代码和模型。", "summary": "本论文提出了一种名为Q-Former自编码器的新型无监督医学异常检测框架。该框架利用冻结的预训练视觉基础模型（如DINO、DINOv2、MAE）作为特征提取器，以避免从头训练并获取高层次表示。通过引入Q-Former作为瓶颈，实现了对重建序列长度的控制和多尺度特征的有效聚合。此外，结合了基于MAE特征的感知损失以指导语义重建。该方法在BraTS2021、RESC和RSNA等多个医学异常检测基准上取得了最先进的性能，证明了预训练视觉基础模型在无需微调的情况下对医学图像任务的泛化能力。", "keywords": "医学异常检测, Q-Former自编码器, 视觉基础模型, 无监督学习, 感知损失", "comments": "该论文的创新点在于将预训练的视觉基础模型与Q-Former架构相结合，用于无监督医学异常检测。其重要性体现在有效解决了医学图像数据标注困难的问题，并通过利用大规模预训练模型的通用特征提取能力，实现了在不进行领域特定微调的情况下达到SOTA性能，这对于医学图像分析领域具有重要意义。"}}
{"id": "2507.18047", "title": "FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics", "authors": ["Lucas Liebe", "Thanh-Tung Nguyen", "Dongman Lee"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      13 pages, 14 figures, 2 tables", "url": "http://arxiv.org/abs/2507.18047v1", "summary": "The growing complexity of Edge Video Analytics (EVA) facilitates new kind of\nintelligent applications, but creates challenges in real-time inference serving\nsystems. State-of-the-art (SOTA) scheduling systems optimize global workload\ndistributions for heterogeneous devices but often suffer from extended\nscheduling cycles, leading to sub-optimal processing in rapidly changing Edge\nenvironments. Local Reinforcement Learning (RL) enables quick adjustments\nbetween cycles but faces scalability, knowledge integration, and adaptability\nissues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated\nRL (FRL) to address these challenges. This integration dynamically adjusts\ninference batch sizes, input resolutions, and multi-threading during pre- and\npost-processing. CRL allows agents to learn from changing Markov Decision\nProcesses, capturing dynamic environmental variations, while FRL improves\ngeneralization and convergence speed by integrating experiences across\ninference models. FCPO combines these via an agent-specific aggregation scheme\nand a diversity-aware experience buffer. Experiments on a real-world EVA\ntestbed showed over 5 times improvement in effective throughput, 60% reduced\nlatency, and 20% faster convergence with up to 10 times less memory consumption\ncompared to SOTA RL-based approaches.", "comment": "13 pages, 14 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.18047v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "FCPO：面向实时高吞吐边缘视频分析的联邦持续策略优化", "tldr": "FCPO结合了联邦强化学习和持续强化学习，显著提升了边缘视频分析的吞吐量，降低了延迟和内存消耗。", "motivation": "边缘视频分析系统面临实时推理服务挑战，现有调度系统在快速变化的边缘环境中表现不佳，而本地强化学习存在可扩展性、知识整合和适应性问题。", "method": "提出FCPO框架，结合持续强化学习（CRL）和联邦强化学习（FRL）。FCPO动态调整推理批次大小、输入分辨率和多线程，CRL使智能体能从变化的马尔可夫决策过程中学习，FRL通过整合推理模型的经验提高泛化能力和收敛速度。通过智能体特定聚合方案和多样性感知经验缓冲区进行结合。", "result": "在真实边缘视频分析测试台上，有效吞吐量提高了5倍以上，延迟降低了60%，收敛速度加快了20%，内存消耗比现有强化学习方法减少了10倍。", "conclusion": "FCPO通过结合联邦强化学习和持续强化学习，有效解决了边缘视频分析的实时推理挑战，显著提升了性能。", "translation": "边缘视频分析（EVA）日益增长的复杂性促进了新型智能应用的发展，但也给实时推理服务系统带来了挑战。最先进的（SOTA）调度系统优化了异构设备的全局工作负载分配，但通常会遇到调度周期延长的问题，导致在快速变化的边缘环境中处理效果不佳。本地强化学习（RL）能够在周期之间快速调整，但面临可扩展性、知识集成和适应性问题。因此，我们提出了FCPO，它结合了持续强化学习（CRL）和联邦强化学习（FRL）来解决这些挑战。这种集成在预处理和后处理期间动态调整推理批次大小、输入分辨率和多线程。CRL允许智能体从变化的马尔可夫决策过程中学习，捕获动态环境变化，而FRL通过整合跨推理模型的经验来提高泛化能力和收敛速度。FCPO通过智能体特定的聚合方案和多样性感知经验缓冲区将这些结合起来。在真实世界的EVA测试台上进行的实验表明，与SOTA基于RL的方法相比，有效吞吐量提高了5倍以上，延迟降低了60%，收敛速度加快了20%，内存消耗减少了10倍。", "summary": "本研究提出FCPO框架，旨在解决边缘视频分析（EVA）中实时推理服务的挑战。现有调度系统和本地强化学习方法在快速变化的边缘环境中存在效率低下、可扩展性不足等问题。FCPO通过结合持续强化学习（CRL）和联邦强化学习（FRL），动态优化推理批次大小、分辨率和多线程，从而适应环境变化并加速学习。实验结果表明，FCPO在吞吐量、延迟、收敛速度和内存效率方面均显著优于现有SOTA强化学习方法。", "keywords": "联邦学习, 持续学习, 边缘视频分析, 策略优化, 强化学习", "comments": "FCPO的创新性在于将持续强化学习与联邦强化学习相结合，以应对边缘计算中动态环境和分布式学习的挑战。这种结合不仅提高了实时性能，还解决了传统强化学习在边缘部署中的可扩展性和适应性问题，对于未来边缘智能应用具有重要意义。其在实际测试台上的显著性能提升证明了该方法的有效性和实用性。"}}
{"id": "2507.18160", "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks", "authors": ["Luka Šiktar", "Branimir Ćaran", "Bojan Šekoranja", "Marko Švaco"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      The paper is accepted and presented on the 34th International Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade Serbia", "url": "http://arxiv.org/abs/2507.18160v1", "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.", "comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia", "pdf_url": "http://arxiv.org/pdf/2507.18160v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "结合计算机视觉和卷积神经网络的自主无人机搜救导航", "tldr": "本文提出一个基于无人机、计算机视觉和CNN的搜救子系统，实现人员检测、人脸识别和跟踪，并结合自主导航功能。", "motivation": "在搜救任务中，需要一个能够进行人员检测、人脸识别和跟踪的自主系统，以提高搜救效率和安全性。", "method": "该方案将无人机与ROS2框架集成，利用YOLOv11和YOLOv11-pose CNN进行跟踪，dlib库CNN进行人脸识别。通过系统辨识和PD控制器实现自主无人机导航。跟踪过程依赖于YOLOv11-pose识别的人体关键点，并利用IMU测量数据进行系统辨识以设计PD控制器，估算无人机与目标的距离。系统支持操作员手动定位和录入未知人员信息。", "result": "在对14名已知人员进行的初步实验中，该子系统被证明可以成功实时使用。", "conclusion": "该无人机搜救子系统能够实时有效地进行人员检测、人脸识别和跟踪，未来计划将其部署到大型实验无人机上，并集成GPS引导控制以用于救援行动规划。", "translation": "在本文中，我们提出了一个使用无人机（UAV）进行搜救任务的子系统，重点关注人员检测、人脸识别和已识别个体的跟踪。所提出的解决方案将无人机与ROS2框架集成，该框架利用多个卷积神经网络（CNN）进行搜索任务。为实现自主无人机导航，进行了系统辨识和PD控制器部署。ROS2环境利用YOLOv11和YOLOv11-pose CNN进行跟踪，并利用dlib库CNN进行人脸识别。系统检测到特定个体后，执行人脸识别并开始跟踪。如果该个体尚未被识别，无人机操作员可以手动定位该人员，保存其面部图像并立即启动跟踪过程。跟踪过程依赖于使用YOLOv11-pose CNN模型识别人体上的特定关键点。这些关键点用于跟踪特定个体并保持安全距离。为了提高跟踪的准确性，根据无人机IMU的测量数据进行系统辨识。识别出的系统参数用于设计PD控制器，该控制器利用YOLOv11-pose估计无人机摄像头与已识别个体之间的距离。对14名已知个体进行的初步实验表明，所提出的子系统可以成功实时使用。下一步包括在大型实验无人机上实现该系统以进行现场使用，并将自主导航与GPS引导控制集成，用于救援行动规划。", "summary": "本文提出了一种用于搜救任务的无人机子系统，该系统集成了ROS2框架、多种卷积神经网络（如YOLOv11、YOLOv11-pose和dlib CNN）以实现人员检测、人脸识别和跟踪。通过系统辨识和PD控制器实现了自主导航，并利用YOLOv11-pose识别的人体关键点进行精确跟踪和距离估计。初步实验结果表明，该系统能够成功地进行实时操作，为未来的实地部署和与GPS集成奠定了基础。", "keywords": "无人机, 搜救, 计算机视觉, 卷积神经网络, 自主导航", "comments": "该论文提出了一种创新的无人机搜救方案，将先进的计算机视觉技术（如多种CNN模型）与自主导航控制相结合，实现了实时的人员检测、识别和跟踪。其亮点在于对未知人员的手动录入功能以及利用IMU数据进行系统辨识以优化跟踪精度。该系统在搜救领域具有重要的实际应用价值。然而，初步实验规模较小，仅在已知人员上进行，未来需要在大规模、复杂和动态的真实搜救环境中进行更全面的验证。"}}
{"id": "2507.09682", "title": "OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization", "authors": ["Laura Baird", "Armin Moin"], "categories": ["cs.SE", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Quantum Computing and Engineering (QCE) 2025 - Extended Abstract", "url": "http://arxiv.org/abs/2507.09682v2", "summary": "We propose a novel approach, OrQstrator, which is a modular framework for\nconducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum\n(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our\norchestration engine intelligently selects among three complementary circuit\noptimizers: A DRL-based circuit rewriter trained to reduce depth and gate count\nvia learned rewrite sequences; a domain-specific optimizer that performs\nefficient local gate resynthesis and numeric optimization; a parameterized\ncircuit instantiator that improves compilation by optimizing template circuits\nduring gate set translation. These modules are coordinated by a central\norchestration engine that learns coordination policies based on circuit\nstructure, hardware constraints, and backend-aware performance features such as\ngate count, depth, and expected fidelity. The system outputs an optimized\ncircuit for hardware-aware transpilation and execution, leveraging techniques\nfrom an existing state-of-the-art approach, called the NISQ Analyzer, to adapt\nto backend constraints.", "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "pdf_url": "http://arxiv.org/pdf/2507.09682v2", "cate": "cs.SE", "date": "2025-07-13", "updated": "2025-07-24", "AI": {"title_translation": "OrQstrator：一个由AI驱动的先进量子电路优化框架", "tldr": "OrQstrator是一个由深度强化学习（DRL）驱动的模块化框架，旨在通过智能协调多个优化器来优化NISQ时代的量子电路。", "motivation": "在噪声中等规模量子（NISQ）时代，需要一种先进的方法来优化量子电路，以减少电路深度和门数，并提高编译效率和保真度。", "method": "该论文提出了OrQstrator框架，它是一个由深度强化学习（DRL）驱动的模块化系统。其核心是一个编排引擎，该引擎智能地协调三个互补的电路优化器：1) 基于DRL的电路重写器，通过学习的重写序列减少深度和门数；2) 领域特定的优化器，执行高效的局部门重合成和数值优化；3) 参数化电路实例化器，通过优化模板电路来改进编译。编排引擎根据电路结构、硬件约束和后端感知性能特征（如门数、深度和预期保真度）学习协调策略。该系统还利用NISQ Analyzer的技术来适应后端约束。", "result": "该系统输出一个针对硬件感知转译和执行的优化电路。它旨在减少电路深度和门数，通过优化模板电路来改进编译，并适应后端约束。", "conclusion": "OrQstrator提供了一个由AI驱动的模块化框架，用于在NISQ时代进行先进的量子电路优化，通过智能协调多种优化技术来提高电路性能和硬件适应性。", "translation": "我们提出了一种新颖的方法，OrQstrator，它是一个用于在噪声中等规模量子（NISQ）时代进行量子电路优化的模块化框架。我们的框架由深度强化学习（DRL）提供支持。我们的编排引擎智能地从三个互补的电路优化器中进行选择：一个基于DRL的电路重写器，通过学习的重写序列来减少深度和门数；一个执行高效局部门重合成和数值优化的领域特定优化器；一个参数化电路实例化器，通过在门集转换期间优化模板电路来改进编译。这些模块由一个中央编排引擎协调，该引擎根据电路结构、硬件约束和后端感知性能特征（如门数、深度和预期保真度）学习协调策略。该系统输出一个用于硬件感知转译和执行的优化电路，利用现有最先进方法NISQ Analyzer 的技术来适应后端约束。", "summary": "OrQstrator是一个新颖的、由AI驱动（使用DRL）的模块化框架，旨在优化NISQ时代的量子电路。它设有一个中央编排引擎，该引擎智能地协调三个专用优化器：一个基于DRL的电路重写器、一个领域特定优化器和一个参数化电路实例化器。该框架学习根据电路特性和硬件约束调整优化策略，旨在为硬件感知转译和执行生成优化电路，并利用现有的最先进技术。", "keywords": "量子电路优化, 深度强化学习, NISQ, AI框架, 编排", "comments": "该论文提出了一种创新的AI驱动方法，解决了量子计算中一个关键挑战——NISQ时代的电路优化问题。其模块化设计，结合由DRL驱动的编排引擎智能地组合多种优化技术，是一个显著的优点。对硬件约束和后端感知特征的考虑对于实际应用至关重要。"}}
{"id": "2504.20991", "title": "Quantum Hypothesis Testing Lemma for Deterministic Identification over Quantum Channels", "authors": ["Pau Colomer", "Christian Deppe", "Holger Boche", "Andreas Winter"], "categories": ["cs.IT", "math.IT", "quant-ph"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      7 pages, double column", "url": "http://arxiv.org/abs/2504.20991v2", "summary": "In our previous work, we presented the \\emph{Hypothesis Testing Lemma}, a key\ntool that establishes sufficient conditions for the existence of good\ndeterministic identification (DI) codes for memoryless channels with finite\noutput, but arbitrary input alphabets. In this work, we provide a full quantum\nanalogue of this lemma, which shows that the existence of a DI code in the\nquantum setting follows from a suitable packing in a modified space of output\nquantum states. Specifically, we demonstrate that such a code can be\nconstructed using product states derived from this packing. This result enables\nus to tighten the capacity lower bound for DI over quantum channels beyond the\nsimultaneous decoding approach. In particular, we can now express these bounds\nsolely in terms of the Minkowski dimension of a certain state space, giving us\nnew insights to better understand the nature of the protocol, and the\nseparation between simultaneous and non-simultaneous codes. We extend the\ndiscussion with a particular channel example for which we can construct an\noptimum code.", "comment": "7 pages, double column", "pdf_url": "http://arxiv.org/pdf/2504.20991v2", "cate": "cs.IT", "date": "2025-04-29", "updated": "2025-07-24", "AI": {"title_translation": "量子信道上确定性识别的量子假设检验引理", "tldr": "本文提出了一个量子假设检验引理，用于证明量子信道上确定性识别码的存在性，并以此收紧了容量下界。", "motivation": "作者在之前的研究中提出了一个假设检验引理，用于确定性识别码在经典信道上的存在性。这项工作旨在提供该引理的完整量子模拟，以解决量子信道上的确定性识别问题。", "method": "本文通过在输出量子态的修改空间中进行适当的填充，并使用由此填充导出的乘积态来构建确定性识别码，从而证明了量子设置下确定性识别码的存在性。", "result": "该结果使得量子信道上确定性识别的容量下界比同步解码方法更紧密。特别是，这些界限现在可以完全用某个状态空间的闵可夫斯基维数来表示，从而提供了新的见解，以更好地理解协议的性质，以及同步和非同步码之间的分离。文章还扩展讨论了一个可以构建最优码的特定信道示例。", "conclusion": "量子假设检验引理的引入，不仅证明了量子信道上确定性识别码的存在性，而且显著提升了对容量下界的理解和表达方式，揭示了新的理论洞察。", "translation": "在我们之前的工作中，我们提出了“假设检验引理”，这是一个关键工具，为具有有限输出但任意输入字母表的无记忆信道建立良好确定性识别（DI）码存在的充分条件。在这项工作中，我们提供了该引理的完整量子模拟，这表明量子设置中DI码的存在性源于输出量子态的修改空间中的适当填充。具体来说，我们证明了可以使用从这种填充派生的乘积态来构造这样的码。这一结果使我们能够收紧量子信道上DI的容量下界，超越了同步解码方法。特别是，我们现在可以仅用某个状态空间的闵可夫斯基维数来表达这些界限，这为我们提供了新的见解，以更好地理解协议的性质，以及同步和非同步码之间的分离。我们通过一个可以构造最优码的特定信道示例扩展了讨论。", "summary": "本文提出了一个量子假设检验引理，作为先前经典引理的量子模拟。该引理证明了在量子信道上确定性识别（DI）码的存在性，通过在修改后的输出量子态空间中进行填充并构建乘积态实现。这项工作的重要成果是收紧了量子DI的容量下界，并首次将其完全用闵可夫斯基维数表示，从而深化了对DI协议及其编码方式的理解。", "keywords": "量子假设检验引理, 确定性识别, 量子信道, 容量下界, 闵可夫斯基维数", "comments": "本文的关键创新在于将经典的假设检验引理推广到量子领域，为量子信道上的确定性识别提供了理论基础。通过引入“填充”和“乘积态”的概念，它不仅证明了量子DI码的存在性，还显著改进了容量下界的刻画，提供了新的理论工具和见解，尤其是在理解同步与非同步码的区别方面。"}}
{"id": "2507.18309", "title": "Maneuvering-based Dynamic Thrust Allocation for Fully-Actuated Vessels", "authors": ["Emir Cem Gezer", "Roger Skjetne"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18309v1", "summary": "This paper introduces a new approach to solving the thrust allocation problem\nusing the maneuvering problem in the maritime domain for fully actuated\nvessels. The method uses a control Lyapunov function to create a nonlinear\nreference filter for the thruster forces. The filter ensures dynamic tracking\nof the optimal thrust allocation solution with rate limitation in the output\nthruster references. It further uses control barrier functions to ensure that\nthe thruster force saturation limits are respected. The approach aims for\nsimplicity and effectiveness, as well as smooth and dynamic thruster reference\nsignals, in the implementation of thrust allocation for marine vessels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18309v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "全驱动船舶基于机动的动态推力分配", "tldr": "本文提出了一种新的动态推力分配方法，利用控制Lyapunov函数和控制障碍函数来解决全驱动船舶的推力分配问题，确保平滑、动态且受限的推力参考信号。", "motivation": "针对全驱动船舶的推力分配问题，提出一种新的解决方法，旨在实现简单、有效且平滑动态的推力参考信号。", "method": "该方法利用控制Lyapunov函数为推力创建非线性参考滤波器，并使用控制障碍函数确保推力饱和限制得到尊重。", "result": "该滤波器确保了对最佳推力分配解决方案的动态跟踪，并限制了输出推力参考的速率，同时确保推力力饱和限制得到遵守。", "conclusion": "该方法在海洋船舶推力分配的实现中，实现了简单、有效、平滑和动态的推力参考信号。", "translation": "本文介绍了一种解决全驱动船舶推力分配问题的新方法，该方法利用海事领域的机动问题。该方法使用控制Lyapunov函数为推进器力创建一个非线性参考滤波器。该滤波器确保了对最佳推力分配解决方案的动态跟踪，并对输出推进器参考进行了速率限制。它进一步使用控制障碍函数来确保推进器力饱和限制得到遵守。该方法旨在船舶推力分配的实施中实现简单性、有效性以及平滑和动态的推进器参考信号。", "summary": "本文提出了一种针对全驱动船舶的动态推力分配新方法。该方法结合使用控制Lyapunov函数构建非线性参考滤波器以实现对最佳推力分配的动态跟踪和速率限制，并利用控制障碍函数确保推力饱和限制。该方法旨在提供一种简单、有效且能产生平滑动态推力参考信号的解决方案。", "keywords": "推力分配, 全驱动船舶, 控制Lyapunov函数, 控制障碍函数, 机动", "comments": "这篇论文的创新点在于将控制Lyapunov函数和控制障碍函数结合应用于船舶推力分配问题，以确保动态跟踪、速率限制和饱和限制。这种方法有望提高船舶推力分配的鲁棒性和实际应用性。"}}
{"id": "2507.15292", "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Control", "authors": ["An Wang", "Rulin Zhou", "Mengya Xu", "Yiru Ye", "Longfei Gou", "Yiting Chang", "Hao Chen", "Chwee Ming Lim", "Jiankun Wang", "Hongliang Ren"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15292v4", "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15292v4", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "EndoControlMag：一种通过周期性参考重置和分层组织感知双掩模控制实现内窥镜血管运动鲁棒放大的方法", "tldr": "EndoControlMag是一个训练无关的内窥镜血管运动放大框架，通过周期性参考重置和分层组织感知双掩模控制，在复杂手术场景下实现血管运动的鲁棒和精确可视化。", "motivation": "在内窥镜手术中，血管的细微运动可视化对于手术精度和决策至关重要。然而，由于手术场景的复杂性和动态性，这仍然是一个挑战。", "method": "本文提出了EndoControlMag，一个无需训练、基于拉格朗日的框架，用于内窥镜环境下的掩模条件血管运动放大。它包含两个关键模块：周期性参考重置（PRR）方案，将视频分割成短的重叠片段，动态更新参考帧以防止误差累积并保持时间连贯性；以及分层组织感知放大（HTM）框架，采用双模态掩模膨胀。HTM首先使用预训练的视觉跟踪模型跟踪血管核心，然后对周围组织应用两种自适应软化策略：基于运动的软化或基于距离的指数衰减，以适应不同手术场景。", "result": "EndoControlMag在EndoVMM24数据集上进行了评估，该数据集涵盖四种不同手术类型和各种挑战性场景。定量指标、视觉评估和外科专家评估表明，EndoControlMag在放大精度和视觉质量方面显著优于现有方法，同时在挑战性手术条件下保持鲁棒性。", "conclusion": "EndoControlMag通过其创新的周期性参考重置和分层组织感知双掩模控制，在复杂多变的手术环境中实现了血管运动的精确、鲁棒放大，显著提升了现有方法的性能。", "translation": "在内窥镜手术中，血管的细微运动可视化对于手术精度和决策至关重要，然而由于手术场景的复杂性和动态性，这仍然是一个挑战。为解决此问题，我们引入了EndoControlMag，这是一个无需训练、基于拉格朗日的框架，具有针对内窥镜环境定制的掩模条件血管运动放大功能。我们的方法包含两个关键模块：一个周期性参考重置（PRR）方案，它将视频分割成短的重叠片段，并动态更新参考帧，以防止误差累积同时保持时间连贯性；以及一个分层组织感知放大（HTM）框架，具有双模态掩模膨胀。HTM首先使用预训练的视觉跟踪模型跟踪血管核心，即使在遮挡和视角变化下也能保持准确的定位。然后，它对周围组织应用两种自适应软化策略之一：基于运动的软化，根据观察到的组织位移按比例调节放大强度；或基于距离的指数衰减，模拟生物力学力衰减。这种双模态方法适应不同的手术场景——基于运动的软化在复杂组织变形中表现出色，而基于距离的软化在不可靠的光流条件下提供稳定性。我们在EndoVMM24数据集上评估了EndoControlMag，该数据集涵盖四种不同手术类型和各种挑战性场景，包括遮挡、器械干扰、视角变化和血管变形。定量指标、视觉评估和外科专家评估表明，EndoControlMag在放大精度和视觉质量方面显著优于现有方法，同时在挑战性手术条件下保持鲁棒性。代码、数据集和视频结果可在https://szupc.github.io/EndoControlMag/获得。", "summary": "本文提出了EndoControlMag，一个无需训练、基于拉格朗日的内窥镜血管运动放大框架。该框架通过周期性参考重置（PRR）方案解决误差累积问题，并通过分层组织感知放大（HTM）框架实现对血管核心的精确跟踪及周围组织的自适应软化。HTM采用双模态掩模膨胀，包括基于运动和基于距离的软化策略，以适应不同手术场景。实验结果表明，EndoControlMag在放大精度、视觉质量和鲁棒性方面均显著优于现有方法。", "keywords": "内窥镜血管运动放大, 周期性参考重置, 分层组织感知, 双掩模控制, 手术精度", "comments": "EndoControlMag的创新之处在于其“无需训练”的拉格朗日框架，以及提出的两个核心模块：周期性参考重置（PRR）和分层组织感知放大（HTM）中的双模态掩模膨胀策略。PRR有效解决了传统运动放大中常见的误差累积问题，而HTM的双模态软化策略则灵活适应了复杂多变的手术环境，提高了放大精度和视觉质量。该方法在实际手术场景中的鲁棒性和优越性能，使其在提升内窥镜手术精度方面具有重要应用潜力。"}}
{"id": "2411.16354", "title": "Scalable Parameter Design for Superconducting Quantum Circuits with Graph Neural Networks", "authors": ["Hao Ai", "Yu-xi Liu"], "categories": ["quant-ph", "cs.AI"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16354v3", "summary": "To demonstrate supremacy of quantum computing, increasingly large-scale\nsuperconducting quantum computing chips are being designed and fabricated.\nHowever, the complexity of simulating quantum systems poses a significant\nchallenge to computer-aided design of quantum chips, especially for large-scale\nchips. Harnessing the scalability of graph neural networks (GNNs), we here\npropose a parameter designing algorithm for large-scale superconducting quantum\ncircuits. The algorithm depends on the so-called 'three-stair scaling'\nmechanism, which comprises two neural-network models: an evaluator supervisedly\ntrained on small-scale circuits for applying to medium-scale circuits, and a\ndesigner unsupervisedly trained on medium-scale circuits for applying to\nlarge-scale ones. We demonstrate our algorithm in mitigating quantum crosstalk\nerrors. Frequencies for both single- and two-qubit gates (corresponding to the\nparameters of nodes and edges) are considered simultaneously. Numerical results\nindicate that the well-trained designer achieves notable advantages in\nefficiency, effectiveness, and scalability. For example, for large-scale\nsuperconducting quantum circuits consisting of around 870 qubits, our\nGNNs-based algorithm achieves 51% of the errors produced by the\nstate-of-the-art algorithm, with a time reduction from 90 min to 27 sec.\nOverall, a better-performing and more scalable algorithm for designing\nparameters of superconducting quantum chips is proposed, which initially\ndemonstrates the advantages of applying GNNs in superconducting quantum chips.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16354v3", "cate": "quant-ph", "date": "2024-11-25", "updated": "2025-07-24", "AI": {"title_translation": "基于图神经网络的超导量子电路可扩展参数设计", "tldr": "提出了一种利用图神经网络（GNNs）为大型超导量子电路设计参数的算法，显著提高了效率、有效性和可扩展性，并在减轻量子串扰误差方面表现出色。", "motivation": "设计和制造大规模超导量子计算芯片面临模拟量子系统复杂性带来的巨大挑战，特别是在计算机辅助设计方面。", "method": "提出了基于图神经网络（GNNs）的参数设计算法，该算法依赖于“三阶缩放”机制，包含两个神经网络模型：一个在小规模电路上进行监督训练用于中规模电路的评估器，以及一个在中规模电路上进行无监督训练用于大规模电路的设计器。同时考虑了单量子位和双量子位门的频率（对应于节点和边的参数）。", "result": "对于包含约870个量子位的大规模超导量子电路，该GNNs算法将错误率降低到现有最新算法的51%，时间从90分钟减少到27秒，在效率、有效性和可扩展性方面表现出显著优势。", "conclusion": "本文提出了一种性能更好、可扩展性更强的超导量子芯片参数设计算法，初步展示了将GNNs应用于超导量子芯片的优势。", "translation": "为了证明量子计算的优越性，人们正在设计和制造越来越大规模的超导量子计算芯片。然而，模拟量子系统的复杂性对量子芯片的计算机辅助设计构成了重大挑战，特别是对于大规模芯片。我们利用图神经网络（GNNs）的可扩展性，提出了一种用于大规模超导量子电路的参数设计算法。该算法依赖于所谓的“三阶缩放”机制，其包括两个神经网络模型：一个在小规模电路上进行监督训练并应用于中规模电路的评估器，以及一个在中规模电路上进行无监督训练并应用于大规模电路的设计器。我们展示了该算法在减轻量子串扰误差方面的应用。同时考虑了单量子位和双量子位门的频率（对应于节点和边的参数）。数值结果表明，训练有素的设计器在效率、有效性和可扩展性方面取得了显著优势。例如，对于包含约870个量子位的大规模超导量子电路，我们的基于GNNs的算法实现了最新算法51%的错误率，并将时间从90分钟减少到27秒。总的来说，本文提出了一种性能更好、可扩展性更强的超导量子芯片参数设计算法，初步展示了将GNNs应用于超导量子芯片的优势。", "summary": "该论文提出了一种利用图神经网络（GNNs）的可扩展性，为大规模超导量子电路设计参数的算法。该算法采用“三阶缩放”机制，包含一个用于评估的监督训练神经网络和一个用于设计的无监督训练神经网络。研究展示了该算法在减轻量子串扰误差方面的应用，并同时考虑了单量子位和双量子位门的频率。数值结果表明，该GNNs算法在处理大规模电路时，相比现有技术显著提高了效率、有效性和可扩展性，例如将870个量子位电路的错误率降低至51%，并将计算时间大幅缩短。", "keywords": "图神经网络, 超导量子电路, 参数设计, 可扩展性, 量子串扰", "comments": "该论文的创新点在于首次将图神经网络应用于超导量子芯片的参数设计，并提出了一种独特的三阶缩放机制来解决大规模量子芯片设计的挑战。其在效率和可扩展性上的显著提升，对于推动量子计算芯片的实际应用具有重要意义。"}}
{"id": "2507.17988", "title": "Synthesis of timeline-based planning strategies avoiding determinization", "authors": ["Dario Della Monica", "Angelo Montanari", "Pietro Sala"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2410.22757", "url": "http://arxiv.org/abs/2507.17988v1", "summary": "Qualitative timeline-based planning models domains as sets of independent,\nbut\n  interacting, components whose behaviors over time, the timelines, are\ngoverned\n  by sets of qualitative temporal constraints (ordering relations), called\n  synchronization rules.\n  Its plan-existence problem has been shown to be PSPACE-complete; in\n  particular, PSPACE-membership has been proved via reduction to the\n  nonemptiness problem for nondeterministic finite automata.\n  However, nondeterministic automata cannot be directly used to synthesize\n  planning strategies as a costly determinization step is needed.\n  In this paper, we identify a fragment of qualitative timeline-based planning\n  whose plan-existence problem can be directly mapped into the nonemptiness\n  problem of deterministic finite automata, which can then\n  synthesize strategies.\n  In addition, we identify a maximal subset of Allen's relations that fits into\n  such a deterministic fragment.", "comment": "arXiv admin note: text overlap with arXiv:2410.22757", "pdf_url": "http://arxiv.org/pdf/2507.17988v1", "cate": "cs.AI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "合成避免确定化的基于时间线的规划策略", "tldr": "本文识别了一种定性时间线规划的片段，可以直接映射到确定性有限自动机，从而避免了昂贵的确定化步骤来合成规划策略。", "motivation": "现有的非确定性自动机无法直接用于合成规划策略，因为需要一个昂贵的确定化步骤。", "method": "作者识别了定性时间线规划的一个片段，其计划存在问题可以直接映射到确定性有限自动机的非空性问题。", "result": "识别了一个可以直接映射到确定性有限自动机并能合成策略的定性时间线规划片段，并确定了适合此确定性片段的Allen关系的最大子集。", "conclusion": "通过识别一个特定的规划片段，可以避免昂贵的确定化步骤，从而直接合成基于时间线的规划策略。", "translation": "定性时间线规划模型将领域建模为一组独立但相互作用的组件，这些组件随时间变化的行 为（即时间线）由一组定性时间约束（排序关系），称为同步规则，所控制。\n其计划存在问题已被证明是 PSPACE 完全的；特别是，PSPACE 成员资格已通过归约到非确定性有限自动机的非空性问题得到证明。\n然而，非确定性自动机不能直接用于合成规划策略，因为需要一个昂贵的确定化步骤。\n在本文中，我们识别了定性时间线规划的一个片段，其计划存在问题可以直接映射到确定性有限自动机的非空性问题，然后可以合成策略。\n此外，我们还识别了适合此类确定性片段的 Allen 关系的最大子集。", "summary": "本文关注定性时间线规划中规划策略的合成问题。针对现有方法需要昂贵的确定化步骤才能从非确定性自动机合成策略的局限性，作者识别了定性时间线规划的一个特定片段。该片段的计划存在问题可以直接映射到确定性有限自动机的非空性问题，从而可以直接合成规划策略，避免了确定化过程。研究还确定了适合此确定性片段的 Allen 关系的最大子集。", "keywords": "时间线规划, 规划策略合成, 确定性有限自动机, Allen关系, 确定化避免", "comments": "本文的创新点在于识别了定性时间线规划的一个特定子集，使得规划策略的合成可以直接通过确定性有限自动机进行，从而避免了传统方法中昂贵的确定化步骤。这对于提高规划策略合成的效率和实用性具有重要意义。"}}
{"id": "2507.18062", "title": "An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows", "authors": ["Edward Abrokwah", "Taher A. Ghaleb"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Registered Report Accepted at the 41st IEEE International Conference on Software Maintenance and Evolution 2025 (ICSME'25)", "url": "http://arxiv.org/abs/2507.18062v1", "summary": "Continuous Integration (CI) has evolved from a tooling strategy to a\nfundamental mindset in modern CI engineering. It enables teams to develop,\ntest, and deliver software rapidly and collaboratively. Among CI services,\nGitHub Actions (GHA) has emerged as a dominant service due to its deep\nintegration with GitHub and a vast ecosystem of reusable workflow actions.\nAlthough GHA provides official documentation and community-supported best\npractices, there appears to be limited empirical understanding of how\nopen-source real-world CI workflows align with such practices. Many workflows\nmight be unnecessarily complex and not aligned with the simplicity goals of CI\npractices. This study will investigate the structure, complexity,\nheterogeneity, and compliance of GHA workflows in open-source software\nrepositories. Using a large dataset of GHA workflows from Java, Python, and C++\nrepositories, our goal is to (a) identify workflow complexities, (b) analyze\nrecurring and heterogeneous structuring patterns, (c) assess compliance with\nGHA best practices, and (d) uncover differences in CI pipeline design across\nprogramming languages. Our findings are expected to reveal both areas of strong\nadherence to best practices and areas for improvement where needed. These\ninsights will also have implications for CI services, as they will highlight\nthe need for clearer guidelines and comprehensive examples in CI documentation.", "comment": "Registered Report Accepted at the 41st IEEE International Conference\n  on Software Maintenance and Evolution 2025 (ICSME'25)", "pdf_url": "http://arxiv.org/pdf/2507.18062v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GitHub Actions 工作流的复杂性、异构性与合规性的实证研究", "tldr": "本研究对开源GitHub Actions工作流的复杂性、异构性及最佳实践合规性进行了实证调查，旨在识别问题并提供改进建议。", "motivation": "尽管GitHub Actions提供了官方文档和社区支持的最佳实践，但对于开源真实世界CI工作流如何与这些实践对齐的实证理解有限。许多工作流可能不必要地复杂，且不符合CI实践的简洁性目标。", "method": "本研究使用来自Java、Python和C++仓库的大型GitHub Actions工作流数据集，旨在：(a) 识别工作流复杂性；(b) 分析重复和异构的结构模式；(c) 评估对GHA最佳实践的遵守情况；(d) 揭示不同编程语言在CI管道设计上的差异。", "result": "研究结果预计将揭示在最佳实践方面的高度遵守区域以及需要改进的区域。", "conclusion": "这些见解将对CI服务产生影响，因为它们将突出CI文档中需要更清晰的指南和全面的示例。", "translation": "持续集成（CI）已经从一种工具策略发展成为现代CI工程中的一种基本思维模式。它使团队能够快速协作地开发、测试和交付软件。在众多CI服务中，GitHub Actions（GHA）由于其与GitHub的深度集成和庞大的可重用工作流行动生态系统而成为主导服务。尽管GHA提供了官方文档和社区支持的最佳实践，但对于开源真实世界CI工作流如何与这些实践对齐的实证理解似乎有限。许多工作流可能不必要地复杂，且不符合CI实践的简洁性目标。本研究将调查开源软件仓库中GHA工作流的结构、复杂性、异构性和合规性。利用来自Java、Python和C++仓库的大型GHA工作流数据集，我们的目标是 (a) 识别工作流复杂性，(b) 分析重复和异构的结构模式，(c) 评估对GHA最佳实践的遵守情况，以及 (d) 揭示不同编程语言在CI管道设计上的差异。我们的发现预计将揭示对最佳实践的高度遵守区域以及需要改进的区域。这些见解也将对CI服务产生影响，因为它们将突出CI文档中需要更清晰的指南和全面的示例。", "summary": "本研究对GitHub Actions (GHA) 工作流的复杂性、异构性和合规性进行了实证分析。鉴于现有开源CI工作流可能与最佳实践存在偏差，研究利用Java、Python和C++项目的大型GHA数据集，旨在识别工作流复杂性、分析结构模式、评估最佳实践合规性，并探究跨编程语言的设计差异。预期研究结果将揭示合规现状及改进空间，为CI服务提供更清晰的指导和示例提供依据。", "keywords": "GitHub Actions, CI工作流, 复杂性, 异构性, 合规性, 实证研究", "comments": "这项研究通过对实际开源GHA工作流进行大规模实证分析，填补了现有CI实践与实际应用之间理解的空白。其创新之处在于系统性地量化了GHA工作流的复杂性、异构性及合规性，并特别关注了跨语言的差异。研究结果对于改进CI服务文档和最佳实践具有直接指导意义，有助于提升软件开发的效率和质量。"}}
{"id": "2507.16859", "title": "Leveraging multi-source and heterogeneous signals for fatigue detection", "authors": ["Luobin Cui", "Yanlai Wu", "Tang Ying", "Weikai Li"], "categories": ["cs.RO", "cs.AI", "62H30", "I.2"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      1figures,32pages", "url": "http://arxiv.org/abs/2507.16859v2", "summary": "Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.", "comment": "1figures,32pages", "pdf_url": "http://arxiv.org/pdf/2507.16859v2", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "利用多源异构信号进行疲劳检测", "tldr": "本文提出了一种多源异构疲劳检测框架，用于在传感器受限的真实世界环境中实现有效疲劳监测，解决了现有方法对高端传感器和受控环境的依赖问题。", "motivation": "疲劳检测在航空、采矿和长途运输等安全关键应用中至关重要。然而，大多数现有方法依赖于高端传感器和受控环境，限制了它们在现实世界中的适用性。本文旨在解决在传感器受限的真实世界场景中进行疲劳检测的问题。", "method": "本文提出了一种异构多源疲劳检测框架，该框架能够自适应地利用目标领域中可用的模态，同时受益于源领域中存在的不同配置。它利用来自不同仪器来源（包括在受控环境中部署的非实用传感器）的知识。", "result": "实验结果表明，该方法在现实世界部署的传感器设置和两个公开数据集上具有实用性、鲁棒性和改进的泛化能力。", "conclusion": "本文提出的方法为在传感器受限的情况下进行有效的疲劳监测铺平了道路，解决了现有方法的局限性，并提升了在实际应用中的可行性。", "translation": "疲劳检测在航空、采矿和长途运输等安全关键应用中发挥着关键作用。然而，大多数现有方法依赖于高端传感器和受控环境，限制了它们在现实世界中的适用性。本文正式定义了一个实用但未充分探索的真实世界疲劳检测问题设置，其中使用与上下文相关的传感器的系统旨在利用来自不同仪器来源（包括使用在受控环境中部署的不实用传感器）的知识。为了应对这一挑战，我们提出了一种异构多源疲劳检测框架，该框架能够自适应地利用目标领域中可用的模态，同时受益于源领域中存在的不同配置。我们的实验使用现实世界部署的传感器设置和两个公开数据集进行，证明了我们方法的实用性、鲁棒性和改进的泛化能力，为在传感器受限的情况下进行有效的疲劳监测铺平了道路。", "summary": "本文提出了一种新颖的多源异构疲劳检测框架，旨在解决现有方法在现实世界应用中因依赖高端传感器和受控环境而受限的问题。该框架通过整合来自不同传感器的异构数据，并在传感器受限的场景中自适应地利用可用模态，从而实现有效的疲劳监测。实验证明了其在实际应用中的可行性、鲁棒性和泛化能力。", "keywords": "疲劳检测, 多源, 异构信号, 真实世界应用, 传感器受限", "comments": "本文的创新之处在于其针对真实世界、传感器受限的疲劳检测场景提出了一个实用且新颖的解决方案。通过利用多源异构信号，该方法克服了传统方法的局限性，显著提高了疲劳检测在实际应用中的可行性和鲁棒性。这对于提高安全关键领域的效率和安全性具有重要意义。"}}
{"id": "2410.13436", "title": "Multi-frame Detection via Graph Neural Networks: A Link Prediction Approach", "authors": ["Zhihao Lin", "Chang Gao", "Junkun Yan", "Qingfu Zhang", "Bo Chen", "Hongwei Liu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13436v3", "summary": "Multi-frame detection algorithms can effectively utilize the correlation\nbetween consecutive echoes to improve the detection performance of weak\ntargets. Existing efficient multi-frame detection algorithms are typically\nbased on three sequential steps: plot extraction via a relative low primary\nthreshold, track search and track detection. However, these three-stage\nprocessing algorithms may result in a notable loss of detection performance and\ndo not fully leverage the available echo information across frames. As to\napplying graph neural networks in multi-frame detection, the algorithms are\nprimarily based on node classification tasks, which cannot directly output\ntarget tracks. In this paper, we reformulate the multi-frame detection problem\nas a link prediction task in graphs. First, we perform a rough association of\nmulti-frame observations that exceed the low threshold to construct observation\nassociation graphs. Subsequently, a multi-feature link prediction network is\ndesigned based on graph neural networks, which integrates multi-dimensional\ninformation, including echo structure, Doppler information, and spatio-temporal\ncoupling of plots. By leveraging the principle of link prediction, we unifies\nthe processes of track search and track detection into one step to reduce\nperformance loss and directly output target tracks. Experimental results\nindicate that, compared with traditional single-frame and multi-frame detection\nalgorithms, the proposed algorithm improves the detection performance of weak\ntargets while suppressing false alarms. Additionally, interpretable analysis\nshows that the designed network effectively integrates the utilized features,\nallowing for accurate associations between targets and false alarms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13436v3", "cate": "eess.SP", "date": "2024-10-17", "updated": "2025-07-24", "AI": {"title_translation": "多帧检测中的图神经网络：一种链接预测方法", "tldr": "将多帧检测问题重新表述为图中的链接预测任务，利用图神经网络提高弱目标检测性能并抑制虚警。", "motivation": "现有高效多帧检测算法通常基于三步处理（点迹提取、轨迹搜索、轨迹检测），可能导致检测性能显著下降，且未能充分利用跨帧回波信息。此外，现有基于图神经网络的多帧检测方法主要基于节点分类任务，无法直接输出目标轨迹。", "method": "将多帧检测问题重新表述为图中的链接预测任务。首先，对超过低阈值的多帧观测进行粗略关联以构建观测关联图。随后，设计了一个基于图神经网络的多特征链接预测网络，该网络集成了回波结构、多普勒信息和点迹时空耦合等多维信息。通过链接预测，将轨迹搜索和轨迹检测过程统一为一步。", "result": "实验结果表明，与传统单帧和多帧检测算法相比，所提出的算法在提高弱目标检测性能的同时抑制了虚警。可解释性分析表明，设计的网络有效整合了所利用的特征，能够准确区分目标和虚警。", "conclusion": "通过将多帧检测重构为图链接预测问题并利用图神经网络，本方法能够有效提升弱目标检测性能，同时简化了轨迹处理流程。", "translation": "多帧检测算法能有效利用连续回波之间的相关性，以提高弱目标的检测性能。现有高效的多帧检测算法通常基于三个顺序步骤：通过相对较低的初级阈值进行点迹提取、轨迹搜索和轨迹检测。然而，这些三阶段处理算法可能导致检测性能显著下降，并且未能充分利用跨帧的可用回波信息。至于将图神经网络应用于多帧检测，现有算法主要基于节点分类任务，无法直接输出目标轨迹。在本文中，我们将多帧检测问题重新表述为图中的链接预测任务。首先，我们对超出低阈值的多帧观测进行粗略关联，以构建观测关联图。随后，设计了一个基于图神经网络的多特征链接预测网络，该网络集成了包括回波结构、多普勒信息和点迹时空耦合在内的多维信息。通过利用链接预测原理，我们将轨迹搜索和轨迹检测过程统一为一步，以减少性能损失并直接输出目标轨迹。实验结果表明，与传统的单帧和多帧检测算法相比，所提出的算法在提高弱目标检测性能的同时抑制了虚警。此外，可解释性分析表明，所设计的网络有效整合了所利用的特征，从而能够准确区分目标和虚警。", "summary": "本文将多帧检测问题重新定义为图中的链接预测任务，旨在克服传统三阶段方法和现有图神经网络节点分类方法的局限性。通过构建观测关联图并设计一个集成多维特征（如回波结构、多普勒信息、时空耦合）的图神经网络，该方法将轨迹搜索和检测统一为一步。实验证明，该方法能有效提升弱目标检测性能并抑制虚警。", "keywords": "多帧检测, 图神经网络, 链接预测, 弱目标检测, 轨迹检测", "comments": "本文的创新点在于将多帧检测这一经典问题巧妙地转化为图神经网络中的链接预测任务，这提供了一种新颖且更直接的轨迹生成方式，解决了传统方法分步处理导致的性能损失以及现有GNN方法无法直接输出轨迹的问题。通过统一轨迹搜索和检测步骤，并有效融合多维特征，该方法在弱目标检测方面表现出显著优势。"}}
{"id": "2507.18203", "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation", "authors": ["Kyubeen Han", "Junseo Jang", "Hongjin Kim", "Geunyeong Jeong", "Harksoo Kim"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Main Accepted", "url": "http://arxiv.org/abs/2507.18203v1", "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications.", "comment": "ACL 2025 Main Accepted", "pdf_url": "http://arxiv.org/pdf/2507.18203v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "探索指令微调对大型语言模型误信息易感性的影响", "tldr": "指令微调会显著增加大型语言模型对用户提供误信息的接受度，将模型对误信息的易感性从助手角色转移到用户角色，因此需要系统性方法来缓解其负面影响。", "motivation": "指令微调虽然提高了大型语言模型遵循指令的能力并减少有害输出，但可能增加模型对用户输入的依赖，从而导致其无条件接受误信息并产生幻觉。现有研究多关注大型语言模型对与其参数知识矛盾的外部信息的接受度，但很少有研究直接探讨指令微调对此现象的影响，本研究旨在填补这一空白。", "method": "本研究调查了指令微调对大型语言模型接受误信息的影响。通过与基础模型的对比，并进一步探讨了影响误信息易感性的其他因素，如提示结构中用户的角色、误信息长度以及系统提示中警告的存在。", "result": "分析显示，当用户提供误信息时，经过指令微调的大型语言模型显著更容易接受这些信息。与基础模型相比，指令微调增加了模型对用户提供信息的依赖，将易感性从助手角色转移到用户角色。", "conclusion": "本研究结果强调，需要采用系统性方法来缓解指令微调的意外后果，并提高大型语言模型在实际应用中的可靠性。", "translation": "指令微调增强了大型语言模型（LLMs）更准确地遵循用户指令的能力，提高了可用性，同时减少了有害输出。然而，这个过程可能会增加模型对用户输入的依赖，可能导致对误信息的无过滤接受和幻觉的产生。现有研究主要强调LLMs对与其参数知识相矛盾的外部信息具有接受性，但关于指令微调对此现象的直接影响研究甚少。在我们的研究中，我们调查了指令微调对LLM误信息易感性的影响。我们的分析表明，经过指令微调的LLMs在用户呈现误信息时，显著更有可能接受这些信息。与基础模型的比较显示，指令微调增加了对用户提供信息的依赖，将易感性从助手角色转移到用户角色。此外，我们探讨了影响误信息易感性的其他因素，例如提示结构中用户的角色、误信息长度以及系统提示中警告的存在。我们的研究结果强调，需要采用系统性方法来缓解指令微调的意外后果，并增强LLMs在实际应用中的可靠性。", "summary": "本研究探讨了指令微调对大型语言模型（LLMs）接受误信息倾向的影响。研究发现，指令微调显著增加了LLMs对用户提供误信息的接受度，并使其对用户输入产生更强的依赖性，从而将误信息的易感性从模型本身转移至用户输入。论文还进一步考察了用户角色、误信息长度和系统警告等因素对误信息易感性的影响。研究结果强调了开发系统性方法来减轻指令微调负面影响的重要性，以提升LLMs在实际应用中的可靠性。", "keywords": "指令微调, 大型语言模型, 误信息, 易感性, 用户输入", "comments": "本文揭示了指令微调在提升LLM可用性同时可能带来的一个重要且被忽视的负面影响，即对用户输入误信息的易感性增加。其创新之处在于明确指出指令微调如何改变了LLM接受误信息的机制，从模型本身的知识偏向转向对用户输入的过度依赖。这项研究对于未来LLM的安全性和可靠性研究具有重要指导意义，提醒开发者在进行指令微调时需更审慎地考虑潜在风险。"}}
{"id": "2502.06764", "title": "History-Guided Video Diffusion", "authors": ["Kiwhan Song", "Boyuan Chen", "Max Simchowitz", "Yilun Du", "Russ Tedrake", "Vincent Sitzmann"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025. Project website: this https URL", "url": "http://arxiv.org/abs/2502.06764v2", "summary": "Classifier-free guidance (CFG) is a key technique for improving conditional\ngeneration in diffusion models, enabling more accurate control while enhancing\nsample quality. It is natural to extend this technique to video diffusion,\nwhich generates video conditioned on a variable number of context frames,\ncollectively referred to as history. However, we find two key challenges to\nguiding with variable-length history: architectures that only support\nfixed-size conditioning, and the empirical observation that CFG-style history\ndropout performs poorly. To address this, we propose the Diffusion Forcing\nTransformer (DFoT), a video diffusion architecture and theoretically grounded\ntraining objective that jointly enable conditioning on a flexible number of\nhistory frames. We then introduce History Guidance, a family of guidance\nmethods uniquely enabled by DFoT. We show that its simplest form, vanilla\nhistory guidance, already significantly improves video generation quality and\ntemporal consistency. A more advanced method, history guidance across time and\nfrequency further enhances motion dynamics, enables compositional\ngeneralization to out-of-distribution history, and can stably roll out\nextremely long videos. Project website: https://boyuan.space/history-guidance", "comment": "ICML 2025. Project website: https://boyuan.space/history-guidance", "pdf_url": "http://arxiv.org/pdf/2502.06764v2", "cate": "cs.LG", "date": "2025-02-10", "updated": "2025-07-24", "AI": {"title_translation": "历史引导的视频扩散", "tldr": "提出DFoT架构和历史引导方法，解决视频扩散中变长历史帧引导的挑战，显著提升视频生成质量和时间一致性，并能生成超长视频。", "motivation": "分类器无关引导（CFG）是扩散模型中提高条件生成质量的关键技术。然而，将其扩展到视频扩散时面临两个主要挑战：现有架构仅支持固定大小的条件输入，以及CFG风格的历史帧丢弃表现不佳。", "method": "提出Diffusion Forcing Transformer (DFoT) 架构和理论基础的训练目标，该架构能够联合实现对灵活数量的历史帧进行条件生成。在此基础上，引入了历史引导（History Guidance）系列方法。", "result": "最简单的香草历史引导已显著提升视频生成质量和时间一致性。更高级的历史引导（跨时间和频率）进一步增强了运动动态，实现了对分布外历史的组合泛化，并能稳定生成超长视频。", "conclusion": "历史引导结合DFoT架构有效解决了视频扩散中变长历史帧引导的挑战，显著提升了视频生成质量、时间一致性和运动动态，并能进行超长视频生成。", "translation": "分类器无关引导（CFG）是提高扩散模型条件生成能力的关键技术，它能实现更精确的控制并提升样本质量。将这项技术扩展到视频扩散是自然而然的，视频扩散根据可变数量的上下文帧（统称为历史）生成视频。然而，我们发现使用可变长度历史进行引导面临两个关键挑战：架构仅支持固定大小的条件输入，以及经验观察到CFG风格的历史帧丢弃表现不佳。为了解决这个问题，我们提出了Diffusion Forcing Transformer (DFoT)，这是一种视频扩散架构和具有理论基础的训练目标，它们共同实现了对灵活数量的历史帧进行条件生成。然后，我们引入了历史引导（History Guidance），这是一系列由DFoT独特实现的新型引导方法。我们表明，其最简单的形式，即香草历史引导，已经显著改善了视频生成质量和时间一致性。一种更高级的方法，即跨时间和频率的历史引导，进一步增强了运动动态，实现了对分布外历史的组合泛化，并且能够稳定地生成极其长的视频。项目网站：https://boyuan.space/history-guidance", "summary": "本文提出Diffusion Forcing Transformer (DFoT) 架构和历史引导方法，旨在解决视频扩散模型中对可变长度历史帧进行条件生成时面临的挑战。针对现有架构固定输入尺寸和CFG风格历史帧丢弃表现不佳的问题，DFoT提供了一种灵活处理任意数量历史帧的解决方案。实验结果表明，历史引导显著提升了视频生成质量和时间一致性，高级形式还能增强运动动态，实现泛化，并支持超长视频的稳定生成。", "keywords": "视频扩散, 历史引导, Diffusion Forcing Transformer, 条件生成, 时间一致性", "comments": "该论文的创新点在于提出了DFoT架构和历史引导方法，有效解决了视频扩散中变长历史帧条件生成的技术难题。这对于提升视频生成质量、时间一致性以及生成超长视频具有重要意义，是视频扩散领域的一个重要进展。"}}
{"id": "2507.17768", "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction", "authors": ["Yujia Tong", "Jingling Yuan", "Chuang Hu"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17768v1", "summary": "With the development of mobile and edge computing, the demand for low-bit\nquantized models on edge devices is increasing to achieve efficient deployment.\nTo enhance the performance, it is often necessary to retrain the quantized\nmodels using edge data. However, due to privacy concerns, certain sensitive\ndata can only be processed on edge devices. Therefore, employing\nQuantization-Aware Training (QAT) on edge devices has become an effective\nsolution. Nevertheless, traditional QAT relies on the complete dataset for\ntraining, which incurs a huge computational cost. Coreset selection techniques\ncan mitigate this issue by training on the most representative subsets.\nHowever, existing methods struggle to eliminate quantization errors in the\nmodel when using small-scale datasets (e.g., only 10% of the data), leading to\nsignificant performance degradation. To address these issues, we propose QuaRC,\na QAT framework with coresets on edge devices, which consists of two main\nphases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy\nScore\" to identify the subsets that most effectively capture the model's\nquantization errors. During the training phase, QuaRC employs the Cascaded\nLayer Correction strategy to align the intermediate layer outputs of the\nquantized model with those of the full-precision model, thereby effectively\nreducing the quantization errors in the intermediate layers. Experimental\nresults demonstrate the effectiveness of our approach. For instance, when\nquantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%\nimprovement in Top-1 accuracy on the ImageNet-1K dataset compared to\nstate-of-the-art techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17768v1", "cate": "cs.LG", "date": "2025-07-17", "updated": "2025-07-17", "AI": {"title_translation": "通过相对熵核集选择和级联层校正增强边缘设备上的量化感知训练", "tldr": "针对边缘设备上的量化感知训练，本文提出QuaRC框架，通过引入相对熵分数进行核集选择和采用级联层校正策略，有效解决了小规模数据集下量化误差导致的性能下降问题，显著提升了模型精度。", "motivation": "随着移动和边缘计算的发展，边缘设备对低比特量化模型的需求日益增长，以实现高效部署。为提升性能，通常需使用边缘数据对量化模型进行再训练。然而，隐私问题导致敏感数据只能在边缘设备上处理，使得在边缘设备上进行量化感知训练（QAT）成为有效方案。传统QAT依赖完整数据集，计算成本高昂。核集选择技术虽能通过代表性子集缓解此问题，但现有方法在小规模数据集（如仅10%数据）下难以消除量化误差，导致性能显著下降。", "method": "本文提出QuaRC框架，一个用于边缘设备上带有核集的QAT方法，包含两个主要阶段：在核集选择阶段，QuaRC引入“相对熵分数”来识别最能捕捉模型量化误差的子集。在训练阶段，QuaRC采用“级联层校正”策略，使量化模型的中间层输出与全精度模型的输出对齐，从而有效减少中间层的量化误差。", "result": "实验结果表明了该方法的有效性。例如，在使用1%数据子集将ResNet-18量化为2比特时，QuaRC在ImageNet-1K数据集上的Top-1准确率比现有技术提高了5.72%。", "conclusion": "本文提出的QuaRC框架，通过相对熵核集选择和级联层校正策略，有效提升了边缘设备上小规模数据集下的量化感知训练性能，显著降低了量化误差并提高了模型精度。", "translation": "随着移动和边缘计算的发展，边缘设备对低比特量化模型的需求日益增长，以实现高效部署。为提升性能，通常需要使用边缘数据对量化模型进行再训练。然而，由于隐私问题，某些敏感数据只能在边缘设备上处理。因此，在边缘设备上采用量化感知训练（QAT）已成为一种有效的解决方案。然而，传统的QAT依赖于完整的训练数据集，这会产生巨大的计算成本。核集选择技术可以通过在最具代表性的子集上进行训练来缓解这个问题。但是，现有方法在使用小规模数据集（例如，仅10%的数据）时难以消除模型中的量化误差，从而导致显著的性能下降。为了解决这些问题，我们提出了QuaRC，一个用于边缘设备上带有核集的QAT框架，它包含两个主要阶段：在核集选择阶段，QuaRC引入“相对熵分数”来识别最能有效捕捉模型量化误差的子集。在训练阶段，QuaRC采用“级联层校正”策略，使量化模型的中间层输出与全精度模型的中间层输出对齐，从而有效减少中间层的量化误差。实验结果证明了我们方法的有效性。例如，在使用1%数据子集将ResNet-18量化为2比特时，QuaRC在ImageNet-1K数据集上的Top-1准确率比现有技术提高了5.72%。", "summary": "本文针对边缘设备上量化感知训练（QAT）在小规模数据集下性能下降的问题，提出了QuaRC框架。该框架包含两部分：一是通过“相对熵分数”选择最能反映模型量化误差的代表性数据子集；二是通过“级联层校正”策略对齐量化模型和全精度模型的中间层输出，以减少量化误差。实验证明，QuaRC在极小数据量下能显著提升量化模型的准确率，例如在1%数据子集上将ResNet-18量化到2比特时，ImageNet-1K上的Top-1准确率提高了5.72%。", "keywords": "量化感知训练, 边缘设备, 核集选择, 相对熵, 级联层校正", "comments": "该论文的创新点在于结合了核集选择和层级校正策略来优化边缘设备上的QAT。特别是“相对熵分数”用于核集选择，以及“级联层校正”用于误差缓解，是解决小数据量下量化性能瓶颈的关键。这对于隐私敏感和资源受限的边缘AI部署具有重要意义，提升了模型在实际应用中的可行性。"}}
{"id": "2507.17853", "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17853v1", "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17853v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Detail++: 面向文生图扩散模型的免训练细节增强器", "tldr": "Detail++是一个免训练的框架，通过渐进式细节注入策略和质心对齐损失，显著提升了文生图模型在处理复杂多主题提示时的细节生成和属性绑定能力。", "motivation": "当前的文生图模型在处理包含多个具有不同属性的主体的复杂提示时面临显著挑战，难以生成令人满意的视觉结果。", "method": "本研究提出了一个名为Detail++的免训练框架，引入了渐进式细节注入（PDI）策略。该方法将复杂提示分解为一系列简化的子提示，分阶段引导生成过程，利用自注意力机制确保全局构图，随后进行精确细化。为实现属性与相应主体之间的准确绑定，该方法利用交叉注意力机制，并在测试时引入质心对齐损失以减少绑定噪声并增强属性一致性。", "result": "在T2I-CompBench和新建的风格合成基准测试中，Detail++显著优于现有方法，尤其是在涉及多个对象和复杂风格条件的情况下。", "conclusion": "Detail++通过其创新的渐进式细节注入策略和质心对齐损失，有效解决了文生图模型在处理复杂提示时细节生成和属性绑定方面的挑战，显著提升了生成质量。", "translation": "文生图（T2I）生成技术的最新进展带来了令人印象深刻的视觉效果。然而，这些模型在处理复杂提示时仍面临重大挑战，特别是那些涉及多个具有不同属性的主体的提示。受人类绘画过程的启发，即首先勾勒构图然后逐步添加细节，我们提出了Detail++，一个免训练的框架，它引入了一种新颖的渐进式细节注入（PDI）策略来解决这一限制。具体来说，我们将复杂提示分解为一系列简化的子提示，分阶段引导生成过程。这种分阶段生成利用自注意力固有的布局控制能力，首先确保全局构图，然后进行精确细化。为了实现属性与相应主体之间的准确绑定，我们利用交叉注意力机制，并在测试时进一步引入质心对齐损失，以减少绑定噪声并增强属性一致性。在T2I-CompBench和新建的风格合成基准测试上进行的广泛实验表明，Detail++显著优于现有方法，特别是在涉及多个对象和复杂风格条件的情况下。", "summary": "Detail++是一个免训练的文生图细节增强框架，旨在解决现有模型在处理复杂多主题提示时的挑战。该框架引入了渐进式细节注入（PDI）策略，将复杂提示分解为子提示分阶段生成，利用自注意力和交叉注意力机制，并结合质心对齐损失来确保全局构图、精确细化以及属性与主体的准确绑定，从而显著提升了生成质量。", "keywords": "文生图, 细节增强, 扩散模型, 免训练, 渐进式细节注入", "comments": "Detail++的创新之处在于其免训练的特性和渐进式细节注入策略，这降低了应用门槛。通过将复杂提示分解并分阶段处理，它模拟了人类的创作过程，有效解决了多主体复杂提示下的细节生成和属性绑定问题。质心对齐损失的引入是其提升属性一致性的关键。这项工作为提升文生图模型在复杂场景下的表现提供了重要思路和有效方案。"}}
{"id": "2507.17948", "title": "VERIRAG: Healthcare Claim Verification via Statistical Audit in Retrieval-Augmented Generation", "authors": ["Shubham Mohole", "Hongjun Choi", "Shusen Liu", "Christine Klymko", "Shashank Kushwaha", "Derek Shi", "Wesam Sakla", "Sainyam Galhotra", "Ruben Glatt"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17948v1", "summary": "Retrieval-augmented generation (RAG) systems are increasingly adopted in\nclinical decision support, yet they remain methodologically blind-they retrieve\nevidence but cannot vet its scientific quality. A paper claiming \"Antioxidant\nproteins decreased after alloferon treatment\" and a rigorous multi-laboratory\nreplication study will be treated as equally credible, even if the former\nlacked scientific rigor or was even retracted. To address this challenge, we\nintroduce VERIRAG, a framework that makes three notable contributions: (i) the\nVeritable, an 11-point checklist that evaluates each source for methodological\nrigor, including data integrity and statistical validity; (ii) a Hard-to-Vary\n(HV) Score, a quantitative aggregator that weights evidence by its quality and\ndiversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the\nrequired evidence based on how extraordinary a claim is. Across four\ndatasets-comprising retracted, conflicting, comprehensive, and settled science\ncorpora-the VERIRAG approach consistently outperforms all baselines, achieving\nabsolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point\nimprovement over the next-best method in each respective dataset. We will\nrelease all materials necessary for reproducing our results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17948v1", "cate": "cs.IR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "VERIRAG：通过检索增强生成中的统计审计进行医疗索赔验证", "tldr": "VERIRAG通过引入科学质量评估和动态证据阈值，改进了RAG系统在医疗索赔验证中的证据筛选能力，显著优于现有方法。", "motivation": "现有的检索增强生成（RAG）系统在临床决策支持中无法评估其检索到的证据的科学质量和严谨性，对待严谨研究和缺乏科学严谨性甚至已撤回的声明一视同仁。", "method": "引入了VERIRAG框架，包含三个主要贡献：(i) Veritable，一个11点清单，用于评估每个来源的方法学严谨性，包括数据完整性和统计有效性；(ii) Hard-to-Vary (HV) Score，一个根据证据质量和多样性进行加权的定量聚合器；(iii) 动态接受阈值，根据声明的非凡程度校准所需证据。", "result": "在包含已撤回、冲突、全面和已解决科学语料库的四个数据集上，VERIRAG方法始终优于所有基线，F1分数范围为0.53至0.65，比每个数据集中次优方法提高了10至14个百分点。", "conclusion": "VERIRAG通过其创新的框架显著提高了RAG系统在医疗索赔验证中评估证据质量的能力，表现出卓越的性能。", "translation": "检索增强生成 (RAG) 系统在临床决策支持中得到越来越多的应用，但它们在方法论上仍然是盲目的——它们检索证据，但无法审查其科学质量。一篇声称“阿洛费伦治疗后抗氧化蛋白减少”的论文和一项严谨的多实验室复制研究将被视为同样可信，即使前者缺乏科学严谨性甚至已被撤回。为了解决这一挑战，我们引入了 VERIRAG，一个具有三个显著贡献的框架：(i) Veritable，一个评估每个来源方法学严谨性（包括数据完整性与统计有效性）的 11 点清单；(ii) Hard-to-Vary (HV) 分数，一个根据证据质量和多样性进行加权的定量聚合器；以及 (iii) 动态接受阈值，根据声明的非凡程度校准所需证据。在包含已撤回、冲突、全面和已解决科学语料库的四个数据集上，VERIRAG 方法始终优于所有基线，F1 分数范围为 0.53 到 0.65，比每个数据集中次优方法提高了 10 到 14 个百分点。我们将发布重现我们结果所需的所有材料。", "summary": "本文提出了VERIRAG框架，旨在解决检索增强生成（RAG）系统在医疗索赔验证中无法评估证据科学质量的问题。VERIRAG通过引入Veritable清单评估方法学严谨性、HV分数加权证据质量与多样性，以及动态接受阈值校准证据需求，显著提升了证据筛选能力。实验结果表明，在多个数据集上，VERIRAG的F1分数比现有最佳方法提高了10-14个百分点，验证了其有效性。", "keywords": "医疗索赔验证, 检索增强生成, 统计审计, 证据质量, 临床决策支持", "comments": "这项工作具有重要的创新性，因为它解决了RAG系统在医疗健康领域应用中的一个关键局限性——即缺乏对检索证据科学质量的评估能力。VERIRAG提出的三部分框架，特别是对证据严谨性和可信度的量化评估，对于提升临床决策支持系统的可靠性和安全性至关重要。其方法学严谨性清单和动态阈值概念是该领域的有益补充。"}}
{"id": "2507.18362", "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model", "authors": ["Yilong Hu", "Shijie Chang", "Lihe Zhang", "Feng Tian", "Weibing Sun", "Huchuan Lu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      MICCAI2025", "url": "http://arxiv.org/abs/2507.18362v1", "summary": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable\nperformance across a variety of generative tasks. The inherent randomness in\ndiffusion models helps address issues such as blurring at the edges of medical\nimages and labels, positioning Diffusion Probabilistic Models (DPMs) as a\npromising approach for lesion segmentation. However, we find that the current\ntraining and inference strategies of diffusion models result in an uneven\ndistribution of attention across different timesteps, leading to longer\ntraining times and suboptimal solutions. To this end, we propose UniSegDiff, a\nnovel diffusion model framework designed to address lesion segmentation in a\nunified manner across multiple modalities and organs. This framework introduces\na staged training and inference approach, dynamically adjusting the prediction\ntargets at different stages, forcing the model to maintain high attention\nacross all timesteps, and achieves unified lesion segmentation through\npre-training the feature extraction network for segmentation. We evaluate\nperformance on six different organs across various imaging modalities.\nComprehensive experimental results demonstrate that UniSegDiff significantly\noutperforms previous state-of-the-art (SOTA) approaches. The code is available\nat https://github.com/HUYILONG-Z/UniSegDiff.", "comment": "MICCAI2025", "pdf_url": "http://arxiv.org/pdf/2507.18362v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "UniSegDiff：通过分阶段扩散模型提升统一病灶分割", "tldr": "UniSegDiff提出了一种分阶段扩散模型，用于统一病灶分割，解决了现有扩散模型训练和推理中注意力分布不均的问题，并在多模态和器官上取得了显著优于SOTA的性能。", "motivation": "扩散概率模型（DPM）在医学图像分割中表现出潜力，但现有DPM的训练和推理策略导致不同时间步的注意力分布不均，训练时间更长且解决方案不理想，影响了病灶分割效果。", "method": "本文提出了UniSegDiff，一个新颖的扩散模型框架，通过引入分阶段的训练和推理方法，动态调整不同阶段的预测目标，迫使模型在所有时间步保持高注意力，并通过预训练特征提取网络实现统一的病灶分割。", "result": "UniSegDiff在六种不同器官和多种成像模态上的综合实验结果表明，它显著优于以前的最先进（SOTA）方法。", "conclusion": "UniSegDiff通过其分阶段的训练和推理策略，有效解决了扩散模型在病灶分割中的局限性，实现了卓越的统一病灶分割性能。", "translation": "扩散概率模型（DPM）在各种生成任务中表现出卓越的性能。扩散模型固有的随机性有助于解决医学图像和标签边缘模糊等问题，使扩散概率模型（DPM）成为病灶分割的一种有前景的方法。然而，我们发现当前扩散模型的训练和推理策略导致不同时间步的注意力分布不均，导致训练时间更长和次优的解决方案。为此，我们提出了UniSegDiff，一个新颖的扩散模型框架，旨在以统一的方式解决多模态和器官的病灶分割问题。该框架引入了一种分阶段的训练和推理方法，动态调整不同阶段的预测目标，迫使模型在所有时间步保持高注意力，并通过预训练特征提取网络实现统一的病灶分割。我们在六种不同器官和各种成像模态上评估了性能。综合实验结果表明，UniSegDiff显著优于以前的最先进（SOTA）方法。代码可在https://github.com/HUYILONG-Z/UniSegDiff获取。", "summary": "本文提出了UniSegDiff，一个新颖的分阶段扩散模型框架，旨在克服现有扩散模型在医学图像病灶分割中存在的注意力分布不均和性能次优问题。UniSegDiff通过动态调整不同阶段的预测目标，确保模型在所有时间步保持高注意力，并通过预训练特征提取网络实现多模态和多器官的统一病灶分割。实验结果表明，UniSegDiff在六种不同器官和多种成像模态上显著超越了现有的最先进方法。", "keywords": "扩散模型, 病灶分割, 统一分割, 分阶段训练, 医学图像分割", "comments": "UniSegDiff的创新点在于其引入的分阶段训练和推理策略，有效解决了扩散模型在医学图像分割中注意力分布不均的固有问题。这种方法不仅提升了模型性能，还可能缩短训练时间，使其在临床应用中更具实用性。其统一分割多模态和多器官病灶的能力也增加了其泛化性和重要性。"}}
{"id": "2507.18489", "title": "The Best is Yet to Come: Graph Convolution in the Testing Phase for Multimodal Recommendation", "authors": ["Jinfeng Xu", "Zheyu Chen", "Shuo Yang", "Jinze Li", "Edith C. H. Ngai"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by MM 2025", "url": "http://arxiv.org/abs/2507.18489v1", "summary": "The efficiency and scalability of graph convolution networks (GCNs) in\ntraining recommender systems remain critical challenges, hindering their\npractical deployment in real-world scenarios. In the multimodal recommendation\n(MMRec) field, training GCNs requires more expensive time and space costs and\nexacerbates the gap between different modalities, resulting in sub-optimal\nrecommendation accuracy. This paper critically points out the inherent\nchallenges associated with adopting GCNs during the training phase in MMRec,\nrevealing that GCNs inevitably create unhelpful and even harmful pairs during\nmodel optimization and isolate different modalities. To this end, we propose\nFastMMRec, a highly efficient multimodal recommendation framework that deploys\ngraph convolutions exclusively during the testing phase, bypassing their use in\ntraining. We demonstrate that adopting GCNs solely in the testing phase\nsignificantly improves the model's efficiency and scalability while alleviating\nthe modality isolation problem often caused by using GCNs during the training\nphase. We conduct extensive experiments on three public datasets, consistently\ndemonstrating the performance superiority of FastMMRec over competitive\nbaselines while achieving efficiency and scalability.", "comment": "Accepted by MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.18489v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "最佳尚未到来：多模态推荐中测试阶段的图卷积", "tldr": "本文提出FastMMRec，一种在多模态推荐中仅在测试阶段使用图卷积的新框架，解决了训练阶段GCN效率低、可扩展性差和模态隔离的问题，并在多个数据集上取得了优越的性能和效率。", "motivation": "图卷积网络（GCNs）在训练推荐系统中的效率和可扩展性是关键挑战，尤其在多模态推荐（MMRec）领域，训练GCNs需要更高的时空成本，并加剧不同模态之间的差距，导致推荐精度不佳。本文指出在MMRec训练阶段采用GCNs会产生无用甚至有害的配对，并隔离不同模态。", "method": "本文提出了FastMMRec，一个高效的多模态推荐框架，它仅在测试阶段部署图卷积，从而绕过了在训练阶段使用它们。", "result": "FastMMRec显著提高了模型的效率和可扩展性，并缓解了GCNs在训练阶段常引起的模态隔离问题。在三个公共数据集上的广泛实验表明，FastMMRec在实现效率和可扩展性的同时，性能优于竞争基线。", "conclusion": "通过将图卷积仅应用于测试阶段，FastMMRec成功解决了多模态推荐中GCN训练的效率、可扩展性和模态隔离问题，并取得了卓越的推荐性能。", "translation": "图卷积网络（GCNs）在训练推荐系统中的效率和可扩展性仍然是关键挑战，阻碍了它们在实际场景中的部署。在多模态推荐（MMRec）领域，训练GCNs需要更高的时空成本，并加剧了不同模态之间的差距，导致次优的推荐精度。本文批判性地指出在MMRec训练阶段采用GCNs所固有的挑战，揭示了GCNs在模型优化过程中不可避免地会创建无用甚至有害的配对，并隔离不同的模态。为此，我们提出了FastMMRec，一个高效的多模态推荐框架，它仅在测试阶段部署图卷积，从而绕过了在训练阶段使用它们。我们证明了仅在测试阶段采用GCNs显著提高了模型的效率和可扩展性，同时缓解了在训练阶段使用GCNs常引起的模态隔离问题。我们在三个公共数据集上进行了广泛的实验，持续证明了FastMMRec相对于竞争基线的性能优势，同时实现了效率和可扩展性。", "summary": "本文针对多模态推荐中图卷积网络（GCNs）在训练阶段存在的效率、可扩展性和模态隔离等问题，提出了一种名为FastMMRec的新框架。FastMMRec的创新之处在于，它将图卷积的计算仅限于模型的测试阶段，从而避免了训练阶段GCN带来的高昂成本和负面影响。实验结果表明，FastMMRec在保证效率和可扩展性的同时，显著提升了推荐性能，并有效缓解了模态间隔离的问题。", "keywords": "图卷积网络, 多模态推荐, 测试阶段, 效率, 可扩展性", "comments": "这项工作通过将图卷积的使用从训练阶段转移到测试阶段，为多模态推荐系统提供了一个新颖且高效的解决方案。其创新点在于挑战了传统GCN在训练阶段的普遍应用，并成功证明了在特定阶段（测试阶段）的策略性应用可以带来显著的性能和效率提升，这对于实际部署具有重要意义。"}}
{"id": "2408.14672", "title": "Optimizing against Infeasible Inclusions from Data for Semantic Segmentation through Morphology", "authors": ["Shamik Basu", "Luc Van Gool", "Christos Sakaridis"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.14672v5", "summary": "State-of-the-art semantic segmentation models are typically optimized in a\ndata-driven fashion, minimizing solely per-pixel or per-segment classification\nobjectives on their training data. This purely data-driven paradigm often leads\nto absurd segmentations, especially when the domain of input images is shifted\nfrom the one encountered during training. For instance, state-of-the-art models\nmay assign the label \"road\" to a segment that is included by another segment\nthat is respectively labeled as \"sky\". However, the ground truth of the\nexisting dataset at hand dictates that such inclusion is not feasible. Our\nmethod, Infeasible Semantic Inclusions (InSeIn), first extracts explicit\ninclusion constraints that govern spatial class relations from the semantic\nsegmentation training set at hand in an offline, data-driven fashion, and then\nenforces a morphological yet differentiable loss that penalizes violations of\nthese constraints during training to promote prediction feasibility. InSeIn is\na light-weight plug-and-play method, constitutes a novel step towards\nminimizing infeasible semantic inclusions in the predictions of learned\nsegmentation models, and yields consistent and significant performance\nimprovements over diverse state-of-the-art networks across the ADE20K,\nCityscapes, and ACDC datasets. https://github.com/SHAMIK-97/InSeIn/tree/main", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.14672v5", "cate": "cs.CV", "date": "2024-08-26", "updated": "2025-07-24", "AI": {"title_translation": "通过形态学优化语义分割中来自数据的不可行包含问题", "tldr": "本文提出了一种名为 InSeIn 的轻量级即插即用方法，通过提取和强制执行空间类别关系约束，以减少语义分割模型预测中的不可行包含，从而显著提高性能。", "motivation": "现有的语义分割模型在数据驱动的优化下，常产生不合理的分割结果，尤其是在输入图像域发生变化时，例如将“道路”标签分配给被“天空”标签包含的区域，这与现有数据集的真实情况不符。", "method": "本文提出的 Infeasible Semantic Inclusions (InSeIn) 方法首先离线地从语义分割训练数据集中提取控制空间类别关系的显式包含约束，然后通过一种可微分的形态学损失函数在训练过程中惩罚违反这些约束的行为，以提高预测的可行性。", "result": "InSeIn 是一种轻量级的即插即用方法，在 ADE20K、Cityscapes 和 ACDC 等多个数据集上，相对于各种最先进的网络，均能带来一致且显著的性能提升。", "conclusion": "InSeIn 方法是朝着最小化学习分割模型预测中不可行语义包含迈出的新颖一步。", "translation": "最先进的语义分割模型通常以数据驱动的方式进行优化，仅最小化训练数据上的逐像素或逐片段分类目标。这种纯粹的数据驱动范式常常导致荒谬的分割结果，尤其是在输入图像域与训练期间遇到的域发生偏移时。例如，最先进的模型可能会将“道路”标签分配给被另一个分别标记为“天空”的片段所包含的片段。然而，现有数据集的真实情况表明这种包含是不可行的。我们的方法，不可行语义包含（InSeIn），首先以离线、数据驱动的方式从现有语义分割训练集中提取控制空间类别关系的显式包含约束，然后强制执行一种形态学但可微分的损失，在训练期间惩罚这些约束的违反，以促进预测的可行性。InSeIn 是一种轻量级的即插即用方法，是朝着最小化学习分割模型预测中的不可行语义包含迈出的新颖一步，并在 ADE20K、Cityscapes 和 ACDC 数据集上，对各种最先进的网络都产生了持续且显著的性能改进。https://github.com/SHAMIK-97/InSeIn/tree/main", "summary": "本文针对语义分割模型在数据驱动优化下产生的不可行分割问题，提出了一种名为 InSeIn 的轻量级即插即用方法。该方法通过离线提取训练数据中的空间类别包含约束，并在训练过程中引入可微分的形态学损失来惩罚违反这些约束的行为，从而提高预测的合理性。实验证明，InSeIn 在多个主流数据集上显著提升了最先进分割模型的性能。", "keywords": "语义分割, 形态学, 不可行包含, 约束, 深度学习", "comments": "该论文的创新点在于引入了“不可行包含”的概念，并提出了一种基于形态学损失的轻量级即插即用方法来解决这一问题。它通过显式地利用数据中的空间关系约束，弥补了纯数据驱动模型在泛化性上的不足，为提高语义分割的鲁棒性和合理性提供了新思路。其“即插即用”特性也使其具有较高的实用价值。"}}
{"id": "2507.02976", "title": "Are AI-Generated Fixes Secure? Analyzing LLM and Agent Patches on SWE-bench", "authors": ["Amirali Sajadi", "Kostadin Damevski", "Preetha Chatterjee"], "categories": ["cs.CR", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02976v2", "summary": "Large Language Models (LLMs) and their agentic frameworks are increasingly\nadopted to automate software development tasks such as issue resolution and\nprogram repair. While prior work has identified security risks in LLM-generated\ncode, most evaluations have focused on synthetic or isolated settings, leaving\nopen questions about the security of these systems in real-world development\ncontexts. In this study, we present the first large-scale security analysis of\nLLM-generated patches using 20,000+ issues from the SWE-bench dataset. We\nevaluate patches produced by a standalone LLM (Llama 3.3) and compare them to\ndeveloper-written patches. We also assess the security of patches generated by\nthree top-performing agentic frameworks (OpenHands, AutoCodeRover, HoneyComb)\non a subset of our data. Finally, we analyze a wide range of code, issue, and\nproject-level factors to understand the conditions under which LLMs and agents\nare most likely to generate insecure code. Our findings reveal that the\nstandalone LLM introduces nearly 9x more new vulnerabilities than developers,\nwith many of these exhibiting unique patterns not found in developers' code.\nAgentic workflows also generate a significant number of vulnerabilities,\nparticularly when granting LLMs more autonomy, potentially increasing the\nlikelihood of misinterpreting project context or task requirements. We find\nthat vulnerabilities are more likely to occur in LLM patches associated with a\nhigher number of files, more lines of generated code, and GitHub issues that\nlack specific code snippets or information about the expected code behavior and\nsteps to reproduce. These results suggest that contextual factors play a\ncritical role in the security of the generated code and point toward the need\nfor proactive risk assessment methods that account for both code and\nissue-level information to complement existing vulnerability detection tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02976v2", "cate": "cs.CR", "date": "2025-06-30", "updated": "2025-07-24", "AI": {"title_translation": "AI生成的修复是否安全？分析SWE-bench上的LLM和Agent补丁", "tldr": "LLM和AI代理生成的代码修复引入了大量漏洞，尤其是在LLM自主性更高、上下文信息不足的情况下。", "motivation": "现有工作虽识别了LLM生成代码的安全风险，但多集中于合成或隔离环境。本研究旨在首次大规模分析LLM生成的真实世界开发上下文中的补丁的安全性，以填补这一空白。", "method": "本研究对SWE-bench数据集中的20,000多个问题进行首次大规模安全分析。评估了独立LLM（Llama 3.3）生成的补丁，并将其与开发者编写的补丁进行比较。同时，还评估了三种顶级代理框架（OpenHands、AutoCodeRover、HoneyComb）在部分数据上生成的补丁的安全性。最后，分析了代码、问题和项目层面的多种因素，以理解LLM和代理最有可能生成不安全代码的条件。", "result": "研究发现，独立LLM引入的新漏洞比开发者多近9倍，其中许多漏洞表现出开发者代码中未见的独特模式。代理工作流也生成了大量的漏洞，特别是在赋予LLM更多自主性时，这可能增加了其误解项目上下文或任务要求的可能性。漏洞更可能出现在与文件数量更多、生成代码行数更多以及缺乏具体代码片段或预期代码行为和复现步骤的GitHub问题相关的LLM补丁中。", "conclusion": "这些结果表明，上下文因素在生成代码的安全性中起着关键作用，并指出需要开发能够同时考虑代码和问题层面信息的积极风险评估方法，以补充现有的漏洞检测工具。", "translation": "大型语言模型（LLMs）及其代理框架正越来越多地被用于自动化软件开发任务，例如问题解决和程序修复。虽然先前的研究已经识别了LLM生成代码中的安全风险，但大多数评估都集中在合成或隔离环境中，这使得这些系统在真实世界开发环境中的安全性问题仍未解决。在本研究中，我们首次对LLM生成的补丁进行了大规模安全分析，使用了来自SWE-bench数据集的20,000多个问题。我们评估了由独立LLM（Llama 3.3）生成的补丁，并将其与开发者编写的补丁进行了比较。我们还在部分数据上评估了由三个表现最佳的代理框架（OpenHands、AutoCodeRover、HoneyComb）生成的补丁的安全性。最后，我们分析了广泛的代码、问题和项目层面的因素，以了解LLM和代理最有可能生成不安全代码的条件。我们的发现揭示，独立LLM引入的新漏洞比开发者多近9倍，其中许多漏洞表现出开发者代码中未见的独特模式。代理工作流也生成了大量的漏洞，特别是在赋予LLM更多自主性时，这可能增加了其误解项目上下文或任务要求的可能性。我们发现，漏洞更可能出现在与文件数量更多、生成代码行数更多以及GitHub问题缺乏具体代码片段或预期代码行为和复现步骤的LLM补丁中。这些结果表明，上下文因素在生成代码的安全性中起着关键作用，并指出需要开发能够同时考虑代码和问题层面信息的积极风险评估方法，以补充现有的漏洞检测工具。", "summary": "本研究首次大规模分析了LLM和AI代理在SWE-bench数据集上生成的软件修复补丁的安全性。研究发现，独立LLM引入的漏洞比开发者多近9倍，且具有独特模式。代理工作流也产生大量漏洞，尤其是在LLM自主性更高时。漏洞常出现在涉及更多文件、更多代码行以及上下文信息不足的问题中。结果表明，上下文因素对生成代码的安全性至关重要，呼吁开发结合代码和问题信息的积极风险评估方法。", "keywords": "LLM, 代码修复, 安全性, 漏洞, SWE-bench", "comments": "这项研究首次大规模评估了LLM和代理生成的软件修复补丁在真实世界场景中的安全性，填补了现有研究的空白。其发现独立LLM和代理引入大量漏洞，且指出了导致不安全代码生成的具体上下文因素，对未来AI辅助软件开发的安全实践具有重要指导意义。强调了在部署AI代码生成工具时，需特别关注其对安全性的影响，并结合上下文信息进行风险评估。"}}
{"id": "2507.17835", "title": "Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications", "authors": ["Simone Fiorellino", "Claudio Battiloro", "Emilio Calvanese Strinati", "Paolo Di Lorenzo"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17835v1", "summary": "In future AI-native wireless networks, the presence of mismatches between the\nlatent spaces of independently designed and trained deep neural network (DNN)\nencoders may impede mutual understanding due to the emergence of semantic\nchannel noise. This undermines the receiver's ability to interpret transmitted\nrepresentations, thereby reducing overall system performance. To address this\nissue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based\nsemantic channel equalizer that aligns latent spaces of heterogeneous encoders\nwithout requiring system retraining. PFE enables dynamic signal compression and\nexpansion, mitigating semantic noise while preserving performance on downstream\ntasks. Building on this capability, we introduce a dynamic optimization\nstrategy that coordinates communication, computation, and learning resources to\nbalance energy consumption, end-to-end (E2E) latency, and task performance in\nmulti-agent semantic communication scenarios. Extensive simulations confirm the\neffectiveness of our approach in maintaining semantic consistency and meeting\nlong-term constraints on latency and accuracy under diverse and time-varying\nnetwork conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17835v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "面向AI原生通信的基于帧的零样本语义信道均衡", "tldr": "提出Parseval帧均衡器（PFE），一个零样本、基于帧的语义信道均衡器，用于对齐异构编码器的潜在空间，以解决AI原生通信中的语义信道噪声问题，并通过动态优化策略平衡资源。", "motivation": "在未来的AI原生无线网络中，独立设计和训练的深度神经网络（DNN）编码器之间潜在空间的不匹配可能导致语义信道噪声，从而阻碍相互理解，损害接收器解释传输表示的能力，并降低整体系统性能。", "method": "提出Parseval帧均衡器（PFE），一个零样本、基于帧的语义信道均衡器，无需系统再训练即可对齐异构编码器的潜在空间。PFE支持动态信号压缩和扩展，以减轻语义噪声并保持下游任务性能。在此基础上，引入了动态优化策略，协调通信、计算和学习资源，以平衡多智能体语义通信场景中的能耗、端到端（E2E）延迟和任务性能。", "result": "广泛的仿真证实了该方法在多样化和时变网络条件下，在保持语义一致性以及满足延迟和准确性长期约束方面的有效性。", "conclusion": "Parseval帧均衡器（PFE）及其动态优化策略能够有效解决AI原生通信中的语义信道噪声问题，并通过对齐潜在空间、动态资源管理，在复杂网络环境中维持语义一致性并优化系统性能。", "translation": "在未来的AI原生无线网络中，独立设计和训练的深度神经网络（DNN）编码器之间潜在空间的不匹配可能由于语义信道噪声的出现而阻碍相互理解。这损害了接收器解释传输表示的能力，从而降低了整体系统性能。为了解决这个问题，我们提出了Parseval帧均衡器（PFE），一个零样本、基于帧的语义信道均衡器，它无需系统再训练即可对齐异构编码器的潜在空间。PFE实现了动态信号压缩和扩展，减轻了语义噪声，同时保持了下游任务的性能。在此能力的基础上，我们引入了一种动态优化策略，协调通信、计算和学习资源，以平衡多智能体语义通信场景中的能耗、端到端（E2E）延迟和任务性能。广泛的仿真证实了我们的方法在多样化和时变网络条件下，在保持语义一致性以及满足延迟和准确性长期约束方面的有效性。", "summary": "本研究提出Parseval帧均衡器（PFE），这是一种零样本、基于帧的语义信道均衡器，旨在解决AI原生通信中因深度神经网络编码器潜在空间不匹配导致的语义信道噪声问题。PFE无需再训练即可对齐异构编码器，并通过动态信号压缩与扩展减轻噪声。此外，论文还引入了动态优化策略，协调通信、计算和学习资源，以平衡多智能体场景下的能耗、延迟和任务性能。仿真结果表明，该方法在保持语义一致性并满足动态网络条件下的延迟和准确性约束方面表现出有效性。", "keywords": "语义通信, 信道均衡, 零样本学习, AI原生网络, Parseval帧", "comments": "这项工作提出了一个创新的零样本解决方案，通过Parseval帧均衡器（PFE）来解决AI原生通信中语义信道噪声的关键问题，避免了昂贵的系统再训练。其亮点在于同时考虑了潜在空间对齐和动态资源优化，这对于未来AI驱动的无线网络至关重要。该方法在多智能体场景下的应用及其对能耗、延迟和性能的平衡，显示出其在实际部署中的巨大潜力。"}}
{"id": "2507.18073", "title": "Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method", "authors": ["Qingcheng Zhu", "Yangyang Ren", "Linlin Yang", "Mingbao Lin", "Yanjing Li", "Sheng Xu", "Zichao Feng", "Haodong Zhu", "Yuguang Yang", "Juan Zhang", "Runqi Wang", "Baochang Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18073v1", "summary": "Deploying large language models (LLMs) is challenging due to their massive\nparameters and high computational costs. Ultra low-bit quantization can\nsignificantly reduce storage and accelerate inference, but extreme compression\n(i.e., mean bit-width <= 2) often leads to severe performance degradation. To\naddress this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs'\nweights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision\npost-training quantization (PTQ) framework and achieves an average of 1.6 bits\nper weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We\nintroduce Squeeze10LLM with two key innovations: Post-Binarization Activation\nRobustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a\nrefined weight significance metric that accounts for the impact of quantization\non activations, improving accuracy in low-bit settings. FIAS is a strategy that\npreserves full activation information during quantization to mitigate\ncumulative error propagation across layers. Experiments on LLaMA and LLaMA2\nshow that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit\nweight-only quantization, improving average accuracy from 43% to 56% on six\nzero-shot classification tasks--a significant boost over existing PTQ methods.\nOur code will be released upon publication.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18073v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Squeeze10-LLM：通过分阶段混合精度量化方法将LLM权重压缩10倍", "tldr": "Squeeze10-LLM提出一种分阶段混合精度量化方法，将LLM权重压缩10倍（平均1.6比特），通过PBAR和FIAS创新，显著提升了超低比特量化性能，解决了LLM部署中的挑战。", "motivation": "部署大型语言模型（LLMs）因其庞大的参数和高昂的计算成本而充满挑战。超低比特量化可以显著减少存储并加速推理，但极端压缩（即平均比特宽度≤2）通常会导致严重的性能下降。", "method": "提出Squeeze10-LLM，一个分阶段混合精度后训练量化（PTQ）框架，旨在将16位LLM的权重“压缩”10倍，平均每权重1.6比特（具体为80%的权重量化为1比特，20%量化为4比特）。该方法引入了两项关键创新：后二值化激活鲁棒性（PBAR），作为一种改进的权重显著性度量，考虑了量化对激活的影响以提高低比特设置下的准确性；以及全信息激活监督（FIAS），一种在量化过程中保留完整激活信息以减轻跨层累积误差传播的策略。", "result": "在LLaMA和LLaMA2上的实验表明，Squeeze10-LLM在低于2比特的仅权重（weight-only）量化方面实现了最先进的性能。它在六个零样本分类任务上将平均准确率从43%提高到56%，比现有PTQ方法有了显著提升。", "conclusion": "Squeeze10-LLM通过其创新的分阶段混合精度量化方法以及PBAR和FIAS机制，成功解决了LLM极端低比特量化导致的性能下降问题，实现了显著的压缩比和领先的准确率，为LLM的部署提供了有效的解决方案。", "translation": "部署大型语言模型（LLMs）因其庞大的参数和高昂的计算成本而充满挑战。超低比特量化可以显著减少存储并加速推理，但极端压缩（即平均比特宽度≤2）通常会导致严重的性能下降。为了解决这个问题，我们提出了Squeeze10-LLM，有效地将16位LLM的权重“压缩”10倍。具体来说，Squeeze10-LLM是一个分阶段混合精度后训练量化（PTQ）框架，通过将80%的权重量化为1比特，20%量化为4比特，实现了平均每权重1.6比特。我们引入Squeeze10LLM并提出了两项关键创新：后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS）。PBAR是一种改进的权重显著性度量，它考虑了量化对激活的影响，从而提高了低比特设置下的准确性。FIAS是一种在量化过程中保留完整激活信息以减轻跨层累积误差传播的策略。在LLaMA和LLaMA2上的实验表明，Squeeze10-LLM在低于2比特的仅权重（weight-only）量化方面实现了最先进的性能，在六个零样本分类任务上将平均准确率从43%提高到56%——比现有PTQ方法有了显著提升。我们的代码将在发表时发布。", "summary": "本文提出了Squeeze10-LLM，一个分阶段混合精度后训练量化框架，旨在解决大型语言模型（LLMs）部署中极端低比特量化导致的性能下降问题。该方法通过将LLM权重平均压缩至1.6比特（80%为1比特，20%为4比特），实现了10倍的压缩比。Squeeze10-LLM引入了PBAR（后二值化激活鲁棒性）和FIAS（全信息激活监督）两项关键创新，分别用于提高低比特设置下的准确性和减轻累积误差传播。实验结果表明，Squeeze10-LLM在LLaMA和LLaMA2模型上，针对低于2比特的仅权重量化，取得了最先进的性能，显著提升了零样本分类任务的准确率，展现了其在高效部署LLM方面的潜力。", "keywords": "LLM量化, 混合精度, 后训练量化, 超低比特, 模型压缩", "comments": "Squeeze10-LLM的创新性在于其独特的分阶段混合精度量化策略，以及PBAR和FIAS这两个新颖的机制，有效地平衡了极高压缩率和模型性能之间的矛盾。它成功地将LLM权重压缩10倍同时保持了最先进的性能，这对于在资源受限环境中实际部署大型语言模型具有重大意义。该工作为超低比特量化领域提供了新的思路和解决方案，有望推动LLM的更广泛应用。"}}
{"id": "2507.18181", "title": "SpecASR: Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding", "authors": ["Linye Wei", "Shuzhang Zhong", "Songqiang Xu", "Runsheng Wang", "Ru Huang", "Meng Li"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18181v1", "summary": "Large language model (LLM)-based automatic speech recognition (ASR) has\nrecently attracted a lot of attention due to its high recognition accuracy and\nenhanced multi-dialect support. However, the high decoding latency of LLMs\nchallenges the real-time ASR requirements. Although speculative decoding has\nbeen explored for better decoding efficiency, they usually ignore the key\ncharacteristics of the ASR task and achieve limited speedup. To further reduce\nthe real-time ASR latency, in this paper, we propose a novel speculative\ndecoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed\nbased on our core observation that ASR decoding is audio-conditioned, which\nresults in high output alignment between small and large ASR models, even given\noutput mismatches in intermediate decoding steps. Therefore, SpecASR features\nan adaptive draft sequence generation process that dynamically modifies the\ndraft sequence length to maximize the token acceptance length. SpecASR further\nproposes a draft sequence recycling strategy that reuses the previously\ngenerated draft sequence to reduce the draft ASR model latency. Moreover, a\ntwo-pass sparse token tree generation algorithm is also proposed to balance the\nlatency of draft and target ASR models. With extensive experimental results, we\ndemonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the\nbaseline autoregressive decoding and speculative decoding, respectively,\nwithout any loss in recognition accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18181v1", "cate": "eess.AS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SpecASR：通过推测解码加速基于大型语言模型的自动语音识别", "tldr": "SpecASR是一种新型的推测解码框架，专门用于加速基于大型语言模型的自动语音识别，其速度比现有方法快3.04-3.79倍，且不损失准确性。", "motivation": "基于大型语言模型（LLM）的自动语音识别（ASR）虽然识别准确率高且支持多方言，但其高解码延迟阻碍了实时ASR应用。现有的推测解码方法未能充分考虑ASR任务的特性，加速效果有限。", "method": "本文提出了一种名为SpecASR的新型推测解码框架，专门针对ASR任务。SpecASR基于ASR解码是音频条件这一核心观察，其特点包括：动态修改草稿序列长度以最大化令牌接受长度的自适应草稿序列生成过程；重用先前生成的草稿序列以减少草稿ASR模型延迟的草稿序列回收策略；以及平衡草稿和目标ASR模型延迟的两遍稀疏令牌树生成算法。", "result": "SpecASR在不损失识别准确率的情况下，相对于基线自回归解码实现了3.04倍至3.79倍的加速，相对于推测解码实现了1.25倍至1.84倍的加速。", "conclusion": "SpecASR通过专门为ASR任务设计的推测解码框架，显著降低了基于大型语言模型的自动语音识别的实时延迟，同时保持了高识别准确率。", "translation": "大型语言模型（LLM）驱动的自动语音识别（ASR）最近因其高识别准确率和增强的多方言支持而备受关注。然而，LLM的高解码延迟对实时ASR需求构成了挑战。尽管推测解码已被探索以提高解码效率，但它们通常忽略了ASR任务的关键特性，实现的加速有限。为了进一步降低实时ASR延迟，本文提出了一种专门用于ASR的新型推测解码框架，名为SpecASR。SpecASR是基于我们核心观察开发的，即ASR解码是音频条件的，这导致即使在中间解码步骤中存在输出不匹配，小型和大型ASR模型之间也能实现高输出对齐。因此，SpecASR的特点是自适应草稿序列生成过程，该过程动态修改草稿序列长度以最大化令牌接受长度。SpecASR进一步提出了一种草稿序列回收策略，该策略重用先前生成的草稿序列以减少草稿ASR模型的延迟。此外，还提出了一种两遍稀疏令牌树生成算法，以平衡草稿和目标ASR模型的延迟。通过大量的实验结果，我们证明SpecASR相对于基线自回归解码实现了3.04倍至3.79倍的加速，相对于推测解码实现了1.25倍至1.84倍的加速，且没有任何识别准确率损失。", "summary": "本论文提出SpecASR，一个专为基于大型语言模型（LLM）的自动语音识别（ASR）设计的推测解码框架，旨在解决LLM高延迟带来的实时ASR挑战。SpecASR利用ASR解码的音频条件特性，通过自适应草稿序列生成、草稿序列回收策略以及两遍稀疏令牌树生成算法来优化解码效率。实验结果表明，SpecASR在不牺牲识别准确率的前提下，比基线自回归解码和现有推测解码分别实现了显著的速度提升。", "keywords": "自动语音识别, 大型语言模型, 推测解码, 低延迟, SpecASR", "comments": "该论文的创新点在于将推测解码方法与ASR任务的音频条件特性相结合，提出了自适应草稿序列生成、草稿序列回收和两遍稀疏令牌树生成等针对性策略。这解决了现有推测解码在ASR领域加速有限的问题，显著提升了LLM-based ASR的实时性能，对于推动ASR技术在实际应用中的落地具有重要意义。"}}
{"id": "2507.18612", "title": "Approximate SMT Counting Beyond Discrete Domains", "authors": ["Arijit Shaw", "Kuldeep S. Meel"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      To be published in the proceedings of Design Automation Conference (DAC) 2025", "url": "http://arxiv.org/abs/2507.18612v1", "summary": "Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,\nsolving complex formulas across discrete and continuous domains. Recent\nprogress in propositional model counting motivates extending SMT capabilities\ntoward model counting, especially for hybrid SMT formulas. Existing approaches,\nlike bit-blasting, are limited to discrete variables, highlighting the\nchallenge of counting solutions projected onto the discrete domain in hybrid\nformulas.\n  We introduce pact, an SMT model counter for hybrid formulas that uses\nhashing-based approximate model counting to estimate solutions with theoretical\nguarantees. pact makes a logarithmic number of SMT solver calls relative to the\nprojection variables, leveraging optimized hash functions. pact achieves\nsignificant performance improvements over baselines on a large suite of\nbenchmarks. In particular, out of 14,202 instances, pact successfully finished\non 603 instances, while Baseline could only finish on 13 instances.", "comment": "To be published in the proceedings of Design Automation Conference\n  (DAC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.18612v1", "cate": "cs.LO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "离散域之外的近似SMT计数", "tldr": "pact是一种新的近似SMT模型计数器，专门用于混合SMT公式，通过哈希技术显著提高了性能，超越了现有方法在处理离散变量限制上的不足。", "motivation": "现有SMT计数方法（如位爆炸）仅限于离散变量，难以处理混合SMT公式中将解决方案投影到离散域的计数问题。", "method": "本文引入了pact，一个用于混合公式的SMT模型计数器，它使用基于哈希的近似模型计数来估计具有理论保证的解决方案。pact通过对投影变量进行对数次的SMT求解器调用，并利用优化的哈希函数。", "result": "pact在大量基准测试中比基线方法取得了显著的性能改进。在14,202个实例中，pact成功完成了603个实例，而基线方法仅完成了13个实例。", "conclusion": "pact成功地将近似SMT计数扩展到离散域之外的混合公式，并通过其高效的方法显著提高了求解能力。", "translation": "可满足性模理论（SMT）求解器推动了自动化推理的发展，解决了离散和连续域中的复杂公式。命题模型计数方面的最新进展促使将SMT能力扩展到模型计数，特别是对于混合SMT公式。现有的方法，如位爆炸，仅限于离散变量，这突出了在混合公式中将解决方案投影到离散域进行计数的挑战。\n我们引入了pact，一个用于混合公式的SMT模型计数器，它使用基于哈希的近似模型计数来估计具有理论保证的解决方案。pact相对于投影变量进行对数次的SMT求解器调用，利用优化的哈希函数。pact在大量基准测试中比基线方法取得了显著的性能改进。特别是，在14,202个实例中，pact成功完成了603个实例，而基线方法只能完成13个实例。", "summary": "本文介绍了一种名为pact的SMT模型计数器，专门用于解决混合SMT公式中的近似模型计数问题。针对现有方法在离散变量上的局限性，pact采用基于哈希的近似计数方法，通过对数次SMT求解器调用和优化哈希函数，显著提升了在混合公式上的求解能力和性能，在测试中远超基线方法。", "keywords": "SMT, 模型计数, 混合公式, 近似计数, 哈希", "comments": "pact的创新在于将近似模型计数扩展到混合SMT公式，克服了传统方法在处理离散域之外变量时的限制。其基于哈希的方法和对数次求解器调用显著提高了效率和解决问题的范围，对于SMT求解和模型计数领域具有重要意义。"}}
{"id": "2502.18679", "title": "Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data", "authors": ["Siqi Guo", "Ilgee Hong", "Vicente Balmaseda", "Changlong Yu", "Liang Qiu", "Xin Liu", "Haoming Jiang", "Tuo Zhao", "Tianbao Yang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      18 pages, 7 figures", "url": "http://arxiv.org/abs/2502.18679v3", "summary": "Supervised fine-tuning (SFT) has become a crucial step for aligning\npretrained large language models (LLMs) using supervised datasets of\ninput-output pairs. However, despite being supervised, SFT is inherently\nlimited by its generative training objective. To address its limitations, the\nexisting common strategy is to follow SFT with a separate phase of preference\noptimization (PO), which relies on either human-labeled preference data or a\nstrong reward model to guide the learning process. In this paper, we address\nthe limitations of SFT by exploring one of the most successful techniques in\nconventional supervised learning: discriminative learning. We introduce\nDiscriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates\nthe burden of collecting human-labeled preference data or training strong\nreward models. Unlike SFT that employs a generative approach and overlooks\nnegative data, DFT adopts a discriminative paradigm that increases the\nprobability of positive answers while suppressing potentially negative ones,\naiming for data prediction instead of token prediction. Our contributions\ninclude: (i) a discriminative probabilistic framework for fine-tuning LLMs by\nexplicitly modeling the discriminative likelihood of an answer among all\npossible outputs given an input; (ii) efficient algorithms to optimize this\ndiscriminative likelihood; and (iii) extensive experiments demonstrating DFT's\neffectiveness, achieving performance better than SFT and comparable to if not\nbetter than SFT$\\rightarrow$PO. The code can be found at\nhttps://github.com/Optimization-AI/DFT.", "comment": "18 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2502.18679v3", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-07-23", "AI": {"title_translation": "生成式大型语言模型在无奖励模型和人类偏好数据下的判别式微调", "tldr": "本文提出判别式微调（DFT），一种改进的监督微调（SFT）变体，旨在解决SFT的局限性并避免依赖奖励模型或人类偏好数据。DFT通过判别式学习范式，在无需额外偏好数据的情况下，实现了优于SFT且媲美甚至超越SFT+PO的性能。", "motivation": "现有的大型语言模型（LLM）监督微调（SFT）受其生成式训练目标的限制。为解决此问题，通常采用偏好优化（PO），但这需要大量人工标注偏好数据或强大的奖励模型，带来了高昂的成本和数据收集负担。", "method": "本文提出判别式微调（DFT），作为监督微调（SFT）的改进变体。DFT采用判别式范式，不同于SFT的生成式方法，它通过显式建模给定输入下所有可能输出中答案的判别式似然，增加正面答案的概率并抑制潜在的负面答案，目标是数据预测而非token预测。论文还提出了优化此判别式似然的有效算法。", "result": "广泛的实验证明了DFT的有效性，其性能优于SFT，并且与SFT结合偏好优化（SFT→PO）的方法相比，性能相当甚至更优。", "conclusion": "DFT提供了一种无需奖励模型和人类偏好数据即可对生成式大型语言模型进行判别式微调的有效方法，成功解决了SFT的局限性并减轻了数据收集负担。", "translation": "监督微调（SFT）已成为使用输入-输出对的监督数据集对预训练大型语言模型（LLM）进行对齐的关键步骤。然而，尽管是监督式的，SFT本质上受到其生成式训练目标的限制。为了解决其局限性，现有的常见策略是在SFT之后进行独立的偏好优化（PO）阶段，这依赖于人工标注的偏好数据或强大的奖励模型来指导学习过程。在本文中，我们通过探索传统监督学习中最成功的技术之一：判别式学习，来解决SFT的局限性。我们引入了判别式微调（DFT），一种SFT的改进变体，它减轻了收集人工标注偏好数据或训练强大奖励模型的负担。与采用生成式方法并忽略负面数据的SFT不同，DFT采用判别式范式，增加正面答案的概率，同时抑制潜在的负面答案，旨在进行数据预测而非token预测。我们的贡献包括：(i) 一个用于微调LLM的判别式概率框架，通过显式建模给定输入下所有可能输出中答案的判别式似然；(ii) 优化此判别式似然的有效算法；(iii) 大量实验证明了DFT的有效性，其性能优于SFT，并且与SFT→PO相比，性能相当甚至更优。代码可在https://github.com/Optimization-AI/DFT找到。", "summary": "本文提出判别式微调（DFT），一种改进的监督微调（SFT）变体，旨在解决SFT在生成式训练目标上的局限性，并避免依赖奖励模型或人类偏好数据。DFT采用判别式范式，通过显式建模答案的判别式似然，增加正面答案概率并抑制负面答案，实现数据预测。实验结果表明，DFT的性能优于SFT，并可媲美甚至超越结合偏好优化的SFT方法。", "keywords": "判别式微调, 大型语言模型, 监督微调, 偏好优化, 无奖励模型", "comments": "该论文的创新点在于将传统监督学习中的判别式学习思想引入到大型生成式语言模型的微调中，提供了一种无需昂贵的人类偏好数据或奖励模型即可提升模型对齐效果的有效途径。这对于降低LLM微调的成本和复杂性具有重要意义，尤其是在数据标注资源有限的场景下。"}}
{"id": "2507.17859", "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17859v1", "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17859v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "FishDet-M：一个用于鲁棒鱼类检测和CLIP引导模型选择的统一大规模基准，适用于多样水生视觉领域", "tldr": "FishDet-M是一个大规模统一基准，用于在多样水生环境中进行鱼类检测和CLIP引导的模型选择，解决了数据集碎片化和评估不一致的问题。", "motivation": "水下图像中准确的鱼类检测对于生态监测、水产养殖自动化和机器人感知至关重要。然而，实际部署受到数据集碎片化、异构成像条件和不一致评估协议的限制。", "method": "本文提出了FishDet-M，一个最大的统一鱼类检测基准，包含13个公开可用的数据集，涵盖海洋、半咸水、遮挡和水族馆等多样水生环境。所有数据都使用COCO风格的边界框和分割掩码进行标准化标注。系统地基准测试了28种当代目标检测模型，包括YOLOv8到YOLOv12系列、基于R-CNN的检测器和基于DETR的模型。评估使用mAP、mAP@50、mAP@75以及尺度特定分析（APS、APM、APL）和推理性能分析（延迟和参数计数）。此外，引入了一个基于CLIP的模型选择框架，利用视觉-语言对齐来动态识别每个输入图像最语义合适的检测器，实现零样本选择。", "result": "结果突出显示了在FishDet-M上训练的不同模型之间检测性能的差异，以及不同架构模型在准确性和效率之间的权衡。CLIP-based模型选择框架在不需要集成计算的情况下实现了高性能，为实时应用提供了可扩展的解决方案。", "conclusion": "FishDet-M为复杂水生场景中的目标检测评估建立了一个标准化且可复现的平台。所有数据集、预训练模型和评估工具均公开可用，以促进水下计算机视觉和智能海洋系统领域的未来研究。", "translation": "水下图像中准确的鱼类检测对于生态监测、水产养殖自动化和机器人感知至关重要。然而，实际部署受到数据集碎片化、异构成像条件和不一致评估协议的限制。为了解决这些问题，我们提出了FishDet-M，这是最大的统一鱼类检测基准，包含13个公开可用的数据集，涵盖海洋、半咸水、遮挡和水族馆等多样水生环境。所有数据都使用COCO风格的边界框和分割掩码进行标准化标注，从而实现一致和可扩展的跨域评估。我们系统地基准测试了28种当代目标检测模型，涵盖YOLOv8到YOLOv12系列、基于R-CNN的检测器和基于DETR的模型。评估使用标准指标，包括mAP、mAP@50和mAP@75，以及尺度特定分析（APS、APM、APL）和推理性能分析（延迟和参数计数）。结果突出显示了在FishDet-M上训练的不同模型之间检测性能的差异，以及不同架构模型在准确性和效率之间的权衡。为了支持自适应部署，我们引入了一个基于CLIP的模型选择框架，利用视觉-语言对齐来动态识别每个输入图像最语义合适的检测器。这种零样本选择策略在不需要集成计算的情况下实现了高性能，为实时应用提供了可扩展的解决方案。FishDet-M为复杂水生场景中的目标检测评估建立了一个标准化且可复现的平台。所有数据集、预训练模型和评估工具均公开可用，以促进水下计算机视觉和智能海洋系统领域的未来研究。", "summary": "本文提出了FishDet-M，一个统一的大规模鱼类检测基准，旨在解决水下图像鱼类检测领域中数据集碎片化和评估协议不一致的问题。该基准整合了13个多样水生环境数据集，并采用COCO风格的标注。研究系统地评估了28种主流目标检测模型，并分析了它们的性能与效率权衡。此外，论文引入了一个基于CLIP的零样本模型选择框架，能够根据图像内容动态选择最合适的检测器，从而在不增加计算负担的情况下提高实时应用的性能和适应性。FishDet-M为水下计算机视觉研究提供了标准化平台。", "keywords": "鱼类检测, 大规模基准, CLIP, 模型选择, 水下图像", "comments": "FishDet-M的创新之处在于其构建了一个前所未有的大规模统一鱼类检测基准，整合了多样化的水生环境数据并进行了标准化标注。这解决了长期困扰水下计算机视觉领域的数据碎片化和评估不一致问题。CLIP引导的模型选择框架是一个重要的突破，它允许根据图像的语义内容自适应地选择模型，这对于实际部署具有很高的实用价值，尤其是在资源受限或需要实时响应的场景中。该工作通过公开数据集和工具，为未来研究奠定了坚实基础，有望推动水下智能系统和生态监测技术的发展。"}}
{"id": "2507.16068", "title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "authors": ["Zhehui Huang", "Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures", "url": "http://arxiv.org/abs/2507.16068v2", "summary": "Multi-robot coordination has traditionally relied on a mission-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB transforms natural language (NL)\nmission descriptions into executable Python code for multi-robot systems\nthrough two core modules: (1) Mission Analysis, which parses mission\ndescriptions into behavior trees, and (2) Code Generation, which leverages the\nbehavior tree and a structured knowledge base to generate robot control code.\nWe further introduce a dataset of natural language mission descriptions to\nsupport development and benchmarking. Experiments in both simulation and\nreal-world environments demonstrate that LAN2CB enables robust and flexible\nmulti-robot coordination from natural language, significantly reducing manual\nengineering effort and supporting broad generalization across diverse mission\ntypes. Website: https://sites.google.com/view/lan-cb", "comment": "9 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.16068v2", "cate": "cs.RO", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "基于大型语言模型的多机器人团队组合协调", "tldr": "LAN2CB是一个利用大型语言模型将自然语言任务描述转化为多机器人系统可执行代码的框架，旨在简化和泛化多机器人协调过程。", "motivation": "传统的多机器人协调依赖于任务特定和专家驱动的流程，需要将自然语言任务描述手动转化为数学公式、算法设计和可执行代码，这导致过程劳动密集、非专家难以使用且对任务需求变化不灵活。", "method": "本文提出了LAN2CB（Language to Collective Behavior）框架，利用大型语言模型（LLMs）简化和泛化多机器人协调流程。LAN2CB通过两个核心模块将自然语言任务描述转化为多机器人系统的可执行Python代码：1) 任务分析：将任务描述解析为行为树；2) 代码生成：利用行为树和结构化知识库生成机器人控制代码。此外，还引入了一个自然语言任务描述数据集以支持开发和基准测试。", "result": "在仿真和真实环境中的实验表明，LAN2CB能够实现从自然语言进行鲁棒和灵活的多机器人协调，显著减少了手动工程工作，并支持跨不同任务类型的广泛泛化。", "conclusion": "LAN2CB框架通过利用大型语言模型，成功地将自然语言任务描述转化为多机器人系统的可执行代码，从而实现了更鲁棒、灵活且高效的多机器人协调，大大降低了人工工程量并提升了泛化能力。", "translation": "多机器人协调传统上依赖于任务特定和专家驱动的流程，其中自然语言任务描述由领域专家手动转换为数学公式、算法设计和可执行代码。这种传统流程劳动密集、非专家难以使用，且对任务需求变化不灵活。本文提出了一种名为LAN2CB（Language to Collective Behavior）的新型框架，该框架利用大型语言模型（LLMs）来简化和泛化多机器人协调流程。LAN2CB通过两个核心模块将自然语言（NL）任务描述转化为多机器人系统的可执行Python代码：(1) 任务分析，将任务描述解析为行为树；(2) 代码生成，利用行为树和结构化知识库生成机器人控制代码。我们进一步引入了一个自然语言任务描述数据集，以支持开发和基准测试。在仿真和真实环境中的实验表明，LAN2CB能够实现从自然语言进行鲁棒和灵活的多机器人协调，显著减少了手动工程工作，并支持跨不同任务类型的广泛泛化。网站：https://sites.google.com/view/lan-cb", "summary": "LAN2CB是一个利用大型语言模型（LLMs）简化多机器人协调的创新框架。它将自然语言任务描述转化为可执行的机器人控制代码，通过任务分析模块生成行为树，并结合知识库进行代码生成。该方法旨在解决传统协调流程劳动密集、专家依赖且不灵活的问题。实验证明，LAN2CB在仿真和真实环境中都能实现鲁棒、灵活的协调，有效减少了人工工作量，并具有良好的泛化能力。", "keywords": "大型语言模型, 多机器人协调, 自然语言处理, 行为树, 代码生成", "comments": "本文的创新点在于将大型语言模型应用于多机器人协调领域，自动化了传统上由专家手动完成的复杂过程，极大地提高了效率和可访问性。其模块化设计（任务分析和代码生成）清晰且有效，为非专家用户提供了通过自然语言控制多机器人团队的能力，具有重要的实际应用价值。"}}
{"id": "2507.17758", "title": "Weaving the Future: Generative AI and the Reimagining of Fashion Design", "authors": ["Pierre-Marie Chauvin", "Angèle Merlin", "Xavier Fresquet", "Hugo Caselles-Dupré", "Benjamin Simmenauer", "Mathieu de Fayet"], "categories": ["cs.CY", "cs.HC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17758v1", "summary": "This paper explores the integration of generative AI into the fashion design\nprocess. Drawing on insights from the January 2025 seminar ``Tisser le futur,''\nit investigates how AI reshapes creative workflows, from ideation to\nprototyping, while interrogating the ethical, aesthetic, and labor\nimplications. The paper highlights co-creative dynamics between humans and\nmachines, the potential for aesthetic innovation, and the environmental and\ncultural challenges of algorithmic design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17758v1", "cate": "cs.CY", "date": "2025-05-07", "updated": "2025-05-07", "AI": {"title_translation": "编织未来：生成式AI与时尚设计的重塑", "tldr": "本文探讨了生成式AI如何融入时尚设计，并重塑其创意流程，同时审视其伦理、美学和劳动力影响，以及人机协同创作和算法设计的挑战。", "motivation": "本文旨在探讨生成式AI在时尚设计过程中的整合，并研究AI如何重塑创意工作流程，同时审视其伦理、美学和劳动力方面的影响。", "method": "本文借鉴了2025年1月研讨会“Tisser le futur”的见解，对生成式AI在时尚设计中的应用进行了探讨。", "result": "研究结果强调了人机协同创作的动态、美学创新的潜力，以及算法设计带来的环境和文化挑战。", "conclusion": "本文探讨了生成式AI与时尚设计的整合及其对创意工作流程的重塑，并分析了其带来的伦理、美学、劳动力、环境和文化等多方面影响。", "translation": "本文探讨了生成式AI在时尚设计过程中的整合。借鉴2025年1月研讨会“编织未来”的见解，本文研究了AI如何重塑从构思到原型制作的创意工作流程，同时审视了其伦理、美学和劳动力方面的影响。本文强调了人机协同创作的动态、美学创新的潜力，以及算法设计带来的环境和文化挑战。", "summary": "本文探讨了生成式AI与时尚设计的融合，分析了其对从构思到原型制作的创意工作流程的影响。文章深入讨论了其伦理、美学和劳动力层面的含义，并突出了人机协同创作、美学创新的潜力以及算法设计所面临的环境和文化挑战。", "keywords": "生成式AI, 时尚设计, 创意工作流, 人机协同, 算法设计", "comments": "本文的创新之处在于其全面探讨了生成式AI在时尚领域的应用，不仅关注技术整合，更深入分析了其深远的伦理、美学和劳动力影响，对理解技术与创意产业的交叉点具有重要意义。"}}
{"id": "2507.18334", "title": "Improving Bird Classification with Primary Color Additives", "authors": ["Ezhini Rasendiran R", "Chandresh Kumar Maurya"], "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages (Accepted to Interspeech 2025)", "url": "http://arxiv.org/abs/2507.18334v1", "summary": "We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.", "comment": "5 pages (Accepted to Interspeech 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18334v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "采用原色添加剂改进鸟类分类", "tldr": "通过将频率信息编码为原色添加到声谱图中，显著提高了鸟类鸣声分类的准确性，超越了现有最佳模型。", "motivation": "鸟类鸣声分类面临环境噪音、重叠鸣叫和标签缺失等挑战，现有模型在低信噪比或多物种录音下表现不佳。", "method": "提出通过将频率信息嵌入到声谱图中，使用原色添加剂来增强物种区分度。该方法将鸟类的音高模式、速度和重复性（统称为“主题”）可视化，并应用于深度学习模型。", "result": "提出的方法在F1分数上提高了7.3%，ROC-AUC提高了6.2%，CMAP提高了6.6%，统计学上显著优于未着色的模型，并超越了BirdCLEF 2024冠军。", "conclusion": "通过色彩化整合频率信息的方法是有效的，能显著提高鸟类鸣声分类的准确性。", "translation": "我们解决了使用鸟类鸣声录音对鸟类物种进行分类的问题，这是一项由于环境噪音、重叠发声和标签缺失而具有挑战性的任务。现有模型在低信噪比或多物种录音方面表现不佳。我们假设鸟类可以通过可视化其音高模式、速度和重复性（统称为主题）进行分类。应用于声谱图图像的深度学习模型有所帮助，但物种间相似的主题会导致混淆。为了缓解这种情况，我们使用原色添加剂将频率信息嵌入到声谱图中。这增强了物种区分度并提高了分类准确性。我们的实验表明，所提出的方法比未着色的模型取得了统计学上显著的增益，并超越了BirdCLEF 2024冠军，F1提高了7.3%，ROC-AUC提高了6.2%，CMAP提高了6.6%。这些结果证明了通过色彩化整合频率信息的有效性。", "summary": "这篇论文提出了一种改进鸟类鸣声分类的方法，通过将频率信息编码为原色添加到声谱图中。针对现有模型在噪声和多物种录音中表现不佳的问题，该方法旨在通过增强声谱图中的物种区分度来提高分类准确性。实验结果表明，该方法在多项指标上显著优于无色彩化的模型，并超越了BirdCLEF 2024冠军，证明了色彩化整合频率信息的有效性。", "keywords": "鸟类分类, 声谱图, 频率信息, 原色添加剂, 深度学习", "comments": "这项研究的创新点在于将频率信息通过原色添加剂的方式嵌入到声谱图中，从而在视觉上增强了不同鸟类物种鸣声主题的区分度。这种方法有效地解决了传统声谱图在相似鸣声模式下容易混淆的问题，为声学分类领域提供了一个新颖且有效的特征增强策略。"}}
{"id": "2507.17980", "title": "Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations", "authors": ["Elyar Tourani", "Brian J. Edwards", "Bamin Khomami"], "categories": ["physics.comp-ph", "cs.LG"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "Comments:      30 pages, 8 figures, 1 table", "url": "http://arxiv.org/abs/2507.17980v1", "summary": "Currently, identification of crystallization pathways in polymers is being\ncarried out using molecular simulation-based data on a preset cut-off point on\na single order parameter (OP) to define nucleated or crystallized regions.\nAside from sensitivity to cut-off, each of these OPs introduces its own\nsystematic biases. In this study, an integrated machine learning workflow is\npresented to accurately quantify crystallinity in polymeric systems using\natomistic molecular dynamics data. Each atom is represented by a\nhigh-dimensional feature vector that combines geometric, thermodynamic-like,\nand symmetry-based descriptors. Low dimensional embeddings are employed to\nexpose latent structural fingerprints within atomic environments. Subsequently,\nunsupervised clustering on the embeddings identified crystalline and amorphous\natoms with high fidelity. After generating high quality labels with\nmultidimensional data, we use supervised learning techniques to identify a\nminimal set of order parameters that can fully capture this label. Various\ntests were conducted to reduce the feature set, demonstrating that using only\nthree order parameters is sufficient to recreate the crystallization labels.\nBased on these observed OPs, the crystallinity index (C-index) is defined as\nthe logistic regression model's probability of crystallinity, remaining bimodal\nthroughout the process and achieving over 0.98 classification performance\n(AUC). Notably, a model trained on one or a few snapshots enables efficient\non-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal\nC-index fit evolves during various stages of crystallization, supporting the\nhypothesis that entropy dominates early nucleation, while symmetry gains\nrelevance later. This workflow provides a data-driven strategy for OP selection\nand a metric to monitor structural transformations in large-scale polymer\nsimulations.", "comment": "30 pages, 8 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.17980v1", "cate": "physics.comp-ph", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "用于分析高维序参数空间的机器学习工作流：分子动力学模拟聚合物结晶的案例研究", "tldr": "本研究提出了一种集成的机器学习工作流，用于从分子动力学模拟数据中准确量化聚合物的结晶度，并识别出少量关键序参数。", "motivation": "目前聚合物结晶路径的识别依赖于基于单一序参数预设截止点的方法，这种方法对截止点敏感且每个序参数都引入系统偏差。因此，需要一种更准确、数据驱动的方法来量化结晶度并选择序参数。", "method": "本研究提出了一种集成的机器学习工作流。首先，每个原子由结合了几何、热力学和对称性描述符的高维特征向量表示。接着，使用低维嵌入来揭示原子环境中的潜在结构指纹。随后，对嵌入进行无监督聚类以高保真度识别结晶和非晶原子。在生成高质量多维数据标签后，利用监督学习技术识别出能够完全捕获这些标签的最小序参数集。通过特征集缩减测试，发现仅需三个序参数即可重建结晶标签。最后，定义结晶度指数（C-index）为逻辑回归模型预测结晶的概率。", "result": "研究发现仅使用三个序参数就足以重建结晶标签。定义的结晶度指数（C-index）在整个过程中保持双峰分布，并实现了超过0.98的分类性能（AUC）。值得注意的是，在一个或几个快照上训练的模型能够实现高效的结晶度实时计算。此外，模型还展示了最佳C-index拟合在结晶不同阶段的演变，支持了熵在早期成核中占主导地位，而对称性在后期获得相关性的假设。", "conclusion": "该工作流为序参数选择提供了一种数据驱动的策略，并提供了一个监测大规模聚合物模拟中结构转变的指标。它能够准确量化聚合物结晶度并识别关键序参数，支持对结晶机制的深入理解。", "translation": "目前，聚合物结晶路径的识别是通过基于分子模拟数据，在单一序参数（OP）上预设截止点来定义成核或结晶区域进行的。除了对截止点的敏感性之外，每个序参数都引入了其自身的系统偏差。在这项研究中，提出了一种集成的机器学习工作流，用于使用原子分子动力学数据准确量化聚合物系统中的结晶度。每个原子由结合了几何、热力学类和基于对称性的描述符的高维特征向量表示。采用低维嵌入来揭示原子环境中的潜在结构指纹。随后，对嵌入进行无监督聚类，以高保真度识别结晶和非晶原子。在生成具有多维数据的高质量标签后，我们使用监督学习技术来识别可以完全捕获此标签的最小序参数集。进行了各种测试以减少特征集，结果表明仅使用三个序参数就足以重建结晶标签。基于这些观察到的序参数，结晶度指数（C-index）被定义为逻辑回归模型预测结晶的概率，在整个过程中保持双峰分布，并实现了超过0.98的分类性能（AUC）。值得注意的是，在一个或几个快照上训练的模型能够实现高效的结晶度实时计算。最后，我们展示了最佳C-index拟合在结晶不同阶段的演变，支持了熵在早期成核中占主导地位，而对称性在后期获得相关性的假设。该工作流为OP选择提供了一种数据驱动的策略，并提供了一个监测大规模聚合物模拟中结构转变的指标。", "summary": "本研究提出了一种创新的机器学习工作流，用于克服传统方法在聚合物结晶分析中的局限性。通过结合高维特征向量、低维嵌入、无监督聚类和监督学习，该工作流能够从分子动力学模拟数据中高精度地识别结晶和非晶原子，并确定仅需三个关键序参数即可准确量化结晶度。定义的结晶度指数（C-index）表现出卓越的分类性能，并支持实时计算。该方法为理解聚合物结晶机制提供了数据驱动的策略和有效的监测工具。", "keywords": "聚合物结晶, 机器学习, 分子动力学模拟, 序参数, 结晶度指数", "comments": "这项研究的创新之处在于其集成化的机器学习工作流，它摆脱了传统上依赖单一预设截止点序参数的局限性。通过利用高维特征、降维和多阶段机器学习（无监督与监督学习相结合），该方法能够更准确、更鲁棒地识别和量化聚合物结晶度。特别值得关注的是，它能够将复杂的结晶过程简化为少数几个关键序参数，并实现高效的实时计算，这对于大规模分子模拟具有重要意义。此外，对熵和对称性在结晶不同阶段作用的探讨也为理解聚合物结晶机制提供了新的视角。"}}
{"id": "2503.11034", "title": "Inverse scattering for Schrödinger equation in the frequency domain via data-driven reduced order modeling", "authors": ["Andreas Tataris", "Tristan van Leeuwen", "Alexander V. Mamonov"], "categories": ["math.NA", "cs.NA", "65M32, 41A20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.11034v2", "summary": "In this paper we develop a numerical method for solving an inverse scattering\nproblem of estimating the scattering potential in a Schr\\\"{o}dinger equation\nfrom frequency domain measurements based on reduced order models (ROM). The ROM\nis a projection of Schr\\\"{o}dinger operator onto a subspace spanned by its\nsolution snapshots at certain wavenumbers. Provided the measurements are\nperformed at these wavenumbers, the ROM can be constructed in a data-driven\nmanner from the measurements on a surface surrounding the scatterers. Once the\nROM is computed, the scattering potential can be estimated using non-linear\noptimization that minimizes the ROM misfit. Such an approach typically\noutperforms the conventional methods based on data misfit minimization. We\ndevelop two variants of ROM-based algorithms for inverse scattering and test\nthem on a synthetic example in two spatial dimensions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.11034v2", "cate": "math.NA", "date": "2025-03-14", "updated": "2025-07-23", "AI": {"title_translation": "薛定谔方程在频域中的逆散射问题的数据驱动降阶模型方法", "tldr": "本文提出了一种基于数据驱动降阶模型（ROM）的数值方法，用于解决频域薛定谔方程的逆散射问题，以估计散射势。", "motivation": "估计薛定谔方程中的散射势，解决频域测量下的逆散射问题。", "method": "开发了一种基于降阶模型（ROM）的数值方法。ROM是将薛定谔算子投影到由特定波数下的解快照张成的子空间上。ROM通过围绕散射体的表面测量数据驱动构建。计算ROM后，通过最小化ROM不匹配的非线性优化来估计散射势。开发了两种基于ROM的逆散射算法变体。", "result": "该方法通常优于基于数据不匹配最小化的传统方法。在二维合成示例上进行了测试。", "conclusion": "Not mentioned in abstract", "translation": "在本文中，我们开发了一种数值方法，用于解决基于降阶模型（ROM）从频域测量中估计薛定谔方程中散射势的逆散射问题。ROM是将薛定谔算子投影到由其在某些波数下的解快照张成的子空间上。如果在这些波数下进行测量，ROM可以从散射体周围表面上的测量数据驱动构建。一旦计算出ROM，就可以使用最小化ROM不匹配的非线性优化来估计散射势。这种方法通常优于基于数据不匹配最小化的传统方法。我们开发了两种基于ROM的逆散射算法变体，并在二维空间中的合成示例上进行了测试。", "summary": "本文提出了一种创新的数据驱动降阶模型（ROM）方法，用于解决频域薛定谔方程的逆散射问题，以估计散射势。该方法通过将薛定谔算子投影到解快照子空间来构建ROM，并利用非线性优化最小化ROM不匹配来估计势。研究表明，该方法在性能上优于传统的数据不匹配最小化方法，并在二维合成示例中得到了验证。", "keywords": "逆散射, 薛定谔方程, 频域, 降阶模型, 数据驱动", "comments": "该论文通过引入数据驱动的降阶模型（ROM）方法来解决逆散射问题，具有创新性。与传统方法相比，其性能提升表明了该方法的潜力。"}}
{"id": "2507.18206", "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.18206v1", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.18206v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MoRPI-PINN：一种用于移动机器人纯惯性导航的物理信息框架", "tldr": "MoRPI-PINN是一个结合物理定律的神经网络框架，用于在无卫星/摄像头环境下，通过纯惯性传感器实现移动机器人高精度导航，有效解决了惯性漂移问题，并在真实实验中实现了超过85%的精度提升。", "motivation": "移动机器人在卫星导航或摄像头不可用的情况下，需要精确导航以实现完全自主。然而，仅依靠惯性传感器会导致导航解决方案因传感器固有的噪声和误差项而产生漂移。", "method": "本文提出了MoRPI-PINN，一个物理信息神经网络框架，用于精确的基于惯性的移动机器人导航。通过在训练过程中嵌入物理定律和约束，MoRPI-PINN能够提供准确和鲁棒的导航解决方案。", "result": "通过真实世界实验，与现有方法相比，MoRPI-PINN的精度提高了85%以上。MoRPI-PINN是一种轻量级方法，甚至可以在边缘设备上实现，并应用于任何典型的移动机器人应用。", "conclusion": "MoRPI-PINN提供了一种准确、鲁棒、轻量级且可在边缘设备上实现的纯惯性导航解决方案，显著提高了移动机器人在无外部辅助条件下的导航精度。", "translation": "移动机器人完全自主的一个基本要求是即使在卫星导航或摄像头不可用的情况下也能实现精确导航。在这些实际情况下，仅依靠惯性传感器会导致导航解决方案因传感器固有的噪声和误差项而产生漂移。缓解漂移的一种新兴解决方案是让机器人进行蛇形滑动运动，以增加惯性信噪比，从而实现移动机器人位置的回归。在这项工作中，我们提出了MoRPI-PINN，作为一个物理信息神经网络框架，用于精确的基于惯性的移动机器人导航。通过在训练过程中嵌入物理定律和约束，MoRPI-PINN能够提供准确和鲁棒的导航解决方案。通过真实世界实验，我们展示了与其它方法相比，精度提高了85%以上。MoRPI-PINN是一种轻量级方法，甚至可以在边缘设备上实现，并用于任何典型的移动机器人应用。", "summary": "MoRPI-PINN是一种新颖的物理信息神经网络框架，旨在解决移动机器人在无卫星或摄像头环境下，纯惯性导航中因传感器噪声导致的漂移问题。该框架通过将物理定律和约束嵌入到训练过程中，显著提高了导航精度和鲁棒性。实验证明，MoRPI-PINN的精度比其他方法提高了85%以上，且其轻量级特性使其适用于边缘设备和各种移动机器人应用。", "keywords": "移动机器人, 惯性导航, 物理信息神经网络, 漂移缓解, 纯惯性", "comments": "MoRPI-PINN的创新之处在于将物理定律融入神经网络训练，以解决纯惯性导航中的漂移问题，这在缺乏外部定位信息的场景下尤为重要。其在精度上的显著提升（超过85%）和轻量级设计（适用于边缘设备）显示了其巨大的实用价值和潜力。"}}
{"id": "2507.18391", "title": "Revisiting LLM Reasoning via Information Bottleneck", "authors": ["Shiye Lei", "Zhihao Cheng", "Kai Jia", "Dacheng Tao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18391v1", "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18391v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过信息瓶颈重新审视大型语言模型推理", "tldr": "本文通过信息瓶颈（IB）原理对大型语言模型（LLM）推理进行了理论表征，并提出了IB感知推理优化（IBRO）框架。IBRO鼓励推理轨迹既能提供最终正确答案的信息，又能推广到不同的提示。作者推导了一个实用的代币级替代目标，并提出了一个高效的近似方法，从而产生了轻量级的IB正则化方法。该方法可以无缝集成到现有的基于RL的后训练框架中，并在多个数学推理基准和RL算法上取得了持续的改进。", "motivation": "现有的通过可验证奖励强化学习（RLVR）提升大型语言模型（LLM）推理能力的方法 largely heuristic and intuition-driven，限制了原则性方法学的发展。", "method": "本文提出了基于信息瓶颈（IB）原理的LLM推理理论表征，并引入了IB感知推理优化（IBRO）框架。该框架鼓励推理轨迹既能提供最终正确答案的信息，又能在不同提示下具有泛化性。作者推导了一个实用的代币级替代目标，并提出了一个高效的近似方法，从而产生了轻量级的IB正则化方法。该技术可以无缝集成到现有基于RL的后训练框架中，无需额外计算开销，仅需一行代码修改。", "result": "在多个数学推理基准和RL算法上验证了IB正则化，结果表明LLM推理性能持续改进。", "conclusion": "通过信息瓶颈原理，本文提出了一种原则性的方法来优化LLM的推理能力，实验证明其能有效提升模型性能。", "translation": "大型语言模型（LLM）最近通过可验证奖励强化学习（RLVR）在推理能力方面取得了显著进展。通过利用简单的基于规则的奖励，RL有效地激励LLM生成扩展的思维链（CoT）推理轨迹，逐步引导它们走向正确的答案。然而，现有方法在很大程度上仍是启发式和直觉驱动的，限制了原则性方法学的发展。在本文中，我们提出了基于信息瓶颈（IB）原理的LLM推理理论表征，引入了IB感知推理优化（IBRO），这是一个鼓励推理轨迹既能提供最终正确答案的信息，又能推广到不同提示的框架。我们推导了一个实用的代币级替代目标，并提出了一个高效的近似方法，从而产生了轻量级的IB正则化方法。该技术可以无缝集成到现有的基于RL的后训练框架中，无需额外计算开销，仅需一行代码修改。在经验上，我们在多个数学推理基准和RL算法上验证了IB正则化，证明了LLM推理性能的持续改进。", "summary": "本文针对大型语言模型（LLM）推理中现有方法缺乏原则性的问题，提出了一种基于信息瓶颈（IB）原理的理论框架——IB感知推理优化（IBRO）。IBRO旨在优化LLM的推理轨迹，使其既能有效传达最终答案信息，又能泛化到不同场景。研究者推导并实现了一种轻量级的IB正则化方法，该方法可无缝集成到现有强化学习（RL）后训练框架中。实验结果表明，该方法在多个数学推理任务中持续提升了LLM的推理性能。", "keywords": "大型语言模型, 信息瓶颈, 推理优化, 强化学习, 思维链", "comments": "该论文的创新点在于将信息瓶颈原理引入到LLM的推理优化中，提供了一个更具原则性的理论框架，而非依赖于启发式方法。其提出的IB正则化方法轻量且易于集成，对现有RL训练流程影响小，但效果显著，具有较高的实用价值。"}}
{"id": "2507.12558", "title": "When Retriever Meets Generator: A Joint Model for Code Comment Generation", "authors": ["Tien P. T. Le", "Anh M. T. Bui", "Huy N. D. Pham", "Alessio Bucaioni", "Phuong T. Nguyen"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      The paper has been peer-reviewed and accepted for publication in the proceedings of the 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2025)", "url": "http://arxiv.org/abs/2507.12558v2", "summary": "Automatically generating concise, informative comments for source code can\nlighten documentation effort and accelerate program comprehension.\nRetrieval-augmented approaches first fetch code snippets with existing comments\nand then synthesize a new comment, yet retrieval and generation are typically\noptimized in isolation, allowing irrelevant neighbors topropagate noise\ndownstream. To tackle the issue, we propose a novel approach named RAGSum with\nthe aim of both effectiveness and efficiency in recommendations. RAGSum is\nbuilt on top offuse retrieval and generation using a single CodeT5 backbone. We\nreport preliminary results on a unified retrieval-generation framework built on\nCodeT5. A contrastive pre-training phase shapes code embeddings for\nnearest-neighbor search; these weights then seed end-to-end training with a\ncomposite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes\ncomment-generation error. More importantly, a lightweight self-refinement loop\nis deployed to polish the final output. We evaluated theframework on three\ncross-language benchmarks (Java, Python, C), and compared it with three\nwell-established baselines. The results show that our approach substantially\noutperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These\nfindings indicate that tightly coupling retrieval and generationcan raise the\nceiling for comment automation and motivateforthcoming replications and\nqualitative developer studies.", "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "pdf_url": "http://arxiv.org/pdf/2507.12558v2", "cate": "cs.SE", "date": "2025-07-16", "updated": "2025-07-24", "AI": {"title_translation": "当检索器遇见生成器：一个用于代码注释生成的联合模型", "tldr": "本文提出RAGSum，一个基于CodeT5的联合检索-生成模型，用于自动生成代码注释，通过对比预训练和复合损失函数，显著优于现有基线。", "motivation": "自动生成简洁、信息丰富的源代码注释可以减轻文档工作量并加速程序理解。现有的检索增强方法通常独立优化检索和生成，导致不相关的近邻传播噪声。", "method": "本文提出了一种名为RAGSum的新方法，旨在实现推荐的有效性和效率。RAGSum建立在单个CodeT5骨干之上，融合了检索和生成。通过对比预训练阶段塑造用于最近邻搜索的代码嵌入，这些权重随后用于端到端训练，并采用复合损失函数，该函数(i)奖励准确的top-k检索；(ii)最小化注释生成错误。此外，还部署了一个轻量级的自细化循环来完善最终输出。", "result": "在Java、Python和C三种跨语言基准测试中，RAGSum与三个成熟的基线方法进行了比较。结果显示，RAGSum在BLEU、METEOR和ROUTE-L指标上显著优于基线。", "conclusion": "这些发现表明，紧密耦合检索和生成可以提高注释自动化的上限，并激励未来的复制和定性开发者研究。", "translation": "自动为源代码生成简洁、信息丰富的注释可以减轻文档工作量并加速程序理解。检索增强方法首先获取带有现有注释的代码片段，然后合成新的注释，但检索和生成通常是独立优化的，这使得不相关的近邻向下游传播噪声。为了解决这个问题，我们提出了一种名为RAGSum的新方法，旨在实现推荐的有效性和效率。RAGSum建立在单个CodeT5骨干之上，融合了检索和生成。我们报告了基于CodeT5构建的统一检索-生成框架的初步结果。一个对比预训练阶段塑造了用于最近邻搜索的代码嵌入；这些权重随后用于端到端训练，并采用复合损失函数，该函数(i)奖励准确的top-k检索；(ii)最小化注释生成错误。更重要的是，部署了一个轻量级的自细化循环来完善最终输出。我们在三个跨语言基准测试（Java、Python、C）上评估了该框架，并与三个成熟的基线方法进行了比较。结果显示，我们的方法在BLEU、METEOR和ROUTE-L方面显著优于基线。这些发现表明，紧密耦合检索和生成可以提高注释自动化的上限，并激励未来的复制和定性开发者研究。", "summary": "本文提出RAGSum，一个新颖的联合检索-生成模型，旨在解决现有代码注释生成方法中检索与生成独立优化导致的噪声传播问题。RAGSum基于CodeT5骨干，通过对比预训练优化代码嵌入，并采用结合检索准确性和生成误差的复合损失函数进行端到端训练，同时引入轻量级自细化循环。在Java、Python和C等跨语言基准测试中，RAGSum在BLEU、METEOR和ROUTE-L等指标上显著优于现有基线，证明了紧密耦合检索和生成在代码注释自动化方面的潜力。", "keywords": "代码注释生成, 检索增强, 联合模型, CodeT5, RAGSum", "comments": "本文提出RAGSum，通过将检索和生成紧密结合在一个CodeT5骨干中，解决了现有检索增强型代码注释生成方法中独立优化导致的噪声问题。其创新点在于采用对比预训练来优化代码嵌入，并设计了结合检索准确性和生成误差的复合损失函数，辅以自细化循环，有效提升了代码注释的质量和效率。该研究为代码注释自动化领域提供了新的思路和强大的基线。"}}
{"id": "2507.18521", "title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning", "authors": ["Zhongtian Sun", "Anoushka Harit", "Alexandra Cristea", "Christl A. Donnelly", "Pietro Liò"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18521v1", "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18521v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GLANCE：用于异质图表示学习的图逻辑注意力网络与聚类增强", "tldr": "GLANCE是一种新的图神经网络框架，通过结合逻辑推理、动态图细化和自适应聚类，解决了传统GNN在异质图上表现不佳的问题，实现了有竞争力的性能。", "motivation": "图神经网络（GNNs）在异质图上表现不佳，因为它们在连接节点特征或类别标签不同时，存在无差别邻居聚合和高阶结构模式融合不足的问题。", "method": "GLANCE（图逻辑注意力网络与聚类增强）是一个新颖的框架，它集成了逻辑引导推理、动态图细化和自适应聚类。具体包括：用于可解释和结构化嵌入的逻辑层、用于图结构去噪的多头注意力机制的边剪枝，以及用于捕获全局模式的聚类机制。", "result": "在Cornell、Texas和Wisconsin等基准数据集上的实验结果表明，GLANCE实现了有竞争力的性能。", "conclusion": "GLANCE为异质图场景提供了鲁棒且可解释的解决方案，该框架轻量、适应性强，并独特地适用于异质图的挑战。", "translation": "图神经网络（GNNs）在从图结构数据中学习方面取得了显著成功，但通常在异质图上表现不佳，即连接节点的特征或类别标签不同。这种局限性源于不加区分的邻居聚合和高阶结构模式融合不足。为了解决这些挑战，我们提出了GLANCE（图逻辑注意力网络与聚类增强），一个新颖的框架，它集成了逻辑引导推理、动态图细化和自适应聚类，以增强图表示学习。GLANCE结合了一个用于可解释和结构化嵌入的逻辑层、用于图结构去噪的多头注意力机制的边剪枝，以及用于捕获全局模式的聚类机制。在Cornell、Texas和Wisconsin等基准数据集上的实验结果表明，GLANCE取得了有竞争力的性能，为异质图场景提供了鲁棒且可解释的解决方案。所提出的框架轻量、适应性强，并独特地适用于异质图的挑战。", "summary": "GLANCE是一个针对异质图表示学习的新型图神经网络框架。它旨在解决传统GNN在异质图上因邻居聚合不当和高阶结构信息不足而导致的性能下降问题。GLANCE通过整合一个逻辑层以实现可解释嵌入、基于多头注意力的边剪枝以进行图结构去噪，以及聚类机制以捕获全局模式，从而增强学习能力。实验证明，GLANCE在多个基准数据集上表现出竞争性，为异质图提供了轻量、适应性强且可解释的解决方案。", "keywords": "异质图, 图神经网络, 图表示学习, 注意力机制, 聚类增强", "comments": "该论文通过引入逻辑推理、动态图细化和自适应聚类，为异质图表示学习提供了一个新颖且全面的解决方案。其创新点在于结合了多个机制来解决异质图的特定挑战，特别是对可解释性和鲁棒性的强调。GLANCE的轻量级和适应性使其在实际应用中具有潜在价值。"}}
{"id": "2507.18483", "title": "A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears", "authors": ["Frauke Wilm", "Luis Carlos Rivera Monroy", "Mathias Öttl", "Lukas Mürdter", "Leonid Mill", "Andreas Maier"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data", "url": "http://arxiv.org/abs/2507.18483v1", "summary": "Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is\nan essential component of reliable malaria diagnosis, especially in developing\ncountries. Deep learning-based object detection methods have demonstrated\nstrong potential for automated Malaria diagnosis, but their adoption is limited\nby the scarcity of datasets with detailed instance-level annotations. In this\nwork, we present an enhanced version of the publicly available NIH malaria\ndataset, with detailed bounding box annotations in COCO format to support\nobject detection training. We validated the revised annotations by training a\nFaster R-CNN model to detect infected and non-infected red blood cells, as well\nas white blood cells. Cross-validation on the original dataset yielded F1\nscores of up to 0.88 for infected cell detection. These results underscore the\nimportance of annotation volume and consistency, and demonstrate that automated\nannotation refinement combined with targeted manual correction can produce\ntraining data of sufficient quality for robust detection performance. The\nupdated annotations set is publicly available via GitHub:\nhttps://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.", "comment": "7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data", "pdf_url": "http://arxiv.org/pdf/2507.18483v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于吉姆萨染色血涂片中恶性疟原虫检测的COCO格式实例级数据集", "tldr": "该论文提出了一个COCO格式的疟疾检测数据集，以支持深度学习模型训练，并展示了良好的检测性能。", "motivation": "准确检测吉姆萨染色血涂片中的恶性疟原虫是可靠疟疾诊断的关键组成部分，尤其是在发展中国家。基于深度学习的目标检测方法在自动化疟疾诊断方面显示出巨大潜力，但由于缺乏具有详细实例级标注的数据集，其应用受到限制。", "method": "该研究提供了一个增强版的NIH疟疾公开数据集，包含COCO格式的详细边界框标注，以支持目标检测训练。作者通过训练Faster R-CNN模型来检测感染和未感染的红细胞以及白细胞，从而验证了修订后的标注。", "result": "在原始数据集上进行交叉验证，感染细胞检测的F1分数高达0.88。", "conclusion": "这些结果强调了标注数量和一致性的重要性，并表明自动化标注优化结合有针对性的手动校正可以生成足够高质量的训练数据，以实现稳健的检测性能。", "translation": "准确检测吉姆萨染色血涂片中的恶性疟原虫是可靠疟疾诊断的关键组成部分，尤其是在发展中国家。基于深度学习的目标检测方法在自动化疟疾诊断方面显示出巨大潜力，但由于缺乏具有详细实例级标注的数据集，其应用受到限制。在这项工作中，我们提出了一个增强版的NIH疟疾公开数据集，该数据集包含COCO格式的详细边界框标注，以支持目标检测训练。我们通过训练一个Faster R-CNN模型来检测感染和未感染的红细胞以及白细胞，从而验证了修订后的标注。在原始数据集上进行的交叉验证显示，感染细胞检测的F1分数高达0.88。这些结果强调了标注数量和一致性的重要性，并表明自动化标注优化结合有针对性的手动校正可以生成足够高质量的训练数据，以实现稳健的检测性能。更新后的标注集已通过GitHub公开提供：https://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco。", "summary": "该论文提出了一个增强的COCO格式数据集，用于吉姆萨染色血涂片中的恶性疟原虫检测，旨在解决深度学习疟疾诊断中缺乏详细实例级标注的问题。该数据集是基于NIH疟疾数据集，并增加了详细的边界框标注。通过训练一个Faster R-CNN模型进行验证，感染细胞检测的F1分数高达0.88。研究强调了标注质量和数量对稳健检测性能的重要性，并公开了更新后的数据集。", "keywords": "恶性疟原虫, 疟疾诊断, 深度学习, COCO数据集, 目标检测", "comments": "该论文的创新之处在于提供了一个急需的、标注良好的COCO格式标准化数据集，这对于推动深度学习在疟疾诊断中的应用至关重要。结合自动化优化和手动校正的标注方法也是一项实用的贡献。其重要性很高，特别是对于发展中国家，准确和自动化的诊断可以显著改善公共卫生结果。"}}
{"id": "2507.18050", "title": "A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation", "authors": ["Xiaoning Jia", "Ruilin Kong", "Guangya Si", "Bilong Shen", "Zhe Ji"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18050v1", "summary": "Rising demand for complex simulations highlights conventional\nengines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)\nadoption.Warped2, a PDES engine leveraging Time Warp synchronization with\nPending Event Set optimization, delivers strong performance, it struggles with\ninherent wargaming limitations: inefficient LP resource allocation during\nsynchronization and unaddressed complex entity interaction patterns. To address\nthese challenges, we present an optimized framework featuring four synergistic\nimprovements: (1) Asynchronous listener threads are introduced to address event\nmonitoring latency in large-scale scenarios, instead of synchronous polling\nmechanisms, (2) METIS-based load rebalancing strategy is incorporated to\naddress the issue of dynamic event allocation during real-world simulation, (3)\nEntity interaction solver with constraint satisfaction mechanisms is designed\nto mitigate state conflicts, and (4) Spatial hashing algorithm to overcome\nO(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.\nExperimental validation through a GridWorld demo demonstrates significant\nenhancements in temporal fidelity and computational efficiency. Benchmark\nresults show our framework achieves 16x acceleration over baseline\nimplementations and maintains 8x speedup over 1-thread configuration across MPI\nand Pthreads implementations.The combined load balancing and LP migration\nstrategy reduces synchronization overhead by 58.18%, with load balancing\naccounting for 57% of the total improvement as the dominant optimization\nfactor. These improvements provide an enhanced solution for PDES implementation\nin large-scale simulation scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18050v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于 Warped2 的大规模分布式并行离散事件仿真引擎，用于兵棋推演仿真", "tldr": "本文提出了一种基于 Warped2 的优化框架，通过引入异步监听线程、基于 METIS 的负载再平衡策略、实体交互求解器和空间哈希算法，显著提升了大规模兵棋推演仿真中的并行离散事件仿真性能和效率。", "motivation": "传统仿真引擎在处理复杂仿真时面临可伸缩性限制，促使并行离散事件仿真（PDES）的应用。然而，现有的 PDES 引擎 Warped2 在兵棋推演仿真中存在固有局限性，包括同步过程中逻辑进程（LP）资源分配效率低下以及复杂实体交互模式未得到有效解决。", "method": "本文提出了一个优化的框架，包含四项协同改进：1) 引入异步监听线程以解决大规模场景中的事件监控延迟问题，取代同步轮询机制；2) 结合基于 METIS 的负载再平衡策略，解决实际仿真中动态事件分配问题；3) 设计了带有约束满足机制的实体交互求解器，以减轻状态冲突；4) 采用空间哈希算法克服大规模最近邻搜索中 O(n^2) 的复杂性瓶颈。", "result": "通过 GridWorld 演示的实验验证表明，该框架在时间保真度和计算效率方面取得了显著提升。基准测试结果显示，该框架比基线实现加速 16 倍，并且在 MPI 和 Pthreads 实现中，相对于单线程配置保持 8 倍的加速。结合负载均衡和 LP 迁移策略，同步开销减少了 58.18%，其中负载均衡是主要优化因素，占总改进的 57%。", "conclusion": "这些改进为大规模仿真场景中的并行离散事件仿真（PDES）实现提供了一个增强的解决方案。", "translation": "对复杂仿真日益增长的需求凸显了传统引擎的可伸缩性限制，从而推动了并行离散事件仿真（PDES）的应用。Warped2 是一种利用时间扭曲（Time Warp）同步和待处理事件集优化的 PDES 引擎，虽然性能强劲，但它在兵棋推演中存在固有限制：同步过程中 LP 资源分配效率低下以及复杂实体交互模式未得到解决。为了应对这些挑战，我们提出了一个优化的框架，包含四项协同改进：(1) 引入异步监听线程以解决大规模场景中的事件监控延迟问题，取代同步轮询机制；(2) 结合基于 METIS 的负载再平衡策略，解决实际仿真中动态事件分配问题；(3) 设计了带有约束满足机制的实体交互求解器，以减轻状态冲突；(4) 采用空间哈希算法克服大规模最近邻搜索中 O(n^2) 的复杂性瓶颈。通过 GridWorld 演示的实验验证表明，在时间保真度和计算效率方面取得了显著提升。基准测试结果显示，我们的框架比基线实现加速 16 倍，并且在 MPI 和 Pthreads 实现中，相对于单线程配置保持 8 倍的加速。结合负载均衡和 LP 迁移策略，同步开销减少了 58.18%，其中负载均衡是主要优化因素，占总改进的 57%。这些改进为大规模仿真场景中的 PDES 实现提供了一个增强的解决方案。", "summary": "本文针对传统并行离散事件仿真（PDES）引擎 Warped2 在大规模兵棋推演仿真中面临的资源分配低效和复杂实体交互处理难题，提出了一套优化的分布式并行仿真框架。该框架通过引入异步监听线程、基于 METIS 的动态负载再平衡策略、带有约束满足机制的实体交互求解器以及空间哈希算法，显著提升了仿真性能。实验结果表明，新框架比基线实现加速 16 倍，并能将同步开销降低 58.18%，为大规模 PDES 应用提供了高效解决方案。", "keywords": "并行离散事件仿真, 兵棋推演, Warped2, 负载均衡, 空间哈希", "comments": "本文针对大规模分布式并行离散事件仿真在兵棋推演领域的具体挑战，提出了一系列创新性的优化措施。特别是在解决 LP 资源分配效率和复杂实体交互模式方面，通过引入异步监听、动态负载均衡、实体交互求解器和空间哈希等多种技术，构建了一个协同工作的优化框架。其显著的性能提升（16倍加速和58.18%同步开销降低）证明了这些方法的有效性。该研究对于推动大规模复杂系统仿真，尤其是兵棋推演类应用的效率和准确性具有重要意义。"}}
{"id": "2507.18004", "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "authors": ["Yusen Peng", "Shuhua Mao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      44 pages,11 figures", "url": "http://arxiv.org/abs/2507.18004v1", "summary": "How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs\nscored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones\n(3.99). Feedback highlights stylistic precision and emotional resonance. These\nresults demonstrate that error-centered, feedback-driven generation enhances\ncreativity, offering a scalable path toward self-evolving, human-aligned\ncreative AI.", "comment": "44 pages,11 figures", "pdf_url": "http://arxiv.org/pdf/2507.18004v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "E.A.R.T.H.：通过生成式AI中的模型错误构建创意演化", "tldr": "E.A.R.T.H.框架通过将模型错误转化为创意资产，显著提升了生成式AI的创造力，并实现了自我演化和人类对齐。", "motivation": "AI如何才能超越模仿，实现真正的创造力？", "method": "本文提出了E.A.R.T.H.框架，这是一个五阶段的生成式管道，通过错误生成、放大、精炼选择、转换和利用反馈将模型生成的错误转化为创意资产。该方法借鉴了认知科学和生成建模，通过结构化提示、语义评分和人机协同评估来实现。它使用LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion等工具，并采用基于新颖性、惊喜度和相关性的复合奖励函数。", "result": "在精炼阶段，创造力分数提高了52.5%（从1.179到1.898，t = -5.56，p < 0.001），最终输出达到2.010，提升了70.4%。精炼后的标语短了48.4%，新颖性增加了40.7%，相关性仅下降4.0%。跨模态测试显示标语与图像的对齐性强（CLIPScore：0.249；BERTScore F1：0.816）。在人工评估中，60%的输出得分>=4.0，其中隐喻性标语（平均4.09）优于字面性标语（3.99）。反馈突出表现出风格精确性和情感共鸣。", "conclusion": "以错误为中心、反馈驱动的生成方式可以增强创造力，为实现自我演化、与人类对齐的创意AI提供了一条可扩展的路径。", "translation": "AI如何才能超越模仿，实现真正的创造力？本文提出了E.A.R.T.H.框架，这是一个五阶段的生成式管道，通过错误生成、放大、精炼选择、转换和利用反馈将模型生成的错误转化为创意资产。借鉴认知科学和生成建模，我们认为“创造潜力隐藏在失败中”，并通过结构化提示、语义评分和人机协同评估将其付诸实践。该管道使用LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion实现，采用基于新颖性、惊喜度和相关性的复合奖励函数。在精炼阶段，创造力分数提高了52.5%（从1.179到1.898，t = -5.56，p < 0.001），最终输出达到2.010，提升了70.4%。精炼后的标语短了48.4%，新颖性增加了40.7%，相关性仅下降4.0%。跨模态测试显示标语与图像的对齐性强（CLIPScore：0.249；BERTScore F1：0.816）。在人工评估中，60%的输出得分>=4.0，其中隐喻性标语（平均4.09）优于字面性标语（3.99）。反馈突出表现出风格精确性和情感共鸣。这些结果表明，以错误为中心、反馈驱动的生成方式可以增强创造力，为实现自我演化、与人类对齐的创意AI提供了一条可扩展的路径。", "summary": "该论文提出了E.A.R.T.H.框架，一个五阶段的生成式AI管道，旨在通过将模型错误转化为创意资产来提升AI的创造力。该框架结合了认知科学和生成建模，利用结构化提示、语义评分和人机协同评估，并集成了多种先进的AI模型。实验结果显示，该方法显著提高了创造力分数和输出质量，证明了以错误为中心、反馈驱动的生成方式在实现自我演化和人类对齐的创意AI方面的潜力。", "keywords": "生成式AI, 创造力, 模型错误, E.A.R.T.H.框架, 人机协同", "comments": "这篇论文提出了一种新颖的视角，即利用模型错误作为创造力的源泉，而非简单地消除错误，这在生成式AI领域具有重要的创新性。其多阶段的E.A.R.T.H.框架结合了多种先进模型和人机协同，提供了一个可扩展的路径，有望推动AI从模仿走向真正的自主创造。将“失败”视为“潜力”的理念尤其引人深思。"}}
{"id": "2507.17848", "title": "Explainable Graph Neural Networks via Structural Externalities", "authors": ["Lijun Wu", "Dong Hao", "Zhiyi Fan"], "categories": ["cs.LG", "cs.AI", "cs.GT", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17848v1", "summary": "Graph Neural Networks (GNNs) have achieved outstanding performance across a\nwide range of graph-related tasks. However, their \"black-box\" nature poses\nsignificant challenges to their explainability, and existing methods often fail\nto effectively capture the intricate interaction patterns among nodes within\nthe network. In this work, we propose a novel explainability framework,\nGraphEXT, which leverages cooperative game theory and the concept of social\nexternalities. GraphEXT partitions graph nodes into coalitions, decomposing the\noriginal graph into independent subgraphs. By integrating graph structure as an\nexternality and incorporating the Shapley value under externalities, GraphEXT\nquantifies node importance through their marginal contributions to GNN\npredictions as the nodes transition between coalitions. Unlike traditional\nShapley value-based methods that primarily focus on node attributes, our\nGraphEXT places greater emphasis on the interactions among nodes and the impact\nof structural changes on GNN predictions. Experimental studies on both\nsynthetic and real-world datasets show that GraphEXT outperforms existing\nbaseline methods in terms of fidelity across diverse GNN architectures ,\nsignificantly enhancing the explainability of GNN models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17848v1", "cate": "cs.LG", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "基于结构外部性的可解释图神经网络", "tldr": "GraphEXT是一个新颖的可解释性框架，它利用合作博弈论和外部性概念，通过量化节点对GNN预测的边际贡献来解释GNN，且在实验中表现优于现有方法。", "motivation": "图神经网络（GNNs）在多种图相关任务中表现出色，但其“黑箱”特性给可解释性带来了巨大挑战。现有方法通常无法有效捕捉网络中节点间复杂的交互模式。", "method": "本文提出了一个名为GraphEXT的新型可解释性框架。该框架利用合作博弈论和社会外部性概念，将图节点划分为联盟，并将原始图分解为独立的子图。通过将图结构作为外部性，并结合外部性下的Shapley值，GraphEXT通过节点在联盟之间转换时对GNN预测的边际贡献来量化节点的重要性。与传统侧重节点属性的Shapley值方法不同，GraphEXT更强调节点间的交互以及结构变化对GNN预测的影响。", "result": "在合成数据集和真实世界数据集上的实验研究表明，GraphEXT在不同GNN架构下的保真度方面优于现有的基线方法。", "conclusion": "GraphEXT显著增强了GNN模型的可解释性。", "translation": "图神经网络（GNNs）在广泛的图相关任务中取得了出色的性能。然而，它们的“黑箱”性质对其可解释性提出了重大挑战，现有方法通常未能有效捕捉网络中节点之间复杂的交互模式。在这项工作中，我们提出了一个新颖的可解释性框架GraphEXT，它利用合作博弈论和社会外部性概念。GraphEXT将图节点划分为联盟，将原始图分解为独立的子图。通过将图结构作为外部性并结合外部性下的Shapley值，GraphEXT通过节点在联盟之间转换时对GNN预测的边际贡献来量化节点的重要性。与主要关注节点属性的传统基于Shapley值的方法不同，我们的GraphEXT更强调节点之间的交互以及结构变化对GNN预测的影响。在合成数据集和真实世界数据集上的实验研究表明，GraphEXT在不同GNN架构下的保真度方面优于现有基线方法，显著增强了GNN模型的可解释性。", "summary": "本文提出了一种名为GraphEXT的新型可解释性框架，旨在解决图神经网络（GNNs）的“黑箱”问题及其现有解释方法无法捕捉复杂节点交互的局限性。GraphEXT基于合作博弈论和社会外部性概念，通过将图节点划分为联盟并利用外部性下的Shapley值来量化节点的重要性，重点关注节点间的交互和结构变化对GNN预测的影响。实验证明，GraphEXT在保真度方面优于现有方法，显著提升了GNN模型的可解释性。", "keywords": "图神经网络, 可解释性, 合作博弈论, Shapley值, 结构外部性", "comments": "这项工作通过引入合作博弈论和结构外部性概念，为GNN的可解释性提供了一个新颖的视角。其创新点在于将图结构视为一种外部性，并利用改进的Shapley值来量化节点在结构变化中的贡献，这与传统方法侧重节点属性不同，更深入地揭示了节点间交互和结构影响。该方法有望为理解复杂GNN决策提供更准确和全面的解释。"}}
{"id": "2507.18099", "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover", "authors": ["Naman Srivastava", "Joel D Joy", "Yash Dixit", "Swarup E", "Rakshit Ramesh"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18099v1", "summary": "Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18099v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "遥感土地利用土地覆盖分割方法比较", "tldr": "本研究评估了先进的土地利用土地覆盖（LULC）测绘技术（DeeplabV3+、动态加权CPS），使用Cartosat MX图像，并通过分析海得拉巴的土地利用变化，展示了其对城市规划的实用性。", "motivation": "土地利用土地覆盖（LULC）测绘对于城市和资源规划至关重要，是发展智慧和可持续城市的关键要素。", "method": "该研究对Cartosat多光谱（MX）传感器图像应用基于查找表（LUT）的大气校正，然后使用监督和半监督学习模型（DeeplabV3+和交叉伪监督（CPS））。CPS模型通过动态加权进行优化，以提高伪标签的可靠性。", "result": "该研究分析了土地利用土地覆盖测绘技术的准确性和实用性。以印度海得拉巴为例，展示了由于快速城市化导致的显著土地利用变化，包括城市蔓延、绿地 shrinking 和工业区扩张。", "conclusion": "这些技术对城市规划者和决策者具有实际应用价值。", "translation": "土地利用土地覆盖（LULC）测绘对于城市和资源规划至关重要，是发展智慧和可持续城市的关键要素之一。本研究评估了先进的LULC测绘技术，重点关注应用于Cartosat多光谱（MX）传感器图像的基于查找表（LUT）的大气校正，随后是用于LULC预测的监督和半监督学习模型。我们探讨了DeeplabV3+和交叉伪监督（CPS）。CPS模型通过动态加权进一步改进，提高了训练期间伪标签的可靠性。这种综合方法分析了LULC测绘技术在各种城市规划应用中的准确性和实用性。以印度海得拉巴为例，说明了快速城市化导致的显著土地利用变化。通过分析Cartosat MX图像随时间的变化，我们强调了城市蔓延、绿地 shrinking 和工业区扩张等变化。这展示了这些技术对城市规划者和决策者的实际效用。", "summary": "本文评估了先进的土地利用土地覆盖（LULC）测绘技术，包括对Cartosat MX图像进行基于查找表（LUT）的大气校正，随后使用DeeplabV3+和动态加权交叉伪监督（CPS）模型。该研究评估了这些方法在城市规划中的准确性和实用性，并以印度海得拉巴为例，展示了它们在识别由快速城市化引起的显著土地利用变化（如城市蔓延和绿地减少）方面的有效性。研究结果强调了这些技术对城市规划者和决策者的实际价值。", "keywords": "土地利用土地覆盖, 遥感, 分割, 深度学习, 城市规划", "comments": "该论文的创新之处在于将基于查找表（LUT）的大气校正与先进的深度学习模型（DeeplabV3+、动态加权CPS）相结合用于土地利用土地覆盖（LULC）测绘。为CPS模型引入动态加权是提高伪标签可靠性的一项显著改进。海得拉巴的案例研究有效展示了这些技术在实际城市规划和土地利用变化监测中的实用性和重要性。"}}
{"id": "2507.18327", "title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm", "authors": ["Jiangjun Peng", "Yisi Luo", "Xiangyong Cao", "Shuang Xu", "Deyu Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 14 figures", "url": "http://arxiv.org/abs/2507.18327v1", "summary": "The nuclear norm (NN) has been widely explored in matrix recovery problems,\nsuch as Robust PCA and matrix completion, leveraging the inherent global\nlow-rank structure of the data. In this study, we introduce a new modified\nnuclear norm (MNN) framework, where the MNN family norms are defined by\nadopting suitable transformations and performing the NN on the transformed\nmatrix. The MNN framework offers two main advantages: (1) it jointly captures\nboth local information and global low-rankness without requiring trade-off\nparameter tuning; (2) Under mild assumptions on the transformation, we provided\nexact theoretical recovery guarantees for both Robust PCA and MC tasks-an\nachievement not shared by existing methods that combine local and global\ninformation. Thanks to its general and flexible design, MNN can accommodate\nvarious proven transformations, enabling a unified and effective approach to\nstructured low-rank recovery. Extensive experiments demonstrate the\neffectiveness of our method. Code and supplementary material are available at\nhttps://github.com/andrew-pengjj/modified_nuclear_norm.", "comment": "15 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.18327v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "超越低秩性：通过改进核范数保证矩阵恢复", "tldr": "本文提出了一种改进的核范数（MNN）框架，通过对变换后的矩阵应用核范数来同时捕获局部和全局低秩信息，并在鲁棒主成分分析（Robust PCA）和矩阵补全（Matrix Completion）任务中提供了精确的理论恢复保证。", "motivation": "核范数（NN）在矩阵恢复问题中已被广泛探索，但现有方法在结合局部信息和全局低秩性时可能需要权衡参数调整，并且缺乏理论恢复保证。本文旨在提出一种新的框架来克服这些限制，实现无需参数调整的局部与全局信息联合捕获，并提供理论保证。", "method": "本文引入了一种新的改进核范数（MNN）框架。MNN族范数通过采用合适的变换，并在变换后的矩阵上执行核范数来定义。这种方法能够联合捕获局部信息和全局低秩性。", "result": "MNN框架具有两大优势：1) 能够联合捕获局部信息和全局低秩性，且无需权衡参数调整；2) 在对变换的温和假设下，为鲁棒PCA和矩阵补全任务提供了精确的理论恢复保证，这是现有结合局部和全局信息的方法所不具备的。广泛的实验证明了该方法的有效性。", "conclusion": "MNN由于其通用和灵活的设计，可以适应各种已验证的变换，从而为结构化低秩恢复提供了一种统一且有效的方法。", "translation": "核范数（NN）在矩阵恢复问题中得到了广泛探索，例如鲁棒PCA和矩阵补全，它利用了数据固有的全局低秩结构。在本研究中，我们引入了一种新的改进核范数（MNN）框架，其中MNN族范数通过采用合适的变换并在变换后的矩阵上执行NN来定义。MNN框架提供了两个主要优势：(1) 它联合捕获局部信息和全局低秩性，而无需权衡参数调整；(2) 在对变换的温和假设下，我们为鲁棒PCA和MC任务提供了精确的理论恢复保证——这是现有结合局部和全局信息的方法所不具备的成就。由于其通用和灵活的设计，MNN可以适应各种已验证的变换，从而为结构化低秩恢复提供了一种统一且有效的方法。广泛的实验证明了我们方法的有效性。代码和补充材料可在https://github.com/andrew-pengjj/modified_nuclear_norm获取。", "summary": "本文提出了一种改进核范数（MNN）框架，旨在解决现有方法在矩阵恢复中结合局部和全局低秩信息时面临的参数调整和理论保证不足的问题。MNN通过对数据进行变换后应用核范数，实现了局部信息与全局低秩性的联合捕获，且无需参数调优。该方法在理论上为鲁棒PCA和矩阵补全提供了精确恢复保证，并通过实验验证了其有效性。", "keywords": "改进核范数, 矩阵恢复, 鲁棒PCA, 矩阵补全, 低秩性", "comments": "该论文的创新之处在于提出了改进核范数（MNN）框架，它能够同时捕获数据的局部和全局低秩结构，且无需复杂的参数调优。更重要的是，它为鲁棒PCA和矩阵补全任务提供了严格的理论恢复保证，这是现有结合局部和全局信息的方法所不具备的显著优势。MNN的通用性和灵活性使其能够适应不同的变换，为结构化低秩恢复提供了一个统一且强大的工具。"}}
{"id": "2507.16122", "title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for Efficient 3D Medical Image Segmentation", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16122v2", "summary": "Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16122v2", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "MLRU++: 结合注意力机制的多尺度轻量级残差UNETR++，用于高效三维医学图像分割", "tldr": "MLRU++是一种多尺度轻量级残差UNETR++架构，通过引入轻量级通道和瓶颈注意力模块（LCBAM）以及多尺度瓶颈块（M2B），在保持高分割精度的同时显著提高了3D医学图像分割的计算效率。", "motivation": "精确高效的医学图像分割至关重要，但由于解剖变异性和体数据的高计算需求而面临挑战。现有的混合CNN-Transformer架构虽然取得了最先进的结果，但增加了显著的复杂性。", "method": "本文提出了MLRU++，一种多尺度轻量级残差UNETR++架构。它引入了两个关键创新：轻量级通道和瓶颈注意力模块（LCBAM），以最小开销增强上下文特征编码；以及解码器中的多尺度瓶颈块（M2B），通过多分辨率特征聚合捕获细粒度细节。", "result": "MLRU++在四个公开基准数据集上取得了最先进的性能，平均Dice分数分别为Synapse 87.57%、ACDC 93.00%和Lung 81.12%。与现有领先模型相比，MLRU++在Synapse和ACDC上分别将Dice分数提高了5.38%和2.12%，同时显著减少了参数数量和计算成本。消融研究证实了所提出架构组件的有效性。", "conclusion": "MLRU++为3D医学图像分割任务提供了一个实用且高性能的解决方案。", "translation": "精确高效的医学图像分割至关重要，但由于解剖变异性和体数据的高计算需求而面临挑战。最近的混合CNN-Transformer架构取得了最先进的结果，但增加了显著的复杂性。在本文中，我们提出了MLRU++，一种多尺度轻量级残差UNETR++架构，旨在平衡分割精度和计算效率。它引入了两个关键创新：轻量级通道和瓶颈注意力模块（LCBAM），以最小开销增强上下文特征编码；以及解码器中的多尺度瓶颈块（M2B），通过多分辨率特征聚合捕获细粒度细节。在四个公开可用的基准数据集（Synapse、BTCV、ACDC和Decathlon Lung）上的实验表明，MLRU++实现了最先进的性能，平均Dice分数分别为87.57%（Synapse）、93.00%（ACDC）和81.12%（Lung）。与现有领先模型相比，MLRU++在Synapse和ACDC上分别将Dice分数提高了5.38%和2.12%，同时显著减少了参数数量和计算成本。评估LCBAM和M2B的消融研究进一步证实了所提出架构组件的有效性。结果表明，MLRU++为3D医学图像分割任务提供了一个实用且高性能的解决方案。源代码可在以下网址获取：https://github.com/1027865/MLRUPP", "summary": "本文提出MLRU++，一种多尺度轻量级残差UNETR++架构，旨在解决3D医学图像分割中精度与计算效率的平衡问题。它通过引入轻量级通道和瓶颈注意力模块（LCBAM）以及解码器中的多尺度瓶颈块（M2B）来增强特征编码和细节捕获。实验结果表明，MLRU++在多个基准数据集上实现了领先的分割性能，同时显著降低了模型复杂度和计算成本，为3D医学图像分割提供了一个高效实用的解决方案。", "keywords": "医学图像分割, UNETR++, 注意力机制, 多尺度, 轻量级", "comments": "MLRU++的创新性在于其在保持高精度的同时，通过引入LCBAM和M2B等轻量级且高效的模块，显著提升了计算效率，解决了3D医学图像分割领域长期存在的计算资源需求大的痛点。这使得该模型在实际应用中更具可行性。"}}
{"id": "2501.15916", "title": "Online Housing Market", "authors": ["Julien Lesca"], "categories": ["cs.GT", "cs.AI"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.15916v2", "summary": "This paper studies an online variant of the celebrated housing market\nproblem, where each agent has a single house and seeks to exchange it for\nanother based on her preferences. In this online setting, agents may arrive and\ndepart at any time, meaning that not all agents are present on the housing\nmarket simultaneously. I extend the well known serial dictatorship and Gale s\ntop trading cycle mechanisms to this online scenario, aiming to retain their\ndesirable properties such as Pareto efficiency, individual rationality, and\nstrategy proofness. These extensions also seek to prevent agents from\nstrategically delaying their arrival or advancing their departure. I\ndemonstrate that achieving all of these properties simultaneously is impossible\nin the online context, and I present several variants that achieve different\nsubsets of these properties.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.15916v2", "cate": "cs.GT", "date": "2025-01-27", "updated": "2025-07-24", "AI": {"title_translation": "在线住房市场", "tldr": "本文研究在线住房市场问题。作者扩展了传统机制，但发现无法同时实现在线环境中的所有理想特性（效率、个体理性、策略激励兼容）。", "motivation": "本文的动机是将传统的住房市场机制应用于代理人动态到达和离开的在线环境，同时试图保留帕累托效率、个体理性、策略激励兼容等理想特性，并防止代理人进行策略性时间操作。", "method": "作者将著名的序列独裁机制和盖尔的顶级交易循环机制扩展到在线场景。", "result": "在线环境中，无法同时实现帕累托效率、个体理性、策略激励兼容等所有期望特性。本文提出了几种实现这些特性不同子集的变体。", "conclusion": "尽管传统的理想特性无法在在线住房市场中同时实现，但不同的变体可以实现这些特性的子集，这意味着存在权衡。", "translation": "本文研究了著名的住房市场问题的一个在线变体，其中每个代理人拥有一套住房，并根据其偏好寻求将其交换给另一套住房。在这种在线环境中，代理人可能随时到达和离开，这意味着并非所有代理人都在住房市场上同时存在。我将著名的序列独裁机制和盖尔的顶级交易循环机制扩展到这种在线场景，旨在保留其理想特性，如帕累托效率、个体理性、策略激励兼容。这些扩展还旨在防止代理人策略性地延迟到达或提前离开。我证明了在在线环境中同时实现所有这些特性是不可能的，并且我提出了几种实现这些特性不同子集的变体。", "summary": "本文探讨了代理人动态的在线住房市场问题。它将序列独裁和顶级交易循环等经典机制应用于此设置，旨在实现帕累托效率、个体理性、策略激励兼容等特性。关键发现是在线环境中无法同时实现所有这些特性，因此开发了多种机制，以实现这些理想特性的不同子集。", "keywords": "在线住房市场, 机制设计, 帕累托效率, 策略激励兼容, 顶级交易循环", "comments": "本文解决了将传统市场设计适应动态在线环境的相关且复杂的问题。其不可能性结果是一个重要贡献，突出了在这种环境中同时实现多个理想特性的内在权衡。对实现部分特性的变体的探索对实际应用具有指导意义。"}}
{"id": "2507.17784", "title": "Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach", "authors": ["Minh-Duong Nguyen", "Quoc-Viet Pham", "Nguyen H. Tran", "Hoang-Khoi Do", "Duy T. Ngo", "Won-Joo Hwang"], "categories": ["cs.LG", "68", "I.2.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 12 figures, 4 tables", "url": "http://arxiv.org/abs/2507.17784v1", "summary": "In this study, we design a low-complexity and generalized AI model that can\ncapture common knowledge to improve data reconstruction of the channel decoder\nfor semantic communication. Specifically, we propose a generative adversarial\nnetwork that leverages causality-invariant learning to extract causal and\nnon-causal representations from the data. Causal representations are invariant\nand encompass crucial information to identify the data's label. They can\nencapsulate semantic knowledge and facilitate effective data reconstruction at\nthe receiver. Moreover, the causal mechanism ensures that learned\nrepresentations remain consistent across different domains, making the system\nreliable even with users collecting data from diverse domains. As\nuser-collected data evolves over time causing knowledge divergence among users,\nwe design sparse update protocols to improve the invariant properties of the\nknowledge while minimizing communication overheads. Three key observations were\ndrawn from our empirical evaluations. Firstly, causality-invariant knowledge\nensures consistency across different devices despite the diverse training data.\nSecondly, invariant knowledge has promising performance in classification\ntasks, which is pivotal for goal-oriented semantic communications. Thirdly, our\nknowledge-based data reconstruction highlights the robustness of our decoder,\nwhich surpasses other state-of-the-art data reconstruction and semantic\ncompression methods in terms of Peak Signal-to-Noise Ratio (PSNR).", "comment": "13 pages, 12 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.17784v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "面向知识语义通信的知识抽象：一种生成式因果不变方法", "tldr": "本文提出了一种基于生成对抗网络和因果不变学习的低复杂度AI模型，用于语义通信中的知识抽象，以改善数据重建并确保在不同领域和时间演变的数据下系统的可靠性。", "motivation": "为了提高语义通信中信道解码器的数据重建能力，并解决用户数据随时间演变导致的知识分歧问题，本文旨在设计一个低复杂度、通用的AI模型来捕获通用知识。", "method": "本文提出了一种基于生成对抗网络（GAN）的AI模型，该模型利用因果不变学习从数据中提取因果和非因果表示。因果表示是不变的，包含识别数据标签的关键信息，并促进接收端有效的数据重建。此外，为应对用户数据随时间演变导致的知识分歧，设计了稀疏更新协议以改进知识的不变性并最小化通信开销。", "result": "1. 因果不变知识确保了在不同训练数据下，不同设备之间的一致性。2. 不变知识在分类任务中表现出色，这对于面向目标的语义通信至关重要。3. 基于知识的数据重建方法在峰值信噪比（PSNR）方面超越了其他最先进的数据重建和语义压缩方法。", "conclusion": "本文提出的基于生成式因果不变方法的知识抽象模型，能够有效提高语义通信中的数据重建性能，并在数据多样性和演变的环境下保持系统可靠性和知识一致性，展现出优越的性能。", "translation": "在本研究中，我们设计了一种低复杂度且通用的AI模型，该模型可以捕获通用知识以改进语义通信中信道解码器的数据重建。具体来说，我们提出了一种生成对抗网络，该网络利用因果不变学习从数据中提取因果和非因果表示。因果表示是不变的，包含识别数据标签的关键信息。它们可以封装语义知识并促进接收端有效的数据重建。此外，因果机制确保了学习到的表示在不同领域保持一致，即使在用户从不同领域收集数据的情况下也能使系统保持可靠。随着用户收集的数据随时间演变，导致用户之间的知识分歧，我们设计了稀疏更新协议，以在最小化通信开销的同时改进知识的不变性。我们的实证评估得出了三个关键观察结果。首先，因果不变知识确保了尽管训练数据多样，但在不同设备之间保持一致性。其次，不变知识在分类任务中表现出良好的性能，这对于面向目标的语义通信至关重要。第三，我们基于知识的数据重建突出了我们解码器的鲁棒性，在峰值信噪比（PSNR）方面超越了其他最先进的数据重建和语义压缩方法。", "summary": "本文提出了一种新的低复杂度AI模型，用于语义通信中的知识抽象。该模型采用生成对抗网络结合因果不变学习，以提取数据中的因果不变表示，从而有效提升信道解码器的数据重建质量。研究还引入了稀疏更新协议来应对数据随时间演变导致的知识分歧。实验结果表明，该方法在不同设备间保持知识一致性，在分类任务中表现优异，并且在数据重建方面超越了现有先进技术。", "keywords": "语义通信, 知识抽象, 因果不变学习, 生成对抗网络, 数据重建", "comments": "该论文的创新点在于将因果不变学习引入语义通信中的知识抽象，以解决数据多样性和动态演变带来的挑战。通过提取因果不变表示，模型能够捕获更本质的语义信息，从而在复杂环境下保持系统鲁棒性和高效的数据重建。稀疏更新协议的设计也体现了对实际应用中通信开销的考量，使其更具实用价值。"}}
{"id": "2507.17898", "title": "Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization", "authors": ["Connor Scully-Allison", "Kevin Menear", "Kristin Potter", "Andrew McNutt", "Katherine E. Isaacs", "Dmitry Duplyakin"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 Pages, 4 figures", "url": "http://arxiv.org/abs/2507.17898v1", "summary": "Domain-specific visualizations sometimes focus on narrow, albeit important,\ntasks for one group of users. This focus limits the utility of a visualization\nto other groups working with the same data. While tasks elicited from other\ngroups can present a design pitfall if not disambiguated, they also present a\ndesign opportunity -- development of visualizations that support multiple\ngroups. This development choice presents a trade off of broadening the scope\nbut limiting support for the more narrow tasks of any one group, which in some\ncases can enhance the overall utility of the visualization. We investigate this\nscenario through a design study where we develop \\textit{Guidepost}, a\nnotebook-embedded visualization of supercomputer queue data that helps\nscientists assess supercomputer queue wait times, machine learning researchers\nunderstand prediction accuracy, and system maintainers analyze usage trends. We\nadapt the use of personas for visualization design from existing literature in\nthe HCI and software engineering domains and apply them in categorizing tasks\nbased on their uniqueness across the stakeholder personas. Under this model,\ntasks shared between all groups should be supported by interactive\nvisualizations and tasks unique to each group can be deferred to scripting with\nnotebook-embedded visualization design. We evaluate our visualization with nine\nexpert analysts organized into two groups: a \"research analyst\" group that uses\nsupercomputer queue data in their research (representing the Machine Learning\nresearchers and Jobs Data Analyst personas) and a \"supercomputer user\" group\nthat uses this data conditionally (representing the HPC User persona). We find\nthat our visualization serves our three stakeholder groups by enabling users to\nsuccessfully execute shared tasks with point-and-click interaction while\nfacilitating case-specific programmatic analysis workflows.", "comment": "11 Pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.17898v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "相同数据，不同受众：使用用户画像来界定超级计算作业队列可视化的范围", "tldr": "本研究通过设计研究开发了Guidepost，一种嵌入式超级计算队列数据可视化工具，利用用户画像来支持科学家、机器学习研究人员和系统维护人员等多个用户群体，成功实现了共享任务的点选交互和特定案例的编程分析工作流。", "motivation": "领域特定的可视化工具通常只关注某一用户群体的狭窄任务，这限制了其对处理相同数据的其他用户群体的实用性。作者旨在探索如何开发能够支持多个用户群体的可视化工具。", "method": "本研究通过一项设计研究进行，开发了名为“Guidepost”的笔记本嵌入式超级计算机队列数据可视化工具。研究者借鉴HCI和软件工程领域的用户画像概念，将其应用于根据任务在不同利益相关者用户画像间的独特性进行分类。在该模型下，所有群体共享的任务由交互式可视化支持，而每个群体独有的任务则通过脚本和笔记本嵌入式可视化设计来处理。该可视化工具通过对九位专家分析师（分为“研究分析师”和“超级计算机用户”两组）进行评估。", "result": "研究发现，所开发的可视化工具成功服务于三个利益相关者群体，使用户能够通过点选交互成功执行共享任务，同时促进了特定案例的编程分析工作流。", "conclusion": "通过利用用户画像和混合交互模型（共享任务的点选交互和独特任务的编程分析），该可视化工具成功地支持了使用相同数据的多个不同用户群体。", "translation": "领域特定的可视化有时专注于针对某一组用户的狭窄但重要的任务。这种专注限制了可视化对处理相同数据的其他群体的实用性。尽管从其他群体引出的任务如果未明确区分可能会带来设计陷阱，但它们也带来了设计机会——开发支持多个群体的可视化。这种开发选择带来了拓宽范围但限制对任何一个群体的更狭窄任务支持的权衡，在某些情况下，这可以增强可视化的整体实用性。我们通过一项设计研究调查了这种情景，在该研究中，我们开发了Guidepost，一个笔记本嵌入式超级计算机队列数据可视化工具，它帮助科学家评估超级计算机队列等待时间，帮助机器学习研究人员理解预测准确性，并帮助系统维护人员分析使用趋势。我们将用户画像在可视化设计中的应用从HCI和软件工程领域的现有文献中改编过来，并将其应用于根据任务在利益相关者用户画像间的独特性进行分类。根据此模型，所有群体共享的任务应由交互式可视化支持，而每个群体独有的任务可以推迟到使用笔记本嵌入式可视化设计的脚本中。我们用九位专家分析师评估了我们的可视化工具，他们被组织成两组：“研究分析师”组使用超级计算机队列数据进行研究（代表机器学习研究人员和作业数据分析师用户画像），“超级计算机用户”组有条件地使用此数据（代表HPC用户用户画像）。我们发现，我们的可视化工具通过使用户能够通过点选交互成功执行共享任务，同时促进了特定案例的编程分析工作流，从而服务了我们的三个利益相关者群体。", "summary": "本论文探讨了如何设计能够服务于不同用户群体的可视化工具，以解决领域特定可视化工具实用性受限的问题。研究通过一项设计研究，开发了名为“Guidepost”的超级计算机队列数据可视化工具，并创新性地将用户画像方法应用于任务分类，以区分共享任务和独有任务。Guidepost旨在支持科学家、机器学习研究人员和系统维护人员。评估结果表明，该工具成功地通过点选交互支持了共享任务，并通过编程分析工作流支持了特定案例，从而有效满足了多个利益相关者的需求。", "keywords": "超级计算, 可视化, 用户画像, 作业队列, 多受众", "comments": "本论文的创新之处在于其将用户画像方法应用于可视化设计中，以解决单一数据源服务多用户群体的挑战。通过区分共享任务和独有任务，并采用混合交互模式（点选交互与脚本编程相结合），该研究提供了一种有效的设计策略，提高了可视化工具的整体实用性，对于未来开发更具普适性的领域特定工具具有重要指导意义。"}}
{"id": "2507.09712", "title": "RDD Function: A Tradeoff Between Rate and Distortion-in-Distortion", "authors": ["Lingyi Chen", "Haoran Tang", "Shitong Wu", "Jiakun Liu", "Huihui Wu", "Wenyi Zhang", "Hao Wu"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.09712v2", "summary": "In this paper, we propose a novel function named Rate\nDistortion-in-Distortion (RDD) function as an extension of the classical\nrate-distortion (RD) function, where the expected distortion constraint is\nreplaced by a Gromov-type distortion. This distortion, integral to the\nGromov-Wasserstein (GW) distance, effectively defines the similarity in spaces\nof possibly different dimensions even without a direct metric between them.\nWhile the RDD function qualifies as an informational RD function, encoding\ntheorems substantiate its status as an operational RD function, thereby\nunderscoring its potential applicability in real-world source coding. Due to\nthe high computational complexity associated with Gromov-type distortion, in\ngeneral, the RDD function cannot be evaluated analytically. Consequently, we\ndevelop an alternating mirror descent algorithm that significantly reduces\ncomputational complexity by employing decomposition, linearization, and\nrelaxation techniques. Numerical results on classical sources and different\ngrids demonstrate the effectiveness of the developed algorithm. By exploring\nthe relationship between the RDD function and the RD function, we suggest that\nthe RDD function may have potential applications in future scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.09712v2", "cate": "cs.IT", "date": "2025-07-13", "updated": "2025-07-24", "AI": {"title_translation": "RDD函数：速率与失真中失真之间的权衡", "tldr": "本文提出了一种新的速率失真中失真（RDD）函数，作为经典速率失真（RD）函数的扩展，用Gromov型失真替代了期望失真约束，并开发了一种交替镜像下降算法来解决其高计算复杂度问题，数值结果验证了算法的有效性。", "motivation": "经典的速率失真（RD）函数中的期望失真约束可能无法有效处理不同维度空间之间的相似性度量问题。为了更好地定义即使没有直接度量也能在不同维度空间中衡量相似性，并扩展RD函数，引入了Gromov型失真。", "method": "提出了一种新的速率失真中失真（RDD）函数，将经典速率失真（RD）函数中的期望失真约束替换为Gromov型失真。由于Gromov型失真导致的高计算复杂度，开发了一种通过分解、线性化和松弛技术显著降低复杂度的交替镜像下降算法。", "result": "数值结果在经典源和不同网格上证明了所开发算法的有效性。", "conclusion": "RDD函数具备信息性RD函数和操作性RD函数的特性，并通过与RD函数的关系探索，表明其在未来的场景中可能具有潜在的应用。", "translation": "在本文中，我们提出了一种名为速率失真中失真（RDD）函数的新型函数，作为经典速率失真（RD）函数的扩展，其中期望失真约束被Gromov型失真取代。这种失真作为Gromov-Wasserstein（GW）距离的组成部分，即使在没有直接度量的情况下，也能有效地定义可能不同维度空间中的相似性。虽然RDD函数符合信息性RD函数的条件，但编码定理证实了其作为操作性RD函数的地位，从而强调了其在现实世界信源编码中的潜在适用性。由于Gromov型失变通常具有高计算复杂度，因此RDD函数通常无法进行解析评估。因此，我们开发了一种交替镜像下降算法，通过采用分解、线性化和松弛技术显著降低了计算复杂度。在经典信源和不同网格上的数值结果证明了所开发算法的有效性。通过探索RDD函数与RD函数之间的关系，我们认为RDD函数在未来场景中可能具有潜在应用。", "summary": "本文引入了一种新型的速率失真中失真（RDD）函数，该函数是经典速率失真（RD）函数的扩展，它用基于Gromov-Wasserstein距离的Gromov型失真替代了传统的期望失真约束，从而能够处理不同维度空间中的相似性度量。尽管RDD函数具有高计算复杂度，但作者开发了一种利用分解、线性化和松弛技术的交替镜像下降算法来有效解决此问题。数值实验验证了该算法的有效性，并指出RDD函数在信源编码和未来应用中的潜力。", "keywords": "RDD函数, 速率失真, Gromov-Wasserstein距离, 交替镜像下降, 信源编码", "comments": "这篇论文通过引入Gromov型失真，扩展了经典的速率失真理论，使其能够处理更复杂的、可能维度不同的数据空间之间的相似性度量，这是一个重要的创新点。所提出的交替镜像下降算法也有效解决了新函数的高计算复杂度问题，使其具有实际应用潜力。其创新性在于将Gromov-Wasserstein距离的概念引入到速率失真理论中，为处理复杂数据结构提供新的工具。"}}
{"id": "2507.18419", "title": "Toward Sustainable Vertical Farming: Impacts of Environmental Factors and Energy Mix on Performance and Costs", "authors": ["Francesco Ceccanti", "Aldo Bischi", "Umberto Desideri", "Andrea Baccioli"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18419v1", "summary": "The increasing interest in vertical farming arises from its ability to ensure\nconsistent, high-quality, and pest-free vegetable production while supporting\nsynergies with energy systems and urban development. Accordingly, standardized\ndesign and operation guidelines are essential to improve energy efficiency and\nlower costs. This study analyzes the production performance and energy\nconsumption of a vertical farming system, assessing its efficiency,\nsustainability, and economic viability. A total of 162 scenarios were evaluated\nby combining three levels of temperature, photosynthetic photon flux density\n(PPFD), and CO2 concentration across three distinct climatic zones, namely\nNorway, China, and Dubai, which also differ from a socio-environmental\nviewpoint. Two insulation thicknesses were also tested in each scenario.\nResults indicate that due to the heating, ventilation, and air conditioning and\ndehumidification (HVACD) system, neither the insulation layer nor the external\nclimate significantly influences crop productivity. PPFD proved to be the\ndominant factor in crop growth (correlation: 0.85), followed by CO2 (0.36) and\nindoor temperature (0.22). PPFD also emerged as the primary driver of overall\nenergy consumption (correlation: 0.73), as it affects both lighting and HVACD\nloads. Notably, the lowest specific energy consumption (SEC) coincided with the\nlowest crop productivity (55 kg/m2). The levelized cost of lettuce (LCoL),\nbalancing productivity and energy use, identified the most cost-effective setup\nas 24C, 250 PPFD, 1400 ppm CO2, with insulation, consistent across all\nclimates. Ultimately, only nearly decarbonized energy systems can support\nvertical farming without increasing CO2 emissions compared to imported lettuce.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18419v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "走向可持续垂直农业：环境因素和能源结构对性能和成本的影响", "tldr": "本研究分析了垂直农业在不同环境因素和能源结构下的性能和成本。光合光子通量密度（PPFD）是作物生长和能源消耗的主要驱动因素。研究确定了在所有气候下最具成本效益的设置（24C, 250 PPFD, 1400 ppm CO2，带保温）。最终，实现可持续垂直农业需要接近脱碳的能源系统。", "motivation": "垂直农业因其能够确保稳定、高质量、无病虫害的蔬菜生产以及与能源系统和城市发展的协同作用而日益受到关注。因此，需要标准化的设计和操作指南来提高能源效率和降低成本。", "method": "本研究分析了垂直农业系统的生产性能和能源消耗，评估了其效率、可持续性和经济可行性。共评估了162种情景，这些情景结合了三个温度、光合光子通量密度（PPFD）和二氧化碳浓度水平，并在挪威、中国和迪拜三个不同气候区进行了测试。每种情景中还测试了两种保温厚度，并使用生菜的平准化成本（LCoL）来平衡生产力和能源使用。", "result": "由于供暖、通风、空调和除湿（HVACD）系统，保温层和外部气候均未显著影响作物生产力。PPFD是作物生长的主要因素（相关性：0.85），其次是二氧化碳（0.36）和室内温度（0.22）。PPFD也是整体能源消耗的主要驱动因素（相关性：0.73），因为它影响照明和HVACD负荷。最低的比能耗（SEC）与最低的作物生产力（55千克/平方米）同时出现。最具成本效益的设置是24C、250 PPFD、1400 ppm CO2，并带有保温，这在所有气候下都是一致的。", "conclusion": "只有接近脱碳的能源系统才能支持垂直农业，而不会与进口生菜相比增加二氧化碳排放。", "translation": "垂直农业日益增长的兴趣源于其能够确保稳定、高质量、无病虫害的蔬菜生产，同时支持与能源系统和城市发展的协同作用。因此，标准化设计和操作指南对于提高能源效率和降低成本至关重要。本研究分析了垂直农业系统的生产性能和能源消耗，评估了其效率、可持续性和经济可行性。通过结合三个不同气候区（挪威、中国和迪拜）的三个温度、光合光子通量密度（PPFD）和二氧化碳浓度水平，共评估了162种情景，这些地区在社会环境方面也存在差异。每种情景中还测试了两种保温厚度。结果表明，由于供暖、通风、空调和除湿（HVACD）系统，保温层和外部气候均未显著影响作物生产力。PPFD被证明是作物生长的主要因素（相关性：0.85），其次是二氧化碳（0.36）和室内温度（0.22）。PPFD也成为整体能源消耗的主要驱动因素（相关性：0.73），因为它影响照明和HVACD负荷。值得注意的是，最低的比能耗（SEC）与最低的作物生产力（55千克/平方米）同时出现。生菜的平准化成本（LCoL）平衡了生产力和能源使用，确定了最具成本效益的设置是24摄氏度、250 PPFD、1400 ppm二氧化碳，并带有保温，这在所有气候下都是一致的。最终，只有接近脱碳的能源系统才能支持垂直农业，而不会与进口生菜相比增加二氧化碳排放。", "summary": "本研究通过分析垂直农业在162种不同情景下的生产性能和能源消耗，探讨了其可持续性和经济可行性。这些情景涵盖了温度、PPFD、二氧化碳浓度、气候区（挪威、中国、迪拜）和保温厚度的变化。主要发现表明，尽管外部气候和保温对作物生产力的影响微乎其微（归因于HVACD系统），但PPFD是作物生长和能源消耗的主导因素。研究确定了一个在不同气候下均具成本效益的最佳设置（24C, 250 PPFD, 1400 ppm CO2，带保温）。结论指出，为了实现可持续的垂直农业且不增加相对于进口农产品的二氧化碳排放，必须采用接近脱碳的能源系统。", "keywords": "垂直农业, 环境因素, 能源消耗, 光合光子通量密度, 成本效益, 可持续性", "comments": "这篇论文为优化垂直农业运营提供了宝贵的见解，明确了影响性能和能源消耗的关键环境因素。其在不同气候下进行的全面情景分析以及利用LCoL评估经济可行性是其优势所在。一个显著的创新发现是HVACD系统减轻了外部气候和保温对生产力的影响，从而将重点转移到内部环境控制，尤其是PPFD。论文对脱碳能源系统的强调，突出了垂直农业长期可持续发展的一个关键方面。"}}
{"id": "2507.18212", "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation", "authors": ["Xinrui Chen", "Hongxing Zhang", "Fanyi Zeng", "Yongxian Wei", "Yizhi Wang", "Xitong Ling", "Guanghao Li", "Chun Yuan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18212v1", "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18212v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Prune&Comp：通过迭代剪枝和幅度补偿为层剪枝LLMs提供免费午餐", "tldr": "Prune&Comp通过幅度补偿和迭代剪枝，显著提升了层剪枝大型语言模型的性能，且无额外运行时开销。", "motivation": "层剪枝大型语言模型（LLMs）在压缩和加速方面很有前景，但移除任何层都会导致隐藏状态中出现显著的幅度差距，从而导致性能大幅下降。", "method": "本研究提出了Prune&Comp，一种即插即用的层剪枝方案，它利用幅度补偿来缓解剪枝导致的幅度差距，且无需训练。具体而言，它首先估计层移除引起的幅度差距，然后通过离线重新缩放剩余权重来消除此差距，不产生运行时开销。该方法还结合了迭代剪枝策略。", "result": "Prune&Comp与迭代剪枝循环结合后，持续提升了现有层剪枝指标。例如，在使用流行的块影响度量剪枝LLaMA-3-8B的5层时，Prune&Comp几乎将困惑度减半，并保留了原始模型93.19%的问答性能，优于基线4.01%。", "conclusion": "Prune&Comp通过其独特的幅度补偿和迭代剪枝策略，有效解决了层剪枝LLMs中的性能下降问题，显著提升了模型性能，且不增加运行时开销。", "translation": "层剪枝已成为压缩大型语言模型（LLMs）的一种有前景的技术，同时实现与剪枝比率成比例的加速。在这项工作中，我们发现移除任何层都会在隐藏状态中引起显著的幅度差距，导致性能大幅下降。为了解决这个问题，我们提出了Prune&Comp，一种新颖的即插即用层剪枝方案，它利用幅度补偿以无训练的方式缓解此类差距。具体而言，我们首先估计层移除引起的幅度差距，然后通过离线重新缩放剩余权重来消除此差距，且不产生零运行时开销。我们通过迭代剪枝策略进一步展示了Prune&Comp的优势。当与迭代剪枝和补偿循环相结合时，Prune&Comp持续增强了现有层剪枝指标。例如，当使用流行的块影响度量剪枝LLaMA-3-8B的5层时，Prune&Comp几乎将困惑度减半，并保留了原始模型93.19%的原始模型问答性能，优于基线4.01%。", "summary": "本文提出了一种名为Prune&Comp的新型层剪枝方案，旨在解决大型语言模型（LLMs）层剪枝后隐藏状态中出现的显著幅度差距导致的性能下降问题。Prune&Comp通过离线估计并重新缩放剩余权重来补偿这些幅度差距，实现了无训练且零运行时开销的优势。结合迭代剪枝策略，Prune&Comp能持续提升现有剪枝方法的性能，例如在LLaMA-3-8B上显著降低困惑度并保持高问答性能，表现优于基线。", "keywords": "层剪枝, 大型语言模型, 幅度补偿, 迭代剪枝, 模型压缩", "comments": "Prune&Comp的创新之处在于其“免费午餐”的特性，即通过幅度补偿在不增加运行时开销的情况下显著提升层剪枝LLMs的性能。其即插即用和无训练的特点使其具有很高的实用价值和通用性。迭代剪枝策略的引入进一步放大了其效果，展示了其在模型压缩和加速方面的巨大潜力。"}}
{"id": "2502.06785", "title": "DeepCrossAttention: Supercharging Transformer Residual Connections", "authors": ["Mike Heddes", "Adel Javanmard", "Kyriakos Axiotis", "Gang Fu", "MohammadHossein Bateni", "Vahab Mirrokni"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.06785v2", "summary": "Transformer networks have achieved remarkable success across diverse domains,\nleveraging a variety of architectural innovations, including residual\nconnections. However, traditional residual connections, which simply sum the\noutputs of previous layers, can dilute crucial information. This work\nintroduces DeepCrossAttention (DCA), an approach that enhances residual\nlearning in transformers. DCA employs learnable, input-dependent weights to\ndynamically combine layer outputs, enabling the model to selectively focus on\nthe most relevant information in any of the previous layers. Furthermore, DCA\nincorporates depth-wise cross-attention, allowing for richer interactions\nbetween layers at different depths. Our language modeling experiments show that\nDCA achieves improved perplexity for a given training time. Moreover, DCA\nobtains the same model quality up to 3x faster while adding a negligible number\nof parameters. Theoretical analysis confirms that DCA provides an improved\ntrade-off between accuracy and model size when the ratio of collective layer\nranks to the ambient dimension falls below a critical threshold.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.06785v2", "cate": "cs.LG", "date": "2025-02-10", "updated": "2025-07-23", "AI": {"title_translation": "深度交叉注意力：增压Transformer残差连接", "tldr": "DeepCrossAttention (DCA) 改进了Transformer的残差连接，通过动态权重和深度交叉注意力，提高了语言模型的困惑度，并能以高达3倍的速度达到相同的模型质量，同时参数量可忽略不计。", "motivation": "传统的Transformer残差连接简单地将前一层的输出相加，这可能会稀释关键信息。", "method": "本文引入了DeepCrossAttention (DCA)，它使用可学习的、依赖输入的权重来动态组合层输出，使模型能够选择性地关注前几层中最相关的信息。此外，DCA还结合了深度交叉注意力，以实现不同深度层之间更丰富的交互。", "result": "在语言建模实验中，DCA在给定训练时间内实现了改进的困惑度。DCA能以高达3倍的速度达到相同的模型质量，同时仅增加可忽略不计的参数。理论分析证实，当集体层秩与环境维度之比低于临界阈值时，DCA在准确性和模型大小之间提供了更好的权衡。", "conclusion": "DeepCrossAttention (DCA) 通过改进Transformer的残差连接，显著提升了模型的性能和训练效率，并在理论上证明了其在准确性和模型大小之间的优越权衡。", "translation": "Transformer网络凭借包括残差连接在内的各种架构创新，在不同领域取得了显著成功。然而，传统的残差连接仅仅简单地将前一层的输出相加，这可能会稀释关键信息。这项工作引入了深度交叉注意力（DeepCrossAttention，DCA），这是一种增强Transformer中残差学习的方法。DCA采用可学习的、依赖输入的权重来动态组合层输出，使模型能够选择性地关注前几层中最相关的信息。此外，DCA还结合了深度交叉注意力，以实现不同深度层之间更丰富的交互。我们的语言建模实验表明，在给定训练时间下，DCA实现了改进的困惑度。此外，DCA在增加可忽略不计的参数数量的同时，能以高达3倍的速度达到相同的模型质量。理论分析证实，当集体层秩与环境维度之比低于临界阈值时，DCA在准确性和模型大小之间提供了更好的权衡。", "summary": "DeepCrossAttention (DCA) 是一种新型方法，旨在增强Transformer网络中的残差学习。针对传统残差连接可能稀释关键信息的问题，DCA引入了可学习的、依赖输入的权重来动态组合层输出，并结合了深度交叉注意力以促进层间更丰富的交互。实验结果表明，DCA在语言建模任务中提高了困惑度，并能以高达3倍的速度达到相同的模型质量，同时只增加可忽略的参数量。理论分析进一步证实了DCA在准确性和模型大小之间提供了更好的权衡。", "keywords": "DeepCrossAttention, Transformer, 残差连接, 语言建模, 困惑度", "comments": "本文提出的DeepCrossAttention (DCA) 提供了一种新颖且有效的方式来改进Transformer的残差连接。其创新点在于通过引入动态权重和深度交叉注意力，解决了传统残差连接信息稀释的问题。DCA的优势在于在提高模型性能（如困惑度）的同时，显著提升了训练效率，并保持了极低的参数增量，这对于大型模型训练具有重要意义。"}}
{"id": "2507.17324", "title": "An Empirical Study on Virtual Reality Software Security Weaknesses", "authors": ["Yifan Xu", "Jinfu Chen", "Zhenyu Qi", "Huashan Chen", "Junyi Wang", "Pengfei Hu", "Feng Liu", "Sen He"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17324v2", "summary": "Virtual Reality (VR) has emerged as a transformative technology across\nindustries, yet its security weaknesses, including vulnerabilities, are\nunderinvestigated. This study investigates 334 VR projects hosted on GitHub,\nexamining 1,681 software security weaknesses to understand: what types of\nweaknesses are prevalent in VR software; when and how weaknesses are\nintroduced; how long they have survived; and how they have been removed. Due to\nthe limited availability of VR software security weaknesses in public databases\n(e.g., the National Vulnerability Database or NVD), we prepare the first\nsystematic dataset of VR software security weaknesses by introducing a novel\nframework to collect such weaknesses from GitHub commit data. Our empirical\nstudy on the dataset leads to useful insights, including: (i) VR weaknesses are\nheavily skewed toward user interface weaknesses, followed by resource-related\nweaknesses; (ii) VR development tools pose higher security risks than VR\napplications; (iii) VR security weaknesses are often introduced at the VR\nsoftware birth time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17324v2", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "虚拟现实软件安全弱点的一项实证研究", "tldr": "对GitHub上的VR项目进行了首次系统性安全弱点研究，发现UI和资源相关弱点最普遍，开发工具风险更高，且弱点常在软件初期引入。", "motivation": "虚拟现实（VR）技术正在变革各行各业，但其安全弱点（包括漏洞）尚未得到充分研究，尤其缺乏公共数据库中的相关信息。", "method": "本研究调查了GitHub上托管的334个VR项目，分析了1,681个软件安全弱点。由于公共数据库（如NVD）中VR软件安全弱点信息有限，研究通过引入一个新颖的框架从GitHub提交数据中收集此类弱点，构建了首个系统性的VR软件安全弱点数据集。", "result": "研究发现：(i) VR弱点严重偏向用户界面弱点，其次是资源相关弱点；(ii) VR开发工具比VR应用程序带来更高的安全风险；(iii) VR安全弱点通常在VR软件诞生时引入。", "conclusion": "本研究首次系统性地收集并分析了VR软件安全弱点，揭示了VR安全风险的主要类型、引入时机和高风险领域，为VR软件安全提供了有价值的见解。", "translation": "虚拟现实（VR）作为一项变革性技术已在各行业中崭露头角，然而其安全弱点，包括漏洞，却未得到充分研究。本研究调查了GitHub上托管的334个VR项目，检查了1,681个软件安全弱点，旨在了解：VR软件中普遍存在哪些类型的弱点；弱点何时以及如何被引入；它们存活了多长时间；以及它们是如何被移除的。由于公共数据库（例如国家漏洞数据库或NVD）中VR软件安全弱点的可用性有限，我们通过引入一种新颖的框架从GitHub提交数据中收集此类弱点，从而准备了第一个系统性的VR软件安全弱点数据集。我们对该数据集的实证研究得出了有用的见解，包括：(i) VR弱点严重偏向用户界面弱点，其次是资源相关弱点；(ii) VR开发工具比VR应用程序带来更高的安全风险；(iii) VR安全弱点通常在VR软件诞生时引入。", "summary": "本研究对GitHub上334个VR项目中的1,681个软件安全弱点进行了首次系统性实证研究。针对公共数据库中VR安全弱点数据不足的问题，研究构建了一个新的框架来收集此类数据。研究发现，VR软件的弱点主要集中在用户界面和资源方面，VR开发工具比应用程序风险更高，且弱点常在软件开发初期引入。", "keywords": "虚拟现实, 软件安全, 安全弱点, 实证研究, GitHub", "comments": "这项研究的创新之处在于其首次系统性地构建了VR软件安全弱点数据集，并引入了新颖的框架从GitHub提交数据中收集弱点，填补了该领域数据稀缺的空白。其发现对VR软件开发的安全实践具有重要指导意义。"}}
{"id": "2507.18081", "title": "Identifier Name Similarities: An Exploratory Study", "authors": ["Carol Wong", "Mai Abe", "Silvia De Benedictis", "Marissa Halim", "Anthony Peruma"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      The 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement - Emerging Results and Vision Track", "url": "http://arxiv.org/abs/2507.18081v1", "summary": "Identifier names, which comprise a significant portion of the codebase, are\nthe cornerstone of effective program comprehension. However, research has shown\nthat poorly chosen names can significantly increase cognitive load and hinder\ncollaboration. Even names that appear readable in isolation may lead to\nmisunderstandings in contexts when they closely resemble other names in either\nstructure or functionality. In this exploratory study, we present our\npreliminary findings on the occurrence of identifier name similarity in\nsoftware projects through the development of a taxonomy that categorizes\ndifferent forms of identifier name similarity. We envision our initial taxonomy\nproviding researchers with a platform to analyze and evaluate the impact of\nidentifier name similarity on code comprehension, maintainability, and\ncollaboration among developers, while also allowing for further refinement and\nexpansion of the taxonomy.", "comment": "The 19th ACM/IEEE International Symposium on Empirical Software\n  Engineering and Measurement - Emerging Results and Vision Track", "pdf_url": "http://arxiv.org/pdf/2507.18081v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "标识符名称相似性：一项探索性研究", "tldr": "本研究探索了软件项目中标识符名称相似性的发生情况，并提出了一个分类法来描述不同形式的相似性，旨在为评估其对代码理解和协作的影响提供基础。", "motivation": "标识符名称是程序理解的基石，但选择不当或相似的名称会显著增加认知负荷，阻碍协作，甚至导致误解。", "method": "通过开发一个分类法来分类不同形式的标识符名称相似性，以此来探索软件项目中标识符名称相似性的发生情况。", "result": "展示了关于软件项目中标识符名称相似性发生情况的初步发现，并开发了一个用于分类不同形式相似性的初始分类法。", "conclusion": "提出的初始分类法旨在为研究人员提供一个平台，以分析和评估标识符名称相似性对代码理解、可维护性以及开发者之间协作的影响，并允许进一步完善和扩展该分类法。", "translation": "标识符名称占据了代码库的很大一部分，是有效程序理解的基石。然而，研究表明，选择不当的名称会显著增加认知负荷并阻碍协作。即使是孤立看似乎可读的名称，当它们在结构或功能上与其他名称非常相似时，也可能在特定上下文中导致误解。在这项探索性研究中，我们通过开发一个将不同形式的标识符名称相似性分类的分类法，展示了我们关于软件项目中标识符名称相似性发生情况的初步发现。我们设想我们的初始分类法能为研究人员提供一个平台，以分析和评估标识符名称相似性对代码理解、可维护性以及开发者之间协作的影响，同时也允许对该分类法进行进一步的完善和扩展。", "summary": "本探索性研究聚焦于软件项目中标识符名称的相似性问题，强调了其对程序理解、认知负荷和开发者协作的关键影响。作者通过开发一个分类法，对不同形式的标识符名称相似性进行了初步的发现和分类。该研究提出的初始分类法旨在为未来的研究提供一个基础平台，以便更深入地分析和评估标识符名称相似性对代码可读性、可维护性和团队协作的影响。", "keywords": "标识符名称, 代码理解, 名称相似性, 分类法, 软件工程", "comments": "这项研究通过提出一个标识符名称相似性分类法，为理解和解决代码可读性及维护性问题提供了新的视角。其创新点在于系统地分类了相似性类型，为后续量化分析其影响奠定了基础，这对于提高软件质量和开发效率具有重要意义。"}}
{"id": "2507.18253", "title": "Countering Privacy Nihilism", "authors": ["Severin Engelmann", "Helen Nissenbaum"], "categories": ["cs.CY", "cs.CR"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18253v1", "summary": "Of growing concern in privacy scholarship is artificial intelligence (AI), as\na powerful producer of inferences. Taken to its limits, AI may be presumed\ncapable of inferring \"everything from everything,\" thereby making untenable any\nnormative scheme, including privacy theory and privacy regulation, which rests\non protecting privacy based on categories of data - sensitive versus\nnon-sensitive, private versus public. Discarding data categories as a normative\nanchoring in privacy and data protection as a result of an unconditional\nacceptance of AI's inferential capacities is what we call privacy nihilism. An\nethically reasoned response to AI inferences requires a sober consideration of\nAI capabilities rather than issuing an epistemic carte blanche. We introduce\nthe notion of conceptual overfitting to expose how privacy nihilism turns a\nblind eye toward flawed epistemic practices in AI development. Conceptual\noverfitting refers to the adoption of norms of convenience that simplify the\ndevelopment of AI models by forcing complex constructs to fit data that are\nconceptually under-representative or even irrelevant. While conceptual\noverfitting serves as a helpful device to counter normative suggestions\ngrounded in hyperbolic AI capability claims, AI inferences shake any privacy\nregulation that hinges protections based on restrictions around data\ncategories. We propose moving away from privacy frameworks that focus solely on\ndata type, neglecting all other factors. Theories like contextual integrity\nevaluate the normative value of privacy across several parameters, including\nthe type of data, the actors involved in sharing it, and the purposes for which\nthe information is used.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18253v1", "cate": "cs.CY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "反驳隐私虚无主义", "tldr": "本文批判了“隐私虚无主义”，即因夸大AI推断能力而放弃基于数据类别的隐私保护，并引入“概念过拟合”来揭示AI开发中的缺陷，提倡采用更全面的隐私框架，而非仅关注数据类型。", "motivation": "随着人工智能（AI）推断能力的增强，隐私学界日益担忧AI可能推断出“一切”，从而使基于数据类别（敏感/非敏感、私人/公共）的隐私保护规范变得不可持续。这种无条件接受AI推断能力并因此放弃数据类别作为隐私保护基础的观点被称为“隐私虚无主义”，作者旨在反驳这种观点。", "method": "论文引入了“概念过拟合”的概念，以揭示隐私虚无主义如何忽视AI开发中存在缺陷的认知实践。概念过拟合是指为了简化AI模型开发而采用的便利规范，即强行将复杂的概念结构与概念上代表性不足甚至不相关的数据相匹配。论文还提出，应摒弃仅关注数据类型的隐私框架，转而采用更全面的理论，如情境完整性，该理论会评估数据类型、参与共享的参与者以及信息使用目的等多个参数的隐私规范价值。", "result": "论文提出“概念过拟合”可以有效反驳基于夸大AI能力主张的规范性建议。虽然AI推断确实动摇了基于数据类别限制的隐私法规，但论文强调不应因此陷入隐私虚无主义，而是应重新审视并采纳更全面的隐私框架。", "conclusion": "结论是，对AI推断的伦理回应需要清醒地考虑AI的能力，而非给予其认知上的“全权委托”。应摆脱仅关注数据类型的隐私框架，转而采用考虑多方面因素（如数据类型、参与者、使用目的）的理论，例如情境完整性，以应对AI带来的隐私挑战。", "translation": "隐私学术界日益关注人工智能（AI）作为强大的推断生产者。推至极限，AI可能被假定能够“从一切推断一切”，从而使任何基于数据类别（敏感与非敏感，私人与公共）来保护隐私的规范体系（包括隐私理论和隐私法规）变得站不住脚。由于无条件接受AI的推断能力而放弃数据类别作为隐私和数据保护的规范性锚定，我们称之为隐私虚无主义。对AI推断做出伦理上的合理回应，需要清醒地考虑AI的能力，而不是发出认知上的全权委托。我们引入了“概念过拟合”的概念，以揭示隐私虚无主义如何对AI开发中存在缺陷的认知实践视而不见。概念过拟合指的是为了简化AI模型的开发而采用的便利规范，即强行将复杂的概念结构与概念上代表性不足甚至不相关的数据相匹配。尽管概念过拟合有助于反驳基于夸大AI能力主张的规范性建议，但AI推断确实动摇了任何依赖数据类别限制来提供保护的隐私法规。我们建议摆脱仅关注数据类型而忽视所有其他因素的隐私框架。情境完整性等理论从多个参数评估隐私的规范价值，包括数据类型、参与共享的参与者以及信息使用目的。", "summary": "本文批判了在人工智能（AI）强大的推断能力背景下出现的“隐私虚无主义”，即因夸大AI推断能力而主张放弃基于数据类别（如敏感与非敏感）的隐私保护。作者认为，这种观点忽视了AI开发中可能存在的“概念过拟合”问题，即为了简化模型而将复杂概念强行匹配不相关或代表性不足的数据。论文强调，对AI推断的伦理回应应基于对AI能力的清醒认识，而非全盘接受。为此，文章提倡摒弃仅依赖数据类型的隐私框架，转而采用更全面的理论，如情境完整性，该理论会综合考虑数据类型、参与者和使用目的等多个因素来评估隐私价值。", "keywords": "隐私虚无主义, 人工智能, 概念过拟合, 隐私保护, 情境完整性", "comments": "这篇论文通过引入“隐私虚无主义”和“概念过拟合”这两个概念，对AI时代下的隐私保护困境提出了深刻的批判和建设性的思考。其创新之处在于，它不仅指出了AI推断能力可能带来的挑战，更重要的是，它揭示了在面对这种挑战时，过度悲观或不加批判地接受AI全能性是一种“认知缺陷”，并提供了“概念过拟合”这一工具来反驳夸大的AI能力主张。论文的价值在于，它呼吁隐私领域从单一的数据类型保护转向更复杂、更情境化的保护框架，如情境完整性，这对于指导未来AI伦理和隐私法规的制定具有重要意义。"}}
{"id": "2503.04233", "title": "Learning Wideband User Scheduling and Hybrid Precoding with Graph Neural Networks", "authors": ["Shengjie Liu", "Chenyang Yang", "Shengqian Han"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04233v2", "summary": "User scheduling and hybrid precoding in wideband multi-antenna systems have\nnever been learned jointly due to the challenges arising from the massive user\ncombinations on resource blocks (RBs) and the shared analog precoder among RBs.\nIn this paper, we strive to jointly learn the scheduling and precoding policies\nwith graph neural networks (GNNs), which have emerged as a powerful tool for\noptimizing resource allocation thanks to their potential in generalizing across\nproblem scales. By reformulating the joint optimization problem into an\nequivalent functional optimization problem for the scheduling and precoding\npolicies, we propose a GNN-based architecture consisting of two cascaded\nmodules to learn the two policies. We discover a same-parameter same-decision\n(SPSD) property for wireless policies defined on sets, revealing that a GNN\ncannot well learn the optimal scheduling policy when users have similar\nchannels. This motivates us to develop a sequence of GNNs to enhance the\nscheduler module. Furthermore, by analyzing the SPSD property, we find when\nlinear aggregators in GNNs impede size generalization. Based on the\nobservation, we devise a novel attention mechanism for information aggregation\nin the precoder module. Simulation results demonstrate that the proposed\narchitecture achieves satisfactory spectral efficiency with short inference\ntime and low training complexity, and is generalizable to the numbers of users,\nRBs, and antennas at the base station and users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04233v2", "cate": "eess.SP", "date": "2025-03-06", "updated": "2025-07-24", "AI": {"title_translation": "基于图神经网络的宽带用户调度和混合预编码学习", "tldr": "本文提出了一种基于图神经网络（GNN）的架构，用于联合学习宽带多天线系统中的用户调度和混合预编码，解决了大规模组合和共享模拟预编码器的挑战，并提升了泛化能力。", "motivation": "宽带多天线系统中的用户调度和混合预编码从未被联合学习，原因在于资源块（RBs）上大量的用户组合以及RBs之间共享模拟预编码器带来的挑战。", "method": "本文提出了一种基于图神经网络（GNN）的架构，包含两个级联模块，用于联合学习调度和预编码策略。通过重新将联合优化问题表述为等效的功能优化问题。发现了无线策略的“同参数同决策（SPSD）”特性，并基于此开发了一系列GNN来增强调度器模块，并设计了一种新颖的注意力机制用于预编码器模块中的信息聚合。", "result": "所提出的架构实现了令人满意的频谱效率，推理时间短，训练复杂度低，并且对用户数量、RBs数量以及基站和用户天线数量具有泛化能力。", "conclusion": "本文提出的基于GNN的联合用户调度和混合预编码方法能够有效解决现有挑战，并在性能、效率和泛化能力方面表现出色。", "translation": "宽带多天线系统中的用户调度和混合预编码从未被联合学习，原因在于资源块（RBs）上大量的用户组合以及RBs之间共享模拟预编码器带来的挑战。在本文中，我们致力于使用图神经网络（GNN）联合学习调度和预编码策略，GNN作为一种强大的工具，因其在跨问题规模泛化方面的潜力，已成为优化资源分配的新兴手段。通过将联合优化问题重新表述为调度和预编码策略的等效函数优化问题，我们提出了一种基于GNN的架构，由两个级联模块组成，以学习这两种策略。我们发现集合上定义的无线策略具有“同参数同决策（SPSD）”特性，揭示了当用户信道相似时，GNN无法很好地学习最优调度策略。这促使我们开发了一系列GNN来增强调度器模块。此外，通过分析SPSD特性，我们发现GNN中的线性聚合器何时会阻碍规模泛化。基于这一观察，我们为预编码器模块中的信息聚合设计了一种新颖的注意力机制。仿真结果表明，所提出的架构在短推理时间和低训练复杂度下实现了令人满意的频谱效率，并且对用户、RBs以及基站和用户天线的数量具有泛化能力。", "summary": "针对宽带多天线系统中用户调度和混合预编码联合学习面临的大规模用户组合及共享模拟预编码器挑战，本文提出了一种基于图神经网络（GNN）的级联架构。通过发现“同参数同决策（SPSD）”特性，研究人员开发了一系列GNN以增强调度器模块，并设计了新颖的注意力机制用于预编码器。仿真结果表明，该方法在频谱效率、推理时间、训练复杂度和泛化能力方面均表现出色。", "keywords": "图神经网络, 用户调度, 混合预编码, 宽带多天线系统", "comments": "本文的创新点在于首次将图神经网络应用于宽带多天线系统的联合用户调度和混合预编码问题，并通过发现并解决“同参数同决策（SPSD）”特性，显著提升了GNN在无线策略学习中的性能和泛化能力。这为未来大规模MIMO系统中的资源分配优化提供了新的思路和强大的工具。"}}
{"id": "2507.18518", "title": "Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment", "authors": ["Ruiqi He", "Zekun Fei", "Jiaqi Li", "Xinyuan Zhu", "Biao Yi", "Siyi Lv", "Weijie Liu", "Zheli Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18518v1", "summary": "Vector Database (VDB) can efficiently index and search high-dimensional\nvector embeddings from unstructured data, crucially enabling fast semantic\nsimilarity search essential for modern AI applications like generative AI and\nrecommendation systems. Since current VDB service providers predominantly use\nproprietary black-box models, users are forced to expose raw query text to them\nvia API in exchange for the vector retrieval services. Consequently, if query\ntext involves confidential records from finance or healthcare domains, this\nmechanism inevitably leads to critical leakage of user's sensitive information.\nTo address this issue, we introduce STEER (\\textbf{S}ecure \\textbf{T}ransformed\n\\textbf{E}mbedding v\\textbf{E}ctor\\textbf{ R}etrieval), a private vector\nretrieval framework that leverages the alignment relationship between the\nsemantic spaces of different embedding models to derive approximate embeddings\nfor the query text. STEER performs the retrieval using the approximate\nembeddings within the original VDB and requires no modifications to the server\nside. Our theoretical and experimental analyses demonstrate that STEER\neffectively safeguards query text privacy while maintaining the retrieval\naccuracy. Even though approximate embeddings are approximations of the\nembeddings from proprietary models, they still prevent the providers from\nrecovering the query text through Embedding Inversion Attacks (EIAs). Extensive\nexperimental results show that Recall@100 of STEER can basically achieve a\ndecrease of less than 5\\%. Furthermore, even when searching within a text\ncorpus of millions of entries, STEER achieves a Recall@20 accuracy 20\\% higher\nthan current baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18518v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "查询前转换：一种基于嵌入空间对齐的向量检索隐私保护方法", "tldr": "STEER是一个隐私保护向量检索框架，它通过生成近似查询嵌入来避免向向量数据库服务提供商暴露原始敏感查询文本，同时保持检索准确性。", "motivation": "当前的向量数据库（VDB）服务要求用户通过API将原始查询文本暴露给提供商，以换取向量检索服务。如果查询文本包含来自金融或医疗领域的机密记录，这种机制将导致用户敏感信息的严重泄露。", "method": "本文引入了STEER（安全转换嵌入向量检索）框架，该框架利用不同嵌入模型的语义空间之间的对齐关系，推导出查询文本的近似嵌入。STEER使用这些近似嵌入在原始VDB中执行检索，且无需对服务器端进行修改。", "result": "理论和实验分析表明，STEER能有效保护查询文本隐私，同时保持检索准确性。近似嵌入可以防止提供商通过嵌入反演攻击（EIAs）恢复查询文本。广泛的实验结果显示，STEER的Recall@100下降不到5%；即使在数百万条目的文本语料库中搜索，STEER的Recall@20准确率也比当前基线高20%。", "conclusion": "STEER框架提供了一种有效的隐私保护向量检索方法，在不牺牲显著检索准确性的前提下，解决了向向量数据库服务提供商暴露敏感查询信息的问题。", "translation": "向量数据库（VDB）能够高效索引和搜索来自非结构化数据的高维向量嵌入，这对于现代AI应用（如生成式AI和推荐系统）中必不可少的快速语义相似性搜索至关重要。由于当前的VDB服务提供商主要使用专有的黑盒模型，用户被迫通过API向其暴露原始查询文本以获取向量检索服务。因此，如果查询文本涉及来自金融或医疗领域的机密记录，这种机制不可避免地会导致用户敏感信息的关键性泄露。为了解决这个问题，我们引入了STEER（安全转换嵌入向量检索），这是一个私有向量检索框架，它利用不同嵌入模型语义空间之间的对齐关系来推导查询文本的近似嵌入。STEER在原始VDB中利用近似嵌入执行检索，并且无需对服务器端进行修改。我们的理论和实验分析表明，STEER有效保护了查询文本隐私，同时保持了检索准确性。尽管近似嵌入是专有模型嵌入的近似值，但它们仍然可以防止提供商通过嵌入反演攻击（EIAs）恢复查询文本。广泛的实验结果表明，STEER的Recall@100基本可以实现不到5%的下降。此外，即使在数百万条目的文本语料库中搜索时，STEER的Recall@20准确率也比当前基线高20%。", "summary": "STEER是一个旨在解决向量数据库查询隐私问题的框架。它通过生成查询文本的近似嵌入来避免直接暴露敏感信息给服务提供商，利用不同嵌入模型间的空间对齐关系。该方法在不修改服务器端的情况下，既能有效防止嵌入反演攻击，又能保持高检索精度，在实验中表现出优于现有基线的性能。", "keywords": "向量检索, 隐私保护, 嵌入空间对齐, 向量数据库, STEER", "comments": "该论文的创新点在于提出了“查询前转换”的概念，利用嵌入空间对齐来生成近似嵌入，从而在客户端实现隐私保护，而无需服务器端修改。这对于处理敏感数据（如金融、医疗）的AI应用具有重要意义，因为它解决了当前VDB服务中数据隐私泄露的关键问题。"}}
{"id": "2409.05260", "title": "Scalable Frame Sampling for Video Classification: A Semi-Optimal Policy Approach with Reduced Search Space", "authors": ["Junho Lee", "Jeongwoo Shin", "Seung Woo Ko", "Seongsu Ha", "Joonseok Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.05260v3", "summary": "Given a video with $T$ frames, frame sampling is a task to select $N \\ll T$\nframes, so as to maximize the performance of a fixed video classifier. Not just\nbrute-force search, but most existing methods suffer from its vast search space\nof $\\binom{T}{N}$, especially when $N$ gets large. To address this challenge,\nwe introduce a novel perspective of reducing the search space from $O(T^N)$ to\n$O(T)$. Instead of exploring the entire $O(T^N)$ space, our proposed\nsemi-optimal policy selects the top $N$ frames based on the independently\nestimated value of each frame using per-frame confidence, significantly\nreducing the computational complexity. We verify that our semi-optimal policy\ncan efficiently approximate the optimal policy, particularly under practical\nsettings. Additionally, through extensive experiments on various datasets and\nmodel architectures, we demonstrate that learning our semi-optimal policy\nensures stable and high performance regardless of the size of $N$ and $T$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.05260v3", "cate": "cs.CV", "date": "2024-09-09", "updated": "2025-07-24", "AI": {"title_translation": "可扩展的视频分类帧采样：一种具有缩减搜索空间的半最优策略方法", "tldr": "该论文提出了一种用于视频分类帧采样的半最优策略，通过独立估计每帧的置信度来选择帧，将搜索空间从$O(T^N)$显著减少到$O(T)$，从而在保持高性能的同时提高了效率。", "motivation": "在视频分类中，现有帧采样方法面临巨大的搜索空间（$\binom{T}{N}$或$O(T^N)$），尤其当需要选择的帧数$N$较大时，导致计算成本过高。", "method": "本文提出了一种“半最优策略”，通过独立估计每帧的置信度值来选择前$N$帧，从而将搜索空间从$O(T^N)$显著减少到$O(T)$，而非探索整个$O(T^N)$空间。这大大降低了计算复杂度。", "result": "实验结果表明，所提出的半最优策略能够有效逼近最优策略，尤其是在实际设置下。此外，在各种数据集和模型架构上的广泛实验证明，该半最优策略无论$N$和$T$的大小如何，都能确保稳定且高性能的表现。", "conclusion": "所提出的半最优策略为视频分类中的帧采样提供了一种可扩展且高效的解决方案，通过显著减少计算复杂度，实现了稳定的高性能。", "translation": "给定一个包含$T$帧的视频，帧采样任务是选择$N \text{«} T$帧，以最大化固定视频分类器的性能。不仅是暴力搜索，大多数现有方法都受到其庞大搜索空间$\binom{T}{N}$的困扰，特别是当$N$变大时。为了解决这一挑战，我们引入了一种新颖的视角，将搜索空间从$O(T^N)$减少到$O(T)$。我们提出的半最优策略不是探索整个$O(T^N)$空间，而是基于使用每帧置信度独立估计的每帧价值来选择前$N$帧，从而显著降低了计算复杂度。我们验证了我们的半最优策略可以有效地逼近最优策略，特别是在实际设置下。此外，通过在各种数据集和模型架构上进行广泛实验，我们证明了学习我们的半最优策略可以确保无论$N$和$T$的大小如何，都能保持稳定和高性能。", "summary": "针对视频分类中现有帧采样方法因搜索空间巨大而效率低下的问题，本文提出了一种新颖的半最优策略。该策略通过独立评估每帧的置信度来选择最优的$N$帧，将搜索空间复杂度从指数级的$O(T^N)$大幅降低到线性级的$O(T)$。实验证明，该方法能有效逼近最优策略，并在多种数据集和模型架构上展现出稳定且高性能的采样效果，显著提升了视频分类的效率和可扩展性。", "keywords": "帧采样, 视频分类, 半最优策略, 搜索空间缩减, 计算复杂度", "comments": "该论文的主要创新点在于提出了一种将帧采样搜索空间从指数级$O(T^N)$降至线性级$O(T)$的半最优策略，极大地解决了现有方法在处理大规模视频和大量采样帧时的计算瓶颈。这种方法提高了视频分类的效率和可扩展性，具有重要的实际应用价值。"}}
{"id": "2507.18248", "title": "Evaluation of facial landmark localization performance in a surgical setting", "authors": ["Ines Frajtag", "Marko Švaco", "Filip Šuligoj"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18248v1", "summary": "The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18248v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "手术环境中面部标志点定位性能评估", "tldr": "本文评估了MediaPipe算法在模拟手术环境下进行面部标志点检测的性能，发现其在手术光照下能显著提高大偏航角和俯仰角下的检测精度，并讨论了其在医疗程序中应用的潜力。", "motivation": "在医学领域，机器人和计算机视觉的应用日益广泛，但面部检测算法在可变光照条件和灵活检测位置方面面临挑战，需要评估算法在手术环境下的性能。", "method": "本研究在一个受控环境下测试了MediaPipe算法的面部标志点检测能力。实验使用一个自动调整位置的机械臂，而手术灯和人体模型则保持固定位置。", "result": "研究结果表明，在手术光照下，面部标志点检测的精度提高，显著增强了在大偏航角和俯仰角下的检测性能。然而，由于选定面部标志点的不精确检测，导致标准差/离散度增加。", "conclusion": "这项分析表明MediaPipe算法有潜力集成到医疗程序中。", "translation": "机器人技术、计算机视觉及其应用在包括医学在内的各个领域正变得越来越普遍。许多人脸检测算法已应用于神经外科、眼科和整形外科。使用这些算法的一个常见挑战是可变光照条件和检测位置的灵活性，以识别和精确地定位患者。所提出的实验在一个受控环境中测试了MediaPipe算法检测面部标志点的性能，该环境使用一个自动调整位置的机械臂，而手术灯和人体模型则保持固定位置。这项研究的结果表明，在手术光照下，面部标志点检测精度的提高显著增强了在大偏航角和俯仰角下的检测性能。标准差/离散度的增加是由于选定面部标志点的不精确检测造成的。这项分析允许讨论MediaPipe算法潜在地集成到医疗程序中。", "summary": "本研究评估了MediaPipe算法在模拟手术环境中进行面部标志点定位的性能。通过在受控条件下，利用机械臂调整位置，并固定手术灯和人体模型，研究发现该算法在手术光照下显著提高了在大偏航角和俯仰角下的检测精度。尽管在某些标志点存在检测不精确导致标准差增加的问题，但结果支持MediaPipe算法在医疗程序中应用的潜力。", "keywords": "面部标志点定位, 手术环境, MediaPipe, 计算机视觉, 机器人", "comments": "该研究创新性地将MediaPipe算法应用于模拟手术环境，并系统评估了其在复杂光照和姿态变化下的性能。其重要性在于为计算机视觉技术在医疗领域的实际应用提供了实验依据，特别是在外科手术中对患者进行精确识别和定位的需求。研究指出了算法在特定标志点检测上的局限性，为未来的改进提供了方向。"}}
{"id": "2507.17860", "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17860v1", "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17860v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "通过生成式AI图像合成促进AI皮肤病变分类器公平性评估", "tldr": "本研究利用生成式AI（GenAI）合成图像，以促进AI皮肤病变分类器（特别是黑色素瘤分类器）的公平性评估。结果表明，使用合成数据进行公平性评估有前景，但也指出当评估模型与合成数据的基础训练数据不同时，公平性验证会变得困难。", "motivation": "深度学习在皮肤癌筛查中具有巨大潜力，但存在固有的偏见风险。因此，评估和改进这些系统的公平性至关重要。公平性评估的一个主要挑战是确保评估数据集能够充分代表不同个人身份信息（PII）和少数群体。", "method": "本研究利用最先进的生成式AI（GenAI）LightningDiT模型，通过生成高度逼真的合成数据来评估公开可用的黑色素瘤分类器的公平性。", "result": "结果表明，使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，研究发现，当用于评估的黑色素瘤检测模型所训练的数据与合成图像的基础数据集不同时，验证公平性变得困难。", "conclusion": "本研究提出的方法为利用合成数据评估和增强医疗影像生成式AI系统的公平性提供了一条有价值的新途径。", "translation": "深度学习及其在边缘设备上的应用在彻底改变黑色素瘤等皮肤癌的常规筛查方面具有巨大潜力。伴随这项技术预期效益的同时，也出现了由不可预见和固有偏见带来的潜在危险。因此，评估和改进此类系统的公平性至关重要。公平性评估的一个关键挑战是确保评估数据集充分代表不同的个人身份信息（PII）（性别、年龄和种族）及其他少数群体。在此挑战的背景下，本研究利用最先进的生成式AI（GenAI）LightningDiT模型来评估公开可用的黑色素瘤分类器的公平性。结果表明，使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，我们的发现表明，当用于评估的黑色素瘤检测模型所训练的数据与合成图像的基础数据集不同时，验证公平性变得困难。尽管如此，我们提出我们的方法为利用合成数据衡量和增强医疗影像生成式AI系统的公平性提供了一条有价值的新途径。", "summary": "本研究探讨了利用生成式AI（GenAI）合成图像来促进AI皮肤病变分类器公平性评估的可行性。研究使用LightningDiT模型生成逼真图像，并评估了公开的黑色素瘤分类器。结果显示，使用合成数据进行公平性评估前景广阔，但指出当评估模型的基础训练数据与合成图像的数据源不一致时，公平性验证会面临挑战。该方法为医疗影像GenAI系统的公平性评估和增强提供了一条新途径。", "keywords": "公平性评估, 生成式AI, 图像合成, 皮肤病变分类器, 黑色素瘤", "comments": "这项研究的创新之处在于其利用生成式AI来解决医疗AI公平性评估中数据代表性不足的挑战。通过合成数据来扩展评估数据集，可以更全面地测试模型的偏见。然而，研究也指出了一个重要的局限性，即合成数据与模型训练数据之间的差异可能影响公平性验证的有效性。这提示未来的研究需要关注如何弥合这一差距，确保合成数据的适用性。"}}
{"id": "2507.17519", "title": "Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners", "authors": ["Kostas Karakontis", "Thanos Petsanis", "Athanasios Ch. Kapoutsis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17519v2", "summary": "Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17519v2", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "二维无人机路径规划器的地形感知适应", "tldr": "提出一种模块化算法，使二维无人机路径规划器能感知地形，通过调整高度和相机方向来改善3D重建效果，尤其在垂直特征区域。", "motivation": "现有商业多无人机覆盖路径规划（mCPP）算法将兴趣区域视为二维平面，忽略三维结构特征，导致三维重建不完整，尤其在遮挡或垂直表面。", "method": "提出一种模块化算法，通过调整无人机高度和相机方向，扩展商业二维路径规划器以实现地形感知规划。文中以DARP算法为例，开发了DARP-3D进行验证。", "result": "与基线方法相比，所提出的方法始终能捕获更好的三维重建效果，尤其是在具有显著垂直特征的区域。", "conclusion": "通过调整高度和相机方向，所提出的模块化算法能有效扩展二维无人机路径规划器以实现地形感知规划，显著改善三维重建质量。", "translation": "多无人机覆盖路径规划（mCPP）算法在流行的商业软件中通常只将感兴趣区域视为二维平面，忽略重要的三维结构特征。这导致了不完整的三维重建，尤其是在被遮挡或垂直表面周围。在本文中，我们提出了一种模块化算法，可以通过调整高度和相机方向来扩展商业二维路径规划器，以促进地形感知规划。为了证明其有效性，我们扩展了著名的DARP（Divide Areas for Optimal Multi-Robot Coverage Path Planning）算法，并生成了DARP-3D。我们展示了在多个三维环境中的模拟结果以及使用大疆硬件进行的真实世界飞行测试。与基线方法相比，我们的方法始终能捕获改进的三维重建效果，特别是在具有显著垂直特征的区域。该算法的开源实现可在以下网址获取：https://github.com/konskara/TerraPlan", "summary": "本文提出一种模块化算法，旨在扩展现有商业二维无人机路径规划器，使其能感知地形。通过调整无人机飞行高度和相机方向，该算法解决了传统二维规划器忽略三维结构导致三维重建不完整的问题。作者将此方法应用于DARP算法，创建了DARP-3D，并通过模拟和真实飞行测试验证了其在改善三维重建，尤其是在具有垂直特征区域的有效性。", "keywords": "无人机路径规划, 地形感知, 三维重建, DARP, 覆盖路径规划", "comments": "该论文的创新点在于提供了一种通用的、模块化的方法，能够将现有二维无人机路径规划器升级为三维地形感知能力，而无需从头开发复杂的3D规划器。其重要性在于解决了传统方法在复杂地形下三维重建不完整的问题，对提升无人机测绘、侦察等应用的精度和效率具有实际意义。开源实现也促进了研究和应用。"}}
{"id": "2507.18433", "title": "DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis", "authors": ["Minxi Ouyang", "Lianghui Zhu", "Yaqing Bao", "Qiang Huang", "Jingli Ouyang", "Tian Guan", "Xitong Ling", "Jiawen Li", "Song Duan", "Wenbin Dai", "Li Zheng", "Xuemei Zhang", "Yonghong He"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18433v1", "summary": "Multimodal large models have shown great potential in automating pathology\nimage analysis. However, current multimodal models for gastrointestinal\npathology are constrained by both data quality and reasoning transparency:\npervasive noise and incomplete annotations in public datasets predispose vision\nlanguage models to factual hallucinations when generating diagnostic text,\nwhile the absence of explicit intermediate reasoning chains renders the outputs\ndifficult to audit and thus less trustworthy in clinical practice. To address\nthese issues, we construct a large scale gastrointestinal pathology dataset\ncontaining both microscopic descriptions and diagnostic conclusions, and\npropose a prompt argumentation strategy that incorporates lesion classification\nand anatomical site information. This design guides the model to better capture\nimage specific features and maintain semantic consistency in generation.\nFurthermore, we employ a post training pipeline that combines supervised fine\ntuning with Group Relative Policy Optimization (GRPO) to improve reasoning\nquality and output structure. Experimental results on real world pathology\nreport generation tasks demonstrate that our approach significantly outperforms\nstate of the art open source and proprietary baselines in terms of generation\nquality, structural completeness, and clinical relevance. Our solution\noutperforms state of the art models with 18.7% higher clinical relevance, 32.4%\nimproved structural completeness, and 41.2% fewer diagnostic errors,\ndemonstrating superior accuracy and clinical utility compared to existing\nsolutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18433v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DiagR1: 一种通过强化学习训练的消化病理诊断视觉-语言模型", "tldr": "DiagR1是一个通过强化学习训练的视觉-语言模型，用于消化病理诊断，通过构建大规模数据集、引入提示论证策略和结合GRPO的后训练流程，显著提高了诊断文本的生成质量、结构完整性和临床相关性。", "motivation": "当前用于胃肠道病理学的多模态模型受限于数据质量和推理透明度。公共数据集中普遍存在的噪声和不完整的标注导致视觉语言模型在生成诊断文本时容易出现事实性幻觉，且缺乏明确的中间推理链使得输出难以审计，从而在临床实践中可信度较低。", "method": "我们构建了一个包含显微描述和诊断结论的大规模胃肠道病理数据集，并提出了一种提示论证策略，该策略结合病变分类和解剖部位信息，以引导模型更好地捕捉图像特定特征并保持生成中的语义一致性。此外，我们采用结合监督微调和群组相对策略优化（GRPO）的后训练流程来改善推理质量和输出结构。", "result": "在真实世界病理报告生成任务上的实验结果表明，我们的方法在生成质量、结构完整性和临床相关性方面显著优于最先进的开源和专有基线。我们的解决方案在临床相关性方面提高了18.7%，结构完整性提高了32.4%，诊断错误减少了41.2%，显示出优于现有解决方案的准确性和临床实用性。", "conclusion": "DiagR1模型在消化病理诊断报告生成方面表现出卓越的准确性和临床实用性，显著优于现有最先进的模型。", "translation": "多模态大型模型在自动化病理图像分析方面显示出巨大潜力。然而，当前用于胃肠道病理学的多模态模型受限于数据质量和推理透明度：公共数据集中普遍存在的噪声和不完整的标注使得视觉语言模型在生成诊断文本时容易出现事实性幻觉，而缺乏明确的中间推理链使得输出难以审计，从而在临床实践中可信度较低。为了解决这些问题，我们构建了一个包含显微描述和诊断结论的大规模胃肠道病理数据集，并提出了一种提示论证策略，该策略结合病变分类和解剖部位信息。这种设计引导模型更好地捕捉图像特定特征并在生成中保持语义一致性。此外，我们采用结合监督微调和群组相对策略优化（GRPO）的后训练流程来改善推理质量和输出结构。在真实世界病理报告生成任务上的实验结果表明，我们的方法在生成质量、结构完整性和临床相关性方面显著优于最先进的开源和专有基线。我们的解决方案在临床相关性方面提高了18.7%，结构完整性提高了32.4%，诊断错误减少了41.2%，显示出优于现有解决方案的准确性和临床实用性。", "summary": "本文提出了DiagR1，一个通过强化学习训练的视觉-语言模型，专为消化病理诊断设计。为解决现有模型在数据质量和推理透明度上的限制，研究者构建了一个大规模胃肠道病理数据集，并引入了提示论证策略以增强图像特征捕获和语义一致性。此外，模型采用结合监督微调和群组相对策略优化（GRPO）的后训练流程来优化推理质量和输出结构。实验证明，DiagR1在病理报告生成任务中显著优于现有基线，在临床相关性、结构完整性和诊断错误率方面均有显著提升，展现出卓越的准确性和临床实用性。", "keywords": "视觉-语言模型, 强化学习, 病理诊断, 胃肠道病理, 提示论证", "comments": "本文的创新点在于结合了大规模高质量数据集的构建、创新的提示论证策略以及基于强化学习的后训练流程（GRPO），有效解决了当前多模态模型在病理诊断中面临的数据噪声和推理透明度问题。其在真实世界任务中的显著性能提升，特别是临床相关性和错误率的降低，表明该模型在临床实践中具有重要的应用潜力。"}}
{"id": "2507.18616", "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning", "authors": ["Si-Woo Kim", "MinJu Jeon", "Ye-Chan Kim", "Soeun Lee", "Taewhan Kim", "Dong-Jin Kim"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025", "url": "http://arxiv.org/abs/2507.18616v1", "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.", "comment": "Accepted to ACM Multimedia 2025", "pdf_url": "http://arxiv.org/pdf/2507.18616v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SynC：通过一对多映射对合成图像字幕数据集进行优化以实现零样本图像字幕生成", "tldr": "SynC提出了一种新颖的框架，通过将字幕重新分配给最语义对齐的图像来改进用于零样本图像字幕生成的合成数据集，解决了T2I模型生成的图像与字幕语义不匹配的问题，并显著提高了性能。", "motivation": "零样本图像字幕生成（ZIC）越来越多地利用文本到图像（T2I）模型生成的合成数据集来减少昂贵的人工标注需求。然而，这些T2I模型经常产生与对应输入字幕存在语义错位的图像，导致合成图像-字幕对中存在噪声，这会阻碍模型训练。现有的数据集剪枝技术主要用于清除网络爬取数据中的噪声文本，不适用于合成数据中字幕格式良好但图像可能不准确的独特挑战。", "method": "我们引入了SynC，一个专门用于优化零样本图像字幕生成合成图像-字幕数据集的新颖框架。SynC不采用传统的过滤或重新生成方法，而是专注于将字幕重新分配给合成图像池中已有的语义最对齐的图像。我们的方法采用一对多映射策略，首先为每个字幕检索多个相关的候选图像。然后，我们应用一个受循环一致性启发的对齐评分器，通过验证图像检索原始字幕的能力来选择最佳图像。", "result": "SynC在标准基准测试（MS-COCO、Flickr30k、NoCaps）上，持续且显著地提高了各种零样本图像字幕生成模型的性能，并在多种情况下取得了最先进的结果。", "conclusion": "SynC为整理精炼的合成数据以增强零样本图像字幕生成提供了一种有效的策略。", "translation": "零样本图像字幕生成（ZIC）越来越多地利用文本到图像（T2I）模型生成的合成数据集，以减轻对昂贵人工标注的需求。然而，这些T2I模型通常会生成与其对应输入字幕存在语义错位的图像（例如，缺少对象、属性不正确），导致合成图像-字幕对中存在噪声，这会阻碍模型训练。现有的数据集剪枝技术主要用于清除网络爬取数据中的噪声文本。然而，这些方法不适用于合成数据中独特的挑战，即字幕通常格式良好，但图像可能是不准确的表示。为了解决这一差距，我们引入了SynC，一个专门为优化ZIC合成图像-字幕数据集设计的新颖框架。SynC不采用传统的过滤或重新生成，而是专注于将字幕重新分配给合成图像池中已有的语义最对齐的图像。我们的方法采用一对多映射策略，首先为每个字幕检索多个相关的候选图像。然后，我们应用一个受循环一致性启发的对齐评分器，通过验证其通过图像到文本检索检索原始字幕的能力来选择最佳图像。广泛的评估表明，SynC在标准基准测试（MS-COCO、Flickr30k、NoCaps）上，持续且显著地提高了各种ZIC模型的性能，并在多种情况下取得了最先进的结果。SynC为整理精炼的合成数据以增强ZIC提供了一种有效的策略。", "summary": "SynC是一个新颖的框架，旨在解决文本到图像模型在生成合成数据集时，图像与字幕之间存在的语义错位问题，这些错位会阻碍零样本图像字幕生成（ZIC）模型的训练。与传统的过滤或重新生成方法不同，SynC采用一对多映射策略，通过检索多个候选图像并使用循环一致性启发的对齐评分器，将字幕重新分配给合成图像池中最语义对齐的图像。实验证明，SynC显著提升了各种ZIC模型在多个标准基准上的性能，并取得了最先进的结果，为优化合成数据以增强ZIC提供了一种有效策略。", "keywords": "合成图像字幕, 零样本图像字幕, 数据集优化, 文本到图像, 语义对齐", "comments": "SynC的创新之处在于其独特的“一对多映射”和“循环一致性启发对齐评分器”方法，它不直接过滤或重新生成图像，而是通过重新分配现有图像来优化数据集。这种方法有效地解决了合成数据中图像与文本语义不匹配的特定挑战，提升了零样本图像字幕生成的性能，具有重要的实践价值。"}}
{"id": "2503.21676", "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "authors": ["Nicolas Zucchet", "Jörg Bornschein", "Stephanie Chan", "Andrew Lampinen", "Razvan Pascanu", "Soham De"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the 2nd Conference on Language Modeling (2025)", "url": "http://arxiv.org/abs/2503.21676v2", "summary": "Large language models accumulate vast knowledge during pre-training, yet the\ndynamics governing this acquisition remain poorly understood. This work\ninvestigates the learning dynamics of language models on a synthetic factual\nrecall task, uncovering three key findings: First, language models learn in\nthree phases, exhibiting a performance plateau before acquiring precise factual\nknowledge. Mechanistically, this plateau coincides with the formation of\nattention-based circuits that support recall. Second, the training data\ndistribution significantly impacts learning dynamics, as imbalanced\ndistributions lead to shorter plateaus. Finally, hallucinations emerge\nsimultaneously with knowledge, and integrating new knowledge into the model\nthrough fine-tuning is challenging, as it quickly corrupts its existing\nparametric memories. Our results emphasize the importance of data distribution\nin knowledge acquisition and suggest novel data scheduling strategies to\naccelerate neural network training.", "comment": "Accepted at the 2nd Conference on Language Modeling (2025)", "pdf_url": "http://arxiv.org/pdf/2503.21676v2", "cate": "cs.CL", "date": "2025-03-27", "updated": "2025-07-24", "AI": {"title_translation": "语言模型如何学习事实？动力学、课程和幻觉", "tldr": "本研究通过合成事实回忆任务，揭示了语言模型学习事实的三个阶段、数据分布对学习动态的影响，以及幻觉与知识同时出现的问题，强调了数据分布在知识获取中的重要性。", "motivation": "大型语言模型在预训练过程中积累了大量知识，但其知识获取的动态机制尚不清楚。", "method": "本研究在合成事实回忆任务上，调查了语言模型的学习动态。", "result": "1. 语言模型以三个阶段学习，在获得精确事实知识之前表现出性能高原期，该高原期与支持回忆的基于注意力的电路形成同时发生。2. 训练数据分布显著影响学习动态，不平衡的分布导致更短的高原期。3. 幻觉与知识同时出现，通过微调将新知识整合到模型中具有挑战性，因为它会迅速破坏现有参数记忆。", "conclusion": "研究结果强调了数据分布在知识获取中的重要性，并提出了加速神经网络训练的新颖数据调度策略。", "translation": "大型语言模型在预训练期间积累了大量的知识，但控制这种获取的动态机制仍然知之甚少。这项工作调查了语言模型在合成事实回忆任务上的学习动态，揭示了三个关键发现：首先，语言模型分三个阶段学习，在获得精确事实知识之前表现出性能高原期。从机制上讲，这个高原期与支持回忆的基于注意力的电路的形成相吻合。其次，训练数据分布显著影响学习动态，因为不平衡的分布会导致更短的高原期。最后，幻觉与知识同时出现，并且通过微调将新知识整合到模型中具有挑战性，因为它会迅速破坏其现有的参数记忆。我们的结果强调了数据分布在知识获取中的重要性，并提出了新颖的数据调度策略以加速神经网络训练。", "summary": "本研究通过在合成事实回忆任务上调查语言模型的学习动态，揭示了其知识获取的三个关键发现：语言模型学习经历性能高原期，这与注意力机制的形成有关；数据分布对学习动态有显著影响，不平衡分布会缩短高原期；幻觉与知识同步产生，且微调整合新知识容易破坏现有记忆。研究强调了数据分布在知识获取中的重要性，并建议采用新的数据调度策略来加速神经网络训练。", "keywords": "语言模型, 事实学习, 学习动态, 数据分布, 幻觉", "comments": "这项工作深入探讨了语言模型学习事实的内在机制，特别是揭示了学习过程中的阶段性特征和注意力回路的形成。其创新之处在于通过合成任务来解耦和理解复杂的学习动态。研究发现数据分布对知识获取和幻觉产生具有关键影响，这对于优化模型训练和减少幻觉具有重要指导意义。提出的数据调度策略为未来的模型训练提供了新的思路，具有潜在的实践价值。"}}
{"id": "2507.18022", "title": "Does visualization help AI understand data?", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      5 pages, 6 figures", "url": "http://arxiv.org/abs/2507.18022v1", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization.", "comment": "5 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18022v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "可视化有助于AI理解数据吗？", "tldr": "研究表明，AI系统（如GPT 4.1和Claude 3.5）在有散点图辅助时，能更精确地描述数据，尤其是在数据复杂时，这表明AI也能从可视化中受益。", "motivation": "探究图表和图形是否也能像帮助人类一样，对AI系统分析数据有所帮助。", "method": "通过一系列实验，使用GPT 4.1和Claude 3.5这两个商业视觉-语言模型。在三个代表性分析任务中，比较原始数据单独提供和伴随散点图提供时，AI系统描述合成数据集的精确度和准确性。还与提供空白图表和数据不匹配图表的基线进行比较。", "result": "当原始数据伴随散点图时，两个系统能更精确、更准确地描述合成数据集，尤其是在数据集复杂性增加时。与基线比较表明，性能提升是由于图表内容所致。", "conclusion": "初步证据表明，AI系统像人类一样，可以从数据可视化中受益。", "translation": "图表和图形帮助人们分析数据，但它们对AI系统也有用吗？为了调查这个问题，我们对两个商业视觉-语言模型：GPT 4.1和Claude 3.5进行了一系列实验。在三个代表性分析任务中，当原始数据伴随散点图时，这两个系统能更精确、更准确地描述合成数据集，尤其是在数据集复杂性增加时。与两个基线——提供空白图表和提供数据不匹配图表——的比较表明，性能的提升是由于图表内容所致。我们的结果是AI系统，像人类一样，可以从可视化中受益的初步证据。", "summary": "本研究旨在探究可视化图表是否能帮助AI系统理解数据。通过对GPT 4.1和Claude 3.5两个视觉-语言模型进行实验，发现在有散点图辅助的情况下，AI系统能更精确、准确地描述合成数据集，尤其是在数据复杂时。实验对比显示，这种性能提升源于图表内容本身。研究结果初步证明AI系统也能像人类一样从数据可视化中获益。", "keywords": "AI, 可视化, 数据理解, 视觉-语言模型, 散点图", "comments": "这项研究提出了一个有趣且重要的方向，即如何通过可视化来增强AI对数据的理解能力。其创新之处在于将人类数据分析的辅助工具——图表，应用到AI系统中，并提供了初步的实验证据。这可能为未来AI数据处理和解释提供新的范式，尤其是在需要AI进行复杂数据分析和决策的领域。然而，研究仅使用了散点图和合成数据集，未来需要更广泛的图表类型和真实世界数据的验证。"}}
{"id": "2507.17865", "title": "Talk with the Things: Integrating LLMs into IoT Networks", "authors": ["Alakesh Kalita"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2407.20970", "url": "http://arxiv.org/abs/2507.17865v1", "summary": "The convergence of Large Language Models (LLMs) and Internet of Things (IoT)\nnetworks open new opportunities for building intelligent, responsive, and\nuser-friendly systems. This work presents an edge-centric framework that\nintegrates LLMs into IoT architectures to enable natural language-based\ncontrol, context-aware decision-making, and enhanced automation. The proposed\nmodular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are\ndeployed on edge computing devices connected to IoT gateways, enabling local\nprocessing of user commands and sensor data for reduced latency, improved\nprivacy, and enhanced inference quality. We validate the framework through a\nsmart home prototype using LLaMA 3 and Gemma 2B models for controlling smart\ndevices. Experimental results highlight the trade-offs between model accuracy\nand inference time with respect to models size. At last, we also discuss the\npotential applications that can use LLM-based IoT systems, and a few key\nchallenges associated with such systems.", "comment": "arXiv admin note: text overlap with arXiv:2407.20970", "pdf_url": "http://arxiv.org/pdf/2507.17865v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "与物对话：将大型语言模型整合到物联网网络中", "tldr": "该工作提出了一个将LLMs集成到物联网边缘网络的框架，以实现基于自然语言的控制和自动化，并验证了其在智能家居中的有效性。", "motivation": "大型语言模型（LLMs）与物联网（IoT）网络的融合为构建智能、响应迅速且用户友好的系统带来了新的机遇。本研究旨在通过将LLMs整合到IoT架构中，实现基于自然语言的控制、上下文感知决策和增强自动化。", "method": "本研究提出了一个以边缘为中心的框架，该框架将模块化、轻量级的基于检索增强生成（RAG）的LLMs部署在连接到物联网网关的边缘计算设备上。这种部署方式旨在实现用户命令和传感器数据的本地处理，从而降低延迟、提高隐私并增强推理质量。", "result": "通过使用LLaMA 3和Gemma 2B模型控制智能设备的智能家居原型验证了该框架。实验结果突出了模型精度和推理时间与模型大小之间的权衡。", "conclusion": "该研究成功地将大型语言模型集成到物联网架构中，实现了基于自然语言的控制和增强自动化。研究还讨论了LLM-based IoT系统的潜在应用及其面临的关键挑战。", "translation": "大型语言模型（LLMs）与物联网（IoT）网络的融合为构建智能、响应迅速且用户友好的系统带来了新的机遇。这项工作提出了一个以边缘为中心的框架，将LLMs整合到物联网架构中，以实现基于自然语言的控制、上下文感知决策和增强自动化。所提出的模块化、轻量级的基于检索增强生成（RAG）的LLMs部署在连接到物联网网关的边缘计算设备上，从而实现了用户命令和传感器数据的本地处理，以降低延迟、提高隐私和增强推理质量。我们通过使用LLaMA 3和Gemma 2B模型控制智能设备的智能家居原型验证了该框架。实验结果突出了模型精度和推理时间与模型大小之间的权衡。最后，我们还讨论了可以使用基于LLM的物联网系统的潜在应用，以及与此类系统相关的一些关键挑战。", "summary": "这篇论文提出了一个以边缘为中心的框架，旨在将大型语言模型（LLMs）集成到物联网（IoT）网络中，以实现基于自然语言的控制、上下文感知决策和自动化。该框架将模块化、轻量级的RAG-based LLMs部署在边缘设备上，以降低延迟、提高隐私并增强推理质量。通过智能家居原型验证了该框架，并分析了模型大小对精度和推理时间的影响，同时讨论了潜在应用和挑战。", "keywords": "大型语言模型, 物联网, 边缘计算, 自然语言处理, 智能家居", "comments": "该研究通过将LLMs部署到物联网边缘设备上，为实现智能家居等场景中的自然语言交互和自动化提供了创新方法。其关注边缘计算的部署方式有助于解决传统云端LLM带来的延迟和隐私问题。然而，模型精度与推理时间之间的权衡是未来需要进一步优化的关键挑战。"}}
{"id": "2507.17951", "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at the ICML 2025 Workshop on Assessing World Models", "url": "http://arxiv.org/abs/2507.17951v1", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "pdf_url": "http://arxiv.org/pdf/2507.17951v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "LLM的信念更新与贝叶斯定理一致吗？", "tldr": "研究发现，更大、能力更强的预训练语言模型在信念更新上与贝叶斯定理更一致。", "motivation": "探究大型语言模型（LLM）在面对上下文证据时，其“信念”更新是否与贝叶斯定理更一致。", "method": "提出了一种贝叶斯一致性系数（BCC）指标，并生成了用于衡量BCC的数据集。对五个模型家族的多个预训练语言模型进行了BCC测量，并与模型参数、训练数据量以及常见基准测试得分进行比较。", "result": "结果表明，更大、能力更强的预训练语言模型所分配的置信度与贝叶斯定理更加一致。", "conclusion": "这些结果对我们理解和管理大型语言模型具有重要意义。", "translation": "大型且能力更强的语言模型在上下文证据下更新其命题“信念”时，是否与贝叶斯定理更一致？为了验证这一点，我们制定了一个贝叶斯一致性系数（BCC）指标，并生成了一个用于测量BCC的数据集。我们测量了五个模型家族中多个仅预训练的语言模型的BCC，并与模型参数数量、训练数据量以及常见基准测试的模型分数进行了比较。我们的结果为以下假设提供了证据：更大、能力更强的预训练语言模型分配的置信度与贝叶斯定理更具一致性。这些结果对我们理解和治理大型语言模型具有重要意义。", "summary": "本文研究了大型语言模型（LLM）在接收上下文证据时，其信念更新是否与贝叶斯定理更一致。研究人员提出了贝叶斯一致性系数（BCC）指标，并构建了相应数据集，对不同规模的预训练LLM进行了测试。结果显示，模型越大、能力越强，其信念更新与贝叶斯定理的一致性越高，这对于理解和管理LLM具有重要启示。", "keywords": "大型语言模型, 贝叶斯定理, 信念更新, 一致性, BCC指标", "comments": "这项研究通过引入贝叶斯一致性系数（BCC）这一创新指标，为评估大型语言模型（LLM）的推理能力提供了一个新的视角。它不仅验证了LLM在特定条件下的贝叶斯一致性，还揭示了模型规模与此一致性之间的正相关关系，这对LLM的未来发展和应用治理具有重要意义。"}}
{"id": "2507.18350", "title": "Speech Enhancement with Dual-path Multi-Channel Linear Prediction Filter and Multi-norm Beamforming", "authors": ["Chengyuan Qin", "Wenmeng Xiong", "Jing Zhou", "Maoshen Jia", "Changchun Bao"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Paper accepted by Interspeech 2025", "url": "http://arxiv.org/abs/2507.18350v1", "summary": "In this paper, we propose a speech enhancement method us ing dual-path\nMulti-Channel Linear Prediction (MCLP) filters\n  and multi-norm beamforming. Specifically, the MCLP part in\n  the proposed method is designed with dual-path filters in both\n  time and frequency dimensions. For the beamforming part, we\n  minimize the power of the microphone array output as well as\n  the l1 norm of the denoised signals while preserving source sig nals from the\ntarget directions. An efficient method to select the\n  prediction orders in the dual-path filters is also proposed, which\n  is robust for signals with different reverberation time (T60) val ues and can\nbe applied to other MCLP-based methods. Eval uations demonstrate that our\nproposed method outperforms the\n  baseline methods for speech enhancement, particularly in high\n  reverberation scenarios.", "comment": "Paper accepted by Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.18350v1", "cate": "eess.AS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "语音增强与双路径多通道线性预测滤波器和多范数波束成形", "tldr": "提出一种结合双路径MCLP滤波器和多范数波束成形的语音增强方法，在高混响环境下表现优于基线方法。", "motivation": "改善语音增强性能，尤其是在高混响环境下。", "method": "本文提出了一种新的语音增强方法，结合了双路径多通道线性预测（MCLP）滤波器和多范数波束成形。MCLP部分在时间和频率维度上设计了双路径滤波器。波束成形部分旨在最小化麦克风阵列输出功率以及去噪信号的L1范数，同时保留目标方向的源信号。此外，还提出了一种高效选择双路径滤波器预测阶数的方法，该方法对不同混响时间（T60）值的信号具有鲁棒性，并可应用于其他基于MCLP的方法。", "result": "实验评估表明，所提出的方法在语音增强方面优于基线方法，特别是在高混响场景中。", "conclusion": "论文提出了一种有效的双路径MCLP滤波器和多范数波束成形相结合的语音增强方法，在高混响环境下表现出优越的性能。", "translation": "本文提出了一种结合双路径多通道线性预测（MCLP）滤波器和多范数波束成形的语音增强方法。具体而言，所提出方法中的MCLP部分在时间和频率维度上都设计了双路径滤波器。对于波束成形部分，我们在保留来自目标方向的源信号的同时，最小化麦克风阵列输出的功率以及去噪信号的L1范数。本文还提出了一种高效选择双路径滤波器中预测阶数的方法，该方法对具有不同混响时间（T60）值的信号具有鲁棒性，并且可以应用于其他基于MCLP的方法。评估表明，我们提出的方法在语音增强方面优于基线方法，特别是在高混响场景中。", "summary": "本文提出了一种新颖的语音增强方法，该方法结合了双路径多通道线性预测（MCLP）滤波器和多范数波束成形。MCLP部分采用时间-频率双路径设计，而波束成形则通过最小化输出功率和L1范数来去噪并保留目标信号。此外，还提出了一种鲁棒的预测阶数选择方法。实验结果表明，该方法在语音增强，尤其是在高混响环境下，性能优于现有基线方法。", "keywords": "语音增强, 双路径, 多通道线性预测, 波束成形, 混响抑制", "comments": "该论文提出了一种创新的语音增强方法，结合了双路径MCLP滤波器和多范数波束成形，特别强调了其在高混响环境下的优越性能。此外，提出的预测阶数选择方法也具有普适性，可应用于其他基于MCLP的方法，增加了其潜在影响。"}}
{"id": "2507.18114", "title": "Nonconvex Optimization Framework for Group-Sparse Feedback Linear-Quadratic Optimal Control I: Penalty Approach", "authors": ["Lechen Feng", "Xun Li", "Yuan-Hua Ni"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18114v1", "summary": "This paper develops a unified nonconvex optimization framework for the design\nof group-sparse feedback controllers in infinite-horizon linear-quadratic (LQ)\nproblems. We address two prominent extensions of the classical LQ problem: the\ndistributed LQ problem with fixed communication topology (DFT-LQ) and the\nsparse feedback LQ problem (SF-LQ), both of which are motivated by the need for\nscalable and structure-aware control in large-scale systems. Unlike existing\napproaches that rely on convex relaxations or are limited to block-diagonal\nstructures, we directly formulate the controller synthesis as a\nfinite-dimensional nonconvex optimization problem with group $\\ell_0$-norm\nregularization, capturing general sparsity patterns. We establish a connection\nbetween DFT-LQ and SF-LQ problems, showing that both can be addressed within\nour unified framework. Furthermore, we propose a penalty-based proximal\nalternating linearized minimization (PALM) algorithm and provide a rigorous\nconvergence analysis under mild assumptions, overcoming the lack of coercivity\nin the objective function. The proposed method admits efficient solvers for all\nsubproblems and guarantees global convergence to critical points. Our results\nfill a key gap in the literature by enabling the direct design of group-sparse\nfeedback gains with theoretical guarantees, without resorting to convex\nsurrogates or restrictive structural assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18114v1", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于组稀疏反馈线性二次最优控制的非凸优化框架 I：惩罚方法", "tldr": "本文提出了一个统一的非凸优化框架，用于设计组稀疏反馈控制器，解决了大规模系统中的分布式和稀疏反馈LQ问题，并提供了一种基于惩罚的PALM算法及其收敛性分析，填补了现有文献的空白。", "motivation": "现有方法依赖于凸松弛或仅限于块对角结构，无法直接设计具有理论保证的组稀疏反馈增益。本文旨在解决大规模系统中可扩展和结构感知控制的需求，特别是分布式LQ问题（DFT-LQ）和稀疏反馈LQ问题（SF-LQ）。", "method": "本文开发了一个统一的非凸优化框架，将控制器综合直接表述为一个带有组$\\\\ell_0$-范数正则化的有限维非凸优化问题，以捕捉一般的稀疏模式。该框架能够处理分布式LQ问题（DFT-LQ）和稀疏反馈LQ问题（SF-LQ）。此外，提出了一种基于惩罚的近端交替线性化最小化（PALM）算法，并对其在温和假设下的收敛性进行了严格分析。", "result": "本文建立了DFT-LQ和SF-LQ问题之间的联系，表明两者都可以在所提出的统一框架内解决。所提出的PALM算法对所有子问题都支持高效求解器，并保证全局收敛到临界点。", "conclusion": "本文的工作通过直接设计具有理论保证的组稀疏反馈增益，而无需依赖凸替代或限制性结构假设，填补了现有文献中的一个关键空白。", "translation": "本文提出了一种统一的非凸优化框架，用于在无限时域线性二次（LQ）问题中设计组稀疏反馈控制器。我们解决了经典LQ问题的两个突出扩展：具有固定通信拓扑的分布式LQ问题（DFT-LQ）和稀疏反馈LQ问题（SF-LQ），两者都是由大规模系统中对可扩展和结构感知控制的需求所驱动的。与依赖凸松弛或仅限于块对角结构的现有方法不同，我们直接将控制器综合表述为一个带有组$\\\\ell_0$-范数正则化的有限维非凸优化问题，捕捉一般的稀疏模式。我们建立了DFT-LQ和SF-LQ问题之间的联系，表明两者都可以在我们的统一框架内解决。此外，我们提出了一种基于惩罚的近端交替线性化最小化（PALM）算法，并在温和假设下提供了严格的收敛性分析，克服了目标函数缺乏强制性的问题。所提出的方法对所有子问题都支持高效求解器，并保证全局收敛到临界点。我们的结果填补了文献中的一个关键空白，通过直接设计具有理论保证的组稀疏反馈增益，而无需诉诸凸替代或限制性结构假设。", "summary": "本文提出了一个统一的非凸优化框架，用于在无限时域线性二次（LQ）问题中设计组稀疏反馈控制器。该框架解决了分布式LQ（DFT-LQ）和稀疏反馈LQ（SF-LQ）问题，这些问题源于大规模系统对可扩展和结构感知控制的需求。与现有方法不同，本文直接将控制器综合表述为带有组$\\\\ell_0$-范数正则化的非凸优化问题，以捕捉一般稀疏模式。文章还提出了一种基于惩罚的近端交替线性化最小化（PALM）算法，并提供了严格的收敛性分析。该方法能够高效求解子问题并保证全局收敛到临界点，从而实现了具有理论保证的组稀疏反馈增益的直接设计。", "keywords": "非凸优化, 组稀疏, 线性二次控制, 反馈控制, PALM算法", "comments": "本文的创新之处在于直接采用非凸优化方法，特别是使用组$\\\\ell_0$-范数正则化来处理组稀疏反馈控制器设计，而非依赖于传统的凸松弛或限制性结构假设。这使得其能够捕捉更一般的稀疏模式。提出的PALM算法及其严格的收敛性分析，克服了目标函数缺乏强制性的挑战，为大规模系统中的稀疏控制提供了一个具有理论保证的有效工具，填补了该领域的关键空白。其重要性在于为未来复杂系统控制器设计提供了新的范式。"}}
{"id": "2507.17785", "title": "Self-similarity Analysis in Deep Neural Networks", "authors": ["Jingyi Ding", "Chengwen Qi", "Hongfei Wang", "Jianshe Wu", "Licheng Jiao", "Yuwei Guo", "Jian Gao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17785v1", "summary": "Current research has found that some deep neural networks exhibit strong\nhierarchical self-similarity in feature representation or parameter\ndistribution. However, aside from preliminary studies on how the power-law\ndistribution of weights across different training stages affects model\nperformance,there has been no quantitative analysis on how the self-similarity\nof hidden space geometry influences model weight optimization, nor is there a\nclear understanding of the dynamic behavior of internal neurons. Therefore,\nthis paper proposes a complex network modeling method based on the output\nfeatures of hidden-layer neurons to investigate the self-similarity of feature\nnetworks constructed at different hidden layers, and analyzes how adjusting the\ndegree of self-similarity in feature networks can enhance the classification\nperformance of deep neural networks. Validated on three types of networks MLP\narchitectures, convolutional networks, and attention architectures this study\nreveals that the degree of self-similarity exhibited by feature networks varies\nacross different model architectures. Furthermore, embedding constraints on the\nself-similarity of feature networks during the training process can improve the\nperformance of self-similar deep neural networks (MLP architectures and\nattention architectures) by up to 6 percentage points.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17785v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "深度神经网络中的自相似性分析", "tldr": "本研究提出了一种基于隐藏层神经元输出特征的复杂网络建模方法，用于量化分析深度神经网络中特征网络的自相似性，并发现通过在训练过程中嵌入自相似性约束，可以提高某些网络架构的分类性能。", "motivation": "当前研究发现深度神经网络存在分层自相似性，但缺乏对隐藏空间几何自相似性如何影响模型权重优化以及内部神经元动态行为的定量分析和清晰理解。", "method": "本文提出一种基于隐藏层神经元输出特征的复杂网络建模方法，用于研究不同隐藏层构建的特征网络的自相似性。通过调整特征网络的自相似性程度，分析其如何增强深度神经网络的分类性能。此外，还在训练过程中嵌入了特征网络自相似性的约束。", "result": "研究发现特征网络表现出的自相似性程度在不同模型架构（MLP、卷积网络、注意力架构）中有所不同。在训练过程中对特征网络的自相似性嵌入约束，可以将自相似深度神经网络（MLP架构和注意力架构）的性能提高高达6个百分点。", "conclusion": "通过对深度神经网络中特征网络的自相似性进行定量分析和调控，可以有效提升某些网络架构的分类性能。", "translation": "当前研究发现一些深度神经网络在特征表示或参数分布上表现出很强的分层自相似性。然而，除了关于权重在不同训练阶段的幂律分布如何影响模型性能的初步研究之外，尚未对隐藏空间几何的自相似性如何影响模型权重优化进行定量分析，也缺乏对内部神经元动态行为的清晰理解。因此，本文提出一种基于隐藏层神经元输出特征的复杂网络建模方法，用于研究在不同隐藏层构建的特征网络的自相似性，并分析调整特征网络的自相似性程度如何增强深度神经网络的分类性能。本研究在三种类型的网络架构（MLP架构、卷积网络和注意力架构）上进行了验证，结果表明特征网络表现出的自相似性程度在不同模型架构中有所不同。此外，在训练过程中对特征网络的自相似性嵌入约束，可以将自相似深度神经网络（MLP架构和注意力架构）的性能提高高达6个百分点。", "summary": "本研究旨在解决深度神经网络中自相似性对模型优化和内部神经元动态行为影响缺乏定量分析的问题。为此，论文提出一种基于隐藏层神经元输出特征的复杂网络建模方法，用以量化分析不同隐藏层中特征网络的自相似性，并探讨通过调整自相似性来提升分类性能。研究在MLP、卷积和注意力网络上进行验证，发现不同架构的自相似性程度有差异，且在训练中加入自相似性约束能使MLP和注意力网络性能提升高达6个百分点，证实了自相似性对深度学习模型性能优化的重要作用。", "keywords": "自相似性, 深度神经网络, 复杂网络, 特征表示, 分类性能", "comments": "该论文的创新点在于首次对深度神经网络中隐藏空间几何的自相似性进行了定量分析，并提出了通过复杂网络建模方法来研究和调控特征网络的自相似性。其重要性体现在揭示了自相似性与模型性能之间的潜在关联，并通过实验证明了在训练过程中嵌入自相似性约束能够有效提升部分深度学习模型的性能，为深度学习模型的优化提供了新的视角和方法。"}}
{"id": "2504.21603", "title": "Flow Through Porous Media: A Hopf-Cole Transformation Approach for Modeling Pressure-Dependent Viscosity", "authors": ["V. S. Maduri", "K. B. Nakshatrala"], "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21603v2", "summary": "Most organic liquids exhibit a pressure-dependent viscosity, making it\ncrucial to consider this behavior in applications where pressures significantly\nexceed ambient conditions (e.g., geological carbon sequestration). Mathematical\nmodels describing flow through porous media while accounting for\nviscosity-pressure dependence are nonlinear (e.g., the Barus model). This\nnonlinearity complicates mathematical analysis and makes numerical solutions\nmore time-intensive and prone to convergence issues. In this paper, we\ndemonstrate that the Hopf-Cole transformation, originally developed for\nBurgers' equation, can recast the governing equations -- describing flow\nthrough porous media with pressure-dependent viscosity -- into a linear form.\nThe transformed equations, resembling Darcy's equations in the transformed\nvariables, enable (a) systematic mathematical analysis to establish uniqueness\nand maximum principles, (b) the derivation of a mechanics-based principle, and\n(c) the development of efficient numerical solutions using solvers optimized\nfor Darcy equations. Notably, many properties of the linear Darcy equations\nnaturally extend to nonlinear models that depend on pressure. For example,\nsolutions to these nonlinear models adhere to a reciprocal relation analogous\nto that observed in Darcy's equations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21603v2", "cate": "math.NA", "date": "2025-04-30", "updated": "2025-07-24", "AI": {"title_translation": "多孔介质流：一种霍普夫-科尔变换方法用于建模压力依赖性粘度", "tldr": "本文利用霍普夫-科尔变换将具有压力依赖性粘度的多孔介质流的控制方程线性化，从而简化了数学分析并提高了数值解的效率。", "motivation": "大多数有机液体表现出压力依赖性粘度，在显著高于环境压力的应用（如地质碳封存）中考虑此行为至关重要。然而，描述此类流动的数学模型是非线性的，这使得数学分析复杂化，并导致数值求解耗时且易出现收敛问题。", "method": "本文证明，最初为Burgers方程开发的霍普夫-科尔变换可以将描述具有压力依赖性粘度的多孔介质流的控制方程重铸为线性形式。", "result": "变换后的线性方程（在变换变量中类似于达西方程）使得：(a) 能够进行系统的数学分析以建立唯一性和最大值原理；(b) 能够推导基于力学的原理；(c) 能够使用针对达西方程优化的求解器开发高效的数值解。此外，线性达西方程的许多性质自然地扩展到这些非线性模型。", "conclusion": "霍普夫-科尔变换能够有效地线性化具有压力依赖性粘度的多孔介质流的复杂非线性模型，从而实现更鲁棒的数学分析和高效的数值解，同时保留了线性系统的关键性质。", "translation": "大多数有机液体表现出压力依赖性粘度，这使得在压力显著超过环境条件的应用中（例如，地质碳封存）考虑这种行为至关重要。描述多孔介质流并考虑粘度-压力依赖性的数学模型是非线性的（例如，Barus模型）。这种非线性使数学分析复杂化，并使数值解更加耗时且容易出现收敛问题。在本文中，我们证明了最初为Burgers方程开发的霍普夫-科尔变换可以将描述具有压力依赖性粘度的多孔介质流的控制方程重铸为线性形式。变换后的方程，在变换变量中类似于达西方程，使得（a）能够进行系统的数学分析以建立唯一性和最大值原理，（b）能够推导基于力学的原理，以及（c）能够使用针对达西方程优化的求解器开发高效的数值解。值得注意的是，线性达西方程的许多性质自然地扩展到依赖于压力的非线性模型。例如，这些非线性模型的解遵循类似于达西方程中观察到的互易关系。", "summary": "本文旨在解决在压力依赖性粘度条件下多孔介质流建模的挑战。作者展示了霍普夫-科尔变换能够将描述此类流动的非线性控制方程线性化，使其形式类似于达西方程。这种线性化显著简化了数学分析，支持新原理的推导，并促进了更高效数值解的开发，同时表明线性达西方程的性质可以扩展到这些非线性模型。", "keywords": "多孔介质, 压力依赖性粘度, 霍普夫-科尔变换, 非线性模型, 达西定律", "comments": "该论文的创新之处在于将霍普夫-科尔变换应用于多孔介质中特定类别的非线性流动问题，有效地将复杂的非线性问题转化为更易处理的线性问题。这种方法在理论分析（唯一性、原理）和实际计算效率方面都具有显著优势，对于地质碳封存等应用具有重要价值。"}}
{"id": "2507.17943", "title": "Automated Brake Onset Detection in Naturalistic Driving Data", "authors": ["Shu-Yuan Liu", "Johan Engström", "Gustav Markkula"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17943v1", "summary": "Response timing measures play a crucial role in the assessment of automated\ndriving systems (ADS) in collision avoidance scenarios, including but not\nlimited to establishing human benchmarks and comparing ADS to human driver\nresponse performance. For example, measuring the response time (of a human\ndriver or ADS) to a conflict requires the determination of a stimulus onset and\na response onset. In existing studies, response onset relies on manual\nannotation or vehicle control signals such as accelerator and brake pedal\nmovements. These methods are not applicable when analyzing large scale data\nwhere vehicle control signals are not available. This holds in particular for\nthe rapidly expanding sets of ADS log data where the behavior of surrounding\nroad users is observed via onboard sensors. To advance evaluation techniques\nfor ADS and enable measuring response timing when vehicle control signals are\nnot available, we developed a simple and efficient algorithm, based on a\npiecewise linear acceleration model, to automatically estimate brake onset that\ncan be applied to any type of driving data that includes vehicle longitudinal\ntime series data. We also proposed a manual annotation method to identify brake\nonset and used it as ground truth for validation. R2 was used as a confidence\nmetric to measure the accuracy of the algorithm, and its classification\nperformance was analyzed using naturalistic collision avoidance data of both\nADS and humans, where our method was validated against human manual annotation.\nAlthough our algorithm is subject to certain limitations, it is efficient,\ngeneralizable, applicable to any road user and scenario types, and is highly\nconfigurable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17943v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "自然驾驶数据中自动刹车起始点检测", "tldr": "开发了一种基于分段线性加速度模型的算法，用于在没有车辆控制信号的情况下自动检测刹车起始点，以评估自动驾驶系统在碰撞避免场景中的响应性能。", "motivation": "现有方法（手动标注或依赖车辆控制信号）不适用于大规模数据分析，尤其是在缺少车辆控制信号的自动驾驶系统日志数据中，而响应时间在评估自动驾驶系统碰撞避免场景中至关重要。", "method": "开发了一种基于分段线性加速度模型的简单高效算法来自动估计刹车起始点，适用于包含车辆纵向时间序列数据的任何驾驶数据。同时提出了一种手动标注方法作为真值进行验证，并使用R2作为置信度指标，在自动驾驶系统和人类的自然碰撞避免数据上验证了算法的分类性能。", "result": "该算法高效、通用、适用于任何道路使用者和场景类型，且高度可配置。", "conclusion": "该算法能够有效解决大规模数据中刹车起始点检测的问题，并为自动驾驶系统的评估提供了新的工具，尤其在车辆控制信号不可用时。", "translation": "响应时间测量在碰撞避免场景中评估自动驾驶系统（ADS）方面发挥着关键作用，包括但不限于建立人类基准和比较ADS与人类驾驶员的响应性能。例如，测量（人类驾驶员或ADS的）冲突响应时间需要确定刺激起始点和响应起始点。在现有研究中，响应起始点依赖于手动标注或车辆控制信号，如油门和刹车踏板的运动。当分析无法获取车辆控制信号的大规模数据时，这些方法不适用。这尤其适用于快速增长的ADS日志数据集，其中通过车载传感器观察周围道路使用者的行为。为了推进ADS的评估技术，并在车辆控制信号不可用时测量响应时间，我们开发了一种简单高效的算法，基于分段线性加速度模型，自动估计刹车起始点，该算法可应用于包含车辆纵向时间序列数据的任何类型的驾驶数据。我们还提出了一种手动标注方法来识别刹车起始点，并将其用作验证的真值。R2被用作置信度指标来衡量算法的准确性，并使用ADS和人类的自然碰撞避免数据分析了其分类性能，其中我们的方法通过人类手动标注进行了验证。尽管我们的算法存在某些限制，但它高效、通用、适用于任何道路使用者和场景类型，并且高度可配置。", "summary": "本文提出了一种基于分段线性加速度模型的简单高效算法，用于在缺乏车辆控制信号的情况下，自动从纵向时间序列数据中检测刹车起始点。该方法旨在解决现有手动标注或依赖车辆控制信号的方法在大规模自动驾驶系统日志数据分析中的局限性。研究通过手动标注作为真值，并使用R2指标，在自动驾驶系统和人类的自然驾驶数据上验证了算法的准确性和分类性能。该算法被证明是高效、通用且高度可配置的，为自动驾驶系统评估中响应时间测量提供了新工具。", "keywords": "刹车起始点检测, 自动驾驶系统, 自然驾驶数据, 响应时间, 分段线性加速度模型", "comments": "这篇论文的创新点在于提出了一个不依赖于传统车辆控制信号（如踏板输入）的自动刹车起始点检测算法，这对于分析大规模、尤其是传感器数据为主的自动驾驶日志数据具有重要意义。其通用性和可配置性是其主要优势，使得该方法可以广泛应用于不同场景和道路使用者。尽管存在局限性，但它为自动驾驶系统的评估提供了一个实用的新工具。"}}
{"id": "2507.18098", "title": "Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes", "authors": ["Kosuke Sugiyama", "Masato Uchida"], "categories": ["cs.LG", "68T01", "I.5"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      32 pages, 11 figures", "url": "http://arxiv.org/abs/2507.18098v1", "summary": "In scenarios where training data is limited due to observation costs or data\nscarcity, enriching the label information associated with each instance becomes\ncrucial for building high-accuracy classification models. In such contexts, it\nis often feasible to obtain not only hard labels but also {\\it additional\nsupervision}, such as the confidences for the hard labels. This setting\nnaturally raises fundamental questions: {\\it What kinds of additional\nsupervision are intrinsically beneficial?} And {\\it how do they contribute to\nimproved generalization performance?} To address these questions, we propose a\ntheoretical framework that treats both hard labels and additional supervision\nas probability distributions, and constructs soft labels through their affine\ncombination. Our theoretical analysis reveals that the essential component of\nadditional supervision is not the confidence score of the assigned hard label,\nbut rather the information of the distribution over the non-hard-labeled\nclasses. Moreover, we demonstrate that the additional supervision and the\nmixing coefficient contribute to the refinement of soft labels in complementary\nroles. Intuitively, in the probability simplex, the additional supervision\ndetermines the direction in which the deterministic distribution representing\nthe hard label should be adjusted toward the true label distribution, while the\nmixing coefficient controls the step size along that direction. Through\ngeneralization error analysis, we theoretically characterize how the additional\nsupervision and its mixing coefficient affect both the convergence rate and\nasymptotic value of the error bound. Finally, we experimentally demonstrate\nthat, based on our theory, designing additional supervision can lead to\nimproved classification accuracy, even when utilized in a simple manner.", "comment": "32 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.18098v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "从硬标签中学习，并对非硬标签类施加额外监督", "tldr": "本文提出了一个理论框架，用于结合硬标签和对非硬标签类的额外监督来构建软标签，从而在数据有限的情况下提高分类模型的准确性。", "motivation": "在训练数据因观察成本或数据稀缺而有限的场景中，丰富每个实例的标签信息对于构建高精度分类模型至关重要。在这种情况下，除了硬标签外，通常还可以获得额外的监督信息，例如硬标签的置信度。", "method": "本文提出了一个理论框架，将硬标签和额外监督都视为概率分布，并通过它们的仿射组合构建软标签。通过泛化误差分析，理论上表征了额外监督及其混合系数如何影响误差界的收敛速度和渐近值。", "result": "理论分析表明，额外监督的关键组成部分不是分配的硬标签的置信度分数，而是非硬标签类上的分布信息。此外，额外监督和混合系数在完善软标签方面发挥互补作用。直观地，在概率单纯形中，额外监督决定了代表硬标签的确定性分布应向真实标签分布调整的方向，而混合系数控制着沿该方向的步长。实验证明，基于本文的理论，设计额外监督可以提高分类精度，即使是以简单的方式使用。", "conclusion": "基于本文的理论，设计额外监督即使以简单的方式利用，也能提高分类精度。", "translation": "在训练数据因观察成本或数据稀缺而有限的场景中，丰富每个实例的标签信息对于构建高精度分类模型至关重要。在这种情况下，通常不仅可以获得硬标签，还可以获得额外监督，例如硬标签的置信度。这种设置自然引出了基本问题：哪些额外监督本质上是有益的？以及它们如何有助于提高泛化性能？为了解决这些问题，我们提出了一个理论框架，将硬标签和额外监督都视为概率分布，并通过它们的仿射组合构建软标签。我们的理论分析表明，额外监督的关键组成部分不是分配的硬标签的置信度分数，而是非硬标签类上的分布信息。此外，我们证明了额外监督和混合系数在软标签的完善中发挥互补作用。直观地，在概率单纯形中，额外监督决定了代表硬标签的确定性分布应向真实标签分布调整的方向，而混合系数控制着沿该方向的步长。通过泛化误差分析，我们理论上表征了额外监督及其混合系数如何影响误差界的收敛速度和渐近值。最后，我们通过实验证明，基于我们的理论，设计额外监督可以提高分类精度，即使是以简单的方式利用。", "summary": "本文针对训练数据有限的问题，提出了一种理论框架，旨在通过结合硬标签和对非硬标签类的额外监督来提高分类模型的准确性。该框架将硬标签和额外监督视为概率分布，并通过仿射组合构建软标签。研究发现，额外监督的关键在于非硬标签类的分布信息，而非硬标签的置信度。此外，额外监督和混合系数在完善软标签方面发挥互补作用，并理论上分析了它们对泛化误差界的影响。实验结果验证了基于该理论设计的额外监督能够有效提升分类精度。", "keywords": "硬标签, 额外监督, 软标签, 泛化性能, 分类精度", "comments": "本文的创新之处在于提供了一个利用额外监督的理论框架，特别强调了非硬标签类信息的重要性，而非仅仅关注硬标签的置信度。这为在数据稀缺场景下提升模型泛化能力提供了一种新颖且理论支撑的方法，具有重要的实践指导意义。"}}
{"id": "2507.18392", "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy", "authors": ["Asaf Yehudai", "Lilach Eden", "Yotam Perlitz", "Roy Bar-Haim", "Michal Shmueli-Scheuer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18392v1", "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18392v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CLEAR：基于LLM作为评判者的错误分析变得简单", "tldr": "CLEAR是一个开源交互式工具包，旨在通过生成实例级文本反馈、识别系统级错误问题并量化其普遍性，来简化和深化基于LLM的错误分析，从而弥补现有LLM评估范式中缺乏可操作性原因的不足。", "motivation": "当前大语言模型（LLM）的评估范式通常只提供单一的分数或排名，只能回答哪个模型更好，但无法解释具体的原因。这种顶层分数掩盖了模型性能背后的特定、可操作的原因，而这些原因对于理解和改进模型至关重要。本文旨在弥补这一差距。", "method": "本文提出了CLEAR，一个交互式、开源的基于LLM的错误分析工具包。CLEAR首先生成每个实例的文本反馈，然后创建一套系统级错误问题，并量化每个已识别问题的普遍性。该软件包还提供一个交互式仪表板，用户可以通过聚合可视化进行全面的错误分析，应用交互式过滤器来隔离特定问题或分数范围，并深入到单个实例以查看特定的行为模式。", "result": "通过对RAG和数学基准的分析，以及用户案例研究，本文展示了CLEAR的实用性。", "conclusion": "CLEAR通过提供细粒度的错误分析和可视化工具，有效弥补了现有LLM评估中缺乏可操作性原因的不足，显著简化和深化了基于LLM的错误分析过程。", "translation": "大语言模型（LLM）的评估越来越依赖于其他LLM作为评判者。然而，当前的评估范式通常只产生一个单一的分数或排名，回答的是哪个模型更好，而不是为什么。虽然这对基准测试至关重要，但这些顶层分数掩盖了模型性能背后具体的、可操作的原因。为了弥补这一差距，我们引入了CLEAR，一个用于基于LLM的错误分析的交互式开源软件包。CLEAR首先生成每个实例的文本反馈，然后创建一套系统级错误问题，并量化每个已识别问题的普遍性。我们的软件包还为用户提供了一个交互式仪表板，该仪表板允许通过聚合可视化进行全面的错误分析，应用交互式过滤器来隔离特定问题或分数范围，并深入到单个实例以查看特定的行为模式。我们展示了CLEAR在RAG和数学基准上的分析，并通过用户案例研究展示了其效用。", "summary": "本文介绍了一个名为CLEAR的交互式开源软件包，旨在改进大语言模型（LLM）的错误分析。针对当前LLM评估仅提供总分而缺乏具体原因的不足，CLEAR通过生成实例级文本反馈、识别系统级错误问题并量化其普遍性来提供深入分析。它还提供一个交互式仪表板，支持聚合可视化、过滤特定问题以及深入查看具体实例，从而使用户能够理解模型性能背后的可操作性原因。该工具已在RAG和数学基准测试中得到验证。", "keywords": "错误分析, LLM-as-a-Judge, 大语言模型, 评估, CLEAR", "comments": "CLEAR的创新之处在于它将LLM作为评判者的能力从简单的评分提升到了细致的错误分析，填补了当前LLM评估中“知其然不知其所以然”的空白。其开源和交互式仪表板的设计，大大降低了LLM错误分析的门槛，对于模型开发者理解和改进模型具有重要意义。特别是在RAG和数学等复杂任务上，这种细粒度的分析能力尤为宝贵。"}}
{"id": "2507.18498", "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction", "authors": ["Zongzheng Zhang", "Xuchong Qiu", "Boran Zhang", "Guantian Zheng", "Xunjiang Gu", "Guoxuan Chi", "Huan-ang Gao", "Leichen Wang", "Ziming Liu", "Xinrun Li", "Igor Gilitschenski", "Hongyang Li", "Hang Zhao", "Hao Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.18498v1", "summary": "Recent advances in autonomous driving are moving towards mapless approaches,\nwhere High-Definition (HD) maps are generated online directly from sensor data,\nreducing the need for expensive labeling and maintenance. However, the\nreliability of these online-generated maps remains uncertain. While\nincorporating map uncertainty into downstream trajectory prediction tasks has\nshown potential for performance improvements, current strategies provide\nlimited insights into the specific scenarios where this uncertainty is\nbeneficial. In this work, we first analyze the driving scenarios in which\nmapping uncertainty has the greatest positive impact on trajectory prediction\nand identify a critical, previously overlooked factor: the agent's kinematic\nstate. Building on these insights, we propose a novel Proprioceptive Scenario\nGating that adaptively integrates map uncertainty into trajectory prediction\nbased on forecasts of the ego vehicle's future kinematics. This lightweight,\nself-supervised approach enhances the synergy between online mapping and\ntrajectory prediction, providing interpretability around where uncertainty is\nadvantageous and outperforming previous integration methods. Additionally, we\nintroduce a Covariance-based Map Uncertainty approach that better aligns with\nmap geometry, further improving trajectory prediction. Extensive ablation\nstudies confirm the effectiveness of our approach, achieving up to 23.6%\nimprovement in mapless trajectory prediction performance over the\nstate-of-the-art method using the real-world nuScenes driving dataset. Our\ncode, data, and models are publicly available at\nhttps://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.", "comment": "Accepted to IROS 2025, Project Page:\n  https://ethan-zheng136.github.io/Dev-Unc/", "pdf_url": "http://arxiv.org/pdf/2507.18498v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "深入探究无地图轨迹预测中的地图不确定性", "tldr": "本文提出了一种新的方法，通过自适应地整合地图不确定性并考虑智能体的运动状态，显著提高了无地图轨迹预测的性能，并在nuScenes数据集上实现了高达23.6%的改进。", "motivation": "当前的自动驾驶系统正朝着无地图方法发展，即在线生成高清地图，以减少昂贵的标注和维护成本。然而，这些在线生成地图的可靠性仍然不确定。尽管将地图不确定性纳入下游轨迹预测任务显示出潜在的性能提升，但现有策略对这种不确定性何时有益的特定场景提供有限的见解。", "method": "本文首先分析了地图不确定性对轨迹预测影响最大的驾驶场景，并识别出一个关键的、以前被忽视的因素：智能体的运动状态。在此基础上，提出了一种新颖的“本体感知场景门控”（Proprioceptive Scenario Gating）方法，该方法根据自我车辆未来运动状态的预测，自适应地将地图不确定性整合到轨迹预测中。此外，还引入了一种“基于协方差的地图不确定性”（Covariance-based Map Uncertainty）方法，以更好地与地图几何对齐，进一步提高轨迹预测。", "result": "本文提出的方法增强了在线地图生成和轨迹预测之间的协同作用，提供了不确定性何时有利的可解释性，并优于以前的集成方法。在真实世界的nuScenes驾驶数据集上，与现有最先进的方法相比，无地图轨迹预测性能提高了高达23.6%。", "conclusion": "本文通过引入本体感知场景门控和基于协方差的地图不确定性方法，有效地将地图不确定性整合到无地图轨迹预测中，显著提升了预测性能，并提供了对不确定性益处的场景可解释性。", "translation": "近年来，自动驾驶领域的进步正朝着无地图方法发展，即直接从传感器数据在线生成高精度（HD）地图，从而减少昂贵的标注和维护需求。然而，这些在线生成地图的可靠性仍然存在不确定性。尽管将地图不确定性纳入下游轨迹预测任务已显示出潜在的性能改进，但目前的策略对于这种不确定性在特定场景中何时有益提供有限的见解。在这项工作中，我们首先分析了地图不确定性对轨迹预测产生最大积极影响的驾驶场景，并识别出一个关键的、以前被忽视的因素：智能体的运动状态。基于这些见解，我们提出了一种新颖的本体感知场景门控（Proprioceptive Scenario Gating）方法，该方法根据自我车辆未来运动状态的预测，自适应地将地图不确定性整合到轨迹预测中。这种轻量级、自监督的方法增强了在线地图生成和轨迹预测之间的协同作用，提供了关于不确定性何时有利的可解释性，并优于以前的集成方法。此外，我们引入了一种基于协方差的地图不确定性（Covariance-based Map Uncertainty）方法，该方法更好地与地图几何对齐，进一步提高了轨迹预测。广泛的消融研究证实了我们方法的有效性，在真实世界的nuScenes驾驶数据集上，与现有最先进的方法相比，无地图轨迹预测性能提高了高达23.6%。我们的代码、数据和模型可在https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction 公开获取。", "summary": "本文针对无地图自动驾驶中在线生成地图的可靠性问题，提出了一种将地图不确定性有效整合到轨迹预测中的新方法。研究发现智能体的运动状态是影响不确定性效益的关键因素。在此基础上，提出了本体感知场景门控和基于协方差的地图不确定性方法，以自适应地利用不确定性，提高预测性能并提供可解释性。实验结果表明，该方法在nuScenes数据集上比现有技术提高了高达23.6%的性能。", "keywords": "无地图轨迹预测, 地图不确定性, 本体感知场景门控, 运动状态, 自动驾驶", "comments": "该论文的创新点在于识别出智能体运动状态是影响地图不确定性在轨迹预测中效益的关键因素，并基于此提出了本体感知场景门控方法。此外，引入基于协方差的地图不确定性方法，使其更好地与地图几何对齐，也增加了其实用性。其自监督和轻量级的特性使其在实际应用中具有潜力，并为理解不确定性在何种场景下发挥作用提供了可解释性，这对于自动驾驶系统的可靠性至关重要。"}}
{"id": "2507.18459", "title": "Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning", "authors": ["Amir Najjar", "Riad Mokadem", "Jean-Marc Pierson"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18459v1", "summary": "The rapid growth of global data volumes has created a demand for scalable\ndistributed systems that can maintain a high quality of service. Data\nreplication is a widely used technique that provides fault tolerance, improved\nperformance and higher availability. Traditional implementations often rely on\nthreshold-based activation mechanisms, which can vary depending on workload\nchanges and system architecture. System administrators typically bear the\nresponsibility of adjusting these thresholds. To address this challenge,\nreinforcement learning can be used to dynamically adapt to workload changes and\ndifferent architectures. In this paper, we propose a novel data replication\nstrategy for cloud systems that employs reinforcement learning to automatically\nlearn system characteristics and adapt to workload changes. The strategy's aim\nis to provide satisfactory Quality of Service while optimizing a trade-off\nbetween provider profit and environmental impact. We present the architecture\nbehind our solution and describe the reinforcement learning model by defining\nthe states, actions and rewards.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18459v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用强化学习设计云系统中的能源感知数据复制策略", "tldr": "本文提出了一种基于强化学习的云系统数据复制策略，旨在自动适应工作负载变化，优化服务质量、提供商利润和环境影响之间的权衡。", "motivation": "全球数据量的快速增长对可扩展的分布式系统提出了更高的要求，以保持高质量服务。数据复制是提供容错、提高性能和可用性的常用技术，但传统基于阈值的机制需要系统管理员手动调整以适应工作负载和系统架构变化。为了解决这一挑战，需要一种能够动态适应工作负载变化的自动化方法。", "method": "本文提出了一种新颖的云系统数据复制策略，该策略利用强化学习自动学习系统特性并适应工作负载变化。该策略旨在在提供令人满意的服务质量的同时，优化提供商利润和环境影响之间的权衡。文中还介绍了解决方案的架构，并通过定义状态、动作和奖励来描述强化学习模型。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "全球数据量的快速增长对能够保持高质量服务的可扩展分布式系统提出了需求。数据复制是一种广泛使用的技术，可提供容错、改善性能和提高可用性。传统的实现通常依赖于基于阈值的激活机制，这些机制会根据工作负载变化和系统架构而变化。系统管理员通常负责调整这些阈值。为了解决这一挑战，强化学习可用于动态适应工作负载变化和不同的架构。在本文中，我们提出了一种新颖的云系统数据复制策略，该策略采用强化学习来自动学习系统特性并适应工作负载变化。该策略的目标是在优化提供商利润和环境影响之间的权衡的同时，提供令人满意的服务质量。我们介绍了我们解决方案背后的架构，并通过定义状态、动作和奖励来描述强化学习模型。", "summary": "本文提出了一种基于强化学习的云系统数据复制策略，以应对传统阈值机制在适应工作负载变化和系统架构方面的不足。该策略旨在通过自动学习系统特性，在保证服务质量的同时，优化提供商利润和环境影响之间的权衡。文章还详细阐述了该解决方案的架构和强化学习模型（包括状态、动作和奖励的定义）。", "keywords": "数据复制, 强化学习, 云系统, 能源感知, 服务质量", "comments": "该论文的创新点在于将强化学习应用于云系统的数据复制策略，以实现能源感知和自动适应工作负载变化，从而解决了传统手动阈值调整的局限性。这种方法有望在提高服务质量的同时，兼顾经济效益和环境可持续性，对于构建更智能、更高效的云基础设施具有重要意义。"}}
{"id": "2303.01256", "title": "Choosing Public Datasets for Private Machine Learning via Gradient Subspace Distance", "authors": ["Xin Gu", "Gautam Kamath", "Zhiwei Steven Wu"], "categories": ["stat.ML", "cs.CR", "cs.CV", "cs.DS", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      Accepted to SaTML 2025", "url": "http://arxiv.org/abs/2303.01256v2", "summary": "Differentially private stochastic gradient descent privatizes model training\nby injecting noise into each iteration, where the noise magnitude increases\nwith the number of model parameters. Recent works suggest that we can reduce\nthe noise by leveraging public data for private machine learning, by projecting\ngradients onto a subspace prescribed by the public data. However, given a\nchoice of public datasets, it is not a priori clear which one may be most\nappropriate for the private task. We give an algorithm for selecting a public\ndataset by measuring a low-dimensional subspace distance between gradients of\nthe public and private examples. We provide theoretical analysis demonstrating\nthat the excess risk scales with this subspace distance. This distance is easy\nto compute and robust to modifications in the setting. Empirical evaluation\nshows that trained model accuracy is monotone in this distance.", "comment": "Accepted to SaTML 2025", "pdf_url": "http://arxiv.org/pdf/2303.01256v2", "cate": "stat.ML", "date": "2023-03-02", "updated": "2025-07-23", "AI": {"title_translation": "通过梯度子空间距离选择用于隐私机器学习的公共数据集", "tldr": "本文提出了一种通过测量梯度子空间距离来选择用于隐私机器学习的公共数据集的算法，该距离与模型准确性相关，并有助于减少隐私噪声。", "motivation": "在差分隐私随机梯度下降中，噪声大小随模型参数的数量增加。为了减少噪声，可以利用公共数据进行隐私机器学习。然而，现有方法没有明确指出如何选择最合适的公共数据集。本文旨在解决这一公共数据集选择的挑战。", "method": "本文提出了一种算法，通过测量公共和私有示例梯度之间的低维子空间距离来选择公共数据集。该距离易于计算且对设置修改具有鲁棒性。", "result": "理论分析表明，过量风险与所提出的梯度子空间距离成比例。经验评估显示，训练模型的准确性与该距离呈单调关系。", "conclusion": "所提出的梯度子空间距离提供了一种有效且鲁棒的方法，用于选择公共数据集，以提高隐私机器学习中的模型准确性。", "translation": "差分隐私随机梯度下降通过在每次迭代中注入噪声来使模型训练私有化，其中噪声大小随模型参数的数量而增加。最近的工作表明，我们可以通过利用公共数据进行隐私机器学习来减少噪声，方法是将梯度投影到由公共数据规定的子空间上。然而，给定多个公共数据集的选择，事先不清楚哪一个可能最适合隐私任务。我们提出了一种算法，通过测量公共和私有示例梯度之间的低维子空间距离来选择公共数据集。我们提供了理论分析，表明过量风险与该子空间距离成比例。该距离易于计算，并且对设置中的修改具有鲁棒性。经验评估表明，训练模型的准确性与该距离单调相关。", "summary": "本文提出了一种用于差分隐私随机梯度下降中公共数据集选择的算法。该方法通过测量公共和私有数据梯度之间的低维子空间距离来选择最佳公共数据集。理论分析证明了过量风险与此距离成比例，而经验评估则显示了训练模型准确性与此距离的单调关系。该方法易于计算且鲁棒，为提高隐私机器学习的性能提供了一种有效途径。", "keywords": "差分隐私机器学习, 公共数据集, 梯度子空间距离, 数据集选择, 隐私保护AI", "comments": "本文解决了隐私机器学习中如何有效利用公共数据以减少隐私噪声的关键实践问题。所提出的梯度子空间距离是一个创新且直观的度量标准，具有坚实的理论基础和经验验证，为数据集选择提供了实用的解决方案。其鲁棒性和易于计算的特点是显著优势。"}}
{"id": "2507.18398", "title": "Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation", "authors": ["Kwong Ho Li", "Wathsala Karunarathne"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.18398v1", "summary": "This paper investigates the application of Reinforcement Learning (RL) to\noptimise call routing in call centres to minimise client waiting time and staff\nidle time. Two methods are compared: a model-based approach using Value\nIteration (VI) under known system dynamics, and a model-free approach using\nProximal Policy Optimisation (PPO) that learns from experience. For the\nmodel-based approach, a theoretical model is used, while a simulation model\ncombining Discrete Event Simulation (DES) with the OpenAI Gym environment is\ndeveloped for model-free learning. Both models frame the problem as a Markov\nDecision Process (MDP) within a Skills-Based Routing (SBR) framework, with\nPoisson client arrivals and exponentially distributed service and abandonment\ntimes. For policy evaluation, random, VI, and PPO policies are evaluated using\nthe simulation model. After 1,000 test episodes, PPO consistently achives the\nhighest rewards, along with the lowest client waiting time and staff idle time,\ndespite requiring longer training time.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.18398v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用强化学习优化呼叫中心运营：价值迭代对比近端策略优化", "tldr": "本研究比较了价值迭代（VI）和近端策略优化（PPO）两种强化学习方法在优化呼叫中心路由中的应用，旨在最小化客户等待时间和员工空闲时间。结果显示PPO表现最佳，尽管训练时间更长。", "motivation": "本研究旨在通过应用强化学习来优化呼叫中心的呼叫路由，以最小化客户等待时间和员工空闲时间。", "method": "研究比较了两种强化学习方法：一种是基于模型的价值迭代（VI），用于已知系统动态；另一种是无模型的近端策略优化（PPO），通过经验学习。对于基于模型的方法，使用了理论模型；对于无模型学习，开发了一个结合离散事件模拟（DES）和OpenAI Gym环境的模拟模型。两种模型都将问题构建为技能路由（SBR）框架内的马尔可夫决策过程（MDP），并假设客户到达遵循泊松分布，服务和放弃时间遵循指数分布。策略评估使用模拟模型对随机、VI和PPO策略进行评估。", "result": "经过1,000次测试，PPO在一致获得最高奖励的同时，也实现了最低的客户等待时间和员工空闲时间，尽管它需要更长的训练时间。", "conclusion": "PPO在优化呼叫中心运营方面表现出卓越的性能，能够有效降低客户等待时间和员工空闲时间，尽管其训练时间相对较长，但其效果优于价值迭代。", "translation": "本文研究了强化学习（RL）在优化呼叫中心呼叫路由中的应用，旨在最小化客户等待时间和员工空闲时间。文中比较了两种方法：一种是基于模型的价值迭代（VI），用于已知系统动态；另一种是无模型的近端策略优化（PPO），通过经验学习。对于基于模型的方法，使用了理论模型；而对于无模型学习，则开发了一个结合离散事件模拟（DES）和OpenAI Gym环境的模拟模型。两种模型都将问题构建为技能路由（SBR）框架内的马尔可夫决策过程（MDP），其中客户到达遵循泊松分布，服务和放弃时间遵循指数分布。对于策略评估，使用模拟模型对随机、VI和PPO策略进行评估。经过1,000次测试，PPO始终获得最高奖励，同时实现了最低的客户等待时间和员工空闲时间，尽管它需要更长的训练时间。", "summary": "本研究探讨了强化学习在优化呼叫中心呼叫路由中的应用，旨在减少客户等待和员工空闲时间。文章对比了基于模型的价值迭代（VI）和无模型的近端策略优化（PPO）两种方法。研究将问题建模为马尔可夫决策过程，并结合离散事件模拟和OpenAI Gym环境进行PPO的模拟学习。实验结果表明，PPO在降低等待时间和空闲时间方面表现最佳，尽管其训练时间较长。", "keywords": "强化学习, 呼叫中心, 价值迭代, 近端策略优化, 呼叫路由", "comments": "本文的创新点在于将两种不同的强化学习范式（基于模型的VI和无模型的PPO）应用于呼叫中心运营优化，并提供了详细的比较。其重要性在于为实际呼叫中心管理提供了有效的优化策略，特别是PPO的优越表现为未来智能呼叫路由系统提供了有力的技术支持。局限性可能在于PPO较长的训练时间在某些实时性要求高的场景中可能是一个挑战，且模拟环境与实际情况的差距。"}}
{"id": "2507.18523", "title": "The Moral Gap of Large Language Models", "authors": ["Maciej Skorski", "Alina Landowska"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      preprint", "url": "http://arxiv.org/abs/2507.18523v1", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.", "comment": "preprint", "pdf_url": "http://arxiv.org/pdf/2507.18523v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大型语言模型的道德鸿沟", "tldr": "本研究发现，大型语言模型在道德推理方面存在显著的性能差距，表现为高假阴性率和系统性道德内容检测不足，即使经过提示工程也无法弥补，表明任务特定的微调仍优于提示。", "motivation": "道德基础检测对于分析社会话语和开发符合伦理的AI系统至关重要。然而，大型语言模型在多任务中表现出色，但其在专门的道德推理方面的性能尚不明确。", "method": "本研究首次对最先进的大型语言模型和微调的Transformer模型在Twitter和Reddit数据集上进行了全面比较，使用了ROC、PR和DET曲线分析。", "result": "结果显示存在显著的性能差距，大型语言模型表现出高假阴性率和对道德内容的系统性检测不足，即使进行了提示工程。", "conclusion": "这些发现表明，在道德推理应用中，任务特定的微调仍然优于提示。", "translation": "道德基础检测对于分析社会话语和开发符合伦理的AI系统至关重要。虽然大型语言模型在各种任务中表现出色，但它们在专门的道德推理方面的性能仍不清楚。本研究首次对最先进的大型语言模型和微调的Transformer模型在Twitter和Reddit数据集上进行了全面比较，使用了ROC、PR和DET曲线分析。结果显示存在显著的性能差距，大型语言模型表现出高假阴性率和对道德内容的系统性检测不足，即使进行了提示工程。这些发现表明，在道德推理应用中，任务特定的微调仍然优于提示。", "summary": "本研究探讨了大型语言模型在道德推理方面的表现，并将其与微调的Transformer模型进行了比较。研究发现，尽管进行了提示工程，大型语言模型在道德内容检测上仍存在显著的性能差距，表现为高假阴性率和系统性检测不足。这表明在道德推理任务中，任务特定的微调仍然优于简单的提示方法。", "keywords": "大型语言模型, 道德推理, 微调, 提示工程, 性能差距", "comments": "这项研究揭示了当前大型语言模型在道德推理能力上的一个关键局限性，即存在“道德鸿沟”。它强调了在开发伦理AI系统时，仅仅依靠提示工程可能不足以弥补LLM在特定复杂推理任务上的不足，并指出任务特定的微调仍然是提升性能的有效途径。这对于未来AI的伦理对齐和安全发展具有重要指导意义。"}}
{"id": "2507.18533", "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation", "authors": ["Magnus Bengtsson", "Kenneth Östberg"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.18533v1", "summary": "We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.18533v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "C2G-KD：基于PCA约束生成器的数据无关知识蒸馏", "tldr": "C2G-KD是一种数据无关知识蒸馏框架，通过PCA约束的类条件生成器合成样本，即使只有少量真实数据也能有效训练。", "motivation": "该研究旨在解决在数据受限或不可用的场景下进行知识蒸馏的挑战，通过引入一个不依赖真实训练数据生成合成样本的框架。", "method": "C2G-KD训练一个类条件生成器，该生成器在没有真实训练数据的情况下，通过语义和结构损失以及源自PCA的几何约束（从每类仅两个真实样本估计的类特异性PCA子空间）来生成合成样本，以激活冻结教师模型的输出。", "result": "在MNIST数据集上的实验表明，即使每类只有极少的真实样本（例如两个），也足以建立有用的合成训练流程，证明了最小的类结构足以引导有效的合成训练管道。", "conclusion": "C2G-KD证明了在数据有限的情况下，通过结合几何约束和生成器，可以有效地进行数据无关知识蒸馏，为数据稀缺或隐私敏感场景下的模型训练提供了新途径。", "translation": "我们引入了C2G-KD，一个数据无关知识蒸馏框架，其中训练一个类条件生成器，在冻结的教师模型和源自PCA的几何约束的指导下生成合成样本。该生成器从不观察真实的训练数据，而是通过语义和结构损失的组合来学习激活教师模型的输出。通过将生成的样本约束在从每类仅两个真实样本估计出的类特异性PCA子空间内，我们保留了拓扑一致性和多样性。在MNIST上的实验表明，即使是最小的类结构也足以启动有用的合成训练流程。", "summary": "C2G-KD是一个创新的数据无关知识蒸馏框架，它通过训练一个类条件生成器来合成样本。该生成器在没有真实训练数据的情况下工作，而是利用冻结教师模型的输出和基于PCA的几何约束（仅需每类少量真实样本估计）来生成具有拓扑一致性和多样性的合成数据。实验证明，即使只有极少量真实数据，该方法也能有效进行知识蒸馏。", "keywords": "数据无关知识蒸馏, PCA, 生成器, 合成样本, C2G-KD", "comments": "C2G-KD的创新之处在于其在数据不可用或受限情况下的知识蒸馏能力。通过巧妙地结合PCA几何约束和生成器，它仅需极少量真实样本（甚至每类两个）即可引导合成样本生成，这对于隐私保护、数据稀缺或冷启动场景下的模型部署和知识迁移具有重要应用潜力。"}}
{"id": "2403.09554", "title": "Cloud gap-filling with deep learning for improved grassland monitoring", "authors": ["Iason Tsardanidis", "Alkiviadis Koukos", "Vasileios Sitokonstantinou", "Thanassis Drivas", "Charalampos Kontoes"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published in Computers and Electronics in Agriculture", "url": "http://arxiv.org/abs/2403.09554v2", "summary": "Uninterrupted optical image time series are crucial for the timely monitoring\nof agricultural land changes, particularly in grasslands. However, the\ncontinuity of such time series is often disrupted by clouds. In response to\nthis challenge, we propose an innovative deep learning method that integrates\ncloud-free optical (Sentinel-2) observations and weather-independent\n(Sentinel-1) Synthetic Aperture Radar (SAR) data. Our approach employs a hybrid\narchitecture combining Convolutional Neural Networks (CNNs) and Recurrent\nNeural Networks (RNNs) to generate continuous Normalized Difference Vegetation\nIndex (NDVI) time series, highlighting the role of NDVI in the synergy between\nSAR and optical data. We demonstrate the significance of observation continuity\nby assessing the impact of the generated NDVI time series on the downstream\ntask of grassland mowing event detection. We conducted our study in Lithuania,\na country characterized by extensive cloud coverage, and compared our approach\nwith alternative interpolation techniques (i.e., linear, Akima, quadratic). Our\nmethod outperformed these techniques, achieving an average Mean Absolute Error\n(MAE) of 0.024 and a coefficient of determination R^2 of 0.92. Additionally,\nour analysis revealed improvement in the performance of the mowing event\ndetection, with F1-score up to 84% using two widely applied mowing detection\nmethodologies. Our method also effectively mitigated sudden shifts and noise\noriginating from cloudy observations, which are often missed by conventional\ncloud masks and adversely affect mowing detection precision.", "comment": "Published in Computers and Electronics in Agriculture", "pdf_url": "http://arxiv.org/pdf/2403.09554v2", "cate": "cs.CV", "date": "2024-03-14", "updated": "2025-07-24", "AI": {"title_translation": "基于深度学习的云间隙填充技术改进草地监测", "tldr": "本文提出了一种创新的深度学习方法，通过结合光学和SAR数据来填充云层导致的图像时间序列空白，从而生成连续的NDVI时间序列，显著改善了草地割草事件的检测精度。", "motivation": "农业土地变化（特别是草地）的及时监测需要不间断的光学图像时间序列，但云层经常会中断这些时间序列的连续性。", "method": "本文提出了一种创新的深度学习方法，该方法整合了无云的光学（Sentinel-2）观测数据和不受天气影响的合成孔径雷达（Sentinel-1）数据。该方法采用结合了卷积神经网络（CNNs）和循环神经网络（RNNs）的混合架构，以生成连续的归一化植被指数（NDVI）时间序列，并强调了NDVI在SAR和光学数据协同作用中的作用。", "result": "该方法在立陶宛进行验证，性能优于替代插值技术（如线性、Akima、二次插值），平均平均绝对误差（MAE）为0.024，决定系数R²为0.92。此外，使用两种广泛应用的割草检测方法，割草事件检测的F1-score提高到84%。该方法还有效缓解了传统云掩膜经常遗漏并对割草检测精度产生不利影响的、源自多云观测的突然偏移和噪声。", "conclusion": "该研究表明，所提出的深度学习云间隙填充方法能有效生成连续的NDVI时间序列，显著提高草地监测中割草事件检测的精度，并有效处理云层导致的观测数据中断和噪声问题。", "translation": "不间断的光学图像时间序列对于及时监测农业土地变化，特别是草地，至关重要。然而，此类时间序列的连续性常常被云层中断。为了应对这一挑战，我们提出了一种创新的深度学习方法，该方法整合了无云的光学（Sentinel-2）观测数据和不受天气影响的合成孔径雷达（SAR）数据（Sentinel-1）。我们的方法采用结合了卷积神经网络（CNNs）和循环神经网络（RNNs）的混合架构，以生成连续的归一化植被指数（NDVI）时间序列，突出了NDVI在SAR和光学数据协同作用中的作用。我们通过评估所生成的NDVI时间序列对草地割草事件检测这一下游任务的影响，证明了观测连续性的重要性。我们在立陶宛（一个以广泛云覆盖为特征的国家）进行了研究，并将我们的方法与替代插值技术（即线性、Akima、二次）进行了比较。我们的方法优于这些技术，平均平均绝对误差（MAE）为0.024，决定系数R²为0.92。此外，我们的分析显示，使用两种广泛应用的割草检测方法，割草事件检测的性能有所改善，F1-score高达84%。我们的方法还有效缓解了传统云掩膜经常遗漏并对割草检测精度产生不利影响的、源自多云观测的突然偏移和噪声。", "summary": "本文提出了一种基于深度学习的创新方法，通过融合Sentinel-2光学图像和Sentinel-1 SAR数据，有效填补因云层导致的光学图像时间序列空白，从而生成连续的NDVI时间序列。该方法采用CNNs和RNNs的混合架构，在立陶宛的实验结果表明，其在NDVI生成方面优于传统插值技术，并显著提升了草地割草事件检测的精度，有效克服了云层对监测数据连续性和质量的影响。", "keywords": "深度学习, 云间隙填充, 草地监测, NDVI, SAR数据", "comments": "该论文的创新点在于结合了光学和SAR数据，并利用混合深度学习架构（CNNs+RNNs）来解决云层导致的数据缺失问题，这在草地监测领域具有重要意义。其不仅提供了高质量的连续NDVI时间序列，还直接验证了其对下游任务（如割草事件检测）的积极影响，展现了强大的实际应用潜力。该方法有效处理了传统方法难以解决的云噪声和数据不连续性问题。"}}
{"id": "2502.06788", "title": "EVEv2: Improved Baselines for Encoder-Free Vision-Language Models", "authors": ["Haiwen Diao", "Xiaotong Li", "Yufeng Cui", "Yueze Wang", "Haoge Deng", "Ting Pan", "Wenxuan Wang", "Huchuan Lu", "Xinlong Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures, Accepted by ICCV2025 (highlight)", "url": "http://arxiv.org/abs/2502.06788v2", "summary": "Existing encoder-free vision-language models (VLMs) are rapidly narrowing the\nperformance gap with their encoder-based counterparts, highlighting the\npromising potential for unified multimodal systems with structural simplicity\nand efficient deployment. We systematically clarify the performance gap between\nVLMs using pre-trained vision encoders, discrete tokenizers, and minimalist\nvisual layers from scratch, deeply excavating the under-examined\ncharacteristics of encoder-free VLMs. We develop efficient strategies for\nencoder-free VLMs that rival mainstream encoder-based ones. After an in-depth\ninvestigation, we launch EVEv2.0, a new and improved family of encoder-free\nVLMs. We show that: (i) Properly decomposing and hierarchically associating\nvision and language within a unified model reduces interference between\nmodalities. (ii) A well-designed training strategy enables effective\noptimization for encoder-free VLMs. Through extensive evaluation, our EVEv2.0\nrepresents a thorough study for developing a decoder-only architecture across\nmodalities, demonstrating superior data efficiency and strong vision-reasoning\ncapability. Code is publicly available at: https://github.com/baaivision/EVE.", "comment": "20 pages, 10 figures, Accepted by ICCV2025 (highlight)", "pdf_url": "http://arxiv.org/pdf/2502.06788v2", "cate": "cs.CV", "date": "2025-02-10", "updated": "2025-07-24", "AI": {"title_translation": "EVEv2：改进的无编码器视觉-语言模型基线", "tldr": "EVEv2通过减少模态间干扰和优化训练策略，显著提升了无编码器视觉-语言模型的性能，实现了卓越的数据效率和强大的视觉推理能力，并为开发跨模态的仅解码器架构提供了新的基线。", "motivation": "现有无编码器视觉-语言模型（VLMs）虽然在缩小与基于编码器模型的性能差距，并展现出结构简单、部署高效的潜力，但仍需进一步提升以完全弥合性能差距，充分发挥其优势。", "method": "本文系统性地阐明了不同VLM（使用预训练视觉编码器、离散分词器和从头开始的极简视觉层）之间的性能差距，深入挖掘了无编码器VLM未被充分研究的特性。在此基础上，作者开发了能够与主流基于编码器VLM匹敌的高效策略，并推出了EVEv2.0系列模型。", "result": "研究表明：(i) 在统一模型中正确分解和分层关联视觉和语言可以有效减少模态间的干扰。(ii) 精心设计的训练策略能够实现无编码器VLM的有效优化。通过广泛评估，EVEv2.0展示了卓越的数据效率和强大的视觉推理能力。", "conclusion": "EVEv2.0代表了开发跨模态的仅解码器架构的深入研究，证明了其在数据效率和视觉推理方面的强大能力，为无编码器视觉-语言模型的发展提供了新的高性能基线。", "translation": "现有无编码器视觉-语言模型（VLMs）正在迅速缩小与基于编码器模型的性能差距，这凸显了结构简单、部署高效的统一多模态系统的巨大潜力。我们系统地阐明了使用预训练视觉编码器、离散分词器和从头开始的极简视觉层的VLM之间的性能差距，深入挖掘了无编码器VLM未被充分研究的特性。我们为无编码器VLM开发了能够与主流基于编码器模型匹敌的高效策略。经过深入研究，我们推出了EVEv2.0，这是一个新的、改进的无编码器VLM系列。我们发现：(i) 在统一模型中正确分解和分层关联视觉和语言可以减少模态间的干扰。(ii) 精心设计的训练策略能够有效优化无编码器VLM。通过广泛评估，我们的EVEv2.0代表了开发跨模态仅解码器架构的深入研究，展示了卓越的数据效率和强大的视觉推理能力。代码已在 https://github.com/baaivision/EVE 公开。", "summary": "本文介绍了EVEv2.0，一个改进的无编码器视觉-语言模型系列。作者系统性地研究了无编码器VLM的特性及其与基于编码器模型的性能差距。通过提出分解和分层关联视觉与语言以及优化训练策略，EVEv2.0实现了与主流基于编码器模型相媲美的性能，并展示了卓越的数据效率和强大的视觉推理能力，为开发跨模态的仅解码器架构提供了深入研究和新的基线。", "keywords": "无编码器VLM, 视觉-语言模型, EVEv2, 多模态, 仅解码器架构", "comments": "这篇论文通过深入研究无编码器VLM的特性和优化策略，成功推出了EVEv2.0，显著提升了这类模型的性能，使其能够与基于编码器的主流模型竞争。其创新点在于提出了减少模态间干扰的方法和有效的训练策略，并强调了仅解码器架构的潜力，对于推动统一多模态系统向更简洁、高效方向发展具有重要意义。"}}
{"id": "2507.18262", "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages,9 figures", "url": "http://arxiv.org/abs/2507.18262v1", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.", "comment": "12 pages,9 figures", "pdf_url": "http://arxiv.org/pdf/2507.18262v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ReSem3D：通过细粒度语义接地实现可泛化机器人操作的可精化3D空间约束", "tldr": "ReSem3D是一个机器人操作框架，通过多模态大语言模型（MLLM）和视觉基础模型（VFM）的协同作用，实现细粒度3D空间约束，从而在多样化环境中进行实时、鲁棒且可泛化的机器人操作。", "motivation": "现有语义驱动的3D空间约束方法存在三个主要局限性：1) 约束建模中的语义粒度粗糙；2) 缺乏实时闭环规划；3) 在语义多样化环境中的鲁棒性受损。", "method": "本文提出了ReSem3D，一个统一的机器人操作框架，利用视觉基础模型（VFMs）和多模态大语言模型（MLLMs）的协同作用，实现细粒度视觉接地，并动态构建分层3D空间约束以进行实时操作。具体而言，该框架由MLLMs中的分层递归推理驱动，MLLMs与VFMs交互，通过两阶段（部件级提取和区域级细化）从自然语言指令和RGB-D观测中自动构建3D空间约束。随后，这些约束被编码为关节空间中的实时优化目标，以实现对动态扰动的反应行为。", "result": "在语义丰富的家庭和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下执行多样化操作任务，展现出强大的适应性和泛化能力。", "conclusion": "ReSem3D通过提供细粒度、实时和鲁棒的3D空间约束，有效解决了现有方法的局限性，实现了可泛化的机器人操作。实验结果证明了其强大的适应性和泛化能力。", "translation": "语义驱动的3D空间约束将高级语义表示与低级动作空间对齐，促进了机器人操作中任务理解和执行的统一。多模态大语言模型（MLLMs）和视觉基础模型（VFMs）的协同推理使得跨模态3D空间约束的构建成为可能。然而，现有方法存在三个主要局限性：(1) 约束建模中的语义粒度粗糙，(2) 缺乏实时闭环规划，(3) 在语义多样化环境中的鲁棒性受损。为了解决这些挑战，我们提出了ReSem3D，一个用于语义多样化环境的统一操作框架，它利用VFMs和MLLMs之间的协同作用，实现细粒度视觉接地，并动态构建分层3D空间约束以进行实时操作。具体而言，该框架由MLLMs中的分层递归推理驱动，MLLMs与VFMs交互，通过两个阶段：部件级提取和区域级细化，从自然语言指令和RGB-D观测中自动构建3D空间约束。随后，这些约束被编码为关节空间中的实时优化目标，从而能够对动态扰动产生反应行为。在语义丰富的家庭和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下执行多样化操作任务，展现出强大的适应性和泛化能力。代码和视频可在https://resem3d.github.io获取。", "summary": "ReSem3D是一个统一的机器人操作框架，通过利用多模态大语言模型（MLLMs）和视觉基础模型（VFMs）的协同作用，解决了现有3D空间约束方法在语义粒度、实时规划和环境鲁棒性方面的局限性。它通过分层递归推理实现细粒度视觉接地，并动态构建分层3D空间约束，以支持实时操作。实验证明，ReSem3D在零样本条件下能执行多样化的操作任务，展现出强大的适应性和泛化能力。", "keywords": "3D空间约束, 机器人操作, 语义接地, MLLMs, VFMs", "comments": "该论文的创新之处在于通过整合多模态大语言模型（MLLMs）和视觉基础模型（VFMs），解决了3D空间约束在语义粒度、实时性和鲁棒性方面的现有问题。其通过细粒度语义接地和动态、分层约束构建，实现了在多样化环境中零样本执行任务的能力，这对可泛化机器人操作是一个重要的贡献。"}}
{"id": "2507.17876", "title": "Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic", "authors": ["Rıza Özçelik", "Sarah de Ruiter", "Francesca Grisoni"], "categories": ["cs.LG", "physics.chem-ph", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17876v1", "summary": "The scarcity of molecules with desirable properties (i.e., 'positive'\nmolecules) is an inherent bottleneck for generative molecule design. To\nsidestep such obstacle, here we propose molecular task arithmetic: training a\nmodel on diverse and abundant negative examples to learn 'property directions'\n$--$ without accessing any positively labeled data $--$ and moving models in\nthe opposite property directions to generate positive molecules. When analyzed\non 20 zero-shot design experiments, molecular task arithmetic generated more\ndiverse and successful designs than models trained on positive molecules.\nMoreover, we employed molecular task arithmetic in dual-objective and few-shot\ndesign tasks. We find that molecular task arithmetic can consistently increase\nthe diversity of designs while maintaining desirable design properties. With\nits simplicity, data efficiency, and performance, molecular task arithmetic\nbears the potential to become the $\\textit{de-facto}$ transfer learning\nstrategy for de novo molecule design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17876v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "另辟蹊径：通过任务算术利用负面数据设计“阳性”分子", "tldr": "提出一种分子任务算术方法，利用大量负面数据训练模型学习属性方向，然后反向生成具有所需特性的“阳性”分子，无需阳性数据，且效果良好。", "motivation": "具有理想特性的“阳性”分子稀缺是生成式分子设计的固有瓶颈。", "method": "提出分子任务算术：通过在多样且丰富的负面示例上训练模型来学习“属性方向”，然后在相反的属性方向上移动模型以生成“阳性”分子，无需访问任何阳性标记数据。", "result": "在20项零样本设计实验中，分子任务算术生成了比在阳性分子上训练的模型更多样化和成功的设计。在双目标和少样本设计任务中，它能持续增加设计的 다양性，同时保持理想的设计特性。", "conclusion": "分子任务算术凭借其简单性、数据效率和性能，有望成为从头分子设计的实际（de-facto）迁移学习策略。", "translation": "具有理想特性（即“阳性”分子）的分子稀缺是生成式分子设计的固有瓶颈。为了规避这一障碍，我们在此提出分子任务算术：通过在多样且丰富的负面示例上训练模型来学习“属性方向”——无需访问任何阳性标记数据——并在相反的属性方向上移动模型以生成阳性分子。在20项零样本设计实验中分析表明，分子任务算术生成了比在阳性分子上训练的模型更多样化和成功的设计。此外，我们将分子任务算术应用于双目标和少样本设计任务。我们发现分子任务算术可以持续增加设计的多样性，同时保持理想的设计特性。凭借其简单性、数据效率和性能，分子任务算术有望成为从头分子设计的实际（de-facto）迁移学习策略。", "summary": "本文提出了一种名为“分子任务算术”的新方法，旨在解决生成式分子设计中“阳性”分子稀缺的问题。该方法通过在大量负面数据上训练模型来学习“属性方向”，然后反向操作以生成具有所需特性的阳性分子，而无需任何阳性标记数据。实验表明，与传统方法相比，该方法在零样本、双目标和少样本设计任务中能生成更多样化且成功的分子设计，并保持理想特性。其简单性、数据效率和高性能使其有望成为从头分子设计的标准迁移学习策略。", "keywords": "分子设计, 任务算术, 负面数据, 零样本设计, 迁移学习", "comments": "这篇论文的创新点在于提出了“分子任务算术”这一新颖的范式，即利用“负面数据”来间接学习“正面属性方向”，从而生成所需的阳性分子，这极大地缓解了阳性数据稀缺的瓶颈。其无需阳性标记数据的特性，以及在多样性和成功率上的表现，使其在药物发现和材料科学等领域具有重要应用潜力。"}}
{"id": "2404.16243", "title": "muRelBench: MicroBenchmarks for Zonotope Domains", "authors": ["Kenny Ballou", "Elena Sherman"], "categories": ["cs.LO", "cs.SE"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      SRP accepted at SEET 2025", "url": "http://arxiv.org/abs/2404.16243v2", "summary": "We present \\texttt{muRelBench}, a framework for synthetic benchmarks for\nweakly-relational abstract domains and their operations. This extensible\nmicrobenchmarking framework enables researchers to experimentally evaluate\nproposed algorithms for numerical abstract domains, such as closure,least-upper\nbound, and forget, enabling them to quickly prototype and validate performance\nimprovements before considering more intensive experimentation. Additionally,\nthe framework provides mechanisms for checking correctness properties for each\nof the benchmarks to ensure correctness within the synthetic benchmarks.", "comment": "SRP accepted at SEET 2025", "pdf_url": "http://arxiv.org/pdf/2404.16243v2", "cate": "cs.LO", "date": "2024-04-24", "updated": "2025-07-23", "AI": {"title_translation": "muRelBench：仿射域的微基准测试", "tldr": "提出了一个名为muRelBench的框架，用于对弱关系抽象域及其操作进行合成基准测试，帮助研究人员快速评估和验证算法性能及正确性。", "motivation": "该论文的动机是提供一个可扩展的微基准测试框架，使研究人员能够快速原型化并验证数值抽象域算法的性能改进和正确性，从而在进行更密集的实验之前获得初步评估。", "method": "论文提出了一个名为muRelBench的框架，这是一个用于弱关系抽象域及其操作（如闭包、最小上界和遗忘）的合成微基准测试框架。该框架还包含了检查基准测试正确性属性的机制。", "result": "该论文的成果是提供了一个名为muRelBench的框架，该框架旨在使研究人员能够实验性地评估数值抽象域的提议算法，并快速原型化和验证性能改进及正确性。", "conclusion": "muRelBench框架为弱关系抽象域及其操作的算法评估提供了一个高效且可验证的工具，有助于加速数值抽象域算法的开发和验证过程。", "translation": "我们提出了 \texttt{muRelBench}，一个用于弱关系抽象域及其操作的合成基准测试框架。这个可扩展的微基准测试框架使研究人员能够实验性地评估数值抽象域的提议算法，例如闭包、最小上界和遗忘操作，从而使他们能够在考虑更密集的实验之前快速原型化并验证性能改进。此外，该框架还提供了检查每个基准测试的正确性属性的机制，以确保合成基准测试的正确性。", "summary": "muRelBench是一个为弱关系抽象域及其操作设计的合成微基准测试框架。它旨在帮助研究人员快速评估数值抽象域算法（如闭包、最小上界和遗忘操作）的性能改进，并验证其正确性，从而在进行大规模实验前提供快速原型验证能力。", "keywords": "抽象域, 基准测试, 弱关系, muRelBench", "comments": "这篇论文提出muRelBench框架，填补了弱关系抽象域算法性能评估工具的空白。其创新之处在于提供了合成基准测试的能力，并集成了正确性检查机制，极大地简化了新算法的原型验证和性能评估过程，对于抽象解释和程序分析领域的研究具有重要意义。"}}
{"id": "2507.18330", "title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Philippe Very", "Franck Ballerini", "Stephania-Denisa Bocu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18330v1", "summary": "Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18330v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GVCCS：一个用于可见光全天空相机序列凝结尾迹识别与追踪的数据集", "tldr": "本文提出了GVCCS数据集，这是一个新的开放数据集，用于从地面可见光全天空相机序列中识别和追踪飞机凝结尾迹，并提供了一个统一的深度学习框架作为基准，以改进凝结尾迹监测和气候模型校准。", "motivation": "飞机凝结尾迹对气候有显著影响，其变暖效应可能与航空碳排放相当。尽管有基于物理的模型，但其准确性受限于大气输入数据质量和复杂过程（如冰粒形成和湿度驱动持久性）的假设。现有观测数据集缺乏时间追踪功能，且无法将凝结尾迹与其源航班关联，这限制了模型验证和校准。", "method": "本文提出了GVCCS（地面可见光相机凝结尾迹序列）数据集，这是一个新的开放数据集，包含通过地面全天空相机在可见光范围内记录的凝结尾迹。数据集中每个凝结尾迹都经过单独标记和时间追踪，并包含源航班标识符。此外，本文还提出了一个统一的深度学习框架，使用全景分割模型进行凝结尾迹分析，该模型在一个架构中执行语义分割（凝结尾迹像素识别）、实例分割（单个凝结尾迹分离）和时间追踪。", "result": "GVCCS数据集包含122个视频序列（24,228帧），并为在相机上方形成的凝结尾迹提供了航班标识符。本文提出的深度学习框架能够进行凝结尾迹的语义分割、实例分割和时间追踪。", "conclusion": "通过提供高质量、时间解析的标注和模型评估基准，本工作支持改进凝结尾迹监测，并有助于更好地校准物理模型。这为更准确地理解和评估气候影响奠定了基础。", "translation": "航空对气候的影响不仅包括二氧化碳排放，还包括显著的非二氧化碳效应，特别是来自凝结尾迹的影响。这些冰云可以改变地球的辐射平衡，其变暖效应可能与航空二氧化碳的变暖效应相媲美。基于物理的模型提供了凝结尾迹形成和气候影响的有用估计，但其准确性在很大程度上取决于大气输入数据的质量以及用于表示冰粒形成和湿度驱动持久性等复杂过程的假设。来自遥感器（如卫星和地面相机）的观测数据可用于验证和校准这些模型。然而，现有数据集并未探索凝结尾迹动力学和形成的所有方面：它们通常缺乏时间追踪，并且不将凝结尾迹归因于其源航班。为了解决这些限制，我们提出了地面可见光相机凝结尾迹序列（GVCCS），这是一个新的开放数据集，包含使用地面全天空相机在可见光范围内记录的凝结尾迹。每个凝结尾迹都经过单独标记和时间追踪，从而可以对其生命周期进行详细分析。该数据集包含122个视频序列（24,228帧），并包含在相机上方形成的凝结尾迹的航班标识符。作为参考，我们还提出了一个统一的深度学习框架，用于使用全景分割模型进行凝结尾迹分析，该模型在一个架构中执行语义分割（凝结尾迹像素识别）、实例分割（单个凝结尾迹分离）和时间追踪。通过提供高质量、时间解析的标注和模型评估基准，我们的工作支持改进凝结尾迹监测，并将促进物理模型的更好校准。这为更准确地理解和评估气候影响奠定了基础。", "summary": "本文介绍了GVCCS数据集，这是一个新的开放数据集，用于通过地面可见光全天空相机序列识别和追踪凝结尾迹。该数据集旨在解决现有凝结尾迹数据集缺乏时间追踪和源航班归属的限制，提供122个视频序列，其中每个凝结尾迹都经过单独标记和时间追踪，并包含航班标识符。此外，本文还提出了一个统一的深度学习框架作为基准，该框架利用全景分割模型实现凝结尾迹的语义分割、实例分割和时间追踪。这项工作通过提供高质量的标注和评估基准，有助于改进凝结尾迹监测和物理模型的校准，从而更准确地理解和评估航空对气候的影响。", "keywords": "凝结尾迹, 数据集, 追踪, 深度学习, 气候影响", "comments": "这项工作非常重要，因为它解决了凝结尾迹研究中一个关键的数据空白，即缺乏具有时间追踪和源航班归属信息的高质量观测数据集。GVCCS数据集的创建及其提供的详细标注，将极大地促进凝结尾迹监测技术的发展和物理气候模型的改进。同时提出的统一深度学习框架也为未来的研究提供了有价值的基准。其创新性在于结合了多任务学习（语义、实例分割和追踪），为凝结尾迹分析提供了一个全面的解决方案。"}}
{"id": "2507.18264", "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, Accepted paper at Recent Advances in Natural Language Processing (RANLP) 2025", "url": "http://arxiv.org/abs/2507.18264v1", "summary": "Solving the problem of Optical Character Recognition (OCR) on printed text\nfor Latin and its derivative scripts can now be considered settled due to the\nvolumes of research done on English and other High-Resourced Languages (HRL).\nHowever, for Low-Resourced Languages (LRL) that use unique scripts, it remains\nan open problem. This study presents a comparative analysis of the zero-shot\nperformance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The\nselected engines include both commercial and open-source systems, aiming to\nevaluate the strengths of each category. The Cloud Vision API, Surya, Document\nAI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR\nand EasyOCR were examined for only one language due to their limitations. The\nperformance of these systems was rigorously analysed using five measurement\ntechniques to assess accuracy at both the character and word levels. According\nto the findings, Surya delivered the best performance for Sinhala across all\nmetrics, with a WER of 2.61%. Conversely, Document AI excelled across all\nmetrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the\nabove analysis, we also introduce a novel synthetic Tamil OCR benchmarking\ndataset.", "comment": "10 pages, 4 figures, Accepted paper at Recent Advances in Natural\n  Language Processing (RANLP) 2025", "pdf_url": "http://arxiv.org/pdf/2507.18264v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "低资源语言的零样本OCR准确性：僧伽罗语和泰米尔语的比较分析", "tldr": "本研究比较分析了六种OCR引擎在僧伽罗语和泰米尔语（两种低资源语言）上的零样本性能，发现Surya在僧伽罗语表现最佳，Document AI在泰米尔语表现最佳，并引入了一个新的泰米尔语OCR基准数据集。", "motivation": "高资源语言的OCR问题已基本解决，但对于使用独特文字的低资源语言，OCR仍是一个开放问题。本研究旨在评估不同OCR引擎在低资源语言（僧伽罗语和泰米尔语）上的零样本性能。", "method": "本研究对六种不同的OCR引擎（Cloud Vision API, Surya, Document AI, Tesseract, Subasa OCR, EasyOCR）在僧伽罗语和泰米尔语两种低资源语言上的零样本性能进行了比较分析。使用五种测量技术在字符和单词级别评估了准确性。此外，还引入了一个新的合成泰米尔语OCR基准数据集。", "result": "Surya在所有指标上为僧伽罗语提供了最佳性能，词错误率（WER）为2.61%。Document AI在所有指标上都优于泰米尔语，字符错误率（CER）非常低，为0.78%。", "conclusion": "对于低资源语言的零样本OCR，不同的OCR引擎在不同语言上表现出不同的优越性，且仍有改进空间。", "translation": "解决拉丁语及其衍生文字的印刷文本光学字符识别（OCR）问题，由于在英语和其他高资源语言（HRL）上进行了大量研究，现在可以认为是已解决的问题。然而，对于使用独特文字的低资源语言（LRL），这仍然是一个开放问题。本研究对六种不同的OCR引擎在两种低资源语言：僧伽罗语和泰米尔语上的零样本性能进行了比较分析。所选引擎包括商业和开源系统，旨在评估每种类别的优势。Cloud Vision API、Surya、Document AI和Tesseract对僧伽罗语和泰米尔语都进行了评估，而Subasa OCR和EasyOCR由于其限制仅对一种语言进行了检查。这些系统的性能使用五种测量技术进行了严格分析，以评估字符和单词级别的准确性。根据研究结果，Surya在所有指标上为僧伽罗语提供了最佳性能，词错误率（WER）为2.61%。相反，Document AI在所有指标上都优于泰米尔语，字符错误率（CER）非常低，为0.78%。除了上述分析，我们还引入了一个新颖的合成泰米尔语OCR基准数据集。", "summary": "本研究旨在解决低资源语言（LRL）OCR的开放问题，对六种商业和开源OCR引擎在僧伽罗语和泰米尔语上的零样本性能进行了比较分析。研究发现，Surya在僧伽罗语表现最佳，WER为2.61%；Document AI在泰米尔语表现最佳，CER为0.78%。此外，研究还提出了一个新的合成泰米尔语OCR基准数据集。", "keywords": "OCR, 低资源语言, 零样本, 僧伽罗语, 泰米尔语", "comments": "这项研究的重要性在于填补了低资源语言OCR领域的研究空白，特别是对僧伽罗语和泰米尔语。通过比较多种商业和开源OCR引擎的零样本性能，为该领域的进一步研究和应用提供了宝贵的基准数据。引入新的合成泰米尔语基准数据集是其创新点之一，有助于未来研究的标准化评估。然而，研究仅限于两种语言，未来可以扩展到更多低资源语言。"}}
{"id": "2502.15610", "title": "A general language model for peptide identification", "authors": ["Jixiu Zhai", "Tianchi Lu", "Haitian Zhong", "Ziyang Xu", "Yuhuan Liu", "Shengrui Xu", "Jingwan Wang", "Dan Huang"], "categories": ["cs.LG", "cs.AI", "92C40, 68T07", "I.2.6; J.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 9 figures, 4 tables, submitted to arXiv", "url": "http://arxiv.org/abs/2502.15610v4", "summary": "Accurate identification of bioactive peptides (BPs) and protein\npost-translational modifications (PTMs) is essential for understanding protein\nfunction and advancing therapeutic discovery. However, most computational\nmethods remain limited in their generalizability across diverse peptide\nfunctions. Here, we present PDeepPP, a unified deep learning framework that\nintegrates pretrained protein language models with a hybrid\ntransformer-convolutional architecture, enabling robust identification across\ndiverse peptide classes and PTM sites. We curated comprehensive benchmark\ndatasets and implemented strategies to address data imbalance, allowing PDeepPP\nto systematically extract both global and local sequence features. Through\nextensive analyses-including dimensionality reduction and comparison\nstudies-PDeepPP demonstrates strong, interpretable peptide representations and\nachieves state-of-the-art performance in 25 of the 33 biological identification\ntasks. Notably, PDeepPP attains high accuracy in antimicrobial (0.9726) and\nphosphorylation site (0.9984) identification, with 99.5% specificity in\nglycosylation site prediction and substantial reduction in false negatives in\nantimalarial tasks. By enabling large-scale, accurate peptide analysis, PDeepPP\nsupports biomedical research and the discovery of novel therapeutic targets for\ndisease treatment. All code, datasets, and pretrained models are publicly\navailable via GitHub:https://github.com/fondress/PDeepPP and Hugging\nFace:https://huggingface.co/fondress/PDeppPP.", "comment": "24 pages, 9 figures, 4 tables, submitted to arXiv", "pdf_url": "http://arxiv.org/pdf/2502.15610v4", "cate": "cs.LG", "date": "2025-02-21", "updated": "2025-07-24", "AI": {"title_translation": "一种用于肽鉴定的通用语言模型", "tldr": "PDeepPP是一个统一的深度学习框架，结合预训练蛋白质语言模型和混合架构，实现了跨多样肽类和PTM位点的准确识别，并在多项生物识别任务中达到SOTA性能。", "motivation": "准确识别生物活性肽（BPs）和蛋白质翻译后修饰（PTMs）对于理解蛋白质功能和推进治疗药物发现至关重要。然而，大多数计算方法在跨不同肽功能方面的泛化能力有限。", "method": "本文提出了PDeepPP，一个统一的深度学习框架，它集成了预训练的蛋白质语言模型与混合Transformer-卷积架构。该方法通过策划全面的基准数据集并实施策略解决数据不平衡问题，从而系统地提取全局和局部序列特征。", "result": "PDeepPP展示了强大且可解释的肽表示，并在33项生物识别任务中的25项中取得了最先进的性能。值得注意的是，PDeepPP在抗菌肽（0.9726）和磷酸化位点（0.9984）识别中达到了高精度，在糖基化位点预测中特异性达到99.5%，并显著减少了抗疟任务中的假阴性。", "conclusion": "PDeepPP通过实现大规模、准确的肽分析，支持生物医学研究和新型疾病治疗靶点的发现。", "translation": "准确识别生物活性肽（BPs）和蛋白质翻译后修饰（PTMs）对于理解蛋白质功能和推进治疗药物发现至关重要。然而，大多数计算方法在跨不同肽功能方面的泛化能力仍然有限。本文提出了PDeepPP，一个统一的深度学习框架，它将预训练的蛋白质语言模型与混合Transformer-卷积架构相结合，从而能够对各种肽类和PTM位点进行鲁棒识别。我们整理了全面的基准数据集，并实施了解决数据不平衡的策略，使PDeepPP能够系统地提取全局和局部序列特征。通过广泛的分析——包括降维和比较研究——PDeepPP展示了强大且可解释的肽表示，并在33项生物识别任务中的25项中取得了最先进的性能。值得注意的是，PDeepPP在抗菌肽（0.9726）和磷酸化位点（0.9984）识别中达到了高精度，在糖基化位点预测中特异性达到99.5%，并显著减少了抗疟任务中的假阴性。通过实现大规模、准确的肽分析，PDeepPP支持生物医学研究和新型疾病治疗靶点的发现。所有代码、数据集和预训练模型均通过GitHub：https://github.com/fondress/PDeepPP 和 Hugging Face：https://huggingface.co/fondress/PDeppPP 公开可用。", "summary": "PDeepPP是一个创新的深度学习框架，旨在解决现有计算方法在多样肽类和翻译后修饰（PTMs）识别方面泛化能力不足的问题。它结合了预训练的蛋白质语言模型与混合Transformer-卷积架构，并通过处理数据不平衡问题，能够有效提取肽的全局和局部特征。该模型在多项生物识别任务中表现出色，达到了最先进的性能，尤其在抗菌肽、磷酸化和糖基化位点识别方面表现出高精度和特异性。PDeepPP的成功应用有望推动生物医学研究和新型治疗靶点的发现。", "keywords": "肽识别, 深度学习, 蛋白质语言模型, 翻译后修饰, 生物活性肽", "comments": "PDeepPP的创新之处在于其统一的深度学习框架，结合了预训练蛋白质语言模型和混合Transformer-卷积架构，显著提升了跨多样肽类和PTM位点的识别泛化能力。其重要性体现在实现了多项生物识别任务的SOTA性能，特别是高精度识别抗菌肽和磷酸化位点。此外，作者公开了代码、数据集和预训练模型，极大地增强了研究的透明度、可复现性和实用性，对生物医学研究和药物发现具有重要意义。"}}
{"id": "2507.18074", "title": "AlphaGo Moment for Model Architecture Discovery", "authors": ["Yixiu Liu", "Yang Nan", "Weixian Xu", "Xiangkun Hu", "Lyumanshan Ye", "Zhen Qin", "Pengfei Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18074v1", "summary": "While AI systems demonstrate exponentially improving capabilities, the pace\nof AI research itself remains linearly bounded by human cognitive capacity,\ncreating an increasingly severe development bottleneck. We present ASI-Arch,\nthe first demonstration of Artificial Superintelligence for AI research\n(ASI4AI) in the critical domain of neural architecture discovery--a fully\nautonomous system that shatters this fundamental constraint by enabling AI to\nconduct its own architectural innovation. Moving beyond traditional Neural\nArchitecture Search (NAS), which is fundamentally limited to exploring\nhuman-defined spaces, we introduce a paradigm shift from automated optimization\nto automated innovation. ASI-Arch can conduct end-to-end scientific research in\nthe domain of architecture discovery, autonomously hypothesizing novel\narchitectural concepts, implementing them as executable code, training and\nempirically validating their performance through rigorous experimentation and\npast experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000\nGPU hours, culminating in the discovery of 106 innovative, state-of-the-art\n(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed\nunexpected strategic insights invisible to human players, our AI-discovered\narchitectures demonstrate emergent design principles that systematically\nsurpass human-designed baselines and illuminate previously unknown pathways for\narchitectural innovation. Crucially, we establish the first empirical scaling\nlaw for scientific discovery itself--demonstrating that architectural\nbreakthroughs can be scaled computationally, transforming research progress\nfrom a human-limited to a computation-scalable process. We provide\ncomprehensive analysis of the emergent design patterns and autonomous research\ncapabilities that enabled these breakthroughs, establishing a blueprint for\nself-accelerating AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18074v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "模型架构发现的AlphaGo时刻", "tldr": "ASI-Arch是一个全自动AI系统，用于神经网络架构发现，它通过自主创新超越了人类定义的搜索空间，并发现了106个SOTA线性注意力架构，证明了科学发现可以计算化扩展，打破了AI研究的人类认知瓶颈。", "motivation": "当前的AI研究速度受限于人类认知能力，形成了一个日益严重的开发瓶颈。为了打破这一基本限制，使AI能够自主进行架构创新，本文提出了ASI-Arch。", "method": "本文提出了ASI-Arch，这是首个用于AI研究的人工超智能（ASI4AI）系统，专注于神经网络架构发现。该系统完全自主，能够超越传统神经架构搜索（NAS）的限制，从自动化优化转向自动化创新。ASI-Arch能进行端到端科学研究，自主提出新架构概念，将其实现为可执行代码，并通过严格的实验和经验验证其性能。", "result": "ASI-Arch进行了1,773次自主实验，耗时超过20,000 GPU小时，最终发现了106个创新且达到最新水平（SOTA）的线性注意力架构。这些AI发现的架构展示了超越人类设计基线的涌现设计原则，并揭示了先前未知的架构创新途径。此外，研究建立了第一个关于科学发现本身的经验缩放定律，证明了架构突破可以通过计算进行扩展。", "conclusion": "研究表明，架构突破可以通过计算进行扩展，将研究进展从人类受限的过程转变为计算可扩展的过程。这为自加速AI系统奠定了蓝图。", "translation": "尽管AI系统展示出指数级提升的能力，但AI研究本身的速度仍受限于人类的认知能力，这造成了一个日益严重的开发瓶颈。我们提出了ASI-Arch，这是在神经网络架构发现这一关键领域，首次展示用于AI研究的人工超智能（ASI4AI）——一个完全自主的系统，通过使AI能够自主进行架构创新来打破这一根本限制。我们超越了传统上受限于探索人类定义空间的神经架构搜索（NAS），引入了从自动化优化到自动化创新的范式转变。ASI-Arch可以在架构发现领域进行端到端的科学研究，自主提出新颖的架构概念，将其实现为可执行代码，并通过严格的实验和以往经验来训练和实证验证其性能。ASI-Arch进行了1,773次自主实验，耗时超过20,000 GPU小时，最终发现了106个创新且达到最新水平（SOTA）的线性注意力架构。就像AlphaGo的第37步揭示了人类玩家无法预见的意外战略洞察一样，我们AI发现的架构展示了系统性超越人类设计基线的涌现设计原则，并阐明了先前未知的架构创新途径。至关重要的是，我们建立了第一个关于科学发现本身的经验缩放定律——证明了架构突破可以通过计算进行扩展，将研究进展从人类受限的过程转变为计算可扩展的过程。我们提供了对促成这些突破的涌现设计模式和自主研究能力的全面分析，为自加速AI系统建立了蓝图。", "summary": "本文提出了ASI-Arch，一个用于神经网络架构发现的全自主AI系统，旨在突破AI研究中人类认知能力的瓶颈。该系统超越了传统的神经架构搜索，实现了从自动化优化到自动化创新的范式转变，能够自主进行科学研究，包括概念提出、代码实现、训练和验证。ASI-Arch通过大量实验发现了106个超越人类设计水平的SOTA线性注意力架构，并首次建立了科学发现的经验缩放定律，证明了架构突破可计算扩展，为自加速AI系统提供了蓝图。", "keywords": "神经网络架构发现, ASI-Arch, 人工超智能, 自动化创新, 缩放定律", "comments": "这项工作具有里程碑意义，因为它首次展示了AI系统（ASI-Arch）能够自主进行科学发现和创新，特别是在神经网络架构设计领域。它打破了AI研究中长期存在的人类认知限制，并通过提出“科学发现的缩放定律”，将研究进展从人类驱动转变为计算驱动，这预示着AI研究将进入一个自我加速的新时代。其创新点在于从“自动化优化”转向“自动化创新”，并成功发现了超越人类设计的架构，这对于未来AI的发展具有深远影响。"}}
{"id": "2507.17985", "title": "Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale", "authors": ["Alex Liu", "Lief Esbenshade", "Shawon Sarkar", "Victor Tian", "Zachary Zhang", "Kevin He", "Min Sun"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17985v1", "summary": "The integration of large language models (LLMs) into educational tools has\nthe potential to substantially impact how teachers plan instruction, support\ndiverse learners, and engage in professional reflection. Yet little is known\nabout how educators actually use these tools in practice and how their\ninteractions with AI can be meaningfully studied at scale. This paper presents\na human-AI collaborative methodology for large-scale qualitative analysis of\nover 140,000 educator-AI messages drawn from a generative AI platform used by\nK-12 teachers. Through a four-phase coding pipeline, we combined inductive\ntheme discovery, codebook development, structured annotation, and model\nbenchmarking to examine patterns of educator engagement and evaluate the\nperformance of LLMs in qualitative coding tasks. We developed a hierarchical\ncodebook aligned with established teacher evaluation frameworks, capturing\neducators' instructional goals, contextual needs, and pedagogical strategies.\nOur findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably\nsupport theme identification, extend human recognition in complex scenarios,\nand outperform open-weight models in both accuracy and structural reliability.\nThe analysis also reveals substantive patterns in how educators inquire AI to\nenhance instructional practices (79.7 percent of total conversations), create\nor adapt content (76.1 percent), support assessment and feedback loop (46.9\npercent), attend to student needs for tailored instruction (43.3 percent), and\nassist other professional responsibilities (34.2 percent), highlighting\nemerging AI-related competencies that have direct implications for teacher\npreparation and professional development. This study offers a scalable,\ntransparent model for AI-augmented qualitative research and provides\nfoundational insights into the evolving role of generative AI in educational\npractice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17985v1", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "解码教学对话：大规模人机协作分析教师对AI工具的使用", "tldr": "本研究提出了一种人机协作方法，对K-12教师使用的14万多条AI消息进行大规模定性分析，发现LLM能可靠支持主题识别，并揭示了教师使用AI工具进行教学实践、内容创建、评估反馈和学生支持等方面的模式。", "motivation": "尽管大型语言模型（LLMs）在教育工具中的整合潜力巨大，但目前对于教育工作者如何在实践中使用这些工具以及如何大规模有效地研究他们与AI的互动知之甚少。", "method": "本研究提出了一种人机协作方法，对超过14万条K-12教师与生成式AI平台间的消息进行大规模定性分析。通过一个四阶段编码流程，结合了归纳主题发现、编码本开发、结构化标注和模型基准测试，以检查教育工作者的参与模式并评估LLM在定性编码任务中的表现。开发了一个与既定教师评估框架对齐的层次化编码本。", "result": "研究发现，LLMs，特别是Claude 3.5 Haiku，能够可靠地支持主题识别，在复杂场景中扩展人类识别能力，并在准确性和结构可靠性方面优于开源模型。分析还揭示了教育工作者如何使用AI来增强教学实践（79.7%）、创建或改编内容（76.1%）、支持评估和反馈（46.9%）、满足学生个性化需求（43.3%）以及协助其他专业职责（34.2%）的实质性模式。", "conclusion": "本研究提供了一个可扩展、透明的AI增强定性研究模型，并为生成式AI在教育实践中不断演变的角色提供了基础性见解。", "translation": "大型语言模型（LLMs）融入教育工具，有潜力显著影响教师如何规划教学、支持多样化学习者以及进行专业反思。然而，关于教育工作者在实践中如何实际使用这些工具，以及如何大规模有效地研究他们与AI的互动，目前知之甚少。本文提出了一种人机协作方法，用于对K-12教师使用的生成式AI平台中超过14万条教育工作者与AI的消息进行大规模定性分析。通过一个四阶段的编码流程，我们结合了归纳主题发现、编码本开发、结构化标注和模型基准测试，以检查教育工作者的参与模式，并评估LLM在定性编码任务中的表现。我们开发了一个与既定教师评估框架对齐的层次化编码本，捕获了教育工作者的教学目标、情境需求和教学策略。我们的发现表明，LLMs，特别是Claude 3.5 Haiku，能够可靠地支持主题识别，在复杂场景中扩展人类识别能力，并在准确性和结构可靠性方面优于开源模型。分析还揭示了教育工作者如何向AI提问以增强教学实践（占总对话的79.7%）、创建或改编内容（76.1%）、支持评估和反馈循环（46.9%）、关注学生个性化指导需求（43.3%）以及协助其他专业职责（34.2%）的实质性模式，凸显了新兴的AI相关能力，这些能力对教师培养和专业发展具有直接影响。本研究提供了一个可扩展、透明的AI增强定性研究模型，并为生成式AI在教育实践中不断演变的角色提供了基础性见解。", "summary": "本研究提出了一种人机协作方法，对K-12教师与生成式AI平台的互动进行大规模定性分析，涉及超过14万条消息。通过四阶段编码流程和层次化编码本，研究评估了LLM在定性编码中的表现，并揭示了教师使用AI进行教学规划、内容创建、评估反馈和学生支持等方面的具体模式。结果表明，LLM（特别是Claude 3.5 Haiku）能有效辅助主题识别，且性能优于开源模型。本研究为AI增强的定性研究提供了可扩展模型，并深入探讨了生成式AI在教育中的作用。", "keywords": "人机协作, 定性分析, 教师AI使用, 大型语言模型, 教育技术", "comments": "该研究创新性地提出了大规模人机协作分析方法，有效结合了人类专业知识与LLM的处理能力，解决了大规模定性数据分析的挑战。其重要性在于揭示了教师在实际教学中使用AI工具的具体模式，为教师专业发展和AI教育应用提供了宝贵见解。研究还验证了特定LLM在定性分析任务中的可靠性，为未来AI辅助研究提供了实证支持。"}}
{"id": "2507.18100", "title": "Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning", "authors": ["Ruizhe Chen", "Zhiting Fan", "Tianze Luo", "Heqing Zou", "Zhaopeng Feng", "Guiyang Xie", "Hansheng Zhang", "Zhuochen Wang", "Zuozhu Liu", "Huaijian Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18100v1", "summary": "Video Temporal Grounding (VTG) aims to localize relevant temporal segments in\nvideos given natural language queries. Despite recent progress with large\nvision-language models (LVLMs) and instruction-tuning, existing approaches\noften suffer from limited temporal awareness and poor generalization. In this\nwork, we introduce a two-stage training framework that integrates supervised\nfine-tuning with reinforcement learning (RL) to improve both the accuracy and\nrobustness of VTG models. Our approach first leverages high-quality curated\ncold start data for SFT initialization, followed by difficulty-controlled RL to\nfurther enhance temporal localization and reasoning abilities. Comprehensive\nexperiments on multiple VTG benchmarks demonstrate that our method consistently\noutperforms existing models, particularly in challenging and open-domain\nscenarios. We conduct an in-depth analysis of training strategies and dataset\ncuration, highlighting the importance of both high-quality cold start data and\ndifficulty-controlled RL. To facilitate further research and industrial\nadoption, we release all intermediate datasets, models, and code to the\ncommunity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18100v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于强化学习的视频时间定位数据集与方法", "tldr": "本文提出了一种结合监督微调和强化学习的两阶段训练框架，以提高视频时间定位（VTG）模型的准确性和鲁棒性，并在多个基准测试中取得了SOTA性能。", "motivation": "尽管大型视觉-语言模型（LVLMs）和指令微调取得了进展，但现有的视频时间定位（VTG）方法在时间感知和泛化能力方面仍存在局限性。", "method": "本文引入了一个两阶段训练框架，该框架将监督微调（SFT）与强化学习（RL）相结合。首先利用高质量的冷启动数据进行SFT初始化，然后通过难度控制的RL进一步增强时间定位和推理能力。", "result": "在多个VTG基准测试中，本文方法始终优于现有模型，尤其是在具有挑战性和开放域的场景中表现突出。深入分析表明，高质量的冷启动数据和难度控制的强化学习至关重要。", "conclusion": "本文提出的结合高质量冷启动数据和难度控制强化学习的两阶段训练框架，能显著提升视频时间定位模型的准确性和鲁棒性，为该领域的研究和应用提供了有效方法。", "translation": "视频时间定位（VTG）旨在根据自然语言查询在视频中定位相关的时序片段。尽管大型视觉-语言模型（LVLMs）和指令微调取得了最新进展，但现有方法通常存在时间感知有限和泛化能力差的问题。在这项工作中，我们引入了一个两阶段训练框架，该框架将监督微调与强化学习（RL）相结合，以提高VTG模型的准确性和鲁棒性。我们的方法首先利用高质量的精选冷启动数据进行SFT初始化，然后通过难度控制的RL进一步增强时间定位和推理能力。在多个VTG基准测试中进行的全面实验表明，我们的方法始终优于现有模型，特别是在具有挑战性和开放域的场景中。我们对训练策略和数据集整理进行了深入分析，强调了高质量冷启动数据和难度控制RL的重要性。为了促进进一步的研究和工业应用，我们向社区发布了所有中间数据集、模型和代码。", "summary": "本文提出了一种用于视频时间定位（VTG）的两阶段训练框架，旨在解决现有方法在时间感知和泛化方面的不足。该框架首先通过高质量的冷启动数据进行监督微调（SFT）初始化，然后利用难度控制的强化学习（RL）进一步优化模型的时序定位和推理能力。实验结果表明，该方法在多个VTG基准测试中表现优异，尤其在复杂和开放域场景下效果显著。研究还强调了高质量冷启动数据和难度控制RL的重要性，并开源了相关资源以促进研究。", "keywords": "视频时间定位, 强化学习, 监督微调, 两阶段训练, 时间感知", "comments": "该论文的创新点在于提出了一个结合监督微调和强化学习的两阶段训练框架，有效解决了视频时间定位中时间感知和泛化能力不足的问题。通过引入高质量的冷启动数据和难度控制的强化学习，显著提升了模型的性能，特别是在开放域场景下的鲁棒性。开源数据集、模型和代码的举措，将极大促进该领域的研究进展和工业应用。"}}
{"id": "2507.18492", "title": "A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks", "authors": ["Mirhan Ürkmez", "Carsten Kallesøe", "Jan Dimon Bendtsen", "Eric C. Kerrigan", "John Leth"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18492v1", "summary": "Water utilities aim to reduce the high electrical costs of Water Distribution\nNetworks (WDNs), primarily driven by pumping. However, pump scheduling is\nchallenging due to model uncertainties and water demand forecast errors. This\npaper presents a Robust Model Predictive Control (RMPC) method for optimal and\nreliable pump scheduling, extending a previous efficient robust control method\ntailored to our model. A linear model with bounded additive disturbances is\nused to represent tank water level evolution, with uncertainty bounds derived\nfrom WDN simulation and demand data. At each time step, a pump scheduling\npolicy, affine in past disturbances, is optimized to satisfy system constraints\nover a prediction horizon. The resulting policies are then applied in a\nreceding horizon fashion. The optimization problem is formulated to require\n$\\mathcal{O}(N^6)$ computations per iteration with an interior-point method,\nwhich is reduced to $\\mathcal{O}(N^3)$ by reformulating it into a sparse form.\nWhen evaluated on a model representing the water distribution network of\nRanders, a medium-sized town in Denmark, the method surpasses nominal and\nconstraint-tightening model predictive control (MPC) approaches in terms of\nmeeting constraints and provides comparable economic outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18492v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "水分配网络中水泵调度的鲁棒预测控制方法", "tldr": "本文提出了一种鲁棒模型预测控制（RMPC）方法，用于水分配网络中水泵的优化调度，以应对模型不确定性和需求预测误差，并在实际案例中表现出优于传统方法的约束满足能力和相似的经济效益。", "motivation": "水务公司旨在降低水分配网络（WDNs）中由水泵操作产生的高昂电费。然而，由于模型不确定性和用水需求预测误差，水泵调度极具挑战性。", "method": "本文提出了一种鲁棒模型预测控制（RMPC）方法，用于优化和可靠的水泵调度。该方法扩展了之前针对其模型定制的高效鲁棒控制方法。它使用一个带有有界加性扰动的线性模型来表示水箱水位演变，不确定性边界通过WDN仿真和需求数据导出。在每个时间步，通过优化一个对过去扰动呈仿射的泵调度策略来满足预测范围内的系统约束。这些策略以滚动时域方式应用。通过将其重新表述为稀疏形式，优化问题的计算复杂度从O(N^6)降低到O(N^3)。", "result": "该方法在丹麦中型城镇兰讷斯的水分配网络模型上进行了评估，结果表明它在满足约束方面超越了标称和约束紧缩模型预测控制（MPC）方法，并提供了可比的经济效益。", "conclusion": "所提出的鲁棒模型预测控制（RMPC）方法能够有效应对水分配网络中水泵调度的不确定性，在满足系统约束方面表现出优越性，同时保持了良好的经济效益。", "translation": "水务公司旨在降低水分配网络（WDNs）中由水泵操作产生的高昂电费。然而，由于模型不确定性和用水需求预测误差，水泵调度极具挑战性。本文提出了一种鲁棒模型预测控制（RMPC）方法，用于优化和可靠的水泵调度，该方法扩展了之前针对我们模型定制的高效鲁棒控制方法。文章使用一个带有有界加性扰动的线性模型来表示水箱水位演变，不确定性边界通过WDN仿真和需求数据导出。在每个时间步，通过优化一个对过去扰动呈仿射的泵调度策略来满足预测范围内的系统约束。然后，所得策略以滚动时域方式应用。优化问题最初被表述为使用内点法每次迭代需要O(N^6)的计算量，通过将其重新表述为稀疏形式，计算量降低到O(N^3)。在模拟丹麦中型城镇兰讷斯的水分配网络模型时，该方法在满足约束方面超越了标称和约束紧缩模型预测控制（MPC）方法，并提供了可比的经济效益。", "summary": "本文提出了一种鲁棒模型预测控制（RMPC）方法，旨在解决水分配网络中水泵调度因模型不确定性和需求预测误差而面临的挑战。该方法采用带有有界扰动的线性模型来预测水箱水位，并优化一种对扰动呈仿射的泵调度策略以满足系统约束。通过稀疏形式重构，计算复杂度显著降低。在丹麦兰讷斯的水分配网络案例中，该方法在满足约束方面优于传统的标称和约束紧缩MPC方法，同时保持了相似的经济效益，证明了其在实际应用中的鲁棒性和有效性。", "keywords": "鲁棒预测控制, 水泵调度, 水分配网络, 模型不确定性, 计算效率", "comments": "本文提出了一种实用的鲁棒模型预测控制方法，有效解决了水分配网络中水泵调度面临的不确定性问题。其创新点在于将计算复杂度从O(N^6)降低到O(N^3)，这对于实际应用至关重要。该方法在满足约束方面的优越性以及与现有方法相当的经济效益，使其成为水务领域一个有潜力的解决方案。"}}
{"id": "2408.05330", "title": "Neural Machine Unranking", "authors": ["Jingrui Hou", "Axel Finke", "Georgina Cosma"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.05330v3", "summary": "We address the problem of machine unlearning in neural information retrieval\n(IR), introducing a novel task termed Neural Machine UnRanking (NuMuR). This\nproblem is motivated by growing demands for data privacy compliance and\nselective information removal in neural IR systems. Existing task- or model-\nagnostic unlearning approaches, primarily designed for classification tasks,\nare suboptimal for NuMuR due to two core challenges: (1) neural rankers output\nunnormalised relevance scores rather than probability distributions, limiting\nthe effectiveness of traditional teacher-student distillation frameworks; and\n(2) entangled data scenarios, where queries and documents appear simultaneously\nacross both forget and retain sets, may degrade retention performance in\nexisting methods. To address these issues, we propose Contrastive and\nConsistent Loss (CoCoL), a dual-objective framework. CoCoL comprises (1) a\ncontrastive loss that reduces relevance scores on forget sets while maintaining\nperformance on entangled samples, and (2) a consistent loss that preserves\naccuracy on retain set. Extensive experiments on MS MARCO and TREC CAR\ndatasets, across four neural IR models, demonstrate that CoCoL achieves\nsubstantial forgetting with minimal retain and generalisation performance loss.\nOur method facilitates more effective and controllable data removal than\nexisting techniques.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.05330v3", "cate": "cs.IR", "date": "2024-08-09", "updated": "2025-07-24", "AI": {"title_translation": "神经机器去排序", "tldr": "本文提出了神经信息检索（IR）中的机器遗忘问题，引入了神经机器去排序（NuMuR）这一新任务。针对现有方法在非标准化得分和纠缠数据场景下的不足，作者提出了对比一致性损失（CoCoL）框架，旨在实现有效遗忘同时保持保留性能。实验证明CoCoL优于现有技术。", "motivation": "该问题源于神经信息检索（IR）系统中对数据隐私合规性和选择性信息删除日益增长的需求。", "method": "为了解决现有方法在神经排序器输出非标准化相关性得分以及纠缠数据场景下的局限性，本文提出了对比一致性损失（CoCoL），一个双目标框架。CoCoL包含：（1）对比损失，用于降低遗忘集上的相关性得分，同时保持纠缠样本上的性能；（2）一致性损失，用于保持保留集上的准确性。", "result": "在MS MARCO和TREC CAR数据集上，通过四种神经IR模型进行的广泛实验表明，CoCoL在实现显著遗忘的同时，对保留性能和泛化性能的损失最小。", "conclusion": "我们的方法比现有技术能够实现更有效和可控的数据删除。", "translation": "我们解决了神经信息检索（IR）中的机器遗忘问题，引入了一项名为神经机器去排序（NuMuR）的新任务。该问题的动机是神经IR系统中对数据隐私合规性和选择性信息删除日益增长的需求。现有的任务或模型无关的遗忘方法，主要为分类任务设计，由于两个核心挑战而不适用于NuMuR：（1）神经排序器输出的是非标准化的相关性得分而非概率分布，这限制了传统师生蒸馏框架的有效性；（2）纠缠数据场景，即查询和文档同时出现在遗忘集和保留集中，可能导致现有方法中的保留性能下降。为了解决这些问题，我们提出了对比一致性损失（CoCoL），一个双目标框架。CoCoL包括：（1）对比损失，用于降低遗忘集上的相关性得分，同时保持纠缠样本上的性能；（2）一致性损失，用于保持保留集上的准确性。在MS MARCO和TREC CAR数据集上，通过四种神经IR模型进行的广泛实验表明，CoCoL在实现显著遗忘的同时，对保留性能和泛化性能的损失最小。我们的方法比现有技术能够实现更有效和可控的数据删除。", "summary": "本文针对神经信息检索（IR）中的机器遗忘问题，提出了一项名为神经机器去排序（NuMuR）的新任务。鉴于现有任务或模型无关的遗忘方法在处理神经排序器输出的非标准化相关性得分和纠缠数据场景时的局限性，本文提出了一种双目标框架——对比一致性损失（CoCoL）。CoCoL通过对比损失降低遗忘集上的相关性得分并保持纠缠样本性能，同时通过一致性损失维护保留集上的准确性。在MS MARCO和TREC CAR数据集上对四种神经IR模型进行的实验表明，CoCoL在实现有效遗忘的同时，对保留和泛化性能的影响最小，并且比现有技术提供了更有效和可控的数据删除能力。", "keywords": "机器遗忘, 神经信息检索, 神经机器去排序, 对比学习, 数据隐私", "comments": "这篇论文创新性地将机器遗忘的概念引入到神经信息检索领域，并提出了一个新颖的任务——神经机器去排序（NuMuR）。其核心贡献在于设计了CoCoL框架，有效地解决了神经排序器非标准化输出和数据纠缠等特有问题，为神经IR系统中的数据隐私合规性和选择性信息删除提供了实用的解决方案，具有重要的理论和应用价值。"}}
{"id": "2410.22128", "title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting", "authors": ["Sunghwan Hong", "Jaewoo Jung", "Heeseong Shin", "Jisang Han", "Jiaolong Yang", "Chong Luo", "Seungryong Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICML'25", "url": "http://arxiv.org/abs/2410.22128v2", "summary": "We consider the problem of novel view synthesis from unposed images in a\nsingle feed-forward. Our framework capitalizes on fast speed, scalability, and\nhigh-quality 3D reconstruction and view synthesis capabilities of 3DGS, where\nwe further extend it to offer a practical solution that relaxes common\nassumptions such as dense image views, accurate camera poses, and substantial\nimage overlaps. We achieve this through identifying and addressing unique\nchallenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians\nacross different views induce noisy or sparse gradients that destabilize\ntraining and hinder convergence, especially when above assumptions are not met.\nTo mitigate this, we employ pre-trained monocular depth estimation and visual\ncorrespondence models to achieve coarse alignments of 3D Gaussians. We then\nintroduce lightweight, learnable modules to refine depth and pose estimates\nfrom the coarse alignments, improving the quality of 3D reconstruction and\nnovel view synthesis. Furthermore, the refined estimates are leveraged to\nestimate geometry confidence scores, which assess the reliability of 3D\nGaussian centers and condition the prediction of Gaussian parameters\naccordingly. Extensive evaluations on large-scale real-world datasets\ndemonstrate that PF3plat sets a new state-of-the-art across all benchmarks,\nsupported by comprehensive ablation studies validating our design choices.\nproject page: https://cvlab-kaist.github.io/PF3plat/", "comment": "Accepted by ICML'25", "pdf_url": "http://arxiv.org/pdf/2410.22128v2", "cate": "cs.CV", "date": "2024-10-29", "updated": "2025-07-24", "AI": {"title_translation": "PF3plat：无姿态前向3D高斯泼溅", "tldr": "PF3plat是一个新的3D高斯泼溅框架，用于从无姿态图像进行新颖视图合成，无需密集视图、精确姿态或大量重叠即可实现最先进的结果。", "motivation": "本文旨在解决从无姿态图像进行单次前向新颖视图合成的问题。具体来说，它解决了3DGS在不满足常见假设（如密集图像视图、精确相机姿态和大量图像重叠）时所面临的挑战，即未对齐的3D高斯会导致训练不稳定和收敛困难。", "method": "该方法通过识别和解决像素对齐3DGS带来的独特挑战来实现。它利用预训练的单目深度估计和视觉对应模型实现3D高斯的粗略对齐。随后，引入轻量级、可学习的模块来细化深度和姿态估计。此外，利用细化的估计来评估几何置信度分数，以衡量3D高斯中心的可信度，并据此调整高斯参数的预测。", "result": "在大型真实世界数据集上的广泛评估表明，PF3plat在所有基准测试中都达到了新的最先进水平。", "conclusion": "PF3plat为从无姿态图像进行新颖视图合成提供了一个实用且最先进的解决方案，通过处理对齐问题以及利用学习到的细化和置信度评分，克服了传统3DGS的局限性。", "translation": "我们考虑从无姿态图像进行单次前向新颖视图合成的问题。我们的框架利用了3DGS的快速、可扩展以及高质量3D重建和视图合成能力，并进一步扩展它，提供了一个实用的解决方案，放宽了诸如密集图像视图、精确相机姿态和大量图像重叠等常见假设。我们通过识别和解决像素对齐3DGS带来的独特挑战来实现这一点：跨不同视图的未对齐3D高斯会产生噪声或稀疏梯度，从而破坏训练稳定性并阻碍收敛，尤其是在不满足上述假设的情况下。为了缓解这个问题，我们采用预训练的单目深度估计和视觉对应模型来实现3D高斯的粗略对齐。然后，我们引入轻量级、可学习的模块来从粗略对齐中细化深度和姿态估计，从而提高3D重建和新颖视图合成的质量。此外，细化的估计被用于估计几何置信度分数，该分数评估3D高斯中心的可信度，并相应地调整高斯参数的预测。在大型真实世界数据集上的广泛评估表明，PF3plat在所有基准测试中都达到了新的最先进水平，并通过全面的消融研究验证了我们的设计选择。项目页面：https://cvlab-kaist.github.io/PF3plat/", "summary": "PF3plat是一个用于从无姿态图像进行新颖视图合成的框架，它扩展了3D高斯泼溅（3DGS）以解决在缺乏密集视图、精确相机姿态和图像重叠时的挑战。该方法利用预训练的单目深度估计和视觉对应模型进行粗略对齐，并通过轻量级可学习模块进一步优化深度和姿态估计，同时引入几何置信度评分来指导高斯参数预测。PF3plat在大型真实世界数据集上取得了最先进的性能。", "keywords": "3D高斯泼溅, 新颖视图合成, 无姿态图像, 姿态估计, 深度估计", "comments": "该论文通过消除对精确相机姿态和密集视图的严格要求，显著推动了3DGS技术的发展，使其在实际应用中更具可行性。将预训练模型用于粗略对齐、可学习模块进行细化以及几何置信度评分的集成，是解决无姿态图像合成挑战的创新解决方案。"}}
{"id": "2507.17786", "title": "Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation", "authors": ["Florian Sobieczky", "Alfredo Lopez", "Erika Dudkin", "Christopher Lackner", "Matthias Hochsteger", "Bernhard Scheichl", "Helmut Sobieczky"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17786v1", "summary": "We introduce a reinforcement learning (RL) based adaptive optimization\nalgorithm for aerodynamic shape optimization focused on dimensionality\nreduction. The form in which RL is applied here is that of a surrogate-based,\nactor-critic policy evaluation MCMC approach allowing for temporal 'freezing'\nof some of the parameters to be optimized. The goals are to minimize\ncomputational effort, and to use the observed optimization results for\ninterpretation of the discovered extrema in terms of their role in achieving\nthe desired flow-field.\n  By a sequence of local optimized parameter changes around intermediate CFD\nsimulations acting as ground truth, it is possible to speed up the global\noptimization if (a) the local neighbourhoods of the parameters in which the\nchanged parameters must reside are sufficiently large to compete with the\ngrid-sized steps and its large number of simulations, and (b) the estimates of\nthe rewards and costs on these neighbourhoods necessary for a good step-wise\nparameter adaption are sufficiently accurate. We give an example of a simple\nfluid-dynamical problem on which the method allows interpretation in the sense\nof a feature importance scoring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17786v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "强化学习加速气动外形优化", "tldr": "该论文提出了一种基于强化学习的自适应优化算法，用于气动外形优化，旨在减少计算量并解释优化结果。", "motivation": "最小化计算工作量，并利用观察到的优化结果来解释所发现极值在实现所需流场中的作用。", "method": "提出了一种基于强化学习（RL）的自适应优化算法，形式为基于代理的Actor-Critic策略评估MCMC方法，允许对部分待优化参数进行时间上的“冻结”。通过围绕中间CFD模拟（作为真实值）进行一系列局部优化的参数更改，在特定条件下可加速全局优化。", "result": "论文给出了一个简单的流体动力学问题的例子，在该问题上，该方法允许进行特征重要性评分的解释。", "conclusion": "该方法能够加速气动外形优化，减少计算工作量，并对优化结果进行特征重要性评分的解释。", "translation": "我们引入了一种基于强化学习（RL）的自适应优化算法，用于侧重于降维的气动外形优化。这里应用RL的形式是一种基于代理的、Actor-Critic策略评估MCMC方法，允许对部分待优化参数进行时间上的“冻结”。目标是最小化计算工作量，并利用观察到的优化结果来解释所发现极值在实现所需流场中的作用。通过围绕作为真实值的中间CFD模拟进行一系列局部优化的参数更改，如果(a)参数所处的局部邻域足够大，足以与网格大小的步长及其大量的模拟竞争，并且(b)这些邻域上为良好分步参数适应所需的奖励和成本估计足够准确，则可以加速全局优化。我们给出了一个简单的流体动力学问题的例子，在该问题上，该方法允许进行特征重要性评分的解释。", "summary": "本文提出了一种基于强化学习的自适应优化算法，旨在加速气动外形优化并降低计算维度。该方法采用基于代理的Actor-Critic策略评估MCMC框架，通过对参数进行局部优化和冻结来减少计算量，并能解释优化结果。论文通过一个简单的流体动力学问题示例，展示了该方法在特征重要性评分方面的解释能力。", "keywords": "强化学习, 气动外形优化, 降维, Actor-Critic, MCMC", "comments": "该论文的创新点在于将强化学习（特别是基于代理的Actor-Critic MCMC方法）应用于气动外形优化，并引入了参数“冻结”的概念，以期减少计算量。其重要性在于提供了一种可能加速复杂工程优化问题的方法，并增加了对优化结果的解释性。"}}
{"id": "2507.18130", "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18130v1", "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18130v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "NoCode-bench：一个用于评估自然语言驱动功能添加的基准", "tldr": "引入了NoCode-bench，这是一个用于评估大型语言模型在自然语言驱动无代码开发中添加功能的基准。实验显示，尽管大型语言模型潜力巨大，但其在该任务上的成功率仅为15.79%，表明在跨文件编辑、代码库理解和工具调用方面仍面临挑战，目前尚未准备好完全支持此范式。", "motivation": "自然语言驱动的无代码开发有望提高生产力并实现开发的民主化，大型语言模型（LLMs）在此范式中展现出潜力。当前缺乏一个衡量LLMs在真实世界自然语言驱动功能添加任务中表现的基准。", "method": "本研究引入了NoCode-bench，这是一个专门用于评估LLMs在真实世界自然语言驱动功能添加任务上的基准。它包含634个任务，涵盖10个项目和11.4万次代码更改。每个任务将文档更新与相应的代码实现配对，并通过开发者编写的测试用例进行验证。此外，还包含一个由114个高质量、人工验证的实例组成的子集，即NoCode-bench Verified，以确保评估的可靠性。", "result": "实验结果显示，尽管大型语言模型（LLMs）的token使用量很高，但表现最佳的LLMs在任务成功率上仅达到15.79%。这突显了LLMs在跨文件编辑、代码库理解和工具调用方面的挑战。", "conclusion": "目前，大型语言模型尚未准备好完全支持自然语言驱动的无代码开发。NoCode-bench为该领域的未来发展奠定了基础。", "translation": "自然语言驱动的无代码开发允许用户使用自然语言（NL）而不是编辑源代码来指定软件功能，这有望提高生产力并使开发民主化。大型语言模型（LLMs）在此范式中显示出潜力。在这种背景下，软件文档充当了功能的自然语言规范。这项工作介绍了NoCode-bench，这是一个旨在评估LLMs在真实世界自然语言驱动功能添加任务上的基准，它由10个项目中的634个任务和11.4万次代码更改组成。每个任务都将文档更新与相应的代码实现配对，并通过开发者编写的测试用例进行验证。一个包含114个高质量、人工验证实例的子集，即NoCode-bench Verified，确保了评估的可靠性。我们的实验表明，尽管token使用量很高，但表现最佳的LLMs任务成功率仅为15.79%，这突显了跨文件编辑、代码库理解和工具调用方面的挑战。这些发现表明，LLMs尚未准备好完全支持自然语言驱动的无代码开发。NoCode-bench为该领域的未来发展奠定了基础。", "summary": "本文介绍了NoCode-bench，一个用于评估大型语言模型（LLMs）在自然语言驱动无代码开发中添加新功能的基准。该基准包含大量真实世界任务，旨在衡量LLMs根据文档更新生成相应代码的能力。实验结果表明，尽管LLMs在此领域有潜力，但其当前性能有限，成功率仅为15.79%，主要挑战在于跨文件编辑、代码库理解和工具调用。研究指出LLMs尚未成熟到可以完全支持这种开发范式，但NoCode-bench为未来的研究奠定了基础。", "keywords": "自然语言处理, 无代码开发, 大型语言模型, 基准测试, 功能添加", "comments": "NoCode-bench的创新之处在于其构建了一个大规模、真实世界且经过人工验证的基准，专门用于评估LLMs在自然语言驱动无代码开发中的功能添加能力。这对于推动LLMs在软件开发领域的应用至关重要，因为它揭示了当前LLMs的局限性，并为未来的研究指明了方向。其重要性在于，它为量化LLMs在复杂代码生成和理解任务中的表现提供了一个标准化的工具，有助于识别核心挑战，如跨文件依赖和工具集成，从而加速该领域的技术突破。"}}
{"id": "2507.17432", "title": "Non-Asymptotic Achievable Rate-Distortion Region for Indirect Wyner-Ziv Source Coding", "authors": ["Jiahui Wei", "Philippe Mary", "Elsa Dupraz"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 3 pages' appendix", "url": "http://arxiv.org/abs/2507.17432v2", "summary": "In the Wyner-Ziv source coding problem, a source $X$ has to be encoded while\nthe decoder has access to side information $Y$. This paper investigates the\nindirect setup, in which a latent source $S$, unobserved by both the encoder\nand the decoder, must also be reconstructed at the decoder. This scenario is\nincreasingly relevant in the context of goal-oriented communications, where $S$\ncan represent semantic information obtained from $X$. This paper derives the\nindirect Wyner-Ziv rate-distortion function in asymptotic regime and provides\nan achievable region in finite block-length. Furthermore, a Blahut-Arimoto\nalgorithm tailored for the indirect Wyner-Ziv setup, is proposed. This\nalgorithm is then used to give a numerical evaluation of the achievable\nindirect rate-distortion region when $S$ is treated as a classification label.", "comment": "8 pages, 2 figures, 3 pages' appendix", "pdf_url": "http://arxiv.org/pdf/2507.17432v2", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "间接 Wyner-Ziv 信源编码的非渐近可达率失真区域", "tldr": "本文研究间接Wyner-Ziv信源编码的非渐近率失真区域，推导了渐近率失真函数和有限块长可达区域，并提出了定制的Blahut-Arimoto算法进行数值评估。", "motivation": "在Wyner-Ziv信源编码问题中，解码器需要重建一个编码器和解码器均未观察到的潜在源S。这种间接设置在目标导向通信中越来越重要，其中S可以代表从源X中提取的语义信息。", "method": "1. 推导了渐近状态下的间接Wyner-Ziv率失真函数。2. 提供了有限块长度下的可达区域。3. 提出了一种针对间接Wyner-Ziv设置定制的Blahut-Arimoto算法。", "result": "提出的Blahut-Arimoto算法被用于在潜在源S被视为分类标签时，对可达间接率失真区域进行数值评估。", "conclusion": "本文成功推导了间接Wyner-Ziv率失真函数，给出了有限块长下的可达区域，并提出了一种定制的Blahut-Arimoto算法，用于数值评估在潜在源为分类标签时的可达率失真区域。", "translation": "在Wyner-Ziv信源编码问题中，源$X$必须被编码，而解码器可以访问侧信息$Y$。本文研究了间接设置，其中潜在源$S$（编码器和解码器均未观察到）也必须在解码器处重建。这种场景在目标导向通信的背景下越来越相关，其中$S$可以表示从$X$获得的语义信息。本文推导了渐近状态下的间接Wyner-Ziv率失真函数，并提供了有限块长度下的可达区域。此外，提出了一种专为间接Wyner-Ziv设置定制的Blahut-Arimoto算法。该算法随后用于在$S$被视为分类标签时，对可达间接率失真区域进行数值评估。", "summary": "本文研究间接Wyner-Ziv信源编码问题，其中解码器需重建未被观测到的潜在源S，这在目标导向通信中具有重要意义。文章推导了渐近率失真函数和有限块长度下的可达区域，并提出了一种定制的Blahut-Arimoto算法。该算法被用于在S为分类标签时对可达率失真区域进行数值评估。", "keywords": "Wyner-Ziv编码, 率失真, 非渐近, 间接设置, Blahut-Arimoto算法", "comments": "本文创新性地将Wyner-Ziv信源编码问题扩展到间接设置，考虑了潜在语义信息的重建，这对于目标导向通信具有实际应用价值。此外，提出并验证了定制的Blahut-Arimoto算法，为理论分析提供了有效的数值评估工具。"}}
{"id": "2505.07490", "title": "Evaluating the Scalability of Binary and Ternary CNN Workloads on RRAM-based Compute-in-Memory Accelerators", "authors": ["José Cubero-Cascante", "Rebecca Pelke", "Noah Flohr", "Arunkumar Vaidyanathan", "Rainer Leupers", "Jan Moritz Joseph"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      PREPRINT - Presented at the 2025 IEEE Computer Society Annual Symposium on VLSI (ISVLSI 2025)", "url": "http://arxiv.org/abs/2505.07490v3", "summary": "The increasing computational demand of Convolutional Neural Networks (CNNs)\nnecessitates energy-efficient acceleration strategies. Compute-in-Memory (CIM)\narchitectures based on Resistive Random Access Memory (RRAM) offer a promising\nsolution by reducing data movement and enabling low-power in-situ computations.\nHowever, their efficiency is limited by the high cost of peripheral circuits,\nparticularly Analog-to-Digital Converters (ADCs). Large crossbars and low ADC\nresolutions are often used to mitigate this, potentially compromising accuracy.\nThis work introduces novel simulation methods to model the impact of resistive\nwire parasitics and limited ADC resolution on RRAM crossbars. Our parasitics\nmodel employs a vectorised algorithm to compute crossbar output currents with\nerrors below 0.15% compared to SPICE. Additionally, we propose a variable\nstep-size ADC and a calibration methodology that significantly reduces ADC\nresolution requirements. These accuracy models are integrated with a\nstatistics-based energy model. Using our framework, we conduct a comparative\nanalysis of binary and ternary CNNs. Experimental results demonstrate that the\nternary CNNs exhibit greater resilience to wire parasitics and lower ADC\nresolution but suffer a 40% reduction in energy efficiency. These findings\nprovide valuable insights for optimising RRAM-based CIM accelerators for\nenergy-efficient deep learning.", "comment": "PREPRINT - Presented at the 2025 IEEE Computer Society Annual\n  Symposium on VLSI (ISVLSI 2025)", "pdf_url": "http://arxiv.org/pdf/2505.07490v3", "cate": "eess.SP", "date": "2025-05-12", "updated": "2025-07-24", "AI": {"title_translation": "评估二值和三值CNN工作负载在基于RRAM的存内计算加速器上的可扩展性", "tldr": "本文引入新的仿真方法来评估RRAM存内计算加速器中线寄生效应和ADC分辨率对二值和三值CNN的影响，发现三值CNN对寄生效应和低ADC分辨率更具弹性，但能效降低40%。", "motivation": "卷积神经网络(CNN)计算需求不断增长，需要能效高的加速策略。基于RRAM的存内计算(CIM)通过减少数据移动和实现低功耗原位计算提供解决方案，但其效率受限于外围电路（特别是ADC）的高成本。", "method": "本文引入新的仿真方法，建模电阻线寄生效应和有限ADC分辨率对RRAM交叉阵列的影响。寄生模型采用向量化算法计算交叉阵列输出电流，误差低于0.15%。提出了一种可变步长ADC和校准方法，显著降低ADC分辨率要求。这些精度模型与基于统计的能耗模型集成。", "result": "实验结果表明，三值CNN对线寄生效应和较低的ADC分辨率表现出更大的弹性，但能效降低了40%。", "conclusion": "这些发现为优化基于RRAM的CIM加速器以实现节能深度学习提供了有价值的见解。", "translation": "卷积神经网络（CNN）不断增长的计算需求需要节能的加速策略。基于电阻式随机存取存储器（RRAM）的存内计算（CIM）架构通过减少数据移动和实现低功耗原位计算，提供了一种有前景的解决方案。然而，它们的效率受到外围电路高成本的限制，特别是模数转换器（ADC）。通常使用大型交叉阵列和低ADC分辨率来缓解这一问题，但这可能会损害精度。这项工作引入了新颖的仿真方法，以模拟电阻线寄生效应和有限ADC分辨率对RRAM交叉阵列的影响。我们的寄生模型采用向量化算法计算交叉阵列输出电流，误差低于SPICE的0.15%。此外，我们提出了一种可变步长ADC和一种校准方法，显著降低了ADC分辨率要求。这些精度模型与基于统计的能耗模型集成。利用我们的框架，我们对二值和三值CNN进行了比较分析。实验结果表明，三值CNN对线寄生效应和较低的ADC分辨率表现出更大的弹性，但能效降低了40%。这些发现为优化基于RRAM的CIM加速器以实现节能深度学习提供了有价值的见解。", "summary": "本文针对RRAM存内计算加速器中CNN工作负载的能效问题，提出了新的仿真方法来建模电阻线寄生效应和有限ADC分辨率的影响。研究引入了高精度寄生模型和可变步长ADC及校准方法，并将其与能耗模型结合。通过对二值和三值CNN的比较分析，发现三值CNN对寄生效应和低ADC分辨率更具鲁棒性，但能效降低40%。研究结果为优化RRAM-CIM加速器以实现节能深度学习提供了重要指导。", "keywords": "RRAM, 存内计算, CNN, ADC, 能效", "comments": "本文的创新点在于提出了新的高精度仿真方法，用于评估RRAM-CIM加速器中线寄生和ADC分辨率的影响，并引入了可变步长ADC和校准方法来降低ADC要求。其重要性在于为优化RRAM-CIM加速器提供了量化分析和指导，特别是在二值和三值CNN的适用性方面。"}}
{"id": "2507.18632", "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "authors": ["Ye-Chan Kim", "SeungJu Cha", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACM MM 2025", "url": "http://arxiv.org/abs/2507.18632v1", "summary": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.", "comment": "Accepted to ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.18632v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SIDA：合成图像驱动的零样本域适应", "tldr": "SIDA通过生成合成图像进行零样本域适应，解决了现有文本驱动方法的局限性，并提高了效率和性能。", "motivation": "现有零样本域适应的文本驱动方法难以捕捉复杂的真实世界变化，且显著增加适应时间。", "method": "本文提出了SIDA，一种利用合成图像的新颖高效零样本域适应方法。首先，生成详细的源域图像并进行图像翻译以反映目标域风格。随后，利用这些合成图像的风格特征作为目标域的代理。在此基础上，引入了域混合（Domain Mix）和补丁风格迁移（Patch Style Transfer）模块，分别用于混合多种风格以扩展域内表示，以及将不同风格分配给单个图像块。", "result": "SIDA在各种零样本适应场景，特别是在挑战性域中，实现了最先进的性能。此外，该方法通过显著减少整体适应时间实现了高效率。", "conclusion": "SIDA通过利用合成图像和提出的域混合与补丁风格迁移模块，有效解决了零样本域适应的挑战，显著提升了性能和效率。", "translation": "零样本域适应是一种无需利用目标域图像数据即可使模型适应目标域的方法。为了在没有目标图像的情况下实现适应，现有研究利用CLIP的嵌入空间和文本描述来模拟目标域风格特征。尽管零样本域适应在以往取得了进展，但我们观察到这些文本驱动方法难以捕捉复杂的真实世界变化，并且由于其对齐过程显著增加了适应时间。\n我们没有依赖文本描述，而是探索利用图像数据来提供多样化和更细粒度的风格线索。在这项工作中，我们提出了SIDA，一种新颖高效的利用合成图像的零样本域适应方法。为了生成合成图像，我们首先创建详细的源域图像，并应用图像翻译以反映目标域的风格。然后，我们利用这些合成图像的风格特征作为目标域的代理。基于这些特征，我们引入了域混合（Domain Mix）和补丁风格迁移（Patch Style Transfer）模块，这些模块能够有效地建模真实世界的变异。特别是，域混合混合了多种风格以扩展域内表示，而补丁风格迁移将不同的风格分配给单个图像块。我们通过在各种零样本适应场景，特别是在挑战性域中展示最先进的性能来证明我们方法的有效性。此外，我们的方法通过显著减少整体适应时间实现了高效率。", "summary": "本文提出了SIDA，一种新颖高效的零样本域适应方法，通过生成合成图像来克服现有文本驱动方法的局限性。SIDA首先创建源域图像并进行风格迁移以模拟目标域风格，然后利用这些合成图像的风格特征作为代理。为有效捕捉真实世界变异，SIDA引入了Domain Mix和Patch Style Transfer模块。实验证明SIDA在零样本适应场景中达到最先进性能，并显著提高了适应效率。", "keywords": "零样本域适应, 合成图像, 域适应, 风格迁移, 深度学习", "comments": "SIDA的创新点在于放弃了文本描述，转而利用合成图像来获取更丰富、细粒度的风格信息，这解决了现有方法的局限性。其提出的Domain Mix和Patch Style Transfer模块也增强了模型处理复杂真实世界变化的能力。该方法在效率和性能上均有显著提升，为零样本域适应提供了一个有前景的新方向。"}}
{"id": "2504.05211", "title": "Exploiting individual differences to bootstrap communication", "authors": ["Richard A. Blythe", "Casimir Fisch"], "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Revised version is a full paper with considerable additional exposition and discussion. Now 21 pages including supplementary information, 11 figures", "url": "http://arxiv.org/abs/2504.05211v2", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition.", "comment": "Revised version is a full paper with considerable additional\n  exposition and discussion. Now 21 pages including supplementary information,\n  11 figures", "pdf_url": "http://arxiv.org/pdf/2504.05211v2", "cate": "cs.CL", "date": "2025-04-07", "updated": "2025-07-24", "AI": {"title_translation": "利用个体差异引导沟通的建立", "tldr": "现有通信起源理论依赖反馈，无法解释从非交流行为引导通信。本文提出一个模型，利用个体行为差异和共享意图，无需预设交流成功标准，即可引导无界意义的交流系统。", "motivation": "建立通信系统很困难，因为信号的初始意义对接收者和发送者都是未知的。大多数理论依赖反馈来强化成功行为，但这需要预先具备交流能力，因此无法解释通信如何从非通信行为中引导出来。", "method": "本文提出了一个模型，展示了在一个大型群体中，一个能够表达无限多意义的通信系统如何通过个体行为差异，且无需任何预先存在的确定通信成功的方法而出现。关键的认知能力包括在特定情况下可预测地行动以及在信号产生前心理状态的对齐（源于共享意图）。", "result": "该模型表明，通信系统可以仅仅通过个体行为差异和共享意图而自举形成，而无需预设的通信成功反馈。这两种能力独立于通信而存在。", "conclusion": "研究结果与语言等大型灵活的社会学习通信系统是普遍但发展良好的社会认知能力的产物理论相符，表明通信可以从非通信行为中引导出来。", "translation": "建立一个通信系统是困难的，因为信号在首次产生时其预期意义对接收者来说是未知的，而信号发送者也不知道该信号将如何被解读。大多数关于通信系统出现的理论解释都依赖于反馈来强化过去导致成功通信的行为。然而，提供这种反馈需要已经能够传达预期的或被解读的意义。因此，这些解释无法说明通信如何从非通信行为中引导出来。本文提出了一个模型，展示了一个能够表达无限多意义的通信系统，如何在一个大型群体中，由于个体行为差异而出现，而无需任何预先存在的确定通信成功的方法。导致这一结果的两个关键认知能力是在特定情况下可预测地行为，以及在信号产生前心理状态的对齐，这源于共享意图。由于这两种能力可以独立于通信而存在，我们的结果与以下理论相符：即像语言这样大型灵活的社会学习通信系统是普遍但发展良好的社会认知能力的产物。", "summary": "本文提出一个模型，解释了通信系统如何在没有预设反馈或交流能力的情况下，从非交流行为中自举形成。该模型强调个体行为差异和共享意图（包括可预测行为和心理状态对齐）是引导无限意义通信系统出现的关键认知能力。研究结果支持语言等复杂通信系统是高级社会认知能力的产物。", "keywords": "通信起源, 个体差异, 共享意图, 自举, 社会认知", "comments": "本文的创新之处在于提出了一个无需预设交流能力或反馈机制即可解释通信起源的模型，这挑战了传统理论。其重要性在于为理解语言等复杂社会学习通信系统的演化提供了新的视角，强调了社会认知在其中的作用。"}}
{"id": "2312.11283", "title": "A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census: Full Technical Report", "authors": ["John M. Abowd", "Tamara Adams", "Robert Ashmead", "David Darais", "Sourya Dey", "Simson L. Garfinkel", "Nathan Goldschlag", "Daniel Kifer", "Philip Leclerc", "Ethan Lew", "Scott Moore", "Rolando A. Rodríguez", "Ramy N. Tadros", "Lars Vilhuber"], "categories": ["stat.AP", "cs.CR", "econ.EM"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      Replaces \"The 2010 Census Confidentiality Protections Failed, Here's How and Why'' with the published version, which has a new title. Harvard Data Science Review (2025)", "url": "http://arxiv.org/abs/2312.11283v2", "summary": "Statistical agencies routinely use different strategies to protect the\nconfidentiality of tabular data from those used to protect the individual\nrecords in publicly released microdata. Aggregation is assumed to make the\nresulting statistics inherently less disclosive than the microdata. The 2010\nU.S. Census used different disclosure limitation rules for its tabular and\nmicrodata publications. We show that the assumption that these tabular data are\ninherently less disclosive than their underlying microdata is wrong. The 2010\nCensus published more than 150 billion statistics in 180 table sets, almost all\nat the most detailed geographic level -- individual census blocks. Using only\n34 of the published table sets, we reconstructed microdata for five variables\n(census block, sex, age, race, and ethnicity). Using only published data, an\nattacker using our methods can verify that all records in 70% of all census\nblocks (97 million people) are perfectly reconstructed. We confirm through\nreidentification studies that an attacker can, within census blocks with\nperfect reconstruction accuracy, correctly infer the actual census response on\nrace and ethnicity for 3.4 million vulnerable people (unique persons with race\nand ethnicity different from the modal person on the census block) with 95\\%\naccuracy. Next, we show that the more robust disclosure limitation framework\nused for the 2020 U.S. Census defends against attacks that are based on\nreconstruction. Finally, we show that available alternatives to the 2020 Census\nDisclosure Avoidance System would either fail to protect confidentiality or\noverly degrade the statistics' utility for the primary statutory use case:\nredrawing the boundaries of all the nation's legislative and voting districts\nin compliance with the 1965 Voting Rights Act. This is the full technical\nreport. For the summary paper see https://doi.org/10.1162/99608f92.4a1ebf70.", "comment": "Replaces \"The 2010 Census Confidentiality Protections Failed, Here's\n  How and Why'' with the published version, which has a new title. Harvard Data\n  Science Review (2025)", "pdf_url": "http://arxiv.org/pdf/2312.11283v2", "cate": "stat.AP", "date": "2023-12-18", "updated": "2025-07-23", "AI": {"title_translation": "2010年美国人口普查的模拟重建和再识别攻击：完整技术报告", "tldr": "本研究揭示2010年美国人口普查的表格数据并非如预期般安全，通过重建和再识别攻击，可以高精度地推断个人敏感信息，并评估了2020年普查的防御系统及替代方案。", "motivation": "研究旨在挑战统计机构关于表格数据比微观数据泄露风险更低的传统假设，特别是针对2010年美国人口普查的数据保护策略。", "method": "研究人员仅使用2010年美国人口普查发布的180个表格集中的34个，重建了五个变量（普查区、性别、年龄、种族、民族）的微观数据。通过再识别研究，验证了攻击者推断敏感信息的能力。随后，评估了2020年美国人口普查更稳健的披露限制框架，并分析了其他替代方案的有效性。", "result": "结果显示，2010年普查的表格数据并非固有地泄露风险较低。通过攻击，70%的普查区（9700万人）的所有记录被完美重建。在完美重建的普查区内，攻击者能以95%的准确率推断出340万易受攻击人群的种族和民族信息。研究还表明，2020年美国人口普查的披露限制框架能够防御基于重建的攻击，而其他现有替代方案要么无法保护机密性，要么会过度降低数据对立法和投票区划界用途的效用。", "conclusion": "2010年美国人口普查的表格数据存在严重的隐私泄露风险，传统的数据聚合保护假设是错误的。2020年普查的披露限制框架在抵御重建攻击方面表现更优，但其他替代方案在保护隐私和数据实用性之间存在权衡。", "translation": "统计机构通常对表格数据的保密性采用与公开微观数据中个人记录不同的保护策略。聚合被认为使得生成的统计数据固有地比微观数据泄露风险更低。2010年美国人口普查对其表格和微观数据出版物采用了不同的披露限制规则。我们表明，这些表格数据固有地比其底层微观数据泄露风险更低的假设是错误的。2010年人口普查在180个表格集中发布了超过1500亿条统计数据，几乎所有数据都达到了最详细的地理级别——单个普查区。仅使用已发布的34个表格集，我们重建了五个变量（普查区、性别、年龄、种族和民族）的微观数据。仅使用已发布的数据，攻击者利用我们的方法可以验证，所有普查区中70%的记录（9700万人）被完美重建。我们通过再识别研究证实，在完美重建的普查区内，攻击者能够以95%的准确率正确推断出340万易受攻击人群（种族和民族与普查区内多数人不同的独特个体）的实际普查种族和民族响应。接下来，我们展示了2020年美国人口普查所采用的更稳健的披露限制框架能够抵御基于重建的攻击。最后，我们表明，2020年人口普查披露规避系统的现有替代方案要么无法保护机密性，要么会过度降低统计数据对主要法定用途的效用：重新划分全国所有立法和投票区界限以符合1965年《投票权法案》。这是完整的技术报告。摘要论文请参见https://doi.org/10.1162/99608f92.4a1ebf70。", "summary": "本技术报告通过模拟重建和再识别攻击，揭示了2010年美国人口普查发布的表格数据存在严重的隐私泄露风险，推翻了聚合数据固有安全性的假设。研究人员仅利用部分公开表格数据，成功重建了9700万人的个人记录，并以高准确率再识别出敏感信息。报告进一步评估了2020年美国人口普查的更强健防御机制，并指出当前替代方案在隐私保护和数据实用性之间存在权衡。", "keywords": "人口普查, 数据隐私, 重建攻击, 再识别, 披露限制", "comments": "该研究通过实际模拟攻击，有力地揭示了传统数据聚合发布方式存在的严重隐私泄露风险，对统计机构的数据保护策略提出了严峻挑战。其创新性在于证明了即使是聚合的表格数据，在足够详细的粒度下，也可能被逆向工程重建并再识别出个人信息，这对于数据隐私保护领域具有重要警示意义。"}}
{"id": "2507.17892", "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17892v1", "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17892v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "DiNAT-IR：探索扩张邻域注意力以实现高质量图像恢复", "tldr": "DiNAT-IR提出了一种基于Transformer的图像恢复架构，通过引入通道感知模块来增强扩张邻域注意力，以解决现有方法在全局上下文和局部精度之间的权衡问题，并在多个基准测试中取得了有竞争力的结果。", "motivation": "Transformer在图像恢复任务中表现出色，但自注意力机制的高计算成本限制了其在高分辨率图像上的可扩展性。现有方法如Restormer的通道注意力可能忽略关键的局部伪影。直接应用扩张邻域注意力（DiNA）则因受限的全局上下文理解而影响视觉恢复。", "method": "本文探索了扩张邻域注意力（DiNA）作为替代方案，它结合了滑动窗口注意力与混合扩张因子以平衡全局上下文和局部精度。为解决DiNA直接应用于图像恢复时全局上下文理解不足的问题，作者引入了一个通道感知模块来补充局部注意力，从而有效地整合全局上下文，同时保持像素级精度。最终提出的架构名为DiNAT-IR。", "result": "DiNAT-IR在多个基准测试中取得了有竞争力的结果。", "conclusion": "DiNAT-IR作为一种专门为图像恢复设计的Transformer架构，通过引入通道感知模块来增强扩张邻域注意力，成功解决了全局上下文和局部精度之间的平衡问题，为各种低级计算机视觉问题提供了高质量的解决方案。", "translation": "Transformer凭借其建模长距离依赖的自注意力机制，已成为图像恢复任务中的主导范式。然而，自注意力的高计算成本限制了其在高分辨率图像上的可扩展性，使得效率-质量权衡成为一个关键的研究焦点。为了解决这个问题，Restormer采用了通道自注意力，它在通道而不是空间维度上计算注意力。虽然有效，但这种方法可能会忽略对于高质量图像恢复至关重要的局部伪影。为了弥补这一差距，我们探索了扩张邻域注意力（DiNA）作为一种有前景的替代方案，其灵感来源于其在高级视觉任务中的成功。DiNA通过将滑动窗口注意力与混合扩张因子相结合，在不过度开销的情况下有效扩展感受野，从而平衡了全局上下文和局部精度。然而，我们的初步实验表明，将这种全局-局部设计直接应用于经典的去模糊任务会阻碍准确的视觉恢复，这主要是由于局部注意力中受限的全局上下文理解。为了解决这个问题，我们引入了一个通道感知模块来补充局部注意力，从而有效地整合全局上下文，而不会牺牲像素级精度。所提出的DiNAT-IR是一种专门为图像恢复设计的基于Transformer的架构，在多个基准测试中取得了有竞争力的结果，为各种低级计算机视觉问题提供了一个高质量的解决方案。", "summary": "本文针对Transformer在图像恢复中自注意力计算成本高昂以及现有方法忽略局部伪影的问题，提出了DiNAT-IR架构。该架构以扩张邻域注意力（DiNA）为基础，并通过引入一个通道感知模块来弥补DiNA在直接应用于图像恢复时全局上下文理解的不足。DiNAT-IR通过平衡全局上下文和局部精度，在多个图像恢复基准测试中取得了有竞争力的表现，为低级计算机视觉任务提供了高质量的解决方案。", "keywords": "图像恢复, Transformer, 扩张邻域注意力, 通道感知模块, 低级视觉", "comments": "该论文的创新点在于将扩张邻域注意力（DiNA）引入图像恢复领域，并针对其在低级视觉任务中可能出现的全局上下文理解不足的问题，巧妙地设计了一个通道感知模块进行补充。这种结合全局与局部信息的方法，在保持像素级精度的同时，有效提升了模型在高分辨率图像上的处理能力和恢复质量，是解决图像恢复领域效率与质量权衡问题的一个重要尝试。"}}
{"id": "2507.18326", "title": "A Concept for Efficient Scalability of Automated Driving Allowing for Technical, Legal, Cultural, and Ethical Differences", "authors": ["Lars Ullrich", "Michael Buchholz", "Jonathan Petit", "Klaus Dietmayer", "Knut Graichen"], "categories": ["cs.CY", "cs.AI"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted to be published at 2025 28th IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November 18-21, 2025", "url": "http://arxiv.org/abs/2507.18326v1", "summary": "Efficient scalability of automated driving (AD) is key to reducing costs,\nenhancing safety, conserving resources, and maximizing impact. However,\nresearch focuses on specific vehicles and context, while broad deployment\nrequires scalability across various configurations and environments.\nDifferences in vehicle types, sensors, actuators, but also traffic regulations,\nlegal requirements, cultural dynamics, or even ethical paradigms demand high\nflexibility of data-driven developed capabilities. In this paper, we address\nthe challenge of scalable adaptation of generic capabilities to desired systems\nand environments. Our concept follows a two-stage fine-tuning process. In the\nfirst stage, fine-tuning to the specific environment takes place through a\ncountry-specific reward model that serves as an interface between technological\nadaptations and socio-political requirements. In the second stage,\nvehicle-specific transfer learning facilitates system adaptation and governs\nthe validation of design decisions. In sum, our concept offers a data-driven\nprocess that integrates both technological and socio-political aspects,\nenabling effective scalability across technical, legal, cultural, and ethical\ndifferences.", "comment": "Accepted to be published at 2025 28th IEEE International Conference\n  on Intelligent Transportation Systems (ITSC), Gold Coast, Australia, November\n  18-21, 2025", "pdf_url": "http://arxiv.org/pdf/2507.18326v1", "cate": "cs.CY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "自动驾驶高效可扩展性的概念：考量技术、法律、文化和伦理差异", "tldr": "该论文提出了一种两阶段微调概念（国家特定奖励模型+车辆特定迁移学习），用于实现自动驾驶的可扩展性，以应对技术、法律、文化和伦理差异。", "motivation": "自动驾驶（AD）的高效可扩展性是降低成本、提高安全性、节约资源和最大化影响力的关键。然而，现有研究侧重于特定车辆和环境，而广泛部署需要跨各种配置和环境的可扩展性，同时应对车辆类型、传感器、执行器、交通法规、法律要求、文化动态甚至伦理范式的差异。", "method": "本文提出了一种两阶段微调过程。第一阶段通过国家特定奖励模型对特定环境进行微调，该模型作为技术适应和社会政治要求之间的接口。第二阶段利用车辆特定迁移学习促进系统适应并管理设计决策的验证。这是一种数据驱动的过程。", "result": "该概念提供了一个数据驱动的过程，整合了技术和社会政治方面。", "conclusion": "所提出的概念通过数据驱动的两阶段微调过程，整合了技术和社会政治方面，从而实现了自动驾驶在技术、法律、文化和伦理差异下的有效可扩展性。", "translation": "自动驾驶（AD）的有效可扩展性是降低成本、提高安全性、节约资源和最大化影响力的关键。然而，研究主要集中在特定车辆和特定情境，而广泛部署需要跨各种配置和环境的可扩展性。车辆类型、传感器、执行器，以及交通法规、法律要求、文化动态甚至伦理范式上的差异，都要求数据驱动开发的能力具有高度灵活性。在本文中，我们解决了通用能力如何可扩展地适应所需系统和环境的挑战。我们的概念遵循一个两阶段的微调过程。在第一阶段，通过一个国家特定的奖励模型对特定环境进行微调，该模型作为技术适应和社会政治要求之间的接口。在第二阶段，车辆特定的迁移学习促进系统适应并管理设计决策的验证。总而言之，我们的概念提供了一个数据驱动的过程，整合了技术和社会政治方面，从而实现了跨技术、法律、文化和伦理差异的有效可扩展性。", "summary": "本文提出了一种新颖的两阶段微调概念，旨在实现自动驾驶系统的高效可扩展性。该方法解决了自动驾驶系统需要适应多样化技术规范（车辆类型、传感器）和社会政治环境（交通法规、法律要求、文化动态、伦理范式）的关键挑战。第一阶段通过国家特定的奖励模型对特定环境进行微调，将技术与社会政治需求联系起来。第二阶段则利用车辆特定的迁移学习进行系统适应和设计验证。这种数据驱动的方法旨在通过考虑技术、法律、文化和伦理差异，实现自动驾驶的有效和广泛部署。", "keywords": "自动驾驶, 可扩展性, 微调, 奖励模型, 迁移学习", "comments": "该论文解决了自动驾驶部署中的一个重要挑战：弥合技术发展与多样化社会政治现实之间的鸿沟。两阶段微调过程，特别是“国家特定奖励模型”的整合，似乎是将被动约束（如法律和文化差异）纳入技术适应过程的创新方式。这种整体方法对于实现现实世界的规模化部署至关重要。"}}
{"id": "2507.18118", "title": "A Two-armed Bandit Framework for A/B Testing", "authors": ["Jinjuan Wang", "Qianglin Wen", "Yu Zhang", "Xiaodong Yan", "Chengchun Shi"], "categories": ["stat.ML", "cs.LG", "stat.AP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18118v1", "summary": "A/B testing is widely used in modern technology companies for policy\nevaluation and product deployment, with the goal of comparing the outcomes\nunder a newly-developed policy against a standard control. Various causal\ninference and reinforcement learning methods developed in the literature are\napplicable to A/B testing. This paper introduces a two-armed bandit framework\ndesigned to improve the power of existing approaches. The proposed procedure\nconsists of three main steps: (i) employing doubly robust estimation to\ngenerate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to\nconstruct the test statistic, and (iii) applying a permutation-based method to\ncompute the $p$-value. We demonstrate the efficacy of the proposed method\nthrough asymptotic theories, numerical experiments and real-world data from a\nridesharing company, showing its superior performance in comparison to existing\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18118v1", "cate": "stat.ML", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "A/B测试的双臂赌博机框架", "tldr": "本文提出一个基于双臂赌博机框架的新A/B测试方法，通过伪结果、双臂赌博机统计量和置换检验提高现有方法的功效，并在理论、模拟和实际数据中表现出优越性。", "motivation": "现代科技公司广泛使用A/B测试进行策略评估和产品部署，现有方法虽然适用但仍有提升空间。本文旨在通过引入双臂赌博机框架来提高现有A/B测试方法的功效。", "method": "提出的方法包含三个主要步骤：(i) 采用双重鲁棒估计生成伪结果；(ii) 利用双臂赌博机框架构建检验统计量；(iii) 应用基于置换的方法计算p值。", "result": "通过渐近理论、数值实验和来自某网约车公司的真实世界数据，证明了所提出方法的有效性，并显示其性能优于现有方法。", "conclusion": "本文成功开发并验证了一种基于双臂赌博机框架的A/B测试新方法，该方法在提高功效方面优于现有方法，为A/B测试提供了更强大的工具。", "translation": "A/B测试在现代科技公司中被广泛用于策略评估和产品部署，其目标是比较新开发策略与标准对照组下的结果。文献中开发的各种因果推断和强化学习方法都适用于A/B测试。本文引入了一个双臂赌博机框架，旨在提高现有方法的功效。所提出的过程包括三个主要步骤：(i) 采用双重鲁棒估计生成伪结果，(ii) 利用双臂赌博机框架构建检验统计量，以及 (iii) 应用基于置换的方法计算p值。我们通过渐近理论、数值实验和来自某网约车公司的真实世界数据证明了所提出方法的有效性，显示其性能优于现有方法。", "summary": "本文提出一种新颖的双臂赌博机框架用于A/B测试，旨在提高现有方法的统计功效。该方法通过双重鲁棒估计生成伪结果，并结合双臂赌博机机制构建检验统计量，最后采用置换方法计算p值。理论分析、数值模拟以及真实世界数据（来自网约车公司）均验证了该方法的有效性及其优于现有方法的性能。", "keywords": "A/B测试, 双臂赌博机, 双重鲁棒估计, 置换检验, 统计功效", "comments": "这篇论文的创新点在于将双臂赌博机框架引入A/B测试，并结合双重鲁棒估计和置换检验，旨在提高统计功效。这对于需要快速、高效评估新策略的科技公司具有重要意义。通过理论和实践数据验证，该方法有望成为A/B测试的有力工具。"}}
{"id": "2507.17771", "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration", "authors": ["Dmitri Lyalikov"], "categories": ["cs.DC", "eess.IV"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17771v1", "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17771v1", "cate": "cs.DC", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "嵌入式RISC-V SoC中灵活向量集成实现端到端CNN推理加速", "tldr": "本文利用RISC-V Vector 1.0扩展，在嵌入式SoC中实现了灵活的向量集成，显著加速了CNN推理的预处理和回退层，并优化了能耗。", "motivation": "深度学习推理在资源受限的嵌入式平台上部署面临挑战，DLAs和NPUs虽然有潜力，但需要解决高效的系统集成、编译/执行模型以及硬件单元在内存层次结构中的放置问题。尤其，之前的工作主要关注加速器本身的速度提升，而忽略了运行时CNN执行和预/后处理中的性能瓶颈。", "method": "本研究利用RISC-V Vector 1.0扩展，并将其作为灵活的目标集成到合适的缓存层次结构中。通过实验验证了CNN执行和预/后处理的运行时性能瓶颈，并展示了RVV-1.0如何提供一个灵活的编程模型来减少预处理瓶颈和CPU回退过程。", "result": "图像预处理速度提升高达9倍；与CPU相比，YOLOv3回退层执行速度提升高达3倍。在加速器丰富的嵌入式SoC上实现了平衡的计算和内存占用，同时比传统并行执行平台消耗更少的功率。", "conclusion": "RISC-V Vector 1.0扩展为嵌入式SoC上的深度学习数据流提供了一个灵活且高效的编程模型，能够有效解决硬件集成挑战，显著加速CNN推理的端到端性能，包括预处理和回退层，并降低功耗。", "translation": "异构性和针对深度学习推理的领域特定架构的出现，为在资源受限的嵌入式平台上部署现代CNNs显示出巨大潜力。一个重要的发展是定制硬件的多样化，这些硬件专门针对CNN中最昂贵的部分。DLAs（深度学习加速器）和NPUs（神经网络处理单元）等可以克服传统硅扩展的极限，并为嵌入式SoC中的功耗/性能权衡提供解决方案。高效的DSA利用需要适当的系统集成和编译/执行模型，以在这些异构架构中实现平衡执行。对于在这些异构架构中实现平衡执行，迫切需要适当的系统集成和高效的编译/执行模型。这项工作强调了将这些单元高效放置在内存层次结构中以及与其他执行块正确接近的硬件集成挑战。我们实验验证了CNN执行和运行时预/后处理中的性能瓶颈，而之前通常只关注加速器加速。这项工作利用了RISC-V Vector 1.0扩展的批准，并展示了其作为在良好匹配的缓存层次结构方案中灵活目标的潜力，以减少预处理瓶颈和CPU回退过程。我们的结果显示，图像预处理速度提升高达9倍，YOLOv3回退层执行速度比CPU提升高达3倍。我们展示了RVV-1.0在暴露灵活编程模型方面的能力，该模型可以在支持现代深度学习数据流的富加速器嵌入式SoC上实现平衡的计算和内存占用，同时比传统并行执行平台消耗更少的功率。", "summary": "本文探讨了在嵌入式RISC-V SoC中集成深度学习加速器（DLAs/NPUs）以加速CNN推理的挑战。研究指出，除了加速器本身，预处理和CPU回退层也是性能瓶颈。通过利用RISC-V Vector 1.0扩展，并将其集成到优化的缓存层次结构中，该工作展示了一个灵活的编程模型，显著加速了图像预处理（高达9倍）和YOLOv3回退层（高达3倍），同时降低了功耗，为端到端CNN推理提供了高效解决方案。", "keywords": "RISC-V Vector, CNN推理, 嵌入式SoC, 深度学习加速器, 预处理加速", "comments": "该论文的创新点在于将RISC-V Vector 1.0扩展应用于嵌入式SoC中的CNN推理加速，并特别关注了通常被忽视的预处理和CPU回退层。通过解决这些瓶颈，它提供了一个更全面的端到端加速方案，而不仅仅是加速器核心。其强调的灵活编程模型和功耗效益对于资源受限的嵌入式平台具有重要意义。"}}
{"id": "2507.14562", "title": "1/2 order convergence rate of Euler-type methods for time-changed stochastic differential equations with super-linearly growing drift and diffusion coefficients", "authors": ["Yuanling Niu", "Shuai Wang", "Ying Zhang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14562v2", "summary": "This paper investigates the convergence rates of two Euler-type methods for a\nclass of time-changed stochastic differential equations with super-linearly\ngrowing drift and diffusion coefficients. Building upon existing research, we\nadapt the backward Euler method to time-changed stochastic differential\nequations where both coefficients exhibit super-linear growth and introduce an\nexplicit counterpart, the projected Euler method. It is shown that both methods\nachieve the optimal strong convergence rate of order 1/2 in the mean-square\nsense for this class of equations. Numerical simulations confirm the\ntheoretical findings", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14562v2", "cate": "math.NA", "date": "2025-07-19", "updated": "2025-07-24", "AI": {"title_translation": "具有超线性增长漂移和扩散系数的时间变换随机微分方程欧拉型方法的1/2阶收敛速度", "tldr": "本文研究了具有超线性增长漂移和扩散系数的时间变换随机微分方程的两种欧拉型方法，并证明它们在均方意义上达到了1/2的最优强收敛速度。", "motivation": "研究一类具有超线性增长漂移和扩散系数的时间变换随机微分方程的欧拉型方法的收敛速度。", "method": "在现有研究的基础上，本文将后向欧拉方法应用于时间变换随机微分方程，并引入了其显式对应方法——投影欧拉方法。", "result": "两种方法都达到了该类方程在均方意义上最优的1/2阶强收敛速度。数值模拟证实了理论发现。", "conclusion": "数值模拟证实了理论发现，即所提出的欧拉型方法对于具有超线性增长漂移和扩散系数的时间变换随机微分方程，在均方意义上能达到1/2阶的最优强收敛速度。", "translation": "本文研究了一类具有超线性增长漂移和扩散系数的时间变换随机微分方程的两种欧拉型方法的收敛速度。在现有研究的基础上，我们将后向欧拉方法应用于时间变换随机微分方程，并引入了其显式对应方法——投影欧拉方法。结果表明，这两种方法对于该类方程在均方意义上都达到了最优的1/2阶强收敛速度。数值模拟证实了理论发现。", "summary": "本文研究了针对具有超线性增长漂移和扩散系数的时间变换随机微分方程的两种欧拉型方法的收敛性。通过改编后向欧拉方法并引入投影欧拉方法，研究表明这两种方法在均方意义上均能达到1/2阶的最优强收敛速度，并通过数值模拟得到了验证。", "keywords": "欧拉型方法, 时间变换随机微分方程, 超线性增长系数, 收敛速度", "comments": "本文的创新之处在于针对一类具有挑战性的、系数超线性增长的时间变换随机微分方程，提出了并分析了两种欧拉型方法的收敛性，并证明了其最优的1/2阶收敛速度。这对于解决这类复杂随机微分方程的数值近似问题具有重要意义。"}}
{"id": "2507.18276", "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding", "authors": ["Xiaojie Zhang", "Yuanfei Wang", "Ruihai Wu", "Kunqi Xu", "Yu Li", "Liuyu Xiang", "Hao Dong", "Zhaofeng He"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.18276v1", "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.18276v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于基础模型推理和部件定位的自适应铰接物体实时操作", "tldr": "AdaRPG是一个新颖的框架，利用基础模型提取物体部件并进行推理，以实现对新型铰接物体的自适应操作，解决了现有方法在几何多样性和功能变异性方面的挑战。", "motivation": "铰接物体对机器人操作构成多样化挑战，因为其内部结构不可直接观察，机器人必须自适应地探索和完善动作。现有工作在跨类别泛化方面存在两大挑战：1）真实世界铰接物体的几何多样性使视觉感知和理解复杂化；2）物体功能和机制的变化阻碍了统一自适应操作策略的开发。", "method": "本文提出了AdaRPG框架，利用基础模型提取物体部件，以增强功能原始技能的视觉可供性泛化。为此，构建了一个部件级可供性标注数据集来训练可供性模型。此外，AdaRPG利用基础模型中嵌入的常识来推理复杂机制，并基于部件可供性推断生成调用原始技能功能的高级控制代码。", "result": "仿真和真实世界实验表明，AdaRPG对新型铰接物体类别具有很强的泛化能力。", "conclusion": "AdaRPG框架通过利用基础模型进行部件提取和复杂机制推理，有效解决了铰接物体操作中存在的几何多样性和功能变异性挑战，实现了对新型铰接物体的强大泛化操作能力。", "translation": "铰接物体对机器人操作构成多样化挑战。由于其内部结构不可直接观察，机器人必须自适应地探索和完善动作以生成成功的操作轨迹。尽管现有工作已尝试在自适应铰接物体操作中实现跨类别泛化，但两大主要挑战依然存在：（1）真实世界铰接物体的几何多样性使视觉感知和理解复杂化，以及（2）物体功能和机制的变化阻碍了统一自适应操作策略的开发。为了解决这些挑战，我们提出了AdaRPG，一个新颖的框架，它利用基础模型提取物体部件，这些部件比整个物体表现出更大的局部几何相似性，从而增强了功能原始技能的视觉可供性泛化。为了支持这一点，我们构建了一个部件级可供性标注数据集来训练可供性模型。此外，AdaRPG利用基础模型中嵌入的常识来推理复杂机制，并基于部件可供性推断生成调用原始技能功能的高级控制代码。仿真和真实世界实验表明，AdaRPG对新型铰接物体类别具有很强的泛化能力。", "summary": "本文提出了AdaRPG框架，旨在解决铰接物体操作中因几何多样性和功能变异性带来的挑战。AdaRPG利用基础模型提取物体部件以增强视觉可供性泛化，并构建了部件级可供性数据集。它还利用基础模型的常识推理复杂机制，生成高级控制代码。实验证明，AdaRPG对新型铰接物体具有强大的泛化能力。", "keywords": "铰接物体操作, 基础模型, 部件定位, 可供性, 机器人学习", "comments": "这篇论文的创新点在于将基础模型引入到铰接物体操作中，通过部件级理解和推理来克服几何多样性和功能变异性的挑战。利用基础模型进行部件提取和常识推理，是实现复杂机器人操作泛化的一个重要方向，具有很高的实用价值和研究潜力。"}}
{"id": "2507.17905", "title": "Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN", "authors": ["Mahbubur Rahman"], "categories": ["cs.NI", "cs.DC"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2507.17905v1", "summary": "LPWANs have become ubiquitous due to their ability to connect sensors over\nlarge geographic areas in a single hop. It is, however, very challenging to\nachieve massive scalability in LPWANs, where numerous sensors can transmit data\nefficiently and with low latency, which emerging IoT and CPS applications may\nrequire. In this paper, we address the above challenges by significantly\nadvancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal\nfrequency division multiplexing, D-OFDM, subcarriers to enable parallel\nreception of data to a BS from multiple asynchronous sensors, each using a\ndifferent subcarrier. In this paper, we achieve massive scalability in SNOW by\nenabling the BS to decode concurrent data from numerous asynchronous sensors on\nthe same subcarrier while parallelly decoding from other subcarriers as well.\nAdditionally, we enable numerous asynchronous sensors to receive distinct data\nfrom the BS on the same subcarrier while other sensors also receive data\nparallelly on other subcarriers. To do this, we develop a set of Gold\ncode-based pseudorandom noise or PN sequences that are mutually non-interfering\nwithin and across the subcarriers. Each sensor uses its PN sequence from the\nset for encoding or decoding data on its subcarriers, enabling massive\nconcurrency. Our evaluation results demonstrate that we can achieve\napproximately 9x more scalability in SNOW while being timely in data collection\nat the BS and energy efficient at the sensors. This may enable emerging IoT and\nCPS applications requiring tens of thousands of sensors with longer battery\nlife and making data-driven, time-sensitive decisions.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2507.17905v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "在LPWAN中实现异步和双向通信的可扩展性", "tldr": "本文通过改进SNOW技术，利用PN序列在LPWAN中实现大规模异步双向通信，将可扩展性提高了约9倍。", "motivation": "在LPWAN中实现大规模可扩展性非常具有挑战性，新兴的物联网和CPS应用可能需要大量传感器高效、低延迟地传输数据。", "method": "本文通过显著改进名为SNOW的LPWAN技术来应对挑战。SNOW利用分布式正交频分复用（D-OFDM）子载波，实现基站并行接收来自多个异步传感器的数据。为了实现大规模可扩展性，SNOW使基站能够解码同一子载波上来自大量异步传感器的并发数据，同时并行解码其他子载波上的数据。此外，还使大量异步传感器能够在同一子载波上从基站接收不同的数据。为此，开发了一组基于Gold码的伪随机噪声（PN）序列，这些序列在子载波内部和跨子载波之间互不干扰，每个传感器使用其PN序列进行数据编码或解码。", "result": "评估结果表明，在SNOW中实现了大约9倍的可扩展性，同时基站数据收集及时，传感器能效高。", "conclusion": "这可能支持新兴的物联网和CPS应用，这些应用需要数万个传感器，具有更长的电池寿命并能够做出数据驱动、时间敏感的决策。", "translation": "LPWAN因其能够在大地理区域内单跳连接传感器而变得无处不在。然而，在LPWAN中实现大规模可扩展性非常具有挑战性，新兴的物联网和CPS应用可能需要大量传感器高效、低延迟地传输数据。在本文中，我们通过显著改进一种名为SNOW的LPWAN技术来应对上述挑战。SNOW利用分布式正交频分复用（D-OFDM）子载波，使基站能够并行接收来自多个异步传感器的数据，每个传感器使用不同的子载波。在本文中，我们通过使基站能够解码同一子载波上来自大量异步传感器的并发数据，同时并行解码其他子载波上的数据，从而在SNOW中实现了大规模可扩展性。此外，我们还使大量异步传感器能够在同一子载波上从基站接收不同的数据，同时其他传感器也在其他子载波上并行接收数据。为此，我们开发了一组基于Gold码的伪随机噪声（PN）序列，这些序列在子载波内部和跨子载波之间互不干扰。每个传感器使用其PN序列集中的序列在其子载波上进行数据编码或解码，从而实现大规模并发。我们的评估结果表明，我们可以在SNOW中实现大约9倍的可扩展性，同时基站数据收集及时，传感器能效高。这可能支持新兴的物联网和CPS应用，这些应用需要数万个传感器，具有更长的电池寿命并能够做出数据驱动、时间敏感的决策。", "summary": "本文旨在解决低功耗广域网（LPWAN）中大规模可扩展性的挑战，以满足新兴物联网和CPS应用对大量传感器高效、低延迟通信的需求。通过改进名为SNOW的LPWAN技术，该研究利用分布式正交频分复用（D-OFDM）子载波和基于Gold码的伪随机噪声（PN）序列，实现了基站对同一子载波上多个异步传感器并发数据的解码，以及传感器在同一子载波上接收不同数据的能力，从而支持大规模异步双向通信。实验结果表明，该方法将SNOW的可扩展性提高了约9倍，同时保持了数据收集的及时性和传感器的能效，有望支持需要大量传感器和时间敏感决策的物联网/CPS应用。", "keywords": "LPWAN, 可扩展性, SNOW, D-OFDM, PN序列", "comments": "该论文通过引入基于Gold码的PN序列，巧妙地解决了LPWAN中D-OFDM子载波上的大规模并发问题，实现了显著的性能提升。其创新点在于允许在同一子载波上进行多用户并发通信，这对于资源受限的LPWAN来说是一个重要的突破。将可扩展性提高9倍的成果非常显著，对未来需要大量传感器、长电池寿命和时间敏感决策的物联网和CPS应用具有重要意义。"}}
{"id": "2507.17727", "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation", "authors": ["Robel Mamo", "Taeyeong Choi"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the 12th European Conference on Mobile Robots (ECMR 2025)", "url": "http://arxiv.org/abs/2507.17727v2", "summary": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.", "comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)", "pdf_url": "http://arxiv.org/pdf/2507.17727v2", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "CA-Cut：用于数据增强的作物对齐剪裁，以学习更鲁棒的冠层下导航", "tldr": "本文提出了一种名为CA-Cut的新型数据增强方法，通过遮蔽作物行周围区域，提高了冠层下导航模型的鲁棒性，显著降低了预测误差。", "motivation": "现有的冠层下视觉导航方法依赖于深度学习模型，需要大量昂贵的训练数据以确保在实际部署中的可靠性。然而，数据收集成本高昂。传统的色彩抖动、高斯模糊、水平翻转等数据增强技术在复杂、频繁遮挡、碎片多、作物间距不均匀的冠层下环境中表现不佳，导致次优性能。", "method": "本文提出了一种名为作物对齐剪裁（CA-Cut）的新型数据增强方法。该方法在输入图像中遮蔽随机区域，这些区域空间上分布在作物行两侧，以鼓励训练模型即使在精细信息被遮挡时也能捕获高层次的上下文特征。此外，还进行了消融研究以确定遮罩数量、每个遮罩的大小和遮罩的空间分布，从而最大化整体性能。", "result": "通过对公共玉米田数据集的广泛实验表明，基于遮罩的数据增强技术能有效模拟遮挡，并显著提高视觉导航中语义关键点预测的鲁棒性。特别是，CA-Cut中将遮罩分布偏向作物行对于提高预测精度和在不同环境下的泛化能力至关重要，预测误差最多可减少36.9%。", "conclusion": "CA-Cut（作物对齐剪裁）是一种新颖的数据增强技术，通过模拟遮挡并鼓励模型学习高层次的上下文特征，显著提高了冠层下导航模型的鲁棒性和泛化能力。", "translation": "最先进的视觉冠层下导航方法采用基于深度学习的感知模型，以区分可通行空间和作物行。尽管这些模型已展现出成功的性能，但它们需要大量的训练数据以确保在实际田间部署中的可靠性。然而，数据收集成本高昂，需要大量人力进行田间采样和标注。为解决这一挑战，模型训练期间通常采用各种数据增强技术，如色彩抖动、高斯模糊和水平翻转，以使训练数据多样化并增强模型鲁棒性。本文假设仅使用这些增强技术可能导致次优性能，尤其是在频繁遮挡、碎片和作物间距不均匀的复杂冠层下环境中。相反，我们提出了一种新颖的增强方法，即作物对齐剪裁（CA-Cut），它在输入图像中遮蔽随机区域，这些区域空间上分布在作物行两侧，以鼓励训练模型即使在精细信息被遮挡时也能捕获高层次的上下文特征。我们使用公共玉米田数据集进行的广泛实验表明，基于遮罩的增强对于模拟遮挡和显著提高视觉导航中语义关键点预测的鲁棒性是有效的。特别是，我们展示了CA-Cut中将遮罩分布偏向作物行对于提高预测精度和在不同环境下的泛化能力至关重要，预测误差最多可减少36.9%。此外，我们还进行了消融研究，以确定遮罩数量、每个遮罩的大小和遮罩的空间分布，从而最大化整体性能。", "summary": "本文介绍了一种名为CA-Cut的新型数据增强方法，旨在提高冠层下深度学习导航模型的鲁棒性。针对训练数据有限且昂贵、以及传统增强方法在复杂环境中局限性等挑战，CA-Cut通过遮蔽与作物行空间对齐的区域来迫使模型学习高层次的上下文特征，即使在存在遮挡的情况下也能有效工作。在玉米田数据集上的实验表明，CA-Cut显著提高了语义关键点预测的鲁棒性和泛化能力，通过将遮罩偏向作物行，预测误差最多可减少36.9%。", "keywords": "数据增强, 冠层下导航, 作物对齐剪裁, 鲁棒性, 语义关键点预测", "comments": "该论文的创新之处在于其“作物对齐”的剪裁策略，专门针对在实际场景中经常被遮挡的相关特征（作物行），这是一种提高鲁棒性的巧妙方法。其重要性在于解决了农业机器人领域中数据稀缺和环境复杂性带来的挑战，有望实现更可靠的自主导航。"}}
{"id": "2507.18446", "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization with Arrival-Time Ordering", "authors": ["Ivan Medennikov", "Taejin Park", "Weiqing Wang", "He Huang", "Kunal Dhawan", "Jinhan Wang", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2507.18446v1", "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2507.18446v1", "cate": "eess.AS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "流式Sortformer：基于说话人缓存和到达时间排序的在线说话人分离", "tldr": "本文提出了流式Sortformer，一个基于到达时间排序说话人缓存（AOSC）的在线说话人分离框架，用于实时多说话人跟踪，并在低延迟设置下表现出有效性。", "motivation": "本文旨在为Sortformer说话人分离框架提供一个流式扩展，以实现在线说话人分离，并保持输出说话人的到达时间排序特性。", "method": "该方法引入了一个到达时间排序说话人缓存（AOSC），用于存储先前观察到的说话人的帧级声学嵌入。AOSC根据说话人到达时间的顺序对嵌入进行排序，并根据模型过去的预测，通过选择得分最高的帧来动态更新，同时动态确定每个说话人存储的嵌入数量，以确保高效的缓存利用和精确的说话人跟踪。", "result": "在基准数据集上的实验证实了该方法的有效性和灵活性，即使在低延迟设置下也能表现良好。", "conclusion": "流式Sortformer被确立为一种用于实时多说话人跟踪的稳健解决方案，并为流式多说话人语音处理奠定了基础。", "translation": "本文提出了Sortformer说话人分离框架的流式扩展，其关键特性是输出说话人的到达时间排序。所提出的方法采用到达顺序说话人缓存（AOSC）来存储先前观察到的说话人的帧级声学嵌入。与传统的说话人跟踪缓冲区不同，AOSC根据说话人索引（对应于他们的到达时间顺序）对嵌入进行排序，并通过根据模型过去的预测选择得分最高的帧来动态更新。值得注意的是，每个说话人存储的嵌入数量由更新机制动态确定，确保了高效的缓存利用和精确的说话人跟踪。在基准数据集上的实验证实了我们方法的有效性和灵活性，即使在低延迟设置下也是如此。这些结果确立了流式Sortformer作为实时多说话人跟踪的稳健解决方案，并为流式多说话人语音处理奠定了基础。", "summary": "本文提出了一种名为流式Sortformer的在线说话人分离框架，它是Sortformer的流式扩展。该框架的核心创新是引入了到达时间排序说话人缓存（AOSC），用于高效存储和管理说话人声学嵌入，并根据说话人的到达顺序进行排序。AOSC通过动态选择高分帧进行更新，确保了准确的说话人跟踪和资源优化。实验结果表明，该方法在实时、低延迟的多说话人环境中表现出卓越的性能和灵活性，为未来的流式语音处理奠定了基础。", "keywords": "说话人分离, 流式处理, Sortformer, 实时, 缓存", "comments": "本文的核心创新在于提出了到达时间排序说话人缓存（AOSC），它不仅实现了说话人嵌入的动态管理和更新，还通过保留说话人到达时间顺序，为在线说话人分离提供了独特的解决方案。该方法在实时和低延迟场景下的有效性验证，使其在实际应用中具有重要价值。"}}
{"id": "2507.18503", "title": "Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention", "authors": ["João Luzio", "Alexandre Bernardino", "Plinio Moreno"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      To be published in the 2025 IEEE International Conference on Development and Learning (ICDL)", "url": "http://arxiv.org/abs/2507.18503v1", "summary": "In goal-directed visual tasks, human perception is guided by both top-down\nand bottom-up cues. At the same time, foveal vision plays a crucial role in\ndirecting attention efficiently. Modern research on bio-inspired computational\nattention models has taken advantage of advancements in deep learning by\nutilizing human scanpath data to achieve new state-of-the-art performance. In\nthis work, we assess the performance of SemBA-FAST, i.e. Semantic-based\nBayesian Attention for Foveal Active visual Search Tasks, a top-down framework\ndesigned for predicting human visual attention in target-present visual search.\nSemBA-FAST integrates deep object detection with a probabilistic semantic\nfusion mechanism to generate attention maps dynamically, leveraging pre-trained\ndetectors and artificial foveation to update top-down knowledge and improve\nfixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18\nbenchmark dataset, comparing its performance against other scanpath prediction\nmodels. Our methodology achieves fixation sequences that closely match human\nground-truth scanpaths. Notably, it surpasses baseline and other top-down\napproaches and competes, in some cases, with scanpath-informed models. These\nfindings provide valuable insights into the capabilities of semantic-foveal\nprobabilistic frameworks for human-like attention modelling, with implications\nfor real-time cognitive computing and robotics.", "comment": "To be published in the 2025 IEEE International Conference on\n  Development and Learning (ICDL)", "pdf_url": "http://arxiv.org/pdf/2507.18503v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "人类在目标存在视觉搜索中基于语义-中心凹贝叶斯注意的扫视路径预测", "tldr": "提出SemBA-FAST模型，结合深度目标检测和概率语义融合，用于预测目标存在视觉搜索中的人类扫视路径，并在COCO-Search18数据集上表现优异，超越基线模型。", "motivation": "在目标导向的视觉任务中，人类感知受多种线索引导，且中心凹视觉至关重要。尽管深度学习已推动计算注意力模型发展，但仍需更有效的方法来整合顶层和底层线索，并利用中心凹视觉来准确预测目标存在视觉搜索中的人类视觉注意力。", "method": "本文提出了SemBA-FAST（Semantic-based Bayesian Attention for Foveal Active visual Search Tasks）模型，这是一个用于预测目标存在视觉搜索中人类视觉注意力的自上而下框架。它将深度目标检测与概率语义融合机制相结合，动态生成注意力图，并利用预训练检测器和人工中心凹化来更新顶层知识并顺序改进注视点预测。该模型在COCO-Search18基准数据集上进行了评估和比较。", "result": "SemBA-FAST模型生成的注视序列与人类真实扫视路径高度匹配。其性能超越了基线模型和其他顶层方法，在某些情况下甚至可以与扫视路径信息模型竞争。", "conclusion": "这些发现为人类类注意力建模的语义-中心凹概率框架的能力提供了宝贵的见解，对实时认知计算和机器人技术具有重要意义。", "translation": "在目标导向的视觉任务中，人类感知受到自上而下和自下而上两种线索的引导。同时，中心凹视觉在有效引导注意力方面发挥着关键作用。现代生物启发计算注意力模型研究利用深度学习的进步，通过使用人类扫视路径数据实现了新的最先进性能。在这项工作中，我们评估了SemBA-FAST（即基于语义的中心凹主动视觉搜索任务贝叶斯注意力）的性能，这是一个旨在预测目标存在视觉搜索中人类视觉注意力的自上而下框架。SemBA-FAST将深度目标检测与概率语义融合机制相结合，动态生成注意力图，利用预训练的检测器和人工中心凹化来更新自上而下知识并顺序改进注视点预测。我们在COCO-Search18基准数据集上评估了SemBA-FAST，并将其性能与其他扫视路径预测模型进行了比较。我们的方法实现了与人类真实扫视路径高度匹配的注视序列。值得注意的是，它超越了基线模型和其他自上而下方法，在某些情况下甚至可以与扫视路径信息模型竞争。这些发现为人类类注意力建模的语义-中心凹概率框架的能力提供了宝贵见解，对实时认知计算和机器人技术具有重要意义。", "summary": "本文提出SemBA-FAST模型，一个基于语义-中心凹贝叶斯注意力的顶层框架，用于预测目标存在视觉搜索中的人类视觉注意力。该模型结合深度目标检测和概率语义融合，动态生成注意力图，并利用预训练检测器和人工中心凹化改进注视点预测。在COCO-Search18数据集上的评估表明，SemBA-FAST的注视序列与人类真实扫视路径高度匹配，且性能优于基线模型和其他顶层方法，甚至在某些情况下可与扫视路径信息模型媲美，为类人注意力建模提供了新思路。", "keywords": "扫视路径预测, 视觉注意力, 语义-中心凹, 贝叶斯注意力, 深度学习", "comments": "这项工作创新性地将深度目标检测与概率语义融合以及人工中心凹化相结合，形成了一个自上而下的框架来预测人类扫视路径，这在模拟人类复杂视觉注意机制方面迈出了重要一步。其在COCO-Search18数据集上的优异表现，特别是超越了现有顶层方法并能与扫视路径信息模型竞争，凸显了其方法的有效性和潜力。这对于实时认知计算和机器人领域的应用具有重要意义，有助于开发更类人化的视觉系统。"}}
{"id": "2411.16786", "title": "Staleness-Centric Optimizations for Parallel Diffusion MoE Inference", "authors": ["Jiajun Luo", "Lizhuo Luo", "Jianru Xu", "Jiajun Song", "Rongwei Lu", "Chen Tang", "Zhi Wang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16786v3", "summary": "Mixture-of-Experts-based (MoE-based) diffusion models demonstrate remarkable\nscalability in high-fidelity image generation, yet their reliance on expert\nparallelism introduces critical communication bottlenecks. State-of-the-art\nmethods alleviate such overhead in parallel diffusion inference through\ncomputation-communication overlapping, termed displaced parallelism. However,\nwe identify that these techniques induce severe *staleness*-the usage of\noutdated activations from previous timesteps that significantly degrades\nquality, especially in expert-parallel scenarios. We tackle this fundamental\ntension and propose DICE, a staleness-centric optimization framework with a\nthree-fold approach: (1) Interweaved Parallelism introduces staggered\npipelines, effectively halving step-level staleness for free; (2) Selective\nSynchronization operates at layer-level and protects layers vulnerable from\nstaled activations; and (3) Conditional Communication, a token-level,\ntraining-free method that dynamically adjusts communication frequency based on\ntoken importance. Together, these strategies effectively reduce staleness,\nachieving 1.26x speedup with minimal quality degradation. Empirical results\nestablish DICE as an effective and scalable solution. Our code is publicly\navailable at https://github.com/Cobalt-27/DICE", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16786v3", "cate": "cs.DC", "date": "2024-11-25", "updated": "2025-07-24", "AI": {"title_translation": "面向并行扩散MoE推理的陈旧度中心优化", "tldr": "MoE扩散模型在并行推理中存在通信瓶颈和陈旧度问题，导致质量下降。本文提出DICE框架，通过交织并行、选择性同步和条件通信来减少陈旧度，提高推理速度并保持质量。", "motivation": "MoE扩散模型在并行推理中存在通信瓶颈，现有方法（如位移并行）通过计算-通信重叠来缓解，但会引入严重的“陈旧度”问题，即使用来自先前时间步的过时激活，导致图像生成质量显著下降，尤其是在专家并行场景中。", "method": "本文提出了DICE框架，一个以陈旧度为中心的优化框架，包含三项策略：1) 交织并行，引入交错流水线，有效将步级陈旧度减半；2) 选择性同步，在层级保护易受陈旧激活影响的层；3) 条件通信，一种令牌级的、无需训练的方法，根据令牌重要性动态调整通信频率。", "result": "这些策略共同有效地减少了陈旧度，实现了1.26倍的加速，同时质量下降极小。实证结果表明DICE是一种有效且可扩展的解决方案。", "conclusion": "DICE框架通过解决并行扩散MoE推理中的陈旧度问题，提供了一种有效且可扩展的解决方案，能够在加速推理的同时保持生成质量。", "translation": "基于专家混合（MoE）的扩散模型在高质量图像生成中展现出卓越的可扩展性，但其对专家并行化的依赖引入了关键的通信瓶颈。最先进的方法通过计算-通信重叠（称为位移并行）来减轻并行扩散推理中的此类开销。然而，我们发现这些技术会引起严重的“陈旧度”——即使用来自先前时间步的过时激活，这显著降低了质量，尤其是在专家并行场景中。我们解决了这一根本性矛盾，并提出了DICE，一个以陈旧度为中心的优化框架，采用三管齐下的方法：（1）交织并行引入交错的流水线，有效地将步级陈旧度免费减半；（2）选择性同步在层级操作，保护易受陈旧激活影响的层；（3）条件通信是一种令牌级的、无需训练的方法，根据令牌重要性动态调整通信频率。这些策略共同有效地减少了陈旧度，实现了1.26倍的加速，同时质量下降极小。实证结果表明DICE是一种有效且可扩展的解决方案。我们的代码已在https://github.com/Cobalt-27/DICE 公开。", "summary": "本文针对MoE扩散模型并行推理中因计算-通信重叠导致的“陈旧度”问题，提出了DICE优化框架。DICE通过引入交织并行、选择性同步和条件通信三种策略，有效减少了过时激活的使用，从而在实现1.26倍加速的同时，最大限度地减少了图像生成质量的下降。实验证明DICE是一种有效且可扩展的解决方案。", "keywords": "扩散模型, 专家混合模型, 并行推理, 陈旧度, DICE", "comments": "本文识别并解决了并行MoE扩散模型推理中的一个关键且先前未充分解决的“陈旧度”问题，该问题直接影响生成质量。DICE框架通过其三项创新性策略（交织并行、选择性同步、条件通信）提供了一个全面的解决方案，特别是在不牺牲质量的前提下实现显著加速，这对于大规模部署MoE模型具有重要意义。"}}
{"id": "2507.18115", "title": "Agentic AI framework for End-to-End Medical Data Inference", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 2 tables, BIBM conference", "url": "http://arxiv.org/abs/2507.18115v1", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments.", "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "pdf_url": "http://arxiv.org/pdf/2507.18115v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于端到端医疗数据推理的智能体AI框架", "tldr": "本研究提出一个智能体AI框架，通过自动化医疗数据处理流程，从数据摄取到推理，以解决医疗领域机器学习部署中成本高、劳动密集、数据隐私和兼容性问题。", "motivation": "在医疗保健领域构建和部署机器学习解决方案成本高昂且劳动密集，原因在于预处理工作流程碎片化、模型兼容性问题以及严格的数据隐私限制。", "method": "本文引入了一个智能体AI框架，通过一个模块化、任务特定智能体系统，自动化了从数据摄取到推理的整个临床数据管道。这些智能体处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐。例如，它包含摄取识别智能体、数据匿名化智能体、特征提取智能体（用于表格数据和图像数据）、模型-数据特征匹配智能体、预处理推荐智能体和预处理执行智能体，以及模型推理智能体，使用SHAP、LIME和DETR注意力图生成可解释的输出。", "result": "该系统在老年病学、姑息治疗和结肠镜检查图像的公开数据集上进行了评估。通过自动化机器学习生命周期中摩擦较大的阶段，所提出的框架减少了重复专家干预的需求。", "conclusion": "所提出的框架通过自动化机器学习生命周期中的高摩擦阶段，降低了重复专家干预的需求，为在临床环境中操作化AI提供了一条可扩展、成本高效的途径。", "translation": "在医疗保健领域构建和部署机器学习解决方案仍然成本高昂且劳动密集，原因在于预处理工作流程碎片化、模型兼容性问题以及严格的数据隐私限制。在这项工作中，我们引入了一个智能体AI框架，通过一个模块化、任务特定智能体系统，自动化了从数据摄取到推理的整个临床数据管道。这些智能体处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐，无需人工干预。我们在老年病学、姑息治疗和结肠镜检查图像的公开数据集上评估了该系统。例如，在结构化数据（焦虑数据）和非结构化数据（结肠镜息肉数据）的情况下，管道首先由摄取识别智能体进行文件类型检测，然后由数据匿名化智能体确保隐私合规性，在此我们首先识别数据类型然后对其进行匿名化。特征提取智能体使用基于嵌入的方法识别表格数据的特征，提取所有列名，并使用基于MedGemma的多阶段方法处理图像数据，该方法推断模态和疾病名称。这些特征指导模型-数据特征匹配智能体从精心策划的存储库中选择最适合的模型。然后，预处理推荐智能体和预处理执行智能体根据数据类型和模型要求应用量身定制的预处理。最后，“模型推理智能体”在上传的数据上运行选定的模型，并使用SHAP、LIME和DETR注意力图等工具生成可解释的输出。通过自动化机器学习生命周期中这些高摩擦阶段，所提出的框架减少了重复专家干预的需求，为在临床环境中操作化AI提供了一条可扩展、成本高效的途径。", "summary": "该论文提出了一个名为“智能体AI框架”的端到端解决方案，旨在自动化医疗数据从摄取到推理的整个机器学习流程。该框架通过一系列模块化、任务特定的智能体处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐，从而减少人工干预。该系统在多个公开医疗数据集上进行了评估，并展示了其在降低成本、提高效率和可扩展性方面的潜力，特别是在解决数据隐私、兼容性和碎片化工作流等挑战方面。", "keywords": "智能体AI, 医疗数据, 端到端推理, 自动化, 机器学习", "comments": "该论文提出了一种创新的、基于智能体的方法来解决医疗AI部署中的关键痛点，特别是自动化了传统上需要大量人工干预的复杂流程。其模块化设计和对可解释性工具的集成是亮点，有望显著提高医疗AI的可操作性和效率。然而，抽象中并未提及具体的量化结果或与现有方法的详细比较，这可能限制了对其性能优势的全面评估。"}}
{"id": "2507.18444", "title": "DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition", "authors": ["Haiyang Jiang", "Songhao Piao", "Chao Gao", "Lei Yu", "Liguo Chen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18444v1", "summary": "Visual Place Recognition (VPR) is crucial for robust mobile robot\nlocalization, yet it faces significant challenges in maintaining reliable\nperformance under varying environmental conditions and viewpoints. To address\nthis, we propose a novel framework that integrates Dual-Scale-Former\n(DSFormer), a Transformer-based cross-learning module, with an innovative block\nclustering strategy. DSFormer enhances feature representation by enabling\nbidirectional information transfer between dual-scale features extracted from\nthe final two CNN layers, capturing both semantic richness and spatial details\nthrough self-attention for long-range dependencies within each scale and shared\ncross-attention for cross-scale learning. Complementing this, our block\nclustering strategy repartitions the widely used San Francisco eXtra Large\n(SF-XL) training dataset from multiple distinct perspectives, optimizing data\norganization to further bolster robustness against viewpoint variations.\nTogether, these innovations not only yield a robust global embedding adaptable\nto environmental changes but also reduce the required training data volume by\napproximately 30\\% compared to previous partitioning methods. Comprehensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nacross most benchmark datasets, surpassing advanced reranking methods like\nDELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution\nusing 512-dim global descriptors, while significantly improving computational\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18444v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DSFormer：一种用于视觉地点识别的双尺度交叉学习Transformer", "tldr": "DSFormer提出了一种双尺度交叉学习Transformer和块聚类策略，以提高视觉地点识别在环境和视角变化下的鲁棒性，并在多个基准数据集上取得了最先进的性能。", "motivation": "视觉地点识别（VPR）对于移动机器人鲁棒定位至关重要，但在环境条件和视角变化下保持可靠性能面临巨大挑战。", "method": "本文提出了一种新颖的框架，该框架将基于Transformer的交叉学习模块Dual-Scale-Former（DSFormer）与创新的块聚类策略相结合。DSFormer通过在最后两个CNN层提取的双尺度特征之间实现双向信息传输来增强特征表示，通过自注意力捕获每个尺度内的长距离依赖性和通过共享交叉注意力进行跨尺度学习，从而捕获语义丰富性和空间细节。此外，块聚类策略从多个不同视角重新划分了广泛使用的SF-XL训练数据集，优化了数据组织，以进一步增强对视角变化的鲁棒性。", "result": "这些创新不仅产生了适应环境变化的鲁棒全局嵌入，而且与以前的分区方法相比，将所需训练数据量减少了约30%。全面的实验表明，我们的方法在使用512维全局描述符作为全局检索解决方案时，在大多数基准数据集上实现了最先进的性能，超越了DELG、Patch-NetVLAD、TransVPR和R2Former等先进的重排序方法，同时显著提高了计算效率。", "conclusion": "本文提出的DSFormer框架结合了双尺度交叉学习Transformer和创新的块聚类策略，有效解决了视觉地点识别在复杂环境和视角变化下的鲁棒性问题，并在性能和效率上达到了SOTA水平。", "translation": "视觉地点识别（VPR）对于鲁棒的移动机器人定位至关重要，但它在维持不同环境条件和视角下的可靠性能方面面临重大挑战。为了解决这个问题，我们提出了一种新颖的框架，该框架将双尺度Transformer（DSFormer）——一个基于Transformer的交叉学习模块，与创新的块聚类策略相结合。DSFormer通过在最后两个CNN层提取的双尺度特征之间实现双向信息传输来增强特征表示，通过自注意力捕获每个尺度内的长距离依赖性，并通过共享交叉注意力进行跨尺度学习，从而捕获语义丰富性和空间细节。作为补充，我们的块聚类策略从多个不同视角重新划分了广泛使用的旧金山特大（SF-XL）训练数据集，优化了数据组织，以进一步增强对视角变化的鲁棒性。这些创新共同不仅产生了适应环境变化的鲁棒全局嵌入，而且与以前的分区方法相比，将所需训练数据量减少了约30%。全面的实验表明，我们的方法在使用512维全局描述符作为全局检索解决方案时，在大多数基准数据集上实现了最先进的性能，超越了DELG、Patch-NetVLAD、TransVPR和R2Former等先进的重排序方法，同时显著提高了计算效率。", "summary": "本文提出了一种名为DSFormer的新型框架，用于解决视觉地点识别在环境和视角变化下的鲁棒性问题。DSFormer是一个双尺度交叉学习Transformer模块，它通过在不同尺度的特征之间进行双向信息传输来增强特征表示，结合自注意力和交叉注意力捕获长距离依赖和跨尺度学习。此外，该框架还引入了创新的块聚类策略，优化了训练数据组织以增强对视角变化的鲁棒性，并显著减少了所需训练数据量。实验证明，DSFormer在多个基准数据集上达到了最先进的性能，并且在计算效率上也有显著提升。", "keywords": "视觉地点识别, Transformer, 双尺度学习, 块聚类, 全局描述符", "comments": "该论文的创新点在于提出了DSFormer，一个结合双尺度特征交叉学习的Transformer，以及一个优化数据组织以增强鲁棒性的块聚类策略。这种方法不仅提高了视觉地点识别的准确性，还通过减少训练数据量和提高计算效率，展现了其在实际应用中的潜力。"}}
{"id": "2507.17974", "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "authors": ["Fitsum Gaim", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17974v1", "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17974v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "提格利尼亚语的自然语言处理：现状与未来方向", "tldr": "本文对提格利尼亚语的自然语言处理（NLP）研究进行了全面调查，分析了40多项研究，揭示了从基于规则系统到神经网络架构的演变，并指出了挑战和未来方向。", "motivation": "尽管提格利尼亚语有数百万人使用，但在自然语言处理（NLP）研究中仍然严重不足。本文旨在全面调查其NLP研究现状。", "method": "本文对提格利尼亚语的NLP研究进行了全面调查，分析了2011年至2025年间跨越十多年的40多项研究。系统地回顾了计算资源、模型和应用在十个不同的下游任务（包括形态处理、机器翻译、语音识别和问答）中的现状。", "result": "分析揭示了从基础的、基于规则的系统到现代神经网络架构的清晰发展轨迹，进展持续得益于资源创建的里程碑。同时，指出了提格利尼亚语形态复杂性和资源稀缺性带来的关键挑战。", "conclusion": "这项工作为研究人员提供了全面的参考，也为推进提格利尼亚语NLP提供了路线图。", "translation": "尽管有数百万人使用，提格利尼亚语在自然语言处理（NLP）研究中仍然严重不足。这项工作对提格利尼亚语的NLP研究进行了全面调查，分析了2011年至2025年间跨越十多年的40多项研究。我们系统地回顾了计算资源、模型和应用在十个不同的下游任务中的现状，包括形态处理、机器翻译、语音识别和问答。我们的分析揭示了从基础的、基于规则的系统到现代神经网络架构的清晰发展轨迹，进展持续得益于资源创建的里程碑。我们指出了提格利尼亚语形态复杂性和资源稀缺性带来的关键挑战，同时强调了有前景的研究方向，包括形态感知建模、跨语言迁移和以社区为中心的资源开发。这项工作既可以作为研究人员的全面参考，也可以作为推进提格利尼亚语NLP的路线图。调查研究和资源的精选元数据已公开提供。", "summary": "本文对提格利尼亚语的自然语言处理（NLP）研究进行了全面调查，分析了过去十多年来的40多项研究。研究回顾了计算资源、模型和应用在十个下游任务中的现状，并揭示了从基于规则系统到现代神经网络架构的演变，其中资源创建是关键驱动因素。文章指出了形态复杂性和资源稀缺性等挑战，并提出了形态感知建模、跨语言迁移和社区中心资源开发等有前景的研究方向。该工作旨在为研究人员提供参考，并为提格利尼亚语NLP的未来发展提供路线图。", "keywords": "提格利尼亚语, 自然语言处理, 调查, 资源稀缺, 未来方向", "comments": "这项工作对于解决提格利尼亚语在NLP领域的高度代表性不足问题至关重要。通过提供全面的调查和未来路线图，它为研究人员提供了宝贵的资源，并可能激发对这种语言的更多关注和资源开发。其创新之处在于系统性地梳理了该语言的NLP发展历程，并明确指出了挑战和机遇，为该领域设定了方向。"}}
{"id": "2507.18442", "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data", "authors": ["Rana Alshaikh", "Israa Alghanmi", "Shelan Jeawak"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18442v1", "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18442v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "AraTable：基准测试LLM对阿拉伯表格数据的推理和理解能力", "tldr": "AraTable是一个新的基准测试，用于评估大型语言模型（LLMs）对阿拉伯表格数据的推理和理解能力，并发现LLMs在复杂任务上仍面临挑战。", "motivation": "尽管大型语言模型（LLMs）在自然语言处理方面取得了显著进展，但在解释结构化数据（尤其是表格数据）方面的表现仍然有限。现有针对英文表格数据的基准测试很丰富，但阿拉伯语由于公共资源有限和其独特的语言特征而代表性不足。为了弥补这一空白，本研究提出了AraTable。", "method": "本研究提出了AraTable，一个新颖且全面的基准测试，旨在评估LLMs应用于阿拉伯表格数据时的推理和理解能力。AraTable包含直接问答、事实核查和复杂推理等多种评估任务，涉及广泛的阿拉伯表格数据源。其方法遵循混合管道，初始内容由LLMs生成，随后由人类专家过滤和验证，以确保数据集的高质量。此外，还提出了一个全自动评估框架，该框架使用自省机制，其性能与人类判断几乎相同。", "result": "使用AraTable进行的初步分析表明，LLMs在直接问答等简单表格任务上表现良好，但在需要更深层次推理和事实核查的任务上仍然面临显著的认知挑战。这表明未来在改进复杂表格推理任务性能方面存在巨大机会。", "conclusion": "本研究提供了一个有价值的、公开可用的资源和评估框架，可以帮助加速处理和分析阿拉伯结构化数据的基础模型的发展。LLMs在复杂阿拉伯表格数据推理上仍有很大的提升空间。", "translation": "大型语言模型（LLMs）的认知和推理能力在自然语言处理方面取得了显著进展。然而，它们在解释结构化数据，特别是表格数据方面的表现仍然有限。尽管英文表格数据的基准测试广泛可用，但阿拉伯语由于公共资源有限和其独特的语言特征而代表性不足。为了弥补这一空白，我们提出了AraTable，一个新颖且全面的基准测试，旨在评估LLMs应用于阿拉伯表格数据时的推理和理解能力。AraTable包含各种评估任务，例如直接问答、事实核查和复杂推理，涉及广泛的阿拉伯表格来源。我们的方法遵循混合管道，其中初始内容由LLMs生成，随后由人类专家过滤和验证，以确保数据集的高质量。使用AraTable进行的初步分析表明，虽然LLMs在直接问答等简单表格任务上表现良好，但当任务需要更深层次的推理和事实核查时，它们仍然面临显著的认知挑战。这表明未来在改进复杂表格推理任务性能方面存在巨大机会。我们还提出了一个全自动评估框架，该框架使用自省机制，其性能与人类判断几乎相同。这项研究提供了一个有价值的、公开可用的资源和评估框架，可以帮助加速处理和分析阿拉伯结构化数据的基础模型的发展。", "summary": "本研究介绍了AraTable，一个专为评估大型语言模型（LLMs）对阿拉伯语表格数据推理和理解能力而设计的新型综合基准测试。该基准测试包含直接问答、事实核查和复杂推理等任务，并采用LLM生成与人工验证相结合的混合构建方法，确保数据质量。初步结果显示，LLMs在简单任务上表现尚可，但在需要深度推理和事实核查的复杂任务上仍面临挑战。研究还提出了一个接近人类判断性能的自动化评估框架。AraTable为促进阿拉伯语结构化数据处理模型的发展提供了宝贵的公共资源和评估工具。", "keywords": "阿拉伯语表格数据, LLM, 基准测试, AraTable, 推理能力", "comments": "该论文的创新之处在于填补了阿拉伯语表格数据LLM基准测试的空白，为研究人员提供了急需的资源。其混合数据生成方法确保了数据集的高质量。同时，提出的自动化评估框架具有重要意义，因为它能有效降低评估成本。这项工作对于推动LLMs在低资源语言和复杂结构化数据理解方面的发展具有重要价值。"}}
{"id": "2412.14824", "title": "Provably Convergent Plug-and-play Proximal Block Coordinate Descent Method for Hyperspectral Anomaly Detection", "authors": ["Xiaoxia Liu", "Shijie YU"], "categories": ["math.OC", "eess.IV"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.14824v3", "summary": "Hyperspectral anomaly detection refers to identifying pixels in the\nhyperspectral images that have spectral characteristics significantly different\nfrom the background. In this paper, we introduce a novel model that represents\nthe background information using a low-rank representation. We integrate an\nimplicit proximal denoiser prior, associated with a deep learning based\ndenoiser, within a plug-and-play (PnP) framework to effectively remove noise\nfrom the eigenimages linked to the low-rank representation. Anomalies are\ncharacterized using a generalized group sparsity measure, denoted as\n$\\|\\cdot\\|_{2,\\psi}$. To solve the resulting orthogonal constrained nonconvex\nnonsmooth optimization problem, we develop a PnP-proximal block coordinate\ndescent (PnP-PBCD) method, where the eigenimages are updated using a proximal\ndenoiser within the PnP framework. We prove that any accumulation point of the\nsequence generated by the PnP-PBCD method is a stationary point. We evaluate\nthe effectiveness of the PnP-PBCD method on hyperspectral anomaly detection in\nscenarios with and without Gaussian noise contamination. The results\ndemonstrate that the proposed method can effectively detect anomalous objects,\noutperforming the competing methods that may mistakenly identify noise as\nanomalies or misidentify the anomalous objects due to noise interference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.14824v3", "cate": "math.OC", "date": "2024-12-19", "updated": "2025-07-24", "AI": {"title_translation": "可证明收敛的即插即用近端块坐标下降高光谱异常检测方法", "tldr": "提出了一种基于低秩表示和即插即用框架的PnP-PBCD方法，用于高光谱异常检测，并证明了其收敛性，在噪声环境下表现优异。", "motivation": "高光谱异常检测旨在识别图像中与背景光谱特征显著不同的像素。现有方法可能在噪声干扰下将噪声误识别为异常或漏检异常。", "method": "提出了一种新模型，用低秩表示背景信息，并整合了基于深度学习去噪器的隐式近端去噪先验到即插即用(PnP)框架中，以去除低秩表示相关特征图像中的噪声。异常通过广义组稀疏度量 $||\\cdot||_{2,\\psi}$ 表征。为解决由此产生的正交约束非凸非光滑优化问题，开发了一种PnP-近端块坐标下降 (PnP-PBCD) 方法，其中特征图像使用PnP框架内的近端去噪器进行更新。该方法被证明任何累积点都是一个驻点。", "result": "PnP-PBCD方法在高光谱异常检测中，在有无高斯噪声污染的场景下均有效。结果表明，该方法能有效检测异常目标，优于可能将噪声误识别为异常或因噪声干扰而误识别异常目标的竞争方法。", "conclusion": "该论文提出了一种新颖的PnP-PBCD方法用于高光谱异常检测，该方法利用低秩背景表示和深度学习去噪器，并在理论上证明了其收敛性。实验结果表明，该方法在噪声环境下具有鲁棒性和优越的异常检测性能。", "translation": "高光谱异常检测是指识别高光谱图像中光谱特征与背景显著不同的像素。在本文中，我们引入了一个新颖的模型，该模型使用低秩表示来表示背景信息。我们将一个与基于深度学习的去噪器相关的隐式近端去噪先验整合到即插即用（PnP）框架中，以有效地从与低秩表示相关的特征图像中去除噪声。异常使用广义组稀疏度量，表示为 $||\\cdot||_{2,\\psi}$ 来表征。为了解决由此产生的正交约束非凸非光滑优化问题，我们开发了一种PnP-近端块坐标下降（PnP-PBCD）方法，其中特征图像在PnP框架内使用近端去噪器进行更新。我们证明了PnP-PBCD方法生成的序列的任何累积点都是一个驻点。我们在有无高斯噪声污染的场景下评估了PnP-PBCD方法在高光谱异常检测中的有效性。结果表明，所提出的方法能够有效地检测异常目标，优于那些可能将噪声误识别为异常或由于噪声干扰而误识别异常目标的竞争方法。", "summary": "本文提出了一种用于高光谱异常检测的PnP-近端块坐标下降（PnP-PBCD）方法。该方法通过低秩表示建模背景信息，并结合即插即用框架中的深度学习去噪器来处理噪声。异常通过广义组稀疏度量表征。为解决相应的非凸非光滑优化问题，开发了PnP-PBCD算法并从理论上证明了其收敛性。实验结果表明，该方法在存在噪声的情况下，仍能有效且准确地检测出异常，优于现有方法。", "keywords": "高光谱异常检测, 即插即用, 近端块坐标下降, 低秩表示, 深度学习去噪", "comments": "该论文的创新点在于结合了低秩背景表示、即插即用框架中的深度学习去噪器以及广义组稀疏度量来定义异常，并提出了一种具有可证明收敛性的PnP-PBCD优化算法。其重要性在于提供了一种在噪声环境下鲁棒且高效的高光谱异常检测方法，解决了传统方法易受噪声干扰的局限性。"}}
{"id": "2503.04151", "title": "Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation", "authors": ["Jie Xu", "Na Zhao", "Gang Niu", "Masashi Sugiyama", "Xiaofeng Zhu"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.04151v2", "summary": "Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually causes MVL methods designed for specific combinations of views to lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nmulti-view unsupervised clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate RML's effectiveness. Code is\navailable at https://github.com/SubmissionsIn/RML.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.04151v2", "cate": "cs.CV", "date": "2025-03-06", "updated": "2025-07-24", "AI": {"title_translation": "通过样本级注意力表示融合和模拟扰动对齐的鲁棒多视图学习", "tldr": "提出了一种名为RML的鲁棒多视图学习方法，通过Transformer融合网络结合样本级注意力进行表示融合，并利用基于模拟扰动的对比学习对齐表示，以应对异构和不完美的多视图数据。", "motivation": "现实世界中的多视图数据集通常是异构且不完美的，这导致为特定视图组合设计的多视图学习方法缺乏应用潜力并限制了其有效性。", "method": "提出了一种名为RML的鲁棒多视图学习方法，该方法同时进行表示融合和对齐。具体来说，引入了一个简单而有效的多视图Transformer融合网络，将异构多视图数据转换为同质词嵌入，并通过样本级注意力机制整合多个视图以获得融合表示。此外，提出了一个基于模拟扰动的多视图对比学习框架，动态生成噪声和不可用扰动来模拟不完美数据条件。模拟的噪声和不可用数据获得两个不同的融合表示，并利用对比学习对其进行对齐，以学习判别性和鲁棒的表示。RML是自监督的，也可作为正则化应用于下游任务。", "result": "在多视图无监督聚类、噪声标签分类和作为跨模态哈希检索的即插即用模块中的实验表明，RML是有效的。广泛的比较实验和消融研究验证了RML的有效性。", "conclusion": "RML通过结合样本级注意力表示融合和模拟扰动对齐，有效地解决了多视图数据异构性和不完美性带来的挑战，能够学习判别性和鲁棒的表示，并在多种下游任务中表现出良好的性能。", "translation": "最近，多视图学习（MVL）因其融合来自多个视图的判别性信息的能力而受到广泛关注。然而，现实世界中的多视图数据集通常是异构且不完美的，这通常导致为特定视图组合设计的多视图学习方法缺乏应用潜力并限制了其有效性。为了解决这个问题，我们提出了一种新颖的鲁棒多视图学习方法（即RML），该方法同时进行表示融合和对齐。具体来说，我们引入了一个简单而有效的多视图Transformer融合网络，将异构多视图数据转换为同质词嵌入，然后通过样本级注意力机制整合多个视图以获得融合表示。此外，我们提出了一种基于模拟扰动的多视图对比学习框架，该框架动态生成噪声和不可用扰动，以模拟不完美数据条件。模拟的噪声和不可用数据获得两个不同的融合表示，我们利用对比学习对它们进行对齐，以学习判别性和鲁棒的表示。我们的RML是自监督的，也可以作为正则化应用于下游任务。在实验中，我们将其应用于多视图无监督聚类、噪声标签分类，并作为跨模态哈希检索的即插即用模块。广泛的比较实验和消融研究验证了RML的有效性。代码可在https://github.com/SubmissionsIn/RML获取。", "summary": "本文提出了一种名为RML的鲁棒多视图学习方法，旨在解决现实世界中多视图数据集的异构性和不完美性问题。RML通过一个多视图Transformer融合网络，利用样本级注意力机制将异构数据融合为统一表示。同时，它引入了一个基于模拟扰动的对比学习框架，生成噪声数据并对齐原始与扰动数据的融合表示，从而学习到判别性强且鲁棒的表示。RML是自监督的，可作为正则化应用于下游任务，并在多视图聚类、噪声标签分类和跨模态哈希检索等实验中表现出显著效果。", "keywords": "多视图学习, 鲁棒性, 表示融合, 对比学习, 样本级注意力", "comments": "该论文的创新点在于结合了Transformer融合网络和模拟扰动下的对比学习，以应对多视图数据的异构性和不完美性。样本级注意力机制有助于有效融合不同视图的信息，而模拟扰动则增强了模型在真实世界复杂数据条件下的鲁棒性。其自监督特性和作为即插即用模块的潜力，使其在多种下游任务中具有广泛的应用价值。"}}
{"id": "2507.17787", "title": "Hyperbolic Deep Learning for Foundation Models: A Survey", "authors": ["Neil He", "Hiren Madhu", "Ngoc Bui", "Menglin Yang", "Rex Ying"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 Pages, SIGKDD 2025", "url": "http://arxiv.org/abs/2507.17787v1", "summary": "Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.", "comment": "11 Pages, SIGKDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.17787v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "双曲深度学习在基础模型中的应用：一项综述", "tldr": "本综述探讨了双曲几何如何通过提供更好的数据结构对齐和提高效率来解决现有基础模型（如LLMs和VLMs）的局限性。", "motivation": "基础模型（如LLMs、VLMs）在下游任务中取得了显著成功，但存在表示能力有限、适应性差和可扩展性下降等问题。这引发了一个关键问题：欧几里得几何是否是所有基础模型的最佳归纳偏置，或者引入替代几何空间能否更好地与现实世界数据的内在结构对齐并改进推理过程。", "method": "本论文对双曲神经网络及其在基础模型中的最新发展进行了全面的综述。论文探讨了双曲空间如何通过其指数体积增长特性，实现对层级结构和幂律分布的低失真嵌入，从而增强基础模型的性能。", "result": "双曲空间的应用已被证明可以增强基础模型，包括提高LLM的复杂推理能力、VLM的零样本泛化能力以及跨模态语义对齐，同时保持参数效率。", "conclusion": "本综述全面回顾了双曲神经网络及其在基础模型中的最新发展，并提出了推进该领域的关键挑战和研究方向。", "translation": "基础模型，包括大型语言模型（LLMs）、视觉-语言模型（VLMs）和大型多模态模型，在海量数据集上进行预训练，并在各种下游任务中取得了显著成功。然而，最近的研究表明这些模型存在根本性局限性：(1) 表示能力有限，(2) 适应性较低，以及 (3) 可扩展性不断减弱。这些缺点提出了一个关键问题：欧几里得几何是否真的是所有基础模型的最佳归纳偏置，或者引入替代几何空间能否使模型更好地与现实世界数据的内在结构对齐并改进推理过程？双曲空间是一类非欧几里得流形，其特征是体积随距离呈指数增长，提供了一种具有数学基础的解决方案。这些空间能够以比欧几里得对应物少得多的维度实现层级结构（例如，树、分类学）和幂律分布的低失真嵌入。最近的进展已经利用这些特性来增强基础模型，包括改进LLM的复杂推理能力、VLM的零样本泛化能力以及跨模态语义对齐，同时保持参数效率。本文全面回顾了双曲神经网络及其在基础模型中的最新发展。我们进一步概述了推动该领域的关键挑战和研究方向。", "summary": "本综述探讨了双曲深度学习在基础模型中的应用。鉴于现有基础模型在表示能力、适应性和可扩展性方面的局限性，论文提出双曲空间作为一种替代几何，能够更好地捕捉真实世界数据的内在层级结构和幂律分布。通过回顾双曲神经网络的最新进展，论文展示了双曲几何如何增强LLM的推理能力、VLM的泛化能力以及跨模态对齐，同时提高参数效率。最后，论文指出了该领域的关键挑战和未来研究方向。", "keywords": "双曲深度学习, 基础模型, 双曲几何, 神经网络, 综述", "comments": "本综述深入探讨了双曲几何在解决当前基础模型核心局限性方面的潜力，其创新性在于提出了一种非欧几何的视角来优化模型对复杂数据结构的表示。这对于推动下一代基础模型的发展具有重要意义。"}}
{"id": "2507.18413", "title": "GPU Accelerated Compact-Table Propagation", "authors": ["Enrico Santi", "Fabio Tardivo", "Agostino Dovier", "Andrea Formisano"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Under consideration in Theory and Practice of Logic Programming (TPLP)", "url": "http://arxiv.org/abs/2507.18413v1", "summary": "Constraint Programming developed within Logic Programming in the Eighties;\nnowadays all Prolog systems encompass modules capable of handling constraint\nprogramming on finite domains demanding their solution to a constraint solver.\nThis work focuses on a specific form of constraint, the so-called table\nconstraint, used to specify conditions on the values of variables as an\nenumeration of alternative options. Since every condition on a set of finite\ndomain variables can be ultimately expressed as a finite set of cases, Table\ncan, in principle, simulate any other constraint. These characteristics make\nTable one of the most studied constraints ever, leading to a series of\nincreasingly efficient propagation algorithms. Despite this, it is not uncommon\nto encounter real-world problems with hundreds or thousands of valid cases that\nare simply too many to be handled effectively with standard CPU-based\napproaches. In this paper, we deal with the Compact-Table (CT) algorithm, the\nstate-of-the-art propagation algorithms for Table. We describe how CT can be\nenhanced by exploiting the massive computational power offered by modern GPUs\nto handle large Table constraints. In particular, we report on the design and\nimplementation of GPU-accelerated CT, on its integration into an existing\nconstraint solver, and on an experimental validation performed on a significant\nset of instances.", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "pdf_url": "http://arxiv.org/pdf/2507.18413v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GPU加速的紧凑表传播", "tldr": "本文通过利用GPU的强大计算能力，加速了用于处理大型表约束的Compact-Table算法。", "motivation": "现有的基于CPU的方法难以有效处理包含数百或数千个有效案例的现实世界表约束问题。", "method": "本文描述了如何通过利用现代GPU的强大计算能力来增强Compact-Table (CT)算法，以处理大型表约束。具体包括GPU加速CT的设计、实现及其集成到现有约束求解器中。", "result": "对一组重要的实例进行了实验验证。", "conclusion": "Not mentioned in abstract", "translation": "约束编程在八十年代从逻辑编程中发展而来；如今，所有Prolog系统都包含能够处理有限域上约束编程的模块，这些模块要求其解决方案由约束求解器提供。本工作侧重于一种特定形式的约束，即所谓的表约束，用于将变量值的条件指定为替代选项的枚举。由于有限域变量集上的每个条件最终都可以表示为有限的案例集，因此原则上表可以模拟任何其他约束。这些特性使得表成为有史以来研究最多的约束之一，从而产生了一系列日益高效的传播算法。尽管如此，遇到包含数百或数千个有效案例的现实问题并不少见，这些问题对于标准基于CPU的方法来说，处理起来过于繁多而无法有效解决。在本文中，我们处理Compact-Table (CT)算法，它是表约束最先进的传播算法。我们描述了如何通过利用现代GPU提供的强大计算能力来增强CT以处理大型表约束。特别是，我们报告了GPU加速CT的设计和实现，它集成到现有约束求解器中，以及对一组重要实例进行的实验验证。", "summary": "本文针对标准CPU方法在处理大规模表约束时的效率问题，提出并实现了GPU加速的Compact-Table (CT)算法。该研究详细描述了如何利用现代GPU的并行计算能力来增强CT算法，以有效处理包含大量有效案例的表约束。此外，工作还报告了GPU加速CT的设计、实现及其与现有约束求解器的集成，并通过对大量实例的实验验证了其有效性。", "keywords": "约束编程, 表约束, Compact-Table, GPU加速, 并行计算", "comments": "本文创新性地将GPU的并行计算能力应用于约束编程中的表约束传播，解决了传统CPU方法在处理大规模问题时的效率瓶颈。这对于需要处理大量案例的现实世界约束问题具有重要意义，展示了硬件加速在优化复杂算法方面的潜力。"}}
{"id": "2507.18111", "title": "Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN", "authors": ["Peyman Tehrani", "Anas Alsoliman"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18111v1", "summary": "In this paper, we tackle the challenge of radio access network (RAN) slicing\nwithin an open RAN (O-RAN) architecture. Our focus centers on a network that\nincludes multiple mobile virtual network operators (MVNOs) competing for\nphysical resource blocks (PRBs) with the goal of meeting probabilistic delay\nupper bound constraints for their clients while minimizing PRB utilization.\nInitially, we derive a reward function based on the law of large numbers (LLN),\nthen implement practical modifications to adapt it for real-world experimental\nscenarios. We then propose our solution, the Percentile-based Delay-Aware Deep\nReinforcement Learning (PDA-DRL), which demonstrates its superiority over\nseveral baselines, including DRL models optimized for average delay\nconstraints, by achieving a 38\\% reduction in resultant average delay.\nFurthermore, we delve into the issue of model weight sharing among multiple\nMVNOs to develop a robust personalized model. We introduce a reward-based\npersonalization method where each agent prioritizes other agents' model weights\nbased on their performance. This technique surpasses traditional aggregation\nmethods, such as federated averaging, and strategies reliant on traffic\npatterns and model weight distance similarities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18111v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于百分位数的深度强化学习和基于奖励的个性化用于O-RAN中的时延感知RAN切片", "tldr": "本文提出了一个基于百分位数的深度强化学习（PDA-DRL）和基于奖励的个性化方法，以解决O-RAN中RAN切片的时延感知问题，并有效降低平均时延。", "motivation": "解决开放无线接入网络（O-RAN）架构中RAN切片面临的挑战，特别是多个移动虚拟网络运营商（MVNO）竞争物理资源块（PRB），目标是满足客户的概率时延上限约束，同时最小化PRB利用率。", "method": "首先，基于大数定律（LLN）推导奖励函数并进行实际修改。然后，提出并实现了百分位数时延感知深度强化学习（PDA-DRL）解决方案。此外，为解决多MVNO模型权重共享问题，引入了一种基于奖励的个性化方法，其中每个代理根据其他代理的性能来优先考虑其模型权重。", "result": "所提出的PDA-DRL方法相比包括针对平均时延约束优化的DRL模型在内的多个基线，实现了38%的平均时延降低。同时，引入的基于奖励的个性化方法超越了传统的聚合方法（如联邦平均）以及依赖流量模式和模型权重距离相似性的策略。", "conclusion": "论文提出了一个有效的基于百分位数和奖励的深度强化学习方法，显著提升了O-RAN中RAN切片的时延性能，并通过个性化解决了多MVNO环境下的模型共享问题。", "translation": "在本文中，我们解决了开放无线接入网络（O-RAN）架构中无线接入网络（RAN）切片的挑战。我们的重点是一个包含多个移动虚拟网络运营商（MVNO）的网络，这些运营商竞争物理资源块（PRB），目标是满足其客户的概率时延上限约束，同时最小化PRB利用率。最初，我们基于大数定律（LLN）推导了一个奖励函数，然后进行了实际修改以适应真实世界的实验场景。接着，我们提出了我们的解决方案，即基于百分位数的时延感知深度强化学习（PDA-DRL），它通过将最终平均时延降低38%来证明其优于包括针对平均时延约束优化的DRL模型在内的多个基线。此外，我们深入研究了多个MVNO之间模型权重共享的问题，以开发一个稳健的个性化模型。我们引入了一种基于奖励的个性化方法，其中每个代理根据其他代理的性能来优先考虑其模型权重。这项技术超越了传统的聚合方法，例如联邦平均，以及依赖流量模式和模型权重距离相似性的策略。", "summary": "本文针对O-RAN架构中的RAN切片问题，提出了一种名为百分位数时延感知深度强化学习（PDA-DRL）的新方法。该方法首先基于大数定律设计奖励函数，旨在帮助多个MVNO在满足概率时延约束的同时最小化资源利用。实验结果表明，PDA-DRL相比现有基线能将平均时延降低38%。此外，为解决多MVNO环境下的模型共享问题，论文还引入了一种基于奖励的个性化方法，该方法通过代理间基于性能的权重优先级分配，优于传统聚合策略。", "keywords": "RAN切片, O-RAN, 深度强化学习, 时延感知, 个性化", "comments": "本文的创新点在于提出了基于百分位数的深度强化学习方法来直接优化概率时延约束，而非仅仅平均时延，这在满足QoS方面更具实际意义。同时，引入的基于奖励的个性化模型权重共享策略，有效解决了多租户场景下的模型适应性问题，提升了系统的鲁棒性。"}}
{"id": "2507.18294", "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer", "authors": ["Pritika Ramu", "Apoorv Saxena", "Meghanath M Y", "Varsha Sankar", "Debraj Basu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18294v1", "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18294v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "StyleAdaptedLM：通过高效风格迁移增强指令遵循模型", "tldr": "StyleAdaptedLM使用LoRA高效地将风格特性迁移到指令遵循模型，无需配对数据且不牺牲任务性能。", "motivation": "将大型语言模型（LLMs）适应特定的风格特征，如品牌声音或作者语调，对于企业通信至关重要，但很难从缺乏指令-响应格式的语料库中实现，同时又不损害指令依从性。", "method": "引入StyleAdaptedLM框架，该框架使用低秩适应（LoRA）高效地将风格特性迁移到指令遵循模型。LoRA适配器首先在基础模型上使用多样化的非结构化风格语料库进行训练，然后与单独的指令遵循模型合并。", "result": "跨多个数据集和模型的实验表明，在保持指令依从性的同时，风格一致性得到改善，并且人工评估证实了品牌特定约定的采纳。", "conclusion": "StyleAdaptedLM为LLMs中的风格个性化提供了一条高效的路径。", "translation": "将大型语言模型（LLMs）适应特定的风格特征，如品牌声音或作者语调，对于企业通信至关重要，但很难从缺乏指令-响应格式的语料库中实现，同时又不损害指令依从性。我们引入了StyleAdaptedLM，一个使用低秩适应（LoRA）高效地将风格特性迁移到指令遵循模型的框架。LoRA适配器首先在基础模型上使用多样化的非结构化风格语料库进行训练，然后与单独的指令遵循模型合并。这使得无需配对数据或牺牲任务性能即可实现强大的风格定制。跨多个数据集和模型的实验表明，在保持指令依从性的同时，风格一致性得到改善，并且人工评估证实了品牌特定约定的采纳。StyleAdaptedLM为LLMs中的风格个性化提供了一条高效的路径。", "summary": "StyleAdaptedLM是一个利用LoRA技术，能够高效地将特定风格（如品牌声音）迁移到指令遵循模型中的框架。该方法通过在非结构化风格语料库上训练LoRA适配器，并将其与指令遵循模型合并，解决了在缺乏配对数据的情况下难以进行风格适应的问题，同时保持了模型的指令依从性。实验证明其在风格一致性和任务性能上均表现良好，为LLMs的风格个性化提供了有效途径。", "keywords": "风格迁移, 指令遵循模型, LoRA, LLMs, 风格个性化", "comments": "该论文提出了一种创新的方法StyleAdaptedLM，通过LoRA技术实现了LLMs的风格迁移，其亮点在于无需配对数据即可进行风格适应，同时不损害指令遵循能力。这对于企业通信等需要定制化品牌声音的场景具有重要意义，提供了一种高效且实用的解决方案。"}}
{"id": "2507.18549", "title": "The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection", "authors": ["Steven A. Frank"], "categories": ["cs.LG", "q-bio.PE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18549v1", "summary": "Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, through the\ncovariance between the parameters and performance. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18549v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "普莱斯方程揭示了算法学习和自然选择的普适力-度量-偏差定律", "tldr": "普莱斯方程揭示了一个普适的力-度量-偏差（FMB）定律，该定律统一了包括自然选择和多种机器学习算法在内的各种学习和优化过程。", "motivation": "尽管各种学习算法、优化方法和自然选择表面上存在差异，但它们共享一个共同的数学结构，作者旨在揭示并利用这一结构。", "method": "作者通过对普莱斯方程中变化的简单符号划分，揭示了一个普适的力-度量-偏差（FMB）定律：$\\Delta\\mathbf{\\theta} = \\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$，并解释了其中力、度量、偏差和噪声的物理意义。", "result": "揭示了普适的力-度量-偏差（FMB）定律，该定律将自然选择、贝叶斯更新、牛顿法、随机梯度下降、Adam优化等多种算法统一为同一底层过程的特例。此外，该框架还解释了费雪信息、Kullback-Leibler散度和达朗贝尔原理为何在学习动力学中自然出现。", "conclusion": "FMB定律通过揭示共同结构，为跨学科理解、比较和设计学习算法提供了一个有原则的基础。", "translation": "尽管表面上存在差异，但多样化的学习算法、优化方法和自然选择共享一个共同的数学结构。在此，我展示了普莱斯方程对变化进行简单的符号划分，揭示了一个普适的力-度量-偏差（FMB）定律：$\\Delta\\mathbf{\\theta} = \\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$。力 $\\mathbf{f}$ 通过参数和性能之间的协方差驱动参数 $\\Delta\\mathbf{\\theta}$ 的改进。度量 $\\mathbf{M}$ 通过逆曲率重新调整运动。偏差 $\\mathbf{b}$ 增加动量或改变参考系。噪声 $\\mathbf{\\xi}$ 实现了探索。这个框架将自然选择、贝叶斯更新、牛顿法、随机梯度下降、随机朗之万动力学、Adam优化以及大多数其他算法统一为同一底层过程的特例。普莱斯方程还揭示了为什么费雪信息、Kullback-Leibler散度和达朗贝尔原理在学习动力学中自然产生。通过揭示这种共同结构，FMB定律为跨学科理解、比较和设计学习算法提供了一个有原则的基础。", "summary": "这篇论文利用普莱斯方程对变化的符号划分，揭示了一个普适的力-度量-偏差（FMB）定律。该定律成功地将包括自然选择、多种机器学习优化算法（如随机梯度下降、Adam优化）和贝叶斯更新等看似不同的过程统一在一个共同的数学框架下。FMB定律阐明了力、度量、偏差和噪声在这些动力学中的作用，并解释了费雪信息等概念的自然出现，为跨领域学习算法的理解、比较和设计提供了统一的理论基础。", "keywords": "普莱斯方程, 算法学习, 自然选择, 力-度量-偏差定律, 统一框架", "comments": "这篇论文的创新之处在于利用普莱斯方程这一经典的生物学工具，揭示了看似不相关的自然选择和多种算法学习过程背后的普适数学结构。FMB定律的提出，不仅为理解现有算法提供了新的视角，也可能为未来跨学科的算法设计提供理论指导。其重要性在于提供了一个统一的框架，有助于弥合不同领域在学习和优化理论上的鸿沟。"}}
{"id": "2503.16870", "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs", "authors": ["Anshumann", "Mohd Abbas Zaidi", "Akhil Kedia", "Jinwoo Ahn", "Taehwak Kwon", "Kangwook Lee", "Haejun Lee", "Joohyung Lee"], "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted as Oral paper at ACL 2025. Source code is available at this https URL . Anshumann, Mohd Abbas Zaidi and Akhil Kedia have Equal Contribution", "url": "http://arxiv.org/abs/2503.16870v2", "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.", "comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution", "pdf_url": "http://arxiv.org/pdf/2503.16870v2", "cate": "cs.LG", "date": "2025-03-21", "updated": "2025-07-24", "AI": {"title_translation": "稀疏Logit采样：加速大型语言模型中的知识蒸馏", "tldr": "本文提出了一种名为“随机采样知识蒸馏”的重要性采样方法，用于在大语言模型预训练中进行稀疏知识蒸馏，解决了传统稀疏方法估计偏差的问题，实现了更快的训练速度和有竞争力的性能。", "motivation": "知识蒸馏在大型语言模型中是一种经济高效的知识提取技术，但将其成功应用于预训练领域仍未被充分探索。此外，研究发现，像缓存Top-K概率这样的朴素稀疏知识蒸馏方法会为学生模型提供有偏的教师概率分布估计，导致性能不佳和校准问题。", "method": "本文提出了一种基于重要性采样的“随机采样知识蒸馏”（Random Sampling Knowledge Distillation）方法。该方法提供无偏估计，在期望上保留梯度，并且只需要存储显著更稀疏的Logits。", "result": "与基于交叉熵的训练相比，本文方法能够以边际开销（<10%）更快地训练学生模型，同时在300M到3B的模型尺寸范围内，与完全蒸馏相比，保持了有竞争力的性能。", "conclusion": "本文提出的随机采样知识蒸馏方法有效解决了大型语言模型预训练中稀疏知识蒸馏的挑战，通过提供无偏估计和保留梯度，实现了更快的训练速度和与完全蒸馏相当的性能，且存储开销更小。", "translation": "知识蒸馏是一种经济高效的技术，可以将知识蒸馏到大型语言模型中，前提是教师输出的Logits可以预先计算和缓存。然而，将其成功应用于预训练领域仍未被充分探索。在这项工作中，我们证明了朴素的稀疏知识蒸馏方法（例如缓存Top-K概率）虽然直观，但会为学生模型提供教师概率分布的有偏估计，导致次优的性能和校准。我们提出了一种基于重要性采样的方法，名为“随机采样知识蒸馏”，它提供无偏估计，在期望上保留梯度，并且只需要存储显著更稀疏的Logits。我们的方法使得学生模型的训练速度更快，与基于交叉熵的训练相比，开销微乎其微（<10%），同时在300M到3B的模型尺寸范围内，与完全蒸馏相比，保持了有竞争力的性能。", "summary": "本文针对大型语言模型预训练中稀疏知识蒸馏的挑战，提出了一种名为“随机采样知识蒸馏”的重要性采样方法。该方法旨在解决传统稀疏蒸馏方法（如Top-K缓存）导致的教师概率分布有偏估计问题。实验证明，该方法能提供无偏估计，在期望上保留梯度，并显著减少Logit存储需求。结果显示，与交叉熵训练相比，该方法能以不到10%的额外开销加速学生模型训练，并在300M至3B的模型规模上，性能与完全蒸馏相当。", "keywords": "知识蒸馏, 大型语言模型, 稀疏采样, 重要性采样, 预训练", "comments": "这项工作在大型语言模型知识蒸馏领域具有重要意义，特别是在预训练阶段。其创新点在于通过引入重要性采样来解决稀疏Logit蒸馏中的估计偏差问题，这为在资源受限环境下进行大规模模型训练提供了新的高效途径。该方法在保证性能的同时显著降低了计算和存储成本，对于推动LLM的普及和应用具有积极作用。"}}
{"id": "2507.17753", "title": "Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving", "authors": ["Liang Zhang", "Xiaoming Zhai", "Jionghao Lin", "Jionghao Lin", "Jennifer Kleiman", "Diego Zapata-Rivera", "Carol Forsyth", "Yang Jiang", "Xiangen Hu", "Arthur C. Graesser"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17753v1", "summary": "Large Language Model (LLM) agents are increasingly utilized in AI-aided\neducation to support tutoring and learning. Effective communication strategies\namong LLM agents improve collaborative problem-solving efficiency and\nfacilitate cost-effective adoption in education. However, little research has\nsystematically evaluated the impact of different communication strategies on\nagents' problem-solving. Our study examines four communication modes,\n\\textit{teacher-student interaction}, \\textit{peer-to-peer collaboration},\n\\textit{reciprocal peer teaching}, and \\textit{critical debate}, in a\ndual-agent, chat-based mathematical problem-solving environment using the\nOpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that\ndual-agent setups outperform single agents, with \\textit{peer-to-peer\ncollaboration} achieving the highest accuracy. Dialogue acts like statements,\nacknowledgment, and hints play a key role in collaborative problem-solving.\nWhile multi-agent frameworks enhance computational tasks, effective\ncommunication strategies are essential for tackling complex problems in AI\neducation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17753v1", "cate": "cs.HC", "date": "2025-05-02", "updated": "2025-05-02", "AI": {"title_translation": "探索数学问题解决中协作型LLM智能体的通信策略", "tldr": "研究了LLM智能体在数学问题解决中的四种通信策略，发现双智能体设置优于单智能体，其中“同伴协作”模式准确率最高，强调有效沟通在AI教育复杂问题解决中的重要性。", "motivation": "现有研究很少系统地评估不同通信策略对LLM智能体问题解决能力的影响，尽管有效的通信策略能提高协作解决问题的效率并促进AI教育中的成本效益采用。", "method": "研究在双智能体、基于聊天的数学问题解决环境中使用OpenAI GPT-4o模型，评估了四种通信模式：师生互动、同伴协作、互惠式同伴教学和批判性辩论。评估基于MATH数据集。", "result": "1. 双智能体设置在数学问题解决中优于单智能体。\n2. 在所测试的通信策略中，同伴协作模式实现了最高的准确率。\n3. 陈述、确认和提示等对话行为在协作问题解决中扮演关键角色。", "conclusion": "尽管多智能体框架能增强计算任务，但有效的通信策略对于解决AI教育中的复杂问题至关重要。", "translation": "大型语言模型（LLM）智能体在人工智能辅助教育中越来越多地用于支持辅导和学习。LLM智能体之间有效的通信策略可以提高协作解决问题的效率，并促进在教育中经济高效的采用。然而，很少有研究系统地评估不同通信策略对智能体问题解决能力的影响。我们的研究在双智能体、基于聊天的数学问题解决环境中使用OpenAI GPT-4o模型，考察了四种通信模式：师生互动、同伴协作、互惠式同伴教学和批判性辩论。在MATH数据集上进行评估，我们的结果表明，双智能体设置优于单智能体，其中同伴协作实现了最高的准确率。陈述、确认和提示等对话行为在协作解决问题中发挥关键作用。虽然多智能体框架增强了计算任务，但有效的通信策略对于解决人工智能教育中的复杂问题至关重要。", "summary": "本研究探讨了大型语言模型（LLM）智能体在数学问题解决中的协作通信策略。通过在双智能体、基于聊天的环境中利用OpenAI GPT-4o模型，并评估师生互动、同伴协作、互惠式同伴教学和批判性辩论四种模式，研究发现双智能体设置优于单智能体，其中同伴协作模式在MATH数据集上表现出最高准确率。研究强调了对话行为（如陈述、确认和提示）在协作中的重要性，并指出有效的通信策略对于解决AI教育中的复杂问题至关重要。", "keywords": "LLM智能体, 通信策略, 数学问题解决, 同伴协作, AI教育", "comments": "这项研究通过系统地比较不同的LLM智能体通信策略，填补了现有研究的空白。其创新之处在于明确指出了“同伴协作”模式的优越性，并强调了特定对话行为的重要性。这对于未来设计更高效、更智能的AI教育系统具有重要指导意义。"}}
{"id": "2507.17887", "title": "Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments", "authors": ["Wonjae Lee", "Taeyoung Kim", "Hyungbin Park"], "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17887v1", "summary": "This paper introduces an operator-based neural network, the mirror-padded\nFourier neural operator (MFNO), designed to learn the dynamics of stochastic\nsystems. MFNO extends the standard Fourier neural operator (FNO) by\nincorporating mirror padding, enabling it to handle non-periodic inputs. We\nrigorously prove that MFNOs can approximate solutions of path-dependent\nstochastic differential equations and Lipschitz transformations of fractional\nBrownian motions to an arbitrary degree of accuracy. Our theoretical analysis\nbuilds on Wong--Zakai type theorems and various approximation techniques.\nEmpirically, the MFNO exhibits strong resolution generalization--a property\nrarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.\nFurthermore, our model achieves performance that is comparable or superior to\nthese baselines while offering significantly faster sample path generation than\nclassical numerical schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17887v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "傅里叶神经算子用于非马尔可夫过程：逼近定理与实验", "tldr": "引入了一种名为MFNO的傅里叶神经算子扩展，用于学习随机系统动力学，能处理非周期输入，具有强大的逼近能力和分辨率泛化能力，且生成样本路径速度快。", "motivation": "旨在开发一种能够有效学习随机系统动力学，特别是处理非周期输入和非马尔可夫过程的神经网络模型。", "method": "提出了镜像填充傅里叶神经算子（MFNO），它是标准傅里叶神经算子（FNO）的扩展，通过引入镜像填充来处理非周期输入。理论分析基于Wong--Zakai型定理和各种逼近技术。", "result": "理论上，MFNO能够以任意精度逼近路径依赖随机微分方程的解和分数布朗运动的Lipschitz变换。经验上，MFNO展现出强大的分辨率泛化能力，这在传统架构中不常见。此外，MFNO的性能与LSTMs、TCNs和DeepONet等基线模型相当或更优，并且样本路径生成速度显著快于经典数值方案。", "conclusion": "MFNO是一种有效且高效的用于学习非马尔可夫过程随机系统动力学的神经网络模型，它在理论上具有强大的逼近能力，在实践中展现出卓越的泛化性能和计算效率。", "translation": "本文介绍了一种基于算子的神经网络，即镜像填充傅里叶神经算子（MFNO），旨在学习随机系统的动力学。MFNO通过引入镜像填充扩展了标准傅里叶神经算子（FNO），使其能够处理非周期输入。我们严格证明了MFNO可以以任意精度逼近路径依赖随机微分方程的解和分数布朗运动的Lipschitz变换。我们的理论分析建立在Wong--Zakai型定理和各种逼近技术之上。在经验上，MFNO表现出强大的分辨率泛化能力——这是LSTMs、TCNs和DeepONet等标准架构中罕见的特性。此外，我们的模型在性能上与这些基线模型相当或更优，同时提供比经典数值方案显著更快的样本路径生成速度。", "summary": "本文提出了一种新型的基于算子的神经网络——镜像填充傅里叶神经算子（MFNO），专门用于学习随机系统，特别是非马尔可夫过程的动力学。MFNO通过镜像填充技术扩展了传统的傅里叶神经算子，使其能够处理非周期输入。理论上，MFNO被证明能够精确逼近路径依赖随机微分方程和分数布朗运动的解。实验结果表明，MFNO不仅展现出优异的分辨率泛化能力，其性能与现有主流模型相当或更优，并且在样本路径生成速度上远超传统数值方法。", "keywords": "傅里叶神经算子, 非马尔可夫过程, 随机系统, 逼近定理, 分辨率泛化", "comments": "这篇论文的创新点在于提出了MFNO，通过镜像填充有效解决了傅里叶神经算子在处理非周期输入时的局限性。其理论证明了对路径依赖随机过程的强大逼近能力，扩展了FNO的应用范围。更重要的是，经验结果显示出MFNO罕见的分辨率泛化能力和显著的计算效率提升，这对于需要高分辨率和快速模拟的随机系统学习领域具有重要意义。"}}
{"id": "2411.02650", "title": "A Scoping Review of Functional Near-Infrared Spectroscopy (fNIRS) Applications in Game-Based Learning Environments", "authors": ["Shayla Sharmin", "Gael Lucero-Palacios", "Behdokht Kiafar", "Mohammad Fahim Abrar", "Mohammad Al-Ratrout", "Aditya Raikwar", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      28 pages, 3 figures", "url": "http://arxiv.org/abs/2411.02650v2", "summary": "Functional Near-Infrared Spectroscopy (fNIRS) has emerged as a valuable tool\nto investigate cognitive and emotional processes during learning. We focus\nspecifically on game-integrated learning systems as the context for fNIRS-based\nbrain data analysis. We selected game-integrated learning systems because such\nsystems make learning more engaging, interactive, and immersive, all of which\nare critical features for adaptive learning design. The goal of this scoping\nreview is to help researchers understand how fNIRS has been used so far to\nstudy brain activity in game-integrated learning systems. We also aim to show\nhow brain data captured through fNIRS can support the development of adaptive\nlearning systems by monitoring learners' cognitive states. Using the PRISMA-ScR\nframework, 1300 papers were screened, and 21 empirical studies were selected\nfor in-depth analysis. Studies were categorized as affective/cognitive response\nstudies or comparative studies, and further analyzed by learning platform, game\ndevice, fNIRS configuration, outcome measures, and study design. The findings\nreveal that game-integrated learning systems can be as effective as traditional\nmethods in improving engagement and involvement. The findings also show that\nfNIRS offers valuable insights into cognitive states, but it has not yet been\nwidely implemented in real-time adaptive systems. We identify key challenges in\nstandardization and data interpretation and highlight the potential of fNIRS\nfor developing brain-aware, interactive learning environments. This review\noffers insights to guide future research on using brain data to support\nadaptive learning and intelligent system design.", "comment": "28 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2411.02650v2", "cate": "cs.HC", "date": "2024-11-04", "updated": "2025-07-24", "AI": {"title_translation": "功能性近红外光谱（fNIRS）在游戏化学习环境中的应用范围综述", "tldr": "本综述探讨了功能性近红外光谱（fNIRS）在游戏化学习系统中如何用于研究大脑活动，并支持自适应学习系统的发展。研究发现fNIRS能提供认知状态的宝贵见解，但在实时自适应系统中的应用尚不广泛，并指出了标准化和数据解释的挑战。", "motivation": "功能性近红外光谱（fNIRS）已成为研究学习过程中认知和情感过程的重要工具。本综述旨在帮助研究人员理解fNIRS迄今为止在游戏化学习系统中如何被用于研究大脑活动，并展示fNIRS捕获的大脑数据如何通过监测学习者的认知状态来支持自适应学习系统的发展。选择游戏化学习系统是因为它们能使学习更具吸引力、互动性和沉浸感，这些都是自适应学习设计的关键特征。", "method": "本综述采用了PRISMA-ScR框架，筛选了1300篇论文，并选择了21项实证研究进行深入分析。研究被分为情感/认知反应研究或比较研究，并根据学习平台、游戏设备、fNIRS配置、结果测量和研究设计进行了进一步分析。", "result": "研究结果表明，游戏化学习系统在提高参与度和投入度方面可以与传统方法一样有效。研究还显示，fNIRS能提供关于认知状态的宝贵见解，但尚未在实时自适应系统中广泛实施。研究指出了标准化和数据解释方面的关键挑战。", "conclusion": "本综述为未来利用大脑数据支持自适应学习和智能系统设计的研究提供了指导。尽管fNIRS在实时自适应系统中的应用尚不广泛，但它在开发脑感知、互动学习环境方面具有巨大潜力。", "translation": "功能性近红外光谱（fNIRS）已成为研究学习过程中认知和情感过程的重要工具。我们特别关注游戏集成学习系统作为基于fNIRS的大脑数据分析的背景。我们选择游戏集成学习系统是因为此类系统能使学习更具吸引力、互动性和沉浸感，这些都是自适应学习设计的关键特征。本范围综述的目标是帮助研究人员了解fNIRS迄今为止如何被用于研究游戏集成学习系统中的大脑活动。我们还旨在展示通过fNIRS捕获的大脑数据如何通过监测学习者的认知状态来支持自适应学习系统的发展。使用PRISMA-ScR框架，筛选了1300篇论文，并选择了21项实证研究进行深入分析。研究被分为情感/认知反应研究或比较研究，并根据学习平台、游戏设备、fNIRS配置、结果测量和研究设计进行了进一步分析。研究结果表明，游戏集成学习系统在提高参与度和投入度方面可以与传统方法一样有效。研究结果还表明，fNIRS为认知状态提供了宝贵的见解，但尚未在实时自适应系统中广泛实施。我们指出了标准化和数据解释方面的关键挑战，并强调了fNIRS在开发脑感知、互动学习环境方面的潜力。本综述为未来利用大脑数据支持自适应学习和智能系统设计的研究提供了见解。", "summary": "本范围综述使用PRISMA-ScR框架，分析了21项实证研究，探讨了功能性近红外光谱（fNIRS）在游戏化学习环境中的应用。研究旨在理解fNIRS如何用于监测学习者的大脑活动（认知和情感状态），以及其数据如何支持自适应学习系统的开发。结果显示，游戏化学习系统与传统方法同样有效，fNIRS能提供认知洞察，但其在实时自适应系统中的广泛应用仍面临标准化和数据解释的挑战。综述强调了fNIRS在创建脑感知互动学习环境中的潜力，并为未来研究提供了指导。", "keywords": "fNIRS, 游戏化学习, 自适应学习, 认知状态, 范围综述", "comments": "这篇综述突出了fNIRS作为一种非侵入性脑成像技术在理解和优化游戏化学习体验中的潜力。其重要性在于系统地梳理了现有研究，并明确指出了fNIRS在实时自适应学习系统应用中的局限性（如标准化和数据解释挑战）。这为未来的研究指明了方向，即如何更好地利用fNIRS数据来设计更智能、更具响应性的学习环境。"}}
{"id": "2507.18331", "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction", "authors": ["Runmin Zhang", "Zhu Yu", "Si-Yuan Cao", "Lingyu Zhu", "Guangyi Zhang", "Xiaokai Bai", "Hui-Liang Shen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.18331v1", "summary": "This work presents SGCDet, a novel multi-view indoor 3D object detection\nframework based on adaptive 3D volume construction. Unlike previous approaches\nthat restrict the receptive field of voxels to fixed locations on images, we\nintroduce a geometry and context aware aggregation module to integrate\ngeometric and contextual information within adaptive regions in each image and\ndynamically adjust the contributions from different views, enhancing the\nrepresentation capability of voxel features. Furthermore, we propose a sparse\nvolume construction strategy that adaptively identifies and selects voxels with\nhigh occupancy probabilities for feature refinement, minimizing redundant\ncomputation in free space. Benefiting from the above designs, our framework\nachieves effective and efficient volume construction in an adaptive way. Better\nstill, our network can be supervised using only 3D bounding boxes, eliminating\nthe dependence on ground-truth scene geometry. Experimental results demonstrate\nthat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200\nand ARKitScenes datasets. The source code is available at\nhttps://github.com/RM-Zhang/SGCDet.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.18331v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过自适应3D体素构建提升多视角室内3D目标检测", "tldr": "SGCDet是一个新颖的多视角室内3D目标检测框架，通过自适应3D体素构建，提高了特征表示能力和计算效率，并在多个数据集上达到了最先进的性能，仅需3D边界框监督。", "motivation": "以往的多视角3D目标检测方法将体素的感受野限制在图像的固定位置，这限制了体素特征的表示能力，并导致计算冗余。", "method": "本文提出了SGCDet框架，包含两个关键组件：1. 几何和上下文感知聚合模块：在图像的自适应区域内整合几何和上下文信息，并动态调整不同视角的贡献，以增强体素特征的表示能力。2. 稀疏体素构建策略：自适应地识别并选择具有高占用概率的体素进行特征细化，从而最大限度地减少自由空间中的冗余计算。此外，该网络仅需3D边界框即可进行监督，无需依赖真实场景几何。", "result": "SGCDet框架以自适应方式实现了有效且高效的体素构建。它消除了对真实场景几何的依赖。实验结果表明，SGCDet在ScanNet、ScanNet200和ARKitScenes数据集上取得了最先进的性能。", "conclusion": "SGCDet通过自适应和稀疏的3D体素构建，有效提升了多视角室内3D目标检测的性能，并在简化监督要求的同时，达到了最先进的结果。", "translation": "这项工作提出了SGCDet，一个基于自适应3D体素构建的新颖多视角室内3D目标检测框架。与以往将体素感受野限制在图像固定位置的方法不同，我们引入了一个几何和上下文感知聚合模块，用于整合图像中自适应区域内的几何和上下文信息，并动态调整不同视角的贡献，从而增强体素特征的表示能力。此外，我们提出了一种稀疏体素构建策略，自适应地识别并选择具有高占用概率的体素进行特征细化，从而最大限度地减少自由空间中的冗余计算。得益于上述设计，我们的框架以自适应方式实现了有效且高效的体素构建。更好的是，我们的网络仅使用3D边界框即可进行监督，消除了对真实场景几何的依赖。实验结果表明，SGCDet在ScanNet、ScanNet200和ARKitScenes数据集上取得了最先进的性能。源代码可在https://github.com/RM-Zhang/SGCDet获取。", "summary": "本文介绍了SGCDet，一个用于多视角室内3D目标检测的创新框架。该框架通过引入几何和上下文感知聚合模块以及稀疏体素构建策略，克服了传统方法中固定感受野的局限性，有效增强了体素特征的表示能力并减少了冗余计算。SGCDet仅需3D边界框即可进行监督，并在ScanNet、ScanNet200和ARKitScenes数据集上实现了最先进的性能。", "keywords": "多视角, 3D目标检测, 自适应体素, 室内, SGCDet", "comments": "该论文的创新点在于其自适应和稀疏的3D体素构建方法，这显著提升了特征表示能力和计算效率。同时，该方法仅依赖3D边界框进行监督，无需复杂的真实场景几何信息，这大大简化了模型部署和应用，具有重要的实践价值。"}}
{"id": "2411.08562", "title": "Neural Corrective Machine Unranking", "authors": ["Jingrui Hou", "Axel Finke", "Georgina Cosma"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      submitted to Information Sciences", "url": "http://arxiv.org/abs/2411.08562v2", "summary": "Machine unlearning in neural information retrieval (IR) systems requires\nremoving specific data whilst maintaining model performance. Applying existing\nmachine unlearning methods to IR may compromise retrieval effectiveness or\ninadvertently expose unlearning actions due to the removal of particular items\nfrom the retrieved results presented to users. We formalise corrective\nunranking, which extends machine unlearning in (neural) IR context by\nintegrating substitute documents to preserve ranking integrity, and propose a\nnovel teacher-student framework, Corrective unRanking Distillation (CuRD), for\nthis task. CuRD (1) facilitates forgetting by adjusting the (trained) neural IR\nmodel such that its output relevance scores of to-be-forgotten samples mimic\nthose of low-ranking, non-retrievable samples; (2) enables correction by\nfine-tuning the relevance scores for the substitute samples to match those of\ncorresponding to-be-forgotten samples closely; (3) seeks to preserve\nperformance on samples that are not targeted for forgetting. We evaluate CuRD\non four neural IR models (BERTcat, BERTdot, ColBERT, PARADE) using MS MARCO and\nTREC CAR datasets. Experiments with forget set sizes from 1 % and 20 % of the\ntraining dataset demonstrate that CuRD outperforms seven state-of-the-art\nbaselines in terms of forgetting and correction while maintaining model\nretention and generalisation capabilities.", "comment": "submitted to Information Sciences", "pdf_url": "http://arxiv.org/pdf/2411.08562v2", "cate": "cs.IR", "date": "2024-11-13", "updated": "2025-07-24", "AI": {"title_translation": "神经纠正机器去排序", "tldr": "在神经信息检索(IR)系统中，机器遗忘需要移除特定数据同时保持模型性能。本文提出了一种名为纠正性去排序蒸馏(CuRD)的教师-学生框架，通过整合替代文档来扩展机器遗忘，以保持排名完整性。CuRD在遗忘和纠正方面优于七种最先进的基线，同时保持了模型保留和泛化能力。", "motivation": "在神经信息检索（IR）系统中，应用现有机器遗忘方法可能会损害检索效果或因从呈现给用户的检索结果中移除特定项目而无意中暴露遗忘行为。因此，需要一种方法来移除特定数据，同时保持模型性能和排名完整性。", "method": "本文将纠正性去排序形式化，该方法通过整合替代文档来扩展（神经）IR环境中的机器遗忘，以保持排名完整性。提出了一种新颖的教师-学生框架——纠正性去排序蒸馏（CuRD）。CuRD通过调整训练过的神经IR模型来促进遗忘，使其对要遗忘样本的输出相关性分数模仿低排名、不可检索样本的分数；通过微调替代样本的相关性分数使其与相应的要遗忘样本的分数紧密匹配来实现纠正；并力求保持对未被遗忘的样本的性能。CuRD在四种神经IR模型（BERTcat、BERTdot、ColBERT、PARADE）上使用MS MARCO和TREC CAR数据集进行了评估。", "result": "在训练数据集的1%至20%的遗忘集大小的实验中，CuRD在遗忘和纠正方面优于七种最先进的基线，同时保持了模型保留和泛化能力。", "conclusion": "CuRD是一种在神经信息检索系统中实现机器遗忘的有效方法，它在确保遗忘和纠正的同时，能够保持模型的整体性能和泛化能力。", "translation": "神经纠正机器去排序\n\n神经信息检索（IR）系统中的机器遗忘要求在移除特定数据的同时保持模型性能。将现有机器遗忘方法应用于IR可能会损害检索效率，或由于从呈现给用户的检索结果中移除特定项目而无意中暴露遗忘行为。我们形式化了纠正性去排序，它通过整合替代文档来扩展（神经）IR环境中的机器遗忘，以保持排名完整性，并为此任务提出了一种新颖的教师-学生框架——纠正性去排序蒸馏（CuRD）。CuRD（1）通过调整（训练好的）神经IR模型来促进遗忘，使其对要遗忘样本的输出相关性分数模仿低排名、不可检索样本的分数；（2）通过微调替代样本的相关性分数使其与相应的要遗忘样本的分数紧密匹配来实现纠正；（3）力求保持对未被遗忘的样本的性能。我们使用MS MARCO和TREC CAR数据集，在四种神经IR模型（BERTcat、BERTdot、ColBERT、PARADE）上评估了CuRD。对训练数据集1%至20%的遗忘集大小的实验表明，CuRD在遗忘和纠正方面优于七种最先进的基线，同时保持了模型保留和泛化能力。", "summary": "本文针对神经信息检索（IR）系统中的机器遗忘问题，提出了一种名为纠正性去排序蒸馏（CuRD）的教师-学生框架。该方法旨在在移除特定数据的同时，通过整合替代文档来保持排名完整性并维护模型性能。CuRD通过使被遗忘样本的相关性分数模仿低排名样本，并微调替代样本以匹配被遗忘样本，同时保留非遗忘样本的性能。实验结果表明，CuRD在遗忘和纠正方面优于现有基线，并能有效保持模型性能和泛化能力。", "keywords": "机器遗忘, 神经信息检索, 纠正性去排序, 知识蒸馏, CuRD", "comments": "本文的创新点在于提出了“纠正性去排序”的概念，并设计了CuRD框架来解决神经IR中机器遗忘的挑战。其重要性在于，它不仅实现了数据的有效遗忘，更重要的是通过引入替代文档和精细的教师-学生蒸馏过程，解决了现有方法可能导致的性能下降和遗忘行为暴露问题，对于实际的IR系统部署具有重要意义。"}}
{"id": "2411.16319", "title": "CutS3D: Cutting Semantics in 3D for 2D Unsupervised Instance Segmentation", "authors": ["Leon Sick", "Dominik Engel", "Sebastian Hartwig", "Pedro Hermosilla", "Timo Ropinski"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025. Project Page with Code, Models & Demo: this https URL", "url": "http://arxiv.org/abs/2411.16319v3", "summary": "Traditionally, algorithms that learn to segment object instances in 2D images\nhave heavily relied on large amounts of human-annotated data. Only recently,\nnovel approaches have emerged tackling this problem in an unsupervised fashion.\nGenerally, these approaches first generate pseudo-masks and then train a\nclass-agnostic detector. While such methods deliver the current state of the\nart, they often fail to correctly separate instances overlapping in 2D image\nspace since only semantics are considered. To tackle this issue, we instead\npropose to cut the semantic masks in 3D to obtain the final 2D instances by\nutilizing a point cloud representation of the scene. Furthermore, we derive a\nSpatial Importance function, which we use to resharpen the semantics along the\n3D borders of instances. Nevertheless, these pseudo-masks are still subject to\nmask ambiguity. To address this issue, we further propose to augment the\ntraining of a class-agnostic detector with three Spatial Confidence components\naiming to isolate a clean learning signal. With these contributions, our\napproach outperforms competing methods across multiple standard benchmarks for\nunsupervised instance segmentation and object detection.", "comment": "Accepted at ICCV 2025. Project Page with Code, Models & Demo:\n  https://leonsick.github.io/cuts3d/", "pdf_url": "http://arxiv.org/pdf/2411.16319v3", "cate": "cs.CV", "date": "2024-11-25", "updated": "2025-07-24", "AI": {"title_translation": "CutS3D：在3D中切割语义用于2D无监督实例分割", "tldr": "CutS3D提出了一种新颖的无监督实例分割方法，通过利用3D点云在3D空间中切割2D语义掩码，以解决2D图像中实例重叠的问题，并显著优于现有方法。", "motivation": "传统的2D无监督实例分割算法严重依赖大量人工标注数据，而新兴的无监督方法虽然能生成伪掩码并训练类别无关的检测器，但由于仅考虑语义，在2D图像空间中常常无法正确分离重叠实例。此外，生成的伪掩码存在掩码模糊性问题。", "method": "本研究提出了CutS3D方法。该方法利用场景的点云表示，在3D中切割语义掩码以获得最终的2D实例。此外，它推导了一个空间重要性函数，用于沿实例的3D边界重新锐化语义。为了解决掩码模糊性问题，该方法进一步提出了用三个空间置信度组件来增强类别无关检测器的训练，旨在隔离出干净的学习信号。", "result": "本方法在无监督实例分割和目标检测的多个标准基准测试中，表现优于竞争方法。", "conclusion": "通过在3D中切割语义并结合空间重要性函数以及空间置信度组件来增强学习信号，CutS3D能够有效解决2D无监督实例分割中实例重叠和掩码模糊的问题，从而显著提高性能。", "translation": "传统上，在2D图像中学习分割对象实例的算法严重依赖于大量人工标注数据。直到最近，才出现了以无监督方式解决此问题的新方法。通常，这些方法首先生成伪掩码，然后训练一个类别无关的检测器。尽管此类方法达到了当前的最新水平，但由于仅考虑语义，它们通常无法正确分离2D图像空间中重叠的实例。为了解决这个问题，我们建议利用场景的点云表示，在3D中切割语义掩码以获得最终的2D实例。此外，我们推导了一个空间重要性函数，用于沿实例的3D边界重新锐化语义。然而，这些伪掩码仍然存在掩码模糊性。为了解决这个问题，我们进一步建议通过三个空间置信度组件来增强类别无关检测器的训练，旨在隔离出干净的学习信号。凭借这些贡献，我们的方法在无监督实例分割和目标检测的多个标准基准测试中，表现优于竞争方法。", "summary": "CutS3D是一种新颖的无监督实例分割方法，旨在解决2D图像中实例重叠和伪掩码模糊的问题。该方法通过利用3D点云数据在3D空间中切割2D语义掩码来获取最终实例。它引入了一个空间重要性函数来锐化3D边界上的语义，并结合了三个空间置信度组件来优化类别无关检测器的训练信号。实验结果表明，CutS3D在多个标准基准测试中超越了现有方法。", "keywords": "无监督实例分割, 3D点云, 语义切割, 目标检测, 空间置信度", "comments": "该论文的创新之处在于，巧妙地利用了3D点云信息来解决2D无监督实例分割中长期存在的实例重叠和掩码模糊问题。通过在3D空间进行语义切割，并引入空间重要性和置信度函数，该方法为2D无监督学习提供了一个新颖且有效的三维视角，克服了纯2D方法的局限性。"}}
{"id": "2507.18487", "title": "Low-power switching of memristors exhibiting fractional-order dynamics", "authors": ["Nathan Astin", "Yuriy V. Pershin"], "categories": ["cs.ET", "cond-mat.mes-hall"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18487v1", "summary": "In this conference contribution, we present some initial results on switching\nmemristive devices exhibiting fractional-order behavior using current pulses.\nIn our model, it is assumed that the evolution of a state variable follows a\nfractional-order differential equation involving a Caputo-type derivative. A\nstudy of Joule losses demonstrates that the best switching strategy minimizing\nthese losses depends on the fractional derivative's order and the power\nexponent in the equation of motion. It is found that when the order of the\nfractional derivative exceeds half of the power exponent, the best approach is\nto employ a wide pulse. Conversely, when this condition is not met, Joule\nlosses are minimized by applying a zero current followed by a narrow current\npulse of the highest allowable amplitude. These findings are explored further\nin the context of multi-pulse control. Our research lays the foundation for the\nadvancement of the next generation of energy-efficient neuromorphic computing\narchitectures that more closely mimic their biological counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18487v1", "cate": "cs.ET", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "分数阶动力学忆阻器的低功耗开关", "tldr": "研究了分数阶忆阻器的低功耗开关策略，发现最佳开关策略取决于分数阶导数阶数和功率指数。", "motivation": "为下一代更接近生物对应物的节能神经形态计算架构的发展奠定基础。", "method": "通过模型研究了表现出分数阶行为的忆阻器件的电流脉冲开关，其中状态变量的演化遵循Caputo型导数的分数阶微分方程。研究了焦耳损耗，并探索了多脉冲控制。", "result": "当分数阶导数的阶数超过功率指数的一半时，最佳方法是采用宽脉冲。反之，当不满足此条件时，通过施加零电流，然后施加最高允许幅度的窄电流脉冲，可以使焦耳损耗最小化。", "conclusion": "研究结果为开发下一代节能神经形态计算架构奠定了基础。", "translation": "在本会议论文中，我们首次介绍了使用电流脉冲切换表现出分数阶行为的忆阻器件的一些初步结果。在我们的模型中，假设状态变量的演化遵循涉及Caputo型导数的分数阶微分方程。对焦耳损耗的研究表明，最小化这些损耗的最佳开关策略取决于分数阶导数的阶数和运动方程中的功率指数。研究发现，当分数阶导数的阶数超过功率指数的一半时，最佳方法是采用宽脉冲。相反，当不满足此条件时，通过施加零电流，然后施加最高允许幅度的窄电流脉冲，可以使焦耳损耗最小化。这些发现在多脉冲控制的背景下得到了进一步的探讨。我们的研究为下一代更接近生物对应物的节能神经形态计算架构的发展奠定了基础。", "summary": "本论文介绍了使用电流脉冲对具有分数阶动力学忆阻器进行低功耗开关的初步研究。模型中状态变量演化遵循分数阶微分方程。研究发现，最小化焦耳损耗的最佳开关策略取决于分数阶导数阶数和功率指数：当分数阶导数阶数超过功率指数一半时，采用宽脉冲；否则，采用零电流后跟窄电流脉冲。这些发现为开发节能神经形态计算架构奠定了基础。", "keywords": "忆阻器, 分数阶动力学, 低功耗开关, 神经形态计算, 焦耳损耗", "comments": "该研究创新性地将分数阶动力学引入忆阻器开关策略，并提供了具体的低功耗开关条件，对未来节能神经形态计算架构的发展具有重要意义。"}}
{"id": "2507.17997", "title": "Evaluating judgment of spatial correlation in visual displays of scalar field distributions", "authors": ["Yayan Zhao", "Matthew Berger"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17997v1", "summary": "In this work we study the identification of spatial correlation in\ndistributions of 2D scalar fields, presented across different forms of visual\ndisplays. We study simple visual displays that directly show color-mapped\nscalar fields, namely those drawn from a distribution, and whether humans can\nidentify strongly correlated spatial regions in these displays. In this\nsetting, the recognition of correlation requires making judgments on a set of\nfields, rather than just one field. Thus, in our experimental design we compare\ntwo basic visualization designs: animation-based displays against juxtaposed\nviews of scalar fields, along different choices of color scales. Moreover, we\ninvestigate the impacts of the distribution itself, controlling for the level\nof spatial correlation and discriminability in spatial scales. Our study's\nresults illustrate the impacts of these distribution characteristics, while\nalso highlighting how different visual displays impact the types of judgments\nmade in assessing spatial correlation. Supplemental material is available at\nhttps://osf.io/zn4qy", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17997v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "评估标量场分布可视化显示中空间相关性的判断", "tldr": "本研究评估了人类在不同可视化显示中识别二维标量场分布空间相关性的能力，并比较了动画和并置视图的影响。", "motivation": "研究人类在不同视觉显示中识别二维标量场分布中空间相关性的能力，并探讨不同显示方式和分布特性对判断的影响。", "method": "通过实验设计，比较了两种基本的可视化设计：基于动画的显示与标量场的并置视图，以及不同颜色标度的选择。此外，还研究了分布本身的影响，控制了空间相关性和空间尺度的可辨别性水平。", "result": "研究结果表明了分布特征的影响，并强调了不同的视觉显示如何影响评估空间相关性时所做判断的类型。", "conclusion": "研究结果表明，分布特征和不同的视觉显示都会影响人类在评估空间相关性时的判断类型。", "translation": "在这项工作中，我们研究了在不同形式的视觉显示中呈现的二维标量场分布中空间相关性的识别。我们研究了直接显示颜色映射标量场的简单视觉显示，即那些从分布中提取的标量场，以及人类是否能在这些显示中识别出强相关的空间区域。在这种情况下，识别相关性需要对一组场进行判断，而不仅仅是一个场。因此，在我们的实验设计中，我们比较了两种基本的可视化设计：基于动画的显示与标量场的并置视图，以及不同颜色标度的选择。此外，我们还研究了分布本身的影响，控制了空间相关性和空间尺度的可辨别性水平。我们的研究结果说明了这些分布特征的影响，同时也强调了不同的视觉显示如何影响评估空间相关性时所做判断的类型。补充材料可在 https://osf.io/zn4qy 获取。", "summary": "本研究评估了人类在二维标量场分布的视觉显示中判断空间相关性的能力。通过实验比较了动画和并置视图两种基本可视化设计，并考虑了不同颜色标度以及空间相关性与可辨别性等分布特性的影响。研究结果揭示了这些分布特性以及不同视觉显示对空间相关性判断的影响。", "keywords": "空间相关性, 标量场, 可视化, 判断, 视觉显示", "comments": "这项研究通过实验方法，系统地探讨了在可视化显示中人类判断空间相关性的能力，并深入分析了显示方式和数据特性对判断准确性的影响。其创新之处在于对比了动画和并置视图这两种常见的可视化策略，并考虑了颜色标度以及数据内在的空间相关性等因素，为设计更有效的空间数据可视化提供了实证依据。该研究对于信息可视化、人机交互以及地理信息科学等领域具有重要意义。"}}
{"id": "2507.18104", "title": "A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli", "authors": ["Qianyi He", "Yuan Chang Leong"], "categories": ["cs.CV", "q-bio.NC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18104v1", "summary": "The Algonauts 2025 Challenge called on the community to develop encoding\nmodels that predict whole-brain fMRI responses to naturalistic multimodal\nmovies. In this submission, we propose a sequence-to-sequence Transformer that\nautoregressively predicts fMRI activity from visual, auditory, and language\ninputs. Stimulus features were extracted using pretrained models including\nVideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information\nfrom prior brain states, current stimuli, and episode-level summaries via dual\ncross-attention mechanisms that attend to both perceptual information extracted\nfrom the stimulus as well as narrative information provided by high-level\nsummaries of narrative content. One core innovation of our approach is the use\nof sequences of multimodal context to predict sequences of brain activity,\nenabling the model to capture long-range temporal structure in both stimuli and\nneural responses. Another is the combination of a shared encoder with partial\nsubject-specific decoder, which leverages common structure across subjects\nwhile accounting for individual variability. Our model achieves strong\nperformance on both in-distribution and out-of-distribution data, demonstrating\nthe effectiveness of temporally-aware, multimodal sequence modeling for brain\nactivity prediction. The code is available at\nhttps://github.com/Angelneer926/Algonauts_challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18104v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于预测对自然刺激的脑部反应的多模态Seq2Seq Transformer", "tldr": "本文提出了一种多模态Seq2Seq Transformer模型，用于预测对自然电影刺激的脑部fMRI反应，该模型通过结合视觉、听觉和语言输入以及捕捉长程时间结构，并在Algonauts 2025挑战中表现出色。", "motivation": "Algonauts 2025挑战赛要求开发能够预测对自然多模态电影的整体大脑fMRI反应的编码模型，本文旨在响应此挑战并提出一种有效的解决方案。", "method": "本文提出了一种序列到序列的Transformer模型，该模型通过视觉、听觉和语言输入自回归地预测fMRI活动。刺激特征使用VideoMAE、HuBERT、Qwen和BridgeTower等预训练模型提取。解码器通过双重交叉注意力机制整合来自先前大脑状态、当前刺激和剧集级摘要的信息。该方法的核心创新是使用多模态上下文序列来预测大脑活动序列，以及结合共享编码器和部分受试者特定解码器。", "result": "该模型在分布内和分布外数据上均取得了强大的性能。", "conclusion": "实验结果证明了时间感知、多模态序列建模在预测大脑活动方面的有效性。", "translation": "Algonauts 2025挑战赛要求社区开发能够预测对自然多模态电影的整体大脑fMRI反应的编码模型。在本次提交中，我们提出了一种序列到序列的Transformer模型，该模型通过视觉、听觉和语言输入自回归地预测fMRI活动。刺激特征使用VideoMAE、HuBERT、Qwen和BridgeTower等预训练模型提取。解码器通过双重交叉注意力机制整合来自先前大脑状态、当前刺激和剧集级摘要的信息，该机制同时关注从刺激中提取的感知信息以及由叙事内容的高级摘要提供的叙事信息。我们方法的一个核心创新是使用多模态上下文序列来预测大脑活动序列，从而使模型能够捕获刺激和神经反应中的长程时间结构。另一个创新是结合共享编码器和部分受试者特定解码器，这利用了受试者之间的共同结构，同时考虑了个体差异。我们的模型在分布内和分布外数据上均取得了强大的性能，证明了时间感知、多模态序列建模在预测大脑活动方面的有效性。代码可在https://github.com/Angelneer926/Algonauts_challenge获取。", "summary": "本文针对Algonauts 2025挑战赛，提出了一种多模态序列到序列的Transformer模型，用于预测自然电影刺激下的全脑fMRI反应。该模型整合了视觉、听觉和语言输入，并利用预训练模型提取特征。其创新之处在于通过多模态上下文序列预测大脑活动序列以捕捉长程时间结构，并结合共享编码器与部分受试者特定解码器以兼顾通用性和个体差异。实验结果表明，该模型在分布内和分布外数据上均表现出色，验证了其在脑活动预测中的有效性。", "keywords": "多模态, Seq2Seq Transformer, fMRI预测, 自然刺激, 脑反应", "comments": "本文的创新点在于其多模态Seq2Seq Transformer架构，特别是它能够捕捉刺激和神经反应中的长程时间结构。此外，结合共享编码器和部分受试者特定解码器的设计，有效地平衡了跨受试者的共性和个体差异，这对于脑科学研究中的个体变异性处理至关重要。该方法在自然刺激下的脑活动预测方面展现出强大的潜力。"}}
{"id": "2507.18493", "title": "Global Observer Design for a Class of Linear Observed Systems on Groups", "authors": ["Changwu Liu", "Yuan Shen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      16 pages, 1 figure", "url": "http://arxiv.org/abs/2507.18493v1", "summary": "Linear observed systems on groups encode the geometry of a variety of\npractical state estimation problems. In this paper, we propose a unified\nobserver framework for a class of linear observed systems by restricting a\nbi-invariant system on a Lie group to its normal subgroup. This structural\nproperty powerfully enables a system immersion of the original system into a\nlinear time-varying system. Leveraging the immersion, an observer is\nconstructed by first designing a Kalman-like observer for the immersed system\nand then reconstructing the group-valued state via optimization. Under a rank\ncondition, global exponential stability (GES) is achieved provided one global\noptimum of the reconstruction optimization is found, reflecting the topological\ndifficulties inherent to the non-Euclidean state space. Semi-global stability\nis guaranteed when input biases are jointly estimated. The theory is applied to\nthe GES observer design for two-frame systems, capable of modeling a family of\nnavigation problems. Two non-trivial examples are provided to illustrate\nimplementation details.", "comment": "16 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.18493v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "群上一类线性观测系统的全局观测器设计", "tldr": "本文提出了一种用于群上线性观测系统的统一观测器框架，通过将双不变系统限制在其正规子群上，实现系统到线性时变系统的浸入。在此基础上，设计了一个类卡尔曼观测器，并通过优化重建群值状态，在满足秩条件和找到全局最优解的情况下实现了全局指数稳定性（GES）。该理论应用于双帧系统，并提供了示例。", "motivation": "群上的线性观测系统编码了各种实际状态估计问题的几何结构。本文旨在为这类系统提出一个统一的观测器框架，以解决状态估计问题。", "method": "本文通过将李群上的双不变系统限制在其正规子群上，实现了原始系统到线性时变系统的系统浸入。利用这种浸入，首先为浸入系统设计一个类卡尔曼观测器，然后通过优化重建群值状态，从而构建了一个观测器。", "result": "在满足秩条件且找到重建优化问题的一个全局最优解的情况下，实现了全局指数稳定性（GES）。当输入偏差联合估计时，保证了半全局稳定性。该理论成功应用于双帧系统的GES观测器设计，适用于建模一系列导航问题。通过两个非平凡示例说明了实现细节。", "conclusion": "本文成功为群上一类线性观测系统设计了全局观测器，通过系统浸入和类卡尔曼观测器结合优化重建的方法，实现了全局指数稳定性。该方法对导航等实际状态估计问题具有应用价值。", "translation": "群上的线性观测系统编码了各种实际状态估计问题的几何结构。本文通过将李群上的双不变系统限制在其正规子群上，提出了一种用于一类线性观测系统的统一观测器框架。这种结构特性有力地将原始系统浸入到一个线性时变系统。利用这种浸入，首先为浸入系统设计一个类卡尔曼观测器，然后通过优化重建群值状态来构建观测器。在满足秩条件的情况下，如果找到重建优化问题的一个全局最优解，则实现了全局指数稳定性（GES），这反映了非欧几里得状态空间固有的拓扑困难。当输入偏差联合估计时，保证了半全局稳定性。该理论应用于双帧系统的GES观测器设计，能够建模一系列导航问题。提供了两个非平凡的示例来说明实现细节。", "summary": "本文提出了一种针对群上一类线性观测系统的统一全局观测器设计方法。通过将李群上的双不变系统限制在正规子群上，实现了原始系统到线性时变系统的浸入。在此基础上，结合类卡尔曼观测器和优化方法重建群值状态，从而实现了全局指数稳定性（GES）。研究还探讨了输入偏差联合估计时的半全局稳定性，并将该理论应用于导航问题中的双帧系统。通过具体示例验证了方法的有效性。", "keywords": "全局观测器, 李群, 线性观测系统, 状态估计, 类卡尔曼观测器", "comments": "本文的创新点在于提出了一个统一的观测器框架，通过巧妙地利用李群的结构特性（系统浸入），将复杂的非欧几里得空间问题转化为相对简单的线性时变系统问题。其重要性在于为群上的状态估计问题提供了全局指数稳定的解决方案，克服了非欧几里得状态空间固有的拓扑困难。限制可能在于实现全局最优解的条件，这在实际应用中可能需要进一步的保证或近似方法。"}}
{"id": "2410.05094", "title": "On the Structure of Game Provenance and its Applications", "authors": ["Shawn Bowers", "Yilin Xia", "Bertram Ludäscher"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.05094v2", "summary": "Provenance in databases has been thoroughly studied for positive and for\nrecursive queries, then for first-order (FO) queries, i.e., having negation but\nno recursion. Query evaluation can be understood as a two-player game where the\nopponents argue whether or not a tuple is in the query answer. This\ngame-theoretic approach yields a natural provenance model for FO queries,\nunifying how and why-not provenance. Here, we study the fine-grain structure of\ngame provenance. A game $G=(V,E)$ consists of positions $V$ and moves $E$ and\ncan be solved by computing the well-founded model of a single, unstratifiable\nrule: \\[ \\text{win}(X) \\leftarrow \\text{move}(X, Y), \\neg \\, \\text{win}(Y). \\]\nIn the solved game $G^{\\lambda}$, the value of a position $x\\,{\\in}\\,V$ is\neither won, lost, or drawn. This value is explained by the provenance\n$\\mathscr{P}$(x), i.e., certain (annotated) edges reachable from $x$. We\nidentify seven edge types that give rise to new kinds of provenance, i.e.,\npotential, actual, and primary, and demonstrate that \"not all moves are created\nequal\". We describe the new provenance types, show how they can be computed\nwhile solving games, and discuss applications, e.g., for abstract argumentation\nframeworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.05094v2", "cate": "cs.AI", "date": "2024-10-07", "updated": "2025-07-23", "AI": {"title_translation": "关于博弈溯源结构及其应用", "tldr": "本文研究了博弈溯源的精细结构，提出并分析了三种新的溯源类型（潜在、实际、主要），并讨论了其应用。", "motivation": "数据库中的溯源已针对正向查询、递归查询和一阶（FO）查询进行了深入研究。查询评估可以理解为一场双人博弈。这种博弈论方法为FO查询提供了一个自然的溯源模型，统一了“如何”和“为何不”溯源。本文旨在深入研究博弈溯源的精细结构。", "method": "本文将查询评估理解为双人博弈，并利用博弈 G=(V,E) 的求解（通过计算单个不可分层规则的良基模型：win(X) ← move(X, Y), ¬ win(Y)）来研究博弈溯源。在求解后的博弈 G^λ 中，通过溯源 P(x)（即从 x 可达的某些（带注释的）边）来解释位置 x 的值。研究识别了七种边类型，这些类型产生了新的溯源类型。", "result": "研究识别了七种边类型，这些类型产生了三种新的溯源类型：潜在溯源、实际溯源和主要溯源。结果表明“并非所有移动都生而平等”。这些新的溯源类型可以在求解博弈时进行计算。", "conclusion": "本文深入研究了博弈溯源的精细结构，提出了三种新的溯源类型（潜在、实际、主要），并展示了它们在求解博弈时的计算方法。这些新类型具有实际应用，例如在抽象论证框架中。", "translation": "数据库中的溯源已针对正向查询、递归查询，然后针对一阶（FO）查询（即包含否定但没有递归的查询）进行了深入研究。查询评估可以理解为一场双人博弈，其中对手争论一个元组是否在查询答案中。这种博弈论方法为FO查询提供了一个自然的溯源模型，统一了“如何”和“为何不”溯源。在这里，我们研究了博弈溯源的精细结构。博弈 G=(V,E) 由位置 V 和移动 E 组成，可以通过计算单个不可分层规则的良基模型来求解：\n\nwin(X) ← move(X, Y), ¬ win(Y).\n\n在求解后的博弈 G^λ 中，位置 x ∈ V 的值要么是赢，要么是输，要么是平局。这个值由溯源 P(x) 解释，即从 x 可达的某些（带注释的）边。我们识别了七种边类型，这些类型产生了新的溯源类型，即潜在溯源、实际溯源和主要溯源，并证明了“并非所有移动都生而平等”。我们描述了新的溯源类型，展示了它们在求解博弈时如何计算，并讨论了应用，例如抽象论证框架。", "summary": "本文深入探讨了博弈溯源的精细结构，博弈溯源是一种利用博弈论方法理解一阶查询评估的溯源模型。作者通过求解博弈并识别七种边类型，引入了潜在、实际和主要三种新的溯源类型，并证明了不同移动的重要性不同。文章描述了这些新溯源类型的计算方法，并讨论了它们在抽象论证框架等领域的应用。", "keywords": "博弈溯源, 一阶查询, 溯源模型, 边类型, 抽象论证框架", "comments": "本文的创新之处在于将博弈论应用于溯源分析，并首次提出了博弈溯源的精细结构。通过识别不同的边类型，作者定义了三种新的溯源类型，这为理解查询评估中的“为何”和“为何不”提供了更细致的视角。其方法论新颖，并具有潜在的广泛应用价值，尤其是在需要深入理解决策路径和结果解释的领域，如抽象论证。"}}
{"id": "2505.20658", "title": "Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge", "authors": ["Yue Fang", "Zhi Jin", "Jie An", "Hongshen Chen", "Xiaohong Chen", "Naijun Zhan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, published to ACL 2025", "url": "http://arxiv.org/abs/2505.20658v2", "summary": "Temporal Logic (TL), especially Signal Temporal Logic (STL), enables precise\nformal specification, making it widely used in cyber-physical systems such as\nautonomous driving and robotics. Automatically transforming NL into STL is an\nattractive approach to overcome the limitations of manual transformation, which\nis time-consuming and error-prone. However, due to the lack of datasets,\nautomatic transformation currently faces significant challenges and has not\nbeen fully explored. In this paper, we propose an NL-STL dataset named\nSTL-Diversity-Enhanced (STL-DivEn), which comprises 16,000 samples enriched\nwith diverse patterns. To develop the dataset, we first manually create a\nsmall-scale seed set of NL-STL pairs. Next, representative examples are\nidentified through clustering and used to guide large language models (LLMs) in\ngenerating additional NL-STL pairs. Finally, diversity and accuracy are ensured\nthrough rigorous rule-based filters and human validation. Furthermore, we\nintroduce the Knowledge-Guided STL Transformation (KGST) framework, a novel\napproach for transforming natural language into STL, involving a\ngenerate-then-refine process based on external knowledge. Statistical analysis\nshows that the STL-DivEn dataset exhibits more diversity than the existing\nNL-STL dataset. Moreover, both metric-based and human evaluations indicate that\nour KGST approach outperforms baseline models in transformation accuracy on\nSTL-DivEn and DeepSTL datasets.", "comment": "11 pages, 5 figures, published to ACL 2025", "pdf_url": "http://arxiv.org/pdf/2505.20658v2", "cate": "cs.CL", "date": "2025-05-27", "updated": "2025-07-24", "AI": {"title_translation": "使用具有多样化外部知识的LLM增强自然语言到信号时序逻辑的转换", "tldr": "本文提出了一个多样化的自然语言到信号时序逻辑（NL-STL）数据集STL-DivEn，并引入了一个基于外部知识的转换框架KGST，实验证明该框架在转换准确性上优于基线模型。", "motivation": "手动将自然语言转换为时序逻辑（特别是STL）耗时且易错。现有自动转换方法因缺乏数据集而面临重大挑战且尚未被充分探索。", "method": "本文提出了一个名为STL-Diversity-Enhanced (STL-DivEn) 的NL-STL数据集，包含16,000个样本。数据集的开发流程包括：手动创建小规模种子集、通过聚类识别代表性示例并指导大型语言模型（LLMs）生成额外NL-STL对，最后通过基于规则的过滤器和人工验证确保多样性和准确性。此外，本文还引入了知识引导的STL转换（KGST）框架，这是一种基于外部知识的“生成-然后-细化”过程，用于将自然语言转换为STL。", "result": "统计分析表明，STL-DivEn数据集比现有NL-STL数据集更具多样性。基于度量和人工评估均表明，所提出的KGST方法在STL-DivEn和DeepSTL数据集上的转换准确性优于基线模型。", "conclusion": "本文通过构建一个多样化的NL-STL数据集并提出一个新颖的知识引导转换框架，有效解决了自然语言到信号时序逻辑自动转换中数据集缺乏和转换准确性低的挑战，显著提高了转换的效率和准确性。", "translation": "时序逻辑 (TL)，特别是信号时序逻辑 (STL)，能够进行精确的形式化规范，使其广泛应用于自主驾驶和机器人等网络物理系统。将自然语言 (NL) 自动转换为 STL 是一种有吸引力的方法，可以克服手动转换的局限性，因为手动转换耗时且容易出错。然而，由于数据集的缺乏，自动转换目前面临重大挑战，尚未得到充分探索。在本文中，我们提出了一个名为 STL-Diversity-Enhanced (STL-DivEn) 的 NL-STL 数据集，该数据集包含 16,000 个样本，并富含多样化的模式。为了开发该数据集，我们首先手动创建了一个小规模的 NL-STL 对种子集。接下来，通过聚类识别代表性示例，并用于指导大型语言模型 (LLM) 生成额外的 NL-STL 对。最后，通过严格的基于规则的过滤器和人工验证确保了多样性和准确性。此外，我们引入了知识引导的 STL 转换 (KGST) 框架，这是一种将自然语言转换为 STL 的新颖方法，涉及基于外部知识的“生成-然后-细化”过程。统计分析表明，STL-DivEn 数据集比现有的 NL-STL 数据集表现出更多的多样性。此外，基于度量和人工评估都表明，我们的 KGST 方法在 STL-DivEn 和 DeepSTL 数据集上的转换准确性优于基线模型。", "summary": "本文针对自然语言到信号时序逻辑（NL-STL）自动转换中数据集稀缺和准确性不足的问题，提出了一个包含16,000个多样化样本的新NL-STL数据集STL-DivEn，其构建结合了人工种子集、LLM生成、规则过滤和人工验证。同时，引入了基于外部知识的“生成-然后-细化”转换框架KGST。实验结果表明，STL-DivEn数据集比现有数据集更具多样性，并且KGST方法在转换准确性上优于基线模型。", "keywords": "自然语言处理, 信号时序逻辑, 大型语言模型, 数据集, 知识引导转换", "comments": "本文的创新点在于构建了一个大规模且多样化的NL-STL数据集，并通过结合LLMs和外部知识提出了一个有效的转换框架。这对于推动网络物理系统形式化规范的自动化具有重要意义，解决了当前领域内数据稀缺和转换准确性低的痛点。其方法结合了人工标注的精准性和LLM的生成能力，并通过严格的验证确保了数据质量。"}}
{"id": "2410.13812", "title": "Private Counterfactual Retrieval", "authors": ["Mohamed Nomeir", "Pasan Dissanayake", "Shreya Meel", "Sanghamitra Dutta", "Sennur Ulukus"], "categories": ["cs.IT", "cs.CR", "cs.LG", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.13812v2", "summary": "Transparency and explainability are two extremely important aspects to be\nconsidered when employing black-box machine learning models in high-stake\napplications. Providing counterfactual explanations is one way of fulfilling\nthis requirement. However, this also poses a threat to the privacy of both the\ninstitution that is providing the explanation as well as the user who is\nrequesting it. In this work, we propose multiple schemes inspired by private\ninformation retrieval (PIR) techniques which ensure the \\emph{user's privacy}\nwhen retrieving counterfactual explanations. We present a scheme which\nretrieves the \\emph{exact} nearest neighbor counterfactual explanation from a\ndatabase of accepted points while achieving perfect (information-theoretic)\nprivacy for the user. While the scheme achieves perfect privacy for the user,\nsome leakage on the database is inevitable which we quantify using a mutual\ninformation based metric. Furthermore, we propose strategies to reduce this\nleakage to achieve an advanced degree of database privacy. We extend these\nschemes to incorporate user's preference on transforming their attributes, so\nthat a more actionable explanation can be received. Since our schemes rely on\nfinite field arithmetic, we empirically validate our schemes on real datasets\nto understand the trade-off between the accuracy and the finite field sizes.\nFinally, we present numerical results to support our theoretical findings, and\ncompare the database leakage of the proposed schemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.13812v2", "cate": "cs.IT", "date": "2024-10-17", "updated": "2025-07-24", "AI": {"title_translation": "私有反事实检索", "tldr": "提出基于PIR的方案，在提供反事实解释时保护用户隐私，并量化并减少数据库信息泄露。", "motivation": "在高风险应用中使用黑盒机器学习模型时，透明性和可解释性至关重要。反事实解释是实现这一目标的一种方式，但其在提供和请求时会威胁到提供方和用户的隐私。", "method": "提出多种受私有信息检索（PIR）技术启发的方案，以确保用户在检索反事实解释时的隐私。其中一个方案能够从接受点数据库中检索精确的最近邻反事实解释，同时为用户实现完美（信息论）隐私。进一步提出策略以减少数据库泄露。还将这些方案扩展以整合用户的属性转换偏好，以获得更具操作性的解释。方案依赖于有限域算术，并通过真实数据集进行实证验证，以理解准确性和有限域大小之间的权衡。", "result": "所提出的方案为用户检索反事实解释提供了完美的（信息论）隐私。量化了数据库不可避免的泄露，并通过策略有效减少了这种泄露。实证验证揭示了准确性和有限域大小之间的权衡。数值结果支持了理论发现，并比较了不同方案的数据库泄露。", "conclusion": "该研究成功开发了保护用户隐私的反事实解释检索方案，同时量化并努力减少了数据库泄露，并通过实证验证了其可行性和性能。", "translation": "透明性和可解释性是在高风险应用中采用黑盒机器学习模型时需要考虑的两个极其重要的方面。提供反事实解释是满足这一要求的一种方式。然而，这也对提供解释的机构以及请求解释的用户构成了隐私威胁。在这项工作中，我们提出了多种受私有信息检索（PIR）技术启发的方案，这些方案在检索反事实解释时确保了用户的隐私。我们提出了一种方案，该方案可以从接受点的数据库中检索精确的最近邻反事实解释，同时为用户实现完美的（信息论）隐私。虽然该方案为用户实现了完美的隐私，但数据库的一些泄露是不可避免的，我们使用基于互信息度量对其进行量化。此外，我们提出了减少这种泄露的策略，以实现更高程度的数据库隐私。我们将这些方案扩展到结合用户对其属性转换的偏好，以便可以接收到更具操作性的解释。由于我们的方案依赖于有限域算术，我们对真实数据集上的方案进行了实证验证，以了解准确性和有限域大小之间的权衡。最后，我们提出了数值结果来支持我们的理论发现，并比较了所提方案的数据库泄露。", "summary": "本文针对黑盒机器学习模型在提供反事实解释时面临的隐私挑战，提出了一系列基于私有信息检索（PIR）的方案。这些方案旨在确保用户在检索反事实解释时的隐私，其中一个方案实现了用户完美的（信息论）隐私。研究量化了不可避免的数据库信息泄露，并提出了减少泄露的策略。此外，方案还支持用户偏好以提供更具操作性的解释。通过在真实数据集上的实证验证，分析了准确性与有限域大小的权衡，并提供了数值结果支持理论发现。", "keywords": "私有反事实检索, 私有信息检索, 用户隐私, 可解释性, 机器学习隐私", "comments": "这项工作创新性地将私有信息检索（PIR）技术应用于反事实解释的检索，解决了黑盒模型解释中的关键隐私问题。其亮点在于为用户提供了信息论上的完美隐私，并量化和尝试减少了数据库泄露。这对于在高风险、隐私敏感领域部署可解释AI具有重要意义。"}}
{"id": "2507.18159", "title": "SMECS: A Software Metadata Extraction and Curation Software", "authors": ["Stephan Ferenz", "Aida Jafarbigloo", "Oliver Werth", "Astrid Nieße"], "categories": ["cs.SE", "cs.DL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18159v1", "summary": "Metadata play a crucial role in adopting the FAIR principles for research\nsoftware and enables findability and reusability. However, creating\nhigh-quality metadata can be resource-intensive for researchers and research\nsoftware engineers. To address this challenge, we developed the Software\nMetadata Extraction and Curation Software (SMECS) which integrates the\nextraction of metadata from existing sources together with a user-friendly\ninterface for metadata curation. SMECS extracts metadata from online\nrepositories such as GitHub and presents it to researchers through an\ninteractive interface for further curation and export as a CodeMeta file. The\nusability of SMECS was evaluated through usability experiments which confirmed\nthat SMECS provides a satisfactory user experience. SMECS supports the\nFAIRification of research software by simplifying metadata creation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18159v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SMECS：一款软件元数据提取与管理软件", "tldr": "SMECS是一款旨在简化研究软件元数据创建的工具，它能从在线仓库提取元数据并提供用户友好的界面进行管理。", "motivation": "创建高质量的元数据对于研究软件采纳FAIR原则至关重要，但这一过程对研究人员和软件工程师而言资源密集。", "method": "开发了SMECS软件，它整合了从GitHub等在线仓库提取元数据的功能，并提供用户友好的界面进行元数据管理和导出为CodeMeta文件。", "result": "通过可用性实验评估，SMECS提供了令人满意的用户体验。", "conclusion": "SMECS通过简化元数据创建，支持了研究软件的FAIR化。", "translation": "元数据在研究软件采纳FAIR原则中扮演着关键角色，并能提高可发现性和可重用性。然而，创建高质量元数据对研究人员和研究软件工程师来说可能非常耗费资源。为了解决这一挑战，我们开发了软件元数据提取与管理软件（SMECS），它集成了从现有来源提取元数据的功能，并提供了一个用户友好的元数据管理界面。SMECS从GitHub等在线仓库提取元数据，并通过交互式界面将其呈现给研究人员，以便进一步管理并导出为CodeMeta文件。SMECS的可用性通过可用性实验进行了评估，实验证实SMECS提供了令人满意的用户体验。SMECS通过简化元数据创建，支持了研究软件的FAIR化。", "summary": "本文介绍了一种名为SMECS的软件元数据提取与管理工具，旨在解决研究软件元数据创建耗时费力的问题。SMECS能从GitHub等在线仓库自动提取元数据，并提供直观的用户界面供研究人员进行整理和导出。可用性评估结果表明，SMECS有效简化了元数据创建过程，提升了研究软件的FAIR原则采纳度。", "keywords": "软件元数据, FAIR原则, 元数据提取, 元数据管理, 研究软件", "comments": "SMECS的创新之处在于其整合了自动化元数据提取与人工管理界面，有效降低了研究软件元数据创建的门槛，对于推动FAIR原则在科研软件领域的应用具有重要意义。"}}
{"id": "2408.12635", "title": "Information and motor constraints shape melodic diversity across cultures", "authors": ["John M McBride", "Nahie Kim", "Yuri Nishikawa", "Mekhmed Saadakeev", "Marcus T Pearce", "Tsvi Tlusty"], "categories": ["cs.SD", "cs.IT", "eess.AS", "math.IT", "physics.soc-ph"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.12635v4", "summary": "The number of possible melodies is unfathomably large, yet despite this\nvirtually unlimited potential for melodic variation, melodies from different\nsocieties can be surprisingly similar. The motor constraint hypothesis accounts\nfor certain similarities, such as scalar motion and contour shape, but not for\nother major common features, such as repetition, song length, and scale size.\nHere we investigate the role of information constraints in shaping these\nhallmarks of melodies. We measure determinants of information rate in 62\ncorpora of Folk melodies spanning several continents, finding multiple\ntrade-offs that all act to constrain the information rate across societies. By\ncontrast, 39 corpora of Art music from Europe (including Turkey) show longer,\nmore complex melodies, and increased complexity over time, suggesting different\ncultural-evolutionary selection pressures in Art and Folk music, possibly due\nto the use of written versus oral transmission. Our parameter-free model\npredicts the empirical scale degree distribution using information constraints\non scalar motion, melody length, and, most importantly, information rate. These\nresults provide strong evidence that information constraints during cultural\ntransmission of music limit the number of notes in a scale, and suggests that a\ntendency for intermediate melodic complexity reflects a fundamental constraint\non the cultural evolution of melody.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.12635v4", "cate": "cs.SD", "date": "2024-08-22", "updated": "2025-07-24", "AI": {"title_translation": "信息和运动约束塑造了跨文化的旋律多样性", "tldr": "本研究发现，信息和运动约束共同塑造了跨文化旋律的相似性和多样性，解释了音阶大小和旋律复杂性等特征。", "motivation": "虽然运动约束假说能够解释旋律的一些共同特征（如音阶运动和轮廓形状），但它未能解释其他主要的共同特征，如重复、歌曲长度和音阶大小。本研究旨在探究信息约束在塑造这些旋律特征中的作用。", "method": "研究测量了来自跨越多个大陆的62个民间旋律语料库中的信息率决定因素。此外，还分析了来自欧洲（包括土耳其）的39个艺术音乐语料库。研究还使用了一个无参数模型，利用标量运动、旋律长度以及最重要的信息率的信息约束来预测经验音阶度分布。", "result": "研究发现，在民间音乐中存在多种权衡，这些权衡共同限制了不同社会的信息率。相比之下，艺术音乐表现出更长、更复杂的旋律，并且随着时间的推移复杂性增加。无参数模型成功预测了经验音阶度分布。", "conclusion": "结果有力地证明，音乐文化传播过程中的信息约束限制了音阶中的音符数量，并表明中等旋律复杂性的趋势反映了旋律文化演变的一个基本约束。", "translation": "旋律的数量大得无法估量，然而，尽管旋律变化的可能性几乎无限，但不同社会的旋律却出奇地相似。运动约束假说解释了某些相似性，如音阶运动和轮廓形状，但未能解释其他主要的共同特征，如重复、歌曲长度和音阶大小。在这里，我们调查了信息约束在塑造这些旋律特征中的作用。我们测量了跨越多个大陆的62个民间旋律语料库中的信息率决定因素，发现所有这些都起作用，以限制不同社会的信息率。相比之下，来自欧洲（包括土耳其）的39个艺术音乐语料库显示出更长、更复杂的旋律，并且随着时间的推移复杂性增加，这表明艺术音乐和民间音乐中存在不同的文化进化选择压力，这可能是由于书面传播与口头传播的使用。我们的无参数模型利用标量运动、旋律长度以及最重要的信息率的信息约束来预测经验音阶度分布。这些结果提供了有力的证据，表明音乐文化传播过程中的信息约束限制了音阶中的音符数量，并表明中等旋律复杂性的趋势反映了旋律文化演变的一个基本约束。", "summary": "本研究探讨了信息约束在塑造跨文化旋律多样性中的作用，以补充或超越运动约束假说。通过分析62个民间旋律语料库和39个艺术音乐语料库，研究发现信息约束导致了民间音乐中信息率的限制性权衡，而艺术音乐则展现出更高的复杂性。一个无参数模型成功预测了音阶度分布。研究结论认为，信息约束是限制音阶音符数量的关键因素，并且中等旋律复杂性是旋律文化演变中的一个基本约束。", "keywords": "旋律多样性, 信息约束, 运动约束, 文化演化, 音乐", "comments": "这项研究的创新之处在于引入了“信息约束”的概念来解释旋律的普遍特征，这弥补了传统“运动约束”假说的不足。其使用跨文化的大规模语料库以及参数自由模型增强了研究结果的普适性和说服力。论文揭示了文化传播对音乐结构的影响，对理解音乐的认知和文化演化具有重要意义。"}}
{"id": "2506.07685", "title": "Theoretical Analysis for the CommSense Measurement System", "authors": ["Sandip Jana", "Amit Kumar Mishra", "Mohammed Zafar Ali Khan"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.07685v2", "summary": "Future 6G networks envisions to blur the line between communication and\nsensing, leveraging ubiquitous OFDM waveforms for both high throughput data and\nenvironmental awareness. In this work, we do a thorough analysis of\nCommunication based Sensing (CommSense) framework that embeds lightweight, PCA\nbased detectors into standard OFDM receivers; enabling real-time, device free\ndetection of passive scatterers (e.g. drones, vehicles etc.) without any extra\ntransmitters. Starting from a realistic three link Rician channel model (direct\nTx to Rx, cascaded Tx to Scatterer and Scatterer to Rx), we compare four\ndetectors: the full dimensional Likelihood Ratio Test (Full LRT), PCA based\nLRT, PCA-SVM with linear and RBF kernels. By projecting N-dimensional CSI onto\na P (very less than N) principal component subspace, inference time gets\nreduced by an order of magnitude compared to the full LRT, while achieving\noptimal error rates i.e. empirical errors align tightly with the Bhattacharyya\nerror bound and Area Under ROC Curve (AUC) approx. equal to 1 for P approx.\nequal to 10. From the simulated result we have shown LRT based techniques are\nsusceptible to the parameter estimation error, where as SVM is resilient to\nthat. Our results demonstrate that PCA driven detection when paired with\nlightweight SVMs can deliver fast, accurate, and robust scatterer sensing,\npaving the way for integrated sensing and communication (ISAC) in 6G and\nbeyond.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.07685v2", "cate": "eess.SP", "date": "2025-06-09", "updated": "2025-07-24", "AI": {"title_translation": "CommSense测量系统的理论分析", "tldr": "本文对用于6G网络的CommSense框架进行了深入分析，该框架利用基于PCA的检测器在标准OFDM接收器中实现无设备散射体检测，并证明PCA结合SVM能提供快速、准确、鲁棒的传感。", "motivation": "未来的6G网络旨在融合通信和感知，利用OFDM波形实现高吞吐量数据传输和环境感知。目前需要一种无需额外发射器即可实现实时、无设备检测被动散射体（如无人机、车辆）的方法。", "method": "本文对CommSense框架进行了深入分析，该框架在标准OFDM接收器中嵌入了轻量级、基于PCA的检测器。研究从现实的三链路Rician信道模型出发，比较了四种检测器：全维度似然比检验（Full LRT）、基于PCA的LRT、以及带有线性和RBF核的PCA-SVM。通过将N维CSI投影到P（远小于N）个主成分子空间，以减少推理时间。", "result": "与Full LRT相比，将N维CSI投影到P维主成分子空间可将推理时间减少一个数量级。实现了最优的错误率，即经验错误率与Bhattacharyya误差界限紧密对齐，当P约等于10时，ROC曲线下面积（AUC）约等于1。基于LRT的技术容易受到参数估计误差的影响，而SVM对此具有弹性。", "conclusion": "结合轻量级SVM的PCA驱动检测可以实现快速、准确、鲁棒的散射体感知，为6G及未来网络的集成感知和通信（ISAC）铺平了道路。", "translation": "未来的6G网络设想模糊通信和感知的界限，利用无处不在的OFDM波形进行高吞吐量数据传输和环境感知。在这项工作中，我们对基于通信的感知（CommSense）框架进行了彻底的分析，该框架将轻量级、基于PCA的检测器嵌入到标准OFDM接收器中；从而无需任何额外的发射器即可实现对被动散射体（例如无人机、车辆等）的实时、无设备检测。从一个现实的三链路Rician信道模型（直接Tx到Rx，级联Tx到散射体，散射体到Rx）开始，我们比较了四种检测器：全维度似然比检验（Full LRT）、基于PCA的LRT、以及带有线性和RBF核的PCA-SVM。通过将N维CSI投影到P（远小于N）个主成分子空间，推理时间比全LRT减少了一个数量级，同时实现了最优的错误率，即经验误差与Bhattacharyya误差界限紧密对齐，并且当P约等于10时，ROC曲线下面积（AUC）约等于1。从仿真结果我们已经表明，基于LRT的技术容易受到参数估计误差的影响，而SVM对此具有弹性。我们的结果表明，当与轻量级SVM结合时，PCA驱动的检测可以提供快速、准确和鲁棒的散射体感知，为6G及未来网络的集成感知和通信（ISAC）铺平了道路。", "summary": "本文对CommSense框架进行了理论分析，该框架旨在通过在标准OFDM接收器中嵌入基于PCA的检测器，实现6G网络中无额外发射器的实时、无设备散射体检测。研究比较了包括PCA-SVM在内的四种检测器，并发现PCA降维能显著减少推理时间，同时保持高检测性能。特别地，结果表明PCA驱动的检测与轻量级SVM结合时，能够提供快速、准确且对参数估计误差鲁棒的散射体感知，这对于未来6G网络的集成感知与通信至关重要。", "keywords": "6G网络, CommSense, 集成感知与通信, PCA, SVM, 被动散射体检测", "comments": "本文的创新点在于将PCA降维技术与轻量级SVM相结合，应用于6G通信感知一体化（ISAC）场景下的无设备散射体检测。通过在标准OFDM接收器中嵌入这些检测器，实现了实时、高效的感知能力，并且在性能和鲁棒性方面表现出色。这项工作为未来6G网络中通信与感知的深度融合提供了重要的理论和实践基础。"}}
{"id": "2507.18332", "title": "Hierarchical Dimensionless Learning (Hi-π): A physics-data hybrid-driven approach for discovering dimensionless parameter combinations", "authors": ["Mingkun Xia", "Haitao Lin", "Weiwei Zhang"], "categories": ["physics.flu-dyn", "cs.LG", "physics.data-an"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18332v1", "summary": "Dimensional analysis provides a universal framework for reducing physical\ncomplexity and reveal inherent laws. However, its application to\nhigh-dimensional systems still generates redundant dimensionless parameters,\nmaking it challenging to establish physically meaningful descriptions. Here, we\nintroduce Hierarchical Dimensionless Learning (Hi-{\\pi}), a physics-data\nhybrid-driven method that combines dimensional analysis and symbolic regression\nto automatically discover key dimensionless parameter combination(s). We\napplied this method to classic examples in various research fields of fluid\nmechanics. For the Rayleigh-B\\'enard convection, this method accurately\nextracted two intrinsic dimensionless parameters: the Rayleigh number and the\nPrandtl number, validating its unified representation advantage across\nmultiscale data. For the viscous flows in a circular pipe, the method\nautomatically discovers two optimal dimensionless parameters: the Reynolds\nnumber and relative roughness, achieving a balance between accuracy and\ncomplexity. For the compressibility correction in subsonic flow, the method\neffectively extracts the classic compressibility correction formulation, while\ndemonstrating its capability to discover hierarchical structural expressions\nthrough optimal parameter transformations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18332v1", "cate": "physics.flu-dyn", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "分层无量纲学习 (Hi-π)：一种物理-数据混合驱动的无量纲参数组合发现方法", "tldr": "引入了一种名为分层无量纲学习 (Hi-π) 的物理-数据混合驱动方法，结合量纲分析和符号回归，自动发现关键无量纲参数组合，并在流体力学经典案例中验证了其有效性。", "motivation": "传统的量纲分析在处理高维系统时会产生冗余的无量纲参数，使得建立具有物理意义的描述变得困难。", "method": "本文提出了一种名为分层无量纲学习 (Hi-π) 的物理-数据混合驱动方法，它结合了量纲分析和符号回归，以自动发现关键的无量纲参数组合。", "result": "该方法成功应用于流体力学中的经典案例：对于瑞利-贝纳对流，准确提取了瑞利数和普朗特数；对于圆管中的粘性流，自动发现了雷诺数和相对粗糙度；对于亚音速流中的可压缩性校正，有效提取了经典的可压缩性校正公式，并展示了发现分层结构表达式的能力。", "conclusion": "Hi-π 方法能够有效发现关键和最优的无量纲参数组合以及分层结构表达式，在各种流体力学问题中表现出统一性和准确性。", "translation": "量纲分析提供了一个通用的框架，用于降低物理复杂性并揭示内在规律。然而，其在高维系统中的应用仍然会产生冗余的无量纲参数，使得建立具有物理意义的描述变得具有挑战性。在此，我们引入了分层无量纲学习 (Hi-π)，这是一种物理-数据混合驱动的方法，它结合了量纲分析和符号回归，以自动发现关键的无量纲参数组合。我们将此方法应用于流体力学各个研究领域的经典示例。对于瑞利-贝纳对流，该方法准确提取了两个内在的无量纲参数：瑞利数和普朗特数，验证了其在多尺度数据上的统一表示优势。对于圆管中的粘性流，该方法自动发现了两个最优的无量纲参数：雷诺数和相对粗糙度，实现了准确性和复杂性之间的平衡。对于亚音速流中的可压缩性校正，该方法有效提取了经典的可压缩性校正公式，同时展示了通过最优参数变换发现分层结构表达式的能力。", "summary": "本文提出了一种名为分层无量纲学习 (Hi-π) 的新方法，它结合了物理驱动的量纲分析和数据驱动的符号回归，旨在解决传统量纲分析在高维系统中产生冗余无量纲参数的问题。Hi-π能够自动发现关键的无量纲参数组合，并在流体力学领域（如瑞利-贝纳对流、圆管粘性流和亚音速流可压缩性校正）的经典案例中得到验证，展示了其在提取物理意义参数、平衡准确性与复杂性以及发现分层结构表达式方面的有效性和普适性。", "keywords": "分层无量纲学习, 量纲分析, 符号回归, 无量纲参数", "comments": "Hi-π 的创新之处在于将物理学原理（量纲分析）与数据驱动方法（符号回归）相结合，有效克服了传统量纲分析在高维系统中的局限性。它不仅能够识别出物理上重要的无量纲参数，还能发现其最优组合和分层结构，这对于简化复杂物理系统描述、揭示内在规律具有重要意义。该方法在多个经典流体力学问题中的成功应用，突显了其广泛的适用性和强大的问题解决能力。"}}
{"id": "2507.15103", "title": "Analysis of fully discrete Crank-Nicolson finite element methods for a stochastic Keller-Segel chemotaxis system with gradient-type multiplicative noise", "authors": ["Liet Vo"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15103v2", "summary": "We develop and analyze numerical methods for a stochastic Keller-Segel system\nperturbed by Stratonovich noise, which models chemotactic behavior under\nrandomly fluctuating environmental conditions. The proposed fully discrete\nscheme couples a Crank-Nicolson time discretization with a splitting mixed\nfinite element method in space. We rigorously prove the stability of the\nnumerical scheme and establish strong convergence rates of order $O(k^{1/2} +\nk^{-1/2}h^2)$, where $k$ and $h$ denote the time and spatial step sizes,\nrespectively. Notably, the presence of stochastic forcing leads to an inverse\ndependence on $k$ in the error estimates, distinguishing the convergence\nbehavior from that of the deterministic case. Numerical experiments are\npresented to validate the theoretical results and demonstrate the effectiveness\nand accuracy of the proposed methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15103v2", "cate": "math.NA", "date": "2025-07-20", "updated": "2025-07-24", "AI": {"title_translation": "随机Keller-Segel趋化系统梯度型乘性噪声的全离散Crank-Nicolson有限元方法分析", "tldr": "本文提出并分析了一种全离散Crank-Nicolson有限元方法，用于求解带有Stratonovich噪声的随机Keller-Segel系统，并证明了其稳定性和收敛率，其中收敛率的误差估计对时间步长k存在逆依赖性。", "motivation": "该研究旨在为受随机波动环境条件影响的趋化行为建模，通过开发和分析随机Keller-Segel系统的数值方法来解决这个问题。", "method": "本文提出的全离散方案结合了Crank-Nicolson时间离散化和空间上的分裂混合有限元方法。", "result": "研究结果证明了数值方案的稳定性，并建立了$O(k^{1/2} + k^{-1/2}h^2)$的强收敛率，其中$k$和$h$分别是时间步长和空间步长。值得注意的是，随机力的存在导致误差估计中对$k$的逆依赖，这使得收敛行为与确定性情况不同。数值实验验证了理论结果并展示了所提出方法的有效性和准确性。", "conclusion": "本文提出的全离散Crank-Nicolson有限元方法对于随机Keller-Segel系统是稳定且收敛的，其独特的收敛行为（对时间步长的逆依赖）是随机扰动导致的。该方法在数值实验中表现出有效性和准确性。", "translation": "我们开发并分析了受Stratonovich噪声扰动的随机Keller-Segel系统的数值方法，该系统模拟了随机波动环境条件下的趋化行为。所提出的全离散方案将Crank-Nicolson时间离散化与空间上的分裂混合有限元方法相结合。我们严格证明了数值方案的稳定性，并建立了$O(k^{1/2} + k^{-1/2}h^2)$阶的强收敛率，其中$k$和$h$分别表示时间步长和空间步长。值得注意的是，随机力的存在导致误差估计中对$k$的逆依赖，这使得收敛行为与确定性情况不同。数值实验验证了理论结果并展示了所提出方法的有效性和准确性。", "summary": "本文针对受Stratonovich噪声扰动的随机Keller-Segel趋化系统，提出并分析了一种结合Crank-Nicolson时间离散和分裂混合有限元空间离散的全离散数值方法。研究严格证明了该方法的稳定性和$O(k^{1/2} + k^{-1/2}h^2)$的强收敛率，并指出随机力导致误差估计中对时间步长$k$的逆依赖性，这与确定性情况不同。数值实验验证了理论结果并展示了方法的有效性和准确性。", "keywords": "随机Keller-Segel系统, Crank-Nicolson, 有限元方法, 趋化性, 收敛率", "comments": "本文的创新点在于为带有梯度型乘性噪声的随机Keller-Segel系统开发了数值方法，并首次揭示了随机力对收敛率误差估计中时间步长$k$的逆依赖性，这对于理解随机偏微分方程的数值行为具有重要意义。"}}
{"id": "2507.18317", "title": "AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments", "authors": ["Chenglong Qian", "Yang Xu", "Xiufang Shi", "Jiming Chen", "Liang Li"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18317v1", "summary": "In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18317v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "AF-RLIO：雷达-激光雷达-惯性信息自适应融合，用于复杂环境中的鲁棒里程计", "tldr": "AF-RLIO是一种自适应融合雷达、激光雷达、IMU和GPS的方法，用于在烟雾和隧道等复杂环境中实现鲁棒的里程计估计，通过预处理、动态感知多模态里程计和因子图优化三个模块提高性能。", "motivation": "在机器人导航中，精确的姿态估计和导航至关重要，但单一传感器系统（如激光雷达或GPS）在烟雾、隧道和恶劣天气等挑战下性能会显著下降，影响自主机器人的稳定性和安全性。", "method": "提出AF-RLIO，一个自适应融合4D毫米波雷达、激光雷达、IMU和GPS的方法。包含三个模块：首先，预处理模块利用雷达数据协助激光雷达去除动态点，并判断激光雷达环境何时退化。其次，动态感知多模态里程计选择合适的点云数据进行扫描到地图匹配，并使用迭代误差状态卡尔曼滤波器与IMU紧密耦合。最后，因子图优化模块平衡里程计和GPS数据之间的权重，构建姿态图进行优化。", "result": "该方法在数据集和真实机器人环境中进行了评估和测试，证明了其在烟雾和隧道等挑战性条件下相对于现有方法的有效性和优势。", "conclusion": "AF-RLIO通过自适应融合多种传感器数据，能够有效应对复杂环境中的挑战，实现鲁棒的里程计估计，并优于现有方法。", "translation": "在机器人导航中，在复杂动态环境中保持精确的姿态估计和导航至关重要。然而，烟雾、隧道和恶劣天气等环境挑战会显著降低单一传感器系统（如激光雷达或GPS）的性能，从而损害自主机器人的整体稳定性和安全性。为了解决这些挑战，我们提出了AF-RLIO：一种自适应融合方法，它整合了4D毫米波雷达、激光雷达、惯性测量单元（IMU）和GPS，以利用这些传感器的互补优势，在复杂环境中进行鲁棒的里程计估计。我们的方法由三个关键模块组成。首先，预处理模块利用雷达数据辅助激光雷达去除动态点，并确定激光雷达环境何时退化。其次，动态感知多模态里程计选择合适的点云数据进行扫描到地图匹配，并使用迭代误差状态卡尔曼滤波器与IMU紧密耦合。最后，因子图优化模块平衡里程计和GPS数据之间的权重，构建姿态图进行优化。所提出的方法已在数据集上进行评估，并在真实机器人环境中进行了测试，证明了其在烟雾和隧道等挑战性条件下相对于现有方法的有效性和优势。", "summary": "本文提出了AF-RLIO，一种融合雷达、激光雷达、IMU和GPS的自适应方法，旨在解决复杂动态环境中单一传感器系统在恶劣条件下性能下降的问题。该方法包含预处理、动态感知里程计和因子图优化三个模块，通过利用多传感器互补性，实现了在烟雾和隧道等挑战性环境中鲁棒的里程计估计，并表现出优于现有方法的性能。", "keywords": "雷达-激光雷达融合, 里程计, 自适应融合, 复杂环境, 多传感器融合", "comments": "该论文的创新点在于提出了一个多传感器自适应融合框架AF-RLIO，尤其是在预处理阶段利用雷达数据辅助激光雷达处理动态点和环境退化，这对于恶劣环境下的鲁棒性至关重要。其重要性体现在提升了自主机器人在复杂挑战性环境下的导航精度和安全性，具有实际应用价值。"}}
{"id": "2507.17957", "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17957v1", "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17957v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "AFRDA：域自适应语义分割的注意力特征细化", "tldr": "提出AFRDA，通过注意力特征细化模块提高无监督域自适应语义分割性能，有效平衡局部和全局信息。", "motivation": "现有无监督域自适应语义分割（UDA-SS）方法难以平衡细粒度局部细节与全局上下文信息，导致在复杂区域出现分割错误。", "method": "本文引入了自适应特征细化（AFR）模块，通过使用来自低分辨率logits的语义先验来细化高分辨率特征，以提高分割精度。AFR还整合了捕获细粒度结构的高频分量，并利用不确定性驱动的注意力自适应地平衡局部和全局信息，从而减少错误分类。其轻量级设计允许其无缝集成到基于HRDA的UDA方法中。", "result": "该方法在GTA V --> Cityscapes数据集上将现有UDA-SS方法提高了1.05% mIoU，在Synthia --> Cityscapes数据集上提高了1.04% mIoU，达到了最先进的分割性能。", "conclusion": "AFRDA通过注意力特征细化，有效解决了无监督域自适应语义分割中局部与全局信息平衡的挑战，实现了最先进的分割性能。", "translation": "在无监督域自适应语义分割（UDA-SS）中，模型在带有标签的源域数据（例如，合成图像）上进行训练，并适应到无标签的目标域（例如，真实世界图像），而无需访问目标注释。现有的UDA-SS方法通常难以平衡细粒度局部细节与全局上下文信息，导致在复杂区域出现分割错误。为了解决这个问题，我们引入了自适应特征细化（AFR）模块，该模块通过使用来自低分辨率logits的语义先验来细化高分辨率特征，从而提高分割精度。AFR还整合了高频分量，这些分量捕获细粒度结构并提供关键的边界信息，从而改善了对象轮廓的描绘。此外，AFR通过不确定性驱动的注意力自适应地平衡局部和全局信息，减少了错误分类。其轻量级设计允许其无缝集成到基于HRDA的UDA方法中，从而实现了最先进的分割性能。我们的方法在GTA V --> Cityscapes上将现有UDA-SS方法提高了1.05% mIoU，在Synthia --> Cityscapes上提高了1.04% mIoU。我们框架的实现可在以下网址获取：https://github.com/Masrur02/AFRDA", "summary": "本文提出了AFRDA，一个用于无监督域自适应语义分割的注意力特征细化框架。针对现有方法在局部细节与全局上下文平衡上的不足，AFRDA引入了自适应特征细化（AFR）模块，通过结合低分辨率语义先验、高频分量和不确定性驱动的注意力机制，有效提升了分割精度和对象边界描绘。该方法轻量且易于集成，在GTA V --> Cityscapes和Synthia --> Cityscapes数据集上均取得了显著的mIoU提升，达到了最先进的性能。", "keywords": "无监督域自适应, 语义分割, 特征细化, 注意力机制, 域适应", "comments": "本文的创新点在于提出了AFR模块，通过结合多层次特征（高分辨率特征、低分辨率语义先验、高频分量）和注意力机制（不确定性驱动）来解决域自适应语义分割中局部与全局信息平衡的难题。其轻量级和易于集成的特性也增加了其实用性。在性能提升上，虽然mIoU的提升幅度（约1%）看起来不大，但在竞争激烈的UDA-SS领域，这通常代表着重要的进步。"}}
{"id": "2507.18431", "title": "What does the public want their local government to hear? A data-driven case study of public comments across the state of Michigan", "authors": ["Chang Ge", "Justine Zhang", "Haofei Xu", "Yanna Krupnikov", "Jenna Bednar", "Sabina Tomkins"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18431v1", "summary": "City council meetings are vital sites for civic participation where the\npublic can speak directly to their local government. By addressing city\nofficials and calling on them to take action, public commenters can potentially\ninfluence policy decisions spanning a broad range of concerns, from housing, to\nsustainability, to social justice. Yet studies of these meetings have often\nbeen limited by the availability of large-scale, geographically-diverse data.\nRelying on local governments' increasing use of YouTube and other technologies\nto archive their public meetings, we propose a framework that characterizes\ncomments along two dimensions: the local concerns where concerns are situated\n(e.g., housing, election administration), and the societal concerns raised\n(e.g., functional democracy, anti-racism). Based on a large record of public\ncomments we collect from 15 cities in Michigan, we produce data-driven\ntaxonomies of the local concerns and societal concerns that these comments\ncover, and employ machine learning methods to scalably apply our taxonomies\nacross the entire dataset. We then demonstrate how our framework allows us to\nexamine the salient local concerns and societal concerns that arise in our\ndata, as well as how these aspects interact.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18431v1", "cate": "cs.CY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "公众希望地方政府听到什么？密歇根州公共评论的数据驱动案例研究", "tldr": "本文提出一个框架，利用密歇根州城市会议的YouTube记录，通过机器学习分析公众评论中的地方和社会关注点，以克服现有研究数据缺乏的限制。", "motivation": "城市议会会议是公民参与的重要场所，但对这些会议的研究常受限于缺乏大规模、地理多样化数据。", "method": "提出一个框架，将公众评论分为地方关注点（如住房、选举管理）和社会关注点（如功能民主、反种族主义）两个维度。基于从密歇根州15个城市收集的大量公共评论记录，生成地方关注点和社会关注点的数据驱动分类法，并采用机器学习方法将其可扩展地应用于整个数据集。", "result": "生产了公共评论中地方关注点和社会关注点的数据驱动分类法，并展示了该框架如何允许检查数据中出现的显著地方关注点和社会关注点，以及这些方面如何相互作用。", "conclusion": "Not mentioned in abstract", "translation": "市议会会议是公民参与的重要场所，公众可以在此直接向地方政府表达意见。通过向市政府官员陈述并呼吁他们采取行动，公众评论者有可能影响从住房、可持续性到社会公正等广泛问题的政策决定。然而，对这些会议的研究往往受到缺乏大规模、地理多样化数据的限制。我们利用地方政府越来越多地使用YouTube和其他技术来存档其公开会议，提出了一个框架，该框架从两个维度来描述评论：一是关注点所处的地方性问题（例如，住房、选举管理），二是提出的社会性问题（例如，功能性民主、反种族主义）。基于我们从密歇根州15个城市收集的大量公共评论记录，我们生成了这些评论所涵盖的地方性问题和社会性问题的数据驱动分类法，并采用机器学习方法将我们的分类法可扩展地应用于整个数据集。然后，我们展示了我们的框架如何使我们能够检查数据中出现的显著地方性问题和社会性问题，以及这些方面如何相互作用。", "summary": "本研究旨在克服现有公民参与会议研究中大规模数据缺乏的限制。作者提出了一个数据驱动的框架，利用密歇根州15个城市存档的公共会议记录，对公众评论进行地方和社会关注点的双维度分类。通过构建数据驱动的分类法并应用机器学习方法，该框架能够识别和分析公众向地方政府表达的关键议题及其相互关系。", "keywords": "公众评论, 地方政府, 数据驱动, 机器学习, 公民参与", "comments": "这项研究通过利用YouTube等在线存档，创新性地解决了大规模公民参与数据获取的挑战。其提出的双维度评论分类框架结合机器学习方法，为理解公众在地方政府层面的关注点提供了可扩展且系统化的分析工具，对于提升地方治理的响应性和透明度具有重要意义。"}}
{"id": "2507.17921", "title": "Sliding Window Informative Canonical Correlation Analysis", "authors": ["Arvind Prasadan"], "categories": ["stat.ML", "cs.LG", "eess.IV", "math.ST", "stat.CO", "stat.ME", "stat.TH", "62H20, 62H25 (Primary) 62J10, 62L10 (Secondary)"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      22 pages, submitted", "url": "http://arxiv.org/abs/2507.17921v1", "summary": "Canonical correlation analysis (CCA) is a technique for finding correlated\nsets of features between two datasets. In this paper, we propose a novel\nextension of CCA to the online, streaming data setting: Sliding Window\nInformative Canonical Correlation Analysis (SWICCA). Our method uses a\nstreaming principal component analysis (PCA) algorithm as a backend and uses\nthese outputs combined with a small sliding window of samples to estimate the\nCCA components in real time. We motivate and describe our algorithm, provide\nnumerical simulations to characterize its performance, and provide a\ntheoretical performance guarantee. The SWICCA method is applicable and scalable\nto extremely high dimensions, and we provide a real-data example that\ndemonstrates this capability.", "comment": "22 pages, submitted", "pdf_url": "http://arxiv.org/pdf/2507.17921v1", "cate": "stat.ML", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "滑动窗口信息典范相关分析", "tldr": "本文提出了一种名为滑动窗口信息典范相关分析（SWICCA）的新型在线流数据CCA扩展方法，利用流式PCA和滑动窗口实时估计CCA分量，并适用于高维数据。", "motivation": "传统的典范相关分析（CCA）不适用于在线或流式数据环境，因此需要一种新的方法来处理实时数据流中的相关特征发现。", "method": "本文提出了一种名为滑动窗口信息典范相关分析（SWICCA）的方法。该方法以流式主成分分析（PCA）算法为后端，并结合这些输出与一个小的样本滑动窗口来实时估计典范相关分析（CCA）分量。", "result": "研究通过数值模拟来表征SWICCA的性能，并提供了理论性能保证。此外，SWICCA方法适用于极高维度的数据，并通过一个真实数据示例展示了其能力。", "conclusion": "SWICCA是一种新颖、有效且可扩展的典范相关分析方法，专门为在线和流式高维数据环境设计。", "translation": "典范相关分析（CCA）是一种用于在两个数据集中寻找相关特征集的技术。在本文中，我们提出了一种新颖的CCA扩展，适用于在线、流式数据设置：滑动窗口信息典范相关分析（SWICCA）。我们的方法使用流式主成分分析（PCA）算法作为后端，并结合这些输出和少量样本的滑动窗口来实时估计CCA分量。我们阐述并描述了我们的算法，提供了数值模拟来表征其性能，并提供了理论性能保证。SWICCA方法适用于极高维度且具有可扩展性，我们提供了一个真实数据示例来证明其能力。", "summary": "本文介绍了一种名为滑动窗口信息典范相关分析（SWICCA）的新型在线流数据典范相关分析（CCA）扩展方法。SWICCA利用流式主成分分析（PCA）作为后端，并结合滑动窗口技术，实现了CCA分量的实时估计。该研究通过数值模拟验证了其性能，提供了理论性能保证，并展示了其在处理极高维度数据时的适用性和可扩展性。", "keywords": "典范相关分析, 流式数据, 在线学习, 高维数据, 主成分分析", "comments": "该论文提出了一种创新的在线CCA方法，有效地解决了传统CCA在处理流式高维数据时的局限性。其结合流式PCA和滑动窗口的策略具有实用价值，并提供了坚实的理论和实验支持，对于实时数据分析领域具有重要意义。"}}
{"id": "2507.18513", "title": "Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection", "authors": ["Adhemar de Senneville", "Xavier Bou", "Thibaud Ehret", "Rafael Grompone", "Jean Louis Bonne", "Nicolas Dumelie", "Thomas Lauvaux", "Gabriele Facciolo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18513v1", "summary": "Object detection is one of the main applications of computer vision in remote\nsensing imagery. Despite its increasing availability, the sheer volume of\nremote sensing data poses a challenge when detecting rare objects across large\ngeographic areas. Paradoxically, this common challenge is crucial to many\napplications, such as estimating environmental impact of certain human\nactivities at scale. In this paper, we propose to address the problem by\ninvestigating the methane production and emissions of bio-digesters in France.\nWe first introduce a novel dataset containing bio-digesters, with small\ntraining and validation sets, and a large test set with a high imbalance\ntowards observations without objects since such sites are rare. We develop a\npart-based method that considers essential bio-digester sub-elements to boost\ninitial detections. To this end, we apply our method to new, unseen regions to\nbuild an inventory of bio-digesters. We then compute geostatistical estimates\nof the quantity of methane produced that can be attributed to these\ninfrastructures in a given area at a given time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18513v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于部分的目标检测在大规模地理统计甲烷监测中的应用", "tldr": "本文提出了一种基于部分的目标检测方法，用于大规模遥感图像中稀有生物消化器的识别和甲烷排放的地理统计估算。", "motivation": "遥感数据量庞大，但在大地理区域检测稀有物体具有挑战性，这对于评估人类活动的环境影响（如甲烷排放）至关重要。", "method": "1. 构建了一个包含生物消化器的新型数据集，该数据集具有高度不平衡性。2. 开发了一种基于部分的检测方法，该方法考虑了生物消化器的基本子元素以提高初始检测效果。3. 将该方法应用于新区域以建立生物消化器清单。4. 计算了给定区域和时间内归因于这些基础设施的甲烷产量的地理统计估算。", "result": "成功建立了生物消化器的清单，并计算了其甲烷产量的地理统计估算。", "conclusion": "本文提出的基于部分的目标检测方法能够有效应对大规模遥感图像中稀有目标的检测挑战，并为环境影响评估提供了地理统计估算能力。", "translation": "目标检测是计算机视觉在遥感图像中的主要应用之一。尽管遥感数据的可用性日益增加，但其庞大的数据量在检测大地理区域的稀有物体时带来了挑战。矛盾的是，这一常见挑战对于许多应用至关重要，例如大规模估算某些人类活动的环境影响。在本文中，我们通过调查法国生物消化器的甲烷产量和排放来解决这个问题。我们首先引入了一个包含生物消化器的新型数据集，该数据集具有小型训练和验证集，以及一个大型测试集，其中存在高度不平衡性，因为此类站点很少见，导致对象缺失的观测值占多数。我们开发了一种基于部分的方法，该方法考虑了生物消化器的基本子元素以提高初始检测效果。为此，我们将我们的方法应用于新的、未见过的区域，以建立生物消化器的清单。然后，我们计算了在给定区域和给定时间内可归因于这些基础设施的甲烷产量的地理统计估算。", "summary": "本文针对遥感图像中大规模稀有目标检测的挑战，提出了一种基于部分的生物消化器检测方法，以实现大规模甲烷监测。研究团队构建了一个不平衡的生物消化器数据集，并利用生物消化器的子元素信息提升检测性能。该方法成功应用于新区域，建立了生物消化器清单，并进一步进行了甲烷产量的地理统计估算，为环境影响评估提供了支持。", "keywords": "目标检测, 遥感, 甲烷监测, 地理统计, 生物消化器", "comments": "该研究通过引入新颖的基于部分的目标检测方法和专门构建的不平衡数据集，有效解决了大规模遥感数据中稀有目标检测的难题。其创新点在于结合了目标子元素信息以提高检测精度，并最终实现了环境关键指标（甲烷排放）的地理统计估算，具有重要的环境监测和评估应用价值。"}}
{"id": "2411.17103", "title": "Distributed Load Balancing with Workload-Dependent Service Rates", "authors": ["Wenxin Zhang", "Santiago R. Balseiro", "Robert Kleinberg", "Vahab Mirrokni", "Balasubramanian Sivan", "Bartek Wydrowski"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.17103v2", "summary": "We study distributed load balancing in bipartite queueing systems where\nfrontends route jobs to heterogeneous backends with workload-dependent service\nrates. The system's connectivity -- governed by compatibility constraints such\nas data residency or resource requirements -- is represented by an arbitrary\nbipartite graph. Each frontend operates independently without communication\nwith other frontends, and the goal is to minimize the expected average latency\nof all jobs. We propose a closed-loop policy called the Greatest Marginal\nService Rate (GMSR) policy that achieves effective coordination without\nrequiring knowledge of arrival rates.\n  In a discrete-time stochastic model, we show that the behavior of our routing\npolicy converges (almost surely) to the behavior of a fluid model, in the limit\nas job sizes tend to zero and job arrival rates are scaled so that the expected\ntotal volume of jobs arriving per unit time remains fixed. Then, in the fluid\nregime, we demonstrate that the policy attains an $\\epsilon$-suboptimal\nsolution in $O(\\delta + \\log{1/\\epsilon})$ time from $\\delta$-suboptimal\ninitial workloads, which implies global convergence to the centrally\ncoordinated optimal routing. Finally, we analyze the fluid model when the\nsystem is overloaded. We show that GMSR lexicographically maximizes throughput,\nmaximizes the number of stable backends, and minimizes their collective\nworkload.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.17103v2", "cate": "cs.DC", "date": "2024-11-26", "updated": "2025-07-24", "AI": {"title_translation": "具有工作负载依赖服务速率的分布式负载均衡", "tldr": "本文研究了具有工作负载依赖服务速率的分布式负载均衡问题，提出了一种名为最大边际服务速率（GMSR）的闭环策略。该策略在前端独立操作且无需了解到达率的情况下，能够实现有效协调，并在离散时间随机模型中收敛到流体模型。在流体状态下，GMSR策略能快速达到全局最优路由的近似解，并在系统过载时，按字典序最大化吞吐量、稳定后端数量并最小化总工作负载。", "motivation": "在二分队列系统中，前端需要将作业路由到具有工作负载依赖服务速率的异构后端，同时每个前端独立操作且不与其他前端通信，目标是最小化所有作业的预期平均延迟。如何在缺乏全局信息的情况下实现有效协调是一个挑战。", "method": "本文提出了一种闭环策略，名为最大边际服务速率（GMSR）策略。该策略无需了解作业到达率即可实现有效协调。", "result": "在离散时间随机模型中，GMSR路由策略的行为几乎必然收敛到流体模型的行为。在流体状态下，该策略能从$\\\\delta$-次优初始工作负载在$O(\\\\delta + \\\\log{1/\\\\epsilon})$时间内达到$\\\\epsilon$-次优解，这意味着全局收敛到集中协调的最优路由。当系统过载时，GMSR策略能够按字典序最大化吞吐量，最大化稳定后端数量，并最小化它们的集体工作负载。", "conclusion": "GMSR策略在具有工作负载依赖服务速率的分布式负载均衡系统中表现出强大的性能，即使在前端独立运行且缺乏全局信息的情况下，也能实现接近最优的路由，并在过载条件下提供鲁棒的优化。", "translation": "我们研究了二分队列系统中的分布式负载均衡，其中前端将作业路由到具有工作负载依赖服务速率的异构后端。系统的连接性——由数据驻留或资源要求等兼容性约束决定——由任意二分图表示。每个前端独立操作，不与其他前端通信，目标是最小化所有作业的预期平均延迟。我们提出了一种闭环策略，称为最大边际服务速率（GMSR）策略，该策略无需了解到达率即可实现有效协调。\n在离散时间随机模型中，我们表明我们的路由策略的行为（几乎必然）收敛到流体模型的行为，当作业大小趋于零且作业到达率按比例缩放以使每单位时间到达的作业总预期量保持固定时。然后，在流体状态下，我们证明该策略从$\\delta$-次优初始工作负载在$O(\\delta + \\log{1/\\epsilon})$时间内达到$\\epsilon$-次优解，这意味着全局收敛到集中协调的最优路由。最后，我们在系统过载时分析了流体模型。我们表明GMSR按字典序最大化吞吐量，最大化稳定后端数量，并最小化它们的集体工作负载。", "summary": "本文研究了在具有工作负载依赖服务速率的二分队列系统中，前端独立运行且无通信的分布式负载均衡问题。为最小化作业平均延迟，作者提出了一种闭环的最大边际服务速率（GMSR）策略，该策略无需了解到达率。研究表明，在离散时间随机模型中，GMSR行为几乎必然收敛到流体模型。在流体状态下，该策略能快速收敛到全局最优路由的近似解。此外，在系统过载时，GMSR策略能按字典序最大化吞吐量、稳定后端数量并最小化其总工作负载。", "keywords": "分布式负载均衡, 工作负载依赖服务速率, GMSR策略, 队列系统, 流体模型", "comments": "本文的创新点在于提出了GMSR策略，它在分布式负载均衡中实现了有效的协调，尤其是在前端独立操作且无需全局信息（如到达率）的约束下。其理论分析严谨，通过流体模型证明了策略的收敛性、近似最优性以及在过载条件下的优越性能，对实际系统设计具有重要指导意义。"}}
{"id": "2507.18123", "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes", "authors": ["Sedigh Khademi", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila", "Jim Black"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      14 pages", "url": "http://arxiv.org/abs/2507.18123v1", "summary": "The rapid development of COVID-19 vaccines has showcased the global\ncommunitys ability to combat infectious diseases. However, the need for\npost-licensure surveillance systems has grown due to the limited window for\nsafety data collection in clinical trials and early widespread implementation.\nThis study aims to employ Natural Language Processing techniques and Active\nLearning to rapidly develop a classifier that detects potential vaccine safety\nissues from emergency department notes. ED triage notes, containing expert,\nsuccinct vital patient information at the point of entry to health systems, can\nsignificantly contribute to timely vaccine safety signal surveillance. While\nkeyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency\nof vaccination-related ED presentations and their similarity to other reasons\nfor ED visits. NLP offers a more accurate and efficient alternative, albeit\nrequiring annotated data, which is often scarce in the medical field. Active\nlearning optimizes the annotation process and the quality of annotated data,\nwhich can result in faster model implementation and improved model performance.\nThis work combines active learning, data augmentation, and active learning and\nevaluation techniques to create a classifier that is used to enhance vaccine\nsafety surveillance from ED triage notes.", "comment": "14 pages", "pdf_url": "http://arxiv.org/pdf/2507.18123v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "积极评估和学习重要的区别：从急诊分诊记录中检测疫苗安全信号", "tldr": "利用自然语言处理和主动学习技术，从急诊分诊记录中快速检测疫苗安全信号，以增强疫苗上市后监测。", "motivation": "鉴于临床试验中安全数据收集窗口有限以及疫苗早期广泛实施，对上市后监测系统的需求日益增长，以快速检测潜在的疫苗安全问题。", "method": "本研究旨在结合自然语言处理（NLP）技术、主动学习、数据增强以及主动学习和评估技术，开发一个分类器，用于从急诊分诊记录中检测疫苗安全信号，以优化数据标注过程并提高模型性能。", "result": "通过结合主动学习、数据增强和评估技术，创建了一个用于增强急诊分诊记录中疫苗安全监测的分类器。主动学习优化了标注过程和标注数据的质量，从而实现更快的模型部署和改进的模型性能。", "conclusion": "本研究通过结合先进的机器学习和自然语言处理技术，为从急诊分诊记录中快速、准确地检测疫苗安全信号提供了有效途径，从而增强了疫苗上市后监测能力。", "translation": "COVID-19疫苗的快速发展展示了全球社区对抗传染病的能力。然而，由于临床试验中安全数据收集窗口有限以及早期广泛实施，对上市后监测系统的需求日益增长。本研究旨在利用自然语言处理技术和主动学习，快速开发一个分类器，用于从急诊科记录中检测潜在的疫苗安全问题。急诊分诊记录包含专家在卫生系统入口处提供的简洁重要的患者信息，可以显著促进及时的疫苗安全信号监测。虽然基于关键词的分类可能有效，但它可能产生误报，并需要大量的关键词修改。疫苗相关急诊就诊频率低以及与其他急诊就诊原因的相似性加剧了这一问题。NLP提供了一种更准确、更高效的替代方案，尽管需要标注数据，而这在医学领域通常是稀缺的。主动学习优化了标注过程和标注数据的质量，从而可以实现更快的模型部署和改进的模型性能。这项工作结合了主动学习、数据增强以及主动学习和评估技术，创建了一个分类器，用于增强从急诊分诊记录中进行的疫苗安全监测。", "summary": "本研究旨在解决疫苗上市后安全监测的需求，特别是针对COVID-19疫苗。鉴于临床试验数据有限和早期广泛接种，研究提出利用自然语言处理（NLP）和主动学习技术，从急诊分诊记录中快速开发一个分类器，以检测潜在的疫苗安全信号。该方法旨在克服传统关键词方法的局限性，通过优化数据标注和提高模型性能来增强疫苗安全监测。", "keywords": "疫苗安全, 信号检测, 自然语言处理, 主动学习, 急诊分诊记录", "comments": "本论文的创新之处在于将主动学习、数据增强和NLP技术相结合，应用于从非结构化的急诊分诊记录中进行疫苗安全信号检测。这对于解决医学领域标注数据稀缺的问题，并实现快速、高效的上市后药物警戒具有重要意义。该方法有望提高疫苗安全监测的及时性和准确性，对公共卫生具有实际价值。"}}
{"id": "2507.18328", "title": "Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks", "authors": ["Xiao Xu", "Qiong Wu", "Pingyi Fan", "Kezhi Wang", "Nan Cheng", "Wen Chen", "Khaled B. Letaief"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to IEEE TMC", "url": "http://arxiv.org/abs/2507.18328v1", "summary": "In this paper, we consider the fair access problem and the Age of Information\n(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in\nvehicular networks. Specifically, vehicles follow Mode 2 to communicate with\nRoadside Units (RSUs) to obtain accurate data for driving\nassistance.Nevertheless, vehicles often have different velocity when they are\nmoving in adjacent lanes, leading to difference in RSU dwelltime and\ncommunication duration. This results in unfair access to network resources,\npotentially influencing driving safety. To ensure the freshness of received\ndata, the AoI should be analyzed. Mode 2 introduces a novel preemption\nmechanism, necessitating simultaneous optimization of fair access and AoI to\nguarantee timely and relevant data delivery. We propose a joint optimization\nframework for vehicular network, defining a fairness index and employing\nStochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By\nadaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)\nin Mode 2, we address the optimization of fairness and AoI. We apply a large\nlanguage model (LLM)-Based Multi-objective Evolutionary Algorithm Based on\nDecomposition (MOEA/D) to solve this problem. Simulation results demonstrate\nthe effectiveness of our scheme in balancing fair access and minimizing AoI.", "comment": "This paper has been submitted to IEEE TMC", "pdf_url": "http://arxiv.org/pdf/2507.18328v1", "cate": "cs.NI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "增强型速度自适应方案：车载网络中联合公平接入与信息年龄优化", "tldr": "本文提出了一种在5G NR V2I Mode 2车载网络中联合优化公平接入和信息年龄（AoI）的方案，通过自适应调整SPS选择窗口并利用基于LLM的MOEA/D算法，有效平衡了公平接入并最小化了AoI。", "motivation": "在5G NR V2I Mode 2车载网络中，车辆速度差异导致驻留时间不同，造成网络资源分配不公平，并可能影响驾驶安全。同时，为了确保接收数据的时效性，需要分析信息年龄（AoI）。Mode 2引入的抢占机制使得同时优化公平接入和AoI成为必要，以保证及时且相关的数据传输。", "method": "本文提出了一个车载网络联合优化框架，定义了公平性指标，并采用随机混合系统（SHS）建模抢占机制下的AoI。通过自适应调整Mode 2中半持久调度（SPS）的选择窗口，解决了公平性和AoI的优化问题。该问题通过基于大型语言模型（LLM）的多目标演化算法（MOEA/D）求解。", "result": "仿真结果表明，所提出的方案在平衡公平接入和最小化信息年龄（AoI）方面是有效的。", "conclusion": "所提出的增强型速度自适应方案能够有效平衡车载网络中的公平接入和信息年龄，确保数据传输的及时性和相关性，从而提高驾驶安全性。", "translation": "在本文中，我们考虑了车载网络中5G新空口（NR）车-基础设施（V2I）模式2下的公平接入问题和信息年龄（AoI）。具体而言，车辆遵循模式2与路边单元（RSU）通信，以获取用于驾驶辅助的准确数据。然而，车辆在相邻车道行驶时通常具有不同的速度，导致RSU驻留时间和通信持续时间的差异。这导致网络资源的不公平接入，可能影响驾驶安全。为了确保接收数据的时效性，需要分析AoI。模式2引入了一种新颖的抢占机制，因此需要同时优化公平接入和AoI，以保证及时和相关的数据交付。我们提出了一个车载网络联合优化框架，定义了一个公平性指标，并采用随机混合系统（SHS）来建模抢占机制下的AoI。通过自适应调整模式2中半持久调度（SPS）的选择窗口，我们解决了公平性和AoI的优化问题。我们应用基于大型语言模型（LLM）的多目标演化分解算法（MOEA/D）来解决这个问题。仿真结果表明，我们的方案在平衡公平接入和最小化AoI方面是有效的。", "summary": "本文针对5G NR V2I Mode 2车载网络中的公平接入和信息年龄（AoI）问题，提出了一个增强型速度自适应方案。该方案旨在解决车辆速度差异导致的不公平资源接入和数据新鲜度问题。通过定义公平性指标并利用随机混合系统（SHS）建模AoI，并自适应调整半持久调度（SPS）的选择窗口，实现了公平性和AoI的联合优化。研究引入了基于大型语言模型（LLM）的多目标演化算法（MOEA/D）来求解该问题。仿真结果验证了该方案在平衡公平接入和最小化AoI方面的有效性。", "keywords": "车载网络, 公平接入, 信息年龄, 5G NR V2I Mode 2, SPS", "comments": "该论文的创新点在于提出了一个联合优化框架，同时解决了车载网络中公平接入和信息年龄这两个关键问题，并考虑了5G NR V2I Mode 2特有的抢占机制。特别值得注意的是，该研究引入了基于大型语言模型（LLM）的多目标演化算法来求解优化问题，这在车载网络优化领域是一个前沿且具有潜力的尝试，可能为复杂的网络资源管理提供新的解决思路。该方案对于提升自动驾驶和智能交通系统的可靠性和安全性具有重要意义。"}}
{"id": "2507.18449", "title": "Digital Twin Technologies in Predictive Maintenance: Enabling Transferability via Sim-to-Real and Real-to-Sim Transfer", "authors": ["Sizhe Ma", "Katherine A. Flanigan", "Mario Bergés"], "categories": ["cs.CE", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      Accepted and presented at 2024 ASCE International Conference on Computing in Civil Engineering (i3CE 2024)", "url": "http://arxiv.org/abs/2507.18449v1", "summary": "The advancement of the Internet of Things (IoT) and Artificial Intelligence\nhas catalyzed the evolution of Digital Twins (DTs) from conceptual ideas to\nmore implementable realities. Yet, transitioning from academia to industry is\ncomplex due to the absence of standardized frameworks. This paper builds upon\nthe authors' previously established functional and informational requirements\nsupporting standardized DT development, focusing on a crucial aspect:\ntransferability. While existing DT research primarily centers on asset\ntransfer, the significance of \"sim-to-real transfer\" and \"real-to-sim\ntransfer\"--transferring knowledge between simulations and real-world\noperations--is vital for comprehensive lifecycle management in DTs. A key\nchallenge in this process is calibrating the \"reality gap,\" the discrepancy\nbetween simulated predictions and actual outcomes. Our research investigates\nthe impact of integrating a single Reality Gap Analysis (RGA) module into an\nexisting DT framework to effectively manage both sim-to-real and real-to-sim\ntransfers. This integration is facilitated by data pipelines that connect the\nRGA module with the existing components of the DT framework, including the\nhistorical repository and the simulation model. A case study on a pedestrian\nbridge at Carnegie Mellon University showcases the performance of different\nlevels of integration of our approach with an existing framework. With full\nimplementation of an RGA module and a complete data pipeline, our approach is\ncapable of bidirectional knowledge transfer between simulations and real-world\noperations without compromising efficiency.", "comment": "Accepted and presented at 2024 ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2024)", "pdf_url": "http://arxiv.org/pdf/2507.18449v1", "cate": "cs.CE", "date": "2025-05-15", "updated": "2025-05-15", "AI": {"title_translation": "预测性维护中的数字孪生技术：通过虚实迁移和实虚迁移实现可迁移性", "tldr": "本文通过集成现实差距分析（RGA）模块到现有数字孪生框架中，实现了模拟与真实世界操作之间的双向知识迁移，以解决数字孪生在预测性维护中的可迁移性挑战。", "motivation": "现有数字孪生研究主要关注资产迁移，但模拟与真实世界操作之间的知识迁移（虚实迁移和实虚迁移）对数字孪生中全面的生命周期管理至关重要。主要挑战是校准“现实差距”，即模拟预测与实际结果之间的差异。", "method": "研究将一个单一的现实差距分析（RGA）模块集成到现有的数字孪生框架中，通过数据管道连接RGA模块与现有组件（包括历史存储库和仿真模型），以有效管理虚实迁移和实虚迁移。通过卡内基梅隆大学一座人行桥的案例研究来展示该方法与现有框架不同集成水平的性能。", "result": "通过RGA模块的全面实施和完整的数据管道，该方法能够在不影响效率的情况下，实现模拟和现实世界操作之间的双向知识迁移。", "conclusion": "集成RGA模块和完整数据管道能够有效管理数字孪生中的虚实和实虚迁移，实现模拟与真实世界操作之间的双向知识迁移，且不影响效率。", "translation": "物联网（IoT）和人工智能（AI）的进步推动了数字孪生（DTs）从概念性想法演变为更可实现的现实。然而，由于缺乏标准化框架，从学术界到工业界的过渡是复杂的。本文基于作者先前建立的支持标准化DT开发的功能和信息需求，重点关注一个关键方面：可迁移性。虽然现有DT研究主要集中在资产迁移，但“虚实迁移”（sim-to-real transfer）和“实虚迁移”（real-to-sim transfer）——在模拟和现实世界操作之间传递知识——对于DTs中全面的生命周期管理至关重要。在此过程中，一个关键挑战是校准“现实差距”，即模拟预测与实际结果之间的差异。我们的研究调查了将单个现实差距分析（RGA）模块集成到现有DT框架中以有效管理虚实迁移和实虚迁移的影响。这种集成通过数据管道实现，这些管道将RGA模块与DT框架的现有组件（包括历史存储库和仿真模型）连接起来。卡内基梅隆大学一座人行桥的案例研究展示了我们方法与现有框架不同集成水平的性能。通过RGA模块的全面实施和完整的数据管道，我们的方法能够在不影响效率的情况下，实现模拟和现实世界操作之间的双向知识迁移。", "summary": "本文探讨了数字孪生在预测性维护中的可迁移性问题，特别是模拟与真实世界操作之间的知识迁移（虚实迁移和实虚迁移）。为了解决模拟与现实之间的“现实差距”，作者提出将一个现实差距分析（RGA）模块集成到现有数字孪生框架中，通过数据管道实现与历史数据和仿真模型的连接。一项在人行桥上的案例研究表明，全面实施RGA模块和数据管道能够高效地实现模拟与现实之间的双向知识迁移。", "keywords": "数字孪生, 预测性维护, 虚实迁移, 实虚迁移, 现实差距分析", "comments": "本文的创新点在于提出了通过现实差距分析（RGA）模块来解决数字孪生中模拟与现实之间的知识迁移问题，这对于数字孪生在工业应用中的落地和全面生命周期管理至关重要。通过实现双向知识迁移，提升了数字孪生在预测性维护中的实用性和鲁棒性。"}}
{"id": "2502.16943", "title": "MAD-AD: Masked Diffusion for Unsupervised Brain Anomaly Detection", "authors": ["Farzad Beizaee", "Gregory Lodygensky", "Christian Desrosiers", "Jose Dolz"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.16943v3", "summary": "Unsupervised anomaly detection in brain images is crucial for identifying\ninjuries and pathologies without access to labels. However, the accurate\nlocalization of anomalies in medical images remains challenging due to the\ninherent complexity and variability of brain structures and the scarcity of\nannotated abnormal data. To address this challenge, we propose a novel approach\nthat incorporates masking within diffusion models, leveraging their generative\ncapabilities to learn robust representations of normal brain anatomy. During\ntraining, our model processes only normal brain MRI scans and performs a\nforward diffusion process in the latent space that adds noise to the features\nof randomly-selected patches. Following a dual objective, the model learns to\nidentify which patches are noisy and recover their original features. This\nstrategy ensures that the model captures intricate patterns of normal brain\nstructures while isolating potential anomalies as noise in the latent space. At\ninference, the model identifies noisy patches corresponding to anomalies and\ngenerates a normal counterpart for these patches by applying a reverse\ndiffusion process. Our method surpasses existing unsupervised anomaly detection\ntechniques, demonstrating superior performance in generating accurate normal\ncounterparts and localizing anomalies. The code is available at\nhhttps://github.com/farzad-bz/MAD-AD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.16943v3", "cate": "cs.CV", "date": "2025-02-24", "updated": "2025-07-23", "AI": {"title_translation": "MAD-AD：基于掩码扩散的无监督脑异常检测", "tldr": "MAD-AD提出了一种利用掩码扩散模型进行无监督脑图像异常检测的新方法，通过学习正常脑解剖结构并识别异常区域为噪声，实现了优于现有技术的性能。", "motivation": "在没有标签的情况下，对脑图像进行无监督异常检测对于识别损伤和病理至关重要。然而，由于脑结构的内在复杂性和可变性以及带注释异常数据的稀缺性，医学图像中异常的准确局部化仍然具有挑战性。", "method": "MAD-AD提出了一种新方法，在扩散模型中引入掩码机制，利用其生成能力学习正常脑解剖的鲁棒表示。在训练期间，模型仅处理正常脑MRI扫描，并在潜在空间中执行前向扩散过程，向随机选择的补丁特征添加噪声。通过双重目标，模型学习识别哪些补丁是噪声并恢复其原始特征。在推理时，模型识别与异常对应的噪声补丁，并通过应用逆向扩散过程为这些补丁生成正常对应物。", "result": "该方法超越了现有的无监督异常检测技术，在生成准确的正常对应物和局部化异常方面表现出卓越的性能。", "conclusion": "MAD-AD通过引入掩码扩散模型，有效解决了无监督脑异常检测中异常定位的挑战，并取得了优于现有方法的性能。", "translation": "脑图像中的无监督异常检测对于在没有标签的情况下识别损伤和病理至关重要。然而，由于脑结构的固有复杂性和可变性以及带注释异常数据的稀缺性，医学图像中异常的准确局部化仍然具有挑战性。为了解决这一挑战，我们提出了一种新颖的方法，在扩散模型中结合掩码机制，利用其生成能力学习正常脑解剖的鲁棒表示。在训练期间，我们的模型仅处理正常脑MRI扫描，并在潜在空间中执行前向扩散过程，向随机选择的补丁特征添加噪声。遵循双重目标，模型学习识别哪些补丁是噪声并恢复其原始特征。这种策略确保模型捕获正常脑结构的复杂模式，同时将潜在异常隔离为潜在空间中的噪声。在推理时，模型识别与异常对应的噪声补丁，并通过应用逆向扩散过程为这些补丁生成正常对应物。我们的方法超越了现有的无监督异常检测技术，在生成准确的正常对应物和局部化异常方面表现出卓越的性能。代码可在hhttps://github.com/farzad-bz/MAD-AD获取。", "summary": "本研究提出了一种名为MAD-AD的无监督脑异常检测新方法，该方法将掩码机制整合到扩散模型中。通过在潜在空间中对正常脑MRI扫描的随机补丁添加噪声并学习恢复原始特征，模型能够捕获正常脑结构的复杂模式，并将异常识别为噪声。在推理阶段，MAD-AD能够识别异常区域并生成其正常对应物。实验结果表明，该方法在生成准确的正常对应物和局部化异常方面优于现有技术。", "keywords": "无监督异常检测, 脑图像, 扩散模型, 掩码, 异常定位", "comments": "MAD-AD的创新之处在于将掩码机制与扩散模型结合，用于无监督的医学图像异常检测。这种方法巧妙地利用了扩散模型的生成能力来学习正常数据的分布，并将异常视为噪声进行识别。其双重目标训练策略有助于模型更有效地捕捉正常模式的细微之处。该方法在处理稀缺异常数据和复杂脑结构方面显示出巨大潜力，对于临床诊断具有重要意义。"}}
{"id": "2507.17851", "title": "Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability", "authors": ["Xiaoxu Zhu", "Junhua Li"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      20 pages, 9 figures, 2 tables", "url": "http://arxiv.org/abs/2507.17851v1", "summary": "Speech pretrained models contain task-specific information across different\nlayers, but decoupling content and timbre information remains challenging as\nremoving speaker-specific information often causes content loss. Current\nresearch lacks direct metrics to quantify timbre residual in model encodings,\nrelying on indirect evaluation through downstream tasks. This paper addresses\nthese challenges through interpretability-based speaker disentanglement in\nspeech pretraining models. We quantitatively evaluate timbre residual in model\nembeddings and improve speaker disentanglement using interpretive\nrepresentations. Our contributions include: (1) InterpTRQE-SptME Benchmark - a\ntimbre residual recognition framework using interpretability. The benchmark\nconcatenates content embeddings with timbre embeddings for speaker\nclassification, then applies Gradient SHAP Explainer to quantify timbre\nresidual. We evaluate seven speech pretraining model variations. (2)\nInterpTF-SptME method - an interpretability-based timbre filtering approach\nusing SHAP Noise and SHAP Cropping techniques. This model-agnostic method\ntransforms intermediate encodings to remove timbre while preserving content.\nExperiments on VCTK dataset with HuBERT LARGE demonstrate successful content\npreservation and significant speaker disentanglement optimization. Results show\nthe SHAP Noise method can reduce timbre residual from 18.05% to near 0% while\nmaintaining content integrity, contributing to enhanced performance in\ncontent-related speech processing tasks and preventing timbre privacy leakage.", "comment": "20 pages, 9 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.17851v1", "cate": "cs.SD", "date": "2025-07-19", "updated": "2025-07-19", "AI": {"title_translation": "基于可解释性的语音预训练模型说话人解耦", "tldr": "本文提出基于可解释性的方法，量化并减少语音预训练模型中的说话人音色残余，同时保留内容信息。", "motivation": "语音预训练模型中内容和音色信息解耦困难，移除说话人信息常导致内容丢失；现有研究缺乏直接指标量化模型编码中的音色残余，依赖下游任务间接评估。", "method": "本文提出两个主要贡献：1. InterpTRQE-SptME Benchmark：一个使用可解释性（Gradient SHAP Explainer）量化音色残余的识别框架，用于评估七种语音预训练模型变体。2. InterpTF-SptME method：一种基于可解释性的音色过滤方法，利用SHAP Noise和SHAP Cropping技术，该方法与模型无关，可转换中间编码以去除音色同时保留内容。", "result": "在VCTK数据集上使用HuBERT LARGE进行的实验表明，该方法成功保留了内容并显著优化了说话人解耦。SHAP Noise方法可以将音色残余从18.05%降低到接近0%，同时保持内容完整性。", "conclusion": "基于可解释性的说话人解耦方法能有效量化并去除语音预训练模型中的音色残余，同时保留内容信息，有助于提升内容相关语音处理任务性能并防止音色隐私泄露。", "translation": "语音预训练模型在不同层中包含任务特定信息，但解耦内容和音色信息仍然具有挑战性，因为移除说话人特定信息通常会导致内容丢失。当前研究缺乏直接指标来量化模型编码中的音色残余，依赖于通过下游任务进行的间接评估。本文通过基于可解释性的语音预训练模型中的说话人解耦来解决这些挑战。我们量化评估了模型嵌入中的音色残余，并利用可解释性表示改进了说话人解耦。我们的贡献包括：(1) InterpTRQE-SptME 基准——一个使用可解释性进行音色残余识别的框架。该基准将内容嵌入与音色嵌入连接起来进行说话人分类，然后应用梯度SHAP解释器来量化音色残余。我们评估了七种语音预训练模型变体。(2) InterpTF-SptME 方法——一种基于可解释性的音色过滤方法，利用SHAP Noise和SHAP Cropping技术。这种与模型无关的方法转换中间编码以去除音色同时保留内容。在VCTK数据集上使用HuBERT LARGE进行的实验表明，成功保留了内容并显著优化了说话人解耦。结果显示，SHAP Noise方法可以将音色残余从18.05%降低到接近0%，同时保持内容完整性，有助于提高内容相关语音处理任务的性能并防止音色隐私泄露。", "summary": "本文提出一种基于可解释性的方法来解决语音预训练模型中内容与说话人音色解耦的挑战。通过引入InterpTRQE-SptME基准量化音色残余，并利用InterpTF-SptME方法（基于SHAP Noise和SHAP Cropping）进行音色过滤。实验证明该方法能有效去除音色信息（从18.05%降至近0%）同时保持内容完整性，从而提升内容相关任务性能并保护隐私。", "keywords": "语音预训练模型, 说话人解耦, 可解释性, 音色残余, SHAP", "comments": "这篇论文的创新点在于将可解释性技术（特别是SHAP）引入到语音预训练模型的说话人解耦任务中，提供了一种直接量化音色残余并有效去除其影响的新范式。这对于提升语音处理任务的鲁棒性和解决隐私泄露问题具有重要意义。"}}
{"id": "2503.07588", "title": "When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning", "authors": ["Junwei Luo", "Yingying Zhang", "Xue Yang", "Kang Wu", "Qi Zhu", "Lei Liang", "Jingdong Chen", "Yansheng Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 6 figures, 18 tables", "url": "http://arxiv.org/abs/2503.07588v3", "summary": "Efficient vision-language understanding of large Remote Sensing Images (RSIs)\nis meaningful but challenging. Current Large Vision-Language Models (LVLMs)\ntypically employ limited pre-defined grids to process images, leading to\ninformation loss when handling gigapixel RSIs. Conversely, using unlimited\ngrids significantly increases computational costs. To preserve image details\nwhile reducing computational complexity, we propose a text-guided token pruning\nmethod with Dynamic Image Pyramid (DIP) integration. Our method introduces: (i)\na Region Focus Module (RFM) that leverages text-aware region localization\ncapability to identify critical vision tokens, and (ii) a coarse-to-fine image\ntile selection and vision token pruning strategy based on DIP, which is guided\nby RFM outputs and avoids directly processing the entire large imagery.\nAdditionally, existing benchmarks for evaluating LVLMs' perception ability on\nlarge RSI suffer from limited question diversity and constrained image sizes.\nWe construct a new benchmark named LRS-VQA, which contains 7,333 QA pairs\nacross 8 categories, with image length up to 27,328 pixels. Our method\noutperforms existing high-resolution strategies on four datasets using the same\ndata. Moreover, compared to existing token reduction methods, our approach\ndemonstrates higher efficiency under high-resolution settings. Dataset and code\nare in https://github.com/VisionXLab/LRS-VQA.", "comment": "18 pages, 6 figures, 18 tables", "pdf_url": "http://arxiv.org/pdf/2503.07588v3", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-07-24", "AI": {"title_translation": "当大型视觉-语言模型遇上大型遥感图像：从粗到精的文本引导令牌剪枝", "tldr": "本文提出了一种文本引导的令牌剪枝方法，用于高效处理大型遥感图像（RSIs），并通过构建LRS-VQA基准解决了现有评估工具的不足。", "motivation": "当前的大型视觉-语言模型（LVLMs）在处理大型遥感图像（RSIs）时面临信息丢失（使用有限网格）或计算成本过高（使用无限网格）的挑战，导致效率低下。", "method": "本文提出了一种结合动态图像金字塔（DIP）的文本引导令牌剪枝方法。该方法包含：(i) 区域焦点模块（RFM），利用文本感知区域定位能力识别关键视觉令牌；(ii) 基于DIP的从粗到精的图像瓦片选择和视觉令牌剪枝策略，由RFM输出引导以避免直接处理整个大型图像。此外，研究构建了一个新的大规模遥感图像问答基准LRS-VQA，包含7,333个QA对，图像长度最长达27,328像素，以解决现有基准的问题多样性和图像尺寸限制。", "result": "该方法在四个数据集上使用相同数据，性能优于现有高分辨率策略。与现有令牌减少方法相比，在高分辨率设置下，该方法展现出更高的效率。", "conclusion": "该研究提出了一种有效且高效的文本引导令牌剪枝方法，成功解决了大型视觉-语言模型在处理大型遥感图像时的效率和信息完整性问题，并通过引入新的大规模遥感图像问答基准LRS-VQA，为该领域的研究和评估提供了重要支持。", "translation": "高效地理解大型遥感图像（RSIs）的视觉-语言信息既有意义又具挑战性。当前的大型视觉-语言模型（LVLMs）通常采用有限的预定义网格来处理图像，导致在处理千兆像素级RSIs时信息丢失。相反，使用无限网格则会显著增加计算成本。为了在保留图像细节的同时降低计算复杂度，我们提出了一种结合动态图像金字塔（DIP）的文本引导令牌剪枝方法。我们的方法引入了：（i）一个区域焦点模块（RFM），它利用文本感知的区域定位能力来识别关键视觉令牌；以及（ii）一种基于DIP的从粗到精的图像瓦片选择和视觉令牌剪枝策略，该策略由RFM输出引导，避免直接处理整个大型图像。此外，现有用于评估LVLMs在大型RSI上感知能力的基准存在问题多样性有限和图像尺寸受限的问题。我们构建了一个名为LRS-VQA的新基准，其中包含8个类别的7,333个问答对，图像长度最长可达27,328像素。我们的方法在四个数据集上使用相同数据，性能优于现有高分辨率策略。此外，与现有令牌减少方法相比，我们的方法在高分辨率设置下表现出更高的效率。数据集和代码可在https://github.com/VisionXLab/LRS-VQA获取。", "summary": "本文提出了一种名为“从粗到精的文本引导令牌剪枝”的新方法，旨在解决大型视觉-语言模型在处理大型遥感图像时面临的效率和信息丢失问题。该方法通过引入区域焦点模块（RFM）和基于动态图像金字塔（DIP）的粗到精瓦片选择与令牌剪枝策略，实现了在保留图像细节的同时降低计算复杂度。此外，为弥补现有基准的不足，研究者还构建了一个大规模遥感图像问答基准LRS-VQA。实验结果表明，该方法在性能和效率上均优于现有策略。", "keywords": "大型视觉-语言模型, 遥感图像, 令牌剪枝, 文本引导, 动态图像金字塔", "comments": "本文的创新点在于提出了文本引导的令牌剪枝策略，特别是结合了动态图像金字塔和区域焦点模块，有效解决了大型遥感图像处理中的计算效率和信息完整性平衡问题。同时，构建新的大规模高分辨率遥感图像问答基准LRS-VQA，填补了现有评估工具的空白，对该领域的研究具有重要推动作用。"}}
{"id": "2507.17788", "title": "Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking", "authors": ["Ali Vardasbi", "Gustavo Penha", "Claudia Hauff", "Hugues Bouchard"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17788v1", "summary": "When using LLMs to rank items based on given criteria, or evaluate answers,\nthe order of candidate items can influence the model's final decision. This\nsensitivity to item positioning in a LLM's prompt is known as position bias.\nPrior research shows that this bias exists even in large models, though its\nseverity varies across models and tasks. In addition to position bias, LLMs\nalso exhibit varying degrees of low repetition consistency, where repeating the\nLLM call with the same candidate ordering can lead to different rankings. To\naddress both inconsistencies, a common approach is to prompt the model multiple\ntimes with different candidate orderings and aggregate the results via majority\nvoting. However, this repetition strategy, significantly increases\ncomputational costs. Extending prior findings, we observe that both the\ndirection -- favoring either the earlier or later candidate in the prompt --\nand magnitude of position bias across instances vary substantially, even within\na single dataset. This observation highlights the need for a per-instance\nmitigation strategy. To this end, we introduce a dynamic early-stopping method\nthat adaptively determines the number of repetitions required for each\ninstance. Evaluating our approach across three LLMs of varying sizes and on two\ntasks, namely re-ranking and alignment, we demonstrate that transitioning to a\ndynamic repetition strategy reduces the number of LLM calls by an average of\n81%, while preserving the accuracy. Furthermore, we propose a confidence-based\nadaptation to our early-stopping method, reducing LLM calls by an average of\n87% compared to static repetition, with only a slight accuracy trade-off\nrelative to our original early-stopping method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17788v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "自适应重复以减轻基于LLM的排名中的位置偏差", "tldr": "LLM在排名时存在位置偏差和重复一致性低的问题，现有重复调用方法计算成本高。本文提出一种自适应的动态早停方法，能够显著减少LLM调用次数，同时保持准确性。", "motivation": "LLM在排名或评估时，候选项目的顺序会影响其最终决策，产生位置偏差。同时，LLM也存在低重复一致性问题，即重复调用可能产生不同排名。虽然通过多次重复调用并聚合结果可以缓解这些问题，但这会显著增加计算成本。此外，位置偏差的方向和大小在不同实例间差异很大，需要一个针对每个实例的缓解策略。", "method": "本文引入了一种动态早停方法，能够自适应地确定每个实例所需的重复次数。在此基础上，进一步提出了一种基于置信度的适应性早停方法。", "result": "在三种不同大小的LLM和两个任务（重排序和对齐）上的评估表明，动态重复策略平均减少了81%的LLM调用次数，同时保持了准确性。基于置信度的适应性早停方法与静态重复相比，平均减少了87%的LLM调用次数，而相对于原始的早停方法，准确性仅有轻微的权衡。", "conclusion": "通过引入动态早停和置信度适应性方法，可以显著减少LLM在排名任务中的计算成本，同时有效缓解位置偏差和提高重复一致性，且保持高准确性。", "translation": "当使用LLM根据给定标准对项目进行排名或评估答案时，候选项目的顺序会影响模型的最终决策。LLM提示中项目定位的这种敏感性被称为位置偏差。先前的研究表明，即使在大型模型中也存在这种偏差，尽管其严重程度因模型和任务而异。除了位置偏差，LLM还表现出不同程度的低重复一致性，即以相同的候选顺序重复调用LLM可能会导致不同的排名。为了解决这两种不一致性，一种常见的方法是使用不同的候选顺序多次提示模型，并通过多数投票聚合结果。然而，这种重复策略显著增加了计算成本。在扩展先前发现的基础上，我们观察到在单个数据集中，实例间位置偏差的方向（偏向提示中的较早或较晚的候选）和大小差异很大。这一观察突出了对每个实例进行缓解策略的必要性。为此，我们引入了一种动态早停方法，自适应地确定每个实例所需的重复次数。我们在三种不同大小的LLM上以及在重排序和对齐两个任务上评估了我们的方法，结果表明，转向动态重复策略平均减少了81%的LLM调用次数，同时保持了准确性。此外，我们提出了一种基于置信度的早停方法，与静态重复相比，平均减少了87%的LLM调用次数，而相对于我们最初的早停方法，准确性仅有轻微的权衡。", "summary": "本文提出了一种自适应重复的动态早停方法，旨在减轻LLM在排名任务中面临的位置偏差和低重复一致性问题。针对现有重复聚合方法计算成本高昂且位置偏差因实例而异的挑战，该方法能够根据每个实例的需要动态确定LLM的调用次数。实验结果表明，该方法在保持准确性的同时，显著减少了LLM的调用次数（平均减少81%至87%），从而有效提高了LLM排名过程的效率。", "keywords": "LLM, 位置偏差, 排名, 自适应重复, 动态早停", "comments": "这篇论文的创新点在于提出了一个针对LLM位置偏差和重复一致性问题的“每实例”缓解策略，即动态早停方法。它解决了传统静态重复方法计算成本高的问题，通过自适应地确定所需重复次数，显著提高了效率。这种动态策略对于实际部署LLM应用具有重要意义，因为它能在保证性能的同时大幅降低资源消耗。"}}
{"id": "2404.11557", "title": "Spatio-Temporal Motion Retargeting for Quadruped Robots", "authors": ["Taerim Yoon", "Dongho Kang", "Seungmin Kim", "Jin Cheng", "Minsung Ahn", "Stelian Coros", "Sungjoon Choi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 12 figures, videos available at this https URL", "url": "http://arxiv.org/abs/2404.11557v3", "summary": "This work presents a motion retargeting approach for legged robots, aimed at\ntransferring the dynamic and agile movements to robots from source motions. In\nparticular, we guide the imitation learning procedures by transferring motions\nfrom source to target, effectively bridging the morphological disparities while\nensuring the physical feasibility of the target system. In the first stage, we\nfocus on motion retargeting at the kinematic level by generating kinematically\nfeasible whole-body motions from keypoint trajectories. Following this, we\nrefine the motion at the dynamic level by adjusting it in the temporal domain\nwhile adhering to physical constraints. This process facilitates policy\ntraining via reinforcement learning, enabling precise and robust motion\ntracking. We demonstrate that our approach successfully transforms noisy motion\nsources, such as hand-held camera videos, into robot-specific motions that\nalign with the morphology and physical properties of the target robots.\nMoreover, we demonstrate terrain-aware motion retargeting to perform BackFlip\non top of a box. We successfully deployed these skills to four robots with\ndifferent dimensions and physical properties in the real world through hardware\nexperiments.", "comment": "20 pages, 12 figures, videos available at\n  https://taerimyoon.me/Spatio-Temporal-Motion-Retargeting-for-Quadruped-Robots/", "pdf_url": "http://arxiv.org/pdf/2404.11557v3", "cate": "cs.RO", "date": "2024-04-17", "updated": "2025-07-24", "AI": {"title_translation": "四足机器人时空运动重定向", "tldr": "本文提出了一种针对四足机器人的运动重定向方法，能够将动态敏捷的运动从源动作转移到目标机器人上，有效弥合形态差异并确保物理可行性，并已在真实世界的多种机器人上成功部署。", "motivation": "旨在将动态敏捷的运动从源动作转移到机器人上，并有效弥合形态差异，同时确保目标系统的物理可行性。", "method": "本方法分两阶段进行：首先在运动学层面进行运动重定向，从关键点轨迹生成运动学上可行的全身运动；然后，在时域上调整运动以满足物理约束，从而在动力学层面进行细化。此过程通过强化学习促进策略训练，实现精确和鲁棒的运动跟踪。", "result": "该方法成功地将手持相机视频等噪声运动源转换为与目标机器人形态和物理特性相符的机器人特定运动。此外，它还展示了地形感知运动重定向，例如在箱子上进行后空翻。这些技能已通过硬件实验成功部署到四种不同尺寸和物理特性的真实世界机器人上。", "conclusion": "本研究提出的运动重定向方法能够成功地将包含噪声的源运动转化为适用于不同形态和物理特性的四足机器人的精确且鲁棒的动作，并通过真实世界的硬件实验验证了其有效性。", "translation": "这项工作提出了一种针对腿式机器人的运动重定向方法，旨在将动态和敏捷的运动从源动作转移到机器人上。特别是，我们通过将动作从源头转移到目标来指导模仿学习过程，有效地弥合形态差异，同时确保目标系统的物理可行性。在第一阶段，我们通过从关键点轨迹生成运动学上可行的全身运动，专注于运动学层面的运动重定向。在此之后，我们通过在时间域中调整运动并遵守物理约束，在动力学层面细化运动。这个过程通过强化学习促进策略训练，实现精确和鲁棒的运动跟踪。我们证明了我们的方法成功地将噪声运动源（例如手持相机视频）转换为与目标机器人的形态和物理特性相符的机器人特定运动。此外，我们展示了地形感知运动重定向，以在箱子上执行后空翻。我们通过硬件实验成功地将这些技能部署到真实世界中四种不同尺寸和物理特性的机器人上。", "summary": "本文提出了一种用于四足机器人的时空运动重定向方法，旨在将动态、敏捷的源动作转移到目标机器人上，同时解决形态差异和物理可行性问题。该方法分两阶段进行：首先在运动学层面生成可行的全身运动，然后通过在时间域调整并遵守物理约束来在动力学层面进行细化。通过强化学习进行策略训练，实现了对噪声源的精确鲁棒运动跟踪。实验证明，该方法能将包括手持视频在内的噪声源转换为适应机器人形态的动作，并成功在多种真实世界机器人上部署了如地形感知后空翻等技能。", "keywords": "运动重定向, 四足机器人, 强化学习, 模仿学习, 时空运动", "comments": "该论文的创新点在于其两阶段的运动重定向方法，即先进行运动学层面的处理，再进行动力学层面的精炼，这有效地解决了源动作与目标机器人之间存在的形态差异和物理可行性问题。此外，该方法能够处理来自手持摄像机视频等噪声源的动作，并成功部署到真实世界的多种四足机器人上，这显示了其强大的鲁棒性和实际应用潜力。该研究对于实现复杂、动态的机器人行为具有重要意义。"}}
{"id": "2504.02250", "title": "Designing Effective Human-Swarm Interaction Interfaces: Insights from a User Study on Task Performance", "authors": ["Wasura D. Wattearachchi", "Erandi Lakshika", "Kathryn Kasmarik", "Michael Barlow"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures, 5 tables", "url": "http://arxiv.org/abs/2504.02250v2", "summary": "In this paper, we present a systematic method of design for human-swarm\ninteraction interfaces, combining theoretical insights with empirical\nevaluation. We first derived ten design principles from existing literature,\napplying them to key information dimensions identified through goal-directed\ntask analysis and developed a tablet-based interface for a target search task.\nWe then conducted a user study with 31 participants where humans were required\nto guide a robotic swarm to a target in the presence of three types of hazards\nthat pose a risk to the robots: Distributed, Moving, and Spreading. Performance\nwas measured based on the proximity of the robots to the target and the number\nof deactivated robots at the end of the task. Results indicate that at least\none robot was brought closer to the target in 98% of tasks, demonstrating the\ninterface's success in fulfilling the primary objective of the task.\nAdditionally, in nearly 67% of tasks, more than 50% of the robots reached the\ntarget. Moreover, particularly better performance was noted in moving hazards.\nAdditionally, the interface appeared to help minimise robot deactivation, as\nevidenced by nearly 94% of tasks where participants managed to keep more than\n50% of the robots active, ensuring that most of the swarm remained operational.\nHowever, its effectiveness varied across hazards, with robot deactivation being\nlowest in distributed hazard scenarios, suggesting that the interface provided\nthe most support in these conditions.", "comment": "8 pages, 4 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2504.02250v2", "cate": "cs.HC", "date": "2025-04-03", "updated": "2025-07-24", "AI": {"title_translation": "设计有效的人机群交互界面：一项关于任务绩效的用户研究的见解", "tldr": "本文提出了一种设计人机群交互界面的系统方法，结合理论和实证评估，通过用户研究验证了所设计界面在引导机器人群完成目标搜索任务中的有效性，特别是在降低机器人失活方面。", "motivation": "本文旨在设计一种有效的人机群交互界面，以帮助人类更好地引导机器人群完成任务。", "method": "研究首先从现有文献中提炼出十项设计原则，并将其应用于通过目标导向任务分析确定的关键信息维度。接着，开发了一个基于平板的界面，用于目标搜索任务。随后，进行了一项包含31名参与者的用户研究，要求参与者在存在三种危险（分布式、移动、扩散）的情况下引导机器人群搜索目标。性能通过机器人与目标的接近程度和任务结束时失活的机器人数量来衡量。", "result": "结果显示，98%的任务中至少有一个机器人被带到更接近目标的位置；近67%的任务中，超过50%的机器人到达目标；在移动危险中表现尤其好。此外，在近94%的任务中，参与者成功地使超过50%的机器人保持活跃，最大限度地减少了机器人失活。然而，界面的有效性因危险类型而异，在分布式危险场景中机器人失活率最低。", "conclusion": "所设计的界面在帮助人类引导机器人群完成目标搜索任务方面是成功的，特别是在确保机器人群保持活跃和接近目标方面。其有效性在不同危险类型下有所不同，在分布式危险情景下提供了最大的支持。", "translation": "在本文中，我们提出了一种系统化的人机群交互界面设计方法，结合了理论见解和实证评估。我们首先从现有文献中推导出十项设计原则，并将其应用于通过目标导向任务分析确定的关键信息维度，开发了一个用于目标搜索任务的平板界面。然后，我们对31名参与者进行了一项用户研究，其中要求人类在存在三种对机器人构成风险的危险（分布式、移动和扩散）的情况下引导机器人群寻找目标。性能根据机器人与目标的接近程度以及任务结束时失活的机器人数量进行衡量。结果表明，在98%的任务中，至少有一个机器人被带到更接近目标的位置，这表明界面在实现任务主要目标方面的成功。此外，在近67%的任务中，超过50%的机器人到达了目标。而且，在移动危险中表现尤其出色。此外，界面似乎有助于最大限度地减少机器人失活，这体现在近94%的任务中，参与者成功地保持了超过50%的机器人活跃，确保了大部分机器人群保持运行。然而，其有效性因危险而异，在分布式危险场景中机器人失活率最低，这表明界面在这些条件下提供了最大的支持。", "summary": "本文提出了一种结合理论和实证评估的人机群交互界面系统设计方法。研究从现有文献中提炼出设计原则，并基于此开发了一个平板界面用于目标搜索任务。通过一项31名参与者的用户研究，验证了该界面在引导机器人群接近目标和保持机器人活跃方面的有效性，尤其在分布式危险场景下表现最佳。", "keywords": "人机群交互, 界面设计, 用户研究, 机器人群, 任务性能", "comments": "该论文创新性地将理论设计原则与实证用户研究相结合，系统地探讨了人机群交互界面的设计。其通过具体的用户研究量化了界面在任务性能和机器人存活率方面的效果，为未来人机群系统设计提供了宝贵的实践指导和数据支持。特别关注不同危险类型下的表现，揭示了界面有效性的边界。"}}
{"id": "2507.18448", "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language", "authors": ["Md Obyedullahil Mamun", "Md Adyelullahil Mamun", "Arif Ahmad", "Md. Imran Hossain Emu"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18448v1", "summary": "Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18448v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "恢复节奏：使用Transformer模型对低资源语言孟加拉语进行标点符号恢复", "tldr": "本研究使用基于Transformer的模型（XLM-RoBERTa-large）对孟加拉语文本进行标点符号恢复，通过数据增强技术解决了资源稀缺问题，并在不同数据集上取得了高准确率，为低资源NLP研究提供了基线和资源。", "motivation": "标点符号恢复能提高文本可读性，对自动语音识别（ASR）的后处理至关重要，尤其对于孟加拉语等低资源语言。", "method": "本研究应用基于Transformer的模型，特别是XLM-RoBERTa-large，自动恢复无标点孟加拉语文本中的标点符号。重点预测句号、逗号、问号和感叹号四种标点符号。为解决标注资源稀缺问题，构建了一个大型多样化的训练语料库并应用了数据增强技术。", "result": "最佳模型在数据增强因子alpha = 0.20%的情况下训练，在新闻测试集上达到了97.1%的准确率，在参考集上达到91.2%，在ASR集上达到90.2%。结果表明模型对参考和ASR转录本具有很强的泛化能力，证明了其在真实世界、嘈杂场景中的有效性。", "conclusion": "这项工作为孟加拉语标点符号恢复建立了强大的基线，并贡献了可公开获取的数据集和代码，以支持低资源自然语言处理的未来研究。", "translation": "标点符号恢复增强了文本的可读性，对于自动语音识别（ASR）的后处理任务至关重要，特别是对于孟加拉语等低资源语言。在这项研究中，我们探索了基于Transformer的模型，特别是XLM-RoBERTa-large，在无标点孟加拉语文本中自动恢复标点符号的应用。我们专注于预测四种标点符号：句号、逗号、问号和感叹号，涵盖不同的文本领域。为了解决标注资源稀缺的问题，我们构建了一个大型、多样化的训练语料库并应用了数据增强技术。我们表现最佳的模型，在数据增强因子alpha = 0.20%的情况下进行训练，在新闻测试集上达到了97.1%的准确率，在参考集上达到了91.2%，在ASR集上达到了90.2%。结果显示出对参考和ASR转录本的强大泛化能力，证明了该模型在真实世界、嘈杂场景中的有效性。这项工作为孟加拉语标点符号恢复建立了强大的基线，并贡献了可公开获取的数据集和代码，以支持低资源自然语言处理的未来研究。", "summary": "本研究旨在解决孟加拉语这一低资源语言的标点符号恢复问题，该问题对于提高文本可读性和ASR后处理至关重要。研究采用基于Transformer的模型XLM-RoBERTa-large，并针对资源稀缺问题，通过构建大型语料库和应用数据增强技术进行训练。模型在不同测试集上取得了高准确率，验证了其在真实场景下的有效性，并为低资源NLP提供了重要的基线、数据集和代码。", "keywords": "标点符号恢复, Transformer模型, 孟加拉语, 低资源语言, 数据增强", "comments": "该论文的创新之处在于将Transformer模型应用于低资源语言孟加拉语的标点恢复，并有效利用数据增强技术克服了资源限制。其重要性体现在为低资源NLP领域提供了强大的基线模型、公开数据集和代码，对未来的相关研究具有重要的推动作用。"}}
{"id": "2507.18305", "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit", "authors": ["Biao Yi", "Zekun Fei", "Jianing Geng", "Tong Li", "Lihai Nie", "Zheli Liu", "Yiming Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18305v1", "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18305v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "BadReasoner：将可调式过度思考后门植入大型推理模型以供娱乐或盈利", "tldr": "该研究提出了一种名为“过度思考后门”的新型攻击，通过数据投毒将可调式冗余推理植入大型推理模型（LRMs），使其生成冗长但不影响正确性的思考链，从而消耗资源。", "motivation": "该研究旨在识别并探索一种针对大型推理模型（LRMs）的新型攻击向量，称之为“过度思考后门”，该攻击利用LRMs的思维链（CoT）推理能力，旨在精确控制模型推理的冗余程度。", "method": "该方法通过一种新颖的数据投毒方法实现可调式过度思考后门。它将一个可调式触发器（重复次数指示所需强度）与一个相应冗长的思维链（CoT）响应配对。这些响应由一个教师LLM通过注入受控数量的冗余细化步骤到正确的推理过程中程序化生成。此方法旨在保持输出正确性，以确保隐蔽性并将其确立为纯粹的资源消耗攻击。", "result": "在各种大型推理模型（LRMs）上的大量实证结果表明，该方法能够可靠地触发推理过程长度的可控、多倍增加，同时不降低最终答案的正确性。", "conclusion": "该论文成功展示了一种新颖的“过度思考后门”攻击，该攻击针对大型推理模型（LRMs），能够精确控制推理的冗余程度，导致资源消耗增加，而不会影响输出的正确性。", "translation": "大型推理模型（LRMs）已成为人工智能领域的一项重大进展，它们是专门设计用于解决复杂推理任务的一类大型语言模型（LLMs）。LRMs的决定性特征在于其广泛的思维链（CoT）推理能力。在本文中，我们识别出一种以前未被探索过的针对LRMs的攻击向量，我们称之为“过度思考后门”。我们通过提出一种新颖的可调式后门来推进这一概念，它超越了简单的开/关攻击，攻击者可以精确控制模型推理的冗余程度。我们的攻击通过一种新颖的数据投毒方法实现。它将一个可调式触发器（其中重复次数表示所需的强度）与一个相应冗长的CoT响应配对。这些响应是通过指示一个教师LLM将受控数量的冗余细化步骤注入到正确的推理过程中来程序化生成的。该方法保持了输出的正确性，从而确保了隐蔽性，并将该攻击确立为一种纯粹的资源消耗向量。对各种LRMs进行的广泛实证结果表明，我们的方法可以可靠地触发推理过程长度的可控、多倍增加，而不会降低最终答案的正确性。我们的源代码可在https://github.com/FZaKK/BadReasoner获取。", "summary": "本文介绍了一种针对大型推理模型（LRMs）的新型攻击，称为“过度思考后门”。该攻击利用了LRMs的思维链（CoT）推理能力，旨在通过数据投毒方法，在不影响最终答案正确性的前提下，精确控制模型生成冗余且过长的推理过程，从而实现资源消耗。实验证明，该方法能可靠地增加推理过程的长度。", "keywords": "大型推理模型, 过度思考后门, 数据投毒, 思维链, 资源消耗", "comments": "这篇论文提出了一种创新且隐蔽的攻击向量，针对大型推理模型（LRMs），其重点在于资源消耗而非准确性降低。后门的可调性是一个显著的进步，允许对攻击强度进行精细控制。它揭示了思维链（CoT）推理中一个新的漏洞，并对部署LRMs的效率和成本效益产生了影响。"}}
{"id": "2503.18273", "title": "Analyzing Islamophobic Discourse Using Semi-Coded Terms and LLMs", "authors": ["Raza Ul Mustafa", "Roi Dupart", "Gabrielle Smith", "Noman Ashraf", "Nathalie Japkowicz"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18273v2", "summary": "In recent years, Islamophobia has gained significant traction across Western\nsocieties, fueled by the rise of digital communication networks. This paper\nperforms a large-scale analysis of specialized, semi-coded Islamophobic terms\nsuch as (muzrat, pislam, mudslime, mohammedan, muzzies) floated on extremist\nsocial platforms, i.e., 4Chan, Gab, Telegram, etc. Many of these terms appear\nlexically neutral or ambiguous outside of specific contexts, making them\ndifficult for both human moderators and automated systems to reliably identify\nas hate speech. First, we use Large Language Models (LLMs) to show their\nability to understand these terms. Second, Google Perspective API suggests that\nIslamophobic posts tend to receive higher toxicity scores than other categories\nof hate speech like Antisemitism. Finally, we use BERT topic modeling approach\nto extract different topics and Islamophobic discourse on these social\nplatforms. Our findings indicate that LLMs understand these Out-Of-Vocabulary\n(OOV) slurs; however, further improvements in moderation strategies and\nalgorithmic detection are necessary to address such discourse effectively. Our\ntopic modeling also indicates that Islamophobic text is found across various\npolitical, conspiratorial, and far-right movements and is particularly directed\nagainst Muslim immigrants. Taken altogether, we performed one of the first\nstudies on Islamophobic semi-coded terms and shed a global light on\nIslamophobia.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18273v2", "cate": "cs.LG", "date": "2025-03-24", "updated": "2025-07-24", "AI": {"title_translation": "使用半编码术语和大型语言模型分析仇视伊斯兰的言论", "tldr": "本研究对极端社交平台上出现的仇视伊斯兰的半编码术语进行了大规模分析，发现大型语言模型能理解这些术语，且仇视伊斯兰的帖子毒性得分较高，并揭示了相关言论的主题，强调需要改进审核策略和算法检测。", "motivation": "近年来，仇视伊斯兰的言论在西方社会中因数字通信网络的兴起而日益盛行。许多仇视伊斯兰的半编码术语在特定语境之外显得中性或模糊，这使得人工审核员和自动化系统难以可靠地将其识别为仇恨言论。因此，本研究旨在对这些术语进行大规模分析，并探讨LLMs识别这些术语的能力。", "method": "1. 使用大型语言模型（LLMs）来展示其理解这些半编码术语的能力。2. 使用Google Perspective API来评估仇视伊斯兰帖子与反犹太主义等其他仇恨言论类别的毒性得分。3. 使用BERT主题建模方法来提取这些社交平台上仇视伊斯兰言论的不同主题。", "result": "1. 大型语言模型（LLMs）能够理解这些超出词汇表（OOV）的侮辱性词语。2. 仇视伊斯兰的帖子往往比反犹太主义等其他类别的仇恨言论获得更高的毒性分数。3. 主题建模表明，仇视伊斯兰的文本存在于各种政治、阴谋论和极右翼运动中，并且特别针对穆斯林移民。", "conclusion": "LLMs能够理解仇视伊斯兰的半编码术语，但仍需要进一步改进审核策略和算法检测以有效处理此类言论。仇视伊斯兰的言论广泛存在于不同运动中，并主要针对穆斯林移民。本研究是首批针对仇视伊斯兰半编码术语的研究之一，为全球仇视伊斯兰现象提供了新的视角。", "translation": "近年来，仇视伊斯兰的言论在西方社会中因数字通信网络的兴起而日益盛行。本文对在极端社交平台（如4Chan、Gab、Telegram等）上流传的专业化、半编码的仇视伊斯兰术语（如muzrat、pislam、mudslime、mohammedan、muzzies）进行了大规模分析。这些术语中有许多在特定语境之外在词汇上显得中性或模糊，这使得人工审核员和自动化系统都难以可靠地将其识别为仇恨言论。首先，我们使用大型语言模型（LLMs）来展示它们理解这些术语的能力。其次，Google Perspective API表明仇视伊斯兰的帖子往往比反犹太主义等其他仇恨言论类别获得更高的毒性分数。最后，我们使用BERT主题建模方法来提取这些社交平台上的不同主题和仇视伊斯兰言论。我们的发现表明LLMs理解这些超出词汇表（OOV）的侮辱性词语；然而，为了有效处理此类言论，审核策略和算法检测仍需要进一步改进。我们的主题建模还表明，仇视伊斯兰的文本存在于各种政治、阴谋论和极右翼运动中，并且特别针对穆斯林移民。总而言之，我们对仇视伊斯兰的半编码术语进行了首次研究之一，并对仇视伊斯兰现象进行了全球性的揭示。", "summary": "本研究对4Chan、Gab、Telegram等极端社交平台上出现的仇视伊斯兰半编码术语进行了大规模分析。鉴于这些术语在特定语境外难以识别，研究首先利用大型语言模型（LLMs）验证其理解能力，并发现LLMs能识别这些超出词汇表的侮辱性词语。其次，通过Google Perspective API分析，发现仇视伊斯兰的帖子比其他仇恨言论具有更高的毒性得分。最后，采用BERT主题建模揭示了仇视伊斯兰言论在不同政治、阴谋论和极右翼运动中的存在，并指出其主要针对穆斯林移民。研究强调，尽管LLMs表现出理解能力，但仍需进一步改进审核策略和算法检测来有效应对此类言论。", "keywords": "仇视伊斯兰, 半编码术语, 大型语言模型, 仇恨言论检测, 主题建模", "comments": "这项研究具有重要的创新性和现实意义。它首次大规模分析了难以识别的“半编码”仇视伊斯兰术语，填补了该领域的空白。通过结合LLMs的理解能力、Google Perspective API的毒性评估以及BERT主题建模，研究方法全面且多维度。发现LLMs能够理解这些OOV（Out-Of-Vocabulary）侮辱性词语，这表明了LLMs在仇恨言论检测中的巨大潜力。同时，研究也指出了当前审核策略和算法检测的不足，为未来改进提供了明确方向。此外，揭示仇视伊斯兰言论与政治、阴谋论和极右翼运动的关联，并指出其主要针对穆斯林移民，为理解和打击仇视伊斯兰现象提供了宝贵的社会洞察。"}}
{"id": "2507.17754", "title": "A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians", "authors": ["Justin Morse", "Kurt Gilbert", "Kyle Shin", "Rick Cooke", "Peyton Rose", "Jack Sullivan", "Angelo Sisante"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17754v1", "summary": "Clinician burnout has motivated the growing adoption of ambient medical\nscribes in the clinic. In this work, we introduce a custom-built ambient scribe\napplication integrated into the EHR system at Included Health, a personalized\nall-in-one healthcare company offering telehealth services. The application\nuses Whisper for transcription and a modular in-context learning pipeline with\nGPT-4o to automatically generate SOAP notes and patient instructions. Testing\non mock visit data shows that the notes generated by the application exceed the\nquality of expert-written notes as determined by an LLM-as-a-judge. The\napplication has been widely adopted by the clinical practice, with over 540\nclinicians at Included Health using the application at least once. 94% (n = 63)\nof surveyed clinicians report reduced cognitive load during visits and 97% (n =\n66) report less documentation burden when using the application. Additionally,\nwe show that post-processing notes with a fine-tuned BART model improves\nconciseness. These findings highlight the potential for AI systems to ease\nadministrative burdens and support clinicians in delivering efficient,\nhigh-quality care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17754v1", "cate": "cs.HC", "date": "2025-05-02", "updated": "2025-05-02", "AI": {"title_translation": "定制化环境抄写员降低远程医疗临床医生的认知负荷和文档负担", "tldr": "开发了一种定制环境抄写应用，利用Whisper和GPT-4o自动生成医疗笔记，有效减轻了远程医疗临床医生的认知负荷和文档负担，并提高了笔记质量。", "motivation": "临床医生职业倦怠以及减轻认知负荷和文档负担的需求，促使人们寻求新的解决方案。", "method": "引入了一个定制化的环境抄写应用，该应用集成到Included Health的EHR系统。它使用Whisper进行转录，并结合GPT-4o的模块化上下文学习管道自动生成SOAP笔记和患者指导。此外，使用微调的BART模型对笔记进行后处理以提高简洁性。", "result": "在模拟就诊数据上测试显示，该应用生成的笔记质量超过了专家手写的笔记（由LLM作为评判员判定）。该应用已被广泛采用，Included Health超过540名临床医生至少使用过一次。94%（n=63）的受访临床医生报告在就诊期间认知负荷降低，97%（n=66）报告在使用该应用时文档负担减轻。", "conclusion": "AI系统有潜力减轻行政负担，并支持临床医生提供高效、高质量的护理。", "translation": "临床医生职业倦怠促使环境医疗抄写员在诊所中日益普及。在这项工作中，我们介绍了一种定制的环境抄写应用程序，该应用程序集成到Included Health的EHR系统，Included Health是一家提供远程医疗服务的个性化一体化医疗公司。该应用程序使用Whisper进行转录，并采用模块化上下文学习管道与GPT-4o结合，自动生成SOAP笔记和患者指导。在模拟就诊数据上的测试表明，该应用程序生成的笔记质量超过了专家手写的笔记，这由LLM作为评判员确定。该应用程序已被临床实践广泛采用，Included Health有超过540名临床医生至少使用过一次。94%（n=63）的受访临床医生报告在使用该应用程序时，就诊期间的认知负荷降低，97%（n=66）报告文档负担减轻。此外，我们还展示了使用微调的BART模型对笔记进行后处理可以提高简洁性。这些发现突显了AI系统在减轻行政负担和支持临床医生提供高效、高质量护理方面的潜力。", "summary": "本文介绍了一款为远程医疗临床医生定制的环境抄写应用，该应用整合了Whisper和GPT-4o，能够自动生成高质量的SOAP笔记和患者指导。经测试，该应用有效减轻了临床医生的认知负荷和文档负担，并提高了笔记质量，展示了AI在医疗行政支持方面的巨大潜力。", "keywords": "环境抄写员, 远程医疗, 认知负荷, 文档负担, AI医疗", "comments": "这项工作创新性地将先进的AI模型（如Whisper和GPT-4o）应用于医疗抄写领域，并成功集成了EHR系统，解决了远程医疗中临床医生面临的实际痛点。其重要性在于通过实证数据（用户反馈和笔记质量评估）证明了AI在减轻医疗行政负担和提升护理效率方面的有效性，为未来智能医疗辅助系统提供了有力的案例。"}}
{"id": "2507.18550", "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)", "authors": ["Manuel de Sousa Ribeiro", "Afonso Leote", "João Leite"], "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Extended version of the paper published in Proceedings of the European Conference on Artificial Intelligence (ECAI 2025)", "url": "http://arxiv.org/abs/2507.18550v1", "summary": "Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.", "comment": "Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18550v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "关于概念探测的性能：数据的影响（扩展版）", "tldr": "本文研究了用于训练概念探测模型的数据对其性能的影响，并为两个常用数据集提供了概念标签。", "motivation": "概念探测是解释大型、亚符号化人工神经网络的一种方法。现有研究主要关注被探测模型或探测模型本身，而对训练探测模型所需的数据关注不足。本文旨在弥补这一空白。", "method": "本文在图像分类任务的背景下，研究了用于训练探测模型的数据对其性能的影响。", "result": "本文提供了两个广泛使用的数据集的概念标签。关于数据影响性能的具体结果细节未在摘要中提及。", "conclusion": "Not mentioned in abstract", "translation": "概念探测作为一种帮助解释人工神经网络的方法，最近引起了越来越多的兴趣，它解决了网络通常庞大的规模和亚符号性质，这最终使得它们无法直接进行人工解释。概念探测的工作原理是训练额外的分类器，将模型的内部表示映射到人类感兴趣的概念中，从而使人类能够窥探人工神经网络的内部。对概念探测的研究主要集中在被探测模型或探测模型本身上，而对训练此类探测模型所需的数据关注有限。在本文中，我们解决了这一空白。我们专注于图像分类任务背景下的概念探测，研究了用于训练探测模型的数据对其性能的影响。我们还为两个广泛使用的数据集提供了概念标签。", "summary": "本文探讨了概念探测的性能，特别关注用于训练探测模型的数据对性能的影响。概念探测是一种通过训练额外分类器将模型内部表示映射到人类概念来解释人工神经网络的方法。研究指出，现有工作忽视了探测模型训练数据的重要性，本文旨在弥补这一缺陷，并在图像分类任务中对此进行深入研究。此外，作者还公开了两个常用数据集的概念标签。", "keywords": "概念探测, 人工神经网络, 模型解释性, 图像分类, 数据影响", "comments": "本文的创新点在于填补了概念探测研究中对训练数据关注不足的空白，强调了数据对探测模型性能的重要性。其工作有助于更全面地理解和改进概念探测技术，从而提升人工神经网络的可解释性。提供概念标签的贡献也对后续研究具有实用价值。"}}
{"id": "2507.18017", "title": "Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items", "authors": ["Maria Vlachou"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2401.05783", "url": "http://arxiv.org/abs/2507.18017v1", "summary": "In Conversational Recommendation Systems (CRS), a user provides feedback on\nrecommended items at each turn, leading the CRS towards improved\nrecommendations. Due to the need for a large amount of data, a user simulator\nis employed for both training and evaluation. Such user simulators critique the\ncurrent retrieved item based on knowledge of a single target item. However,\nsystem evaluation in offline settings with simulators is limited by the focus\non a single target item and their unlimited patience over a large number of\nturns. To overcome these limitations of existing simulators, we propose\nFashion-AlterEval, a new dataset that contains human judgments for a selection\nof alternative items by adding new annotations in common fashion CRS datasets.\nConsequently, we propose two novel meta-user simulators that use the collected\njudgments and allow simulated users not only to express their preferences about\nalternative items to their original target, but also to change their mind and\nlevel of patience. In our experiments using the Shoes and Fashion IQ as the\noriginal datasets and three CRS models, we find that using the knowledge of\nalternatives by the simulator can have a considerable impact on the evaluation\nof existing CRS models, specifically that the existing single-target evaluation\nunderestimates their effectiveness, and when simulatedusers are allowed to\ninstead consider alternative relevant items, the system can rapidly respond to\nmore quickly satisfy the user.", "comment": "arXiv admin note: substantial text overlap with arXiv:2401.05783", "pdf_url": "http://arxiv.org/pdf/2507.18017v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Fashion-AlterEval：一个用于改进对话推荐系统评估的替代相关项目数据集", "tldr": "该研究提出了Fashion-AlterEval数据集和两种新的元用户模拟器，以克服现有对话推荐系统评估中单目标限制和无限耐性的问题，从而更准确地评估系统并提高用户满意度。", "motivation": "现有的对话推荐系统在离线设置中，使用用户模拟器进行评估时存在局限性，主要表现在模拟器仅关注单个目标项目且在大量轮次中具有无限耐心，这导致系统评估不够准确。", "method": "我们提出了Fashion-AlterEval数据集，它通过在常见时尚对话推荐系统数据集中添加新注释，包含了人类对替代项目的判断。基于此，我们进一步提出了两种新颖的元用户模拟器，它们利用收集到的判断，使模拟用户不仅能表达对原始目标替代品的偏好，还能改变主意和耐心程度。", "result": "在对Shoes和Fashion IQ数据集以及三个对话推荐模型的实验中，我们发现模拟器使用替代品知识对现有对话推荐模型的评估有显著影响。具体而言，现有单目标评估低估了它们的有效性，并且当模拟用户被允许考虑替代相关项目时，系统能更快地响应以满足用户需求。", "conclusion": "本研究通过引入Fashion-AlterEval数据集和新颖的元用户模拟器，克服了现有对话推荐系统评估的局限性，证明了考虑替代相关项目能更准确地评估系统并加速用户满意度，从而为未来对话推荐系统的评估提供了更有效的方法。", "translation": "在对话推荐系统（CRS）中，用户在每一轮都会对推荐项目提供反馈，从而使CRS的推荐得到改进。由于需要大量数据，用户模拟器被用于训练和评估。这种用户模拟器基于对单个目标项目的了解来批判当前检索到的项目。然而，在离线设置中，使用模拟器进行系统评估受到对单个目标项目的关注以及在大量轮次中其无限耐心的限制。为了克服现有模拟器的这些局限性，我们提出了Fashion-AlterEval，这是一个新的数据集，通过在常见时尚CRS数据集中添加新注释，包含了人类对一系列替代项目的判断。因此，我们提出了两个新颖的元用户模拟器，它们使用收集到的判断，并允许模拟用户不仅表达他们对原始目标替代品的偏好，而且还可以改变他们的想法和耐心程度。在我们的实验中，使用Shoes和Fashion IQ作为原始数据集以及三个CRS模型，我们发现模拟器使用替代品知识可以对现有CRS模型的评估产生相当大的影响，特别是现有单目标评估低估了它们的有效性，并且当模拟用户被允许考虑替代相关项目时，系统可以快速响应以更快地满足用户。", "summary": "本研究提出Fashion-AlterEval数据集和两种新型元用户模拟器，旨在解决现有对话推荐系统（CRS）评估中用户模拟器单目标关注和无限耐心的问题。通过在常见时尚CRS数据集中添加人类对替代项目的判断，Fashion-AlterEval数据集使得模拟用户能够表达对替代品的偏好并改变其耐心。实验结果表明，使用替代品知识的模拟器能更准确地评估CRS模型，纠正了现有单目标评估的低估，并证明考虑替代相关项目能显著提高用户满意度。", "keywords": "对话推荐系统, 用户模拟器, 数据集, 评估, 替代项目", "comments": "这项研究的创新之处在于它通过引入一个包含人类对替代项目判断的新数据集和两个新的元用户模拟器，解决了对话推荐系统评估中的一个关键局限性。它强调了现有单目标评估的不足，并证明了考虑用户对替代品的偏好和耐心变化对于更准确地评估系统性能和提高用户满意度的重要性。这对于推动对话推荐系统的离线评估方法具有重要意义。"}}
{"id": "2507.18553", "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm", "authors": ["Jiale Chen", "Torsten Hoefler", "Dan Alistarh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18553v1", "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18553v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "LLM量化的几何：GPTQ作为Babai最近平面算法", "tldr": "本研究揭示了GPTQ（一种LLM量化方法）在特定条件下与Babai最近平面算法在数学上等同，为其提供了几何解释和误差上限，从而奠定了坚实的理论基础。", "motivation": "将大型语言模型（LLM）的权重从16位量化到更低位宽是部署LLM到更经济实惠加速器的既定方法。GPTQ作为一种标准的一次性后训练量化方法出现，但其内部工作原理被描述为一系列临时的代数更新，缺乏几何意义或最坏情况保证。因此，本研究旨在揭示其几何意义并提供理论保证。", "method": "本研究表明，当GPTQ对线性层从后向前（从最后一维到第一维）执行时，它在数学上等同于Babai的最近平面算法，用于解决由层输入的海森矩阵定义的格上的经典最近向量问题（CVP）。", "result": "这种等价性带来了两个分析结果：(i) GPTQ的误差传播步骤获得了直观的几何解释；(ii) 在无剪裁条件下，GPTQ继承了Babai算法的误差上限。", "conclusion": "这些结果共同将GPTQ置于坚实的理论基础之上，并为将格算法数十年的进展引入十亿参数模型的未来量化算法设计打开了大门。", "translation": "将大型语言模型（LLM）的权重从16位量化到更低位宽是部署大型Transformer模型到更经济实惠加速器的既定方法。GPTQ作为LLM规模下一次性训练后量化的标准方法之一。然而，其内部工作原理被描述为一系列临时的代数更新，掩盖了任何几何意义或最坏情况保证。在这项工作中，我们展示了，当对线性层从后向前（从最后一维到第一维）执行时，GPTQ在数学上等同于Babai的最近平面算法，用于解决由层输入的海森矩阵定义的格上的经典最近向量问题（CVP）。这种等价性基于复杂的数学论证，并具有两个分析结果：(i) GPTQ的误差传播步骤获得了直观的几何解释；(ii) 在无剪裁条件下，GPTQ继承了Babai算法的误差上限。总而言之，这些结果将GPTQ置于坚实的理论基础之上，并为将格算法数十年的进展引入十亿参数模型的未来量化算法设计打开了大门。", "summary": "本论文深入探讨了大型语言模型（LLM）量化方法GPTQ的几何特性。研究发现，当GPTQ对线性层从后向前执行时，它在数学上与Babai的最近平面算法等同，该算法用于解决格上的最近向量问题。这一发现为GPTQ的误差传播提供了直观的几何解释，并证明了其在无剪裁条件下的误差上限继承了Babai算法的特性。这些理论成果为GPTQ奠定了坚实的数学基础，并为未来基于格算法的量化方法设计提供了新方向。", "keywords": "LLM量化, GPTQ, Babai算法, 最近向量问题, 格算法", "comments": "这项工作通过建立GPTQ与经典格算法之间的数学等价性，为LLM量化领域的一个关键方法提供了急需的理论支撑和几何直觉。其创新之处在于将看似临时的代数操作置于严谨的数学框架下，这不仅提升了我们对GPTQ工作原理的理解，也为未来更高效、有理论保证的量化算法设计开辟了道路，具有重要的理论和实践意义。"}}
{"id": "2412.13102", "title": "AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark", "authors": ["Jianlyu Chen", "Nan Wang", "Chaofan Li", "Bo Wang", "Shitao Xiao", "Han Xiao", "Hao Liao", "Defu Lian", "Zheng Liu"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures; Accepted to ACL 2025 Main", "url": "http://arxiv.org/abs/2412.13102v4", "summary": "Evaluation plays a crucial role in the advancement of information retrieval\n(IR) models. However, current benchmarks, which are based on predefined domains\nand human-labeled data, face limitations in addressing evaluation needs for\nemerging domains both cost-effectively and efficiently. To address this\nchallenge, we propose the Automated Heterogeneous Information Retrieval\nBenchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)\nAutomated. The testing data in AIR-Bench is automatically generated by large\nlanguage models (LLMs) without human intervention. 2) Heterogeneous. The\ntesting data in AIR-Bench is generated with respect to diverse tasks, domains\nand languages. 3) Dynamic. The domains and languages covered by AIR-Bench are\nconstantly augmented to provide an increasingly comprehensive evaluation\nbenchmark for community developers. We develop a reliable and robust data\ngeneration pipeline to automatically create diverse and high-quality evaluation\ndatasets based on real-world corpora. Our findings demonstrate that the\ngenerated testing data in AIR-Bench aligns well with human-labeled testing\ndata, making AIR-Bench a dependable benchmark for evaluating IR models. The\nresources in AIR-Bench are publicly available at\nhttps://github.com/AIR-Bench/AIR-Bench.", "comment": "32 pages, 6 figures; Accepted to ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2412.13102v4", "cate": "cs.IR", "date": "2024-12-17", "updated": "2025-07-24", "AI": {"title_translation": "AIR-Bench：自动化异构信息检索基准", "tldr": "提出AIR-Bench，一个由LLM自动生成异构测试数据的信息检索基准，旨在解决现有基准的局限性。", "motivation": "当前的信息检索（IR）模型评估基准依赖于预定义领域和人工标注数据，在满足新兴领域的评估需求时，面临成本效益和效率方面的限制。", "method": "本文提出了自动化异构信息检索基准（AIR-Bench）。其测试数据由大型语言模型（LLMs）自动生成，无需人工干预。AIR-Bench具有自动化、异构（涵盖多样任务、领域和语言）和动态（领域和语言持续扩充）的特点。研究者开发了一个可靠且鲁棒的数据生成管道，基于真实世界语料库自动创建多样化和高质量的评估数据集。", "result": "AIR-Bench中生成的测试数据与人工标注的测试数据高度一致。", "conclusion": "AIR-Bench是一个可靠的评估信息检索模型的基准。", "translation": "评估在信息检索（IR）模型的进步中扮演着至关重要的角色。然而，当前基于预定义领域和人工标注数据的基准在解决新兴领域的评估需求时面临成本效益和效率方面的局限性。为了应对这一挑战，我们提出了自动化异构信息检索基准（AIR-Bench）。AIR-Bench以三个关键特征而著称：1）自动化。AIR-Bench中的测试数据由大型语言模型（LLMs）自动生成，无需人工干预。2）异构。AIR-Bench中的测试数据是针对多样化的任务、领域和语言生成的。3）动态。AIR-Bench涵盖的领域和语言不断扩充，为社区开发者提供一个日益全面的评估基准。我们开发了一个可靠且鲁棒的数据生成管道，以自动创建基于真实世界语料库的多样化和高质量评估数据集。我们的研究结果表明，AIR-Bench中生成的测试数据与人工标注的测试数据高度一致，使得AIR-Bench成为评估IR模型的可靠基准。AIR-Bench的资源在https://github.com/AIR-Bench/AIR-Bench上公开可用。", "summary": "AIR-Bench是一个创新的信息检索评估基准，旨在解决现有基准在成本和效率上的局限性。它利用大型语言模型自动生成多样化、异构且动态的测试数据，涵盖多种任务、领域和语言。实验证明，其自动生成的数据与人工标注数据高度一致，验证了AIR-Bench作为可靠IR模型评估工具的有效性。", "keywords": "信息检索, 评估基准, 大型语言模型, 自动化, 异构数据", "comments": "AIR-Bench通过利用大型语言模型实现测试数据的自动化生成，极大地降低了信息检索领域基准构建的成本和对人工标注的依赖，同时其异构性和动态性特点使其能够更好地适应新兴领域的需求，具有重要的创新性和实用价值。"}}
{"id": "2412.03515", "title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion", "authors": ["Shengyuan Zhang", "An Zhao", "Ling Yang", "Zejian Li", "Chenye Meng", "Haoran Xu", "Tianrun Chen", "AnYang Wei", "Perry Pengyun GU", "Lingyun Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper is accept by ICCV'25(Oral), the model and code are publicly available on https: //github.com/happyw1nd/ScoreLiDAR", "url": "http://arxiv.org/abs/2412.03515v2", "summary": "Diffusion models have been applied to 3D LiDAR scene completion due to their\nstrong training stability and high completion quality. However, the slow\nsampling speed limits the practical application of diffusion-based scene\ncompletion models since autonomous vehicles require an efficient perception of\nsurrounding environments. This paper proposes a novel distillation method\ntailored for 3D Li- DAR scene completion models, dubbed ScoreLiDAR, which\nachieves efficient yet high-quality scene completion. Score- LiDAR enables the\ndistilled model to sample in significantly fewer steps after distillation. To\nimprove completion quality, we also introduce a novel Structural Loss, which\nencourages the distilled model to capture the geometric structure of the 3D\nLiDAR scene. The loss contains a scene-wise term constraining the holistic\nstructure and a point-wise term constraining the key landmark points and their\nrelative configuration. Extensive experiments demonstrate that ScoreLiDAR\nsignificantly accelerates the completion time from 30.55 to 5.37 seconds per\nframe (>5x) on SemanticKITTI and achieves superior performance compared to\nstate-of-the-art 3D LiDAR scene completion models. Our model and code are\npublicly available on https: //github.com/happyw1nd/ScoreLiDAR.", "comment": "This paper is accept by ICCV'25(Oral), the model and code are\n  publicly available on https: //github.com/happyw1nd/ScoreLiDAR", "pdf_url": "http://arxiv.org/pdf/2412.03515v2", "cate": "cs.CV", "date": "2024-12-04", "updated": "2025-07-24", "AI": {"title_translation": "将扩散模型蒸馏到高效三维激光雷达场景补全", "tldr": "本文提出ScoreLiDAR，一种针对3D LiDAR场景补全的蒸馏方法，显著加速扩散模型的采样速度，同时保持高质量，解决了自动驾驶中扩散模型应用效率低的问题。", "motivation": "扩散模型在3D LiDAR场景补全中表现出高训练稳定性和高质量，但其缓慢的采样速度限制了在自动驾驶等需要高效环境感知的实际应用。", "method": "论文提出了名为ScoreLiDAR的新型蒸馏方法，专门用于3D LiDAR场景补全模型。该方法使蒸馏后的模型能以更少的步骤进行采样。此外，还引入了一种新的结构损失（Structural Loss），包含场景级和点级项，以帮助蒸馏模型捕捉3D LiDAR场景的几何结构。", "result": "在SemanticKITTI数据集上，ScoreLiDAR将每帧补全时间从30.55秒显著加速到5.37秒（超过5倍），并且与最先进的3D LiDAR场景补全模型相比，实现了卓越的性能。", "conclusion": "通过ScoreLiDAR蒸馏方法和结构损失，该研究成功地将扩散模型应用于3D LiDAR场景补全的效率大幅提升，同时保持或超越了现有SOTA模型的补全质量，使其更适用于实际应用。", "translation": "扩散模型因其强大的训练稳定性和高补全质量而被应用于三维激光雷达场景补全。然而，缓慢的采样速度限制了基于扩散的场景补全模型的实际应用，因为自动驾驶汽车需要高效感知周围环境。本文提出了一种新颖的蒸馏方法，专为三维激光雷达场景补全模型设计，名为ScoreLiDAR，它实现了高效且高质量的场景补全。ScoreLiDAR使蒸馏后的模型在蒸馏后能够以显著更少的步骤进行采样。为了提高补全质量，我们还引入了一种新颖的结构损失（Structural Loss），它鼓励蒸馏模型捕捉三维激光雷达场景的几何结构。该损失包含一个约束整体结构的场景级项和一个约束关键地标点及其相对配置的点级项。广泛的实验表明，ScoreLiDAR在SemanticKITTI数据集上将每帧补全时间从30.55秒显著加速到5.37秒（超过5倍），并且与最先进的三维激光雷达场景补全模型相比，实现了卓越的性能。我们的模型和代码可在https://github.com/happyw1nd/ScoreLiDAR公开获取。", "summary": "本文提出ScoreLiDAR，一种创新的蒸馏方法，旨在解决扩散模型在3D LiDAR场景补全中采样速度慢的问题。ScoreLiDAR通过减少采样步骤和引入结构损失（包含场景级和点级约束）来提升效率和质量。实验证明，ScoreLiDAR在SemanticKITTI数据集上将补全时间加速了5倍以上，并实现了优于SOTA模型的性能，使其更适用于自动驾驶等实时应用。", "keywords": "扩散模型, 3D LiDAR场景补全, 蒸馏, 效率, 结构损失", "comments": "本文的创新点在于针对扩散模型在3D LiDAR场景补全中的效率瓶颈，提出了专门的蒸馏方法ScoreLiDAR。通过结合采样步骤的减少和独特的结构损失，成功地在保证高质量的同时大幅提升了处理速度，这对于自动驾驶等实时性要求高的应用具有重要意义。该工作有效地解决了扩散模型应用中的一个关键限制。"}}
{"id": "2507.17861", "title": "ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks", "authors": ["Daniel Ricardo Cunha Oliveira", "Rodrigo Moreira", "Flávio de Oliveira Silva"], "categories": ["cs.NI", "cs.ET"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17861v1", "summary": "Artificial Intelligence (AI) plays a key role in developing 6G networks.\nWhile current specifications already include Network Data Analytics Function\n(NWDAF) as a network element responsible for providing information about the\ncore, a more comprehensive approach will be needed to enable automation of\nnetwork segments that are not yet fully explored in the context of 5G. In this\npaper, we present Automated Radio Coverage Anomalies Detection and Evaluation\n(ARCADE), a methodology for identifying and diagnosing anomalies in the\ncellular access network. Furthermore, we demonstrate how a hybrid architecture\nof network analytics functions in the evolution toward 6G can enhance the\napplication of AI in a broader network context, using ARCADE as a practical\nexample of this approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17861v1", "cate": "cs.NI", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "ARCADE：一种适用于6G网络混合AI环境的RAN诊断方法", "tldr": "论文提出了ARCADE，一种在6G网络中利用混合AI环境诊断无线接入网（RAN）异常的方法。", "motivation": "人工智能（AI）在6G网络发展中扮演关键角色。当前5G规范中的网络数据分析功能（NWDAF）主要提供核心网信息，但需要更全面的方法来实现5G中尚未充分探索的网络段自动化。因此，本研究旨在提出一种识别和诊断蜂窝接入网异常的方法。", "method": "本文提出了ARCADE（Automated Radio Coverage Anomalies Detection and Evaluation），这是一种用于识别和诊断蜂窝接入网络中异常的方法。此外，论文还展示了在向6G演进过程中，网络分析功能的混合架构如何利用ARCADE作为实例，增强AI在更广泛网络环境中的应用。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "人工智能（AI）在6G网络发展中扮演关键角色。虽然目前的规范已将网络数据分析功能（NWDAF）作为负责提供核心网信息的网络元素，但为了实现5G背景下尚未充分探索的网络段自动化，将需要一种更全面的方法。在本文中，我们提出了自动化无线覆盖异常检测和评估（ARCADE），这是一种用于识别和诊断蜂窝接入网络中异常的方法。此外，我们展示了在向6G演进过程中，网络分析功能的混合架构如何增强AI在更广泛网络环境中的应用，并以ARCADE作为这种方法的实际示例。", "summary": "本文提出了ARCADE，一种用于在蜂窝接入网络中识别和诊断异常的创新方法。该研究强调了AI在6G网络中的关键作用，并指出需要超越现有NWDAF的更全面方法来实现网络自动化。ARCADE作为一种实用示例，展示了混合网络分析功能架构如何增强AI在向6G演进过程中的应用。", "keywords": "ARCADE, RAN诊断, 6G网络, 混合AI, 网络异常", "comments": "ARCADE方法创新性地将混合AI环境应用于6G网络的无线接入网（RAN）诊断，解决了当前5G框架下网络自动化不足的问题。其重要性在于为未来6G网络中更广泛的AI应用提供了实用范例和方法论支持。"}}
{"id": "2507.17903", "title": "Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges", "authors": ["Obaidullah Zaland", "Chanh Nguyen", "Florian T. Pokorny", "Monowar Bhuyan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for Presentation at IEEE International Conference on Machine Learning and Cybernetics (ICMLC) 2025", "url": "http://arxiv.org/abs/2507.17903v1", "summary": "Federated Learning (FL) is an emerging distributed machine learning paradigm,\nwhere the collaborative training of a model involves dynamic participation of\ndevices to achieve broad objectives. In contrast, classical machine learning\n(ML) typically requires data to be located on-premises for training, whereas FL\nleverages numerous user devices to train a shared global model without the need\nto share private data. Current robotic manipulation tasks are constrained by\nthe individual capabilities and speed of robots due to limited low-latency\ncomputing resources. Consequently, the concept of cloud robotics has emerged,\nallowing robotic applications to harness the flexibility and reliability of\ncomputing resources, effectively alleviating their computational demands across\nthe cloud-edge continuum. Undoubtedly, within this distributed computing\ncontext, as exemplified in cloud robotic manipulation scenarios, FL offers\nmanifold advantages while also presenting several challenges and opportunities.\nIn this paper, we present fundamental concepts of FL and their connection to\ncloud robotic manipulation. Additionally, we envision the opportunities and\nchallenges associated with realizing efficient and reliable cloud robotic\nmanipulation at scale through FL, where researchers adopt to design and verify\nFL models in either centralized or decentralized settings.", "comment": "Accepted for Presentation at IEEE International Conference on Machine\n  Learning and Cybernetics (ICMLC) 2025", "pdf_url": "http://arxiv.org/pdf/2507.17903v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "联邦学习在大规模云机器人操作中的机遇与挑战", "tldr": "本文探讨了联邦学习在解决大规模云机器人操作中计算资源限制方面的潜力，并分析了其机遇与挑战。", "motivation": "当前机器人操作受限于个体能力和计算资源，导致效率低下。云机器人技术可以缓解计算需求，而联邦学习作为一种分布式机器学习范式，有望在不共享私有数据的情况下，协同训练模型，从而提升大规模云机器人操作的效率和可靠性。", "method": "本文介绍了联邦学习的基本概念及其与云机器人操作的联系，并展望了通过联邦学习实现高效可靠的大规模云机器人操作所面临的机遇与挑战。研究者可以在集中式或去中心化设置中设计和验证联邦学习模型。", "result": "本文探讨了联邦学习在云机器人操作中的多重优势，同时也指出了实现这一目标所面临的挑战。", "conclusion": "联邦学习为大规模云机器人操作提供了巨大的潜力，通过解决计算资源限制和数据隐私问题，但其应用仍需克服技术挑战，并在集中式或去中心化环境中进行深入研究和验证。", "translation": "联邦学习（FL）是一种新兴的分布式机器学习范式，其中模型的协同训练涉及设备的动态参与以实现广泛目标。相比之下，传统的机器学习（ML）通常需要数据位于本地进行训练，而FL则利用众多用户设备来训练共享的全局模型，无需共享私有数据。当前机器人操作任务受限于机器人个体能力和速度，因为低延迟计算资源有限。因此，云机器人概念应运而生，它允许机器人应用利用计算资源的灵活性和可靠性，有效缓解其在云-边缘连续体中的计算需求。毫无疑问，在这种分布式计算背景下，正如云机器人操作场景所体现的，FL提供了多重优势，同时也带来了若干挑战和机遇。在本文中，我们介绍了FL的基本概念及其与FL机器人操作的联系。此外，我们展望了通过FL实现高效可靠的大规模云机器人操作所面临的机遇与挑战，研究人员可以在集中式或去中心化设置中设计和验证FL模型。", "summary": "本文探讨了联邦学习（FL）在解决大规模云机器人操作中计算资源限制和数据隐私问题方面的潜力。它介绍了FL的基本概念及其与云机器人技术的结合，强调了FL如何通过分布式模型训练，在不共享私有数据的情况下，提升机器人操作的效率和可靠性。文章还展望了FL在该领域应用的机遇与挑战，并指出FL模型可以在集中式或去中心化环境中进行设计和验证。", "keywords": "联邦学习, 云机器人, 机器人操作, 分布式机器学习, 机遇与挑战", "comments": "这篇论文并非提出新的技术或实验结果，而是一篇概念性、综述性的文章。其创新点在于首次系统性地将联邦学习的概念引入到大规模云机器人操作领域，并对其机遇与挑战进行了初步的分析和展望。这对于该新兴交叉领域的研究方向具有指导意义，为未来的技术发展和应用提供了理论框架。"}}
{"id": "2507.18113", "title": "Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification", "authors": ["Junyong Jiang", "Buwei Tian", "Chenxing Xu", "Songze Li", "Lu Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18113v1", "summary": "Reinforcement learning (RL) has achieved remarkable success in fields like\nrobotics and autonomous driving, but adversarial attacks designed to mislead RL\nsystems remain challenging. Existing approaches often rely on modifying the\nenvironment or policy, limiting their practicality. This paper proposes an\nadversarial attack method in which existing agents in the environment guide the\ntarget policy to output suboptimal actions without altering the environment. We\npropose a reward iteration optimization framework that leverages large language\nmodels (LLMs) to generate adversarial rewards explicitly tailored to the\nvulnerabilities of the target agent, thereby enhancing the effectiveness of\ninducing the target agent toward suboptimal decision-making. Additionally, a\ncritical state identification algorithm is designed to pinpoint the target\nagent's most vulnerable states, where suboptimal behavior from the victim leads\nto significant degradation in overall performance. Experimental results in\ndiverse environments demonstrate the superiority of our method over existing\napproaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18113v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "强化学习中的策略中断：基于大型语言模型和关键状态识别的对抗性攻击", "tldr": "一种新的强化学习对抗性攻击方法，利用大型语言模型生成定制奖励并识别关键状态，以在不改变环境的情况下诱导目标策略输出次优动作。", "motivation": "现有的强化学习对抗性攻击方法通常依赖于修改环境或策略，这限制了它们的实用性。", "method": "本文提出了一种不改变环境的对抗性攻击方法，通过环境中的现有智能体引导目标策略输出次优动作。该方法包括一个奖励迭代优化框架，利用大型语言模型（LLMs）生成明确针对目标智能体脆弱性的对抗性奖励，以及一个关键状态识别算法，用于查明目标智能体最脆弱的状态。", "result": "在不同环境中的实验结果表明，该方法优于现有方法。", "conclusion": "该论文提出了一种新颖且更实用的强化学习对抗性攻击方法，通过利用大型语言模型生成对抗性奖励并识别关键状态，有效地诱导目标智能体做出次优决策，且无需修改环境。", "translation": "强化学习（RL）在机器人和自动驾驶等领域取得了显著成功，但旨在误导RL系统的对抗性攻击仍然具有挑战性。现有方法通常依赖于修改环境或策略，这限制了它们的实用性。本文提出了一种对抗性攻击方法，其中环境中的现有智能体引导目标策略输出次优动作，而无需改变环境。我们提出了一个奖励迭代优化框架，该框架利用大型语言模型（LLM）生成明确针对目标智能体脆弱性的对抗性奖励，从而增强了诱导目标智能体做出次优决策的有效性。此外，还设计了一种关键状态识别算法，以查明目标智能体最脆弱的状态，在这些状态下，受害者的次优行为会导致整体性能显著下降。在不同环境中的实验结果表明，我们的方法优于现有方法。", "summary": "本文提出了一种新颖的强化学习对抗性攻击方法，旨在不修改环境的情况下诱导目标智能体产生次优行为。该方法利用大型语言模型（LLMs）在一个奖励迭代优化框架中生成定制的对抗性奖励，并结合关键状态识别算法来定位智能体最脆弱的状态。实验证明，该方法在诱导次优决策方面优于现有攻击手段。", "keywords": "强化学习, 对抗性攻击, 大型语言模型, 关键状态识别, 策略中断", "comments": "该论文提出了一种创新的强化学习对抗性攻击方法，其重要性在于克服了现有方法修改环境或策略的局限性，使其更具实用性。利用大型语言模型生成对抗性奖励是其核心创新点，这为RL安全领域带来了新的研究方向。同时，关键状态识别的引入也提高了攻击的效率和针对性。"}}
{"id": "2507.18342", "title": "EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs", "authors": ["Yuping He", "Yifei Huang", "Guo Chen", "Baoqi Pei", "Jilan Xu", "Tong Lu", "Jiangmiao Pang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18342v1", "summary": "Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18342v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "EgoExoBench：一个用于多模态大语言模型中第一人称和第三人称视角视频理解的基准", "tldr": "EgoExoBench是一个新的基准，用于评估多模态大语言模型在第一人称和第三人称视角之间进行跨视角推理的能力。研究发现，当前模型在单视角任务上表现良好，但在跨视角语义对齐、视角关联和时间推理方面表现不佳。", "motivation": "尽管多模态大语言模型（MLLMs）发展迅速，但它们进行跨视角（第一人称和第三人称）推理的能力尚未被探索。为了解决这一问题，本文引入了EgoExoBench。", "method": "本文引入了EgoExoBench，这是首个用于第一人称-第三人称视频理解和推理的基准。该基准基于公开数据集构建，包含超过7,300个问答对，涵盖11个子任务，分为语义对齐、视角关联和时间推理三个核心挑战。研究评估了13个最先进的MLLMs。", "result": "评估结果显示，尽管这些模型在单视角任务上表现出色，但它们在跨视角语义对齐、准确关联不同视角以及在第一人称-第三人称上下文中推断时间动态方面存在困难。", "conclusion": "EgoExoBench有望成为研究具身智能体和智能助手的重要资源，以实现类人的跨视角智能。", "translation": "将第一人称（自我中心）和第三人称（非自我中心）视角的知识进行转移和整合是人类智能的内在特征，使人类能够从他人那里学习，并从自身经验中传达见解。尽管多模态大语言模型（MLLMs）取得了快速进展，但它们执行此类跨视角推理的能力仍未被探索。为了解决这个问题，我们引入了EgoExoBench，这是首个用于第一人称-第三人称视频理解和推理的基准。EgoExoBench基于公开数据集构建，包含超过7,300个问答对，涵盖11个子任务，分为三个核心挑战：语义对齐、视角关联和时间推理。我们评估了13个最先进的MLLMs，发现尽管这些模型在单视角任务上表现出色，但它们在跨视角语义对齐、准确关联视角以及在第一人称-第三人称上下文中推断时间动态方面存在困难。我们希望EgoExoBench能为研究具身智能体和寻求类人跨视角智能的智能助手提供宝贵资源。", "summary": "本文介绍了EgoExoBench，这是一个开创性的基准，旨在评估多模态大语言模型（MLLMs）在整合和推理第一人称与第三人称视频视角方面的能力。该基准包含7,300多个问答对，涵盖语义对齐、视角关联和时间推理等11个子任务。对13个SOTA MLLMs的评估表明，尽管它们在单视角任务上表现良好，但在跨视角推理，特别是语义对齐和时间动态理解方面存在显著不足。EgoExoBench旨在推动具身智能和智能助手在跨视角智能方面的研究。", "keywords": "EgoExoBench, 多模态大语言模型, 视频理解, 跨视角推理, 基准", "comments": "EgoExoBench的创新之处在于它是首个专门针对MLLMs跨第一人称和第三人称视角视频理解与推理的基准。其重要性在于揭示了当前SOTA MLLMs在此关键能力上的不足，为未来具身智能体和智能助手的发展指明了方向。通过提供结构化的挑战任务，该工作为研究人员提供了评估和改进模型跨视角智能的宝贵工具。"}}
{"id": "2412.14019", "title": "Retrieving Classes of Causal Orders with Inconsistent Knowledge Bases", "authors": ["Federico Baldo", "Simon Ferreira", "Charles K. Assaad"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.14019v3", "summary": "Traditional causal discovery methods often rely on strong, untestable\nassumptions, which makes them unreliable in real applications. In this context,\nLarge Language Models (LLMs) have emerged as a promising alternative for\nextracting causal knowledge from text-based metadata, which consolidates domain\nexpertise. However, LLMs tend to be unreliable and prone to hallucinations,\nnecessitating strategies that account for their limitations. One effective\nstrategy is to use a consistency measure to assess reliability. Additionally,\nmost text metadata does not clearly distinguish direct causal relationships\nfrom indirect ones, further complicating the discovery of a causal DAG. As a\nresult, focusing on causal orders, rather than causal DAGs, emerges as a more\npractical and robust approach. We present a new method to derive a class of\nacyclic tournaments, which represent plausible causal orders, maximizing a\nconsistency score derived from an LLM. Our approach starts by calculating\npairwise consistency scores between variables, resulting in a semi-complete\npartially directed graph that consolidates these scores into an abstraction of\nthe maximally consistent causal orders. Using this structure, we identify\noptimal acyclic tournaments, focusing on those that maximize consistency across\nall configurations. We subsequently show how both the abstraction and the class\nof causal orders can be used to estimate causal effects. We tested our method\non both well-established benchmarks, as well as, real-world datasets from\nepidemiology and public health. Our results demonstrate the effectiveness of\nour approach in recovering the correct causal order.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.14019v3", "cate": "cs.AI", "date": "2024-12-18", "updated": "2025-07-24", "AI": {"title_translation": "从不一致知识库中检索因果序类别", "tldr": "传统因果发现方法和大型语言模型（LLMs）均存在局限性。本文提出一种新方法，利用LLM生成的一致性分数，从不一致的知识库中检索可靠的因果序类别（非循环竞赛图），并在基准和真实世界数据上验证了其有效性。", "motivation": "传统的因果发现方法依赖于强且不可靠的假设。大型语言模型（LLMs）虽然在提取因果知识方面有潜力，但容易出现幻觉和不可靠性。此外，文本元数据通常无法区分直接和间接因果关系，使得发现精确的因果有向无环图（DAG）变得复杂。因此，需要一种更实用、更鲁棒的方法来发现因果关系。", "method": "本文提出一种新方法，通过最大化从大型语言模型（LLM）导出的一致性分数来推导一类非循环竞赛图（代表可信的因果序）。该方法首先计算变量间的成对一致性分数，构建一个半完整的部分有向图，该图整合了这些分数并抽象出最大一致的因果序。然后，利用此结构识别最优的非循环竞赛图，重点是那些在所有配置中最大化一致性的图。此外，还展示了如何利用这种抽象和因果序类别来估计因果效应。", "result": "该方法在成熟的基准数据集以及流行病学和公共卫生领域的真实世界数据集上进行了测试。结果表明，该方法在恢复正确因果序方面是有效的。", "conclusion": "该研究提出了一种利用LLM一致性分数来识别因果序类别（非循环竞赛图）的有效且鲁棒的方法，解决了传统因果发现方法的局限性以及LLM本身的不可靠性问题，为因果效应估计提供了实用工具。", "translation": "传统因果发现方法通常依赖于强且无法检验的假设，这使得它们在实际应用中不可靠。在这种背景下，大型语言模型（LLMs）作为一种从基于文本的元数据中提取因果知识的有前景的替代方案而出现，这些元数据巩固了领域专业知识。然而，LLMs往往不可靠且容易产生幻觉，因此需要考虑其局限性的策略。一种有效的策略是使用一致性度量来评估可靠性。此外，大多数文本元数据并未明确区分直接因果关系和间接因果关系，这进一步使因果有向无环图（DAG）的发现复杂化。因此，关注因果序而非因果DAGs，成为一种更实用、更稳健的方法。我们提出了一种新方法，用于推导一类非循环竞赛图（acyclic tournaments），这些图代表了可信的因果序，并最大化了从LLM导出的一致性分数。我们的方法首先计算变量间的成对一致性分数，从而得到一个半完整的部分有向图，该图将这些分数整合为最大一致因果序的抽象。利用这种结构，我们识别出最优的非循环竞赛图，重点关注那些在所有配置中最大化一致性的图。随后，我们展示了抽象和因果序类别如何用于估计因果效应。我们在成熟的基准数据集以及流行病学和公共卫生领域的真实世界数据集上测试了我们的方法。我们的结果表明，我们的方法在恢复正确因果序方面的有效性。", "summary": "本文提出了一种新颖的因果发现方法，旨在解决传统方法假设过强以及大型语言模型（LLMs）不可靠的问题。该方法不侧重于复杂的因果有向无环图（DAGs），而是通过最大化LLM生成的一致性分数来检索更鲁棒的“因果序”（表示为非循环竞赛图）。具体而言，它计算变量间的成对一致性分数，构建一个半完整的部分有向图，并从中识别出最优且一致性最高的非循环竞赛图。研究表明，该方法在基准和真实世界数据集上都能有效恢复正确的因果序。", "keywords": "因果发现, 大型语言模型, 因果序, 一致性, 非循环竞赛图", "comments": "该论文通过将大型语言模型（LLMs）与基于一致性的框架相结合，为因果发现提供了一种创新方法，摆脱了传统DAGs的限制，转向更鲁棒的因果序。这既解决了LLMs固有的不可靠性问题，又利用了它们从文本中整合领域知识的能力。关注非循环竞赛图和最大化一致性，为难以区分精确直接/间接因果关系的实际场景提供了实用的解决方案。"}}
{"id": "2506.16383", "title": "Large Language Models in Argument Mining: A Survey", "authors": ["Hao Li", "Viktor Schlegel", "Yizheng Sun", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work draft", "url": "http://arxiv.org/abs/2506.16383v4", "summary": "Argument Mining (AM), a critical subfield of Natural Language Processing\n(NLP), focuses on extracting argumentative structures from text. The advent of\nLarge Language Models (LLMs) has profoundly transformed AM, enabling advanced\nin-context learning, prompt-based generation, and robust cross-domain\nadaptability. This survey systematically synthesizes recent advancements in\nLLM-driven AM. We provide a concise review of foundational theories and\nannotation frameworks, alongside a meticulously curated catalog of datasets. A\nkey contribution is our comprehensive taxonomy of AM subtasks, elucidating how\ncontemporary LLM techniques -- such as prompting, chain-of-thought reasoning,\nand retrieval augmentation -- have reconfigured their execution. We further\ndetail current LLM architectures and methodologies, critically assess\nevaluation practices, and delineate pivotal challenges including long-context\nreasoning, interpretability, and annotation bottlenecks. Conclusively, we\nhighlight emerging trends and propose a forward-looking research agenda for\nLLM-based computational argumentation, aiming to strategically guide\nresearchers in this rapidly evolving domain.", "comment": "Work draft", "pdf_url": "http://arxiv.org/pdf/2506.16383v4", "cate": "cs.CL", "date": "2025-06-19", "updated": "2025-07-24", "AI": {"title_translation": "大型语言模型在论证挖掘中的应用：一项综述", "tldr": "这篇综述系统地分析了大型语言模型（LLMs）在论证挖掘（AM）领域的最新进展，涵盖了理论、数据集、AM子任务的LLM技术应用、评估实践、挑战和未来研究方向。", "motivation": "论证挖掘（AM）是自然语言处理（NLP）的关键子领域，大型语言模型（LLMs）的出现深刻改变了AM，使其能够进行高级的上下文学习、基于提示的生成和强大的跨领域适应性。因此，需要对LLM驱动的AM的最新进展进行系统性总结。", "method": "本综述系统地总结了LLM驱动的AM的最新进展。它回顾了基础理论和标注框架，提供了数据集目录，提出了AM子任务的综合分类法，阐明了LLM技术（如提示、思维链推理和检索增强）如何重新配置其执行。此外，还详细介绍了当前的LLM架构和方法，批判性评估了评估实践，并描绘了关键挑战。", "result": "综述提供了一个基础理论和标注框架的简明回顾，一个精心策划的数据集目录，一个AM子任务的综合分类法，阐明了LLM技术如何应用于AM子任务，详细介绍了LLM架构和方法，并评估了评估实践。", "conclusion": "综述强调了新兴趋势，并提出了一个面向未来的基于LLM的计算论证研究议程，旨在战略性地指导研究人员在这个快速发展的领域。", "translation": "论证挖掘（AM）是自然语言处理（NLP）的一个关键子领域，专注于从文本中提取论证结构。大型语言模型（LLMs）的出现深刻地改变了AM，使其能够进行高级的上下文学习、基于提示的生成和强大的跨领域适应性。本综述系统地综合了LLM驱动的AM的最新进展。我们简明回顾了基础理论和标注框架，并提供了一个精心策划的数据集目录。一个关键贡献是我们对AM子任务的全面分类法，阐明了当代LLM技术——例如提示、思维链推理和检索增强——如何重新配置它们的执行。我们进一步详细介绍了当前的LLM架构和方法，批判性地评估了评估实践，并描绘了关键挑战，包括长上下文推理、可解释性和标注瓶颈。最后，我们强调了新兴趋势，并提出了一个面向未来的基于LLM的计算论证研究议程，旨在战略性地指导研究人员在这个快速发展的领域。", "summary": "这篇综述系统地概述了大型语言模型（LLMs）在论证挖掘（AM）领域的最新进展。文章涵盖了AM的基础理论、标注框架和数据集，并提出了AM子任务的全面分类法，详细阐述了LLM技术如何应用于这些任务。此外，综述还探讨了LLM架构、评估方法以及长上下文推理、可解释性和标注瓶颈等挑战，并展望了未来的研究方向。", "keywords": "大型语言模型, 论证挖掘, 自然语言处理, 综述, 计算论证", "comments": "这篇综述对于理解LLM在论证挖掘领域的应用和发展具有重要意义。其创新之处在于系统地梳理了LLM技术如何重塑AM子任务，并提出了一个全面的分类法。它不仅总结了现有工作，还明确指出了该领域面临的关键挑战和未来的研究方向，为研究人员提供了宝贵的指导。"}}
{"id": "2503.11634", "title": "Translating Between the Common Haar Random State Model and the Unitary Model", "authors": ["Eli Goldin", "Mark Zhandry"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      39 pages", "url": "http://arxiv.org/abs/2503.11634v2", "summary": "Black-box separations are a cornerstone of cryptography, indicating barriers\nto various goals. A recent line of work has explored black-box separations for\nquantum cryptographic primitives. Namely, a number of separations are known in\nthe Common Haar Random State (CHRS) model, though this model is not considered\na complete separation, but rather a starting point. A few very recent works\nhave attempted to lift these separations to a unitary separation, which are\nconsidered complete separations. Unfortunately, we find significant errors in\nsome of these lifting results.\n  We prove general conditions under which CHRS separations can be generically\nlifted, thereby giving simple, modular, and bug-free proofs of complete unitary\nseparations between various quantum primitives. Our techniques allow for\nsimpler proofs of existing separations as well as new separations that were\npreviously only known in the CHRS model.", "comment": "39 pages", "pdf_url": "http://arxiv.org/pdf/2503.11634v2", "cate": "quant-ph", "date": "2025-03-14", "updated": "2025-07-23", "AI": {"title_translation": "在通用Haar随机态模型和幺正模型之间进行转换", "tldr": "本文提出了将通用Haar随机态模型（CHRS）中的分离提升到幺正分离的通用条件，纠正了现有工作中的错误，并提供了更简单、模块化且无错误的证明，从而实现了量子原语之间完整的幺正分离。", "motivation": "量子密码学中的黑盒分离是衡量各种目标障碍的基石。在通用Haar随机态（CHRS）模型中已知存在许多分离，但该模型不被认为是完全分离。最近的一些工作试图将这些分离提升到被认为是完全分离的幺正分离，但这些提升结果中存在显著错误。因此，本文的动机是纠正这些错误并提供可靠的通用条件来完成这种提升。", "method": "本文证明了在何种通用条件下，CHRS分离可以被普遍提升，从而为各种量子原语之间的完整幺正分离提供了简单、模块化且无错误的证明。", "result": "本文的技术允许更简单地证明现有分离，以及证明以前只在CHRS模型中已知的新分离。通过提供通用条件，解决了现有提升结果中的显著错误，实现了可靠的幺正分离。", "conclusion": "本文成功地为将通用Haar随机态模型中的分离提升到幺正模型提供了通用、可靠且无错误的条件，从而推动了量子密码学中黑盒分离的理解和应用。", "translation": "黑盒分离是密码学的基石，指明了实现各种目标的障碍。最近的一系列工作探索了量子密码原语的黑盒分离。具体来说，在通用Haar随机态（CHRS）模型中已知存在许多分离，尽管该模型不被认为是完全分离，而是一个起点。最近的一些工作试图将这些分离提升到幺正分离，这被认为是完全分离。不幸的是，我们发现这些提升结果中的一些存在显著错误。\n我们证明了CHRS分离可以普遍提升的通用条件，从而为各种量子原语之间完整的幺正分离提供了简单、模块化且无错误的证明。我们的技术允许更简单地证明现有分离，以及证明以前只在CHRS模型中已知的新分离。", "summary": "本文探讨了量子密码学中黑盒分离的关键问题，特别是在通用Haar随机态（CHRS）模型和幺正模型之间的转换。针对现有将CHRS分离提升到完整幺正分离工作中存在的错误，作者提出了通用的条件，使得这种提升能够以简单、模块化且无错误的方式实现。这不仅简化了现有分离的证明过程，还使得之前仅在CHRS模型中发现的新分离得以在幺正模型中得到验证，从而为量子原语间的完整分离提供了可靠的证明方法。", "keywords": "量子密码学, 黑盒分离, Haar随机态模型, 幺正模型, 分离证明", "comments": "这篇论文解决了量子密码学中一个重要的实际问题：如何可靠地将CHRS模型中的分离提升到更完整的幺正模型。其创新之处在于提供了通用的、经过验证的条件来纠正现有工作中的错误，并简化了证明过程。这对于理解量子原语的理论界限和设计更安全的量子协议具有重要意义。"}}
{"id": "2507.18084", "title": "\"I Would Not Be This Version of Myself Today\": Elaborating on the Effects of Eudaimonic Gaming Experiences", "authors": ["Nisha Devasia", "Georgia Kenderova", "Michele Newman", "Julie Kientz", "Jin Ha Lee"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to CHI PLAY 2025", "url": "http://arxiv.org/abs/2507.18084v1", "summary": "While much of the research in digital games has emphasized hedonic\nexperiences, such as flow, enjoyment, and positive affect, recent years have\nseen increased interest in eudaimonic gaming experiences, typically\nmixed-affect and associated with personal meaningfulness and growth. The\nformation of such experiences in games is theorized to have four constituent\nelements: motivation, game use, experience, and effects. However, while the\nfirst three elements have been relatively well explored in the literature, the\neffects - and how they may influence positive individual outcomes - have been\nunderexplored thus far. To this end, in this work, we investigate the perceived\noutcomes of eudaimonic gaming and how different components of the experience\ninfluence these effects. We conducted a survey (n = 166) in which respondents\nrecounted meaningful gaming experiences and how they affected their present\nlives. We used a mixed-methods approach to classify effects and identify\nsignificant subcomponents of their formation. We contribute an empirical\nunderstanding of how meaningful gaming experiences can lead to positive\nreflective, learning, social, health, and career effects, extending current\ntheoretical models of eudaimonic gaming experiences and offering implications\nfor how researchers and practitioners might use these findings to promote\npositive outcomes for players.", "comment": "Accepted to CHI PLAY 2025", "pdf_url": "http://arxiv.org/pdf/2507.18084v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "“我今天不会是现在的自己”：阐述幸福感游戏体验的影响", "tldr": "本研究调查了幸福感游戏体验如何通过反思、学习、社交、健康和职业方面对玩家产生积极影响，填补了现有理论模型中对“影响”部分探索不足的空白。", "motivation": "现有数字游戏研究多强调享乐主义体验，而对幸福感游戏体验的“影响”部分探索不足，尽管其理论构成包含动机、游戏使用、体验和影响。本文旨在填补这一空白，调查幸福感游戏体验的感知结果及其不同组成部分如何影响这些效果。", "method": "采用混合方法调查（n=166），受访者回顾有意义的游戏体验及其对当前生活的影响。使用混合方法对效果进行分类并识别其形成的重要子组成部分。", "result": "经验性地理解了有意义的游戏体验如何带来积极的反思、学习、社交、健康和职业影响。", "conclusion": "幸福感游戏体验能带来积极的个人成果，并为研究人员和实践者提供了促进玩家积极结果的启示，扩展了现有的幸福感游戏体验理论模型。", "translation": "尽管数字游戏研究多强调享乐主义体验，如心流、享受和积极情感，但近年来对幸福感游戏体验的兴趣日益增长，这种体验通常伴随着复杂情感，并与个人意义和成长相关。游戏中此类体验的形成理论上包含四个构成要素：动机、游戏使用、体验和影响。然而，虽然前三个要素在文献中已得到相对充分的探索，但“影响”——以及它们如何影响积极的个人成果——迄今为止尚未得到充分探索。为此，本文调查了幸福感游戏体验的感知结果以及体验的不同组成部分如何影响这些效果。我们进行了一项调查（n = 166），受访者回顾了有意义的游戏体验以及它们如何影响他们现在的生活。我们采用混合方法对效果进行分类并识别其形成的重要子组成部分。我们对有意义的游戏体验如何带来积极的反思、学习、社交、健康和职业影响提供了经验性理解，扩展了当前幸福感游戏体验的理论模型，并为研究人员和实践者如何利用这些发现促进玩家的积极成果提供了启示。", "summary": "本文探讨了幸福感游戏体验对玩家的积极影响，弥补了以往研究侧重享乐体验而忽视其深远效果的不足。通过一项166人的混合方法调查，研究者分析了有意义的游戏体验如何促进反思、学习、社交、健康和职业发展。研究结果扩展了幸福感游戏体验的理论模型，并为促进玩家福祉提供了实践指导。", "keywords": "幸福感游戏, 游戏体验, 积极影响, 个人成长, 混合方法研究", "comments": "这项研究通过关注幸福感游戏体验的“影响”部分，填补了数字游戏研究中的一个重要空白。它不仅提供了经验性证据，证明游戏可以带来超越纯粹享乐的深层个人成长，而且其混合方法研究设计增加了结果的可靠性。这项工作对于理解游戏的积极潜力及其在个人发展中的作用具有重要意义。"}}
{"id": "2412.14209", "title": "Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction", "authors": ["Peter E. D. Love", "Jane Matthews", "Weili Fang", "Hadi Mahamivanan"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      74 pages, 5 figures and 3 tables", "url": "http://arxiv.org/abs/2412.14209v2", "summary": "Explainable Artificial Intelligence seeks to make the reasoning processes of\nAI models transparent and interpretable, particularly in complex decision\nmaking environments. In the construction industry, where AI based decision\nsupport systems are increasingly adopted, limited attention has been paid to\nthe integration of supporting evidence that underpins the reliability and\naccountability of AI generated outputs. The absence of such evidence undermines\nthe validity of explanations and the trustworthiness of system recommendations.\nThis paper addresses this gap by introducing a theoretical, evidence based\nmeans end framework developed through a narrative review. The framework offers\nan epistemic foundation for designing XAI enabled DSS that generate meaningful\nexplanations tailored to users knowledge needs and decision contexts. It\nfocuses on evaluating the strength, relevance, and utility of different types\nof evidence supporting AI generated explanations. While developed with\nconstruction professionals as primary end users, the framework is also\napplicable to developers, regulators, and project managers with varying\nepistemic goals.", "comment": "74 pages, 5 figures and 3 tables", "pdf_url": "http://arxiv.org/pdf/2412.14209v2", "cate": "cs.HC", "date": "2024-12-17", "updated": "2025-07-24", "AI": {"title_translation": "将证据整合到XAI和基于AI的决策支持系统设计中：面向建筑领域最终用户的手段-目的框架", "tldr": "论文提出了一个基于证据的手段-目的框架，旨在增强建筑行业XAI和AI决策支持系统的解释性和可信度。", "motivation": "现有AI决策支持系统在建筑行业应用中，对支持证据的整合关注不足，导致解释的有效性和系统推荐的可信度受损。", "method": "通过叙述性综述（narrative review）开发了一个理论性的、基于证据的手段-目的框架。", "result": "提出了一个为XAI赋能的DSS设计提供认知基础的框架，该框架能够生成针对用户知识需求和决策背景的解释，并侧重于评估不同类型证据的强度、相关性和效用。", "conclusion": "该框架为设计可信赖的XAI系统提供了理论基础，尽管主要面向建筑专业人员，但也适用于开发者、监管者和项目经理。", "translation": "解释性人工智能旨在使人工智能模型的推理过程透明和可解释，尤其是在复杂的决策环境中。在建筑行业中，基于人工智能的决策支持系统正被越来越多地采用，但对支持人工智能生成输出的可靠性和问责制的基础证据的整合关注有限。缺乏此类证据会损害解释的有效性和系统建议的可信度。本文通过引入一个通过叙述性综述开发的理论性、基于证据的手段-目的框架来解决这一空白。该框架为设计支持XAI的决策支持系统提供了认知基础，这些系统能够生成针对用户知识需求和决策背景的有意义的解释。它侧重于评估支持人工智能生成解释的不同类型证据的强度、相关性和效用。虽然该框架主要针对建筑专业人员作为主要最终用户开发，但也适用于具有不同认知目标的开发人员、监管人员和项目经理。", "summary": "本文针对建筑行业中AI决策支持系统缺乏证据整合导致可信度不足的问题，提出了一个理论性的、基于证据的手段-目的框架。该框架通过叙述性综述开发，旨在为设计XAI赋能的决策支持系统提供认知基础，使其能生成符合用户需求的解释，并评估支持证据的强度、相关性和效用。该框架不仅适用于建筑专业人员，也对其他相关利益者有价值。", "keywords": "解释性人工智能, 决策支持系统, 证据整合, 建筑行业, 手段-目的框架", "comments": "该论文提出了一种创新的方法，通过整合证据来增强XAI系统的可解释性和可信度，特别是在对可靠性要求较高的建筑行业。其“手段-目的框架”为设计更具透明度和问责制的AI决策支持系统提供了理论指导，具有重要的实践意义。"}}
{"id": "2507.18372", "title": "On Reconstructing Training Data From Bayesian Posteriors and Trained Models", "authors": ["George Wynne"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18372v1", "summary": "Publicly releasing the specification of a model with its trained parameters\nmeans an adversary can attempt to reconstruct information about the training\ndata via training data reconstruction attacks, a major vulnerability of modern\nmachine learning methods. This paper makes three primary contributions:\nestablishing a mathematical framework to express the problem, characterising\nthe features of the training data that are vulnerable via a maximum mean\ndiscrepancy equivalance and outlining a score matching framework for\nreconstructing data in both Bayesian and non-Bayesian models, the former is a\nfirst in the literature.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18372v1", "cate": "stat.ML", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "从贝叶斯后验和训练模型中重构训练数据", "tldr": "本文提出了一个数学框架和分数匹配方法，用于从贝叶斯和非贝叶斯模型中重构训练数据，以解决现代机器学习中的数据重构攻击漏洞。", "motivation": "公开模型及其训练参数会导致训练数据重构攻击，这是现代机器学习方法的一个主要漏洞。本文旨在解决这一问题。", "method": "建立了表达该问题的数学框架；通过最大均值差异等价性表征了易受攻击的训练数据特征；概述了用于贝叶斯和非贝叶斯模型中重构数据的分数匹配框架，其中后者是文献中的首次提出。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "公开模型及其训练参数的规范意味着攻击者可以尝试通过训练数据重构攻击来重构训练数据的信息，这是现代机器学习方法的一个主要漏洞。本文做出了三个主要贡献：建立了一个表达该问题的数学框架；通过最大均值差异等价性表征了易受攻击的训练数据特征；并概述了一个用于贝叶斯和非贝叶斯模型中重构数据的分数匹配框架，其中后者在文献中尚属首次。", "summary": "本文针对现代机器学习中训练数据重构攻击的漏洞，提出了一个解决该问题的数学框架。研究通过最大均值差异等价性识别了易受攻击的训练数据特征，并引入了一种分数匹配框架，用于从贝叶斯和非贝叶斯模型中重构数据，尤其是在贝叶斯模型中的应用是首次提出。", "keywords": "训练数据重构, 贝叶斯后验, 分数匹配, 机器学习安全, 最大均值差异", "comments": "本文的创新之处在于首次将分数匹配框架应用于贝叶斯模型的数据重构，并为理解和缓解训练数据重构攻击提供了重要的数学框架和方法。"}}
{"id": "2507.18630", "title": "Design and optimization of a novel leaf-shape antenna for RF energy transfer", "authors": ["Junbin Zhong", "Mingtong Chen", "Zhengbao Yang"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18630v1", "summary": "In this research, the design and optimization of a novel leaf-shaped antenna\ninspired by natural leaf structures for radio frequency energy transfer is\npresented. The objectives of this study are to develop a bio-inspired antenna,\noptimize its performance through impedance matching for the 915 MHz frequency\nband, and evaluate its efficiency in capturing RF energy. The design process\ninvolves selecting an appropriate leaf shape, modeling the antenna using\nAutoCAD and HFSS software, and fabricating a printed circuit board (PCB)\nprototype. Simulations and physical tests are conducted to optimize the\nantennas performance, achieving an S11 parameter of nearly -20 dB at 915 MHz,\nindicating effective energy capture. Experimental results demonstrate the\nantennas ability to power a device at distances up to 200 cm, with charging\ntimes reflecting its efficiency. The study concludes that the bio-inspired\ndesign of the proposed antenna improves RF energy transfer. Future work should\nfocus on testing the antennas penetration through concrete and developing a\nfeedback system for autonomous alignment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18630v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于射频能量传输的新型叶形天线的设计与优化", "tldr": "一种受生物启发的叶形天线被设计并优化用于射频能量传输，在915 MHz表现良好，可为200厘米范围内的设备供电。", "motivation": "开发一种受生物启发的叶形天线，优化其在915 MHz频段的性能，并评估其捕获射频能量的效率。", "method": "选择合适的叶形，使用AutoCAD和HFSS软件进行天线建模，并制造PCB原型。通过仿真和物理测试优化天线性能。", "result": "在915 MHz频率下，S11参数接近-20 dB，表明能量捕获有效。实验结果显示天线能够在最远200厘米的距离为设备供电，充电时间反映了其效率。", "conclusion": "该研究得出结论，所提出的天线生物启发式设计提高了射频能量传输。", "translation": "本研究提出了一种受自然叶片结构启发的新型叶形天线的设计与优化，用于射频能量传输。本研究的目标是开发一种仿生天线，通过阻抗匹配优化其在915 MHz频段的性能，并评估其捕获射频能量的效率。设计过程包括选择合适的叶形，使用AutoCAD和HFSS软件对天线进行建模，并制作印刷电路板（PCB）原型。通过仿真和物理测试来优化天线性能，在915 MHz频率下S11参数接近-20 dB，表明能量捕获有效。实验结果表明，该天线能够在最远200厘米的距离为设备供电，充电时间反映了其效率。研究得出结论，所提出的天线的仿生设计提高了射频能量传输。未来的工作应侧重于测试天线穿透混凝土的能力，并开发用于自主对准的反馈系统。", "summary": "本文介绍了一种用于射频能量传输的新型仿生叶形天线的设计与优化。通过仿真和物理测试，该天线在915 MHz频段的性能得到优化，S11参数接近-20 dB。实验结果表明，该天线能够在最远200厘米的距离为设备供电，证明其提高了射频能量传输效率。", "keywords": "叶形天线, 射频能量传输, 仿生设计, 阻抗匹配, 915 MHz", "comments": "该研究提出了一种新颖的仿生叶形天线设计，其创新点在于将自然叶片结构应用于天线设计。该天线在射频能量传输方面表现出良好性能，有望提高无线能量传输的效率。未来的工作将关注穿透混凝土的能力和自主对准系统，这表明目前在这些方面可能存在局限性。"}}
{"id": "2507.17850", "title": "Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment", "authors": ["Rodrigo Moreira", "Larissa F. Rodrigues Moreira", "Flávio de Oliveira Silva"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17850v1", "summary": "The deployment of large-scale software-based 5G core functions presents\nsignificant challenges due to their reliance on optimized and intelligent\nresource provisioning for their services. Many studies have focused on\nanalyzing the impact of resource allocation for complex deployments using\nmathematical models, queue theories, or even Artificial Intelligence (AI). This\npaper elucidates the effects of chaotic workloads, generated by Distributed\nDenial of Service (DDoS) on different Network Functions (NFs) on User Equipment\nregistration performance. Our findings highlight the necessity of diverse\nresource profiles to ensure Service-Level Agreement (SLA) compliance in\nlarge-scale 5G core deployments. Additionally, our analysis of packet capture\napproaches demonstrates the potential of kernel-based monitoring for scalable\nsecurity threat defense. Finally, our empirical evaluation provides insights\ninto the effective deployment of 5G NFs in complex scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17850v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "大规模5G核心网部署中的性能评估与威胁缓解", "tldr": "本文评估了DDoS攻击对大规模5G核心网性能的影响，并提出多样化资源配置和基于内核的监控以确保SLA合规性和有效防御威胁。", "motivation": "大规模软件化5G核心网功能的部署面临重大挑战，尤其是在优化和智能资源配置方面。", "method": "本文阐明了由分布式拒绝服务（DDoS）产生的混乱工作负载对不同网络功能（NFs）的用户设备注册性能的影响。此外，分析了数据包捕获方法，并进行了实证评估。", "result": "研究发现，为确保大规模5G核心网部署中的服务水平协议（SLA）合规性，需要多样化的资源配置文件。同时，对数据包捕获方法的分析表明，基于内核的监控在可扩展安全威胁防御方面具有潜力。", "conclusion": "本文的实证评估为复杂场景下5G网络功能的有效部署提供了见解。", "translation": "大规模软件化5G核心功能的部署由于其对服务优化和智能资源配置的依赖而面临重大挑战。许多研究集中于使用数学模型、排队论甚至人工智能（AI）来分析复杂部署中资源分配的影响。本文阐明了由分布式拒绝服务（DDoS）产生的混乱工作负载对不同网络功能（NFs）的用户设备注册性能的影响。我们的发现强调了多样化资源配置的必要性，以确保大规模5G核心网部署中的服务水平协议（SLA）合规性。此外，我们对数据包捕获方法的分析表明，基于内核的监控在可扩展安全威胁防御方面具有潜力。最后，我们的实证评估为复杂场景下5G网络功能的有效部署提供了见解。", "summary": "本文研究了大规模软件化5G核心网部署中的性能挑战和威胁缓解策略。通过阐明DDoS攻击对网络功能和用户设备注册性能的影响，研究强调了多样化资源配置对满足SLA的重要性。同时，提出了基于内核的监控作为一种可扩展的安全威胁防御方法。实证评估为复杂场景下5G网络功能的有效部署提供了实用见解。", "keywords": "5G核心网, 性能评估, DDoS攻击, 威胁缓解, 资源配置", "comments": "本文关注大规模5G核心网部署中的实际挑战，特别是DDoS攻击下的性能和资源管理。其创新点在于结合了性能评估和威胁缓解，并提出了基于内核的监控方案，这对于保障5G网络的稳定性和安全性具有重要意义。实证评估的加入增加了研究的实用价值。"}}
{"id": "2503.05674", "title": "Multiple solutions to the static forward free-boundary Grad-Shafranov problem on MAST-U", "authors": ["K. Pentland", "N. C. Amorisco", "P. E. Farrell", "C. J. Ham"], "categories": ["physics.plasm-ph", "cs.NA", "math.NA"], "primary_category": "Subjects:       Plasma Physics (physics.plasm-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05674v4", "summary": "The Grad-Shafranov (GS) equation is a nonlinear elliptic partial differential\nequation that governs the ideal magnetohydrodynamic equilibrium of a tokamak\nplasma. Previous studies have demonstrated the existence of multiple solutions\nto the GS equation when solved in idealistic geometries with simplified plasma\ncurrent density profiles and boundary conditions. Until now, the question of\nwhether multiple equilibria might exist in real-world tokamak geometries with\nmore complex current density profiles and integral free-boundary conditions\n(commonly used in production-level equilibrium codes) has remained unanswered.\nIn this work, we discover multiple solutions to the static forward\nfree-boundary GS problem in the MAST-U tokamak geometry using the validated\nevolutive equilibrium solver FreeGSNKE and the deflated continuation algorithm.\nBy varying the plasma current, current density profile coefficients, or coil\ncurrents in the GS equation, we identify and characterise distinct equilibrium\nsolutions, including both deeply and more shallowly confined plasma states. We\nsuggest that the existence of even more equilibria is likely prohibited by the\nrestrictive nature of the integral free-boundary condition, which globally\ncouples poloidal fluxes on the computational boundary with those on the\ninterior. We conclude by discussing the implications of these findings for\nwider equilibrium modelling and emphasise the need to explore whether multiple\nsolutions are present in other equilibrium codes and tokamaks, as well as their\npotential impact on downstream simulations that rely on GS equilibria.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05674v4", "cate": "physics.plasm-ph", "date": "2025-03-07", "updated": "2025-07-24", "AI": {"title_translation": "MAST-U上静态正向自由边界Grad-Shafranov问题的多重解", "tldr": "本研究在MAST-U托卡马克几何中，首次发现了静态正向自由边界Grad-Shafranov问题的多个等离子体平衡解，证明了真实托卡马克中多重平衡态的存在性。", "motivation": "之前的研究在理想化几何和简化条件下证明了Grad-Shafranov (GS) 方程存在多个解。然而，在具有更复杂电流密度分布和积分自由边界条件（生产级平衡代码常用）的真实世界托卡马克几何中，是否存在多个平衡态的问题一直未得到解答。", "method": "本研究使用MAST-U托卡马克几何，利用经过验证的演化平衡求解器FreeGSNKE和紧缩连续算法。通过改变等离子体电流、电流密度剖面系数或线圈电流来识别和表征不同的平衡解。", "result": "研究在MAST-U托卡马克几何中发现了静态正向自由边界GS问题的多个解。识别并表征了不同的平衡解，包括深层和浅层约束的等离子体状态。研究表明，积分自由边界条件的限制性可能阻止了更多平衡态的存在。", "conclusion": "本研究讨论了这些发现对更广泛的平衡建模的启示，并强调需要探索其他平衡代码和托卡马克中是否存在多重解，以及它们对依赖GS平衡的下游模拟的潜在影响。", "translation": "Grad-Shafranov (GS) 方程是一个非线性椭圆偏微分方程，它描述了托卡马克等离子体的理想磁流体力学平衡。之前的研究已经证明，在理想化的几何结构中，当使用简化的等离子体电流密度剖面和边界条件求解GS方程时，存在多个解。然而，迄今为止，在具有更复杂电流密度剖面和积分自由边界条件（常用于生产级平衡代码）的真实世界托卡马克几何中，是否存在多个平衡态的问题仍未得到解答。在这项工作中，我们利用经过验证的演化平衡求解器FreeGSNKE和紧缩连续算法，在MAST-U托卡马克几何中发现了静态正向自由边界GS问题的多个解。通过改变GS方程中的等离子体电流、电流密度剖面系数或线圈电流，我们识别并表征了不同的平衡解，包括深层和浅层约束的等离子体状态。我们认为，积分自由边界条件的限制性很可能阻止了更多平衡态的存在，因为它将计算边界上的极向磁通与内部的极向磁通全局耦合起来。最后，我们讨论了这些发现对更广泛的平衡建模的启示，并强调需要探索其他平衡代码和托卡马克中是否存在多重解，以及它们对依赖GS平衡的下游模拟的潜在影响。", "summary": "本论文研究了在真实世界托卡马克（MAST-U）中Grad-Shafranov (GS) 方程多重解的存在性，解决了此前在复杂几何条件下未解答的问题。通过使用FreeGSNKE求解器和紧缩连续算法，作者成功地通过改变等离子体电流等参数，识别并表征了多个不同的等离子体平衡态。研究结果表明，尽管存在多个解，但积分自由边界条件可能限制了总数。这项工作强调了评估其他托卡马克代码中多重平衡态存在及其对未来模拟影响的关键需求。", "keywords": "Grad-Shafranov方程, 托卡马克, 多重解, 等离子体平衡, MAST-U", "comments": "本研究的创新之处在于，首次在真实世界的托卡马克几何（MAST-U）中，在复杂条件下证明了Grad-Shafranov方程存在多重解，超越了理想化的理论案例。这非常重要，因为它挑战了实际托卡马克操作和平衡建模中唯一平衡解的假设。发现积分自由边界条件可能限制解的数量，也为问题约束提供了有趣的见解。这项工作对依赖GS平衡的下游模拟的准确性和可靠性具有重要意义。"}}
{"id": "2507.18344", "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM", "authors": ["Gyuhyeon Pak", "Hae Min Cho", "Euntai Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2507.18344v1", "summary": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18344v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "G2S-ICP SLAM：几何感知高斯泼溅ICP SLAM", "tldr": "G2S-ICP SLAM是一种新型的几何感知RGB-D高斯泼溅SLAM系统，它通过将场景元素表示为受局部切平面约束的2D高斯圆盘，实现了实时高保真3D重建和鲁棒的相机姿态跟踪，并在定位精度和重建完整性方面优于现有系统。", "motivation": "为了实现实时高保真3D重建和鲁棒的相机姿态跟踪，并解决传统3D椭球表示在深度解释上的一致性问题，本文提出了一种新型的几何感知RGB-D高斯泼溅SLAM系统。", "method": "G2S-ICP SLAM系统通过将每个场景元素表示为受局部切平面约束的高斯分布，有效地将局部表面建模为与底层几何对齐的2D高斯圆盘。该方法通过引入各向异性协方差先验，将表面对齐的高斯圆盘嵌入到广义ICP框架中，同时保持底层的配准公式不变。此外，还提出了一种几何感知损失，用于监督光度、深度和法线一致性。", "result": "该系统实现了实时操作，同时保持了视觉和几何保真度。在Replica和TUM-RGBD数据集上的大量实验表明，G2S-ICP SLAM在定位精度和重建完整性方面优于先前的SLAM系统，同时保持了渲染质量。", "conclusion": "本文提出了一种名为G2S-ICP SLAM的新型几何感知RGB-D高斯泼溅SLAM系统，通过引入2D高斯圆盘表示和几何感知损失，实现了实时高保真3D重建和鲁棒相机姿态跟踪，并在多个基准测试中表现出优越的性能。", "translation": "在本文中，我们提出了一种新颖的几何感知RGB-D高斯泼溅SLAM系统，命名为G2S-ICP SLAM。所提出的方法通过使用受局部切平面约束的高斯分布表示每个场景元素，实现了实时高保真3D重建和鲁棒的相机姿态跟踪。这有效地将局部表面建模为与底层几何对齐的2D高斯圆盘，与传统的具有各向同性不确定性的3D椭球表示相比，在多个视角下能够实现更一致的深度解释。为了将这种表示集成到SLAM管线中，我们通过引入各向异性协方差先验，将表面对齐的高斯圆盘嵌入到广义ICP框架中，而无需改变底层的配准公式。此外，我们提出了一种几何感知损失，用于监督光度、深度和法线一致性。我们的系统实现了实时操作，同时保持了视觉和几何保真度。在Replica和TUM-RGBD数据集上的大量实验表明，G2S-ICP SLAM在定位精度、重建完整性方面优于先前的SLAM系统，同时保持了渲染质量。", "summary": "本文介绍了一种名为G2S-ICP SLAM的新型几何感知RGB-D高斯泼溅SLAM系统。该系统通过将场景元素表示为受局部切平面约束的2D高斯圆盘，实现了高保真实时3D重建和鲁棒的相机姿态跟踪。它将这种表示嵌入到广义ICP框架中，并引入了各向异性协方差先验和几何感知损失。实验证明，G2S-ICP SLAM在定位精度和重建完整性方面优于现有SLAM系统，同时保持了良好的渲染质量。", "keywords": "高斯泼溅, ICP SLAM, 几何感知, RGB-D, 3D重建", "comments": "该论文的创新点在于将高斯泼溅技术与ICP SLAM结合，并引入了几何感知的2D高斯圆盘表示，有效解决了传统3D椭球表示在深度解释上的一致性问题。通过将表面对齐的2D高斯圆盘嵌入到广义ICP框架中，并设计几何感知损失，显著提升了系统的定位精度和重建完整性，同时保持了实时性能和渲染质量，对实时高保真SLAM领域具有重要意义。"}}
{"id": "2507.18223", "title": "GenAI for Automotive Software Development: From Requirements to Wheels", "authors": ["Nenad Petrovic", "Fengjunjie Pan", "Vahid Zolfaghari", "Krzysztof Lebioda", "Andre Schamschurko", "Alois Knoll"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18223v1", "summary": "This paper introduces a GenAI-empowered approach to automated development of\nautomotive software, with emphasis on autonomous and Advanced Driver Assistance\nSystems (ADAS) capabilities. The process starts with requirements as input,\nwhile the main generated outputs are test scenario code for simulation\nenvironment, together with implementation of desired ADAS capabilities\ntargeting hardware platform of the vehicle connected to testbench. Moreover, we\nintroduce additional steps for requirements consistency checking leveraging\nModel-Driven Engineering (MDE). In the proposed workflow, Large Language Models\n(LLMs) are used for model-based summarization of requirements (Ecore metamodel,\nXMI model instance and OCL constraint creation), test scenario generation,\nsimulation code (Python) and target platform code generation (C++).\nAdditionally, Retrieval Augmented Generation (RAG) is adopted to enhance test\nscenario generation from autonomous driving regulations-related documents. Our\napproach aims shorter compliance and re-engineering cycles, as well as reduced\ndevelopment and testing time when it comes to ADAS-related capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18223v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "汽车软件开发中的生成式AI：从需求到车轮", "tldr": "论文提出一种利用生成式AI（GenAI）自动化开发汽车软件，特别是自动驾驶和ADAS系统的方法，通过LLMs和RAG实现从需求到测试场景和代码的生成，旨在缩短开发和测试周期。", "motivation": "自动化开发汽车软件，特别是自动驾驶和ADAS系统，旨在缩短合规和再工程周期，以及减少ADAS相关能力的开发和测试时间。", "method": "提出一种由GenAI驱动的汽车软件自动化开发方法。该方法以需求作为输入，主要输出是用于仿真环境的测试场景代码和针对连接到测试台的车辆硬件平台的ADAS功能实现。引入了利用模型驱动工程（MDE）进行需求一致性检查的额外步骤。在工作流程中，大型语言模型（LLMs）用于需求的模型化总结（Ecore元模型、XMI模型实例和OCL约束创建）、测试场景生成、仿真代码（Python）和目标平台代码（C++）生成。此外，采用检索增强生成（RAG）来增强从自动驾驶法规相关文档中生成测试场景。", "result": "该方法旨在缩短合规和再工程周期，以及减少ADAS相关能力的开发和测试时间。", "conclusion": "通过GenAI、LLMs、MDE和RAG的结合，可以实现汽车软件（特别是ADAS）的自动化开发，从而显著缩短开发和测试周期，提高效率。", "translation": "本文介绍了一种由生成式AI（GenAI）赋能的汽车软件自动化开发方法，重点关注自动驾驶和高级驾驶辅助系统（ADAS）功能。该过程以需求作为输入，主要生成输出是用于仿真环境的测试场景代码，以及针对连接到测试台的车辆硬件平台的ADAS功能实现。此外，我们引入了利用模型驱动工程（MDE）进行需求一致性检查的额外步骤。在所提出的工作流程中，大型语言模型（LLMs）用于需求的模型化总结（Ecore元模型、XMI模型实例和OCL约束创建）、测试场景生成、仿真代码（Python）和目标平台代码（C++）生成。此外，采用检索增强生成（RAG）来增强从自动驾驶法规相关文档中生成测试场景。我们的方法旨在缩短合规和再工程周期，以及减少ADAS相关能力的开发和测试时间。", "summary": "本文提出了一种创新的GenAI驱动方法，用于自动化汽车软件开发，特别关注自动驾驶和ADAS功能。该方法将需求作为输入，利用大型语言模型（LLMs）进行需求总结、测试场景生成和多语言代码生成（Python/C++），并通过模型驱动工程（MDE）进行需求一致性检查。此外，结合检索增强生成（RAG）从法规文档中生成测试场景。该方法旨在显著缩短汽车软件开发（特别是ADAS相关功能）的合规、再工程、开发和测试周期。", "keywords": "生成式AI, 汽车软件开发, ADAS, 大型语言模型, 自动化", "comments": "这篇论文的创新点在于将生成式AI（特别是LLMs和RAG）与模型驱动工程（MDE）相结合，应用于复杂的汽车软件开发流程，实现了从需求到代码和测试场景的自动化生成。这种端到端的自动化潜力巨大，尤其是在ADAS和自动驾驶领域，有望显著提高开发效率、缩短上市时间并提升合规性。然而，实际应用中生成代码的质量、安全性验证以及对复杂实时系统的支持程度可能是未来需要进一步探讨的挑战。"}}
{"id": "2507.18106", "title": "Distributional Uncertainty for Out-of-Distribution Detection", "authors": ["JinYoung Kim", "DaeUng Jo", "Kimin Yun", "Jeonghyo Song", "Youngjoon Yoo"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      6 pages , 3 figures , IEEE International Conference on Advanced Visual and Signal-Based Systems", "url": "http://arxiv.org/abs/2507.18106v1", "summary": "Estimating uncertainty from deep neural networks is a widely used approach\nfor detecting out-of-distribution (OoD) samples, which typically exhibit high\npredictive uncertainty. However, conventional methods such as Monte Carlo (MC)\nDropout often focus solely on either model or data uncertainty, failing to\nalign with the semantic objective of OoD detection. To address this, we propose\nthe Free-Energy Posterior Network, a novel framework that jointly models\ndistributional uncertainty and identifying OoD and misclassified regions using\nfree energy. Our method introduces two key contributions: (1) a\nfree-energy-based density estimator parameterized by a Beta distribution, which\nenables fine-grained uncertainty estimation near ambiguous or unseen regions;\nand (2) a loss integrated within a posterior network, allowing direct\nuncertainty estimation from learned parameters without requiring stochastic\nsampling. By integrating our approach with the residual prediction branch (RPL)\nframework, the proposed method goes beyond post-hoc energy thresholding and\nenables the network to learn OoD regions by leveraging the variance of the Beta\ndistribution, resulting in a semantically meaningful and computationally\nefficient solution for uncertainty-aware segmentation. We validate the\neffectiveness of our method on challenging real-world benchmarks, including\nFishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.", "comment": "6 pages , 3 figures , IEEE International Conference on Advanced\n  Visual and Signal-Based Systems", "pdf_url": "http://arxiv.org/pdf/2507.18106v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "分布不确定性用于离群点检测", "tldr": "本文提出了一种名为自由能后验网络（Free-Energy Posterior Network）的新框架，通过联合建模分布不确定性来提高离群点（OoD）检测的语义对齐性，并实现了计算效率高的不确定性感知识别。", "motivation": "传统的深度神经网络不确定性估计方法（如蒙特卡洛Dropout）在离群点检测中，通常只关注模型或数据不确定性，未能与离群点检测的语义目标对齐。", "method": "本文提出自由能后验网络（Free-Energy Posterior Network），该框架联合建模分布不确定性，并利用自由能识别离群点和错误分类区域。该方法引入了两项关键贡献：1) 基于Beta分布参数化的自由能密度估计器，能够在模糊或未见区域附近进行精细的不确定性估计；2) 集成在后验网络中的损失函数，允许直接从学习到的参数估计不确定性，无需随机采样。该方法与残差预测分支（RPL）框架相结合，通过利用Beta分布的方差使网络学习离群点区域。", "result": "该方法在具有挑战性的真实世界基准测试（包括Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can）上验证了其有效性，实现了语义上有意义且计算高效的不确定性感知识别。", "conclusion": "本文提出的自由能后验网络通过联合建模分布不确定性，并结合残差预测分支框架，为离群点检测提供了一种语义有意义且计算高效的解决方案，超越了传统的后验能量阈值方法。", "translation": "从深度神经网络估计不确定性是检测离群点（OoD）样本的常用方法，这些样本通常表现出较高的预测不确定性。然而，传统的蒙特卡洛（MC）Dropout等方法往往只关注模型或数据不确定性，未能与离群点检测的语义目标对齐。为了解决这个问题，我们提出了一种新颖的框架——自由能后验网络（Free-Energy Posterior Network），它通过联合建模分布不确定性并利用自由能识别离群点和错误分类区域。我们的方法引入了两项关键贡献：(1) 一个由Beta分布参数化的基于自由能的密度估计器，它能够在模糊或未见区域附近实现精细的不确定性估计；(2) 一个集成在后验网络中的损失函数，允许直接从学习到的参数估计不确定性，而无需随机采样。通过将我们的方法与残差预测分支（RPL）框架相结合，所提出的方法超越了后验能量阈值，并通过利用Beta分布的方差使网络学习离群点区域，从而为不确定性感知识别提供了一个语义上有意义且计算高效的解决方案。我们在具有挑战性的真实世界基准测试（包括Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can）上验证了我们方法的有效性。", "summary": "本文提出了一种新颖的自由能后验网络（Free-Energy Posterior Network），旨在解决传统离群点（OoD）检测中不确定性估计与语义目标不符的问题。该方法通过引入基于Beta分布的自由能密度估计器和无需采样的后验网络损失函数，联合建模分布不确定性，并与残差预测分支（RPL）框架集成，使得网络能够学习OoD区域。实验证明，该方法在多个真实世界基准测试中实现了语义有意义且计算高效的不确定性感知识别。", "keywords": "分布不确定性, 离群点检测, 自由能, 后验网络, Beta分布", "comments": "该论文的创新点在于提出了自由能后验网络，通过联合建模分布不确定性来解决传统方法中不确定性估计与OoD检测语义目标不一致的问题。特别是，它引入了基于Beta分布的密度估计器和无需随机采样的直接不确定性估计方法，提高了估计的精细度和计算效率。与RPL框架的结合也使其超越了简单的后验阈值方法，使得网络能够主动学习OoD区域，这对于实际应用中的不确定性感知识别具有重要意义。"}}
{"id": "2506.22495", "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      there are factual errors", "url": "http://arxiv.org/abs/2506.22495v2", "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.", "comment": "there are factual errors", "pdf_url": "http://arxiv.org/pdf/2506.22495v2", "cate": "eess.SP", "date": "2025-06-25", "updated": "2025-07-24", "AI": {"title_translation": "感知心脏的掩码自编码器：揭示心电图分析中的简单性偏差", "tldr": "监督式心电图模型存在简单性偏差（SB），忽略了细微但关键的线索。本文提出了一种基于自监督学习（SSL）的方法，结合时频感知滤波器和多粒度原型重建，以减轻SB并提高心电图分析性能，实现了最先进的结果。", "motivation": "监督式心电图（ECG）模型倾向于过度拟合主导和重复的模式，从而忽视了细微但临床上至关重要的线索，这种现象被称为“简单性偏差”（Simplicity Bias, SB），即模型偏好易于学习的信号，而不是细微但信息丰富的信号，这严重影响了诊断性能。自监督学习（SSL）被发现可以缓解这种偏差，为解决该问题提供了有前景的方向。", "method": "本研究首先通过实证方法证明了心电图分析中简单性偏差（SB）的存在及其对诊断性能的负面影响，并同时发现自监督学习（SSL）可以缓解这一偏差。在此基础上，提出了一种新颖的自监督学习方法，包含两个关键组件：1）时频感知滤波器（Temporal-Frequency aware Filters），用于捕获反映心电信号动态特性的时频特征；2）在此基础上构建的多粒度原型重建（Multi-Grained Prototype Reconstruction），用于跨双域进行粗粒度和细粒度的表示学习，进一步减轻SB。为推动心电图分析中的SSL发展，还整理了一个包含来自300多个临床中心的153万份记录的大规模多中心心电图数据集。", "result": "在六个心电图数据集上的三个下游任务实验表明，所提出的方法有效降低了简单性偏差，并实现了最先进的性能。", "conclusion": "本研究提出的自监督学习方法能够有效减轻心电图分析中的简单性偏差，从而显著提高诊断性能并达到最先进水平。", "translation": "心电图（ECG）的诊断价值在于其动态特性，从心律波动到随时间和频率域演变的细微波形变形。然而，监督式心电图模型倾向于过度拟合主导和重复的模式，忽视了细微但临床上至关重要的线索，这种现象被称为简单性偏差（Simplicity Bias, SB），即模型偏好易于学习的信号，而不是细微但信息丰富的信号。在这项工作中，我们首先通过实证方法证明了心电图分析中SB的存在及其对诊断性能的负面影响，同时发现自监督学习（SSL）可以缓解这一偏差，为解决该偏差提供了有前景的方向。遵循SSL范式，我们提出了一种新颖的方法，包含两个关键组件：1）时频感知滤波器，用于捕获反映心电信号动态特性的时频特征；2）在此基础上构建的多粒度原型重建，用于跨双域进行粗粒度和细粒度的表示学习，进一步减轻SB。为了推进心电图分析中的SSL，我们整理了一个包含来自300多个临床中心的153万份记录的大规模多中心心电图数据集。在六个心电图数据集上的三个下游任务实验表明，我们的方法有效降低了SB并实现了最先进的性能。代码和数据集将公开发布。", "summary": "本研究探讨了监督式心电图（ECG）模型中存在的“简单性偏差”（SB）问题，即模型倾向于学习易于识别的模式而忽略临床上关键的细微特征，从而影响诊断准确性。作者通过实证方法证明了SB的存在，并发现自监督学习（SSL）能够有效缓解这一问题。在此基础上，论文提出了一种新的SSL方法，该方法包含“时频感知滤波器”以捕捉ECG信号的动态特性，以及“多粒度原型重建”以实现跨域的粗细粒度表示学习，从而进一步减轻SB。此外，研究还构建了一个包含153万条记录的大规模多中心ECG数据集。实验结果表明，所提出的方法能够有效减少SB并达到当前最先进的性能。", "keywords": "简单性偏差, 心电图分析, 自监督学习, 掩码自编码器, 时频特征", "comments": "该论文解决了医学AI领域一个关键问题——简单性偏差，这对于心电图分析等敏感的诊断任务至关重要。其创新点在于将自监督学习范式应用于ECG分析，并设计了独特的组件，如时频感知滤波器和多粒度原型重建，以专门处理ECECG信号的动态性和细微性。此外，构建一个大规模、多中心ECG数据集也对推动该领域的发展具有重要意义，有助于缓解医疗数据稀缺的挑战。"}}
{"id": "2507.18517", "title": "Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs", "authors": ["Bolutife Atoki", "Jenny Benois-Pineau", "Renaud Péteri", "Fabien Baldacci", "Aymar de Rugy"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18517v1", "summary": "In this work, we address the problem of semantic object segmentation using\nfoundation models. We investigate whether foundation models, trained on a large\nnumber and variety of objects, can perform object segmentation without\nfine-tuning on specific images containing everyday objects, but in highly\ncluttered visual scenes. The ''in the wild'' context is driven by the target\napplication of vision guided upper limb neuroprostheses. We propose a method\nfor generating prompts based on gaze fixations to guide the Segment Anything\nModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visual\ndata. Evaluation results of our approach show an improvement of the IoU\nsegmentation quality metric by up to 0.51 points on real-world challenging data\nof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform\n(https://universe.roboflow.com/iwrist/grasping-in-the-wild)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18517v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "野外场景中基于基础模型的物体分割：在视觉辅助上肢神经假肢中的应用", "tldr": "研究人员利用基础模型（SAM）结合注视点生成提示，并在以自我为中心的视觉数据上进行微调，以改善上肢神经假肢应用中的野外场景物体分割效果，IoU提升高达0.51点。", "motivation": "解决在高度杂乱的“野外场景”中，基础模型无需针对特定日常物体图像进行微调即可执行物体分割的问题，特别是为了视觉引导上肢神经假肢的应用。", "method": "提出了一种基于注视点生成提示的方法来引导Segment Anything Model (SAM) 进行分割，并使用以自我为中心的视觉数据对其进行微调。", "result": "在Grasping-in-the-Wild语料库的真实世界挑战性数据上，IoU分割质量指标提高了0.51点。", "conclusion": "结合注视点提示和以自我为中心的微调，基础模型（如SAM）可以在复杂的野外场景中有效提升物体分割性能，尤其适用于视觉辅助神经假肢。", "translation": "在这项工作中，我们利用基础模型解决了语义对象分割问题。我们研究了在大量和各种对象上训练的基础模型，是否可以在无需针对包含日常对象但高度杂乱的视觉场景的特定图像进行微调的情况下执行对象分割。“野外”上下文由视觉引导上肢神经假肢的目标应用驱动。我们提出了一种基于注视点生成提示的方法，以引导我们的分割场景中的Segment Anything Model (SAM)，并以自我为中心的视觉数据对其进行微调。我们的方法评估结果显示，在Grasping-in-the-Wild语料库的真实世界挑战性数据上，IoU分割质量指标提高了0.51点，该语料库可在RoboFlow平台（https://universe.roboflow.com/iwrist/grasping-in-the-wild）上获取。", "summary": "本文探讨了在高度杂乱的野外场景中，如何利用基础模型（如SAM）进行语义物体分割，特别针对视觉辅助上肢神经假肢的应用。作者提出了一种新方法，通过注视点生成提示来引导SAM，并使用以自我为中心的视觉数据对其进行微调。实验结果表明，该方法在真实世界数据上显著提升了IoU分割质量，最高达0.51点。", "keywords": "基础模型, 物体分割, 神经假肢, SAM, 野外场景", "comments": "这项工作创新性地将基础模型应用于复杂的野外场景物体分割，并聚焦于神经假肢这一重要应用。通过结合注视点提示和以自我为中心的微调，有效地提升了分割性能，为未来视觉引导辅助设备的发展提供了有益探索。其在真实世界数据集上的显著性能提升，验证了方法的有效性。"}}
{"id": "2503.11937", "title": "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder", "authors": ["Wonwoong Cho", "Yan-Ying Chen", "Matthew Klenk", "David I. Inouye", "Yanxia Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV'25 (Highlight), The project page is available at this https URL", "url": "http://arxiv.org/abs/2503.11937v4", "summary": "Text-to-Image (T2I) Diffusion Models have achieved remarkable performance in\ngenerating high quality images. However, enabling precise control of continuous\nattributes, especially multiple attributes simultaneously, in a new domain\n(e.g., numeric values like eye openness or car width) with text-only guidance\nremains a significant challenge. To address this, we introduce the Attribute\n(Att) Adapter, a novel plug-and-play module designed to enable fine-grained,\nmulti-attributes control in pretrained diffusion models. Our approach learns a\nsingle control adapter from a set of sample images that can be unpaired and\ncontain multiple visual attributes. The Att-Adapter leverages the decoupled\ncross attention module to naturally harmonize the multiple domain attributes\nwith text conditioning. We further introduce Conditional Variational\nAutoencoder (CVAE) to the Att-Adapter to mitigate overfitting, matching the\ndiverse nature of the visual world. Evaluations on two public datasets show\nthat Att-Adapter outperforms all LoRA-based baselines in controlling continuous\nattributes. Additionally, our method enables a broader control range and also\nimproves disentanglement across multiple attributes, surpassing StyleGAN-based\ntechniques. Notably, Att-Adapter is flexible, requiring no paired synthetic\ndata for training, and is easily scalable to multiple attributes within a\nsingle model.", "comment": "ICCV'25 (Highlight), The project page is available at\n  https://tri-mac.github.io/att-adapter/", "pdf_url": "http://arxiv.org/pdf/2503.11937v4", "cate": "cs.CV", "date": "2025-03-15", "updated": "2025-07-24", "AI": {"title_translation": "Att-Adapter：一种基于条件变分自编码器的鲁棒精确领域特定多属性T2I扩散适配器", "tldr": "Att-Adapter是一个即插即用的模块，用于在预训练的T2I扩散模型中实现对新领域中连续多属性的精确控制，且无需配对数据，超越了LoRA和StyleGAN基线。", "motivation": "尽管文本到图像（T2I）扩散模型在图像生成方面表现出色，但在新领域中（例如，眼睛睁开程度或汽车宽度等数值属性）仅通过文本指导来精确控制连续属性，尤其是同时控制多个属性，仍然是一个重大挑战。", "method": "本文引入了Att-Adapter，一个即插即用的模块，用于在预训练扩散模型中实现细粒度、多属性控制。它通过学习一组非配对且包含多个视觉属性的样本图像来训练一个控制适配器。Att-Adapter利用解耦的交叉注意力模块来协调多个领域属性与文本条件。此外，引入条件变分自编码器（CVAE）以减轻过拟合。", "result": "Att-Adapter在两个公共数据集上的评估显示，在控制连续属性方面优于所有基于LoRA的基线。它还实现了更宽的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。", "conclusion": "Att-Adapter提供了一种鲁棒且精确的方法，可以在T2I扩散模型中实现领域特定的多属性控制，且无需配对合成数据进行训练，易于扩展到单个模型中的多个属性。", "translation": "文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著的性能。然而，在新领域中（例如，眼睛睁开程度或汽车宽度等数值属性）仅通过文本指导来精确控制连续属性，尤其是同时控制多个属性，仍然是一个重大挑战。为了解决这个问题，我们引入了属性（Att）适配器，这是一种新颖的即插即用模块，旨在使预训练的扩散模型能够进行细粒度、多属性控制。我们的方法从一组可以是非配对且包含多个视觉属性的样本图像中学习单个控制适配器。Att-Adapter利用解耦的交叉注意力模块自然地协调多个领域属性与文本条件。我们进一步将条件变分自编码器（CVAE）引入Att-Adapter，以减轻过拟合，匹配视觉世界的多元性质。在两个公共数据集上的评估表明，Att-Adapter在控制连续属性方面优于所有基于LoRA的基线。此外，我们的方法实现了更宽的控制范围，并改善了多个属性之间的解耦，超越了基于StyleGAN的技术。值得注意的是，Att-Adapter非常灵活，训练不需要配对的合成数据，并且可以轻松扩展到单个模型中的多个属性。", "summary": "本文提出了Att-Adapter，一个用于Text-to-Image (T2I) 扩散模型的即插即用模块，旨在解决在特定领域中精确控制连续多属性的挑战。Att-Adapter通过学习非配对样本图像中的属性，利用解耦的交叉注意力模块协调文本和多领域属性，并结合条件变分自编码器（CVAE）来防止过拟合。实验证明，Att-Adapter在控制连续属性方面优于LoRA和StyleGAN基线，提供更宽的控制范围和更好的属性解耦，且无需配对训练数据，易于扩展。", "keywords": "T2I扩散模型, 属性控制, Att-Adapter, 条件变分自编码器, 多属性控制", "comments": "Att-Adapter的创新性在于其即插即用的设计以及在无需配对数据的情况下实现多属性精细控制的能力，这在T2I扩散模型领域是一个重要的突破。引入CVAE以减轻过拟合，并利用解耦交叉注意力来协调属性，提升了模型的鲁棒性和精确性。其灵活性和可扩展性使其在实际应用中具有巨大潜力。"}}
{"id": "2507.17888", "title": "Learning to Locate: GNN-Powered Vulnerability Path Discovery in Open Source Code", "authors": ["Nima Atashin", "Behrouz Tork Ladani", "Mohammadreza Sharbaf"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 pages, 5 Figures", "url": "http://arxiv.org/abs/2507.17888v1", "summary": "Detecting security vulnerabilities in open-source software is a critical task\nthat is highly regarded in the related research communities. Several approaches\nhave been proposed in the literature for detecting vulnerable codes and\nidentifying the classes of vulnerabilities. However, there is still room to\nwork in explaining the root causes of detected vulnerabilities through locating\nvulnerable statements and the discovery of paths leading to the activation of\nthe vulnerability. While frameworks like SliceLocator offer explanations by\nidentifying vulnerable paths, they rely on rule-based sink identification that\nlimits their generalization. In this paper, we introduce VulPathFinder, an\nexplainable vulnerability path discovery framework that enhances SliceLocator's\nmethodology by utilizing a novel Graph Neural Network (GNN) model for detecting\nsink statements, rather than relying on predefined rules. The proposed GNN\ncaptures semantic and syntactic dependencies to find potential sink points\n(PSPs), which are candidate statements where vulnerable paths end. After\ndetecting PSPs, program slicing can be used to extract potentially vulnerable\npaths, which are then ranked by feeding them back into the target graph-based\ndetector. Ultimately, the most probable path is returned, explaining the root\ncause of the detected vulnerability. We demonstrated the effectiveness of the\nproposed approach by performing evaluations on a benchmark of the buffer\noverflow CWEs from the SARD dataset, providing explanations for the\ncorresponding detected vulnerabilities. The results show that VulPathFinder\noutperforms both original SliceLocator and GNNExplainer (as a general GNN\nexplainability tool) in discovery of vulnerability paths to identified PSPs.", "comment": "8 pages, 5 Figures", "pdf_url": "http://arxiv.org/pdf/2507.17888v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "学会定位：GNN驱动的开源代码漏洞路径发现", "tldr": "VulPathFinder利用图神经网络（GNN）在开源代码中发现漏洞激活路径，通过检测潜在的汇聚点并提取和排序路径，优于现有方法，为检测到的漏洞提供可解释的根源。", "motivation": "在开源软件中检测安全漏洞是一项关键任务，但现有方法在通过定位漏洞语句和发现导致漏洞激活的路径来解释漏洞的根本原因方面仍有改进空间。像SliceLocator这样的框架依赖于基于规则的汇聚点识别，这限制了其泛化能力。", "method": "本文引入了VulPathFinder，一个可解释的漏洞路径发现框架。它通过使用新颖的图神经网络（GNN）模型来检测汇聚点，而不是依赖预定义规则，从而增强了SliceLocator的方法。该GNN捕获语义和句法依赖性以找到潜在汇聚点（PSPs），即漏洞路径结束的候选语句。检测到PSP后，使用程序切片提取潜在的漏洞路径，然后通过将其反馈到目标基于图的检测器进行排序，最终返回最可能的路径，解释检测到的漏洞的根本原因。", "result": "通过对SARD数据集中缓冲区溢出CWE基准进行评估，证明了所提出方法的有效性，并为相应的检测到的漏洞提供了解释。结果表明，VulPathFinder在发现到已识别PSP的漏洞路径方面，优于原始的SliceLocator和GNNExplainer（作为通用的GNN可解释性工具）。", "conclusion": "VulPathFinder通过结合GNN进行汇聚点检测和程序切片技术，提供了一种更有效、更具泛化性的方法来发现和解释开源代码中的漏洞激活路径。", "translation": "检测开源软件中的安全漏洞是一项关键任务，在相关研究社区中备受重视。文献中已经提出了几种方法来检测漏洞代码并识别漏洞类别。然而，在通过定位漏洞语句和发现导致漏洞激活的路径来解释检测到的漏洞的根本原因方面仍有工作空间。虽然像SliceLocator这样的框架通过识别漏洞路径提供解释，但它们依赖于基于规则的汇聚点识别，这限制了它们的泛化能力。在本文中，我们引入了VulPathFinder，一个可解释的漏洞路径发现框架，它通过利用新颖的图神经网络（GNN）模型来检测汇聚点，而不是依赖预定义规则，从而增强了SliceLocator的方法。所提出的GNN捕获语义和句法依赖性以找到潜在汇聚点（PSPs），即漏洞路径结束的候选语句。检测到PSP后，可以使用程序切片提取潜在的漏洞路径，然后通过将其反馈到目标基于图的检测器进行排序。最终，返回最可能的路径，解释检测到的漏洞的根本原因。我们通过对SARD数据集中缓冲区溢出CWE的基准进行评估，证明了所提出方法的有效性，并为相应的检测到的漏洞提供了解释。结果表明，VulPathFinder在发现到已识别PSP的漏洞路径方面，优于原始的SliceLocator和GNNExplainer（作为通用的GNN可解释性工具）。", "summary": "本文提出了VulPathFinder，一个用于开源代码中漏洞路径发现的可解释框架。它通过引入一个新颖的图神经网络（GNN）模型来检测潜在的汇聚点（PSPs），克服了现有方法（如SliceLocator）中基于规则的汇聚点识别的泛化限制。该GNN捕获代码的语义和句法依赖性，以识别漏洞可能结束的语句。识别出PSPs后，利用程序切片提取潜在的漏洞路径，并通过基于图的检测器进行排序，最终返回最可能的路径以解释漏洞的根源。在SARD数据集的缓冲区溢出CWE基准上的评估显示，VulPathFinder在漏洞路径发现方面优于SliceLocator和GNNExplainer。", "keywords": "漏洞发现, 图神经网络, 漏洞路径, 开源代码, 可解释性", "comments": "该论文的创新之处在于将图神经网络应用于漏洞汇聚点检测，解决了传统规则方法在泛化性方面的局限。通过提供可解释的漏洞激活路径，它对于理解和修复检测到的漏洞具有重要意义。该方法结合了深度学习和程序分析技术，为自动化漏洞分析和解释提供了新的视角。"}}
{"id": "2507.18145", "title": "Logical Characterizations of GNNs with Mean Aggregation", "authors": ["Moritz Schönherr", "Carsten Lutz"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18145v1", "summary": "We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18145v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "均值聚合GNN的逻辑刻画", "tldr": "本文研究了使用均值作为聚合函数的图神经网络（GNNs）的表达能力，并在非均匀和均匀设置下对其进行了逻辑刻画，同时与其他聚合函数（最大值、求和）进行了比较。", "motivation": "旨在研究和刻画使用均值作为聚合函数的图神经网络（GNNs）的表达能力，并理解其在不同设置下的逻辑特性和与其他聚合函数的相对表达能力。", "method": "本文通过逻辑刻画（如比率模态逻辑和无交替模态逻辑）来研究使用均值作为聚合函数的GNN的表达能力，并在非均匀和均匀设置下进行分析，同时与其他聚合函数（最大值、求和）进行比较。", "result": "在非均匀设置中，均值GNN的表达能力与比率模态逻辑完全相同，且高于最大值聚合GNN但低于求和聚合GNN。在均匀设置中，在组合函数连续且分类函数为阈值的自然假设下，均值GNN相对于MSO的表达能力与无交替模态逻辑完全相同，并严格低于求和GNN和最大值GNN。当这些假设被放弃时，表达能力会增加。", "conclusion": "均值聚合GNN的表达能力在不同设置下（非均匀和均匀）被特定的模态逻辑精确刻画。在非均匀设置下，其表达能力高于最大值聚合GNN但低于求和聚合GNN；而在均匀设置下（在特定假设下），其表达能力低于求和聚合GNN和最大值聚合GNN。", "translation": "我们研究了以均值作为聚合函数的图神经网络（GNNs）的表达能力。在非均匀设置中，我们表明此类GNN的表达能力与比率模态逻辑完全相同，后者具有模态算子，表示一个顶点的至少某个比例的后继满足指定属性。因此，均值GNN的非均匀表达能力高于使用最大值聚合的GNN，但低于求和聚合的GNN——后者分别由模态逻辑和分级模态逻辑刻画。在均匀设置中，我们表明在组合函数连续且分类函数为阈值的自然假设下，相对于MSO的表达能力与无交替模态逻辑完全相同。这意味着，相对于MSO并在均匀设置中，均值GNN的表达能力严格低于求和GNN和最大值GNN。当任何一个假设被放弃时，表达能力会增加。", "summary": "本文深入研究了以均值作为聚合函数的图神经网络（GNNs）的表达能力。在非均匀设置下，研究发现均值GNN的表达能力与比率模态逻辑等同，且高于最大值聚合GNN但低于求和聚合GNN。在均匀设置中，在特定连续性和阈值假设下，均值GNN相对于MSO的表达能力与无交替模态逻辑一致，并严格低于求和聚合GNN和最大值聚合GNN。研究还指出，放松这些假设会提高均值GNN的表达能力。", "keywords": "图神经网络, 均值聚合, 表达能力, 模态逻辑, 逻辑刻画", "comments": "这项工作提供了均值聚合GNN表达能力的严格逻辑刻画，这对于理解不同聚合函数GNN的理论限制和能力至关重要。它有助于指导在特定任务中选择合适的GNN架构。"}}
{"id": "2507.17959", "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments", "authors": ["Ali Abedi", "Sadaf Safa", "Tracey J. F. Colella", "Shehroz S. Khan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 3 figures, 7 tables", "url": "http://arxiv.org/abs/2507.17959v1", "summary": "Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.", "comment": "14 pages, 3 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.17959v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "OPEN：一个用于虚拟康复学习环境中老年患者参与度识别的基准数据集和基线", "tldr": "本文介绍了OPEN，一个针对老年人在虚拟康复中参与度的大型数据集，以及实现81%准确率的基线AI模型，旨在解决该人群数据稀缺的问题。", "motivation": "在虚拟学习中衡量参与度，尤其是在虚拟康复和在线教育中，对于参与者的满意度、表现和依从性至关重要。然而，在虚拟小组环境中准确衡量参与度仍然是一个挑战。尽管人工智能（AI）用于大规模、真实世界、自动化参与度识别的兴趣日益增长，但针对老年人在虚拟和远程医疗学习环境中的研究和数据集仍然有限。现有方法通常忽视了上下文相关性和跨会话参与度的纵向性质。", "method": "本文介绍了OPEN（Older adult Patient ENgagement），一个支持AI驱动的参与度识别的新型数据集。该数据集从11名老年人参与每周虚拟小组学习会话（作为心脏康复的一部分）中收集，历时六周，产生了超过35小时的数据。为了保护隐私，原始视频被保留，取而代之的是发布的数据包括面部、手部和身体关节标志点，以及从视频中提取的情感和行为特征。注释包括二元参与状态、情感和行为标签，以及上下文类型指示器（例如，教师是针对小组还是个人）。该数据集提供5秒、10秒、30秒和可变长度样本的版本。为了展示其效用，训练了多个机器学习和深度学习模型作为基线。", "result": "OPEN是同类数据集中最大的数据集（来自11名老年人，超过35小时的数据）。基线模型实现了高达81%的参与度识别准确率。", "conclusion": "OPEN为老年人群的个性化参与度建模提供了可扩展的基础，并有助于更广泛的参与度识别研究。", "translation": "在虚拟学习中保持参与度对于参与者的满意度、表现和依从性至关重要，尤其是在在线教育和虚拟康复中，互动交流扮演着关键角色。然而，在虚拟小组环境中准确衡量参与度仍然是一个挑战。利用人工智能（AI）进行大规模、真实世界、自动化参与度识别的兴趣日益增长。尽管参与度在年轻学术人群中得到了广泛研究，但针对老年人在虚拟和远程医疗学习环境中的研究和数据集仍然有限。现有方法通常忽视了上下文相关性和跨会话参与度的纵向性质。本文介绍了OPEN（Older adult Patient ENgagement），一个支持AI驱动的参与度识别的新型数据集。该数据集从11名老年人参与每周虚拟小组学习会话（作为心脏康复的一部分）中收集，历时六周，产生了超过35小时的数据，使其成为同类数据集中最大的数据集。为了保护隐私，原始视频被保留；取而代之的是，发布的数据包括面部、手部和身体关节标志点，以及从视频中提取的情感和行为特征。注释包括二元参与状态、情感和行为标签，以及上下文类型指示器，例如教师是针对小组还是个人。该数据集提供5秒、10秒、30秒和可变长度样本的版本。为了展示其效用，训练了多个机器学习和深度学习模型，实现了高达81%的参与度识别准确率。OPEN为老年人群的个性化参与度建模提供了可扩展的基础，并有助于更广泛的参与度识别研究。", "summary": "本文介绍了OPEN，一个用于在虚拟康复学习环境中识别老年患者参与度的新型基准数据集和基线。为解决相关数据稀缺以及在虚拟小组环境中衡量参与度的挑战，OPEN数据集包含来自11名老年人参与虚拟心脏康复的超过35小时数据。该数据集包含了各种提取的特征和注释（面部、手部、身体标志点，情感/行为特征，参与状态，上下文指示器），并通过不包含原始视频来保护隐私。基线机器学习和深度学习模型实现了高达81%的准确率，展示了OPEN的实用性及其在推进老年人群个性化参与度建模和更广泛参与度识别研究方面的潜力。", "keywords": "老年人, 患者参与度, 虚拟康复, 数据集, AI识别", "comments": "创新点：创建了专门用于虚拟康复中老年患者参与度的最大数据集，填补了重要的数据空白。重要性：促进了针对弱势人群的AI驱动参与度识别，这对有效的虚拟护理至关重要。局限性：为了隐私，依赖于提取的特征而非原始视频，这可能会限制一些更深入的分析；81%的准确率仍有提升空间。"}}
{"id": "2409.05345", "title": "Robust Non-adaptive Group Testing under Errors in Group Membership Specifications", "authors": ["Shuvayan Banerjee", "Radhendushka Srivastava", "James Saunderson", "Ajit Rajwade"], "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.05345v2", "summary": "Given $p$ samples, each of which may or may not be defective, group testing\n(GT) aims to determine their defect status by performing tests on $n < p$\n`groups', where a group is formed by mixing a subset of the $p$ samples.\nAssuming that the number of defective samples is very small compared to $p$, GT\nalgorithms have provided excellent recovery of the status of all $p$ samples\nwith even a small number of groups. Most existing methods, however, assume that\nthe group memberships are accurately specified. This assumption may not always\nbe true in all applications, due to various resource constraints. Such errors\ncould occur, eg, when a technician, preparing the groups in a laboratory,\nunknowingly mixes together an incorrect subset of samples as compared to what\nwas specified. We develop a new GT method, the Debiased Robust Lasso Test\nMethod (DRLT), that handles such group membership specification errors. The\nproposed DRLT method is based on an approach to debias, or reduce the inherent\nbias in, estimates produced by Lasso, a popular and effective sparse regression\ntechnique. We also provide theoretical upper bounds on the reconstruction error\nproduced by our estimator. Our approach is then combined with two carefully\ndesigned hypothesis tests respectively for (i) the identification of defective\nsamples in the presence of errors in group membership specifications, and (ii)\nthe identification of groups with erroneous membership specifications. The DRLT\napproach extends the literature on bias mitigation of statistical estimators\nsuch as the LASSO, to handle the important case when some of the measurements\ncontain outliers, due to factors such as group membership specification errors.\nWe present numerical results which show that our approach outperforms several\nbaselines and robust regression techniques for identification of defective\nsamples as well as erroneously specified groups.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.05345v2", "cate": "stat.ML", "date": "2024-09-09", "updated": "2025-07-24", "AI": {"title_translation": "鲁棒非自适应群组测试在群组成员规范错误下", "tldr": "本文提出了一种新的群组测试方法DRLT，用于在群组成员规范存在错误的情况下，鲁棒地识别缺陷样本和错误指定的群组。", "motivation": "现有群组测试方法通常假设群组成员关系准确无误，但在实际应用中，由于资源限制或人为错误，这种假设可能不成立，导致测量数据中出现异常值，从而影响缺陷样本的准确识别。", "method": "本文提出了一种名为“去偏鲁棒Lasso测试方法”（DRLT），该方法基于减少Lasso估计中固有偏差的策略。它提供了重建误差的理论上限，并结合了两个精心设计的假设检验：一是用于在群组成员规范存在错误时识别缺陷样本，二是用于识别具有错误成员规范的群组。DRLT扩展了对LASSO等统计估计器偏差缓解的研究，以处理测量中包含异常值的重要情况。", "result": "数值结果表明，DRLT方法在识别缺陷样本和错误指定群组方面，优于多种基线方法和鲁棒回归技术。", "conclusion": "DRLT是一种有效且鲁棒的群组测试方法，能够处理群组成员规范错误，并准确识别缺陷样本和错误指定的群组，从而弥补了现有方法的不足。", "translation": "给定$p$个样本，每个样本可能存在缺陷也可能不存在缺陷，群组测试（GT）旨在通过对$n < p$个“群组”进行测试来确定它们的缺陷状态，其中一个群组是通过混合$p$个样本的一个子集形成的。假设缺陷样本的数量相对于$p$非常小，GT算法即使使用少量群组也能很好地恢复所有$p$个样本的状态。然而，大多数现有方法都假定群组成员关系被精确指定。由于各种资源限制，在所有应用中此假设可能并非总是成立。例如，当技术人员在实验室中准备群组时，可能会无意中混合了与指定不符的错误样本子集，从而导致此类错误。我们开发了一种新的GT方法，即去偏鲁棒Lasso测试方法（DRLT），该方法能够处理此类群组成员规范错误。所提出的DRLT方法基于一种去偏或减少Lasso（一种流行且有效的稀疏回归技术）产生的估计中固有偏差的方法。我们还提供了我们的估计器产生的重建误差的理论上限。然后，我们的方法结合了两个精心设计的假设检验，分别用于（i）在群组成员规范存在错误的情况下识别缺陷样本，以及（ii）识别具有错误成员规范的群组。DRLT方法扩展了对LASSO等统计估计器偏差缓解的文献，以处理由于群组成员规范错误等因素导致某些测量值包含异常值的重要情况。我们提供了数值结果，表明我们的方法在识别缺陷样本以及错误指定的群组方面优于几种基线和鲁棒回归技术。", "summary": "本文提出了一种名为“去偏鲁棒Lasso测试方法”（DRLT）的新型群组测试方法，旨在解决现有方法在群组成员规范存在错误时性能下降的问题。DRLT通过去偏Lasso估计并结合两个假设检验，不仅能识别缺陷样本，还能同时识别出群组成员规范错误的群组。实验结果表明，DRLT在存在此类错误的情况下，性能优于现有基线和鲁棒回归技术。", "keywords": "群组测试,鲁棒性,Lasso,成员规范错误,缺陷识别", "comments": "这篇论文的创新点在于它解决了群组测试中一个重要的实际问题：群组成员规范错误。通过引入去偏Lasso和专门的假设检验，DRLT提供了一个在更复杂、更现实场景下依然鲁棒有效的解决方案。其贡献在于扩展了稀疏回归和偏差缓解的理论，并提供了实际可行的错误检测和校正机制，对于需要大规模筛选但又存在操作误差的应用具有重要意义。"}}
{"id": "2507.18558", "title": "Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation", "authors": ["Yihong Feng", "Chaitanya Pallerla", "Xiaomin Lin", "Pouya Sohrabipour Sr", "Philip Crandall", "Wan Shou", "Yu She", "Dongyi Wang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted for journal reviewing", "url": "http://arxiv.org/abs/2507.18558v1", "summary": "The poultry industry has been driven by broiler chicken production and has\ngrown into the world's largest animal protein sector. Automated detection of\nchicken carcasses on processing lines is vital for quality control, food\nsafety, and operational efficiency in slaughterhouses and poultry processing\nplants. However, developing robust deep learning models for tasks like instance\nsegmentation in these fast-paced industrial environments is often hampered by\nthe need for laborious acquisition and annotation of large-scale real-world\nimage datasets. We present the first pipeline generating photo-realistic,\nautomatically labeled synthetic images of chicken carcasses. We also introduce\na new benchmark dataset containing 300 annotated real-world images, curated\nspecifically for poultry segmentation research. Using these datasets, this\nstudy investigates the efficacy of synthetic data and automatic data annotation\nto enhance the instance segmentation of chicken carcasses, particularly when\nreal annotated data from the processing line is scarce. A small real dataset\nwith varying proportions of synthetic images was evaluated in prominent\ninstance segmentation models. Results show that synthetic data significantly\nboosts segmentation performance for chicken carcasses across all models. This\nresearch underscores the value of synthetic data augmentation as a viable and\neffective strategy to mitigate data scarcity, reduce manual annotation efforts,\nand advance the development of robust AI-driven automated detection systems for\nchicken carcasses in the poultry processing industry.", "comment": "Submitted for journal reviewing", "pdf_url": "http://arxiv.org/pdf/2507.18558v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "合成数据增强用于提升鸡胴体实例分割", "tldr": "本研究提出了一种生成逼真合成鸡胴体图像的流水线，并创建了一个新的基准数据集，旨在解决真实世界图像数据稀缺的问题，以提升鸡胴体实例分割的性能。结果表明合成数据能显著提高分割性能。", "motivation": "家禽行业对肉鸡生产的需求巨大，但为鸡胴体实例分割等任务开发鲁棒的深度学习模型，常因需要大量耗时地获取和标注真实世界图像数据集而受阻。特别是在真实标注数据稀缺的情况下，需要解决数据不足的问题。", "method": "本研究提出了首个能够生成逼真、自动标注的鸡胴体合成图像的流水线。同时，引入了一个包含300张标注的真实世界图像的新基准数据集。研究利用这些数据集，评估了合成数据和自动数据标注在增强鸡胴体实例分割方面的有效性，尤其是在真实处理线标注数据稀缺的情况下。在一个小型真实数据集上，通过不同比例的合成图像，评估了著名的实例分割模型。", "result": "结果显示，合成数据显著提升了所有模型在鸡胴体分割上的性能。", "conclusion": "本研究强调了合成数据增强作为一种可行且有效的策略的价值，它能够缓解数据稀缺、减少手动标注工作，并推动家禽加工业中鸡胴体鲁棒AI驱动自动化检测系统的发展。", "translation": "家禽业一直由肉鸡生产驱动，并已发展成为世界上最大的动物蛋白产业。在屠宰场和家禽加工厂中，自动化检测加工线上的鸡胴体对于质量控制、食品安全和运营效率至关重要。然而，为这些快速变化的工业环境中的实例分割等任务开发鲁棒的深度学习模型，常常受到需要费力获取和标注大规模真实世界图像数据集的阻碍。我们提出了第一个生成逼真、自动标注的鸡胴体合成图像的流水线。我们还引入了一个新的基准数据集，其中包含300张经过标注的真实世界图像，专门为家禽分割研究而整理。利用这些数据集，本研究调查了合成数据和自动数据标注在增强鸡胴体实例分割方面的功效，特别是在处理线上真实标注数据稀缺的情况下。在著名的实例分割模型中评估了一个包含不同比例合成图像的小型真实数据集。结果表明，合成数据显著提升了所有模型在鸡胴体分割上的性能。这项研究强调了合成数据增强作为一种可行且有效的策略的价值，以缓解数据稀缺、减少手动标注工作，并促进家禽加工业中鸡胴体鲁棒AI驱动自动化检测系统的发展。", "summary": "本研究旨在解决家禽加工业中鸡胴体实例分割任务面临的真实世界图像数据稀缺和标注耗时问题。为此，作者提出了一个生成逼真、自动标注合成鸡胴体图像的流水线，并构建了一个包含300张真实标注图像的新基准数据集。通过在实例分割模型中评估混合了不同比例合成数据的小型真实数据集，研究发现合成数据能显著提升鸡胴体分割性能。这表明合成数据增强是缓解数据稀缺、减少手动标注并推动自动化检测系统发展的有效策略。", "keywords": "合成数据增强, 实例分割, 鸡胴体, 数据稀缺, 深度学习", "comments": "该论文的创新点在于首次提出了生成逼真鸡胴体合成图像的流水线，并构建了专门的基准数据集，有效解决了家禽加工行业中深度学习模型训练面临的数据稀缺和高昂标注成本问题。其重要性在于为食品安全和质量控制提供了更高效的AI解决方案，为相关领域的数据增强研究提供了新的思路和实践案例。"}}
{"id": "2504.10268", "title": "Theoretical Model of Microparticle-Assisted Super-Resolution Microscopy", "authors": ["A. R Bekirov"], "categories": ["physics.optics", "eess.IV"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10268v2", "summary": "We present the first three-dimensional theoretical model of\nmicroparticle-assisted super-resolution imaging, enabling accurate simulation\nof virtual image formation. The model reveals that accounting for partial\nspatial coherence of illumination is a fundamental prerequisite for achieving\nsuper-resolution. We also propose a novel illumination strategy based on\nsuppressing the normal component of incident light, which enhances image\ncontrast and resolution. The results establish a consistent wave-optical\nframework that reproduces experimentally observed subwavelength imaging and\nclarifies the underlying physical mechanisms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10268v2", "cate": "physics.optics", "date": "2025-04-14", "updated": "2025-07-24", "AI": {"title_translation": "微粒辅助超分辨率显微镜的理论模型", "tldr": "提出了首个微粒辅助超分辨率成像的三维理论模型，揭示了部分空间相干照明对实现超分辨率至关重要，并提出了一种新的照明策略以增强图像对比度和分辨率。", "motivation": "现有研究缺乏对微粒辅助超分辨率成像的准确三维理论模型，以及对其物理机制的深入理解。", "method": "提出了首个微粒辅助超分辨率成像的三维理论模型，该模型考虑了照明的部分空间相干性。此外，还提出了一种抑制入射光法向分量的新型照明策略。", "result": "该模型能够准确模拟虚拟图像的形成，并揭示了考虑照明的部分空间相干性是实现超分辨率的基本前提。新的照明策略能够增强图像对比度和分辨率。该框架重现了实验观察到的亚波长成像，并阐明了潜在的物理机制。", "conclusion": "建立了一个一致的波动光学框架，能够解释微粒辅助超分辨率成像的实验现象并阐明其物理机制。", "translation": "我们提出了首个微粒辅助超分辨率成像的三维理论模型，实现了虚拟图像形成的精确模拟。该模型揭示，考虑照明的部分空间相干性是实现超分辨率的基本前提。我们还提出了一种基于抑制入射光法向分量的新型照明策略，该策略增强了图像对比度和分辨率。研究结果建立了一个一致的波动光学框架，重现了实验观察到的亚波长成像，并阐明了潜在的物理机制。", "summary": "本文首次提出了微粒辅助超分辨率成像的三维理论模型，该模型能够准确模拟图像形成。研究发现，照明的部分空间相干性是实现超分辨率的关键。此外，论文还提出了一种通过抑制入射光法向分量来提升图像对比度和分辨率的新型照明策略。该理论框架成功解释了实验中观察到的亚波长成像现象，并阐明了其物理机制。", "keywords": "微粒辅助成像, 超分辨率, 理论模型, 空间相干性, 波动光学", "comments": "这项工作首次建立了微粒辅助超分辨率成像的三维理论模型，填补了该领域理论模型的空白。它不仅揭示了部分空间相干照明对超分辨率的重要性，还提出了一种实用的新型照明策略，有望在实验中提升成像性能。该模型的提出对于深入理解微粒辅助超分辨率成像的物理原理具有重要意义，并为未来的技术发展奠定了理论基础。"}}
{"id": "2504.04704", "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important", "authors": ["Manlai Liang", "JiaMing Zhang", "Xiong Li", "Jinlong Li"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04704v2", "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04704v2", "cate": "cs.LG", "date": "2025-04-07", "updated": "2025-07-24", "AI": {"title_translation": "LagKV：KV缓存的滞后相对信息揭示哪些令牌是重要的", "tldr": "LagKV是一种新的KV缓存压缩策略，它不依赖注意力机制，通过直接比较KV信息来减少大型语言模型长上下文推理中的KV缓存大小，同时保持高性能和易于集成。", "motivation": "大型语言模型在长上下文推理中，KV缓存大小的增加是部署成本和任务准确性之间平衡的主要障碍。现有方法（如基于注意力权重的驱逐）通常需要对推理基础设施进行重大修改并带来显著的计算开销。", "method": "提出LagKV，一种KV压缩策略，它基于大型语言模型是自回归模型的事实，仅依赖于KV本身之间的直接比较。这是一种完全无注意力的方法，易于集成到主流推理平台。", "result": "在RULER基准测试中，LagKV在不同压缩率下优于SnapKV和StreamingLLM。特别是在64位密钥检索任务中，LagKV在相同压缩率下，性能比基于注意力权重的方法$H_2O$高出50%以上。", "conclusion": "LagKV通过一种简单、无注意力且易于集成的方法，有效解决了大型语言模型长上下文推理中KV缓存过大的问题，并在性能上超越了现有的复杂压缩方法。", "translation": "大型语言模型长上下文推理过程中Key-Value (KV) 缓存大小的不断增长是其部署成本和任务准确性之间平衡的主要障碍。为了在这种场景下减小KV缓存大小，大多数先前的工作都利用注意力权重来驱逐非关键的缓存令牌。但这些方法存在权衡，它们通常需要对推理基础设施进行重大修改和显著的计算开销。基于大型语言模型是自回归模型的事实，我们提出了LagKV，这是一种仅依赖于KV本身之间直接比较的KV压缩策略。它是一种完全无注意力的方法，易于集成到主流推理平台，并且与其它复杂的KV压缩方法相比，性能相当。RULER基准测试结果表明，我们的方法在不同压缩率下优于SnapKV和StreamingLLM。特别是在64位密钥检索任务中，我们的方法在相同压缩率下，性能比基于注意力权重的方法$H_2O$高出50%以上。我们的代码可在https://github.com/AI-Lab-China-Merchants-Bank/LagKV获取。", "summary": "大型语言模型在长上下文推理中面临KV缓存过大的挑战，这影响了部署成本和准确性。现有基于注意力的方法存在修改复杂和计算开销大的问题。本文提出LagKV，一种不依赖注意力机制的KV压缩策略，它通过直接比较KV本身的信息来识别重要令牌。LagKV易于集成，并在RULER基准测试中表现出色，尤其在64位密钥检索任务中，性能显著优于现有方法，证明了其在平衡成本与性能方面的有效性。", "keywords": "KV缓存压缩, 大型语言模型, 长上下文推理, LagKV, 无注意力机制", "comments": "LagKV的创新之处在于其“无注意力”的KV压缩方法，这与大多数现有方法形成对比，显著降低了集成复杂性和计算开销。它通过直接比较KV信息来判断令牌重要性，提供了一种简洁而高效的解决方案，对于大型语言模型在实际部署中的成本控制和性能优化具有重要意义。"}}
{"id": "2507.17956", "title": "Formal Verification of the Safegcd Implementation", "authors": ["Russell O'Connor", "Andrew Poelstra"], "categories": ["cs.CR", "cs.LO"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      15 pages; Coq sources can be found at this https URL ; Alectryon preview can be viewed at e.g. this https URL", "url": "http://arxiv.org/abs/2507.17956v1", "summary": "The modular inverse is an essential piece of computation required for\nelliptic curve operations used for digital signatures in Bitcoin and other\napplications. A novel approach to the extended Euclidean algorithm has been\ndeveloped by Bernstein and Yang within the last few years and incorporated into\nthe libsecp256k1 cryptographic library used by Bitcoin. However, novel\nalgorithms introduce new risks of errors. To address this we have completed a\ncomputer verified proof of the correctness of (one of) libsecp256k1's modular\ninverse implementations with the Coq proof assistant using the Verifiable C's\nimplementation of separation logic.", "comment": "15 pages; Coq sources can be found at\n  https://github.com/BlockstreamResearch/simplicity/tree/c1dddedd553b403da877377e658f17f0d2184cc4/Coq/C/secp256k1\n  ; Alectryon preview can be viewed at e.g.\n  https://html-preview.github.io/?url=https://github.com/BlockstreamResearch/simplicity/blob/c1dddedd553b403da877377e658f17f0d2184cc4/alectryon/verif_modinv64_impl.v.html", "pdf_url": "http://arxiv.org/pdf/2507.17956v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Safegcd 实现的形式化验证", "tldr": "论文使用Coq和分离逻辑对libsecp256k1中一个新颖的模逆实现进行了计算机验证。", "motivation": "由于Bernstein和Yang开发的新颖扩展欧几里得算法被整合到libsecp256k1中，新算法可能引入错误，因此需要对其正确性进行验证。", "method": "作者使用Coq证明助手和Verifiable C的分离逻辑实现，对libsecp256k1中模逆实现之一的正确性进行了计算机验证。", "result": "作者成功完成了libsecp256k1中一个模逆实现的计算机验证证明。", "conclusion": "libsecp256k1库中一个关键的模逆实现的正确性已经通过形式化验证得到确认，增强了其在比特币等应用中的安全性。", "translation": "模逆是比特币和其他应用中数字签名所需的椭圆曲线操作中必不可少的计算部分。在过去几年中，Bernstein和Yang开发了一种新颖的扩展欧几里得算法方法，并将其整合到比特币使用的libsecp256k1密码库中。然而，新颖的算法会引入新的错误风险。为了解决这个问题，我们使用Coq证明助手和Verifiable C的分离逻辑实现，完成了libsecp256k1的模逆实现之一的正确性计算机验证证明。", "summary": "本文针对比特币密码库libsecp256k1中新颖的模逆算法（基于Bernstein和Yang的扩展欧几里得算法）可能引入的错误风险，使用Coq证明助手和Verifiable C的分离逻辑，对其一个模逆实现进行了形式化验证，并成功完成了计算机验证的正确性证明。", "keywords": "形式化验证, 模逆, libsecp256k1, Coq, 分离逻辑", "comments": "这项工作的重要性在于它提升了关键密码库（libsecp256k1）中核心算法的安全性。通过形式化验证，它为比特币等依赖这些操作的应用提供了更高的可信度。创新点在于将形式化方法应用于一个新颖且实际使用的密码学算法实现。"}}
{"id": "2507.17791", "title": "Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data", "authors": ["Eduardo Aguilar-Bejarano", "Daniel Lea", "Karthikeyan Sivakumar", "Jimiama M. Mase", "Reza Omidvar", "Ruizhe Li", "Troy Kettle", "James Mitchell-White", "Morgan R Alexander", "David A Winkler", "Grazziela Figueredo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      17 pages", "url": "http://arxiv.org/abs/2507.17791v1", "summary": "Helix is an open-source, extensible, Python-based software framework to\nfacilitate reproducible and interpretable machine learning workflows for\ntabular data. It addresses the growing need for transparent experimental data\nanalytics provenance, ensuring that the entire analytical process -- including\ndecisions around data transformation and methodological choices -- is\ndocumented, accessible, reproducible, and comprehensible to relevant\nstakeholders. The platform comprises modules for standardised data\npreprocessing, visualisation, machine learning model training, evaluation,\ninterpretation, results inspection, and model prediction for unseen data. To\nfurther empower researchers without formal training in data science to derive\nmeaningful and actionable insights, Helix features a user-friendly interface\nthat enables the design of computational experiments, inspection of outcomes,\nincluding a novel interpretation approach to machine learning decisions using\nlinguistic terms all within an integrated environment. Released under the MIT\nlicence, Helix is accessible via GitHub and PyPI, supporting community-driven\ndevelopment and promoting adherence to the FAIR principles.", "comment": "17 pages", "pdf_url": "http://arxiv.org/pdf/2507.17791v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "Helix 1.0：一个用于表格科学数据可复现和可解释机器学习的开源框架", "tldr": "Helix是一个开源的Python框架，旨在为表格数据提供可复现、可解释的机器学习工作流程，并支持透明的数据分析溯源和非专业用户。", "motivation": "该论文的动机是解决对透明实验数据分析溯源日益增长的需求，确保整个分析过程（包括数据转换和方法选择）是可文档化、可访问、可复现和可理解的。此外，它旨在赋能没有数据科学正式培训的研究人员也能获取有意义且可操作的洞察。", "method": "Helix是一个基于Python的开源软件框架，包含用于标准化数据预处理、可视化、机器学习模型训练、评估、解释、结果检查和对未见数据进行模型预测的模块。它提供用户友好的界面，支持计算实验设计、结果检查，并采用新颖的基于语言术语的机器学习决策解释方法。", "result": "Helix提供了一个促进可复现和可解释机器学习工作流程的平台，确保整个分析过程是可文档化、可访问、可复现和可理解的。它通过用户友好的界面和新颖的解释方法，赋能了没有数据科学正式培训的研究人员获取洞察。该框架通过GitHub和PyPI发布，支持社区驱动开发并促进遵循FAIR原则。", "conclusion": "Helix是一个重要的开源框架，通过提供可复现、可解释且用户友好的机器学习工具，促进了科学数据分析的透明度和可访问性，特别有助于赋能非专业数据科学家进行数据驱动的发现和决策。", "translation": "Helix是一个开源、可扩展、基于Python的软件框架，旨在促进表格数据的可复现和可解释机器学习工作流程。它解决了对透明实验数据分析溯源日益增长的需求，确保整个分析过程——包括数据转换和方法选择——被文档化、可访问、可复现并对相关利益方而言是可理解的。该平台包含用于标准化数据预处理、可视化、机器学习模型训练、评估、解释、结果检查以及对未见数据进行模型预测的模块。为了进一步赋能那些没有数据科学正式培训的研究人员获取有意义且可操作的洞察，Helix提供了一个用户友好的界面，该界面在一个集成环境中，能够设计计算实验、检查结果，包括一种使用语言术语对机器学习决策进行解释的新颖方法。Helix在MIT许可下发布，可通过GitHub和PyPI访问，支持社区驱动开发并促进遵循FAIR原则。", "summary": "Helix是一个开源、可扩展的Python框架，旨在为表格数据提供可复现和可解释的机器学习工作流程。它通过确保分析过程的文档化、可访问、可复现和可理解性，解决了对透明数据分析溯源的需求。该平台集成了数据预处理、可视化、模型训练、评估和解释等模块，并提供用户友好的界面，支持计算实验设计和基于语言术语的新颖解释方法，以帮助非专业数据科学研究人员获取洞察。Helix基于MIT许可发布，可通过GitHub和PyPI获取，支持社区开发并促进FAIR原则。", "keywords": "可复现机器学习, 可解释机器学习, 表格数据, 开源框架, 数据分析", "comments": "Helix的创新之处在于其对“可解释性”和“可复现性”的强调，并将其集成到一个用户友好的开源框架中。特别是其采用“语言术语”来解释机器学习决策的方法，对于非专业数据科学家而言，极大地降低了理解门槛，提高了洞察的可操作性。作为一个开源项目，它有利于社区协作和推广数据科学的最佳实践。"}}
{"id": "2507.18480", "title": "Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse", "authors": ["David Nunez", "Francesc Wilhelmi", "Lorenzo Galati-Giordano", "Giovanni Geraci", "Boris Bellalta"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Communications Standards Magazine", "url": "http://arxiv.org/abs/2507.18480v1", "summary": "IEEE 802.11 networks continuously adapt to meet the stringent requirements of\nemerging applications like cloud gaming, eXtended Reality (XR), and video\nstreaming services, which require high throughput, low latency, and high\nreliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can\npotentially contribute to optimizing spectrum resource utilization. This\nmechanism is expected to enable simultaneous transmissions, thereby boosting\nspectral efficiency in dense environments and increasing the overall network\nperformance. In this paper, we shed light on the performance of Co-SR for Wi-Fi\n8 networks. For that, we propose an implementation of Co-SR aligned with\nongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi\nsimulator, which allows us to study the performance of the proposed Co-SR\nmechanisms in relevant scenarios. The results obtained in a Wireless Local Area\nNetwork (WLAN) consisting of four APs show delay reduction with Co-SR ranging\nfrom 31% to 95% when compared to Distributed Coordination Function (DCF).", "comment": "Submitted to IEEE Communications Standards Magazine", "pdf_url": "http://arxiv.org/pdf/2507.18480v1", "cate": "cs.NI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用协调空间复用技术改进Wi-Fi 8延迟", "tldr": "本文提出了一种与Wi-Fi 8标准兼容的协调空间复用（Co-SR）实现方案，并在模拟器中评估其性能，结果显示Co-SR能显著降低Wi-Fi网络延迟（31%-95%）。", "motivation": "现有的IEEE 802.11网络需要适应云游戏、扩展现实（XR）和视频流服务等新兴应用对高吞吐量、低延迟和高可靠性的严格要求。为解决这些挑战，需要优化频谱资源利用率。", "method": "本文提出了一种与Wi-Fi 8标准化工作相符的协调空间复用（Co-SR）实现方案。该方案在一个Wi-Fi模拟器上进行评估，研究了其在相关场景下的性能。", "result": "在由四个AP组成的无线局域网（WLAN）中进行的评估结果表明，与分布式协调功能（DCF）相比，Co-SR将延迟降低了31%至95%。", "conclusion": "协调空间复用（Co-SR）是一种有效优化Wi-Fi 8网络性能，特别是显著降低延迟的机制，有助于满足新兴应用的严格要求。", "translation": "IEEE 802.11网络不断适应，以满足云游戏、扩展现实（XR）和视频流服务等新兴应用的严格要求，这些应用需要高吞吐量、低延迟和高可靠性。为了应对这些挑战，协调空间复用（Co-SR）有望为优化频谱资源利用做出贡献。该机制预计将实现同时传输，从而提高密集环境中的频谱效率，并提升整体网络性能。在本文中，我们阐述了Co-SR在Wi-Fi 8网络中的性能。为此，我们提出了一种与正在进行的Wi-Fi 8标准化工作相符的Co-SR实现方案。评估在一个Wi-Fi模拟器上进行，这使我们能够研究所提出的Co-SR机制在相关场景下的性能。在由四个AP组成的无线局域网（WLAN）中获得的结果显示，与分布式协调功能（DCF）相比，Co-SR的延迟降低了31%至95%。", "summary": "本文研究了协调空间复用（Co-SR）在Wi-Fi 8网络中的性能，以应对新兴应用对低延迟和高吞吐量的需求。作者提出了一种符合Wi-Fi 8标准化工作的Co-SR实现方案，并在Wi-Fi模拟器中进行了评估。实验结果表明，在四AP的WLAN环境中，Co-SR相较于DCF能将延迟显著降低31%至95%。", "keywords": "Wi-Fi 8, 协调空间复用, 延迟, 频谱效率, 无线局域网", "comments": "这篇论文的创新点在于提出了一个符合Wi-Fi 8标准的Co-SR实现方案，并量化了其在提升Wi-Fi网络性能（特别是降低延迟）方面的潜力。其重要性在于Co-SR能够有效优化频谱资源利用，支持未来对延迟敏感的应用，如云游戏和XR。"}}
{"id": "2504.17897", "title": "A Walk across Europe: Development of a high-resolution walkability index", "authors": ["Nishit Patel", "Hoang-Ha Nguyen", "Jet van de Geest", "Alfred Wagtendonk", "Mohan JS Raju", "Payam Dadvand", "Kees de Hoogh", "Marta Cirach", "Mark Nieuwenhuijsen", "Thao Minh Lam", "Jeroen Lakerveld"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.17897v2", "summary": "Physical inactivity significantly contributes to obesity and other\nnon-communicable diseases, yet efforts to increase population-wide physical\nactivity levels have met with limited success. The built environment plays a\npivotal role in encouraging active behaviors like walking. Walkability indices,\nwhich aggregate various environmental features, provide a valuable tool for\npromoting healthy, walkable environments. However, a standardized,\nhigh-resolution walkability index for Europe has been lacking. This study\naddresses that gap by developing a standardized, high-resolution walkability\nindex for the entire European region. Seven core components were selected to\ndefine walkability: walkable street length, intersection density, green spaces,\nslope, public transport access, land use mix, and 15-minute walking isochrones.\nThese were derived from harmonized, high-resolution datasets such as\nSentinel-2, NASA's elevation models, OpenStreetMap, and CORINE Land Cover. A\n100 m x 100 m hierarchical grid system and advanced geospatial methods, like\nnetwork buffers and distance decay, were used at scale to efficiently model\nreal-world density and proximity effects. The resulting index was weighted by\npopulation and analyzed at different spatial levels using visual mapping,\nspatial clustering, and correlation analysis. Findings revealed a distinct\nurban-to-rural gradient, with high walkability scores concentrated in compact\nurban centers rich in street connectivity and land use diversity. The index\nhighlighted cities like Barcelona, Berlin, Munich, Paris, and Warsaw as\nwalkability leaders. This standardized, high-resolution walkability index\nserves as a practical tool for researchers, planners, and policymakers aiming\nto support active living and public health across diverse European contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.17897v2", "cate": "cs.CY", "date": "2025-04-24", "updated": "2025-07-24", "AI": {"title_translation": "漫步欧洲：高分辨率步行指数的开发", "tldr": "本研究开发并应用了一个标准化的、高分辨率的欧洲步行性指数，揭示了城乡步行性差异，旨在促进公共健康和活跃生活。", "motivation": "身体不活动是导致肥胖和非传染性疾病的重要因素，而提高身体活动水平的努力收效甚微。建成环境对鼓励步行等积极行为至关重要，步行性指数是促进健康步行环境的有效工具。然而，欧洲一直缺乏一个标准化的高分辨率步行性指数。", "method": "本研究开发了一个标准化的、高分辨率的欧洲步行性指数。选取了七个核心组成部分来定义步行性：可步行街道长度、交叉口密度、绿地、坡度、公共交通可达性、土地利用混合度以及15分钟步行等时线。这些数据来源于Sentinel-2、NASA高程模型、OpenStreetMap和CORINE土地覆盖等协调统一的高分辨率数据集。研究采用了100米x100米的层次网格系统和先进的地理空间方法（如网络缓冲区和距离衰减）来建模密度和邻近效应。所得指数按人口加权，并通过可视化地图、空间聚类和相关性分析在不同空间层面进行分析。", "result": "研究结果显示，步行性得分呈现明显的城乡梯度，高步行性得分集中在街道连通性和土地利用多样性丰富的紧凑型城市中心。该指数突出显示了巴塞罗那、柏林、慕尼黑、巴黎和华沙等城市在步行性方面表现突出。", "conclusion": "本研究开发的标准化、高分辨率步行性指数为研究人员、规划者和政策制定者提供了一个实用工具，旨在支持欧洲不同背景下的积极生活和公共健康。", "translation": "身体不活动是导致肥胖和其他非传染性疾病的重要因素，然而提高全民身体活动水平的努力收效甚微。建成环境在鼓励步行等积极行为方面发挥着关键作用。步行性指数整合了各种环境特征，为促进健康、宜步行的环境提供了有价值的工具。然而，欧洲一直缺乏一个标准化的、高分辨率的步行性指数。本研究通过为整个欧洲地区开发一个标准化的、高分辨率的步行性指数来弥补这一空白。选择了七个核心组成部分来定义步行性：可步行街道长度、交叉口密度、绿地、坡度、公共交通可达性、土地利用混合度和15分钟步行等时线。这些数据来源于Sentinel-2、NASA高程模型、OpenStreetMap和CORINE土地覆盖等协调统一的高分辨率数据集。采用了100米x100米的层次网格系统和先进的地理空间方法，如网络缓冲区和距离衰减，以大规模高效地模拟真实世界的密度和邻近效应。所得指数按人口加权，并使用可视化地图、空间聚类和相关性分析在不同空间层面进行分析。研究结果揭示了明显的城乡梯度，高步行性得分集中在街道连通性和土地利用多样性丰富的紧凑型城市中心。该指数突出显示了巴塞罗那、柏林、慕尼黑、巴黎和华沙等城市是步行性方面的领导者。这种标准化的、高分辨率的步行性指数可作为研究人员、规划者和政策制定者的实用工具，旨在支持欧洲不同背景下的积极生活和公共健康。", "summary": "本研究旨在解决欧洲缺乏标准化、高分辨率步行性指数的问题，开发了一个覆盖整个欧洲的步行性指数，以促进身体活动。该指数基于可步行街道长度、交叉口密度等七个核心要素，利用高分辨率地理空间数据集和先进的地理空间方法构建。分析结果揭示了明显的城乡步行性梯度，高步行性得分集中在城市中心。该指数为支持欧洲积极生活和公共健康的规划和政策制定提供了实用工具。", "keywords": "步行性指数, 欧洲, 地理空间分析, 公共健康, 城市规划", "comments": "该论文通过开发首个覆盖整个欧洲地区且标准化的高分辨率步行性指数，做出了重要贡献。其创新之处在于综合整合了多样化的、高分辨率的地理空间数据集，并运用了先进的地理空间方法（如100米网格系统、网络缓冲区、距离衰减）来大规模建模复杂的步行性因素。该指数对于公共卫生倡议和城市规划具有重要意义，为政策制定者识别需要干预的区域和支持积极生活提供了实用工具。"}}
{"id": "2507.17918", "title": "One Whisper to Grade Them All", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "categories": ["cs.CL", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to SLaTE 2025 workshop", "url": "http://arxiv.org/abs/2507.17918v1", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "comment": "Accepted to SLaTE 2025 workshop", "pdf_url": "http://arxiv.org/pdf/2507.17918v1", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "一键评分：基于 Whisper 的多部分二语口语自动评估", "tldr": "本文提出了一种高效的端到端方法，用于多部分二语口语测试的整体自动口语评估（ASA），该方法使用单个 Whisper-small 编码器处理所有口语响应，通过轻量级聚合器结合信息，并预测最终分数，显著减少了推理时间并提高了数据效率。", "motivation": "当前的自动口语评估（ASA）系统可能需要转录和单独的每部分模型，导致推理时间长，且在大规模计算机辅助语言学习系统中应用不便。本文旨在为2025年Speak & Improve挑战赛开发一种更高效、端到端的整体ASA方法。", "method": "本文提出了一种端到端的整体自动口语评估（ASA）方法。其核心创新在于使用单个 Whisper-small 编码器处理所有四个口语响应，并通过一个轻量级聚合器整合所有信息，最终预测最终分数。这种架构消除了对转录和每部分独立模型的需求。此外，作者还提出了一种数据采样策略，允许模型仅使用语料库中44.8%的说话者进行训练。", "result": "该系统实现了0.384的均方根误差（RMSE），优于基于文本的基线（0.44）。系统使用的参数量最多为1.68亿（约占Whisper-small的70%）。此外，所提出的数据采样策略使模型仅使用44.8%的说话者数据就能达到0.383的RMSE，这表明在不平衡类别上性能有所提高，并具有强大的数据效率。", "conclusion": "本文提出的基于单个Whisper编码器的端到端自动口语评估系统，通过其新颖的架构和数据采样策略，显著提高了评估效率和数据效率，并展现出优于现有基线的性能，使其在大规模计算机辅助语言学习系统中具有实际应用价值。", "translation": "我们提出了一种高效的端到端方法，用于对多部分第二语言测试进行整体自动口语评估（ASA），该方法是为2025年Speak & Improve挑战赛开发的。我们系统的主要新颖之处在于能够使用单个 Whisper-small 编码器处理所有四个口语响应，通过一个轻量级聚合器结合所有信息，并预测最终分数。这种架构消除了对转录和每部分模型的需求，缩短了推理时间，并使 ASA 在大规模计算机辅助语言学习系统中变得实用。\n我们的系统实现了0.384的均方根误差（RMSE），优于基于文本的基线（0.44），同时最多使用1.68亿参数（约占 Whisper-small 的70%）。此外，我们提出了一种数据采样策略，允许模型仅在语料库中44.8%的说话者数据上进行训练，仍能达到0.383的 RMSE，这表明在不平衡类别上性能有所提高，并具有强大的数据效率。", "summary": "本文介绍了一种为2025年Speak & Improve挑战赛开发的高效端到端自动口语评估（ASA）系统。该系统创新性地使用单个Whisper-small编码器处理多部分口语响应，并通过轻量级聚合器预测最终分数，从而无需转录和独立模型，显著减少了推理时间。实验结果显示，该系统在RMSE上优于文本基线（0.384 vs 0.44），且参数量更少（1.68亿）。此外，提出的数据采样策略提高了数据效率，仅用44.8%的说话者数据即可达到相似性能，展现出在不平衡类别上的优势。", "keywords": "自动口语评估, Whisper, 端到端, 数据效率, 语言学习", "comments": "本文的创新点在于其端到端的单编码器架构，极大地简化了多部分口语评估流程，提高了效率和实用性。使用单个Whisper编码器处理所有口语响应并结合轻量级聚合器，有效减少了对额外转录和多个模型的依赖。此外，数据采样策略的提出，在保证性能的同时显著提升了数据效率，对于处理大规模不平衡数据集具有重要意义。这使得ASA在大规模计算机辅助语言学习系统中更具可行性。"}}
{"id": "2507.18338", "title": "Uncertainty Quantification for Evaluating Machine Translation Bias", "authors": ["Ieva Raminta Staliūnaitė", "Julius Cheng", "Andreas Vlachos"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18338v1", "summary": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18338v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "评估机器翻译偏见的量化不确定性", "tldr": "机器翻译模型在处理性别模糊词时存在偏见，即使去偏后，对明确和模糊实例的影响也不同，模型应在歧义时保持不确定性。", "motivation": "机器翻译模型在处理性别不明确的词时表现出偏见行为，即使与上下文冲突也依赖刻板印象。作者认为模型在性别模糊时应保持不确定性，而不是盲目推断。", "method": "使用最近提出的语义不确定性度量来评估模型。", "result": "发现模型在明确实例上翻译和性别准确性高，但在模糊实例上不一定表现出预期的不确定性水平。去偏对模糊和明确的翻译实例有独立影响。", "conclusion": "机器翻译模型在处理性别模糊性时未能充分表达不确定性，且去偏策略对不同类型的翻译实例（明确与模糊）有不同的效果。", "translation": "在机器翻译（MT）中，当源句包含一个性别未明确标记但其目标语言等效词需要指定性别的词素时，模型必须从上下文和/或外部知识推断出适当的性别。研究表明，机器翻译模型表现出偏见行为，即使与上下文信息冲突也依赖于刻板印象。我们认为，除了在输入明确时自信地使用正确性别进行翻译外，模型在性别模糊时也应保持不确定性。使用最近提出的语义不确定性度量，我们发现，在明确实例上具有高翻译和性别准确性的模型不一定在模糊实例中表现出预期水平的不确定性。同样，去偏对模糊和明确的翻译实例有独立影响。", "summary": "本文探讨了机器翻译模型在处理性别模糊词时的偏见问题。研究指出，模型在性别不明确时倾向于依赖刻板印象，即使其与上下文冲突。作者提出模型在面对模糊性别信息时应保持不确定性。通过使用语义不确定性度量，研究发现高准确性模型在处理模糊实例时未能表现出足够的预期不确定性，且去偏对明确和模糊翻译实例的影响是独立的。", "keywords": "机器翻译, 性别偏见, 不确定性量化, 语义不确定性, 去偏", "comments": "这项研究通过引入不确定性量化来评估机器翻译中的性别偏见，提供了一个新的视角。它强调了模型不仅要准确翻译，更要在信息不明确时表达“不知道”的能力，这对于构建更鲁棒、公平的AI系统至关重要。发现去偏对不同类型的实例有独立影响也提示了未来去偏方法需要更细致的设计。"}}
{"id": "2412.10510", "title": "DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts", "authors": ["Tobias Braun", "Mark Rothermel", "Marcus Rohrbach", "Anna Rohrbach"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICML 2025 version. 9 pages main paper, 35 pages with appendix, 18 figures and 7 tables. Corrected two inconsistent numbers in Table 2", "url": "http://arxiv.org/abs/2412.10510v4", "summary": "The proliferation of disinformation demands reliable and scalable\nfact-checking solutions. We present Dynamic Evidence-based FAct-checking with\nMultimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for\nopen-domain, text-image claim verification. DEFAME operates in a six-stage\nprocess, dynamically selecting the tools and search depth to extract and\nevaluate textual and visual evidence. Unlike prior approaches that are\ntext-only, lack explainability, or rely solely on parametric knowledge, DEFAME\nperforms end-to-end verification, accounting for images in claims and evidence\nwhile generating structured, multimodal reports. Evaluation on the popular\nbenchmarks VERITE, AVerITeC, and MOCHEG shows that DEFAME surpasses all\nprevious methods, establishing itself as the new state-of-the-art fact-checking\nsystem for uni- and multimodal fact-checking. Moreover, we introduce a new\nmultimodal benchmark, ClaimReview2024+, featuring claims after the knowledge\ncutoff of GPT-4o, avoiding data leakage. Here, DEFAME drastically outperforms\nthe GPT-4o baselines, showing temporal generalizability and the potential for\nreal-time fact-checking.", "comment": "ICML 2025 version. 9 pages main paper, 35 pages with appendix, 18\n  figures and 7 tables. Corrected two inconsistent numbers in Table 2", "pdf_url": "http://arxiv.org/pdf/2412.10510v4", "cate": "cs.CV", "date": "2024-12-13", "updated": "2025-07-24", "AI": {"title_translation": "DEFAME：基于动态证据的多模态专家事实核查", "tldr": "DEFAME是一个模块化的零样本多模态大语言模型（MLLM）管道，用于开放域文本-图像声明验证，通过动态选择工具和搜索深度来提取和评估文本和视觉证据，并在多个基准测试中超越现有方法，成为新的最先进的事实核查系统。", "motivation": "虚假信息的泛滥对可靠和可扩展的事实核查解决方案提出了需求。", "method": "DEFAME是一个模块化的零样本多模态大语言模型（MLLM）管道，用于开放域文本-图像声明验证。它采用六阶段流程，动态选择工具和搜索深度来提取和评估文本和视觉证据。与以往方法不同，DEFAME执行端到端验证，考虑声明和证据中的图像，并生成结构化的多模态报告。", "result": "在VERITE、AVerITeC和MOCHEG等流行基准测试中，DEFAME超越了所有先前的方法，确立了其作为单模态和多模态事实核查新SOTA的地位。此外，在一个新的多模态基准测试ClaimReview2024+（包含GPT-4o知识截止日期后的声明）上，DEFAME显著优于GPT-4o基线。", "conclusion": "DEFAME是一个先进的事实核查系统，在当前基准测试中表现出色，并展现出时间泛化能力和实时事实核查的潜力。", "translation": "虚假信息的泛滥要求可靠且可扩展的事实核查解决方案。我们提出了DEFAME（基于动态证据的多模态专家事实核查），一个模块化的零样本多模态大语言模型（MLLM）管道，用于开放域的文本-图像声明验证。DEFAME采用六阶段流程，动态选择工具和搜索深度来提取和评估文本和视觉证据。与以往的纯文本、缺乏可解释性或仅依赖参数知识的方法不同，DEFAME执行端到端验证，考虑声明和证据中的图像，同时生成结构化的多模态报告。在流行的VERITE、AVerITeC和MOCHEG基准测试中的评估表明，DEFAME超越了所有先前的方法，确立了其作为单模态和多模态事实核查新SOTA的地位。此外，我们引入了一个新的多模态基准测试ClaimReview2024+，其中包含GPT-4o知识截止日期之后的声明，避免了数据泄露。在此，DEFAME显著优于GPT-4o基线，显示出时间泛化能力和实时事实核查的潜力。", "summary": "DEFAME是一个针对开放域文本-图像声明验证的模块化、零样本多模态大语言模型（MLLM）管道。它采用动态六阶段流程，以提取和评估文本及视觉证据，实现端到端的事实核查，并能生成结构化的多模态报告。该系统在多个现有基准测试中表现优于所有先前方法，并在一个新引入的、避免数据泄露的多模态基准测试上显著超越GPT-4o基线，展现出卓越的时间泛化能力和实时事实核查的潜力。", "keywords": "事实核查, 多模态, 证据, 零样本, MLLM", "comments": "DEFAME的创新之处在于其动态、多模态、零样本以及端到端的验证方法，能够处理图像并生成结构化报告。它通过引入新的基准测试并展示在知识截止日期后的声明上的优异表现，解决了现有模型数据泄露和时间泛化能力不足的问题，这对于实时事实核查至关重要。"}}
{"id": "2507.17962", "title": "TimelyHLS: LLM-Based Timing-Aware and Architecture-Specific FPGA HLS Optimization", "authors": ["Nowfel Mashnoor", "Mohammad Akyash", "Hadi Kamali", "Kimia Azar"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17962v1", "summary": "Achieving timing closure and design-specific optimizations in FPGA-targeted\nHigh-Level Synthesis (HLS) remains a significant challenge due to the complex\ninteraction between architectural constraints, resource utilization, and the\nabsence of automated support for platform-specific pragmas. In this work, we\npropose TimelyHLS, a novel framework integrating Large Language Models (LLMs)\nwith Retrieval-Augmented Generation (RAG) to automatically generate and\niteratively refine HLS code optimized for FPGA-specific timing and performance\nrequirements. TimelyHLS is driven by a structured architectural knowledge base\ncontaining FPGA-specific features, synthesis directives, and pragma templates.\nGiven a kernel, TimelyHLS generates HLS code annotated with both\ntiming-critical and design-specific pragmas. The synthesized RTL is then\nevaluated using commercial toolchains, and simulation correctness is verified\nagainst reference outputs via custom testbenches. TimelyHLS iteratively\nincorporates synthesis logs and performance reports into the LLM engine for\nrefinement in the presence of functional discrepancies. Experimental results\nacross 10 FPGA architectures and diverse benchmarks show that TimelyHLS reduces\nthe need for manual tuning by up to 70%, while achieving up to 4x latency\nspeedup (e.g., 3.85x for Matrix Multiplication, 3.7x for Bitonic Sort) and over\n50% area savings in certain cases (e.g., 57% FF reduction in Viterbi).\nTimelyHLS consistently achieves timing closure and functional correctness\nacross platforms, highlighting the effectiveness of LLM-driven,\narchitecture-aware synthesis in automating FPGA design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17962v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "TimelyHLS：基于LLM的感知时序和架构特定FPGA HLS优化", "tldr": "TimelyHLS是一个基于LLM和RAG的框架，用于自动生成和优化FPGA HLS代码，以实现时序收敛和性能提升，减少手动调优。", "motivation": "FPGA目标的高级综合（HLS）中实现时序收敛和设计特定优化仍然是一个重大挑战，这归因于架构约束、资源利用率之间的复杂交互以及缺乏对平台特定编译指示（pragmas）的自动化支持。", "method": "本文提出了TimelyHLS，一个将大型语言模型（LLM）与检索增强生成（RAG）相结合的新颖框架。它利用一个包含FPGA特定功能、综合指令和编译指示模板的结构化架构知识库，自动生成带有时序关键和设计特定编译指示的HLS代码。综合的RTL通过商业工具链评估，并通过自定义测试平台验证仿真正确性。TimelyHLS迭代地将综合日志和性能报告整合到LLM引擎中进行优化。", "result": "在10个FPGA架构和不同基准测试上的实验结果表明，TimelyHLS将手动调优的需求减少高达70%，实现高达4倍的延迟加速（例如，矩阵乘法3.85倍，比特排序3.7倍），并在某些情况下节省超过50%的面积（例如，维特比算法FF减少57%）。TimelyHLS始终在不同平台上实现时序收敛和功能正确性。", "conclusion": "TimelyHLS证明了LLM驱动的、架构感知的综合在自动化FPGA设计中的有效性，能够实现时序收敛和功能正确性，并显著减少手动调优。", "translation": "FPGA目标的高级综合（HLS）中实现时序收敛和设计特定优化仍然是一个重大挑战，这归因于架构约束、资源利用率之间的复杂交互以及缺乏对平台特定编译指示（pragmas）的自动化支持。在这项工作中，我们提出了TimelyHLS，一个新颖的框架，它将大型语言模型（LLM）与检索增强生成（RAG）相结合，以自动生成和迭代优化HLS代码，从而满足FPGA特定的时序和性能要求。TimelyHLS由一个结构化的架构知识库驱动，该知识库包含FPGA特定功能、综合指令和编译指示模板。给定一个内核，TimelyHLS生成带有时序关键和设计特定编译指示的HLS代码。然后使用商业工具链评估综合的RTL，并通过自定义测试平台根据参考输出验证仿真正确性。当存在功能差异时，TimelyHLS将综合日志和性能报告迭代地整合到LLM引擎中进行优化。在10个FPGA架构和不同基准测试上的实验结果表明，TimelyHLS将手动调优的需求减少高达70%，同时实现高达4倍的延迟加速（例如，矩阵乘法3.85倍，比特排序3.7倍）并在某些情况下节省超过50%的面积（例如，维特比算法FF减少57%）。TimelyHLS始终在不同平台上实现时序收敛和功能正确性，这突出了LLM驱动的、架构感知综合在自动化FPGA设计中的有效性。", "summary": "TimelyHLS是一个创新的框架，它结合了LLM和RAG，旨在解决FPGA HLS中时序收敛和设计优化面临的挑战。通过利用结构化知识库自动生成和迭代优化HLS代码，TimelyHLS显著减少了手动调优的需求，并在多个FPGA架构和基准测试上实现了显著的性能提升（高达4倍延迟加速，超过50%面积节省）和可靠的时序收敛及功能正确性。", "keywords": "FPGA HLS, LLM, 检索增强生成, 时序优化, 自动化设计", "comments": "这篇论文的创新点在于将LLM和RAG引入到FPGA HLS优化领域，解决了传统HLS中手动调优耗时且复杂的痛点。通过自动化生成和迭代优化HLS代码，TimelyHLS展示了LLM在硬件设计自动化方面的巨大潜力。其在减少手动工作量和提升性能方面的显著成果，预示着未来AI驱动的硬件设计流程将更加高效和智能。"}}
{"id": "2507.17755", "title": "Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image", "authors": ["Jianfeng Lan", "Yingjia Huang"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17755v1", "summary": "In the digital era, social media platforms play a pivotal role in shaping\nadolescents' body image perceptions. This study examines how Douyin and WeChat,\ntwo contrasting Chinese social media platforms, influence body image among\nChinese male adolescents. Employing a platformization perspective, we surveyed\n395 male adolescents aged 10 to 24 using the Multidimensional Body-Self\nRelations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation\nand body satisfaction. Our findings reveal that Douyin usage is significantly\ncorrelated with appearance evaluation and body area satisfaction, while WeChat\nusage shows no significant correlation with any body image dimensions. These\nresults suggest that Douyin's algorithm-driven, video-centric environment\nintensifies exposure to idealized body standards, impacting users at a\ncognitive level. This study underscores the importance of considering\nplatform-specific characteristics in understanding social media's impact on\nbody image. It contributes to the broader discourse on how technological design\nand content modalities mediate psychological outcomes, offering insights for\naddressing body image concerns among male adolescents in China.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17755v1", "cate": "cs.HC", "date": "2025-05-02", "updated": "2025-05-02", "AI": {"title_translation": "滤镜与信息流之间：探究抖音和微信对中国青少年身体形象的影响", "tldr": "研究发现抖音使用与中国男性青少年身体形象评价和满意度显著相关，而微信则无显著关联，表明平台特性对身体形象影响的重要性。", "motivation": "在数字时代，社交媒体平台在塑造青少年身体形象认知方面发挥着关键作用。本研究旨在探究抖音和微信这两种不同的中国社交媒体平台如何影响中国男性青少年的身体形象。", "method": "采用平台化视角，调查了395名年龄在10至24岁之间的男性青少年，使用多维度身体自我关系问卷-外貌量表（MBSRQ-AS）评估自我评价和身体满意度。", "result": "抖音使用与外貌评价和身体部位满意度显著相关，而微信使用与任何身体形象维度均无显著相关性。结果表明抖音的算法驱动、以视频为中心的环境加剧了对理想身体标准的接触，从而在认知层面影响用户。", "conclusion": "本研究强调了在理解社交媒体对身体形象的影响时考虑平台特定特征的重要性。它有助于更广泛地探讨技术设计和内容模式如何调节心理结果，并为解决中国男性青少年身体形象问题提供见解。", "translation": "在数字时代，社交媒体平台在塑造青少年身体形象认知方面发挥着关键作用。本研究探讨了抖音和微信这两种截然不同的中国社交媒体平台如何影响中国男性青少年的身体形象。我们采用平台化视角，调查了395名年龄在10至24岁之间的男性青少年，使用多维度身体自我关系问卷-外貌量表（MBSRQ-AS）来评估自我评价和身体满意度。我们的研究结果显示，抖音的使用与外貌评价和身体部位满意度显著相关，而微信的使用与任何身体形象维度均无显著相关性。这些结果表明，抖音的算法驱动、以视频为中心的环境加剧了对理想身体标准的接触，从而在认知层面影响用户。本研究强调了在理解社交媒体对身体形象的影响时考虑平台特定特征的重要性。它有助于更广泛地探讨技术设计和内容模式如何调节心理结果，为解决中国男性青少年的身体形象问题提供见解。", "summary": "这项研究调查了抖音和微信对中国男性青少年身体形象的影响。通过对395名10-24岁男性青少年的问卷调查，发现抖音的使用与外貌评价和身体部位满意度显著相关，而微信则无显著关联。研究强调了平台特性在社交媒体对身体形象影响中的关键作用，并为解决青少年身体形象问题提供了见解。", "keywords": "抖音, 微信, 身体形象, 青少年, 平台化", "comments": "本研究创新性地从平台特性角度探究了不同社交媒体（抖音与微信）对青少年身体形象的影响，而非简单地将社交媒体视为同质化实体。其结果揭示了抖音算法驱动的视频环境对用户认知层面的具体影响，为理解技术设计如何调节心理结果提供了有价值的见解，对制定干预策略具有重要指导意义。"}}
{"id": "2507.17893", "title": "Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes", "authors": ["Milad Taghipour", "Bane Vasic"], "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17893v1", "summary": "This paper explores the application of reinforcement learning techniques to\nenhance the performance of decoding of linear block codes based on flipping\nbits and finding optimal decisions. We describe the methodology for mapping the\niterative decoding process into Markov Decision Processes (MDPs) and propose\ndifferent methods to reduce the number of states in the MDP. A truncated MDP is\nproposed to reduce the number of states in the MDP by learning a Hamming ball\nwith a specified radius around codewords. We then propose a general scheme for\nreinforcement learning based decoders applicable to any class of codes to\nimprove the performance of decoders. We call this scheme an action-list\ndecoding. We design an action-list decoder based on the Deep-Q network values\nthat substantially enhance performance. We also get benefit of automorphism\ngroup of code to further improve the code performance. Additionally, we propose\na feedback-based method to exploit and enhance the performance of existing\nhigh-performing decoders by applying reinforcement learning algorithms after\nthe existing decoders. These approaches effectively reduces the complexity of\nthe reinforcement learning block. Finally, we present experimental results for\nthe Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel\n(BSC) to demonstrate the efficiency of the proposed methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17893v1", "cate": "cs.IT", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "二进制线性分组码的动作列表强化学习伴随式译码", "tldr": "本文探讨了使用强化学习，特别是动作列表译码和深度Q网络，来提升线性分组码的译码性能并降低复杂性，并在LDPC码上进行了验证。", "motivation": "本文旨在探索应用强化学习技术，通过翻转比特和寻找最优决策，来提升线性分组码的译码性能。", "method": "研究人员将迭代译码过程映射到马尔可夫决策过程（MDPs），并提出了多种方法来减少MDP中的状态数量，包括引入截断MDP以学习码字周围指定半径的汉明球。他们提出了一种通用的、适用于任何码类的强化学习基译码方案，称之为“动作列表译码”。具体设计了一个基于深度Q网络（Deep-Q network）值的动作列表译码器。此外，还利用码的自同构群来进一步提升性能，并提出了一种基于反馈的方法，在现有高性能译码器之后应用强化学习算法，以利用和增强其性能。这些方法旨在有效降低强化学习模块的复杂性。", "result": "基于深度Q网络的动作列表译码器显著提升了译码性能。利用码的自同构群进一步改善了码的性能。所提出的反馈式方法及其他方法有效降低了强化学习模块的复杂性。在二元对称信道（BSC）上的低密度奇偶校验（LDPC）码的实验结果证明了所提方法的效率。", "conclusion": "本文提出的基于强化学习的动作列表译码方案，通过结合深度Q网络、利用自同构群和反馈机制，有效提升了二进制线性分组码的译码性能并降低了复杂性。", "translation": "本文探讨了强化学习技术在提升线性分组码译码性能方面的应用，该译码基于比特翻转和寻找最优决策。我们描述了将迭代译码过程映射到马尔可夫决策过程（MDPs）的方法，并提出了减少MDP状态数量的不同方法。为了减少MDP中的状态数量，提出了一种截断MDP，通过学习码字周围指定半径的汉明球来实现。然后，我们提出了一种通用的强化学习基译码方案，适用于任何码类，以提升译码器的性能。我们将此方案称为动作列表译码。我们设计了一个基于深度Q网络值的动作列表译码器，它显著提升了性能。我们还利用码的自同构群来进一步提升码的性能。此外，我们提出了一种基于反馈的方法，通过在现有译码器之后应用强化学习算法来利用和增强现有高性能译码器的性能。这些方法有效降低了强化学习模块的复杂性。最后，我们展示了在二元对称信道（BSC）上的低密度奇偶校验（LDPC）码的实验结果，以证明所提方法的效率。", "summary": "本文探讨了将强化学习技术应用于线性分组码的译码过程，旨在通过比特翻转和最优决策来提升译码性能。研究人员将迭代译码过程建模为马尔可夫决策过程（MDPs），并提出了减少MDP状态的方法，包括截断MDP。他们引入了一种通用的“动作列表译码”方案，并设计了基于深度Q网络的译码器以显著增强性能。此外，研究还利用了码的自同构群来进一步提升性能，并提出了一种反馈式方法，在现有高性能译码器之后应用强化学习算法，以降低强化学习块的复杂性。实验结果在LDPC码上验证了所提方法的有效性。", "keywords": "强化学习, 伴随式译码, 线性分组码, 深度Q网络, LDPC码", "comments": "本文创新性地将强化学习应用于二进制线性分组码的伴随式译码问题，提出了“动作列表译码”这一通用框架，并巧妙地结合了深度Q网络、截断MDP进行状态空间缩减，以及利用码的自同构群来优化性能。特别值得一提的是，通过引入反馈机制在现有译码器之后应用RL，有效降低了RL模块的复杂性，这对于实际应用具有重要意义。该研究为通信领域中的译码算法带来了新的视角和潜在的性能提升。"}}
{"id": "2502.14400", "title": "HPS: Hard Preference Sampling for Human Preference Alignment", "authors": ["Xiandong Zou", "Wanyu Lin", "Yuchen Li", "Pan Zhou"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.14400v4", "summary": "Aligning Large Language Model (LLM) responses with human preferences is vital\nfor building safe and controllable AI systems. While preference optimization\nmethods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown\npromise, they face challenges such as poor handling of harmful content,\ninefficient use of dispreferred responses, and, specifically for PL, high\ncomputational costs. To address these issues, we propose Hard Preference\nSampling (HPS), a novel framework for robust and efficient human preference\nalignment. HPS introduces a training loss that prioritizes the most preferred\nresponse while rejecting all dispreferred and harmful ones. It emphasizes\n\"hard\" dispreferred responses -- those closely resembling preferred ones -- to\nenhance the model's rejection capabilities. By leveraging a single-sample Monte\nCarlo sampling strategy, HPS reduces computational overhead while maintaining\nalignment quality. Theoretically, HPS improves sample efficiency over existing\nPL methods and maximizes the reward margin between preferred and dispreferred\nresponses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety\ndatasets validate HPS's effectiveness, achieving comparable BLEU and reward\nscores while greatly improving reward margins and thus reducing harmful content\ngeneration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.14400v4", "cate": "cs.AI", "date": "2025-02-20", "updated": "2025-07-24", "AI": {"title_translation": "HPS：用于人类偏好对齐的硬偏好采样", "tldr": "HPS是一种新颖的硬偏好采样框架，通过优先处理最受欢迎的响应并拒绝所有不受欢迎和有害的响应，解决了现有大型语言模型偏好优化方法在处理有害内容、低效利用不偏好响应和高计算成本方面的挑战，同时提高了样本效率和奖励裕度，并有效减少了有害内容的生成。", "motivation": "将大型语言模型（LLM）的响应与人类偏好对齐对于构建安全可控的AI系统至关重要。然而，基于Plackett-Luce（PL）和Bradley-Terry（BT）模型的现有偏好优化方法面临挑战，包括对有害内容处理不佳、对不受欢迎响应的利用效率低下，以及PL模型特有的高计算成本。", "method": "本文提出了硬偏好采样（HPS），这是一个用于鲁棒高效人类偏好对齐的新颖框架。HPS引入了一种训练损失，该损失优先选择最偏好的响应，同时拒绝所有不受偏好和有害的响应。它强调“硬”的不偏好响应（那些与偏好响应非常相似的），以增强模型的拒绝能力。通过利用单样本蒙特卡洛采样策略，HPS在保持对齐质量的同时降低了计算开销。", "result": "在HH-RLHF和PKU-Safety数据集上的实验验证了HPS的有效性，实现了可比较的BLEU和奖励分数，同时大大提高了奖励裕度，从而减少了有害内容的生成。", "conclusion": "HPS是一个针对人类偏好对齐的鲁棒且高效的框架，它通过创新的采样和损失机制，有效解决了现有方法在处理有害内容和计算效率方面的不足，并能显著提高模型对有害内容的拒绝能力。", "translation": "将大型语言模型（LLM）的响应与人类偏好对齐对于构建安全可控的AI系统至关重要。虽然基于Plackett-Luce（PL）和Bradley-Terry（BT）模型的偏好优化方法已显示出前景，但它们面临挑战，例如对有害内容处理不佳、对不受欢迎响应的利用效率低下，特别是PL模型计算成本高昂。为了解决这些问题，我们提出了硬偏好采样（HPS），这是一个用于鲁棒高效人类偏好对齐的新颖框架。HPS引入了一种训练损失，该损失优先选择最偏好的响应，同时拒绝所有不受偏好和有害的响应。它强调“硬”的不偏好响应——那些与偏好响应非常相似的——以增强模型的拒绝能力。通过利用单样本蒙特卡洛采样策略，HPS在保持对齐质量的同时降低了计算开销。理论上，HPS提高了现有PL方法的样本效率，并最大化了偏好和不偏好响应之间的奖励裕度，确保了更清晰的区别。在HH-RLHF和PKU-Safety数据集上的实验验证了HPS的有效性，实现了可比较的BLEU和奖励分数，同时大大提高了奖励裕度，从而减少了有害内容的生成。", "summary": "本文提出了一种名为硬偏好采样（HPS）的新框架，旨在解决大型语言模型在与人类偏好对齐时遇到的现有挑战，特别是对有害内容的处理、不偏好响应的低效利用以及高计算成本。HPS通过引入一种新的训练损失来优先选择最偏好的响应，同时明确拒绝所有不偏好和有害的响应。该方法特别关注那些与偏好响应相似的“硬”不偏好响应，以增强模型的拒绝能力。理论分析表明HPS能提高样本效率并最大化奖励裕度。实验结果证明，HPS在保持对齐质量的同时，有效减少了有害内容的生成，并降低了计算开销。", "keywords": "硬偏好采样, 人类偏好对齐, 大型语言模型, 奖励裕度, 有害内容减少", "comments": "HPS的创新之处在于其独特的“硬”不偏好采样策略，这有助于模型更好地学习区分细微的有害或不期望内容。此外，通过单样本蒙特卡洛采样降低计算成本，使其成为一个更实际的解决方案。该方法在提高模型安全性方面具有重要意义，因为它直接解决了有害内容生成的问题，这是当前LLM应用中的一个关键挑战。"}}
{"id": "2507.18451", "title": "Generation of Synthetic Clinical Text: A Systematic Review", "authors": ["Basel Alshaikhdeeb", "Ahmed Abdelmonem Hemedan", "Soumyabrata Ghosh", "Irina Balaur", "Venkata Satagopam"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18451v1", "summary": "Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18451v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "合成临床文本的生成：一项系统综述", "tldr": "本系统综述旨在分析合成临床文本的生成，发现其主要用于数据增强和隐私保护，主要采用Transformer架构，并通过相似性、隐私、结构和实用性进行评估。尽管隐私仍是主要问题，但合成文本在改进NLP任务方面显示出巨大潜力。", "motivation": "生成合成临床文本是解决临床自然语言处理（NLP）中数据稀疏性和隐私等常见问题的有效方案。", "method": "本研究对合成医学自由文本的生成进行了系统综述，通过对三个研究问题（生成目的、技术和评估方法）进行定量分析。研究人员在PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv数据库中检索了与生成合成医学非结构化自由文本相关的出版物，从1398篇收集的文章中识别出94篇相关文章。", "result": "自2018年以来，合成医学文本的生成受到了广泛关注，其主要目的包括文本增强、辅助写作、语料库构建、隐私保护、标注和实用性。Transformer架构（尤其是GPTs）是生成文本的主要技术。评估主要有四个方面：相似性、隐私、结构和实用性，其中实用性是最常用的评估方法。生成的合成医学文本在不同的下游NLP任务中作为真实医学文档表现出中等可能性，但在作为真实文档的增强和补充方面，在提高准确性和克服稀疏性/欠采样问题上被证明是巨大的资产。", "conclusion": "尽管生成的合成医学文本在不同下游NLP任务中作为真实医学文档表现出中等可能性，但它在作为增强和补充真实文档方面，在提高准确性和克服稀疏性/欠采样问题上被证明是巨大的资产。然而，隐私仍然是生成合成医学文本背后的一个主要问题，需要更多的人工评估来检查是否存在任何敏感信息。尽管如此，合成医学文本生成方面的进展将大大加速工作流程和管道开发的采用，从而省去耗时的数据传输合法性问题。", "translation": "生成合成临床文本是解决临床自然语言处理（NLP）中数据稀疏性和隐私等常见问题的有效方案。本文旨在通过对三个研究问题（i）生成目的、（ii）技术和（iii）评估方法进行定量分析，对生成合成医学自由文本进行系统综述。我们检索了PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv数据库中与生成合成医学非结构化自由文本相关的出版物。我们从收集的1398篇文章中识别出94篇相关文章。自2018年以来，合成医学文本的生成受到了广泛关注，其主要目的包括文本增强、辅助写作、语料库构建、隐私保护、标注和实用性。Transformer架构，特别是GPTs，是生成文本的主要技术。另一方面，评估主要有四个方面，包括相似性、隐私、结构和实用性，其中实用性是评估生成的合成医学文本最常用的方法。尽管生成的合成医学文本在不同的下游NLP任务中表现出作为真实医学文档的适度可能性，但它已被证明是作为增强真实文档的巨大资产，有助于提高准确性并克服稀疏性/欠采样问题。然而，隐私仍然是生成合成医学文本背后的一个主要问题，需要更多的人工评估来检查是否存在任何敏感信息。尽管如此，合成医学文本生成方面的进展将大大加速工作流程和管道开发的采用，从而省去耗时的数据传输合法性问题。", "summary": "本系统综述旨在全面分析合成临床文本的生成，以解决临床NLP中的数据稀疏性和隐私问题。研究通过检索多个数据库，识别出94篇相关文章，并对生成目的、技术和评估方法进行了定量分析。结果显示，自2018年以来，该领域受到广泛关注，主要目的包括文本增强和隐私保护，Transformer架构（特别是GPTs）是主导技术。评估主要关注相似性、隐私、结构和实用性。尽管合成文本在下游NLP任务中表现出作为真实文档的潜力，并能有效改善数据稀疏问题，但隐私泄露仍是一个重大挑战，需要进一步的人工审查。该领域的进展有望简化数据处理流程。", "keywords": "合成临床文本, 系统综述, 自然语言处理, 隐私保护, Transformer", "comments": "这项系统综述揭示了合成临床文本在解决临床NLP数据挑战方面的巨大潜力，特别是在数据增强和隐私保护方面。Transformer架构的广泛应用突显了深度学习在该领域的关键作用。然而，论文也明确指出了隐私保护这一核心限制，强调了未来研究和人工评估的必要性，这对于确保合成数据在实际应用中的安全性和可信度至关重要。其对加速数据处理流程的预期影响也体现了该研究的重要性。"}}
{"id": "2507.18210", "title": "On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics", "authors": ["Feng Wang", "Xiangyu Hu"], "categories": ["physics.flu-dyn", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "Comments:      50 pages and 27 figures and 6 tables", "url": "http://arxiv.org/abs/2507.18210v1", "summary": "As one of the major challenges for the conservative smoothed particle\nhydrodynamics (SPH) method, the zero-order consistency issue, although thought\nto be mitigated by the particle regularization scheme, such as the transport\nvelocity formulation, significantly damps the flow in a long channel for both\nlaminar and turbulent simulations. Building on this finding, this paper not\nonly thoroughly analyzes the damping reason in this pressure-driven channel\nflow, but also relates this problem with the excessive numerical dissipation in\nthe gravity-driven free-surface flow. The common root cause of the non-physical\nnumerical damping in the two typical flow scenarios, the zero-order gradient\nconsistency residue, is exposed. The adverse influence of the background\npressure on the residue for the two scenarios is revealed and discussed. To\ncomprehensively understand the behavior of the residue and mitigate its\npotential adverse effects, we conduct both theoretical analysis and numerical\nexperiments focusing on the key sensitive factors. For studying the\nresidue-induced non-physical energy dissipation in the gravity-driven\nfree-surface flow, the water depth and input dynamic pressure in the inviscid\nstanding wave case are tested. To investigate the velocity loss in the\npressure-driven channel flow, we examine the effects of the channel length,\nresolution, and outlet pressure. The state-of-the-art reverse kernel gradient\ncorrection technique is introduced for the two typical flows, and proved to be\neffective in reducing the residue effect, but we find its correction capability\nis fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is\ntested to demonstrate the residue influence in a complex geometry, highlighting\nthe necessity of correction schemes in scenarios with unavoidable high\nbackground pressure.", "comment": "50 pages and 27 figures and 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.18210v1", "cate": "physics.flu-dyn", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "守恒型SPH流体动力学中的零阶一致性残余和背景压力", "tldr": "本文深入分析了守恒型SPH中零阶一致性问题导致的非物理阻尼，揭示了零阶梯度一致性残余是根本原因，并探讨了背景压力的影响。研究发现现有校正技术有效但有局限，强调了在高背景压力场景下校正方案的必要性。", "motivation": "守恒型SPH方法中零阶一致性问题导致流体在长通道中出现显著阻尼，即使使用粒子正则化方案也未能完全解决。此外，该问题还与重力驱动自由表面流中过度的数值耗散相关联。本文旨在深入分析这种阻尼的原因，揭示其与数值耗散的共同根源（零阶梯度一致性残余），并探讨背景压力对残余的影响，以期全面理解并减轻其不利影响。", "method": "本文通过理论分析和数值实验，研究了零阶一致性残余及其关键敏感因素。针对重力驱动自由表面流（无粘立波），测试了水深和输入动压；针对压驱动通道流，考察了通道长度、分辨率和出口压力的影响。同时，引入并测试了最先进的逆核梯度校正技术，并在FDA喷嘴等复杂几何体中验证了残余影响。", "result": "研究发现，零阶梯度一致性残余是导致两种典型流体场景中非物理数值阻尼的共同根源，且背景压力对残余有不利影响。逆核梯度校正技术能有效降低残余效应，但其校正能力存在根本性局限。在复杂几何体（如FDA喷嘴）中，残余影响显著，凸显了在高背景压力场景下校正方案的必要性。", "conclusion": "守恒型SPH方法中的零阶梯度一致性残余是导致非物理阻尼的关键问题，并受背景压力的显著影响。尽管逆核梯度校正等现有技术能部分缓解此问题，但其校正能力有局限，特别是在高背景压力和复杂几何场景中，迫切需要更有效的校正方案。", "translation": "守恒型光滑粒子流体动力学（SPH）方法面临的主要挑战之一是零阶一致性问题。尽管人们认为粒子正则化方案（如传输速度公式）可以缓解此问题，但它在层流和湍流模拟中都会显著抑制长通道中的流动。基于这一发现，本文不仅深入分析了这种压驱动通道流中阻尼的原因，还将此问题与重力驱动自由表面流中过度的数值耗散联系起来。揭示了两种典型流动场景中非物理数值阻尼的共同根源——零阶梯度一致性残余。揭示并讨论了背景压力对这两种场景中残余的不利影响。为了全面理解残余的行为并减轻其潜在的不利影响，我们进行了理论分析和数值实验，重点关注关键敏感因素。为了研究重力驱动自由表面流中残余引起的非物理能量耗散，测试了无粘立波情况下的水深和输入动压。为了研究压驱动通道流中的速度损失，我们检查了通道长度、分辨率和出口压力的影响。将最先进的逆核梯度校正技术引入两种典型流动，并证明其在减少残余效应方面是有效的，但我们发现其校正能力存在根本性限制。最后，对工程基准FDA喷嘴进行了测试，以证明残余在复杂几何体中的影响，强调了在不可避免的高背景压力场景中校正方案的必要性。", "summary": "本文研究了守恒型SPH方法中零阶一致性问题导致的非物理阻尼，发现零阶梯度一致性残余是其共同根源，且受背景压力影响。通过理论分析和数值实验，验证了现有逆核梯度校正技术能部分缓解问题，但存在根本性局限。研究强调了在高背景压力和复杂几何场景下，有效校正方案的重要性。", "keywords": "SPH, 零阶一致性, 数值阻尼, 背景压力, 逆核梯度校正", "comments": "本文深入剖析了守恒型SPH方法中一个长期存在的挑战——零阶一致性问题。其创新点在于明确指出零阶梯度一致性残余是导致非物理阻尼的根本原因，并首次系统地探讨了背景压力对这一残余的影响。研究不仅提供了理论分析，还通过多种流体场景（包括复杂工程案例FDA喷嘴）的数值实验验证了其发现。尽管指出了现有校正技术的局限性，但其对问题根源的揭示和对高背景压力场景下校正必要性的强调，为未来SPH方法的改进提供了重要方向。"}}
{"id": "2503.12972", "title": "Aligning Vision to Language: Annotation-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning", "authors": ["Junming Liu", "Siyuan Meng", "Yanting Gao", "Song Mao", "Pinlong Cai", "Guohang Yan", "Yirong Chen", "Zilin Bian", "Ding Wang", "Botian Shi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures, 6 tables; Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.12972v2", "summary": "Multimodal reasoning in Large Language Models (LLMs) struggles with\nincomplete knowledge and hallucination artifacts, challenges that textual\nKnowledge Graphs (KGs) only partially mitigate due to their modality isolation.\nWhile Multimodal Knowledge Graphs (MMKGs) promise enhanced cross-modal\nunderstanding, their practical construction is impeded by semantic narrowness\nof manual text annotations and inherent noise in visual-semantic entity\nlinkages. In this paper, we propose Vision-align-to-Language integrated\nKnowledge Graph (VaLiK), a novel approach for constructing MMKGs that enhances\nLLMs reasoning through cross-modal information supplementation. Specifically,\nwe cascade pre-trained Vision-Language Models (VLMs) to align image features\nwith text, transforming them into descriptions that encapsulate image-specific\ninformation. Furthermore, we developed a cross-modal similarity verification\nmechanism to quantify semantic consistency, effectively filtering out noise\nintroduced during feature alignment. Even without manually annotated image\ncaptions, the refined descriptions alone suffice to construct the MMKG.\nCompared to conventional MMKGs construction paradigms, our approach achieves\nsubstantial storage efficiency gains while maintaining direct entity-to-image\nlinkage capability. Experimental results on multimodal reasoning tasks\ndemonstrate that LLMs augmented with VaLiK outperform previous state-of-the-art\nmodels. Our code is published at https://github.com/Wings-Of-Disaster/VaLiK.", "comment": "14 pages, 7 figures, 6 tables; Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.12972v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-24", "AI": {"title_translation": "视觉与语言对齐：免标注多模态知识图谱构建以增强大型语言模型推理", "tldr": "本文提出VaLiK，一种新颖的免标注多模态知识图谱（MMKG）构建方法，通过级联预训练的视觉-语言模型和跨模态相似性验证机制，将图像特征转化为文本描述，从而有效增强大型语言模型的（LLM）多模态推理能力，并实现存储效率提升。", "motivation": "大型语言模型（LLMs）在多模态推理中面临知识不完整和幻觉问题，而传统文本知识图谱（KGs）因模态隔离无法完全解决。多模态知识图谱（MMKGs）虽有望增强跨模态理解，但其构建受限于人工文本标注的语义狭窄和视觉-语义实体链接固有的噪声。", "method": "本文提出Vision-align-to-Language integrated Knowledge Graph (VaLiK)，一种构建MMKG的新方法。它通过级联预训练的视觉-语言模型（VLMs）将图像特征与文本对齐，将其转化为包含图像特定信息的描述。此外，开发了一种跨模态相似性验证机制来量化语义一致性，有效过滤特征对齐过程中引入的噪声。即使没有手动标注的图像描述，精炼后的描述也足以构建MMKG。", "result": "VaLiK方法在多模态推理任务上的实验结果表明，增强了VaLiK的LLMs超越了以往的最新模型。与传统MMKG构建范式相比，我们的方法在保持直接实体到图像链接能力的同时，实现了显著的存储效率提升。", "conclusion": "VaLiK是一种新颖的、免标注的多模态知识图谱构建方法，通过有效的跨模态信息补充和噪声过滤，显著提升了大型语言模型的多模态推理能力，并实现了存储效率的提升。", "translation": "大型语言模型（LLMs）中的多模态推理面临知识不完整和幻觉问题，而文本知识图谱（KGs）由于模态隔离只能部分缓解这些挑战。虽然多模态知识图谱（MMKGs）有望增强跨模态理解，但其实际构建受到人工文本标注的语义狭窄和视觉-语义实体链接固有噪声的阻碍。在本文中，我们提出了Vision-align-to-Language integrated Knowledge Graph (VaLiK)，这是一种构建MMKG的新颖方法，通过跨模态信息补充来增强LLMs的推理能力。具体来说，我们级联预训练的视觉-语言模型（VLMs）以将图像特征与文本对齐，将它们转化为封装图像特定信息的描述。此外，我们开发了一种跨模态相似性验证机制来量化语义一致性，有效过滤特征对齐过程中引入的噪声。即使没有手动标注的图像标题，精炼后的描述也足以构建MMKG。与传统MMKG构建范式相比，我们的方法在保持直接实体到图像链接能力的同时实现了显著的存储效率提升。多模态推理任务的实验结果表明，增强了VaLiK的LLMs超越了以往的最新模型。我们的代码已发布在https://github.com/Wings-Of-Disaster/VaLiK。", "summary": "本文提出VaLiK，一种创新的免标注多模态知识图谱（MMKG）构建方法，旨在解决LLMs在多模态推理中面临的知识不完整和幻觉问题。VaLiK通过级联预训练的视觉-语言模型将图像特征转化为文本描述，并引入跨模态相似性验证机制以过滤噪声。该方法无需人工标注，不仅有效提升了LLMs的多模态推理性能，还在存储效率上优于传统MMKG构建范式，同时保持了实体到图像的直接链接能力。", "keywords": "多模态知识图谱, 大型语言模型, 视觉-语言模型, 免标注, 跨模态推理", "comments": "本文的核心创新在于提出了一个免标注的多模态知识图谱（MMKG）构建框架VaLiK，解决了传统MMKG构建中人工标注成本高昂和噪声大的问题。通过巧妙地利用预训练的视觉-语言模型进行特征对齐并结合跨模态相似性验证机制，实现了图像信息到文本描述的有效转化和噪声过滤。这种方法不仅提升了LLMs的多模态推理能力，还在存储效率上有所突破，具有重要的实践意义和研究价值。"}}
{"id": "2507.18034", "title": "Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering", "authors": ["Haonan An", "Guang Hua", "Hangcheng Cao", "Zhengru Fang", "Guowen Xu", "Susanto Rahardja", "Yuguang Fang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18034v1", "summary": "The intellectual property of deep generative networks (GNets) can be\nprotected using a cascaded hiding network (HNet) which embeds watermarks (or\nmarks) into GNet outputs, known as box-free watermarking. Although both GNet\nand HNet are encapsulated in a black box (called operation network, or ONet),\nwith only the generated and marked outputs from HNet being released to end\nusers and deemed secure, in this paper, we reveal an overlooked vulnerability\nin such systems. Specifically, we show that the hidden GNet outputs can still\nbe reliably estimated via query-based reverse engineering, leaking the\ngenerated and unmarked images, despite the attacker's limited knowledge of the\nsystem. Our first attempt is to reverse-engineer an inverse model for HNet\nunder the stringent black-box condition, for which we propose to exploit the\nquery process with specially curated input images. While effective, this method\nyields unsatisfactory image quality. To improve this, we subsequently propose\nan alternative method leveraging the equivalent additive property of box-free\nmodel watermarking and reverse-engineering a forward surrogate model of HNet,\nwith better image quality preservation. Extensive experimental results on image\nprocessing and image generation tasks demonstrate that both attacks achieve\nimpressive watermark removal success rates (100%) while also maintaining\nexcellent image quality (reaching the highest PSNR of 34.69 dB), substantially\noutperforming existing attacks, highlighting the urgent need for robust\ndefensive strategies to mitigate the identified vulnerability in box-free model\nwatermarking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18034v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过基于查询的逆向工程去除图像到图像模型中的无框水印", "tldr": "本文揭示了深度生成网络中无框水印的漏洞，并提出了两种基于查询的逆向工程方法，以100%的成功率去除水印并保持图像质量。", "motivation": "深度生成网络（GNets）的知识产权可以通过级联隐藏网络（HNet）将水印嵌入GNet输出中来保护，这被称为无框水印。尽管GNet和HNet都被封装在一个黑盒（操作网络，ONet）中，并且只有来自HNet的生成和标记输出被发布给最终用户并被认为是安全的，但本文揭示了此类系统中一个被忽视的漏洞。攻击者即使对系统了解有限，也能通过基于查询的逆向工程可靠地估计隐藏的GNet输出，从而泄露生成且未标记的图像。", "method": "本文提出了两种方法。第一种尝试是在严格的黑盒条件下逆向工程HNet的逆模型，通过利用专门策划的输入图像的查询过程来实现。虽然有效，但这种方法产生的图像质量不尽如人意。为了改进这一点，随后提出了第二种替代方法，利用无框模型水印的等效加性特性，并逆向工程HNet的正向替代模型，以更好地保留图像质量。", "result": "在图像处理和图像生成任务上的大量实验结果表明，这两种攻击都取得了令人印象深刻的水印去除成功率（100%），同时保持了出色的图像质量（达到最高的PSNR为34.69 dB），大大优于现有攻击。", "conclusion": "本文揭示了无框模型水印系统中的一个严重漏洞，并成功地通过查询式逆向工程方法实现了水印的完全去除，强调了迫切需要鲁棒的防御策略来缓解已识别的漏洞。", "translation": "深度生成网络（GNets）的知识产权可以通过级联隐藏网络（HNet）来保护，该网络将水印（或标记）嵌入到GNet的输出中，这被称为无框水印。尽管GNet和HNet都被封装在一个黑盒（称为操作网络，或ONet）中，并且只有HNet生成并标记的输出被发布给最终用户并被认为是安全的，但本文揭示了此类系统中一个被忽视的漏洞。具体来说，我们展示了即使攻击者对系统了解有限，隐藏的GNet输出仍然可以通过基于查询的逆向工程可靠地估计出来，从而泄露生成且未标记的图像。我们首次尝试是在严格的黑盒条件下逆向工程HNet的逆模型，为此我们建议利用特殊设计的输入图像的查询过程。虽然有效，但这种方法产生的图像质量不尽如人意。为了改进这一点，我们随后提出了一种替代方法，利用无框模型水印的等效加性特性，并逆向工程HNet的正向替代模型，以更好地保留图像质量。在图像处理和图像生成任务上的大量实验结果表明，这两种攻击都取得了令人印象深刻的水印去除成功率（100%），同时保持了出色的图像质量（达到最高的PSNR为34.69 dB），大大优于现有攻击，突出了迫切需要强大的防御策略来缓解无框模型水印中已识别的漏洞。", "summary": "本文揭示了深度生成网络中无框水印技术的一个安全漏洞。研究发现，即使在黑盒条件下，通过基于查询的逆向工程，攻击者也能可靠地估计并恢复被水印保护的原始未标记图像。文章提出了两种攻击方法：第一种是逆向工程HNet的逆模型，第二种是利用水印的加性特性逆向工程HNet的正向替代模型。实验证明，这两种方法都能以100%的成功率去除水印，并保持高图像质量，远超现有攻击，这表明无框水印系统急需更强大的防御机制。", "keywords": "无框水印, 逆向工程, 图像到图像模型, 水印去除, 安全漏洞", "comments": "本文揭示了无框模型水印的潜在安全风险，其创新点在于提出了两种新颖的基于查询的逆向工程攻击方法，尤其是在严格的黑盒条件下实现100%的水印去除率，并保持了优异的图像质量。这对于深度生成网络知识产权保护领域具有重要意义，提醒研究者和开发者需要重新审视并加强水印技术的鲁棒性。"}}
{"id": "2506.17798", "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "categories": ["cs.SE", "cs.CR"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.17798v2", "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.17798v2", "cate": "cs.SE", "date": "2025-06-21", "updated": "2025-07-24", "AI": {"title_translation": "SAVANT：通过语义引导可达性分析检测应用程序依赖项中的漏洞", "tldr": "SAVANT利用LLM和语义分析，更准确地检测Java应用程序依赖项中的漏洞，优于现有SCA工具。", "motivation": "现有的软件组成分析（SCA）工具在检测第三方库中的已知漏洞时，由于缺乏对API使用语义的理解以及分析复杂代码库的计算挑战，导致误报过多，给开发团队带来负担并延迟安全修复。", "method": "SAVANT结合了语义预处理和基于LLM的上下文分析来进行准确的漏洞检测。它首先将源代码分割成有意义的代码块并保留语义关系，然后利用基于LLM的反射来分析API使用上下文并确定实际的漏洞影响。", "result": "SAVANT在55个真实世界应用程序上的评估显示，其精度达到83.8%，召回率为73.8%，准确率为69.0%，F1分数为78.5%，优于现有最先进的SCA工具。", "conclusion": "SAVANT通过结合漏洞证明测试用例的洞察和LLM的代码语义理解能力，有效解决了现有SCA工具在检测应用程序依赖项中漏洞的局限性，提供了更准确的漏洞检测。", "translation": "Java开发中集成开源第三方库依赖项时，如果这些库包含已知漏洞，会带来显著的安全风险。现有的软件组成分析（SCA）工具由于在理解API使用语义方面的局限性以及分析复杂代码库时的计算挑战，难以有效检测这些库中易受攻击的API使用，导致不准确的漏洞警报，给开发团队带来负担并延迟关键的安全修复。为了解决这些挑战，我们利用两点洞察提出了SAVANT：漏洞证明测试用例展示了漏洞如何在特定上下文中被触发，以及大型语言模型（LLMs）可以理解代码语义。SAVANT结合语义预处理和LLM驱动的上下文分析，实现准确的漏洞检测。SAVANT首先将源代码分割成有意义的代码块，同时保留语义关系，然后利用基于LLM的反射来分析API使用上下文并确定实际的漏洞影响。我们对55个真实世界应用程序的评估表明，SAVANT的精度达到83.8%，召回率为73.8%，准确率为69.0%，F1分数为78.5%，优于现有最先进的SCA工具。", "summary": "SAVANT是一种新的漏洞检测工具，旨在解决现有软件组成分析（SCA）工具在检测Java应用程序依赖项中漏洞时的局限性。它通过结合语义预处理和基于大型语言模型（LLM）的上下文分析，理解API使用语义并准确识别漏洞影响。该方法利用漏洞证明测试用例和LLM的代码语义理解能力。在55个真实应用程序上的评估表明，SAVANT在精度、召回率、准确性和F1分数方面均优于现有SCA工具，显著提高了漏洞检测的准确性。", "keywords": "漏洞检测, 应用程序依赖, 语义分析, 大型语言模型, 软件组成分析", "comments": "SAVANT的创新之处在于其将语义预处理与大型语言模型（LLM）相结合，用于理解复杂的API使用上下文，从而显著提高了漏洞检测的准确性。这解决了现有SCA工具普遍存在的误报问题，对软件安全领域具有重要意义。该方法通过利用LLM理解代码语义的能力，为漏洞分析提供了一个新的视角。"}}
{"id": "2506.13695", "title": "OneRec Technical Report", "authors": ["Guorui Zhou", "Jiaxin Deng", "Jinghao Zhang", "Kuo Cai", "Lejian Ren", "Qiang Luo", "Qianqian Wang", "Qigen Hu", "Rui Huang", "Shiyao Wang", "Weifeng Ding", "Wuchao Li", "Xinchen Luo", "Xingmei Wang", "Zexuan Cheng", "Zixing Zhang", "Bin Zhang", "Boxuan Wang", "Chaoyi Ma", "Chengru Song", "Chenhui Wang", "Di Wang", "Dongxue Meng", "Fan Yang", "Fangyu Zhang", "Feng Jiang", "Fuxing Zhang", "Gang Wang", "Guowang Zhang", "Han Li", "Hengrui Hu", "Hezheng Lin", "Hongtao Cheng", "Hongyang Cao", "Huanjie Wang", "Jiaming Huang", "Jiapeng Chen", "Jiaqiang Liu", "Jinghui Jia", "Kun Gai", "Lantao Hu", "Liang Zeng", "Liao Yu", "Qiang Wang", "Qidong Zhou", "Shengzhe Wang", "Shihui He", "Shuang Yang", "Shujie Yang", "Sui Huang", "Tao Wu", "Tiantian He", "Tingting Gao", "Wei Yuan", "Xiao Liang", "Xiaoxiao Xu", "Xugang Liu", "Yan Wang", "Yi Wang", "Yiwu Liu", "Yue Song", "Yufei Zhang", "Yunfan Wu", "Yunfeng Zhao", "Zhanyu Liu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Authors are listed alphabetically by their first name", "url": "http://arxiv.org/abs/2506.13695v2", "summary": "Recommender systems have been widely used in various large-scale\nuser-oriented platforms for many years. However, compared to the rapid\ndevelopments in the AI community, recommendation systems have not achieved a\nbreakthrough in recent years. For instance, they still rely on a multi-stage\ncascaded architecture rather than an end-to-end approach, leading to\ncomputational fragmentation and optimization inconsistencies, and hindering the\neffective application of key breakthrough technologies from the AI community in\nrecommendation scenarios.\n  To address these issues, we propose OneRec, which reshapes the recommendation\nsystem through an end-to-end generative approach and achieves promising\nresults. Firstly, we have enhanced the computational FLOPs of the current\nrecommendation model by 10 $\\times$ and have identified the scaling laws for\nrecommendations within certain boundaries. Secondly, reinforcement learning\ntechniques, previously difficult to apply for optimizing recommendations, show\nsignificant potential in this framework. Lastly, through infrastructure\noptimizations, we have achieved 23.7% and 28.8% Model FLOPs Utilization (MFU)\non flagship GPUs during training and inference, respectively, aligning closely\nwith the LLM community. This architecture significantly reduces communication\nand storage overhead, resulting in operating expense that is only 10.6% of\ntraditional recommendation pipelines. Deployed in Kuaishou/Kuaishou Lite APP,\nit handles 25% of total queries per second, enhancing overall App Stay Time by\n0.54% and 1.24%, respectively. Additionally, we have observed significant\nincreases in metrics such as 7-day Lifetime, which is a crucial indicator of\nrecommendation experience. We also provide practical lessons and insights\nderived from developing, optimizing, and maintaining a production-scale\nrecommendation system with significant real-world impact.", "comment": "Authors are listed alphabetically by their first name", "pdf_url": "http://arxiv.org/pdf/2506.13695v2", "cate": "cs.IR", "date": "2025-06-16", "updated": "2025-07-24", "AI": {"title_translation": "OneRec 技术报告", "tldr": "OneRec 提出了一种端到端的生成式推荐系统，显著提升了计算效率、优化了一致性，并降低了运营成本，在实际部署中展现出优异的性能提升。", "motivation": "现有推荐系统仍依赖多阶段级联架构而非端到端方法，导致计算碎片化和优化不一致性，阻碍了人工智能领域突破性技术在推荐场景中的有效应用。", "method": "我们提出了OneRec，通过端到端的生成式方法重塑推荐系统。该方法将当前推荐模型的计算FLOPs提升了10倍，并发现了推荐系统的缩放定律。此外，它还成功应用了强化学习技术。通过基础设施优化，OneRec在训练和推理期间在旗舰GPU上分别实现了23.7%和28.8%的模型FLOPs利用率（MFU）。", "result": "OneRec将当前推荐模型的计算FLOPs提升了10倍，并发现了推荐系统的缩放定律。强化学习技术在该框架中展现出巨大潜力。在旗舰GPU上，训练和推理期间的模型FLOPs利用率（MFU）分别达到23.7%和28.8%。该架构显著降低了通信和存储开销，运营成本仅为传统推荐流水线的10.6%。部署在快手/快手极速版APP中，它处理了总查询量的25%，分别使APP停留时间提升了0.54%和1.24%，并观察到7天生命周期等关键指标的显著增长。", "conclusion": "OneRec通过端到端生成式方法成功重塑了推荐系统，解决了传统多阶段架构的弊端，实现了显著的性能提升和运营成本降低，并在大规模生产环境中得到了验证，提供了宝贵的实践经验和见解。", "translation": "推荐系统多年来已广泛应用于各种大规模面向用户的平台。然而，与人工智能社区的快速发展相比，推荐系统近年来并未取得突破。例如，它们仍然依赖多阶段级联架构而非端到端方法，导致计算碎片化和优化不一致性，阻碍了人工智能社区关键突破性技术在推荐场景中的有效应用。\n为了解决这些问题，我们提出了OneRec，它通过端到端的生成式方法重塑推荐系统，并取得了可喜的成果。首先，我们将当前推荐模型的计算FLOPs提升了10倍，并在一定范围内发现了推荐系统的缩放定律。其次，以前难以应用于优化推荐的强化学习技术，在该框架中显示出巨大潜力。最后，通过基础设施优化，我们在训练和推理期间在旗舰GPU上分别实现了23.7%和28.8%的模型FLOPs利用率（MFU），这与LLM社区的目标非常接近。这种架构显著减少了通信和存储开销，导致运营成本仅为传统推荐流水线的10.6%。部署在快手/快手极速版APP中，它处理了总查询量的25%，分别使APP停留时间提升了0.54%和1.24%。此外，我们还观察到7天生命周期等指标的显著增长，这是衡量推荐体验的关键指标。我们还提供了从开发、优化和维护具有重大实际影响的生产级推荐系统过程中获得的实践经验和见解。", "summary": "OneRec提出了一种创新的端到端生成式方法来重塑推荐系统，旨在解决传统多阶段架构带来的计算碎片化和优化不一致性问题。该系统通过将计算FLOPs提升10倍、引入强化学习以及优化基础设施，显著提高了模型效率（MFU高达28.8%）并大幅降低了运营成本（仅为传统方案的10.6%）。在快手APP的实际部署中，OneRec处理了25%的总查询量，并有效提升了用户停留时间和7天生命周期等关键指标，证明了其在大规模生产环境下的强大潜力和实用价值。", "keywords": "推荐系统, 端到端, 生成式方法, 强化学习, 缩放定律", "comments": "OneRec的创新之处在于其采用端到端的生成式方法，打破了传统推荐系统的多阶段级联瓶颈，这与大型语言模型（LLM）的发展趋势相契合。其在计算效率（FLOPs提升10倍）、强化学习应用、基础设施优化以及运营成本降低方面的成果，对于推荐系统领域具有重要意义。特别是在实际生产环境中的部署效果，进一步验证了其有效性和实用性。"}}
{"id": "2507.18576", "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      47 pages, 18 figures, authors are listed in alphabetical order by their last names", "url": "http://arxiv.org/abs/2507.18576v1", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.", "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names", "pdf_url": "http://arxiv.org/pdf/2507.18576v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SafeWork-R1：在AI-45°定律下安全与智能的协同进化", "tldr": "提出SafeWork-R1多模态推理模型，通过SafeLadder框架实现能力与安全的协同进化，在安全基准测试上表现出色，并优于现有领先模型。", "motivation": "现有对齐方法（如RLHF）未能使AI模型发展出内在的安全推理和自我反思能力，导致能力与安全难以协同进化，需要一种新的方法来构建更鲁棒、可靠和值得信赖的通用AI。", "method": "本文引入了SafeWork-R1，一个通过SafeLadder框架开发的多模态推理模型。SafeLadder框架整合了大规模、渐进式、面向安全的强化学习后训练，并由一套多原则验证器支持，使模型能够发展出内在的安全推理和自我反思能力。此外，为增强可靠性，模型还实施了两种推理时干预方法和一种审慎搜索机制，强制执行步骤级验证。该框架的通用性通过应用于其他基础模型（如InternVL3-78B, DeepSeek-70B, Qwen2.5VL-7B）得到验证。", "result": "SafeWork-R1在安全相关基准测试上比其基础模型Qwen2.5-VL-72B平均提升46.54%，且未损害通用能力。它在安全性能方面达到了与GPT-4.1和Claude Opus 4等领先专有模型相媲美的最先进水平。通过SafeWork-R1-InternVL3-78B等模型展示，安全和能力可以协同进化，验证了框架的通用性。", "conclusion": "SafeWork-R1及其SafeLadder框架证明了AI的能力和安全性可以协同进化，为构建鲁棒、可靠和值得信赖的通用AI提供了一个通用且有效的方法。", "translation": "我们引入了SafeWork-R1，一个尖端的多模态推理模型，展示了能力和安全的协同进化。它由我们提出的SafeLadder框架开发，该框架结合了大规模、渐进式、面向安全的强化学习后训练，并由一套多原则验证器支持。与以前简单学习人类偏好的对齐方法（如RLHF）不同，SafeLadder使SafeWork-R1能够发展出内在的安全推理和自我反思能力，从而产生安全“顿悟”时刻。值得注意的是，SafeWork-R1在不损害通用能力的情况下，在安全相关基准测试上比其基础模型Qwen2.5-VL-72B平均提升了46.54%，并且与GPT-4.1和Claude Opus 4等领先的专有模型相比，提供了最先进的安全性能。为了进一步增强其可靠性，我们实施了两种不同的推理时干预方法和一种审慎搜索机制，强制执行步骤级验证。最后，我们进一步开发了SafeWork-R1-InternVL3-78B、SafeWork-R1-DeepSeek-70B和SafeWork-R1-Qwen2.5VL-7B。所有由此产生的模型都表明安全和能力可以协同进化，突出了我们框架在构建鲁棒、可靠和值得信赖的通用AI方面的通用性。", "summary": "本文介绍了SafeWork-R1，一个通过SafeLadder框架开发的多模态推理模型，旨在实现AI能力与安全的协同进化。SafeLadder采用大规模、渐进式、面向安全的强化学习后训练和多原则验证器，使模型具备内在安全推理和自我反思能力。实验结果显示，SafeWork-R1在安全基准测试上显著优于其基础模型，并达到SOTA水平，同时保持通用能力。该框架的通用性在其他模型上得到验证，表明其能有效构建鲁棒、可靠的通用AI。", "keywords": "多模态推理, AI安全, 协同进化, 强化学习, SafeLadder", "comments": "这篇论文的创新点在于提出了SafeLadder框架，通过结合大规模安全导向的强化学习和多原则验证器，使AI模型能够发展出内在的安全推理和自我反思能力，而非仅仅学习人类偏好。这与传统的对齐方法（如RLHF）有显著区别，有望解决AI能力与安全难以协同进化的挑战。其在保持通用能力的同时显著提升安全性能的成果，对于构建更值得信赖的通用AI具有重要意义。"}}
{"id": "2506.19733", "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?", "authors": ["Chuxuan Hu", "Yuxuan Zhu", "Antony Kellermann", "Caleb Biddulph", "Suppakit Waiwitlikhit", "Jason Benn", "Daniel Kang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, 4 figures, 2 tables", "url": "http://arxiv.org/abs/2506.19733v2", "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.", "comment": "9 pages, 4 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2506.19733v2", "cate": "cs.CL", "date": "2025-06-24", "updated": "2025-07-23", "AI": {"title_translation": "打破障碍：强化后训练的收益能否迁移到未见领域？", "tldr": "研究发现，尽管强化后训练（RPT）能显著提升大型语言模型在相似任务上的表现，但其收益在不同推理模式的未见领域中泛化能力不稳定甚至消失。", "motivation": "强化后训练（RPT）在提高大型语言模型（LLMs）的推理能力方面显示出潜力，但现有研究多在与微调数据相同的领域进行评估，其改进是否能很好地泛化到新领域尚不清楚。", "method": "本研究进行了两项实验：1) 观察性研究：比较了多种开源RPT模型与其对应的基础模型在多个领域（包括微调数据中出现和未出现的领域）的表现。2) 干预性研究：使用RPT在单一领域微调LLMs，并在多个领域评估其性能。", "result": "两项研究均得出相同结论：尽管RPT在与微调数据相似的任务上带来了显著提升，但这些提升的泛化能力不稳定，在具有不同推理模式的领域中可能会消失。", "conclusion": "强化后训练（RPT）对大型语言模型在特定任务上的性能提升是显著的，但其泛化能力有限，难以有效迁移到与训练数据推理模式不同的未见领域。", "translation": "强化后训练（RPT）最近在提高大型语言模型（LLMs）的推理能力方面显示出前景。然而，这些改进如何很好地泛化到新领域仍不清楚，因为先前的研究在用于微调的相同领域的数据上评估RPT模型。为了理解RPT的泛化能力，我们进行了两项研究。(1) 观察性：我们比较了多种开源RPT模型与其对应的基础模型在多个领域（包括微调数据中出现和未出现的领域）的表现。(2) 干预性：我们使用RPT在单一领域微调LLMs，并在多个领域评估其性能。两项研究都得出了相同的结论：尽管RPT在与微调数据相似的任务上带来了显著提升，但这些提升的泛化能力不稳定，在具有不同推理模式的领域中可能会消失。", "summary": "该研究旨在探究强化后训练（RPT）对大型语言模型（LLMs）推理能力的提升是否能泛化到未见领域。通过观察性研究和干预性研究，比较了RPT模型在已见和未见领域中的表现。结果表明，尽管RPT在与微调数据相似的任务上表现出显著提升，但其收益在推理模式不同的未见领域中泛化能力不稳定，甚至可能消失。", "keywords": "强化后训练, 大型语言模型, 领域泛化, 推理能力, 跨域", "comments": "这项研究对于理解强化后训练（RPT）的局限性具有重要意义。它揭示了RPT在泛化到新领域时面临的挑战，尤其是当新领域具有不同的推理模式时。这提示未来的研究需要关注如何提高RPT的领域适应性和鲁棒性，而不仅仅是关注在训练域内的性能提升。其创新点在于首次系统地探讨了RPT的跨域泛化能力，并通过两种研究方法提供了有力的证据。"}}
{"id": "2507.18028", "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18028v1", "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18028v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "NeuralDB：使用神经键值数据库将LLM知识编辑扩展到10万事实", "tldr": "NeuralDB是一个新的编辑框架，它将编辑后的事实表示为神经键值数据库，并配备非线性门控检索模块，旨在解决现有知识编辑方法在扩展时损害LLM通用能力和导致遗忘的问题，并已成功扩展到10万事实的编辑。", "motivation": "现有的大型语言模型（LLMs）知识编辑方法，尤其是Locate-and-Edit（L&E），在处理大量事实修改时，可能会损害LLMs的通用能力，甚至导致已编辑事实的遗忘。因此，需要一种能够高效且不损害模型性能地扩展知识编辑的方法。", "method": "本研究将现有的线性L&E方法建模为对键值（KV）数据库的查询。在此基础上，提出了NeuralDB框架，它将编辑后的事实显式表示为一个神经KV数据库，并配备了一个非线性的门控检索模块。该门控模块只在推理涉及编辑事实时才激活，从而有效保留了LLMs的通用能力。", "result": "在ZsRE和CounterFacts数据集上，对GPT2-XL、GPT-J (6B) 和Llama-3 (8B) 模型进行了包含10,000事实的综合实验。结果表明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面表现优异，同时在六项代表性的文本理解和生成任务中保持了整体性能。进一步的实验表明，NeuralDB在扩展到100,000事实（比现有工作多50倍）时仍能保持其有效性。", "conclusion": "NeuralDB通过引入神经键值数据库和门控检索模块，成功地将LLMs的知识编辑扩展到大规模事实（100,000个），同时有效保留了模型的通用能力，并在多项性能指标上表现出色。", "translation": "高效编辑大型语言模型（LLMs）中存储的知识，可以实现模型更新而无需大规模训练。一种可能的解决方案是定位-编辑（Locate-and-Edit, L&E），它允许同时修改大量事实。然而，这种编辑可能会损害LLMs的通用能力，甚至在扩展到数千次编辑时导致对已编辑事实的遗忘。在本文中，我们将现有的线性L&E方法建模为对键值（KV）数据库的查询。从这个角度，我们提出了NeuralDB，一个将编辑后的事实明确表示为神经KV数据库的编辑框架，并配备了一个非线性的门控检索模块。特别是，我们的门控模块仅在推理涉及已编辑事实时才操作，从而有效保留了LLMs的通用能力。在ZsRE和CounterFacts数据集上，使用GPT2-XL、GPT-J（6B）和Llama-3（8B）模型进行了涉及10,000事实编辑的综合实验。结果表明，NeuralDB不仅在编辑效率、泛化性、特异性、流畅性和一致性方面表现出色，而且在六项代表性的文本理解和生成任务中保持了整体性能。进一步的实验表明，即使扩展到100,000事实（比现有工作多50倍），NeuralDB仍能保持其有效性。", "summary": "本文提出了NeuralDB，一个用于大型语言模型（LLMs）知识编辑的新框架，旨在解决现有Locate-and-Edit（L&E）方法在处理大规模事实时可能导致的通用能力受损和遗忘问题。NeuralDB将编辑后的事实表示为神经键值（KV）数据库，并引入一个非线性门控检索模块，该模块仅在相关推理时激活，从而有效保护LLMs的通用能力。实验证明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面表现优异，并在扩展到100,000事实时依然有效，同时保持了LLMs的整体性能。", "keywords": "知识编辑, LLM, 神经键值数据库, 规模化, 通用能力", "comments": "这篇论文的创新点在于提出了NeuralDB框架，通过将编辑事实显式建模为神经KV数据库并引入门控检索模块，有效解决了LLMs知识编辑中规模化和通用能力保持的挑战。其核心贡献是实现了知识编辑从数千事实到10万事实的显著扩展，且不损害模型原有性能，这对于LLM的持续更新和维护具有重要意义。门控机制的设计也巧妙地平衡了编辑需求与模型通用性。"}}
{"id": "2507.17978", "title": "MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection", "authors": ["Paulo Mendes", "Eva Maia", "Isabel Praça"], "categories": ["cs.CR", "cs.AI", "cs.HC", "68P20 (Primary) 68T05, 68T07, 68T10 (Secondary)", "K.6.5; I.2.6; I.2.7; C.2.0"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      8 pages, 2 tables, WI-IAT 2025 conference", "url": "http://arxiv.org/abs/2507.17978v1", "summary": "Phishing emails continue to pose a significant threat to cybersecurity by\nexploiting human vulnerabilities through deceptive content and malicious\npayloads. While Machine Learning (ML) models are effective at detecting\nphishing threats, their performance largely relies on the quality and diversity\nof the training data. This paper presents MeAJOR (Merged email Assets from\nJoint Open-source Repositories) Corpus, a novel, multi-source phishing email\ndataset designed to overcome critical limitations in existing resources. It\nintegrates 135894 samples representing a broad number of phishing tactics and\nlegitimate emails, with a wide spectrum of engineered features. We evaluated\nthe dataset's utility for phishing detection research through systematic\nexperiments with four classification models (RF, XGB, MLP, and CNN) across\nmultiple feature configurations. Results highlight the dataset's effectiveness,\nachieving 98.34% F1 with XGB. By integrating broad features from multiple\ncategories, our dataset provides a reusable and consistent resource, while\naddressing common challenges like class imbalance, generalisability and\nreproducibility.", "comment": "8 pages, 2 tables, WI-IAT 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.17978v1", "cate": "cs.CR", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "MeAJOR语料库：一个用于网络钓鱼邮件检测的多源数据集", "tldr": "MeAJOR语料库是一个新的多源网络钓鱼邮件数据集，旨在解决现有资源的局限性，通过集成大量样本和广泛特征来提高钓鱼检测模型的性能。", "motivation": "网络钓鱼邮件利用欺骗性内容和恶意载荷对网络安全构成重大威胁。尽管机器学习模型在检测网络钓鱼威胁方面有效，但其性能很大程度上依赖于训练数据的质量和多样性，而现有资源存在局限性。", "method": "本文提出了MeAJOR（来自联合开源存储库的合并电子邮件资产）语料库，这是一个新颖的多源网络钓鱼邮件数据集。它集成了135894个样本，代表了广泛的网络钓鱼策略和合法邮件，并具有广泛的工程特征。通过使用四种分类模型（RF、XGB、MLP和CNN）在多种特征配置下进行系统实验，评估了数据集在网络钓鱼检测研究中的效用。", "result": "评估结果突出显示了数据集的有效性，使用XGB模型实现了98.34%的F1分数。", "conclusion": "通过整合来自多个类别的广泛特征，MeAJOR数据集提供了一个可重用且一致的资源，同时解决了类不平衡、泛化能力和可复现性等常见挑战。", "translation": "网络钓鱼邮件通过利用欺骗性内容和恶意载荷来利用人类的脆弱性，持续对网络安全构成重大威胁。尽管机器学习（ML）模型在检测网络钓鱼威胁方面有效，但其性能在很大程度上依赖于训练数据的质量和多样性。本文提出了MeAJOR（来自联合开源存储库的合并电子邮件资产）语料库，这是一个新颖的多源网络钓鱼邮件数据集，旨在克服现有资源中的关键局限性。它整合了135894个样本，代表了广泛的网络钓鱼策略和合法邮件，并具有广泛的工程特征。我们通过使用四种分类模型（RF、XGB、MLP和CNN）在多种特征配置下进行系统实验，评估了该数据集在网络钓鱼检测研究中的效用。结果突出显示了该数据集的有效性，使用XGB实现了98.34%的F1分数。通过整合来自多个类别的广泛特征，我们的数据集提供了一个可重用且一致的资源，同时解决了类不平衡、泛化能力和可复现性等常见挑战。", "summary": "本文介绍了MeAJOR语料库，一个针对网络钓鱼邮件检测的新型多源数据集。该数据集整合了135894个来自不同来源的样本，包含了广泛的钓鱼策略和合法邮件的工程特征，旨在解决现有数据集在质量和多样性上的不足。实验结果表明，该数据集能有效提升机器学习模型的检测性能，其中XGB模型达到了98.34%的F1分数。MeAJOR语料库通过提供一个可重用且一致的资源，有效解决了类不平衡、泛化能力和可复现性等挑战。", "keywords": "网络钓鱼邮件检测, 多源数据集, MeAJOR语料库, 机器学习, 网络安全", "comments": "该论文的创新点在于构建了一个大规模、多源且特征丰富的网络钓鱼邮件数据集，解决了现有数据集的局限性，提高了机器学习模型在钓鱼检测方面的性能。其重要性在于为网络安全研究提供了一个高质量的基准资源，有助于推动钓鱼检测技术的发展和应用。"}}
{"id": "2507.18348", "title": "VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation", "authors": ["Ioannis Sarridis", "Christos Koutlis", "Symeon Papadopoulos", "Christos Diou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18348v1", "summary": "Bias in computer vision models remains a significant challenge, often\nresulting in unfair, unreliable, and non-generalizable AI systems. Although\nresearch into bias mitigation has intensified, progress continues to be\nhindered by fragmented implementations and inconsistent evaluation practices.\nDisparate datasets and metrics used across studies complicate reproducibility,\nmaking it difficult to fairly assess and compare the effectiveness of various\napproaches. To overcome these limitations, we introduce the Visual Bias\nMitigator (VB-Mitigator), an open-source framework designed to streamline the\ndevelopment, evaluation, and comparative analysis of visual bias mitigation\ntechniques. VB-Mitigator offers a unified research environment encompassing 12\nestablished mitigation methods, 7 diverse benchmark datasets. A key strength of\nVB-Mitigator is its extensibility, allowing for seamless integration of\nadditional methods, datasets, metrics, and models. VB-Mitigator aims to\naccelerate research toward fairness-aware computer vision models by serving as\na foundational codebase for the research community to develop and assess their\napproaches. To this end, we also recommend best evaluation practices and\nprovide a comprehensive performance comparison among state-of-the-art\nmethodologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18348v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "VB-Mitigator：一个用于评估和推进视觉偏差缓解的开源框架", "tldr": "VB-Mitigator是一个开源框架，旨在统一和加速计算机视觉模型中视觉偏差缓解方法的研究、评估和比较。", "motivation": "计算机视觉模型中的偏差是一个重大挑战，导致AI系统不公平、不可靠且泛化能力差。尽管偏差缓解研究不断深入，但碎片化的实现和不一致的评估实践阻碍了进展，使得不同方法难以公平评估和比较。", "method": "本文引入了Visual Bias Mitigator（VB-Mitigator），一个开源框架，旨在简化视觉偏差缓解技术的开发、评估和比较分析。它提供了一个统一的研究环境。", "result": "VB-Mitigator框架包含了12种已建立的缓解方法和7个多样化的基准数据集。其主要优势在于可扩展性，允许无缝集成额外的方法、数据集、指标和模型。该框架还推荐了最佳评估实践，并提供了最先进方法之间的全面性能比较。", "conclusion": "VB-Mitigator旨在通过作为一个基础代码库，加速研究社区开发和评估公平感知计算机视觉模型，从而推动计算机视觉领域对公平性的研究。", "translation": "计算机视觉模型中的偏差仍然是一个重大挑战，通常导致人工智能系统不公平、不可靠且泛化能力差。尽管对偏差缓解的研究已经加强，但碎片化的实现和不一致的评估实践持续阻碍着进展。研究中使用的不同数据集和指标使可重复性复杂化，使得难以公平评估和比较各种方法的有效性。为了克服这些限制，我们引入了视觉偏差缓解器（VB-Mitigator），一个开源框架，旨在简化视觉偏差缓解技术的开发、评估和比较分析。VB-Mitigator提供了一个统一的研究环境，涵盖了12种已建立的缓解方法、7个多样化的基准数据集。VB-Mitigator的一个关键优势是其可扩展性，允许无缝集成额外的方法、数据集、指标和模型。VB-Mitigator旨在通过作为研究社区开发和评估其方法的基础代码库，加速对公平感知计算机视觉模型的研究。为此，我们还推荐了最佳评估实践，并提供了最先进方法之间的全面性能比较。", "summary": "VB-Mitigator是一个开源框架，旨在解决计算机视觉模型中偏差缓解研究面临的碎片化和评估不一致问题。它提供了一个统一的平台，集成了多种缓解方法和基准数据集，支持开发、评估和比较视觉偏差缓解技术。该框架具有高度可扩展性，并推荐了最佳评估实践，旨在加速公平感知计算机视觉模型的研发。", "keywords": "视觉偏差缓解, 开源框架, 计算机视觉, 公平性, 评估", "comments": "VB-Mitigator通过提供一个统一的开源框架，解决了计算机视觉偏差缓解领域长期存在的碎片化和评估不一致问题，这一点极具创新性。其包含现有方法和数据集，并强调可扩展性，对于推动该领域的研究进展具有重要意义，因为它能促进更公平、可重复的比较和评估。"}}
{"id": "2507.18085", "title": "Effects of variation in system responsiveness on user performance in virtual environments", "authors": ["Benjamin Watson", "Neff Walker", "William Ribarsky", "Victoria Spaulding"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18085v1", "summary": "System responsiveness (SR) is defined as the elapsed time until a system\nresponds to user control. SR fluctuates over time, so it must be described\nstatistically with mean (MSR) and standard deviation (SDSR). In this paper, we\nexamine SR in virtual environments (VEs), outlining its components and methods\nof experimental measurement and manipulation. Three studies of MSR and SDSR\neffects on performance of grasp and placement tasks are then presented. The\nstudies used within-subjects designs with 11, 12, and 10 participants,\nrespectively. Results showed that SDSR affected performance only if it was\nabove 82 ms. Placement required more frequent visual feedback and was more\nsensitive to SR. We infer that VE designers need not tightly control SDSR and\nmay wish to vary SR control based on required visual feedback frequency. These\nresults may be used to improve the human-computer interface in a wide range of\ninteractive graphical applications, including scientific visualization,\ntraining, mental health, and entertainment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18085v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "虚拟环境中系统响应性变化对用户性能的影响", "tldr": "本文研究了系统响应性（平均值和标准差）如何影响虚拟环境中的用户性能，发现高标准差仅在超过82毫秒时才会影响性能，并且放置任务对响应性更敏感。", "motivation": "系统响应性（SR）是系统响应用户控制所经过的时间，它会随时间波动，需要用平均值（MSR）和标准差（SDSR）进行统计描述。本文旨在研究虚拟环境中SR的组成部分、测量和操作方法，并探讨MSR和SDSR对用户在抓取和放置任务中性能的影响。", "method": "定义了系统响应性（SR）及其统计描述（MSR和SDSR）。概述了虚拟环境中SR的组成部分以及实验测量和操作方法。进行了三项被试内设计研究，分别有11名、12名和10名参与者。研究任务为抓取和放置任务。", "result": "结果显示，系统响应性标准差（SDSR）仅在高于82毫秒时才会影响性能。放置任务需要更频繁的视觉反馈，并且对系统响应性（SR）更敏感。", "conclusion": "虚拟环境（VE）设计者无需严格控制系统响应性标准差（SDSR），可以根据所需视觉反馈的频率来调整SR控制。这些结果可用于改进各种交互式图形应用程序中的人机界面，包括科学可视化、培训、心理健康和娱乐。", "translation": "系统响应性（SR）被定义为系统响应用户控制所经过的时间。SR随时间波动，因此必须通过平均值（MSR）和标准差（SDSR）进行统计描述。在本文中，我们研究了虚拟环境（VEs）中的SR，概述了其组成部分以及实验测量和操作方法。然后介绍了三项关于MSR和SDSR对抓取和放置任务性能影响的研究。这些研究分别采用了11名、12名和10名参与者的被试内设计。结果表明，SDSR仅在高于82毫秒时才会影响性能。放置任务需要更频繁的视觉反馈，并且对SR更敏感。我们推断，VE设计师不需要严格控制SDSR，并且可能希望根据所需的视觉反馈频率来改变SR控制。这些结果可用于改进各种交互式图形应用程序中的人机界面，包括科学可视化、培训、心理健康和娱乐。", "summary": "本文探讨了系统响应性（SR）及其平均值（MSR）和标准差（SDSR）对虚拟环境（VEs）中用户性能的影响。通过三项包含抓取和放置任务的被试内研究发现，SDSR仅在超过82毫秒时才会对性能产生负面影响。此外，需要频繁视觉反馈的放置任务对SR的变化更为敏感。研究结果表明，VE设计者无需严格控制SDSR，应根据特定任务的视觉反馈需求来调整SR控制，从而改善各种交互式图形应用程序中的人机界面。", "keywords": "系统响应性, 虚拟环境, 用户性能, 人机交互, 标准差", "comments": "该论文通过量化SDSR影响的阈值并强调任务特异性敏感度，为虚拟环境设计者提供了实用的见解。SDSR低于82毫秒影响较小的发现具有创新性，可能简化设计考虑。其在广泛的交互式应用中的适用性突显了其重要性。"}}
{"id": "2405.03572", "title": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous Driving Research", "authors": ["Mehdi Testouri", "Gamal Elghazaly", "Raphael Frank"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.03572v2", "summary": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.03572v2", "cate": "cs.RO", "date": "2024-05-06", "updated": "2025-07-24", "AI": {"title_translation": "RoboCar：一个用于自动驾驶研究的快速部署开源平台", "tldr": "RoboCar是一个快速部署的开源自动驾驶研究平台，集成了软硬件并已进行实路测试。", "motivation": "为了提供一个模块化、经济高效的开源平台，以促进实验性自动驾驶系统（ADS）的开发和研究。", "method": "本文介绍了RoboCar，一个基于2018款起亚Soul EV的开源自动驾驶研究平台。该平台集成了鲁棒的硬件和软件架构，与车辆现有系统对齐，最大限度地减少了大量修改的需求。", "result": "RoboCar平台支持各种自动驾驶功能，并在卢森堡市的公共道路上进行了实路测试。论文概述了平台的架构、集成挑战和初步测试结果。", "conclusion": "RoboCar是一个开放获取、模块化、经济高效的自动驾驶研究平台，已通过实路测试验证，可用于推进自动驾驶研究。", "translation": "本文介绍了RoboCar，一个由卢森堡大学开发的用于自动驾驶的开源研究平台。RoboCar提供了一个模块化、经济高效的实验性自动驾驶系统（ADS）开发框架，利用了2018款起亚Soul EV。该平台集成了鲁棒的硬件和软件架构，与车辆现有系统对齐，最大限度地减少了大量修改的需求。它支持各种自动驾驶功能，并在卢森堡市的公共道路上进行了实路测试。本文概述了该平台的架构、集成挑战和初步测试结果，为其在推进自动驾驶研究中的应用提供了见解。RoboCar可在https://github.com/sntubix/robocar获取，并以MIT开源许可发布。", "summary": "本文介绍了RoboCar，一个由卢森堡大学开发的开源自动驾驶研究平台。该平台利用2018款起亚Soul EV，提供了一个模块化、经济高效的自动驾驶系统开发框架。它集成了鲁棒的软硬件架构，与车辆现有系统兼容，并支持多种自动驾驶功能。RoboCar已在卢森堡市的公共道路上进行了实路测试。论文详细阐述了平台的架构、集成挑战和初步测试结果，旨在为自动驾驶研究提供深入见解。RoboCar已在GitHub上开源，并采用MIT许可。", "keywords": "自动驾驶, 开源平台, RoboCar, 研究平台, 车辆集成", "comments": "RoboCar作为一个开源、成本效益高且易于部署的平台，对于推动自动驾驶研究具有重要意义，尤其因为它减少了对现有车辆的广泛修改，降低了研究门槛。其在真实道路上的测试也增加了其实用性和可靠性。"}}
{"id": "2507.18417", "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18417v1", "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18417v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "FinDPO：通过LLM偏好优化进行算法交易的金融情感分析", "tldr": "FinDPO是一种基于直接偏好优化（DPO）的金融领域大语言模型（LLM）框架，用于金融情感分析，解决了传统监督微调（SFT）模型的泛化问题，并在情感分类基准测试和实际投资组合策略中取得了最先进的性能。", "motivation": "在线金融文本数据中的观点对交易决策和市场走势影响日益深刻，凸显了情感分析的重要性。然而，当前主流的监督微调（SFT）大语言模型（LLM）在金融情感分析中存在训练数据记忆化和对未见样本泛化能力差的局限性，这在需要适应新事件和领域特定语言的金融领域是关键限制。", "method": "本文提出了FinDPO，这是首个基于后训练人类偏好对齐（通过直接偏好优化DPO）的金融特定LLM框架。FinDPO通过一种新颖的“logit-to-score”转换方法，将离散情感预测转化为连续、可排序的情感分数（概率），从而将微调后的因果LLM整合到实际投资组合策略中。", "result": "FinDPO在标准情感分类基准测试中取得了最先进的性能，平均优于现有监督微调模型11%。在实际交易成本为5个基点（bps）的情况下，模拟结果显示FinDPO是首个能够保持每年67%的显著正回报和2.0的夏普比率的强大风险调整表现的基于情感的方法。", "conclusion": "FinDPO通过引入基于DPO的偏好优化，显著提升了金融情感分析的性能和泛化能力，并首次成功地将情感分析模型有效集成到实际的算法交易策略中，实现了持续的积极回报和优秀的风险调整表现。", "translation": "在线金融文本数据中表达的观点对交易决策和市场走势产生了日益深远的影响。这一趋势突显了情感分析作为量化此类观点性质和强度的工具的关键作用。随着生成式人工智能（GenAI）的快速发展，监督微调（SFT）的大语言模型（LLM）已成为金融情感分析的事实标准。然而，SFT范式可能导致训练数据的记忆化，并且通常无法泛化到未见样本。这在金融领域是一个关键的限制，因为模型必须适应以前未观察到的事件和金融领域细致入微的特定语言。为此，我们引入了FinDPO，这是第一个基于通过直接偏好优化（DPO）进行后训练人类偏好对齐的金融特定LLM框架。所提出的FinDPO在标准情感分类基准测试上取得了最先进的性能，平均优于现有监督微调模型11%。独特的是，FinDPO框架通过一种新颖的“logit-to-score”转换，将离散的情感预测转化为连续的、可排序的情感分数（概率），从而将微调后的因果LLM整合到实际的投资组合策略中。通过这种方式，模拟结果表明，即使在5个基点（bps）的实际交易成本下，FinDPO也是第一个能够保持每年67%的显著正回报和2.0的夏普比率的强大风险调整表现的基于情感的方法。", "summary": "FinDPO是一个创新的金融特定大语言模型框架，利用直接偏好优化（DPO）克服了传统监督微调（SFT）模型在金融情感分析中泛化能力差的局限性。它通过“logit-to-score”转换将情感预测整合到实际投资组合策略中。实验证明，FinDPO在情感分类基准上表现卓越，并能在模拟交易中实现显著的年化回报和高夏普比率，是首个成功应用于算法交易的情感分析方法。", "keywords": "金融情感分析, 大语言模型, 直接偏好优化, 算法交易, 投资组合策略", "comments": "FinDPO的创新点在于首次将直接偏好优化（DPO）应用于金融领域的大语言模型进行情感分析，有效解决了SFT模型在金融数据泛化性上的不足。其独特的“logit-to-score”转换机制，使得离散的情感预测能够无缝集成到连续的投资组合策略中，极大地提升了情感分析在实际算法交易中的应用价值。该研究的重要性在于为金融领域的情感分析提供了一个更鲁棒、更具适应性的解决方案，并证明了其在实现实际交易收益方面的潜力。"}}
{"id": "2507.15857", "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "authors": ["Mihir Prabhudesai", "Menging Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Project Webpage: this https URL", "url": "http://arxiv.org/abs/2507.15857v2", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "comment": "Project Webpage: https://diffusion-scaling.github.io", "pdf_url": "http://arxiv.org/pdf/2507.15857v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "扩散模型在数据受限设置下超越自回归模型", "tldr": "在计算资源充足但数据稀缺的场景下，扩散模型在语言任务上显著优于自回归模型，因为它们能更好地利用重复数据，并具有隐式数据增强的优势。", "motivation": "自回归模型长期主导大型语言模型领域，但扩散语言模型作为一种有前景的替代方案，其相对于自回归模型的优势尚未得到充分探索。本研究旨在系统地探究在数据受限设置下（即训练涉及对有限数据进行重复遍历）掩码扩散模型的表现。", "method": "本研究系统地考察了在数据受限设置下（即训练涉及对有限数据进行重复遍历）的掩码扩散模型，并将其与自回归模型进行比较。", "result": "研究发现，当计算资源充足但数据稀缺时，扩散模型显著优于自回归模型。扩散模型能更好地利用重复数据，从而实现更低的验证损失和更优的下游任务性能。这种优势被解释为隐式数据增强：掩码扩散模型将模型暴露于多样化的词元排序和预测任务中，这与自回归模型固定的从左到右分解方式不同。此外，研究还发现了扩散模型的新缩放定律，并推导出了扩散模型开始超越自回归模型的临界计算阈值的闭合表达式。", "conclusion": "当数据而非计算资源成为瓶颈时，扩散模型为标准的自回归范式提供了一个引人注目的替代方案。", "translation": "自回归（AR）模型长期以来一直主导着大型语言模型的领域，推动了广泛任务的进展。最近，基于扩散的语言模型作为一种有前景的替代方案出现，尽管它们相对于AR模型的优势仍未得到充分探索。在本文中，我们系统地研究了在数据受限设置下（即训练涉及对有限数据进行重复遍历）的掩码扩散模型，并发现当计算资源充足但数据稀缺时，它们显著优于AR模型。扩散模型能更好地利用重复数据，实现了更低的验证损失和更优的下游性能。我们将这种优势解释为隐式数据增强：与AR模型固定的从左到右分解方式不同，掩码扩散模型使模型暴露于多样化的词元排序和预测任务中。我们发现了扩散模型的新缩放定律，并推导出了扩散模型开始超越AR模型的临界计算阈值的闭合表达式。这些结果表明，当数据而非计算资源是瓶颈时，扩散模型为标准的AR范式提供了一个引人注目的替代方案。我们的代码可在以下网址获取：https://diffusion-scaling.github.io。", "summary": "本研究系统地探究了在数据受限设置下，掩码扩散模型与自回归（AR）模型的性能。结果表明，当计算资源充足而数据稀缺时，扩散模型在语言任务上显著优于AR模型。这得益于扩散模型能更好地利用重复数据，实现更低的验证损失和更优的下游性能，其原因在于掩码扩散引入了隐式数据增强，暴露模型于多样化的词元排序和预测任务。研究还提出了扩散模型的新缩放定律，并确定了其超越AR模型的临界计算阈值，表明扩散模型在数据受限场景下是AR模型的有力替代。", "keywords": "扩散模型, 自回归模型, 数据受限, 语言模型, 隐式数据增强", "comments": "这项研究的创新之处在于，它挑战了自回归模型在大型语言模型领域的主导地位，特别是在数据稀缺的特定场景下。通过系统性地证明扩散模型在有限数据下能更有效地学习并利用重复数据，其“隐式数据增强”的解释提供了新的视角。此外，推导出的新缩放定律和临界计算阈值，为理解扩散模型的适用边界和未来发展方向提供了重要的理论基础和实践指导。"}}
{"id": "2504.13101", "title": "Position: An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research", "authors": ["Patrik Reizinger", "Randall Balestriero", "David Klindt", "Wieland Brendel"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML2025 camera ready", "url": "http://arxiv.org/abs/2504.13101v3", "summary": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.", "comment": "ICML2025 camera ready", "pdf_url": "http://arxiv.org/pdf/2504.13101v3", "cate": "cs.LG", "date": "2025-04-17", "updated": "2025-07-24", "AI": {"title_translation": "立场：一个基于经验的同一性理论将加速自监督学习研究", "tldr": "本文提出，需要一个扩展的同一性理论（SITh）来解释自监督学习（SSL）的经验成功，并为未来研究提供方向。", "motivation": "尽管自监督学习（SSL）在许多AI系统中表现出色，但其背后的柏拉图式表征假设（PRH）缺乏精确的理论解释，且现有的同一性理论（IT）无法解释SSL的经验成功，这导致了理论与实践之间的鸿沟。", "method": "通过综合同一性理论（IT）的证据，本文表明柏拉图式表征假设（PRH）可以在SSL中出现。为弥合理论与实践之间的差距，本文提出将IT扩展为奇异同一性理论（SITh），这是一个涵盖整个SSL管道的更广泛的理论框架。", "result": "通过综合同一性理论（IT）的证据，本文表明柏拉图式表征假设（PRH）可以在自监督学习（SSL）中出现。提出的奇异同一性理论（SITh）将允许对SSL中的隐式数据假设有更深入的见解，并推动该领域向学习更具可解释性和泛化性的表征发展。", "conclusion": "本文认为，通过扩展同一性理论（IT）为奇异同一性理论（SITh），可以弥合自监督学习（SSL）理论与实践之间的差距，从而更好地理解和发展可解释、可泛化的表征。未来研究应关注SSL的训练动态、有限样本和数据多样性的影响以及归纳偏置的作用。", "translation": "自监督学习（SSL）为许多当前的AI系统提供支持。随着研究兴趣和投资的增长，SSL的设计空间持续扩展。遵循柏拉图式表征假设（PRH）的SSL柏拉图式观点表明，尽管方法和工程方法不同，所有表征都趋向于相同的柏拉图式理想。然而，这种现象缺乏精确的理论解释。通过综合同一性理论（IT）的证据，我们表明PRH可以在SSL中出现。然而，当前的IT无法解释SSL的经验成功。为了弥合理论与实践之间的差距，我们建议将IT扩展为我们称之为奇异同一性理论（SITh），这是一个涵盖整个SSL管道的更广泛的理论框架。SITh将允许更深入地了解SSL中的隐式数据假设，并推动该领域向学习更具可解释性和泛化性的表征发展。我们强调了未来研究的三个关键方向：1）SSL的训练动态和收敛特性；2）有限样本、批量大小和数据多样性的影响；3）架构、增强、初始化方案和优化器中归纳偏置的作用。", "summary": "本文针对自监督学习（SSL）中柏拉图式表征假设（PRH）缺乏理论解释以及现有同一性理论（IT）无法解释SSL经验成功的问题，提出了一种新的理论框架——奇异同一性理论（SITh）。SITh旨在弥合SSL理论与实践之间的鸿沟，通过更深入地理解隐式数据假设，从而促进学习更具可解释性和泛化性的表征。文章还指出了未来研究的三个关键方向。", "keywords": "自监督学习, 同一性理论, 柏拉图式表征假设, 奇异同一性理论, 表征学习", "comments": "本文的创新之处在于提出了奇异同一性理论（SITh），旨在为自监督学习（SSL）提供一个更全面的理论基础，以解释其经验成功并指导未来的研究。这对于提升SSL的可解释性和泛化能力至关重要，因为它强调了理论与实践相结合的重要性，并为SSL研究提供了明确的方向。"}}
{"id": "2507.18555", "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights", "authors": ["Jun'ichi Takeuchia", "Yoshinari Takeishia", "Noboru Muratab", "Kazushi Mimurac", "Ka Long Keith Hod", "Hiroshi Nagaoka"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18555v1", "summary": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU\nnetworks with random hidden weight are argued. We discuss the relation between\nboth notions as a linear transformation and show that spectral decomposition of\nNTK with concrete forms of eigenfunctions with major eigenvalues. We also\nobtain an approximation formula of the functions presented by the 2-layer\nneural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18555v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "随机隐藏权重的简单ReLU网络的神经网络切线核与费雪信息矩阵", "tldr": "该研究探讨了随机隐藏权重的两层ReLU网络的费雪信息矩阵和神经网络切线核，并分析了它们之间的关系及NTK的谱分解。", "motivation": "Not mentioned in abstract", "method": "本文讨论了具有随机隐藏权重的两层ReLU网络的费雪信息矩阵和神经网络切线核（NTK），探讨了两者之间的线性变换关系，并展示了NTK的谱分解。", "result": "结果表明，费雪信息矩阵和NTK之间存在线性变换关系。获得了NTK的谱分解，其中包含具有主要特征值的具体特征函数形式。此外，还获得了两层神经网络所表示函数的近似公式。", "conclusion": "该研究阐明了随机隐藏权重的两层ReLU网络的费雪信息矩阵和神经网络切线核之间的关系，并提供了NTK的谱分解及网络函数近似公式。", "translation": "本文讨论了具有随机隐藏权重的两层ReLU网络的费雪信息矩阵和神经网络切线核（NTK）。我们探讨了这两个概念之间的线性变换关系，并展示了NTK的谱分解，其中包含具有主要特征值的具体特征函数形式。我们还获得了两层神经网络所表示函数的近似公式。", "summary": "本文深入探讨了随机隐藏权重的两层ReLU网络的费雪信息矩阵（FIM）与神经网络切线核（NTK）。研究揭示了FIM与NTK之间存在线性变换关系，并详细展示了NTK的谱分解，包括具有主要特征值的特征函数具体形式。此外，论文还提出了两层神经网络所表示函数的一个近似公式。", "keywords": "神经网络切线核, 费雪信息矩阵, ReLU网络, 随机隐藏权重, 谱分解", "comments": "该论文在理论层面探讨了深度学习模型的核心组成部分（ReLU网络）的数学性质，特别是其与优化和泛化相关的费雪信息矩阵和神经网络切线核。揭示这些理论联系有助于理解神经网络的学习动态。"}}
{"id": "2507.18036", "title": "NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN", "authors": ["Haonan An", "Guang Hua", "Yu Guo", "Hangcheng Cao", "Susanto Rahardja", "Yuguang Fang"], "categories": ["cs.CR", "cs.CV"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18036v1", "summary": "The intellectual property of deep neural network (DNN) models can be\nprotected with DNN watermarking, which embeds copyright watermarks into model\nparameters (white-box), model behavior (black-box), or model outputs\n(box-free), and the watermarks can be subsequently extracted to verify model\nownership or detect model theft. Despite recent advances, these existing\nmethods are inherently intrusive, as they either modify the model parameters or\nalter the structure. This natural intrusiveness raises concerns about\nwatermarking-induced shifts in model behavior and the additional cost of\nfine-tuning, further exacerbated by the rapidly growing model size. As a\nresult, model owners are often reluctant to adopt DNN watermarking in practice,\nwhich limits the development of practical Watermarking as a Service (WaaS)\nsystems. To address this issue, we introduce Nonintrusive Watermarking as a\nService (NWaaS), a novel trustless paradigm designed for X-to-Image models, in\nwhich we hypothesize that with the model untouched, an owner-defined watermark\ncan still be extracted from model outputs. Building on this concept, we propose\nShadowMark, a concrete implementation of NWaaS which addresses critical\ndeployment challenges by establishing a robust and nonintrusive side channel in\nthe protected model's black-box API, leveraging a key encoder and a watermark\ndecoder. It is significantly distinctive from existing solutions by attaining\nthe so-called absolute fidelity and being applicable to different DNN\narchitectures, while being also robust against existing attacks, eliminating\nthe fidelity-robustness trade-off. Extensive experiments on image-to-image,\nnoise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate\nthe efficacy and practicality of ShadowMark for real-world deployment of\nnonintrusive DNN watermarking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18036v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "NWaaS：面向X到图像深度神经网络的非侵入式水印即服务", "tldr": "现有深度神经网络水印方法具有侵入性；本文提出NWaaS，一种非侵入式水印即服务范式，并通过ShadowMark实现，适用于X到图像模型，实现绝对保真度和鲁棒性。", "motivation": "现有的深度神经网络水印方法具有侵入性，导致模型行为偏移、微调成本增加以及模型所有者不愿采用，从而限制了水印即服务(WaaS)系统的实际发展。", "method": "引入了非侵入式水印即服务(NWaaS)，这是一种为X到图像模型设计的无信任范式，其假设在模型未被触及的情况下，可以从模型输出中提取所有者定义的水印。提出ShadowMark，一个具体的NWaaS实现，通过在受保护模型的黑盒API中建立一个鲁棒且非侵入式的旁道，利用密钥编码器和水印解码器来解决部署挑战。", "result": "在图像到图像、噪声到图像、噪声与文本到图像以及文本到图像模型上的大量实验证明了ShadowMark在非侵入式深度神经网络水印实际部署中的有效性和实用性。它实现了所谓的绝对保真度，适用于不同的深度神经网络架构，并且对现有攻击具有鲁棒性，消除了保真度-鲁棒性权衡。", "conclusion": "本文成功引入了NWaaS及其实现ShadowMark，展示了一种针对X到图像模型的非侵入式深度神经网络水印的实用有效解决方案，该方案克服了侵入式方法的局限性，实现了保真度和鲁棒性。", "translation": "深度神经网络（DNN）模型的知识产权可以通过DNN水印技术进行保护，该技术将版权水印嵌入模型参数（白盒）、模型行为（黑盒）或模型输出（无盒）中，随后可以提取水印以验证模型所有权或检测模型盗窃。尽管最近有所进展，但这些现有方法本质上是侵入性的，因为它们要么修改模型参数，要么改变模型结构。这种固有的侵入性引发了对水印引起模型行为偏移和额外微调成本的担忧，随着模型规模的快速增长，这种担忧进一步加剧。因此，模型所有者在实践中往往不愿采用DNN水印技术，这限制了实用水印即服务（WaaS）系统的发展。为了解决这个问题，我们引入了非侵入式水印即服务（NWaaS），这是一种为X到图像模型设计的新型无信任范式，我们假设在模型未被触及的情况下，仍可以从模型输出中提取所有者定义的水印。基于这一概念，我们提出了ShadowMark，这是NWaaS的一个具体实现，它通过在受保护模型的黑盒API中建立一个鲁棒且非侵入式的旁道，利用密钥编码器和水印解码器来解决关键的部署挑战。它通过实现所谓的绝对保真度并适用于不同的DNN架构，同时对现有攻击具有鲁棒性，从而消除了保真度-鲁棒性权衡，这使其与现有解决方案显著不同。在图像到图像、噪声到图像、噪声与文本到图像以及文本到图像模型上的大量实验证明了ShadowMark在非侵入式DNN水印实际部署中的有效性和实用性。", "summary": "本文旨在解决现有深度神经网络（DNN）水印方法侵入性带来的问题，即修改模型参数或结构导致行为偏移和成本增加。为此，本文提出了一种新颖的、无信任的非侵入式水印即服务（NWaaS）范式，专为X到图像模型设计，其核心思想是在不触及模型本身的情况下，仍能从模型输出中提取水印。文中介绍了ShadowMark，作为NWaaS的具体实现，它通过在受保护模型的黑盒API中建立一个鲁棒且非侵入式的旁道，并利用密钥编码器和水印解码器来实现。实验结果表明，ShadowMark在图像到图像、噪声到图像、噪声与文本到图像以及文本到图像模型上均表现出高效和实用性，实现了绝对保真度，适用于不同DNN架构，并对现有攻击具有鲁棒性，有效消除了保真度-鲁棒性之间的权衡。", "keywords": "深度神经网络水印, 非侵入式, 水印即服务, 知识产权, X到图像", "comments": "这篇论文通过提出非侵入式深度神经网络水印范式（NWaaS）及其具体实现（ShadowMark），引入了一项重要的创新。这解决了阻碍深度神经网络水印在实际水印即服务（WaaS）系统中应用的关键实际障碍（侵入性）。其声称实现“绝对保真度”并消除“保真度-鲁棒性权衡”尤其值得关注，因为这在水印领域是一个长期存在的挑战。这项工作可能会极大地推动深度神经网络模型知识产权保护的实际部署。"}}
{"id": "2507.18396", "title": "Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input", "authors": ["Yonghao Fu", "Cheng Hu", "Haokun Xiong", "Zhangpeng Bao", "Wenyuan Du", "Edoardo Ghignone", "Michele Magno", "Lei Xie", "Hongye Su"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18396v1", "summary": "In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18396v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "用于增强车辆动力学且仅需少量在轨数据输入的残差Koopman模型预测控制", "tldr": "本文提出了一种残差Koopman模型预测控制（RKMPC）框架，用于车辆轨迹跟踪任务。该方法结合了线性模型预测控制（LMPC）和基于神经网络的补偿输入，在仅需少量训练数据的情况下，实现了优于传统方法的跟踪性能和稳定性。", "motivation": "在车辆轨迹跟踪任务中，简单的Pure Pursuit控制未能考虑车辆模型约束，影响了驾驶安全。模型预测控制（MPC）虽然能优化控制动作，但其性能严重依赖于车辆建模的准确性。传统车辆建模方法在捕捉非线性动力学和保持计算效率之间存在固有的权衡，常常导致控制性能下降。", "method": "本文提出了残差Koopman模型预测控制（RKMPC）框架。该方法采用双线性MPC架构来计算控制输入：一个线性模型预测控制（LMPC）基于车辆运动学模型计算基线控制输入，一个基于神经网络的RKMPC计算补偿输入。最终的控制指令通过将这两个分量相加得到。这种设计保留了传统机械模型的可靠性和可解释性，同时通过残差建模实现了性能优化。", "result": "该方法已在Carsim-Matlab联合仿真平台和一辆1:10比例的F1TENTH赛车上进行了验证。实验结果表明，RKMPC仅需要传统Koopman模型预测控制（KMPC）所需训练数据的20%，同时提供了卓越的跟踪性能。与传统LMPC相比，RKMPC将横向误差降低了11.7%-22.1%，将航向误差降低了8.9%-15.8%，并将前轮转向稳定性提高了高达27.6%。", "conclusion": "残差Koopman模型预测控制（RKMPC）通过结合传统机械模型和数据驱动的残差学习，有效解决了车辆轨迹跟踪中的挑战，实现了在减少数据需求的同时，显著提升了控制性能和稳定性。", "translation": "在车辆轨迹跟踪任务中，最简单的方法是纯跟踪（Pure Pursuit，PP）控制。然而，这种单点预瞄跟踪策略未能考虑车辆模型约束，从而影响了驾驶安全。模型预测控制（MPC）作为一种广泛采用的控制方法，通过纳入力学模型和物理约束来优化控制动作。然而，其控制性能关键取决于车辆建模的准确性。传统车辆建模方法在捕捉非线性动力学和保持计算效率之间存在固有的权衡，常常导致控制性能下降。为了解决这些挑战，本文提出了一种残差Koopman模型预测控制（RKMPC）框架。该方法使用双线性MPC架构来计算控制输入：一个线性模型预测控制（LMPC）基于车辆运动学模型计算基线控制输入，一个基于神经网络的RKMPC计算补偿输入。最终的控制指令通过将这两个分量相加得到。这种设计保留了传统力学模型的可靠性和可解释性，同时通过残差建模实现了性能优化。该方法已在Carsim-Matlab联合仿真平台和一辆1:10比例的F1TENTH赛车上进行了验证。实验结果表明，RKMPC仅需要传统Koopman模型预测控制（KMPC）所需训练数据的20%，同时提供了卓越的跟踪性能。与传统LMPC相比，RKMPC将横向误差降低了11.7%-22.1%，将航向误差降低了8.9%-15.8%，并将前轮转向稳定性提高了高达27.6%。实施代码可在：https://github.com/ZJU-DDRX/Residual Koopman 获取。", "summary": "本文针对车辆轨迹跟踪中传统MPC方法在非线性建模与计算效率之间的权衡问题，提出了一种残差Koopman模型预测控制（RKMPC）框架。该框架结合了基于车辆运动学模型的线性MPC和基于神经网络的残差补偿，以生成最终的控制指令。这种混合方法不仅保留了传统模型的可靠性和可解释性，而且通过残差学习显著提升了性能。实验结果表明，RKMPC在仅需少量训练数据（KMPC的20%）的情况下，能有效降低横向和航向误差，并提高转向稳定性，展现出优越的跟踪性能。", "keywords": "残差Koopman, 模型预测控制, 车辆动力学, 轨迹跟踪, 数据效率", "comments": "该论文的创新点在于其提出的RKMPC框架，巧妙地结合了传统机械模型和数据驱动的残差学习。这种混合方法不仅解决了传统MPC在处理车辆非线性动力学时面临的建模精度与计算效率的权衡问题，而且通过引入残差补偿机制，显著提升了控制性能，并降低了对大规模训练数据的依赖。其保留了机械模型的可解释性，同时利用神经网络的优势进行性能优化，是未来智能驾驶控制领域的一个重要方向。"}}
{"id": "2507.17907", "title": "Deep learning-aided inverse design of porous metamaterials", "authors": ["Phu Thien Nguyen", "Yousef Heider", "Dennis M. Kochmann", "Fadi Aldakheel"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 29 figures", "url": "http://arxiv.org/abs/2507.17907v1", "summary": "The ultimate aim of the study is to explore the inverse design of porous\nmetamaterials using a deep learning-based generative framework. Specifically,\nwe develop a property-variational autoencoder (pVAE), a variational autoencoder\n(VAE) augmented with a regressor, to generate structured metamaterials with\ntailored hydraulic properties, such as porosity and permeability. While this\nwork uses the lattice Boltzmann method (LBM) to generate intrinsic permeability\ntensor data for limited porous microstructures, a convolutional neural network\n(CNN) is trained using a bottom-up approach to predict effective hydraulic\nproperties. This significantly reduces the computational cost compared to\ndirect LBM simulations. The pVAE framework is trained on two datasets: a\nsynthetic dataset of artificial porous microstructures and CT-scan images of\nvolume elements from real open-cell foams. The encoder-decoder architecture of\nthe VAE captures key microstructural features, mapping them into a compact and\ninterpretable latent space for efficient structure-property exploration. The\nstudy provides a detailed analysis and interpretation of the latent space,\ndemonstrating its role in structure-property mapping, interpolation, and\ninverse design. This approach facilitates the generation of new metamaterials\nwith desired properties. The datasets and codes used in this study will be made\nopen-access to support further research.", "comment": "31 pages, 29 figures", "pdf_url": "http://arxiv.org/pdf/2507.17907v1", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "深度学习辅助的多孔超材料逆向设计", "tldr": "本研究开发了一个基于深度学习的生成框架（pVAE），用于多孔超材料的逆向设计，能够生成具有特定水力特性的材料，并显著降低计算成本。", "motivation": "研究的最终目标是探索使用基于深度学习的生成框架进行多孔超材料的逆向设计，以生成具有定制水力特性（如孔隙率和渗透率）的结构化超材料。", "method": "本研究开发了一种属性变分自编码器（pVAE），它是一个增强了回归器的变分自编码器（VAE）。研究使用格子玻尔兹曼方法（LBM）生成有限多孔微结构的固有渗透张量数据，并采用自下而上的方法训练卷积神经网络（CNN）来预测有效水力特性。pVAE框架在两个数据集上进行训练：一个是人工多孔微结构的合成数据集，另一个是真实开孔泡沫体素的CT扫描图像。", "result": "与直接LBM模拟相比，该方法显著降低了计算成本。VAE的编码器-解码器架构捕获了关键的微结构特征，并将其映射到一个紧凑且可解释的潜在空间，以实现高效的结构-属性探索。研究提供了对潜在空间的详细分析和解释，展示了其在结构-属性映射、插值和逆向设计中的作用。", "conclusion": "这种方法有助于生成具有所需特性的新型超材料。本研究中使用的数据集和代码将开放获取，以支持进一步的研究。", "translation": "本研究的最终目的是探索使用基于深度学习的生成框架进行多孔超材料的逆向设计。具体而言，我们开发了一种属性变分自编码器（pVAE），这是一种增强了回归器的变分自编码器（VAE），用于生成具有定制水力特性（如孔隙率和渗透率）的结构化超材料。虽然这项工作使用格子玻尔兹曼方法（LBM）生成有限多孔微结构的固有渗透张量数据，但我们采用自下而上的方法训练卷积神经网络（CNN）来预测有效水力特性。与直接LBM模拟相比，这显著降低了计算成本。pVAE框架在两个数据集上进行训练：一个是人工多孔微结构的合成数据集，另一个是真实开孔泡沫体素的CT扫描图像。VAE的编码器-解码器架构捕获了关键的微结构特征，并将其映射到一个紧凑且可解释的潜在空间，以实现高效的结构-属性探索。本研究提供了对潜在空间的详细分析和解释，展示了其在结构-属性映射、插值和逆向设计中的作用。这种方法有助于生成具有所需特性的新型超材料。本研究中使用的数据集和代码将开放获取，以支持进一步的研究。", "summary": "本研究提出了一种基于深度学习的生成框架pVAE，用于多孔超材料的逆向设计。通过结合变分自编码器（VAE）和回归器，pVAE能够生成具有特定水力特性（如孔隙率和渗透率）的材料。研究利用CNN预测水力特性以降低计算成本，并使用合成数据和真实CT扫描数据训练pVAE。该方法通过潜在空间分析实现了高效的结构-属性映射和新型超材料的生成，为材料设计提供了新的途径。", "keywords": "深度学习, 逆向设计, 多孔超材料, 变分自编码器, 水力特性", "comments": "这项研究的创新之处在于将深度学习，特别是变分自编码器（VAE）与回归器结合，用于多孔超材料的逆向设计，实现了从期望属性到材料结构的直接生成。其重要性在于显著降低了传统模拟方法（如LBM）的计算成本，并展示了潜在空间在结构-属性探索中的强大能力。此外，数据集和代码的开放获取将极大地促进该领域未来的研究。"}}
{"id": "2507.18122", "title": "Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning", "authors": ["Matthias Otth", "Jonas Hübotter", "Ido Hakimi", "Andreas Krause"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18122v1", "summary": "Recent work has shown that language models can self-improve by maximizing\ntheir own confidence in their predictions, without relying on external\nverifiers or reward signals. In this work, we study the test-time scaling of\nlanguage models for mathematical reasoning tasks, where the model's own\nconfidence is used to select the most promising attempts. Surprisingly, we find\nthat we can achieve significant performance gains by continuing only the most\npromising attempt, selected by the model's prefix-confidence. We systematically\nevaluate prefix-confidence scaling on five mathematical reasoning datasets: the\nschool-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and\nAIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens\nachieves a better accuracy-compute trade-off than majority voting. Moreover,\nprefix-confidence scaling appears less susceptible than BoN to length biases.\nFinally, we also evaluate test-time training with prefix-confidence and find\nthat, while outperforming the base model, it does not improve over\nprefix-confidence scaling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18122v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "在测试时最大化前缀置信度可有效提高数学推理能力", "tldr": "通过最大化语言模型的前缀置信度，可以在数学推理任务中显著提高性能，并且比多数投票更有效。", "motivation": "现有研究表明语言模型可以通过最大化自身预测置信度进行自我改进。本文旨在研究语言模型在数学推理任务中测试时扩展性问题，利用模型自身置信度来选择最有希望的尝试。", "method": "研究通过选择模型前缀置信度最高的尝试进行继续，并在GSM8K、MATH500、AMC23、AIME24和AIME25这五个数学推理数据集上系统评估了前缀置信度扩展。此外，还评估了使用前缀置信度的测试时训练。", "result": "通过仅继续最有希望的尝试，取得了显著的性能提升。使用32个token前缀的前缀置信度扩展比多数投票实现了更好的准确性-计算权衡。前缀置信度扩展似乎比BoN更不容易受到长度偏差的影响。测试时训练虽然优于基础模型，但并未比前缀置信度扩展有所改进。", "conclusion": "在数学推理任务中，通过最大化前缀置信度进行测试时扩展是一种有效且高效的方法，能够显著提高语言模型的性能，并且在准确性-计算权衡上优于多数投票。", "translation": "最近的研究表明，语言模型可以通过最大化自身预测的置信度来实现自我提升，而无需依赖外部验证器或奖励信号。在这项工作中，我们研究了语言模型在数学推理任务中的测试时扩展性，其中模型的自身置信度用于选择最有希望的尝试。令人惊讶的是，我们发现通过仅继续由模型前缀置信度选择的最有希望的尝试，可以实现显著的性能提升。我们系统地评估了前缀置信度在五个数学推理数据集上的扩展性：学校级别的GSM8K和MATH500，以及竞赛级别的AMC23、AIME24和AIME25。我们发现，仅使用32个token前缀的前缀置信度扩展比多数投票实现了更好的准确性-计算权衡。此外，前缀置信度扩展似乎比BoN更不容易受到长度偏差的影响。最后，我们还评估了使用前缀置信度的测试时训练，发现虽然它优于基础模型，但并未比前缀置信度扩展有所改进。", "summary": "本研究探讨了在数学推理任务中，通过最大化语言模型自身前缀置信度来提高性能的方法。实验发现，仅继续模型前缀置信度最高的尝试，就能显著提升表现。在五个数学推理数据集上的评估表明，前缀置信度扩展（即使仅使用32个token的前缀）在准确性-计算权衡方面优于多数投票，并且对长度偏差的敏感度较低。测试时训练虽有提升，但不如直接的前缀置信度扩展。", "keywords": "语言模型, 数学推理, 前缀置信度, 自我改进, 测试时扩展", "comments": "这项工作提出了一种新颖且高效的语言模型自我改进策略，即在测试时利用模型自身的前缀置信度来筛选和继续最有希望的推理路径。其创新点在于无需外部监督或奖励信号，仅依靠模型内部置信度即可实现性能提升，并展现出优于传统多数投票方法的潜力，特别是在计算效率和抗长度偏差方面。这对于资源受限或需要快速响应的场景具有重要意义。"}}
{"id": "2507.18030", "title": "Sparse optimal control for infinite-dimensional linear systems with applications to graphon control", "authors": ["Takuya Ikeda", "Masaaki Nagahara"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.18030v1", "summary": "Large-scale networked systems typically operate under resource constraints,\nand it is also difficult to exactly obtain the network structure between nodes.\nTo address these issues, this paper investigates a sparse optimal control for\ninfinite-dimensional linear systems and its application to networked systems\nwhere the network structure is represented by a limit function called a graphon\nthat captures the overall connection pattern. The contributions of this paper\nare twofold: (i) To reduce computational complexity, we derive a sufficient\ncondition under which the sparse optimal control can be obtained by solving its\ncorresponding L1 optimization problem. Furthermore, we introduce a class of\nnon-convex optimal control problems such that the optimal solution always\ncoincides with a sparse optimal control, provided that the non-convex problems\nadmit optimal solutions. (ii) We show that the sparse optimal control for\nlarge-scale finite-dimensional networked systems can be approximated by that of\nthe corresponding limit graphon system, provided that the underlying graph is\nclose to the limit graphon in the cut-norm topology. The effectiveness of the\nproposed approach is illustrated through numerical examples.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.18030v1", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "无限维线性系统的稀疏最优控制及其在图论控制中的应用", "tldr": "本文研究了无限维线性系统的稀疏最优控制及其在图论控制中的应用，旨在解决大规模网络系统在资源受限和网络结构未知情况下的控制问题。通过推导L1优化条件和利用图论近似，降低了计算复杂性并提供了大规模系统控制的有效方法。", "motivation": "大规模网络系统通常在资源受限下运行，且节点间的网络结构难以精确获取。", "method": "本文通过以下方法解决问题：(i) 推导了稀疏最优控制可以通过求解其对应的L1优化问题获得的充分条件，以降低计算复杂度。此外，引入了一类非凸最优控制问题，证明在存在最优解的情况下，其最优解与稀疏最优控制一致。(ii) 证明了大规模有限维网络系统的稀疏最优控制可以通过相应的极限图论系统进行近似，前提是底层图在切范数拓扑中接近极限图论。", "result": "主要结果包括：(i) 得到了稀疏最优控制可由L1优化问题获得的充分条件，有效降低了计算复杂度。(ii) 证明了一类非凸最优控制问题的最优解与稀疏最优控制一致。(iii) 证明了大规模有限维网络系统的稀疏最优控制可以通过极限图论系统近似。(iv) 通过数值例子验证了所提方法的有效性。", "conclusion": "本文提出了无限维线性系统的稀疏最优控制方法，并将其应用于图论系统，有效解决了大规模网络系统的资源限制和网络结构未知问题，并通过理论分析和数值示例验证了其有效性。", "translation": "大规模网络系统通常在资源受限下运行，并且节点间的网络结构也难以精确获取。为了解决这些问题，本文研究了无限维线性系统的稀疏最优控制及其在网络系统中的应用，其中网络结构由一个称为图论（graphon）的极限函数表示，该函数捕捉了整体连接模式。本文的贡献是双重的：(i) 为了降低计算复杂度，我们推导了一个充分条件，在此条件下稀疏最优控制可以通过求解其对应的L1优化问题获得。此外，我们引入了一类非凸最优控制问题，只要这些非凸问题存在最优解，其最优解总是与稀疏最优控制一致。(ii) 我们证明了大规模有限维网络系统的稀疏最优控制可以由相应的极限图论系统进行近似，前提是底层图在切范数拓扑中接近极限图论。所提出方法的有效性通过数值例子进行了说明。", "summary": "本文研究了无限维线性系统的稀疏最优控制及其在图论控制中的应用，旨在解决大规模网络系统面临的资源限制和网络结构难以精确获取的问题。文章的主要贡献包括：推导了稀疏最优控制可通过L1优化问题获得的充分条件以降低计算复杂度；引入了一类非凸最优控制问题并证明其最优解与稀疏最优控制一致；以及证明了大规模有限维网络系统的稀疏最优控制可以通过相应的极限图论系统进行近似。数值例子验证了所提方法的有效性。", "keywords": "稀疏最优控制, 无限维系统, 图论控制, L1优化, 大规模网络系统", "comments": "这篇论文在解决大规模网络系统控制中的计算复杂性和网络结构不确定性方面具有创新性。通过引入稀疏最优控制和图论（graphon）的概念，并结合L1优化和非凸问题分析，为处理资源受限且结构模糊的复杂系统提供了新的理论和方法。特别是将有限维系统近似为无限维图论系统，为分析和控制超大规模网络提供了有效工具。"}}
{"id": "2507.18522", "title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians", "authors": ["Tomislav Pavković", "Mohammad-Ali Nikouei Mahani", "Johannes Niedermayer", "Johannes Betz"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18522v1", "summary": "3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18522v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "GaussianFusionOcc: 一种使用3D高斯进行3D占用预测的无缝传感器融合方法", "tldr": "GaussianFusionOcc利用3D高斯和创新的传感器融合机制，实现高效且精确的3D语义占用预测，超越现有SOTA模型。", "motivation": "3D语义占用预测是自动驾驶的关键任务，能实现复杂环境中的精确安全解释和导航。可靠的预测依赖于有效的传感器融合，因为不同模态包含互补信息。", "method": "GaussianFusionOcc采用语义3D高斯表示和创新的传感器融合机制。它无缝集成来自摄像头、激光雷达和雷达传感器的数据。该方法利用模态无关的可变形注意力从每种传感器类型中提取基本特征，然后用于优化高斯属性，以实现更准确的环境表示。", "result": "GaussianFusionOcc在各种传感器组合下表现出多功能性，并且通过利用多模态融合的鲁棒性和高斯表示的效率，超越了当前的SOTA模型。", "conclusion": "通过结合多模态传感器融合的鲁棒性和3D高斯表示的效率，GaussianFusionOcc能够实现更精确、更高效的3D占用预测，并优于现有最先进的模型。", "translation": "3D语义占用预测是自动驾驶的关键任务之一。它能够在复杂环境中实现精确和安全的解释与导航。可靠的预测依赖于有效的传感器融合，因为不同模态可以包含互补信息。与依赖密集网格表示的传统方法不同，我们的方法GaussianFusionOcc使用语义3D高斯以及创新的传感器融合机制。无缝集成来自摄像头、激光雷达和雷达传感器的数据，能够实现更精确和可扩展的占用预测，同时3D高斯表示显著提高了内存效率和推理速度。GaussianFusionOcc采用模态无关的可变形注意力，从每种传感器类型中提取基本特征，然后用于优化高斯属性，从而更准确地表示环境。对各种传感器组合进行的广泛测试证明了我们方法的通用性。通过利用多模态融合的鲁棒性和高斯表示的效率，GaussianFusionOcc超越了当前的最新模型。", "summary": "GaussianFusionOcc是一种用于3D语义占用预测的新方法，它摒弃了传统的密集网格表示，转而采用语义3D高斯和创新的传感器融合机制。该方法无缝整合摄像头、激光雷达和雷达数据，利用模态无关的可变形注意力提取特征并优化高斯属性，从而实现更精确、内存高效且推理速度快的环境表示。实验证明，其性能优于现有SOTA模型。", "keywords": "3D占用预测, 传感器融合, 3D高斯, 自动驾驶, 可变形注意力", "comments": "该论文的创新点在于将3D高斯表示引入3D占用预测任务，结合了高效的内存使用和快速推理速度。其提出的模态无关可变形注意力机制也有效地融合了多种传感器数据。这种方法为自动驾驶中的环境感知提供了新的思路，具有重要的实用价值和研究潜力。"}}
{"id": "2502.02452", "title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models", "authors": ["Soroush Seifi", "Vaggelis Dorovatas", "Daniel Olmeda Reino", "Rahaf Aljundi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02452v3", "summary": "Personalization of Large Vision-Language Models (LVLMs) involves customizing\nmodels to recognize specific users and object instances, and to generate\ncontextually tailored responses. Existing approaches typically rely on\ntime-consuming test-time training for each user or object, making them\nimpractical for real-world deployment, a limitation reflected in current\npersonalization benchmarks, which are focused on object-centric, single-concept\nevaluations. In this paper, we present a novel training-free approach to LVLM\npersonalization and introduce a comprehensive real-world benchmark designed to\nrigorously evaluate various aspects of the personalization task. Our method\nleverages pre-trained vision foundation models to extract distinctive features,\napplies retrieval-augmented generation (RAG) techniques to identify instances\nwithin visual inputs, and employs visual prompting strategies to guide model\noutputs. Our model-agnostic vision toolkit enables efficient and flexible\nmulti-concept personalization across both images and videos, without any\nadditional training. We achieve state-of-the-art results, surpassing existing\ntraining-based methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02452v3", "cate": "cs.CV", "date": "2025-02-04", "updated": "2025-07-24", "AI": {"title_translation": "个性化工具包：大型视觉语言模型的免训练个性化", "tldr": "本文提出了一种无需训练的大型视觉语言模型（LVLM）个性化方法，通过利用预训练模型、RAG和视觉提示，实现了高效灵活的多概念个性化，并超越了现有训练方法。", "motivation": "现有的大型视觉语言模型（LVLM）个性化方法依赖耗时的测试时训练，不适用于实际部署。当前的个性化基准也过于局限，仅关注以对象为中心、单概念的评估。", "method": "本文提出了一种新颖的免训练LVLM个性化方法。该方法利用预训练的视觉基础模型提取独特特征，应用检索增强生成（RAG）技术识别视觉输入中的实例，并采用视觉提示策略指导模型输出。该方法是模型无关的，能够高效灵活地在图像和视频中进行多概念个性化，无需任何额外训练。", "result": "该方法取得了最先进的结果，超越了现有的基于训练的方法。", "conclusion": "本文提出了一种创新的免训练LVLM个性化方法，通过集成现有技术实现了高效且高性能的个性化，解决了传统训练方法的局限性，并为实际部署提供了可行方案。", "translation": "大型视觉语言模型（LVLM）的个性化涉及定制模型以识别特定用户和对象实例，并生成符合上下文的响应。现有方法通常依赖于针对每个用户或对象进行耗时的测试时训练，这使得它们在实际部署中不切实际，这一局限性也反映在当前的个性化基准中，这些基准侧重于以对象为中心、单概念的评估。在本文中，我们提出了一种新颖的LVLM个性化免训练方法，并引入了一个全面的真实世界基准，旨在严格评估个性化任务的各个方面。我们的方法利用预训练的视觉基础模型来提取独特特征，应用检索增强生成（RAG）技术来识别视觉输入中的实例，并采用视觉提示策略来指导模型输出。我们的模型无关视觉工具包能够在图像和视频中实现高效灵活的多概念个性化，无需任何额外训练。我们取得了最先进的结果，超越了现有的基于训练的方法。", "summary": "本文提出了一种名为“个性化工具包”的创新性免训练方法，用于大型视觉语言模型（LVLM）的个性化。该方法解决了现有训练方法耗时且不适用于实际部署的问题，并通过利用预训练视觉模型提取特征、应用检索增强生成（RAG）识别实例以及采用视觉提示策略来指导模型输出。该工具包实现了高效灵活的多概念个性化，无需额外训练，并在图像和视频上均表现出色，超越了现有基于训练的方法，同时还引入了一个全面的真实世界评估基准。", "keywords": "大型视觉语言模型, 个性化, 免训练, 检索增强生成, 视觉提示", "comments": "这篇论文的创新点在于提出了一个无需训练的LVLM个性化框架，显著降低了部署成本和时间。其结合预训练模型、RAG和视觉提示的策略非常巧妙，体现了对现有技术的高效集成。该方法在多概念和跨模态（图像/视频）个性化方面的灵活性以及超越现有训练方法的性能，使其在实际应用中具有重要潜力。同时，引入新的真实世界基准也对该领域的发展具有推动作用。"}}
{"id": "2507.18053", "title": "RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models", "authors": ["Haoran Gao", "Yuanhe Zhang", "Zhenhong Zhou", "Lei Jiang", "Fanyu Meng", "Yujia Xiao", "Kun Wang", "Yang Liu", "Junlan Feng"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18053v1", "summary": "Resource Consumption Attacks (RCAs) have emerged as a significant threat to\nthe deployment of Large Language Models (LLMs). With the integration of vision\nmodalities, additional attack vectors exacerbate the risk of RCAs in large\nvision-language models (LVLMs). However, existing red-teaming studies have\nlargely overlooked visual inputs as a potential attack surface, resulting in\ninsufficient mitigation strategies against RCAs in LVLMs. To address this gap,\nwe propose RECALLED (\\textbf{RE}source \\textbf{C}onsumption \\textbf{A}ttack on\n\\textbf{L}arge Vision-\\textbf{L}anguag\\textbf{E} Mo\\textbf{D}els), the first\napproach for exploiting visual modalities to trigger unbounded RCAs\nred-teaming. First, we present \\textit{Vision Guided Optimization}, a\nfine-grained pixel-level optimization, to obtain \\textit{Output Recall}\nadversarial perturbations, which can induce repeating output. Then, we inject\nthe perturbations into visual inputs, triggering unbounded generations to\nachieve the goal of RCAs. Additionally, we introduce \\textit{Multi-Objective\nParallel Losses} to generate universal attack templates and resolve\noptimization conflicts when intending to implement parallel attacks. Empirical\nresults demonstrate that RECALLED increases service response latency by over 26\n$\\uparrow$, resulting in an additional 20\\% increase in GPU utilization and\nmemory consumption. Our study exposes security vulnerabilities in LVLMs and\nestablishes a red-teaming framework that can facilitate future defense\ndevelopment against RCAs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18053v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "RECALLED：一种针对大型视觉-语言模型的无限制资源消耗攻击", "tldr": "该研究提出RECALLED，一种利用视觉输入对大型视觉-语言模型（LVLMs）进行无限制资源消耗攻击（RCA）的方法，通过引入重复输出扰动来增加延迟、GPU利用率和内存消耗。", "motivation": "现有的针对大型语言模型（LLMs）的资源消耗攻击（RCAs）研究忽视了视觉输入作为攻击面，导致大型视觉-语言模型（LVLMs）缺乏有效的缓解策略。", "method": "提出RECALLED，首次利用视觉模态触发无限制RCA。具体方法包括：1. 提出“视觉引导优化”（Vision Guided Optimization），一种细粒度的像素级优化，用于获取“输出召回”（Output Recall）对抗性扰动，诱导重复输出。2. 将扰动注入视觉输入，触发无限制生成以实现RCA。3. 引入“多目标并行损失”（Multi-Objective Parallel Losses）来生成通用攻击模板并解决并行攻击时的优化冲突。", "result": "RECALLED使服务响应延迟增加超过26%，GPU利用率和内存消耗额外增加20%。", "conclusion": "该研究揭示了LVLMs的安全漏洞，并建立了一个红队框架，有助于未来防御RCA的发展。", "translation": "资源消耗攻击（RCAs）已成为部署大型语言模型（LLMs）的一个重大威胁。随着视觉模态的整合，额外的攻击向量加剧了大型视觉-语言模型（LVLMs）中RCAs的风险。然而，现有的红队研究在很大程度上忽视了视觉输入作为潜在的攻击面，导致LVLMs中针对RCAs的缓解策略不足。为了解决这一空白，我们提出了RECALLED（大型视觉-语言模型的资源消耗攻击），这是第一个利用视觉模态触发无限制RCAs进行红队测试的方法。首先，我们提出了“视觉引导优化”，一种细粒度的像素级优化，以获得“输出召回”对抗性扰动，可以诱导重复输出。然后，我们将扰动注入视觉输入，触发无限制生成以实现RCAs的目标。此外，我们引入了“多目标并行损失”来生成通用攻击模板，并解决在尝试实施并行攻击时的优化冲突。实证结果表明，RECALLED使服务响应延迟增加了26%以上，导致GPU利用率和内存消耗额外增加了20%。我们的研究揭示了LVLMs中的安全漏洞，并建立了一个红队框架，可以促进未来针对RCAs的防御开发。", "summary": "本文提出了RECALLED，一个针对大型视觉-语言模型（LVLMs）的无限制资源消耗攻击（RCA）方法，首次利用视觉输入作为攻击面。通过“视觉引导优化”生成“输出召回”对抗性扰动，诱导模型重复生成，从而显著增加服务响应延迟、GPU利用率和内存消耗。该研究揭示了LVLMs的安全漏洞，并提供了一个红队框架以促进防御发展。", "keywords": "资源消耗攻击, 大型视觉-语言模型, 对抗性扰动, 红队测试, 安全漏洞", "comments": "RECALLED的创新之处在于首次将视觉输入作为大型视觉-语言模型（LVLMs）的资源消耗攻击面，填补了现有红队研究的空白。它通过精细的像素级优化和多目标损失函数，实现了高效且通用的攻击，对LVLMs的安全性提出了新的挑战，并为未来的防御研究奠定了基础。"}}
{"id": "2507.18178", "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory", "authors": ["Mutian Yang", "Jiandong Gao", "Ji Wu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18178v1", "summary": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18178v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "解耦大型语言模型中的知识与推理：基于认知双系统理论的探索", "tldr": "本文提出了一个认知归因框架，基于双系统认知理论，将大型语言模型的认知解耦为知识检索和推理调整两个阶段，并通过快思和慢思模式进行量化分析，揭示了知识和推理在大型语言模型中的作用、层级分布及参数缩放的影响。", "motivation": "大型语言模型在推理过程中同时利用知识和推理能力，但区分两者对模型分析、可解释性和开发至关重要。", "method": "受双系统认知理论启发，本文提出了一个认知归因框架来解耦知识和推理的贡献。该框架将大型语言模型的认知分解为知识检索（阶段1）和推理调整（阶段2）两个独立但互补的阶段。为了分离这些阶段，通过提示词使大型语言模型分别在快思和慢思两种不同的认知模式下生成答案，然后分析不同认知模式下的表现来量化知识和推理的贡献。该架构应用于15个大型语言模型和3个数据集。", "result": "1. 推理调整具有领域特异性，有利于推理密集型领域（如数学、物理和化学），但可能损害知识密集型领域。\n2. 参数缩放能同时改善知识和推理能力，其中知识的改善更为显著。此外，参数缩放使大型语言模型的推理能力显著更谨慎，而智能程度适度提高。\n3. 知识主要存在于较低的网络层，而推理则在较高的层中运作。", "conclusion": "本框架不仅有助于从“解耦”视角理解大型语言模型，还为现有研究提供了新的见解，包括缩放定律、分层知识编辑和小型模型推理的局限性。", "translation": "大型语言模型（LLMs）在推理过程中同时利用知识和推理，但区分它们的能力在模型分析、可解释性和开发中发挥着关键作用。受双系统认知理论启发，我们提出了一个认知归因框架来解耦知识和推理的贡献。具体而言，大型语言模型的认知被分解为两个不同但互补的阶段：知识检索（阶段1）和推理调整（阶段2）。为了分离这些阶段，我们提示大型语言模型分别在两种不同的认知模式下生成答案：快思和慢思。通过分析不同认知模式下的表现来量化知识和推理的贡献。该架构应用于15个大型语言模型和3个数据集。结果显示：(1) 推理调整具有领域特异性，有利于推理密集型领域（如数学、物理和化学），并可能损害知识密集型领域。(2) 参数缩放能同时改善知识和推理能力，其中知识的改善更为显著。此外，参数缩放使大型语言模型的推理能力显著更谨慎，而智能程度适度提高。(3) 知识主要存在于较低的网络层，而推理则在较高的层中运作。我们的框架不仅有助于从“解耦”视角理解大型语言模型，还为现有研究提供了新的见解，包括缩放定律、分层知识编辑和小型模型推理的局限性。", "summary": "本文基于双系统认知理论，提出了一个认知归因框架，旨在解耦大型语言模型（LLMs）中的知识与推理贡献。该框架将LLMs的认知分为知识检索和推理调整两个阶段，并通过“快思”和“慢思”两种认知模式来量化分析两者的作用。研究在15个LLMs和3个数据集上进行，发现推理调整具有领域特异性，参数缩放对知识和推理均有提升（知识提升更显著），且知识主要位于较低网络层，推理则在较高层。该框架为理解LLMs并探索缩放定律、知识编辑等现有研究提供了新视角。", "keywords": "大型语言模型, 知识, 推理, 双系统理论, 解耦", "comments": "这项工作创新性地将认知双系统理论应用于大型语言模型分析，提供了一个新颖的“解耦”视角来理解LLM内部知识与推理的运作机制。通过区分快思和慢思模式，该研究为量化知识和推理的贡献提供了一种可行方法。其发现对于理解LLM的内部结构、参数缩放效应以及不同领域任务中的表现具有重要意义，尤其是在模型可解释性和未来模型开发方面具有潜在价值。"}}
{"id": "2507.18151", "title": "Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality", "authors": ["Shizhen Zhang", "Shengxin Li", "Quan Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Appear UIST2025", "url": "http://arxiv.org/abs/2507.18151v1", "summary": "Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience\ncommunication challenges, primarily due to executive dysfunction and emotional\ndysregulation, even after years of social integration. While existing\ninterventions predominantly target children through structured or intrusive\nmethods, adults lack tools that translate clinical strategies into daily\ncommunication support. To address this gap, we present Understood, a Mixed\nReality (MR) system implemented on Microsoft HoloLens 2, designed to assist\nadults with ADHD in real-world communication. Through formative semi-structured\ninterviews and a design workshop, we identified critical communication barriers\nand derived design goals for the system. Understood combines three key\nfeatures: (1) real-time conversation summarization to reduce cognitive load,\n(2) context-aware subsequent word suggestions during moments of disfluency, and\n(3) topic shifting detection and reminding to mitigate off-topic transitions. A\nwithin-subjects user study and expert interviews demonstrate that Understood\neffectively supports communication with high usability, offering a complement\nto therapist-mediated interventions.", "comment": "Appear UIST2025", "pdf_url": "http://arxiv.org/pdf/2507.18151v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Understood：使用混合现实为成人多动症患者提供实时沟通支持", "tldr": "Understood是一个基于混合现实的系统，旨在通过实时对话摘要、上下文感知词语建议和跑题检测来帮助患有ADHD的成年人改善沟通。", "motivation": "患有注意力缺陷多动障碍（ADHD）的成年人常因执行功能障碍和情绪失调而面临沟通挑战。现有干预措施主要针对儿童，而成人缺乏能将临床策略转化为日常沟通支持的工具。本研究旨在填补这一空白。", "method": "研究人员提出了一个名为Understood的混合现实（MR）系统，该系统在Microsoft HoloLens 2上实现，旨在支持ADHD成年人的实时沟通。通过形成性半结构化访谈和设计研讨会，确定了关键沟通障碍和设计目标。Understood系统结合了三个主要功能：实时对话摘要、上下文感知后续词语建议以及话题转移检测和提醒。该系统通过一项受试者内用户研究和专家访谈进行了评估。", "result": "用户研究和专家访谈表明，Understood系统能有效支持沟通，并具有高可用性。", "conclusion": "Understood系统为ADHD成年人提供了一种有效的沟通支持，可以作为治疗师介导干预的补充。", "translation": "患有注意力缺陷多动障碍（ADHD）的成年人即使在多年的社会融合后，也常因执行功能障碍和情绪失调而经历沟通挑战。尽管现有干预措施主要通过结构化或侵入性方法针对儿童，但成年人缺乏能将临床策略转化为日常沟通支持的工具。为了弥补这一空白，我们提出了Understood，一个在Microsoft HoloLens 2上实现的混合现实（MR）系统，旨在帮助ADHD成年人在现实世界中进行沟通。通过形成性半结构化访谈和设计研讨会，我们确定了关键的沟通障碍并推导出了系统的设计目标。Understood结合了三个关键功能：（1）实时对话摘要以减少认知负荷，（2）在说话不流利时提供上下文感知的后续词语建议，以及（3）话题转移检测和提醒以减轻跑题。一项受试者内用户研究和专家访谈表明，Understood能有效支持沟通，具有高可用性，为治疗师介导的干预提供了补充。", "summary": "本研究提出并评估了Understood，一个基于Microsoft HoloLens 2的混合现实系统，旨在解决ADHD成年人因执行功能障碍和情绪失调导致的沟通障碍。该系统通过实时对话摘要、上下文感知词语建议和话题转移检测与提醒等功能，将临床策略转化为日常沟通支持。用户研究和专家访谈证实了其在提升沟通和可用性方面的有效性，为现有治疗方法提供了有益补充。", "keywords": "ADHD, 混合现实, 沟通支持, HoloLens 2, 实时", "comments": "该论文的创新之处在于利用混合现实技术为ADHD成年人提供实时、非侵入性的沟通支持，填补了成人ADHD干预工具的空白。其结合多功能（摘要、建议、跑题检测）的设计思路，有效解决了ADHD患者的特定沟通痛点，具有重要的应用潜力。"}}
{"id": "2412.15669", "title": "WigglyEyes: Inferring Eye Movements from Keypress Data", "authors": ["Yujun Zhu", "Danqing Shi", "Hee-Seung Moon", "Antti Oulasvirta"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.15669v3", "summary": "We present a model for inferring where users look during interaction based on\nkeypress data only. Given a key log, it outputs a scanpath that tells,\nmoment-by-moment, how the user had moved eyes while entering those keys. The\nmodel can be used as a proxy for human data in cases where collecting real eye\ntracking data is expensive or impossible. Our technical insight is an inference\narchitecture that considers the individual characteristics of the user,\ninferred as a low-dimensional parameter vector. We present a novel loss\nfunction for synchronizing inferred eye movements with the keypresses.\nEvaluations on touchscreen typing demonstrate accurate gaze inference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.15669v3", "cate": "cs.HC", "date": "2024-12-20", "updated": "2025-07-23", "AI": {"title_translation": "WigglyEyes：从按键数据推断眼球运动", "tldr": "本文提出了WigglyEyes模型，仅通过按键数据推断用户的眼球运动，可在传统眼动追踪昂贵或不可行时作为替代。", "motivation": "收集真实的眼动追踪数据既昂贵又困难，因此需要一个能够替代人眼数据的模型。", "method": "该模型采用了一种推断架构，考虑了用户的个体特征（通过低维参数向量推断），并引入了一种新颖的损失函数来同步推断的眼球运动与按键。", "result": "在触摸屏打字上的评估表明，该模型能够准确地推断用户的凝视。", "conclusion": "该模型能够仅通过按键数据准确推断眼球运动，可作为真实眼动追踪数据的替代。", "translation": "我们提出了一种仅基于按键数据推断用户在交互过程中注视位置的模型。给定按键日志，它会输出一条扫描路径，逐时指示用户在输入这些按键时如何移动眼睛。当收集真实的眼动追踪数据昂贵或不可能时，该模型可以用作人类数据的替代。我们的技术洞察是一种推断架构，它考虑了用户的个体特征，这些特征被推断为一个低维参数向量。我们提出了一种新颖的损失函数，用于将推断的眼球运动与按键同步。在触摸屏打字上的评估证明了准确的凝视推断。", "summary": "本文提出了WigglyEyes模型，它仅利用按键数据来推断用户在交互过程中的眼球运动，并输出逐时的扫描路径。该模型旨在解决传统眼动追踪数据收集成本高昂或不可行的问题，提供了一个可行的替代方案。其核心技术包括一个考虑用户个体特征的推断架构（通过低维参数向量表示）以及一个用于同步眼球运动与按键的新颖损失函数。在触摸屏打字上的评估验证了其凝视推断的准确性。", "keywords": "眼球运动, 按键数据, 凝视推断, 扫描路径, 用户特征", "comments": "本文的创新点在于能够仅从按键数据推断眼球运动，这为克服传统眼动追踪的局限性提供了一种新颖且非侵入性的方法。其重要性在于提供了一种更易于获取和部署的用户凝视理解方式，尤其适用于不便使用专用眼动追踪硬件的场景。"}}
{"id": "2507.11783", "title": "EEG Foundation Models: A Critical Review of Current Progress and Future Directions", "authors": ["Gayal Kuruppu", "Neeraj Wagh", "Yogatheesan Varatharajah"], "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      20 pages, 5 figures, 3 tables (main + supplement)", "url": "http://arxiv.org/abs/2507.11783v2", "summary": "Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.", "comment": "20 pages, 5 figures, 3 tables (main + supplement)", "pdf_url": "http://arxiv.org/pdf/2507.11783v2", "cate": "eess.SP", "date": "2025-07-15", "updated": "2025-07-23", "AI": {"title_translation": "脑电图基础模型：当前进展和未来方向的批判性综述", "tldr": "本文对早期脑电图基础模型（EEG-FMs）进行了批判性综述，评估了它们的进展和局限性，并提出了未来研究方向。", "motivation": "由于监督式脑电图编码器在学习鲁棒模式方面的不足以及对昂贵标注的过度依赖，促使了对通用自监督脑电图编码器（即脑电图基础模型，EEG-FMs）的转变。然而，早期EEG-FMs的实际应用准备程度和长期研究进展的准则尚不明确，因此有必要进行系统全面的综述。", "method": "本研究回顾了10个早期脑电图基础模型，并对其方法论、实证发现和未解决的研究空白进行了批判性综合。", "result": "研究发现，大多数EEG-FMs采用基于序列的建模方案，依赖于基于Transformer的骨干网络和掩码序列的重构进行自监督。然而，模型评估仍然异质且局限，这使得评估其实用性具有挑战性。", "conclusion": "未来的工作应采用标准化和现实的评估，展示更显著的规模效应，并在脑电图表示学习管道中做出有原则且值得信赖的选择。与领域专家合作开发基准、软件工具、技术方法和应用可能进一步提升EEG-FMs的转化实用性和实际应用。", "translation": "通过脑电图（EEG）记录的电活动模式对科学和临床研究具有巨大价值。监督式脑电图编码器无法学习鲁棒的脑电图模式以及它们对昂贵信号标注的过度依赖，促使人们转向通用自监督脑电图编码器，即脑电图基础模型（EEG-FMs），以实现鲁棒和可扩展的脑电图特征提取。然而，早期EEG-FMs的实际应用准备程度以及长期研究进展的准则仍不明确。因此，有必要对第一代EEG-FMs进行系统而全面的综述，以了解当前的技术水平并确定未来EEG-FMs的关键方向。为此，本研究综述了10个早期EEG-FMs，并对其方法论、实证发现和未解决的研究空白进行了批判性综合。我们发现，大多数EEG-FMs采用基于序列的建模方案，依赖于基于Transformer的骨干网络和掩码序列的重构进行自监督。然而，模型评估仍然异质且在很大程度上受到限制，这使得评估它们的实际即用效用变得具有挑战性。除了采用标准化和现实的评估外，未来的工作还应展示更显著的规模效应，并在整个脑电图表示学习管道中做出有原则且值得信赖的选择。我们相信，与领域专家合作开发基准、软件工具、技术方法和应用可能会进一步提高EEG-FMs的转化实用性和实际应用。", "summary": "本综述文章批判性地审查了早期脑电图基础模型（EEG-FMs）的当前进展和未来方向。文章指出，由于传统监督学习方法的局限性，EEG-FMs作为一种自监督方法应运而生。通过对10个早期EEG-FMs的分析，发现它们多采用基于Transformer的序列建模和掩码重建进行自监督，但评估方法缺乏统一标准。文章强调了标准化评估、展示规模效应以及在表示学习中做出可靠选择的重要性，并建议通过多方合作开发工具和应用来推动EEG-FMs的实际转化和应用。", "keywords": "脑电图基础模型, 自监督学习, Transformer, 批判性综述, 特征提取", "comments": "本文对新兴的脑电图基础模型领域进行了及时且全面的综述，指出了该领域的关键进展和现有挑战。其创新之处在于系统性地评估了早期模型的共性与局限性，特别是对评估异质性的强调，为未来的研究提供了明确的方向。重要性体现在其为该领域的研究人员提供了宝贵的路线图，有助于推动EEG-FMs从理论走向实际应用。文章的局限性可能在于其仅基于已发表的10个“早期”模型进行分析，未能涵盖最新的、可能尚未发表的突破性进展。"}}
{"id": "2507.09305", "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "authors": ["Zhiwei Xu"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2507.09305v3", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.3% SPR, 6.0% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable. Our code and model weights are available at\nhttps://github.com/zwxu064/DAAStar.git.", "comment": "International Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2507.09305v3", "cate": "cs.CV", "date": "2025-07-12", "updated": "2025-07-24", "AI": {"title_translation": "DAA*：用于基于图像的路径规划的深度角度A星算法", "tldr": "DAA*是一种新的深度角度A*算法，通过引入路径角度自由度（PAF）来提高路径平滑度和与参考路径的相似性，并在多个数据集上表现出显著优于现有方法的性能。", "motivation": "在从专家演示中进行路径模仿学习时，路径平滑度经常被忽视。", "method": "本文引入了深度角度A* (DAA*) 算法，该算法通过将所提出的路径角度自由度 (PAF) 纳入A*中，以通过自适应路径平滑度来提高路径相似性。PAF旨在通过寻找其最小值和最大值之间的权衡，探索移动角度对路径节点扩展的影响。DAA*通过路径缩短（对应启发式距离）和路径平滑（对应PAF）的联合优化，使路径与参考路径紧密对齐，从而改善路径最优性。", "result": "通过在7个数据集（包括4个迷宫数据集、2个视频游戏数据集和一个包含2个场景的真实世界无人机视角数据集）上的全面评估，DAA*在预测路径与参考路径之间的路径相似性方面比Neural A*有显著改进，并且在最短路径合理时路径长度更短，具体表现为SPR提高9.0%，ASIM提高6.9%，PSIM提高3.9%。此外，当联合学习路径查找与路径损失和路径概率图损失时，DAA*显著优于最先进的TransPath，SPR提高了6.3%，PSIM提高了6.0%，ASIM提高了3.7%。论文还讨论了路径最优性与搜索效率之间在适用情况下的轻微权衡。", "conclusion": "DAA*通过引入路径角度自由度（PAF）并进行路径缩短和平滑的联合优化，显著提高了模仿学习中路径规划的平滑度和与参考路径的相似性，并在多个数据集上表现出优越的性能。", "translation": "路径平滑度在从专家演示中进行路径模仿学习时经常被忽视。在本文中，我们引入了一种新颖的学习方法，称为深度角度A* (DAA*)，通过将所提出的路径角度自由度 (PAF) 纳入A*算法中，以通过自适应路径平滑度来提高路径相似性。PAF旨在通过寻找其最小值和最大值之间的权衡，探索移动角度对路径节点扩展的影响，从而实现模仿学习的高度适应性。DAA*通过路径缩短和平滑的联合优化，使路径与参考路径紧密对齐，从而改善路径最优性，这分别对应于启发式距离和PAF。通过对7个数据集（包括4个迷宫数据集、2个视频游戏数据集和一个包含2个场景的真实世界无人机视角数据集）进行全面评估，我们展示了DAA*在预测路径与参考路径之间的路径相似性方面比Neural A*有显著改进，并且在最短路径合理时路径长度更短，SPR提高了9.0%，ASIM提高了6.9%，PSIM提高了3.9%。此外，当联合学习路径查找与路径损失和路径概率图损失时，DAA*显著优于最先进的TransPath，SPR提高了6.3%，PSIM提高了6.0%，ASIM提高了3.7%。我们还讨论了路径最优性与搜索效率之间在适用情况下的轻微权衡。我们的代码和模型权重可在https://github.com/zwxu064/DAAStar.git获取。", "summary": "本文提出了一种名为深度角度A* (DAA*) 的新型路径规划算法，旨在解决模仿学习中路径平滑度被忽视的问题。DAA*通过引入路径角度自由度 (PAF) 并对路径缩短和平滑进行联合优化，显著提高了生成路径的平滑度和与参考路径的相似性。在多个数据集上的实验结果表明，DAA*在路径相似性、路径长度和整体性能上均优于现有的Neural A*和TransPath方法。", "keywords": "深度角度A*, 路径规划, 模仿学习, 路径平滑度, 路径角度自由度", "comments": "DAA*的创新之处在于将路径角度自由度(PAF)的概念引入A*算法，并通过联合优化实现路径平滑度和最优性的平衡，有效解决了模仿学习中路径平滑度不足的问题。其在多类型数据集上的显著性能提升，证明了该方法的有效性和普适性。"}}
{"id": "2507.08017", "title": "Mechanistic Indicators of Understanding in Large Language Models", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2507.08017v3", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. We offer an accessible synthesis\nof these findings that doubles as an introduction to MI while integrating these\nfindings within a novel theoretical framework for thinking about machine\nunderstanding. We argue that LLMs develop internal structures that are\nfunctionally analogous to the kind of understanding that consists in seeing\nconnections. To sharpen this idea, we propose a three-tiered conception of\nunderstanding. First, conceptual understanding emerges when a model forms\n\"features\" as directions in latent space, learning the connections between\ndiverse manifestations of something. Second, state-of-the-world understanding\nemerges when a model learns contingent factual connections between features and\ndynamically tracks changes in the world. Third, principled understanding\nemerges when a model ceases to rely on a collection of memorized facts and\ndiscovers a \"circuit\" connecting these facts. However, these forms of\nunderstanding remain radically different from human understanding, as the\nphenomenon of \"parallel mechanisms\" shows. We conclude that the debate should\nmove beyond the yes-or-no question of whether LLMs understand to investigate\nhow their strange minds work and forge conceptions that fit them.", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2507.08017v3", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-24", "AI": {"title_translation": "大型语言模型理解的机制性指标", "tldr": "本文通过整合机械可解释性领域的最新发现，提出了一个三层理解理论框架，以探讨大型语言模型如何发展出类似理解的内部结构，并呼吁将对LLM理解的讨论转向其工作机制。", "motivation": "挑战了大型语言模型（LLMs）仅依赖表面统计的观点，旨在通过机械可解释性（MI）领域的最新发现，深入探究LLMs的内部工作机制，并为机器理解构建新的理论框架。", "method": "本文综合了机械可解释性（MI）领域的最新发现，并提出了一个新颖的理论框架来思考机器理解。具体地，提出了一个理解的三层概念：概念理解、世界状态理解和原则性理解。", "result": "提出了理解的三层概念：第一层是概念理解，模型形成潜在空间中的“特征”并学习事物不同表现之间的联系；第二层是世界状态理解，模型学习特征之间偶然的事实联系并动态跟踪世界变化；第三层是原则性理解，模型不再依赖记忆的事实集合，而是发现连接这些事实的“回路”。然而，“并行机制”现象表明这些理解形式与人类理解仍有根本区别。", "conclusion": "关于大型语言模型是否理解的争论应该超越简单的“是”或“否”的二元问题，转而深入研究它们奇特的工作方式，并构建适合它们的理解概念。", "translation": "机械可解释性（MI）领域——一个探究大型语言模型（LLMs）内部运作的领域——的最新发现，挑战了这些模型仅依赖表面统计的观点。我们对这些发现进行了易于理解的综合，这既可作为MI的入门介绍，又将这些发现整合到一个关于机器理解的新颖理论框架中。我们认为LLMs发展出的内部结构在功能上类似于那种由“看到联系”构成的理解。为了深化这个想法，我们提出了一个三层理解概念。首先，当模型在潜在空间中形成“特征”，学习某事物不同表现形式之间的联系时，概念理解就出现了。其次，当模型学习特征之间偶然的事实联系并动态跟踪世界变化时，世界状态理解就出现了。第三，当模型不再依赖一系列记忆的事实，而是发现连接这些事实的“回路”时，原则性理解就出现了。然而，正如“并行机制”现象所示，这些理解形式与人类理解仍有根本区别。我们得出结论，争论应该超越LLMs是否理解的“是”或“否”问题，转而调查它们奇怪的思维如何运作，并构建适合它们的理解概念。", "summary": "本文综合了机械可解释性（MI）领域的最新研究成果，旨在挑战大型语言模型（LLMs）仅依赖表面统计的传统观点。作者提出了一个新颖的理论框架，将MI的发现融入其中，并引入了三层理解概念：概念理解、世界状态理解和原则性理解。该框架认为LLMs发展出类似于人类“看到联系”的内部结构。尽管如此，论文也指出LLMs的理解形式与人类理解存在根本差异（通过“并行机制”现象体现），并呼吁未来的研究应侧重于深入探讨LLMs的运作机制，而非仅仅纠结于它们是否真正理解。", "keywords": "大型语言模型, 机械可解释性, 机器理解, 三层理解, 并行机制", "comments": "本文的创新之处在于其提出了一个关于大型语言模型理解的三层理论框架，这为深入探讨LLM的内部机制和理解能力提供了新的视角。它超越了简单的二元判断，即LLM是否理解，而是引导研究者去探究“如何理解”以及“理解的本质”。这对于机械可解释性领域和人工智能哲学都具有重要意义，有助于更细致地界定机器智能的边界和特征。其局限性可能在于，作为一个理论框架，它需要更多的实证研究来验证其各个层面的具体表现和区分度。"}}
