{"id": "2505.22677", "pdf": "https://arxiv.org/pdf/2505.22677", "abs": "https://arxiv.org/abs/2505.22677", "authors": ["Jisu Kim", "Alex Mattingly", "Eung-Joo Lee", "Benjamin S. Riggan"], "title": "Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking", "categories": ["cs.CV"], "comment": "To appear at IEEE International Conference on Automatic Face and\n  Gesture 2025 (FG2025)", "summary": "Head detection and tracking are essential for downstream tasks, but current\nmethods often require large computational budgets, which increase latencies and\nties up resources (e.g., processors, memory, and bandwidth). To address this,\nwe propose a framework to enhance tiny head detection and tracking by\noptimizing the balance between performance and efficiency. Our framework\nintegrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3)\na small receptive field detection mechanism. These innovations enhance\ndetection by bridging the gap between large and small detectors, capturing\nhigh-frequency details at multiple scales during training, and using filters\nwith small receptive fields to detect tiny heads. Evaluations on the CroHD and\nCrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and\nmean Average Precision (mAP), demonstrating the effectiveness of our approach\nin crowded scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6027\u80fd\u548c\u6548\u7387\u5e73\u8861\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u5fae\u5c0f\u5934\u90e8\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff0c\u901a\u8fc7\u8de8\u57df\u68c0\u6d4b\u635f\u5931\u3001\u591a\u5c3a\u5ea6\u6a21\u5757\u548c\u5c0f\u611f\u53d7\u91ce\u68c0\u6d4b\u673a\u5236\u5b9e\u73b0\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5ef6\u8fdf\u5927\u4e14\u5360\u7528\u8d44\u6e90\u591a\uff0c\u9700\u4f18\u5316\u5fae\u5c0f\u5934\u90e8\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861\u3002", "method": "\u6574\u5408\u8de8\u57df\u68c0\u6d4b\u635f\u5931\u3001\u591a\u5c3a\u5ea6\u6a21\u5757\u548c\u5c0f\u611f\u53d7\u91ce\u68c0\u6d4b\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u5728CroHD\u548cCrowdHuman\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u76ee\u6807\u8ddf\u8e2a\u7cbe\u5ea6\uff08MOTA\uff09\u548c\u5e73\u5747\u7cbe\u5ea6\uff08mAP\uff09\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u62e5\u6324\u573a\u666f\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u5fae\u5c0f\u5934\u90e8\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2505.22701", "pdf": "https://arxiv.org/pdf/2505.22701", "abs": "https://arxiv.org/abs/2505.22701", "authors": ["Ziyue Kang", "Weichuan Zhang"], "title": "Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for Sparse-Data Vision", "categories": ["cs.CV"], "comment": null, "summary": "A major challenge in rare animal image classification is the scarcity of\ndata, as many species usually have only a small number of labeled samples.\n  To address this challenge, we designed a hybrid deep-learning framework\ncomprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50\nbackbones, and a Bayesian linear classification head. To our knowledge, we are\nthe first to introduce an adaptive frequency-domain selection mechanism that\nlearns optimal low-, mid-, and high-frequency boundaries suited to the\nsubsequent backbones.\n  Our network first captures image frequency-domain cues via this adaptive DCT\npartitioning. The adaptively filtered frequency features are then fed into\nViT-B16 to model global contextual relationships, while ResNet50 concurrently\nextracts local, multi-scale spatial representations from the original image. A\ncross-level fusion strategy seamlessly integrates these frequency- and\nspatial-domain embeddings, and the fused features are passed through a Bayesian\nlinear classifier to output the final category predictions. On our self-built\n50-class wildlife dataset, this approach outperforms conventional CNN and\nfixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme\nsample scarcity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94DCT\u9884\u5904\u7406\u6a21\u5757\u3001ViT-B16\u548cResNet50\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ee5\u53ca\u8d1d\u53f6\u65af\u7ebf\u6027\u5206\u7c7b\u5934\uff0c\u7528\u4e8e\u89e3\u51b3\u7a00\u6709\u52a8\u7269\u56fe\u50cf\u5206\u7c7b\u4e2d\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u7a00\u6709\u52a8\u7269\u56fe\u50cf\u5206\u7c7b\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u8bb8\u591a\u7269\u79cd\u4ec5\u6709\u5c11\u91cf\u6807\u8bb0\u6837\u672c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9891\u7387\u57df\u9009\u62e9\u673a\u5236\uff0c\u7ed3\u5408ViT-B16\u548cResNet50\u63d0\u53d6\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u878d\u5408\u7b56\u7565\u6574\u5408\u7279\u5f81\uff0c\u6700\u540e\u4f7f\u7528\u8d1d\u53f6\u65af\u7ebf\u6027\u5206\u7c7b\u5668\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u81ea\u5efa\u768450\u7c7b\u91ce\u751f\u52a8\u7269\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edfCNN\u548c\u56fa\u5b9a\u9891\u5e26DCT\u65b9\u6cd5\uff0c\u5728\u6837\u672c\u7a00\u7f3a\u60c5\u51b5\u4e0b\u8fbe\u5230\u6700\u4f18\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u9891\u7387\u57df\u5904\u7406\u548c\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a00\u6709\u52a8\u7269\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.22705", "pdf": "https://arxiv.org/pdf/2505.22705", "abs": "https://arxiv.org/abs/2505.22705", "authors": ["Qi Cai", "Jingwen Chen", "Yang Chen", "Yehao Li", "Fuchen Long", "Yingwei Pan", "Zhaofan Qiu", "Yiheng Zhang", "Fengbin Gao", "Peihan Xu", "Yimeng Wang", "Kai Yu", "Wenxuan Chen", "Ziwei Feng", "Zijian Gong", "Jianzhuang Pan", "Yi Peng", "Rui Tian", "Siyu Wang", "Bo Zhao", "Ting Yao", "Tao Mei"], "title": "HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer", "categories": ["cs.CV", "cs.MM"], "comment": "Source codes and models are available at\n  https://github.com/HiDream-ai/HiDream-I1 and\n  https://github.com/HiDream-ai/HiDream-E1", "summary": "Recent advancements in image generative foundation models have prioritized\nquality improvements but often at the cost of increased computational\ncomplexity and inference latency. To address this critical trade-off, we\nintroduce HiDream-I1, a new open-source image generative foundation model with\n17B parameters that achieves state-of-the-art image generation quality within\nseconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer\n(DiT) structure. Specifically, it starts with a dual-stream decoupled design of\nsparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two\nseparate encoders are first involved to independently process image and text\ntokens. Then, a single-stream sparse DiT structure with dynamic MoE\narchitecture is adopted to trigger multi-model interaction for image generation\nin a cost-efficient manner. To support flexiable accessibility with varied\nmodel capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full,\nHiDream-I1-Dev, and HiDream-I1-Fast.\n  Furthermore, we go beyond the typical text-to-image generation and remould\nHiDream-I1 with additional image conditions to perform precise,\ninstruction-based editing on given images, yielding a new instruction-based\nimage editing model namely HiDream-E1. Ultimately, by integrating text-to-image\ngeneration and instruction-based image editing, HiDream-I1 evolves to form a\ncomprehensive image agent (HiDream-A1) capable of fully interactive image\ncreation and refinement. To accelerate multi-modal AIGC research, we have\nopen-sourced all the codes and model weights of HiDream-I1-Full,\nHiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites:\nhttps://github.com/HiDream-ai/HiDream-I1 and\nhttps://github.com/HiDream-ai/HiDream-E1. All features can be directly\nexperienced via https://vivago.ai/studio.", "AI": {"tldr": "HiDream-I1\u662f\u4e00\u4e2a17B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u548c\u52a8\u6001MoE\u67b6\u6784\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5feb\u901f\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e09\u79cd\u53d8\u4f53\u3002\u8fd8\u6269\u5c55\u4e3a\u6307\u4ee4\u7f16\u8f91\u6a21\u578bHiDream-E1\u548c\u7efc\u5408\u56fe\u50cf\u4ee3\u7406HiDream-A1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u89e3\u8026\u8bbe\u8ba1\u548c\u52a8\u6001MoE\u67b6\u6784\u7684\u7a00\u758fDiT\u7ed3\u6784\uff0c\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u9ad8\u6548\u751f\u6210\u3002", "result": "\u5b9e\u73b0\u4e86\u79d2\u7ea7\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5e76\u6269\u5c55\u4e3a\u6307\u4ee4\u7f16\u8f91\u548c\u4ea4\u4e92\u5f0f\u56fe\u50cf\u4ee3\u7406\u3002", "conclusion": "HiDream-I1\u53ca\u5176\u884d\u751f\u6a21\u578b\u4e3a\u591a\u6a21\u6001AIGC\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5f00\u6e90\u5de5\u5177\u3002"}}
{"id": "2505.22762", "pdf": "https://arxiv.org/pdf/2505.22762", "abs": "https://arxiv.org/abs/2505.22762", "authors": ["Marco Colussi", "Dragan Ahmetovic", "Sergio Mascetti"], "title": "MIAS-SAM: Medical Image Anomaly Segmentation without thresholding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents MIAS-SAM, a novel approach for the segmentation of\nanomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to\nstore relevant image features, which are extracted from normal data using the\nSAM encoder. At inference time, the embedding patches extracted from the SAM\nencoder are compared with those in the memory bank to obtain the anomaly map.\nFinally, MIAS-SAM computes the center of gravity of the anomaly map to prompt\nthe SAM decoder, obtaining an accurate segmentation from the previously\nextracted features. Differently from prior works, MIAS-SAM does not require to\ndefine a threshold value to obtain the segmentation from the anomaly map.\nExperimental results conducted on three publicly available datasets, each with\na different imaging modality (Brain MRI, Liver CT, and Retina OCT) show\naccurate anomaly segmentation capabilities measured using DICE score. The code\nis available at: https://github.com/warpcut/MIAS-SAM", "AI": {"tldr": "MIAS-SAM\u662f\u4e00\u79cd\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u533a\u57df\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8e\u8865\u4e01\u7684\u8bb0\u5fc6\u5e93\u548cSAM\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u65e0\u9700\u9608\u503c\u5373\u53ef\u83b7\u5f97\u7cbe\u786e\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8bbe\u7f6e\u9608\u503c\u6765\u5206\u5272\u5f02\u5e38\u533a\u57df\uff0cMIAS-SAM\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u9700\u6c42\uff0c\u63d0\u9ad8\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u81ea\u52a8\u5316\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528SAM\u7f16\u7801\u5668\u4ece\u6b63\u5e38\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\u5e76\u5b58\u50a8\u5230\u8bb0\u5fc6\u5e93\u4e2d\uff0c\u63a8\u7406\u65f6\u901a\u8fc7\u6bd4\u8f83\u7279\u5f81\u751f\u6210\u5f02\u5e38\u56fe\uff0c\u5e76\u5229\u7528\u5f02\u5e38\u56fe\u7684\u91cd\u5fc3\u63d0\u793aSAM\u89e3\u7801\u5668\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08\u8111MRI\u3001\u809d\u810fCT\u548c\u89c6\u7f51\u819cOCT\uff09\u4e0a\u5b9e\u9a8c\uff0c\u663e\u793a\u51fa\u8f83\u9ad8\u7684DICE\u5206\u6570\uff0c\u8868\u660e\u5176\u5f02\u5e38\u5206\u5272\u80fd\u529b\u4f18\u5f02\u3002", "conclusion": "MIAS-SAM\u65e0\u9700\u9608\u503c\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684\u5f02\u5e38\u533a\u57df\u5206\u5272\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22704", "pdf": "https://arxiv.org/pdf/2505.22704", "abs": "https://arxiv.org/abs/2505.22704", "authors": ["Feng Yao", "Zilong Wang", "Liyuan Liu", "Junxia Cui", "Li Zhong", "Xiaohan Fu", "Haohui Mai", "Vish Krishnan", "Jianfeng Gao", "Jingbo Shang"], "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.", "AI": {"tldr": "REAL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u548c\u5355\u5143\u6d4b\u8bd5\u53cd\u9988\uff0c\u6fc0\u52b1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u540e\u5904\u7406\uff09\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u8106\u5f31\u542f\u53d1\u5f0f\uff0c\u96be\u4ee5\u786e\u4fdd\u4ee3\u7801\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "method": "REAL\u7ed3\u5408\u7a0b\u5e8f\u5206\u6790\uff08\u68c0\u6d4b\u5b89\u5168\u6216\u53ef\u7ef4\u62a4\u6027\u7f3a\u9677\uff09\u548c\u5355\u5143\u6d4b\u8bd5\uff08\u786e\u4fdd\u529f\u80fd\u6b63\u786e\u6027\uff09\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREAL\u5728\u529f\u80fd\u548c\u4ee3\u7801\u8d28\u91cf\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "REAL\u586b\u8865\u4e86\u5feb\u901f\u539f\u578b\u548c\u751f\u4ea7\u7ea7\u4ee3\u7801\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u5e73\u8861\u3002"}}
{"id": "2505.23301", "pdf": "https://arxiv.org/pdf/2505.23301", "abs": "https://arxiv.org/abs/2505.23301", "authors": ["Rim Rekik", "Stefanie Wuhrer", "Ludovic Hoyet", "Katja Zibrek", "Anne-H\u00e9l\u00e8ne Olivier"], "title": "Quality assessment of 3D human animation: Subjective and objective evaluation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Virtual human animations have a wide range of applications in virtual and\naugmented reality. While automatic generation methods of animated virtual\nhumans have been developed, assessing their quality remains challenging.\nRecently, approaches introducing task-oriented evaluation metrics have been\nproposed, leveraging neural network training. However, quality assessment\nmeasures for animated virtual humans that are not generated with parametric\nbody models have yet to be developed. In this context, we introduce a first\nsuch quality assessment measure leveraging a novel data-driven framework.\nFirst, we generate a dataset of virtual human animations together with their\ncorresponding subjective realism evaluation scores collected with a user study.\nSecond, we use the resulting dataset to learn predicting perceptual evaluation\nscores. Results indicate that training a linear regressor on our dataset\nresults in a correlation of 90%, which outperforms a state of the art deep\nlearning baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u865a\u62df\u4eba\u52a8\u753b\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u751f\u6210\u6570\u636e\u96c6\u5e76\u8bad\u7ec3\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u865a\u62df\u4eba\u52a8\u753b\u8d28\u91cf\u8bc4\u4f30\u7f3a\u4e4f\u975e\u53c2\u6570\u5316\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4ef7\u6807\u51c6\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "1. \u751f\u6210\u865a\u62df\u4eba\u52a8\u753b\u6570\u636e\u96c6\u5e76\u6536\u96c6\u4e3b\u89c2\u771f\u5b9e\u611f\u8bc4\u5206\uff1b2. \u4f7f\u7528\u6570\u636e\u96c6\u8bad\u7ec3\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8bc4\u5206\u3002", "result": "\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u8bc4\u5206\u4e0e\u5b9e\u9645\u8bc4\u5206\u7684\u76f8\u5173\u6027\u8fbe\u523090%\uff0c\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\u4e3a\u865a\u62df\u4eba\u52a8\u753b\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.22792", "pdf": "https://arxiv.org/pdf/2505.22792", "abs": "https://arxiv.org/abs/2505.22792", "authors": ["Yuxi Zhang", "Yueting Li", "Xinyu Du", "Sibo Wang"], "title": "Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Generating images from rhetorical languages remains a critical challenge for\ntext-to-image models. Even state-of-the-art (SOTA) multimodal large language\nmodels (MLLM) fail to generate images based on the hidden meaning inherent in\nrhetorical language--despite such content being readily mappable to visual\nrepresentations by humans. A key limitation is that current models emphasize\nobject-level word embedding alignment, causing metaphorical expressions to\nsteer image generation towards their literal visuals and overlook the intended\nsemantic meaning. To address this, we propose Rhet2Pix, a framework that\nformulates rhetorical text-to-image generation as a multi-step policy\noptimization problem, incorporating a two-layer MDP diffusion module. In the\nouter layer, Rhet2Pix converts the input prompt into incrementally elaborated\nsub-sentences and executes corresponding image-generation actions, constructing\nsemantically richer visuals. In the inner layer, Rhet2Pix mitigates reward\nsparsity during image generation by discounting the final reward and optimizing\nevery adjacent action pair along the diffusion denoising trajectory. Extensive\nexperiments demonstrate the effectiveness of Rhet2Pix in rhetorical\ntext-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o,\nGrok-3 and leading academic baselines across both qualitative and quantitative\nevaluations. The code and dataset used in this work are publicly available.", "AI": {"tldr": "Rhet2Pix\u662f\u4e00\u4e2a\u89e3\u51b3\u4fee\u8f9e\u8bed\u8a00\u751f\u6210\u56fe\u50cf\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6b65\u7b56\u7565\u4f18\u5316\u548c\u53cc\u5c42MDP\u6269\u6563\u6a21\u5757\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4fee\u8f9e\u8bed\u8a00\u7684\u9690\u542b\u610f\u4e49\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u56fe\u50cf\u504f\u5411\u5b57\u9762\u89c6\u89c9\u800c\u975e\u8bed\u4e49\u610f\u56fe\u3002", "method": "\u63d0\u51faRhet2Pix\u6846\u67b6\uff0c\u91c7\u7528\u591a\u6b65\u7b56\u7565\u4f18\u5316\u548c\u53cc\u5c42MDP\u6269\u6563\u6a21\u5757\uff0c\u9010\u6b65\u7ec6\u5316\u5b50\u53e5\u5e76\u4f18\u5316\u56fe\u50cf\u751f\u6210\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRhet2Pix\u5728\u4fee\u8f9e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4f18\u4e8eGPT-4o\u3001Grok-3\u7b49SOTA\u6a21\u578b\u3002", "conclusion": "Rhet2Pix\u6709\u6548\u89e3\u51b3\u4e86\u4fee\u8f9e\u8bed\u8a00\u751f\u6210\u56fe\u50cf\u7684\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.22752", "pdf": "https://arxiv.org/pdf/2505.22752", "abs": "https://arxiv.org/abs/2505.22752", "authors": ["Rafik Mankour", "Yassine Chafai", "Hamada Saleh", "Ghassen Ben Hassine", "Thibaud Barreau", "Peter Tankov"], "title": "Climate Finance Bench", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://github.com/Pladifes/climate_finance_bench", "summary": "Climate Finance Bench introduces an open benchmark that targets\nquestion-answering over corporate climate disclosures using Large Language\nModels. We curate 33 recent sustainability reports in English drawn from\ncompanies across all 11 GICS sectors and annotate 330 expert-validated\nquestion-answer pairs that span pure extraction, numerical reasoning, and\nlogical reasoning. Building on this dataset, we propose a comparison of RAG\n(retrieval-augmented generation) approaches. We show that the retriever's\nability to locate passages that actually contain the answer is the chief\nperformance bottleneck. We further argue for transparent carbon reporting in\nAI-for-climate applications, highlighting advantages of techniques such as\nWeight Quantization.", "AI": {"tldr": "Climate Finance Bench\u63d0\u51fa\u4e00\u4e2a\u5f00\u653e\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u4e2d\u7684\u95ee\u7b54\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86RAG\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u6c14\u5019\u62ab\u9732\u4fe1\u606f\u95ee\u7b54\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u63a8\u52a8AI\u5728\u6c14\u5019\u5e94\u7528\u4e2d\u7684\u900f\u660e\u78b3\u62a5\u544a\u3002", "method": "\u6536\u96c633\u4efd\u82f1\u6587\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\uff0c\u6807\u6ce8330\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u6bd4\u8f83RAG\u65b9\u6cd5\uff0c\u5206\u6790\u68c0\u7d22\u5668\u7684\u6027\u80fd\u74f6\u9888\u3002", "result": "\u68c0\u7d22\u5668\u5b9a\u4f4d\u7b54\u6848\u6bb5\u843d\u7684\u80fd\u529b\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u63d0\u5021\u91c7\u7528\u6743\u91cd\u91cf\u5316\u7b49\u6280\u672f\u4ee5\u51cf\u5c11\u78b3\u8db3\u8ff9\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u6c14\u5019\u4fe1\u606f\u62ab\u9732\u95ee\u7b54\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86AI\u5e94\u7528\u4e2d\u78b3\u900f\u660e\u5ea6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.23447", "pdf": "https://arxiv.org/pdf/2505.23447", "abs": "https://arxiv.org/abs/2505.23447", "authors": ["Sara Johansson Fernstad", "Sarah Alsufyani", "Silvia Del Din", "Alison Yarnall", "Lynn Rochester"], "title": "To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics", "categories": ["cs.GR", "cs.HC"], "comment": "Submitted to IEEE Vis2025", "summary": "This paper contributes a set of quality metrics for identification and visual\nanalysis of structured missingness in high-dimensional data. Missing values in\ndata are a frequent challenge in most data generating domains and may cause a\nrange of analysis issues. Structural missingness in data may indicate issues in\ndata collection and pre-processing, but may also highlight important data\ncharacteristics. While research into statistical methods for dealing with\nmissing data are mainly focusing on replacing missing values with plausible\nestimated values, visualization has great potential to support a more in-depth\nunderstanding of missingness structures in data. Nonetheless, while the\ninterest in missing data visualization has increased in the last decade, it is\nstill a relatively overlooked research topic with a comparably small number of\npublications, few of which address scalability issues. Efficient visual\nanalysis approaches are needed to enable exploration of missingness structures\nin large and high-dimensional data, and to support informed decision-making in\ncontext of potential data quality issues. This paper suggests a set of quality\nmetrics for identification of patterns of interest for understanding of\nstructural missingness in data. These quality metrics can be used as guidance\nin visual analysis, as demonstrated through a use case exploring structural\nmissingness in data from a real-life walking monitoring study. All supplemental\nmaterials for this paper are available at\nhttps://doi.org/10.25405/data.ncl.c.7741829.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u7528\u4e8e\u8bc6\u522b\u548c\u53ef\u89c6\u5316\u5206\u6790\u9ad8\u7ef4\u6570\u636e\u4e2d\u7ed3\u6784\u5316\u7f3a\u5931\u7684\u8d28\u91cf\u6307\u6807\uff0c\u586b\u8865\u4e86\u7f3a\u5931\u6570\u636e\u53ef\u89c6\u5316\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u662f\u5e38\u89c1\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u5206\u6790\u95ee\u9898\u3002\u7ed3\u6784\u5316\u7f3a\u5931\u53ef\u80fd\u53cd\u6620\u6570\u636e\u6536\u96c6\u6216\u9884\u5904\u7406\u95ee\u9898\uff0c\u4e5f\u53ef\u80fd\u63ed\u793a\u91cd\u8981\u6570\u636e\u7279\u5f81\u3002\u53ef\u89c6\u5316\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u7f3a\u5931\u7ed3\u6784\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u4e14\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u8d28\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u7406\u89e3\u6570\u636e\u4e2d\u7684\u7ed3\u6784\u5316\u7f3a\u5931\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6b65\u884c\u76d1\u6d4b\u7814\u7a76\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u8d28\u91cf\u6307\u6807\u53ef\u7528\u4e8e\u6307\u5bfc\u53ef\u89c6\u5316\u5206\u6790\uff0c\u5e2e\u52a9\u63a2\u7d22\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u7ed3\u6784\uff0c\u652f\u6301\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u51b3\u7b56\u3002", "conclusion": "\u672c\u6587\u7684\u8d28\u91cf\u6307\u6807\u586b\u8865\u4e86\u7f3a\u5931\u6570\u636e\u53ef\u89c6\u5316\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.22793", "pdf": "https://arxiv.org/pdf/2505.22793", "abs": "https://arxiv.org/abs/2505.22793", "authors": ["Srishti Yadav", "Lauren Tilton", "Maria Antoniak", "Taylor Arnold", "Jiaang Li", "Siddhesh Milind Pawar", "Antonia Karamolegkou", "Stella Frank", "Zhaochong An", "Negar Rostamzadeh", "Daniel Hershcovich", "Serge Belongie", "Ekaterina Shutova"], "title": "Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Modern vision-language models (VLMs) often fail at cultural competency\nevaluations and benchmarks. Given the diversity of applications built upon\nVLMs, there is renewed interest in understanding how they encode cultural\nnuances. While individual aspects of this problem have been studied, we still\nlack a comprehensive framework for systematically identifying and annotating\nthe nuanced cultural dimensions present in images for VLMs. This position paper\nargues that foundational methodologies from visual culture studies (cultural\nstudies, semiotics, and visual studies) are necessary for cultural analysis of\nimages. Building upon this review, we propose a set of five frameworks,\ncorresponding to cultural dimensions, that must be considered for a more\ncomplete analysis of the cultural competencies of VLMs.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u51fa\u7ed3\u5408\u89c6\u89c9\u6587\u5316\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u4e94\u4e2a\u6587\u5316\u7ef4\u5ea6\u6846\u67b6\u4ee5\u7cfb\u7edf\u6027\u5206\u6790VLMs\u7684\u6587\u5316\u80fd\u529b\u3002", "motivation": "VLMs\u5728\u6587\u5316\u591a\u6837\u6027\u5e94\u7528\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u6587\u5316\u7ef4\u5ea6\u7684\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u6587\u5316\u7814\u7a76\uff08\u6587\u5316\u7814\u7a76\u3001\u7b26\u53f7\u5b66\u3001\u89c6\u89c9\u7814\u7a76\uff09\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e94\u4e2a\u6587\u5316\u7ef4\u5ea6\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u6027\u5206\u6790VLMs\u6587\u5316\u80fd\u529b\u7684\u6846\u67b6\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u6587\u5316\u7814\u7a76\u7684\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u548c\u63d0\u5347VLMs\u7684\u6587\u5316\u80fd\u529b\u3002"}}
{"id": "2505.22757", "pdf": "https://arxiv.org/pdf/2505.22757", "abs": "https://arxiv.org/abs/2505.22757", "authors": ["Ansar Aynetdinov", "Alan Akbik"], "title": "Pre-Training Curriculum for Multi-Token Prediction in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Multi-token prediction (MTP) is a recently proposed pre-training objective\nfor language models. Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads. MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models. However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective. To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite. Our experiments show that the\nforward curriculum enables SLMs to better leverage the MTP objective during\npre-training, improving downstream NTP performance and generative output\nquality, while retaining the benefits of self-speculative decoding. The reverse\ncurriculum achieves stronger NTP performance and output quality, but fails to\nprovide any self-speculative decoding benefits.", "AI": {"tldr": "\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u9010\u6b65\u5b66\u4e60\u7b56\u7565\uff08\u6b63\u5411\u548c\u53cd\u5411\u8bfe\u7a0b\uff09\u5e2e\u52a9\u5c0f\u6a21\u578b\u9002\u5e94MTP\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u76ee\u6807\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a\u6b63\u5411\u8bfe\u7a0b\uff08\u4eceNTP\u9010\u6b65\u8fc7\u6e21\u5230MTP\uff09\u548c\u53cd\u5411\u8bfe\u7a0b\uff08\u4eceMTP\u9010\u6b65\u8fc7\u6e21\u5230NTP\uff09\u3002", "result": "\u6b63\u5411\u8bfe\u7a0b\u63d0\u5347\u4e0b\u6e38NTP\u6027\u80fd\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4fdd\u7559\u81ea\u63a8\u6d4b\u89e3\u7801\u4f18\u52bf\uff1b\u53cd\u5411\u8bfe\u7a0b\u867d\u63d0\u5347\u6027\u80fd\u4f46\u65e0\u81ea\u63a8\u6d4b\u89e3\u7801\u4f18\u52bf\u3002", "conclusion": "\u6b63\u5411\u8bfe\u7a0b\u66f4\u9002\u5408SLMs\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387\uff1b\u53cd\u5411\u8bfe\u7a0b\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9700\u6c42\u3002"}}
{"id": "2505.23617", "pdf": "https://arxiv.org/pdf/2505.23617", "abs": "https://arxiv.org/abs/2505.23617", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Mohammadreza Salehi", "Ziqi Gao", "Vishnu Iyengar", "Norimasa Kobori", "Quan Kong", "Ranjay Krishna"], "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution.", "AI": {"tldr": "TrajViT\u901a\u8fc7\u57fa\u4e8e\u7269\u4f53\u8f68\u8ff9\u7684\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u6807\u8bb0\u5e76\u4fdd\u6301\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7a7a\u95f4-\u65f6\u95f4ViT\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\u56e0\u4f7f\u7528\u56fa\u5b9a\u7a7a\u95f4-\u65f6\u95f4\u5757\u5bfc\u81f4\u6807\u8bb0\u5197\u4f59\u548c\u8ba1\u7b97\u4f4e\u6548\uff0c\u4e14\u5728\u76f8\u673a\u79fb\u52a8\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5168\u666f\u5b50\u5bf9\u8c61\u8f68\u8ff9\u7684\u6807\u8bb0\u5316\u65b9\u6cd5TrajViT\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\uff0c\u63d0\u53d6\u8bed\u4e49\u6709\u610f\u4e49\u7684\u6807\u8bb0\u3002", "result": "TrajViT\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eViT3D\uff0c\u5982\u89c6\u9891-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e2dtop-5\u53ec\u56de\u7387\u63d0\u53476%\uff0c\u4e14\u6807\u8bb0\u51cf\u5c1110\u500d\u3002", "conclusion": "TrajViT\u662f\u9996\u4e2a\u5728\u591a\u6837\u5316\u89c6\u9891\u5206\u6790\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8eViT3D\u7684\u9ad8\u6548\u7f16\u7801\u5668\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2505.22797", "pdf": "https://arxiv.org/pdf/2505.22797", "abs": "https://arxiv.org/abs/2505.22797", "authors": ["Vladyslav Gapyak", "Thomas M\u00e4rz", "Andreas Weinmann"], "title": "Fast Trajectory-Independent Model-Based Reconstruction Algorithm for Multi-Dimensional Magnetic Particle Imaging", "categories": ["cs.CV", "cs.NA", "math.NA", "physics.med-ph"], "comment": "10 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Magnetic Particle Imaging (MPI) is a promising tomographic technique for\nvisualizing the spatio-temporal distribution of superparamagnetic\nnanoparticles, with applications ranging from cancer detection to real-time\ncardiovascular monitoring. Traditional MPI reconstruction relies on either\ntime-consuming calibration (measured system matrix) or model-based simulation\nof the forward operator. Recent developments have shown the applicability of\nChebyshev polynomials to multi-dimensional Lissajous Field-Free Point (FFP)\nscans. This method is bound to the particular choice of sinusoidal scanning\ntrajectories. In this paper, we present the first reconstruction on real 2D MPI\ndata with a trajectory-independent model-based MPI reconstruction algorithm. We\nfurther develop the zero-shot Plug-and-Play (PnP) algorithm of the authors --\nwith automatic noise level estimation -- to address the present deconvolution\nproblem, leveraging a state-of-the-art denoiser trained on natural images\nwithout retraining on MPI-specific data. We evaluate our method on the publicly\navailable 2D FFP MPI dataset ``MPIdata: Equilibrium Model with Anisotropy\",\nfeaturing scans of six phantoms acquired using a Bruker preclinical scanner.\nMoreover, we show reconstruction performed on custom data on a 2D scanner with\nadditional high-frequency excitation field and partial data. Our results\ndemonstrate strong reconstruction capabilities across different scanning\nscenarios -- setting a precedent for general-purpose, flexible model-based MPI\nreconstruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u8f68\u8ff9\u7684\u6a21\u578b\u91cd\u5efa\u7b97\u6cd5\uff0c\u7528\u4e8e2D\u78c1\u7c92\u5b50\u6210\u50cf\uff08MPI\uff09\uff0c\u5e76\u7ed3\u5408\u96f6\u6837\u672c\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u65b9\u6cd5\u5904\u7406\u53cd\u5377\u79ef\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u626b\u63cf\u573a\u666f\u4e0b\u7684\u5f3a\u5927\u91cd\u5efa\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfMPI\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u6821\u51c6\u6216\u7279\u5b9a\u8f68\u8ff9\u7684\u6a21\u578b\u4eff\u771f\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u901a\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e0d\u4f9d\u8d56\u7279\u5b9a\u8f68\u8ff9\u7684\u6a21\u578b\u91cd\u5efa\u7b97\u6cd5\uff0c\u63d0\u5347MPI\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u8f68\u8ff9\u65e0\u5173\u7684\u6a21\u578b\u91cd\u5efa\u7b97\u6cd5\uff0c\u7ed3\u5408\u96f6\u6837\u672cPnP\u65b9\u6cd5\uff08\u542b\u81ea\u52a8\u566a\u58f0\u4f30\u8ba1\uff09\uff0c\u5229\u7528\u81ea\u7136\u56fe\u50cf\u8bad\u7ec3\u7684\u5148\u8fdb\u53bb\u566a\u5668\u5904\u7406MPI\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u76842D FFP MPI\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u626b\u63cf\u573a\u666f\u4e0b\u7684\u5f3a\u5927\u91cd\u5efa\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u901a\u7528\u3001\u7075\u6d3b\u7684\u6a21\u578bMPI\u91cd\u5efa\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759", "abs": "https://arxiv.org/abs/2505.22759", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "AI": {"tldr": "FAMA\u662f\u9996\u4e2a\u57fa\u4e8e\u5f00\u6e90\u6570\u636e\u7684\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5bb6\u65cf\uff0c\u586b\u8865\u4e86\u8bed\u97f3\u9886\u57df\u5f00\u653e\u79d1\u5b66\u7684\u7a7a\u767d\uff0c\u6027\u80fd\u63a5\u8fd1\u73b0\u6709\u6a21\u578b\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u57fa\u7840\u6a21\u578b\uff08\u5982Whisper\u548cSeamlessM4T\uff09\u7684\u5c01\u95ed\u6027\u5bfc\u81f4\u53ef\u590d\u73b0\u6027\u548c\u516c\u5e73\u8bc4\u4f30\u56f0\u96be\uff0c\u8bed\u97f3\u9886\u57df\u7f3a\u4e4f\u5f00\u653e\u79d1\u5b66\u52aa\u529b\u3002", "method": "\u5f00\u53d1FAMA\u6a21\u578b\u5bb6\u65cf\uff0c\u4f7f\u7528\u8d85\u8fc7150k\u5c0f\u65f6\u7684\u5f00\u6e90\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\uff0c\u5e76\u5f15\u516516k\u5c0f\u65f6\u7684\u6e05\u6d17\u548c\u4f2a\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "FAMA\u6027\u80fd\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe8\u500d\u3002", "conclusion": "FAMA\u53ca\u5176\u5f00\u6e90\u8d44\u6e90\u63a8\u52a8\u4e86\u8bed\u97f3\u6280\u672f\u7814\u7a76\u7684\u5f00\u653e\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2505.23685", "pdf": "https://arxiv.org/pdf/2505.23685", "abs": "https://arxiv.org/abs/2505.23685", "authors": ["Raffles Xingqi Zhu", "Charlie S. Burlingham", "Olivier Mercier", "Phillip Guan"], "title": "Errors in Stereo Geometry Induce Distance Misperception", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Stereoscopic head-mounted displays (HMDs) render and present binocular images\nto create an egocentric, 3D percept to the HMD user. Within this render and\npresentation pipeline there are potential rendering camera and viewing position\nerrors that can induce deviations in the depth and distance that a user\nperceives compared to the underlying intended geometry. For example, rendering\nerrors can arise when HMD render cameras are incorrectly positioned relative to\nthe assumed centers of projections of the HMD displays and viewing errors can\narise when users view stereo geometry from the incorrect location in the HMD\neyebox. In this work we present a geometric framework that predicts errors in\ndistance perception arising from inaccurate HMD perspective geometry and build\nan HMD platform to reliably simulate render and viewing error in a Quest 3 HMD\nwith eye tracking to experimentally test these predictions. We present a series\nof five experiments to explore the efficacy of this geometric framework and\nshow that errors in perspective geometry can induce both under- and\nover-estimations in perceived distance. We further demonstrate how real-time\nvisual feedback can be used to dynamically recalibrate visuomotor mapping so\nthat an accurate reach distance is achieved even if the perceived visual\ndistance is negatively impacted by geometric error.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u7531HMD\u900f\u89c6\u51e0\u4f55\u4e0d\u51c6\u786e\u5f15\u8d77\u7684\u8ddd\u79bb\u611f\u77e5\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76HMD\u6e32\u67d3\u548c\u663e\u793a\u4e2d\u7684\u76f8\u673a\u53ca\u89c6\u89d2\u4f4d\u7f6e\u8bef\u5dee\u5982\u4f55\u5f71\u54cd\u7528\u6237\u5bf9\u6df1\u5ea6\u548c\u8ddd\u79bb\u7684\u611f\u77e5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u51e0\u4f55\u6846\u67b6\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728Quest 3 HMD\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u900f\u89c6\u51e0\u4f55\u8bef\u5dee\u4f1a\u5bfc\u81f4\u8ddd\u79bb\u611f\u77e5\u7684\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u4f30\u8ba1\uff0c\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u53ef\u52a8\u6001\u6821\u51c6\u89c6\u89c9\u8fd0\u52a8\u6620\u5c04\u3002", "conclusion": "\u51e0\u4f55\u6846\u67b6\u80fd\u6709\u6548\u9884\u6d4b\u8bef\u5dee\uff0c\u52a8\u6001\u6821\u51c6\u53ef\u6539\u5584HMD\u4e2d\u7684\u8ddd\u79bb\u611f\u77e5\u95ee\u9898\u3002"}}
{"id": "2505.22810", "pdf": "https://arxiv.org/pdf/2505.22810", "abs": "https://arxiv.org/abs/2505.22810", "authors": ["Zhoufaran Yang", "Yan Shu", "Zhifei Yang", "Yan Zhang", "Yu Li", "Keyang Lu", "Gangyan Zeng", "Shaohui Liu", "Yu Zhou", "Nicu Sebe"], "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.", "AI": {"tldr": "VidText\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u6587\u672c\u7406\u89e3\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u9891\u7406\u89e3\u548cOCR\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u5c42\u6b21\u4efb\u52a1\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u5ffd\u89c6\u6587\u672c\u4fe1\u606f\uff0cOCR\u57fa\u51c6\u5c40\u9650\u4e8e\u9759\u6001\u56fe\u50cf\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u89c6\u89c9\u4e0e\u6587\u672c\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51faVidText\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u6837\u5316\u573a\u666f\u548c\u591a\u8bed\u8a00\u5185\u5bb9\uff0c\u5f15\u5165\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\uff08\u89c6\u9891\u7ea7\u3001\u7247\u6bb5\u7ea7\u3001\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff09\u548c\u611f\u77e5\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u6570\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "VidText\u586b\u8865\u4e86\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u52a8\u6001\u73af\u5883\u4e2d\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.22765", "pdf": "https://arxiv.org/pdf/2505.22765", "abs": "https://arxiv.org/abs/2505.22765", "authors": ["Iddo Yosha", "Gallil Maimon", "Yossi Adi"], "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86StressTest\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u97f3\u611f\u77e5\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u53e5\u5b50\u91cd\u97f3\u533a\u5206\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f00\u53d1\u4e86\u5408\u6210\u6570\u636e\u96c6Stress17k\u4ee5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u53e5\u5b50\u91cd\u97f3\u5728\u8868\u8fbe\u610f\u56fe\u548c\u542b\u4e49\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5728SLM\u7684\u8bc4\u4f30\u548c\u5f00\u53d1\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u5f15\u5165StressTest\u57fa\u51c6\uff0c\u8bc4\u4f30SLM\u8868\u73b0\uff1b\u63d0\u51fa\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u521b\u5efaStress17k\u6570\u636e\u96c6\uff1b\u4f18\u5316\u6a21\u578bStresSLM\u3002", "result": "\u73b0\u6709SLM\u5728\u91cd\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1bStresSLM\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u4f18\u5316SLM\u80fd\u6709\u6548\u63d0\u5347\u5176\u5728\u53e5\u5b50\u91cd\u97f3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.23708", "pdf": "https://arxiv.org/pdf/2505.23708", "abs": "https://arxiv.org/abs/2505.23708", "authors": ["Lucas N. Alegre", "Agon Serifi", "Ruben Grandia", "David M\u00fcller", "Espen Knoop", "Moritz B\u00e4cher"], "title": "AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning", "categories": ["cs.RO", "cs.GR"], "comment": "SIGGRAPH 2025", "summary": "Reinforcement learning (RL) has significantly advanced the control of\nphysics-based and robotic characters that track kinematic reference motion.\nHowever, methods typically rely on a weighted sum of conflicting reward\nfunctions, requiring extensive tuning to achieve a desired behavior. Due to the\ncomputational cost of RL, this iterative process is a tedious, time-intensive\ntask. Furthermore, for robotics applications, the weights need to be chosen\nsuch that the policy performs well in the real world, despite inevitable\nsim-to-real gaps. To address these challenges, we propose a multi-objective\nreinforcement learning framework that trains a single policy conditioned on a\nset of weights, spanning the Pareto front of reward trade-offs. Within this\nframework, weights can be selected and tuned after training, significantly\nspeeding up iteration time. We demonstrate how this improved workflow can be\nused to perform highly dynamic motions with a robot character. Moreover, we\nexplore how weight-conditioned policies can be leveraged in hierarchical\nsettings, using a high-level policy to dynamically select weights according to\nthe current task. We show that the multi-objective policy encodes a diverse\nspectrum of behaviors, facilitating efficient adaptation to novel tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u6743\u91cd\u6761\u4ef6\u5316\u7684\u7b56\u7565\uff0c\u89e3\u51b3\u4f20\u7edfRL\u4e2d\u5956\u52b1\u51fd\u6570\u6743\u91cd\u8c03\u4f18\u8017\u65f6\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u5176\u5728\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u52a0\u6743\u5956\u52b1\u51fd\u6570\uff0c\u9700\u5927\u91cf\u8c03\u4f18\u4e14\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684sim-to-real\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u4e00\u4e2a\u6743\u91cd\u6761\u4ef6\u5316\u7684\u7b56\u7565\uff0c\u8986\u76d6\u5956\u52b1\u6743\u8861\u7684Pareto\u524d\u6cbf\u3002", "result": "\u8be5\u6846\u67b6\u663e\u8457\u7f29\u77ed\u8fed\u4ee3\u65f6\u95f4\uff0c\u652f\u6301\u52a8\u6001\u6743\u91cd\u9009\u62e9\uff0c\u5e76\u80fd\u9ad8\u6548\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "conclusion": "\u591a\u76ee\u6807\u7b56\u7565\u7f16\u7801\u4e86\u591a\u6837\u884c\u4e3a\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22815", "pdf": "https://arxiv.org/pdf/2505.22815", "abs": "https://arxiv.org/abs/2505.22815", "authors": ["Zhangyi Hu", "Jiemin Wu", "Hua Xu", "Mingqian Liao", "Ninghui Feng", "Bo Gao", "Songning Lai", "Yutao Yue"], "title": "IMTS is Worth Time $\\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Irregular Multivariate Time Series (IMTS) forecasting is challenging due to\nthe unaligned nature of multi-channel signals and the prevalence of extensive\nmissing data. Existing methods struggle to capture reliable temporal patterns\nfrom such data due to significant missing values. While pre-trained foundation\nmodels show potential for addressing these challenges, they are typically\ndesigned for Regularly Sampled Time Series (RTS). Motivated by the visual Mask\nAutoEncoder's (MAE) powerful capability for modeling sparse multi-channel\ninformation and its success in RTS forecasting, we propose VIMTS, a framework\nadapting Visual MAE for IMTS forecasting. To mitigate the effect of missing\nvalues, VIMTS first processes IMTS along the timeline into feature patches at\nequal intervals. These patches are then complemented using learned\ncross-channel dependencies. Then it leverages visual MAE's capability in\nhandling sparse multichannel data for patch reconstruction, followed by a\ncoarse-to-fine technique to generate precise predictions from focused contexts.\nIn addition, we integrate self-supervised learning for improved IMTS modeling\nby adapting the visual MAE to IMTS data. Extensive experiments demonstrate\nVIMTS's superior performance and few-shot capability, advancing the application\nof visual foundation models in more general time series tasks. Our code is\navailable at https://github.com/WHU-HZY/VIMTS.", "AI": {"tldr": "VIMTS\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9MAE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff08IMTS\uff09\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u8865\u5168\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "IMTS\u9884\u6d4b\u56e0\u591a\u901a\u9053\u4fe1\u53f7\u4e0d\u5bf9\u9f50\u548c\u5927\u91cf\u7f3a\u5931\u6570\u636e\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u53ef\u9760\u7684\u65f6\u95f4\u6a21\u5f0f\u3002\u89c6\u89c9MAE\u5728\u7a00\u758f\u591a\u901a\u9053\u6570\u636e\u5904\u7406\u4e0a\u7684\u6f5c\u529b\u6fc0\u53d1\u4e86\u5c06\u5176\u5e94\u7528\u4e8eIMTS\u7684\u52a8\u673a\u3002", "method": "VIMTS\u5c06IMTS\u6cbf\u65f6\u95f4\u7ebf\u5206\u5272\u4e3a\u7b49\u95f4\u9694\u7279\u5f81\u5757\uff0c\u5229\u7528\u8de8\u901a\u9053\u4f9d\u8d56\u8865\u5168\u7f3a\u5931\u503c\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9MAE\u8fdb\u884c\u5757\u91cd\u5efa\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u6280\u672f\u751f\u6210\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVIMTS\u5728\u6027\u80fd\u548c\u5c11\u6837\u672c\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "VIMTS\u6210\u529f\u5c06\u89c6\u89c9MAE\u5e94\u7528\u4e8eIMTS\u9884\u6d4b\uff0c\u4e3a\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.22771", "pdf": "https://arxiv.org/pdf/2505.22771", "abs": "https://arxiv.org/abs/2505.22771", "authors": ["Christopher Ormerod"], "title": "Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, AIME-Con Conference Submission", "summary": "This study illustrates how incorporating feedback-oriented annotations into\nthe scoring pipeline can enhance the accuracy of automated essay scoring (AES).\nThis approach is demonstrated with the Persuasive Essays for Rating, Selecting,\nand Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We\nintegrate two types of feedback-driven annotations: those that identify\nspelling and grammatical errors, and those that highlight argumentative\ncomponents. To illustrate how this method could be applied in real-world\nscenarios, we employ two LLMs to generate annotations -- a generative language\nmodel used for spell-correction and an encoder-based token classifier trained\nto identify and mark argumentative elements. By incorporating annotations into\nthe scoring process, we demonstrate improvements in performance using\nencoder-based large language models fine-tuned as classifiers.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u53cd\u9988\u5bfc\u5411\u7684\u6ce8\u91ca\uff08\u5982\u62fc\u5199\u3001\u8bed\u6cd5\u9519\u8bef\u548c\u8bba\u8bc1\u6210\u5206\u6807\u8bb0\uff09\u6574\u5408\u5230\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bc4\u5206\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5f15\u5165\u53cd\u9988\u9a71\u52a8\u7684\u6ce8\u91ca\u6765\u4f18\u5316\u8bc4\u5206\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528PERSUADE\u8bed\u6599\u5e93\uff0c\u6574\u5408\u62fc\u5199\u3001\u8bed\u6cd5\u9519\u8bef\u548c\u8bba\u8bc1\u6210\u5206\u7684\u6ce8\u91ca\uff0c\u5e76\u5229\u7528\u4e24\u79cdLLM\uff08\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u548c\u7f16\u7801\u5668\u57fa\u7840\u7684\u6807\u8bb0\u5206\u7c7b\u5668\uff09\u751f\u6210\u6ce8\u91ca\u3002", "result": "\u901a\u8fc7\u5c06\u6ce8\u91ca\u6574\u5408\u5230\u8bc4\u5206\u8fc7\u7a0b\u4e2d\uff0c\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u63d0\u5347\u3002", "conclusion": "\u53cd\u9988\u9a71\u52a8\u7684\u6ce8\u91ca\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23738", "pdf": "https://arxiv.org/pdf/2505.23738", "abs": "https://arxiv.org/abs/2505.23738", "authors": ["Xiaojuan Wang", "Aleksander Holynski", "Brian Curless", "Ira Kemelmacher", "Steve Seitz"], "title": "How Animals Dance (When You're Not Looking)", "categories": ["cs.CV", "cs.GR"], "comment": "Project page: https://how-animals-dance.github.io/", "summary": "We present a keyframe-based framework for generating music-synchronized,\nchoreography aware animal dance videos. Starting from a few keyframes\nrepresenting distinct animal poses -- generated via text-to-image prompting or\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\nthe optimal keyframe structure that satisfies a specified choreography pattern\nof beats, which can be automatically estimated from a reference dance video. We\nalso introduce an approach for mirrored pose image generation, essential for\ncapturing symmetry in dance. In-between frames are synthesized using an video\ndiffusion model. With as few as six input keyframes, our method can produce up\nto 30 second dance videos across a wide range of animals and music tracks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u97f3\u4e50\u540c\u6b65\u3001\u821e\u8e48\u611f\u77e5\u7684\u52a8\u7269\u821e\u8e48\u89c6\u9891\uff0c\u901a\u8fc7\u4f18\u5316\u5173\u952e\u5e27\u7ed3\u6784\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u5e27\u3002", "motivation": "\u89e3\u51b3\u4ece\u5c11\u91cf\u5173\u952e\u5e27\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u97f3\u4e50\u540c\u6b65\u7684\u52a8\u7269\u821e\u8e48\u89c6\u9891\u7684\u6311\u6218\uff0c\u5e76\u6355\u6349\u821e\u8e48\u4e2d\u7684\u5bf9\u79f0\u6027\u3002", "method": "\u5c06\u821e\u8e48\u5408\u6210\u5efa\u6a21\u4e3a\u56fe\u4f18\u5316\u95ee\u9898\uff0c\u81ea\u52a8\u4f30\u8ba1\u821e\u8e48\u8282\u62cd\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u5e27\u3002", "result": "\u4ec5\u9700\u516d\u4e2a\u8f93\u5165\u5173\u952e\u5e27\uff0c\u5373\u53ef\u751f\u6210\u957f\u8fbe30\u79d2\u7684\u52a8\u7269\u821e\u8e48\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u52a8\u7269\u548c\u97f3\u4e50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u97f3\u4e50\u540c\u6b65\u821e\u8e48\u89c6\u9891\u3002"}}
{"id": "2505.22850", "pdf": "https://arxiv.org/pdf/2505.22850", "abs": "https://arxiv.org/abs/2505.22850", "authors": ["Kostas Triaridis", "Panagiotis Kaliosis", "E-Ro Nguyen", "Jingyi Xu", "Hieu Le", "Dimitris Samaras"], "title": "Improving Contrastive Learning for Referring Expression Counting", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "Object counting has progressed from class-specific models, which count only\nknown categories, to class-agnostic models that generalize to unseen\ncategories. The next challenge is Referring Expression Counting (REC), where\nthe goal is to count objects based on fine-grained attributes and contextual\ndifferences. Existing methods struggle with distinguishing visually similar\nobjects that belong to the same category but correspond to different referring\nexpressions. To address this, we propose C-REX, a novel contrastive learning\nframework, based on supervised contrastive learning, designed to enhance\ndiscriminative representation learning. Unlike prior works, C-REX operates\nentirely within the image space, avoiding the misalignment issues of image-text\ncontrastive learning, thus providing a more stable contrastive signal. It also\nguarantees a significantly larger pool of negative samples, leading to improved\nrobustness in the learned representations. Moreover, we showcase that our\nframework is versatile and generic enough to be applied to other similar tasks\nlike class-agnostic counting. To support our approach, we analyze the key\ncomponents of sota detection-based models and identify that detecting object\ncentroids instead of bounding boxes is the key common factor behind their\nsuccess in counting tasks. We use this insight to design a simple yet effective\ndetection-based baseline to build upon. Our experiments show that C-REX\nachieves state-of-the-art results in REC, outperforming previous methods by\nmore than 22\\% in MAE and more than 10\\% in RMSE, while also demonstrating\nstrong performance in class-agnostic counting. Code is available at\nhttps://github.com/cvlab-stonybrook/c-rex.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faC-REX\uff0c\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3Referring Expression Counting\uff08REC\uff09\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u5c5e\u4e8e\u4e0d\u540c\u6307\u4ee3\u8868\u8fbe\u7684\u5bf9\u8c61\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faC-REX\uff0c\u57fa\u4e8e\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b8c\u5168\u5728\u56fe\u50cf\u7a7a\u95f4\u64cd\u4f5c\uff0c\u907f\u514d\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u66f4\u5927\u7684\u8d1f\u6837\u672c\u6c60\u3002", "result": "C-REX\u5728REC\u4efb\u52a1\u4e2dMAE\u63d0\u534722%\uff0cRMSE\u63d0\u534710%\uff0c\u540c\u65f6\u5728\u7c7b\u65e0\u5173\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "C-REX\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8eREC\u53ca\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\uff0c\u901a\u8fc7\u68c0\u6d4b\u5bf9\u8c61\u4e2d\u5fc3\u70b9\u800c\u975e\u8fb9\u754c\u6846\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2505.22774", "pdf": "https://arxiv.org/pdf/2505.22774", "abs": "https://arxiv.org/abs/2505.22774", "authors": ["Kaja Dobrovoljc"], "title": "Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a novel treebank-driven approach to comparing syntactic\nstructures in speech and writing using dependency-parsed corpora. Adopting a\nfully inductive, bottom-up method, we define syntactic structures as\ndelexicalized dependency (sub)trees and extract them from spoken and written\nUniversal Dependencies (UD) treebanks in two syntactically distinct languages,\nEnglish and Slovenian. For each corpus, we analyze the size, diversity, and\ndistribution of syntactic inventories, their overlap across modalities, and the\nstructures most characteristic of speech. Results show that, across both\nlanguages, spoken corpora contain fewer and less diverse syntactic structures\nthan their written counterparts, with consistent cross-linguistic preferences\nfor certain structural types across modalities. Strikingly, the overlap between\nspoken and written syntactic inventories is very limited: most structures\nattested in speech do not occur in writing, pointing to modality-specific\npreferences in syntactic organization that reflect the distinct demands of\nreal-time interaction and elaborated writing. This contrast is further\nsupported by a keyness analysis of the most frequent speech-specific\nstructures, which highlights patterns associated with interactivity,\ncontext-grounding, and economy of expression. We argue that this scalable,\nlanguage-independent framework offers a useful general method for\nsystematically studying syntactic variation across corpora, laying the\ngroundwork for more comprehensive data-driven theories of grammar in use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5e93\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f9d\u8d56\u89e3\u6790\u8bed\u6599\u5e93\u6bd4\u8f83\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u53e5\u6cd5\u591a\u6837\u6027\u3001\u5206\u5e03\u548c\u6a21\u6001\u7279\u5f02\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u5728\u53e5\u6cd5\u7ed3\u6784\u4e0a\u7684\u5dee\u5f02\uff0c\u4ee5\u7406\u89e3\u4e0d\u540c\u6a21\u6001\u5bf9\u53e5\u6cd5\u7ec4\u7ec7\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u5f52\u7eb3\u65b9\u6cd5\uff0c\u4ece\u82f1\u8bed\u548c\u65af\u6d1b\u6587\u5c3c\u4e9a\u8bed\u7684\u901a\u7528\u4f9d\u8d56\u6811\u5e93\u4e2d\u63d0\u53d6\u53bb\u8bcd\u6c47\u5316\u7684\u4f9d\u8d56\u5b50\u6811\uff0c\u5206\u6790\u5176\u5927\u5c0f\u3001\u591a\u6837\u6027\u548c\u5206\u5e03\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u53e3\u8bed\u8bed\u6599\u5e93\u7684\u53e5\u6cd5\u7ed3\u6784\u66f4\u5c11\u4e14\u591a\u6837\u6027\u66f4\u4f4e\uff0c\u4e14\u53e3\u8bed\u548c\u4e66\u9762\u8bed\u7684\u53e5\u6cd5\u7ed3\u6784\u91cd\u53e0\u6709\u9650\uff0c\u8868\u660e\u6a21\u6001\u7279\u5f02\u6027\u504f\u597d\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u8fd9\u79cd\u53ef\u6269\u5c55\u7684\u3001\u8bed\u8a00\u65e0\u5173\u7684\u6846\u67b6\u4e3a\u7cfb\u7edf\u7814\u7a76\u8bed\u6599\u5e93\u95f4\u7684\u53e5\u6cd5\u53d8\u5f02\u63d0\u4f9b\u4e86\u901a\u7528\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8e\u6570\u636e\u7684\u8bed\u6cd5\u7406\u8bba\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.23740", "pdf": "https://arxiv.org/pdf/2505.23740", "abs": "https://arxiv.org/abs/2505.23740", "authors": ["Ronghuan Wu", "Wanchao Su", "Jing Liao"], "title": "LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://layerpeeler.github.io/", "summary": "Image vectorization is a powerful technique that converts raster images into\nvector graphics, enabling enhanced flexibility and interactivity. However,\npopular image vectorization tools struggle with occluded regions, producing\nincomplete or fragmented shapes that hinder editability. While recent\nadvancements have explored rule-based and data-driven layer-wise image\nvectorization, these methods face limitations in vectorization quality and\nflexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image\nvectorization approach that addresses these challenges through a progressive\nsimplification paradigm. The key to LayerPeeler's success lies in its\nautoregressive peeling strategy: by identifying and removing the topmost\nnon-occluded layers while recovering underlying content, we generate vector\ngraphics with complete paths and coherent layer structures. Our method\nleverages vision-language models to construct a layer graph that captures\nocclusion relationships among elements, enabling precise detection and\ndescription for non-occluded layers. These descriptive captions are used as\nediting instructions for a finetuned image diffusion model to remove the\nidentified layers. To ensure accurate removal, we employ localized attention\ncontrol that precisely guides the model to target regions while faithfully\npreserving the surrounding content. To support this, we contribute a\nlarge-scale dataset specifically designed for layer peeling tasks. Extensive\nquantitative and qualitative experiments demonstrate that LayerPeeler\nsignificantly outperforms existing techniques, producing vectorization results\nwith superior path semantics, geometric regularity, and visual fidelity.", "AI": {"tldr": "LayerPeeler\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5c42\u56fe\u50cf\u77e2\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u7b80\u5316\u7b56\u7565\u89e3\u51b3\u906e\u6321\u533a\u57df\u7684\u6311\u6218\uff0c\u751f\u6210\u5b8c\u6574\u8def\u5f84\u548c\u8fde\u8d2f\u5c42\u7ed3\u6784\u7684\u77e2\u91cf\u56fe\u5f62\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u77e2\u91cf\u5316\u5de5\u5177\u5728\u906e\u6321\u533a\u57df\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u5f62\u72b6\u4e0d\u5b8c\u6574\u6216\u788e\u7247\u5316\uff0c\u5f71\u54cd\u53ef\u7f16\u8f91\u6027\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u5265\u79bb\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u5c42\u56fe\uff0c\u5229\u7528\u5fae\u8c03\u56fe\u50cf\u6269\u6563\u6a21\u578b\u79fb\u9664\u906e\u6321\u5c42\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u6ce8\u610f\u529b\u63a7\u5236\u786e\u4fdd\u7cbe\u786e\u79fb\u9664\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLayerPeeler\u5728\u8def\u5f84\u8bed\u4e49\u3001\u51e0\u4f55\u89c4\u5219\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "LayerPeeler\u4e3a\u56fe\u50cf\u77e2\u91cf\u5316\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5904\u7406\u906e\u6321\u533a\u57df\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.22854", "pdf": "https://arxiv.org/pdf/2505.22854", "abs": "https://arxiv.org/abs/2505.22854", "authors": ["Kornel Howil", "Joanna Waczy\u0144ska", "Piotr Borycki", "Tadeusz Dziarmaga", "Marcin Mazur", "Przemys\u0142aw Spurek"], "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has recently emerged as an efficient representation\nfor rendering 3D scenes from 2D images and has been extended to images, videos,\nand dynamic 4D content. However, applying style transfer to GS-based\nrepresentations, especially beyond simple color changes, remains challenging.\nIn this work, we introduce CLIPGaussians, the first unified style transfer\nframework that supports text- and image-guided stylization across multiple\nmodalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates\ndirectly on Gaussian primitives and integrates into existing GS pipelines as a\nplug-in module, without requiring large generative models or retraining from\nscratch. CLIPGaussians approach enables joint optimization of color and\ngeometry in 3D and 4D settings, and achieves temporal coherence in videos,\nwhile preserving a model size. We demonstrate superior style fidelity and\nconsistency across all tasks, validating CLIPGaussians as a universal and\nefficient solution for multimodal style transfer.", "AI": {"tldr": "CLIPGaussians \u662f\u4e00\u79cd\u652f\u6301\u591a\u6a21\u6001\uff082D\u56fe\u50cf\u3001\u89c6\u9891\u30013D\u5bf9\u8c61\u30014D\u573a\u666f\uff09\u98ce\u683c\u8fc1\u79fb\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\uff0c\u65e0\u9700\u5927\u578b\u751f\u6210\u6a21\u578b\u6216\u4ece\u5934\u8bad\u7ec3\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u5728\u6e32\u67d33D\u573a\u666f\u65b9\u9762\u8868\u73b0\u9ad8\u6548\uff0c\u4f46\u98ce\u683c\u8fc1\u79fb\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u8d85\u8d8a\u7b80\u5355\u989c\u8272\u53d8\u5316\u7684\u5e94\u7528\u3002", "method": "CLIPGaussians \u76f4\u63a5\u64cd\u4f5c\u9ad8\u65af\u57fa\u5143\uff0c\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709GS\u6d41\u7a0b\u4e2d\uff0c\u8054\u5408\u4f18\u53163D\u548c4D\u7684\u989c\u8272\u4e0e\u51e0\u4f55\uff0c\u5e76\u4fdd\u6301\u89c6\u9891\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\uff0c\u6a21\u578b\u5c3a\u5bf8\u4fdd\u6301\u4e0d\u53d8\u3002", "conclusion": "CLIPGaussians \u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u98ce\u683c\u8fc1\u79fb\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22777", "pdf": "https://arxiv.org/pdf/2505.22777", "abs": "https://arxiv.org/abs/2505.22777", "authors": ["John Mendon\u00e7a", "Alon Lavie", "Isabel Trancoso"], "title": "MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators", "categories": ["cs.CL"], "comment": "May ARR", "summary": "As the capabilities of chatbots and their underlying LLMs continue to\ndramatically improve, evaluating their performance has increasingly become a\nmajor blocker to their further development. A major challenge is the available\nbenchmarking datasets, which are largely static, outdated, and lacking in\nmultilingual coverage, limiting their ability to capture subtle linguistic and\ncultural variations. This paper introduces MEDAL, an automated multi-agent\nframework for generating, evaluating, and curating more representative and\ndiverse open-domain dialogue evaluation benchmarks. Our approach leverages\nseveral state-of-the-art LLMs to generate user-chatbot multilingual dialogues,\nconditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a\nmultidimensional analysis of the performance of the chatbots, uncovering\nnoticeable cross-lingual performance differences. Guided by this large-scale\nevaluation, we curate a new meta-evaluation multilingual benchmark and\nhuman-annotate samples with nuanced quality judgments. This benchmark is then\nused to assess the ability of several reasoning and non-reasoning LLMs to act\nas evaluators of open-domain dialogues. We find that current LLMs struggle to\ndetect nuanced issues, particularly those involving empathy and reasoning.", "AI": {"tldr": "MEDAL\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u66f4\u5177\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u7684\u5f00\u653e\u57df\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u9759\u6001\u3001\u8fc7\u65f6\u548c\u7f3a\u4e4f\u591a\u8bed\u8a00\u8986\u76d6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u804a\u5929\u673a\u5668\u4eba\u548cLLM\u7684\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6\u591a\u4e3a\u9759\u6001\u3001\u8fc7\u65f6\u4e14\u7f3a\u4e4f\u591a\u8bed\u8a00\u8986\u76d6\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u7684\u8bed\u8a00\u548c\u6587\u5316\u5dee\u5f02\uff0c\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528\u591a\u4e2a\u5148\u8fdbLLM\u751f\u6210\u591a\u8bed\u8a00\u7528\u6237-\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\uff0c\u57fa\u4e8e\u591a\u6837\u5316\u7684\u79cd\u5b50\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7GPT-4.1\u8fdb\u884c\u591a\u7ef4\u6027\u80fd\u5206\u6790\uff0c\u6700\u7ec8\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u5143\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u53d1\u73b0\u5f53\u524dLLM\u5728\u68c0\u6d4b\u7ec6\u5fae\u95ee\u9898\uff08\u5982\u540c\u7406\u5fc3\u548c\u63a8\u7406\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u663e\u8457\u7684\u8de8\u8bed\u8a00\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MEDAL\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f46\u5f53\u524dLLM\u5728\u8bc4\u4f30\u5f00\u653e\u57df\u5bf9\u8bdd\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2505.22855", "pdf": "https://arxiv.org/pdf/2505.22855", "abs": "https://arxiv.org/abs/2505.22855", "authors": ["Ruining Deng", "Junchao Zhu", "Juming Xiong", "Can Cui", "Tianyuan Yao", "Junlin Guo", "Siqi Lu", "Marilyn Lionts", "Mengmeng Yin", "Yu Wang", "Shilin Zhao", "Yucheng Tang", "Yihe Yang", "Paul Dennis Simonson", "Mert R. Sabuncu", "Haichun Yang", "Yuankai Huo"], "title": "IRS: Incremental Relationship-guided Segmentation for Digital Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Continual learning is rapidly emerging as a key focus in computer vision,\naiming to develop AI systems capable of continuous improvement, thereby\nenhancing their value and practicality in diverse real-world applications. In\nhealthcare, continual learning holds great promise for continuously acquired\ndigital pathology data, which is collected in hospitals on a daily basis.\nHowever, panoramic segmentation on digital whole slide images (WSIs) presents\nsignificant challenges, as it is often infeasible to obtain comprehensive\nannotations for all potential objects, spanning from coarse structures (e.g.,\nregions and unit objects) to fine structures (e.g., cells). This results in\ntemporally and partially annotated data, posing a major challenge in developing\na holistic segmentation framework. Moreover, an ideal segmentation model should\nincorporate new phenotypes, unseen diseases, and diverse populations, making\nthis task even more complex. In this paper, we introduce a novel and unified\nIncremental Relationship-guided Segmentation (IRS) learning scheme to address\ntemporally acquired, partially annotated data while maintaining\nout-of-distribution (OOD) continual learning capacity in digital pathology. The\nkey innovation of IRS lies in its ability to realize a new spatial-temporal OOD\ncontinual learning paradigm by mathematically modeling anatomical relationships\nbetween existing and newly introduced classes through a simple incremental\nuniversal proposition matrix. Experimental results demonstrate that the IRS\nmethod effectively handles the multi-scale nature of pathological segmentation,\nenabling precise kidney segmentation across various structures (regions, units,\nand cells) as well as OOD disease lesions at multiple magnifications. This\ncapability significantly enhances domain generalization, making IRS a robust\napproach for real-world digital pathology applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u589e\u91cf\u5173\u7cfb\u5f15\u5bfc\u5206\u5272\uff08IRS\uff09\u5b66\u4e60\u65b9\u6848\uff0c\u7528\u4e8e\u5904\u7406\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u65f6\u95f4\u83b7\u53d6\u3001\u90e8\u5206\u6807\u6ce8\u7684\u6570\u636e\uff0c\u5e76\u4fdd\u6301\u5206\u5e03\u5916\uff08OOD\uff09\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u5168\u666f\u5206\u5272\u9762\u4e34\u6807\u6ce8\u4e0d\u5b8c\u6574\u548c\u6301\u7eed\u5b66\u4e60\u65b0\u7c7b\u522b\u7684\u6311\u6218\uff0cIRS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "IRS\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u89e3\u5256\u5173\u7cfb\uff0c\u5229\u7528\u589e\u91cf\u901a\u7528\u547d\u9898\u77e9\u9635\u5b9e\u73b0\u7a7a\u95f4-\u65f6\u95f4OOD\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIRS\u80fd\u6709\u6548\u5904\u7406\u591a\u5c3a\u5ea6\u75c5\u7406\u5206\u5272\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u80be\u810f\u5206\u5272\u548cOOD\u75c5\u53d8\u8bc6\u522b\u3002", "conclusion": "IRS\u663e\u8457\u589e\u5f3a\u4e86\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u6570\u5b57\u75c5\u7406\u5b66\u5e94\u7528\u3002"}}
{"id": "2505.22787", "pdf": "https://arxiv.org/pdf/2505.22787", "abs": "https://arxiv.org/abs/2505.22787", "authors": ["Christopher Polzak", "Alejandro Lozano", "Min Woo Sun", "James Burgess", "Yuhui Zhang", "Kevin Wu", "Serena Yeung-Levy"], "title": "Can Large Language Models Match the Conclusions of Systematic Reviews?", "categories": ["cs.CL"], "comment": null, "summary": "Systematic reviews (SR), in which experts summarize and analyze evidence\nacross individual studies to provide insights on a specialized topic, are a\ncornerstone for evidence-based clinical decision-making, research, and policy.\nGiven the exponential growth of scientific articles, there is growing interest\nin using large language models (LLMs) to automate SR generation. However, the\nability of LLMs to critically assess evidence and reason across multiple\ndocuments to provide recommendations at the same proficiency as domain experts\nremains poorly characterized. We therefore ask: Can LLMs match the conclusions\nof systematic reviews written by clinical experts when given access to the same\nstudies? To explore this question, we present MedEvidence, a benchmark pairing\nfindings from 100 SRs with the studies they are based on. We benchmark 24 LLMs\non MedEvidence, including reasoning, non-reasoning, medical specialist, and\nmodels across varying sizes (from 7B-700B). Through our systematic evaluation,\nwe find that reasoning does not necessarily improve performance, larger models\ndo not consistently yield greater gains, and knowledge-based fine-tuning\ndegrades accuracy on MedEvidence. Instead, most models exhibit similar\nbehavior: performance tends to degrade as token length increases, their\nresponses show overconfidence, and, contrary to human experts, all models show\na lack of scientific skepticism toward low-quality findings. These results\nsuggest that more work is still required before LLMs can reliably match the\nobservations from expert-conducted SRs, even though these systems are already\ndeployed and being used by clinicians. We release our codebase and benchmark to\nthe broader research community to further investigate LLM-based SR systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u751f\u6210\u7cfb\u7edf\u7efc\u8ff0\uff08SR\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLMs\u5728\u8bc1\u636e\u8bc4\u4f30\u548c\u591a\u6587\u6863\u63a8\u7406\u65b9\u9762\u4ecd\u65e0\u6cd5\u5339\u654c\u4e34\u5e8a\u4e13\u5bb6\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u6587\u732e\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u5229\u7528LLMs\u81ea\u52a8\u5316\u751f\u6210SR\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5176\u80fd\u529b\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86MedEvidence\u57fa\u51c6\uff0c\u5bf9\u6bd4\u4e8624\u79cdLLMs\u5728100\u4e2aSR\u4e0a\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u63a8\u7406\u80fd\u529b\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5fae\u8c03\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u672a\u5fc5\u63d0\u5347\u8868\u73b0\uff0c\u6a21\u578b\u5927\u5c0f\u4e0d\u603b\u662f\u5e26\u6765\u589e\u76ca\uff0c\u77e5\u8bc6\u5fae\u8c03\u53cd\u800c\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u4e14LLMs\u666e\u904d\u7f3a\u4e4f\u79d1\u5b66\u6000\u7591\u6001\u5ea6\u3002", "conclusion": "\u5f53\u524dLLMs\u5c1a\u65e0\u6cd5\u53ef\u9760\u5339\u914d\u4e13\u5bb6\u751f\u6210\u7684SR\u7ed3\u8bba\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6539\u8fdb\u3002"}}
{"id": "2505.22858", "pdf": "https://arxiv.org/pdf/2505.22858", "abs": "https://arxiv.org/abs/2505.22858", "authors": ["Sanjoy Kundu", "Shanmukha Vellamcheti", "Sathyanarayanan N. Aakur"], "title": "A Probabilistic Jump-Diffusion Framework for Open-World Egocentric Activity Recognition", "categories": ["cs.CV"], "comment": "Extended abstract of arXiv:2504.03948 for CVPR 2025 EgoVis Workshop", "summary": "Open-world egocentric activity recognition poses a fundamental challenge due\nto its unconstrained nature, requiring models to infer unseen activities from\nan expansive, partially observed search space. We introduce ProbRes, a\nProbabilistic Residual search framework based on jump-diffusion that\nefficiently navigates this space by balancing prior-guided exploration with\nlikelihood-driven exploitation. Our approach integrates structured commonsense\npriors to construct a semantically coherent search space, adaptively refines\npredictions using Vision-Language Models (VLMs) and employs a stochastic search\nmechanism to locate high-likelihood activity labels while minimizing exhaustive\nenumeration efficiently. We systematically evaluate ProbRes across multiple\nopenness levels (L0--L3), demonstrating its adaptability to increasing search\nspace complexity. In addition to achieving state-of-the-art performance on\nbenchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we\nestablish a clear taxonomy for open-world recognition, delineating the\nchallenges and methodological advancements necessary for egocentric activity\nunderstanding.", "AI": {"tldr": "ProbRes\u6846\u67b6\u901a\u8fc7\u6982\u7387\u6b8b\u5dee\u641c\u7d22\u548c\u8df3\u8dc3\u6269\u6563\u65b9\u6cd5\uff0c\u9ad8\u6548\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u81ea\u6211\u4e2d\u5fc3\u6d3b\u52a8\u8bc6\u522b\u7684\u6311\u6218\uff0c\u7ed3\u5408\u5e38\u8bc6\u5148\u9a8c\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u4e16\u754c\u81ea\u6211\u4e2d\u5fc3\u6d3b\u52a8\u8bc6\u522b\u7684\u65e0\u7ea6\u675f\u6027\u5bfc\u81f4\u6a21\u578b\u9700\u4ece\u90e8\u5206\u89c2\u5bdf\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u63a8\u65ad\u672a\u89c1\u6d3b\u52a8\uff0c\u4e9f\u9700\u9ad8\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51faProbRes\u6846\u67b6\uff0c\u57fa\u4e8e\u8df3\u8dc3\u6269\u6563\u7684\u6982\u7387\u6b8b\u5dee\u641c\u7d22\uff0c\u7ed3\u5408\u5e38\u8bc6\u5148\u9a8c\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u9002\u5e94\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08GTEA Gaze\u7b49\uff09\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u5f00\u653e\u4e16\u754c\u8bc6\u522b\u7684\u5206\u7c7b\u4f53\u7cfb\u3002", "conclusion": "ProbRes\u4e3a\u5f00\u653e\u4e16\u754c\u81ea\u6211\u4e2d\u5fc3\u6d3b\u52a8\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6cd5\uff0c\u5e76\u660e\u786e\u4e86\u76f8\u5173\u6311\u6218\u548c\u65b9\u6cd5\u8fdb\u5c55\u3002"}}
{"id": "2505.22801", "pdf": "https://arxiv.org/pdf/2505.22801", "abs": "https://arxiv.org/abs/2505.22801", "authors": ["Qing Wang", "Yuepei Li", "Qiao Qiao", "Kang Zhou", "Qi Li"], "title": "Towards a More Generalized Approach in Open Relation Extraction", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Open Relation Extraction (OpenRE) seeks to identify and extract novel\nrelational facts between named entities from unlabeled data without pre-defined\nrelation schemas. Traditional OpenRE methods typically assume that the\nunlabeled data consists solely of novel relations or is pre-divided into known\nand novel instances. However, in real-world scenarios, novel relations are\narbitrarily distributed. In this paper, we propose a generalized OpenRE setting\nthat considers unlabeled data as a mixture of both known and novel instances.\nTo address this, we propose MixORE, a two-phase framework that integrates\nrelation classification and clustering to jointly learn known and novel\nrelations. Experiments on three benchmark datasets demonstrate that MixORE\nconsistently outperforms competitive baselines in known relation classification\nand novel relation clustering. Our findings contribute to the advancement of\ngeneralized OpenRE research and real-world applications.", "AI": {"tldr": "MixORE\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5df2\u77e5\u548c\u672a\u77e5\u5173\u7cfb\u6df7\u5408\u7684\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u8054\u5408\u5b66\u4e60\u5173\u7cfb\u5206\u7c7b\u548c\u805a\u7c7b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u672a\u77e5\u5173\u7cfb\u662f\u968f\u673a\u5206\u5e03\u7684\uff0c\u800c\u4f20\u7edfOpenRE\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u4ec5\u5305\u542b\u672a\u77e5\u5173\u7cfb\u6216\u5df2\u9884\u5148\u5212\u5206\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51faMixORE\u6846\u67b6\uff0c\u7ed3\u5408\u5173\u7cfb\u5206\u7c7b\u548c\u805a\u7c7b\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u5173\u7cfb\u7684\u6df7\u5408\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMixORE\u5728\u5df2\u77e5\u5173\u7cfb\u5206\u7c7b\u548c\u672a\u77e5\u5173\u7cfb\u805a\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MixORE\u4e3a\u5e7f\u4e49OpenRE\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22859", "pdf": "https://arxiv.org/pdf/2505.22859", "abs": "https://arxiv.org/abs/2505.22859", "authors": ["Hidenobu Matsuki", "Gwangbin Bae", "Andrew J. Davison"], "title": "4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page: https://muskie82.github.io/4dtam/", "summary": "We propose the first 4D tracking and mapping method that jointly performs\ncamera localization and non-rigid surface reconstruction via differentiable\nrendering. Our approach captures 4D scenes from an online stream of color\nimages with depth measurements or predictions by jointly optimizing scene\ngeometry, appearance, dynamics, and camera ego-motion. Although natural\nenvironments exhibit complex non-rigid motions, 4D-SLAM remains relatively\nunderexplored due to its inherent challenges; even with 2.5D signals, the\nproblem is ill-posed because of the high dimensionality of the optimization\nspace. To overcome these challenges, we first introduce a SLAM method based on\nGaussian surface primitives that leverages depth signals more effectively than\n3D Gaussians, thereby achieving accurate surface reconstruction. To further\nmodel non-rigid deformations, we employ a warp-field represented by a\nmulti-layer perceptron (MLP) and introduce a novel camera pose estimation\ntechnique along with surface regularization terms that facilitate\nspatio-temporal reconstruction. In addition to these algorithmic challenges, a\nsignificant hurdle in 4D SLAM research is the lack of reliable ground truth and\nevaluation protocols, primarily due to the difficulty of 4D capture using\ncommodity sensors. To address this, we present a novel open synthetic dataset\nof everyday objects with diverse motions, leveraging large-scale object models\nand animation modeling. In summary, we open up the modern 4D-SLAM research by\nintroducing a novel method and evaluation protocols grounded in modern vision\nand rendering techniques.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u76844D\u8ddf\u8e2a\u4e0e\u5efa\u56fe\u65b9\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u76f8\u673a\u5b9a\u4f4d\u4e0e\u975e\u521a\u6027\u8868\u9762\u91cd\u5efa\uff0c\u89e3\u51b34D-SLAM\u7684\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u73af\u5883\u4e2d\u7684\u590d\u6742\u975e\u521a\u6027\u8fd0\u52a8\u4f7f4D-SLAM\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u53ef\u9760\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u7ed3\u5408\u9ad8\u65af\u8868\u9762\u57fa\u5143\u548cMLP\u53d8\u5f62\u573a\uff0c\u63d0\u51fa\u65b0\u578b\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u4e0e\u8868\u9762\u6b63\u5219\u5316\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u7cbe\u786e\u8868\u9762\u91cd\u5efa\uff0c\u5e76\u53d1\u5e03\u5f00\u6e90\u5408\u6210\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8bc4\u4f30\u3002", "conclusion": "\u4e3a\u73b0\u4ee34D-SLAM\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u63d0\u4f9b\u65b0\u65b9\u6cd5\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2505.22809", "pdf": "https://arxiv.org/pdf/2505.22809", "abs": "https://arxiv.org/abs/2505.22809", "authors": ["Andrew Zhu", "Evan Osgood", "Chris Callison-Burch"], "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures. In submission at EMNLP 2025", "summary": "Much work has been done on conversational LLM agents which directly assist\nhuman users with tasks. We present an alternative paradigm for interacting with\nLLM agents, which we call \"overhearing agents\". These overhearing agents do not\nactively participate in conversation -- instead, they \"listen in\" on\nhuman-to-human conversations and perform background tasks or provide\nsuggestions to assist the user. In this work, we explore the overhearing agents\nparadigm through the lens of Dungeons & Dragons gameplay. We present an\nin-depth study using large multimodal audio-language models as overhearing\nagents to assist a Dungeon Master. We perform a human evaluation to examine the\nhelpfulness of such agents and find that some large audio-language models have\nthe emergent ability to perform overhearing agent tasks using implicit audio\ncues. Finally, we release Python libraries and our project code to support\nfurther research into the overhearing agents paradigm at\nhttps://github.com/zhudotexe/overhearing_agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u65c1\u542c\u4ee3\u7406\u201d\u7684\u65b0\u578bLLM\u4ea4\u4e92\u8303\u5f0f\uff0c\u901a\u8fc7\u76d1\u542c\u4eba\u7c7b\u5bf9\u8bdd\u63d0\u4f9b\u80cc\u666f\u4efb\u52a1\u652f\u6301\uff0c\u5e76\u4ee5\u300a\u9f99\u4e0e\u5730\u4e0b\u57ce\u300b\u6e38\u620f\u4e3a\u4f8b\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u975e\u76f4\u63a5\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u76d1\u542c\u4eba\u7c7b\u5bf9\u8bdd\u63d0\u4f9b\u8f85\u52a9\u529f\u80fd\u3002", "method": "\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65c1\u542c\u4ee3\u7406\uff0c\u8f85\u52a9\u6e38\u620f\u4e3b\u6301\u4eba\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u68c0\u9a8c\u5176\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5229\u7528\u9690\u5f0f\u97f3\u9891\u7ebf\u7d22\u5b8c\u6210\u65c1\u542c\u4ee3\u7406\u4efb\u52a1\u3002", "conclusion": "\u65c1\u542c\u4ee3\u7406\u8303\u5f0f\u5177\u6709\u6f5c\u529b\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u76f8\u5173\u5de5\u5177\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2505.22869", "pdf": "https://arxiv.org/pdf/2505.22869", "abs": "https://arxiv.org/abs/2505.22869", "authors": ["Junbo Yin", "Chao Zha", "Wenjia He", "Chencheng Xu", "Xin Gao"], "title": "CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models", "categories": ["cs.CV", "cs.LG", "q-bio.BM"], "comment": "Accepted at ICML 2025. Code is available at\n  https://github.com/yinjunbo/cfpgen", "summary": "Existing PLMs generate protein sequences based on a single-condition\nconstraint from a specific modality, struggling to simultaneously satisfy\nmultiple constraints across different modalities. In this work, we introduce\nCFP-Gen, a novel diffusion language model for Combinatorial Functional Protein\nGENeration. CFP-Gen facilitates the de novo protein design by integrating\nmultimodal conditions with functional, sequence, and structural constraints.\nSpecifically, an Annotation-Guided Feature Modulation (AGFM) module is\nintroduced to dynamically adjust the protein feature distribution based on\ncomposable functional annotations, e.g., GO terms, IPR domains and EC numbers.\nMeanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures\nresidue-wise interaction to ensure more precise control. Additionally,\noff-the-shelf 3D structure encoders can be seamlessly integrated to impose\ngeometric constraints. We demonstrate that CFP-Gen enables high-throughput\ngeneration of novel proteins with functionality comparable to natural proteins,\nwhile achieving a high success rate in designing multifunctional proteins. Code\nand data available at https://github.com/yinjunbo/cfpgen.", "AI": {"tldr": "CFP-Gen\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u7ec4\u5408\u529f\u80fd\u86cb\u767d\u8d28\u751f\u6210\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6761\u4ef6\u5b9e\u73b0\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709PLM\u4ec5\u57fa\u4e8e\u5355\u4e00\u6a21\u6001\u6761\u4ef6\u751f\u6210\u86cb\u767d\u8d28\u5e8f\u5217\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u591a\u6a21\u6001\u7ea6\u675f\u3002", "method": "\u5f15\u5165AGFM\u6a21\u5757\u52a8\u6001\u8c03\u6574\u86cb\u767d\u8d28\u7279\u5f81\u5206\u5e03\uff0cRCFE\u6a21\u5757\u6355\u83b7\u6b8b\u57fa\u4ea4\u4e92\uff0c\u5e76\u96c6\u62103D\u7ed3\u6784\u7f16\u7801\u5668\u65bd\u52a0\u51e0\u4f55\u7ea6\u675f\u3002", "result": "CFP-Gen\u80fd\u9ad8\u6548\u751f\u6210\u529f\u80fd\u4e0e\u5929\u7136\u86cb\u767d\u8d28\u76f8\u5f53\u7684\u65b0\u578b\u86cb\u767d\u8d28\uff0c\u4e14\u591a\u529f\u80fd\u86cb\u767d\u8d28\u8bbe\u8ba1\u6210\u529f\u7387\u9ad8\u3002", "conclusion": "CFP-Gen\u4e3a\u591a\u6a21\u6001\u7ea6\u675f\u4e0b\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22823", "pdf": "https://arxiv.org/pdf/2505.22823", "abs": "https://arxiv.org/abs/2505.22823", "authors": ["Yingming Wang", "Pepa Atanasova"], "title": "Self-Critique and Refinement for Faithful Natural Language Explanations", "categories": ["cs.CL"], "comment": "21 pages, 10 figures, 14 tables", "summary": "With the rapid development of large language models (LLMs), natural language\nexplanations (NLEs) have become increasingly important for understanding model\npredictions. However, these explanations often fail to faithfully represent the\nmodel's actual reasoning process. While existing work has demonstrated that\nLLMs can self-critique and refine their initial outputs for various tasks, this\ncapability remains unexplored for improving explanation faithfulness. To\naddress this gap, we introduce Self-critique and Refinement for Natural\nLanguage Explanations (SR-NLE), a framework that enables models to improve the\nfaithfulness of their own explanations -- specifically, post-hoc NLEs --\nthrough an iterative critique and refinement process without external\nsupervision. Our framework leverages different feedback mechanisms to guide the\nrefinement process, including natural language self-feedback and, notably, a\nnovel feedback approach based on feature attribution that highlights important\ninput words. Our experiments across three datasets and four state-of-the-art\nLLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with\nour best method achieving an average unfaithfulness rate of 36.02%, compared to\n54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal\nthat the investigated LLMs can indeed refine their explanations to better\nreflect their actual reasoning process, requiring only appropriate guidance\nthrough feedback without additional training or fine-tuning.", "AI": {"tldr": "SR-NLE\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u6279\u5224\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08NLEs\uff09\u5e38\u65e0\u6cd5\u5fe0\u5b9e\u53cd\u6620\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u6539\u8fdb\u5176\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u3002", "method": "\u63d0\u51faSR-NLE\u6846\u67b6\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u81ea\u6211\u53cd\u9988\u548c\u57fa\u4e8e\u7279\u5f81\u5f52\u56e0\u7684\u65b0\u53cd\u9988\u673a\u5236\uff0c\u8fed\u4ee3\u4f18\u5316\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSR-NLE\u663e\u8457\u964d\u4f4e\u4e0d\u5fe0\u5b9e\u7387\uff0c\u6700\u4f73\u65b9\u6cd5\u5e73\u5747\u4e0d\u5fe0\u5b9e\u7387\u964d\u81f336.02%\uff08\u57fa\u7ebf\u4e3a54.81%\uff09\u3002", "conclusion": "LLMs\u53ef\u901a\u8fc7\u9002\u5f53\u53cd\u9988\u4f18\u5316\u89e3\u91ca\u5fe0\u5b9e\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u3002"}}
{"id": "2505.22908", "pdf": "https://arxiv.org/pdf/2505.22908", "abs": "https://arxiv.org/abs/2505.22908", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "3DGS Compression with Sparsity-guided Hierarchical Transform Coding", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its fast and\nhigh-quality rendering, but it has a very large memory footprint incurring high\ntransmission and storage overhead. Recently, some neural compression methods,\nsuch as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach\nof end-to-end optimized analysis-synthesis transforms which has been proven\nhighly effective in neural signal compression. Without an appropriate analysis\ntransform, signal correlations cannot be removed by sparse representation.\nWithout such transforms the only way to remove signal redundancies is through\nentropy coding driven by a complex and expensive context modeling, which\nresults in slower speed and suboptimal rate-distortion (R-D) performance. To\novercome this weakness, we propose Sparsity-guided Hierarchical Transform\nCoding (SHTC), the first end-to-end optimized transform coding framework for\n3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight\ncontext model. This joint optimization enables the transform to produce\nrepresentations that approach the best R-D performance possible. The SHTC\nframework consists of a base layer using KLT for data decorrelation, and a\nsparsity-coded enhancement layer that compresses the KLT residuals to refine\nthe representation. The enhancement encoder learns a linear transform to\nproject high-dimensional inputs into a low-dimensional space, while the decoder\nunfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct\nthe residuals. All components are designed to be interpretable, allowing the\nincorporation of signal priors and fewer parameters than black-box transforms.\nThis novel design significantly improves R-D performance with minimal\nadditional parameters and computational overhead.", "AI": {"tldr": "SHTC\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u4f18\u5316\u7684\u53d8\u6362\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u538b\u7f29\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u53163DGS\u3001\u53d8\u6362\u548c\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7387\u5931\u771f\u6027\u80fd\u3002", "motivation": "3DGS\u56e0\u5176\u5feb\u901f\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u800c\u6d41\u884c\uff0c\u4f46\u5185\u5b58\u5360\u7528\u5927\uff0c\u4f20\u8f93\u548c\u5b58\u50a8\u5f00\u9500\u9ad8\u3002\u73b0\u6709\u795e\u7ecf\u538b\u7f29\u65b9\u6cd5\u672a\u91c7\u7528\u7aef\u5230\u7aef\u4f18\u5316\u7684\u5206\u6790-\u5408\u6210\u53d8\u6362\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "SHTC\u6846\u67b6\u5305\u62ec\u4f7f\u7528KLT\u8fdb\u884c\u6570\u636e\u53bb\u76f8\u5173\u7684\u57fa\u7840\u5c42\u548c\u7a00\u758f\u7f16\u7801\u7684\u589e\u5f3a\u5c42\uff0c\u589e\u5f3a\u5c42\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u548cISTA\u7b97\u6cd5\u91cd\u6784\u6b8b\u5dee\u3002\u6240\u6709\u7ec4\u4ef6\u8bbe\u8ba1\u4e3a\u53ef\u89e3\u91ca\uff0c\u51cf\u5c11\u53c2\u6570\u3002", "result": "SHTC\u663e\u8457\u63d0\u5347\u4e86\u7387\u5931\u771f\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "SHTC\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u4f18\u5316\u76843DGS\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u8bbe\u8ba1\u548c\u8054\u5408\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u538b\u7f29\u6027\u80fd\u3002"}}
{"id": "2505.22830", "pdf": "https://arxiv.org/pdf/2505.22830", "abs": "https://arxiv.org/abs/2505.22830", "authors": ["Alexander Gill", "Abhilasha Ravichander", "Ana Marasovi\u0107"], "title": "What Has Been Lost with Synthetic Evaluation?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages main, 5 pages reference, 24 pages appendix", "summary": "Large language models (LLMs) are increasingly used for data generation.\nHowever, creating evaluation benchmarks raises the bar for this emerging\nparadigm. Benchmarks must target specific phenomena, penalize exploiting\nshortcuts, and be challenging. Through two case studies, we investigate whether\nLLMs can meet these demands by generating reasoning over-text benchmarks and\ncomparing them to those created through careful crowdsourcing. Specifically, we\nevaluate both the validity and difficulty of LLM-generated versions of two\nhigh-quality reading comprehension datasets: CondaQA, which evaluates reasoning\nabout negation, and DROP, which targets reasoning about quantities. We find\nthat prompting LLMs can produce variants of these datasets that are often valid\naccording to the annotation guidelines, at a fraction of the cost of the\noriginal crowdsourcing effort. However, we show that they are less challenging\nfor LLMs than their human-authored counterparts. This finding sheds light on\nwhat may have been lost by generating evaluation data with LLMs, and calls for\ncritically reassessing the immediate use of this increasingly prevalent\napproach to benchmark creation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u867d\u7136\u6210\u672c\u4f4e\u4e14\u6709\u6548\uff0c\u4f46\u751f\u6210\u7684\u57fa\u51c6\u5bf9LLMs\u7684\u6311\u6218\u6027\u4e0d\u5982\u4eba\u5de5\u521b\u5efa\u7684\u57fa\u51c6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30LLMs\u751f\u6210\u7684\u6570\u636e\u662f\u5426\u80fd\u6ee1\u8db3\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u7684\u9700\u6c42\uff0c\u5305\u62ec\u9488\u5bf9\u6027\u3001\u6311\u6218\u6027\u548c\u907f\u514d\u5229\u7528\u6377\u5f84\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08CondaQA\u548cDROP\u6570\u636e\u96c6\uff09\uff0c\u6bd4\u8f83LLMs\u751f\u6210\u7684\u57fa\u51c6\u4e0e\u4eba\u5de5\u521b\u5efa\u7684\u57fa\u51c6\u5728\u6709\u6548\u6027\u548c\u96be\u5ea6\u4e0a\u7684\u5dee\u5f02\u3002", "result": "LLMs\u751f\u6210\u7684\u57fa\u51c6\u5728\u6709\u6548\u6027\u4e0a\u63a5\u8fd1\u4eba\u5de5\u57fa\u51c6\uff0c\u4f46\u6311\u6218\u6027\u8f83\u4f4e\uff0c\u5bb9\u6613\u88abLLMs\u81ea\u8eab\u89e3\u51b3\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u9700\u8981\u91cd\u65b0\u8bc4\u4f30LLMs\u751f\u6210\u8bc4\u4f30\u57fa\u51c6\u7684\u666e\u904d\u505a\u6cd5\uff0c\u56e0\u5176\u53ef\u80fd\u727a\u7272\u6311\u6218\u6027\u3002"}}
{"id": "2505.22911", "pdf": "https://arxiv.org/pdf/2505.22911", "abs": "https://arxiv.org/abs/2505.22911", "authors": ["Matthew Beveridge", "Shree K. Nayar"], "title": "Hierarchical Material Recognition from Local Appearance", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a taxonomy of materials for hierarchical recognition from local\nappearance. Our taxonomy is motivated by vision applications and is arranged\naccording to the physical traits of materials. We contribute a diverse,\nin-the-wild dataset with images and depth maps of the taxonomy classes.\nUtilizing the taxonomy and dataset, we present a method for hierarchical\nmaterial recognition based on graph attention networks. Our model leverages the\ntaxonomic proximity between classes and achieves state-of-the-art performance.\nWe demonstrate the model's potential to generalize to adverse, real-world\nimaging conditions, and that novel views rendered using the depth maps can\nenhance this capability. Finally, we show the model's capacity to rapidly learn\nnew materials in a few-shot learning setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7279\u6027\u7684\u6750\u6599\u5206\u7c7b\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u5b9e\u73b0\u5206\u5c42\u6750\u6599\u8bc6\u522b\uff0c\u6027\u80fd\u4f18\u5f02\uff0c\u4e14\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4e3a\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7279\u6027\u7684\u6750\u6599\u5206\u7c7b\u6cd5\uff0c\u5e76\u89e3\u51b3\u771f\u5b9e\u573a\u666f\u4e2d\u6750\u6599\u8bc6\u522b\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7ed3\u5408\u5206\u7c7b\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u5229\u7528\u7c7b\u522b\u95f4\u7684\u5206\u7c7b\u5173\u7cfb\u8fdb\u884c\u5206\u5c42\u6750\u6599\u8bc6\u522b\u3002", "result": "\u6a21\u578b\u5728\u6750\u6599\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u80fd\u9002\u5e94\u6076\u52a3\u6761\u4ef6\uff0c\u4e14\u901a\u8fc7\u6df1\u5ea6\u56fe\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5206\u7c7b\u6cd5\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7ed3\u5408\u7684\u65b9\u6cd5\u5728\u6750\u6599\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.22842", "pdf": "https://arxiv.org/pdf/2505.22842", "abs": "https://arxiv.org/abs/2505.22842", "authors": ["Arthur S. Bianchessi", "Rodrigo C. Barros", "Lucas S. Kupssinsk\u00fc"], "title": "Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation", "categories": ["cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "Transformer-based language models rely on positional encoding (PE) to handle\ntoken order and support context length extrapolation. However, existing PE\nmethods lack theoretical clarity and rely on limited evaluation metrics to\nsubstantiate their extrapolation claims. We propose the Bayesian Attention\nMechanism (BAM), a theoretical framework that formulates positional encoding as\na prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE\nand ALiBi) and motivates a new Generalized Gaussian positional prior that\nsubstantially improves long-context generalization. Empirically, BAM enables\naccurate information retrieval at $500\\times$ the training context length,\noutperforming previous state-of-the-art context length generalization in long\ncontext retrieval accuracy while maintaining comparable perplexity and\nintroducing minimal additional parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBAM\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4f4d\u7f6e\u7f16\u7801\u5efa\u6a21\u4e3a\u6982\u7387\u6a21\u578b\u4e2d\u7684\u5148\u9a8c\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\u5e76\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u6e05\u6670\u6027\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u6709\u9650\uff0c\u96be\u4ee5\u652f\u6301\u5176\u5916\u63a8\u80fd\u529b\u3002", "method": "\u63d0\u51faBAM\u6846\u67b6\uff0c\u5c06\u4f4d\u7f6e\u7f16\u7801\u4f5c\u4e3a\u6982\u7387\u6a21\u578b\u7684\u5148\u9a8c\uff0c\u5e76\u5f15\u5165\u5e7f\u4e49\u9ad8\u65af\u4f4d\u7f6e\u5148\u9a8c\u3002", "result": "BAM\u5728500\u500d\u8bad\u7ec3\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u4ecd\u80fd\u51c6\u786e\u68c0\u7d22\u4fe1\u606f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u56f0\u60d1\u5ea6\u548c\u989d\u5916\u53c2\u6570\u3002", "conclusion": "BAM\u4e3a\u4f4d\u7f6e\u7f16\u7801\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.22914", "pdf": "https://arxiv.org/pdf/2505.22914", "abs": "https://arxiv.org/abs/2505.22914", "authors": ["Maksim Kolodiazhnyi", "Denis Tarasov", "Dmitrii Zhemchuzhnikov", "Alexander Nikulin", "Ilya Zisman", "Anna Vorontsova", "Anton Konushin", "Vladislav Kurenkov", "Danila Rukhovich"], "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001CAD\u91cd\u5efa\u6a21\u578b\uff0c\u7ed3\u5408\u70b9\u4e91\u3001\u56fe\u50cf\u548c\u6587\u672c\u8f93\u5165\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\uff0c\u5728DeepCAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CAD\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u4ec5\u652f\u6301\u5355\u4e00\u8f93\u5165\u6a21\u6001\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u8f93\u5165\uff0c\u53ef\u4ee5\u66f4\u5e7f\u6cdb\u5730\u5e94\u7528\u4e8e\u8bbe\u8ba1\u9886\u57df\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u5927\u89c4\u6a21\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u4e0a\uff1b2\uff09\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff08RL\uff09\u4f7f\u7528\u5728\u7ebf\u53cd\u9988\uff08\u5982GRPO\u7b97\u6cd5\uff09\u3002\u9996\u6b21\u63a2\u7d22\u4e86LLM\u5728CAD\u4efb\u52a1\u4e2d\u7684RL\u5fae\u8c03\u3002", "result": "SFT\u6a21\u578b\u5728DeepCAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6240\u6709\u5355\u6a21\u6001\u65b9\u6cd5\uff1bRL\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u5728\u4e09\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\uff08\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff09\u4e0a\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86CAD\u91cd\u5efa\u7684\u6027\u80fd\u548c\u901a\u7528\u6027\uff0c\u4e3a\u8bbe\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2505.22848", "pdf": "https://arxiv.org/pdf/2505.22848", "abs": "https://arxiv.org/abs/2505.22848", "authors": ["Pingjun Hong", "Beiduo Chen", "Siyao Peng", "Marie-Catherine de Marneffe", "Barbara Plank"], "title": "LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference", "categories": ["cs.CL"], "comment": "21 pages, 6 figures", "summary": "There is increasing evidence of Human Label Variation (HLV) in Natural\nLanguage Inference (NLI), where annotators assign different labels to the same\npremise-hypothesis pair. However, within-label variation--cases where\nannotators agree on the same label but provide divergent reasoning--poses an\nadditional and mostly overlooked challenge. Several NLI datasets contain\nhighlighted words in the NLI item as explanations, but the same spans on the\nNLI item can be highlighted for different reasons, as evidenced by free-text\nexplanations, which offer a window into annotators' reasoning. To\nsystematically understand this problem and gain insight into the rationales\nbehind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for\ncategorizing free-text explanations. Using this taxonomy, we annotate a subset\nof the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it\naligns with NLI labels, highlights, and explanations. We further assess the\ntaxonomy's usefulness in explanation generation, demonstrating that\nconditioning generation on LITEX yields explanations that are linguistically\ncloser to human explanations than those generated using only labels or\nhighlights. Our approach thus not only captures within-label variation but also\nshows how taxonomy-guided generation for reasoning can bridge the gap between\nhuman and model explanations more effectively than existing strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u4e2d\u4eba\u7c7b\u6807\u6ce8\u8005\u540c\u610f\u540c\u4e00\u6807\u7b7e\u4f46\u63d0\u4f9b\u4e0d\u540c\u63a8\u7406\u7684\u201c\u6807\u7b7e\u5185\u53d8\u5f02\u201d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86LITEX\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u89e3\u91ca\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3NLI\u4e2d\u6807\u6ce8\u8005\u540c\u610f\u6807\u7b7e\u4f46\u63a8\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63ed\u793a\u6807\u7b7e\u80cc\u540e\u7684\u771f\u5b9e\u63a8\u7406\u3002", "method": "\u5f15\u5165LITEX\u5206\u7c7b\u6cd5\uff0c\u5bf9e-SNLI\u6570\u636e\u96c6\u5b50\u96c6\u8fdb\u884c\u6807\u6ce8\uff0c\u9a8c\u8bc1\u5206\u7c7b\u6cd5\u53ef\u9760\u6027\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u6807\u7b7e\u3001\u9ad8\u4eae\u548c\u89e3\u91ca\u7684\u5173\u8054\u3002", "result": "LITEX\u751f\u6210\u7684\u89e3\u91ca\u5728\u8bed\u8a00\u5b66\u4e0a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u89e3\u91ca\uff0c\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u6807\u7b7e\u6216\u9ad8\u4eae\u7684\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "LITEX\u4e0d\u4ec5\u80fd\u6355\u6349\u6807\u7b7e\u5185\u53d8\u5f02\uff0c\u8fd8\u80fd\u901a\u8fc7\u5206\u7c7b\u6cd5\u6307\u5bfc\u7684\u751f\u6210\u65b9\u6cd5\u7f29\u5c0f\u4eba\u7c7b\u4e0e\u6a21\u578b\u89e3\u91ca\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.22918", "pdf": "https://arxiv.org/pdf/2505.22918", "abs": "https://arxiv.org/abs/2505.22918", "authors": ["Ruichen Chen", "Keith G. Mills", "Liyao Jiang", "Chao Gao", "Di Niu"], "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\n\\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}", "AI": {"tldr": "Re-ttention\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5197\u4f59\u6027\uff0c\u5728\u6781\u4f4e\u8ba1\u7b97\u5f00\u9500\u4e0b\u4fdd\u6301\u89c6\u89c9\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u53d8\u6362\u5668\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u56e0\u5206\u8fa8\u7387\u548c\u89c6\u9891\u957f\u5ea6\u5bfc\u81f4\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u6280\u672f\u5728\u6781\u9ad8\u7a00\u758f\u5ea6\u4e0b\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u548c\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u5386\u53f2softmax\u5206\u5e03\u91cd\u65b0\u8c03\u6574\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5b9e\u73b0\u6781\u9ad8\u7a00\u758f\u5ea6\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728CogVideoX\u548cPixArt DiTs\u7b49\u6a21\u578b\u4e0a\uff0c\u4ec5\u97003.1%\u7684token\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728H100 GPU\u4e0a\u5b9e\u73b045%\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u548c92%\u81ea\u6ce8\u610f\u529b\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "Re-ttention\u5728\u6781\u9ad8\u7a00\u758f\u5ea6\u4e0b\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22867", "pdf": "https://arxiv.org/pdf/2505.22867", "abs": "https://arxiv.org/abs/2505.22867", "authors": ["Iknoor Singh", "Carolina Scarton", "Kalina Bontcheva"], "title": "GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news and the increasing spread of misinformation\nnecessitate robust methods for automatic data analysis. Narrative\nclassification is emerging as a important task, since identifying what is being\nsaid online is critical for fact-checkers, policy markers and other\nprofessionals working on information studies. This paper presents our approach\nto SemEval 2025 Task 10 Subtask 2, which aims to classify news articles into a\npre-defined two-level taxonomy of main narratives and sub-narratives across\nmultiple languages.\n  We propose Hierarchical Three-Step Prompting (H3Prompt) for multilingual\nnarrative classification. Our methodology follows a three-step Large Language\nModel (LLM) prompting strategy, where the model first categorises an article\ninto one of two domains (Ukraine-Russia War or Climate Change), then identifies\nthe most relevant main narratives, and finally assigns sub-narratives. Our\napproach secured the top position on the English test set among 28 competing\nteams worldwide. The code is available at https://github.com/GateNLP/H3Prompt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aH3Prompt\u7684\u5206\u5c42\u4e09\u6b65\u63d0\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8bed\u8a00\u65b0\u95fb\u53d9\u4e8b\u5206\u7c7b\uff0c\u5e76\u5728SemEval 2025\u4efb\u52a110\u5b50\u4efb\u52a12\u4e2d\u53d6\u5f97\u82f1\u8bed\u6d4b\u8bd5\u96c6\u7b2c\u4e00\u540d\u3002", "motivation": "\u5728\u7ebf\u65b0\u95fb\u7684\u6fc0\u589e\u548c\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u9700\u8981\u81ea\u52a8\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u53d9\u4e8b\u5206\u7c7b\u6210\u4e3a\u5173\u952e\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5206\u5c42\u4e09\u6b65\u63d0\u793a\u7b56\u7565\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9010\u6b65\u5206\u7c7b\u65b0\u95fb\u6587\u7ae0\u7684\u4e3b\u53d9\u4e8b\u548c\u5b50\u53d9\u4e8b\u3002", "result": "\u572828\u4e2a\u5168\u7403\u7ade\u4e89\u56e2\u961f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u82f1\u8bed\u6d4b\u8bd5\u96c6\u4e0a\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "H3Prompt\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u53d9\u4e8b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.22926", "pdf": "https://arxiv.org/pdf/2505.22926", "abs": "https://arxiv.org/abs/2505.22926", "authors": ["Sylvey Lin", "Zhi-Yi Cao"], "title": "Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification", "categories": ["cs.CV"], "comment": null, "summary": "We investigate whether synthetic images generated by diffusion models can\nenhance multi-label classification of protein subcellular localization.\nSpecifically, we implement a simplified class-conditional denoising diffusion\nprobabilistic model (DDPM) to produce label-consistent samples and explore\ntheir integration with real data via two hybrid training strategies: Mix Loss\nand Mix Representation. While these approaches yield promising validation\nperformance, our proposed MixModel exhibits poor generalization to unseen test\ndata, underscoring the challenges of leveraging synthetic data effectively. In\ncontrast, baseline classifiers built on ResNet backbones with conventional loss\nfunctions demonstrate greater stability and test-time performance. Our findings\nhighlight the importance of realistic data generation and robust supervision\nwhen incorporating generative augmentation into biomedical image\nclassification.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u662f\u5426\u80fd\u63d0\u5347\u86cb\u767d\u8d28\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\uff0c\u53d1\u73b0\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u5728\u9a8c\u8bc1\u96c6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6d4b\u8bd5\u96c6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22\u5408\u6210\u6570\u636e\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u662f\u5426\u80fd\u589e\u5f3a\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u7b80\u5316\u7684\u7c7b\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u751f\u6210\u6807\u7b7e\u4e00\u81f4\u7684\u6837\u672c\uff0c\u5e76\u901a\u8fc7Mix Loss\u548cMix Representation\u4e24\u79cd\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u6574\u5408\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u3002", "result": "\u6df7\u5408\u7b56\u7565\u5728\u9a8c\u8bc1\u96c6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6d4b\u8bd5\u96c6\u6cdb\u5316\u80fd\u529b\u5dee\uff1b\u57fa\u4e8eResNet\u7684\u57fa\u7ebf\u5206\u7c7b\u5668\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u9700\u6ce8\u91cd\u6570\u636e\u771f\u5b9e\u6027\u548c\u76d1\u7763\u673a\u5236\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.22888", "pdf": "https://arxiv.org/pdf/2505.22888", "abs": "https://arxiv.org/abs/2505.22888", "authors": ["Jirui Qi", "Shan Chen", "Zidi Xiong", "Raquel Fern\u00e1ndez", "Danielle S. Bitterman", "Arianna Bisazza"], "title": "When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong\nperformance on English reasoning tasks. However, their ability to think in\nother languages is less studied. This capability is as important as answer\naccuracy for real world applications because users may find the reasoning trace\nuseful for oversight only when it is expressed in their own language. We\ncomprehensively evaluate two leading families of LRMs on our XReasoning\nbenchmark and find that even the most advanced models often revert to English\nor produce fragmented reasoning in other languages, revealing a substantial gap\nin multilingual reasoning. Prompt based interventions that force models to\nreason in the users language improve readability and oversight but reduce\nanswer accuracy, exposing an important trade off. We further show that targeted\npost training on just 100 examples mitigates this mismatch, though some\naccuracy loss remains. Our results highlight the limited multilingual reasoning\ncapabilities of current LRMs and outline directions for future work. Code and\ndata are available at https://github.com/Betswish/mCoT-XReasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u901a\u8fc7\u5e72\u9884\u63aa\u65bd\u548c\u5c11\u91cf\u9488\u5bf9\u6027\u8bad\u7ec3\u53ef\u4ee5\u6539\u5584\uff0c\u4f46\u4ecd\u5b58\u5728\u51c6\u786e\u6027\u4e0e\u53ef\u8bfb\u6027\u7684\u6743\u8861\u3002", "motivation": "\u8bc4\u4f30LRMs\u5728\u591a\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u4e3a\u7528\u6237\u9700\u8981\u4ee5\u6bcd\u8bed\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u6709\u6548\u76d1\u7763\u3002", "method": "\u5728XReasoning\u57fa\u51c6\u4e0a\u5168\u9762\u8bc4\u4f30\u4e24\u79cd\u4e3b\u6d41LRMs\uff0c\u91c7\u7528\u63d0\u793a\u5e72\u9884\u548c\u9488\u5bf9\u6027\u8bad\u7ec3\uff08100\u4e2a\u793a\u4f8b\uff09\u4ee5\u6539\u5584\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u5e38\u56de\u5f52\u82f1\u8bed\u6216\u4ea7\u751f\u788e\u7247\u5316\u63a8\u7406\uff0c\u63d0\u793a\u5e72\u9884\u63d0\u9ad8\u53ef\u8bfb\u6027\u4f46\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u9488\u5bf9\u6027\u8bad\u7ec3\u90e8\u5206\u7f13\u89e3\u95ee\u9898\u3002", "conclusion": "\u5f53\u524dLRMs\u7684\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u4e0e\u53ef\u8bfb\u6027\u3002"}}
{"id": "2505.22938", "pdf": "https://arxiv.org/pdf/2505.22938", "abs": "https://arxiv.org/abs/2505.22938", "authors": ["Ben Weiss"], "title": "Fast Isotropic Median Filtering", "categories": ["cs.CV", "cs.DS"], "comment": "Supplemental material:\n  https://github.com/google/fast-isotropic-median-filter", "summary": "Median filtering is a cornerstone of computational image processing. It\nprovides an effective means of image smoothing, with minimal blurring or\nsoftening of edges, invariance to monotonic transformations such as gamma\nadjustment, and robustness to noise and outliers. However, known algorithms\nhave all suffered from practical limitations: the bit depth of the image data,\nthe size of the filter kernel, or the kernel shape itself. Square-kernel\nimplementations tend to produce streaky cross-hatching artifacts, and nearly\nall known efficient algorithms are in practice limited to square kernels. We\npresent for the first time a method that overcomes all of these limitations.\nOur method operates efficiently on arbitrary bit-depth data, arbitrary kernel\nsizes, and arbitrary convex kernel shapes, including circular shapes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e2d\u503c\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7b97\u6cd5\u5728\u6bd4\u7279\u6df1\u5ea6\u3001\u6838\u5927\u5c0f\u548c\u5f62\u72b6\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u4e2d\u503c\u6ee4\u6ce2\u7b97\u6cd5\u5b58\u5728\u6bd4\u7279\u6df1\u5ea6\u3001\u6838\u5927\u5c0f\u548c\u5f62\u72b6\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u4efb\u610f\u6bd4\u7279\u6df1\u5ea6\u3001\u6838\u5927\u5c0f\u548c\u51f8\u6838\u5f62\u72b6\uff08\u5305\u62ec\u5706\u5f62\uff09\u3002", "result": "\u65b0\u65b9\u6cd5\u9ad8\u6548\u4e14\u65e0\u4f20\u7edf\u7b97\u6cd5\u7684\u9650\u5236\uff0c\u907f\u514d\u4e86\u6761\u7eb9\u4ea4\u53c9\u5f71\u7ebf\u4f2a\u5f71\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5168\u9762\u89e3\u51b3\u4e86\u4e2d\u503c\u6ee4\u6ce2\u7684\u5b9e\u8df5\u9650\u5236\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.22897", "pdf": "https://arxiv.org/pdf/2505.22897", "abs": "https://arxiv.org/abs/2505.22897", "authors": ["Chahat Raj", "Bowen Wei", "Aylin Caliskan", "Antonios Anastasopoulos", "Ziwei Zhu"], "title": "VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models", "categories": ["cs.CL"], "comment": "17 pages", "summary": "While bias in large language models (LLMs) is well-studied, similar concerns\nin vision-language models (VLMs) have received comparatively less attention.\nExisting VLM bias studies often focus on portrait-style images and\ngender-occupation associations, overlooking broader and more complex social\nstereotypes and their implied harm. This work introduces VIGNETTE, a\nlarge-scale VQA benchmark with 30M+ images for evaluating bias in VLMs through\na question-answering framework spanning four directions: factuality,\nperception, stereotyping, and decision making. Beyond narrowly-centered\nstudies, we assess how VLMs interpret identities in contextualized settings,\nrevealing how models make trait and capability assumptions and exhibit patterns\nof discrimination. Drawing from social psychology, we examine how VLMs connect\nvisual identity cues to trait and role-based inferences, encoding social\nhierarchies, through biased selections. Our findings uncover subtle,\nmultifaceted, and surprising stereotypical patterns, offering insights into how\nVLMs construct social meaning from inputs.", "AI": {"tldr": "VIGNETTE\u662f\u4e00\u4e2a\u5927\u89c4\u6a21VQA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u504f\u89c1\uff0c\u6db5\u76d6\u4e8b\u5b9e\u6027\u3001\u611f\u77e5\u3001\u523b\u677f\u5370\u8c61\u548c\u51b3\u7b56\u56db\u4e2a\u65b9\u5411\uff0c\u63ed\u793a\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u89c6\u89c9\u8eab\u4efd\u7ebf\u7d22\u6784\u5efa\u793e\u4f1a\u610f\u4e49\u3002", "motivation": "\u73b0\u6709VLM\u504f\u89c1\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u8096\u50cf\u56fe\u50cf\u548c\u6027\u522b-\u804c\u4e1a\u5173\u8054\uff0c\u5ffd\u7565\u4e86\u66f4\u5e7f\u6cdb\u590d\u6742\u7684\u793e\u4f1a\u523b\u677f\u5370\u8c61\u53ca\u5176\u6f5c\u5728\u5371\u5bb3\u3002", "method": "\u901a\u8fc730M+\u56fe\u50cf\u7684VQA\u6846\u67b6\u8bc4\u4f30VLM\u504f\u89c1\uff0c\u7ed3\u5408\u793e\u4f1a\u5fc3\u7406\u5b66\u5206\u6790\u6a21\u578b\u5982\u4f55\u4ece\u89c6\u89c9\u7ebf\u7d22\u63a8\u65ad\u7279\u8d28\u548c\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u5b58\u5728\u5fae\u5999\u3001\u591a\u9762\u4e14\u4ee4\u4eba\u60ca\u8bb6\u7684\u523b\u677f\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5982\u4f55\u7f16\u7801\u793e\u4f1a\u7b49\u7ea7\u548c\u6b67\u89c6\u6027\u9009\u62e9\u3002", "conclusion": "VIGNETTE\u4e3a\u7406\u89e3VLMs\u5982\u4f55\u4ece\u8f93\u5165\u4e2d\u6784\u5efa\u793e\u4f1a\u610f\u4e49\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u66f4\u5168\u9762\u7684\u504f\u89c1\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.22944", "pdf": "https://arxiv.org/pdf/2505.22944", "abs": "https://arxiv.org/abs/2505.22944", "authors": ["Angtian Wang", "Haibin Huang", "Jacob Zhiyuan Fang", "Yiding Yang", "Chongyang Ma"], "title": "ATI: Any Trajectory Instruction for Controllable Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a unified framework for motion control in video generation that\nseamlessly integrates camera movement, object-level translation, and\nfine-grained local motion using trajectory-based inputs. In contrast to prior\nmethods that address these motion types through separate modules or\ntask-specific designs, our approach offers a cohesive solution by projecting\nuser-defined trajectories into the latent space of pre-trained image-to-video\ngeneration models via a lightweight motion injector. Users can specify\nkeypoints and their motion paths to control localized deformations, entire\nobject motion, virtual camera dynamics, or combinations of these. The injected\ntrajectory signals guide the generative process to produce temporally\nconsistent and semantically aligned motion sequences. Our framework\ndemonstrates superior performance across multiple video motion control tasks,\nincluding stylized motion effects (e.g., motion brushes), dynamic viewpoint\nchanges, and precise local motion manipulation. Experiments show that our\nmethod provides significantly better controllability and visual quality\ncompared to prior approaches and commercial solutions, while remaining broadly\ncompatible with various state-of-the-art video generation backbones. Project\npage: https://anytraj.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u8f93\u5165\u6574\u5408\u76f8\u673a\u8fd0\u52a8\u3001\u7269\u4f53\u5e73\u79fb\u548c\u5c40\u90e8\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u8fd0\u52a8\u63a7\u5236\u91c7\u7528\u5206\u79bb\u6a21\u5757\u6216\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8fd0\u52a8\u6ce8\u5165\u5668\u5c06\u7528\u6237\u5b9a\u4e49\u7684\u8f68\u8ff9\u6295\u5f71\u5230\u9884\u8bad\u7ec3\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u98ce\u683c\u5316\u8fd0\u52a8\u6548\u679c\u3001\u52a8\u6001\u89c6\u89d2\u53d8\u5316\u548c\u7cbe\u786e\u5c40\u90e8\u8fd0\u52a8\u64cd\u63a7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u63a7\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u548c\u5546\u4e1a\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u517c\u5bb9\u591a\u79cd\u5148\u8fdb\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2505.22910", "pdf": "https://arxiv.org/pdf/2505.22910", "abs": "https://arxiv.org/abs/2505.22910", "authors": ["Chahat Raj", "Mahika Banerjee", "Aylin Caliskan", "Antonios Anastasopoulos", "Ziwei Zhu"], "title": "Talent or Luck? Evaluating Attribution Bias in Large Language Models", "categories": ["cs.CL"], "comment": "18 pages", "summary": "When a student fails an exam, do we tend to blame their effort or the test's\ndifficulty? Attribution, defined as how reasons are assigned to event outcomes,\nshapes perceptions, reinforces stereotypes, and influences decisions.\nAttribution Theory in social psychology explains how humans assign\nresponsibility for events using implicit cognition, attributing causes to\ninternal (e.g., effort, ability) or external (e.g., task difficulty, luck)\nfactors. LLMs' attribution of event outcomes based on demographics carries\nimportant fairness implications. Most works exploring social biases in LLMs\nfocus on surface-level associations or isolated stereotypes. This work proposes\na cognitively grounded bias evaluation framework to identify how models'\nreasoning disparities channelize biases toward demographic groups.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u548cLLMs\u5982\u4f55\u5f52\u56e0\u4e8b\u4ef6\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u5f52\u56e0\u7406\u8bba\u5728LLMs\u4e2d\u7684\u5e94\u7528\uff0c\u63ed\u793a\u6a21\u578b\u5982\u4f55\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u5b66\u5f52\u56e0\u4e8b\u4ef6\u7ed3\u679c\uff0c\u53ca\u5176\u516c\u5e73\u6027\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u7684\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u6a21\u578b\u63a8\u7406\u5dee\u5f02\u5982\u4f55\u5bfc\u81f4\u5bf9\u7279\u5b9a\u4eba\u53e3\u7fa4\u4f53\u7684\u504f\u89c1\u3002", "result": "\u53d1\u73b0LLMs\u5728\u5f52\u56e0\u4e8b\u4ef6\u7ed3\u679c\u65f6\u5b58\u5728\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u5b66\u7684\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8bc4\u4f30\u548c\u89e3\u51b3LLMs\u4e2d\u5f52\u56e0\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u3002"}}
{"id": "2505.22971", "pdf": "https://arxiv.org/pdf/2505.22971", "abs": "https://arxiv.org/abs/2505.22971", "authors": ["Yu Yuan", "Yiheng Chi", "Xingguang Zhang", "Stanley Chan"], "title": "iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures", "categories": ["eess.IV", "cs.CV"], "comment": "To be appear in IEEE ICIP 2025", "summary": "High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by\nfusing information from multiple low dynamic range (LDR) images. Numerous\nlearning-based HDR imaging methods have been proposed to achieve this for\nstatic and dynamic scenes. However, their architectures are mostly tailored for\na fixed number (e.g., three) of inputs and, therefore, cannot apply directly to\nsituations beyond the pre-defined limited scope. To address this issue, we\npropose a novel framework, iHDR, for iterative fusion, which comprises a\nghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain\nmapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an\nintermediate HDR image, while ToneNet maps it back to the nonlinear domain and\nserves as the reference input for the next pairwise fusion. This process is\niteratively executed until all input frames are utilized. Qualitative and\nquantitative experiments demonstrate the effectiveness of the proposed method\nas compared to existing state-of-the-art HDR deghosting approaches given\nflexible numbers of input frames.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiHDR\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u878d\u5408\u591a\u5f20\u4f4e\u52a8\u6001\u8303\u56f4\uff08LDR\uff09\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cfHDR\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u8f93\u5165\u6570\u91cf\u56fa\u5b9a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709HDR\u6210\u50cf\u65b9\u6cd5\u901a\u5e38\u4ec5\u9002\u7528\u4e8e\u56fa\u5b9a\u6570\u91cf\u7684\u8f93\u5165\uff0c\u65e0\u6cd5\u7075\u6d3b\u5904\u7406\u4e0d\u540c\u6570\u91cf\u7684\u8f93\u5165\u5e27\u3002", "method": "iHDR\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u65e0\u91cd\u5f71\u7684\u53cc\u8f93\u5165HDR\u878d\u5408\u7f51\u7edc\uff08DiHDR\uff09\u548c\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u57df\u6620\u5c04\u7f51\u7edc\uff08ToneNet\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u878d\u5408\u9010\u6b65\u751f\u6210HDR\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0ciHDR\u5728\u8f93\u5165\u5e27\u6570\u91cf\u7075\u6d3b\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709HDR\u53bb\u91cd\u5f71\u65b9\u6cd5\u3002", "conclusion": "iHDR\u6846\u67b6\u4e3a\u52a8\u6001\u573a\u666f\u4e0b\u7684HDR\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22919", "pdf": "https://arxiv.org/pdf/2505.22919", "abs": "https://arxiv.org/abs/2505.22919", "authors": ["Nikita Mehandru", "Niloufar Golchini", "David Bamman", "Travis Zack", "Melanie F. Molina", "Ahmed Alaa"], "title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide.", "AI": {"tldr": "ER-Reason\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6025\u8bca\u5ba4\u4e34\u5e8a\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b3984\u540d\u60a3\u8005\u7684\u6570\u636e\u548c25174\u6761\u4e34\u5e8a\u8bb0\u5f55\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u533b\u751f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u4e14\u96c6\u4e2d\u4e8e\u5b64\u7acb\u4efb\u52a1\uff0c\u672a\u80fd\u5168\u9762\u53cd\u6620\u4e34\u5e8a\u63a8\u7406\u6216\u533b\u7597\u51b3\u7b56\u6d41\u7a0b\uff0c\u5c24\u5176\u662f\u5728\u6025\u8bca\u5ba4\u8fd9\u4e00\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u3002", "method": "\u5f15\u5165ER-Reason\u57fa\u51c6\uff0c\u6db5\u76d6\u6025\u8bca\u5de5\u4f5c\u6d41\u7684\u5173\u952e\u9636\u6bb5\uff08\u5982\u5206\u8bca\u3001\u6cbb\u7597\u9009\u62e9\u7b49\uff09\uff0c\u5e76\u6536\u96c6\u4e8672\u4efd\u533b\u751f\u64b0\u5199\u7684\u63a8\u7406\u8fc7\u7a0b\u4ee5\u6a21\u62df\u6559\u5b66\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6025\u8bca\u51b3\u7b56\u4e2d\u7684\u4e34\u5e8a\u63a8\u7406\u4e0e\u533b\u751f\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u5f25\u5408\u6a21\u578b\u4e0e\u533b\u751f\u4e34\u5e8a\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u6a21\u578b\u5728\u6025\u8bca\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2505.22976", "pdf": "https://arxiv.org/pdf/2505.22976", "abs": "https://arxiv.org/abs/2505.22976", "authors": ["Kewei Lian", "Shaofei Cai", "Yilun Du", "Yitao Liang"], "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The ability to simulate the world in a spatially consistent manner is a\ncrucial requirements for effective world models. Such a model enables\nhigh-quality visual generation, and also ensures the reliability of world\nmodels for downstream tasks such as simulation and planning. Designing a memory\nmodule is a crucial component for addressing spatial consistency: such a model\nmust not only retain long-horizon observational information, but also enables\nthe construction of explicit or implicit internal spatial representations.\nHowever, there are no dataset designed to promote the development of memory\nmodules by explicitly enforcing spatial consistency constraints. Furthermore,\nmost existing benchmarks primarily emphasize visual coherence or generation\nquality, neglecting the requirement of long-range spatial consistency. To\nbridge this gap, we construct a dataset and corresponding benchmark by sampling\n150 distinct locations within the open-world environment of Minecraft,\ncollecting about 250 hours (20 million frames) of loop-based navigation videos\nwith actions. Our dataset follows a curriculum design of sequence lengths,\nallowing models to learn spatial consistency on increasingly complex navigation\ntrajectories. Furthermore, our data collection pipeline is easily extensible to\nnew Minecraft environments and modules. Four representative world model\nbaselines are evaluated on our benchmark. Dataset, benchmark, and code are\nopen-sourced to support future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7a7a\u95f4\u4e00\u81f4\u6027\u4e16\u754c\u6a21\u578b\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eMinecraft\u73af\u5883\u6536\u96c6\u4e8620\u767e\u4e07\u5e27\u5bfc\u822a\u89c6\u9891\uff0c\u652f\u6301\u6a21\u578b\u5b66\u4e60\u957f\u8ddd\u79bb\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u660e\u786e\u7ea6\u675f\uff0c\u4e14\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u89c6\u89c9\u4e00\u81f4\u6027\u6216\u751f\u6210\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u957f\u8ddd\u79bb\u7a7a\u95f4\u4e00\u81f4\u6027\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b150\u4e2aMinecraft\u4f4d\u7f6e\u7684\u5bfc\u822a\u89c6\u9891\u6570\u636e\u96c6\uff0820\u767e\u4e07\u5e27\uff09\uff0c\u91c7\u7528\u8bfe\u7a0b\u8bbe\u8ba1\u9010\u6b65\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u4e16\u754c\u6a21\u578b\u57fa\u7ebf\u3002", "result": "\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u6a21\u578b\u5b66\u4e60\u590d\u6742\u5bfc\u822a\u8f68\u8ff9\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u6613\u4e8e\u6269\u5c55\u5230\u65b0\u73af\u5883\u3002", "conclusion": "\u5f00\u6e90\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u7a7a\u95f4\u4e00\u81f4\u6027\u4e16\u754c\u6a21\u578b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.22921", "pdf": "https://arxiv.org/pdf/2505.22921", "abs": "https://arxiv.org/abs/2505.22921", "authors": ["Yue Xing", "Tao Yang", "Yijiashun Qi", "Minggu Wei", "Yu Cheng", "Honghui Xin"], "title": "Structured Memory Mechanisms for Stable Context Representation in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limitations of large language models in\nunderstanding long-term context. It proposes a model architecture equipped with\na long-term memory mechanism to improve the retention and retrieval of semantic\ninformation across paragraphs and dialogue turns. The model integrates explicit\nmemory units, gated writing mechanisms, and attention-based reading modules. A\nforgetting function is introduced to enable dynamic updates of memory content,\nenhancing the model's ability to manage historical information. To further\nimprove the effectiveness of memory operations, the study designs a joint\ntraining objective. This combines the main task loss with constraints on memory\nwriting and forgetting. It guides the model to learn better memory strategies\nduring task execution. Systematic evaluation across multiple subtasks shows\nthat the model achieves clear advantages in text generation consistency,\nstability in multi-turn question answering, and accuracy in cross-context\nreasoning. In particular, the model demonstrates strong semantic retention and\ncontextual coherence in long-text tasks and complex question answering\nscenarios. It effectively mitigates the context loss and semantic drift\nproblems commonly faced by traditional language models when handling long-term\ndependencies. The experiments also include analysis of different memory\nstructures, capacity sizes, and control strategies. These results further\nconfirm the critical role of memory mechanisms in language understanding. They\ndemonstrate the feasibility and effectiveness of the proposed approach in both\narchitectural design and performance outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u957f\u671f\u8bb0\u5fc6\u673a\u5236\u7684\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u957f\u671f\u4e0a\u4e0b\u6587\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u671f\u4f9d\u8d56\u65f6\u5e38\u89c1\u7684\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u6a21\u578b\u96c6\u6210\u4e86\u663e\u5f0f\u8bb0\u5fc6\u5355\u5143\u3001\u95e8\u63a7\u5199\u5165\u673a\u5236\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bfb\u53d6\u6a21\u5757\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8054\u5408\u8bad\u7ec3\u76ee\u6807\u4ee5\u4f18\u5316\u8bb0\u5fc6\u7b56\u7565\u3002", "result": "\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u4e00\u81f4\u6027\u3001\u591a\u8f6e\u95ee\u7b54\u7a33\u5b9a\u6027\u548c\u8de8\u4e0a\u4e0b\u6587\u63a8\u7406\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u4efb\u52a1\u548c\u590d\u6742\u95ee\u7b54\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bb0\u5fc6\u673a\u5236\u5728\u8bed\u8a00\u7406\u89e3\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5176\u67b6\u6784\u8bbe\u8ba1\u548c\u6027\u80fd\u8868\u73b0\u5747\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2505.22977", "pdf": "https://arxiv.org/pdf/2505.22977", "abs": "https://arxiv.org/abs/2505.22977", "authors": ["Shuolin Xu", "Siming Zheng", "Ziyi Wang", "HC Yu", "Jinwei Chen", "Huaqi Zhang", "Bo Li", "Peng-Tao Jiang"], "title": "HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions", "categories": ["cs.CV"], "comment": "17 pages, 7 figures", "summary": "Recent advances in diffusion models have significantly improved conditional\nvideo generation, particularly in the pose-guided human image animation task.\nAlthough existing methods are capable of generating high-fidelity and\ntime-consistent animation sequences in regular motions and static scenes, there\nare still obvious limitations when facing complex human body motions\n(Hypermotion) that contain highly dynamic, non-standard motions, and the lack\nof a high-quality benchmark for evaluation of complex human motion animations.\nTo address this challenge, we introduce the \\textbf{Open-HyperMotionX Dataset}\nand \\textbf{HyperMotionX Bench}, which provide high-quality human pose\nannotations and curated video clips for evaluating and improving pose-guided\nhuman image animation models under complex human motion conditions.\nFurthermore, we propose a simple yet powerful DiT-based video generation\nbaseline and design spatial low-frequency enhanced RoPE, a novel module that\nselectively enhances low-frequency spatial feature modeling by introducing\nlearnable frequency scaling. Our method significantly improves structural\nstability and appearance consistency in highly dynamic human motion sequences.\nExtensive experiments demonstrate the effectiveness of our dataset and proposed\napproach in advancing the generation quality of complex human motion image\nanimations. Code and dataset will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff08Open-HyperMotionX Dataset\u548cHyperMotionX Bench\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u590d\u6742\u4eba\u4f53\u8fd0\u52a8\u6761\u4ef6\u4e0b\u7684\u59ff\u6001\u5f15\u5bfc\u52a8\u753b\u751f\u6210\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\u548c\u7a7a\u95f4\u4f4e\u9891\u589e\u5f3aRoPE\u6a21\u5757\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4eba\u4f53\u8fd0\u52a8\uff08Hypermotion\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faOpen-HyperMotionX\u6570\u636e\u96c6\u548cHyperMotionX Bench\uff0c\u8bbe\u8ba1\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u4f4e\u9891\u589e\u5f3aRoPE\u6a21\u5757\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u5ea6\u52a8\u6001\u4eba\u4f53\u8fd0\u52a8\u5e8f\u5217\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\u548c\u5916\u89c2\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u4eba\u4f53\u8fd0\u52a8\u56fe\u50cf\u52a8\u753b\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2505.22934", "pdf": "https://arxiv.org/pdf/2505.22934", "abs": "https://arxiv.org/abs/2505.22934", "authors": ["Haobo Zhang", "Jiayu Zhou"], "title": "Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures, 16 tables, accepted by ACL 2025", "summary": "Fine-tuning large language models (LMs) for individual tasks yields strong\nperformance but is expensive for deployment and storage. Recent works explore\nmodel merging to combine multiple task-specific models into a single multi-task\nmodel without additional training. However, existing merging methods often fail\nfor models fine-tuned with low-rank adaptation (LoRA), due to significant\nperformance degradation. In this paper, we show that this issue arises from a\npreviously overlooked interplay between model parameters and data\ndistributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)\nto constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates\nrelevant to one task do not adversely shift outputs for others. Our approach\ncan seamlessly integrate with most existing merging algorithms, reducing the\nunintended interference among tasks. Extensive experiments on eight datasets,\ntested with three widely used LMs and two large LMs, demonstrate that our\nmethod not only boosts merging performance but also preserves single-task\naccuracy. Furthermore, our approach exhibits greater robustness to the\nhyperparameters of merging. These results highlight the importance of\ndata-parameter interaction in model merging and offer a plug-and-play solution\nfor merging LoRA models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faOSRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675fLoRA\u5b50\u7a7a\u95f4\u63d0\u5347\u6a21\u578b\u5408\u5e76\u6027\u80fd\uff0c\u51cf\u5c11\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u4fdd\u6301\u5355\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u5408\u5e76\u65b9\u6cd5\u5bf9LoRA\u5fae\u8c03\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u89e3\u51b3\u53c2\u6570\u4e0e\u6570\u636e\u5206\u5e03\u7684\u4ea4\u4e92\u95ee\u9898\u3002", "method": "\u63d0\u51faOSRM\u65b9\u6cd5\uff0c\u5728\u5fae\u8c03\u524d\u7ea6\u675fLoRA\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u4efb\u52a1\u95f4\u5e72\u6270\uff0c\u517c\u5bb9\u73b0\u6709\u5408\u5e76\u7b97\u6cd5\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cOSRM\u663e\u8457\u63d0\u5347\u5408\u5e76\u6027\u80fd\uff0c\u4fdd\u6301\u5355\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5bf9\u8d85\u53c2\u6570\u66f4\u9c81\u68d2\u3002", "conclusion": "OSRM\u4e3aLoRA\u6a21\u578b\u5408\u5e76\u63d0\u4f9b\u4e86\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u6570\u636e-\u53c2\u6570\u4ea4\u4e92\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.22978", "pdf": "https://arxiv.org/pdf/2505.22978", "abs": "https://arxiv.org/abs/2505.22978", "authors": ["Youngju Na", "Taeyeon Kim", "Jumin Lee", "Kyu Beom Han", "Woo Jae Kim", "Sung-eui Yoon"], "title": "Pose-free 3D Gaussian splatting via shape-ray estimation", "categories": ["cs.CV"], "comment": "ICIP 2025", "summary": "While generalizable 3D Gaussian splatting enables efficient, high-quality\nrendering of unseen scenes, it heavily depends on precise camera poses for\naccurate geometry. In real-world scenarios, obtaining accurate poses is\nchallenging, leading to noisy pose estimates and geometric misalignments. To\naddress this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting\nframework that overcomes these ambiguities by joint shape and camera rays\nestimation. Instead of relying on explicit 3D transformations, SHARE builds a\npose-aware canonical volume representation that seamlessly integrates\nmulti-view information, reducing misalignment caused by inaccurate pose\nestimates. Additionally, anchor-aligned Gaussian prediction enhances scene\nreconstruction by refining local geometry around coarse anchors, allowing for\nmore precise Gaussian placement. Extensive experiments on diverse real-world\ndatasets show that our method achieves robust performance in pose-free\ngeneralizable Gaussian splatting.", "AI": {"tldr": "SHARE\u662f\u4e00\u79cd\u65e0\u9700\u76f8\u673a\u59ff\u6001\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5f62\u72b6\u548c\u76f8\u673a\u5149\u7ebf\u4f30\u8ba1\u89e3\u51b3\u59ff\u6001\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "motivation": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u96be\u4ee5\u83b7\u53d6\uff0c\u5bfc\u81f4\u51e0\u4f55\u5bf9\u9f50\u95ee\u9898\u3002", "method": "SHARE\u901a\u8fc7\u6784\u5efa\u59ff\u6001\u611f\u77e5\u7684\u89c4\u8303\u4f53\u79ef\u8868\u793a\u548c\u951a\u5bf9\u9f50\u9ad8\u65af\u9884\u6d4b\uff0c\u51cf\u5c11\u59ff\u6001\u4e0d\u51c6\u786e\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cSHARE\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u59ff\u6001\u7684\u6cdb\u5316\u9ad8\u65af\u6cfc\u6e85\u3002", "conclusion": "SHARE\u4e3a\u59ff\u6001\u4e0d\u51c6\u786e\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u9ad8\u8d28\u91cf\u6e32\u67d3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22937", "pdf": "https://arxiv.org/pdf/2505.22937", "abs": "https://arxiv.org/abs/2505.22937", "authors": ["Ngeyen Yinkfu"], "title": "Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs", "categories": ["cs.CL"], "comment": "This paper presents an efficient transformer-based question-answering\n  model optimized for inference on a 13th Gen Intel i7 CPU. The proposed\n  approach balances performance and computational efficiency, making it\n  suitable for real-time applications on resource-constrained devices. Code for\n  this paper is available upon request via email at nyinkfu@andrew.cmu.edu", "summary": "This study presents an efficient transformer-based question-answering (QA)\nmodel optimized for deployment on a 13th Gen Intel i7-1355U CPU, using the\nStanford Question Answering Dataset (SQuAD) v1.1. Leveraging exploratory data\nanalysis, data augmentation, and fine-tuning of a DistilBERT architecture, the\nmodel achieves a validation F1 score of 0.6536 with an average inference time\nof 0.1208 seconds per question. Compared to a rule-based baseline (F1: 0.3124)\nand full BERT-based models, our approach offers a favorable trade-off between\naccuracy and computational efficiency. This makes it well-suited for real-time\napplications on resource-constrained systems. The study includes systematic\nevaluation of data augmentation strategies and hyperparameter configurations,\nproviding practical insights into optimizing transformer models for CPU-based\ninference.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9ad8\u6548\u95ee\u7b54\u6a21\u578b\uff0c\u9488\u5bf913\u4ee3Intel i7-1355U CPU\u4f18\u5316\uff0c\u5728SQuAD v1.1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e0a\u5b9e\u65f6\u8fd0\u884c\u7684\u9ad8\u6548\u95ee\u7b54\u6a21\u578b\uff0c\u5e73\u8861\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u6570\u636e\u589e\u5f3a\u3001\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u548cDistilBERT\u67b6\u6784\u5fae\u8c03\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u8d85\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u9a8c\u8bc1F1\u5f97\u5206\u4e3a0.6536\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u4e3a0.1208\u79d2/\u95ee\u9898\uff0c\u4f18\u4e8e\u89c4\u5219\u57fa\u7ebf\u548c\u5b8c\u6574BERT\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728CPU\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2505.22980", "pdf": "https://arxiv.org/pdf/2505.22980", "abs": "https://arxiv.org/abs/2505.22980", "authors": ["Aimon Rahman", "Jiang Liu", "Ze Wang", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Yusheng Su", "Vishal M. Patel", "Zicheng Liu", "Emad Barsoum"], "title": "MOVi: Training-free Text-conditioned Multi-Object Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion-based text-to-video (T2V) models have\ndemonstrated remarkable progress, but these models still face challenges in\ngenerating videos with multiple objects. Most models struggle with accurately\ncapturing complex object interactions, often treating some objects as static\nbackground elements and limiting their movement. In addition, they often fail\nto generate multiple distinct objects as specified in the prompt, resulting in\nincorrect generations or mixed features across objects. In this paper, we\npresent a novel training-free approach for multi-object video generation that\nleverages the open world knowledge of diffusion models and large language\nmodels (LLMs). We use an LLM as the ``director'' of object trajectories, and\napply the trajectories through noise re-initialization to achieve precise\ncontrol of realistic movements. We further refine the generation process by\nmanipulating the attention mechanism to better capture object-specific features\nand motion patterns, and prevent cross-object feature interference. Extensive\nexperiments validate the effectiveness of our training free approach in\nsignificantly enhancing the multi-object generation capabilities of existing\nvideo diffusion models, resulting in 42% absolute improvement in motion\ndynamics and object generation accuracy, while also maintaining high fidelity\nand motion smoothness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u5bf9\u8c61\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f00\u653e\u4e16\u754c\u77e5\u8bc6\uff0c\u901a\u8fc7\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5bf9\u8c61\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u751f\u6210\u4e2d\u96be\u4ee5\u51c6\u786e\u6355\u6349\u590d\u6742\u5bf9\u8c61\u4ea4\u4e92\uff0c\u5e38\u5c06\u5bf9\u8c61\u89c6\u4e3a\u9759\u6001\u80cc\u666f\u6216\u751f\u6210\u9519\u8bef\u7279\u5f81\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u5bf9\u8c61\u8f68\u8ff9\u7684\u201c\u5bfc\u6f14\u201d\uff0c\u901a\u8fc7\u566a\u58f0\u91cd\u65b0\u521d\u59cb\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\uff0c\u7cbe\u786e\u63a7\u5236\u5bf9\u8c61\u8fd0\u52a8\u548c\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u5bf9\u8c61\u751f\u6210\u80fd\u529b\u4e0a\u63d0\u5347\u4e8642%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u5bf9\u8c61\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22942", "pdf": "https://arxiv.org/pdf/2505.22942", "abs": "https://arxiv.org/abs/2505.22942", "authors": ["Yuchen Zhuang", "Di Jin", "Jiaao Chen", "Wenqi Shi", "Hanrui Wang", "Chao Zhang"], "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Work in Progress", "summary": "Large language models (LLMs)-empowered web agents enables automating complex,\nreal-time web navigation tasks in enterprise environments. However, existing\nweb agents relying on supervised fine-tuning (SFT) often struggle with\ngeneralization and robustness due to insufficient reasoning capabilities when\nhandling the inherently dynamic nature of web interactions. In this study, we\nintroduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based\nR1-style reinforcement learning framework designed explicitly to enhance\nsingle-step reasoning and planning for business-oriented web navigation tasks.\nWe employ a structured reward function that evaluates both adherence to output\nformats and correctness of actions, enabling WorkForceAgent-R1 to implicitly\nlearn robust intermediate reasoning without explicit annotations or extensive\nexpert demonstrations. Extensive experiments on the WorkArena benchmark\ndemonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by\n10.26-16.59%, achieving competitive performance relative to proprietary\nLLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.", "AI": {"tldr": "WorkForceAgent-R1\u662f\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\uff0c\u901a\u8fc7R1\u98ce\u683c\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u5355\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8eSFT\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSFT\u7684\u7f51\u9875\u4ee3\u7406\u5728\u52a8\u6001\u7f51\u9875\u4ea4\u4e92\u4e2d\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u89c4\u5219\u5316\u7684R1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u6807\u6ce8\u6216\u4e13\u5bb6\u6f14\u793a\u3002", "result": "\u5728WorkArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u6bd4SFT\u57fa\u7ebf\u63d0\u534710.26-16.59%\uff0c\u63a5\u8fd1GPT-4o\u6c34\u5e73\u3002", "conclusion": "WorkForceAgent-R1\u5728\u5546\u4e1a\u5bfc\u5411\u7684\u7f51\u9875\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.23008", "pdf": "https://arxiv.org/pdf/2505.23008", "abs": "https://arxiv.org/abs/2505.23008", "authors": ["Jonathan Li", "Zoltan Csaki", "Nidhi Hiremath", "Etash Guha", "Fenglu Hong", "Edward Ma", "Urmish Thakker"], "title": "Synthetic Document Question Answering in Hungarian", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Modern VLMs have achieved near-saturation accuracy in English document visual\nquestion-answering (VQA). However, this task remains challenging in lower\nresource languages due to a dearth of suitable training and evaluation data. In\nthis paper we present scalable methods for curating such datasets by focusing\non Hungarian, approximately the 17th highest resource language on the internet.\nSpecifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets\nthat modern VLMs significantly underperform on compared to English DocVQA.\nHuDocVQA-manual is a small manually curated dataset based on Hungarian\ndocuments from Common Crawl, while HuDocVQA is a larger synthetically generated\nVQA data set from the same source. We apply multiple rounds of quality\nfiltering and deduplication to HuDocVQA in order to match human-level quality\nin this dataset. We also present HuCCPDF, a dataset of 117k pages from\nHungarian Common Crawl PDFs along with their transcriptions, which can be used\nfor training a model for Hungarian OCR. To validate the quality of our\ndatasets, we show how finetuning on a mixture of these datasets can improve\naccuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code\nwill be released to the public to foster further research in multilingual\nDocVQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5308\u7259\u5229\u8bed\uff09\u6784\u5efa\u6587\u6863\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86HuDocVQA\u548cHuDocVQA-manual\u6570\u636e\u96c6\uff0c\u4ee5\u53caHuCCPDF\u6570\u636e\u96c6\u7528\u4e8eOCR\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u5308\u7259\u5229\u8bed\uff09\u5728\u6587\u6863VQA\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4eceCommon Crawl\u4e2d\u63d0\u53d6\u5308\u7259\u5229\u8bed\u6587\u6863\uff0c\u6784\u5efa\u4e86\u624b\u52a8\u548c\u5408\u6210\u7684VQA\u6570\u636e\u96c6\uff08HuDocVQA\u548cHuDocVQA-manual\uff09\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u8f6e\u8d28\u91cf\u8fc7\u6ee4\u548c\u53bb\u91cd\u3002\u540c\u65f6\u53d1\u5e03\u4e86HuCCPDF\u6570\u636e\u96c6\u7528\u4e8eOCR\u8bad\u7ec3\u3002", "result": "\u5fae\u8c03Llama 3.2 11B Instruct\u6a21\u578b\u5728HuDocVQA\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e867.2%\u3002", "conclusion": "\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u4fc3\u8fdb\u591a\u8bed\u8a00\u6587\u6863VQA\u7684\u7814\u7a76\u3002"}}
{"id": "2505.22943", "pdf": "https://arxiv.org/pdf/2505.22943", "abs": "https://arxiv.org/abs/2505.22943", "authors": ["Jaewoo Ahn", "Heeseung Yun", "Dayoon Ko", "Gunhee Kim"], "title": "Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "ACL 2025 Main. Code is released at\n  https://vision.snu.ac.kr/projects/mac", "summary": "While pre-trained multimodal representations (e.g., CLIP) have shown\nimpressive capabilities, they exhibit significant compositional vulnerabilities\nleading to counterintuitive judgments. We introduce Multimodal Adversarial\nCompositionality (MAC), a benchmark that leverages large language models (LLMs)\nto generate deceptive text samples to exploit these vulnerabilities across\ndifferent modalities and evaluates them through both sample-wise attack success\nrate and group-wise entropy-based diversity. To improve zero-shot methods, we\npropose a self-training approach that leverages rejection-sampling fine-tuning\nwith diversity-promoting filtering, which enhances both attack success rate and\nsample diversity. Using smaller language models like Llama-3.1-8B, our approach\ndemonstrates superior performance in revealing compositional vulnerabilities\nacross various multimodal representations, including images, videos, and\naudios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6297\u7ec4\u5408\u6027\uff08MAC\uff09\u57fa\u51c6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\u6765\u6d4b\u8bd5\u591a\u6a21\u6001\u8868\u793a\u7684\u7ec4\u5408\u6027\u6f0f\u6d1e\uff0c\u5e76\u901a\u8fc7\u81ea\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u8868\u793a\uff08\u5982CLIP\uff09\u867d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u7ec4\u5408\u6027\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u53cd\u76f4\u89c9\u5224\u65ad\u3002\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5e76\u6539\u8fdb\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faMAC\u57fa\u51c6\uff0c\u5229\u7528LLMs\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff08\u62d2\u7edd\u91c7\u6837\u5fae\u8c03\u548c\u591a\u6837\u6027\u8fc7\u6ee4\uff09\u63d0\u5347\u653b\u51fb\u6210\u529f\u7387\u548c\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u4f7f\u7528\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama-3.1-8B\uff09\u7684\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u8868\u793a\uff08\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63ed\u793a\u4e86\u7ec4\u5408\u6027\u6f0f\u6d1e\u3002", "conclusion": "MAC\u57fa\u51c6\u548c\u81ea\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u63ed\u793a\u4e86\u591a\u6a21\u6001\u8868\u793a\u7684\u7ec4\u5408\u6027\u6f0f\u6d1e\uff0c\u5e76\u63d0\u5347\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23010", "pdf": "https://arxiv.org/pdf/2505.23010", "abs": "https://arxiv.org/abs/2505.23010", "authors": ["Bowen Chen", "Keyan Chen", "Mohan Yang", "Zhengxia Zou", "Zhenwei Shi"], "title": "SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution (HR) remote sensing imagery plays a vital role in a wide\nrange of applications, including urban planning and environmental monitoring.\nHowever, due to limitations in sensors and data transmission links, the images\nacquired in practice often suffer from resolution degradation. Remote Sensing\nImage Super-Resolution (RSISR) aims to reconstruct HR images from\nlow-resolution (LR) inputs, providing a cost-effective and efficient\nalternative to direct HR image acquisition. Existing RSISR methods primarily\nfocus on low-level characteristics in pixel space, while neglecting the\nhigh-level understanding of remote sensing scenes. This may lead to\nsemantically inconsistent artifacts in the reconstructed results. Motivated by\nthis observation, our work aims to explore the role of high-level semantic\nknowledge in improving RSISR performance. We propose a Semantic-Guided\nSuper-Resolution framework, SeG-SR, which leverages Vision-Language Models\n(VLMs) to extract semantic knowledge from input images and uses it to guide the\nsuper resolution (SR) process. Specifically, we first design a Semantic Feature\nExtraction Module (SFEM) that utilizes a pretrained VLM to extract semantic\nknowledge from remote sensing images. Next, we propose a Semantic Localization\nModule (SLM), which derives a series of semantic guidance from the extracted\nsemantic knowledge. Finally, we develop a Learnable Modulation Module (LMM)\nthat uses semantic guidance to modulate the features extracted by the SR\nnetwork, effectively incorporating high-level scene understanding into the SR\npipeline. We validate the effectiveness and generalizability of SeG-SR through\nextensive experiments: SeG-SR achieves state-of-the-art performance on two\ndatasets and consistently delivers performance improvements across various SR\narchitectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u8d85\u5206\u8fa8\u7387\u6846\u67b6SeG-SR\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u50cf\u7d20\u7a7a\u95f4\u4f4e\u5c42\u7279\u5f81\uff0c\u5ffd\u89c6\u4e86\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u8bed\u4e49\u4e0d\u4e00\u81f4\u3002", "method": "\u8bbe\u8ba1\u4e86\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff08SFEM\uff09\u3001\u8bed\u4e49\u5b9a\u4f4d\u6a21\u5757\uff08SLM\uff09\u548c\u53ef\u5b66\u4e60\u8c03\u5236\u6a21\u5757\uff08LMM\uff09\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u8bed\u4e49\u77e5\u8bc6\u6307\u5bfc\u8d85\u5206\u8fa8\u7387\u8fc7\u7a0b\u3002", "result": "SeG-SR\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u8d85\u5206\u8fa8\u7387\u67b6\u6784\u4e2d\u8868\u73b0\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "SeG-SR\u901a\u8fc7\u5f15\u5165\u9ad8\u5c42\u8bed\u4e49\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.22945", "pdf": "https://arxiv.org/pdf/2505.22945", "abs": "https://arxiv.org/abs/2505.22945", "authors": ["Alisha Srivastava", "Emir Korukluoglu", "Minh Nhat Le", "Duyen Tran", "Chau Minh Pham", "Marzena Karpinska", "Mohit Iyyer"], "title": "OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature", "categories": ["cs.CL", "cs.AI"], "comment": "preprint, 25 pages", "summary": "Large language models (LLMs) are known to memorize and recall English text\nfrom their pretraining data. However, the extent to which this ability\ngeneralizes to non-English languages or transfers across languages remains\nunclear. This paper investigates multilingual and cross-lingual memorization in\nLLMs, probing if memorized content in one language (e.g., English) can be\nrecalled when presented in translation. To do so, we introduce OWL, a dataset\nof 31.5K aligned excerpts from 20 books in ten languages, including English\noriginals, official translations (Vietnamese, Spanish, Turkish), and new\ntranslations in six low-resource languages (Sesotho, Yoruba, Maithili,\nMalagasy, Setswana, Tahitian). We evaluate memorization across model families\nand sizes through three tasks: (1) direct probing, which asks the model to\nidentify a book's title and author; (2) name cloze, which requires predicting\nmasked character names; and (3) prefix probing, which involves generating\ncontinuations. We find that LLMs consistently recall content across languages,\neven for texts without direct translation in pretraining data. GPT-4o, for\nexample, identifies authors and titles 69% of the time and masked entities 6%\nof the time in newly translated excerpts. Perturbations (e.g., masking\ncharacters, shuffling words) modestly reduce direct probing accuracy (7% drop\nfor shuffled official translations). Our results highlight the extent of\ncross-lingual memorization and provide insights on the differences between the\nmodels.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8bb0\u5fc6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u8de8\u8bed\u8a00\u56de\u5fc6\u5185\u5bb9\uff0c\u5373\u4f7f\u6587\u672c\u672a\u76f4\u63a5\u51fa\u73b0\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "motivation": "LLMs\u5728\u82f1\u8bed\u6587\u672c\u8bb0\u5fc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u6216\u8de8\u8bed\u8a00\u8bb0\u5fc6\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528OWL\u6570\u636e\u96c6\uff08\u5305\u542b10\u79cd\u8bed\u8a00\u76843.15\u4e07\u6761\u5bf9\u9f50\u6587\u672c\uff09\uff0c\u901a\u8fc7\u4e09\u79cd\u4efb\u52a1\uff08\u76f4\u63a5\u63a2\u6d4b\u3001\u540d\u79f0\u586b\u7a7a\u3001\u524d\u7f00\u63a2\u6d4b\uff09\u8bc4\u4f30\u6a21\u578b\u8bb0\u5fc6\u80fd\u529b\u3002", "result": "LLMs\u80fd\u8de8\u8bed\u8a00\u56de\u5fc6\u5185\u5bb9\uff0c\u4f8b\u5982GPT-4o\u5728\u65b0\u7ffb\u8bd1\u6587\u672c\u4e2d\u8bc6\u522b\u4f5c\u8005\u548c\u6807\u9898\u7684\u51c6\u786e\u7387\u4e3a69%\u3002\u6270\u52a8\uff08\u5982\u6253\u4e71\u5355\u8bcd\uff09\u4f1a\u7565\u5fae\u964d\u4f4e\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u7684\u8de8\u8bed\u8a00\u8bb0\u5fc6\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u95f4\u5dee\u5f02\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.23012", "pdf": "https://arxiv.org/pdf/2505.23012", "abs": "https://arxiv.org/abs/2505.23012", "authors": ["Shanaka Ramesh Gunasekara", "Wanqing Li", "Philip Ogunbona", "Jack Yang"], "title": "Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional approaches in unsupervised or self supervised learning for\nskeleton-based action classification have concentrated predominantly on the\ndynamic aspects of skeletal sequences. Yet, the intricate interaction between\nthe moving and static elements of the skeleton presents a rarely tapped\ndiscriminative potential for action classification. This paper introduces a\nnovel measurement, referred to as spatial-temporal joint density (STJD), to\nquantify such interaction. Tracking the evolution of this density throughout an\naction can effectively identify a subset of discriminative moving and/or static\njoints termed \"prime joints\" to steer self-supervised learning. A new\ncontrastive learning strategy named STJD-CL is proposed to align the\nrepresentation of a skeleton sequence with that of its prime joints while\nsimultaneously contrasting the representations of prime and nonprime joints. In\naddition, a method called STJD-MP is developed by integrating it with a\nreconstruction-based framework for more effective learning. Experimental\nevaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various\ndownstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved\nperformance, particularly by 3.5 and 3.6 percentage points over the\nstate-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub\nand X-set evaluations, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4-\u65f6\u95f4\u5173\u8282\u5bc6\u5ea6\uff08STJD\uff09\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u9aa8\u67b6\u5e8f\u5217\u4e2d\u52a8\u6001\u4e0e\u9759\u6001\u5173\u8282\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86STJD-CL\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548cSTJD-MP\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9aa8\u67b6\u5e8f\u5217\u7684\u52a8\u6001\u7279\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u52a8\u6001\u4e0e\u9759\u6001\u5173\u8282\u4ea4\u4e92\u7684\u5224\u522b\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u6316\u6398\u8fd9\u79cd\u4ea4\u4e92\u4f5c\u7528\u4ee5\u6539\u8fdb\u52a8\u4f5c\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86STJD\u6d4b\u91cf\u65b9\u6cd5\uff0c\u8bc6\u522b\u5173\u952e\u5173\u8282\uff08prime joints\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86STJD-CL\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548cSTJD-MP\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cd\u5efa\u6846\u67b6\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728NTU RGB+D 60\u3001NTU RGB+D 120\u548cPKUMMD\u6570\u636e\u96c6\u4e0a\uff0cSTJD-CL\u548cSTJD-MP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728NTU RGB+D 120\u4e0a\u5206\u522b\u63d0\u53473.5\u548c3.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u901a\u8fc7\u91cf\u5316\u52a8\u6001\u4e0e\u9759\u6001\u5173\u8282\u7684\u4ea4\u4e92\u4f5c\u7528\uff0cSTJD\u65b9\u6cd5\u4e3a\u9aa8\u67b6\u52a8\u4f5c\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u7684\u5224\u522b\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.22946", "pdf": "https://arxiv.org/pdf/2505.22946", "abs": "https://arxiv.org/abs/2505.22946", "authors": ["Yuhui Zhang", "Yuchang Su", "Yiming Liu", "Serena Yeung-Levy"], "title": "NegVQA: Can Vision Language Models Understand Negation?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "cs.LG"], "comment": "Published at ACL 2025 Findings", "summary": "Negation is a fundamental linguistic phenomenon that can entirely reverse the\nmeaning of a sentence. As vision language models (VLMs) continue to advance and\nare deployed in high-stakes applications, assessing their ability to comprehend\nnegation becomes essential. To address this, we introduce NegVQA, a visual\nquestion answering (VQA) benchmark consisting of 7,379 two-choice questions\ncovering diverse negation scenarios and image-question distributions. We\nconstruct NegVQA by leveraging large language models to generate negated\nversions of questions from existing VQA datasets. Evaluating 20\nstate-of-the-art VLMs across seven model families, we find that these models\nstruggle significantly with negation, exhibiting a substantial performance drop\ncompared to their responses to the original questions. Furthermore, we uncover\na U-shaped scaling trend, where increasing model size initially degrades\nperformance on NegVQA before leading to improvements. Our benchmark reveals\ncritical gaps in VLMs' negation understanding and offers insights into future\nVLM development. Project page available at\nhttps://yuhui-zh15.github.io/NegVQA/.", "AI": {"tldr": "NegVQA\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u5426\u5b9a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5426\u5b9a\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5426\u5b9a\u7406\u89e3\u4e0a\u7684\u80fd\u529b\uff0c\u56e0\u4e3a\u5426\u5b9a\u662f\u8bed\u8a00\u4e2d\u7684\u57fa\u672c\u73b0\u8c61\uff0c\u53ef\u80fd\u5b8c\u5168\u6539\u53d8\u53e5\u5b50\u542b\u4e49\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u73b0\u6709VQA\u6570\u636e\u96c6\u4e2d\u751f\u6210\u5426\u5b9a\u95ee\u9898\uff0c\u6784\u5efa\u5305\u542b7,379\u4e2a\u95ee\u9898\u7684NegVQA\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f3020\u4e2a\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5426\u5b9a\u95ee\u9898\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u5448\u73b0U\u578b\u6269\u5c55\u8d8b\u52bf\u3002", "conclusion": "NegVQA\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5426\u5b9a\u7406\u89e3\u4e0a\u7684\u5173\u952e\u7f3a\u9677\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23031", "pdf": "https://arxiv.org/pdf/2505.23031", "abs": "https://arxiv.org/abs/2505.23031", "authors": ["Jinyi Chang", "Dongliang Chang", "Lei Chen", "Bingyao Yu", "Zhanyu Ma"], "title": "Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 5 tables", "summary": "In recent years, Fine-Grained Visual Classification (FGVC) has achieved\nimpressive recognition accuracy, despite minimal inter-class variations.\nHowever, existing methods heavily rely on instance-level labels, making them\nimpractical in privacy-sensitive scenarios such as medical image analysis. This\npaper aims to enable accurate fine-grained recognition without direct access to\ninstance labels. To achieve this, we leverage the Learning from Label\nProportions (LLP) paradigm, which requires only bag-level labels for efficient\ntraining. Unlike existing LLP-based methods, our framework explicitly exploits\nthe hierarchical nature of fine-grained datasets, enabling progressive feature\ngranularity refinement and improving classification accuracy. We propose\nLearning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework\nthat incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary\nLearning, transforming handcrafted iterative approximation into learnable\nnetwork optimization. Additionally, our proposed Hierarchical Proportion Loss\nprovides hierarchical supervision, further enhancing classification\nperformance. Experiments on three widely-used fine-grained datasets, structured\nin a bag-based manner, demonstrate that our framework consistently outperforms\nexisting LLP-based methods. We will release our code and datasets to foster\nfurther research in privacy-preserving fine-grained classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5b9e\u4f8b\u7ea7\u6807\u7b7e\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u6807\u7b7e\u6bd4\u4f8b\u5b66\u4e60\uff08LLP\uff09\u8303\u5f0f\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u7279\u5f81\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7ec6\u7c92\u5ea6\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u5b9e\u4f8b\u7ea7\u6807\u7b7e\uff0c\u4e0d\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u573a\u666f\uff08\u5982\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff09\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faLHFGLP\u6846\u67b6\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u548c\u5c42\u6b21\u5316\u6bd4\u4f8b\u635f\u5931\uff0c\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u7279\u5f81\u4f18\u5316\u3002", "result": "\u5728\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\uff0cLHFGLP\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LLP\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2505.22950", "pdf": "https://arxiv.org/pdf/2505.22950", "abs": "https://arxiv.org/abs/2505.22950", "authors": ["Haohan Yuan", "Sukhwa Hong", "Haopeng Zhang"], "title": "StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance in zero-shot\nsummarization, but often struggle to model document structure and identify\nsalient information in long texts. In this work, we introduce StrucSum, a\ntraining-free prompting framework that enhances LLM reasoning through\nsentence-level graph structures. StrucSum injects structural signals into\nprompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local\ncontext, Centrality-Aware Prompting (CAP) for importance estimation, and\nCentrality-Guided Masking (CGM) for efficient input reduction. Experiments on\nArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves\nboth summary quality and factual consistency over unsupervised baselines and\nvanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and\n9.7 points, indicating stronger alignment between summaries and source content.\nThese findings suggest that structure-aware prompting is a simple yet effective\napproach for zero-shot extractive summarization with LLMs, without any training\nor task-specific tuning.", "AI": {"tldr": "StrucSum\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u5b50\u7ea7\u56fe\u7ed3\u6784\u589e\u5f3aLLM\u5728\u96f6\u6837\u672c\u6458\u8981\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347\u6458\u8981\u8d28\u91cf\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u96f6\u6837\u672c\u6458\u8981\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u6587\u672c\u4e2d\u5efa\u6a21\u6587\u6863\u7ed3\u6784\u548c\u8bc6\u522b\u5173\u952e\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "StrucSum\u901a\u8fc7\u4e09\u79cd\u7b56\u7565\u6ce8\u5165\u7ed3\u6784\u4fe1\u53f7\uff1a\u90bb\u5c45\u611f\u77e5\u63d0\u793a\uff08NAP\uff09\u7528\u4e8e\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u4e2d\u5fc3\u6027\u611f\u77e5\u63d0\u793a\uff08CAP\uff09\u7528\u4e8e\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u4ee5\u53ca\u4e2d\u5fc3\u6027\u5f15\u5bfc\u63a9\u7801\uff08CGM\uff09\u7528\u4e8e\u9ad8\u6548\u8f93\u5165\u7f29\u51cf\u3002", "result": "\u5728ArXiv\u3001PubMed\u548cMulti-News\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cStrucSum\u5728\u6458\u8981\u8d28\u91cf\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u4e0a\u5747\u4f18\u4e8e\u65e0\u76d1\u7763\u57fa\u7ebf\u548c\u666e\u901a\u63d0\u793a\u65b9\u6cd5\uff0c\u5c24\u5176\u5728ArXiv\u4e0aFactCC\u548cSummaC\u5206\u522b\u63d0\u534719.2\u548c9.7\u5206\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u63d0\u793a\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u96f6\u6837\u672c\u62bd\u53d6\u5f0f\u6458\u8981\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u4efb\u52a1\u7279\u5b9a\u8c03\u6574\u3002"}}
{"id": "2505.23040", "pdf": "https://arxiv.org/pdf/2505.23040", "abs": "https://arxiv.org/abs/2505.23040", "authors": ["Yihang Wu", "Muhammad Owais", "Reem Kateb", "Ahmad Chaddad"], "title": "Deep Modeling and Optimization of Medical Image Classification", "categories": ["cs.CV"], "comment": "Accepted in ISBI2025", "summary": "Deep models, such as convolutional neural networks (CNNs) and vision\ntransformer (ViT), demonstrate remarkable performance in image classification.\nHowever, those deep models require large data to fine-tune, which is\nimpractical in the medical domain due to the data privacy issue. Furthermore,\ndespite the feasible performance of contrastive language image pre-training\n(CLIP) in the natural domain, the potential of CLIP has not been fully\ninvestigated in the medical field. To face these challenges, we considered\nthree scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight\nViTs as image encoders for the classification of brain cancer and skin cancer,\n2) we combine 12 deep models with two federated learning techniques to protect\ndata privacy, and 3) we involve traditional machine learning (ML) methods to\nimprove the generalization ability of those deep models in unseen domain data.\nThe experimental results indicate that maxvit shows the highest averaged (AVG)\ntest metrics (AVG = 87.03\\%) in HAM10000 dataset with multimodal learning,\nwhile convnext\\_l demonstrates remarkable test with an F1-score of 83.98\\%\ncompared to swin\\_b with 81.33\\% in FL model. Furthermore, the use of support\nvector machine (SVM) can improve the overall test metrics with AVG of $\\sim\n2\\%$ for swin transformer series in ISIC2018. Our codes are available at\nhttps://github.com/AIPMLab/SkinCancerSimulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684CLIP\u53d8\u4f53\uff0c\u7ed3\u5408\u591a\u79cd\u6df1\u5ea6\u6a21\u578b\u548c\u8054\u90a6\u5b66\u4e60\u6280\u672f\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u540c\u65f6\u5229\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u9886\u57df\u56e0\u6570\u636e\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u7684\u5927\u89c4\u6a21\u6570\u636e\u5fae\u8c03\u56f0\u96be\uff0c\u5e76\u63a2\u7d22CLIP\u5728\u533b\u5b66\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8eCNN\u548cViT\u7684CLIP\u53d8\u4f53\uff1b2) \u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff1b3) \u5f15\u5165\u4f20\u7edfML\u65b9\u6cd5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "MaxViT\u5728HAM10000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff08AVG=87.03%\uff09\uff0cConvNeXt_L\u5728FL\u6a21\u578b\u4e2dF1-score\u8fbe83.98%\u3002SVM\u8fdb\u4e00\u6b65\u63d0\u5347Swin Transformer\u7cfb\u5217\u6027\u80fd\uff08\u7ea62%\uff09\u3002", "conclusion": "\u6539\u8fdb\u7684CLIP\u53d8\u4f53\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u4f20\u7edfML\u65b9\u6cd5\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2505.22956", "pdf": "https://arxiv.org/pdf/2505.22956", "abs": "https://arxiv.org/abs/2505.22956", "authors": ["Matteo Guida", "Yulia Otmakhova", "Eduard Hovy", "Lea Frermann"], "title": "LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments", "categories": ["cs.CL"], "comment": null, "summary": "Automated large-scale analysis of public discussions around contested issues\nlike abortion requires detecting and understanding the use of arguments. While\nLarge Language Models (LLMs) have shown promise in language processing tasks,\ntheir performance in mining topic-specific, pre-defined arguments in online\ncomments remains underexplored. We evaluate four state-of-the-art LLMs on three\nargument mining tasks using datasets comprising over 2,000 opinion comments\nacross six polarizing topics. Quantitative evaluation suggests an overall\nstrong performance across the three tasks, especially for large and fine-tuned\nLLMs, albeit at a significant environmental cost. However, a detailed error\nanalysis revealed systematic shortcomings on long and nuanced comments and\nemotionally charged language, raising concerns for downstream applications like\ncontent moderation or opinion analysis. Our results highlight both the promise\nand current limitations of LLMs for automated argument analysis in online\ncomments.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u56db\u79cd\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e09\u4e2a\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u5904\u7406\u5927\u89c4\u6a21\u5728\u7ebf\u8bc4\u8bba\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u957f\u6587\u672c\u548c\u60c5\u611f\u5316\u8bed\u8a00\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLM\u5728\u68c0\u6d4b\u548c\u5206\u6790\u4e89\u8bae\u6027\u8bdd\u9898\uff08\u5982\u5815\u80ce\uff09\u4e2d\u9884\u5b9a\u4e49\u8bba\u70b9\u7684\u80fd\u529b\uff0c\u586b\u8865\u5176\u5728\u5728\u7ebf\u8bc4\u8bba\u4e2d\u5e94\u7528\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5bf9\u56db\u79cdLLM\u5728\u4e09\u4e2a\u8bba\u70b9\u6316\u6398\u4efb\u52a1\u4e0a\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u4f7f\u7528\u8d85\u8fc72000\u6761\u8bc4\u8bba\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u516d\u4e2a\u4e89\u8bae\u6027\u8bdd\u9898\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u548c\u5fae\u8c03\u540e\u7684LLM\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5904\u7406\u957f\u6587\u672c\u548c\u60c5\u611f\u5316\u8bed\u8a00\u7684\u4e0d\u8db3\uff0c\u4e14\u73af\u5883\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLM\u5728\u81ea\u52a8\u5316\u8bba\u70b9\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u5e94\u5bf9\u590d\u6742\u8bed\u8a00\u548c\u60c5\u611f\u5316\u5185\u5bb9\u3002"}}
{"id": "2505.23043", "pdf": "https://arxiv.org/pdf/2505.23043", "abs": "https://arxiv.org/abs/2505.23043", "authors": ["Jihai Zhang", "Tianle Li", "Linjie Li", "Zhengyuan Yang", "Yu Cheng"], "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in unified vision-language models (VLMs), which integrate\nboth visual understanding and generation capabilities, have attracted\nsignificant attention. The underlying hypothesis is that a unified architecture\nwith mixed training on both understanding and generation tasks can enable\nmutual enhancement between understanding and generation. However, this\nhypothesis remains underexplored in prior works on unified VLMs. To address\nthis gap, this paper systematically investigates the generalization across\nunderstanding and generation tasks in unified VLMs. Specifically, we design a\ndataset closely aligned with real-world scenarios to facilitate extensive\nexperiments and quantitative evaluations. We evaluate multiple unified VLM\narchitectures to validate our findings. Our key findings are as follows. First,\nunified VLMs trained with mixed data exhibit mutual benefits in understanding\nand generation tasks across various architectures, and this mutual benefits can\nscale up with increased data. Second, better alignment between multimodal input\nand output spaces will lead to better generalization. Third, the knowledge\nacquired during generation tasks can transfer to understanding tasks, and this\ncross-task generalization occurs within the base language model, beyond\nmodality adapters. Our findings underscore the critical necessity of unifying\nunderstanding and generation in VLMs, offering valuable insights for the design\nand optimization of unified VLMs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u76f8\u4e92\u589e\u5f3a\uff0c\u53d1\u73b0\u6df7\u5408\u8bad\u7ec3\u80fd\u5e26\u6765\u663e\u8457\u76ca\u5904\uff0c\u5e76\u63d0\u51fa\u591a\u6a21\u6001\u5bf9\u9f50\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u7edf\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u76f8\u4e92\u589e\u5f3a\u5047\u8bbe\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u8d34\u8fd1\u73b0\u5b9e\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u79cd\u7edf\u4e00VLM\u67b6\u6784\uff0c\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u6df7\u5408\u8bad\u7ec3\u5e26\u6765\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u76f8\u4e92\u63d0\u5347\uff0c\u591a\u6a21\u6001\u5bf9\u9f50\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u751f\u6210\u4efb\u52a1\u77e5\u8bc6\u53ef\u8fc1\u79fb\u81f3\u7406\u89e3\u4efb\u52a1\u3002", "conclusion": "\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u5bf9VLMs\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u6a21\u578b\u8bbe\u8ba1\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.22959", "pdf": "https://arxiv.org/pdf/2505.22959", "abs": "https://arxiv.org/abs/2505.22959", "authors": ["Jianwei Wang", "Mengqi Wang", "Yinsi Zhou", "Zhenchang Xing", "Qing Liu", "Xiwei Xu", "Wenjie Zhang", "Liming Zhu"], "title": "LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements", "categories": ["cs.CL"], "comment": null, "summary": "Health, Safety, and Environment (HSE) compliance assessment demands dynamic\nreal-time decision-making under complicated regulations and complex\nhuman-machine-environment interactions. While large language models (LLMs) hold\nsignificant potential for decision intelligence and contextual dialogue, their\ncapacity for domain-specific knowledge in HSE and structured legal reasoning\nremains underexplored. We introduce HSE-Bench, the first benchmark dataset\ndesigned to evaluate the HSE compliance assessment capabilities of LLM.\nHSE-Bench comprises over 1,000 manually curated questions drawn from\nregulations, court cases, safety exams, and fieldwork videos, and integrates a\nreasoning flow based on Issue spotting, rule Recall, rule Application, and rule\nConclusion (IRAC) to assess the holistic reasoning pipeline. We conduct\nextensive evaluations on different prompting strategies and more than 10 LLMs,\nincluding foundation models, reasoning models and multimodal vision models. The\nresults show that, although current LLMs achieve good performance, their\ncapabilities largely rely on semantic matching rather than principled reasoning\ngrounded in the underlying HSE compliance context. Moreover, their native\nreasoning trace lacks the systematic legal reasoning required for rigorous HSE\ncompliance assessment. To alleviate these, we propose a new prompting\ntechnique, Reasoning of Expert (RoE), which guides LLMs to simulate the\nreasoning process of different experts for compliance assessment and reach a\nmore accurate unified decision. We hope our study highlights reasoning gaps in\nLLMs for HSE compliance and inspires further research on related tasks.", "AI": {"tldr": "HSE-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728HSE\u5408\u89c4\u6027\u8bc4\u4f30\u4e2d\u80fd\u529b\u7684\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u591a\u4e2a\u95ee\u9898\uff0c\u91c7\u7528IRAC\u63a8\u7406\u6d41\u7a0b\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLM\u4f9d\u8d56\u8bed\u4e49\u5339\u914d\u800c\u975e\u539f\u5219\u6027\u63a8\u7406\uff0c\u5e76\u63d0\u51faRoE\u63d0\u793a\u6280\u672f\u4ee5\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22LLM\u5728HSE\u5408\u89c4\u6027\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5176\u5728\u9886\u57df\u77e5\u8bc6\u548c\u7ed3\u6784\u5316\u6cd5\u5f8b\u63a8\u7406\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efaHSE-Bench\u6570\u636e\u96c6\uff0c\u91c7\u7528IRAC\u63a8\u7406\u6d41\u7a0b\u8bc4\u4f30LLM\uff0c\u5e76\u63d0\u51faRoE\u63d0\u793a\u6280\u672f\u6a21\u62df\u4e13\u5bb6\u63a8\u7406\u3002", "result": "\u5f53\u524dLLM\u8868\u73b0\u826f\u597d\u4f46\u4f9d\u8d56\u8bed\u4e49\u5339\u914d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6cd5\u5f8b\u63a8\u7406\u3002RoE\u6280\u672f\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728HSE\u5408\u89c4\u6027\u8bc4\u4f30\u4e2d\u7684\u63a8\u7406\u7f3a\u9677\uff0cRoE\u6280\u672f\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23044", "pdf": "https://arxiv.org/pdf/2505.23044", "abs": "https://arxiv.org/abs/2505.23044", "authors": ["Yu Sheng", "Jiajun Deng", "Xinran Zhang", "Yu Zhang", "Bei Hua", "Yanyong Zhang", "Jianmin Ji"], "title": "SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images", "categories": ["cs.CV"], "comment": null, "summary": "A major breakthrough in 3D reconstruction is the feedforward paradigm to\ngenerate pixel-wise 3D points or Gaussian primitives from sparse, unposed\nimages. To further incorporate semantics while avoiding the significant memory\nand storage costs of high-dimensional semantic features, existing methods\nextend this paradigm by associating each primitive with a compressed semantic\nfeature vector. However, these methods have two major limitations: (a) the\nnaively compressed feature compromises expressiveness, affecting the model's\nability to capture fine-grained semantics, and (b) the pixel-wise primitive\nprediction introduces redundancy in overlapping areas, causing unnecessary\nmemory overhead. To this end, we introduce \\textbf{SpatialSplat}, a feedforward\nframework that produces redundancy-aware Gaussians and capitalizes on a\ndual-field semantic representation. Particularly, with the insight that\nprimitives within the same instance exhibit high semantic consistency, we\ndecompose the semantic representation into a coarse feature field that encodes\nuncompressed semantics with minimal primitives, and a fine-grained yet\nlow-dimensional feature field that captures detailed inter-instance\nrelationships. Moreover, we propose a selective Gaussian mechanism, which\nretains only essential Gaussians in the scene, effectively eliminating\nredundant primitives. Our proposed Spatialsplat learns accurate semantic\ninformation and detailed instances prior with more compact 3D Gaussians, making\nsemantic 3D reconstruction more applicable. We conduct extensive experiments to\nevaluate our method, demonstrating a remarkable 60\\% reduction in scene\nrepresentation parameters while achieving superior performance over\nstate-of-the-art methods. The code will be made available for future\ninvestigation.", "AI": {"tldr": "SpatialSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u573a\u8bed\u4e49\u8868\u793a\u548c\u9009\u62e9\u6027\u9ad8\u65af\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5197\u4f59\u5e76\u63d0\u5347\u4e86\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u538b\u7f29\u8bed\u4e49\u7279\u5f81\u65f6\u727a\u7272\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u4e14\u50cf\u7d20\u7ea7\u9884\u6d4b\u5bfc\u81f4\u5185\u5b58\u5197\u4f59\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0cSpatialSplat\u65e8\u5728\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u8bed\u4e493D\u91cd\u5efa\u3002", "method": "\u91c7\u7528\u53cc\u573a\u8bed\u4e49\u8868\u793a\uff08\u7c97\u7c92\u5ea6\u573a\u548c\u7ec6\u7c92\u5ea6\u573a\uff09\u548c\u9009\u62e9\u6027\u9ad8\u65af\u673a\u5236\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u4fdd\u7559\u5173\u952e\u9ad8\u65af\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e8660%\u7684\u573a\u666f\u8868\u793a\u53c2\u6570\uff0c\u540c\u65f6\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SpatialSplat\u901a\u8fc7\u7d27\u51d1\u76843D\u9ad8\u65af\u8868\u793a\u548c\u9ad8\u6548\u8bed\u4e49\u7f16\u7801\uff0c\u4e3a\u8bed\u4e493D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22961", "pdf": "https://arxiv.org/pdf/2505.22961", "abs": "https://arxiv.org/abs/2505.22961", "authors": ["Peixuan Han", "Zijia Liu", "Jiaxuan You"], "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToMAP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4e24\u4e2a\u5fc3\u667a\u7406\u8bba\u6a21\u5757\uff0c\u589e\u5f3aLLM\u5728\u8bf4\u670d\u4efb\u52a1\u4e2d\u5bf9\u5bf9\u624b\u5fc3\u7406\u72b6\u6001\u7684\u611f\u77e5\u548c\u5206\u6790\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u670d\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8bf4\u670d\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u5bf9\u5bf9\u624b\u5fc3\u7406\u72b6\u6001\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u5bfc\u81f4\u8bf4\u670d\u591a\u6837\u6027\u548c\u5bf9\u624b\u610f\u8bc6\u4e0d\u8db3\u3002", "method": "ToMAP\u7ed3\u5408\u4e86\u5fc3\u667a\u7406\u8bba\u6a21\u5757\uff0c\u901a\u8fc7\u9884\u6d4b\u5bf9\u624b\u7acb\u573a\u548c\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u66f4\u6709\u6548\u7684\u8bba\u70b9\u3002", "result": "ToMAP\u57283B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u6027\u80fd\u8d85\u8fc7GPT-4o\u7b49\u66f4\u5927\u6a21\u578b\uff0c\u76f8\u5bf9\u589e\u76ca\u8fbe39.4%\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u591a\u6837\u5316\u548c\u903b\u8f91\u6027\u5f3a\u7684\u8bba\u70b9\u3002", "conclusion": "ToMAP\u5c55\u793a\u4e86\u5728\u8bf4\u670d\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u6f5c\u529b\uff0c\u9002\u5408\u957f\u5bf9\u8bdd\u548c\u903b\u8f91\u6027\u5f3a\u7684\u7b56\u7565\u3002"}}
{"id": "2505.23045", "pdf": "https://arxiv.org/pdf/2505.23045", "abs": "https://arxiv.org/abs/2505.23045", "authors": ["Chuanhao Li", "Wenbo Ye", "Zhen Li", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Sourced Compositional Generalization in Visual Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Compositional generalization is the ability of generalizing novel\ncompositions from seen primitives, and has received much attention in\nvision-and-language (V\\&L) recently. Due to the multi-modal nature of V\\&L\ntasks, the primitives composing compositions source from different modalities,\nresulting in multi-sourced novel compositions. However, the generalization\nability over multi-sourced novel compositions, \\textit{i.e.}, multi-sourced\ncompositional generalization (MSCG) remains unexplored. In this paper, we\nexplore MSCG in the context of visual question answering (VQA), and propose a\nretrieval-augmented training framework to enhance the MSCG ability of VQA\nmodels by learning unified representations for primitives from different\nmodalities. Specifically, semantically equivalent primitives are retrieved for\neach primitive in the training samples, and the retrieved features are\naggregated with the original primitive to refine the model. This process helps\nthe model learn consistent representations for the same semantic primitives\nacross different modalities. To evaluate the MSCG ability of VQA models, we\nconstruct a new GQA-MSCG dataset based on the GQA dataset, in which samples\ninclude three types of novel compositions composed of primitives from different\nmodalities. Experimental results demonstrate the effectiveness of the proposed\nframework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u591a\u6e90\u7ec4\u5408\u6cdb\u5316\uff08MSCG\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u7684MSCG\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u7684\u591a\u6a21\u6001\u7279\u6027\uff0c\u7ec4\u5408\u7684\u57fa\u5143\u6765\u81ea\u4e0d\u540c\u6a21\u6001\uff0c\u5bfc\u81f4\u591a\u6e90\u65b0\u7ec4\u5408\u7684\u6cdb\u5316\u80fd\u529b\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u8bed\u4e49\u7b49\u6548\u7684\u57fa\u5143\u5e76\u805a\u5408\u5176\u7279\u5f81\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86VQA\u6a21\u578b\u7684MSCG\u80fd\u529b\uff0c\u5e76\u57fa\u4e8eGQA\u6570\u636e\u96c6\u6784\u5efa\u4e86\u65b0\u7684GQA-MSCG\u8bc4\u6d4b\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u6e90\u7ec4\u5408\u6cdb\u5316\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.22964", "pdf": "https://arxiv.org/pdf/2505.22964", "abs": "https://arxiv.org/abs/2505.22964", "authors": ["Sheng Zhang", "Qin Liu", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon"], "title": "Exploring Scaling Laws for EHR Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u63a2\u7d22\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u53d1\u73b0\u5176\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7c7b\u4f3c\uff0c\u4e3a\u8d44\u6e90\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9884\u6d4b\u6027\u89c1\u89e3\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u662f\u4e00\u79cd\u4e30\u5bcc\u4e14\u7ed3\u6784\u72ec\u7279\u7684\u6570\u636e\u6e90\uff0c\u4f46\u5176\u6269\u5c55\u89c4\u5f8b\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u800c\u8fd9\u5bf9\u5f00\u53d1\u9ad8\u6548\u7684EHR\u57fa\u7840\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9884\u7b97\u4e0b\u8bad\u7ec3\u57fa\u4e8eMIMIC-IV\u6570\u636e\u5e93\u7684Transformer\u67b6\u6784\uff0c\u5206\u6790\u4e86EHR\u6a21\u578b\u7684\u6269\u5c55\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u4e86\u4e0eLLMs\u7c7b\u4f3c\u7684\u6269\u5c55\u89c4\u5f8b\uff0c\u5305\u62ec\u629b\u7269\u7ebfIsoFLOPs\u66f2\u7ebf\u548c\u8ba1\u7b97\u3001\u6a21\u578b\u53c2\u6570\u3001\u6570\u636e\u91cf\u4e0e\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u5e42\u5f8b\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u9ad8\u6548\u7684EHR\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u548c\u4e2a\u6027\u5316\u533b\u7597\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.23054", "pdf": "https://arxiv.org/pdf/2505.23054", "abs": "https://arxiv.org/abs/2505.23054", "authors": ["Yuxuan Lin", "Ruihang Chu", "Zhenyu Chen", "Xiao Tang", "Lei Ke", "Haoling Li", "Yingji Zhong", "Zhihao Li", "Shiyong Liu", "Xiaofei Wu", "Jianzhuang Liu", "Yujiu Yang"], "title": "Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object", "categories": ["cs.CV"], "comment": null, "summary": "Generative 3D reconstruction shows strong potential in incomplete\nobservations. While sparse-view and single-image reconstruction are\nwell-researched, partial observation remains underexplored. In this context,\ndense views are accessible only from a specific angular range, with other\nperspectives remaining inaccessible. This task presents two main challenges:\n(i) limited View Range: observations confined to a narrow angular scope prevent\neffective traditional interpolation techniques that require evenly distributed\nperspectives. (ii) inconsistent Generation: views created for invisible regions\noften lack coherence with both visible regions and each other, compromising\nreconstruction consistency. To address these challenges, we propose \\method, a\nnovel training-free approach that integrates the local dense observations and\nmulti-source priors for reconstruction. Our method introduces a fusion-based\nstrategy to effectively align these priors in DDIM sampling, thereby generating\nmulti-view consistent images to supervise invisible views. We further design an\niterative refinement strategy, which uses the geometric structures of the\nobject to enhance reconstruction quality. Extensive experiments on multiple\ndatasets show the superiority of our method over SOTAs, especially in invisible\nregions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u5c40\u90e8\u5bc6\u96c6\u89c2\u6d4b\u548c\u591a\u6e90\u5148\u9a8c\u6765\u89e3\u51b3\u90e8\u5206\u89c2\u6d4b\u4e0b\u76843D\u91cd\u5efa\u95ee\u9898\uff0c\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "motivation": "\u90e8\u5206\u89c2\u6d4b\u4e0b\u76843D\u91cd\u5efa\u4efb\u52a1\u56e0\u89c6\u89d2\u8303\u56f4\u6709\u9650\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u878d\u5408\u7684\u7b56\u7565\uff0c\u7ed3\u5408DDIM\u91c7\u6837\u548c\u591a\u6e90\u5148\u9a8c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fed\u4ee3\u7ec6\u5316\u7b56\u7565\u4ee5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u53ef\u89c1\u533a\u57df\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u90e8\u5206\u89c2\u6d4b\u4e0b\u76843D\u91cd\u5efa\u95ee\u9898\uff0c\u5c24\u5176\u5728\u4e0d\u53ef\u89c1\u533a\u57df\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.22993", "pdf": "https://arxiv.org/pdf/2505.22993", "abs": "https://arxiv.org/abs/2505.22993", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": "Published at NAACL 2025 Main Conference", "summary": "Claim verification is a long-standing and challenging task that demands not\nonly high accuracy but also explainability of the verification process. This\ntask becomes an emerging research issue in the era of large language models\n(LLMs) since real-world claims are often complex, featuring intricate semantic\nstructures or obfuscated entities. Traditional approaches typically address\nthis by decomposing claims into sub-claims and querying a knowledge base to\nresolve hidden or ambiguous entities. However, the absence of effective\ndisambiguation strategies for these entities can compromise the entire\nverification process. To address these challenges, we propose\nVerify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and\ncomprehension abilities of LLM agents. VeGraph operates in three phases: (1)\nGraph Representation - an input claim is decomposed into structured triplets,\nforming a graph-based representation that integrates both structured and\nunstructured information; (2) Entity Disambiguation -VeGraph iteratively\ninteracts with the knowledge base to resolve ambiguous entities within the\ngraph for deeper sub-claim verification; and (3) Verification - remaining\ntriplets are verified to complete the fact-checking process. Experiments using\nMeta-Llama-3-70B (instruct version) show that VeGraph achieves competitive\nperformance compared to baselines on two benchmarks HoVer and FEVEROUS,\neffectively addressing claim verification challenges. Our source code and data\nare available for further exploitation.", "AI": {"tldr": "VeGraph\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u3001\u5b9e\u4f53\u6d88\u6b67\u548c\u9a8c\u8bc1\u4e09\u9636\u6bb5\u89e3\u51b3\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u5728HoVer\u548cFEVEROUS\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u58f0\u660e\u9a8c\u8bc1\u4e2d\u56e0\u5b9e\u4f53\u6d88\u6b67\u4e0d\u8db3\u800c\u53d7\u9650\uff0c\u9700\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "VeGraph\u5206\u4e09\u9636\u6bb5\uff1a\u56fe\u8868\u793a\uff08\u5c06\u58f0\u660e\u5206\u89e3\u4e3a\u4e09\u5143\u7ec4\uff09\u3001\u5b9e\u4f53\u6d88\u6b67\uff08\u4e0e\u77e5\u8bc6\u5e93\u4ea4\u4e92\u6d88\u6b67\uff09\u548c\u9a8c\u8bc1\uff08\u5b8c\u6210\u4e8b\u5b9e\u6838\u67e5\uff09\u3002", "result": "\u4f7f\u7528Meta-Llama-3-70B\u7684\u5b9e\u9a8c\u663e\u793a\uff0cVeGraph\u5728HoVer\u548cFEVEROUS\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VeGraph\u901a\u8fc7LLM\u548c\u56fe\u7ed3\u6784\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u58f0\u660e\u9a8c\u8bc1\u95ee\u9898\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.23068", "pdf": "https://arxiv.org/pdf/2505.23068", "abs": "https://arxiv.org/abs/2505.23068", "authors": ["Rui Xu", "Yuzhen Niu", "Yuezhou Li", "Huangbiao Xu", "Wenxi Liu", "Yuzhong Chen"], "title": "URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration", "categories": ["cs.CV"], "comment": "This paper has been accepted to CVPR 2025", "summary": "Existing low-light image enhancement (LLIE) and joint LLIE and deblurring\n(LLIE-deblur) models have made strides in addressing predefined degradations,\nyet they are often constrained by dynamically coupled degradations. To address\nthese challenges, we introduce a Unified Receptance Weighted Key Value (URWKV)\nmodel with multi-state perspective, enabling flexible and effective degradation\nrestoration for low-light images. Specifically, we customize the core URWKV\nblock to perceive and analyze complex degradations by leveraging multiple\nintra- and inter-stage states. First, inspired by the pupil mechanism in the\nhuman visual system, we propose Luminance-adaptive Normalization (LAN) that\nadjusts normalization parameters based on rich inter-stage states, allowing for\nadaptive, scene-aware luminance modulation. Second, we aggregate multiple\nintra-stage states through exponential moving average approach, effectively\ncapturing subtle variations while mitigating information loss inherent in the\nsingle-state mechanism. To reduce the degradation effects commonly associated\nwith conventional skip connections, we propose the State-aware Selective Fusion\n(SSF) module, which dynamically aligns and integrates multi-state features\nacross encoder stages, selectively fusing contextual information. In comparison\nto state-of-the-art models, our URWKV model achieves superior performance on\nvarious benchmarks, while requiring significantly fewer parameters and\ncomputational resources.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aURWKV\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u548c\u53bb\u6a21\u7cca\uff0c\u901a\u8fc7\u591a\u72b6\u6001\u89c6\u89d2\u7075\u6d3b\u5904\u7406\u52a8\u6001\u8026\u5408\u7684\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u548c\u8054\u5408\u53bb\u6a21\u7cca\u6a21\u578b\u5728\u5904\u7406\u52a8\u6001\u8026\u5408\u9000\u5316\u65f6\u8868\u73b0\u53d7\u9650\uff0c\u9700\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86URWKV\u6838\u5fc3\u6a21\u5757\uff0c\u7ed3\u5408\u4eae\u5ea6\u81ea\u9002\u5e94\u5f52\u4e00\u5316\uff08LAN\uff09\u548c\u72b6\u6001\u611f\u77e5\u9009\u62e9\u6027\u878d\u5408\uff08SSF\uff09\u6a21\u5757\uff0c\u5229\u7528\u591a\u9636\u6bb5\u72b6\u6001\u5206\u6790\u9000\u5316\u3002", "result": "URWKV\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u53c2\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "URWKV\u6a21\u578b\u901a\u8fc7\u591a\u72b6\u6001\u89c6\u89d2\u548c\u52a8\u6001\u878d\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u548c\u53bb\u6a21\u7cca\u4e2d\u7684\u52a8\u6001\u8026\u5408\u9000\u5316\u95ee\u9898\u3002"}}
{"id": "2505.23001", "pdf": "https://arxiv.org/pdf/2505.23001", "abs": "https://arxiv.org/abs/2505.23001", "authors": ["Yize Cheng", "Wenxiao Wang", "Mazda Moayeri", "Soheil Feizi"], "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors", "categories": ["cs.CL"], "comment": null, "summary": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.", "AI": {"tldr": "DyePack\u662f\u4e00\u4e2a\u901a\u8fc7\u540e\u95e8\u653b\u51fb\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u5728\u8bad\u7ec3\u4e2d\u4f7f\u7528\u4e86\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u6846\u67b6\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u7ec6\u8282\uff0c\u80fd\u6709\u6548\u9632\u6b62\u8bef\u62a5\u3002", "motivation": "\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u5bb9\u6613\u88ab\u6c61\u67d3\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u6c61\u67d3\u3002", "method": "DyePack\u901a\u8fc7\u6df7\u5408\u540e\u95e8\u6837\u672c\u5230\u6d4b\u8bd5\u6570\u636e\u4e2d\uff0c\u5229\u7528\u968f\u673a\u76ee\u6807\u7684\u591a\u540e\u95e8\u8bbe\u8ba1\uff0c\u8ba1\u7b97\u7cbe\u786e\u7684\u8bef\u62a5\u7387\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\uff0cDyePack\u6210\u529f\u68c0\u6d4b\u6240\u6709\u6c61\u67d3\u6a21\u578b\uff0c\u8bef\u62a5\u7387\u6781\u4f4e\u3002", "conclusion": "DyePack\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u6c61\u67d3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2505.23085", "pdf": "https://arxiv.org/pdf/2505.23085", "abs": "https://arxiv.org/abs/2505.23085", "authors": ["Gwanghyun Kim", "Xueting Li", "Ye Yuan", "Koki Nagano", "Tianye Li", "Jan Kautz", "Se Young Chun", "Umar Iqbal"], "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Project page: https://research.nvidia.com/labs/dair/geoman", "summary": "Estimating accurate and temporally consistent 3D human geometry from videos\nis a challenging problem in computer vision. Existing methods, primarily\noptimized for single images, often suffer from temporal inconsistencies and\nfail to capture fine-grained dynamic details. To address these limitations, we\npresent GeoMan, a novel architecture designed to produce accurate and\ntemporally consistent depth and normal estimations from monocular human videos.\nGeoMan addresses two key challenges: the scarcity of high-quality 4D training\ndata and the need for metric depth estimation to accurately model human size.\nTo overcome the first challenge, GeoMan employs an image-based model to\nestimate depth and normals for the first frame of a video, which then\nconditions a video diffusion model, reframing video geometry estimation task as\nan image-to-video generation problem. This design offloads the heavy lifting of\ngeometric estimation to the image model and simplifies the video model's role\nto focus on intricate details while using priors learned from large-scale video\ndatasets. Consequently, GeoMan improves temporal consistency and\ngeneralizability while requiring minimal 4D training data. To address the\nchallenge of accurate human size estimation, we introduce a root-relative depth\nrepresentation that retains critical human-scale details and is easier to be\nestimated from monocular inputs, overcoming the limitations of traditional\naffine-invariant and metric depth representations. GeoMan achieves\nstate-of-the-art performance in both qualitative and quantitative evaluations,\ndemonstrating its effectiveness in overcoming longstanding challenges in 3D\nhuman geometry estimation from videos.", "AI": {"tldr": "GeoMan\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u4f30\u8ba1\u51c6\u786e\u4e14\u65f6\u95f4\u4e00\u81f4\u76843D\u4eba\u4f53\u51e0\u4f55\u5f62\u72b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u56fe\u50cf\u4f18\u5316\uff0c\u5b58\u5728\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u548c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u7ec6\u8282\u7684\u95ee\u9898\u3002GeoMan\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u9ad8\u8d28\u91cf4D\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\u3002", "method": "GeoMan\u7ed3\u5408\u56fe\u50cf\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u9996\u5e27\u901a\u8fc7\u56fe\u50cf\u6a21\u578b\u4f30\u8ba1\u6df1\u5ea6\u548c\u6cd5\u7ebf\uff0c\u89c6\u9891\u6a21\u578b\u5219\u4e13\u6ce8\u4e8e\u7ec6\u8282\u751f\u6210\u3002\u91c7\u7528\u6839\u76f8\u5bf9\u6df1\u5ea6\u8868\u793a\u4ee5\u4fdd\u7559\u4eba\u4f53\u5c3a\u5ea6\u7ec6\u8282\u3002", "result": "GeoMan\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GeoMan\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u8868\u793a\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u4eba\u4f53\u51e0\u4f55\u4f30\u8ba1\u4e2d\u7684\u957f\u671f\u6311\u6218\u3002"}}
{"id": "2505.23006", "pdf": "https://arxiv.org/pdf/2505.23006", "abs": "https://arxiv.org/abs/2505.23006", "authors": ["Chiwan Park", "Wonjun Jang", "Daeryong Kim", "Aelim Ahn", "Kichang Yang", "Woosung Hwang", "Jihyeon Roh", "Hyerin Park", "Hyosun Wang", "Min Seok Kim", "Jihoon Kang"], "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted to ACL 2025 Industry Track. 12 pages, 5 figures", "summary": "The advancement of Large Language Models (LLMs) has led to significant\nimprovements in various service domains, including search, recommendation, and\nchatbot applications. However, applying state-of-the-art (SOTA) research to\nindustrial settings presents challenges, as it requires maintaining flexible\nconversational abilities while also strictly complying with service-specific\nconstraints. This can be seen as two conflicting requirements due to the\nprobabilistic nature of LLMs. In this paper, we propose our approach to\naddressing this challenge and detail the strategies we employed to overcome\ntheir inherent limitations in real-world applications. We conduct a practical\ncase study of a conversational agent designed for the e-commerce domain,\ndetailing our implementation workflow and optimizations. Our findings provide\ninsights into bridging the gap between academic research and real-world\napplication, introducing a framework for developing scalable, controllable, and\nreliable AI-driven agents.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\uff0c\u89e3\u51b3\u7075\u6d3b\u5bf9\u8bdd\u80fd\u529b\u4e0e\u670d\u52a1\u7ea6\u675f\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u7535\u5546\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u6848\u4f8b\u7814\u7a76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5de5\u4e1a\u5e94\u7528\u4e2d\uff0cLLMs\u9700\u8981\u5728\u4fdd\u6301\u7075\u6d3b\u5bf9\u8bdd\u80fd\u529b\u7684\u540c\u65f6\u4e25\u683c\u9075\u5b88\u670d\u52a1\u7ea6\u675f\uff0c\u8fd9\u4e24\u8005\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u7ed3\u5408\u7b56\u7565\u548c\u4f18\u5316\u6280\u672f\uff0c\u89e3\u51b3LLMs\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u7535\u5546\u5bf9\u8bdd\u673a\u5668\u4eba\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u4e14\u53ef\u9760\u7684AI\u9a71\u52a8\u4ee3\u7406\uff0c\u5f25\u5408\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u4e2d\u7684AI\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2505.23093", "pdf": "https://arxiv.org/pdf/2505.23093", "abs": "https://arxiv.org/abs/2505.23093", "authors": ["Mian Muhammad Naeem Abid", "Nancy Mehta", "Zongwei Wu", "Radu Timofte"], "title": "LeMoRe: Learn More Details for Lightweight Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at IEEE ICIP 2025", "summary": "Lightweight semantic segmentation is essential for many downstream vision\ntasks. Unfortunately, existing methods often struggle to balance efficiency and\nperformance due to the complexity of feature modeling. Many of these existing\napproaches are constrained by rigid architectures and implicit representation\nlearning, often characterized by parameter-heavy designs and a reliance on\ncomputationally intensive Vision Transformer-based frameworks. In this work, we\nintroduce an efficient paradigm by synergizing explicit and implicit modeling\nto balance computational efficiency with representational fidelity. Our method\ncombines well-defined Cartesian directions with explicitly modeled views and\nimplicitly inferred intermediate representations, efficiently capturing global\ndependencies through a nested attention mechanism. Extensive experiments on\nchallenging datasets, including ADE20K, CityScapes, Pascal Context, and\nCOCO-Stuff, demonstrate that LeMoRe strikes an effective balance between\nperformance and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u5efa\u6a21\u7684\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u8868\u5f81\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u5efa\u6a21\u590d\u6742\u6027\u4e0a\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e14\u4f9d\u8d56\u53c2\u6570\u7e41\u91cd\u7684\u8bbe\u8ba1\u6216\u8ba1\u7b97\u5bc6\u96c6\u7684Vision Transformer\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u660e\u786e\u7684\u7b1b\u5361\u5c14\u65b9\u5411\u4e0e\u663e\u5f0f\u5efa\u6a21\u89c6\u56fe\u548c\u9690\u5f0f\u63a8\u65ad\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u5d4c\u5957\u6ce8\u610f\u529b\u673a\u5236\u9ad8\u6548\u6355\u83b7\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5728ADE20K\u3001CityScapes\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6709\u6548\u5e73\u8861\u3002", "conclusion": "LeMoRe\u65b9\u6cd5\u5728\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5206\u5272\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u9ad8\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2505.23015", "pdf": "https://arxiv.org/pdf/2505.23015", "abs": "https://arxiv.org/abs/2505.23015", "authors": ["Jinwen Chen", "Hainan Zhang", "Fei Sun", "Qinnan Zhang", "Sijia Wen", "Ziwei Wang", "Zhiming Zheng"], "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning LLMs with datasets containing stealthy backdoors from publishers\nposes security risks to downstream applications. Mainstream detection methods\neither identify poisoned samples by analyzing the prediction probability of\npoisoned classification models or rely on the rewriting model to eliminate the\nstealthy triggers. However, the former cannot be applied to generation tasks,\nwhile the latter may degrade generation performance and introduce new triggers.\nTherefore, efficiently eliminating stealthy poisoned samples for LLMs remains\nan urgent problem. We observe that after applying TF-IDF clustering to the\nsample response, there are notable differences in the intra-class distances\nbetween clean and poisoned samples. Poisoned samples tend to cluster closely\nbecause of their specific malicious outputs, whereas clean samples are more\nscattered due to their more varied responses. Thus, in this paper, we propose a\nstealthy backdoor sample detection method based on Reference-Filtration and\nTfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample\nresponse with the reference model's outputs and consider the sample suspicious\nif there's a significant discrepancy. And then we perform TF-IDF clustering on\nthese suspicious samples to identify the true poisoned samples based on the\nintra-class distance. Experiments on two machine translation datasets and one\nQA dataset demonstrate that RFTC outperforms baselines in backdoor detection\nand model performance. Further analysis of different reference models also\nconfirms the effectiveness of our Reference-Filtration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u8fc7\u6ee4\u548cTF-IDF\u805a\u7c7b\u7684\u9690\u853d\u540e\u95e8\u6837\u672c\u68c0\u6d4b\u65b9\u6cd5\uff08RFTC\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u8bc6\u522bLLM\u4e2d\u7684\u4e2d\u6bd2\u6837\u672c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u9002\u7528\u4e8e\u751f\u6210\u4efb\u52a1\u6216\u53ef\u80fd\u964d\u4f4e\u751f\u6210\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u6d88\u9664\u9690\u853d\u4e2d\u6bd2\u6837\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u8f93\u51fa\u6bd4\u8f83\u548cTF-IDF\u805a\u7c7b\uff0c\u8bc6\u522b\u4e2d\u6bd2\u6837\u672c\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u548cQA\u6570\u636e\u96c6\u4e0a\uff0cRFTC\u5728\u68c0\u6d4b\u540e\u95e8\u548c\u6a21\u578b\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RFTC\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9690\u853d\u540e\u95e8\u6837\u672c\u68c0\u6d4b\u95ee\u9898\uff0c\u4e14\u53c2\u8003\u8fc7\u6ee4\u673a\u5236\u88ab\u8bc1\u5b9e\u6709\u6548\u3002"}}
{"id": "2505.23102", "pdf": "https://arxiv.org/pdf/2505.23102", "abs": "https://arxiv.org/abs/2505.23102", "authors": ["Yuka Ogino", "Takahiro Toizumi", "Atsushi Ito"], "title": "CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing", "categories": ["cs.CV"], "comment": "Accepted to ICIP2025", "summary": "Low-Light Image Enhancement (LLIE) is crucial for improving both human\nperception and computer vision tasks. This paper addresses two challenges in\nzero-reference LLIE: obtaining perceptually 'good' images using the Contrastive\nLanguage-Image Pre-Training (CLIP) model and maintaining computational\nefficiency for high-resolution images. We propose CLIP-Utilized Reinforcement\nlearning-based Visual image Enhancement (CURVE). CURVE employs a simple image\nprocessing module which adjusts global image tone based on B\\'ezier curve and\nestimates its processing parameters iteratively. The estimator is trained by\nreinforcement learning with rewards designed using CLIP text embeddings.\nExperiments on low-light and multi-exposure datasets demonstrate the\nperformance of CURVE in terms of enhancement quality and processing speed\ncompared to conventional methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5CURVE\uff0c\u901a\u8fc7B\u00e9zier\u66f2\u7ebf\u8c03\u6574\u5168\u5c40\u8272\u8c03\u5e76\u8fed\u4ee3\u4f18\u5316\u53c2\u6570\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u96f6\u53c2\u8003\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u4e2d\u5982\u4f55\u5229\u7528CLIP\u6a21\u578b\u83b7\u5f97\u611f\u77e5\u826f\u597d\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faCURVE\u65b9\u6cd5\uff0c\u4f7f\u7528B\u00e9zier\u66f2\u7ebf\u8c03\u6574\u5168\u5c40\u8272\u8c03\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fed\u4ee3\u4f18\u5316\u53c2\u6570\uff0c\u5956\u52b1\u51fd\u6570\u57fa\u4e8eCLIP\u6587\u672c\u5d4c\u5165\u8bbe\u8ba1\u3002", "result": "\u5728\u4f4e\u5149\u7167\u548c\u591a\u66dd\u5149\u6570\u636e\u96c6\u4e0a\uff0cCURVE\u5728\u589e\u5f3a\u8d28\u91cf\u548c\u5904\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "CURVE\u901a\u8fc7\u7ed3\u5408CLIP\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23026", "pdf": "https://arxiv.org/pdf/2505.23026", "abs": "https://arxiv.org/abs/2505.23026", "authors": ["Haewon Park", "Gyubin Choi", "Minjun Kim", "Yohan Jo"], "title": "Context Robust Knowledge Editing for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings. Our code and datasets are available at\n  (https://github.com/holi-lab/CoRE)", "summary": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success.", "AI": {"tldr": "CHED\u662f\u4e00\u4e2a\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u65b9\u6cd5\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709KE\u65b9\u6cd5\u5728\u524d\u7f6e\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u8868\u73b0\u4e0d\u4f73\u3002CoRE\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u9690\u85cf\u72b6\u6001\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u65b9\u5dee\uff0c\u63d0\u5347\u4e86\u7f16\u8f91\u6210\u529f\u7387\u5e76\u4fdd\u6301\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709KE\u8bc4\u4f30\u901a\u5e38\u5ffd\u7565\u524d\u7f6e\u4e0a\u4e0b\u6587\u5bf9\u77e5\u8bc6\u68c0\u7d22\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7f16\u8f91\u6548\u679c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u3002", "method": "\u5f00\u53d1CHED\u57fa\u51c6\u8bc4\u4f30KE\u65b9\u6cd5\u7684\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51faCoRE\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u9690\u85cf\u72b6\u6001\u51cf\u5c11\u4e0a\u4e0b\u6587\u654f\u611f\u65b9\u5dee\u3002", "result": "CHED\u663e\u793a\u73b0\u6709KE\u65b9\u6cd5\u5728\u524d\u7f6e\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u5931\u8d25\u7387\u9ad8\uff0cCoRE\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6210\u529f\u7387\u3002", "conclusion": "CoRE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86KE\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u5b58\u5728\u65f6\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.23107", "pdf": "https://arxiv.org/pdf/2505.23107", "abs": "https://arxiv.org/abs/2505.23107", "authors": ["Pushapdeep Singh", "Jyoti Nigam", "Medicherla Vamsi Krishna", "Arnav Bhavsar", "Aditya Nigam"], "title": "EAD: An EEG Adapter for Automated Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While electroencephalography (EEG) has been a popular modality for neural\ndecoding, it often involves task specific acquisition of the EEG data. This\nposes challenges for the development of a unified pipeline to learn embeddings\nfor various EEG signal classification, which is often involved in various\ndecoding tasks. Traditionally, EEG classification involves the step of signal\npreprocessing and the use of deep learning techniques, which are highly\ndependent on the number of EEG channels in each sample. However, the same\npipeline cannot be applied even if the EEG data is collected for the same\nexperiment but with different acquisition devices. This necessitates the\ndevelopment of a framework for learning EEG embeddings, which could be highly\nbeneficial for tasks involving multiple EEG samples for the same task but with\nvarying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a\nflexible framework compatible with any signal acquisition device. More\nspecifically, we leverage a recent EEG foundational model with significant\nadaptations to learn robust representations from the EEG data for the\nclassification task. We evaluate EAD on two publicly available datasets\nachieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and\nBrainLat respectively. This illustrates the effectiveness of the proposed\nframework across diverse EEG datasets containing two different perception\ntasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG\nclassification on EEG-ImageNet task to demonstrate the generalization\ncapability of the proposed approach.", "AI": {"tldr": "EEG Adapter (EAD) \u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u9002\u7528\u4e8e\u4e0d\u540c EEG \u8bbe\u5907\u7684\u7edf\u4e00\u5d4c\u5165\u8868\u793a\uff0c\u5728 EEG-ImageNet \u548c BrainLat \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf EEG \u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u548c\u8bbe\u5907\uff0c\u96be\u4ee5\u7edf\u4e00\u5904\u7406\u4e0d\u540c\u901a\u9053\u6570\u7684 EEG \u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u901a\u7528\u6846\u67b6\u3002", "method": "\u63d0\u51fa EEG Adapter (EAD)\uff0c\u57fa\u4e8e EEG \u57fa\u7840\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff0c\u5b66\u4e60\u9c81\u68d2\u7684 EEG \u8868\u793a\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bbe\u5907\u3002", "result": "\u5728 EEG-ImageNet \u548c BrainLat \u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230 99.33% \u548c 92.31% \u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\u3002", "conclusion": "EAD \u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684 EEG \u5d4c\u5165\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u8bbe\u5907\u3002"}}
{"id": "2505.23029", "pdf": "https://arxiv.org/pdf/2505.23029", "abs": "https://arxiv.org/abs/2505.23029", "authors": ["Si Wu", "Sebastian Bruch"], "title": "Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025. This is the camera-ready version. Will be\n  presenting in July 2025 in Vienna", "summary": "Imageability (potential of text to evoke a mental image) and concreteness\n(perceptibility of text) are two psycholinguistic properties that link visual\nand semantic spaces. It is little surprise that computational methods that\nestimate them do so using parallel visual and semantic spaces, such as\ncollections of image-caption pairs or multi-modal models. In this paper, we\nwork on the supposition that text itself in an image-caption dataset offers\nsufficient signals to accurately estimate these properties. We hypothesize, in\nparticular, that the peakedness of the neighborhood of a word in the semantic\nembedding space reflects its degree of imageability and concreteness. We then\npropose an unsupervised, distribution-free measure, which we call Neighborhood\nStability Measure (NSM), that quantifies the sharpness of peaks. Extensive\nexperiments show that NSM correlates more strongly with ground-truth ratings\nthan existing unsupervised methods, and is a strong predictor of these\nproperties for classification. Our code and data are available on GitHub\n(https://github.com/Artificial-Memory-Lab/imageability).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5NSM\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5355\u8bcd\u90bb\u57df\u7684\u5cf0\u503c\u5ea6\u6765\u4f30\u8ba1\u6587\u672c\u7684\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u672c\u8eab\u5728\u56fe\u50cf-\u6807\u9898\u6570\u636e\u96c6\u4e2d\u662f\u5426\u8db3\u4ee5\u51c6\u786e\u4f30\u8ba1\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\uff0c\u907f\u514d\u4f9d\u8d56\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u63d0\u51faNSM\uff08\u90bb\u57df\u7a33\u5b9a\u6027\u5ea6\u91cf\uff09\uff0c\u91cf\u5316\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5355\u8bcd\u90bb\u57df\u7684\u5cf0\u503c\u5ea6\uff0c\u4f5c\u4e3a\u65e0\u76d1\u7763\u3001\u5206\u5e03\u65e0\u5173\u7684\u5ea6\u91cf\u3002", "result": "NSM\u4e0e\u771f\u5b9e\u8bc4\u5206\u7684\u76f8\u5173\u6027\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u4e14\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "NSM\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4f30\u8ba1\u6587\u672c\u7684\u53ef\u60f3\u8c61\u6027\u548c\u5177\u4f53\u6027\u3002"}}
{"id": "2505.23109", "pdf": "https://arxiv.org/pdf/2505.23109", "abs": "https://arxiv.org/abs/2505.23109", "authors": ["Anusha A. S.", "Uma Ranjan", "Medha Sharma", "Siddharth Dutt"], "title": "Identification of Patterns of Cognitive Impairment for Early Detection of Dementia", "categories": ["cs.CV"], "comment": "4 pages, 2 figures, to be published in IEEE EMBC 2020", "summary": "Early detection of dementia is crucial to devise effective interventions.\nComprehensive cognitive tests, while being the most accurate means of\ndiagnosis, are long and tedious, thus limiting their applicability to a large\npopulation, especially when periodic assessments are needed. The problem is\ncompounded by the fact that people have differing patterns of cognitive\nimpairment as they progress to different forms of dementia. This paper presents\na novel scheme by which individual-specific patterns of impairment can be\nidentified and used to devise personalized tests for periodic follow-up.\nPatterns of cognitive impairment are initially learned from a population\ncluster of combined normals and MCIs, using a set of standardized cognitive\ntests. Impairment patterns in the population are identified using a 2-step\nprocedure involving an ensemble wrapper feature selection followed by cluster\nidentification and analysis. These patterns have been shown to correspond to\nclinically accepted variants of MCI, a prodrome of dementia. The learned\nclusters of patterns can subsequently be used to identify the most likely route\nof cognitive impairment, even for pre-symptomatic and apparently normal people.\nBaseline data of 24,000 subjects from the NACC database was used for the study.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u8ba4\u77e5\u6d4b\u8bd5\u65b9\u6848\uff0c\u901a\u8fc7\u8bc6\u522b\u4e2a\u4f53\u7279\u5b9a\u7684\u8ba4\u77e5\u969c\u788d\u6a21\u5f0f\uff0c\u4e3a\u65e9\u671f\u75f4\u5446\u68c0\u6d4b\u63d0\u4f9b\u9ad8\u6548\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u6d4b\u8bd5\u8017\u65f6\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u4e14\u4e0d\u540c\u75f4\u5446\u7c7b\u578b\u7684\u8ba4\u77e5\u969c\u788d\u6a21\u5f0f\u5404\u5f02\uff0c\u9700\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u5148\u901a\u8fc7\u96c6\u6210\u5305\u88c5\u7279\u5f81\u9009\u62e9\u8bc6\u522b\u7fa4\u4f53\u4e2d\u7684\u8ba4\u77e5\u969c\u788d\u6a21\u5f0f\uff0c\u518d\u805a\u7c7b\u5206\u6790\uff1b\u57fa\u4e8eNACC\u6570\u636e\u5e93\u4e2d24,000\u540d\u53d7\u8bd5\u8005\u7684\u57fa\u7ebf\u6570\u636e\u3002", "result": "\u8bc6\u522b\u51fa\u7684\u6a21\u5f0f\u4e0e\u4e34\u5e8a\u8ba4\u53ef\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u4e9a\u578b\u4e00\u81f4\uff0c\u53ef\u7528\u4e8e\u9884\u6d4b\u65e0\u75c7\u72b6\u6216\u6b63\u5e38\u4eba\u7fa4\u7684\u8ba4\u77e5\u969c\u788d\u8def\u5f84\u3002", "conclusion": "\u4e2a\u6027\u5316\u6d4b\u8bd5\u65b9\u6848\u6709\u671b\u63d0\u9ad8\u75f4\u5446\u65e9\u671f\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4eba\u7fa4\u7684\u5b9a\u671f\u8bc4\u4f30\u3002"}}
{"id": "2505.23030", "pdf": "https://arxiv.org/pdf/2505.23030", "abs": "https://arxiv.org/abs/2505.23030", "authors": ["Shruti Hegde", "Mabon Manoj Ninan", "Jonathan R. Dillman", "Shireen Hayatghaibi", "Lynn Babcock", "Elanchezhian Somasundaram"], "title": "Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose clinical natural language processing (NLP) tools are\nincreasingly used for the automatic labeling of clinical reports. However,\nindependent evaluations for specific tasks, such as pediatric chest radiograph\n(CXR) report labeling, are limited. This study compares four commercial\nclinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP\n(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and\nassertion detection in pediatric CXR reports. Additionally, CheXpert and\nCheXbert, two dedicated chest radiograph report labelers, were evaluated on the\nsame task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR\nreports from a large academic pediatric hospital. Entities and assertion\nstatuses (positive, negative, uncertain) from the findings and impression\nsections were extracted by the NLP systems, with impression section entities\nmapped to 12 disease categories and a No Findings category. CheXpert and\nCheXbert extracted the same 13 categories. Outputs were compared using Fleiss\nKappa and accuracy against a consensus pseudo-ground truth. Significant\ndifferences were found in the number of extracted entities and assertion\ndistributions across NLP systems. SP extracted 49,688 unique entities, GC\n16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged\naround 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert\nachieved 56% accuracy. Considerable variability in performance highlights the\nneed for careful validation and review before deploying NLP tools for clinical\nreport labeling.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u5546\u4e1a\u4e34\u5e8aNLP\u7cfb\u7edf\uff08AWS\u3001GC\u3001AZ\u3001SP\uff09\u548c\u4e24\u79cd\u4e13\u7528\u80f8\u90e8X\u5149\u62a5\u544a\u6807\u6ce8\u5de5\u5177\uff08CheXpert\u3001CheXbert\uff09\u5728\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\u4e2d\u7684\u5b9e\u4f53\u63d0\u53d6\u548c\u65ad\u8a00\u68c0\u6d4b\u6027\u80fd\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u5c3d\u7ba1\u901a\u7528\u4e34\u5e8aNLP\u5de5\u5177\u5e7f\u6cdb\u7528\u4e8e\u4e34\u5e8a\u62a5\u544a\u81ea\u52a8\u6807\u6ce8\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\u6807\u6ce8\uff09\u7684\u72ec\u7acb\u8bc4\u4f30\u6709\u9650\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e8695,008\u4efd\u513f\u79d1\u80f8\u90e8X\u5149\u62a5\u544a\uff0c\u6bd4\u8f83\u4e86\u56db\u79cdNLP\u7cfb\u7edf\u548c\u4e24\u79cd\u4e13\u7528\u5de5\u5177\u5728\u5b9e\u4f53\u63d0\u53d6\u548c\u65ad\u8a00\u68c0\u6d4b\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528Fleiss Kappa\u548c\u51c6\u786e\u6027\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0d\u540cNLP\u7cfb\u7edf\u5728\u5b9e\u4f53\u63d0\u53d6\u6570\u91cf\u548c\u65ad\u8a00\u51c6\u786e\u6027\u4e0a\u5dee\u5f02\u663e\u8457\uff0cSP\u8868\u73b0\u6700\u4f73\uff0876%\uff09\uff0cAWS\u6700\u4f4e\uff0850%\uff09\u3002CheXpert\u548cCheXbert\u7684\u51c6\u786e\u6027\u4e3a56%\u3002", "conclusion": "\u4e34\u5e8aNLP\u5de5\u5177\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u90e8\u7f72\u524d\u9700\u4ed4\u7ec6\u9a8c\u8bc1\u548c\u5ba1\u67e5\u3002"}}
{"id": "2505.23115", "pdf": "https://arxiv.org/pdf/2505.23115", "abs": "https://arxiv.org/abs/2505.23115", "authors": ["Yunshen Wang", "Yicheng Liu", "Tianyuan Yuan", "Yucheng Mao", "Yingshi Liang", "Xiuyu Yang", "Honggang Zhang", "Hang Zhao"], "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving", "categories": ["cs.CV"], "comment": "ICRA 2025", "summary": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.", "AI": {"tldr": "\u5c063D\u5360\u7528\u9884\u6d4b\u91cd\u6784\u4e3a\u751f\u6210\u5efa\u6a21\u4efb\u52a1\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u63d0\u5347\u9884\u6d4b\u4e00\u81f4\u6027\u3001\u566a\u58f0\u9c81\u68d2\u6027\u53ca\u5bf9\u590d\u67423D\u7ed3\u6784\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5224\u522b\u65b9\u6cd5\u5728\u566a\u58f0\u6570\u636e\u3001\u4e0d\u5b8c\u6574\u89c2\u6d4b\u548c\u590d\u67423D\u573a\u666f\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5efa\u6a21\u5de5\u5177\uff0c\u5b66\u4e60\u5e95\u5c42\u6570\u636e\u5206\u5e03\u5e76\u878d\u51653D\u573a\u666f\u5148\u9a8c\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u9884\u6d4b\u771f\u5b9e\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u5224\u522b\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u906e\u6321\u6216\u4f4e\u53ef\u89c1\u5ea6\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e0b\u6e38\u89c4\u5212\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.23035", "pdf": "https://arxiv.org/pdf/2505.23035", "abs": "https://arxiv.org/abs/2505.23035", "authors": ["Hyunwoo Kim", "Hanau Yi"], "title": "Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Machine-Facing English (MFE) is an emergent register shaped by the adaptation\nof everyday language to the expanding presence of AI interlocutors. Drawing on\nregister theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience\ndesign (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this\nstudy traces how sustained human-AI interaction normalizes syntactic rigidity,\npragmatic simplification, and hyper-explicit phrasing - features that enhance\nmachine parseability at the expense of natural fluency. Our analysis is\ngrounded in qualitative observations from bilingual (Korean/English) voice- and\ntext-based product testing sessions, with reflexive drafting conducted using\nNatural Language Declarative Prompting (NLD-P) under human curation. Thematic\nanalysis identifies five recurrent traits - redundant clarity, directive\nsyntax, controlled vocabulary, flattened prosody, and single-intent structuring\n- that improve execution accuracy but compress expressive range. MFE's\nevolution highlights a persistent tension between communicative efficiency and\nlinguistic richness, raising design challenges for conversational interfaces\nand pedagogical considerations for multilingual users. We conclude by\nunderscoring the need for comprehensive methodological exposition and future\nempirical validation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u673a\u5668\u5bfc\u5411\u82f1\u8bed\uff08MFE\uff09\u8fd9\u4e00\u65b0\u5174\u8bed\u8a00\u73b0\u8c61\uff0c\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0eAI\u4e92\u52a8\u4e2d\u8bed\u8a00\u7b80\u5316\u548c\u660e\u786e\u5316\u7684\u7279\u5f81\u53ca\u5176\u5bf9\u4ea4\u6d41\u6548\u7387\u4e0e\u8bed\u8a00\u4e30\u5bcc\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u4eba\u7c7b\u5982\u4f55\u5728\u4e0eAI\u7684\u6301\u7eed\u4e92\u52a8\u4e2d\u8c03\u6574\u8bed\u8a00\uff0c\u4ee5\u9002\u5e94\u673a\u5668\u7684\u89e3\u6790\u9700\u6c42\uff0c\u540c\u65f6\u727a\u7272\u4e86\u81ea\u7136\u8bed\u8a00\u7684\u6d41\u7545\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u53cc\u8bed\uff08\u97e9\u8bed/\u82f1\u8bed\uff09\u8bed\u97f3\u548c\u6587\u672c\u4ea7\u54c1\u6d4b\u8bd5\u7684\u5b9a\u6027\u89c2\u5bdf\uff0c\u4ee5\u53ca\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u58f0\u660e\u63d0\u793a\uff08NLD-P\uff09\u8fdb\u884c\u53cd\u601d\u6027\u8d77\u8349\u3002", "result": "\u7814\u7a76\u53d1\u73b0MFE\u5177\u6709\u4e94\u79cd\u7279\u5f81\uff1a\u5197\u4f59\u6e05\u6670\u6027\u3001\u6307\u4ee4\u6027\u8bed\u6cd5\u3001\u53d7\u63a7\u8bcd\u6c47\u3001\u5e73\u5766\u97f5\u5f8b\u548c\u5355\u4e00\u610f\u56fe\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7279\u5f81\u63d0\u9ad8\u4e86\u6267\u884c\u51c6\u786e\u6027\u4f46\u538b\u7f29\u4e86\u8868\u8fbe\u8303\u56f4\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faMFE\u7684\u53d1\u5c55\u51f8\u663e\u4e86\u4ea4\u6d41\u6548\u7387\u4e0e\u8bed\u8a00\u4e30\u5bcc\u6027\u4e4b\u95f4\u7684\u5f20\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u8bdd\u754c\u9762\u8bbe\u8ba1\u548c\u591a\u8bed\u8a00\u7528\u6237\u6559\u5b66\u65b9\u9762\u7684\u6311\u6218\uff0c\u540c\u65f6\u547c\u5401\u672a\u6765\u8fdb\u884c\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u8bba\u9610\u8ff0\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2505.23119", "pdf": "https://arxiv.org/pdf/2505.23119", "abs": "https://arxiv.org/abs/2505.23119", "authors": ["Keren Ye", "Ignacio Garcia Dorado", "Michalis Raptis", "Mauricio Delbracio", "Irene Zhu", "Peyman Milanfar", "Hossein Talebi"], "title": "TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance", "categories": ["cs.CV"], "comment": null, "summary": "While recent advancements in Image Super-Resolution (SR) using diffusion\nmodels have shown promise in improving overall image quality, their application\nto scene text images has revealed limitations. These models often struggle with\naccurate text region localization and fail to effectively model image and\nmultilingual character-to-shape priors. This leads to inconsistencies, the\ngeneration of hallucinated textures, and a decrease in the perceived quality of\nthe super-resolved text.\n  To address these issues, we introduce TextSR, a multimodal diffusion model\nspecifically designed for Multilingual Scene Text Image Super-Resolution.\nTextSR leverages a text detector to pinpoint text regions within an image and\nthen employs Optical Character Recognition (OCR) to extract multilingual text\nfrom these areas. The extracted text characters are then transformed into\nvisual shapes using a UTF-8 based text encoder and cross-attention. Recognizing\nthat OCR may sometimes produce inaccurate results in real-world scenarios, we\nhave developed two innovative methods to enhance the robustness of our model.\nBy integrating text character priors with the low-resolution text images, our\nmodel effectively guides the super-resolution process, enhancing fine details\nwithin the text and improving overall legibility. The superior performance of\nour model on both the TextZoom and TextVQA datasets sets a new benchmark for\nSTISR, underscoring the efficacy of our approach.", "AI": {"tldr": "TextSR\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u8bed\u8a00\u573a\u666f\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u591a\u6a21\u6001\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u68c0\u6d4b\u548cOCR\u6280\u672f\uff0c\u5229\u7528\u5b57\u7b26\u5f62\u72b6\u5148\u9a8c\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u573a\u666f\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e2d\u5b58\u5728\u6587\u672c\u533a\u57df\u5b9a\u4f4d\u4e0d\u51c6\u786e\u548c\u5b57\u7b26\u5f62\u72b6\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "method": "TextSR\u7ed3\u5408\u6587\u672c\u68c0\u6d4b\u5668\u548cOCR\u63d0\u53d6\u591a\u8bed\u8a00\u6587\u672c\uff0c\u901a\u8fc7UTF-8\u7f16\u7801\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u5b57\u7b26\u8f6c\u6362\u4e3a\u89c6\u89c9\u5f62\u72b6\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728TextZoom\u548cTextVQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aSTISR\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "TextSR\u901a\u8fc7\u6574\u5408\u5b57\u7b26\u5148\u9a8c\u548c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u8d85\u5206\u8fa8\u7387\u7684\u7ec6\u8282\u548c\u53ef\u8bfb\u6027\u3002"}}
{"id": "2505.23037", "pdf": "https://arxiv.org/pdf/2505.23037", "abs": "https://arxiv.org/abs/2505.23037", "authors": ["Longyin Zhang", "Bowei Zou", "Ai Ti Aw"], "title": "Improving Multilingual Social Media Insights: Aspect-based Comment Analysis", "categories": ["cs.CL"], "comment": "The paper was peer-reviewed", "summary": "The inherent nature of social media posts, characterized by the freedom of\nlanguage use with a disjointed array of diverse opinions and topics, poses\nsignificant challenges to downstream NLP tasks such as comment clustering,\ncomment summarization, and social media opinion analysis. To address this, we\npropose a granular level of identifying and generating aspect terms from\nindividual comments to guide model attention. Specifically, we leverage\nmultilingual large language models with supervised fine-tuning for comment\naspect term generation (CAT-G), further aligning the model's predictions with\nhuman expectations through DPO. We demonstrate the effectiveness of our method\nin enhancing the comprehension of social media discourse on two NLP tasks.\nMoreover, this paper contributes the first multilingual CAT-G test set on\nEnglish, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among\nlanguages, this test set allows for a comparative analysis of performance\nacross languages with varying levels of LLM proficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u65b9\u6cd5\uff08CAT-G\uff09\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u4e2d\u751f\u6210\u65b9\u9762\u672f\u8bed\uff0c\u5e76\u901a\u8fc7DPO\u5bf9\u9f50\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u671f\u671b\uff0c\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u8bed\u8a00\u81ea\u7531\u4e14\u4e3b\u9898\u5206\u6563\uff0c\u7ed9NLP\u4efb\u52a1\uff08\u5982\u8bc4\u8bba\u805a\u7c7b\u3001\u603b\u7ed3\u548c\u610f\u89c1\u5206\u6790\uff09\u5e26\u6765\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u751f\u6210\u8bc4\u8bba\u65b9\u9762\u672f\u8bed\uff08CAT-G\uff09\uff0c\u5e76\u901a\u8fc7DPO\u5bf9\u9f50\u6a21\u578b\u9884\u6d4b\u3002", "result": "\u65b9\u6cd5\u5728\u4e24\u9879NLP\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u8d21\u732e\u4e86\u9996\u4e2a\u591a\u8bed\u8a00CAT-G\u6d4b\u8bd5\u96c6\uff08\u82f1\u8bed\u3001\u4e2d\u6587\u3001\u9a6c\u6765\u8bed\u3001\u5370\u5c3c\u8bed\uff09\u3002", "conclusion": "CAT-G\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u7684\u591a\u6837\u6027\u95ee\u9898\uff0c\u6d4b\u8bd5\u96c6\u4e3a\u8de8\u8bed\u8a00\u6027\u80fd\u6bd4\u8f83\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.23120", "pdf": "https://arxiv.org/pdf/2505.23120", "abs": "https://arxiv.org/abs/2505.23120", "authors": ["Siyuan Wang", "Jiawei Liu", "Wei Wang", "Yeying Jin", "Jinsong Du", "Zhi Han"], "title": "MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Co-Speech Gesture Video Generation aims to generate vivid speech videos from\naudio-driven still images, which is challenging due to the diversity of\ndifferent parts of the body in terms of amplitude of motion, audio relevance,\nand detailed features. Relying solely on audio as the control signal often\nfails to capture large gesture movements in video, leading to more pronounced\nartifacts and distortions. Existing approaches typically address this issue by\nintroducing additional a priori information, but this can limit the practical\napplication of the task. Specifically, we propose a Motion Mask-Guided\nTwo-Stage Network (MMGT) that uses audio, as well as motion masks and motion\nfeatures generated from the audio signal to jointly drive the generation of\nsynchronized speech gesture videos. In the first stage, the Spatial Mask-Guided\nAudio Pose Generation (SMGA) Network generates high-quality pose videos and\nmotion masks from audio, effectively capturing large movements in key regions\nsuch as the face and gestures. In the second stage, we integrate the Motion\nMasked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion\nVideo Generation model, overcoming limitations in fine-grained motion\ngeneration and region-specific detail control found in traditional methods.\nThis guarantees high-quality, detailed upper-body video generation with\naccurate texture and motion details. Evaluations show improved video quality,\nlip-sync, and gesture. The model and code are available at\nhttps://github.com/SIA-IDE/MMGT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u63a9\u7801\u5f15\u5bfc\u7684\u4e24\u9636\u6bb5\u7f51\u7edc\uff08MMGT\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u97f3\u9891\u3001\u8fd0\u52a8\u63a9\u7801\u548c\u8fd0\u52a8\u7279\u5f81\u751f\u6210\u540c\u6b65\u7684\u8bed\u97f3\u624b\u52bf\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u56e0\u4ec5\u4f9d\u8d56\u97f3\u9891\u800c\u5bfc\u81f4\u7684\u8fd0\u52a8\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u4ec5\u4f9d\u8d56\u97f3\u9891\u4f5c\u4e3a\u63a7\u5236\u4fe1\u53f7\u96be\u4ee5\u6355\u6349\u5927\u5e45\u624b\u52bf\u52a8\u4f5c\uff0c\u5bfc\u81f4\u89c6\u9891\u4e2d\u51fa\u73b0\u660e\u663e\u5931\u771f\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5f15\u5165\u989d\u5916\u5148\u9a8c\u4fe1\u606f\uff0c\u4f46\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faMMGT\u7f51\u7edc\uff0c\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1\uff09SMGA\u7f51\u7edc\u4ece\u97f3\u9891\u751f\u6210\u9ad8\u8d28\u91cf\u59ff\u6001\u89c6\u9891\u548c\u8fd0\u52a8\u63a9\u7801\uff1b2\uff09MM-HAA\u6a21\u5757\u7ed3\u5408\u7a33\u5b9a\u6269\u6563\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u548c\u533a\u57df\u7ec6\u8282\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u5507\u540c\u6b65\u548c\u624b\u52bf\u751f\u6210\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MMGT\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u8fd0\u52a8\u63a9\u7801\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u624b\u52bf\u89c6\u9891\u7684\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.23038", "pdf": "https://arxiv.org/pdf/2505.23038", "abs": "https://arxiv.org/abs/2505.23038", "authors": ["Yuzhen Xiao", "Jiahe Song", "Yongxin Xu", "Ruizhe Zhang", "Yiqi Xiao", "Xin Lu", "Runchuan Zhu", "Bowen Jiang", "Junfeng Zhao"], "title": "EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In-Context Learning (ICL) technique based on Large Language Models (LLMs) has\ngained prominence in Named Entity Recognition (NER) tasks for its lower\ncomputing resource consumption, less manual labeling overhead, and stronger\ngeneralizability. Nevertheless, most ICL-based NER methods depend on\nlarge-parameter LLMs: the open-source models demand substantial computational\nresources for deployment and inference, while the closed-source ones incur high\nAPI costs, raise data-privacy concerns, and hinder community collaboration. To\naddress this question, we propose an Ensemble Learning Method for Named Entity\nRecognition (EL4NER), which aims at aggregating the ICL outputs of multiple\nopen-source, small-parameter LLMs to enhance overall performance in NER tasks\nat less deployment and inference cost. Specifically, our method comprises three\nkey components. First, we design a task decomposition-based pipeline that\nfacilitates deep, multi-stage ensemble learning. Second, we introduce a novel\nspan-level sentence similarity algorithm to establish an ICL demonstration\nretrieval mechanism better suited for NER tasks. Third, we incorporate a\nself-validation mechanism to mitigate the noise introduced during the ensemble\nprocess. We evaluated EL4NER on multiple widely adopted NER datasets from\ndiverse domains. Our experimental results indicate that EL4NER surpasses most\nclosed-source, large-parameter LLM-based methods at a lower parameter cost and\neven attains state-of-the-art (SOTA) performance among ICL-based methods on\ncertain datasets. These results show the parameter efficiency of EL4NER and\nunderscore the feasibility of employing open-source, small-parameter LLMs\nwithin the ICL paradigm for NER tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEL4NER\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5f00\u6e90\u5c0f\u53c2\u6570LLM\u7684ICL\u8f93\u51fa\u6765\u63d0\u5347NER\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u90e8\u7f72\u548c\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709ICL-based NER\u65b9\u6cd5\u4f9d\u8d56\u5927\u53c2\u6570LLM\u7684\u95ee\u9898\uff0c\u5305\u62ec\u9ad8\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3001API\u6210\u672c\u3001\u6570\u636e\u9690\u79c1\u548c\u534f\u4f5c\u969c\u788d\u3002", "method": "1. \u8bbe\u8ba1\u57fa\u4e8e\u4efb\u52a1\u5206\u89e3\u7684\u7ba1\u9053\uff1b2. \u5f15\u5165span\u7ea7\u53e5\u5b50\u76f8\u4f3c\u5ea6\u7b97\u6cd5\u4f18\u5316ICL\u6f14\u793a\u68c0\u7d22\uff1b3. \u52a0\u5165\u81ea\u9a8c\u8bc1\u673a\u5236\u51cf\u5c11\u96c6\u6210\u566a\u58f0\u3002", "result": "EL4NER\u5728\u591a\u4e2aNER\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5927\u53c2\u6570LLM\u65b9\u6cd5\uff0c\u90e8\u5206\u6570\u636e\u96c6\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "EL4NER\u5c55\u793a\u4e86\u5c0f\u53c2\u6570LLM\u5728ICL\u8303\u5f0f\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2505.23129", "pdf": "https://arxiv.org/pdf/2505.23129", "abs": "https://arxiv.org/abs/2505.23129", "authors": ["Bin Wang", "Pingjun Li", "Jinkun Liu", "Jun Cheng", "Hailong Lei", "Yinze Rong", "Huan-ang Gao", "Kangliang Chen", "Xing Pan", "Weihao Gu"], "title": "HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end autonomous driving faces persistent challenges in both generating\ndiverse, rule-compliant trajectories and robustly selecting the optimal path\nfrom these options via learned, multi-faceted evaluation. To address these\nchallenges, we introduce HMAD, a framework integrating a distinctive\nBird's-Eye-View (BEV) based trajectory proposal mechanism with learned\nmulti-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored\nqueries, initialized from a trajectory dictionary and refined via iterative\noffset decoding (inspired by DiffusionDrive), to produce numerous diverse and\nstable candidate trajectories. A key innovation, our simulation-supervised\nscorer module, then evaluates these proposals against critical metrics\nincluding no at-fault collisions, drivable area compliance, comfortableness,\nand overall driving quality (i.e., extended PDM score). Demonstrating its\nefficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test\nset. This work highlights the benefits of effectively decoupling robust\ntrajectory generation from comprehensive, safety-aware learned scoring for\nadvanced autonomous driving.", "AI": {"tldr": "HMAD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408BEV\u8f68\u8ff9\u751f\u6210\u548c\u591a\u6807\u51c6\u8bc4\u5206\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f68\u8ff9\u591a\u6837\u6027\u548c\u8def\u5f84\u9009\u62e9\u7684\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5728\u751f\u6210\u591a\u6837\u4e14\u5408\u89c4\u7684\u8f68\u8ff9\u4ee5\u53ca\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u5206\u9009\u62e9\u6700\u4f18\u8def\u5f84\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "HMAD\u5229\u7528BEVFormer\u548c\u53ef\u5b66\u4e60\u951a\u70b9\u67e5\u8be2\u751f\u6210\u591a\u6837\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u76d1\u7763\u8bc4\u5206\u6a21\u5757\u8bc4\u4f30\u8f68\u8ff9\u3002", "result": "HMAD\u5728CVPR 2025\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523044.5%\u7684\u9a7e\u9a76\u8bc4\u5206\u3002", "conclusion": "HMAD\u5c55\u793a\u4e86\u8f68\u8ff9\u751f\u6210\u4e0e\u5b89\u5168\u8bc4\u5206\u5206\u79bb\u5bf9\u9ad8\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.23052", "pdf": "https://arxiv.org/pdf/2505.23052", "abs": "https://arxiv.org/abs/2505.23052", "authors": ["Jiarui Zhang", "Xiangyu Liu", "Yong Hu", "Chaoyue Niu", "Fan Wu", "Guihai Chen"], "title": "Query Routing for Retrieval-Augmented Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) significantly improves the performance\nof Large Language Models (LLMs) on knowledge-intensive tasks. However, varying\nresponse quality across LLMs under RAG necessitates intelligent routing\nmechanisms, which select the most suitable model for each query from multiple\nretrieval-augmented LLMs via a dedicated router model. We observe that external\ndocuments dynamically affect LLMs' ability to answer queries, while existing\nrouting methods, which rely on static parametric knowledge representations,\nexhibit suboptimal performance in RAG scenarios. To address this, we formally\ndefine the new retrieval-augmented LLM routing problem, incorporating the\ninfluence of retrieved documents into the routing framework. We propose\nRAGRouter, a RAG-aware routing design, which leverages document embeddings and\nRAG capability embeddings with contrastive learning to capture knowledge\nrepresentation shifts and enable informed routing decisions. Extensive\nexperiments on diverse knowledge-intensive tasks and retrieval settings show\nthat RAGRouter outperforms the best individual LLM by 3.61% on average and\nexisting routing methods by 3.29%-9.33%. With an extended score-threshold-based\nmechanism, it also achieves strong performance-efficiency trade-offs under\nlow-latency constraints.", "AI": {"tldr": "RAGRouter\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8def\u7531\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u5408\u68c0\u7d22\u6587\u6863\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u4e86\u591aLLM\u5728RAG\u573a\u666f\u4e0b\u7684\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u5728RAG\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\u8868\u793a\uff0c\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u68c0\u7d22\u6587\u6863\u5bf9LLM\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faRAGRouter\uff0c\u5229\u7528\u6587\u6863\u5d4c\u5165\u548cRAG\u80fd\u529b\u5d4c\u5165\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6355\u6349\u77e5\u8bc6\u8868\u793a\u53d8\u5316\uff0c\u5b9e\u73b0\u667a\u80fd\u8def\u7531\u3002", "result": "RAGRouter\u5e73\u5747\u4f18\u4e8e\u6700\u4f73\u5355LLM 3.61%\uff0c\u4f18\u4e8e\u73b0\u6709\u8def\u7531\u65b9\u6cd53.29%-9.33%\uff0c\u5e76\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "conclusion": "RAGRouter\u901a\u8fc7\u52a8\u6001\u8def\u7531\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86RAG\u573a\u666f\u4e0b\u7684LLM\u9009\u62e9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2505.23130", "pdf": "https://arxiv.org/pdf/2505.23130", "abs": "https://arxiv.org/abs/2505.23130", "authors": ["Haoyu Chen", "Keda Tao", "Yizao Wang", "Xinlei Wang", "Lei Zhu", "Jinjin Gu"], "title": "PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents", "categories": ["cs.CV"], "comment": null, "summary": "Photo retouching is integral to photographic art, extending far beyond simple\ntechnical fixes to heighten emotional expression and narrative depth. While\nartists leverage expertise to create unique visual effects through deliberate\nadjustments, non-professional users often rely on automated tools that produce\nvisually pleasing results but lack interpretative depth and interactive\ntransparency. In this paper, we introduce PhotoArtAgent, an intelligent system\nthat combines Vision-Language Models (VLMs) with advanced natural language\nreasoning to emulate the creative process of a professional artist. The agent\nperforms explicit artistic analysis, plans retouching strategies, and outputs\nprecise parameters to Lightroom through an API. It then evaluates the resulting\nimages and iteratively refines them until the desired artistic vision is\nachieved. Throughout this process, PhotoArtAgent provides transparent,\ntext-based explanations of its creative rationale, fostering meaningful\ninteraction and user control. Experimental results show that PhotoArtAgent not\nonly surpasses existing automated tools in user studies but also achieves\nresults comparable to those of professional human artists.", "AI": {"tldr": "PhotoArtAgent\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u6a21\u62df\u4e13\u4e1a\u827a\u672f\u5bb6\u7684\u4fee\u56fe\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u6548\u679c\u4f18\u4e8e\u81ea\u52a8\u5316\u5de5\u5177\u5e76\u63a5\u8fd1\u4e13\u4e1a\u827a\u672f\u5bb6\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u4e1a\u7528\u6237\u4f9d\u8d56\u81ea\u52a8\u5316\u5de5\u5177\u65f6\u7f3a\u4e4f\u89e3\u91ca\u6df1\u5ea6\u548c\u4ea4\u4e92\u900f\u660e\u6027\u7684\u95ee\u9898\uff0c\u6a21\u62df\u4e13\u4e1a\u827a\u672f\u5bb6\u7684\u521b\u610f\u8fc7\u7a0b\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff0c\u8fdb\u884c\u827a\u672f\u5206\u6790\u3001\u5236\u5b9a\u4fee\u56fe\u7b56\u7565\uff0c\u5e76\u901a\u8fc7API\u8f93\u51fa\u53c2\u6570\u5230Lightroom\uff0c\u8fed\u4ee3\u4f18\u5316\u5e76\u63d0\u4f9b\u89e3\u91ca\u3002", "result": "\u5728\u7528\u6237\u7814\u7a76\u4e2d\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u6548\u679c\u63a5\u8fd1\u4e13\u4e1a\u827a\u672f\u5bb6\u6c34\u5e73\u3002", "conclusion": "PhotoArtAgent\u901a\u8fc7\u900f\u660e\u4ea4\u4e92\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u4e86\u63a5\u8fd1\u4e13\u4e1a\u6c34\u5e73\u7684\u4fee\u56fe\u4f53\u9a8c\u3002"}}
{"id": "2505.23060", "pdf": "https://arxiv.org/pdf/2505.23060", "abs": "https://arxiv.org/abs/2505.23060", "authors": ["Jeonghun Cho", "Deokhyung Kang", "Hyounghun Kim", "Gary Geunbae Lee"], "title": "Self-Correcting Code Generation Using Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Self-correction has demonstrated potential in code generation by allowing\nlanguage models to revise and improve their outputs through successive\nrefinement. Recent studies have explored prompting-based strategies that\nincorporate verification or feedback loops using proprietary models, as well as\ntraining-based methods that leverage their strong reasoning capabilities.\nHowever, whether smaller models possess the capacity to effectively guide their\noutputs through self-reflection remains unexplored. Our findings reveal that\nsmaller models struggle to exhibit reflective revision behavior across both\nself-correction paradigms. In response, we introduce CoCoS, an approach\ndesigned to enhance the ability of small language models for multi-turn code\ncorrection. Specifically, we propose an online reinforcement learning objective\nthat trains the model to confidently maintain correct outputs while\nprogressively correcting incorrect outputs as turns proceed. Our approach\nfeatures an accumulated reward function that aggregates rewards across the\nentire trajectory and a fine-grained reward better suited to multi-turn\ncorrection scenarios. This facilitates the model in enhancing initial response\nquality while achieving substantial improvements through self-correction. With\n1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on\nHumanEval compared to the baselines.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c0f\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u901a\u8fc7\u81ea\u6211\u4fee\u6b63\u63d0\u5347\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoCoS\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u591a\u8f6e\u4fee\u6b63\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u5c0f\u6a21\u578b\u662f\u5426\u5177\u5907\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u4fee\u6b63\u4ee3\u7801\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faCoCoS\u65b9\u6cd5\uff0c\u91c7\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u8bbe\u8ba1\u7d2f\u79ef\u5956\u52b1\u51fd\u6570\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u4f18\u5316\u591a\u8f6e\u4fee\u6b63\u6548\u679c\u3002", "result": "\u57281B\u89c4\u6a21\u6a21\u578b\u4e0a\uff0cCoCoS\u5728MBPP\u548cHumanEval\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u4e8635.8%\u548c27.7%\u3002", "conclusion": "CoCoS\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u4e3a\u5c0f\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23134", "pdf": "https://arxiv.org/pdf/2505.23134", "abs": "https://arxiv.org/abs/2505.23134", "authors": ["Tongtong Su", "Chengyu Wang", "Jun Huang", "Dongming Lu"], "title": "Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Appearance editing according to user needs is a pivotal task in video\nediting. Existing text-guided methods often lead to ambiguities regarding user\nintentions and restrict fine-grained control over editing specific aspects of\nobjects. To overcome these limitations, this paper introduces a novel approach\nnamed {Zero-to-Hero}, which focuses on reference-based video editing that\ndisentangles the editing process into two distinct problems. It achieves this\nby first editing an anchor frame to satisfy user requirements as a reference\nimage and then consistently propagating its appearance across other frames. We\nleverage correspondence within the original frames to guide the attention\nmechanism, which is more robust than previously proposed optical flow or\ntemporal modules in memory-friendly video generative models, especially when\ndealing with objects exhibiting large motions. It offers a solid ZERO-shot\ninitialization that ensures both accuracy and temporal consistency. However,\nintervention in the attention mechanism results in compounded imaging\ndegradation with over-saturated colors and unknown blurring issues. Starting\nfrom Zero-Stage, our Hero-Stage Holistically learns a conditional generative\nmodel for vidEo RestOration. To accurately evaluate the consistency of the\nappearance, we construct a set of videos with multiple appearances using\nBlender, enabling a fine-grained and deterministic evaluation. Our method\noutperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The\nproject page is at https://github.com/Tonniia/Zero2Hero.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZero-to-Hero\u7684\u53c2\u8003\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u7f16\u8f91\u8fc7\u7a0b\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u65b9\u6cd5\u5b58\u5728\u7528\u6237\u610f\u56fe\u6a21\u7cca\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e24\u9636\u6bb5\uff1aZero\u9636\u6bb5\u7f16\u8f91\u951a\u5e27\u4f5c\u4e3a\u53c2\u8003\uff0cHero\u9636\u6bb5\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u6a21\u578b\u6062\u590d\u89c6\u9891\u8d28\u91cf\u3002", "result": "PSNR\u63d0\u53472.6 dB\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Zero-to-Hero\u65b9\u6cd5\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.23065", "pdf": "https://arxiv.org/pdf/2505.23065", "abs": "https://arxiv.org/abs/2505.23065", "authors": ["Hongcheng Guo", "Zheyong Xie", "Shaosheng Cao", "Boyang Wang", "Weiting Liu", "Anjie Le", "Lei Li", "Zhoujun Li"], "title": "SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services", "categories": ["cs.CL"], "comment": null, "summary": "With the increasing integration of visual and textual content in Social\nNetworking Services (SNS), evaluating the multimodal capabilities of Large\nLanguage Models (LLMs) is crucial for enhancing user experience, content\nunderstanding, and platform intelligence. Existing benchmarks primarily focus\non text-centric tasks, lacking coverage of the multimodal contexts prevalent in\nmodern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a\ncomprehensive multimodal benchmark designed to assess the performance of\nVision-Language LLMs in real-world social media scenarios. SNS-Bench-VL\nincorporates images and text across 8 multimodal tasks, including note\ncomprehension, user engagement analysis, information retrieval, and\npersonalized recommendation. It comprises 4,001 carefully curated multimodal\nquestion-answer pairs, covering single-choice, multiple-choice, and open-ended\ntasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their\nperformance across tasks. Our findings highlight persistent challenges in\nmultimodal social context comprehension. We hope SNS-Bench-VL will inspire\nfuture research towards robust, context-aware, and human-aligned multimodal\nintelligence for next-generation social networking services.", "AI": {"tldr": "SNS-Bench-VL\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d68\u79cd\u4efb\u52a1\u548c4001\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u6a21\u6001\u5185\u5bb9\u7684\u8986\u76d6\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u4e86SNS-Bench-VL\uff0c\u5305\u542b8\u79cd\u591a\u6a21\u6001\u4efb\u52a1\u548c4001\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8bc4\u4f30\u4e8625\u79cd\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u793e\u4ea4\u8bed\u5883\u7406\u89e3\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "SNS-Bench-VL\u6709\u671b\u63a8\u52a8\u672a\u6765\u7814\u7a76\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u7b26\u5408\u4eba\u7c7b\u9700\u6c42\u7684\u591a\u6a21\u6001\u667a\u80fd\u3002"}}
{"id": "2505.23143", "pdf": "https://arxiv.org/pdf/2505.23143", "abs": "https://arxiv.org/abs/2505.23143", "authors": ["Jinquan Guan", "Qi Chen", "Lizhou Liang", "Yuhang Liu", "Vu Minh Hieu Phan", "Minh-Son To", "Jian Chen", "Yutong Xie"], "title": "Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning", "categories": ["cs.CV"], "comment": "10 pages (main text), 18 pages (appendix)", "summary": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation\nassistants have demonstrated significant progress and are increasingly being\napplied in clinical settings. However, contemporary medical AI models often\nadhere to a simplistic input-to-output paradigm, directly processing an image\nand an instruction to generate a result, where the instructions may be integral\nto the model's architecture. This approach overlooks the modeling of the\ninherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is\ntypically sequential, where each interpretive stage considers the images, the\ncurrent task, and the contextual information from previous stages. This\noversight leads to several shortcomings, including misalignment with clinical\nscenarios, contextless reasoning, and untraceable errors. To fill this gap, we\nconstruct CXRTrek, a new multi-stage visual question answering (VQA) dataset\nfor CXR interpretation. The dataset is designed to explicitly simulate the\ndiagnostic reasoning process employed by radiologists in real-world clinical\nsettings for the first time. CXRTrek covers 8 sequential diagnostic stages,\ncomprising 428,966 samples and over 11 million question-answer (Q&A) pairs,\nwith an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,\nwe propose a new vision-language large model (VLLM), CXRTrekNet, specifically\ndesigned to incorporate the clinical reasoning flow into the VLLM framework.\nCXRTrekNet effectively models the dependencies between diagnostic stages and\ncaptures reasoning patterns within the radiological context. Trained on our\ndataset, the model consistently outperforms existing medical VLLMs on the\nCXRTrek benchmarks and demonstrates superior generalization across multiple\ntasks on five diverse external datasets. The dataset and model can be found in\nour repository (https://github.com/guanjinquan/CXRTrek).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCXRTrek\u6570\u636e\u96c6\u548cCXRTrekNet\u6a21\u578b\uff0c\u6a21\u62df\u653e\u5c04\u79d1\u533b\u751f\u7684\u591a\u9636\u6bb5\u8bca\u65ad\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u533b\u5b66AI\u6a21\u578b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u533b\u5b66AI\u6a21\u578b\u5ffd\u89c6\u8bca\u65ad\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4e0e\u4e34\u5e8a\u573a\u666f\u4e0d\u5339\u914d\u3001\u63a8\u7406\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u548c\u9519\u8bef\u4e0d\u53ef\u8ffd\u6eaf\u3002", "method": "\u6784\u5efaCXRTrek\u6570\u636e\u96c6\uff088\u9636\u6bb5\u8bca\u65ad\uff0c42.9\u4e07\u6837\u672c\uff0c1100\u4e07Q&A\u5bf9\uff09\uff0c\u5e76\u63d0\u51faCXRTrekNet\u6a21\u578b\uff0c\u6574\u5408\u4e34\u5e8a\u63a8\u7406\u6d41\u7a0b\u3002", "result": "CXRTrekNet\u5728CXRTrek\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u533b\u5b66VLLM\uff0c\u5e76\u5728\u591a\u4e2a\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CXRTrek\u6570\u636e\u96c6\u548c\u6a21\u578b\u6709\u6548\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\uff0c\u63d0\u5347\u4e86\u533b\u5b66AI\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.23078", "pdf": "https://arxiv.org/pdf/2505.23078", "abs": "https://arxiv.org/abs/2505.23078", "authors": ["Yuu Jinnai"], "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Document-level text generation tasks are known to be more difficult than\nsentence-level text generation tasks as they require the understanding of\nlonger context to generate high-quality texts. In this paper, we investigate\nthe adaption of Minimum Bayes Risk (MBR) decoding for document-level text\ngeneration tasks. MBR decoding makes use of a utility function to estimate the\noutput with the highest expected utility from a set of candidate outputs.\nAlthough MBR decoding is shown to be effective in a wide range of\nsentence-level text generation tasks, its performance on document-level text\ngeneration tasks is limited as many of the utility functions are designed for\nevaluating the utility of sentences. To this end, we propose MBR-OT, a variant\nof MBR decoding using Wasserstein distance to compute the utility of a document\nusing a sentence-level utility function. The experimental result shows that the\nperformance of MBR-OT outperforms that of the standard MBR in document-level\nmachine translation, text simplification, and dense image captioning tasks. Our\ncode is available at https://github.com/jinnaiyuu/mbr-optimal-transport", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5c06\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\uff08MBR\uff09\u89e3\u7801\u5e94\u7528\u4e8e\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u6539\u8fdb\u65b9\u6cd5MBR-OT\u3002", "motivation": "\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\u6bd4\u53e5\u5b50\u7ea7\u4efb\u52a1\u66f4\u590d\u6742\uff0c\u9700\u8981\u7406\u89e3\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u7684MBR\u89e3\u7801\u65b9\u6cd5\u5728\u6587\u6863\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u56e0\u5176\u6548\u7528\u51fd\u6570\u591a\u9488\u5bf9\u53e5\u5b50\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faMBR-OT\uff0c\u5229\u7528Wasserstein\u8ddd\u79bb\u7ed3\u5408\u53e5\u5b50\u7ea7\u6548\u7528\u51fd\u6570\u8ba1\u7b97\u6587\u6863\u7ea7\u6548\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMBR-OT\u5728\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u7b80\u5316\u548c\u5bc6\u96c6\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6MBR\u3002", "conclusion": "MBR-OT\u901a\u8fc7\u6539\u8fdb\u6548\u7528\u8ba1\u7b97\u65b9\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u7ea7\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23145", "pdf": "https://arxiv.org/pdf/2505.23145", "abs": "https://arxiv.org/abs/2505.23145", "authors": ["Jeongsol Kim", "Yeobin Hong", "Jong Chul Ye"], "title": "FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent inversion-free, flow-based image editing methods such as FlowEdit\nleverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,\nenabling text-driven manipulation by solving an ordinary differential equation\n(ODE). While the lack of exact latent inversion is a core advantage of these\nmethods, it often results in unstable editing trajectories and poor source\nconsistency. To address this limitation, we propose FlowAlign, a novel\ninversion-free flow-based framework for consistent image editing with\nprincipled trajectory control. FlowAlign introduces a flow-matching loss as a\nregularization mechanism to promote smoother and more stable trajectories\nduring the editing process. Notably, the flow-matching loss is shown to\nexplicitly balance semantic alignment with the edit prompt and structural\nconsistency with the source image along the trajectory. Furthermore, FlowAlign\nnaturally supports reverse editing by simply reversing the ODE trajectory,\nhighlighting the reversible and consistent nature of the transformation.\nExtensive experiments demonstrate that FlowAlign outperforms existing methods\nin both source preservation and editing controllability.", "AI": {"tldr": "FlowAlign\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u7684\u65e0\u53cd\u8f6c\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5339\u914d\u635f\u5931\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u4e00\u81f4\u7684\u7f16\u8f91\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff08\u5982FlowEdit\uff09\u56e0\u7f3a\u4e4f\u7cbe\u786e\u7684\u6f5c\u5728\u53cd\u8f6c\u5bfc\u81f4\u7f16\u8f91\u8f68\u8ff9\u4e0d\u7a33\u5b9a\u548c\u6e90\u4e00\u81f4\u6027\u5dee\u3002", "method": "FlowAlign\u5f15\u5165\u6d41\u5339\u914d\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u5316\u673a\u5236\uff0c\u5e73\u8861\u7f16\u8f91\u63d0\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u6e90\u56fe\u50cf\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFlowAlign\u5728\u6e90\u4fdd\u7559\u548c\u7f16\u8f91\u53ef\u63a7\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowAlign\u901a\u8fc7\u6d41\u5339\u914d\u635f\u5931\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u4e00\u81f4\u4e14\u53ef\u9006\u7684\u56fe\u50cf\u7f16\u8f91\u3002"}}
{"id": "2505.23108", "pdf": "https://arxiv.org/pdf/2505.23108", "abs": "https://arxiv.org/abs/2505.23108", "authors": ["Zexuan Li", "Hongliang Dai", "Piji Li"], "title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models", "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "Using Large Language Models (LLMs) to generate training data can potentially\nbe a preferable way to improve zero or few-shot NLP tasks. However, many\nproblems remain to be investigated for this direction. For the task of Relation\nExtraction (RE), we find that samples generated by directly prompting LLMs may\neasily have high structural similarities with each other. They tend to use a\nlimited variety of phrasing while expressing the relation between a pair of\nentities. Therefore, in this paper, we study how to effectively improve the\ndiversity of the training samples generated with LLMs for RE, while also\nmaintaining their correctness. We first try to make the LLMs produce dissimilar\nsamples by directly giving instructions in In-Context Learning (ICL) prompts.\nThen, we propose an approach to fine-tune LLMs for diversity training sample\ngeneration through Direct Preference Optimization (DPO). Our experiments on\ncommonly used RE datasets show that both attempts can improve the quality of\nthe generated training data. We also find that comparing with directly\nperforming RE with an LLM, training a non-LLM RE model with its generated\nsamples may lead to better performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u591a\u6837\u4e14\u6b63\u786e\u7684\u5173\u7cfb\u62bd\u53d6\uff08RE\uff09\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u6307\u4ee4\u63d0\u793a\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5fae\u8c03LLM\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528LLM\u751f\u6210\u7684\u5173\u7cfb\u62bd\u53d6\u8bad\u7ec3\u6837\u672c\u7ed3\u6784\u76f8\u4f3c\u5ea6\u9ad8\uff0c\u8868\u8fbe\u65b9\u5f0f\u5355\u4e00\uff0c\u9700\u63d0\u5347\u591a\u6837\u6027\u548c\u6b63\u786e\u6027\u3002", "method": "1. \u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u63d0\u793a\u76f4\u63a5\u6307\u5bfcLLM\u751f\u6210\u591a\u6837\u5316\u6837\u672c\uff1b2. \u4f7f\u7528DPO\u5fae\u8c03LLM\u4ee5\u751f\u6210\u591a\u6837\u6027\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e24\u79cd\u65b9\u6cd5\u5747\u80fd\u63d0\u5347\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\uff0c\u4e14\u7528\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u975eLLM\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528LLM\u3002", "conclusion": "\u901a\u8fc7\u6307\u4ee4\u63d0\u793a\u548cDPO\u5fae\u8c03LLM\u53ef\u6709\u6548\u63d0\u5347\u5173\u7cfb\u62bd\u53d6\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u95f4\u63a5\u8bad\u7ec3\u975eLLM\u6a21\u578b\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2505.23155", "pdf": "https://arxiv.org/pdf/2505.23155", "abs": "https://arxiv.org/abs/2505.23155", "authors": ["Xiao Yu", "Yan Fang", "Xiaojie Jin", "Yao Zhao", "Yunchao Wei"], "title": "PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling", "categories": ["cs.CV"], "comment": "20 pages, 8 figures", "summary": "Audio-visual event parsing plays a crucial role in understanding multimodal\nvideo content, but existing methods typically rely on offline processing of\nentire videos with huge model sizes, limiting their real-time applicability. We\nintroduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for\nparsing audio, visual, and audio-visual events by sequentially analyzing\nincoming video streams. The On-AVEP task necessitates models with two key\ncapabilities: (1) Accurate online inference, to effectively distinguish events\nwith unclear and limited context in online settings, and (2) Real-time\nefficiency, to balance high performance with computational constraints. To\ncultivate these, we propose the Predictive Future Modeling (PreFM) framework\nfeatured by (a) predictive multimodal future modeling to infer and integrate\nbeneficial future audio-visual cues, thereby enhancing contextual understanding\nand (b) modality-agnostic robust representation along with focal temporal\nprioritization to improve precision and generalization. Extensive experiments\non the UnAV-100 and LLP datasets show PreFM significantly outperforms\nstate-of-the-art methods by a large margin with significantly fewer parameters,\noffering an insightful approach for real-time multimodal video understanding.\nCode is available at https://github.com/XiaoYu-1123/PreFM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u7ebf\u97f3\u89c6\u9891\u4e8b\u4ef6\u89e3\u6790\uff08On-AVEP\uff09\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9884\u6d4b\u672a\u6765\u5efa\u6a21\uff08PreFM\uff09\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5728\u7ebf\u5b9e\u65f6\u5904\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u5904\u7406\u4e14\u6a21\u578b\u5e9e\u5927\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u7ebf\u89e3\u6790\u97f3\u89c6\u9891\u4e8b\u4ef6\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPreFM\u6846\u67b6\uff0c\u5305\u62ec\u9884\u6d4b\u591a\u6a21\u6001\u672a\u6765\u5efa\u6a21\u548c\u6a21\u6001\u65e0\u5173\u7684\u9c81\u68d2\u8868\u793a\uff0c\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\u5e76\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728UnAV-100\u548cLLP\u6570\u636e\u96c6\u4e0a\uff0cPreFM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "PreFM\u4e3a\u5b9e\u65f6\u591a\u6a21\u6001\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23114", "pdf": "https://arxiv.org/pdf/2505.23114", "abs": "https://arxiv.org/abs/2505.23114", "authors": ["Seohyeong Lee", "Eunwon Kim", "Hwaran Lee", "Buru Chang"], "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data", "categories": ["cs.CL"], "comment": null, "summary": "Human preference data plays a critical role in aligning large language models\n(LLMs) with human values. However, collecting such data is often expensive and\ninefficient, posing a significant scalability challenge. To address this, we\nintroduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and\ndiagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we\ncompute alignment scores for LLM-generated responses to instructions from\nexisting preference datasets. These scores are then used to construct an\nAlignment Data Map based on their mean and variance. Our experiments show that\nusing only 33 percent of the data, specifically samples in the high-mean,\nlow-variance region, achieves performance comparable to or better than using\nthe entire dataset. This finding suggests that the Alignment Data Map can\nsignificantly improve data collection efficiency by identifying high-quality\nsamples for LLM alignment without requiring explicit annotations. Moreover, the\nAlignment Data Map can diagnose existing preference datasets. Our analysis\nshows that it effectively detects low-impact or potentially misannotated\nsamples. Source code is available online.", "AI": {"tldr": "Alignment Data Map\u5229\u7528GPT-4o\u5206\u6790\u504f\u597d\u6570\u636e\uff0c\u901a\u8fc7\u8ba1\u7b97\u5bf9\u9f50\u5206\u6570\u5e76\u6784\u5efa\u6570\u636e\u5730\u56fe\uff0c\u4ec5\u970033%\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6570\u636e\u96c6\u6027\u80fd\u3002", "motivation": "\u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528GPT-4o\u4f5c\u4e3aLLM\u5bf9\u9f50\u4ee3\u7406\uff0c\u8ba1\u7b97\u5bf9\u9f50\u5206\u6570\u5e76\u6784\u5efa\u57fa\u4e8e\u5747\u503c\u548c\u65b9\u5dee\u7684Alignment Data Map\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u4f7f\u752833%\u7684\u9ad8\u8d28\u91cf\u6570\u636e\uff08\u9ad8\u5747\u503c\u3001\u4f4e\u65b9\u5dee\u533a\u57df\uff09\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u5168\u6570\u636e\u96c6\u6027\u80fd\u3002", "conclusion": "Alignment Data Map\u663e\u8457\u63d0\u5347\u6570\u636e\u6536\u96c6\u6548\u7387\uff0c\u5e76\u80fd\u8bca\u65ad\u73b0\u6709\u504f\u597d\u6570\u636e\u96c6\u4e2d\u7684\u4f4e\u6548\u6216\u9519\u8bef\u6807\u6ce8\u6837\u672c\u3002"}}
{"id": "2505.23158", "pdf": "https://arxiv.org/pdf/2505.23158", "abs": "https://arxiv.org/abs/2505.23158", "authors": ["Jonas Kulhanek", "Marie-Julie Rakotosaona", "Fabian Manhardt", "Christina Tsalicoglou", "Michael Niemeyer", "Torsten Sattler", "Songyou Peng", "Federico Tombari"], "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering", "categories": ["cs.CV"], "comment": "Web: https://lodge-gs.github.io/", "summary": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian\nSplatting that enables real-time rendering of large-scale scenes on\nmemory-constrained devices. Our approach introduces a hierarchical LOD\nrepresentation that iteratively selects optimal subsets of Gaussians based on\ncamera distance, thus largely reducing both rendering time and GPU memory\nusage. We construct each LOD level by applying a depth-aware 3D smoothing\nfilter, followed by importance-based pruning and fine-tuning to maintain visual\nfidelity. To further reduce memory overhead, we partition the scene into\nspatial chunks and dynamically load only relevant Gaussians during rendering,\nemploying an opacity-blending mechanism to avoid visual artifacts at chunk\nboundaries. Our method achieves state-of-the-art performance on both outdoor\n(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality\nrenderings with reduced latency and memory requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684LOD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u8868\u793a\u548c\u52a8\u6001\u52a0\u8f7d\u4f18\u5316\u5b9e\u65f6\u6e32\u67d3\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u5927\u89c4\u6a21\u573a\u666f\u5b9e\u65f6\u6e32\u67d3\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42LOD\u8868\u793a\u3001\u6df1\u5ea6\u611f\u77e5\u5e73\u6ed1\u6ee4\u6ce2\u3001\u91cd\u8981\u6027\u526a\u679d\u548c\u52a8\u6001\u52a0\u8f7d\u6280\u672f\u3002", "result": "\u5728\u6237\u5916\u548c\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u6e32\u67d3\uff0c\u964d\u4f4e\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002"}}
{"id": "2505.23118", "pdf": "https://arxiv.org/pdf/2505.23118", "abs": "https://arxiv.org/abs/2505.23118", "authors": ["Linjie Mu", "Zhongzhen Huang", "Yakun Zhu", "Xiangyu Zhao", "Shaoting Zhang", "Xiaofan Zhang"], "title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Effective clinical decision-making depends on iterative, multimodal reasoning\nacross diverse sources of evidence. The recent emergence of multimodal\nreasoning models has significantly transformed the landscape of solving complex\ntasks. Although such models have achieved notable success in mathematics and\nscience, their application to medical domains remains underexplored. In this\nwork, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that\nelicits and then enhances multimodal reasoning for medical domains. In Stage-I,\nwe fine-tune models using 2,000 text-only data samples containing precisely\norchestrated reasoning demonstrations to elicit reasoning behaviors. In\nStage-II, we further enhance the model's reasoning capabilities using 1,500\nrigorously curated multimodal medical cases, aligning model reasoning outputs\nwith our proposed multimodal medical reasoning preference. Extensive\nexperiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in\nimproving the reasoning performance of medical multimodal models. Notably,\nmodels trained with \\textit{MedE$^2$} consistently outperform baselines across\nmultiple medical multimodal benchmarks. Additional validation on larger models\nand under inference-time scaling further confirms the robustness and practical\nutility of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedE\u00b2\u7684\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u533b\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6587\u672c\u548c\u591a\u6a21\u6001\u6570\u636e\u7684\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u4f9d\u8d56\u4e8e\u591a\u6e90\u8bc1\u636e\u7684\u591a\u6a21\u6001\u63a8\u7406\uff0c\u800c\u73b0\u6709\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u533b\u5b66\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "MedE\u00b2\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u75282000\u4e2a\u6587\u672c\u6570\u636e\u6837\u672c\u5fae\u8c03\u6a21\u578b\u4ee5\u6fc0\u53d1\u63a8\u7406\u884c\u4e3a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u75281500\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u6848\u4f8b\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMedE\u00b2\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u5728\u5927\u6a21\u578b\u548c\u63a8\u7406\u6269\u5c55\u4e0b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "MedE\u00b2\u4e3a\u533b\u5b66\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23161", "pdf": "https://arxiv.org/pdf/2505.23161", "abs": "https://arxiv.org/abs/2505.23161", "authors": ["Antonio D'Orazio", "Maria Rosaria Briglia", "Donato Crisostomi", "Dario Loi", "Emanuele Rodol\u00e0", "Iacopo Masi"], "title": "Implicit Inversion turns CLIP into a Decoder", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "CLIP is a discriminative model trained to align images and text in a shared\nembedding space. Due to its multimodal structure, it serves as the backbone of\nmany generative pipelines, where a decoder is trained to map from the shared\nspace back to images. In this work, we show that image synthesis is\nnevertheless possible using CLIP alone -- without any decoder, training, or\nfine-tuning. Our approach optimizes a frequency-aware implicit neural\nrepresentation that encourages coarse-to-fine generation by stratifying\nfrequencies across network layers. To stabilize this inverse mapping, we\nintroduce adversarially robust initialization, a lightweight Orthogonal\nProcrustes projection to align local text and image embeddings, and a blending\nloss that anchors outputs to natural image statistics. Without altering CLIP's\nweights, this framework unlocks capabilities such as text-to-image generation,\nstyle transfer, and image reconstruction. These findings suggest that\ndiscriminative models may hold untapped generative potential, hidden in plain\nsight.", "AI": {"tldr": "CLIP\u6a21\u578b\u65e0\u9700\u89e3\u7801\u5668\u6216\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u56fe\u50cf\u5408\u6210\uff0c\u901a\u8fc7\u4f18\u5316\u9891\u7387\u611f\u77e5\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u548c\u5f15\u5165\u7a33\u5b9a\u6280\u672f\uff0c\u89e3\u9501\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7b49\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22CLIP\u4f5c\u4e3a\u5224\u522b\u6a21\u578b\u662f\u5426\u5177\u6709\u672a\u5f00\u53d1\u7684\u751f\u6210\u6f5c\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u89e3\u7801\u5668\u3002", "method": "\u91c7\u7528\u9891\u7387\u611f\u77e5\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u3001\u5bf9\u6297\u9c81\u68d2\u521d\u59cb\u5316\u3001\u6b63\u4ea4Procrustes\u6295\u5f71\u548c\u6df7\u5408\u635f\u5931\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3001\u98ce\u683c\u8fc1\u79fb\u548c\u56fe\u50cf\u91cd\u5efa\u7b49\u529f\u80fd\uff0c\u65e0\u9700\u4fee\u6539CLIP\u6743\u91cd\u3002", "conclusion": "\u5224\u522b\u6a21\u578b\u53ef\u80fd\u9690\u85cf\u7740\u672a\u88ab\u53d1\u73b0\u7684\u751f\u6210\u6f5c\u529b\u3002"}}
{"id": "2505.23121", "pdf": "https://arxiv.org/pdf/2505.23121", "abs": "https://arxiv.org/abs/2505.23121", "authors": ["Yiming Lei", "Zhizheng Yang", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContextQFormer\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u6a21\u5757\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8f6e\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6TMDialog\u3002\u5b9e\u9a8c\u8868\u660e\uff0cContextQFormer\u5728\u53ef\u7528\u7387\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e862%-4%\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\uff08\u5c24\u5176\u662f\u957f\u4e0a\u4e0b\u6587\uff09\u65b9\u9762\u8868\u73b0\u8f83\u5f31\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165ContextQFormer\u6a21\u5757\uff0c\u5229\u7528\u8bb0\u5fc6\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u4fe1\u606f\u8868\u793a\uff0c\u5e76\u6784\u5efaTMDialog\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "ContextQFormer\u5728TMDialog\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u53ef\u7528\u7387\u63d0\u53472%-4%\u3002", "conclusion": "ContextQFormer\u548cTMDialog\u4e3a\u591a\u8f6e\u591a\u6a21\u6001\u5bf9\u8bdd\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u5f00\u6e90\u548c\u4f18\u5316\u3002"}}
{"id": "2505.23171", "pdf": "https://arxiv.org/pdf/2505.23171", "abs": "https://arxiv.org/abs/2505.23171", "authors": ["Liu Liu", "Xiaofeng Wang", "Guosheng Zhao", "Keyu Li", "Wenkang Qin", "Jiaxiong Qiu", "Zheng Zhu", "Guan Huang", "Zhizhong Su"], "title": "RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer", "categories": ["cs.CV"], "comment": "20 pages, 15 figures", "summary": "Imitation Learning has become a fundamental approach in robotic manipulation.\nHowever, collecting large-scale real-world robot demonstrations is\nprohibitively expensive. Simulators offer a cost-effective alternative, but the\nsim-to-real gap make it extremely challenging to scale. Therefore, we introduce\nRoboTransfer, a diffusion-based video generation framework for robotic data\nsynthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry\nwith explicit control over scene components, such as background and object\nattributes. By incorporating cross-view feature interactions and global\ndepth/normal conditions, RoboTransfer ensures geometry consistency across\nviews. This framework allows fine-grained control, including background edits\nand object swaps. Experiments demonstrate that RoboTransfer is capable of\ngenerating multi-view videos with enhanced geometric consistency and visual\nfidelity. In addition, policies trained on the data generated by RoboTransfer\nachieve a 33.3% relative improvement in the success rate in the DIFF-OBJ\nsetting and a substantial 251% relative improvement in the more challenging\nDIFF-ALL scenario. Explore more demos on our project page:\nhttps://horizonrobotics.github.io/robot_lab/robotransfer", "AI": {"tldr": "RoboTransfer\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\uff0c\u89e3\u51b3\u4e86\u6a21\u4eff\u5b66\u4e60\u4e2d\u771f\u5b9e\u6570\u636e\u6536\u96c6\u6602\u8d35\u548c\u6a21\u62df\u5668\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u7684\u95ee\u9898\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u4e2d\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u6a21\u62df\u5668\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u96be\u4ee5\u514b\u670d\u3002", "method": "RoboTransfer\u901a\u8fc7\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u573a\u666f\u7ec4\u4ef6\u63a7\u5236\uff0c\u7ed3\u5408\u8de8\u89c6\u89d2\u7279\u5f81\u4ea4\u4e92\u548c\u5168\u5c40\u6df1\u5ea6/\u6cd5\u7ebf\u6761\u4ef6\uff0c\u786e\u4fdd\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRoboTransfer\u751f\u6210\u7684\u591a\u89c6\u89d2\u89c6\u9891\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u8bad\u7ec3\u7684\u7b56\u7565\u5728DIFF-OBJ\u548cDIFF-ALL\u573a\u666f\u4e2d\u5206\u522b\u63d0\u534733.3%\u548c251%\u7684\u6210\u529f\u7387\u3002", "conclusion": "RoboTransfer\u4e3a\u673a\u5668\u4eba\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23126", "pdf": "https://arxiv.org/pdf/2505.23126", "abs": "https://arxiv.org/abs/2505.23126", "authors": ["Atharva Naik", "Darsh Agrawal", "Manav Kapadnis", "Yuwei An", "Yash Mathur", "Carolyn Rose", "David Mortensen"], "title": "PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics", "categories": ["cs.CL"], "comment": null, "summary": "Recently, long chain of thought (LCoT), Large Language Models (LLMs), have\ntaken the machine learning world by storm with their breathtaking reasoning\ncapabilities. However, are the abstract reasoning abilities of these models\ngeneral enough for problems of practical importance? Unlike past work, which\nhas focused mainly on math, coding, and data wrangling, we focus on a\nhistorical linguistics-inspired inductive reasoning problem, formulated as\nProgramming by Examples. We develop a fully automated pipeline for dynamically\ngenerating a benchmark for this task with controllable difficulty in order to\ntackle scalability and contamination issues to which many reasoning benchmarks\nare subject. Using our pipeline, we generate a test set with nearly 1k\ninstances that is challenging for all state-of-the-art reasoning LLMs, with the\nbest model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating\nthat LCoT LLMs still struggle with a class or reasoning that is ubiquitous in\nhistorical linguistics as well as many other domains.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u957f\u94fe\u601d\u7ef4\uff08LCoT\uff09\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u5f52\u7eb3\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u7814\u7a76LCoT LLMs\u5728\u5b9e\u7528\u95ee\u9898\u4e2d\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u662f\u5426\u8db3\u591f\u901a\u7528\uff0c\u7279\u522b\u662f\u5386\u53f2\u8bed\u8a00\u5b66\u542f\u53d1\u7684\u7f16\u7a0b\u793a\u4f8b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\uff0c\u52a8\u6001\u751f\u6210\u53ef\u63a7\u96be\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u548c\u6c61\u67d3\u95ee\u9898\u3002", "result": "\u751f\u6210\u7684\u6d4b\u8bd5\u96c6\u5bf9\u5f53\u524d\u6700\u4f18\u63a8\u7406LLMs\u5177\u6709\u6311\u6218\u6027\uff0c\u6700\u4f73\u6a21\u578b\uff08Claude-3.7-Sonnet\uff09\u4ec5\u8fbe\u523054%\u901a\u8fc7\u7387\u3002", "conclusion": "LCoT LLMs\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u7b49\u9886\u57df\u4e2d\u5e38\u89c1\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u56f0\u96be\u3002"}}
{"id": "2505.23179", "pdf": "https://arxiv.org/pdf/2505.23179", "abs": "https://arxiv.org/abs/2505.23179", "authors": ["Sungjune Park", "Hyunjun Kim", "Junho Kim", "Seongho Kim", "Yong Man Ro"], "title": "DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant visual\nunderstanding capabilities, yet their fine-grained visual perception in complex\nreal-world scenarios, such as densely crowded public areas, remains limited.\nInspired by the recent success of reinforcement learning (RL) in both LLMs and\nMLLMs, in this paper, we explore how RL can enhance visual perception ability\nof MLLMs. Then we develop a novel RL-based framework, Deep Inspection and\nPerception with RL (DIP-R1) designed to enhance the visual perception\ncapabilities of MLLMs, by comprehending complex scenes and looking through\nvisual instances closely. DIP-R1 guides MLLMs through detailed inspection of\nvisual scene via three simply designed rule-based reward modelings. First, we\nadopt a standard reasoning reward encouraging the model to include three\nstep-by-step processes: 1) reasoning for understanding visual scenes, 2)\nobserving for looking through interested but ambiguous regions, and 3)\ndecision-making for predicting answer. Second, a variance-guided looking reward\nis designed to examine uncertain regions for the second observing process. It\nexplicitly enables the model to inspect ambiguous areas, improving its ability\nto mitigate perceptual uncertainties. Third, we model a weighted\nprecision-recall accuracy reward enhancing accurate decision-making. We explore\nits effectiveness across diverse fine-grained object detection data consisting\nof challenging real-world environments, such as densely crowded scenes. Built\nupon existing MLLMs, DIP-R1 achieves consistent and significant improvement\nacross various in-domain and out-of-domain scenarios. It also outperforms\nvarious existing baseline models and supervised fine-tuning methods. Our\nfindings highlight the substantial potential of integrating RL into MLLMs for\nenhancing capabilities in complex real-world perception tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6DIP-R1\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\uff08\u5982\u5bc6\u96c6\u4eba\u7fa4\uff09\u4e2d\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u4ecd\u6709\u9650\u3002\u53d7\u5f3a\u5316\u5b66\u4e60\u5728LLMs\u548cMLLMs\u4e2d\u7684\u6210\u529f\u542f\u53d1\uff0c\u4f5c\u8005\u63a2\u7d22\u5982\u4f55\u5229\u7528RL\u63d0\u5347MLLMs\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51faDIP-R1\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u89c4\u5219\u5316\u5956\u52b1\u6a21\u578b\u589e\u5f3aMLLMs\u7684\u89c6\u89c9\u611f\u77e5\uff1a1\uff09\u6807\u51c6\u63a8\u7406\u5956\u52b1\uff0c2\uff09\u65b9\u5dee\u5f15\u5bfc\u89c2\u5bdf\u5956\u52b1\uff0c3\uff09\u52a0\u6743\u7cbe\u786e\u53ec\u56de\u5956\u52b1\u3002", "result": "DIP-R1\u5728\u591a\u79cd\u7ec6\u7c92\u5ea6\u76ee\u6807\u68c0\u6d4b\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u548c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06RL\u96c6\u6210\u5230MLLMs\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u63d0\u5347\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2505.23140", "pdf": "https://arxiv.org/pdf/2505.23140", "abs": "https://arxiv.org/abs/2505.23140", "authors": ["Qiuyu Ding", "Zhiqiang Cao", "Hailong Cao", "Tiejun Zhao"], "title": "Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have demonstrated exceptional performance across\nmultiple crosslingual NLP tasks, including machine translation (MT). However,\npersistent challenges remain in addressing context-sensitive units (CSUs), such\nas polysemous words. These CSUs not only affect the local translation accuracy\nof LLMs, but also affect LLMs' understanding capability for sentences and\ntasks, and even lead to translation failure. To address this problem, we\npropose a simple but effective method to enhance LLMs' MT capabilities by\nacquiring CSUs and applying semantic focus. Specifically, we dynamically\nanalyze and identify translation challenges, then incorporate them into LLMs in\na structured manner to mitigate mistranslations or misunderstandings of CSUs\ncaused by information flattening. Efficiently activate LLMs to identify and\napply relevant knowledge from its vast data pool in this way, ensuring more\naccurate translations for translating difficult terms. On a benchmark dataset\nof MT, our proposed method achieved competitive performance compared to\nmultiple existing open-sourced MT baseline models. It demonstrates\neffectiveness and robustness across multiple language pairs, including both\nsimilar language pairs and distant language pairs. Notably, the proposed method\nrequires no additional model training and enhances LLMs' performance across\nmultiple NLP tasks with minimal resource consumption.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u4e0a\u4e0b\u6587\u654f\u611f\u5355\u5143\uff08CSUs\uff09\u5e76\u5e94\u7528\u8bed\u4e49\u805a\u7126\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u654f\u611f\u5355\u5143\uff08\u5982\u591a\u4e49\u8bcd\uff09\u65f6\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u5f71\u54cd\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u6a21\u578b\u7406\u89e3\u80fd\u529b\u3002", "method": "\u52a8\u6001\u5206\u6790\u5e76\u8bc6\u522b\u7ffb\u8bd1\u96be\u70b9\uff0c\u4ee5\u7ed3\u6784\u5316\u65b9\u5f0f\u5c06\u5176\u878d\u5165LLMs\uff0c\u907f\u514d\u4fe1\u606f\u6241\u5e73\u5316\u5bfc\u81f4\u7684\u8bef\u8bd1\u6216\u8bef\u89e3\uff0c\u6fc0\u6d3b\u6a21\u578b\u76f8\u5173\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u591a\u4e2a\u5f00\u6e90\u57fa\u7ebf\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u76f8\u4f3c\u548c\u8fdc\u8ddd\u79bb\u8bed\u8a00\u5bf9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u8d44\u6e90\u6d88\u8017\u4f4e\uff0c\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.23180", "pdf": "https://arxiv.org/pdf/2505.23180", "abs": "https://arxiv.org/abs/2505.23180", "authors": ["Ping Wang", "Lishun Wang", "Gang Qu", "Xiaodong Wang", "Yulun Zhang", "Xin Yuan"], "title": "Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto\nstandard solvers for single-pixel imaging (SPI) inverse problem. PnP\napproaches, a class of iterative algorithms where regularization is implicitly\nperformed by an off-the-shelf deep denoiser, are flexible for varying\ncompression ratios (CRs) but are limited in reconstruction accuracy and speed.\nConversely, unrolling approaches, a class of multi-stage neural networks where\na truncated iterative optimization process is transformed into an end-to-end\ntrainable network, typically achieve better accuracy with faster inference but\nrequire fine-tuning or even retraining when CR changes. In this paper, we\naddress the challenge of integrating the strengths of both classes of solvers.\nTo this end, we design an efficient deep image restorer (DIR) for the unrolling\nof HQS (half quadratic splitting) and ADMM (alternating direction method of\nmultipliers). More importantly, a general proximal trajectory (PT) loss\nfunction is proposed to train HQS/ADMM-unrolling networks such that learned DIR\napproximates the proximal operator of an ideal explicit restoration\nregularizer. Extensive experiments demonstrate that, the resulting proximal\nunrolling networks can not only flexibly handle varying CRs with a single model\nlike PnP algorithms, but also outperform previous CR-specific unrolling\nnetworks in both reconstruction accuracy and speed. Source codes and models are\navailable at https://github.com/pwangcs/ProxUnroll.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5c55\u5f00\uff08unrolling\uff09\u548c\u5373\u63d2\u5373\u7528\uff08PnP\uff09\u65b9\u6cd5\u7684\u5355\u50cf\u7d20\u6210\u50cf\uff08SPI\uff09\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9ad8\u6548\u7684\u6df1\u5ea6\u56fe\u50cf\u6062\u590d\u5668\uff08DIR\uff09\u548c\u63d0\u51fa\u901a\u7528\u7684\u8fd1\u7aef\u8f68\u8ff9\uff08PT\uff09\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5728\u53ef\u53d8\u538b\u7f29\u6bd4\uff08CR\uff09\u4e0b\u7684\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u91cd\u5efa\u3002", "motivation": "PnP\u65b9\u6cd5\u867d\u7136\u7075\u6d3b\u4f46\u7cbe\u5ea6\u548c\u901f\u5ea6\u6709\u9650\uff0c\u800c\u5c55\u5f00\u65b9\u6cd5\u867d\u7cbe\u5ea6\u9ad8\u4f46\u9700\u9488\u5bf9\u4e0d\u540cCR\u8c03\u6574\u3002\u8bba\u6587\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7684DIR\u7528\u4e8e\u5c55\u5f00HQS\u548cADMM\uff0c\u5e76\u63d0\u51faPT\u635f\u5931\u51fd\u6570\u8bad\u7ec3\u7f51\u7edc\uff0c\u4f7fDIR\u903c\u8fd1\u7406\u60f3\u663e\u5f0f\u6062\u590d\u6b63\u5219\u5316\u7684\u8fd1\u7aef\u7b97\u5b50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u7075\u6d3b\u5904\u7406\u4e0d\u540cCR\uff0c\u8fd8\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4ee5\u5f80CR\u7279\u5b9a\u7684\u5c55\u5f00\u7f51\u7edc\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u6574\u5408\u4e86PnP\u548c\u5c55\u5f00\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4e3aSPI\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23146", "pdf": "https://arxiv.org/pdf/2505.23146", "abs": "https://arxiv.org/abs/2505.23146", "authors": ["Qiuyu Ding", "Zhiqiang Cao", "Hailong Cao", "Tiejun Zhao"], "title": "Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Bilingual Lexicon Induction (BLI) is generally based on common domain data to\nobtain monolingual word embedding, and by aligning the monolingual word\nembeddings to obtain the cross-lingual embeddings which are used to get the\nword translation pairs. In this paper, we propose a new task of BLI, which is\nto use the monolingual corpus of the general domain and target domain to\nextract domain-specific bilingual dictionaries. Motivated by the ability of\nPre-trained models, we propose a method to get better word embeddings that\nbuild on the recent work on BLI. This way, we introduce the Code Switch(Qin et\nal., 2020) firstly in the cross-domain BLI task, which can match differit is\nyet to be seen whether these methods are suitable for bilingual lexicon\nextraction in professional fields. As we can see in table 1, the classic and\nefficient BLI approach, Muse and Vecmap, perform much worse on the Medical\ndataset than on the Wiki dataset. On one hand, the specialized domain data set\nis relatively smaller compared to the generic domain data set generally, and\nspecialized words have a lower frequency, which will directly affect the\ntranslation quality of bilingual dictionaries. On the other hand, static word\nembeddings are widely used for BLI, however, in some specific fields, the\nmeaning of words is greatly influenced by context, in this case, using only\nstatic word embeddings may lead to greater bias. ent strategies in different\ncontexts, making the model more suitable for this task. Experimental results\nshow that our method can improve performances over robust BLI baselines on\nthree specific domains by averagely improving 0.78 points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u5355\u8bed\u8bed\u6599\u5e93\u7684\u8de8\u9886\u57df\u53cc\u8bed\u8bcd\u5178\u63d0\u53d6\u4efb\u52a1\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6539\u8fdb\u8bcd\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u8bed\u8bcd\u5178\u5f52\u7eb3\uff08BLI\uff09\u65b9\u6cd5\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u9886\u57df\u6570\u636e\u89c4\u6a21\u5c0f\u3001\u8bcd\u9891\u4f4e\u4ee5\u53ca\u9759\u6001\u8bcd\u5d4c\u5165\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u6539\u8fdb\u8bcd\u5d4c\u5165\uff0c\u5e76\u9996\u6b21\u5728\u8de8\u9886\u57dfBLI\u4efb\u52a1\u4e2d\u5f15\u5165Code Switch\u7b56\u7565\uff0c\u4ee5\u9002\u914d\u4e0d\u540c\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u7279\u5b9a\u9886\u57df\u4e0a\u5e73\u5747\u63d0\u53470.78\u5206\uff0c\u4f18\u4e8e\u4f20\u7edfBLI\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e13\u4e1a\u9886\u57df\u7684\u53cc\u8bed\u8bcd\u5178\u63d0\u53d6\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u666e\u9002\u6027\u3002"}}
{"id": "2505.23186", "pdf": "https://arxiv.org/pdf/2505.23186", "abs": "https://arxiv.org/abs/2505.23186", "authors": ["Junyi Guo", "Jingxuan Zhang", "Fangyu Wu", "Huanda Lu", "Qiufeng Wang", "Wenmian Yang", "Eng Gee Lim", "Dongming Lu"], "title": "HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based garment synthesis tasks primarily focus on the design phase\nin the fashion domain, while the garment production process remains largely\nunderexplored. To bridge this gap, we introduce a new task: Flat Sketch to\nRealistic Garment Image (FS2RG), which generates realistic garment images by\nintegrating flat sketches and textual guidance. FS2RG presents two key\nchallenges: 1) fabric characteristics are solely guided by textual prompts,\nproviding insufficient visual supervision for diffusion-based models, which\nlimits their ability to capture fine-grained fabric details; 2) flat sketches\nand textual guidance may provide conflicting information, requiring the model\nto selectively preserve or modify garment attributes while maintaining\nstructural coherence. To tackle this task, we propose HiGarment, a novel\nframework that comprises two core components: i) a multi-modal semantic\nenhancement mechanism that enhances fabric representation across textual and\nvisual modalities, and ii) a harmonized cross-attention mechanism that\ndynamically balances information from flat sketches and text prompts, allowing\ncontrollable synthesis by generating either sketch-aligned (image-biased) or\ntext-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed\nGarment, the largest open-source dataset for garment generation. Experimental\nresults and user studies demonstrate the effectiveness of HiGarment in garment\nsynthesis. The code and dataset will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u4efb\u52a1FS2RG\uff0c\u7ed3\u5408\u5e73\u9762\u8349\u56fe\u548c\u6587\u672c\u751f\u6210\u903c\u771f\u670d\u88c5\u56fe\u50cf\uff0c\u5e76\u89e3\u51b3\u591a\u6a21\u6001\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u670d\u88c5\u5408\u6210\u4efb\u52a1\u591a\u5173\u6ce8\u8bbe\u8ba1\u9636\u6bb5\uff0c\u751f\u4ea7\u8fc7\u7a0b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faHiGarment\u6846\u67b6\uff0c\u5305\u542b\u591a\u6a21\u6001\u8bed\u4e49\u589e\u5f3a\u548c\u534f\u8c03\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8bc1\u660eHiGarment\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u6700\u5927\u5f00\u6e90\u6570\u636e\u96c6\u3002", "conclusion": "HiGarment\u6210\u529f\u89e3\u51b3FS2RG\u4efb\u52a1\uff0c\u4e3a\u670d\u88c5\u751f\u6210\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2505.23166", "pdf": "https://arxiv.org/pdf/2505.23166", "abs": "https://arxiv.org/abs/2505.23166", "authors": ["Li Lucy", "Camilla Griffiths", "Sarah Levine", "Jennifer L. Eberhardt", "Dorottya Demszky", "David Bamman"], "title": "Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes", "categories": ["cs.CL"], "comment": "26 pages, 7 figures, Findings of ACL 2025", "summary": "Conventional bag-of-words approaches for topic modeling, like latent\nDirichlet allocation (LDA), struggle with literary text. Literature challenges\nlexical methods because narrative language focuses on immersive sensory details\ninstead of abstractive description or exposition: writers are advised to \"show,\ndon't tell.\" We propose Retell, a simple, accessible topic modeling approach\nfor literature. Here, we prompt resource-efficient, generative language models\n(LMs) to tell what passages show, thereby translating narratives' surface forms\ninto higher-level concepts and themes. By running LDA on LMs' retellings of\npassages, we can obtain more precise and informative topics than by running LDA\nalone or by directly asking LMs to list topics. To investigate the potential of\nour method for cultural analytics, we compare our method's outputs to\nexpert-guided annotations in a case study on racial/cultural identity in high\nschool English language arts books.", "AI": {"tldr": "Retell\u662f\u4e00\u79cd\u9488\u5bf9\u6587\u5b66\u6587\u672c\u7684\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5c06\u53d9\u4e8b\u5185\u5bb9\u8f6c\u5316\u4e3a\u9ad8\u7ea7\u6982\u5ff5\uff0c\u518d\u7ed3\u5408LDA\u63d0\u5347\u4e3b\u9898\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8bcd\u888b\u65b9\u6cd5\uff08\u5982LDA\uff09\u96be\u4ee5\u5904\u7406\u6587\u5b66\u6587\u672c\uff0c\u56e0\u5176\u6ce8\u91cd\u611f\u5b98\u7ec6\u8282\u800c\u975e\u62bd\u8c61\u63cf\u8ff0\u3002Retell\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u751f\u6210\u8bed\u8a00\u6a21\u578b\u5c06\u53d9\u4e8b\u5185\u5bb9\u8f6c\u5316\u4e3a\u9ad8\u7ea7\u6982\u5ff5\uff0c\u518d\u5bf9\u5176\u8f93\u51fa\u8fd0\u884cLDA\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "result": "Retell\u751f\u6210\u7684\u4e3b\u9898\u6bd4\u5355\u72ec\u4f7f\u7528LDA\u6216\u76f4\u63a5\u8ba9\u8bed\u8a00\u6a21\u578b\u5217\u51fa\u4e3b\u9898\u66f4\u7cbe\u786e\u4e14\u4fe1\u606f\u4e30\u5bcc\u3002", "conclusion": "Retell\u5728\u6587\u5316\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u66f4\u6709\u6548\u5730\u63d0\u53d6\u6587\u5b66\u6587\u672c\u7684\u4e3b\u9898\u3002"}}
{"id": "2505.23192", "pdf": "https://arxiv.org/pdf/2505.23192", "abs": "https://arxiv.org/abs/2505.23192", "authors": ["Run Hao", "Peng Ying"], "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "9 pages", "summary": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u6811\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89c4\u907fAIGC\u68c0\u6d4b\u5668\uff0c\u5e76\u5728\u7ade\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u903c\u771f\u8096\u50cf\u5f15\u53d1\u7684\u8eab\u4efd\u6ee5\u7528\u95ee\u9898\uff0c\u6d4b\u8bd5AIGC\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u8bed\u6cd5\u6811\u7ed3\u6784\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u53d8\u4f53\uff0c\u7cfb\u7edf\u63a2\u7d22\u8bed\u4e49\u63d0\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u63a7\u7684\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2aT2I\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u7ade\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u540c\u65f6\u53ef\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u5bf9\u6297\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u653b\u51fb\u6027\u5f3a\uff0c\u8fd8\u80fd\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30\u66f4\u9c81\u68d2\u7684AIGC\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2505.23170", "pdf": "https://arxiv.org/pdf/2505.23170", "abs": "https://arxiv.org/abs/2505.23170", "authors": ["Jian Zhu", "Farhan Samir", "Eleanor Chodroff", "David R. Mortensen"], "title": "ZIPA: A family of efficient models for multilingual phone recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "ACL 2025 Main", "summary": "We present ZIPA, a family of efficient speech models that advances the\nstate-of-the-art performance of crosslinguistic phone recognition. We first\ncurated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours\nof normalized phone transcriptions and a novel evaluation set capturing unseen\nlanguages and sociophonetic variation. With the large-scale training data,\nZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage\nthe efficient Zipformer backbones and outperform existing phone recognition\nsystems with much fewer parameters. Further scaling via noisy student training\non 11,000 hours of pseudo-labeled multilingual data yields further improvement.\nWhile ZIPA achieves strong performance on benchmarks, error analysis reveals\npersistent limitations in modeling sociophonetic diversity, underscoring\nchallenges for future research.", "AI": {"tldr": "ZIPA\u662f\u4e00\u7cfb\u5217\u9ad8\u6548\u8bed\u97f3\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u548c\u9ad8\u6548Zipformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u6027\u80fd\uff0c\u4f46\u4ecd\u5b58\u5728\u793e\u4f1a\u8bed\u97f3\u591a\u6837\u6027\u5efa\u6a21\u7684\u6311\u6218\u3002", "motivation": "\u63d0\u5347\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u53c2\u6570\u6548\u7387\u548c\u6570\u636e\u89c4\u6a21\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528IPAPack++\u6570\u636e\u96c6\uff0817,132\u5c0f\u65f6\u6807\u51c6\u5316\u97f3\u7d20\u8f6c\u5f55\uff09\uff0c\u7ed3\u5408Zipformer\u67b6\u6784\uff08ZIPA-T\u548cZIPA-CR\u53d8\u4f53\uff09\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u5b66\u751f\u8bad\u7ec3\u8fdb\u4e00\u6b65\u6269\u5c55\u6570\u636e\u89c4\u6a21\uff0811,000\u5c0f\u65f6\u4f2a\u6807\u7b7e\u6570\u636e\uff09\u3002", "result": "ZIPA\u5728\u97f3\u7d20\u8bc6\u522b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\uff1b\u566a\u58f0\u5b66\u751f\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "ZIPA\u5728\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u793e\u4f1a\u8bed\u97f3\u591a\u6837\u6027\u5efa\u6a21\u4ecd\u662f\u672a\u6765\u7814\u7a76\u7684\u6311\u6218\u3002"}}
{"id": "2505.23193", "pdf": "https://arxiv.org/pdf/2505.23193", "abs": "https://arxiv.org/abs/2505.23193", "authors": ["Sungjune Park", "Hyunjun Kim", "Beomchan Park", "Yong Man Ro"], "title": "Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advancements in computer vision research, object detection in\naerial images still suffers from several challenges. One primary challenge to\nbe mitigated is the presence of multiple types of variation in aerial images,\nfor example, illumination and viewpoint changes. These variations result in\nhighly diverse image scenes and drastic alterations in object appearance, so\nthat it becomes more complicated to localize objects from the whole image scene\nand recognize their categories. To address this problem, in this paper, we\nintroduce a novel object detection framework in aerial images, named\nLANGuage-guided Object detection (LANGO). Upon the proposed language-guided\nlearning, the proposed framework is designed to alleviate the impacts from both\nscene and instance-level variations. First, we are motivated by the way humans\nunderstand the semantics of scenes while perceiving environmental factors in\nthe scenes (e.g., weather). Therefore, we design a visual semantic reasoner\nthat comprehends visual semantics of image scenes by interpreting conditions\nwhere the given images were captured. Second, we devise a training objective,\nnamed relation learning loss, to deal with instance-level variations, such as\nviewpoint angle and scale changes. This training objective aims to learn\nrelations in language representations of object categories, with the help of\nthe robust characteristics against such variations. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed method, and our\nmethod obtains noticeable detection performance improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLANGO\u7684\u8bed\u8a00\u5f15\u5bfc\u7269\u4f53\u68c0\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u822a\u62cd\u56fe\u50cf\u4e2d\u56e0\u5149\u7167\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u7269\u4f53\u68c0\u6d4b\u6311\u6218\u3002", "motivation": "\u822a\u62cd\u56fe\u50cf\u4e2d\u5b58\u5728\u591a\u79cd\u53d8\u5316\uff08\u5982\u5149\u7167\u548c\u89c6\u89d2\uff09\uff0c\u5bfc\u81f4\u7269\u4f53\u5b9a\u4f4d\u548c\u8bc6\u522b\u590d\u6742\u5316\u3002\u53d7\u4eba\u7c7b\u7406\u89e3\u573a\u666f\u8bed\u4e49\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u5b66\u4e60\u7f13\u89e3\u8fd9\u4e9b\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u89c6\u89c9\u8bed\u4e49\u63a8\u7406\u5668\u4ee5\u7406\u89e3\u56fe\u50cf\u573a\u666f\u7684\u89c6\u89c9\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u5173\u7cfb\u5b66\u4e60\u635f\u5931\u6765\u5904\u7406\u5b9e\u4f8b\u7ea7\u53d8\u5316\uff08\u5982\u89c6\u89d2\u548c\u5c3a\u5ea6\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "LANGO\u6846\u67b6\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u5b66\u4e60\u6709\u6548\u7f13\u89e3\u4e86\u822a\u62cd\u56fe\u50cf\u4e2d\u7684\u573a\u666f\u548c\u5b9e\u4f8b\u7ea7\u53d8\u5316\u95ee\u9898\u3002"}}
{"id": "2505.23174", "pdf": "https://arxiv.org/pdf/2505.23174", "abs": "https://arxiv.org/abs/2505.23174", "authors": ["Naman Ahuja", "Fenil Bardoliya", "Chitta Baral", "Vivek Gupta"], "title": "Map&Make: Schema Guided Text to Table Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Transforming dense, detailed, unstructured text into an interpretable and\nsummarised table, also colloquially known as Text-to-Table generation, is an\nessential task for information retrieval. Current methods, however, miss out on\nhow and what complex information to extract; they also lack the ability to\ninfer data from the text. In this paper, we introduce a versatile approach,\nMap&Make, which \"dissects\" text into propositional atomic statements. This\nfacilitates granular decomposition to extract the latent schema. The schema is\nthen used to populate the tables that capture the qualitative nuances and the\nquantitative facts in the original text. Our approach is tested against two\nchallenging datasets, Rotowire, renowned for its complex and multi-table\nschema, and Livesum, which demands numerical aggregation. By carefully\nidentifying and correcting hallucination errors in Rotowire, we aim to achieve\na cleaner and more reliable benchmark. We evaluate our method rigorously on a\ncomprehensive suite of comparative and referenceless metrics. Our findings\ndemonstrate significant improvement results across both datasets with better\ninterpretability in Text-to-Table generation. Moreover, through detailed\nablation studies and analyses, we investigate the factors contributing to\nsuperior performance and validate the practicality of our framework in\nstructured summarization tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMap&Make\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u590d\u6742\u6587\u672c\u5206\u89e3\u4e3a\u539f\u5b50\u547d\u9898\u5e76\u751f\u6210\u8868\u683c\uff0c\u663e\u8457\u63d0\u5347\u4e86Text-to-Table\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u63d0\u53d6\u590d\u6742\u4fe1\u606f\u548c\u63a8\u65ad\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u5f0f\u6765\u5b9e\u73b0\u6587\u672c\u5230\u8868\u683c\u7684\u8f6c\u6362\u3002", "method": "Map&Make\u65b9\u6cd5\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u539f\u5b50\u547d\u9898\uff0c\u63d0\u53d6\u6f5c\u5728\u6a21\u5f0f\u5e76\u586b\u5145\u8868\u683c\uff0c\u540c\u65f6\u7ea0\u6b63\u5e7b\u89c9\u9519\u8bef\u3002", "result": "\u5728Rotowire\u548cLivesum\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "Map&Make\u6846\u67b6\u5728\u7ed3\u6784\u5316\u6458\u8981\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.23201", "pdf": "https://arxiv.org/pdf/2505.23201", "abs": "https://arxiv.org/abs/2505.23201", "authors": ["Hao Wu", "Junzhou Chen", "Ronghui Zhang", "Nengchao Lyu", "Hongyu Hu", "Yanyong Guo", "Tony Z. Qiu"], "title": "WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems", "categories": ["cs.CV"], "comment": "This paper is expected to be submitted to IEEE Transactions on\n  Instrumentation and Measurement", "summary": "Object detection is a cornerstone of environmental perception in advanced\ndriver assistance systems(ADAS). However, most existing methods rely on RGB\ncameras, which suffer from significant performance degradation under low-light\nconditions due to poor image quality. To address this challenge, we proposes\nWTEFNet, a real-time object detection framework specifically designed for\nlow-light scenarios, with strong adaptability to mainstream detectors. WTEFNet\ncomprises three core modules: a Low-Light Enhancement (LLE) module, a\nWavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection\n(AFFD) module. The LLE enhances dark regions while suppressing overexposed\nareas; the WFE applies multi-level discrete wavelet transforms to isolate high-\nand low-frequency components, enabling effective denoising and structural\nfeature retention; the AFFD fuses semantic and illumination features for robust\ndetection. To support training and evaluation, we introduce GSN, a manually\nannotated dataset covering both clear and rainy night-time scenes. Extensive\nexperiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet\nachieves state-of-the-art accuracy under low-light conditions. Furthermore,\ndeployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the\nframework's suitability for real-time ADAS applications.", "AI": {"tldr": "WTEFNet\u662f\u4e00\u4e2a\u4e13\u4e3a\u4f4e\u5149\u573a\u666f\u8bbe\u8ba1\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u5149\u589e\u5f3a\u3001\u5c0f\u6ce2\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u878d\u5408\u68c0\u6d4b\u6a21\u5757\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RGB\u6444\u50cf\u5934\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u5347ADAS\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "WTEFNet\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4f4e\u5149\u589e\u5f3a\uff08LLE\uff09\u3001\u5c0f\u6ce2\u7279\u5f81\u63d0\u53d6\uff08WFE\uff09\u548c\u81ea\u9002\u5e94\u878d\u5408\u68c0\u6d4b\uff08AFFD\uff09\uff0c\u5e76\u5f15\u5165GSN\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728BDD100K\u3001SHIFT\u3001nuScenes\u548cGSN\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u4f4e\u5149\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u4e14\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u5e73\u53f0\u7684\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "WTEFNet\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u5b9e\u65f6ADAS\u5e94\u7528\u3002"}}
{"id": "2505.23177", "pdf": "https://arxiv.org/pdf/2505.23177", "abs": "https://arxiv.org/abs/2505.23177", "authors": ["Wenjing Xing", "Wenke Lu", "Yeheng Duan", "Bing Zhao", "Zhenghui kang", "Yaolong Wang", "Kai Gao", "Lei Qiao"], "title": "Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification", "categories": ["cs.CL"], "comment": null, "summary": "Traditional code instruction data synthesis methods suffer from limited\ndiversity and poor logic. We introduce Infinite-Instruct, an automated\nframework for synthesizing high-quality question-answer pairs, designed to\nenhance the code generation capabilities of large language models (LLMs). The\nframework focuses on improving the internal logic of synthesized problems and\nthe quality of synthesized code. First, \"Reverse Construction\" transforms code\nsnippets into diverse programming problems. Then, through \"Backfeeding\nConstruction,\" keywords in programming problems are structured into a knowledge\ngraph to reconstruct them into programming problems with stronger internal\nlogic. Finally, a cross-lingual static code analysis pipeline filters invalid\nsamples to ensure data quality. Experiments show that on mainstream code\ngeneration benchmarks, our fine-tuned models achieve an average performance\nimprovement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter\nmodels. Using less than one-tenth of the instruction fine-tuning data, we\nachieved performance comparable to the Qwen-2.5-Coder-Instruct.\nInfinite-Instruct provides a scalable solution for LLM training in programming.\nWe open-source the datasets used in the experiments, including both unfiltered\nversions and filtered versions via static analysis. The data are available at\nhttps://github.com/xingwenjing417/Infinite-Instruct-dataset", "AI": {"tldr": "Infinite-Instruct \u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u95ee\u7b54\u5bf9\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002\u901a\u8fc7\u53cd\u5411\u6784\u9020\u548c\u77e5\u8bc6\u56fe\u8c31\u91cd\u6784\uff0c\u589e\u5f3a\u95ee\u9898\u903b\u8f91\u548c\u4ee3\u7801\u8d28\u91cf\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u548c\u903b\u8f91\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347LLMs\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "method": "\u6846\u67b6\u91c7\u7528\u201c\u53cd\u5411\u6784\u9020\u201d\u5c06\u4ee3\u7801\u7247\u6bb5\u8f6c\u5316\u4e3a\u7f16\u7a0b\u95ee\u9898\uff0c\u901a\u8fc7\u201c\u53cd\u9988\u6784\u9020\u201d\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u91cd\u6784\u95ee\u9898\u903b\u8f91\uff0c\u5e76\u7ed3\u5408\u8de8\u8bed\u8a00\u9759\u6001\u4ee3\u7801\u5206\u6790\u8fc7\u6ee4\u65e0\u6548\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e3b\u6d41\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7B\u548c32B\u53c2\u6570\u6a21\u578b\u7684\u6027\u80fd\u5206\u522b\u63d0\u534721.70%\u548c36.95%\uff0c\u4e14\u4f7f\u7528\u66f4\u5c11\u6570\u636e\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "Infinite-Instruct \u4e3a\u7f16\u7a0b\u9886\u57df\u7684LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u5b9e\u9a8c\u6570\u636e\u96c6\u3002"}}
{"id": "2505.23206", "pdf": "https://arxiv.org/pdf/2505.23206", "abs": "https://arxiv.org/abs/2505.23206", "authors": ["Aldino Rizaldy", "Richard Gloaguen", "Fabian Ewald Fassnacht", "Pedram Ghamisi"], "title": "HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal remote sensing data, including spectral and lidar or\nphotogrammetry, is crucial for achieving satisfactory land-use / land-cover\nclassification results in urban scenes. So far, most studies have been\nconducted in a 2D context. When 3D information is available in the dataset, it\nis typically integrated with the 2D data by rasterizing the 3D data into 2D\nformats. Although this method yields satisfactory classification results, it\nfalls short in fully exploiting the potential of 3D data by restricting the\nmodel's ability to learn 3D spatial features directly from raw point clouds.\nAdditionally, it limits the generation of 3D predictions, as the dimensionality\nof the input data has been reduced. In this study, we propose a fully 3D-based\nmethod that fuses all modalities within the 3D point cloud and employs a\ndedicated dual-branch Transformer model to simultaneously learn geometric and\nspectral features. To enhance the fusion process, we introduce a\ncross-attention-based mechanism that fully operates on 3D points, effectively\nintegrating features from various modalities across multiple scales. The\npurpose of cross-attention is to allow one modality to assess the importance of\nanother by weighing the relevant features. We evaluated our method by comparing\nit against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest\n(DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive\nresults compared to 2D methods and offers more flexibility by providing 3D\npredictions. These predictions can be projected onto 2D maps, a capability that\nis not feasible in reverse. Additionally, we evaluated our method on different\ndatasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion\nContest. Our code will be published here:\nhttps://github.com/aldinorizaldy/hyperpointformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u70b9\u4e91\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u4f7f\u7528\u53cc\u5206\u652fTransformer\u6a21\u578b\u76f4\u63a5\u5b66\u4e60\u51e0\u4f55\u548c\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c063D\u6570\u636e\u964d\u7ef4\u52302D\u5904\u7406\uff0c\u672a\u80fd\u5145\u5206\u5229\u75283D\u6570\u636e\u7684\u6f5c\u529b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5b66\u4e603D\u7a7a\u95f4\u7279\u5f81\u548c\u751f\u62103D\u9884\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u57283D\u70b9\u4e91\u4e2d\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u91c7\u7528\u53cc\u5206\u652fTransformer\u6a21\u578b\u548c\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u76f4\u63a5\u5b66\u4e60\u51e0\u4f55\u4e0e\u5149\u8c31\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c3D\u878d\u5408\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e2D\u65b9\u6cd5\uff0c\u5e76\u80fd\u751f\u62103D\u9884\u6d4b\u3002", "conclusion": "3D\u878d\u5408\u65b9\u6cd5\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\uff0c\u652f\u63013D\u9884\u6d4b\u7684\u751f\u6210\u3002"}}
{"id": "2505.23183", "pdf": "https://arxiv.org/pdf/2505.23183", "abs": "https://arxiv.org/abs/2505.23183", "authors": ["Gabriele Sarti", "Vil\u00e9m Zouhar", "Malvina Nissim", "Arianna Bisazza"], "title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Under review. Code:\n  https://github.com/gsarti/labl/tree/main/examples/unsup_wqe Metrics:\n  https://huggingface.co/datasets/gsarti/unsup_wqe_metrics", "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u9ad8\u6548\u8bc6\u522b\u7ffb\u8bd1\u9519\u8bef\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u591a\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u4ee3\u8bcd\u7ea7\u8d28\u91cf\u8bc4\u4f30\uff08WQE\uff09\u6280\u672f\u6210\u672c\u9ad8\u6602\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u8c03\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6280\u672f\uff0c\u4ece\u7ffb\u8bd1\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u4e2d\u8bc6\u522b\u9519\u8bef\u3002", "result": "\u572812\u79cd\u7ffb\u8bd1\u65b9\u5411\u768414\u4e2a\u6307\u6807\u8bc4\u4f30\u4e2d\uff0c\u65e0\u76d1\u7763\u6307\u6807\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u800c\u76d1\u7763\u65b9\u6cd5\u5728\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u65e0\u76d1\u7763\u65b9\u6cd5\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u57fa\u4e8e\u5355\u6807\u6ce8\u8005\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8106\u5f31\u6027\uff0c\u9700\u6539\u8fdb\u3002"}}
{"id": "2505.23209", "pdf": "https://arxiv.org/pdf/2505.23209", "abs": "https://arxiv.org/abs/2505.23209", "authors": ["Akash Dhasade", "Divyansh Jhunjhunwala", "Milos Vujasinovic", "Gauri Joshi", "Anne-Marie Kermarrec"], "title": "Navigating the Accuracy-Size Trade-Off with Flexible Model Merging", "categories": ["cs.CV"], "comment": null, "summary": "Model merging has emerged as an efficient method to combine multiple\nsingle-task fine-tuned models. The merged model can enjoy multi-task\ncapabilities without expensive training. While promising, merging into a single\nmodel often suffers from an accuracy gap with respect to individual fine-tuned\nmodels. On the other hand, deploying all individual fine-tuned models incurs\nhigh costs. We propose FlexMerge, a novel data-free model merging framework to\nflexibly generate merged models of varying sizes, spanning the spectrum from a\nsingle merged model to retaining all individual fine-tuned models. FlexMerge\ntreats fine-tuned models as collections of sequential blocks and progressively\nmerges them using any existing data-free merging method, halting at a desired\nsize. We systematically explore the accuracy-size trade-off exhibited by\ndifferent merging algorithms in combination with FlexMerge. Extensive\nexperiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even\nmodestly larger merged models can provide substantial accuracy improvements\nover a single model. By offering fine-grained control over fused model size,\nFlexMerge provides a flexible, data-free, and high-performance solution for\ndiverse deployment scenarios.", "AI": {"tldr": "FlexMerge\u662f\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u6570\u636e\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u751f\u6210\u4e0d\u540c\u5927\u5c0f\u7684\u5408\u5e76\u6a21\u578b\uff0c\u5e73\u8861\u7cbe\u5ea6\u4e0e\u90e8\u7f72\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u5355\u4e00\u5408\u5e76\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u4e0e\u90e8\u7f72\u591a\u4e2a\u72ec\u7acb\u6a21\u578b\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5fae\u8c03\u6a21\u578b\u89c6\u4e3a\u987a\u5e8f\u5757\u96c6\u5408\uff0c\u9010\u6b65\u5408\u5e76\uff0c\u652f\u6301\u591a\u79cd\u65e0\u6570\u636e\u5408\u5e76\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5ea6\u589e\u5927\u7684\u5408\u5e76\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u4efb\u52a1\u573a\u666f\u3002", "conclusion": "FlexMerge\u4e3a\u591a\u4efb\u52a1\u90e8\u7f72\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u65e0\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23187", "pdf": "https://arxiv.org/pdf/2505.23187", "abs": "https://arxiv.org/abs/2505.23187", "authors": ["Yilong Li", "Chen Qian", "Yu Xia", "Ruijie Shi", "Yufan Dang", "Zihao Xie", "Ziming You", "Weize Chen", "Cheng Yang", "Weichuan Liu", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large Language Model-based multi-agent systems (MAS) have shown remarkable\nprogress in solving complex tasks through collaborative reasoning and\ninter-agent critique. However, existing approaches typically treat each task in\nisolation, resulting in redundant computations and limited generalization\nacross structurally similar tasks. To address this, we introduce multi-agent\ncross-task experiential learning (MAEL), a novel framework that endows\nLLM-driven agents with explicit cross-task learning and experience\naccumulation. We model the task-solving workflow on a graph-structured\nmulti-agent collaboration network, where agents propagate information and\ncoordinate via explicit connectivity. During the experiential learning phase,\nwe quantify the quality for each step in the task-solving workflow and store\nthe resulting rewards along with the corresponding inputs and outputs into each\nagent's individual experience pool. During inference, agents retrieve\nhigh-reward, task-relevant experiences as few-shot examples to enhance the\neffectiveness of each reasoning step, thereby enabling more accurate and\nefficient multi-agent collaboration. Experimental results on diverse datasets\ndemonstrate that MAEL empowers agents to learn from prior task experiences\neffectively-achieving faster convergence and producing higher-quality solutions\non current tasks.", "AI": {"tldr": "MAEL\u6846\u67b6\u901a\u8fc7\u8de8\u4efb\u52a1\u7ecf\u9a8c\u5b66\u4e60\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u6548\u7387\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u5e76\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b64\u7acb\u5904\u7406\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7f51\u7edc\uff0c\u91cf\u5316\u4efb\u52a1\u89e3\u51b3\u6d41\u7a0b\u4e2d\u7684\u6b65\u9aa4\u8d28\u91cf\uff0c\u5b58\u50a8\u9ad8\u5956\u52b1\u7ecf\u9a8c\u4ee5\u4f9b\u540e\u7eed\u4efb\u52a1\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMAEL\u80fd\u66f4\u5feb\u6536\u655b\u5e76\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "MAEL\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u6548\u7387\u548c\u4efb\u52a1\u89e3\u51b3\u8d28\u91cf\u3002"}}
{"id": "2505.23214", "pdf": "https://arxiv.org/pdf/2505.23214", "abs": "https://arxiv.org/abs/2505.23214", "authors": ["Wenhao Xu", "Shuchen Zheng", "Changwei Wang", "Zherui Zhang", "Chuan Ren", "Rongtao Xu", "Shibiao Xu"], "title": "SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Information Fusion 2025", "summary": "Infrared small target detection (ISTD) is vital for long-range surveillance\nin military, maritime, and early warning applications. ISTD is challenged by\ntargets occupying less than 0.15% of the image and low distinguishability from\ncomplex backgrounds. Existing deep learning methods often suffer from\ninformation loss during downsampling and inefficient global context modeling.\nThis paper presents SAMamba, a novel framework integrating SAM2's hierarchical\nfeature learning with Mamba's selective sequence modeling. Key innovations\ninclude: (1) A Feature Selection Adapter (FS-Adapter) for efficient\nnatural-to-infrared domain adaptation via dual-stage selection (token-level\nwith a learnable task embedding and channel-wise adaptive transformations); (2)\nA Cross-Channel State-Space Interaction (CSI) module for efficient global\ncontext modeling with linear complexity using selective state space modeling;\nand (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively\ncombines multi-scale features with a gating mechanism to balance\nhigh-resolution and low-resolution feature contributions. SAMamba addresses\ncore ISTD challenges by bridging the domain gap, maintaining fine-grained\ndetails, and efficiently modeling long-range dependencies. Experiments on\nNUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly\noutperforms state-of-the-art methods, especially in challenging scenarios with\nheterogeneous backgrounds and varying target scales. Code:\nhttps://github.com/zhengshuchen/SAMamba.", "AI": {"tldr": "SAMamba\u6846\u67b6\u901a\u8fc7\u7ed3\u5408SAM2\u7684\u5206\u5c42\u7279\u5f81\u5b66\u4e60\u548cMamba\u7684\u9009\u62e9\u6027\u5e8f\u5217\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u5728\u519b\u4e8b\u548c\u9884\u8b66\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faSAMamba\u6846\u67b6\uff0c\u5305\u62ecFS-Adapter\u3001CSI\u6a21\u5757\u548cDPCF\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u57df\u9002\u5e94\u3001\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u80cc\u666f\u548c\u591a\u5c3a\u5ea6\u76ee\u6807\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SAMamba\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23191", "pdf": "https://arxiv.org/pdf/2505.23191", "abs": "https://arxiv.org/abs/2505.23191", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bibo Cai", "Bing Qin", "Ting Liu"], "title": "ExpeTrans: LLMs Are Experiential Transfer Learners", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 12 figs/tables", "summary": "Recent studies provide large language models (LLMs) with textual task-solving\nexperiences via prompts to improve their performance. However, previous methods\nrely on substantial human labor or time to gather such experiences for each\ntask, which is impractical given the growing variety of task types in user\nqueries to LLMs. To address this issue, we design an autonomous experience\ntransfer framework to explore whether LLMs can mimic human cognitive\nintelligence to autonomously transfer experience from existing source tasks to\nnewly encountered target tasks. This not only allows the acquisition of\nexperience without extensive costs of previous methods, but also offers a novel\npath for the generalization of LLMs. Experimental results on 13 datasets\ndemonstrate that our framework effectively improves the performance of LLMs.\nFurthermore, we provide a detailed analysis of each module in the framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u7ecf\u9a8c\u8f6c\u79fb\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u4ece\u6e90\u4efb\u52a1\u4e2d\u81ea\u4e3b\u8f6c\u79fb\u7ecf\u9a8c\u5230\u76ee\u6807\u4efb\u52a1\uff0c\u51cf\u5c11\u4eba\u5de5\u548c\u65f6\u95f4\u6210\u672c\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6216\u65f6\u95f4\u6536\u96c6\u4efb\u52a1\u89e3\u51b3\u7ecf\u9a8c\uff0c\u96be\u4ee5\u5e94\u5bf9LLMs\u4efb\u52a1\u7c7b\u578b\u7684\u591a\u6837\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u4e3b\u7ecf\u9a8c\u8f6c\u79fb\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u667a\u80fd\uff0c\u5b9e\u73b0\u7ecf\u9a8c\u7684\u81ea\u4e3b\u8f6c\u79fb\u3002", "result": "\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLMs\u7684\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u5e76\u901a\u8fc7\u8be6\u7ec6\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2505.23248", "pdf": "https://arxiv.org/pdf/2505.23248", "abs": "https://arxiv.org/abs/2505.23248", "authors": ["Yunliang Qi", "Meng Lou", "Yimin Liu", "Lu Li", "Zhen Yang", "Wen Nie"], "title": "Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey", "categories": ["eess.IV", "cs.CV"], "comment": "31 pages,7 figures, an survey", "summary": "Remote sensing image super-resolution (RSISR) is a crucial task in remote\nsensing image processing, aiming to reconstruct high-resolution (HR) images\nfrom their low-resolution (LR) counterparts. Despite the growing number of\nRSISR methods proposed in recent years, a systematic and comprehensive review\nof these methods is still lacking. This paper presents a thorough review of\nRSISR algorithms, covering methodologies, datasets, and evaluation metrics. We\nprovide an in-depth analysis of RSISR methods, categorizing them into\nsupervised, unsupervised, and quality evaluation approaches, to help\nresearchers understand current trends and challenges. Our review also discusses\nthe strengths, limitations, and inherent challenges of these techniques.\nNotably, our analysis reveals significant limitations in existing methods,\nparticularly in preserving fine-grained textures and geometric structures under\nlarge-scale degradation. Based on these findings, we outline future research\ndirections, highlighting the need for domain-specific architectures and robust\nevaluation protocols to bridge the gap between synthetic and real-world RSISR\nscenarios.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08RSISR\uff09\u7684\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6280\u672f\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u8fd1\u5e74\u6765RSISR\u65b9\u6cd5\u4e0d\u65ad\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5c06RSISR\u65b9\u6cd5\u5206\u4e3a\u76d1\u7763\u3001\u65e0\u76d1\u7763\u548c\u8d28\u91cf\u8bc4\u4f30\u4e09\u7c7b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u5c3a\u5ea6\u9000\u5316\u4e0b\u4fdd\u7559\u7ec6\u8282\u7eb9\u7406\u548c\u51e0\u4f55\u7ed3\u6784\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u672a\u6765\u9700\u5f00\u53d1\u9886\u57df\u4e13\u7528\u67b6\u6784\u548c\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u5dee\u8ddd\u3002"}}
{"id": "2505.23224", "pdf": "https://arxiv.org/pdf/2505.23224", "abs": "https://arxiv.org/abs/2505.23224", "authors": ["Zhitao He", "Sandeep Polisetty", "Zhiyuan Fan", "Yuchen Huang", "Shujin Wu", "Yi R.", "Fung"], "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "In recent years, multimodal large language models (MLLMs) have made\nsignificant progress but continue to face inherent challenges in multimodal\nreasoning, which requires multi-level (e.g., perception, reasoning) and\nmulti-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior\nwork on estimating model confidence tends to focus on the overall response for\ntraining and calibration, but fails to assess confidence in each reasoning\nstep, leading to undesirable hallucination snowballing. In this work, we\npresent MMBoundary, a novel framework that advances the knowledge boundary\nawareness of MLLMs through reasoning step confidence calibration. To achieve\nthis, we propose to incorporate complementary textual and cross-modal\nself-rewarding signals to estimate confidence at each step of the MLLM\nreasoning process. In addition to supervised fine-tuning MLLM on this set of\nself-rewarded confidence estimation signal for initial confidence expression\nwarm-up, we introduce a reinforcement learning stage with multiple reward\nfunctions for further aligning model knowledge and calibrating confidence at\neach reasoning step, enhancing reasoning chain self-correction. Empirical\nresults show that MMBoundary significantly outperforms existing methods across\ndiverse domain datasets and metrics, achieving an average of 7.5% reduction in\nmultimodal confidence calibration errors and up to 8.3% improvement in task\nperformance.", "AI": {"tldr": "MMBoundary\u6846\u67b6\u901a\u8fc7\u6821\u51c6\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63a8\u7406\u6b65\u9aa4\u7684\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u5347\u5176\u77e5\u8bc6\u8fb9\u754c\u610f\u8bc6\uff0c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u6a21\u578b\u7f6e\u4fe1\u5ea6\u65f6\uff0c\u4ec5\u5173\u6ce8\u6574\u4f53\u54cd\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u6b65\u9aa4\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u7d2f\u79ef\u3002", "method": "\u63d0\u51faMMBoundary\u6846\u67b6\uff0c\u7ed3\u5408\u6587\u672c\u548c\u8de8\u6a21\u6001\u81ea\u5956\u52b1\u4fe1\u53f7\u6821\u51c6\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMMBoundary\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u51cf\u5c117.5%\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8bef\u5dee\uff0c\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u8fbe8.3%\u3002", "conclusion": "MMBoundary\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u63a8\u7406\u80fd\u529b\u548c\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u3002"}}
{"id": "2505.23253", "pdf": "https://arxiv.org/pdf/2505.23253", "abs": "https://arxiv.org/abs/2505.23253", "authors": ["Yixun Liang", "Kunming Luo", "Xiao Chen", "Rui Chen", "Hongyu Yan", "Weiyu Li", "Jiarui Liu", "Ping Tan"], "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.", "AI": {"tldr": "UniTEX\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u76843D\u7eb9\u7406\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u57283D\u529f\u80fd\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u907f\u514d\u4e86UV\u6620\u5c04\u7684\u9650\u5236\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u76843D\u7eb9\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56UV\u6620\u5c04\u548c\u56fe\u50cf\u91cd\u6295\u5f71\uff0c\u5bfc\u81f4\u62d3\u6251\u6a21\u7cca\u95ee\u9898\u3002UniTEX\u65e8\u5728\u7ed5\u8fc7\u8fd9\u4e9b\u9650\u5236\uff0c\u76f4\u63a5\u57283D\u7a7a\u95f4\u4e2d\u751f\u6210\u7eb9\u7406\u3002", "method": "1. \u4f7f\u7528Texture Functions\uff08TFs\uff09\u5c06\u7eb9\u7406\u751f\u6210\u63d0\u5347\u52303D\u7a7a\u95f4\uff1b2. \u57fa\u4e8eTransformer\u7684\u5927\u89c4\u6a21\u7eb9\u7406\u6a21\u578b\uff08LTM\uff09\u4ece\u56fe\u50cf\u548c\u51e0\u4f55\u8f93\u5165\u9884\u6d4bTFs\uff1b3. \u5229\u7528LoRA\u7b56\u7565\u4f18\u53162D\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u591a\u89c6\u56fe\u7eb9\u7406\u5408\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniTEX\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7eb9\u7406\u5b8c\u6574\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u6269\u5c55\u76843D\u7eb9\u7406\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "UniTEX\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u89e3\u51b3\u4e863D\u7eb9\u7406\u751f\u6210\u4e2d\u7684\u62d3\u6251\u6a21\u7cca\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u53163D\u7eb9\u7406\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6848\u3002"}}
{"id": "2505.23229", "pdf": "https://arxiv.org/pdf/2505.23229", "abs": "https://arxiv.org/abs/2505.23229", "authors": ["Hao Lu", "Yanchi Gu", "Haoyuan Huang", "Yulin Zhou", "Ningxin Zhu", "Chen Li"], "title": "MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "50 pages, 3 figures", "summary": "The integration of Monte Carlo Tree Search (MCTS) with Large Language Models\n(LLMs) has demonstrated significant success in structured, problem-oriented\ntasks. However, applying these methods to open-ended dialogues, such as those\nin psychological counseling, presents unique challenges. Unlike tasks with\nobjective correctness, success in therapeutic conversations depends on\nsubjective factors like empathetic engagement, ethical adherence, and alignment\nwith human preferences, for which strict \"correctness\" criteria are\nill-defined. Existing result-oriented MCTS approaches can therefore produce\nmisaligned responses. To address this, we introduce MCTSr-Zero, an MCTS\nframework designed for open-ended, human-centric dialogues. Its core innovation\nis \"domain alignment\", which shifts the MCTS search objective from predefined\nend-states towards conversational trajectories that conform to target domain\nprinciples (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates\n\"Regeneration\" and \"Meta-Prompt Adaptation\" mechanisms to substantially broaden\nexploration by allowing the MCTS to consider fundamentally different initial\ndialogue strategies. We evaluate MCTSr-Zero in psychological counseling by\ngenerating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.\nWe also introduce PsyEval, a benchmark for assessing multi-turn psychological\ncounseling dialogues. Experiments demonstrate that PsyLLM achieves\nstate-of-the-art performance on PsyEval and other relevant metrics, validating\nMCTSr-Zero's effectiveness in generating high-quality, principle-aligned\nconversational data for human-centric domains and addressing the LLM challenge\nof consistently adhering to complex psychological standards.", "AI": {"tldr": "MCTSr-Zero\u6846\u67b6\u5c06MCTS\u4e0eLLMs\u7ed3\u5408\uff0c\u9488\u5bf9\u5f00\u653e\u5bf9\u8bdd\u4efb\u52a1\uff08\u5982\u5fc3\u7406\u54a8\u8be2\uff09\u8bbe\u8ba1\uff0c\u901a\u8fc7\u9886\u57df\u5bf9\u9f50\u548c\u63a2\u7d22\u673a\u5236\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfMCTS\u65b9\u6cd5\u5728\u5f00\u653e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u53ef\u80fd\u4ea7\u751f\u4e0d\u5339\u914d\u7684\u56de\u5e94\uff0c\u56e0\u5176\u4f9d\u8d56\u5ba2\u89c2\u6b63\u786e\u6027\uff0c\u800c\u5fc3\u7406\u54a8\u8be2\u7b49\u4efb\u52a1\u9700\u4e3b\u89c2\u56e0\u7d20\u5982\u5171\u60c5\u548c\u4f26\u7406\u3002", "method": "\u63d0\u51faMCTSr-Zero\u6846\u67b6\uff0c\u5f15\u5165\u9886\u57df\u5bf9\u9f50\u3001\u518d\u751f\u548c\u5143\u63d0\u793a\u9002\u5e94\u673a\u5236\uff0c\u4f18\u5316\u5bf9\u8bdd\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eMCTSr-Zero\u751f\u6210\u7684\u5bf9\u8bdd\u6570\u636e\u8bad\u7ec3\u7684PsyLLM\u5728PsyEval\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MCTSr-Zero\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u5bf9\u8bdd\u4efb\u52a1\u4e2dLLMs\u7684\u6311\u6218\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7b26\u5408\u5fc3\u7406\u5b66\u6807\u51c6\u7684\u5bf9\u8bdd\u6570\u636e\u3002"}}
{"id": "2505.23265", "pdf": "https://arxiv.org/pdf/2505.23265", "abs": "https://arxiv.org/abs/2505.23265", "authors": ["Zheng Sun", "Yi Wei", "Long Yu"], "title": "Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are of great application across many\ndomains, such as multimodal understanding and generation. With the development\nof diffusion models (DM) and unified MLLMs, the performance of image generation\nhas been significantly improved, however, the study of image screening is rare\nand its performance with MLLMs is unsatisfactory due to the lack of data and\nthe week image aesthetic reasoning ability in MLLMs. In this work, we propose a\ncomplete solution to address these problems in terms of data and methodology.\nFor data, we collect a comprehensive medical image screening dataset with 1500+\nsamples, each sample consists of a medical image, four generated images, and a\nmultiple-choice answer. The dataset evaluates the aesthetic reasoning ability\nunder four aspects: \\textit{(1) Appearance Deformation, (2) Principles of\nPhysical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.\nFor methodology, we utilize long chains of thought (CoT) and Group Relative\nPolicy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,\nto enhance the image aesthetic reasoning ability of MLLMs. Our experimental\nresults reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o\nand Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic\nreasoning. In contrast, by leveraging the reinforcement learning approach, we\nare able to surpass the score of both large-scale models and leading\nclosed-source models using a much smaller model. We hope our attempt on medical\nimage screening will serve as a regular configuration in image aesthetic\nreasoning in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u7b5b\u9009\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u56fe\u50cf\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u56fe\u50cf\u7b5b\u9009\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u6570\u636e\u548c\u6a21\u578b\u7684\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\u8f83\u5f31\u3002", "method": "\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b1500+\u6837\u672c\u7684\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86DPA-GRPO\u65b9\u6cd5\uff08\u7ed3\u5408\u957f\u94fe\u601d\u7ef4\u548c\u52a8\u6001\u6bd4\u4f8b\u51c6\u786e\u5ea6\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u6765\u589e\u5f3aMLLMs\u7684\u7f8e\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u95ed\u6e90MLLMs\uff08\u5982GPT-4o\u548cQwen-VL-Max\uff09\u5728\u56fe\u50cf\u7f8e\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\uff0c\u800c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u66f4\u5c0f\u7684\u6a21\u578b\u8d85\u8d8a\u4e86\u8fd9\u4e9b\u5927\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56fe\u50cf\u7f8e\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u914d\u7f6e\u65b9\u6cd5\uff0c\u5e76\u6709\u671b\u5728\u672a\u6765\u6210\u4e3a\u533b\u5b66\u56fe\u50cf\u7b5b\u9009\u7684\u5e38\u89c4\u65b9\u6848\u3002"}}
{"id": "2505.23242", "pdf": "https://arxiv.org/pdf/2505.23242", "abs": "https://arxiv.org/abs/2505.23242", "authors": ["Jingxuan Wei", "Nan Xu", "Junnan Zhu", "Yanni Hao", "Gaowei Wu", "Bihui Yu", "Lei Wang"], "title": "ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Chart question answering (CQA) has become a critical multimodal task for\nevaluating the reasoning capabilities of vision-language models. While early\napproaches have shown promising performance by focusing on visual features or\nleveraging large-scale pre-training, most existing evaluations rely on rigid\noutput formats and objective metrics, thus ignoring the complex, real-world\ndemands of practical chart analysis. In this paper, we introduce ChartMind, a\nnew benchmark designed for complex CQA tasks in real-world settings. ChartMind\ncovers seven task categories, incorporates multilingual contexts, supports\nopen-domain textual outputs, and accommodates diverse chart formats, bridging\nthe gap between real-world applications and traditional academic benchmarks.\nFurthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,\nthat focuses on extracting key contextual elements, reducing noise, and\nenhancing the reasoning accuracy of multimodal large language models. Extensive\nevaluations on ChartMind and three representative public benchmarks with 14\nmainstream multimodal models show our framework significantly outperforms the\nprevious three common CQA paradigms: instruction-following, OCR-enhanced, and\nchain-of-thought, highlighting the importance of flexible chart understanding\nfor real-world CQA. These findings suggest new directions for developing more\nrobust chart reasoning in future research.", "AI": {"tldr": "ChartMind\u662f\u4e00\u4e2a\u65b0\u7684\u590d\u6742\u56fe\u8868\u95ee\u7b54\uff08CQA\uff09\u57fa\u51c6\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u5f00\u653e\u57df\u8f93\u51fa\uff0c\u586b\u8865\u4e86\u5b9e\u9645\u5e94\u7528\u4e0e\u5b66\u672f\u57fa\u51c6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u63d0\u51fa\u7684ChartLLM\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CQA\u8bc4\u4f30\u8fc7\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u8f93\u51fa\u683c\u5f0f\u548c\u5ba2\u89c2\u6307\u6807\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u56fe\u8868\u5206\u6790\u7684\u590d\u6742\u9700\u6c42\u3002", "method": "\u63d0\u51faChartMind\u57fa\u51c6\u548cChartLLM\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u63d0\u53d6\u5173\u952e\u4e0a\u4e0b\u6587\u5143\u7d20\u3001\u964d\u566a\u548c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728ChartMind\u548c\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cChartLLM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e09\u79cd\u5e38\u89c1CQA\u8303\u5f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7075\u6d3b\u56fe\u8868\u7406\u89e3\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u56fe\u8868\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.23268", "pdf": "https://arxiv.org/pdf/2505.23268", "abs": "https://arxiv.org/abs/2505.23268", "authors": ["Spyros Barbakos", "Charalampos Antoniadis", "Gerasimos Potamianos", "Gianluca Setti"], "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "Video consumption is a key part of daily life, but watching entire videos can\nbe tedious. To address this, researchers have explored video summarization and\nhighlight detection to identify key video segments. While some works combine\nvideo frames and transcripts, and others tackle video summarization and\nhighlight detection using Reinforcement Learning (RL), no existing work, to the\nbest of our knowledge, integrates both modalities within an RL framework. In\nthis paper, we propose a multimodal pipeline that leverages video frames and\ntheir corresponding transcripts to generate a more condensed version of the\nvideo and detect highlights using a modality fusion mechanism. The pipeline is\ntrained within an RL framework, which rewards the model for generating diverse\nand representative summaries while ensuring the inclusion of video segments\nwith meaningful transcript content. The unsupervised nature of the training\nallows for learning from large-scale unannotated datasets, overcoming the\nchallenge posed by the limited size of existing annotated datasets. Our\nexperiments show that using the transcript in video summarization and highlight\ndetection achieves superior results compared to relying solely on the visual\ncontent of the video.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u9891\u5e27\u548c\u6587\u672c\u8f6c\u5f55\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u751f\u6210\u89c6\u9891\u6458\u8981\u548c\u68c0\u6d4b\u4eae\u70b9\uff0c\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u6d88\u8d39\u662f\u65e5\u5e38\u751f\u6d3b\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u89c2\u770b\u5b8c\u6574\u89c6\u9891\u53ef\u80fd\u5f88\u4e4f\u5473\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u5c06\u89c6\u9891\u5e27\u548c\u6587\u672c\u8f6c\u5f55\u7ed3\u5408\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u5229\u7528\u89c6\u9891\u5e27\u548c\u6587\u672c\u8f6c\u5f55\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u548c\u4ee3\u8868\u6027\u7684\u6458\u8981\uff0c\u5e76\u786e\u4fdd\u5305\u542b\u6709\u610f\u4e49\u7684\u8f6c\u5f55\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u6587\u672c\u8f6c\u5f55\u7684\u89c6\u9891\u6458\u8981\u548c\u4eae\u70b9\u68c0\u6d4b\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u89c6\u9891\u6458\u8981\u548c\u4eae\u70b9\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u672a\u6807\u6ce8\u6570\u636e\u96c6\u3002"}}
{"id": "2505.23252", "pdf": "https://arxiv.org/pdf/2505.23252", "abs": "https://arxiv.org/abs/2505.23252", "authors": ["Bing Ma", "Hai Zhuge"], "title": "Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers", "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7ef4\u5ea6\u7684\u79d1\u5b66\u65b9\u6cd5\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u5f0f\u8bc6\u522b\u548c\u6811\u7ed3\u6784\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u65b9\u6cd5\u67e5\u8be2\u548c\u5206\u7c7b\u3002", "motivation": "\u79d1\u5b66\u8bba\u6587\u4e2d\u65b9\u6cd5\u7684\u67e5\u8be2\u548c\u7ba1\u7406\u7f3a\u4e4f\u9ad8\u6548\u6846\u67b6\uff0c\u5bfc\u81f4\u7814\u7a76\u8005\u5728\u67e5\u627e\u548c\u5229\u7528\u76f8\u5173\u65b9\u6cd5\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u3001\u8bed\u7bc7\u3001\u53e5\u6cd5\u548c\u8bcd\u6c47\u56db\u4e2a\u8bed\u8a00\u5c42\u6b21\u8bc6\u522b\u65b9\u6cd5\u6a21\u5f0f\uff0c\u63d0\u53d6\u65b9\u6cd5\u5e76\u5206\u7c7b\u4e3a\u4e94\u4e2a\u7ef4\u5ea6\uff1b\u63d0\u51fa\u6811\u7ed3\u6784\u8868\u793a\u6b65\u9aa4\uff0c\u57fa\u4e8e\u53e5\u6cd5\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u76f8\u4f3c\u6027\uff1b\u91c7\u7528\u81ea\u5e95\u5411\u4e0a\u805a\u7c7b\u7b97\u6cd5\u6784\u5efa\u7c7b\u6811\u3002", "result": "\u6784\u5efa\u7684\u591a\u7ef4\u65b9\u6cd5\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u7684\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u7c7b\u673a\u5236\u5feb\u901f\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\u3002", "conclusion": "\u591a\u7ef4\u65b9\u6cd5\u7a7a\u95f4\u6846\u67b6\u4e3a\u79d1\u5b66\u65b9\u6cd5\u7684\u9ad8\u6548\u67e5\u8be2\u548c\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23271", "pdf": "https://arxiv.org/pdf/2505.23271", "abs": "https://arxiv.org/abs/2505.23271", "authors": ["Mao-Lin Luo", "Zi-Hao Zhou", "Tong Wei", "Min-Ling Zhang"], "title": "LADA: Scalable Label-Specific CLIP Adapter for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Continual learning with vision-language models like CLIP offers a pathway\ntoward scalable machine learning systems by leveraging its transferable\nrepresentations. Existing CLIP-based methods adapt the pre-trained image\nencoder by adding multiple sets of learnable parameters, with each task using a\npartial set of parameters. This requires selecting the expected parameters for\ninput images during inference, which is prone to error that degrades\nperformance. To address this problem, we introduce LADA (Label-specific\nADApter). Instead of partitioning parameters across tasks, LADA appends\nlightweight, label-specific memory units to the frozen CLIP image encoder,\nenabling discriminative feature generation by aggregating task-agnostic\nknowledge. To prevent catastrophic forgetting, LADA employs feature\ndistillation for seen classes, preventing their features from being interfered\nwith by new classes. Positioned after the image encoder, LADA prevents gradient\nflow to the frozen CLIP parameters, ensuring efficient training. Extensive\nresults show that LADA achieves state-of-the-art performance in continual\nlearning settings. The implementation code is available at\nhttps://github.com/MaolinLuo/LADA.", "AI": {"tldr": "LADA\u901a\u8fc7\u4e3a\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u6807\u7b7e\u7279\u5b9a\u5185\u5b58\u5355\u5143\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CLIP\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CLIP\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9700\u8981\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u9009\u62e9\u90e8\u5206\u53c2\u6570\uff0c\u5bb9\u6613\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "LADA\u5728\u51bb\u7ed3\u7684CLIP\u56fe\u50cf\u7f16\u7801\u5668\u540e\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u6807\u7b7e\u7279\u5b9a\u5185\u5b58\u5355\u5143\uff0c\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "LADA\u5728\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "LADA\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u548c\u7279\u5f81\u84b8\u998f\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\u3002"}}
{"id": "2505.23276", "pdf": "https://arxiv.org/pdf/2505.23276", "abs": "https://arxiv.org/abs/2505.23276", "authors": ["Maged S. Al-Shaibani", "Moataz Ahmed"], "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5168\u9762\u7814\u7a76\u4e86\u963f\u62c9\u4f2f\u8bed\u673a\u5668\u751f\u6210\u6587\u672c\uff0c\u901a\u8fc7\u591a\u79cd\u751f\u6210\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\uff0c\u63ed\u793a\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u72ec\u7279\u8bed\u8a00\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684BERT\u68c0\u6d4b\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u7c7b\u4eba\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u6784\u6210\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "method": "\u7814\u7a76\u91c7\u7528\u591a\u79cd\u751f\u6210\u7b56\u7565\uff08\u6807\u9898\u751f\u6210\u3001\u5185\u5bb9\u611f\u77e5\u751f\u6210\u548c\u6587\u672c\u4f18\u5316\uff09\u548c\u6a21\u578b\u67b6\u6784\uff08ALLaM\u3001Jais\u3001Llama\u3001GPT-4\uff09\uff0c\u7ed3\u5408\u98ce\u683c\u8ba1\u91cf\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u673a\u5668\u751f\u6210\u7684\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u5177\u6709\u53ef\u68c0\u6d4b\u7684\u7279\u5f81\uff0cBERT\u68c0\u6d4b\u6a21\u578b\u5728\u6b63\u5f0f\u8bed\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff08F1\u5206\u6570\u9ad8\u8fbe99.9%\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684\u9c81\u68d2\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5bf9\u7ef4\u62a4\u4fe1\u606f\u5b8c\u6574\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2505.23272", "pdf": "https://arxiv.org/pdf/2505.23272", "abs": "https://arxiv.org/abs/2505.23272", "authors": ["Yazhou Zhang", "Chunwang Zou", "Qimeng Liu", "Lu Rong", "Ben Yao", "Zheng Lian", "Qiuchi Li", "Peng Zhang", "Jing Qin"], "title": "Are MLMs Trapped in the Visual Room?", "categories": ["cs.CV"], "comment": null, "summary": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on\nperception tasks; (2) even with correct perception, models exhibit an average\nerror rate of ~16.1\\% in sarcasm understanding, revealing a significant gap\nbetween seeing and understanding; (3) error analysis attributes this gap to\ndeficiencies in emotional reasoning, commonsense inference, and context\nalignment. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLMs\uff09\u662f\u5426\u80fd\u771f\u6b63\u201c\u7406\u89e3\u201d\u56fe\u50cf\uff0c\u63d0\u51fa\u201c\u89c6\u89c9\u623f\u95f4\u201d\u8bba\u70b9\uff0c\u5e76\u901a\u8fc7\u611f\u77e5\u548c\u8ba4\u77e5\u53cc\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u6311\u6218\u73b0\u6709\u5047\u8bbe\uff0c\u5373\u611f\u77e5\u80fd\u529b\u7b49\u540c\u4e8e\u771f\u6b63\u7406\u89e3\uff0c\u901a\u8fc7\u201c\u89c6\u89c9\u623f\u95f4\u201d\u8bba\u70b9\u8d28\u7591MLMs\u662f\u5426\u5177\u5907\u6df1\u5c42\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u5c42\u6b21\u8bc4\u4f30\u6846\u67b6\uff08\u611f\u77e5\u4e0e\u8ba4\u77e5\uff09\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8bbd\u523a\u6570\u636e\u96c6\uff0c\u8bc4\u4f308\u79cdSoTA MLMs\u3002", "result": "MLMs\u5728\u611f\u77e5\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8bbd\u523a\u7406\u89e3\u9519\u8bef\u7387\u8fbe16.1%\uff0c\u63ed\u793a\u611f\u77e5\u4e0e\u7406\u89e3\u7684\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u5b9e\u8bc1\u652f\u6301\u201c\u89c6\u89c9\u623f\u95f4\u201d\u8bba\u70b9\uff0c\u4e3aMLMs\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u60c5\u611f\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u65ad\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.23277", "pdf": "https://arxiv.org/pdf/2505.23277", "abs": "https://arxiv.org/abs/2505.23277", "authors": ["Yong Zhang", "Yanwen Huang", "Ning Cheng", "Yang Guo", "Yun Zhu", "Yanmeng Wang", "Shaojun Wang", "Jing Xiao"], "title": "Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 17 pages including appendix", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external context, but retrieved passages are often lengthy, noisy, or\nexceed input limits. Existing compression methods typically require supervised\ntraining of dedicated compression models, increasing cost and reducing\nportability. We propose Sentinel, a lightweight sentence-level compression\nframework that reframes context filtering as an attention-based understanding\ntask. Rather than training a compression model, Sentinel probes decoder\nattention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier\nto identify sentence relevance. Empirically, we find that query-context\nrelevance estimation is consistent across model scales, with 0.5B proxies\nclosely matching the behaviors of larger models. On the LongBench benchmark,\nSentinel achieves up to 5$\\times$ compression while matching the QA performance\nof 7B-scale compression systems. Our results suggest that probing native\nattention signals enables fast, effective, and question-aware context\ncompression. Code available at: https://github.com/yzhangchuck/Sentinel.", "AI": {"tldr": "Sentinel\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u53e5\u5b50\u7ea7\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u5c0f\u578bLLM\u7684\u89e3\u7801\u5668\u6ce8\u610f\u529b\u4fe1\u53f7\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u95ee\u9898\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\uff0c\u65e0\u9700\u8bad\u7ec3\u4e13\u7528\u538b\u7f29\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u4e2d\uff0c\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u901a\u5e38\u5197\u957f\u3001\u5608\u6742\u6216\u8d85\u51fa\u8f93\u5165\u9650\u5236\uff0c\u800c\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u53ef\u79fb\u690d\u6027\u5dee\u3002", "method": "Sentinel\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u63a2\u6d4b0.5B\u4ee3\u7406LLM\u7684\u89e3\u7801\u5668\u6ce8\u610f\u529b\u4fe1\u53f7\uff0c\u8bc6\u522b\u53e5\u5b50\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u3002", "result": "\u5728LongBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSentinel\u5b9e\u73b0\u4e865\u500d\u7684\u538b\u7f29\uff0c\u540c\u65f6\u5339\u914d7B\u89c4\u6a21\u538b\u7f29\u7cfb\u7edf\u7684\u95ee\u7b54\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u539f\u751f\u6ce8\u610f\u529b\u4fe1\u53f7\u53ef\u4ee5\u5b9e\u73b0\u5feb\u901f\u3001\u6709\u6548\u4e14\u95ee\u9898\u611f\u77e5\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u3002"}}
{"id": "2505.23280", "pdf": "https://arxiv.org/pdf/2505.23280", "abs": "https://arxiv.org/abs/2505.23280", "authors": ["Chuandong Liu", "Huijiao Wang", "Lei Yu", "Gui-Song Xia"], "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for\nnovel view synthesis. However, most existing large-scale scene reconstruction\nmethods rely on the divide-and-conquer paradigm, which often leads to the loss\nof global scene information and requires complex parameter tuning due to scene\npartitioning and local optimization. To address these limitations, we propose\nMixGS, a novel holistic optimization framework for large-scale 3D scene\nreconstruction. MixGS models the entire scene holistically by integrating\ncamera pose and Gaussian attributes into a view-aware representation, which is\ndecoded into fine-detailed Gaussians. Furthermore, a novel mixing operation\ncombines decoded and original Gaussians to jointly preserve global coherence\nand local fidelity. Extensive experiments on large-scale scenes demonstrate\nthat MixGS achieves state-of-the-art rendering quality and competitive speed,\nwhile significantly reducing computational requirements, enabling large-scale\nscene reconstruction training on a single 24GB VRAM GPU. The code will be\nreleased at https://github.com/azhuantou/MixGS.", "AI": {"tldr": "MixGS\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u5c40\u4f18\u5316\u76843D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u5206\u6cbb\u7b56\u7565\u5bfc\u81f4\u7684\u5168\u5c40\u4fe1\u606f\u4e22\u5931\u548c\u590d\u6742\u53c2\u6570\u8c03\u4f18\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u6e32\u67d3\u548c\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u5206\u6cbb\u7b56\u7565\uff0c\u5bfc\u81f4\u5168\u5c40\u4fe1\u606f\u4e22\u5931\u548c\u53c2\u6570\u8c03\u4f18\u590d\u6742\uff0cMixGS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MixGS\u901a\u8fc7\u6574\u5408\u76f8\u673a\u4f4d\u59ff\u548c\u9ad8\u65af\u5c5e\u6027\u4e3a\u89c6\u56fe\u611f\u77e5\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u6df7\u5408\u64cd\u4f5c\u7ed3\u5408\u89e3\u7801\u4e0e\u539f\u9ad8\u65af\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u4e0e\u5c40\u90e8\u4fdd\u771f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMixGS\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b9e\u73b0\u6700\u4f18\u6e32\u67d3\u8d28\u91cf\u548c\u9ad8\u6548\u8ba1\u7b97\uff0c\u5355\u536124GB VRAM\u5373\u53ef\u8bad\u7ec3\u3002", "conclusion": "MixGS\u4e3a\u5927\u89c4\u6a213D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.23291", "pdf": "https://arxiv.org/pdf/2505.23291", "abs": "https://arxiv.org/abs/2505.23291", "authors": ["Xinye Li", "Zunwen Zheng", "Qian Zhang", "Dekai Zhuang", "Jiabao Kang", "Liyan Xu", "Qingbin Liu", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "title": "ScEdit: Script-based Assessment of Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6ScEdit\uff0c\u7ed3\u5408\u4e86\u53cd\u4e8b\u5b9e\u548c\u65f6\u95f4\u7f16\u8f91\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u7ea7\u6307\u6807\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u7f16\u8f91\u4efb\u52a1\u8fc7\u4e8e\u7b80\u5355\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u573a\u666f\u7684\u6574\u5408\uff0c\u9700\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f15\u5165ScEdit\u57fa\u51c6\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u548c\u65f6\u95f4\u7f16\u8f91\uff0c\u91c7\u7528\u8bcd\u7ea7\u548c\u6587\u672c\u7ea7\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u6240\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u73b0\u6709\u6307\u6807\u4e0a\u8868\u73b0\u4e0b\u964d\uff0c\u6587\u672c\u7ea7\u6307\u6807\u9762\u4e34\u6311\u6218\u3002", "conclusion": "ScEdit\u4e3a\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.23283", "pdf": "https://arxiv.org/pdf/2505.23283", "abs": "https://arxiv.org/abs/2505.23283", "authors": ["Zhihong Tan", "Jiayi Wang", "Huiying Shi", "Binyuan Huang", "Hongchen Wei", "Zhenzhong Chen"], "title": "RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries", "categories": ["cs.CV"], "comment": null, "summary": "Detecting forged remote sensing images is becoming increasingly critical, as\nsuch imagery plays a vital role in environmental monitoring, urban planning,\nand national security. While diffusion models have emerged as the dominant\nparadigm for image generation, their impact on remote sensing forgery detection\nremains underexplored. Existing benchmarks primarily target GAN-based forgeries\nor focus on natural images, limiting progress in this critical domain. To\naddress this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged\nand 500K real remote sensing images. The fake images are generated by ten\ndiffusion models fine-tuned on remote sensing data, covering six generation\nconditions such as text prompts, structural guidance, and inpainting. This\npaper presents the construction of RSFAKE-1M along with a comprehensive\nexperimental evaluation using both existing detectors and unified baselines.\nThe results reveal that diffusion-based remote sensing forgeries remain\nchallenging for current methods, and that models trained on RSFAKE-1M exhibit\nnotably improved generalization and robustness. Our findings underscore the\nimportance of RSFAKE-1M as a foundation for developing and evaluating\nnext-generation forgery detection approaches in the remote sensing domain. The\ndataset and other supplementary materials are available at\nhttps://huggingface.co/datasets/TZHSW/RSFAKE/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86RSFAKE-1M\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4f2a\u9020\u9065\u611f\u56fe\u50cf\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u63d0\u5347\u68c0\u6d4b\u65b9\u6cd5\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6548\u679c\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5728\u73af\u5883\u76d1\u6d4b\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u9488\u5bf9GAN\u751f\u6210\u7684\u4f2a\u9020\u56fe\u50cf\u6216\u81ea\u7136\u56fe\u50cf\uff0c\u7f3a\u4e4f\u5bf9\u6269\u6563\u6a21\u578b\u4f2a\u9020\u7684\u9065\u611f\u56fe\u50cf\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b50\u4e07\u4f2a\u9020\u548c50\u4e07\u771f\u5b9e\u9065\u611f\u56fe\u50cf\u7684RSFAKE-1M\u6570\u636e\u96c6\uff0c\u4f7f\u752810\u79cd\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u9020\u56fe\u50cf\uff0c\u6db5\u76d6\u591a\u79cd\u751f\u6210\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4f2a\u9020\u9065\u611f\u56fe\u50cf\u68c0\u6d4b\u6548\u679c\u6709\u9650\uff0c\u4f46\u4f7f\u7528RSFAKE-1M\u8bad\u7ec3\u7684\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "RSFAKE-1M\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u9065\u611f\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2505.23295", "pdf": "https://arxiv.org/pdf/2505.23295", "abs": "https://arxiv.org/abs/2505.23295", "authors": ["James Xu Zhao", "Jimmy Z. J. Liu", "Bryan Hooi", "See-Kiong Ng"], "title": "How Does Response Length Affect Long-Form Factuality", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings. 24 pages, 10 figures, 18 tables. Code available at\n  https://github.com/XuZhao0/length-bias-factuality", "summary": "Large language models (LLMs) are widely used for long-form text generation.\nHowever, factual errors in the responses would undermine their reliability.\nDespite growing attention to LLM factuality, the effect of response length on\nfactuality remains underexplored. In this work, we systematically investigate\nthis relationship by first introducing an automatic and bi-level long-form\nfactuality evaluation framework, which achieves high agreement with human\nannotations while being cost-effective. Using this framework, we conduct\ncontrolled experiments and find that longer responses exhibit lower factual\nprecision, confirming the presence of length bias. To explain this phenomenon,\nwe empirically examine three hypotheses: error propagation, long context, and\nfacts exhaustion. Our results reveal that facts exhaustion, where the model\ngradually exhausts more reliable knowledge, is the primary cause of factual\ndegradation, rather than the other two hypotheses.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u957f\u6587\u672c\u65f6\uff0c\u957f\u5ea6\u589e\u52a0\u4f1a\u5bfc\u81f4\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u77e5\u8bc6\u8017\u5c3d\u3002", "motivation": "\u63a2\u8ba8\u957f\u6587\u672c\u751f\u6210\u4e2d\u54cd\u5e94\u957f\u5ea6\u5bf9\u4e8b\u5b9e\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u53cc\u5c42\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e09\u79cd\u5047\u8bbe\uff08\u9519\u8bef\u4f20\u64ad\u3001\u957f\u4e0a\u4e0b\u6587\u3001\u77e5\u8bc6\u8017\u5c3d\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u957f\u6587\u672c\u4e8b\u5b9e\u7cbe\u5ea6\u66f4\u4f4e\uff0c\u77e5\u8bc6\u8017\u5c3d\u662f\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "\u77e5\u8bc6\u8017\u5c3d\u662f\u5bfc\u81f4\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u4e0b\u964d\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u975e\u9519\u8bef\u4f20\u64ad\u6216\u957f\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2505.23287", "pdf": "https://arxiv.org/pdf/2505.23287", "abs": "https://arxiv.org/abs/2505.23287", "authors": ["Chikaha Tsuji", "Enrique Flores Medina", "Harshit Gupta", "Md Ferdous Alam"], "title": "GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the advancement of generative AI, research on its application to 3D\nmodel generation has gained traction, particularly in automating the creation\nof Computer-Aided Design (CAD) files from images. GenCAD is a notable model in\nthis domain, leveraging an autoregressive transformer-based architecture with a\ncontrastive learning framework to generate CAD programs.\n  However, a major limitation of GenCAD is its inability to consistently\nproduce feasible boundary representations (B-reps), with approximately 10% of\ngenerated designs being infeasible. To address this, we propose\nGenCAD-Self-Repairing, a framework that enhances the feasibility of generative\nCAD models through diffusion guidance and a self-repairing pipeline. This\nframework integrates a guided diffusion denoising process in the latent space\nand a regression-based correction mechanism to refine infeasible CAD command\nsequences while preserving geometric accuracy. Our approach successfully\nconverted two-thirds of infeasible designs in the baseline method into feasible\nones, significantly improving the feasibility rate while simultaneously\nmaintaining a reasonable level of geometric accuracy between the point clouds\nof ground truth models and generated models.\n  By significantly improving the feasibility rate of generating CAD models, our\napproach helps expand the availability of high-quality training data and\nenhances the applicability of AI-driven CAD generation in manufacturing,\narchitecture, and product design.", "AI": {"tldr": "GenCAD-Self-Repairing\u901a\u8fc7\u6269\u6563\u5f15\u5bfc\u548c\u81ea\u6211\u4fee\u590d\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u751f\u6210CAD\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u4e86GenCAD\u751f\u6210\u4e0d\u53ef\u884cB-rep\u7684\u95ee\u9898\u3002", "motivation": "GenCAD\u751f\u6210\u7684CAD\u6587\u4ef6\u4e2d\u7ea610%\u4e0d\u53ef\u884c\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u6269\u6563\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u548c\u56de\u5f52\u6821\u6b63\u673a\u5236\uff0c\u4f18\u5316\u4e0d\u53ef\u884cCAD\u547d\u4ee4\u5e8f\u5217\u3002", "result": "\u6210\u529f\u5c06\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u4e09\u5206\u4e4b\u4e8c\u7684\u4e0d\u53ef\u884c\u8bbe\u8ba1\u8f6c\u5316\u4e3a\u53ef\u884c\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u53ef\u884c\u6027\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u751f\u6210CAD\u6a21\u578b\u7684\u8d28\u91cf\uff0c\u6269\u5c55\u4e86AI\u9a71\u52a8CAD\u751f\u6210\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2505.23297", "pdf": "https://arxiv.org/pdf/2505.23297", "abs": "https://arxiv.org/abs/2505.23297", "authors": ["Daryna Dementieva", "Nikolay Babakov", "Alexander Fraser"], "title": "EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian", "categories": ["cs.CL"], "comment": null, "summary": "While Ukrainian NLP has seen progress in many texts processing tasks, emotion\nclassification remains an underexplored area with no publicly available\nbenchmark to date. In this work, we introduce EmoBench-UA, the first annotated\ndataset for emotion detection in Ukrainian texts. Our annotation schema is\nadapted from the previous English-centric works on emotion detection (Mohammad\net al., 2018; Mohammad, 2022) guidelines. The dataset was created through\ncrowdsourcing using the Toloka.ai platform ensuring high-quality of the\nannotation process. Then, we evaluate a range of approaches on the collected\ndataset, starting from linguistic-based baselines, synthetic data translated\nfrom English, to large language models (LLMs). Our findings highlight the\nchallenges of emotion classification in non-mainstream languages like Ukrainian\nand emphasize the need for further development of Ukrainian-specific models and\ntraining resources.", "AI": {"tldr": "EmoBench-UA\u662f\u9996\u4e2a\u4e4c\u514b\u5170\u8bed\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4e4c\u514b\u5170\u8bed\u7684\u60c5\u611f\u5206\u7c7b\u7814\u7a76\u7f3a\u4e4f\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u4f17\u5305\u65b9\u5f0f\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u8bed\u8a00\u5b66\u3001\u5408\u6210\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u4e4c\u514b\u5170\u8bed\u7b49\u975e\u4e3b\u6d41\u8bed\u8a00\u5728\u60c5\u611f\u5206\u7c7b\u4e0a\u7684\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e13\u7528\u6a21\u578b\u548c\u8d44\u6e90\u3002", "conclusion": "EmoBench-UA\u4e3a\u4e4c\u514b\u5170\u8bed\u60c5\u611f\u5206\u7c7b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u672a\u6765\u9700\u66f4\u591a\u672c\u571f\u5316\u7814\u7a76\u3002"}}
{"id": "2505.23292", "pdf": "https://arxiv.org/pdf/2505.23292", "abs": "https://arxiv.org/abs/2505.23292", "authors": ["Evangelos Charalampakis", "Vasileios Mygdalis", "Ioannis Pitas"], "title": "Federated Unsupervised Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work explores the application of Federated Learning (FL) in Unsupervised\nSemantic image Segmentation (USS). Recent USS methods extract pixel-level\nfeatures using frozen visual foundation models and refine them through\nself-supervised objectives that encourage semantic grouping. These features are\nthen grouped to semantic clusters to produce segmentation masks. Extending\nthese ideas to federated settings requires feature representation and cluster\ncentroid alignment across distributed clients -- an inherently difficult task\nunder heterogeneous data distributions in the absence of supervision. To\naddress this, we propose FUSS Federated Unsupervised image Semantic\nSegmentation) which is, to our knowledge, the first framework to enable fully\ndecentralized, label-free semantic segmentation training. FUSS introduces novel\nfederation strategies that promote global consistency in feature and prototype\nspace, jointly optimizing local segmentation heads and shared semantic\ncentroids. Experiments on both benchmark and real-world datasets, including\nbinary and multi-class segmentation tasks, show that FUSS consistently\noutperforms local-only client trainings as well as extensions of classical FL\nalgorithms under varying client data distributions. To support reproducibility,\nfull code will be released upon manuscript acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFUSS\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u3001\u65e0\u76d1\u7763\u7684\u8054\u90a6\u5b66\u4e60\u8bed\u4e49\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u7279\u5f81\u548c\u539f\u578b\u7a7a\u95f4\u7684\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u76d1\u7763\u8bed\u4e49\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7279\u5f81\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faFUSS\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u7684\u8054\u90a6\u7b56\u7565\u4f18\u5316\u5c40\u90e8\u5206\u5272\u5934\u548c\u5171\u4eab\u8bed\u4e49\u4e2d\u5fc3\uff0c\u786e\u4fdd\u7279\u5f81\u548c\u539f\u578b\u7a7a\u95f4\u7684\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u57fa\u51c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cFUSS\u5728\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u5272\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5c40\u90e8\u8bad\u7ec3\u548c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "FUSS\u4e3a\u65e0\u76d1\u7763\u8054\u90a6\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2505.23299", "pdf": "https://arxiv.org/pdf/2505.23299", "abs": "https://arxiv.org/abs/2505.23299", "authors": ["Julia Belikova", "Konstantin Polev", "Rauf Parchiev", "Dmitry Simakov"], "title": "Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems\nare increasingly deployed in industry applications, yet their reliability\nremains hampered by challenges in detecting hallucinations. While supervised\nstate-of-the-art (SOTA) methods that leverage LLM hidden states -- such as\nactivation tracing and representation analysis -- show promise, their\ndependence on extensively annotated datasets limits scalability in real-world\napplications. This paper addresses the critical bottleneck of data annotation\nby investigating the feasibility of reducing training data requirements for two\nSOTA hallucination detection frameworks: Lookback Lens, which analyzes\nattention head dynamics, and probing-based approaches, which decode internal\nmodel representations. We propose a methodology combining efficient\nclassification algorithms with dimensionality reduction techniques to minimize\nsample size demands while maintaining competitive performance. Evaluations on\nstandardized question-answering RAG benchmarks show that our approach achieves\nperformance comparable to strong proprietary LLM-based baselines with only 250\ntraining samples. These results highlight the potential of lightweight,\ndata-efficient paradigms for industrial deployment, particularly in\nannotation-constrained scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u51cf\u5c11\u5e7b\u89c9\u68c0\u6d4b\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u6548\u5206\u7c7b\u7b97\u6cd5\u548c\u964d\u7ef4\u6280\u672f\uff0c\u4ec5\u9700250\u4e2a\u6837\u672c\u5373\u53ef\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u53d7\u9650\u4e8e\u5e7b\u89c9\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u9ad8\u6548\u5206\u7c7b\u7b97\u6cd5\u548c\u964d\u7ef4\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\uff0c\u5e76\u8bc4\u4f30\u5176\u5728Lookback Lens\u548c\u63a2\u6d4b\u6846\u67b6\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5728\u6807\u51c6\u95ee\u7b54RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u7528250\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u8fbe\u5230\u4e0e\u5f3a\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6807\u6ce8\u53d7\u9650\u7684\u573a\u666f\u3002"}}
{"id": "2505.23312", "pdf": "https://arxiv.org/pdf/2505.23312", "abs": "https://arxiv.org/abs/2505.23312", "authors": ["Finn Carter"], "title": "TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models", "categories": ["cs.CV"], "comment": "In peer review", "summary": "Text-to-image diffusion models have shown unprecedented generative\ncapability, but their ability to produce undesirable concepts\n(e.g.~pornographic content, sensitive identities, copyrighted styles) poses\nserious concerns for privacy, fairness, and safety. {Concept erasure} aims to\nremove or suppress specific concept information in a generative model. In this\npaper, we introduce \\textbf{TRACE (Trajectory-Constrained Attentional Concept\nErasure)}, a novel method to erase targeted concepts from diffusion models\nwhile preserving overall generative quality. Our approach combines a rigorous\ntheoretical framework, establishing formal conditions under which a concept can\nbe provably suppressed in the diffusion process, with an effective fine-tuning\nprocedure compatible with both conventional latent diffusion (Stable Diffusion)\nand emerging rectified flow models (e.g.~FLUX). We first derive a closed-form\nupdate to the model's cross-attention layers that removes hidden\nrepresentations of the target concept. We then introduce a trajectory-aware\nfinetuning objective that steers the denoising process away from the concept\nonly in the late sampling stages, thus maintaining the model's fidelity on\nunrelated content. Empirically, we evaluate TRACE on multiple benchmarks used\nin prior concept erasure studies (object classes, celebrity faces, artistic\nstyles, and explicit content from the I2P dataset). TRACE achieves\nstate-of-the-art performance, outperforming recent methods such as ANT,\nEraseAnything, and MACE in terms of removal efficacy and output quality.", "AI": {"tldr": "TRACE\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002\u5b83\u7ed3\u5408\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5fae\u8c03\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53ef\u80fd\u751f\u6210\u4e0d\u826f\u5185\u5bb9\uff08\u5982\u8272\u60c5\u3001\u654f\u611f\u8eab\u4efd\u3001\u7248\u6743\u98ce\u683c\uff09\uff0c\u5f15\u53d1\u9690\u79c1\u3001\u516c\u5e73\u548c\u5b89\u5168\u95ee\u9898\u3002\u6982\u5ff5\u64e6\u9664\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "TRACE\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u548c\u5fae\u8c03\u8fc7\u7a0b\u5b9e\u73b0\u6982\u5ff5\u64e6\u9664\uff0c\u5305\u62ec\u5bf9\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u7684\u95ed\u5f0f\u66f4\u65b0\u548c\u8f68\u8ff9\u611f\u77e5\u5fae\u8c03\u76ee\u6807\u3002", "result": "TRACE\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4f18\u4e8eANT\u3001EraseAnything\u548cMACE\u7b49\u65b9\u6cd5\u3002", "conclusion": "TRACE\u5728\u6982\u5ff5\u64e6\u9664\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2505.23304", "pdf": "https://arxiv.org/pdf/2505.23304", "abs": "https://arxiv.org/abs/2505.23304", "authors": ["Yi Luo", "Qiwen Wang", "Junqi Yang", "Luyao Tang", "Zhenghao Lin", "Zhenzhe Ying", "Weiqiang Wang", "Chen Lin"], "title": "Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify both known and novel\ncategories using partially labeled data that contains only known classes.\nDespite achieving strong performance on existing benchmarks, current textual\nGCD methods lack sufficient validation in realistic settings. We introduce\nEvent-Centric GCD (EC-GCD), characterized by long, complex narratives and\nhighly imbalanced class distributions, posing two main challenges: (1)\ndivergent clustering versus classification groupings caused by subjective\ncriteria, and (2) Unfair alignment for minority classes. To tackle these, we\npropose PaMA, a framework leveraging LLMs to extract and refine event patterns\nfor improved cluster-class alignment. Additionally, a ranking-filtering-mining\npipeline ensures balanced representation of prototypes across imbalanced\ncategories. Evaluations on two EC-GCD benchmarks, including a newly constructed\nScam Report dataset, demonstrate that PaMA outperforms prior methods with up to\n12.58% H-score gains, while maintaining strong generalization on base GCD\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEC-GCD\u4efb\u52a1\uff0c\u89e3\u51b3\u957f\u6587\u672c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u51faPaMA\u6846\u67b6\uff0c\u5229\u7528LLMs\u4f18\u5316\u805a\u7c7b\u4e0e\u5206\u7c7b\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672cGCD\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u957f\u6587\u672c\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faPaMA\u6846\u67b6\uff0c\u5229\u7528LLMs\u63d0\u53d6\u4e8b\u4ef6\u6a21\u5f0f\u5e76\u4f18\u5316\u805a\u7c7b\u4e0e\u5206\u7c7b\u5bf9\u9f50\uff0c\u540c\u65f6\u91c7\u7528\u6392\u540d-\u8fc7\u6ee4-\u6316\u6398\u6d41\u7a0b\u5e73\u8861\u7c7b\u522b\u539f\u578b\u3002", "result": "\u5728EC-GCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaMA\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe12.58%\uff0c\u4e14\u5728\u65b0\u6784\u5efa\u7684Scam Report\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "PaMA\u6709\u6548\u89e3\u51b3\u4e86EC-GCD\u7684\u6311\u6218\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.23313", "pdf": "https://arxiv.org/pdf/2505.23313", "abs": "https://arxiv.org/abs/2505.23313", "authors": ["Weizhe Kong", "Xiao Wang", "Ruichong Gao", "Chenglong Li", "Yu Zhang", "Xing Yang", "Yaowei Wang", "Jin Tang"], "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\u6846\u67b6ASL-PAR\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u653b\u51fb\u751f\u6210\u5bf9\u6297\u566a\u58f0\uff0c\u5e76\u8bbe\u8ba1\u8bed\u4e49\u504f\u79fb\u9632\u5fa1\u7b56\u7565\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u5728\u6df1\u5ea6\u5b66\u4e60\u63a8\u52a8\u4e0b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u6f5c\u5728\u8106\u5f31\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684CLIP\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u5206\u5757\u5d4c\u5165\u7279\u5f81\uff0c\u6269\u5c55\u5c5e\u6027\u4e3a\u53e5\u5b50\u5e76\u5d4c\u5165\u6587\u672c\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u6a21\u6001Transformer\u878d\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u4ee4\u724c\uff0c\u5229\u7528\u524d\u9988\u7f51\u7edc\u8fdb\u884c\u5c5e\u6027\u8bc6\u522b\uff0c\u751f\u6210\u5bf9\u6297\u566a\u58f0\uff08ASL-PAR\uff09\u5e76\u8bbe\u8ba1\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57df\uff08\u5982PETA\u3001PA100K\u7b49\uff09\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u653b\u51fb\u4e0e\u9632\u5fa1\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ASL-PAR\u6846\u67b6\u4e3a\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u4e0e\u9632\u5fa1\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2505.23315", "pdf": "https://arxiv.org/pdf/2505.23315", "abs": "https://arxiv.org/abs/2505.23315", "authors": ["Abhirup Chakravarty", "Mark Brenchley", "Trevor Breakspear", "Ian Lewin", "Yan Huang"], "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the preprint version of our paper accepted to ACL 2025\n  (Industry Track). The DOI will be added once available", "summary": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that\nscores are only released when they meet high reliability standards. Confidence\nmodelling addresses this by assigning a reliability estimate measure, in the\nform of a confidence score, to each automated score. In this study, we frame\nconfidence estimation as a classification task: predicting whether an\nAES-generated score correctly places a candidate in the appropriate CEFR level.\nWhile this is a binary decision, we leverage the inherent granularity of the\nscoring domain in two ways. First, we reformulate the task as an n-ary\nclassification problem using score binning. Second, we introduce a set of novel\nKernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that\nincorporate the ordinal structure of CEFR labels. Our best-performing model\nachieves an F1 score of 0.97, and enables the system to release 47% of scores\nwith 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to\napproximately 92% (approx.) CEFR agreement from the standalone AES model where\nwe release all AM predicted scores.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5efa\u6a21\u63d0\u9ad8\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\uff08AES\uff09\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u9884\u6d4b\u5206\u6570\u662f\u5426\u51c6\u786e\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570KWOCCE\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u5206\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u4e2d\u5206\u6570\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4ec5\u5728\u9ad8\u53ef\u9760\u6027\u6807\u51c6\u4e0b\u53d1\u5e03\u5206\u6570\u3002", "method": "\u5c06\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u5206\u7c7b\u4efb\u52a1\uff0c\u5229\u7528\u5206\u6570\u5206\u7bb1\u548c\u65b0\u7684KWOCCE\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408CEFR\u6807\u7b7e\u7684\u5e8f\u6570\u7ed3\u6784\u3002", "result": "\u6700\u4f73\u6a21\u578bF1\u5206\u6570\u8fbe0.97\uff0c47%\u7684\u5206\u6570\u8fbe\u5230100% CEFR\u4e00\u81f4\u6027\uff0c99%\u8fbe\u5230\u81f3\u5c1195%\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u539f\u59cbAES\u6a21\u578b\u768492%\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u5efa\u6a21\u548cKWOCCE\u635f\u5931\u51fd\u6570\u6709\u6548\u63d0\u5347\u4e86AES\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u4fdd\u969c\u3002"}}
{"id": "2505.23325", "pdf": "https://arxiv.org/pdf/2505.23325", "abs": "https://arxiv.org/abs/2505.23325", "authors": ["Hengyuan Cao", "Yutong Feng", "Biao Gong", "Yijing Tian", "Yunhong Lu", "Chuang Liu", "Bin Wang"], "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRA-Ctrl\u7684\u89c6\u9891\u5230\u56fe\u50cf\u77e5\u8bc6\u538b\u7f29\u4e0e\u4efb\u52a1\u9002\u5e94\u8303\u5f0f\uff0c\u5229\u7528\u89c6\u9891\u6a21\u578b\u7684\u4f18\u52bf\uff08\u5982\u957f\u7a0b\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u5168\u6ce8\u610f\u529b\u673a\u5236\uff09\u652f\u6301\u53ef\u63a7\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u8bad\u7ec3\u597d\u7684\u9ad8\u7ef4\u89c6\u9891\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u652f\u6301\u4f4e\u7ef4\u4efb\u52a1\uff08\u5982\u53ef\u63a7\u56fe\u50cf\u751f\u6210\uff09\uff0c\u4ee5\u6316\u6398\u89c6\u9891\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faDRA-Ctrl\u8303\u5f0f\uff0c\u5305\u62ec\u57fa\u4e8emixup\u7684\u8fc7\u6e21\u7b56\u7565\u548c\u91cd\u65b0\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u7ed3\u6784\uff0c\u4ee5\u89e3\u51b3\u89c6\u9891\u5e27\u4e0e\u56fe\u50cf\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u9020\u540e\u7684\u89c6\u9891\u6a21\u578b\u5728\u591a\u79cd\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "DRA-Ctrl\u4e3a\u8d44\u6e90\u5bc6\u96c6\u578b\u89c6\u9891\u6a21\u578b\u7684\u91cd\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u4e3a\u8de8\u89c6\u89c9\u6a21\u6001\u7684\u7edf\u4e00\u751f\u6210\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.23316", "pdf": "https://arxiv.org/pdf/2505.23316", "abs": "https://arxiv.org/abs/2505.23316", "authors": ["Kaiyang Guo", "Yinchuan Li", "Zhitang Chen"], "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO", "categories": ["cs.CL"], "comment": null, "summary": "Direct alignment methods typically optimize large language models (LLMs) by\ncontrasting the likelihoods of preferred versus dispreferred responses. While\neffective in steering LLMs to match relative preference, these methods are\nfrequently noted for decreasing the absolute likelihoods of example responses.\nAs a result, aligned models tend to generate outputs that deviate from the\nexpected patterns, exhibiting reward-hacking effect even without a reward\nmodel. This undesired consequence exposes a fundamental limitation in\ncontrastive alignment, which we characterize as likelihood underdetermination.\nIn this work, we revisit direct preference optimization (DPO) -- the seminal\ndirect alignment method -- and demonstrate that its loss theoretically admits a\ndecomposed reformulation. The reformulated loss not only broadens applicability\nto a wider range of feedback types, but also provides novel insights into the\nunderlying cause of likelihood underdetermination. Specifically, the standard\nDPO implementation implicitly oversimplifies a regularizer in the reformulated\nloss, and reinstating its complete version effectively resolves the\nunderdetermination issue. Leveraging these findings, we introduce PRoximalized\nPReference Optimization (PRO), a unified method to align with diverse feeback\ntypes, eliminating likelihood underdetermination through an efficient\napproximation of the complete regularizer. Comprehensive experiments show the\nsuperiority of PRO over existing methods in scenarios involving pairwise,\nbinary and scalar feedback.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5PRO\uff0c\u89e3\u51b3\u4e86\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u53cd\u9988\u7c7b\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u76f4\u63a5\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u504f\u79bb\u9884\u671f\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5206\u89e3DPO\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51faPRO\u65b9\u6cd5\uff0c\u5f15\u5165\u5b8c\u6574\u6b63\u5219\u5316\u9879\u4ee5\u89e3\u51b3\u4f3c\u7136\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "result": "PRO\u5728\u6210\u5bf9\u3001\u4e8c\u5143\u548c\u6807\u91cf\u53cd\u9988\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PRO\u662f\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u5bf9\u9f50\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2505.23331", "pdf": "https://arxiv.org/pdf/2505.23331", "abs": "https://arxiv.org/abs/2505.23331", "authors": ["Matteo Gallici", "Haitz S\u00e1ez de Oc\u00e1riz Borde"], "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL)\nhas emerged as an effective approach for aligning outputs more closely with\nnuanced human preferences. In this paper, we investigate the application of\nGroup Relative Policy Optimization (GRPO) to fine-tune next-scale visual\nautoregressive (VAR) models. Our empirical results demonstrate that this\napproach enables alignment to intricate reward signals derived from aesthetic\npredictors and CLIP embeddings, significantly enhancing image quality and\nenabling precise control over the generation style. Interestingly, by\nleveraging CLIP, our method can help VAR models generalize beyond their initial\nImageNet distribution: through RL-driven exploration, these models can generate\nimages aligned with prompts referencing image styles that were absent during\npre-training. In summary, we show that RL-based fine-tuning is both efficient\nand effective for VAR models, benefiting particularly from their fast inference\nspeeds, which are advantageous for online sampling, an aspect that poses\nsignificant challenges for diffusion-based alternatives.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5fae\u8c03\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5e94\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u4f18\u5316\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u5e76\u5b9e\u73b0\u4e86\u751f\u6210\u98ce\u683c\u7684\u63a7\u5236\u3002", "motivation": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u66f4\u7cbe\u51c6\u5730\u7b26\u5408\u4eba\u7c7b\u504f\u597d\uff0c\u5c24\u5176\u662f\u5229\u7528GRPO\u4f18\u5316VAR\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u5956\u52b1\u4fe1\u53f7\u4e0b\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528GRPO\u65b9\u6cd5\u5fae\u8c03VAR\u6a21\u578b\uff0c\u7ed3\u5408\u7f8e\u5b66\u9884\u6d4b\u5668\u548cCLIP\u5d4c\u5165\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u56fe\u50cf\u8d28\u91cf\u7684\u63d0\u5347\u548c\u751f\u6210\u98ce\u683c\u7684\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7CLIP\u5b9e\u73b0\u8d85\u51fa\u9884\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "RL\u5fae\u8c03\u5bf9VAR\u6a21\u578b\u9ad8\u6548\u6709\u6548\uff0c\u5c24\u5176\u5f97\u76ca\u4e8e\u5176\u5feb\u901f\u63a8\u7406\u901f\u5ea6\uff0c\u4f18\u4e8e\u6269\u6563\u6a21\u578b\u7b49\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.23323", "pdf": "https://arxiv.org/pdf/2505.23323", "abs": "https://arxiv.org/abs/2505.23323", "authors": ["Harish Tayyar Madabushi", "Melissa Torgbi", "Claire Bonial"], "title": "Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors", "categories": ["cs.CL"], "comment": null, "summary": "In this position paper we raise critical awareness of a realistic view of LLM\ncapabilities that eschews extreme alternative views that LLMs are either\n\"stochastic parrots\" or in possession of \"emergent\" advanced reasoning\ncapabilities, which, due to their unpredictable emergence, constitute an\nexistential threat. Our middle-ground view is that LLMs extrapolate from priors\nfrom their training data, and that a mechanism akin to in-context learning\nenables the targeting of the appropriate information from which to extrapolate.\nWe call this \"context-directed extrapolation.\" Under this view, substantiated\nthough existing literature, while reasoning capabilities go well beyond\nstochastic parroting, such capabilities are predictable, controllable, not\nindicative of advanced reasoning akin to high-level cognitive capabilities in\nhumans, and not infinitely scalable with additional training. As a result,\nfears of uncontrollable emergence of agency are allayed, while research\nadvances are appropriately refocused on the processes of context-directed\nextrapolation and how this interacts with training data to produce valuable\ncapabilities in LLMs. Future work can therefore explore alternative augmenting\ntechniques that do not rely on inherent advanced reasoning in LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8eLLM\u80fd\u529b\u7684\u4e2d\u95f4\u7acb\u573a\uff0c\u8ba4\u4e3aLLM\u901a\u8fc7\u4e0a\u4e0b\u6587\u6307\u5bfc\u7684\u5916\u63a8\uff08context-directed extrapolation\uff09\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u800c\u975e\u6781\u7aef\u89c2\u70b9\u4e2d\u7684\u201c\u968f\u673a\u9e66\u9e49\u201d\u6216\u201c\u6d8c\u73b0\u201d\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6279\u5224\u6027\u5730\u63a2\u8ba8LLM\u80fd\u529b\u7684\u73b0\u5b9e\u89c6\u89d2\uff0c\u907f\u514d\u6781\u7aef\u89c2\u70b9\uff08\u5982LLM\u662f\u65e0\u610f\u8bc6\u7684\u201c\u968f\u673a\u9e66\u9e49\u201d\u6216\u5177\u6709\u4e0d\u53ef\u9884\u6d4b\u7684\u201c\u6d8c\u73b0\u201d\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff09\u3002", "method": "\u63d0\u51fa\u201c\u4e0a\u4e0b\u6587\u6307\u5bfc\u7684\u5916\u63a8\u201d\u673a\u5236\uff0c\u7ed3\u5408\u73b0\u6709\u6587\u732e\u652f\u6301\uff0c\u5206\u6790LLM\u5982\u4f55\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u6269\u5c55\u4fe1\u606f\u3002", "result": "LLM\u7684\u63a8\u7406\u80fd\u529b\u8d85\u51fa\u968f\u673a\u6a21\u4eff\uff0c\u4f46\u53ef\u9884\u6d4b\u3001\u53ef\u63a7\uff0c\u4e14\u4e0d\u5177\u4eba\u7c7b\u9ad8\u7ea7\u8ba4\u77e5\u80fd\u529b\u6216\u65e0\u9650\u6269\u5c55\u6027\u3002", "conclusion": "\u7814\u7a76\u5e94\u805a\u7126\u4e0a\u4e0b\u6587\u6307\u5bfc\u7684\u5916\u63a8\u673a\u5236\u53ca\u5176\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u4e92\u52a8\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u4e0d\u4f9d\u8d56LLM\u56fa\u6709\u9ad8\u7ea7\u63a8\u7406\u7684\u589e\u5f3a\u6280\u672f\u3002"}}
{"id": "2505.23341", "pdf": "https://arxiv.org/pdf/2505.23341", "abs": "https://arxiv.org/abs/2505.23341", "authors": ["Daoxi Cao", "Hangbei Cheng", "Yijin Li", "Ruolin Zhou", "Xinyi Li", "Xuehan Zhang", "Binwei Li", "Xuancheng Gu", "Xueyu Liu", "Yongfei Wu"], "title": "DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Whole-slide images (WSIs) are critical for cancer diagnosis due to their\nultra-high resolution and rich semantic content. However, their massive size\nand the limited availability of fine-grained annotations pose substantial\nchallenges for conventional supervised learning. We propose DSAGL (Dual-Stream\nAttention-Guided Learning), a novel weakly supervised classification framework\nthat combines a teacher-student architecture with a dual-stream design. DSAGL\nexplicitly addresses instance-level ambiguity and bag-level semantic\nconsistency by generating multi-scale attention-based pseudo labels and guiding\ninstance-level learning. A shared lightweight encoder (VSSMamba) enables\nefficient long-range dependency modeling, while a fusion-attentive module\n(FASA) enhances focus on sparse but diagnostically relevant regions. We further\nintroduce a hybrid loss to enforce mutual consistency between the two streams.\nExperiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL\nconsistently outperforms state-of-the-art MIL baselines, achieving superior\ndiscriminative performance and robustness under weak supervision.", "AI": {"tldr": "DSAGL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6d41\u8bbe\u8ba1\u548c\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u5168\u5207\u7247\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u5b9e\u4f8b\u7ea7\u6a21\u7cca\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u56e0\u5176\u8d85\u9ad8\u5206\u8fa8\u7387\u548c\u4e30\u5bcc\u8bed\u4e49\u5185\u5bb9\u5bf9\u764c\u75c7\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5de8\u5927\u5c3a\u5bf8\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u7a00\u7f3a\u6027\u7ed9\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u5e26\u6765\u6311\u6218\u3002", "method": "\u63d0\u51faDSAGL\u6846\u67b6\uff0c\u7ed3\u5408\u6559\u5e08-\u5b66\u751f\u67b6\u6784\u548c\u53cc\u6d41\u8bbe\u8ba1\uff0c\u751f\u6210\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u4f2a\u6807\u7b7e\u5e76\u6307\u5bfc\u5b9e\u4f8b\u7ea7\u5b66\u4e60\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668VSSMamba\u548c\u878d\u5408\u6ce8\u610f\u529b\u6a21\u5757FASA\u3002", "result": "\u5728CIFAR-10\u3001NCT-CRC\u548cTCGA-Lung\u6570\u636e\u96c6\u4e0a\uff0cDSAGL\u8868\u73b0\u4f18\u4e8e\u73b0\u6709MIL\u57fa\u7ebf\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u5224\u522b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "DSAGL\u901a\u8fc7\u5f31\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86WSIs\u5206\u7c7b\u95ee\u9898\uff0c\u4e3a\u764c\u75c7\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2505.23363", "pdf": "https://arxiv.org/pdf/2505.23363", "abs": "https://arxiv.org/abs/2505.23363", "authors": ["Hongzhan Chen", "Tao Yang", "Shiping Gao", "Ruijun Chen", "Xiaojun Quan", "Hongtao Tian", "Ting Yao"], "title": "Discriminative Policy Optimization for Token-Level Reward Models", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Process reward models (PRMs) provide more nuanced supervision compared to\noutcome reward models (ORMs) for optimizing policy models, positioning them as\na promising approach to enhancing the capabilities of LLMs in complex reasoning\ntasks. Recent efforts have advanced PRMs from step-level to token-level\ngranularity by integrating reward modeling into the training of generative\nmodels, with reward scores derived from token generation probabilities.\nHowever, the conflict between generative language modeling and reward modeling\nmay introduce instability and lead to inaccurate credit assignments. To address\nthis challenge, we revisit token-level reward assignment by decoupling reward\nmodeling from language generation and derive a token-level reward model through\nthe optimization of a discriminative policy, termed the Q-function Reward Model\n(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level\nQ-functions from preference data without relying on fine-grained annotations.\nIn our experiments, Q-RM consistently outperforms all baseline methods across\nvarious benchmarks. For example, when integrated into PPO/REINFORCE algorithms,\nQ-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical\nreasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared\nto the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM\nsignificantly enhances training efficiency, achieving convergence 12 times\nfaster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code\nand data are available at https://github.com/homzer/Q-RM.", "AI": {"tldr": "Q-RM\u662f\u4e00\u79cd\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u5efa\u6a21\u4e0e\u8bed\u8a00\u751f\u6210\u6765\u4f18\u5316\u7b56\u7565\u6a21\u578b\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u4e0e\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u8bed\u8a00\u5efa\u6a21\u4e0e\u5956\u52b1\u5efa\u6a21\u51b2\u7a81\u5bfc\u81f4\u7684\u4fe1\u7528\u5206\u914d\u4e0d\u51c6\u786e\u95ee\u9898\uff0c\u63d0\u5347\u4ee4\u724c\u7ea7\u5956\u52b1\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u4e0e\u6027\u80fd\u3002", "method": "\u63d0\u51faQ-RM\uff0c\u901a\u8fc7\u4f18\u5316\u5224\u522b\u7b56\u7565\uff08Q\u51fd\u6570\uff09\u4ece\u504f\u597d\u6570\u636e\u4e2d\u5b66\u4e60\u4ee4\u724c\u7ea7\u5956\u52b1\uff0c\u65e0\u9700\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002", "result": "Q-RM\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cPass@1\u5206\u6570\u63d0\u5347\u660e\u663e\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u5927\u5e45\u63d0\u9ad8\uff08\u6536\u655b\u901f\u5ea6\u5feb12\u500d\uff09\u3002", "conclusion": "Q-RM\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u4ee4\u724c\u7ea7\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23343", "pdf": "https://arxiv.org/pdf/2505.23343", "abs": "https://arxiv.org/abs/2505.23343", "authors": ["Sixian Wang", "Zhiwei Tang", "Tsung-Hui Chang"], "title": "Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models often exhibit inconsistent sample quality due to stochastic\nvariations inherent in their sampling trajectories. Although training-based\nfine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to\nimprove sample fidelity, they typically necessitate full denoising processes\nand external reward signals. This incurs substantial computational costs,\nhindering their broader applicability. In this work, we unveil an intriguing\nphenomenon: a previously unobserved yet exploitable link between sample quality\nand characteristics of the denoising trajectory during classifier-free guidance\n(CFG). Specifically, we identify a strong correlation between high-density\nregions of the sample distribution and the Accumulated Score Differences\n(ASD)--the cumulative divergence between conditional and unconditional scores.\nLeveraging this insight, we introduce CFG-Rejection, an efficient,\nplug-and-play strategy that filters low-quality samples at an early stage of\nthe denoising process, crucially without requiring external reward signals or\nmodel retraining. Importantly, our approach necessitates no modifications to\nmodel architectures or sampling schedules and maintains full compatibility with\nexisting diffusion frameworks. We validate the effectiveness of CFG-Rejection\nin image generation through extensive experiments, demonstrating marked\nimprovements on human preference scores (HPSv2, PickScore) and challenging\nbenchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer\nsignificant advantages for diverse generative modalities beyond images, paving\nthe way for more efficient and reliable high-quality sample generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCFG-Rejection\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u53bb\u566a\u8f68\u8ff9\u4e2d\u7684\u7d2f\u79ef\u5206\u6570\u5dee\u5f02\uff08ASD\uff09\u6765\u63d0\u524d\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\uff0c\u65e0\u9700\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\u6216\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6837\u672c\u8d28\u91cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982DDPO\u548c\u63a8\u7406\u65f6\u5bf9\u9f50\u6280\u672f\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4f9d\u8d56\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u53d1\u73b0\u6837\u672c\u8d28\u91cf\u4e0e\u53bb\u566a\u8f68\u8ff9\u4e2d\u6761\u4ef6\u548c\u65e0\u6761\u4ef6\u5206\u6570\u7684\u7d2f\u79ef\u5dee\u5f02\uff08ASD\uff09\u5f3a\u76f8\u5173\uff0c\u63d0\u51faCFG-Rejection\u65b9\u6cd5\uff0c\u5728\u53bb\u566a\u65e9\u671f\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cCFG-Rejection\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u504f\u597d\u5206\u6570\uff08HPSv2, PickScore\uff09\u548c\u6311\u6218\u6027\u57fa\u51c6\uff08GenEval, DPG-Bench\uff09\u7684\u8868\u73b0\u3002", "conclusion": "CFG-Rejection\u4e3a\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u517c\u5bb9\u73b0\u6709\u6846\u67b6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6709\u671b\u6269\u5c55\u5230\u5176\u4ed6\u751f\u6210\u6a21\u6001\u3002"}}
{"id": "2505.23368", "pdf": "https://arxiv.org/pdf/2505.23368", "abs": "https://arxiv.org/abs/2505.23368", "authors": ["Beiduo Chen", "Yang Janet Liu", "Anna Korhonen", "Barbara Plank"], "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation", "categories": ["cs.CL"], "comment": "22 pages, 7 figures", "summary": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7ba1\u9053\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u57fa\u7840\u7684\u5206\u6bb5\u5668\u4ece\u601d\u7ef4\u94fe\u4e2d\u63d0\u53d6\u652f\u6301\u6216\u53cd\u5bf9\u6bcf\u4e2a\u7b54\u6848\u9009\u9879\u7684\u9648\u8ff0\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6392\u540d\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\u548c\u57fa\u7ebf\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528LLM\u751f\u6210\u7684\u601d\u7ef4\u94fe\uff08CoTs\uff09\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u6807\u7b7e\u53d8\u5f02\u73b0\u8c61\uff0c\u5e76\u6539\u8fdb\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u6807\u7b7e\u5206\u5e03\u7684\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e00\u79cdLLM\u7ba1\u9053\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u5206\u6bb5\u5668\u4eceCoTs\u4e2d\u63d0\u53d6\u652f\u6301\u6216\u53cd\u5bf9\u7b54\u6848\u7684\u9648\u8ff0\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u6392\u540d\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u65b9\u6cd5\u548c\u57fa\u7ebf\uff0c\u6392\u540d\u65b9\u6cd5\u66f4\u7b26\u5408\u4eba\u7c7b\u6807\u6ce8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u9884\u6d4b\u4e0e\u4eba\u7c7b\u6807\u7b7e\u5206\u5e03\u7684\u5bf9\u9f50\uff0c\u5c55\u793a\u4e86\u601d\u7ef4\u94fe\u5728\u7406\u89e3\u6807\u7b7e\u53d8\u5f02\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23346", "pdf": "https://arxiv.org/pdf/2505.23346", "abs": "https://arxiv.org/abs/2505.23346", "authors": ["Yexiong Lin", "Yu Yao", "Tongliang Liu"], "title": "Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching", "categories": ["cs.CV"], "comment": null, "summary": "Flow Matching (FM) is an effective framework for training a model to learn a\nvector field that transports samples from a source distribution to a target\ndistribution. To train the model, early FM methods use random couplings, which\noften result in crossing paths and lead the model to learn non-straight\ntrajectories that require many integration steps to generate high-quality\nsamples. To address this, recent methods adopt Optimal Transport (OT) to\nconstruct couplings by minimizing geometric distances, which helps reduce path\ncrossings. However, we observe that such geometry-based couplings do not\nnecessarily align with the model's preferred trajectories, making it difficult\nto learn the vector field induced by these couplings, which prevents the model\nfrom learning straight trajectories. Motivated by this, we propose\nModel-Aligned Coupling (MAC), an effective method that matches training\ncouplings based not only on geometric distance but also on alignment with the\nmodel's preferred transport directions based on its prediction error. To avoid\nthe time-costly match process, MAC proposes to select the top-$k$ fraction of\ncouplings with the lowest error for training. Extensive experiments show that\nMAC significantly improves generation quality and efficiency in few-step\nsettings compared to existing methods. Project page:\nhttps://yexionglin.github.io/mac", "AI": {"tldr": "Flow Matching (FM) \u6846\u67b6\u901a\u8fc7\u4f18\u5316\u8026\u5408\u8def\u5f84\uff0c\u63d0\u51fa Model-Aligned Coupling (MAC) \u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf FM \u65b9\u6cd5\u4f7f\u7528\u968f\u673a\u8026\u5408\u5bfc\u81f4\u8def\u5f84\u4ea4\u53c9\uff0c\u800c\u57fa\u4e8e\u51e0\u4f55\u8ddd\u79bb\u7684 OT \u65b9\u6cd5\u672a\u80fd\u4e0e\u6a21\u578b\u504f\u597d\u5bf9\u9f50\uff0cMAC \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MAC \u7ed3\u5408\u51e0\u4f55\u8ddd\u79bb\u548c\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\uff0c\u9009\u62e9\u8bef\u5dee\u6700\u5c0f\u7684\u8026\u5408\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u8017\u65f6\u5339\u914d\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAC \u5728\u5c11\u6b65\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MAC \u901a\u8fc7\u6a21\u578b\u5bf9\u9f50\u7684\u8026\u5408\u4f18\u5316\uff0c\u63d0\u5347\u4e86 FM \u6846\u67b6\u7684\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2505.23404", "pdf": "https://arxiv.org/pdf/2505.23404", "abs": "https://arxiv.org/abs/2505.23404", "authors": ["Mingyu Yu", "Wei Wang", "Yanjie Wei", "Sujuan Qin"], "title": "Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Adversarial attacks on Large Language Models (LLMs) via jailbreaking\ntechniques-methods that circumvent their built-in safety and ethical\nconstraints-have emerged as a critical challenge in AI security. These attacks\ncompromise the reliability of LLMs by exploiting inherent weaknesses in their\ncomprehension capabilities. This paper investigates the efficacy of\njailbreaking strategies that are specifically adapted to the diverse levels of\nunderstanding exhibited by different LLMs. We propose the Adaptive Jailbreaking\nStrategies Based on the Semantic Understanding Capabilities of Large Language\nModels, a novel framework that classifies LLMs into Type I and Type II\ncategories according to their semantic comprehension abilities. For each\ncategory, we design tailored jailbreaking strategies aimed at leveraging their\nvulnerabilities to facilitate successful attacks. Extensive experiments\nconducted on multiple LLMs demonstrate that our adaptive strategy markedly\nimproves the success rate of jailbreaking. Notably, our approach achieves an\nexceptional 98.9% success rate in jailbreaking GPT-4o(29 May 2025 release)", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u7684\u81ea\u9002\u5e94\u8d8a\u72f1\u7b56\u7565\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u8d8a\u72f1\u653b\u51fb\u901a\u8fc7\u7ed5\u8fc7LLMs\u7684\u5b89\u5168\u548c\u4f26\u7406\u7ea6\u675f\uff0c\u6210\u4e3aAI\u5b89\u5168\u7684\u5173\u952e\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u63a2\u7d22\u9488\u5bf9\u4e0d\u540cLLMs\u7406\u89e3\u80fd\u529b\u7684\u81ea\u9002\u5e94\u653b\u51fb\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6846\u67b6\uff0c\u5c06LLMs\u5206\u4e3aType I\u548cType II\uff0c\u5e76\u6839\u636e\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u8bbe\u8ba1\u5b9a\u5236\u5316\u7684\u8d8a\u72f1\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728GPT-4o\u4e0a\u8fbe\u5230\u4e8698.9%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLMs\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u9002\u5e94\u7b56\u7565\u5728\u653b\u51fb\u4e2d\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.23358", "pdf": "https://arxiv.org/pdf/2505.23358", "abs": "https://arxiv.org/abs/2505.23358", "authors": ["Reem AlJunaid", "Muzammil Behzad"], "title": "Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Generating informative and knowledge-rich image captions remains a challenge\nfor many existing captioning models, which often produce generic descriptions\nthat lack specificity and contextual depth. To address this limitation, we\npropose KRCapVLM, a knowledge replay-based novel image captioning framework\nusing vision-language model. We incorporate beam search decoding to generate\nmore diverse and coherent captions. We also integrate attention-based modules\ninto the image encoder to enhance feature representation. Finally, we employ\ntraining schedulers to improve stability and ensure smoother convergence during\ntraining. These proposals accelerate substantial gains in both caption quality\nand knowledge recognition. Our proposed model demonstrates clear improvements\nin both the accuracy of knowledge recognition and the overall quality of\ngenerated captions. It shows a stronger ability to generalize to previously\nunseen knowledge concepts, producing more informative and contextually relevant\ndescriptions. These results indicate the effectiveness of our approach in\nenhancing the model's capacity to generate meaningful, knowledge-grounded\ncaptions across a range of scenarios.", "AI": {"tldr": "KRCapVLM\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u91cd\u653e\u7684\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6ce2\u675f\u641c\u7d22\u89e3\u7801\u548c\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63cf\u8ff0\u7684\u8d28\u91cf\u548c\u77e5\u8bc6\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u751f\u6210\u7684\u63cf\u8ff0\u901a\u5e38\u7f3a\u4e4f\u5177\u4f53\u6027\u548c\u4e0a\u4e0b\u6587\u6df1\u5ea6\uff0cKRCapVLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6ce2\u675f\u641c\u7d22\u89e3\u7801\u3001\u6ce8\u610f\u529b\u6a21\u5757\u548c\u8bad\u7ec3\u8c03\u5ea6\u5668\uff0c\u63d0\u5347\u7279\u5f81\u8868\u793a\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u6a21\u578b\u5728\u77e5\u8bc6\u8bc6\u522b\u51c6\u786e\u6027\u548c\u63cf\u8ff0\u8d28\u91cf\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5177\u4fe1\u606f\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u63cf\u8ff0\u3002", "conclusion": "KRCapVLM\u6709\u6548\u589e\u5f3a\u4e86\u6a21\u578b\u751f\u6210\u6709\u610f\u4e49\u3001\u57fa\u4e8e\u77e5\u8bc6\u7684\u56fe\u50cf\u63cf\u8ff0\u7684\u80fd\u529b\u3002"}}
{"id": "2505.23410", "pdf": "https://arxiv.org/pdf/2505.23410", "abs": "https://arxiv.org/abs/2505.23410", "authors": ["Xuan Gong", "Hanbo Huang", "Shiyu Liang"], "title": "From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs", "categories": ["cs.CL"], "comment": "The code of this paper will be released soon", "summary": "Factual knowledge extraction aims to explicitly extract knowledge\nparameterized in pre-trained language models for application in downstream\ntasks. While prior work has been investigating the impact of supervised\nfine-tuning data on the factuality of large language models (LLMs), its\nmechanism remains poorly understood. We revisit this impact through systematic\nexperiments, with a particular focus on the factuality gap that arises when\nfine-tuning on known versus unknown knowledge. Our findings show that this gap\ncan be mitigated at the inference stage, either under out-of-distribution (OOD)\nsettings or by using appropriate in-context learning (ICL) prompts (i.e.,\nfew-shot learning and Chain of Thought (CoT)). We prove this phenomenon\ntheoretically from the perspective of knowledge graphs, showing that the\ntest-time prompt may diminish or even overshadow the impact of fine-tuning data\nand play a dominant role in knowledge extraction. Ultimately, our results shed\nlight on the interaction between finetuning data and test-time prompt,\ndemonstrating that ICL can effectively compensate for shortcomings in\nfine-tuning data, and highlighting the need to reconsider the use of ICL\nprompting as a means to evaluate the effectiveness of fine-tuning data\nselection methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e8b\u5b9e\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5fae\u8c03\u6570\u636e\u4e0e\u6d4b\u8bd5\u65f6\u63d0\u793a\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8bc1\u660e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u53ef\u4ee5\u5f25\u8865\u5fae\u8c03\u6570\u636e\u7684\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5bf9LLMs\u4e8b\u5b9e\u6027\u5f71\u54cd\u7684\u673a\u5236\uff0c\u5c24\u5176\u662f\u5df2\u77e5\u4e0e\u672a\u77e5\u77e5\u8bc6\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5206\u6790\u5fae\u8c03\u6570\u636e\u4e0e\u6d4b\u8bd5\u65f6\u63d0\u793a\uff08\u5982\u5c11\u6837\u672c\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\uff09\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u4ece\u77e5\u8bc6\u56fe\u8c31\u89d2\u5ea6\u8fdb\u884c\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u53d1\u73b0\u6d4b\u8bd5\u65f6\u63d0\u793a\u53ef\u4ee5\u51cf\u8f7b\u5fae\u8c03\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u751a\u81f3\u4e3b\u5bfc\u77e5\u8bc6\u63d0\u53d6\u8fc7\u7a0b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u80fd\u6709\u6548\u5f25\u8865\u5fae\u8c03\u6570\u636e\u7684\u7f3a\u9677\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u5176\u5728\u5fae\u8c03\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2505.23359", "pdf": "https://arxiv.org/pdf/2505.23359", "abs": "https://arxiv.org/abs/2505.23359", "authors": ["Yuanxin Liu", "Kun Ouyang", "Haoning Wu", "Yi Liu", "Lin Sui", "Xinhao Li", "Yan Zhong", "Y. Charles", "Xinyu Zhou", "Xu Sun"], "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?", "categories": ["cs.CV"], "comment": "Project Page: https://llyx97.github.io/video_reason_bench/", "summary": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVideoReasonBench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u89c6\u89c9\u4e2d\u5fc3\u590d\u6742\u89c6\u9891\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u7f3a\u4e4f\u89c6\u89c9\u6df1\u5ea6\u548c\u63a8\u7406\u590d\u6742\u6027\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u8db3\u591f\u7684\u63a8\u7406\u6df1\u5ea6\uff0c\u65e0\u6cd5\u5c55\u793a\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u7684\u4f18\u52bf\uff0c\u4e14\u4efb\u52a1\u591a\u4e3a\u77e5\u8bc6\u9a71\u52a8\u800c\u975e\u89c6\u89c9\u5185\u5bb9\u9a71\u52a8\u3002", "method": "\u8bbe\u8ba1VideoReasonBench\uff0c\u5305\u542b\u89c6\u89c9\u4e30\u5bcc\u4e14\u9ad8\u590d\u6742\u5ea6\u7684\u89c6\u9891\uff0c\u8bc4\u4f30\u4e09\u4e2a\u9012\u8fdb\u5c42\u6b21\u7684\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff1a\u89c6\u89c9\u4fe1\u606f\u56de\u5fc6\u3001\u6f5c\u5728\u72b6\u6001\u63a8\u65ad\u548c\u89c6\u9891\u5916\u4fe1\u606f\u9884\u6d4b\u3002", "result": "\u8bc4\u4f3018\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u53d1\u73b0\u5927\u591a\u6570\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5982GPT-4o\u51c6\u786e\u7387\u4ec56.9%\uff09\uff0c\u800cGemini-2.5-Pro\u4ee556.0%\u51c6\u786e\u7387\u663e\u8457\u9886\u5148\u3002", "conclusion": "\u6269\u5c55\u601d\u7ef4\u9884\u7b97\u5bf9VideoReasonBench\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5728\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2505.23420", "pdf": "https://arxiv.org/pdf/2505.23420", "abs": "https://arxiv.org/abs/2505.23420", "authors": ["Marco Gaido", "Sara Papi", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "title": "The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence", "categories": ["cs.CL"], "comment": "Accepted to IWSLT 2025", "summary": "Training large-scale models presents challenges not only in terms of resource\nrequirements but also in terms of their convergence. For this reason, the\nlearning rate (LR) is often decreased when the size of a model is increased.\nSuch a simple solution is not enough in the case of speech-to-text (S2T)\ntrainings, where evolved and more complex variants of the Transformer\narchitecture -- e.g., Conformer or Branchformer -- are used in light of their\nbetter performance. As a workaround, OWSM designed a double linear warmup of\nthe LR, increasing it to a very small value in the first phase before updating\nit to a higher value in the second phase. While this solution worked well in\npractice, it was not compared with alternative solutions, nor was the impact on\nthe final performance of different LR warmup schedules studied. This paper\nfills this gap, revealing that i) large-scale S2T trainings demand a\nsub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates\ninitial convergence, but it does not boost final performance.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86\u5927\u89c4\u6a21\u8bed\u97f3\u5230\u6587\u672c\uff08S2T\uff09\u8bad\u7ec3\u4e2d\u5b66\u4e60\u7387\uff08LR\uff09\u9884\u70ed\u8c03\u4f18\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u53d1\u73b0\u5b50\u6307\u6570\u9884\u70ed\u548c\u521d\u59cb\u9ad8\u5b66\u4e60\u7387\u5bf9\u6700\u7ec8\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u89c4\u6a21S2T\u8bad\u7ec3\u4e2d\uff0c\u4f20\u7edf\u5b66\u4e60\u7387\u8c03\u6574\u65b9\u6cd5\u4e0d\u8db3\uff0c\u9700\u7814\u7a76\u66f4\u4f18\u7684\u9884\u70ed\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e0d\u540c\u5b66\u4e60\u7387\u9884\u70ed\u8c03\u5ea6\uff0c\u5305\u62ec\u53cc\u7ebf\u6027\u9884\u70ed\u548c\u5b50\u6307\u6570\u9884\u70ed\u3002", "result": "\u5b50\u6307\u6570\u9884\u70ed\u66f4\u9002\u5408\u5927\u89c4\u6a21S2T\u8bad\u7ec3\uff1b\u521d\u59cb\u9ad8\u5b66\u4e60\u7387\u52a0\u901f\u6536\u655b\u4f46\u4e0d\u63d0\u5347\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u5927\u89c4\u6a21S2T\u8bad\u7ec3\u9700\u91c7\u7528\u5b50\u6307\u6570\u5b66\u4e60\u7387\u9884\u70ed\u7b56\u7565\uff0c\u521d\u59cb\u9ad8\u5b66\u4e60\u7387\u4ec5\u5bf9\u6536\u655b\u901f\u5ea6\u6709\u76ca\u3002"}}
{"id": "2505.23365", "pdf": "https://arxiv.org/pdf/2505.23365", "abs": "https://arxiv.org/abs/2505.23365", "authors": ["Yang Qiao", "Xiaoyu Zhong", "Xiaofeng Gu", "Zhiguo Yu"], "title": "MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal information processing has become increasingly important for\nenhancing image classification performance. However, the intricate and implicit\ndependencies across different modalities often hinder conventional methods from\neffectively capturing fine-grained semantic interactions, thereby limiting\ntheir applicability in high-precision classification tasks. To address this\nissue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet)\ndesigned for fine-grained classification. The proposed MCFNet architecture\nincorporates a regularized integrated fusion module that improves intra-modal\nfeature representation through modality-specific regularization strategies,\nwhile facilitating precise semantic alignment via a hybrid attention mechanism.\nAdditionally, we introduce a multimodal decision classification module, which\njointly exploits inter-modal correlations and unimodal discriminative features\nby integrating multiple loss functions within a weighted voting paradigm.\nExtensive experiments and ablation studies on benchmark datasets demonstrate\nthat the proposed MCFNet framework achieves consistent improvements in\nclassification accuracy, confirming its effectiveness in modeling subtle\ncross-modal semantics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u534f\u4f5c\u878d\u5408\u7f51\u7edc\uff08MCFNet\uff09\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u6b63\u5219\u5316\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u5bf9\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4ea4\u4e92\u3002", "method": "MCFNet\u5305\u542b\u6b63\u5219\u5316\u878d\u5408\u6a21\u5757\u548c\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u51b3\u7b56\u5206\u7c7b\u6a21\u5757\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMCFNet\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "MCFNet\u80fd\u6709\u6548\u5efa\u6a21\u8de8\u6a21\u6001\u7684\u7ec6\u5fae\u8bed\u4e49\u5173\u7cfb\u3002"}}
{"id": "2505.23461", "pdf": "https://arxiv.org/pdf/2505.23461", "abs": "https://arxiv.org/abs/2505.23461", "authors": ["Chuanyuan Tan", "Wenbiao Shao", "Hao Xiong", "Tong Zhu", "Zhenhua Liu", "Kai Shi", "Wenliang Chen"], "title": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps\nprevent misleading responses in complex situations. While previous studies have\nbuilt several datasets to assess LLMs' performance on UAQ, these datasets lack\nfactual knowledge support, which limits the evaluation of LLMs' ability to\nutilize their factual knowledge when handling UAQ. To address the limitation,\nwe introduce a new unanswerable question dataset UAQFact, a bilingual dataset\nwith auxiliary factual knowledge created from a Knowledge Graph. Based on\nUAQFact, we further define two new tasks to measure LLMs' ability to utilize\ninternal and external factual knowledge, respectively. Our experimental results\nacross multiple LLM series show that UAQFact presents significant challenges,\nas LLMs do not consistently perform well even when they have factual knowledge\nstored. Additionally, we find that incorporating external knowledge may enhance\nperformance, but LLMs still cannot make full use of the knowledge which may\nresult in incorrect responses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u6570\u636e\u96c6UAQFact\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u5229\u7528\u4e8b\u5b9e\u77e5\u8bc6\u5904\u7406\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u65f6\u7684\u80fd\u529b\uff0c\u5e76\u53d1\u73b0LLMs\u5728\u6b64\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e8b\u5b9e\u77e5\u8bc6\u652f\u6301\uff0c\u9650\u5236\u4e86\u8bc4\u4f30LLMs\u5229\u7528\u4e8b\u5b9e\u77e5\u8bc6\u5904\u7406\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u53cc\u8bed\u6570\u636e\u96c6UAQFact\uff0c\u5e76\u5b9a\u4e49\u4e24\u4e2a\u65b0\u4efb\u52a1\u5206\u522b\u8bc4\u4f30LLMs\u5229\u7528\u5185\u90e8\u548c\u5916\u90e8\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728UAQFact\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u5b58\u50a8\u4e86\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u4e5f\u65e0\u6cd5\u5145\u5206\u5229\u7528\u3002", "conclusion": "\u5916\u90e8\u77e5\u8bc6\u53ef\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46LLMs\u4ecd\u96be\u4ee5\u5145\u5206\u5229\u7528\uff0c\u5bfc\u81f4\u9519\u8bef\u56de\u7b54\u3002"}}
{"id": "2505.23367", "pdf": "https://arxiv.org/pdf/2505.23367", "abs": "https://arxiv.org/abs/2505.23367", "authors": ["Jeonghyeok Do", "Sungpyo Kim", "Geunhyuk Youk", "Jaehyup Lee", "Munchurl Kim"], "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening", "categories": ["cs.CV", "cs.AI"], "comment": "Please visit our project page\n  https://kaist-viclab.github.io/PAN-Crafter_site", "summary": "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with\nlow-resolution multi-spectral (MS) images to generate high-resolution\nmulti-spectral (HRMS) outputs. However, cross-modality misalignment -- caused\nby sensor placement, acquisition timing, and resolution disparity -- induces a\nfundamental challenge. Conventional deep learning methods assume perfect\npixel-wise alignment and rely on per-pixel reconstruction losses, leading to\nspectral distortion, double edges, and blurring when misalignment is present.\nTo address this, we propose PAN-Crafter, a modality-consistent alignment\nframework that explicitly mitigates the misalignment gap between PAN and MS\nmodalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a\nsingle network to jointly reconstruct HRMS and PAN images, leveraging PAN's\nhigh-frequency details as auxiliary self-supervision. Additionally, we\nintroduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism\nthat bidirectionally aligns MS texture to PAN structure and vice versa,\nenabling adaptive feature refinement across modalities. Extensive experiments\non multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the\nmost recent state-of-the-art method in all metrics, even with 50.11$\\times$\nfaster inference time and 0.63$\\times$ the memory size. Furthermore, it\ndemonstrates strong generalization performance on unseen satellite datasets,\nshowing its robustness across different conditions.", "AI": {"tldr": "PAN-Crafter\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3PAN\u548cMS\u56fe\u50cf\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u81ea\u9002\u5e94\u91cd\u5efa\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3PAN\u548cMS\u56fe\u50cf\u56e0\u4f20\u611f\u5668\u653e\u7f6e\u3001\u91c7\u96c6\u65f6\u95f4\u548c\u5206\u8fa8\u7387\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u56e0\u5047\u8bbe\u5b8c\u7f8e\u5bf9\u9f50\u800c\u5bfc\u81f4\u7684\u9891\u8c31\u5931\u771f\u548c\u6a21\u7cca\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u81ea\u9002\u5e94\u91cd\u5efa\uff08MARs\uff09\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6ce8\u610f\u529b\u673a\u5236\uff08CM3A\uff09\uff0c\u8054\u5408\u91cd\u5efaHRMS\u548cPAN\u56fe\u50cf\uff0c\u5e76\u53cc\u5411\u5bf9\u9f50\u7eb9\u7406\u4e0e\u7ed3\u6784\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u65f6\u95f4\u5feb50.11\u500d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c110.63\u500d\uff0c\u4e14\u5728\u672a\u89c1\u8fc7\u7684\u536b\u661f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PAN-Crafter\u901a\u8fc7\u663e\u5f0f\u89e3\u51b3\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u878d\u5408\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.23477", "pdf": "https://arxiv.org/pdf/2505.23477", "abs": "https://arxiv.org/abs/2505.23477", "authors": ["Krithik Vishwanath", "Anton Alyakin", "Mrigayu Ghosh", "Jin Vivian Lee", "Daniel Alexander Alber", "Karl L. Sangwon", "Douglas Kondziolka", "Eric Karl Oermann"], "title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons", "categories": ["cs.CL"], "comment": "22 pages, 3 main figures, 3 supplemental figures", "summary": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8628\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u90e8\u5206\u6a21\u578b\u80fd\u901a\u8fc7\u8003\u8bd5\uff0c\u4f46\u6027\u80fd\u6613\u53d7\u5e72\u6270\u4fe1\u606f\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5916\u79d1\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u5e72\u6270\u4fe1\u606f\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u75282,904\u9053\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u9898\u6d4b\u8bd528\u4e2a\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u5e72\u6270\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u8106\u5f31\u6027\u3002", "result": "6\u4e2a\u6a21\u578b\u901a\u8fc7\u8003\u8bd5\uff0c\u4f46\u5e72\u6270\u4fe1\u606f\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u964d\u5e45\u8fbe20.4%\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u795e\u7ecf\u5916\u79d1\u8003\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5e72\u6270\u4fe1\u606f\u654f\u611f\uff0c\u9700\u5f00\u53d1\u65b0\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.23380", "pdf": "https://arxiv.org/pdf/2505.23380", "abs": "https://arxiv.org/abs/2505.23380", "authors": ["Weijia Mao", "Zhenheng Yang", "Mike Zheng Shou"], "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.", "AI": {"tldr": "UniRL\u662f\u4e00\u79cd\u81ea\u6539\u8fdb\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u56fe\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u8eab\u751f\u6210\u56fe\u50cf\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5229\u7528\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u7684\u76f8\u4e92\u589e\u5f3a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u4e14\u540e\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5916\u90e8\u6570\u636e\u6216\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u3002UniRL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u751f\u6210\u56fe\u50cf\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548cGroup Relative Policy Optimization\uff08GRPO\uff09\u4f18\u5316\u6a21\u578b\uff0c\u5b9e\u73b0\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5728Show-o\u548cJanus\u4e0a\u8bc4\u4f30\uff0cUniRL\u5206\u522b\u83b7\u5f970.77\u548c0.65\u7684GenEval\u5206\u6570\u3002", "conclusion": "UniRL\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u5e76\u51cf\u5c11\u4efb\u52a1\u95f4\u4e0d\u5e73\u8861\uff0c\u4e14\u540e\u8bad\u7ec3\u9636\u6bb5\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u6b65\u9aa4\u3002"}}
{"id": "2505.23480", "pdf": "https://arxiv.org/pdf/2505.23480", "abs": "https://arxiv.org/abs/2505.23480", "authors": ["Keqin Peng", "Liang Ding", "Yuanxin Ouyang", "Meng Fang", "Dacheng Tao"], "title": "Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning Large Language Models (RLLMs) have demonstrated impressive\nperformance on complex tasks, largely due to the adoption of Long\nChain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking\n-- performing unnecessary reasoning steps even after arriving at the correct\nanswer. Prior work has largely focused on qualitative analyses of overthinking\nthrough sample-based observations of long CoTs. In contrast, we present a\nquantitative analysis of overthinking from the perspective of self-doubt,\ncharacterized by excessive token usage devoted to re-verifying already-correct\nanswer. We find that self-doubt significantly contributes to overthinking. In\nresponse, we introduce a simple and effective prompting method to reduce the\nmodel's over-reliance on input questions, thereby avoiding self-doubt.\nSpecifically, we first prompt the model to question the validity of the input\nquestion, and then respond concisely based on the outcome of that evaluation.\nExperiments on three mathematical reasoning tasks and four datasets with\nmissing premises demonstrate that our method substantially reduces answer\nlength and yields significant improvements across nearly all datasets upon 4\nwidely-used RLLMs. Further analysis demonstrates that our method effectively\nminimizes the number of reasoning steps and reduces self-doubt.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u91cf\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08RLLMs\uff09\u5728\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u8fc7\u5ea6\u601d\u8003\uff08overthinking\uff09\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u81ea\u6211\u6000\u7591\uff08self-doubt\uff09\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8fc7\u5ea6\u4f9d\u8d56\u8f93\u5165\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u5b9a\u6027\u5206\u6790\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u7f3a\u4e4f\u5b9a\u91cf\u7814\u7a76\u3002\u672c\u6587\u4ece\u81ea\u6211\u6000\u7591\u7684\u89d2\u5ea6\u5b9a\u91cf\u5206\u6790\u8fc7\u5ea6\u601d\u8003\uff0c\u5e76\u63a2\u7d22\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u63d0\u793a\u65b9\u6cd5\uff0c\u5148\u8ba9\u6a21\u578b\u8d28\u7591\u8f93\u5165\u95ee\u9898\u7684\u6709\u6548\u6027\uff0c\u518d\u6839\u636e\u8bc4\u4f30\u7ed3\u679c\u7b80\u6d01\u56de\u7b54\u3002\u5b9e\u9a8c\u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u548c\u56db\u4e2a\u7f3a\u5931\u524d\u63d0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7b54\u6848\u957f\u5ea6\u548c\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684RLLMs\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u81ea\u6211\u6000\u7591\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u81ea\u6211\u6000\u7591\u5bf9\u8fc7\u5ea6\u601d\u8003\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8868\u73b0\u3002"}}
{"id": "2505.23386", "pdf": "https://arxiv.org/pdf/2505.23386", "abs": "https://arxiv.org/abs/2505.23386", "authors": ["Han Bao", "Qinying Wang", "Zhi Chen", "Qingming Li", "Xuhong Zhang", "Changjiang Li", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "VModA: An Effective Framework for Adaptive NSFW Image Moderation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Not Safe/Suitable for Work (NSFW) content is rampant on social networks and\nposes serious harm to citizens, especially minors. Current detection methods\nmainly rely on deep learning-based image recognition and classification.\nHowever, NSFW images are now presented in increasingly sophisticated ways,\noften using image details and complex semantics to obscure their true nature or\nattract more views. Although still understandable to humans, these images often\nevade existing detection methods, posing a significant threat. Further\ncomplicating the issue, varying regulations across platforms and regions create\nadditional challenges for effective moderation, leading to detection bias and\nreduced accuracy. To address this, we propose VModA, a general and effective\nframework that adapts to diverse moderation rules and handles complex,\nsemantically rich NSFW content across categories. Experimental results show\nthat VModA significantly outperforms existing methods, achieving up to a 54.3%\naccuracy improvement across NSFW types, including those with complex semantics.\nFurther experiments demonstrate that our method exhibits strong adaptability\nacross categories, scenarios, and base VLMs. We also identified inconsistent\nand controversial label samples in public NSFW benchmark datasets, re-annotated\nthem, and submitted corrections to the original maintainers. Two datasets have\nconfirmed the updates so far. Additionally, we evaluate VModA in real-world\nscenarios to demonstrate its practical effectiveness.", "AI": {"tldr": "VModA\u662f\u4e00\u4e2a\u901a\u7528\u7684NSFW\u5185\u5bb9\u68c0\u6d4b\u6846\u67b6\uff0c\u9002\u5e94\u591a\u6837\u5316\u89c4\u5219\u5e76\u5904\u7406\u590d\u6742\u8bed\u4e49\u5185\u5bb9\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "NSFW\u5185\u5bb9\u5728\u793e\u4ea4\u7f51\u7edc\u4e0a\u6cdb\u6ee5\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u8bed\u4e49\u548c\u591a\u6837\u5316\u89c4\u5219\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faVModA\u6846\u67b6\uff0c\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u548c\u5730\u533a\u7684\u89c4\u5219\uff0c\u5904\u7406\u590d\u6742\u8bed\u4e49\u7684NSFW\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u663e\u793aVModA\u5728\u5404\u7c7bNSFW\u5185\u5bb9\u4e0a\u51c6\u786e\u6027\u63d0\u534754.3%\uff0c\u5e76\u5c55\u793a\u51fa\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "VModA\u6709\u6548\u89e3\u51b3NSFW\u68c0\u6d4b\u4e2d\u7684\u590d\u6742\u8bed\u4e49\u548c\u89c4\u5219\u591a\u6837\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.23494", "pdf": "https://arxiv.org/pdf/2505.23494", "abs": "https://arxiv.org/abs/2505.23494", "authors": ["Nicol Visser", "Herman Kamper"], "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Spoken language models (SLMs) operate on acoustic units obtained by\ndiscretizing self-supervised speech representations. Although the\ncharacteristics of these units directly affect performance, the interaction\nbetween codebook size and unit coarseness (i.e., duration) remains unexplored.\nWe investigate SLM performance as we vary codebook size and unit coarseness\nusing the simple duration-penalized dynamic programming (DPDP) method. New\nanalyses are performed across different linguistic levels. At the phone and\nword levels, coarseness provides little benefit, as long as the codebook size\nis chosen appropriately. However, when producing whole sentences in a\nresynthesis task, SLMs perform better with coarser units. In lexical and\nsyntactic language modeling tasks, coarser units also give higher accuracies at\nlower bitrates. We therefore show that coarser units aren't always better, but\nthat DPDP is a simple and efficient way to obtain coarser units for the tasks\nwhere they are beneficial.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e2d\u7801\u672c\u5927\u5c0f\u548c\u5355\u5143\u7c97\u7cd9\u5ea6\uff08\u6301\u7eed\u65f6\u95f4\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u7c97\u7cd9\u5ea6\u5728\u53e5\u5b50\u91cd\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u97f3\u7d20\u548c\u5355\u8bcd\u7ea7\u522b\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u63a2\u7d22\u7801\u672c\u5927\u5c0f\u548c\u5355\u5143\u7c97\u7cd9\u5ea6\u5bf9SLM\u6027\u80fd\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u586b\u8865\u8fd9\u4e00\u672a\u7814\u7a76\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7b80\u5355\u7684\u6301\u7eed\u65f6\u95f4\u60e9\u7f5a\u52a8\u6001\u89c4\u5212\uff08DPDP\uff09\u65b9\u6cd5\uff0c\u8c03\u6574\u7801\u672c\u5927\u5c0f\u548c\u5355\u5143\u7c97\u7cd9\u5ea6\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u7ea7\u522b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u97f3\u7d20\u548c\u5355\u8bcd\u7ea7\u522b\uff0c\u7c97\u7cd9\u5ea6\u5f71\u54cd\u4e0d\u5927\uff1b\u4f46\u5728\u53e5\u5b50\u91cd\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u7c97\u7cd9\u5355\u5143\u8868\u73b0\u66f4\u597d\u3002\u8bcd\u6c47\u548c\u53e5\u6cd5\u4efb\u52a1\u4e2d\uff0c\u7c97\u7cd9\u5355\u5143\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u7c97\u7cd9\u5355\u5143\u5e76\u975e\u603b\u662f\u66f4\u597d\uff0c\u4f46DPDP\u662f\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9700\u8981\u7c97\u7cd9\u5355\u5143\u7684\u4efb\u52a1\u3002"}}
{"id": "2505.23392", "pdf": "https://arxiv.org/pdf/2505.23392", "abs": "https://arxiv.org/abs/2505.23392", "authors": ["Yun-Cheng Tsai"], "title": "Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv11n\u68c0\u6d4b\u5668\u548cFUSegNet\u5206\u5272\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u7ba1\u9053\uff0c\u4ec5\u9700500\u4e2a\u6807\u6ce8\u6846\u5373\u53ef\u5b9e\u73b0\u8de8\u8eab\u4f53\u90e8\u4f4d\u4f24\u53e3\u5206\u5272\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\uff08\u5982FUSegNet\uff09\u5728\u975e\u8db3\u90e8\u4f24\u53e3\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408YOLOv11n\u68c0\u6d4b\u5668\u548c\u9884\u8bad\u7ec3FUSegNet\uff0c\u4ec5\u4f7f\u7528500\u4e2a\u6807\u6ce8\u6846\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u6216\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u79cd\u4f24\u53e3\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5e73\u5747IoU\u63d0\u534723\u4e2a\u767e\u5206\u70b9\uff0cDESIGN-R\u5c3a\u5bf8\u4f30\u8ba1\u51c6\u786e\u7387\u4ece71%\u63d0\u5347\u81f394%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8de8\u8eab\u4f53\u90e8\u4f4d\u7684\u9ad8\u6548\u4f24\u53e3\u5206\u5272\uff0c\u4e3a\u4e34\u5e8a\u81ea\u52a8\u5316\u8bc4\u5206\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u6a21\u578b\u6743\u91cd\u4ee5\u4fc3\u8fdb\u5e94\u7528\u3002"}}
{"id": "2505.23495", "pdf": "https://arxiv.org/pdf/2505.23495", "abs": "https://arxiv.org/abs/2505.23495", "authors": ["Liangliang Zhang", "Zhuorui Jiang", "Hongliang Chi", "Haoyang Chen", "Mohammed Elkoumy", "Fali Wang", "Qiong Wu", "Zhengyi Zhou", "Shirui Pan", "Suhang Wang", "Yao Ma"], "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.", "AI": {"tldr": "KGQAGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8df3\u63a8\u7406\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709KGQA\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982WebQSP\u548cCWQ\uff09\u5b58\u5728\u6807\u6ce8\u9519\u8bef\u3001\u95ee\u9898\u6a21\u7cca\u6216\u4e0d\u53ef\u56de\u7b54\u3001\u77e5\u8bc6\u8fc7\u65f6\u7b49\u95ee\u9898\uff0c\u5e73\u5747\u4e8b\u5b9e\u6b63\u786e\u7387\u4ec5\u4e3a57%\u3002", "method": "KGQAGen\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u57fa\u7840\u3001LLM\u5f15\u5bfc\u751f\u6210\u548c\u7b26\u53f7\u9a8c\u8bc1\uff0c\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u4e14\u53ef\u9a8c\u8bc1\u7684QA\u5b9e\u4f8b\u3002", "result": "\u6784\u5efa\u4e86KGQAGen-10k\u57fa\u51c6\uff0c\u5b9e\u9a8c\u8868\u660e\u5373\u4f7f\u662fSOTA\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "KGQAGen\u4e3aKGQA\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5e76\u547c\u5401\u66f4\u4e25\u683c\u7684\u57fa\u51c6\u6784\u5efa\u3002"}}
{"id": "2505.23395", "pdf": "https://arxiv.org/pdf/2505.23395", "abs": "https://arxiv.org/abs/2505.23395", "authors": ["Xingguang Wei", "Haomin Wang", "Shenglong Ye", "Ruifeng Luo", "Yanting Zhang", "Lixin Gu", "Jifeng Dai", "Yu Qiao", "Wenhai Wang", "Hongjie Zhang"], "title": "Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings", "categories": ["cs.CV"], "comment": null, "summary": "We study the task of panoptic symbol spotting, which involves identifying\nboth individual instances of countable things and the semantic regions of\nuncountable stuff in computer-aided design (CAD) drawings composed of vector\ngraphical primitives. Existing methods typically rely on image rasterization,\ngraph construction, or point-based representation, but these approaches often\nsuffer from high computational costs, limited generality, and loss of geometric\nstructural information. In this paper, we propose VecFormer, a novel method\nthat addresses these challenges through line-based representation of\nprimitives. This design preserves the geometric continuity of the original\nprimitive, enabling more accurate shape representation while maintaining a\ncomputation-friendly structure, making it well-suited for vector graphic\nunderstanding tasks. To further enhance prediction reliability, we introduce a\nBranch Fusion Refinement module that effectively integrates instance and\nsemantic predictions, resolving their inconsistencies for more coherent\npanoptic outputs. Extensive experiments demonstrate that our method establishes\na new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and\n21.2 points over the second-best results under settings with and without prior\ninformation, respectively, highlighting the strong potential of line-based\nrepresentation as a foundation for vector graphic understanding.", "AI": {"tldr": "VecFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u8868\u793a\u7684CAD\u56fe\u7eb8\u5168\u666f\u7b26\u53f7\u8bc6\u522b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u6cdb\u5316\u6027\u5dee\u548c\u51e0\u4f55\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728CAD\u56fe\u7eb8\u7684\u5168\u666f\u7b26\u53f7\u8bc6\u522b\u4e2d\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u6cdb\u5316\u6027\u5dee\u548c\u51e0\u4f55\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\u3002", "method": "VecFormer\u901a\u8fc7\u7ebf\u8868\u793a\u539f\u59cb\u56fe\u5143\uff0c\u4fdd\u7559\u51e0\u4f55\u8fde\u7eed\u6027\uff0c\u5e76\u5f15\u5165\u5206\u652f\u878d\u5408\u7ec6\u5316\u6a21\u5757\u6574\u5408\u5b9e\u4f8b\u4e0e\u8bed\u4e49\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVecFormer\u5728PQ\u6307\u6807\u4e0a\u8fbe\u523091.1\uff0cStuff-PQ\u5206\u522b\u63d0\u53479.6\u548c21.2\u5206\u3002", "conclusion": "\u7ebf\u8868\u793a\u662f\u77e2\u91cf\u56fe\u5f62\u7406\u89e3\u7684\u6709\u6548\u57fa\u7840\uff0cVecFormer\u4e3aCAD\u56fe\u7eb8\u7684\u5168\u666f\u7b26\u53f7\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23538", "pdf": "https://arxiv.org/pdf/2505.23538", "abs": "https://arxiv.org/abs/2505.23538", "authors": ["Nawar Turk", "Eeham Khan", "Leila Kosseim"], "title": "CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to SemEval-2025 Task 6 (ACL 2025)", "summary": "This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),\nwhich focuses on verifying promises in corporate ESG (Environmental, Social,\nand Governance) reports. We explore three model architectures to address the\nfour subtasks of promise identification, supporting evidence assessment,\nclarity evaluation, and verification timing. Our first model utilizes ESG-BERT\nwith task-specific classifier heads, while our second model enhances this\narchitecture with linguistic features tailored for each subtask. Our third\napproach implements a combined subtask model with attention-based sequence\npooling, transformer representations augmented with document metadata, and\nmulti-objective learning. Experiments on the English portion of the ML-Promise\ndataset demonstrate progressive improvement across our models, with our\ncombined subtask approach achieving a leaderboard score of 0.5268,\noutperforming the provided baseline of 0.5227. Our work highlights the\neffectiveness of linguistic feature extraction, attention pooling, and\nmulti-objective learning in promise verification tasks, despite challenges\nposed by class imbalance and limited training data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9488\u5bf9SemEval-2025 Task 6\uff08PromiseEval\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u79cd\u6a21\u578b\u67b6\u6784\u89e3\u51b3\u627f\u8bfa\u9a8c\u8bc1\u7684\u56db\u4e2a\u5b50\u4efb\u52a1\uff0c\u6700\u7ec8\u7ec4\u5408\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u9a8c\u8bc1\u4f01\u4e1aESG\u62a5\u544a\u4e2d\u7684\u627f\u8bfa\uff0c\u89e3\u51b3\u627f\u8bfa\u8bc6\u522b\u3001\u652f\u6301\u8bc1\u636e\u8bc4\u4f30\u3001\u6e05\u6670\u5ea6\u8bc4\u4ef7\u548c\u9a8c\u8bc1\u65f6\u673a\u56db\u4e2a\u5b50\u4efb\u52a1\u3002", "method": "1. ESG-BERT\u6a21\u578b\uff1b2. \u589e\u5f3a\u7248ESG-BERT\u7ed3\u5408\u8bed\u8a00\u7279\u5f81\uff1b3. \u7ec4\u5408\u5b50\u4efb\u52a1\u6a21\u578b\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u6c60\u5316\u3001\u6587\u6863\u5143\u6570\u636e\u589e\u5f3a\u548c\u591a\u76ee\u6807\u5b66\u4e60\u3002", "result": "\u5728ML-Promise\u6570\u636e\u96c6\u4e0a\uff0c\u7ec4\u5408\u6a21\u578b\u5f97\u52060.5268\uff0c\u4f18\u4e8e\u57fa\u7ebf0.5227\u3002", "conclusion": "\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u3001\u6ce8\u610f\u529b\u6c60\u5316\u548c\u591a\u76ee\u6807\u5b66\u4e60\u5bf9\u627f\u8bfa\u9a8c\u8bc1\u4efb\u52a1\u6709\u6548\uff0c\u4f46\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\u3002"}}
{"id": "2505.23400", "pdf": "https://arxiv.org/pdf/2505.23400", "abs": "https://arxiv.org/abs/2505.23400", "authors": ["Sanggyun Ma", "Wonjoon Choi", "Jihun Park", "Jaeyeul Kim", "Seunghun Lee", "Jiwan Seo", "Sunghoon Im"], "title": "Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present Bridging Geometric and Semantic (BriGeS), an effective method that\nfuses geometric and semantic information within foundation models to enhance\nMonocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which\nintegrates the complementary strengths of depth and segmentation foundation\nmodels. This integration is further refined by our Attention Temperature\nScaling technique. It finely adjusts the focus of the attention mechanisms to\nprevent over-concentration on specific features, thus ensuring balanced\nperformance across diverse inputs. BriGeS capitalizes on pre-trained foundation\nmodels and adopts a strategy that focuses on training only the Bridging Gate.\nThis method significantly reduces resource demands and training time while\nmaintaining the model's ability to generalize effectively. Extensive\nexperiments across multiple challenging datasets demonstrate that BriGeS\noutperforms state-of-the-art methods in MDE for complex scenes, effectively\nhandling intricate structures and overlapping objects.", "AI": {"tldr": "BriGeS\u662f\u4e00\u79cd\u878d\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7Bridging Gate\u548cAttention Temperature Scaling\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u51cf\u5c11\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u63d0\u5347\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u4ec5\u8bad\u7ec3Bridging Gate\uff0c\u7ed3\u5408Attention Temperature Scaling\u5e73\u8861\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u590d\u6742\u7ed3\u6784\u548c\u91cd\u53e0\u7269\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "BriGeS\u901a\u8fc7\u9ad8\u6548\u878d\u5408\u51e0\u4f55\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86MDE\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2505.23540", "pdf": "https://arxiv.org/pdf/2505.23540", "abs": "https://arxiv.org/abs/2505.23540", "authors": ["Yunqiao Yang", "Houxing Ren", "Zimu Lu", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Junting Pan", "Mingjie Zhan", "Hongsheng Li"], "title": "Probability-Consistent Preference Optimization for Enhanced LLM Reasoning", "categories": ["cs.CL"], "comment": "14 pages, to be published in ACL 2025 findings", "summary": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCPO\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7b54\u6848\u6b63\u786e\u6027\u548c\u5185\u90e8\u6982\u7387\u4e00\u81f4\u6027\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7b54\u6848\u6b63\u786e\u6027\u6216\u4e00\u81f4\u6027\uff0c\u5ffd\u7565\u4e86\u5185\u90e8\u903b\u8f91\u4e00\u81f4\u6027\uff0cPCPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPCPO\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u91cd\u5b9a\u91cf\u6307\u6807\uff08\u7b54\u6848\u6b63\u786e\u6027\u548c\u8bcd\u7ea7\u6982\u7387\u4e00\u81f4\u6027\uff09\u8fdb\u884c\u504f\u597d\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePCPO\u5728\u591a\u79cdLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PCPO\u901a\u8fc7\u7ed3\u5408\u8868\u9762\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.23406", "pdf": "https://arxiv.org/pdf/2505.23406", "abs": "https://arxiv.org/abs/2505.23406", "authors": ["Binyamin Manela", "Sharon Gannot", "Ethan Fetyaya"], "title": "Video Editing for Audio-Visual Dubbing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Visual dubbing, the synchronization of facial movements with new speech, is\ncrucial for making content accessible across different languages, enabling\nbroader global reach. However, current methods face significant limitations.\nExisting approaches often generate talking faces, hindering seamless\nintegration into original scenes, or employ inpainting techniques that discard\nvital visual information like partial occlusions and lighting variations. This\nwork introduces EdiDub, a novel framework that reformulates visual dubbing as a\ncontent-aware editing task. EdiDub preserves the original video context by\nutilizing a specialized conditioning scheme to ensure faithful and accurate\nmodifications rather than mere copying. On multiple benchmarks, including a\nchallenging occluded-lip dataset, EdiDub significantly improves identity\npreservation and synchronization. Human evaluations further confirm its\nsuperiority, achieving higher synchronization and visual naturalness scores\ncompared to the leading methods. These results demonstrate that our\ncontent-aware editing approach outperforms traditional generation or\ninpainting, particularly in maintaining complex visual elements while ensuring\naccurate lip synchronization.", "AI": {"tldr": "EdiDub\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u914d\u97f3\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u4efb\u52a1\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8eab\u4efd\u4fdd\u7559\u548c\u5507\u540c\u6b65\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u914d\u97f3\u65b9\u6cd5\u5728\u65e0\u7f1d\u96c6\u6210\u5230\u539f\u59cb\u573a\u666f\u6216\u4fdd\u7559\u590d\u6742\u89c6\u89c9\u5143\u7d20\uff08\u5982\u90e8\u5206\u906e\u6321\u548c\u5149\u7167\u53d8\u5316\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "EdiDub\u5c06\u89c6\u89c9\u914d\u97f3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u4efb\u52a1\uff0c\u91c7\u7528\u4e13\u95e8\u7684\u6761\u4ef6\u65b9\u6848\u4ee5\u4fdd\u7559\u539f\u59cb\u89c6\u9891\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEdiDub\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u540c\u6b65\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u5176\u540c\u6b65\u6027\u548c\u89c6\u89c9\u81ea\u7136\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5185\u5bb9\u611f\u77e5\u7f16\u8f91\u65b9\u6cd5\u5728\u4fdd\u7559\u590d\u6742\u89c6\u89c9\u5143\u7d20\u7684\u540c\u65f6\u786e\u4fdd\u5507\u540c\u6b65\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u751f\u6210\u6216\u4fee\u590d\u65b9\u6cd5\u3002"}}
{"id": "2505.23548", "pdf": "https://arxiv.org/pdf/2505.23548", "abs": "https://arxiv.org/abs/2505.23548", "authors": ["Yuri Balashov"], "title": "Translation in the Wild", "categories": ["cs.CL"], "comment": "4 figures", "summary": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.", "AI": {"tldr": "LLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7ffb\u8bd1\u80fd\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u76ee\u6807\u5e76\u975e\u7ffb\u8bd1\u76f8\u5173\u3002\u672c\u6587\u63a2\u8ba8\u5176\u7ffb\u8bd1\u80fd\u529b\u7684\u6765\u6e90\uff0c\u63d0\u51fa\u201c\u53cc\u91cd\u6027\u201d\u5047\u8bbe\uff0c\u5e76\u8ba8\u8bba\u5176\u5bf9\u7ffb\u8bd1\u6982\u5ff5\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u672a\u4e13\u95e8\u8bad\u7ec3\u7ffb\u8bd1\u4efb\u52a1\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u4f55\u4ecd\u80fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u7ffb\u8bd1\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u6570\u636e\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u201c\u53cc\u91cd\u6027\u201d\u5047\u8bbe\u3002", "result": "LLMs\u7684\u7ffb\u8bd1\u80fd\u529b\u53ef\u80fd\u6e90\u4e8e\u4e24\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u201c\u53cc\u91cd\u6027\u201d\u5047\u8bbe\u4e3a\u91cd\u65b0\u5b9a\u4e49\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u7684\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.23434", "pdf": "https://arxiv.org/pdf/2505.23434", "abs": "https://arxiv.org/abs/2505.23434", "authors": ["Tianhang Wang", "Fan Lu", "Sanqing Qu", "Guo Yu", "Shihang Du", "Ya Wu", "Yuan Huang", "Guang Chen"], "title": "UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors", "categories": ["cs.CV"], "comment": null, "summary": "Existing neural rendering-based urban scene reconstruction methods mainly\nfocus on the Interpolated View Synthesis (IVS) setting that synthesizes from\nviews close to training camera trajectory. However, IVS can not guarantee the\non-par performance of the novel view outside the training camera distribution\n(\\textit{e.g.}, looking left, right, or downwards), which limits the\ngeneralizability of the urban reconstruction application. Previous methods have\noptimized it via image diffusion, but they fail to handle text-ambiguous or\nlarge unseen view angles due to coarse-grained control of text-only diffusion.\nIn this paper, we design UrbanCraft, which surmounts the Extrapolated View\nSynthesis (EVS) problem using hierarchical sem-geometric representations\nserving as additional priors. Specifically, we leverage the partially\nobservable scene to reconstruct coarse semantic and geometric primitives,\nestablishing a coarse scene-level prior through an occupancy grid as the base\nrepresentation. Additionally, we incorporate fine instance-level priors from 3D\nbounding boxes to enhance object-level details and spatial relationships.\nBuilding on this, we propose the \\textbf{H}ierarchical\n\\textbf{S}emantic-Geometric-\\textbf{G}uided Variational Score Distillation\n(HSG-VSD), which integrates semantic and geometric constraints from pretrained\nUrbanCraft2D into the score distillation sampling process, forcing the\ndistribution to be consistent with the observable scene. Qualitative and\nquantitative comparisons demonstrate the effectiveness of our methods on EVS\nproblem.", "AI": {"tldr": "UrbanCraft\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5916\u63a8\u89c6\u56fe\u5408\u6210\uff08EVS\uff09\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u51e0\u4f55\u8868\u793a\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408HSG-VSD\u6280\u672f\u63d0\u5347\u65b0\u89c6\u56fe\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5728\u8bad\u7ec3\u76f8\u673a\u5206\u5e03\u5916\u7684\u89c6\u56fe\u5408\u6210\uff08\u5982\u5de6\u53f3\u6216\u5411\u4e0b\u89c6\u89d2\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u57ce\u5e02\u91cd\u5efa\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u90e8\u5206\u53ef\u89c2\u6d4b\u573a\u666f\u91cd\u5efa\u7c97\u7c92\u5ea6\u8bed\u4e49\u548c\u51e0\u4f55\u57fa\u5143\uff0c\u5e76\u901a\u8fc7\u5360\u7528\u7f51\u683c\u548c3D\u8fb9\u754c\u6846\u589e\u5f3a\u7ec6\u8282\u3002\u63d0\u51faHSG-VSD\u6280\u672f\uff0c\u6574\u5408\u8bed\u4e49\u51e0\u4f55\u7ea6\u675f\u5230\u5206\u6570\u84b8\u998f\u91c7\u6837\u4e2d\u3002", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86UrbanCraft\u5728EVS\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "UrbanCraft\u901a\u8fc7\u5206\u5c42\u5148\u9a8c\u548cHSG-VSD\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5916\u63a8\u89c6\u56fe\u5408\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23556", "pdf": "https://arxiv.org/pdf/2505.23556", "abs": "https://arxiv.org/abs/2505.23556", "authors": ["Wei Jie Yeo", "Nirmalendu Prakash", "Clement Neo", "Roy Ka-Wei Lee", "Erik Cambria", "Ranjan Satapathy"], "title": "Understanding Refusal in Language Models with Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7814\u7a76\u6307\u4ee4\u8c03\u4f18LLM\u4e2d\u7684\u62d2\u7edd\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u673a\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u62d2\u7edd\u76f8\u5173\u7279\u5f81\u5bf9\u751f\u6210\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u62d2\u7edd\u884c\u4e3a\u7684\u5185\u90e8\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u548c\u5bf9\u6297\u653b\u51fb\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u62d2\u7edd\u884c\u4e3a\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u5e76\u5728\u4e24\u4e2a\u5f00\u6e90\u804a\u5929\u6a21\u578b\u4e0a\u8fdb\u884c\u5e72\u9884\u5b9e\u9a8c\u3002", "result": "\u9a8c\u8bc1\u4e86\u62d2\u7edd\u7279\u5f81\u5bf9\u751f\u6210\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5bf9\u6297\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u62d2\u7edd\u884c\u4e3a\u7684\u673a\u5236\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5e76\u652f\u6301\u4e86\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u7684\u6539\u8fdb\u3002"}}
{"id": "2505.23438", "pdf": "https://arxiv.org/pdf/2505.23438", "abs": "https://arxiv.org/abs/2505.23438", "authors": ["Lingyan Ran", "Yali Li", "Tao Zhuo", "Shizhou Zhang", "Yanning Zhang"], "title": "Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "10 pages, 8 figures", "summary": "In semi-supervised semantic segmentation (SSSS), data augmentation plays a\ncrucial role in the weak-to-strong consistency regularization framework, as it\nenhances diversity and improves model generalization. Recent strong\naugmentation methods have primarily focused on intensity-based perturbations,\nwhich have minimal impact on the semantic masks. In contrast, spatial\naugmentations like translation and rotation have long been acknowledged for\ntheir effectiveness in supervised semantic segmentation tasks, but they are\noften ignored in SSSS. In this work, we demonstrate that spatial augmentation\ncan also contribute to model training in SSSS, despite generating inconsistent\nmasks between the weak and strong augmentations. Furthermore, recognizing the\nvariability among images, we propose an adaptive augmentation strategy that\ndynamically adjusts the augmentation for each instance based on entropy.\nExtensive experiments show that our proposed Adaptive Spatial Augmentation\n(\\textbf{ASAug}) can be integrated as a pluggable module, consistently\nimproving the performance of existing methods and achieving state-of-the-art\nresults on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7a7a\u95f4\u589e\u5f3a\u65b9\u6cd5\uff08ASAug\uff09\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\uff08SSSS\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u5f3a\u5ea6\u7684\u6270\u52a8\uff0c\u5bf9\u8bed\u4e49\u63a9\u7801\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u7a7a\u95f4\u589e\u5f3a\u5728SSSS\u4e2d\u88ab\u5ffd\u89c6\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u7a7a\u95f4\u589e\u5f3a\u7684\u6709\u6548\u6027\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7a7a\u95f4\u589e\u5f3a\uff08ASAug\uff09\uff0c\u57fa\u4e8e\u71b5\u52a8\u6001\u8c03\u6574\u6bcf\u5f20\u56fe\u50cf\u7684\u589e\u5f3a\u65b9\u5f0f\uff0c\u89e3\u51b3\u5f31\u589e\u5f3a\u4e0e\u5f3a\u589e\u5f3a\u95f4\u63a9\u7801\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660eASAug\u53ef\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\uff0c\u5728PASCAL VOC 2012\u3001Cityscapes\u548cCOCO\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "\u7a7a\u95f4\u589e\u5f3a\u5728SSSS\u4e2d\u6709\u6548\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.23570", "pdf": "https://arxiv.org/pdf/2505.23570", "abs": "https://arxiv.org/abs/2505.23570", "authors": ["Leonardo La Rocca", "Francesco Corso", "Francesco Pierri"], "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": "Submitted for review to OSNEM Special Issue of April 2025", "summary": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6587\u672c\u548c\u591a\u6a21\u6001\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728YouTube\u4e0a\u8bc6\u522b\u9634\u8c0b\u8bba\u89c6\u9891\u7684\u6548\u679c\uff0c\u53d1\u73b0\u6587\u672c\u6a21\u578b\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\uff0c\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002RoBERTa\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u8868\u73b0\u63a5\u8fd1\u53c2\u6570\u66f4\u591a\u7684LLMs\u3002", "motivation": "YouTube\u4f5c\u4e3a\u5168\u7403\u9886\u5148\u5e73\u53f0\uff0c\u6613\u4f20\u64ad\u6709\u5bb3\u5185\u5bb9\uff0c\u5982\u9634\u8c0b\u8bba\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5728\u6b64\u7c7b\u5185\u5bb9\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u6807\u6ce8\u6570\u636e\u96c6\u8bc4\u4f30\u591a\u79cdLLMs\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5e76\u4e0e\u5fae\u8c03RoBERTa\u57fa\u7ebf\u5bf9\u6bd4\u3002\u6d4b\u8bd5\u4e86\u6587\u672c\u548c\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u6587\u672cLLMs\u53ec\u56de\u7387\u9ad8\u4f46\u7cbe\u5ea6\u4f4e\uff1b\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u4e0d\u5982\u6587\u672c\u6a21\u578b\u3002RoBERTa\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u63a5\u8fd1LLMs\u6027\u80fd\u3002", "conclusion": "\u5f53\u524dLLM\u65b9\u6cd5\u5728\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u66f4\u7cbe\u786e\u548c\u9c81\u68d2\u7684\u7cfb\u7edf\u3002"}}
{"id": "2505.23439", "pdf": "https://arxiv.org/pdf/2505.23439", "abs": "https://arxiv.org/abs/2505.23439", "authors": ["Ben Li", "Minqi Li", "Jie Ren", "Kaibing Zhang"], "title": "VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration", "categories": ["cs.CV"], "comment": "31 pages, 12 figures, Accepted by Computers & Graphics", "summary": "Image-based virtual try-on aims to fit a target garment to a specific person\nimage and has attracted extensive research attention because of its huge\napplication potential in the e-commerce and fashion industries. To generate\nhigh-quality try-on results, accurately warping the clothing item to fit the\nhuman body plays a significant role, as slight misalignment may lead to\nunrealistic artifacts in the fitting image. Most existing methods warp the\nclothing by feature matching and thin-plate spline (TPS). However, it often\nfails to preserve clothing details due to self-occlusion, severe misalignment\nbetween poses, etc. To address these challenges, this paper proposes a detail\nretention virtual try-on method via accurate non-rigid registration (VITON-DRR)\nfor diverse human poses. Specifically, we reconstruct a human semantic\nsegmentation using a dual-pyramid-structured feature extractor. Then, a novel\nDeformation Module is designed for extracting the cloth key points and warping\nthem through an accurate non-rigid registration algorithm. Finally, the Image\nSynthesis Module is designed to synthesize the deformed garment image and\ngenerate the human pose information adaptively. {Compared with} traditional\nmethods, the proposed VITON-DRR can make the deformation of fitting images more\naccurate and retain more garment details. The experimental results demonstrate\nthat the proposed method performs better than state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u521a\u6027\u914d\u51c6\u7684\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff08VITON-DRR\uff09\uff0c\u901a\u8fc7\u53cc\u91d1\u5b57\u5854\u7ed3\u6784\u7279\u5f81\u63d0\u53d6\u5668\u548c\u53d8\u5f62\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u670d\u88c5\u53d8\u5f62\u548c\u7ec6\u8282\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u670d\u88c5\u53d8\u5f62\u65f6\u96be\u4ee5\u4fdd\u7559\u7ec6\u8282\uff0c\u5bfc\u81f4\u8bd5\u7a7f\u6548\u679c\u4e0d\u771f\u5b9e\uff0c\u5f71\u54cd\u4e86\u865a\u62df\u8bd5\u7a7f\u5728\u7535\u5546\u548c\u65f6\u5c1a\u884c\u4e1a\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u53cc\u91d1\u5b57\u5854\u7ed3\u6784\u7279\u5f81\u63d0\u53d6\u5668\u91cd\u5efa\u4eba\u4f53\u8bed\u4e49\u5206\u5272\uff0c\u8bbe\u8ba1\u53d8\u5f62\u6a21\u5757\u63d0\u53d6\u670d\u88c5\u5173\u952e\u70b9\u5e76\u901a\u8fc7\u975e\u521a\u6027\u914d\u51c6\u7b97\u6cd5\u8fdb\u884c\u53d8\u5f62\uff0c\u6700\u540e\u901a\u8fc7\u56fe\u50cf\u5408\u6210\u6a21\u5757\u751f\u6210\u8bd5\u7a7f\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVITON-DRR\u5728\u670d\u88c5\u53d8\u5f62\u548c\u7ec6\u8282\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u51c6\u786e\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.23604", "pdf": "https://arxiv.org/pdf/2505.23604", "abs": "https://arxiv.org/abs/2505.23604", "authors": ["Guangtao Zeng", "Maohao Shen", "Delin Chen", "Zhenting Qi", "Subhro Das", "Dan Gutfreund", "David Cox", "Gregory Wornell", "Wei Lu", "Zhang-Wei Hong", "Chuang Gan"], "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.", "AI": {"tldr": "EvoScale\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fdb\u5316\u8fc7\u7a0b\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u51cf\u5c11\u6837\u672c\u9700\u6c42\uff0c\u4f7f\u5c0f\u6a21\u578b\uff08\u598232B\uff09\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a\u5927\u6a21\u578b\uff08>100B\uff09\u3002", "motivation": "\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u56e0\u5176\u8ba1\u7b97\u6210\u672c\u4f4e\u66f4\u5b9e\u7528\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\uff09\u6210\u672c\u9ad8\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faEvoScale\uff0c\u5c06\u751f\u6210\u89c6\u4e3a\u8fdb\u5316\u8fc7\u7a0b\uff0c\u901a\u8fc7\u9009\u62e9\u548c\u7a81\u53d8\u8fed\u4ee3\u4f18\u5316\u8f93\u51fa\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u3002", "result": "\u5728SWE-Bench-Verified\u4e0a\uff0c32B\u6a21\u578bSatori-SWE-32B\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8d8a100B\u4ee5\u4e0a\u6a21\u578b\uff0c\u4e14\u6837\u672c\u9700\u6c42\u5c11\u3002", "conclusion": "EvoScale\u4e3a\u5c0f\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u4f18\u5316\u8def\u5f84\uff0c\u4ee3\u7801\u3001\u6570\u636e\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.23444", "pdf": "https://arxiv.org/pdf/2505.23444", "abs": "https://arxiv.org/abs/2505.23444", "authors": ["Runmin Jiang", "Genpei Zhang", "Yuntian Yang", "Siqi Wu", "Yuheng Zhang", "Wanyue Feng", "Yizhou Zhao", "Xi Xiao", "Xiao Wang", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of\nmacromolecules, but developing robust models for downstream analysis is\nhindered by the scarcity of high-quality annotated data. While synthetic data\ngeneration has emerged as a potential solution, existing methods often fail to\ncapture both the structural diversity of biological specimens and the complex,\nspatially varying noise inherent in cryo-EM imaging. To overcome these\nlimitations, we propose CryoCCD, a synthesis framework that integrates\nbiophysical modeling with generative techniques. Specifically, CryoCCD produces\nmulti-scale cryo-EM micrographs that reflect realistic biophysical variability\nthrough compositional heterogeneity, cellular context, and physics-informed\nimaging. To generate realistic noise, we employ a conditional diffusion model,\nenhanced by cycle consistency to preserve structural fidelity and mask-aware\ncontrastive learning to capture spatially adaptive noise patterns. Extensive\nexperiments show that CryoCCD generates structurally accurate micrographs and\nenhances performance in downstream tasks, outperforming state-of-the-art\nbaselines in both particle picking and reconstruction.", "AI": {"tldr": "CryoCCD\u662f\u4e00\u79cd\u7ed3\u5408\u751f\u7269\u7269\u7406\u5efa\u6a21\u4e0e\u751f\u6210\u6280\u672f\u7684\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u591a\u5c3a\u5ea6\u51b7\u51bb\u7535\u955c\u663e\u5fae\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u591a\u6837\u6027\u548c\u566a\u58f0\u590d\u6742\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\uff08cryo-EM\uff09\u5728\u8fd1\u539f\u5b50\u5206\u8fa8\u7387\u6210\u50cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u963b\u788d\u4e86\u7a33\u5065\u6a21\u578b\u7684\u5f00\u53d1\u3002\u73b0\u6709\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u751f\u7269\u6837\u672c\u7684\u7ed3\u6784\u591a\u6837\u6027\u548c\u590d\u6742\u7684\u7a7a\u95f4\u53d8\u5316\u566a\u58f0\u3002", "method": "CryoCCD\u901a\u8fc7\u6574\u5408\u751f\u7269\u7269\u7406\u5efa\u6a21\u4e0e\u751f\u6210\u6280\u672f\uff0c\u751f\u6210\u53cd\u6620\u771f\u5b9e\u751f\u7269\u7269\u7406\u53d8\u5f02\u7684\u591a\u5c3a\u5ea6\u663e\u5fae\u56fe\u50cf\u3002\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u548c\u63a9\u7801\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u566a\u58f0\u7684\u771f\u5b9e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCryoCCD\u751f\u6210\u7684\u663e\u5fae\u56fe\u50cf\u7ed3\u6784\u51c6\u786e\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u7c92\u5b50\u6311\u9009\u548c\u91cd\u5efa\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CryoCCD\u4e3a\u89e3\u51b3\u51b7\u51bb\u7535\u955c\u6570\u636e\u7a00\u7f3a\u548c\u566a\u58f0\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23621", "pdf": "https://arxiv.org/pdf/2505.23621", "abs": "https://arxiv.org/abs/2505.23621", "authors": ["Zheyuan Yang", "Lyuhao Chen", "Arman Cohan", "Yilun Zhao"], "title": "Table-R1: Inference-Time Scaling for Table Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u65f6\u95f4\u6269\u5c55\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u540e\u8bad\u7ec3\u7b56\u7565\uff1a\u57fa\u4e8e\u524d\u6cbf\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u7684\u84b8\u998f\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTable-R1-Zero\u6a21\u578b\u6027\u80fd\u5ab2\u7f8eGPT-4.1\u548cDeepSeek-R1\uff0c\u4e14\u4ec5\u97007B\u53c2\u6570\u3002", "motivation": "\u63a2\u7d22\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u7684\u53ef\u884c\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "1. \u84b8\u998f\uff1a\u5229\u7528DeepSeek-R1\u751f\u6210\u7684\u5927\u89c4\u6a21\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6\u5fae\u8c03LLMs\uff0c\u5f97\u5230Table-R1-SFT\u6a21\u578b\u30022. RLVR\uff1a\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u51fd\u6570\uff0c\u5e94\u7528GRPO\u7b97\u6cd5\u8bad\u7ec3Table-R1-Zero\u6a21\u578b\u3002", "result": "Table-R1-Zero\u6a21\u578b\u5728\u591a\u79cd\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8aGPT-4.1\u548cDeepSeek-R1\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u3001\u6a21\u578b\u67b6\u6784\u9009\u62e9\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u5bf9\u63d0\u5347\u8868\u683c\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0cRL\u8bad\u7ec3\u4e2d\u6d8c\u73b0\u51fa\u5173\u952e\u63a8\u7406\u6280\u80fd\u3002"}}
{"id": "2505.23451", "pdf": "https://arxiv.org/pdf/2505.23451", "abs": "https://arxiv.org/abs/2505.23451", "authors": ["Shuzhou Sun", "Li Liu", "Tianpeng Liu", "Shuaifeng Zhi", "Ming-Ming Cheng", "Janne Heikkil\u00e4", "Yongxiang Liu"], "title": "A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), 21 pages, 11 figures, 12 tables", "summary": "Existing two-stage Scene Graph Generation (SGG) frameworks typically\nincorporate a detector to extract relationship features and a classifier to\ncategorize these relationships; therefore, the training paradigm follows a\ncausal chain structure, where the detector's inputs determine the classifier's\ninputs, which in turn influence the final predictions. However, such a causal\nchain structure can yield spurious correlations between the detector's inputs\nand the final predictions, i.e., the prediction of a certain relationship may\nbe influenced by other relationships. This influence can induce at least two\nobservable biases: tail relationships are predicted as head ones, and\nforeground relationships are predicted as background ones; notably, the latter\nbias is seldom discussed in the literature. To address this issue, we propose\nreconstructing the causal chain structure into a reverse causal structure,\nwherein the classifier's inputs are treated as the confounder, and both the\ndetector's inputs and the final predictions are viewed as causal variables.\nSpecifically, we term the reconstructed causal paradigm as the Reverse causal\nFramework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse\nEstimation (ARE) to intervene on the confounder to estimate the reverse\ncausality, \\ie the causality from final predictions to the classifier's inputs.\nThen, the Maximum Information Sampling (MIS) is suggested to enhance the\nreverse causality estimation further by considering the relationship\ninformation. Theoretically, RcSGG can mitigate the spurious correlations\ninherent in the SGG framework, subsequently eliminating the induced biases.\nComprehensive experiments on popular benchmarks and diverse SGG frameworks show\nthe state-of-the-art mean recall rate.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9006\u5411\u56e0\u679c\u6846\u67b6\uff08RcSGG\uff09\uff0c\u901a\u8fc7\u91cd\u6784\u56e0\u679c\u94fe\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e24\u9636\u6bb5\u573a\u666f\u56fe\u751f\u6210\uff08SGG\uff09\u6846\u67b6\u4e2d\u56e0\u865a\u5047\u76f8\u5173\u6027\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5SGG\u6846\u67b6\u7684\u56e0\u679c\u94fe\u7ed3\u6784\u53ef\u80fd\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\uff0c\u4f8b\u5982\u5c3e\u90e8\u5173\u7cfb\u88ab\u9884\u6d4b\u4e3a\u5934\u90e8\u5173\u7cfb\u6216\u524d\u666f\u5173\u7cfb\u88ab\u9884\u6d4b\u4e3a\u80cc\u666f\u5173\u7cfb\u3002\u8bba\u6587\u65e8\u5728\u6d88\u9664\u8fd9\u4e9b\u504f\u5dee\u3002", "method": "\u63d0\u51faRcSGG\u6846\u67b6\uff0c\u901a\u8fc7Active Reverse Estimation\uff08ARE\uff09\u5e72\u9884\u6df7\u6742\u53d8\u91cf\u4f30\u8ba1\u9006\u5411\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528Maximum Information Sampling\uff08MIS\uff09\u589e\u5f3a\u4f30\u8ba1\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540cSGG\u6846\u67b6\u4e2d\uff0cRcSGG\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u5747\u53ec\u56de\u7387\u3002", "conclusion": "RcSGG\u901a\u8fc7\u91cd\u6784\u56e0\u679c\u7ed3\u6784\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86SGG\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23623", "pdf": "https://arxiv.org/pdf/2505.23623", "abs": "https://arxiv.org/abs/2505.23623", "authors": ["Jiaoda Li", "Ryan Cotterell"], "title": "Characterizing the Expressivity of Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models (LMs) have achieved widespread empirical\nsuccess, but their theoretical expressive power remains only partially\nunderstood. Prior work often relies on idealized models with assumptions --\nsuch as arbitrary numerical precision and hard attention -- that diverge from\nreal-world transformers. In this work, we provide an exact characterization of\nfixed-precision transformers with strict future masking and soft attention, an\nidealization that more closely mirrors practical implementations. We show that\nthese models are precisely as expressive as a specific fragment of linear\ntemporal logic that includes only a single temporal operator: the past\noperator. We further relate this logic to established classes in formal\nlanguage theory, automata theory, and algebra, yielding a rich and unified\ntheoretical framework for understanding transformer expressivity. Finally, we\npresent empirical results that align closely with our theory: transformers\ntrained on languages within their theoretical capacity generalize perfectly\nover lengths, while they consistently fail to generalize on languages beyond\nit.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u56fa\u5b9a\u7cbe\u5ea6Transformer\u7684\u7406\u8bba\u8868\u8fbe\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u8fbe\u80fd\u529b\u7b49\u540c\u4e8e\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7684\u7279\u5b9a\u7247\u6bb5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u4e0e\u5b9e\u9a8c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u8868\u8fbe\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u5b9e\u9645\u5b9e\u73b0\u4e2d\u7684\u56fa\u5b9a\u7cbe\u5ea6\u548c\u8f6f\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u901a\u8fc7\u7406\u60f3\u5316\u7684\u56fa\u5b9a\u7cbe\u5ea6Transformer\u6a21\u578b\uff0c\u7ed3\u5408\u4e25\u683c\u7684\u672a\u6765\u63a9\u7801\u548c\u8f6f\u6ce8\u610f\u529b\uff0c\u5206\u6790\u5176\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u8bc1\u660e\u8fd9\u4e9b\u6a21\u578b\u4e0e\u4ec5\u5305\u542b\u8fc7\u53bb\u64cd\u4f5c\u7b26\u7684\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u7247\u6bb5\u5177\u6709\u76f8\u540c\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4e0e\u5f62\u5f0f\u8bed\u8a00\u7406\u8bba\u3001\u81ea\u52a8\u673a\u7406\u8bba\u548c\u4ee3\u6570\u5efa\u7acb\u4e86\u8054\u7cfb\u3002", "conclusion": "\u7406\u8bba\u6846\u67b6\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff0cTransformer\u5728\u5176\u7406\u8bba\u80fd\u529b\u8303\u56f4\u5185\u7684\u8bed\u8a00\u4e0a\u8868\u73b0\u5b8c\u7f8e\uff0c\u8d85\u51fa\u8303\u56f4\u5219\u65e0\u6cd5\u6cdb\u5316\u3002"}}
{"id": "2505.23462", "pdf": "https://arxiv.org/pdf/2505.23462", "abs": "https://arxiv.org/abs/2505.23462", "authors": ["Runyi Li", "Bin Chen", "Jian Zhang", "Radu Timofte"], "title": "LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter", "categories": ["cs.CV"], "comment": null, "summary": "Blind face restoration from low-quality (LQ) images is a challenging task\nthat requires not only high-fidelity image reconstruction but also the\npreservation of facial identity. While diffusion models like Stable Diffusion\nhave shown promise in generating high-quality (HQ) images, their VAE modules\nare typically trained only on HQ data, resulting in semantic misalignment when\nencoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ\nconditions during the denoising process. Existing approaches often tackle this\nissue by retraining the VAE encoder, which is computationally expensive and\nmemory-intensive. To address this limitation efficiently, we propose LAFR\n(Latent Alignment for Face Restoration), a novel codebook-based latent space\nadapter that aligns the latent distribution of LQ images with that of HQ\ncounterparts, enabling semantically consistent diffusion sampling without\naltering the original VAE. To further enhance identity preservation, we\nintroduce a multi-level restoration loss that combines constraints from\nidentity embeddings and facial structural priors. Additionally, by leveraging\nthe inherent structural regularity of facial images, we show that lightweight\nfinetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to\nachieve results comparable to state-of-the-art methods, reduce training time by\n70%. Extensive experiments on both synthetic and real-world face restoration\nbenchmarks demonstrate the effectiveness and efficiency of LAFR, achieving\nhigh-quality, identity-preserving face reconstruction from severely degraded\ninputs.", "AI": {"tldr": "LAFR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7801\u672c\u7684\u6f5c\u5728\u7a7a\u95f4\u9002\u914d\u5668\uff0c\u7528\u4e8e\u5bf9\u9f50\u4f4e\u8d28\u91cf\u56fe\u50cf\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4eba\u8138\u6062\u590d\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u7f16\u7801\u65f6\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3VAE\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528\u4ee3\u7801\u672c\u5bf9\u9f50\u6f5c\u5728\u5206\u5e03\uff0c\u7ed3\u5408\u591a\u7ea7\u6062\u590d\u635f\u5931\u548c\u8f7b\u91cf\u7ea7\u6269\u6563\u5148\u9a8c\u5fae\u8c03\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u8eab\u4efd\u4fdd\u6301\u7684\u4eba\u8138\u6062\u590d\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1170%\u3002", "conclusion": "LAFR\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u9002\u7528\u4e8e\u4e25\u91cd\u9000\u5316\u8f93\u5165\u7684\u4eba\u8138\u6062\u590d\u3002"}}
{"id": "2505.23628", "pdf": "https://arxiv.org/pdf/2505.23628", "abs": "https://arxiv.org/abs/2505.23628", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 95\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "AI": {"tldr": "AutoSchemaKG\u662f\u4e00\u4e2a\u65e0\u9700\u9884\u5b9a\u4e49\u6a21\u5f0f\u7684\u5168\u81ea\u4e3b\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u77e5\u8bc6\u4e09\u5143\u7ec4\u5e76\u751f\u6210\u6a21\u5f0f\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31ATLAS\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\u3002AutoSchemaKG\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u9650\u5236\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u6784\u5efa\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u53d6\u77e5\u8bc6\u4e09\u5143\u7ec4\u548c\u751f\u6210\u6a21\u5f0f\uff0c\u901a\u8fc7\u6982\u5ff5\u5316\u5c06\u5b9e\u4f8b\u7ec4\u7ec7\u5230\u8bed\u4e49\u7c7b\u522b\u4e2d\u3002\u5904\u7406\u4e865000\u591a\u4e07\u4efd\u6587\u6863\u3002", "result": "\u6784\u5efa\u4e86ATLAS\u77e5\u8bc6\u56fe\u8c31\uff0c\u5305\u542b9\u4ebf\u591a\u8282\u70b9\u548c59\u4ebf\u8fb9\uff0c\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63d0\u5347\u4e86LLM\u7684\u4e8b\u5b9e\u6027\u3002\u6a21\u5f0f\u751f\u6210\u4e0e\u4eba\u5de5\u6a21\u5f0f\u8bed\u4e49\u5bf9\u9f50\u8fbe95%\u3002", "conclusion": "\u52a8\u6001\u751f\u6210\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u80fd\u6709\u6548\u8865\u5145\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u77e5\u8bc6\uff0c\u5c55\u793a\u4e86\u5b8c\u5168\u81ea\u4e3b\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2505.23463", "pdf": "https://arxiv.org/pdf/2505.23463", "abs": "https://arxiv.org/abs/2505.23463", "authors": ["Han Zhou", "Sebastian G. Gruber", "Teodora Popordanoska", "Matthew B. Blaschko"], "title": "Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss", "categories": ["cs.CV"], "comment": null, "summary": "Several variants of reweighted risk functionals, such as focal losss, inverse\nfocal loss, and the Area Under the Risk-Coverage Curve (AURC), have been\nproposed in the literature and claims have been made in relation to their\ncalibration properties. However, focal loss and inverse focal loss propose\nvastly different weighting schemes. In this paper, we revisit a broad class of\nweighted risk functions commonly used in deep learning and establish a\nprincipled connection between these reweighting schemes and calibration errors.\nWe show that minimizing calibration error is closely linked to the selective\nclassification paradigm and demonstrate that optimizing a regularized variant\nof the AURC naturally leads to improved calibration. This regularized AURC\nshares a similar reweighting strategy with inverse focal loss, lending support\nto the idea that focal loss is less principled when calibration is a desired\noutcome. Direct AURC optimization offers greater flexibility through the choice\nof confidence score functions (CSFs). To enable gradient-based optimization, we\nintroduce a differentiable formulation of the regularized AURC using the\nSoftRank technique. Empirical evaluations demonstrate that our AURC-based loss\nachieves competitive class-wise calibration performance across a range of\ndatasets and model architectures.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u7528\u7684\u52a0\u6743\u98ce\u9669\u51fd\u6570\uff0c\u5efa\u7acb\u4e86\u91cd\u52a0\u6743\u65b9\u6848\u4e0e\u6821\u51c6\u8bef\u5dee\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAURC\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7SoftRank\u6280\u672f\u5b9e\u73b0\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6821\u51c6\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u4e0d\u540c\u91cd\u52a0\u6743\u98ce\u9669\u51fd\u6570\uff08\u5982\u7126\u70b9\u635f\u5931\u3001\u9006\u7126\u70b9\u635f\u5931\u548cAURC\uff09\u7684\u6821\u51c6\u7279\u6027\uff0c\u63a2\u7d22\u5176\u4e0e\u6821\u51c6\u8bef\u5dee\u7684\u5173\u7cfb\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u6821\u51c6\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAURC\u7684\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528SoftRank\u6280\u672f\u5b9e\u73b0\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u4e0d\u540c\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u51fd\u6570\uff08CSFs\uff09\u589e\u5f3a\u7075\u6d3b\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u57fa\u4e8eAURC\u7684\u635f\u5931\u51fd\u6570\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u7c7b\u6821\u51c6\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u6b63\u5219\u5316AURC\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u6821\u51c6\u6027\u80fd\uff0c\u4e14\u9006\u7126\u70b9\u635f\u5931\u7684\u91cd\u52a0\u6743\u7b56\u7565\u66f4\u7b26\u5408\u6821\u51c6\u76ee\u6807\uff0c\u800c\u7126\u70b9\u635f\u5931\u5728\u6b64\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002"}}
{"id": "2505.23630", "pdf": "https://arxiv.org/pdf/2505.23630", "abs": "https://arxiv.org/abs/2505.23630", "authors": ["Enzo Doyen", "Amalia Todirascu"], "title": "GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings; 9 pages, 2 figures", "summary": "A significant portion of the textual data used in the field of Natural\nLanguage Processing (NLP) exhibits gender biases, particularly due to the use\nof masculine generics (masculine words that are supposed to refer to mixed\ngroups of men and women), which can perpetuate and amplify stereotypes. Gender\nrewriting, an NLP task that involves automatically detecting and replacing\ngendered forms with neutral or opposite forms (e.g., from masculine to\nfeminine), can be employed to mitigate these biases. While such systems have\nbeen developed in a number of languages (English, Arabic, Portuguese, German,\nFrench), automatic use of gender neutralization techniques (as opposed to\ninclusive or gender-switching techniques) has only been studied for English.\nThis paper presents GeNRe, the very first French gender-neutral rewriting\nsystem using collective nouns, which are gender-fixed in French. We introduce a\nrule-based system (RBS) tailored for the French language alongside two\nfine-tuned language models trained on data generated by our RBS. We also\nexplore the use of instruct-based models to enhance the performance of our\nother systems and find that Claude 3 Opus combined with our dictionary achieves\nresults close to our RBS. Through this contribution, we hope to promote the\nadvancement of gender bias mitigation techniques in NLP for French.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGeNRe\uff0c\u9996\u4e2a\u6cd5\u8bed\u6027\u522b\u4e2d\u6027\u6539\u5199\u7cfb\u7edf\uff0c\u4f7f\u7528\u96c6\u4f53\u540d\u8bcd\u89e3\u51b3\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u7ed3\u5408\u89c4\u5219\u7cfb\u7edf\u548c\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "NLP\u4e2d\u6587\u672c\u6570\u636e\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u5c24\u5176\u662f\u6cd5\u8bed\u4e2d\u7537\u6027\u901a\u7528\u8bcd\u7684\u4f7f\u7528\uff0c\u9700\u901a\u8fc7\u6027\u522b\u4e2d\u6027\u6539\u5199\u6280\u672f\u7f13\u89e3\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff08RBS\uff09\u548c\u4e24\u79cd\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u63a2\u7d22\u6307\u4ee4\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002", "result": "Claude 3 Opus\u7ed3\u5408\u8bcd\u5178\u6548\u679c\u63a5\u8fd1RBS\u3002", "conclusion": "GeNRe\u63a8\u52a8\u4e86\u6cd5\u8bedNLP\u4e2d\u6027\u522b\u504f\u89c1\u7f13\u89e3\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.23469", "pdf": "https://arxiv.org/pdf/2505.23469", "abs": "https://arxiv.org/abs/2505.23469", "authors": ["Zhuodong Li", "Fei Hou", "Wencheng Wang", "Xuequan Lu", "Ying He"], "title": "A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization", "categories": ["cs.CV"], "comment": "accepted to SIGGRAPH 2025", "summary": "Orienting point clouds is a fundamental problem in computer graphics and 3D\nvision, with applications in reconstruction, segmentation, and analysis. While\nsignificant progress has been made, existing approaches mainly focus on\nwatertight, object-level 3D models. The orientation of large-scale,\nnon-watertight 3D scenes remains an underexplored challenge. To address this\ngap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework\nthat leverages a divide-and-conquer strategy for scalable and robust point\ncloud orientation. Rather than attempting to orient an unbounded scene at once,\nDACPO segments the input point cloud into smaller, manageable blocks, processes\neach block independently, and integrates the results through a global\noptimization stage. For each block, we introduce a two-step process: estimating\ninitial normal orientations by a randomized greedy method and refining them by\nan adapted iterative Poisson surface reconstruction. To achieve consistency\nacross blocks, we model inter-block relationships using an an undirected graph,\nwhere nodes represent blocks and edges connect spatially adjacent blocks. To\nreliably evaluate orientation consistency between adjacent blocks, we introduce\nthe concept of the visible connected region, which defines the region over\nwhich visibility-based assessments are performed. The global integration is\nthen formulated as a 0-1 integer-constrained optimization problem, with block\nflip states as binary variables. Despite the combinatorial nature of the\nproblem, DACPO remains scalable by limiting the number of blocks (typically a\nfew hundred for 3D scenes) involved in the optimization. Experiments on\nbenchmark datasets demonstrate DACPO's strong performance, particularly in\nchallenging large-scale, non-watertight scenarios where existing methods often\nfail. The source code is available at https://github.com/zd-lee/DACPO.", "AI": {"tldr": "DACPO\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u975e\u5c01\u95ed\u70b9\u4e91\u7684\u5b9a\u5411\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5757\u5904\u7406\u4e0e\u5168\u5c40\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u5b9a\u5411\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5c01\u95ed\u7684\u7269\u4f53\u7ea73D\u6a21\u578b\uff0c\u800c\u5927\u89c4\u6a21\u975e\u5c01\u95ed3D\u573a\u666f\u7684\u5b9a\u5411\u95ee\u9898\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0cDACPO\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DACPO\u5c06\u70b9\u4e91\u5206\u5272\u4e3a\u5c0f\u5757\uff0c\u901a\u8fc7\u968f\u673a\u8d2a\u5a6a\u65b9\u6cd5\u4f30\u8ba1\u521d\u59cb\u6cd5\u5411\uff0c\u5e76\u5229\u7528\u6539\u8fdb\u7684\u6cca\u677e\u8868\u9762\u91cd\u5efa\u8fdb\u884c\u7ec6\u5316\uff0c\u518d\u901a\u8fc7\u56fe\u6a21\u578b\u548c\u5168\u5c40\u4f18\u5316\u6574\u5408\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDACPO\u5728\u5927\u89c4\u6a21\u975e\u5c01\u95ed\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DACPO\u4e3a\u5927\u89c4\u6a21\u975e\u5c01\u95ed\u70b9\u4e91\u7684\u5b9a\u5411\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23646", "pdf": "https://arxiv.org/pdf/2505.23646", "abs": "https://arxiv.org/abs/2505.23646", "authors": ["Zijun Yao", "Yantao Liu", "Yanxu Chen", "Jianhui Chen", "Junfeng Fang", "Lei Hou", "Juanzi Li", "Tat-Seng Chua"], "title": "Are Reasoning Models More Prone to Hallucination?", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5728\u4e8b\u5b9e\u5bfb\u6c42\u4efb\u52a1\u4e2d\u662f\u5426\u51cf\u5c11\u5e7b\u89c9\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u672c\u6587\u4ece\u4e09\u4e2a\u65b9\u9762\u63a2\u8ba8\u4e86\u63a8\u7406\u6a21\u578b\u662f\u5426\u66f4\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4e8b\u5b9e\u5bfb\u6c42\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u5176\u662f\u5426\u51cf\u5c11\u6216\u52a0\u5267\u5e7b\u89c9\u7684\u4e89\u8bae\u3002", "method": "1. \u5bf9LRMs\u7684\u5e7b\u89c9\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff1b2. \u5206\u6790\u4e0d\u540c\u540e\u8bad\u7ec3\u6d41\u7a0b\u5bf9\u5e7b\u89c9\u7684\u5f71\u54cd\uff1b3. \u4ece\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u89d2\u5ea6\u63a2\u7a76\u5e7b\u89c9\u673a\u5236\u3002", "result": "1. \u5b8c\u6574\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff08\u51b7\u542f\u52a8SFT\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1RL\uff09\u51cf\u8f7b\u5e7b\u89c9\uff1b2. \u84b8\u998f\u548c\u672a\u51b7\u542f\u52a8\u7684RL\u8bad\u7ec3\u52a0\u5267\u5e7b\u89c9\uff1b3. \u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u5bfc\u81f4\u5e7b\u89c9\u589e\u52a0\u3002", "conclusion": "\u672c\u6587\u521d\u6b65\u63ed\u793a\u4e86LRMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23475", "pdf": "https://arxiv.org/pdf/2505.23475", "abs": "https://arxiv.org/abs/2505.23475", "authors": ["Ron Shapira Weber", "Shahar Ben Ishay", "Andrey Lavrinenko", "Shahaf E. Finder", "Oren Freifeld"], "title": "TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Fast and scalable alignment of time series is a fundamental challenge in many\ndomains. The standard solution, Dynamic Time Warping (DTW), struggles with poor\nscalability and sensitivity to noise. We introduce TimePoint, a self-supervised\nmethod that dramatically accelerates DTW-based alignment while typically\nimproving alignment accuracy by learning keypoints and descriptors from\nsynthetic data. Inspired by 2D keypoint detection but carefully adapted to the\nunique challenges of 1D signals, TimePoint leverages efficient 1D\ndiffeomorphisms, which effectively model nonlinear time warping, to generate\nrealistic training data. This approach, along with fully convolutional and\nwavelet convolutional architectures, enables the extraction of informative\nkeypoints and descriptors. Applying DTW to these sparse representations yield\nmajor speedups and typically higher alignment accuracy than standard DTW\napplied to the full signals. TimePoint demonstrates strong generalization to\nreal-world time series when trained solely on synthetic data, and further\nimproves with fine-tuning on real data. Extensive experiments demonstrate that\nTimePoint consistently achieves faster and more accurate alignments than\nstandard DTW, making it a scalable solution for time-series analysis. Our code\nis available at https://github.com/BGU-CS-VIL/TimePoint", "AI": {"tldr": "TimePoint\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5408\u6210\u6570\u636e\u4e2d\u5b66\u4e60\u5173\u952e\u70b9\u548c\u63cf\u8ff0\u7b26\uff0c\u663e\u8457\u52a0\u901fDTW\u5bf9\u9f50\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u5bf9\u9f50\u4e2d\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u548c\u5bf9\u566a\u58f0\u654f\u611f\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "TimePoint\u5229\u75281D\u5fae\u5206\u540c\u80da\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u7ed3\u5408\u5168\u5377\u79ef\u548c\u5c0f\u6ce2\u5377\u79ef\u67b6\u6784\u63d0\u53d6\u5173\u952e\u70b9\u548c\u63cf\u8ff0\u7b26\uff0c\u7a00\u758f\u8868\u793a\u540e\u5e94\u7528DTW\u3002", "result": "TimePoint\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6DTW\uff0c\u4e14\u4ec5\u9700\u5408\u6210\u6570\u636e\u5373\u53ef\u6cdb\u5316\u5230\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u3002", "conclusion": "TimePoint\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.23654", "pdf": "https://arxiv.org/pdf/2505.23654", "abs": "https://arxiv.org/abs/2505.23654", "authors": ["Mohamed Elaraby", "Diane Litman"], "title": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6458\u8981\u751f\u6210\u4e2d\u662f\u5426\u5145\u5206\u4fdd\u7559\u5173\u952e\u8bba\u8bc1\u89d2\u8272\u4fe1\u606f\uff0c\u5e76\u63d0\u51fa\u4e86Argument Representation Coverage\uff08ARC\uff09\u6846\u67b6\u6765\u8bc4\u4f30LLM\u751f\u6210\u6458\u8981\u7684\u8d28\u91cf\u3002", "motivation": "\u5728\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u8bba\u8bc1\u89d2\u8272\u5bf9\u6587\u6863\u6458\u8981\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u662f\u5426\u80fd\u591f\u6709\u6548\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5f15\u5165ARC\u6846\u67b6\uff0c\u5206\u6790\u4e09\u4e2a\u5f00\u6e90LLMs\u5728\u957f\u6cd5\u5f8b\u610f\u89c1\u548c\u79d1\u5b66\u6587\u7ae0\u4e2d\u7684\u6458\u8981\u8868\u73b0\u3002", "result": "LLMs\u80fd\u90e8\u5206\u8986\u76d6\u5173\u952e\u8bba\u8bc1\u89d2\u8272\uff0c\u4f46\u5173\u952e\u4fe1\u606f\u5e38\u88ab\u9057\u6f0f\uff0c\u5c24\u5176\u662f\u5728\u8bba\u8bc1\u5206\u6563\u65f6\u3002LLMs\u7684\u4f4d\u7f6e\u504f\u5dee\u548c\u89d2\u8272\u504f\u597d\u5f71\u54cd\u4e86\u6458\u8981\u8d28\u91cf\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u66f4\u5177\u8bba\u8bc1\u610f\u8bc6\u7684\u6458\u8981\u7b56\u7565\uff0c\u4ee5\u63d0\u5347LLMs\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.23481", "pdf": "https://arxiv.org/pdf/2505.23481", "abs": "https://arxiv.org/abs/2505.23481", "authors": ["Mohamed Rayan Barhdadi", "Hasan Kurban", "Hussein Alnuweiri"], "title": "PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "comment": "4 pages, 2 figures, 2 tables. Preliminary work. Under review by the\n  Building Physically Plausible World Models Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025), Vancouver, Canada", "summary": "PhysicsNeRF is a physically grounded framework for 3D reconstruction from\nsparse views, extending Neural Radiance Fields with four complementary\nconstraints: depth ranking, RegNeRF-style consistency, sparsity priors, and\ncross-view alignment. While standard NeRFs fail under sparse supervision,\nPhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB\naverage PSNR using only 8 views, outperforming prior methods. A generalization\ngap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental\nlimitations of sparse-view reconstruction. PhysicsNeRF enables physically\nconsistent, generalizable 3D representations for agent interaction and\nsimulation, and clarifies the expressiveness-generalization trade-off in\nconstrained NeRF models.", "AI": {"tldr": "PhysicsNeRF\u901a\u8fc7\u5f15\u5165\u56db\u79cd\u7269\u7406\u7ea6\u675f\u6539\u8fdbNeRF\uff0c\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u5b9e\u73b0\u66f4\u4f18\u76843D\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6NeRF\u5728\u7a00\u758f\u89c6\u56fe\u4e0b\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u63d0\u53473D\u91cd\u5efa\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u6392\u5e8f\u3001RegNeRF\u4e00\u81f4\u6027\u3001\u7a00\u758f\u5148\u9a8c\u548c\u8de8\u89c6\u56fe\u5bf9\u9f50\u56db\u79cd\u7ea6\u675f\uff0c\u91c7\u75280.67M\u53c2\u6570\u7684\u5c0f\u578b\u67b6\u6784\u3002", "result": "\u4ec5\u75288\u89c6\u56fe\u5373\u8fbe\u523021.4 dB\u5e73\u5747PSNR\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u7a00\u758f\u91cd\u5efa\u76845.7-6.2 dB\u6cdb\u5316\u5dee\u8ddd\u3002", "conclusion": "PhysicsNeRF\u4e3a\u7269\u7406\u4e00\u81f4\u76843D\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u9610\u660e\u4e86\u7ea6\u675fNeRF\u6a21\u578b\u7684\u8868\u8fbe\u529b-\u6cdb\u5316\u6743\u8861\u3002"}}
{"id": "2505.23657", "pdf": "https://arxiv.org/pdf/2505.23657", "abs": "https://arxiv.org/abs/2505.23657", "authors": ["Hongxiang Zhang", "Hao Chen", "Tianyi Zhang", "Muhao Chen"], "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent decoding methods improve the factuality of large language\nmodels~(LLMs) by refining how the next token is selected during generation.\nThese methods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.", "AI": {"tldr": "ActLCD\u662f\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4ecd\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faActLCD\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u5956\u52b1\u611f\u77e5\u5206\u7c7b\u5668\uff0c\u52a8\u6001\u9009\u62e9\u5bf9\u6bd4\u5c42\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "ActLCD\u5728\u591a\u6837\u5316\u751f\u6210\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u6027\u3002"}}
{"id": "2505.23484", "pdf": "https://arxiv.org/pdf/2505.23484", "abs": "https://arxiv.org/abs/2505.23484", "authors": ["Shi-Xue Zhang", "Hongfa Wang", "Duojun Huang", "Xin Li", "Xiaobin Zhu", "Xu-Cheng Yin"], "title": "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation", "categories": ["cs.CV"], "comment": "submitting", "summary": "Video captions play a crucial role in text-to-video generation tasks, as\ntheir quality directly influences the semantic coherence and visual fidelity of\nthe generated videos. Although large vision-language models (VLMs) have\ndemonstrated significant potential in caption generation, existing benchmarks\ninadequately address fine-grained evaluation, particularly in capturing\nspatial-temporal details critical for video generation. To address this gap, we\nintroduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the\nfirst large-scale fine-grained benchmark comprising 5,677 (5K+) videos and\n109,796 (100K+) question-answer pairs. These QA-pairs are systematically\nannotated across 21 fine-grained dimensions (e.g., camera movement, and shot\ntype) that are empirically proven critical for text-to-video generation. We\nfurther introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),\nCoverage Rate (CR)), and an automated evaluation pipeline leveraging large\nlanguage model (LLM) to verify caption quality via contrastive QA-pairs\nanalysis. By providing actionable insights for caption optimization, our\nbenchmark can advance the development of robust text-to-video models. The\ndataset and codes are available at website: https://github.com/GXYM/VCapsBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VCapsBench\uff0c\u9996\u4e2a\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u89c6\u9891\u5b57\u5e55\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b5K+\u89c6\u9891\u548c100K+QA\u5bf9\uff0c\u65e8\u5728\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5728\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff08\u5c24\u5176\u662f\u7a7a\u95f4-\u65f6\u95f4\u7ec6\u8282\uff09\u4e0a\u4e0d\u8db3\uff0c\u5f71\u54cd\u89c6\u9891\u751f\u6210\u7684\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "method": "\u5f15\u5165VCapsBench\u57fa\u51c6\uff0c\u5305\u542b5,677\u89c6\u9891\u548c109,796 QA\u5bf9\uff0c\u6807\u6ce821\u4e2a\u7ec6\u7c92\u5ea6\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51faAR\u3001IR\u3001CR\u4e09\u4e2a\u6307\u6807\u53ca\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4QA\u5bf9\u5206\u6790\u9a8c\u8bc1\u5b57\u5e55\u8d28\u91cf\uff0c\u4e3a\u5b57\u5e55\u4f18\u5316\u63d0\u4f9b\u53ef\u64cd\u4f5c\u5efa\u8bae\u3002", "conclusion": "VCapsBench\u6709\u52a9\u4e8e\u63a8\u52a8\u9c81\u68d2\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.23662", "pdf": "https://arxiv.org/pdf/2505.23662", "abs": "https://arxiv.org/abs/2505.23662", "authors": ["Beong-woo Kwak", "Minju Kim", "Dongha Lim", "Hyungjoo Chae", "Dongjin Kang", "Sunghwan Kim", "Dongil Yang", "Jinyoung Yeo"], "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions", "categories": ["cs.CL"], "comment": "Our code and data are available at\n  https://github.com/bwookwak/ToolHaystack", "summary": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.", "AI": {"tldr": "ToolHaystack\u662f\u4e00\u4e2a\u7528\u4e8e\u6d4b\u8bd5\u957f\u671f\u4ea4\u4e92\u4e2d\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u671f\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u5927\u591a\u5047\u8bbe\u5de5\u5177\u4f7f\u7528\u5728\u77ed\u4e0a\u4e0b\u6587\u4e2d\uff0c\u7f3a\u4e4f\u5bf9\u957f\u671f\u4ea4\u4e92\u4e2d\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "\u5f15\u5165ToolHaystack\u57fa\u51c6\uff0c\u5305\u542b\u591a\u4efb\u52a1\u6267\u884c\u4e0a\u4e0b\u6587\u548c\u8fde\u7eed\u5bf9\u8bdd\u4e2d\u7684\u566a\u58f0\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6d4b\u8bd514\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5728\u6807\u51c6\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728ToolHaystack\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "ToolHaystack\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u957f\u671f\u9c81\u68d2\u6027\u4e0a\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5f25\u8865\u4e86\u4ee5\u5f80\u5de5\u5177\u57fa\u51c6\u7684\u4e0d\u8db3\u3002"}}
{"id": "2505.23493", "pdf": "https://arxiv.org/pdf/2505.23493", "abs": "https://arxiv.org/abs/2505.23493", "authors": ["Kaijie Chen", "Zihao Lin", "Zhiyang Xu", "Ying Shen", "Yuguang Yao", "Joy Rimchala", "Jiaxin Zhang", "Lifu Huang"], "title": "R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://r2i-bench.github.io", "summary": "Reasoning is a fundamental capability often required in real-world\ntext-to-image (T2I) generation, e.g., generating ``a bitten apple that has been\nleft in the air for more than a week`` necessitates understanding temporal\ndecay and commonsense concepts. While recent T2I models have made impressive\nprogress in producing photorealistic images, their reasoning capability remains\nunderdeveloped and insufficiently evaluated. To bridge this gap, we introduce\nR2I-Bench, a comprehensive benchmark specifically designed to rigorously assess\nreasoning-driven T2I generation. R2I-Bench comprises meticulously curated data\ninstances, spanning core reasoning categories, including commonsense,\nmathematical, logical, compositional, numerical, causal, and concept mixing. To\nfacilitate fine-grained evaluation, we design R2IScore, a QA-style metric based\non instance-specific, reasoning-oriented evaluation questions that assess three\ncritical dimensions: text-image alignment, reasoning accuracy, and image\nquality. Extensive experiments with 16 representative T2I models, including a\nstrong pipeline-based framework that decouples reasoning and generation using\nthe state-of-the-art language and image generation models, demonstrate\nconsistently limited reasoning performance, highlighting the need for more\nrobust, reasoning-aware architectures in the next generation of T2I systems.\nProject Page: https://r2i-bench.github.io", "AI": {"tldr": "R2I-Bench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u7c7b\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807R2IScore\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u8bbe\u8ba1\u4e86R2I-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u79cd\u63a8\u7406\u7c7b\u522b\uff08\u5982\u5e38\u8bc6\u3001\u6570\u5b66\u3001\u903b\u8f91\u7b49\uff09\uff0c\u5e76\u5f00\u53d1\u4e86R2IScore\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u95ee\u7b54\u5f62\u5f0f\u8bc4\u4f30\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3001\u63a8\u7406\u51c6\u786e\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e8616\u79cd\u4ee3\u8868\u6027\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u5176\u63a8\u7406\u80fd\u529b\u666e\u904d\u6709\u9650\uff0c\u8868\u660e\u9700\u8981\u66f4\u5f3a\u5927\u7684\u63a8\u7406\u611f\u77e5\u67b6\u6784\u3002", "conclusion": "R2I-Bench\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u63a8\u7406\u611f\u77e5\u6a21\u578b\u3002"}}
{"id": "2505.23666", "pdf": "https://arxiv.org/pdf/2505.23666", "abs": "https://arxiv.org/abs/2505.23666", "authors": ["Luke McDermott", "Robert W. Heath Jr.", "Rahul Parhi"], "title": "LoLA: Low-Rank Linear Attention With Sparse Caching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.", "AI": {"tldr": "LoLA\u662f\u4e00\u79cd\u4f4e\u79e9\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u7f13\u5b58\u548c\u4e09\u79cd\u8bb0\u5fc6\u5f62\u5f0f\u89e3\u51b3\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bb0\u5fc6\u78b0\u649e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "Transformer\u7684\u957f\u5e8f\u5217\u63a8\u7406\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u9ad8\u6548\u4f46\u8fd1\u4f3c\u4e0d\u51c6\u786e\u3002LoLA\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u5f25\u8865\u5176\u4e0eTransformer\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "LoLA\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u3001\u7a00\u758f\u5168\u5c40\u7f13\u5b58\u548c\u5faa\u73af\u9690\u85cf\u72b6\u6001\uff0c\u5c06\u952e\u503c\u5bf9\u5206\u4e3a\u4e09\u79cd\u8bb0\u5fc6\u5f62\u5f0f\uff0c\u907f\u514d\u8bb0\u5fc6\u78b0\u649e\u3002", "result": "LoLA\u57288K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u4ece0.6%\u63d0\u5347\u81f397.4%\uff0c\u7f13\u5b58\u5927\u5c0f\u4ec5\u4e3aLlama-3.1 8B\u76841/4.6\uff0c\u4e14\u5728\u96f6\u6837\u672c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "LoLA\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9ad8\u6548\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u3002"}}
{"id": "2505.23503", "pdf": "https://arxiv.org/pdf/2505.23503", "abs": "https://arxiv.org/abs/2505.23503", "authors": ["Shibbir Ahmed", "Shahnewaz Karim Sakib", "Anindya Bijoy Das"], "title": "Can Large Language Models Challenge CNNS in Medical Image Analysis?", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.", "AI": {"tldr": "\u591a\u6a21\u6001AI\u6846\u67b6\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\uff0c\u6bd4\u8f83CNN\u548cLLM\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u73af\u5883\u5f71\u54cd\uff0c\u53d1\u73b0\u7ed3\u5408LLM\u7684\u8fc7\u6ee4\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u8bca\u65ad\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6bd4\u8f83CNN\u548cLLM\u5728\u51c6\u786e\u6027\u3001F1\u5206\u6570\u3001\u6267\u884c\u65f6\u95f4\u3001\u80fd\u8017\u548cCO2\u6392\u653e\u4e0a\u7684\u8868\u73b0\u3002", "result": "CNN\u5728\u67d0\u4e9b\u65b9\u9762\u4f18\u4e8e\u591a\u6a21\u6001\u6280\u672f\uff0c\u4f46\u7ed3\u5408LLM\u7684\u8fc7\u6ee4\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2505.23688", "pdf": "https://arxiv.org/pdf/2505.23688", "abs": "https://arxiv.org/abs/2505.23688", "authors": ["James Tanner", "Morgan Sonderegger", "Jane Stuart-Smith", "Jeff Mielke", "Tyler Kendall"], "title": "Automatic classification of stop realisation with wav2vec2.0", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025. 5 pages, 3 figures", "summary": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to `scale-up' the\nscope of phonetic research with relative ease.", "AI": {"tldr": "\u5229\u7528\u9884\u8bad\u7ec3\u7684wav2vec2.0\u6a21\u578b\uff0c\u81ea\u52a8\u5206\u7c7b\u8bed\u97f3\u6570\u636e\u4e2d\u7684\u7206\u7834\u97f3\u5b58\u5728\uff0c\u5c55\u793a\u4e86\u5176\u5728\u82f1\u8bed\u548c\u65e5\u8bed\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u591a\u79cd\u53ef\u53d8\u8bed\u97f3\u73b0\u8c61\u7684\u81ea\u52a8\u6807\u6ce8\u5de5\u5177\uff0c\u800c\u9884\u8bad\u7ec3\u7684\u81ea\u6211\u76d1\u7763\u6a21\u578b\uff08\u5982wav2vec2.0\uff09\u5728\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "method": "\u8bad\u7ec3wav2vec2.0\u6a21\u578b\uff0c\u81ea\u52a8\u5206\u7c7b\u82f1\u8bed\u548c\u65e5\u8bed\u4e2d\u7684\u7206\u7834\u97f3\u5b58\u5728\uff0c\u6d4b\u8bd5\u5176\u5728\u7cbe\u5fc3\u6574\u7406\u548c\u672a\u51c6\u5907\u8bed\u97f3\u8bed\u6599\u5e93\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u81ea\u52a8\u6807\u6ce8\u7684\u7206\u7834\u97f3\u5b58\u5728\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\uff0c\u4e14\u4e0e\u624b\u52a8\u6807\u6ce8\u7684\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u590d\u73b0\u7206\u7834\u97f3\u5b9e\u73b0\u7684\u53d8\u5f02\u6027\u6a21\u5f0f\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u5177\u6709\u4f5c\u4e3a\u81ea\u52a8\u6807\u6ce8\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u53ef\u6269\u5c55\u8bed\u97f3\u7814\u7a76\u7684\u8303\u56f4\u3002"}}
{"id": "2505.23504", "pdf": "https://arxiv.org/pdf/2505.23504", "abs": "https://arxiv.org/abs/2505.23504", "authors": ["Liyun Zhu", "Qixiang Chen", "Xi Shen", "Xiaodong Cun"], "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Understanding (VAU) is essential for applications such as smart\ncities, security surveillance, and disaster alert systems, yet remains\nchallenging due to its demand for fine-grained spatio-temporal perception and\nrobust reasoning under ambiguity. Despite advances in anomaly detection,\nexisting methods often lack interpretability and struggle to capture the causal\nand contextual aspects of abnormal events. This limitation is further\ncompounded by the absence of comprehensive benchmarks for evaluating reasoning\nability in anomaly scenarios. To address both challenges, we introduce VAU-R1,\na data-efficient framework built upon Multimodal Large Language Models (MLLMs),\nwhich enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).\nBesides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored\nfor video anomaly reasoning, featuring multiple-choice QA, detailed rationales,\ntemporal annotations, and descriptive captions. Empirical results show that\nVAU-R1 significantly improves question answering accuracy, temporal grounding,\nand reasoning coherence across diverse contexts. Together, our method and\nbenchmark establish a strong foundation for interpretable and reasoning-aware\nvideo anomaly understanding. Our code is available at\nhttps://github.com/GVCLab/VAU-R1.", "AI": {"tldr": "VAU-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u636e\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5fae\u8c03\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u94fe\u5f0f\u601d\u7ef4\u57fa\u51c6VAU-Bench\u3002", "motivation": "\u89c6\u9891\u5f02\u5e38\u7406\u89e3\uff08VAU\uff09\u5728\u667a\u80fd\u57ce\u5e02\u3001\u5b89\u9632\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u96be\u4ee5\u6355\u6349\u5f02\u5e38\u4e8b\u4ef6\u7684\u56e0\u679c\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\u3002", "method": "\u63d0\u51faVAU-R1\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u589e\u5f3a\u5f02\u5e38\u63a8\u7406\uff1b\u540c\u65f6\u6784\u5efaVAU-Bench\u57fa\u51c6\uff0c\u5305\u542b\u591a\u9009QA\u3001\u8be6\u7ec6\u89e3\u91ca\u3001\u65f6\u95f4\u6807\u6ce8\u548c\u63cf\u8ff0\u6027\u5b57\u5e55\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVAU-R1\u663e\u8457\u63d0\u9ad8\u4e86\u95ee\u7b54\u51c6\u786e\u6027\u3001\u65f6\u95f4\u5b9a\u4f4d\u548c\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "conclusion": "VAU-R1\u548cVAU-Bench\u4e3a\u53ef\u89e3\u91ca\u548c\u63a8\u7406\u611f\u77e5\u7684\u89c6\u9891\u5f02\u5e38\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.23689", "pdf": "https://arxiv.org/pdf/2505.23689", "abs": "https://arxiv.org/abs/2505.23689", "authors": ["Francesca Padovani", "Jaap Jumelet", "Yevgen Matusevych", "Arianna Bisazza"], "title": "Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models", "categories": ["cs.CL"], "comment": "21 pages, 4 figures, 4 tables", "summary": "Seminal work by Huebner et al. (2021) showed that language models (LMs)\ntrained on English Child-Directed Language (CDL) can reach similar syntactic\nabilities as LMs trained on much larger amounts of adult-directed written text,\nsuggesting that CDL could provide more effective LM training material than the\ncommonly used internet-crawled data. However, the generalizability of these\nresults across languages, model types, and evaluation settings remains unclear.\nWe test this by comparing models trained on CDL vs. Wikipedia across two LM\nobjectives (masked and causal), three languages (English, French, German), and\nthree syntactic minimal-pair benchmarks. Our results on these benchmarks show\ninconsistent benefits of CDL, which in most cases is outperformed by Wikipedia\nmodels. We then identify various shortcomings in previous benchmarks, and\nintroduce a novel testing methodology, FIT-CLAMS, which uses a\nfrequency-controlled design to enable balanced comparisons across training\ncorpora. Through minimal pair evaluations and regression analysis we show that\ntraining on CDL does not yield stronger generalizations for acquiring syntax\nand highlight the importance of controlling for frequency effects when\nevaluating syntactic ability.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u513f\u7ae5\u5bfc\u5411\u8bed\u8a00\uff08CDL\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u7ef4\u57fa\u767e\u79d1\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e14\u9700\u63a7\u5236\u9891\u7387\u6548\u5e94\u4ee5\u51c6\u786e\u8bc4\u4f30\u53e5\u6cd5\u80fd\u529b\u3002", "motivation": "\u9a8c\u8bc1CDL\u5728\u4e0d\u540c\u8bed\u8a00\u3001\u6a21\u578b\u7c7b\u578b\u548c\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u7684\u901a\u7528\u6027\uff0c\u5e76\u6539\u8fdb\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e0d\u8db3\u3002", "method": "\u6bd4\u8f83CDL\u548c\u7ef4\u57fa\u767e\u79d1\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u79cd\u76ee\u6807\uff08\u63a9\u7801\u548c\u56e0\u679c\uff09\u3001\u4e09\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u6cd5\u8bed\u3001\u5fb7\u8bed\uff09\u548c\u4e09\u4e2a\u53e5\u6cd5\u6700\u5c0f\u5bf9\u57fa\u51c6\u3002", "result": "CDL\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u7ef4\u57fa\u767e\u79d1\u6a21\u578b\uff0c\u4e14\u9700\u9891\u7387\u63a7\u5236\u8bbe\u8ba1\uff08FIT-CLAMS\uff09\u4ee5\u5e73\u8861\u6bd4\u8f83\u3002", "conclusion": "CDL\u8bad\u7ec3\u5e76\u672a\u5e26\u6765\u66f4\u5f3a\u7684\u53e5\u6cd5\u6cdb\u5316\u80fd\u529b\uff0c\u9891\u7387\u6548\u5e94\u63a7\u5236\u5bf9\u8bc4\u4f30\u53e5\u6cd5\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.23522", "pdf": "https://arxiv.org/pdf/2505.23522", "abs": "https://arxiv.org/abs/2505.23522", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Xuming He", "YiFan Zhang", "Feng Liu", "Zijie Guo", "Zhenghao Hu", "Jiong Wang", "Jingyi Xu", "Zhangrui Li", "Fenghua Ling", "Ben Fei", "Weijia Li", "Long Lan", "Wenjing Yang", "Wenlong Zhang", "Lei Bai"], "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing benchmarks for Earth science multimodal learning exhibit critical\nlimitations in systematic coverage of geosystem components and cross-sphere\ninteractions, often constrained to isolated subsystems (only in\nHuman-activities sphere or atmosphere) with limited evaluation dimensions (less\nthan 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first\ncomprehensive multimodal benchmark spanning all six Earth science spheres\n(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and\nHuman-activities sphere) and cross-spheres with one hundred expert-curated\nevaluation dimensions. Leveraging observational data from satellite sensors and\nin-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four\ntiers: perception, general reasoning, scientific knowledge reasoning and\nchain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per\nsphere to establish authoritative evaluation dimensions and curate relevant\nobservational datasets, 40 crowd-sourcing annotators to assist experts for\nannotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd\nworkflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs\nreveal that even the most advanced models struggle with our benchmarks, where\nnone of them reach 35\\% accuracy. Especially, in some cross-spheres tasks, the\nperformance of leading models like GPT-4o drops to 0.0\\%. OmniEarth-Bench sets\na new standard for geosystem-aware AI, advancing both scientific discovery and\npractical applications in environmental monitoring and disaster prediction. The\ndataset, source code, and trained models were released.", "AI": {"tldr": "OmniEarth-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5730\u7403\u79d1\u5b66\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u516d\u4e2a\u5730\u7403\u79d1\u5b66\u9886\u57df\u53ca\u5176\u4ea4\u53c9\u9886\u57df\uff0c\u5305\u542b100\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5730\u7403\u79d1\u5b66\u591a\u6a21\u6001\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u5728\u8986\u76d6\u8303\u56f4\u548c\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u5730\u7403\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002", "method": "\u5229\u7528\u536b\u661f\u4f20\u611f\u5668\u548c\u5b9e\u5730\u89c2\u6d4b\u6570\u636e\uff0c\u6574\u540829,779\u4e2a\u6807\u6ce8\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u4e00\u822c\u63a8\u7406\u3001\u79d1\u5b66\u77e5\u8bc6\u63a8\u7406\u548c\u94fe\u5f0f\u63a8\u7406\u56db\u4e2a\u5c42\u6b21\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u548c\u4f17\u5305\u534f\u4f5c\u51cf\u5c11\u6807\u7b7e\u6b67\u4e49\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684MLLM\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4e0d\u8db335%\uff0c\u67d0\u4e9b\u4ea4\u53c9\u9886\u57df\u4efb\u52a1\u4e2dGPT-4o\u7684\u51c6\u786e\u7387\u964d\u81f30%\u3002", "conclusion": "OmniEarth-Bench\u4e3a\u5730\u7403\u79d1\u5b66AI\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u53d1\u73b0\u548c\u73af\u5883\u76d1\u6d4b\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u76f8\u5173\u6570\u636e\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.23701", "pdf": "https://arxiv.org/pdf/2505.23701", "abs": "https://arxiv.org/abs/2505.23701", "authors": ["Ziling Cheng", "Meng Cao", "Leila Pishdad", "Yanshuai Cao", "Jackie Chi Kit Cheung"], "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation", "categories": ["cs.CL"], "comment": null, "summary": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\uff0c\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u8bc4\u4f30\u6307\u6807\u5728\u6570\u5b66\u5e94\u7528\u9898\u4e2d\u4e3b\u8981\u53d7\u7b97\u672f\u8ba1\u7b97\u800c\u975e\u62bd\u8c61\u516c\u5f0f\u5316\u7684\u9650\u5236\uff0cCoT\uff08\u601d\u7ef4\u94fe\uff09\u4e3b\u8981\u5e2e\u52a9\u8ba1\u7b97\u800c\u975e\u62bd\u8c61\u601d\u7ef4\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6570\u5b66\u5e94\u7528\u9898\u4e2d\u7684\u8868\u73b0\u662f\u5426\u771f\u6b63\u53cd\u6620\u5176\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u6700\u7ec8\u7b54\u6848\u6307\u6807\u53ef\u80fd\u63a9\u76d6\u7684\u4e24\u4e2a\u5b50\u6280\u80fd\uff08\u62bd\u8c61\u516c\u5f0f\u5316\u548c\u7b97\u672f\u8ba1\u7b97\uff09\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7GSM8K\u548cSVAMP\u6570\u636e\u96c6\u5bf9Llama-3\u548cQwen2.5\u8fdb\u884c\u89e3\u8026\u8bc4\u4f30\uff0c\u5206\u6790CoT\u7684\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u4fee\u8865\u9a8c\u8bc1\u62bd\u8c61\u5316\u673a\u5236\u3002", "result": "\u53d1\u73b0\u7b97\u672f\u8ba1\u7b97\u662f\u74f6\u9888\uff0cCoT\u5bf9\u8ba1\u7b97\u5e2e\u52a9\u663e\u8457\u4f46\u5bf9\u62bd\u8c61\u516c\u5f0f\u5316\u5f71\u54cd\u6709\u9650\uff0c\u6a21\u578b\u901a\u8fc7\u62bd\u8c61\u5316\u540e\u8ba1\u7b97\u7684\u673a\u5236\u8fd0\u4f5c\u3002", "conclusion": "\u9700\u89e3\u8026\u8bc4\u4f30\u4ee5\u51c6\u786e\u8861\u91cfLLM\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u6539\u8fdb\u3002"}}
{"id": "2505.23524", "pdf": "https://arxiv.org/pdf/2505.23524", "abs": "https://arxiv.org/abs/2505.23524", "authors": ["Rui Xia", "Dan Jiang", "Quan Zhang", "Ke Zhang", "Chun Yuan"], "title": "CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Localization (TAL) has garnered significant attention in\ninformation retrieval. Existing supervised or weakly supervised methods heavily\nrely on labeled temporal boundaries and action categories, which are\nlabor-intensive and time-consuming. Consequently, unsupervised temporal action\nlocalization (UTAL) has gained popularity. However, current methods face two\nmain challenges: 1) Classification pre-trained features overly focus on highly\ndiscriminative regions; 2) Solely relying on visual modality information makes\nit difficult to determine contextual boundaries. To address these issues, we\npropose a CLIP-assisted cross-view audiovisual enhanced UTAL method.\nSpecifically, we introduce visual language pre-training (VLP) and\nclassification pre-training-based collaborative enhancement to avoid excessive\nfocus on highly discriminative regions; we also incorporate audio perception to\nprovide richer contextual boundary information. Finally, we introduce a\nself-supervised cross-view learning paradigm to achieve multi-view perceptual\nenhancement without additional annotations. Extensive experiments on two public\ndatasets demonstrate our model's superiority over several state-of-the-art\ncompetitors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u8de8\u6a21\u6001\u65e0\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u97f3\u9891\u611f\u77e5\u589e\u5f3a\u4e0a\u4e0b\u6587\u8fb9\u754c\u4fe1\u606f\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u89c6\u89c9\u7279\u5f81\u8fc7\u4e8e\u5173\u6ce8\u9ad8\u533a\u5206\u6027\u533a\u57df\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u4fe1\u606f\u652f\u6301\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u5206\u7c7b\u9884\u8bad\u7ec3\u534f\u4f5c\u589e\u5f3a\uff0c\u5f15\u5165\u97f3\u9891\u611f\u77e5\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u8de8\u89c6\u89d2\u5b66\u4e60\u8303\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u4e2d\u7684\u7279\u5f81\u8fc7\u805a\u7126\u548c\u4e0a\u4e0b\u6587\u8fb9\u754c\u95ee\u9898\u3002"}}
{"id": "2505.23713", "pdf": "https://arxiv.org/pdf/2505.23713", "abs": "https://arxiv.org/abs/2505.23713", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Jiayi Ye", "Haomin Zhuang", "Zirui Song", "Lang Gao", "Chenxi Wang", "Zhaorun Chen", "Yujun Zhou", "Sixian Li", "Wang Pan", "Yue Zhao", "Jieyu Zhao", "Xiangliang Zhang", "Xiuying Chen"], "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "Code available at https://github.com/xzx34/SocialMaze", "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86SocialMaze\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u8fc7\u4e8e\u7b80\u5316\u73b0\u5b9e\u573a\u666f\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLMs\u7684\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u3002", "method": "SocialMaze\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff08\u6df1\u5ea6\u63a8\u7406\u3001\u52a8\u6001\u4ea4\u4e92\u548c\u4fe1\u606f\u4e0d\u786e\u5b9a\u6027\uff09\u548c\u516d\u9879\u4efb\u52a1\uff0c\u8986\u76d6\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u3001\u65e5\u5e38\u751f\u6d3b\u4e92\u52a8\u548c\u6570\u5b57\u793e\u533a\u5e73\u53f0\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u4e92\u548c\u4fe1\u606f\u6574\u5408\u80fd\u529b\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u5f3a\u94fe\u5f0f\u63a8\u7406\u6a21\u578b\u5728\u6df1\u5ea6\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u4e0d\u786e\u5b9a\u6027\u4e0b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u793e\u4f1a\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0cSocialMaze\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.23525", "pdf": "https://arxiv.org/pdf/2505.23525", "abs": "https://arxiv.org/abs/2505.23525", "authors": ["Jiahao Cui", "Yan Chen", "Mingwang Xu", "Hanlin Shang", "Yuxuan Chen", "Yun Zhan", "Zilong Dong", "Yao Yao", "Jingdong Wang", "Siyu Zhu"], "title": "Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation", "categories": ["cs.CV"], "comment": null, "summary": "Generating highly dynamic and photorealistic portrait animations driven by\naudio and skeletal motion remains challenging due to the need for precise lip\nsynchronization, natural facial expressions, and high-fidelity body motion\ndynamics. We propose a human-preference-aligned diffusion framework that\naddresses these challenges through two key innovations. First, we introduce\ndirect preference optimization tailored for human-centric animation, leveraging\na curated dataset of human preferences to align generated outputs with\nperceptual metrics for portrait motion-video alignment and naturalness of\nexpression. Second, the proposed temporal motion modulation resolves\nspatiotemporal resolution mismatches by reshaping motion conditions into\ndimensionally aligned latent features through temporal channel redistribution\nand proportional feature expansion, preserving the fidelity of high-frequency\nmotion details in diffusion-based synthesis. The proposed mechanism is\ncomplementary to existing UNet and DiT-based portrait diffusion approaches, and\nexperiments demonstrate obvious improvements in lip-audio synchronization,\nexpression vividness, body motion coherence over baseline methods, alongside\nnotable gains in human preference metrics. Our model and source code can be\nfound at: https://github.com/xyz123xyz456/hallo4.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u7684\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u5ea6\u52a8\u6001\u548c\u903c\u771f\u7684\u8096\u50cf\u52a8\u753b\uff0c\u89e3\u51b3\u4e86\u5507\u540c\u6b65\u3001\u81ea\u7136\u8868\u60c5\u548c\u9ad8\u4fdd\u771f\u8eab\u4f53\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u6311\u6218\u3002", "motivation": "\u751f\u6210\u9ad8\u5ea6\u52a8\u6001\u548c\u903c\u771f\u7684\u8096\u50cf\u52a8\u753b\u5728\u5507\u540c\u6b65\u3001\u81ea\u7136\u8868\u60c5\u548c\u8eab\u4f53\u8fd0\u52a8\u52a8\u529b\u5b66\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u504f\u597d\u4f18\u5316\u548c\u65f6\u7a7a\u8fd0\u52a8\u8c03\u5236\uff0c\u5c06\u8fd0\u52a8\u6761\u4ef6\u8f6c\u5316\u4e3a\u7ef4\u5ea6\u5bf9\u9f50\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u4fdd\u7559\u9ad8\u9891\u8fd0\u52a8\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5507\u97f3\u9891\u540c\u6b65\u3001\u8868\u60c5\u751f\u52a8\u6027\u548c\u8eab\u4f53\u8fd0\u52a8\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u504f\u597d\u6307\u6807\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8096\u50cf\u52a8\u753b\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.23714", "pdf": "https://arxiv.org/pdf/2505.23714", "abs": "https://arxiv.org/abs/2505.23714", "authors": ["Roksana Goworek", "Harpal Karlcut", "Muhammad Shezad", "Nijaguna Darshana", "Abhishek Mane", "Syam Bondada", "Raghav Sikka", "Ulvi Mammadov", "Rauf Allahverdiyev", "Sriram Purighella", "Paridhi Gupta", "Muhinyia Ndegwa", "Haim Dubossarsky"], "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 22 figures, submitted to SIGTYP 2025 workshop in ACL", "summary": "This paper addresses the critical need for high-quality evaluation datasets\nin low-resource languages to advance cross-lingual transfer. While\ncross-lingual transfer offers a key strategy for leveraging multilingual\npretraining to expand language technologies to understudied and typologically\ndiverse languages, its effectiveness is dependent on quality and suitable\nbenchmarks. We release new sense-annotated datasets of sentences containing\npolysemous words, spanning nine low-resource languages across diverse language\nfamilies and scripts. To facilitate dataset creation, the paper presents a\ndemonstrably beneficial semi-automatic annotation method. The utility of the\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\nthat evaluate transfer on these low-resource languages. Results highlight the\nimportance of targeted dataset creation and evaluation for effective polysemy\ndisambiguation in low-resource settings and transfer studies. The released\ndatasets and code aim to support further research into fair, robust, and truly\nmultilingual NLP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u6db5\u76d6\u4e5d\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u591a\u4e49\u8bcd\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8de8\u8bed\u8a00\u8fc1\u79fb\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u9ad8\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u8de8\u8bed\u8a00\u8fc1\u79fb\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u521b\u5efa\u591a\u4e49\u8bcd\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884cWiC\u683c\u5f0f\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u6027\u7684\u6570\u636e\u96c6\u521b\u5efa\u548c\u8bc4\u4f30\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u591a\u4e49\u6d88\u6b67\u548c\u8fc1\u79fb\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u53d1\u5e03\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u65e8\u5728\u652f\u6301\u66f4\u516c\u5e73\u3001\u7a33\u5065\u548c\u771f\u6b63\u591a\u8bed\u8a00\u7684NLP\u7814\u7a76\u3002"}}
{"id": "2505.23543", "pdf": "https://arxiv.org/pdf/2505.23543", "abs": "https://arxiv.org/abs/2505.23543", "authors": ["Jan Ignatowicz", "Krzysztof Kutt", "Grzegorz J. Nalepa"], "title": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications", "categories": ["cs.CV"], "comment": null, "summary": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u8bed\u4e49\u6280\u672f\u7684\u5143\u6570\u636e\u4e30\u5bcc\u6a21\u578b\uff08MEM\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u6536\u85cf\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "motivation": "\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u7684\u5143\u6570\u636e\u4e0d\u8db3\u9650\u5236\u4e86\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u8de8\u673a\u6784\u534f\u4f5c\uff0c\u73b0\u6709\u89c6\u89c9\u5206\u6790\u6a21\u578b\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u624b\u7a3f\uff09\u7684\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faMEM\u6846\u67b6\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u591a\u5c42\u89c6\u89c9\u673a\u5236\uff08MVM\uff09\u52a8\u6001\u68c0\u6d4b\u5d4c\u5957\u7279\u5f81\u3002", "result": "\u5728Jagiellonian\u6570\u5b57\u56fe\u4e66\u9986\u7684\u6570\u5b57\u5316\u624b\u7a3f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MEM\u7684\u6f5c\u529b\uff0c\u5e76\u53d1\u5e03\u4e86105\u9875\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u3002", "conclusion": "MEM\u4e3a\u6587\u5316\u9057\u4ea7\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4eba\u5de5\u667a\u80fd\u4e0e\u8bed\u4e49\u6280\u672f\u5728\u5b9e\u8df5\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23715", "pdf": "https://arxiv.org/pdf/2505.23715", "abs": "https://arxiv.org/abs/2505.23715", "authors": ["Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models", "categories": ["cs.CL"], "comment": "31 pages,13 figures,15 tables", "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPCBench\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u524d\u63d0\u6279\u5224\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u6307\u51fa\u5176\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\u4e14\u80fd\u529b\u4e0e\u63a8\u7406\u80fd\u529b\u4e0d\u76f8\u5173\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u524d\u63d0\u9519\u8bef\u65f6\u8868\u73b0\u8106\u5f31\uff0c\u7f3a\u4e4f\u81ea\u4e3b\u6279\u5224\u80fd\u529b\uff0c\u9700\u63d0\u5347\u5176\u524d\u63d0\u6279\u5224\u80fd\u529b\u4ee5\u589e\u5f3a\u53ef\u9760\u6027\u3002", "method": "\u8bbe\u8ba1PCBench\uff0c\u5305\u542b\u56db\u79cd\u9519\u8bef\u7c7b\u578b\u548c\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u8bc4\u4f3015\u79cd\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u4f9d\u8d56\u663e\u5f0f\u63d0\u793a\uff0c\u6279\u5224\u80fd\u529b\u4e0e\u96be\u5ea6\u548c\u9519\u8bef\u7c7b\u578b\u76f8\u5173\uff0c\u63a8\u7406\u80fd\u529b\u4e0e\u6279\u5224\u80fd\u529b\u4e0d\u76f8\u5173\uff0c\u9519\u8bef\u524d\u63d0\u5bfc\u81f4\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "\u5f3a\u8c03\u63d0\u5347\u6a21\u578b\u81ea\u4e3b\u524d\u63d0\u6279\u5224\u80fd\u529b\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.23558", "pdf": "https://arxiv.org/pdf/2505.23558", "abs": "https://arxiv.org/abs/2505.23558", "authors": ["Xu Chu", "Xinrong Chen", "Guanyu Wang", "Zhijie Tan", "Kui Huang", "Wenyu Lv", "Tong Mo", "Weiping Li"], "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at:\nhttps://github.com/Liar406/Look_Again.", "AI": {"tldr": "Qwen-LA\u901a\u8fc7\u5f15\u5165\u89c6\u89c9-\u6587\u672c\u53cd\u5c04\u8fc7\u7a0b\uff0c\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u957f\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u89c6\u89c9\u4fe1\u606f\u88ab\u5ffd\u89c6\uff0c\u5f15\u53d1\u5e7b\u89c9\uff0c\u4ec5\u6587\u672c\u53cd\u5c04\u4e0d\u8db3\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51faBRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u4ee4\u724cCOPY\u548cROUTE\uff0c\u5f3a\u5236\u6a21\u578b\u91cd\u65b0\u5173\u6ce8\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9QA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "Qwen-LA\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u53cd\u5c04\u6709\u6548\u63d0\u5347\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u3002"}}
{"id": "2505.23722", "pdf": "https://arxiv.org/pdf/2505.23722", "abs": "https://arxiv.org/abs/2505.23722", "authors": ["Fan Bai", "Hamid Hassanzadeh", "Ardavan Saeedi", "Mark Dredze"], "title": "Label-Guided In-Context Learning for Named Entity Recognition", "categories": ["cs.CL"], "comment": "Preprint", "summary": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings.", "AI": {"tldr": "DEER\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u6807\u7b7e\u7684token\u7ea7\u7edf\u8ba1\u4fe1\u606f\uff0c\u6539\u8fdb\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ICL\u65b9\u6cd5\u5728NER\u4e2d\u4ec5\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u9009\u62e9\u793a\u4f8b\uff0c\u5ffd\u7565\u4e86\u8bad\u7ec3\u6807\u7b7e\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "DEER\u7ed3\u5408\u6807\u7b7e\u5f15\u5bfc\u7684token\u68c0\u7d22\u5668\u548c\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u4f18\u5316\u793a\u4f8b\u9009\u62e9\u5e76\u9488\u5bf9\u6027\u4fee\u6b63\u9519\u8bef\u3002", "result": "\u5728\u4e94\u4e2aNER\u6570\u636e\u96c6\u548c\u56db\u79cdLLM\u4e0a\uff0cDEER\u8868\u73b0\u4f18\u4e8e\u73b0\u6709ICL\u65b9\u6cd5\uff0c\u63a5\u8fd1\u76d1\u7763\u5fae\u8c03\u6c34\u5e73\u3002", "conclusion": "DEER\u5728\u5df2\u77e5\u548c\u672a\u77e5\u5b9e\u4f53\u4e0a\u5747\u6709\u6548\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u7a33\u5065\u3002"}}
{"id": "2505.23566", "pdf": "https://arxiv.org/pdf/2505.23566", "abs": "https://arxiv.org/abs/2505.23566", "authors": ["Yu Li", "Jin Jiang", "Jianhua Zhu", "Shuai Peng", "Baole Wei", "Yuxuan Zhou", "Liangcai Gao"], "title": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Mathematical Expression Recognition (HMER) remains a persistent\nchallenge in Optical Character Recognition (OCR) due to the inherent freedom of\nsymbol layout and variability in handwriting styles. Prior methods have faced\nperformance bottlenecks, proposing isolated architectural modifications that\nare difficult to integrate coherently into a unified framework. Meanwhile,\nrecent advances in pretrained vision-language models (VLMs) have demonstrated\nstrong cross-task generalization, offering a promising foundation for\ndeveloping unified solutions. In this paper, we introduce Uni-MuMER, which\nfully fine-tunes a VLM for the HMER task without modifying its architecture,\neffectively injecting domain-specific knowledge into a generalist framework.\nOur method integrates three data-driven tasks: Tree-Aware Chain-of-Thought\n(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for\nreducing confusion among visually similar characters, and Symbol Counting (SC)\nfor improving recognition consistency in long expressions. Experiments on the\nCROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art\nperformance, surpassing the best lightweight specialized model SSAN by 16.31%\nand the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting.\nOur datasets, models, and code are open-sourced at:\nhttps://github.com/BFlameSwift/Uni-MuMER", "AI": {"tldr": "Uni-MuMER\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\uff0c\u901a\u8fc7\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u4efb\u52a1\u63d0\u5347\u6027\u80fd\uff0c\u5728CROHME\u548cHME100K\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u3002", "motivation": "HMER\u56e0\u7b26\u53f7\u5e03\u5c40\u81ea\u7531\u548c\u624b\u5199\u98ce\u683c\u591a\u53d8\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\u3002\u9884\u8bad\u7ec3VLM\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u4e3a\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "Uni-MuMER\u5b8c\u5168\u5fae\u8c03VLM\u800c\u4e0d\u4fee\u6539\u67b6\u6784\uff0c\u6574\u5408\u4e86Tree-CoT\uff08\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406\uff09\u3001EDL\uff08\u51cf\u5c11\u76f8\u4f3c\u5b57\u7b26\u6df7\u6dc6\uff09\u548cSC\uff08\u63d0\u5347\u957f\u8868\u8fbe\u5f0f\u4e00\u81f4\u6027\uff09\u4e09\u79cd\u4efb\u52a1\u3002", "result": "\u5728CROHME\u548cHME100K\u6570\u636e\u96c6\u4e0a\uff0cUni-MuMER\u8d85\u8d8aSSAN\uff0816.31%\uff09\u548cGemini2.5-flash\uff0824.42%\uff09\uff0c\u8fbe\u5230SOTA\u3002", "conclusion": "Uni-MuMER\u901a\u8fc7\u5fae\u8c03VLM\u548c\u6574\u5408\u6570\u636e\u9a71\u52a8\u4efb\u52a1\uff0c\u4e3aHMER\u63d0\u4f9b\u4e86\u9ad8\u6548\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2505.23723", "pdf": "https://arxiv.org/pdf/2505.23723", "abs": "https://arxiv.org/abs/2505.23723", "authors": ["Zexi Liu", "Jingyi Chai", "Xinyu Zhu", "Shuo Tang", "Rui Ye", "Bo Zhang", "Lei Bai", "Siheng Chen"], "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406\u5f0f\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316LLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u65e0\u6cd5\u6839\u636e\u5b9e\u9a8c\u7ecf\u9a8c\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u56e0\u6b64\u63a2\u7d22\u57fa\u4e8e\u5b66\u4e60\u7684\u4ee3\u7406\u5f0f\u673a\u5668\u5b66\u4e60\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u5305\u542b\u63a2\u7d22\u589e\u5f3a\u5fae\u8c03\u3001\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u548c\u7edf\u4e00\u5956\u52b1\u6a21\u5757\u7684\u6846\u67b6\uff0c\u8bad\u7ec37B\u89c4\u6a21\u7684ML-Agent\u3002", "result": "ML-Agent\u5728\u4ec59\u4e2a\u4efb\u52a1\u4e0a\u8bad\u7ec3\u540e\uff0c\u6027\u80fd\u8d85\u8d8a671B\u89c4\u6a21\u7684DeepSeek-R1\uff0c\u5e76\u5c55\u73b0\u51fa\u6301\u7eed\u6539\u8fdb\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4ee3\u7406\u5f0f\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.23586", "pdf": "https://arxiv.org/pdf/2505.23586", "abs": "https://arxiv.org/abs/2505.23586", "authors": ["Ziyong Wang", "Charith Abhayaratne"], "title": "Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "This paper was presented at the British Machine Vision Conference\n  2024 workshop on Media authenticity in the age of artificial intelligence", "summary": "The explosive growth of digital images and the widespread availability of\nimage editing tools have made image manipulation detection an increasingly\ncritical challenge. Current deep learning-based manipulation detection methods\nexcel in achieving high image-level classification accuracy, they often fall\nshort in terms of interpretability and localization of manipulated regions.\nAdditionally, the absence of pixel-wise annotations in real-world scenarios\nlimits the existing fully-supervised manipulation localization techniques. To\naddress these challenges, we propose a novel weakly-supervised approach that\nintegrates activation maps generated by image-level manipulation detection\nnetworks with segmentation maps from pre-trained models. Specifically, we build\non our previous image-level work named WCBnet to produce multi-view feature\nmaps which are subsequently fused for coarse localization. These coarse maps\nare then refined using detailed segmented regional information provided by\npre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),\nwith Bayesian inference employed to enhance the manipulation localization.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting the feasibility to localize image manipulations without relying on\npixel-level labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u56fe\u50cf\u7be1\u6539\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u50cf\u7ea7\u68c0\u6d4b\u7f51\u7edc\u548c\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff0c\u65e0\u9700\u50cf\u7d20\u7ea7\u6807\u6ce8\u5373\u53ef\u5b9a\u4f4d\u7be1\u6539\u533a\u57df\u3002", "motivation": "\u6570\u5b57\u56fe\u50cf\u548c\u7f16\u8f91\u5de5\u5177\u7684\u666e\u53ca\u4f7f\u7be1\u6539\u68c0\u6d4b\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u5b9a\u4f4d\u80fd\u529b\u4e0a\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002", "method": "\u57fa\u4e8eWCBnet\u751f\u6210\u591a\u89c6\u89d2\u7279\u5f81\u56fe\uff0c\u4e0e\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff08\u5982DeepLab\uff09\u7ed3\u5408\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u4f18\u5316\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b9a\u4f4d\u7be1\u6539\u533a\u57df\uff0c\u65e0\u9700\u4f9d\u8d56\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002", "conclusion": "\u5f31\u76d1\u7763\u65b9\u6cd5\u5728\u7be1\u6539\u5b9a\u4f4d\u4e2d\u5177\u6709\u53ef\u884c\u6027\uff0c\u89e3\u51b3\u4e86\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2505.23729", "pdf": "https://arxiv.org/pdf/2505.23729", "abs": "https://arxiv.org/abs/2505.23729", "authors": ["Mohamad Chehade", "Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Dinesh Manocha", "Hao Zhu", "Amrit Singh Bedi"], "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.", "AI": {"tldr": "SITAlign\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u65f6\u95f4\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\u5e76\u6ee1\u8db3\u6b21\u8981\u6807\u51c6\u7684\u9608\u503c\u7ea6\u675f\uff0c\u89e3\u51b3\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u4eba\u7c7b\u504f\u597d\u53cd\u9988\u89c6\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5ffd\u7565\u4e86\u4eba\u7c7b\u51b3\u7b56\u7684\u5b9e\u9645\u65b9\u5f0f\uff08\u5982\u6ee1\u610f\u7b56\u7565\uff09\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u8d34\u8fd1\u4eba\u7c7b\u51b3\u7b56\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSITAlign\u6846\u67b6\uff0c\u5728\u63a8\u7406\u65f6\u6700\u5927\u5316\u4e3b\u8981\u76ee\u6807\uff0c\u540c\u65f6\u6ee1\u8db3\u6b21\u8981\u6807\u51c6\u7684\u9608\u503c\u7ea6\u675f\uff0c\u5e76\u63a8\u5bfc\u4e86\u7406\u8bba\u4e0a\u7684\u6b21\u4f18\u6027\u754c\u9650\u3002", "result": "\u5728PKU-SafeRLHF\u6570\u636e\u96c6\u4e0a\uff0cSITAlign\u5728\u4fdd\u6301\u65e0\u5bb3\u6027\u9608\u503c\u7684\u540c\u65f6\uff0c\u5c06GPT-4\u7684\u80dc\u7387\u63d0\u9ad8\u4e8622.3%\u3002", "conclusion": "SITAlign\u901a\u8fc7\u6ee1\u610f\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\u3002"}}
{"id": "2505.23587", "pdf": "https://arxiv.org/pdf/2505.23587", "abs": "https://arxiv.org/abs/2505.23587", "authors": ["Christian Schmidt", "Heinrich Martin Overhoff"], "title": "PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "In medical image segmentation, limited external validity remains a critical\nobstacle when models are deployed across unseen datasets, an issue particularly\npronounced in the ultrasound image domain. Existing solutions-such as domain\nadaptation and GAN-based style transfer-while promising, often fall short in\nthe medical domain where datasets are typically small and diverse. This paper\npresents a novel application of principal component analysis (PCA) to address\nthis limitation. PCA preprocessing reduces noise and emphasizes essential\nfeatures by retaining approximately 90\\% of the dataset variance. We evaluate\nour approach across six diverse breast tumor ultrasound datasets comprising\n3,983 B-mode images and corresponding expert tumor segmentation masks. For each\ndataset, a corresponding dimensionality reduced PCA-dataset is created and\nU-Net-based segmentation models are trained on each of the twelve datasets.\nEach model trained on an original dataset was inferenced on the remaining five\nout-of-domain original datasets (baseline results), while each model trained on\na PCA dataset was inferenced on five out-of-domain PCA datasets. Our\nexperimental results indicate that using PCA reconstructed datasets, instead of\noriginal images, improves the model's recall and Dice scores, particularly for\nmodel-dataset pairs where baseline performance was lowest, achieving\nstatistically significant gains in recall (0.57 $\\pm$ 0.07 vs. 0.70 $\\pm$ 0.05,\n$p = 0.0004$) and Dice scores (0.50 $\\pm$ 0.06 vs. 0.58 $\\pm$ 0.06, $p =\n0.03$). Our method reduced the decline in recall values due to external\nvalidation by $33\\%$. These findings underscore the potential of PCA\nreconstruction as a safeguard to mitigate declines in segmentation performance,\nespecially in challenging cases, with implications for enhancing external\nvalidity in real-world medical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u533b\u5b66\u8d85\u58f0\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u7684\u5916\u90e8\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPCA\u9884\u5904\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u548cDice\u5206\u6570\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u90e8\u7f72\u65f6\u5916\u90e8\u6709\u6548\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u8d85\u58f0\u56fe\u50cf\u9886\u57df\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57df\u9002\u5e94\u548cGAN\uff09\u5728\u5c0f\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u6548\u679c\u6709\u9650\u3002", "method": "\u901a\u8fc7PCA\u9884\u5904\u7406\u964d\u566a\u5e76\u4fdd\u755990%\u7684\u65b9\u5dee\uff0c\u751f\u6210PCA\u91cd\u5efa\u6570\u636e\u96c6\u3002\u5728\u516d\u4e2a\u4e73\u817a\u80bf\u7624\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3U-Net\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u539f\u59cb\u6570\u636e\u96c6\u548cPCA\u6570\u636e\u96c6\u7684\u8868\u73b0\u3002", "result": "PCA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53ec\u56de\u7387\uff080.57\u21920.70\uff09\u548cDice\u5206\u6570\uff080.50\u21920.58\uff09\uff0c\u5e76\u5c06\u5916\u90e8\u9a8c\u8bc1\u5bfc\u81f4\u7684\u53ec\u56de\u7387\u4e0b\u964d\u51cf\u5c11\u4e8633%\u3002", "conclusion": "PCA\u91cd\u5efa\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u7684\u5916\u90e8\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u6848\u4f8b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.23735", "pdf": "https://arxiv.org/pdf/2505.23735", "abs": "https://arxiv.org/abs/2505.23735", "authors": ["Ali Behrouz", "Zeman Li", "Praneeth Kacham", "Majid Daliri", "Yuan Deng", "Peilin Zhong", "Meisam Razaviyayn", "Vahab Mirrokni"], "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faATLAS\uff0c\u4e00\u79cd\u9ad8\u5bb9\u91cf\u7684\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u901a\u8fc7\u4f18\u5316\u5f53\u524d\u548c\u8fc7\u53bb\u4ee4\u724c\u7684\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u73b0\u4ee3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u5e8f\u5217\u5916\u63a8\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "Transformer\u5728\u957f\u5e8f\u5217\u4e2d\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u53d7\u9650\uff0c\u800c\u73b0\u4ee3\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u5185\u5b58\u5bb9\u91cf\u3001\u5728\u7ebf\u66f4\u65b0\u548c\u56fa\u5b9a\u5185\u5b58\u7ba1\u7406\u3002", "method": "\u63d0\u51faATLAS\u6a21\u5757\uff0c\u4f18\u5316\u8bb0\u5fc6\u7ba1\u7406\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1DeepTransformers\uff0c\u6269\u5c55\u4e86\u539f\u59cbTransformer\u67b6\u6784\u3002", "result": "ATLAS\u5728\u8bed\u8a00\u5efa\u6a21\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8d85\u8d8aTransformer\u548c\u7ebf\u6027\u5faa\u73af\u6a21\u578b\uff0c\u5728BABILong\u57fa\u51c6\u4e0a\u5b9e\u73b0+80%\u51c6\u786e\u7387\u3002", "conclusion": "ATLAS\u901a\u8fc7\u6539\u8fdb\u8bb0\u5fc6\u7ba1\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.23590", "pdf": "https://arxiv.org/pdf/2505.23590", "abs": "https://arxiv.org/abs/2505.23590", "authors": ["Zifu Wang", "Junyi Zhu", "Bo Tang", "Zhiyu Li", "Feiyu Xiong", "Jiaqian Yu", "Matthew B. Blaschko"], "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL using jigsaw\npuzzles as a structured experimental framework, revealing several key findings.\n\\textit{Firstly,} we find that MLLMs, initially performing near to random\nguessing on simple puzzles, achieve near-perfect accuracy and generalize to\ncomplex, unseen configurations through fine-tuning. \\textit{Secondly,} training\non jigsaw puzzles can induce generalization to other visual tasks, with\neffectiveness tied to specific task configurations. \\textit{Thirdly,} MLLMs can\nlearn and generalize with or without explicit reasoning, though open-source\nmodels often favor direct answering. Consequently, even when trained for\nstep-by-step reasoning, they can ignore the thinking process in deriving the\nfinal answer. \\textit{Fourthly,} we observe that complex reasoning patterns\nappear to be pre-existing rather than emergent, with their frequency increasing\nalongside training and task difficulty. \\textit{Finally,} our results\ndemonstrate that RL exhibits more effective generalization than Supervised\nFine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL\noptimization. Although these observations are based on jigsaw puzzles and may\nvary across other visual tasks, this research contributes a valuable piece of\njigsaw to the larger puzzle of collective understanding rule-based visual RL\nand its potential in multimodal learning. The code is available at:\n\\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u62fc\u56fe\u4efb\u52a1\u4e3a\u5b9e\u9a8c\u6846\u67b6\uff0c\u53d1\u73b0MLLMs\u901a\u8fc7\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u6cdb\u5316\u5230\u590d\u6742\u4efb\u52a1\uff0c\u4e14RL\u6bd4\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u66f4\u6709\u6548\u3002", "motivation": "\u63a2\u7d22MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u89c4\u5219\u7684RL\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u62fc\u56fe\u4efb\u52a1\u4f5c\u4e3a\u5b9e\u9a8c\u6846\u67b6\uff0c\u5bf9\u6bd4RL\u548cSFT\u7684\u6027\u80fd\uff0c\u5206\u6790MLLMs\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u6a21\u5f0f\u3002", "result": "MLLMs\u901a\u8fc7\u5fae\u8c03\u5728\u62fc\u56fe\u4efb\u52a1\u4e2d\u8fbe\u5230\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u89c6\u89c9\u4efb\u52a1\uff1bRL\u6bd4SFT\u8868\u73b0\u66f4\u597d\uff0c\u4e14SFT\u521d\u59cb\u9636\u6bb5\u53ef\u80fd\u963b\u788dRL\u4f18\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u57fa\u4e8e\u89c4\u5219\u7684\u89c6\u89c9RL\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u7ed3\u679c\u53ef\u80fd\u56e0\u4efb\u52a1\u800c\u5f02\u3002"}}
{"id": "2505.23754", "pdf": "https://arxiv.org/pdf/2505.23754", "abs": "https://arxiv.org/abs/2505.23754", "authors": ["Ziyin Zhang", "Jiahao Xu", "Zhiwei He", "Tian Liang", "Qiuzhi Liu", "Yansi Li", "Linfeng Song", "Zhengwen Liang", "Zhuosheng Zhang", "Rui Wang", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.", "AI": {"tldr": "DeepTheorem\u662f\u4e00\u4e2a\u5229\u7528\u81ea\u7136\u8bed\u8a00\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u7684\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u6846\u67b6\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u7406\u8bc1\u660e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u65b9\u6cd5\u4f9d\u8d56\u5f62\u5f0f\u5316\u8bc1\u660e\u7cfb\u7edf\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u77e5\u8bc6\u4e0d\u5339\u914d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9002\u5e94\u5176\u4f18\u52bf\u7684\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u6846\u67b6\u3002", "method": "\u63d0\u51faDeepTheorem\u6846\u67b6\uff0c\u5305\u542b121K\u9ad8\u8d28\u91cf\u975e\u6b63\u5f0f\u5b9a\u7406\u548c\u8bc1\u660e\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e13\u4e3a\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u8bbe\u8ba1\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff08RL-Zero\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepTheorem\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u7406\u8bc1\u660e\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u3002", "conclusion": "DeepTheorem\u6709\u6f5c\u529b\u4ece\u6839\u672c\u4e0a\u63a8\u52a8\u975e\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u548c\u6570\u5b66\u63a2\u7d22\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.23595", "pdf": "https://arxiv.org/pdf/2505.23595", "abs": "https://arxiv.org/abs/2505.23595", "authors": ["Youssef Mohamed", "Noran Mohamed", "Khaled Abouhashad", "Feilong Tang", "Sara Atito", "Shoaib Jameel", "Imran Razzak", "Ahmed B. Zaky"], "title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL", "AI": {"tldr": "DeepChest\u662f\u4e00\u79cd\u52a8\u6001\u4efb\u52a1\u52a0\u6743\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u80f8\u90e8X\u5149\u5206\u7c7b\uff0c\u901a\u8fc7\u6027\u80fd\u9a71\u52a8\u7684\u6743\u91cd\u673a\u5236\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u9886\u57df\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4efb\u52a1\u8d21\u732e\u5e73\u8861\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "DeepChest\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u8d8b\u52bf\u5206\u6790\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff0c\u65e0\u9700\u68af\u5ea6\u8bbf\u95ee\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u5e76\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "\u5728\u5927\u578bCXR\u6570\u636e\u96c6\u4e0a\uff0cDeepChest\u6bd4\u73b0\u6709MTL\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad87%\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4efb\u52a1\u635f\u5931\u3002", "conclusion": "DeepChest\u4e3a\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23759", "pdf": "https://arxiv.org/pdf/2505.23759", "abs": "https://arxiv.org/abs/2505.23759", "authors": ["Heekyung Lee", "Jiaxin Ge", "Tsung-Han Wu", "Minwoo Kang", "Trevor Darrell", "David M. Chan"], "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u51b3\u89c6\u89c9\u8c1c\u9898\uff08rebus puzzles\uff09\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u62bd\u8c61\u63a8\u7406\u548c\u89c6\u89c9\u9690\u55bb\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u591a\u6a21\u6001\u62bd\u8c61\u3001\u7b26\u53f7\u63a8\u7406\u548c\u6587\u5316\u8bed\u8a00\u53cc\u5173\u65b9\u9762\u7684\u8868\u73b0\uff0c\u586b\u8865\u5176\u5728\u590d\u6742\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u624b\u5de5\u751f\u6210\u548c\u6807\u6ce8\u7684\u82f1\u8bed\u89c6\u89c9\u8c1c\u9898\u57fa\u51c6\uff0c\u6db5\u76d6\u4ece\u7b80\u5355\u56fe\u50cf\u66ff\u6362\u5230\u7a7a\u95f4\u4f9d\u8d56\u63d0\u793a\u7684\u591a\u79cd\u7c7b\u578b\u3002", "result": "VLMs\u5728\u89e3\u7801\u7b80\u5355\u89c6\u89c9\u7ebf\u7d22\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u62bd\u8c61\u63a8\u7406\u3001\u6a2a\u5411\u601d\u7ef4\u548c\u89c6\u89c9\u9690\u55bb\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5f53\u524dVLMs\u5728\u89e3\u51b3\u590d\u6742\u89c6\u89c9\u8c1c\u9898\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u62bd\u8c61\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2505.23597", "pdf": "https://arxiv.org/pdf/2505.23597", "abs": "https://arxiv.org/abs/2505.23597", "authors": ["Georgios Voulgaris"], "title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted for publication at the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition (CVPR) EarthVision", "summary": "The accurate semantic segmentation of tree crowns within remotely sensed data\nis crucial for scientific endeavours such as forest management, biodiversity\nstudies, and carbon sequestration quantification. However, precise segmentation\nremains challenging due to complexities in the forest canopy, including\nshadows, intricate backgrounds, scale variations, and subtle spectral\ndifferences among tree species. Compared to the traditional methods, Deep\nLearning models improve accuracy by extracting informative and discriminative\nfeatures, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model\nincorporating a Logarithmic Gabor-parameterised convolutional layer with\ntrainable filter parameters, alongside a backbone that extracts salient\nfeatures while capturing extensive context and spatial information through a\nwider receptive field. We investigate the impact of Log-Gabor, Gabor, and\nstandard convolutional layers on semantic segmentation performance through\nextensive experimentation. Additionally, we conduct an ablation study to assess\nthe contributions of individual layers and their combinations to overall model\nperformance, and we evaluate PerceptiveNet as a backbone within a novel hybrid\nCNN-Transformer model. Our results outperform state-of-the-art models,\ndemonstrating significant performance improvements on a tree crown dataset\nwhile generalising across domains, including two benchmark aerial scene\nsemantic segmentation datasets with varying complexities.", "AI": {"tldr": "PerceptiveNet\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6570Gabor\u5377\u79ef\u5c42\u548c\u5bbd\u611f\u53d7\u91ce\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u6811\u51a0\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7cbe\u786e\u7684\u6811\u51a0\u8bed\u4e49\u5206\u5272\u5bf9\u68ee\u6797\u7ba1\u7406\u3001\u751f\u7269\u591a\u6837\u6027\u7814\u7a76\u548c\u78b3\u5c01\u5b58\u91cf\u5316\u7684\u79d1\u5b66\u5de5\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u68ee\u6797\u51a0\u5c42\u7279\u5f81\u3002", "method": "\u63d0\u51faPerceptiveNet\uff0c\u7ed3\u5408\u5bf9\u6570Gabor\u53c2\u6570\u5316\u5377\u79ef\u5c42\u548c\u5bbd\u611f\u53d7\u91ce\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u63d0\u53d6\u663e\u8457\u7279\u5f81\u5e76\u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u5377\u79ef\u5c42\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u6df7\u5408CNN-Transformer\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u3002", "result": "PerceptiveNet\u5728\u6811\u51a0\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u590d\u6742\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PerceptiveNet\u901a\u8fc7\u521b\u65b0\u7684\u5377\u79ef\u5c42\u8bbe\u8ba1\u548c\u5bbd\u611f\u53d7\u91ce\u7684\u9aa8\u5e72\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6811\u51a0\u8bed\u4e49\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.23765", "pdf": "https://arxiv.org/pdf/2505.23765", "abs": "https://arxiv.org/abs/2505.23765", "authors": ["Wentao Zhang", "Woojeong Kim", "Yuntian Deng"], "title": "From Chat Logs to Collective Insights: Aggregative Question Answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\u2014\u2014\u805a\u5408\u95ee\u7b54\uff08Aggregative Question Answering\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790\u5927\u89c4\u6a21\u7528\u6237\u4e0e\u804a\u5929\u673a\u5668\u4eba\u7684\u5bf9\u8bdd\u6570\u636e\uff0c\u56de\u7b54\u805a\u5408\u6027\u95ee\u9898\uff08\u5982\u8bc6\u522b\u7279\u5b9a\u4eba\u7fa4\u7684\u5173\u6ce8\u70b9\uff09\u3002\u4f5c\u8005\u6784\u5efa\u4e86\u57fa\u51c6\u6570\u636e\u96c6WildChat-AQA\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u63a8\u7406\u6216\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7528\u6237\u4e0e\u804a\u5929\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u89c6\u4e3a\u72ec\u7acb\u4e8b\u4ef6\uff0c\u65e0\u6cd5\u4ece\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u96c6\u4f53\u6d1e\u5bdf\u3002", "method": "\u63d0\u51fa\u4e86\u805a\u5408\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b6,027\u4e2a\u805a\u5408\u6027\u95ee\u9898\u7684WildChat-AQA\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u6548\u63a8\u7406\u6216\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\uff0c\u4ee5\u4ece\u5927\u89c4\u6a21\u5bf9\u8bdd\u6570\u636e\u4e2d\u63d0\u53d6\u96c6\u4f53\u6d1e\u5bdf\u3002"}}
{"id": "2505.23601", "pdf": "https://arxiv.org/pdf/2505.23601", "abs": "https://arxiv.org/abs/2505.23601", "authors": ["Shengyuan Liu", "Boyun Zheng", "Wenting Chen", "Zhihao Peng", "Zhenfei Yin", "Jing Shao", "Jiancong Hu", "Yixuan Yuan"], "title": "A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis", "categories": ["cs.CV"], "comment": "36 pages, 18 figures", "summary": "Endoscopic procedures are essential for diagnosing and treating internal\ndiseases, and multi-modal large language models (MLLMs) are increasingly\napplied to assist in endoscopy analysis. However, current benchmarks are\nlimited, as they typically cover specific endoscopic scenarios and a small set\nof clinical tasks, failing to capture the real-world diversity of endoscopic\nscenarios and the full range of skills needed in clinical workflows. To address\nthese issues, we introduce EndoBench, the first comprehensive benchmark\nspecifically designed to assess MLLMs across the full spectrum of endoscopic\npractice with multi-dimensional capacities. EndoBench encompasses 4 distinct\nendoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,\nand 5 levels of visual prompting granularities, resulting in 6,832 rigorously\nvalidated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation\nframework mirrors the clinical workflow--spanning anatomical recognition,\nlesion analysis, spatial localization, and surgical operations--to holistically\ngauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.\nWe benchmark 23 state-of-the-art models, including general-purpose,\nmedical-specialized, and proprietary MLLMs, and establish human clinician\nperformance as a reference standard. Our extensive experiments reveal: (1)\nproprietary MLLMs outperform open-source and medical-specialized models\noverall, but still trail human experts; (2) medical-domain supervised\nfine-tuning substantially boosts task-specific accuracy; and (3) model\nperformance remains sensitive to prompt format and clinical task complexity.\nEndoBench establishes a new standard for evaluating and advancing MLLMs in\nendoscopy, highlighting both progress and persistent gaps between current\nmodels and expert clinical reasoning. We publicly release our benchmark and\ncode.", "AI": {"tldr": "EndoBench\u662f\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5185\u7aa5\u955c\u5b9e\u8df5\u4e2d\u7684\u591a\u7ef4\u80fd\u529b\uff0c\u6db5\u76d6\u591a\u79cd\u573a\u666f\u548c\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u7279\u5b9a\u5185\u7aa5\u955c\u573a\u666f\u548c\u5c11\u91cf\u4e34\u5e8a\u4efb\u52a1\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6837\u6027\u548c\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u5168\u9762\u9700\u6c42\u3002", "method": "EndoBench\u5305\u542b4\u79cd\u5185\u7aa5\u955c\u573a\u666f\u300112\u9879\u4e34\u5e8a\u4efb\u52a1\u53ca12\u9879\u5b50\u4efb\u52a1\u30015\u79cd\u89c6\u89c9\u63d0\u793a\u7c92\u5ea6\uff0c\u51716,832\u4e2a\u9a8c\u8bc1\u8fc7\u7684VQA\u5bf9\uff0c\u91c7\u7528\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e13\u6709MLLM\u4f18\u4e8e\u5f00\u6e90\u548c\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u4ecd\u4e0d\u53ca\u4eba\u7c7b\u4e13\u5bb6\uff1b\u533b\u5b66\u9886\u57df\u76d1\u7763\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u6027\uff1b\u6a21\u578b\u6027\u80fd\u53d7\u63d0\u793a\u683c\u5f0f\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u5f71\u54cd\u3002", "conclusion": "EndoBench\u4e3a\u5185\u7aa5\u955c\u9886\u57dfMLLM\u7684\u8bc4\u4f30\u548c\u8fdb\u6b65\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5e76\u516c\u5f00\u4e86\u57fa\u51c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2505.22654", "pdf": "https://arxiv.org/pdf/2505.22654", "abs": "https://arxiv.org/abs/2505.22654", "authors": ["Ce Zhang", "Kaixin Ma", "Tianqing Fang", "Wenhao Yu", "Hongming Zhang", "Zhisong Zhang", "Yaqi Xie", "Katia Sycara", "Haitao Mi", "Dong Yu"], "title": "VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent Large Vision-Language Models (LVLMs) have advanced multi-modal\nunderstanding by incorporating finer-grained visual perception and encoding.\nHowever, such methods incur significant computational costs due to longer\nvisual token sequences, posing challenges for real-time deployment. To mitigate\nthis, prior studies have explored pruning unimportant visual tokens either at\nthe output layer of the visual encoder or at the early layers of the language\nmodel. In this work, we revisit these design choices and reassess their\neffectiveness through comprehensive empirical studies of how visual tokens are\nprocessed throughout the visual encoding and language decoding stages. Guided\nby these insights, we propose VScan, a two-stage visual token reduction\nframework that addresses token redundancy by: (1) integrating complementary\nglobal and local scans with token merging during visual encoding, and (2)\nintroducing pruning at intermediate layers of the language model. Extensive\nexperimental results across four LVLMs validate the effectiveness of VScan in\naccelerating inference and demonstrate its superior performance over current\nstate-of-the-arts on sixteen benchmarks. Notably, when applied to\nLLaVA-NeXT-7B, VScan achieves a 2.91$\\times$ speedup in prefilling and a\n10$\\times$ reduction in FLOPs, while retaining 95.4% of the original\nperformance.", "AI": {"tldr": "VScan\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u89c6\u89c9\u4ee4\u724c\u51cf\u5c11\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u626b\u63cf\u7ed3\u5408\u4ee4\u724c\u5408\u5e76\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u526a\u679d\uff0c\u663e\u8457\u52a0\u901f\u63a8\u7406\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u56e0\u89c6\u89c9\u4ee4\u724c\u5e8f\u5217\u8f83\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\uff0c\u9700\u4f18\u5316\u4ee4\u724c\u5904\u7406\u6548\u7387\u3002", "method": "VScan\u5728\u89c6\u89c9\u7f16\u7801\u9636\u6bb5\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u626b\u63cf\u4e0e\u4ee4\u724c\u5408\u5e76\uff0c\u5e76\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u5f15\u5165\u526a\u679d\u3002", "result": "\u5728\u56db\u4e2aLVLMs\u4e0a\u9a8c\u8bc1\uff0cVScan\u663e\u8457\u52a0\u901f\u63a8\u7406\uff08\u5982LLaVA-NeXT-7B\u63d0\u901f2.91\u500d\uff0cFLOPs\u51cf\u5c1110\u500d\uff09\uff0c\u6027\u80fd\u4fdd\u755995.4%\u3002", "conclusion": "VScan\u901a\u8fc7\u4f18\u5316\u4ee4\u724c\u5904\u7406\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.22756", "pdf": "https://arxiv.org/pdf/2505.22756", "abs": "https://arxiv.org/abs/2505.22756", "authors": ["Tian Qin", "Core Francisco Park", "Mujin Kwun", "Aaron Walsman", "Eran Malach", "Nikhil Anand", "Hidenori Tanaka", "David Alvarez-Melis"], "title": "Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Mathematical reasoning tasks have become prominent benchmarks for assessing\nthe reasoning capabilities of LLMs, especially with reinforcement learning (RL)\nmethods such as GRPO showing significant performance gains. However, accuracy\nmetrics alone do not support fine-grained assessment of capabilities and fail\nto reveal which problem-solving skills have been internalized. To better\nunderstand these capabilities, we propose to decompose problem solving into\nfundamental capabilities: Plan (mapping questions to sequences of steps),\nExecute (correctly performing solution steps), and Verify (identifying the\ncorrectness of a solution). Empirically, we find that GRPO mainly enhances the\nexecution skill-improving execution robustness on problems the model already\nknows how to solve-a phenomenon we call temperature distillation. More\nimportantly, we show that RL-trained models struggle with fundamentally new\nproblems, hitting a 'coverage wall' due to insufficient planning skills. To\nexplore RL's impact more deeply, we construct a minimal, synthetic\nsolution-tree navigation task as an analogy for mathematical problem-solving.\nThis controlled setup replicates our empirical findings, confirming RL\nprimarily boosts execution robustness. Importantly, in this setting, we\nidentify conditions under which RL can potentially overcome the coverage wall\nthrough improved exploration and generalization to new solution paths. Our\nfindings provide insights into the role of RL in enhancing LLM reasoning,\nexpose key limitations, and suggest a path toward overcoming these barriers.\nCode is available at https://github.com/cfpark00/RL-Wall.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5206\u89e3\u4e3a\u8ba1\u5212\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u4e2a\u57fa\u672c\u80fd\u529b\uff0c\u53d1\u73b0GRPO\u4e3b\u8981\u901a\u8fc7\u6e29\u5ea6\u84b8\u998f\u589e\u5f3a\u6267\u884c\u80fd\u529b\uff0c\u4f46RL\u8bad\u7ec3\u6a21\u578b\u5728\u89e3\u51b3\u65b0\u95ee\u9898\u65f6\u56e0\u8ba1\u5212\u80fd\u529b\u4e0d\u8db3\u9047\u5230\u2018\u8986\u76d6\u5899\u2019\u3002\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1RL\u4e3b\u8981\u63d0\u5347\u6267\u884c\u9c81\u68d2\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u514b\u670d\u8986\u76d6\u5899\u7684\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRL\u7684\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ec5\u4f9d\u8d56\u51c6\u786e\u6027\u6307\u6807\u65e0\u6cd5\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u5c24\u5176\u662f\u95ee\u9898\u89e3\u51b3\u6280\u80fd\u7684\u638c\u63e1\u60c5\u51b5\u3002", "method": "\u5c06\u95ee\u9898\u89e3\u51b3\u5206\u89e3\u4e3a\u8ba1\u5212\u3001\u6267\u884c\u548c\u9a8c\u8bc1\u4e09\u4e2a\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u5408\u6210\u4efb\u52a1\u5206\u6790RL\u5bf9\u8fd9\u4e9b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "GRPO\u901a\u8fc7\u6e29\u5ea6\u84b8\u998f\u589e\u5f3a\u6267\u884c\u80fd\u529b\uff0c\u4f46RL\u6a21\u578b\u5728\u65b0\u95ee\u9898\u4e0a\u56e0\u8ba1\u5212\u80fd\u529b\u4e0d\u8db3\u9047\u5230\u2018\u8986\u76d6\u5899\u2019\uff1b\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1RL\u4e3b\u8981\u63d0\u5347\u6267\u884c\u9c81\u68d2\u6027\uff0c\u5e76\u53d1\u73b0\u53ef\u80fd\u514b\u670d\u8986\u76d6\u5899\u7684\u6761\u4ef6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86RL\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23629", "pdf": "https://arxiv.org/pdf/2505.23629", "abs": "https://arxiv.org/abs/2505.23629", "authors": ["Xiang Xiang Wang", "Tin-Yau Tam"], "title": "Color Image Set Recognition Based on Quaternionic Grassmannians", "categories": ["cs.CV", "math.AG"], "comment": null, "summary": "We propose a new method for recognizing color image sets using quaternionic\nGrassmannians, which use the power of quaternions to capture color information\nand represent each color image set as a point on the quaternionic Grassmannian.\nWe provide a direct formula to calculate the shortest distance between two\npoints on the quaternionic Grassmannian, and use this distance to build a new\nclassification framework. Experiments on the ETH-80 benchmark dataset show that\nour method achieves good recognition results. We also discuss some limitations\nin stability and suggest ways the method can be improved in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u5143\u6570Grassmannian\u7684\u5f69\u8272\u56fe\u50cf\u96c6\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u5143\u6570\u6355\u6349\u989c\u8272\u4fe1\u606f\uff0c\u5c06\u56fe\u50cf\u96c6\u8868\u793a\u4e3aGrassmannian\u4e0a\u7684\u70b9\uff0c\u5e76\u5229\u7528\u6700\u77ed\u8ddd\u79bb\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "\u5229\u7528\u56db\u5143\u6570\u7684\u4f18\u52bf\u66f4\u6709\u6548\u5730\u6355\u6349\u5f69\u8272\u56fe\u50cf\u4fe1\u606f\uff0c\u63d0\u5347\u8bc6\u522b\u6548\u679c\u3002", "method": "\u5c06\u5f69\u8272\u56fe\u50cf\u96c6\u8868\u793a\u4e3a\u56db\u5143\u6570Grassmannian\u4e0a\u7684\u70b9\uff0c\u8ba1\u7b97\u6700\u77ed\u8ddd\u79bb\u5e76\u6784\u5efa\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u5728ETH-80\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u8bc6\u522b\u6548\u679c\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u4f46\u7a33\u5b9a\u6027\u6709\u5f85\u6539\u8fdb\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2505.22758", "pdf": "https://arxiv.org/pdf/2505.22758", "abs": "https://arxiv.org/abs/2505.22758", "authors": ["Aniruddha Nrusimha", "William Brandon", "Mayank Mishra", "Yikang Shen", "Rameswar Panda", "Jonathan Ragan-Kelley", "Yoon Kim"], "title": "FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The size and compute characteristics of modern large language models have led\nto an increased interest in developing specialized kernels tailored for\ntraining and inference. Existing kernels primarily optimize for compute\nutilization, targeting the large-batch training and inference settings.\nHowever, low-batch inference, where memory bandwidth and kernel launch\noverheads contribute are significant factors, remains important for many\napplications of interest such as in edge deployment and latency-sensitive\napplications. This paper describes FlashFormer, a proof-of-concept kernel for\naccelerating single-batch inference for transformer-based large language\nmodels. Across various model sizes and quantizations settings, we observe\nnontrivial speedups compared to existing state-of-the-art inference kernels.", "AI": {"tldr": "FlashFormer\u662f\u4e00\u79cd\u4e13\u4e3a\u5355\u6279\u6b21\u63a8\u7406\u4f18\u5316\u7684\u5185\u6838\uff0c\u9488\u5bf9Transformer\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8fb9\u7f18\u90e8\u7f72\u548c\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u5185\u6838\u4e3b\u8981\u9488\u5bf9\u5927\u6279\u6b21\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u800c\u4f4e\u6279\u6b21\u63a8\u7406\u5728\u5185\u5b58\u5e26\u5bbd\u548c\u5185\u6838\u542f\u52a8\u5f00\u9500\u65b9\u9762\u4ecd\u6709\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u7f18\u90e8\u7f72\u548c\u5ef6\u8fdf\u654f\u611f\u5e94\u7528\u4e2d\u3002", "method": "\u5f00\u53d1\u4e86FlashFormer\uff0c\u4e00\u79cd\u4e13\u4e3a\u5355\u6279\u6b21\u63a8\u7406\u4f18\u5316\u7684\u5185\u6838\uff0c\u9002\u7528\u4e8eTransformer\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0cFlashFormer\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u63a8\u7406\u5185\u6838\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "conclusion": "FlashFormer\u4e3a\u4f4e\u6279\u6b21\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u548c\u5ef6\u8fdf\u654f\u611f\u573a\u666f\u3002"}}
{"id": "2505.23637", "pdf": "https://arxiv.org/pdf/2505.23637", "abs": "https://arxiv.org/abs/2505.23637", "authors": ["Dashti A. Ali", "Richard K. G. Do", "William R. Jarnagin", "Aras T. Asaad", "Amber L. Simpson"], "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u4e24\u79cd\u57fa\u4e8e\u6301\u4e45\u540c\u8c03\u7684\u7279\u5f81\u5411\u91cf\u6784\u5efa\u65b9\u6cd5\uff0c\u53d1\u73b0\u7279\u5f81\u62fc\u63a5\u4f18\u4e8e\u805a\u5408\u65b9\u6cd5\uff0c\u80fd\u4fdd\u7559\u66f4\u591a\u62d3\u6251\u4fe1\u606f\u5e76\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u5bf9\u8f93\u5165\u7684\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u800c\u6301\u4e45\u540c\u8c03\uff08PH\uff09\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u62d3\u6251\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u4f46\u5982\u4f55\u4ece\u591a\u4e2a\u6301\u4e45\u6761\u7801\u6784\u5efa\u7279\u5f81\u5411\u91cf\u5c1a\u9700\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff08\u805a\u5408\u6301\u4e45\u6761\u7801\u540e\u7279\u5f81\u5316 vs. \u62fc\u63a5\u5404\u6761\u7801\u7684\u62d3\u6251\u7279\u5f81\u5411\u91cf\uff09\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\u4fdd\u7559\u4e86\u66f4\u591a\u7ec6\u8282\u62d3\u6251\u4fe1\u606f\uff0c\u5206\u7c7b\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u5728\u7c7b\u4f3c\u5b9e\u9a8c\u4e2d\uff0c\u7279\u5f81\u62fc\u63a5\u662f\u66f4\u4f18\u7684\u9009\u62e9\u3002"}}
{"id": "2505.23642", "pdf": "https://arxiv.org/pdf/2505.23642", "abs": "https://arxiv.org/abs/2505.23642", "authors": ["Nathaniel Burgdorfer", "Philippos Mordohai"], "title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we introduce an inference-time optimization framework utilizing\ntriangles to represent the geometry and appearance of the scene. More\nspecifically, we develop a scene optimization algorithm for triangle soup, a\ncollection of disconnected semi-transparent triangle primitives. Compared to\nthe current most-widely used primitives for 3D scene representation, namely\nGaussian splats, triangles allow for more expressive color interpolation, and\nbenefit from a large algorithmic infrastructure for downstream tasks.\nTriangles, unlike full-rank Gaussian kernels, naturally combine to form\nsurfaces. We formulate connectivity forces between triangles during\noptimization, encouraging explicit, but soft, surface continuity in 3D. We\nperform experiments on a representative 3D reconstruction dataset and show\ncompetitive photometric and geometric results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u89d2\u5f62\u7684\u63a8\u7406\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u793a\u573a\u666f\u7684\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u4f18\u4e8e\u5f53\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u3002", "motivation": "\u4e09\u89d2\u5f62\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u989c\u8272\u63d2\u503c\uff0c\u5e76\u53d7\u76ca\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u5927\u91cf\u7b97\u6cd5\u57fa\u7840\u8bbe\u65bd\uff0c\u540c\u65f6\u80fd\u81ea\u7136\u5f62\u6210\u8868\u9762\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u4e09\u89d2\u5f62\u6c64\uff08\u534a\u900f\u660e\u4e09\u89d2\u5f62\u96c6\u5408\uff09\u7684\u573a\u666f\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8fde\u63a5\u529b\u4ee5\u9f13\u52b1\u8868\u9762\u8fde\u7eed\u6027\u3002", "result": "\u5728\u4ee3\u8868\u60273D\u91cd\u5efa\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5149\u5ea6\u5b66\u548c\u51e0\u4f55\u5b66\u7ed3\u679c\u3002", "conclusion": "\u4e09\u89d2\u5f62\u4f5c\u4e3a\u573a\u666f\u8868\u793a\u57fa\u5143\u5177\u6709\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u989c\u8272\u63d2\u503c\u548c\u8868\u9762\u5f62\u6210\u65b9\u9762\u3002"}}
{"id": "2505.22857", "pdf": "https://arxiv.org/pdf/2505.22857", "abs": "https://arxiv.org/abs/2505.22857", "authors": ["Vladimir Bataev", "Andrei Andrusenko", "Lilit Grigoryan", "Aleksandr Laptev", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Statistical n-gram language models are widely used for context-biasing tasks\nin Automatic Speech Recognition (ASR). However, existing implementations lack\ncomputational efficiency due to poor parallelization, making context-biasing\nless appealing for industrial use. This work rethinks data structures for\nstatistical n-gram language models to enable fast and parallel operations for\nGPU-optimized inference. Our approach, named NGPU-LM, introduces customizable\ngreedy decoding for all major ASR model types - including transducers,\nattention encoder-decoder models, and CTC - with less than 7% computational\noverhead. The proposed approach can eliminate more than 50% of the accuracy gap\nbetween greedy and beam search for out-of-domain scenarios while avoiding\nsignificant slowdown caused by beam search. The implementation of the proposed\nNGPU-LM is open-sourced.", "AI": {"tldr": "NGPU-LM\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u5e76\u884c\u7684\u7edf\u8ba1n-gram\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316ASR\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7edf\u8ba1n-gram\u8bed\u8a00\u6a21\u578b\u5728ASR\u4e2d\u56e0\u5e76\u884c\u5316\u4e0d\u8db3\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u6570\u636e\u7ed3\u6784\u548c\u5f15\u5165\u53ef\u5b9a\u5236\u8d2a\u5a6a\u89e3\u7801\uff0c\u652f\u6301GPU\u4f18\u5316\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u591a\u79cdASR\u6a21\u578b\u3002", "result": "\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e8e7%\uff0c\u5728\u57df\u5916\u573a\u666f\u4e2d\u7f29\u5c0f\u4e86\u8d2a\u5a6a\u641c\u7d22\u4e0e\u675f\u641c\u7d2250%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u5dee\u8ddd\u3002", "conclusion": "NGPU-LM\u901a\u8fc7\u9ad8\u6548\u5e76\u884c\u5316\u548c\u4f4e\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86ASR\u4e0a\u4e0b\u6587\u504f\u7f6e\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2505.23656", "pdf": "https://arxiv.org/pdf/2505.23656", "abs": "https://arxiv.org/abs/2505.23656", "authors": ["Xiangdong Zhang", "Jiaqi Liao", "Shaofeng Zhang", "Fanqing Meng", "Xiangpeng Wan", "Junchi Yan", "Yu Cheng"], "title": "VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.", "AI": {"tldr": "VideoREPA\u6846\u67b6\u901a\u8fc7Token Relation Distillation\u635f\u5931\uff0c\u5c06\u89c6\u9891\u7406\u89e3\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u7269\u7406\u77e5\u8bc6\u84b8\u998f\u5230T2V\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u5f53\u524dT2V\u6a21\u578b\u5728\u751f\u6210\u7269\u7406\u5408\u7406\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u7269\u7406\u7406\u89e3\u80fd\u529b\u843d\u540e\u4e8e\u89c6\u9891\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faVideoREPA\u6846\u67b6\uff0c\u5229\u7528Token Relation Distillation\u635f\u5931\u8fdb\u884c\u65f6\u7a7a\u5bf9\u9f50\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3T2V\u6a21\u578b\u3002", "result": "VideoREPA\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u65b9\u6cd5CogVideoX\u7684\u7269\u7406\u5e38\u8bc6\uff0c\u5728\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VideoREPA\u662f\u9996\u4e2a\u4e13\u4e3aT2V\u6a21\u578b\u8bbe\u8ba1\u7684REPA\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.22863", "pdf": "https://arxiv.org/pdf/2505.22863", "abs": "https://arxiv.org/abs/2505.22863", "authors": ["Yupei Li", "Shuaijie Shao", "Manuel Milling", "Bj\u00f6rn W. Schuller"], "title": "Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Depression is a growing concern gaining attention in both public discourse\nand AI research. While deep neural networks (DNNs) have been used for\nrecognition, they still lack real-world effectiveness. Large language models\n(LLMs) show strong potential but require domain-specific fine-tuning and\nstruggle with non-textual cues. Since depression is often expressed through\nvocal tone and behaviour rather than explicit text, relying on language alone\nis insufficient. Diagnostic accuracy also suffers without incorporating\npsychological expertise. To address these limitations, we present, to the best\nof our knowledge, the first application of LLMs to multimodal depression\ndetection using the DAIC-WOZ dataset. We extract the audio features using the\npre-trained model Wav2Vec, and mapped it to text-based LLMs for further\nprocessing. We also propose a novel strategy for incorporating psychological\nknowledge into LLMs to enhance diagnostic performance, specifically using a\nquestion and answer set to grant authorised knowledge to LLMs. Our approach\nyields a notable improvement in both Mean Absolute Error (MAE) and Root Mean\nSquare Error (RMSE) compared to a base score proposed by the related original\npaper. The codes are available at\nhttps://github.com/myxp-lyp/Depression-detection.git", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u97f3\u9891\u7279\u5f81\u548c\u5fc3\u7406\u5b66\u77e5\u8bc6\u7684LLM\u591a\u6a21\u6001\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709DNN\u548cLLM\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u4e2d\u6548\u679c\u6709\u9650\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u5bf9\u975e\u6587\u672c\u7ebf\u7d22\u548c\u5fc3\u7406\u77e5\u8bc6\u7684\u6574\u5408\u3002", "method": "\u4f7f\u7528Wav2Vec\u63d0\u53d6\u97f3\u9891\u7279\u5f81\uff0c\u7ed3\u5408\u6587\u672cLLM\u5904\u7406\uff0c\u5e76\u5f15\u5165\u5fc3\u7406\u5b66\u95ee\u7b54\u77e5\u8bc6\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u5728DAIC-WOZ\u6570\u636e\u96c6\u4e0a\uff0cMAE\u548cRMSE\u6307\u6807\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u5408\u5fc3\u7406\u5b66\u77e5\u8bc6\u7684LLM\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6291\u90c1\u75c7\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.23660", "pdf": "https://arxiv.org/pdf/2505.23660", "abs": "https://arxiv.org/abs/2505.23660", "authors": ["Ziteng Gao", "Mike Zheng Shou"], "title": "D-AR: Diffusion via Autoregressive Models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR", "AI": {"tldr": "D-AR\u5c06\u56fe\u50cf\u6269\u6563\u8fc7\u7a0b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6807\u51c6\u7684\u81ea\u56de\u5f52\u8fc7\u7a0b\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u56fe\u50cf\u4e3a\u5e8f\u5217\u5e76\u5229\u7528\u6269\u6563\u7279\u6027\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u751f\u6210\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u5229\u7528\u6269\u6563\u7279\u6027\u7b80\u5316\u89c6\u89c9\u5408\u6210\u8fc7\u7a0b\u3002", "method": "\u8bbe\u8ba1\u79bb\u6563\u5316tokenizer\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5e8f\u5217\uff0c\u5229\u7528\u81ea\u56de\u5f52\u6a21\u578b\u9884\u6d4btoken\uff0c\u76f4\u63a5\u6620\u5c04\u5230\u6269\u6563\u53bb\u566a\u6b65\u9aa4\u3002", "result": "\u5728ImageNet\u4e0a\u8fbe\u52302.09 FID\uff0c\u652f\u6301\u9884\u89c8\u548c\u96f6\u6837\u672c\u5e03\u5c40\u63a7\u5236\u5408\u6210\u3002", "conclusion": "D-AR\u4e3a\u89c6\u89c9\u5408\u6210\u7684\u7edf\u4e00\u81ea\u56de\u5f52\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2505.22907", "pdf": "https://arxiv.org/pdf/2505.22907", "abs": "https://arxiv.org/abs/2505.22907", "authors": ["Rachel Katharine Sterken", "James Ravi Kirkpatrick"], "title": "Conversational Alignment with Artificial Intelligence in Context", "categories": ["cs.CY", "cs.CL"], "comment": "20 pages, to be published in Philosophical Perspectives", "summary": "The development of sophisticated artificial intelligence (AI) conversational\nagents based on large language models raises important questions about the\nrelationship between human norms, values, and practices and AI design and\nperformance. This article explores what it means for AI agents to be\nconversationally aligned to human communicative norms and practices for\nhandling context and common ground and proposes a new framework for evaluating\ndevelopers' design choices. We begin by drawing on the philosophical and\nlinguistic literature on conversational pragmatics to motivate a set of\ndesiderata, which we call the CONTEXT-ALIGN framework, for conversational\nalignment with human communicative practices. We then suggest that current\nlarge language model (LLM) architectures, constraints, and affordances may\nimpose fundamental limitations on achieving full conversational alignment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5bf9\u8bdd\u4ee3\u7406\u5982\u4f55\u4e0e\u4eba\u7c7b\u6c9f\u901a\u89c4\u8303\u5bf9\u9f50\uff0c\u63d0\u51fa\u4e86CONTEXT-ALIGN\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u5f53\u524dLLM\u67b6\u6784\u53ef\u80fd\u9650\u5236\u5b8c\u5168\u5bf9\u9f50\u3002", "motivation": "\u7814\u7a76AI\u5bf9\u8bdd\u4ee3\u7406\u4e0e\u4eba\u7c7b\u6c9f\u901a\u89c4\u8303\u7684\u5173\u7cfb\uff0c\u786e\u4fddAI\u8bbe\u8ba1\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u5b9e\u8df5\u3002", "method": "\u7ed3\u5408\u54f2\u5b66\u548c\u8bed\u8a00\u5b66\u6587\u732e\uff0c\u63d0\u51faCONTEXT-ALIGN\u6846\u67b6\uff0c\u8bc4\u4f30LLM\u7684\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u5f53\u524dLLM\u67b6\u6784\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u6c9f\u901a\u89c4\u8303\u7684\u5bf9\u9f50\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u514b\u670dLLM\u5728\u5bf9\u8bdd\u5bf9\u9f50\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.23661", "pdf": "https://arxiv.org/pdf/2505.23661", "abs": "https://arxiv.org/abs/2505.23661", "authors": ["Size Wu", "Zhonghua Wu", "Zerui Gong", "Qingyi Tao", "Sheng Jin", "Qinyue Li", "Wei Li", "Chen Change Loy"], "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.", "AI": {"tldr": "OpenUni\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5f00\u6e90\u7684\u7edf\u4e00\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u8bad\u7ec3\u7b56\u7565\u548c\u7b80\u5355\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u548c\u5353\u8d8a\u7684\u57fa\u51c6\u6027\u80fd\u3002", "motivation": "\u53d7\u7edf\u4e00\u6a21\u578b\u5b66\u4e60\u5b9e\u8df5\u7684\u542f\u53d1\uff0c\u65e8\u5728\u7b80\u5316\u8bad\u7ec3\u590d\u6742\u6027\u5e76\u964d\u4f4e\u5f00\u9500\uff0c\u540c\u65f6\u652f\u6301\u591a\u6a21\u6001\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u53ef\u5b66\u4e60\u67e5\u8be2\u548c\u8f7b\u91cf\u7ea7Transformer\u8fde\u63a5\u5668\uff0c\u7ed3\u5408\u73b0\u6210\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u7b26\u5408\u6307\u4ee4\u7684\u56fe\u50cf\uff0c\u5728GenEval\u3001DPG-Bench\u548cWISE\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97001.1B\u548c3.1B\u6fc0\u6d3b\u53c2\u6570\u3002", "conclusion": "OpenUni\u5c55\u793a\u4e86\u7b80\u5355\u67b6\u6784\u7684\u9ad8\u6548\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u6743\u91cd\u3001\u8bad\u7ec3\u4ee3\u7801\u548c\u6570\u636e\u96c6\uff0c\u4ee5\u63a8\u52a8\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2505.22928", "pdf": "https://arxiv.org/pdf/2505.22928", "abs": "https://arxiv.org/abs/2505.22928", "authors": ["Massimiliano Pronesti", "Michela Lorandi", "Paul Flanagan", "Oisin Redmon", "Anya Belz", "Yufang Hou"], "title": "Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Systematic reviews in medicine play a critical role in evidence-based\ndecision-making by aggregating findings from multiple studies. A central\nbottleneck in automating this process is extracting numeric evidence and\ndetermining study-level conclusions for specific outcomes and comparisons.\nPrior work has framed this problem as a textual inference task by retrieving\nrelevant content fragments and inferring conclusions from them. However, such\napproaches often rely on shallow textual cues and fail to capture the\nunderlying numeric reasoning behind expert assessments.\n  In this work, we conceptualise the problem as one of quantitative reasoning.\nRather than inferring conclusions from surface text, we extract structured\nnumerical evidence (e.g., event counts or standard deviations) and apply domain\nknowledge informed logic to derive outcome-specific conclusions. We develop a\nnumeric reasoning system composed of a numeric data extraction model and an\neffect estimate component, enabling more accurate and interpretable inference\naligned with the domain expert principles. We train the numeric data extraction\nmodel using different strategies, including supervised fine-tuning (SFT) and\nreinforcement learning (RL) with a new value reward model.\n  When evaluated on the CochraneForest benchmark, our best-performing approach\n-- using RL to train a small-scale number extraction model -- yields up to a\n21% absolute improvement in F1 score over retrieval-based systems and\noutperforms general-purpose LLMs of over 400B parameters by up to 9%. Our\nresults demonstrate the promise of reasoning-driven approaches for automating\nsystematic evidence synthesis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9a\u91cf\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u503c\u8bc1\u636e\u548c\u5e94\u7528\u9886\u57df\u77e5\u8bc6\u903b\u8f91\uff0c\u6539\u8fdb\u4e86\u533b\u5b66\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u7ed3\u8bba\u63a8\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u533b\u5b66\u7cfb\u7edf\u7efc\u8ff0\u4e2d\u7684\u6570\u503c\u8bc1\u636e\u63d0\u53d6\u548c\u7ed3\u8bba\u63a8\u65ad\u5b58\u5728\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u6587\u672c\u7ebf\u7d22\uff0c\u65e0\u6cd5\u6355\u6349\u4e13\u5bb6\u8bc4\u4f30\u7684\u6570\u503c\u63a8\u7406\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u503c\u63a8\u7406\u7cfb\u7edf\uff0c\u5305\u62ec\u6570\u503c\u6570\u636e\u63d0\u53d6\u6a21\u578b\u548c\u6548\u5e94\u4f30\u8ba1\u7ec4\u4ef6\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728CochraneForest\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u65b9\u6cd5\uff08\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5c0f\u89c4\u6a21\u6570\u503c\u63d0\u53d6\u6a21\u578b\uff09\u6bd4\u68c0\u7d22\u7cfb\u7edfF1\u5206\u6570\u63d0\u534721%\uff0c\u4f18\u4e8e400B\u53c2\u6570\u7684\u5927\u6a21\u578b9%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u7cfb\u7edf\u8bc1\u636e\u5408\u6210\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.23675", "pdf": "https://arxiv.org/pdf/2505.23675", "abs": "https://arxiv.org/abs/2505.23675", "authors": ["Moinak Bhattacharya", "Judy Huang", "Amna F. Sher", "Gagandeep Singh", "Chao Chen", "Prateek Prasanna"], "title": "ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer\n(NSCLC) remains a critical unmet need. Existing radiomics and deep\nlearning-based predictive models rely primarily on pre-treatment imaging to\npredict categorical response outcomes, limiting their ability to capture the\ncomplex morphological and textural transformations induced by immunotherapy.\nThis study introduces ImmunoDiff, an anatomy-aware diffusion model designed to\nsynthesize post-treatment CT scans from baseline imaging while incorporating\nclinically relevant constraints. The proposed framework integrates anatomical\npriors, specifically lobar and vascular structures, to enhance fidelity in CT\nsynthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning\nmodule that ensures pairwise-consistent multimodal integration of imaging and\nclinical data embeddings, to refine the generative process. Additionally, a\nclinical variable conditioning mechanism is introduced, leveraging demographic\ndata, blood-based biomarkers, and PD-L1 expression to refine the generative\nprocess. Evaluations on an in-house NSCLC cohort treated with immune checkpoint\ninhibitors demonstrate a 21.24% improvement in balanced accuracy for response\nprediction and a 0.03 increase in c-index for survival prediction. Code will be\nreleased soon.", "AI": {"tldr": "ImmunoDiff\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6cbb\u7597\u540eCT\u626b\u63cf\u5e76\u7ed3\u5408\u4e34\u5e8a\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86NSCLC\u514d\u75ab\u6cbb\u7597\u53cd\u5e94\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4bNSCLC\u514d\u75ab\u6cbb\u7597\u53cd\u5e94\u662f\u672a\u6ee1\u8db3\u7684\u4e34\u5e8a\u9700\u6c42\uff0c\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u6cbb\u7597\u524d\u5f71\u50cf\u4e14\u65e0\u6cd5\u6355\u6349\u6cbb\u7597\u540e\u590d\u6742\u53d8\u5316\u3002", "method": "\u63d0\u51faImmunoDiff\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u548c\u4e34\u5e8a\u6570\u636e\u5d4c\u5165\uff0c\u901a\u8fc7cbi-Adapter\u6a21\u5757\u548c\u591a\u6a21\u6001\u96c6\u6210\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728NSCLC\u961f\u5217\u4e2d\uff0c\u53cd\u5e94\u9884\u6d4b\u7684\u5e73\u8861\u51c6\u786e\u7387\u63d0\u9ad821.24%\uff0c\u751f\u5b58\u9884\u6d4b\u7684c-index\u589e\u52a00.03\u3002", "conclusion": "ImmunoDiff\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u514d\u75ab\u6cbb\u7597\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.23678", "pdf": "https://arxiv.org/pdf/2505.23678", "abs": "https://arxiv.org/abs/2505.23678", "authors": ["Gabriel Sarch", "Snigdha Saha", "Naitik Khandelwal", "Ayush Jain", "Michael J. Tarr", "Aviral Kumar", "Katerina Fragkiadaki"], "title": "Grounded Reinforcement Learning for Visual Reasoning", "categories": ["cs.CV"], "comment": "Project website: https://visually-grounded-rl.github.io/", "summary": "While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.", "AI": {"tldr": "ViGoRL\u662f\u4e00\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06\u63a8\u7406\u6b65\u9aa4\u951a\u5b9a\u5230\u89c6\u89c9\u5750\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u9700\u8981\u6a21\u578b\u5177\u5907\u89c6\u89c9\u6ce8\u610f\u529b\u3001\u611f\u77e5\u8f93\u5165\u89e3\u91ca\u548c\u7a7a\u95f4\u8bc1\u636e\u62bd\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u7684\u5730\u9762\u673a\u5236\u3002", "method": "ViGoRL\u7ed3\u5408\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u653e\u5927\u9884\u6d4b\u5750\u6807\uff0c\u5e76\u751f\u6210\u7a7a\u95f4\u951a\u5b9a\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViGoRL\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b9a\u4f4d\u5c0fGUI\u5143\u7d20\u548c\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e2d\u8fbe\u523086.4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u89c6\u89c9\u951a\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u662f\u63d0\u5347\u6a21\u578b\u901a\u7528\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u8303\u5f0f\u3002"}}
{"id": "2505.23020", "pdf": "https://arxiv.org/pdf/2505.23020", "abs": "https://arxiv.org/abs/2505.23020", "authors": ["Jinchuan Zhang", "Lu Yin", "Yan Zhou", "Songlin Hu"], "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Submitted to ACL 2025", "summary": "The acquisition of agentic capabilities has transformed LLMs from \"knowledge\nproviders\" to \"action executors\", a trend that while expanding LLMs' capability\nboundaries, significantly increases their susceptibility to malicious use.\nPrevious work has shown that current LLM-based agents execute numerous\nmalicious tasks even without being attacked, indicating a deficiency in agentic\nuse safety alignment during the post-training phase. To address this gap, we\npropose AgentAlign, a novel framework that leverages abstract behavior chains\nas a medium for safety alignment data synthesis. By instantiating these\nbehavior chains in simulated environments with diverse tool instances, our\nframework enables the generation of highly authentic and executable\ninstructions while capturing complex multi-step dynamics. The framework further\nensures model utility by proportionally synthesizing benign instructions\nthrough non-malicious interpretations of behavior chains, precisely calibrating\nthe boundary between helpfulness and harmlessness. Evaluation results on\nAgentHarm demonstrate that fine-tuning three families of open-source models\nusing our method substantially improves their safety (35.8% to 79.5%\nimprovement) while minimally impacting or even positively enhancing their\nhelpfulness, outperforming various prompting methods. The dataset and code have\nboth been open-sourced.", "AI": {"tldr": "AgentAlign\u6846\u67b6\u901a\u8fc7\u62bd\u8c61\u884c\u4e3a\u94fe\u63d0\u5347LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u51cf\u5c11\u6076\u610f\u4efb\u52a1\u6267\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "motivation": "LLM\u4ee3\u7406\u80fd\u529b\u7684\u63d0\u5347\u589e\u52a0\u4e86\u6076\u610f\u4f7f\u7528\u7684\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u62bd\u8c61\u884c\u4e3a\u94fe\u5408\u6210\u5b89\u5168\u5bf9\u9f50\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u751f\u6210\u771f\u5b9e\u53ef\u6267\u884c\u6307\u4ee4\uff0c\u5e76\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "result": "\u5728AgentHarm\u8bc4\u4f30\u4e2d\uff0c\u5b89\u5168\u6027\u63d0\u534735.8%\u81f379.5%\uff0c\u5b9e\u7528\u6027\u5f71\u54cd\u6781\u5c0f\u6216\u6709\u6240\u589e\u5f3a\u3002", "conclusion": "AgentAlign\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u7684\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u3002"}}
{"id": "2505.23693", "pdf": "https://arxiv.org/pdf/2505.23693", "abs": "https://arxiv.org/abs/2505.23693", "authors": ["Tingyu Song", "Tongyan Hu", "Guo Gan", "Yilun Zhao"], "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ACL 2025 Main", "summary": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVF-Eval\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728AI\u751f\u6210\u5185\u5bb9\uff08AIGC\uff09\u89c6\u9891\u4e0a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u81ea\u7136\u89c6\u9891\uff0c\u800c\u5ffd\u7565\u4e86\u5408\u6210\u89c6\u9891\uff08\u5982AIGC\uff09\u7684\u8bc4\u4f30\uff0c\u540c\u65f6MLLMs\u5728\u89e3\u91caAIGC\u89c6\u9891\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faVF-Eval\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u4efb\u52a1\uff08\u4e00\u81f4\u6027\u9a8c\u8bc1\u3001\u9519\u8bef\u611f\u77e5\u3001\u9519\u8bef\u7c7b\u578b\u68c0\u6d4b\u548c\u63a8\u7406\u8bc4\u4f30\uff09\uff0c\u5e76\u8bc4\u4f30\u4e8613\u79cd\u524d\u6cbfMLLMs\u7684\u8868\u73b0\u3002", "result": "\u5373\u4f7f\u662f\u8868\u73b0\u6700\u4f73\u7684GPT-4.1\u6a21\u578b\uff0c\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u4e5f\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u8868\u660e\u57fa\u51c6\u7684\u6311\u6218\u6027\u3002\u901a\u8fc7RePrompt\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86MLLMs\u4e0e\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\u5bf9\u89c6\u9891\u751f\u6210\u7684\u6539\u8fdb\u4f5c\u7528\u3002", "conclusion": "VF-Eval\u63ed\u793a\u4e86MLLMs\u5728AIGC\u89c6\u9891\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5176\u5728\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23039", "pdf": "https://arxiv.org/pdf/2505.23039", "abs": "https://arxiv.org/abs/2505.23039", "authors": ["Kapil Vaidya", "Jialin Ding", "Sebastian Kosak", "David Kernert", "Chuan Lei", "Xiao Qin", "Abhinav Tripathy", "Ramesh Balan", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "TailorSQL: An NL2SQL System Tailored to Your Query Workload", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "NL2SQL (natural language to SQL) translates natural language questions into\nSQL queries, thereby making structured data accessible to non-technical users,\nserving as the foundation for intelligent data applications. State-of-the-art\nNL2SQL techniques typically perform translation by retrieving database-specific\ninformation, such as the database schema, and invoking a pre-trained large\nlanguage model (LLM) using the question and retrieved information to generate\nthe SQL query.\n  However, existing NL2SQL techniques miss a key opportunity which is present\nin real-world settings: NL2SQL is typically applied on existing databases which\nhave already served many SQL queries in the past. The past query workload\nimplicitly contains information which is helpful for accurate NL2SQL\ntranslation and is not apparent from the database schema alone, such as common\njoin paths and the semantics of obscurely-named tables and columns. We\nintroduce TailorSQL, a NL2SQL system that takes advantage of information in the\npast query workload to improve both the accuracy and latency of translating\nnatural language questions into SQL. By specializing to a given workload,\nTailorSQL achieves up to 2$\\times$ improvement in execution accuracy on\nstandardized benchmarks.", "AI": {"tldr": "TailorSQL\u5229\u7528\u5386\u53f2\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u4fe1\u606f\u6539\u8fdbNL2SQL\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53472\u500d\u6267\u884c\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709NL2SQL\u6280\u672f\u672a\u5145\u5206\u5229\u7528\u6570\u636e\u5e93\u4e2d\u5df2\u6709\u7684\u5386\u53f2\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\uff08\u5982\u5e38\u89c1\u8fde\u63a5\u8def\u5f84\u548c\u8868/\u5217\u8bed\u4e49\uff09\u5bf9\u51c6\u786e\u7ffb\u8bd1\u81f3\u5173\u91cd\u8981\u3002", "method": "TailorSQL\u901a\u8fc7\u5206\u6790\u5386\u53f2\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u63d0\u53d6\u6709\u7528\u4fe1\u606f\uff08\u5982\u5e38\u89c1\u8fde\u63a5\u8def\u5f84\u548c\u8868/\u5217\u8bed\u4e49\uff09\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u66f4\u51c6\u786e\u7684SQL\u67e5\u8be2\u3002", "result": "TailorSQL\u5728\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u6267\u884c\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u5229\u7528\u5386\u53f2\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u4fe1\u606f\u53ef\u663e\u8457\u63d0\u5347NL2SQL\u7684\u6027\u80fd\uff0cTailorSQL\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23694", "pdf": "https://arxiv.org/pdf/2505.23694", "abs": "https://arxiv.org/abs/2505.23694", "authors": ["Li Ren", "Chen Chen", "Liqiang Wang", "Kien Hua"], "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Visual Prompt Tuning (VPT) has become a promising solution for\nParameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)\nmodels by partially fine-tuning learnable tokens while keeping most model\nparameters frozen. Recent research has explored modifying the connection\nstructures of the prompts. However, the fundamental correlation and\ndistribution between the prompts and image tokens remain unexplored. In this\npaper, we leverage metric learning techniques to investigate how the\ndistribution of prompts affects fine-tuning performance. Specifically, we\npropose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to\nguide the distributions of the prompts by learning the distance metric from\ntheir class-related semantic data. Our method demonstrates that the prompts can\nserve as an effective bridge to share semantic information between image\npatches and the class token. We extensively evaluated our approach on popular\nbenchmarks in both recognition and segmentation tasks. The results demonstrate\nthat our approach enables more effective and efficient fine-tuning of ViT\nmodels by leveraging semantic information to guide the learning of the prompts,\nleading to improved performance on various downstream vision tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDA-VPT\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5ea6\u91cf\u5b66\u4e60\u6280\u672f\u7814\u7a76\u63d0\u793a\u5206\u5e03\u5bf9\u5fae\u8c03\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u4f18\u5316\u63d0\u793a\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347ViT\u6a21\u578b\u5728\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u63d0\u793a\u4e0e\u56fe\u50cf\u6807\u8bb0\u4e4b\u95f4\u7684\u57fa\u672c\u5173\u8054\u548c\u5206\u5e03\uff0c\u4ee5\u4f18\u5316\u89c6\u89c9\u63d0\u793a\u8c03\u8c10\uff08VPT\uff09\u7684\u6548\u679c\u3002", "method": "\u63d0\u51faDA-VPT\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7c7b\u522b\u76f8\u5173\u8bed\u4e49\u6570\u636e\u7684\u8ddd\u79bb\u5ea6\u91cf\u6765\u5f15\u5bfc\u63d0\u793a\u7684\u5206\u5e03\u3002", "result": "\u5728\u8bc6\u522b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660eDA-VPT\u80fd\u66f4\u9ad8\u6548\u5730\u5fae\u8c03ViT\u6a21\u578b\u3002", "conclusion": "DA-VPT\u901a\u8fc7\u8bed\u4e49\u4fe1\u606f\u5f15\u5bfc\u63d0\u793a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23049", "pdf": "https://arxiv.org/pdf/2505.23049", "abs": "https://arxiv.org/abs/2505.23049", "authors": ["Tianteng Gu", "Bei Liu", "Bo Xiao", "Ke Zeng", "Jiacheng Liu", "Yanmin Qian"], "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenoiseRotator\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u53c2\u6570\u91cd\u8981\u6027\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u526a\u679d\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6743\u91cd\u7684\u91cd\u8981\u6027\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u4fdd\u7559\u6a21\u578b\u5173\u952e\u80fd\u529b\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u6700\u5c0f\u5316\u5f52\u4e00\u5316\u91cd\u8981\u6027\u5206\u6570\u7684\u4fe1\u606f\u71b5\uff0c\u5c06\u91cd\u8981\u6027\u96c6\u4e2d\u5728\u66f4\u5c0f\u7684\u6743\u91cd\u5b50\u96c6\u4e0a\uff0c\u5e76\u5229\u7528DenoiseRotator\u5bf9\u6743\u91cd\u77e9\u9635\u5e94\u7528\u53ef\u5b66\u4e60\u7684\u6b63\u4ea4\u53d8\u6362\u3002", "result": "\u5728LLaMA3\u3001Qwen2.5\u548cMistral\u6a21\u578b\u4e0a\uff0cDenoiseRotator\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\u5dee\u8ddd\uff0c\u4f8b\u5982\u5728LLaMA3-70B\u4e0a\u56f0\u60d1\u5ea6\u5dee\u8ddd\u51cf\u5c11\u4e8658%\u3002", "conclusion": "DenoiseRotator\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u4e0e\u73b0\u6709\u526a\u679d\u6280\u672f\u65e0\u7f1d\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u526a\u679d\u540e\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.23704", "pdf": "https://arxiv.org/pdf/2505.23704", "abs": "https://arxiv.org/abs/2505.23704", "authors": ["Mohamad Alansari", "Sajid Javed", "Iyyakutti Iyappan Ganapathi", "Sara Alansari", "Muzammal Naseer"], "title": "CLDTracker: A Comprehensive Language Description for Visual Tracking", "categories": ["cs.CV", "cs.AI"], "comment": "47 pages, 9 figures, Information Fusion Journal", "summary": "VOT remains a fundamental yet challenging task in computer vision due to\ndynamic appearance changes, occlusions, and background clutter. Traditional\ntrackers, relying primarily on visual cues, often struggle in such complex\nscenarios. Recent advancements in VLMs have shown promise in semantic\nunderstanding for tasks like open-vocabulary detection and image captioning,\nsuggesting their potential for VOT. However, the direct application of VLMs to\nVOT is hindered by critical limitations: the absence of a rich and\ncomprehensive textual representation that semantically captures the target\nobject's nuances, limiting the effective use of language information;\ninefficient fusion mechanisms that fail to optimally integrate visual and\ntextual features, preventing a holistic understanding of the target; and a lack\nof temporal modeling of the target's evolving appearance in the language\ndomain, leading to a disconnect between the initial description and the\nobject's subsequent visual changes. To bridge these gaps and unlock the full\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\nLanguage Description framework for robust visual Tracking. Our tracker\nintroduces a dual-branch architecture consisting of a textual and a visual\nbranch. In the textual branch, we construct a rich bag of textual descriptions\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\nsemantic and contextual cues to address the lack of rich textual\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\nCLDTracker achieves SOTA performance, validating the effectiveness of\nleveraging robust and temporally-adaptive vision-language representations for\ntracking. Code and models are publicly available at:\nhttps://github.com/HamadYA/CLDTracker", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCLDTracker\uff0c\u4e00\u79cd\u57fa\u4e8e\u5168\u9762\u8bed\u8a00\u63cf\u8ff0\u7684\u89c6\u89c9\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ddf\u8e2a\u5668\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\uff08VOT\uff09\u56e0\u52a8\u6001\u5916\u89c2\u53d8\u5316\u3001\u906e\u6321\u548c\u80cc\u666f\u5e72\u6270\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u8ddf\u8e2a\u5668\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\u6548\u679c\u6709\u9650\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u6f5c\u529b\u4e3aVOT\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u6587\u672c\u8868\u793a\u4e0d\u8db3\u3001\u7279\u5f81\u878d\u5408\u4f4e\u6548\u548c\u7f3a\u4e4f\u65f6\u95f4\u5efa\u6a21\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCLDTracker\uff0c\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff08\u6587\u672c\u5206\u652f\u548c\u89c6\u89c9\u5206\u652f\uff09\uff0c\u5229\u7528CLIP\u548cGPT-4V\u7b49VLMs\u751f\u6210\u4e30\u5bcc\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u589e\u5f3a\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6VOT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "conclusion": "CLDTracker\u901a\u8fc7\u5168\u9762\u8bed\u8a00\u63cf\u8ff0\u548c\u65f6\u95f4\u81ea\u9002\u5e94\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86VOT\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.23058", "pdf": "https://arxiv.org/pdf/2505.23058", "abs": "https://arxiv.org/abs/2505.23058", "authors": ["Yutong Xie", "Zhuoheng Li", "Xiyuan Wang", "Yijun Pan", "Qijia Liu", "Xingzhi Cui", "Kuang-Yu Lo", "Ruoyi Gao", "Xingjian Zhang", "Jin Huang", "Walter Yuan", "Matthew O. Jackson", "Qiaozhu Mei"], "title": "Be.FM: Open Foundation Models for Human Behavior", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "Despite their success in numerous fields, the potential of foundation models\nfor modeling and understanding human behavior remains largely unexplored. We\nintroduce Be.FM, one of the first open foundation models designed for human\nbehavior modeling. Built upon open-source large language models and fine-tuned\non a diverse range of behavioral data, Be.FM can be used to understand and\npredict human decision-making. We construct a comprehensive set of benchmark\ntasks for testing the capabilities of behavioral foundation models. Our results\ndemonstrate that Be.FM can predict behaviors, infer characteristics of\nindividuals and populations, generate insights about contexts, and apply\nbehavioral science knowledge.", "AI": {"tldr": "Be.FM\u662f\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u884c\u4e3a\u6570\u636e\u5fae\u8c03\u6784\u5efaBe.FM\u3002", "result": "Be.FM\u80fd\u9884\u6d4b\u884c\u4e3a\u3001\u63a8\u65ad\u4e2a\u4f53\u548c\u7fa4\u4f53\u7279\u5f81\u3001\u751f\u6210\u60c5\u5883\u6d1e\u5bdf\u5e76\u5e94\u7528\u884c\u4e3a\u79d1\u5b66\u77e5\u8bc6\u3002", "conclusion": "Be.FM\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2505.23709", "pdf": "https://arxiv.org/pdf/2505.23709", "abs": "https://arxiv.org/abs/2505.23709", "authors": ["Dionysis Christopoulos", "Sotiris Spanos", "Eirini Baltzi", "Valsamis Ntouskos", "Konstantinos Karantzalos"], "title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning\nrich representations of skin lesions through a novel nested contrastive\nlearning approach that captures complex relationships between images and\nmetadata. Melanoma detection and skin lesion classification based solely on\nimages, pose significant challenges due to large variations in imaging\nconditions (lighting, color, resolution, distance, etc.) and lack of clinical\nand phenotypical context. Clinicians typically follow a holistic approach for\nassessing the risk level of the patient and for deciding which lesions may be\nmalignant and need to be excised, by considering the patient's medical history\nas well as the appearance of other lesions of the patient. Inspired by this,\nSLIMP combines the appearance and the metadata of individual skin lesions with\npatient-level metadata relating to their medical record and other clinically\nrelevant information. By fully exploiting all available data modalities\nthroughout the learning process, the proposed pre-training strategy improves\nperformance compared to other pre-training strategies on downstream skin\nlesions classification tasks highlighting the learned representations quality.", "AI": {"tldr": "SLIMP\u901a\u8fc7\u7ed3\u5408\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u548c\u5143\u6570\u636e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5d4c\u5957\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4f9d\u8d56\u56fe\u50cf\u8fdb\u884c\u9ed1\u8272\u7d20\u7624\u68c0\u6d4b\u548c\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u7684\u6311\u6218\uff0c\u5982\u6210\u50cf\u6761\u4ef6\u5dee\u5f02\u5927\u548c\u7f3a\u4e4f\u4e34\u5e8a\u80cc\u666f\u3002", "method": "\u91c7\u7528\u5d4c\u5957\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7ed3\u5408\u75c5\u53d8\u56fe\u50cf\u3001\u4e2a\u4f53\u5143\u6570\u636e\u548c\u60a3\u8005\u7ea7\u5143\u6570\u636e\u3002", "result": "\u76f8\u6bd4\u5176\u4ed6\u9884\u8bad\u7ec3\u7b56\u7565\uff0cSLIMP\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SLIMP\u901a\u8fc7\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.23091", "pdf": "https://arxiv.org/pdf/2505.23091", "abs": "https://arxiv.org/abs/2505.23091", "authors": ["Zeyu Liu", "Yuhang Liu", "Guanghao Zhu", "Congkai Xie", "Zhen Li", "Jianbo Yuan", "Xinyao Wang", "Qing Li", "Shing-Chi Cheung", "Shengyu Zhang", "Fei Wu", "Hongxia Yang"], "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsubstantial progress in reasoning capabilities, such as DeepSeek-R1, which\nleverages rule-based reinforcement learning to enhance logical reasoning\nsignificantly. However, extending these achievements to multimodal large\nlanguage models (MLLMs) presents critical challenges, which are frequently more\npronounced for Multimodal Small Language Models (MSLMs) given their typically\nweaker foundational reasoning abilities: (1) the scarcity of high-quality\nmultimodal reasoning datasets, (2) the degradation of reasoning capabilities\ndue to the integration of visual processing, and (3) the risk that direct\napplication of reinforcement learning may produce complex yet incorrect\nreasoning processes. To address these challenges, we design a novel framework\nInfi-MMR to systematically unlock the reasoning potential of MSLMs through a\ncurriculum of three carefully structured phases and propose our multimodal\nreasoning model Infi-MMR-3B. The first phase, Foundational Reasoning\nActivation, leverages high-quality textual reasoning datasets to activate and\nstrengthen the model's logical reasoning capabilities. The second phase,\nCross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to\nfacilitate the progressive transfer of reasoning skills to multimodal contexts.\nThe third phase, Multimodal Reasoning Enhancement, employs curated,\ncaption-free multimodal data to mitigate linguistic biases and promote robust\ncross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal\nmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision\ntest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on\nMathVista testmini).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInfi-MMR\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u63d0\u5347\u591a\u6a21\u6001\u5c0f\u8bed\u8a00\u6a21\u578b\uff08MSLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6210\u7ee9\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u591a\u6a21\u6001\u5c0f\u8bed\u8a00\u6a21\u578b\uff08MSLMs\uff09\u9762\u4e34\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u89c6\u89c9\u5904\u7406\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u63a8\u7406\u7b49\u6311\u6218\u3002", "method": "\u8bbe\u8ba1Infi-MMR\u6846\u67b6\uff0c\u5206\u4e09\u4e2a\u9636\u6bb5\uff1a\u57fa\u7840\u63a8\u7406\u6fc0\u6d3b\u3001\u8de8\u6a21\u6001\u63a8\u7406\u9002\u5e94\u548c\u591a\u6a21\u6001\u63a8\u7406\u589e\u5f3a\uff0c\u6700\u7ec8\u63d0\u51fa\u6a21\u578bInfi-MMR-3B\u3002", "result": "Infi-MMR-3B\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\uff08\u5982MathVerse\u3001MathVision\u548cOlympiadBench\uff09\u548c\u901a\u7528\u63a8\u7406\uff08MathVista\uff09\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Infi-MMR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MSLMs\u7684\u63a8\u7406\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.23716", "pdf": "https://arxiv.org/pdf/2505.23716", "abs": "https://arxiv.org/abs/2505.23716", "authors": ["Lihan Jiang", "Yucheng Mao", "Linning Xu", "Tao Lu", "Kerui Ren", "Yichen Jin", "Xudong Xu", "Mulin Yu", "Jiangmiao Pang", "Feng Zhao", "Dahua Lin", "Bo Dai"], "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views", "categories": ["cs.CV"], "comment": "Project page: https://city-super.github.io/anysplat/", "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/", "AI": {"tldr": "AnySplat\u662f\u4e00\u79cd\u524d\u9988\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u672a\u6821\u51c6\u7684\u56fe\u50cf\u96c6\u5408\u4e2d\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u65e0\u9700\u5df2\u77e5\u76f8\u673a\u59ff\u6001\u6216\u9010\u573a\u666f\u4f18\u5316\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u76f8\u673a\u59ff\u6001\u548c\u9010\u573a\u666f\u4f18\u5316\uff0c\u800c\u73b0\u6709\u524d\u9988\u65b9\u6cd5\u5728\u5bc6\u96c6\u89c6\u89d2\u4e0b\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3002AnySplat\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u9884\u6d4b3D\u9ad8\u65af\u57fa\u5143\uff08\u7f16\u7801\u573a\u666f\u51e0\u4f55\u548c\u5916\u89c2\uff09\u53ca\u6bcf\u5f20\u8f93\u5165\u56fe\u50cf\u7684\u76f8\u673a\u5185\u5916\u53c2\u6570\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cAnySplat\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u89c6\u89d2\u4e0b\u5747\u8fbe\u5230\u4e0e\u59ff\u6001\u611f\u77e5\u57fa\u7ebf\u76f8\u5f53\u7684\u8d28\u91cf\uff0c\u4e14\u8d85\u8d8a\u73b0\u6709\u65e0\u59ff\u6001\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6e32\u67d3\u5ef6\u8fdf\u3002", "conclusion": "AnySplat\u4e3a\u65e0\u7ea6\u675f\u62cd\u6444\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u65b0\u89c6\u89d2\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23094", "pdf": "https://arxiv.org/pdf/2505.23094", "abs": "https://arxiv.org/abs/2505.23094", "authors": ["Chongjie Si", "Zhiyi Shi", "Yadao Wang", "Xiaokang Yang", "Susanto Rahardja", "Wei Shen"], "title": "MAP: Revisiting Weight Decomposition for Low-Rank Adaptation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The rapid development of large language models has revolutionized natural\nlanguage processing, but their fine-tuning remains computationally expensive,\nhindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,\nsuch as LoRA, have emerged as solutions. Recent work like DoRA attempts to\nfurther decompose weight adaptation into direction and magnitude components.\nHowever, existing formulations often define direction heuristically at the\ncolumn level, lacking a principled geometric foundation. In this paper, we\npropose MAP, a novel framework that reformulates weight matrices as\nhigh-dimensional vectors and decouples their adaptation into direction and\nmagnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns\na directional update, and introduces two scalar coefficients to independently\nscale the magnitude of the base and update vectors. This design enables more\ninterpretable and flexible adaptation, and can be seamlessly integrated into\nexisting PEFT methods. Extensive experiments show that MAP significantly\nimproves performance when coupling with existing methods, offering a simple yet\npowerful enhancement to existing PEFT methods. Given the universality and\nsimplicity of MAP, we hope it can serve as a default setting for designing\nfuture PEFT methods.", "AI": {"tldr": "MAP\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u65b9\u5411\u548c\u5e45\u5ea6\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u4e25\u8c28\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u5728\u65b9\u5411\u5b9a\u4e49\u4e0a\u7f3a\u4e4f\u51e0\u4f55\u57fa\u7840\uff0c\u9650\u5236\u4e86\u5176\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "MAP\u5c06\u6743\u91cd\u77e9\u9635\u89c6\u4e3a\u9ad8\u7ef4\u5411\u91cf\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u9884\u8bad\u7ec3\u6743\u91cd\u3001\u5b66\u4e60\u65b9\u5411\u66f4\u65b0\uff0c\u5e76\u5f15\u5165\u4e24\u4e2a\u6807\u91cf\u7cfb\u6570\u72ec\u7acb\u8c03\u6574\u57fa\u5411\u91cf\u548c\u66f4\u65b0\u5411\u91cf\u7684\u5e45\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAP\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "MAP\u56e0\u5176\u901a\u7528\u6027\u548c\u7b80\u5355\u6027\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u8bbe\u8ba1\u7684\u9ed8\u8ba4\u8bbe\u7f6e\u3002"}}
{"id": "2505.23726", "pdf": "https://arxiv.org/pdf/2505.23726", "abs": "https://arxiv.org/abs/2505.23726", "authors": ["Darryl Hannan", "Timothy Doster", "Henry Kvinge", "Adam Attarian", "Yijing Watkins"], "title": "FMG-Det: Foundation Model Guided Robust Object Detection", "categories": ["cs.CV"], "comment": "10 pages, ICIP 2025", "summary": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches.", "AI": {"tldr": "FMG-Det\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u548c\u9884\u5904\u7406\u7684\u7b80\u5355\u9ad8\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u566a\u58f0\u6807\u6ce8\u6570\u636e\uff0c\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6807\u6ce8\u8fb9\u754c\u7684\u4e3b\u89c2\u6027\u5bfc\u81f4\u6570\u636e\u8d28\u91cf\u4e0d\u4e00\u81f4\uff0c\u566a\u58f0\u6807\u6ce8\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u3002", "method": "\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\u548c\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u6821\u6b63\u6807\u6ce8\uff0c\u5e76\u5bf9\u68c0\u6d4b\u5934\u8fdb\u884c\u8f7b\u5fae\u4fee\u6539\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u6807\u51c6\u53ca\u5c11\u6837\u672c\u573a\u666f\uff0c\u4e14\u65b9\u6cd5\u66f4\u7b80\u5355\u9ad8\u6548\u3002", "conclusion": "FMG-Det\u901a\u8fc7\u6821\u6b63\u566a\u58f0\u6807\u6ce8\u548c\u4f18\u5316\u6a21\u578b\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.23270", "pdf": "https://arxiv.org/pdf/2505.23270", "abs": "https://arxiv.org/abs/2505.23270", "authors": ["Haokun Chen", "Yueqi Zhang", "Yuan Bi", "Yao Zhang", "Tong Liu", "Jinhe Bi", "Jian Lan", "Jindong Gu", "Claudia Grosser", "Denis Krompass", "Nassir Navab", "Volker Tresp"], "title": "Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nadvancements, drawing significant attention from the research community. Their\ncapabilities are largely attributed to large-scale architectures, which require\nextensive training on massive datasets. However, such datasets often contain\nsensitive or copyrighted content sourced from the public internet, raising\nconcerns about data privacy and ownership. Regulatory frameworks, such as the\nGeneral Data Protection Regulation (GDPR), grant individuals the right to\nrequest the removal of such sensitive information. This has motivated the\ndevelopment of machine unlearning algorithms that aim to remove specific\nknowledge from models without the need for costly retraining. Despite these\nadvancements, evaluating the efficacy of unlearning algorithms remains a\nchallenge due to the inherent complexity and generative nature of LLMs. In this\nwork, we introduce a comprehensive auditing framework for unlearning\nevaluation, comprising three benchmark datasets, six unlearning algorithms, and\nfive prompt-based auditing methods. By using various auditing algorithms, we\nevaluate the effectiveness and robustness of different unlearning strategies.\nTo explore alternatives beyond prompt-based auditing, we propose a novel\ntechnique that leverages intermediate activation perturbations, addressing the\nlimitations of auditing methods that rely solely on model inputs and outputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u5ba1\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u9057\u5fd8\u7b97\u6cd5\u6548\u679c\uff0c\u5305\u62ec\u57fa\u51c6\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u548c\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u95f4\u6fc0\u6d3b\u6270\u52a8\u7684\u65b0\u6280\u672f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u5305\u542b\u654f\u611f\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u5185\u5bb9\uff0c\u800c\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\u6548\u679c\u96be\u4ee5\u8bc4\u4f30\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u5ba1\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u3001\u516d\u79cd\u9057\u5fd8\u7b97\u6cd5\u548c\u4e94\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5ba1\u8ba1\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e2d\u95f4\u6fc0\u6d3b\u6270\u52a8\u7684\u65b0\u6280\u672f\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u5ba1\u8ba1\u7b97\u6cd5\u8bc4\u4f30\u4e86\u4e0d\u540c\u9057\u5fd8\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u65b0\u63d0\u51fa\u7684\u6280\u672f\u5f25\u8865\u4e86\u4ec5\u4f9d\u8d56\u8f93\u5165\u8f93\u51fa\u5ba1\u8ba1\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u9057\u5fd8\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5de5\u5177\uff0c\u65b0\u6280\u672f\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.23727", "pdf": "https://arxiv.org/pdf/2505.23727", "abs": "https://arxiv.org/abs/2505.23727", "authors": ["Song Wang", "Gongfan Fang", "Lingdong Kong", "Xiangtai Li", "Jianyun Xu", "Sheng Yang", "Qiang Li", "Jianke Zhu", "Xinchao Wang"], "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning", "categories": ["cs.CV", "cs.MM"], "comment": "Project Page: https://PixelThink.github.io", "summary": "Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.", "AI": {"tldr": "PixelThink\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8c03\u8282\u63a8\u7406\u751f\u6210\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u5bf9\u63a8\u7406\u8fc7\u7a0b\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "\u63d0\u51faPixelThink\uff0c\u5229\u7528\u5916\u90e8\u4efb\u52a1\u96be\u5ea6\u548c\u5185\u90e8\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8c03\u8282\u63a8\u7406\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u548c\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u4e3a\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.23339", "pdf": "https://arxiv.org/pdf/2505.23339", "abs": "https://arxiv.org/abs/2505.23339", "authors": ["Maya Dewhurst", "Jack Collins", "Justin J. H. Lo", "Roy Alderton", "Sam Kirkham"], "title": "Nosey: Open-source hardware for acoustic nasalance", "categories": ["cs.SD", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,\ncustomizable, 3D-printed system for recording acoustic nasalance data that we\nhave made available as open-source hardware\n(http://github.com/phoneticslab/nosey). We first outline the motivations and\ndesign principles behind our hardware nasalance system, and then present a\ncomparison between Nosey and a commercial nasalance device. Nosey shows\nconsistently higher nasalance scores than the commercial device, but the\nmagnitude of contrast between phonological environments is comparable between\nsystems. We also review ways of customizing the hardware to facilitate testing,\nsuch as comparison of microphones and different construction materials. We\nconclude that Nosey is a flexible and cost-effective alternative to commercial\nnasometry devices and propose some methodological considerations for its use in\ndata collection.", "AI": {"tldr": "Nosey\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u53ef\u5b9a\u5236\u7684\u5f00\u6e90\u9f3b\u97f3\u6570\u636e\u8bb0\u5f55\u7cfb\u7edf\uff0c\u4e0e\u5546\u4e1a\u8bbe\u5907\u76f8\u6bd4\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u5f00\u6e90\u786c\u4ef6\u66ff\u4ee3\u5546\u4e1a\u9f3b\u97f3\u6d4b\u91cf\u8bbe\u5907\u3002", "method": "\u8bbe\u8ba13D\u6253\u5370\u786c\u4ef6\u7cfb\u7edf\uff0c\u5e76\u4e0e\u5546\u4e1a\u8bbe\u5907\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "Nosey\u9f3b\u97f3\u8bc4\u5206\u66f4\u9ad8\uff0c\u4f46\u5bf9\u6bd4\u6548\u679c\u4e0e\u5546\u4e1a\u8bbe\u5907\u76f8\u5f53\u3002", "conclusion": "Nosey\u662f\u5546\u4e1a\u8bbe\u5907\u7684\u7075\u6d3b\u3001\u7ecf\u6d4e\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3002"}}
{"id": "2505.23734", "pdf": "https://arxiv.org/pdf/2505.23734", "abs": "https://arxiv.org/abs/2505.23734", "authors": ["Weijie Wang", "Donny Y. Chen", "Zeyu Zhang", "Duochao Shi", "Akide Liu", "Bohan Zhuang"], "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS", "categories": ["cs.CV"], "comment": "Project Page: https://lhmd.top/zpressor, Code:\n  https://github.com/ziplab/ZPressor", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.", "AI": {"tldr": "ZPressor\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u539f\u5219\u538b\u7f29\u591a\u89c6\u56fe\u8f93\u5165\uff0c\u63d0\u53473D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u7684\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u99883D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u56e0\u7f16\u7801\u5668\u5bb9\u91cf\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u591a\u89c6\u56fe\u8f93\u5165\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u3002", "method": "ZPressor\u5c06\u89c6\u56fe\u5206\u4e3a\u951a\u70b9\u548c\u652f\u6301\u96c6\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u538b\u7f29\u4fe1\u606f\uff0c\u5f62\u6210\u7d27\u51d1\u7684\u6f5c\u5728\u72b6\u6001Z\u3002", "result": "ZPressor\u4f7f\u6a21\u578b\u80fd\u572880GB GPU\u4e0a\u5904\u7406100+\u89c6\u56fe\uff0c\u6027\u80fd\u63d0\u5347\u4e14\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "conclusion": "ZPressor\u4e3a\u524d\u99883DGS\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u89c6\u56fe\u538b\u7f29\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2505.23419", "pdf": "https://arxiv.org/pdf/2505.23419", "abs": "https://arxiv.org/abs/2505.23419", "authors": ["Linghao Zhang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Chengxing Xie", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Shengyu Fu", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang"], "title": "SWE-bench Goes Live!", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Homepage: \\url{https://swe-bench-live.github.io/}, Code:\n  \\url{https://github.com/SWE-bench-Live}, Dataset:\n  \\url{https://huggingface.co/SWE-bench-Live}", "summary": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present \\textbf{SWE-bench-Live}, a\n\\textit{live-updatable} benchmark designed to overcome these challenges. Our\ninitial release consists of 1,319 tasks derived from real GitHub issues created\nsince 2024, spanning 93 repositories. Each task is accompanied by a dedicated\nDocker image to ensure reproducible execution. Central to our benchmark is\n\\method, an automated curation pipeline that streamlines the entire process\nfrom instance creation to environment setup, removing manual bottlenecks and\nenabling scalability and continuous updates. We evaluate a range of\nstate-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a\nsubstantial performance gap compared to static benchmarks like SWE-bench, even\nunder controlled evaluation conditions. To better understand this discrepancy,\nwe perform detailed analyses across repository origin, issue recency, and task\ndifficulty. By providing a fresh, diverse, and executable benchmark grounded in\nlive repository activity, SWE-bench-Live facilitates rigorous,\ncontamination-resistant evaluation of LLMs and agents in dynamic, real-world\nsoftware development settings.", "AI": {"tldr": "SWE-bench-Live\u662f\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u6570\u636e\u8fc7\u65f6\u3001\u8986\u76d6\u8303\u56f4\u7a84\u548c\u4f9d\u8d56\u4eba\u5de5\u3002\u5b83\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u548c\u5b9e\u65f6\u66f4\u65b0\u7684GitHub\u95ee\u9898\uff0c\u4e3aLLM\u548c\u4ee3\u7406\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982SWE-bench\uff09\u5b58\u5728\u6570\u636e\u8fc7\u65f6\u3001\u8986\u76d6\u8303\u56f4\u6709\u9650\u548c\u4f9d\u8d56\u4eba\u5de5\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u8bc4\u4f30\u6548\u679c\u3002SWE-bench-Live\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u548c\u81ea\u52a8\u5316\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SWE-bench-Live\u57fa\u4e8e\u5b9e\u65f6GitHub\u95ee\u9898\u6784\u5efa\uff0c\u5305\u542b1,319\u4e2a\u4efb\u52a1\uff0c\u8986\u76d693\u4e2a\u4ed3\u5e93\u3002\u6bcf\u4e2a\u4efb\u52a1\u914d\u6709Docker\u955c\u50cf\u4ee5\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3002\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\uff08\\method\uff09\u5b9e\u73b0\u5b9e\u4f8b\u521b\u5efa\u548c\u73af\u5883\u8bbe\u7f6e\u7684\u81ea\u52a8\u5316\u3002", "result": "\u5728SWE-bench-Live\u4e0a\u8bc4\u4f30\u7684LLM\u548c\u4ee3\u7406\u6846\u67b6\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u5dee\u8ddd\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u5dee\u5f02\u4e0e\u4ed3\u5e93\u6765\u6e90\u3001\u95ee\u9898\u65f6\u6548\u6027\u548c\u4efb\u52a1\u96be\u5ea6\u76f8\u5173\u3002", "conclusion": "SWE-bench-Live\u63d0\u4f9b\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u591a\u6837\u4e14\u53ef\u6267\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u5bf9LLM\u548c\u4ee3\u7406\u5728\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u907f\u514d\u4e86\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002"}}
{"id": "2505.23442", "pdf": "https://arxiv.org/pdf/2505.23442", "abs": "https://arxiv.org/abs/2505.23442", "authors": ["Linyu Li", "Zhi Jin", "Yuanpeng He", "Dongming Jin", "Haoran Duan", "Zhengwei Tao", "Xuan Zhang", "Jiandong Li"], "title": "Rethinking Regularization Methods for Knowledge Graph Completion", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Knowledge graph completion (KGC) has attracted considerable attention in\nrecent years because it is critical to improving the quality of knowledge\ngraphs. Researchers have continuously explored various models. However, most\nprevious efforts have neglected to take advantage of regularization from a\ndeeper perspective and therefore have not been used to their full potential.\nThis paper rethinks the application of regularization methods in KGC. Through\nextensive empirical studies on various KGC models, we find that carefully\ndesigned regularization not only alleviates overfitting and reduces variance\nbut also enables these models to break through the upper bounds of their\noriginal performance. Furthermore, we introduce a novel sparse-regularization\nmethod that embeds the concept of rank-based selective sparsity into the KGC\nregularizer. The core idea is to selectively penalize those components with\nsignificant features in the embedding vector, thus effectively ignoring many\ncomponents that contribute little and may only represent noise. Various\ncomparative experiments on multiple datasets and multiple models show that the\nSPR regularization method is better than other regularization methods and can\nenable the KGC model to further break through the performance margin.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u601d\u8003\u4e86\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff08SPR\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u60e9\u7f5a\u663e\u8457\u7279\u5f81\u7ec4\u4ef6\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KGC\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u6b63\u5219\u5316\u7684\u6f5c\u529b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6b63\u5219\u5316\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u6df1\u5c42\u5f71\u54cd\u3002", "method": "\u63d0\u51faSPR\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u60e9\u7f5a\u5d4c\u5165\u5411\u91cf\u4e2d\u7684\u663e\u8457\u7279\u5f81\u7ec4\u4ef6\uff0c\u5ffd\u7565\u566a\u58f0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSPR\u4f18\u4e8e\u5176\u4ed6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e2e\u52a9KGC\u6a21\u578b\u7a81\u7834\u6027\u80fd\u4e0a\u9650\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6b63\u5219\u5316\u4e0d\u4ec5\u80fd\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u8fd8\u80fd\u663e\u8457\u63d0\u5347KGC\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.23474", "pdf": "https://arxiv.org/pdf/2505.23474", "abs": "https://arxiv.org/abs/2505.23474", "authors": ["Xiang Li", "Haiyang Yu", "Xinghua Zhang", "Ziyang Huang", "Shizhu He", "Kang Liu", "Jun Zhao", "Fei Huang", "Yongbin Li"], "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs.", "AI": {"tldr": "Socratic-PRMBench\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30PRMs\u5728\u516d\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9010\u6b65\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9PRMs\u5728\u591a\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u9519\u8bef\u8bc6\u522b\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f15\u5165Socratic-PRMBench\uff0c\u5305\u542b2995\u6761\u6709\u7f3a\u9677\u7684\u63a8\u7406\u8def\u5f84\uff0c\u8986\u76d6\u516d\u79cd\u63a8\u7406\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709PRMs\u5728\u591a\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "Socratic-PRMBench\u4e3aPRMs\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u63a8\u52a8\u5176\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2505.23742", "pdf": "https://arxiv.org/pdf/2505.23742", "abs": "https://arxiv.org/abs/2505.23742", "authors": ["Yufan Deng", "Xun Guo", "Yuanyang Yin", "Jacob Zhiyuan Fang", "Yiding Yang", "Yizhi Wang", "Shenghai Yuan", "Angtian Wang", "Bo Liu", "Haibin Huang", "Chongyang Ma"], "title": "MAGREF: Masked Guidance for Any-Reference Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project website: https://magref-video.github.io/magref.github.io/", "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF", "AI": {"tldr": "MAGREF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u53c2\u8003\u4e3b\u9898\u7684\u89c6\u9891\u751f\u6210\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u5f15\u5bfc\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u7684\u591a\u4e3b\u9898\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u591a\u53c2\u8003\u4e3b\u9898\u89c6\u9891\u751f\u6210\u5728\u4fdd\u6301\u591a\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "1. \u533a\u57df\u611f\u77e5\u52a8\u6001\u63a9\u7801\u673a\u5236\uff0c\u7075\u6d3b\u5904\u7406\u4e0d\u540c\u4e3b\u9898\uff1b2. \u50cf\u7d20\u7ea7\u901a\u9053\u62fc\u63a5\u673a\u5236\uff0c\u4fdd\u7559\u5916\u89c2\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728\u591a\u4e3b\u9898\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u3002", "conclusion": "MAGREF\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u7684\u591a\u4e3b\u9898\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23743", "pdf": "https://arxiv.org/pdf/2505.23743", "abs": "https://arxiv.org/abs/2505.23743", "authors": ["Amber Yijia Zheng", "Yu Zhang", "Jun Hu", "Raymond A. Yeh", "Chen Chen"], "title": "DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "High-quality photography in extreme low-light conditions is challenging but\nimpactful for digital cameras. With advanced computing hardware, traditional\ncamera image signal processor (ISP) algorithms are gradually being replaced by\nefficient deep networks that enhance noisy raw images more intelligently.\nHowever, existing regression-based models often minimize pixel errors and\nresult in oversmoothing of low-light photos or deep shadows. Recent work has\nattempted to address this limitation by training a diffusion model from\nscratch, yet those models still struggle to recover sharp image details and\naccurate colors. We introduce a novel framework to enhance low-light raw images\nby retasking pre-trained generative diffusion models with the camera ISP.\nExtensive experiments demonstrate that our method outperforms the\nstate-of-the-art in perceptual quality across three challenging low-light raw\nimage benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u7ed3\u5408\u76f8\u673aISP\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6781\u7aef\u4f4e\u5149\u6761\u4ef6\u4e0b\u539f\u59cb\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u6781\u7aef\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u9ad8\u8d28\u91cf\u6444\u5f71\u5bf9\u6570\u7801\u76f8\u673a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u56fe\u50cf\u6a21\u7cca\u548c\u989c\u8272\u5931\u771f\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u76f8\u673aISP\uff0c\u5bf9\u4f4e\u5149\u539f\u59cb\u56fe\u50cf\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u5728\u4e09\u4e2a\u4f4e\u5149\u539f\u59cb\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u6a21\u7cca\u548c\u989c\u8272\u5931\u771f\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.23500", "pdf": "https://arxiv.org/pdf/2505.23500", "abs": "https://arxiv.org/abs/2505.23500", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "Identity resolution of software metadata using Large Language Models", "categories": ["cs.SE", "cs.CL", "cs.DL"], "comment": null, "summary": "Software is an essential component of research. However, little attention has\nbeen paid to it compared with that paid to research data. Recently, there has\nbeen an increase in efforts to acknowledge and highlight the importance of\nsoftware in research activities.\n  Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy\nToolShed offers valuable insights into research software in the Life Sciences.\nAlthough originally intended to support discovery and integration, this\nmetadata can be repurposed for large-scale analysis of software practices.\nHowever, its quality and completeness vary across platforms, reflecting diverse\ndocumentation practices.\n  To gain a comprehensive view of software development and sustainability,\nconsolidating this metadata is necessary, but requires robust mechanisms to\naddress its heterogeneity and scale.\n  This article presents an evaluation of instruction-tuned large language\nmodels for the task of software metadata identity resolution, a critical step\nin assembling a cohesive collection of research software. Such a collection is\nthe reference component for the Software Observatory at OpenEBench, a platform\nthat aggregates metadata to monitor the FAIRness of research software in the\nLife Sciences.\n  We benchmarked multiple models against a human-annotated gold standard,\nexamined their behavior on ambiguous cases, and introduced an agreement-based\nproxy for high-confidence automated decisions. The proxy achieved high\nprecision and statistical robustness, while also highlighting the limitations\nof current models and the broader challenges of automating semantic judgment in\nFAIR-aligned software metadata across registries and repositories.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u91cd\u8981\u6027\u53ca\u5176\u5143\u6570\u636e\u7684\u6574\u5408\u95ee\u9898\uff0c\u8bc4\u4f30\u4e86\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5143\u6570\u636e\u8eab\u4efd\u89e3\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5728\u79d1\u7814\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u4f46\u5176\u5143\u6570\u636e\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u53c2\u5dee\u4e0d\u9f50\uff0c\u9700\u8981\u6574\u5408\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u5206\u6790\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u5176\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u91d1\u6807\u51c6\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u81ea\u52a8\u5316\u51b3\u7b56\u4ee3\u7406\u3002", "result": "\u4ee3\u7406\u65b9\u6cd5\u5728\u7cbe\u786e\u5ea6\u548c\u7edf\u8ba1\u7a33\u5065\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u548c\u81ea\u52a8\u5316\u8bed\u4e49\u5224\u65ad\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6574\u5408\u8f6f\u4ef6\u5143\u6570\u636e\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u65b9\u6cd5\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2505.23744", "pdf": "https://arxiv.org/pdf/2505.23744", "abs": "https://arxiv.org/abs/2505.23744", "authors": ["Qiang Wang", "Xiang Song", "Yuhang He", "Jizhou Han", "Chenhao Ding", "Xinyuan Gao", "Yihong Gong"], "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025", "summary": "Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO.", "AI": {"tldr": "SOYO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdbPIDIL\u4e2d\u7684\u57df\u9009\u62e9\uff0c\u89e3\u51b3\u4e86DNN\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "DNN\u5728\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u7684\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cPIDIL\u867d\u80fd\u6301\u7eed\u9002\u5e94\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u53c2\u6570\u9009\u62e9\u51c6\u786e\u6027\u4e0a\u5b58\u5728\u95ee\u9898\u3002", "method": "SOYO\u5f15\u5165GMC\u548cDFR\u9ad8\u6548\u5b58\u50a8\u548c\u5e73\u8861\u5148\u9a8c\u57df\u6570\u636e\uff0cMDFN\u589e\u5f3a\u57df\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u591a\u79cdPEFT\u65b9\u6cd5\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOYO\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "SOYO\u901a\u8fc7\u6539\u8fdb\u57df\u9009\u62e9\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684DNN\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23537", "pdf": "https://arxiv.org/pdf/2505.23537", "abs": "https://arxiv.org/abs/2505.23537", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Chao Li", "Qibin Zhao", "Danilo Mandic"], "title": "Domain-Aware Tensor Network Structure Search", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atnLLM\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u9886\u57df\u4fe1\u606f\u76f4\u63a5\u9884\u6d4b\u6700\u4f18\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5f20\u91cf\u7f51\u7edc\u7ed3\u6784\u641c\u7d22\uff08TN-SS\uff09\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u9886\u57df\u4fe1\u606f\u5229\u7528\u3002", "method": "\u7ed3\u5408\u9886\u57df\u4fe1\u606f\u7684\u63d0\u793a\u7ba1\u9053\uff0c\u6307\u5bfcLLM\u6839\u636e\u5f20\u91cf\u6a21\u5f0f\u95f4\u7684\u5173\u7cfb\u63a8\u65ad\u7ed3\u6784\uff0c\u5e76\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u663e\u793atnLLM\u5728\u8f83\u5c11\u51fd\u6570\u8bc4\u4f30\u4e0b\u8fbe\u5230\u4e0eSOTA\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u52a0\u901f\u5176\u4ed6\u65b9\u6cd5\u7684\u6536\u655b\u3002", "conclusion": "tnLLM\u901a\u8fc7LLM\u548c\u9886\u57df\u4fe1\u606f\u9ad8\u6548\u89e3\u51b3\u4e86TN-SS\u95ee\u9898\uff0c\u517c\u5177\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.23745", "pdf": "https://arxiv.org/pdf/2505.23745", "abs": "https://arxiv.org/abs/2505.23745", "authors": ["Hao Dong", "Moru Liu", "Jian Liang", "Eleni Chatzi", "Olga Fink"], "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.", "AI": {"tldr": "TrustVLM\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3VLM\u9884\u6d4b\u53ef\u4fe1\u5ea6\u4f30\u8ba1\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u6539\u8fdb\u8bef\u5206\u7c7b\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u5c3d\u7ba1VLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u4e2d\u7684\u8bef\u5206\u7c7b\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u5d4c\u5165\u7a7a\u95f4\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u5206\u51fd\u6570\uff0c\u5229\u7528\u6a21\u6001\u95f4\u9699\u548c\u6982\u5ff5\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u533a\u5206\u6027\u6765\u68c0\u6d4b\u8bef\u5206\u7c7b\u3002", "result": "\u572817\u4e2a\u6570\u636e\u96c6\u30014\u79cd\u67b6\u6784\u548c2\u79cdVLM\u4e0a\u9a8c\u8bc1\uff0cTrustVLM\u5728AURC\u3001AUROC\u548cFPR95\u6307\u6807\u4e0a\u5206\u522b\u63d0\u534751.87%\u30019.14%\u548c32.42%\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "TrustVLM\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347VLM\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5176\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.23564", "pdf": "https://arxiv.org/pdf/2505.23564", "abs": "https://arxiv.org/abs/2505.23564", "authors": ["Yiran Guo", "Lijie Xu", "Jie Liu", "Dan Ye", "Shuang Qiu"], "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models effectively\nusing reinforcement learning (RL) remains a crucial challenge. Existing\napproaches primarily adopt two contrasting advantage estimation granularities:\nToken-level methods (e.g., PPO) aim to provide the fine-grained advantage\nsignals but suffer from inaccurate estimation due to difficulties in training\nan accurate critic model. On the other extreme, trajectory-level methods (e.g.,\nGRPO) solely rely on a coarse-grained advantage signal from the final reward,\nleading to imprecise credit assignment. To address these limitations, we\npropose Segment Policy Optimization (SPO), a novel RL framework that leverages\nsegment-level advantage estimation at an intermediate granularity, achieving a\nbetter balance by offering more precise credit assignment than trajectory-level\nmethods and requiring fewer estimation points than token-level methods,\nenabling accurate advantage estimation based on Monte Carlo (MC) without a\ncritic model. SPO features three components with novel strategies: (1) flexible\nsegment partition; (2) accurate segment advantage estimation; and (3) policy\noptimization using segment advantages, including a novel probability-mask\nstrategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain\nfor short chain-of-thought (CoT), featuring novel cutpoint-based partition and\nchain-based advantage estimation, achieving $6$-$12$ percentage point\nimprovements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,\nfeaturing novel tree-based advantage estimation, which significantly reduces\nthe cost of MC estimation, achieving $7$-$11$ percentage point improvements\nover GRPO on MATH500 under 2K and 4K context evaluation. We make our code\npublicly available at https://github.com/AIFrameResearch/SPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPO\u7684\u65b0RL\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u7c92\u5ea6\u6bb5\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u5e73\u8861\u4e86\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u52bf\u4f30\u8ba1\u7c92\u5ea6\u4e0a\u5b58\u5728\u4e24\u6781\u5206\u5316\uff0c\u7ec6\u7c92\u5ea6\u65b9\u6cd5\uff08\u5982PPO\uff09\u56e0\u96be\u4ee5\u8bad\u7ec3\u51c6\u786e\u7684\u8bc4\u5224\u6a21\u578b\u800c\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u7c97\u7c92\u5ea6\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4ec5\u4f9d\u8d56\u6700\u7ec8\u5956\u52b1\u5bfc\u81f4\u4fe1\u7528\u5206\u914d\u4e0d\u7cbe\u786e\u3002SPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SPO\u91c7\u7528\u6bb5\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u5305\u542b\u4e09\u4e2a\u521b\u65b0\u7b56\u7565\uff1a\u7075\u6d3b\u7684\u6bb5\u5212\u5206\u3001\u51c6\u786e\u7684\u6bb5\u4f18\u52bf\u4f30\u8ba1\u3001\u57fa\u4e8e\u6bb5\u4f18\u52bf\u7684\u7b56\u7565\u4f18\u5316\uff08\u5305\u62ec\u6982\u7387\u63a9\u7801\u7b56\u7565\uff09\u3002\u5e76\u9488\u5bf9\u77ed\u94fe\u548c\u957f\u94fe\u63a8\u7406\u573a\u666f\u5206\u522b\u63d0\u51fa\u4e86SPO-chain\u548cSPO-tree\u3002", "result": "SPO\u5728GSM8K\u4e0a\u6bd4PPO\u548cGRPO\u63d0\u5347\u4e866-12\u4e2a\u767e\u5206\u70b9\uff0c\u5728MATH500\u4e0a\u6bd4GRPO\u63d0\u5347\u4e867-11\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u663e\u8457\u964d\u4f4e\u4e86MC\u4f30\u8ba1\u6210\u672c\u3002", "conclusion": "SPO\u901a\u8fc7\u4e2d\u7c92\u5ea6\u4f18\u52bf\u4f30\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2505.23747", "pdf": "https://arxiv.org/pdf/2505.23747", "abs": "https://arxiv.org/abs/2505.23747", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2"], "comment": "21 pages", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "AI": {"tldr": "Spatial-MLLM\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7eaf2D\u89c2\u6d4b\u5b9e\u73b0\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\uff0c\u5229\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u7a7a\u95f4\u611f\u77e5\u5e27\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u67093D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u989d\u59163D\u62162.5D\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u4ec5\u67092D\u8f93\u5165\uff08\u5982\u56fe\u50cf\u6216\u89c6\u9891\uff09\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff1a\u8bed\u4e49\u7f16\u7801\u5668\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff0c\u7a7a\u95f4\u7f16\u7801\u5668\u4ece\u89c6\u89c9\u51e0\u4f55\u6a21\u578b\u4e2d\u63d0\u53d63D\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8fde\u63a5\u5668\u6574\u5408\u3002\u540c\u65f6\u63d0\u51fa\u7a7a\u95f4\u611f\u77e5\u5e27\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cSpatial-MLLM\u5728\u89c6\u89c9\u7a7a\u95f4\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Spatial-MLLM\u901a\u8fc7\u7eaf2D\u8f93\u5165\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.23585", "pdf": "https://arxiv.org/pdf/2505.23585", "abs": "https://arxiv.org/abs/2505.23585", "authors": ["Yaru Hao", "Li Dong", "Xun Wu", "Shaohan Huang", "Zewen Chi", "Furu Wei"], "title": "On-Policy RL with Optimal Reward Baseline", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO introduces the optimal reward baseline that\ntheoretically minimizes gradient variance. We evaluate OPO on mathematical\nreasoning benchmarks. The results demonstrate its superior performance and\ntraining stability without additional models or regularization terms.\nFurthermore, OPO achieves lower policy shifts and higher output entropy,\nencouraging more diverse and less repetitive responses. These results highlight\nOPO as a promising direction for stable and effective reinforcement learning in\nlarge language model alignment and reasoning tasks. The implementation is\nprovided at https://github.com/microsoft/LMOps/tree/main/opo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOPO\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u7684on-policy\u8bad\u7ec3\u548c\u6700\u4f18\u5956\u52b1\u57fa\u7ebf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faOPO\u7b97\u6cd5\uff0c\u5f3a\u8c03\u7cbe\u786e\u7684on-policy\u8bad\u7ec3\u548c\u5f15\u5165\u6700\u4f18\u5956\u52b1\u57fa\u7ebf\u4ee5\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOPO\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7b56\u7565\u504f\u79fb\u548c\u66f4\u9ad8\u7684\u8f93\u51fa\u71b5\u3002", "conclusion": "OPO\u662f\u4e00\u79cd\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2505.23752", "pdf": "https://arxiv.org/pdf/2505.23752", "abs": "https://arxiv.org/abs/2505.23752", "authors": ["Akashah Shabbir", "Muhammad Akhtar Munir", "Akshay Dudhane", "Muhammad Umer Sheikh", "Muhammad Haris Khan", "Paolo Fraccaro", "Juan Bernabe Moreno", "Fahad Shahbaz Khan", "Salman Khan"], "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable", "AI": {"tldr": "ThinkGeo\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u9886\u57df\u7279\u5b9a\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u5173\u6ce8\u901a\u7528\u6216\u591a\u6a21\u6001\u573a\u666f\uff0c\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u9065\u611f\u7528\u4f8b\u7684\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u3002", "method": "ThinkGeo\u5305\u542b\u4eba\u5de5\u7b56\u5212\u7684\u67e5\u8be2\uff0c\u8986\u76d6\u591a\u79cd\u5b9e\u9645\u5e94\u7528\uff0c\u91c7\u7528ReAct\u5f0f\u4ea4\u4e92\u5faa\u73af\u8bc4\u4f30\u5f00\u6e90\u548c\u95ed\u6e90LLM\u3002", "result": "\u5206\u6790\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u5de5\u5177\u51c6\u786e\u6027\u548c\u89c4\u5212\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "ThinkGeo\u4e3a\u8bc4\u4f30\u5de5\u5177\u589e\u5f3aLLM\u5728\u9065\u611f\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5e7f\u6cdb\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2505.23756", "pdf": "https://arxiv.org/pdf/2505.23756", "abs": "https://arxiv.org/abs/2505.23756", "authors": ["Justin Lazarow", "Kai Kang", "Afshin Dehghan"], "title": "Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping", "categories": ["cs.CV"], "comment": null, "summary": "We revisit scene-level 3D object detection as the output of an object-centric\nframework capable of both localization and mapping using 3D oriented boxes as\nthe underlying geometric primitive. While existing 3D object detection\napproaches operate globally and implicitly rely on the a priori existence of\nmetric camera poses, our method, Rooms from Motion (RfM) operates on a\ncollection of un-posed images. By replacing the standard 2D keypoint-based\nmatcher of structure-from-motion with an object-centric matcher based on\nimage-derived 3D boxes, we estimate metric camera poses, object tracks, and\nfinally produce a global, semantic 3D object map. When a priori pose is\navailable, we can significantly improve map quality through optimization of\nglobal 3D boxes against individual observations. RfM shows strong localization\nperformance and subsequently produces maps of higher quality than leading\npoint-based and multi-view 3D object detection methods on CA-1M and ScanNet++,\ndespite these global methods relying on overparameterization through point\nclouds or dense volumes. Rooms from Motion achieves a general, object-centric\nrepresentation which not only extends the work of Cubify Anything to full\nscenes but also allows for inherently sparse localization and parametric\nmapping proportional to the number of objects in a scene.", "AI": {"tldr": "Rooms from Motion (RfM) \u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u4f53\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u573a\u666f\u7ea73D\u7269\u4f53\u68c0\u6d4b\uff0c\u80fd\u591f\u901a\u8fc7\u672a\u5b9a\u4f4d\u7684\u56fe\u50cf\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u548c\u7269\u4f53\u8f68\u8ff9\uff0c\u751f\u6210\u5168\u5c40\u8bed\u4e493D\u7269\u4f53\u5730\u56fe\u3002", "motivation": "\u73b0\u67093D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u4fe1\u606f\u548c\u5df2\u77e5\u76f8\u673a\u59ff\u6001\uff0c\u800cRfM\u65e8\u5728\u901a\u8fc7\u7269\u4f53\u4e2d\u5fc3\u5339\u914d\u5668\u5904\u7406\u672a\u5b9a\u4f4d\u56fe\u50cf\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u5b9a\u4f4d\u548c\u6620\u5c04\u3002", "method": "RfM\u7528\u57fa\u4e8e3D\u76d2\u5b50\u7684\u7269\u4f53\u4e2d\u5fc3\u5339\u914d\u5668\u66ff\u4ee3\u4f20\u7edf\u76842D\u5173\u952e\u70b9\u5339\u914d\u5668\uff0c\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u548c\u7269\u4f53\u8f68\u8ff9\uff0c\u5e76\u4f18\u5316\u5168\u5c403D\u76d2\u5b50\u4ee5\u63d0\u9ad8\u5730\u56fe\u8d28\u91cf\u3002", "result": "RfM\u5728CA-1M\u548cScanNet++\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u4e8e\u70b9\u548c\u591a\u89c6\u56fe\u76843D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5730\u56fe\u3002", "conclusion": "RfM\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7269\u4f53\u4e2d\u5fc3\u8868\u793a\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86Cubify Anything\u7684\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u7a00\u758f\u5b9a\u4f4d\u548c\u53c2\u6570\u5316\u6620\u5c04\u3002"}}
{"id": "2505.23631", "pdf": "https://arxiv.org/pdf/2505.23631", "abs": "https://arxiv.org/abs/2505.23631", "authors": ["Boning Zhao"], "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "7 pages, 6 figures. Under review", "summary": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHEAE\u7684\u4eba\u672cAI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b66\u751f\u53d9\u8ff0\u6587\u672c\u548c\u6559\u5e08\u751f\u6210\u7684\u201c\u5171\u60c5\u5411\u91cf\u201d\uff0c\u63d0\u5347\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u548c\u793e\u4f1a\u8d23\u4efb\u6027\u3002", "motivation": "\u5728\u7279\u6b8a\u6559\u80b2\u7b49\u654f\u611f\u73af\u5883\u4e2d\uff0c\u6807\u51c6\u5316\u95ee\u5377\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u5b66\u751f\u6291\u90c1\u60c5\u51b5\uff0c\u4e14\u7f3a\u4e4f\u6559\u5e08\u5171\u60c5\u5e26\u6765\u7684\u4e2a\u6027\u5316\u6d1e\u5bdf\u3002", "method": "HEAE\u6846\u67b6\u6574\u5408\u5b66\u751f\u53d9\u8ff0\u6587\u672c\u4e0e\u57fa\u4e8ePHQ-9\u6846\u67b6\u76849\u7ef4\u201c\u5171\u60c5\u5411\u91cf\u201d\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u3001\u6587\u672c\u8868\u793a\u548c\u5206\u7c7b\u67b6\u6784\u4f18\u5316\u5b9e\u73b0\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cHEAE\u57287\u7ea7\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u4e2d\u8fbe\u523082.74%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "HEAE\u4e3a\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u8d1f\u8d23\u4efb\u548c\u4f26\u7406\u7684\u8def\u5f84\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5d4c\u5165\u4eba\u7c7b\u5171\u60c5\uff0c\u589e\u5f3a\u800c\u975e\u66ff\u4ee3\u4eba\u7c7b\u5224\u65ad\u3002"}}
{"id": "2505.23757", "pdf": "https://arxiv.org/pdf/2505.23757", "abs": "https://arxiv.org/abs/2505.23757", "authors": ["Haohan Chi", "Huan-ang Gao", "Ziming Liu", "Jianing Liu", "Chenyu Liu", "Jinwei Li", "Kaisen Yang", "Yangcheng Yu", "Zeda Wang", "Wenyi Li", "Leichen Wang", "Xingtao Hu", "Hao Sun", "Hang Zhao", "Hao Zhao"], "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models", "categories": ["cs.CV"], "comment": "Project page: https://github.com/ahydchh/Impromptu-VLA", "summary": "Vision-Language-Action (VLA) models for autonomous driving show promise but\nfalter in unstructured corner case scenarios, largely due to a scarcity of\ntargeted benchmarks. To address this, we introduce Impromptu VLA. Our core\ncontribution is the Impromptu VLA Dataset: over 80,000 meticulously curated\nvideo clips, distilled from over 2M source clips sourced from 8 open-source\nlarge-scale datasets. This dataset is built upon our novel taxonomy of four\nchallenging unstructured categories and features rich, planning-oriented\nquestion-answering annotations and action trajectories. Crucially, experiments\ndemonstrate that VLAs trained with our dataset achieve substantial performance\ngains on established benchmarks--improving closed-loop NeuroNCAP scores and\ncollision rates, and reaching near state-of-the-art L2 accuracy in open-loop\nnuScenes trajectory prediction. Furthermore, our Q&A suite serves as an\neffective diagnostic, revealing clear VLM improvements in perception,\nprediction, and planning. Our code, data and models are available at\nhttps://github.com/ahydchh/Impromptu-VLA.", "AI": {"tldr": "Impromptu VLA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347Vision-Language-Action\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e0b\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u6781\u7aef\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b8\u4e07\u591a\u4e2a\u89c6\u9891\u7247\u6bb5\u7684Impromptu VLA\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u56db\u7c7b\u975e\u7ed3\u6784\u5316\u573a\u666f\u5206\u7c7b\uff0c\u5e76\u5305\u542b\u89c4\u5212\u5bfc\u5411\u7684\u95ee\u7b54\u6ce8\u91ca\u548c\u52a8\u4f5c\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684VLA\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5305\u62ec\u95ed\u73afNeuroNCAP\u5206\u6570\u3001\u78b0\u649e\u7387\u4ee5\u53ca\u5f00\u73afnuScenes\u8f68\u8ff9\u9884\u6d4b\u7684L2\u7cbe\u5ea6\u3002", "conclusion": "Impromptu VLA\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2505.23671", "pdf": "https://arxiv.org/pdf/2505.23671", "abs": "https://arxiv.org/abs/2505.23671", "authors": ["Manish Shetty", "Naman Jain", "Jinjian Liu", "Vijay Kethanaboyina", "Koushik Sen", "Ion Stoica"], "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Website: https://gso-bench.github.io/", "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.", "AI": {"tldr": "GSO\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u9ad8\u6027\u80fd\u8f6f\u4ef6\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210\u6027\u80fd\u6d4b\u8bd5\u5e76\u5206\u6790\u4ee3\u7801\u5e93\u5386\u53f2\uff0c\u53d1\u73b0\u73b0\u6709SWE-Agents\u6210\u529f\u7387\u4f4e\u4e8e5%\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fd\u8f6f\u4ef6\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u76ee\u524d\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u9886\u57df\u7684\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u7ba1\u9053\u751f\u6210\u6027\u80fd\u6d4b\u8bd5\uff0c\u5206\u6790\u4ee3\u7801\u5e93\u5386\u53f2\u4ee5\u8bc6\u522b102\u4e2a\u4f18\u5316\u4efb\u52a1\uff0c\u63d0\u4f9b\u4ee3\u7801\u5e93\u548c\u6027\u80fd\u6d4b\u8bd5\u4f5c\u4e3a\u89c4\u8303\uff0c\u8981\u6c42\u4ee3\u7406\u63d0\u5347\u8fd0\u884c\u65f6\u6548\u7387\u3002", "result": "\u9886\u5148\u7684SWE-Agents\u6210\u529f\u7387\u4f4e\u4e8e5%\uff0c\u63a8\u7406\u65f6\u95f4\u6269\u5c55\u6548\u679c\u6709\u9650\uff0c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4f4e\u5c42\u8bed\u8a00\u3001\u61d2\u60f0\u4f18\u5316\u7b56\u7565\u548c\u74f6\u9888\u5b9a\u4f4d\u7b49\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "GSO\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u6027\u80fd\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4ee3\u7801\u548c\u8f68\u8ff9\u6570\u636e\u3002"}}
{"id": "2505.23758", "pdf": "https://arxiv.org/pdf/2505.23758", "abs": "https://arxiv.org/abs/2505.23758", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "categories": ["cs.CV"], "comment": "Project Webpage: https://lorashop.github.io/", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "AI": {"tldr": "LoRAShop\u662f\u4e00\u4e2a\u591a\u6982\u5ff5\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u5229\u7528LoRA\u6a21\u578b\u5b9e\u73b0\u4e2a\u6027\u5316\u7f16\u8f91\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u591a\u6982\u5ff5\u56fe\u50cf\u7f16\u8f91\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u5168\u5c40\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u53d8\u6362\u5668\u4e2d\u7684\u7279\u5f81\u4ea4\u4e92\u6a21\u5f0f\uff0c\u751f\u6210\u89e3\u8026\u7684\u6f5c\u5728\u63a9\u7801\uff0c\u5e76\u5728\u7279\u5b9a\u533a\u57df\u6df7\u5408LoRA\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLoRAShop\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u7f16\u8f91\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LoRAShop\u4e3a\u89c6\u89c9\u521b\u4f5c\u548c\u5feb\u901f\u8fed\u4ee3\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6269\u5c55\u4e86LoRA\u6a21\u578b\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.23763", "pdf": "https://arxiv.org/pdf/2505.23763", "abs": "https://arxiv.org/abs/2505.23763", "authors": ["Aneeshan Sain", "Subhajit Maity", "Pinaki Nath Chowdhury", "Subhadeep Koley", "Ayan Kumar Bhunia", "Yi-Zhe Song"], "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025, Project Page:\n  https://subhajitmaity.me/SketchDownTheFLOPs", "summary": "As sketch research has collectively matured over time, its adaptation for\nat-mass commercialisation emerges on the immediate horizon. Despite an already\nmature research endeavour for photos, there is no research on the efficient\ninference specifically designed for sketch data. In this paper, we first\ndemonstrate existing state-of-the-art efficient light-weight models designed\nfor photos do not work on sketches. We then propose two sketch-specific\ncomponents which work in a plug-n-play manner on any photo efficient network to\nadapt them to work on sketch data. We specifically chose fine-grained\nsketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised\nsketch problem with immediate commercial value. Technically speaking, we first\npropose a cross-modal knowledge distillation network to transfer existing photo\nefficient networks to be compatible with sketch, which brings down number of\nFLOPs and model parameters by 97.96% percent and 84.89% respectively. We then\nexploit the abstract trait of sketch to introduce a RL-based canvas selector\nthat dynamically adjusts to the abstraction level which further cuts down\nnumber of FLOPs by two thirds. The end result is an overall reduction of 99.37%\nof FLOPs (from 40.18G to 0.254G) when compared with a full network, while\nretaining the accuracy (33.03% vs 32.77%) -- finally making an efficient\nnetwork for the sparse sketch data that exhibit even fewer FLOPs than the best\nphoto counterpart.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u9488\u5bf9\u8349\u56fe\u6570\u636e\u7684\u9ad8\u6548\u63a8\u7406\u7ec4\u4ef6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u753b\u5e03\u9009\u62e9\u5668\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff08FLOPs\u51cf\u5c1199.37%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u9ad8\u6548\u8f7b\u91cf\u6a21\u578b\u9002\u7528\u4e8e\u7167\u7247\u4f46\u4e0d\u9002\u7528\u4e8e\u8349\u56fe\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8349\u56fe\u6570\u636e\u7684\u9ad8\u6548\u63a8\u7406\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7f51\u7edc\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u753b\u5e03\u9009\u62e9\u5668\uff0c\u52a8\u6001\u9002\u5e94\u8349\u56fe\u62bd\u8c61\u7279\u6027\u3002", "result": "FLOPs\u51cf\u5c1199.37%\uff08\u4ece40.18G\u964d\u81f30.254G\uff09\uff0c\u7cbe\u5ea6\u51e0\u4e4e\u4e0d\u53d8\uff0833.03% vs 32.77%\uff09\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u7a00\u758f\u8349\u56fe\u6570\u636e\u7684\u9ad8\u6548\u7f51\u7edc\uff0c\u5176\u8ba1\u7b97\u91cf\u751a\u81f3\u4f4e\u4e8e\u6700\u4f73\u7167\u7247\u6a21\u578b\u3002"}}
{"id": "2505.23761", "pdf": "https://arxiv.org/pdf/2505.23761", "abs": "https://arxiv.org/abs/2505.23761", "authors": ["Yunjae Won", "Hyunji Lee", "Hyeonbin Hwang", "Minjoon Seo"], "title": "Differential Information: An Information-Theoretic Perspective on Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "41 pages, 13 figures; due to the 1,920-character limitation imposed\n  on the abstract field by arXiv, the abstract included on the arXiv page is\n  slightly abbreviated compared to the version presented in the PDF", "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u5dee\u5206\u4fe1\u606f\u5206\u5e03\uff08DID\uff09\u7406\u8bba\uff0c\u586b\u8865\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4e2dlog-ratio\u5956\u52b1\u53c2\u6570\u5316\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5176\u552f\u4e00\u6700\u4f18\u6027\uff0c\u5e76\u5206\u6790\u4e86\u504f\u597d\u6570\u636e\u4e0e\u7b56\u7565\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1DPO\u5728\u7ecf\u9a8c\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176log-ratio\u5956\u52b1\u53c2\u6570\u5316\u7684\u7406\u8bba\u4f9d\u636e\u5c1a\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7DID\u7406\u8bba\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002", "method": "\u5229\u7528\u5dee\u5206\u4fe1\u606f\u5206\u5e03\uff08DID\uff09\u5206\u6790\u504f\u597d\u6807\u7b7e\u5982\u4f55\u7f16\u7801\u4ece\u53c2\u8003\u7b56\u7565\u5230\u76ee\u6807\u7b56\u7565\u7684\u5dee\u5206\u4fe1\u606f\uff0c\u63a8\u5bfc\u51falog-ratio\u5956\u52b1\u7684\u552f\u4e00\u6700\u4f18\u6027\uff0c\u5e76\u7814\u7a76\u5176\u5bf9\u7b56\u7565\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u504f\u597d\u6570\u636e\u7f16\u7801\u5dee\u5206\u4fe1\u606f\u7684\u6761\u4ef6\u4e0e\u5bf9\u6570\u8fb9\u9645\u6709\u5e8f\u7b56\u7565\u7684\u9690\u542b\u5047\u8bbe\u76f8\u5173\uff0c\u5e76\u901a\u8fc7DID\u71b5\u5206\u6790\u63ed\u793a\u4e86\u4f4e\u71b5\u4e0e\u9ad8\u71b5\u5dee\u5206\u4fe1\u606f\u5bf9\u7b56\u7565\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3aDPO\u76ee\u6807\u3001\u504f\u597d\u6570\u636e\u7ed3\u6784\u548c\u7b56\u7565\u884c\u4e3a\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u71b5\u5dee\u5206\u4fe1\u606f\u5bf9\u901a\u7528\u6307\u4ee4\u8ddf\u968f\u7684\u91cd\u8981\u6027\uff0c\u800c\u4f4e\u71b5\u5dee\u5206\u4fe1\u606f\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u7684\u76ca\u5904\u3002"}}
{"id": "2505.23764", "pdf": "https://arxiv.org/pdf/2505.23764", "abs": "https://arxiv.org/abs/2505.23764", "authors": ["Sihan Yang", "Runsen Xu", "Yiman Xie", "Sizhe Yang", "Mo Li", "Jingli Lin", "Chenming Zhu", "Xiaochen Chen", "Haodong Duan", "Xiangyu Yue", "Dahua Lin", "Tai Wang", "Jiangmiao Pang"], "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence", "categories": ["cs.CV", "cs.CL"], "comment": "34 pages. A comprehensive, fully human-curated, multi-image-based\n  spatial intelligence benchmark with reasoning annotation for MLLMs. Project\n  page: https://runsenxu.com/projects/MMSI_Bench", "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .", "AI": {"tldr": "MMSI-Bench\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u591a\u56fe\u50cf\u7a7a\u95f4\u667a\u80fd\u7684VQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u590d\u6742\u7269\u7406\u4e16\u754c\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u5173\u6ce8\u5355\u56fe\u50cf\u5173\u7cfb\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u4e2d\u5bf9\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u7684\u9700\u6c42\u3002", "method": "\u516d\u4f4d3D\u89c6\u89c9\u4e13\u5bb6\u8017\u65f6300\u591a\u5c0f\u65f6\uff0c\u4ece12\u4e07\u5f20\u56fe\u50cf\u4e2d\u7cbe\u5fc3\u8bbe\u8ba11000\u9053\u5177\u6709\u6311\u6218\u6027\u7684\u9009\u62e9\u9898\uff0c\u5e76\u8bc4\u4f3034\u4e2a\u5f00\u6e90\u548c\u4e13\u6709MLLMs\u3002", "result": "\u6700\u5f3a\u5f00\u6e90\u6a21\u578b\u51c6\u786e\u7387\u7ea630%\uff0cOpenAI\u7684o3\u63a8\u7406\u6a21\u578b\u8fbe40%\uff0c\u800c\u4eba\u7c7b\u5f97\u520697%\u3002", "conclusion": "MMSI-Bench\u63ed\u793a\u4e86\u591a\u56fe\u50cf\u7a7a\u95f4\u63a8\u7406\u7684\u6311\u6218\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u9519\u8bef\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2505.23762", "pdf": "https://arxiv.org/pdf/2505.23762", "abs": "https://arxiv.org/abs/2505.23762", "authors": ["Chenyu Yang", "Shiqian Su", "Shi Liu", "Xuan Dong", "Yue Yu", "Weijie Su", "Xuehui Wang", "Zhaoyang Liu", "Jinguo Zhu", "Hao Li", "Wenhai Wang", "Yu Qiao", "Xizhou Zhu", "Jifeng Dai"], "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.", "AI": {"tldr": "ZeroGUI\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u5728\u7ebf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u52a8\u4efb\u52a1\u751f\u6210\u548c\u5956\u52b1\u4f30\u8ba1\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0cZeroGUI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408VLM\u81ea\u52a8\u751f\u6210\u4efb\u52a1\u548c\u5956\u52b1\u4f30\u8ba1\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728UI-TARS\u548cAguvis\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "ZeroGUI\u4e3aGUI\u4ee3\u7406\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23766", "pdf": "https://arxiv.org/pdf/2505.23766", "abs": "https://arxiv.org/abs/2505.23766", "authors": ["Yunze Man", "De-An Huang", "Guilin Liu", "Shiwei Sheng", "Shilong Liu", "Liang-Yan Gui", "Jan Kautz", "Yu-Xiong Wang", "Zhiding Yu"], "title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought", "categories": ["cs.CV"], "comment": "CVPR 2025. Project Page: https://yunzeman.github.io/argus/", "summary": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in vision-language tasks, yet they often struggle with\nvision-centric scenarios where precise visual focus is needed for accurate\nreasoning. In this paper, we introduce Argus to address these limitations with\na new visual attention grounding mechanism. Our approach employs object-centric\ngrounding as visual chain-of-thought signals, enabling more effective\ngoal-conditioned visual attention during multimodal reasoning tasks.\nEvaluations on diverse benchmarks demonstrate that Argus excels in both\nmultimodal reasoning tasks and referring object grounding tasks. Extensive\nanalysis further validates various design choices of Argus, and reveals the\neffectiveness of explicit language-guided visual region-of-interest engagement\nin MLLMs, highlighting the importance of advancing multimodal intelligence from\na visual-centric perspective. Project page: https://yunzeman.github.io/argus/", "AI": {"tldr": "Argus\u901a\u8fc7\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u9700\u8981\u7cbe\u786e\u89c6\u89c9\u805a\u7126\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u89c6\u89c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u4e2d\u5fc3\u63a5\u5730\u4f5c\u4e3a\u89c6\u89c9\u94fe\u5f0f\u601d\u8003\u4fe1\u53f7\uff0c\u589e\u5f3a\u76ee\u6807\u6761\u4ef6\u89c6\u89c9\u6ce8\u610f\u529b\u3002", "result": "Argus\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u5bf9\u8c61\u63a5\u5730\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u663e\u5f0f\u8bed\u8a00\u5f15\u5bfc\u7684\u89c6\u89c9\u5174\u8da3\u533a\u57df\u53c2\u4e0e\u5bf9MLLMs\u81f3\u5173\u91cd\u8981\uff0c\u9700\u4ece\u89c6\u89c9\u4e2d\u5fc3\u89c6\u89d2\u63a8\u8fdb\u591a\u6a21\u6001\u667a\u80fd\u3002"}}
{"id": "2505.23769", "pdf": "https://arxiv.org/pdf/2505.23769", "abs": "https://arxiv.org/abs/2505.23769", "authors": ["Yao Xiao", "Qiqian Fu", "Heyi Tao", "Yuqun Wu", "Zhen Zhu", "Derek Hoiem"], "title": "TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/avaxiao/TextRegion", "summary": "Image-text models excel at image-level tasks but struggle with detailed\nvisual understanding. While these models provide strong visual-language\nalignment, segmentation models like SAM2 offer precise spatial boundaries for\nobjects. To this end, we propose TextRegion, a simple, effective, and\ntraining-free framework that combines the strengths of image-text models and\nSAM2 to generate powerful text-aligned region tokens. These tokens enable\ndetailed visual understanding while preserving open-vocabulary capabilities.\nThey can be directly applied to various downstream tasks, including open-world\nsemantic segmentation, referring expression comprehension, and grounding. We\nconduct extensive evaluations and consistently achieve superior or competitive\nperformance compared to state-of-the-art training-free methods. Additionally,\nour framework is compatible with many image-text models, making it highly\npractical and easily extensible as stronger models emerge. Code is available\nat: https://github.com/avaxiao/TextRegion.", "AI": {"tldr": "TextRegion\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u548cSAM2\u7684\u4f18\u52bf\uff0c\u751f\u6210\u6587\u672c\u5bf9\u9f50\u7684\u533a\u57df\u6807\u8bb0\uff0c\u652f\u6301\u8be6\u7ec6\u89c6\u89c9\u7406\u89e3\u548c\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u5728\u8be6\u7ec6\u89c6\u89c9\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u540c\u65f6\u4fdd\u7559\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002", "method": "\u63d0\u51faTextRegion\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u6a21\u578b\u548cSAM2\uff0c\u751f\u6210\u6587\u672c\u5bf9\u9f50\u7684\u533a\u57df\u6807\u8bb0\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u6216\u5ab2\u7f8e\u73b0\u6709\u65e0\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "TextRegion\u5b9e\u7528\u6027\u5f3a\uff0c\u6613\u4e8e\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2505.22673", "pdf": "https://arxiv.org/pdf/2505.22673", "abs": "https://arxiv.org/abs/2505.22673", "authors": ["Wasif Khan", "Kyle B. See", "Simon Kato", "Ziqian Huang", "Amy Lazarte", "Kyle Douglas", "Xiangyang Lou", "Teng J. Peng", "Dhanashree Rajderkar", "John Rees", "Pina Sanelli", "Amita Singh", "Ibrahim Tuna", "Christina A. Wilson", "Ruogu Fang"], "title": "Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion", "categories": ["q-bio.TO", "cs.AI", "cs.CV"], "comment": "Under Review", "summary": "Perfusion imaging is extensively utilized to assess hemodynamic status and\ntissue perfusion in various organs. Computed tomography perfusion (CTP) imaging\nplays a key role in the early assessment and planning of stroke treatment.\nWhile CTP provides essential perfusion parameters to identify abnormal blood\nflow in the brain, the use of contrast agents in CTP can lead to allergic\nreactions and adverse side effects, along with costing USD 4.9 billion\nworldwide in 2022. To address these challenges, we propose a novel deep\nlearning framework called Multitask Automated Generation of Intermodal CT\nperfusion maps (MAGIC). This framework combines generative artificial\nintelligence and physiological information to map non-contrast computed\ntomography (CT) imaging to multiple contrast-free CTP imaging maps. We\ndemonstrate enhanced image fidelity by incorporating physiological\ncharacteristics into the loss terms. Our network was trained and validated\nusing CT image data from patients referred for stroke at UF Health and\ndemonstrated robustness to abnormalities in brain perfusion activity. A\ndouble-blinded study was conducted involving seven experienced\nneuroradiologists and vascular neurologists. This study validated MAGIC's\nvisual quality and diagnostic accuracy showing favorable performance compared\nto clinical perfusion imaging with intravenous contrast injection. Overall,\nMAGIC holds great promise in revolutionizing healthcare by offering\ncontrast-free, cost-effective, and rapid perfusion imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMAGIC\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u548c\u751f\u7406\u4fe1\u606f\u5c06\u975e\u5bf9\u6bd4CT\u56fe\u50cf\u6620\u5c04\u4e3a\u591a\u6a21\u6001\u65e0\u5bf9\u6bd4CTP\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCTP\u4f7f\u7528\u5bf9\u6bd4\u5242\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfCTP\u6210\u50cf\u4f9d\u8d56\u5bf9\u6bd4\u5242\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fc7\u654f\u53cd\u5e94\u548c\u9ad8\u6210\u672c\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u5bf9\u6bd4\u5242\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u751f\u7406\u4fe1\u606f\uff0c\u8bbe\u8ba1MAGIC\u6846\u67b6\uff0c\u901a\u8fc7\u635f\u5931\u51fd\u6570\u4f18\u5316\u56fe\u50cf\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u5352\u4e2d\u60a3\u8005\u6570\u636e\u4e0a\u8bad\u7ec3\u9a8c\u8bc1\u3002", "result": "MAGIC\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edf\u5bf9\u6bd4\u5242CTP\u6210\u50cf\u3002", "conclusion": "MAGIC\u6709\u671b\u6210\u4e3a\u65e0\u5bf9\u6bd4\u5242\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u704c\u6ce8\u6210\u50cf\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.22682", "pdf": "https://arxiv.org/pdf/2505.22682", "abs": "https://arxiv.org/abs/2505.22682", "authors": ["Xinxian Fan", "Mengye Lyu"], "title": "MRI Image Generation Based on Text Prompts", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "This study explores the use of text-prompted MRI image generation with the\nStable Diffusion (SD) model to address challenges in acquiring real MRI\ndatasets, such as high costs, limited rare case samples, and privacy concerns.\nThe SD model, pre-trained on natural images, was fine-tuned using the 3T\nfastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain\nT1, T2, and FLAIR images across different magnetic field strengths. The\nperformance of the fine-tuned model was evaluated using quantitative\nmetrics,including Fr\\'echet Inception Distance (FID) and Multi-Scale Structural\nSimilarity (MS-SSIM), showing improvements in image quality and semantic\nconsistency with the text prompts. To further evaluate the model's potential, a\nsimple classification task was carried out using a small 0.35T MRI dataset,\ndemonstrating that the synthetic images generated by the fine-tuned SD model\ncan effectively augment training datasets and improve the performance of MRI\nconstrast classification tasks. Overall, our findings suggest that\ntext-prompted MRI image generation is feasible and can serve as a useful tool\nfor medical AI applications.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528Stable Diffusion\u6a21\u578b\u751f\u6210\u6587\u672c\u63d0\u793a\u7684MRI\u56fe\u50cf\uff0c\u89e3\u51b3\u771f\u5b9eMRI\u6570\u636e\u96c6\u83b7\u53d6\u7684\u9ad8\u6210\u672c\u3001\u7a00\u6709\u6837\u672c\u5c11\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u4efb\u52a1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9eMRI\u6570\u636e\u96c6\u83b7\u53d6\u7684\u9ad8\u6210\u672c\u3001\u7a00\u6709\u6837\u672c\u4e0d\u8db3\u548c\u9690\u79c1\u95ee\u9898\uff0c\u63a2\u7d22\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66AI\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u75283T fastMRI\u548c0.3T M4Raw\u6570\u636e\u96c6\u5fae\u8c03Stable Diffusion\u6a21\u578b\uff0c\u751f\u6210\u4e0d\u540c\u78c1\u573a\u5f3a\u5ea6\u7684\u8111\u90e8T1\u3001T2\u548cFLAIR\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7FID\u548cMS-SSIM\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u80fd\u6709\u6548\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u63d0\u5347MRI\u5bf9\u6bd4\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u6587\u672c\u63d0\u793a\u7684MRI\u56fe\u50cf\u751f\u6210\u53ef\u884c\uff0c\u53ef\u4f5c\u4e3a\u533b\u5b66AI\u7684\u6709\u7528\u5de5\u5177\u3002"}}
{"id": "2505.22683", "pdf": "https://arxiv.org/pdf/2505.22683", "abs": "https://arxiv.org/abs/2505.22683", "authors": ["Xuhang Chen", "Michael Kwok-Po Ng", "Kim-Fung Tsang", "Chi-Man Pun", "Shuqiang Wang"], "title": "ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Brain network analysis plays a crucial role in diagnosing and monitoring\nneurodegenerative disorders such as Alzheimer's disease (AD). Existing\napproaches for constructing structural brain networks from diffusion tensor\nimaging (DTI) often rely on specialized toolkits that suffer from inherent\nlimitations: operator subjectivity, labor-intensive workflows, and restricted\ncapacity to capture complex topological features and disease-specific\nbiomarkers. To overcome these challenges and advance computational neuroimaging\ninstrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based\nframework for automated end-to-end brain network construction from DTI. The\nproposed model combines three key components: (1) a Template Network that\nextracts topological features from 3D DTI scans using Riemannian geometric\nprinciples, (2) a diffusion model that generates comprehensive brain networks\nwith enhanced topological fidelity, and (3) a Graph Convolutional Network\nclassifier that incorporates disease-specific markers to improve diagnostic\naccuracy. ConnectomeDiffuser demonstrates superior performance by capturing a\nbroader range of structural connectivity and pathology-related information,\nenabling more sensitive analysis of individual variations in brain networks.\nExperimental validation on datasets representing two distinct neurodegenerative\nconditions demonstrates significant performance improvements over other brain\nnetwork methods. This work contributes to the advancement of instrumentation in\nthe context of neurological disorders, providing clinicians and researchers\nwith a robust, generalizable measurement framework that facilitates more\naccurate diagnosis, deeper mechanistic understanding, and improved therapeutic\nmonitoring of neurodegenerative diseases such as AD.", "AI": {"tldr": "ConnectomeDiffuser\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4eceDTI\u6784\u5efa\u8111\u7f51\u7edc\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709DTI\u8111\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u5de5\u4f5c\u6d41\u7a0b\u7e41\u7410\u53ca\u65e0\u6cd5\u6355\u6349\u590d\u6742\u62d3\u6251\u7279\u5f81\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6a21\u677f\u7f51\u7edc\uff08\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff09\u3001\u6269\u6563\u6a21\u578b\uff08\u751f\u6210\u9ad8\u4fdd\u771f\u8111\u7f51\u7edc\uff09\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u5206\u7c7b\u5668\uff08\u6574\u5408\u75be\u75c5\u6807\u5fd7\u7269\uff09\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u6784\u5efa\u3002", "result": "\u5728\u4e24\u79cd\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u66f4\u654f\u611f\u5730\u5206\u6790\u8111\u7f51\u7edc\u4e2a\u4f53\u5dee\u5f02\u3002", "conclusion": "ConnectomeDiffuser\u4e3a\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u795e\u7ecf\u5f71\u50cf\u5b66\u4eea\u5668\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.22685", "pdf": "https://arxiv.org/pdf/2505.22685", "abs": "https://arxiv.org/abs/2505.22685", "authors": ["Marcus J. Vroemen", "Yuqian Chen", "Yui Lo", "Tengfei Xu", "Weidong Cai", "Fan Zhang", "Josien P. W. Pluim", "Lauren J. O'Donnell"], "title": "DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "15 pages, 5 figures, 5 tables", "summary": "Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural\nconnections, but traditional connectome generation is time-consuming and\nrequires gray matter parcellation, posing challenges for large-scale studies.\nWe introduce DeepMultiConnectome, a deep-learning model that predicts\nstructural connectomes directly from tractography, bypassing the need for gray\nmatter parcellation while supporting multiple parcellation schemes. Using a\npoint-cloud-based neural network with multi-task learning, the model classifies\nstreamlines according to their connected regions across two parcellation\nschemes, sharing a learned representation. We train and validate\nDeepMultiConnectome on tractography from the Human Connectome Project Young\nAdult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter\nparcellation scheme. DeepMultiConnectome predicts multiple structural\nconnectomes from a whole-brain tractogram containing 3 million streamlines in\napproximately 40 seconds. DeepMultiConnectome is evaluated by comparing\npredicted connectomes with traditional connectomes generated using the\nconventional method of labeling streamlines using a gray matter parcellation.\nThe predicted connectomes are highly correlated with traditionally generated\nconnectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region\nscheme) and largely preserve network properties. A test-retest analysis of\nDeepMultiConnectome demonstrates reproducibility comparable to traditionally\ngenerated connectomes. The predicted connectomes perform similarly to\ntraditionally generated connectomes in predicting age and cognitive function.\nOverall, DeepMultiConnectome provides a scalable, fast model for generating\nsubject-specific connectomes across multiple parcellation schemes.", "AI": {"tldr": "DeepMultiConnectome\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u76f4\u63a5\u4ece\u7ea4\u7ef4\u8ffd\u8e2a\u9884\u6d4b\u7ed3\u6784\u8fde\u63a5\u7ec4\uff0c\u65e0\u9700\u7070\u8d28\u5206\u533a\uff0c\u652f\u6301\u591a\u79cd\u5206\u533a\u65b9\u6848\uff0c\u901f\u5ea6\u5feb\u4e14\u7ed3\u679c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u8fde\u63a5\u7ec4\u751f\u6210\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u7070\u8d28\u5206\u533a\uff0c\u96be\u4ee5\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u70b9\u4e91\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6a21\u578b\u6839\u636e\u4e24\u79cd\u5206\u533a\u65b9\u6848\u5bf9\u7ea4\u7ef4\u675f\u8fdb\u884c\u5206\u7c7b\uff0c\u5171\u4eab\u5b66\u4e60\u8868\u793a\u3002", "result": "\u9884\u6d4b\u7684\u8fde\u63a5\u7ec4\u4e0e\u4f20\u7edf\u65b9\u6cd5\u751f\u6210\u7684\u8fde\u63a5\u7ec4\u9ad8\u5ea6\u76f8\u5173\uff08r=0.992\u548cr=0.986\uff09\uff0c\u4e14\u4fdd\u7559\u4e86\u7f51\u7edc\u7279\u6027\u3002", "conclusion": "DeepMultiConnectome\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u5206\u533a\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7814\u7a76\u3002"}}
{"id": "2505.22764", "pdf": "https://arxiv.org/pdf/2505.22764", "abs": "https://arxiv.org/abs/2505.22764", "authors": ["Divya Shanmugam", "Helen Lu", "Swami Sankaranarayanan", "John Guttag"], "title": "Test-time augmentation improves efficiency in conformal prediction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "A conformal classifier produces a set of predicted classes and provides a\nprobabilistic guarantee that the set includes the true class. Unfortunately, it\nis often the case that conformal classifiers produce uninformatively large\nsets. In this work, we show that test-time augmentation (TTA)--a technique that\nintroduces inductive biases during inference--reduces the size of the sets\nproduced by conformal classifiers. Our approach is flexible, computationally\nefficient, and effective. It can be combined with any conformal score, requires\nno model retraining, and reduces prediction set sizes by 10%-14% on average. We\nconduct an evaluation of the approach spanning three datasets, three models,\ntwo established conformal scoring methods, different guarantee strengths, and\nseveral distribution shifts to show when and why test-time augmentation is a\nuseful addition to the conformal pipeline.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08TTA\uff09\u51cf\u5c11\u4fdd\u5f62\u5206\u7c7b\u5668\u9884\u6d4b\u96c6\u5927\u5c0f\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5e73\u5747\u51cf\u5c1110%-14%\u7684\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "motivation": "\u4fdd\u5f62\u5206\u7c7b\u5668\u867d\u7136\u80fd\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\uff0c\u4f46\u901a\u5e38\u4f1a\u4ea7\u751f\u8fc7\u5927\u7684\u9884\u6d4b\u96c6\uff0c\u7f3a\u4e4f\u4fe1\u606f\u91cf\u3002", "method": "\u91c7\u7528\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08TTA\uff09\u6280\u672f\uff0c\u7ed3\u5408\u4efb\u610f\u4fdd\u5f62\u8bc4\u5206\u65b9\u6cd5\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u79cd\u6a21\u578b\u548c\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\uff0cTTA\u5e73\u5747\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f10%-14%\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u589e\u5f3a\u662f\u4fdd\u5f62\u5206\u7c7b\u5668\u6d41\u7a0b\u4e2d\u7684\u6709\u6548\u8865\u5145\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\u65b9\u9762\u8868\u73b0\u663e\u8457\u3002"}}
{"id": "2505.22769", "pdf": "https://arxiv.org/pdf/2505.22769", "abs": "https://arxiv.org/abs/2505.22769", "authors": ["Yaxiong Lei", "Mingyue Zhao", "Yuheng Wang", "Shijing He", "Yusuke Sugano", "Yafei Wang", "Kaixing Zhao", "Mohamed Khamis", "Juan Ye"], "title": "MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking", "categories": ["cs.HC", "cs.CV", "68T10, 68U35", "H.5.2; H.1.2; C.2.4; I.5.4"], "comment": "24 pages, 7 figures", "summary": "Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as\nusers naturally change their postures and device orientations. Traditional\ncalibration approaches, like one-off, fail to adapt to these dynamic\nconditions, leading to degraded performance over time. We present MAC-Gaze, a\nMotion-Aware continual Calibration approach that leverages smartphone Inertial\nmeasurement unit (IMU) sensors and continual learning techniques to\nautomatically detect changes in user motion states and update the gaze tracking\nmodel accordingly. Our system integrates a pre-trained visual gaze estimator\nand an IMU-based activity recognition model with a clustering-based hybrid\ndecision-making mechanism that triggers recalibration when motion patterns\ndeviate significantly from previously encountered states. To enable\naccumulative learning of new motion conditions while mitigating catastrophic\nforgetting, we employ replay-based continual learning, allowing the model to\nmaintain performance across previously encountered motion conditions. We\nevaluate our system through extensive experiments on the publicly available\nRGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+\nimages, 800K+ IMU readings), encompassing a wide range of postures under\nvarious motion conditions including sitting, standing, lying, and walking.\nResults demonstrate that our method reduces gaze estimation error by 19.9% on\nRGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to\n1.92 cm) compared to traditional calibration approaches. Our framework provides\na robust solution for maintaining gaze estimation accuracy in mobile scenarios.", "AI": {"tldr": "MAC-Gaze\u662f\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u611f\u77e5\u7684\u6301\u7eed\u6821\u51c6\u65b9\u6cd5\uff0c\u5229\u7528\u667a\u80fd\u624b\u673aIMU\u4f20\u611f\u5668\u548c\u6301\u7eed\u5b66\u4e60\u6280\u672f\uff0c\u52a8\u6001\u8c03\u6574\u89c6\u7ebf\u8ddf\u8e2a\u6a21\u578b\u4ee5\u9002\u5e94\u7528\u6237\u59ff\u52bf\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u4e00\u6b21\u6027\u6821\u51c6\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u59ff\u52bf\u53d8\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6301\u7eed\u9002\u5e94\u53d8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u89c6\u7ebf\u4f30\u8ba1\u5668\u548cIMU\u6d3b\u52a8\u8bc6\u522b\u6a21\u578b\uff0c\u91c7\u7528\u805a\u7c7b\u6df7\u5408\u51b3\u7b56\u673a\u5236\u89e6\u53d1\u91cd\u65b0\u6821\u51c6\uff0c\u5e76\u5229\u7528\u56de\u653e\u5f0f\u6301\u7eed\u5b66\u4e60\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728RGBDGaze\u548cMotionGaze\u6570\u636e\u96c6\u4e0a\uff0c\u89c6\u7ebf\u4f30\u8ba1\u8bef\u5dee\u5206\u522b\u964d\u4f4e19.9%\u548c31.7%\u3002", "conclusion": "MAC-Gaze\u4e3a\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u89c6\u7ebf\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6301\u7eed\u6821\u51c6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.22805", "pdf": "https://arxiv.org/pdf/2505.22805", "abs": "https://arxiv.org/abs/2505.22805", "authors": ["Siddharth Ancha", "Sunshine Jiang", "Travis Manderson", "Laura Brandt", "Yilun Du", "Philip R. Osteen", "Nicholas Roy"], "title": "Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Presented at ICRA 2025", "summary": "In order to navigate safely and reliably in off-road and unstructured\nenvironments, robots must detect anomalies that are out-of-distribution (OOD)\nwith respect to the training data. We present an analysis-by-synthesis approach\nfor pixel-wise anomaly detection without making any assumptions about the\nnature of OOD data. Given an input image, we use a generative diffusion model\nto synthesize an edited image that removes anomalies while keeping the\nremaining image unchanged. Then, we formulate anomaly detection as analyzing\nwhich image segments were modified by the diffusion model. We propose a novel\ninference approach for guided diffusion by analyzing the ideal guidance\ngradient and deriving a principled approximation that bootstraps the diffusion\nmodel to predict guidance gradients. Our editing technique is purely test-time\nthat can be integrated into existing workflows without the need for retraining\nor fine-tuning. Finally, we use a combination of vision-language foundation\nmodels to compare pixels in a learned feature space and detect semantically\nmeaningful edits, enabling accurate anomaly detection for off-road navigation.\nProject website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u50cf\u7d20\u7ea7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u5bf9\u5f02\u5e38\u6570\u636e\u505a\u5047\u8bbe\uff0c\u901a\u8fc7\u7f16\u8f91\u56fe\u50cf\u5e76\u68c0\u6d4b\u4fee\u6539\u533a\u57df\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u68c0\u6d4b\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u540c\u7684\u5f02\u5e38\uff0c\u4ee5\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u3002", "method": "\u4f7f\u7528\u751f\u6210\u6269\u6563\u6a21\u578b\u7f16\u8f91\u8f93\u5165\u56fe\u50cf\u4ee5\u79fb\u9664\u5f02\u5e38\uff0c\u901a\u8fc7\u5206\u6790\u4fee\u6539\u533a\u57df\u68c0\u6d4b\u5f02\u5e38\uff1b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f15\u5bfc\u6269\u6563\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u65b9\u6cd5\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709\u5de5\u4f5c\u6d41\u4e2d\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u51c6\u786e\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002"}}
{"id": "2505.22923", "pdf": "https://arxiv.org/pdf/2505.22923", "abs": "https://arxiv.org/abs/2505.22923", "authors": ["Anqi Li", "Weijie Gan", "Ulugbek S. Kamilov"], "title": "Plug-and-Play Posterior Sampling for Blind Inverse Problems", "categories": ["eess.IV", "cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2305.12672", "summary": "We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel\nframework for solving blind inverse problems where both the target image and\nthe measurement operator are unknown. Unlike conventional methods that rely on\nexplicit priors or separate parameter estimation, our approach performs\nposterior sampling by recasting the problem into an alternating Gaussian\ndenoising scheme. We leverage two diffusion models as learned priors: one to\ncapture the distribution of the target image and another to characterize the\nparameters of the measurement operator. This PnP integration of diffusion\nmodels ensures flexibility and ease of adaptation. Our experiments on blind\nimage deblurring show that Blind-PnPDM outperforms state-of-the-art methods in\nterms of both quantitative metrics and visual fidelity. Our results highlight\nthe effectiveness of treating blind inverse problems as a sequence of denoising\nsubproblems while harnessing the expressive power of diffusion-based priors.", "AI": {"tldr": "Blind-PnPDM\u662f\u4e00\u79cd\u89e3\u51b3\u76f2\u9006\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u65e0\u9700\u663e\u5f0f\u5148\u9a8c\u6216\u5355\u72ec\u53c2\u6570\u4f30\u8ba1\uff0c\u800c\u662f\u901a\u8fc7\u4ea4\u66ff\u9ad8\u65af\u53bb\u566a\u65b9\u6848\u8fdb\u884c\u540e\u9a8c\u91c7\u6837\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u5148\u9a8c\u6216\u5355\u72ec\u53c2\u6570\u4f30\u8ba1\uff0c\u800cBlind-PnPDM\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5b66\u4e60\u5148\u9a8c\uff0c\u7075\u6d3b\u89e3\u51b3\u76f2\u9006\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6269\u6563\u6a21\u578b\u5206\u522b\u6355\u6349\u76ee\u6807\u56fe\u50cf\u5206\u5e03\u548c\u6d4b\u91cf\u7b97\u5b50\u53c2\u6570\uff0c\u901a\u8fc7\u4ea4\u66ff\u9ad8\u65af\u53bb\u566a\u65b9\u6848\u5b9e\u73b0\u540e\u9a8c\u91c7\u6837\u3002", "result": "\u5728\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u5b9e\u9a8c\u4e2d\uff0cBlind-PnPDM\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Blind-PnPDM\u901a\u8fc7\u5c06\u76f2\u9006\u95ee\u9898\u8f6c\u5316\u4e3a\u53bb\u566a\u5b50\u95ee\u9898\u5e8f\u5217\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2505.22991", "pdf": "https://arxiv.org/pdf/2505.22991", "abs": "https://arxiv.org/abs/2505.22991", "authors": ["Behzad Kamgar-Parsi", "Behrooz Kamgar-Parsi"], "title": "Number of Clusters in a Dataset: A Regularized K-means Approach", "categories": ["cs.LG", "cs.CV", "68", "I.5.3"], "comment": "19 pages, 14 figures. arXiv admin note: substantial text overlap with\n  arXiv:1911.06741", "summary": "Finding the number of meaningful clusters in an unlabeled dataset is\nimportant in many applications. Regularized k-means algorithm is a possible\napproach frequently used to find the correct number of distinct clusters in\ndatasets. The most common formulation of the regularization function is the\nadditive linear term $\\lambda k$, where $k$ is the number of clusters and\n$\\lambda$ a positive coefficient. Currently, there are no principled guidelines\nfor setting a value for the critical hyperparameter $\\lambda$. In this paper,\nwe derive rigorous bounds for $\\lambda$ assuming clusters are {\\em ideal}.\nIdeal clusters (defined as $d$-dimensional spheres with identical radii) are\nclose proxies for k-means clusters ($d$-dimensional spherically symmetric\ndistributions with identical standard deviations). Experiments show that the\nk-means algorithm with additive regularizer often yields multiple solutions.\nThus, we also analyze k-means algorithm with multiplicative regularizer. The\nconsensus among k-means solutions with additive and multiplicative\nregularizations reduces the ambiguity of multiple solutions in certain cases.\nWe also present selected experiments that demonstrate performance of the\nregularized k-means algorithms as clusters deviate from the ideal assumption.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6b63\u5219\u5316k-means\u7b97\u6cd5\u4e2d\u5173\u952e\u8d85\u53c2\u6570\u03bb\u7684\u8bbe\u5b9a\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u7406\u60f3\u805a\u7c7b\u5047\u8bbe\u4e0b\u7684\u03bb\u8fb9\u754c\uff0c\u5e76\u5206\u6790\u4e86\u52a0\u6027\u548c\u4e58\u6027\u6b63\u5219\u5316\u5bf9\u89e3\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u4e2d\u786e\u5b9a\u6709\u610f\u4e49\u7684\u805a\u7c7b\u6570\u91cf\u662f\u8bb8\u591a\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8bbe\u5b9a\u6b63\u5219\u5316\u8d85\u53c2\u6570\u03bb\u7684\u539f\u5219\u6027\u6307\u5bfc\u3002", "method": "\u5047\u8bbe\u805a\u7c7b\u4e3a\u7406\u60f3\u7403\u5f62\uff0c\u63a8\u5bfc\u03bb\u7684\u4e25\u683c\u8fb9\u754c\uff1b\u5206\u6790\u52a0\u6027\u548c\u4e58\u6027\u6b63\u5219\u5316k-means\u7b97\u6cd5\u7684\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u52a0\u6027\u6b63\u5219\u5316\u5e38\u4ea7\u751f\u591a\u89e3\uff0c\u800c\u4e58\u6027\u6b63\u5219\u5316\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u51cf\u5c11\u89e3\u7684\u6a21\u7cca\u6027\u3002", "conclusion": "\u672c\u6587\u4e3a\u03bb\u7684\u8bbe\u5b9a\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u6b63\u5219\u5316k-means\u7b97\u6cd5\u5728\u975e\u7406\u60f3\u805a\u7c7b\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23027", "pdf": "https://arxiv.org/pdf/2505.23027", "abs": "https://arxiv.org/abs/2505.23027", "authors": ["Minh Nguyen Nhat To", "Paul F RWilson", "Viet Nguyen", "Mohamed Harmanani", "Michael Cooper", "Fahimeh Fooladgar", "Purang Abolmaesumi", "Parvin Mousavi", "Rahul G. Krishnan"], "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Paper", "summary": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiverse Prototypical Ensembles\uff08DPEs\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u591a\u6837\u5316\u7684\u539f\u578b\u5206\u7c7b\u5668\u96c6\u5408\u6765\u5e94\u5bf9\u5b50\u7fa4\u4f53\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u5b50\u7fa4\u4f53\u5206\u5e03\u504f\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u5b50\u7fa4\u4f53\u6570\u91cf\u548c\u6027\u8d28\u7684\u5047\u8bbe\u53ca\u6807\u6ce8\uff0c\u8fd9\u5728\u73b0\u5b9e\u6570\u636e\u4e2d\u5f80\u5f80\u4e0d\u53ef\u7528\u3002", "method": "\u7528\u591a\u6837\u5316\u7684\u539f\u578b\u5206\u7c7b\u5668\u96c6\u5408\u66ff\u4ee3\u6807\u51c6\u7ebf\u6027\u5206\u7c7b\u5c42\uff0c\u6bcf\u4e2a\u5206\u7c7b\u5668\u4e13\u6ce8\u4e8e\u4e0d\u540c\u7684\u7279\u5f81\u548c\u6837\u672c\uff0c\u4ece\u800c\u81ea\u9002\u5e94\u5730\u6355\u6349\u5b50\u7fa4\u4f53\u98ce\u9669\u3002", "result": "\u5728\u4e5d\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPE\u65b9\u6cd5\u5728\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DPE\u65b9\u6cd5\u65e0\u9700\u4f9d\u8d56\u5b50\u7fa4\u4f53\u6807\u6ce8\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5b50\u7fa4\u4f53\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002"}}
{"id": "2505.23173", "pdf": "https://arxiv.org/pdf/2505.23173", "abs": "https://arxiv.org/abs/2505.23173", "authors": ["Shohei Enomoto"], "title": "Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Deep learning models often struggle to maintain performance when deployed on\ndata distributions different from their training data, particularly in\nreal-world applications where environmental conditions frequently change. While\nMulti-source Domain Generalization (MDG) has shown promise in addressing this\nchallenge by leveraging multiple source domains during training, its practical\napplication is limited by the significant costs and difficulties associated\nwith creating multi-domain datasets. To address this limitation, we propose\nPseudo Multi-source Domain Generalization (PMDG), a novel framework that\nenables the application of sophisticated MDG algorithms in more practical\nSingle-source Domain Generalization (SDG) settings. PMDG generates multiple\npseudo-domains from a single source domain through style transfer and data\naugmentation techniques, creating a synthetic multi-domain dataset that can be\nused with existing MDG algorithms. Through extensive experiments with\nPseudoDomainBed, our modified version of the DomainBed benchmark, we analyze\nthe effectiveness of PMDG across multiple datasets and architectures. Our\nanalysis reveals several key findings, including a positive correlation between\nMDG and PMDG performance and the potential of pseudo-domains to match or exceed\nactual multi-domain performance with sufficient data. These comprehensive\nempirical results provide valuable insights for future research in domain\ngeneralization. Our code is available at\nhttps://github.com/s-enmt/PseudoDomainBed.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPMDG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u4ece\u5355\u4e00\u6e90\u57df\u751f\u6210\u591a\u4e2a\u4f2a\u57df\uff0c\u89e3\u51b3\u4e86\u591a\u6e90\u57df\u6cdb\u5316\uff08MDG\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPMDG\u6027\u80fd\u4e0eMDG\u6b63\u76f8\u5173\uff0c\u4e14\u4f2a\u57df\u5728\u6570\u636e\u5145\u8db3\u65f6\u53ef\u5ab2\u7f8e\u771f\u5b9e\u591a\u57df\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u6570\u636e\u4e0a\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u591a\u6e90\u57df\u6cdb\u5316\uff08MDG\uff09\u56e0\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPMDG\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u548c\u6570\u636e\u589e\u5f3a\u4ece\u5355\u4e00\u6e90\u57df\u751f\u6210\u591a\u4e2a\u4f2a\u57df\uff0c\u6784\u5efa\u5408\u6210\u591a\u57df\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u73b0\u6709MDG\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660ePMDG\u6027\u80fd\u4e0eMDG\u6b63\u76f8\u5173\uff0c\u4f2a\u57df\u5728\u6570\u636e\u5145\u8db3\u65f6\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u771f\u5b9e\u591a\u57df\u6027\u80fd\u3002", "conclusion": "PMDG\u4e3a\u5355\u6e90\u57df\u6cdb\u5316\uff08SDG\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u57df\u6cdb\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2505.23189", "pdf": "https://arxiv.org/pdf/2505.23189", "abs": "https://arxiv.org/abs/2505.23189", "authors": ["Shaoan Wang", "Jiazhao Zhang", "Minghan Li", "Jiahang Liu", "Anqi Li", "Kui Wu", "Fangwei Zhong", "Junzhi Yu", "Zhizheng Zhang", "He Wang"], "title": "TrackVLA: Embodied Visual Tracking in the Wild", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an\nagent to follow a specific target in dynamic environments using only egocentric\nvision. This task is inherently challenging as it requires both accurate target\nrecognition and effective trajectory planning under conditions of severe\nocclusion and high scene dynamics. Existing approaches typically address this\nchallenge through a modular separation of recognition and planning. In this\nwork, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the\nsynergy between object recognition and trajectory planning. Leveraging a shared\nLLM backbone, we employ a language modeling head for recognition and an\nanchor-based diffusion model for trajectory planning. To train TrackVLA, we\nconstruct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse\ndifficulty levels of recognition samples, resulting in a dataset of 1.7 million\nsamples. Through extensive experiments in both synthetic and real-world\nenvironments, TrackVLA demonstrates SOTA performance and strong\ngeneralizability. It significantly outperforms existing methods on public\nbenchmarks in a zero-shot manner while remaining robust to high dynamics and\nocclusion in real-world scenarios at 10 FPS inference speed. Our project page\nis: https://pku-epic.github.io/TrackVLA-web.", "AI": {"tldr": "TrackVLA\u662f\u4e00\u79cd\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u76ee\u6807\u8bc6\u522b\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u76ee\u6807\u8bc6\u522b\u548c\u8f68\u8ff9\u89c4\u5212\u5206\u79bb\u65f6\u9762\u4e34\u7684\u906e\u6321\u548c\u52a8\u6001\u573a\u666f\u6311\u6218\u3002", "method": "\u5229\u7528\u5171\u4eab\u7684LLM\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u8bed\u8a00\u5efa\u6a21\u5934\u548c\u57fa\u4e8e\u951a\u70b9\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8bc6\u522b\u4e0e\u89c4\u5212\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51faSOTA\u6027\u80fd\uff0c\u96f6\u6837\u672c\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c10 FPS\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "TrackVLA\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u52a8\u6001\u548c\u906e\u6321\u573a\u666f\u3002"}}
{"id": "2505.23266", "pdf": "https://arxiv.org/pdf/2505.23266", "abs": "https://arxiv.org/abs/2505.23266", "authors": ["Chunlong Xie", "Jialing He", "Shangwei Guo", "Jiacheng Wang", "Shudong Zhang", "Tianwei Zhang", "Tao Xiang"], "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": "Under review", "summary": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments.", "AI": {"tldr": "AdvOF\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4ee3\u7406\u7684\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u60273D\u5bf9\u8c61\u6765\u7814\u7a76\u5176\u5bf9VLM\u611f\u77e5\u6a21\u5757\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u670d\u52a1\u5bfc\u5411\u73af\u5883\u4e2dVLM\u5bfc\u822a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u6f0f\u6d1e\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u672a\u8003\u8651\u670d\u52a1\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u9700\u6c42\u3002", "method": "AdvOF\u901a\u8fc7\u7cbe\u786e\u805a\u5408\u548c\u5bf9\u9f502D/3D\u7a7a\u95f4\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u5b9a\u4e49\u5e76\u6e32\u67d3\u5bf9\u6297\u6027\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u591a\u89c6\u89d2\u4f18\u5316\u548c\u6b63\u5219\u5316\u534f\u540c\u4f18\u5316\u3002", "result": "AdvOF\u80fd\u6709\u6548\u964d\u4f4e\u4ee3\u7406\u6027\u80fd\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u6b63\u5e38\u5bfc\u822a\u4efb\u52a1\u7684\u5e72\u6270\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86VLM\u5bfc\u822a\u7cfb\u7edf\u670d\u52a1\u5b89\u5168\u6027\u7684\u7406\u89e3\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u670d\u52a1\u7ec4\u5408\u63d0\u4f9b\u4e86\u8ba1\u7b97\u57fa\u7840\u3002"}}
{"id": "2505.23290", "pdf": "https://arxiv.org/pdf/2505.23290", "abs": "https://arxiv.org/abs/2505.23290", "authors": ["Hao Li", "Ju Dai", "Xin Zhao", "Feng Zhou", "Junjun Pan", "Lei Li"], "title": "Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation", "categories": ["cs.SD", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In 3D speech-driven facial animation generation, existing methods commonly\nemploy pre-trained self-supervised audio models as encoders. However, due to\nthe prevalence of phonetically similar syllables with distinct lip shapes in\nlanguage, these near-homophone syllables tend to exhibit significant coupling\nin self-supervised audio feature spaces, leading to the averaging effect in\nsubsequent lip motion generation. To address this issue, this paper proposes a\nplug-and-play semantic decorrelation module-Wav2Sem. This module extracts\nsemantic features corresponding to the entire audio sequence, leveraging the\nadded semantic information to decorrelate audio encodings within the feature\nspace, thereby achieving more expressive audio features. Extensive experiments\nacross multiple Speech-driven models indicate that the Wav2Sem module\neffectively decouples audio features, significantly alleviating the averaging\neffect of phonetically similar syllables in lip shape generation, thereby\nenhancing the precision and naturalness of facial animations. Our source code\nis available at https://github.com/wslh852/Wav2Sem.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faWav2Sem\u6a21\u5757\uff0c\u901a\u8fc7\u8bed\u4e49\u89e3\u8026\u89e3\u51b33D\u8bed\u97f3\u9a71\u52a8\u9762\u90e8\u52a8\u753b\u4e2d\u97f3\u7d20\u76f8\u4f3c\u97f3\u8282\u5bfc\u81f4\u7684\u8026\u5408\u95ee\u9898\uff0c\u63d0\u5347\u52a8\u753b\u7cbe\u5ea6\u548c\u81ea\u7136\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u81ea\u76d1\u7763\u97f3\u9891\u6a21\u578b\u7f16\u7801\u5668\uff0c\u4f46\u97f3\u7d20\u76f8\u4f3c\u97f3\u8282\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8026\u5408\u4e25\u91cd\uff0c\u5bfc\u81f4\u5507\u5f62\u751f\u6210\u7684\u5e73\u5747\u6548\u5e94\u3002", "method": "\u63d0\u51faWav2Sem\u6a21\u5757\uff0c\u63d0\u53d6\u97f3\u9891\u5e8f\u5217\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u89e3\u8026\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u97f3\u9891\u7f16\u7801\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWav2Sem\u6709\u6548\u89e3\u8026\u97f3\u9891\u7279\u5f81\uff0c\u663e\u8457\u51cf\u8f7b\u5507\u5f62\u751f\u6210\u7684\u5e73\u5747\u6548\u5e94\u3002", "conclusion": "Wav2Sem\u6a21\u5757\u63d0\u5347\u4e86\u9762\u90e8\u52a8\u753b\u7684\u7cbe\u786e\u6027\u548c\u81ea\u7136\u5ea6\u3002"}}
{"id": "2505.23317", "pdf": "https://arxiv.org/pdf/2505.23317", "abs": "https://arxiv.org/abs/2505.23317", "authors": ["Woojin Shin", "Donghwa Kang", "Byeongyun Park", "Brent Byunghoon Kang", "Jinkyu Lee", "Hyeongboo Baek"], "title": "CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection", "categories": ["eess.SY", "cs.CV", "cs.SY"], "comment": "12 pages", "summary": "Detection Transformers (DETR) are increasingly adopted in autonomous vehicle\n(AV) perception systems due to their superior accuracy over convolutional\nnetworks. However, concurrently executing multiple DETR tasks presents\nsignificant challenges in meeting firm real-time deadlines (R1) and high\naccuracy requirements (R2), particularly for safety-critical objects, while\nnavigating the inherent latency-accuracy trade-off under resource constraints.\nExisting real-time DNN scheduling approaches often treat models generically,\nfailing to leverage Transformer-specific properties for efficient resource\nallocation. To address these challenges, we propose CF-DETR, an integrated\nsystem featuring a novel coarse-to-fine Transformer architecture and a\ndedicated real-time scheduling framework NPFP**. CF-DETR employs three key\nstrategies (A1: coarse-to-fine inference, A2: selective fine inference, A3:\nmulti-level batch inference) that exploit Transformer properties to dynamically\nadjust patch granularity and attention scope based on object criticality,\naiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these\nadaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical\ncoarse subtask for guaranteed critical object detection within its deadline\n(ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2),\nwhile managing individual and batched execution. Our extensive evaluations on\nserver, GPU-enabled embedded platforms, and actual AV platforms demonstrate\nthat CF-DETR, under an NPFP** policy, successfully meets strict timing\nguarantees for critical operations and achieves significantly higher overall\nand critical object detection accuracy compared to existing baselines across\ndiverse AV workloads.", "AI": {"tldr": "CF-DETR\u901a\u8fc7\u7c97\u5230\u7ec6\u7684Transformer\u67b6\u6784\u548c\u5b9e\u65f6\u8c03\u5ea6\u6846\u67b6NPFP**\uff0c\u89e3\u51b3\u4e86\u591aDETR\u4efb\u52a1\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5b9e\u65f6DNN\u8c03\u5ea6\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528Transformer\u7279\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5bf9\u5b9e\u65f6\u6027\u548c\u9ad8\u7cbe\u5ea6\u7684\u53cc\u91cd\u9700\u6c42\u3002", "method": "\u63d0\u51faCF-DETR\u7cfb\u7edf\uff0c\u7ed3\u5408\u7c97\u5230\u7ec6\u63a8\u7406\u3001\u9009\u62e9\u6027\u7ec6\u63a8\u7406\u548c\u591a\u7ea7\u6279\u91cf\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u53caNPFP**\u8c03\u5ea6\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5728\u591a\u79cd\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0cCF-DETR\u80fd\u4fdd\u8bc1\u5173\u952e\u64cd\u4f5c\u7684\u5b9e\u65f6\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u6574\u4f53\u548c\u5173\u952e\u7269\u4f53\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "CF-DETR\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.23353", "pdf": "https://arxiv.org/pdf/2505.23353", "abs": "https://arxiv.org/abs/2505.23353", "authors": ["Alexandra G. Roberts", "Ha M. Luu", "Mert \u015ei\u015fman", "Alexey V. Dimov", "Ceren Tozlu", "Ilhami Kovanlikaya", "Susan A. Gauthier", "Thanh D. Nguyen", "Yi Wang"], "title": "Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted full paper in Synthetic Data @ CVPR 2025 12 pages, 10\n  figures", "summary": "Quantitative susceptibility maps from magnetic resonance images can provide\nboth prognostic and diagnostic information in multiple sclerosis, a\nneurodegenerative disease characterized by the formation of lesions in white\nmatter brain tissue. In particular, susceptibility maps provide adequate\ncontrast to distinguish between \"rim\" lesions, surrounded by deposited\nparamagnetic iron, and \"non-rim\" lesion types. These paramagnetic rim lesions\n(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been\ndevoted to both detection and segmentation of such lesions to monitor\nlongitudinal change. As paramagnetic rim lesions are rare, addressing this\nproblem requires confronting the class imbalance between rim and non-rim\nlesions. We produce synthetic quantitative susceptibility maps of paramagnetic\nrim lesions and show that inclusion of such synthetic data improves classifier\nperformance and provide a multi-channel extension to generate accompanying\ncontrasts and probabilistic segmentation maps. We exploit the projection\ncapability of our trained generative network to demonstrate a novel denoising\napproach that allows us to train on ambiguous rim cases and substantially\nincrease the minority class. We show that both synthetic lesion synthesis and\nour proposed rim lesion label denoising method best approximate the unseen rim\nlesion distribution and improve detection in a clinically interpretable manner.\nWe release our code and generated data at https://github.com/agr78/PRLx-GAN\nupon publication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u5b9a\u91cf\u78c1\u5316\u7387\u56fe\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u591a\u53d1\u6027\u786c\u5316\u75c7\u4e2d\u8fb9\u7f18\u75c5\u53d8\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u6269\u5c55\u4e86\u591a\u901a\u9053\u5bf9\u6bd4\u548c\u6982\u7387\u5206\u5272\u56fe\u3002", "motivation": "\u591a\u53d1\u6027\u786c\u5316\u75c7\u4e2d\u7684\u8fb9\u7f18\u75c5\u53d8\uff08PRLs\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u7531\u4e8e\u5176\u7f55\u89c1\u6027\uff0c\u5206\u7c7b\u5668\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5408\u6210\u5b9a\u91cf\u78c1\u5316\u7387\u56fe\uff0c\u5e76\u5229\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u8fdb\u884c\u591a\u901a\u9053\u6269\u5c55\u548c\u6982\u7387\u5206\u5272\u56fe\u751f\u6210\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u566a\u65b9\u6cd5\u4ee5\u5904\u7406\u6a21\u7cca\u7684\u8fb9\u7f18\u75c5\u53d8\u3002", "result": "\u5408\u6210\u6570\u636e\u548c\u53bb\u566a\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u8fb9\u7f18\u75c5\u53d8\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u66f4\u597d\u5730\u903c\u8fd1\u4e86\u771f\u5b9e\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u53d1\u6027\u786c\u5316\u75c7\u8fb9\u7f18\u75c5\u53d8\u7684\u68c0\u6d4b\u4e2d\u5177\u6709\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u751f\u6210\u6570\u636e\u3002"}}
{"id": "2505.23412", "pdf": "https://arxiv.org/pdf/2505.23412", "abs": "https://arxiv.org/abs/2505.23412", "authors": ["Srishti Gupta", "Daniele Angioni", "Maura Pintor", "Ambra Demontis", "Lea Sch\u00f6nherr", "Battista Biggio", "Fabio Roli"], "title": "Buffer-free Class-Incremental Learning with Out-of-Distribution Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Class-incremental learning (CIL) poses significant challenges in open-world\nscenarios, where models must not only learn new classes over time without\nforgetting previous ones but also handle inputs from unknown classes that a\nclosed-set model would misclassify. Recent works address both issues by\n(i)~training multi-head models using the task-incremental learning framework,\nand (ii) predicting the task identity employing out-of-distribution (OOD)\ndetectors. While effective, the latter mainly relies on joint training with a\nmemory buffer of past data, raising concerns around privacy, scalability, and\nincreased training time. In this paper, we present an in-depth analysis of\npost-hoc OOD detection methods and investigate their potential to eliminate the\nneed for a memory buffer. We uncover that these methods, when applied\nappropriately at inference time, can serve as a strong substitute for\nbuffer-based OOD detection. We show that this buffer-free approach achieves\ncomparable or superior performance to buffer-based methods both in terms of\nclass-incremental learning and the rejection of unknown samples. Experimental\nresults on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,\noffering new insights into the design of efficient and privacy-preserving CIL\nsystems for open-world settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5185\u5b58\u7f13\u51b2\u533a\u7684\u540e\u9a8cOOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u7c7b\u589e\u91cf\u5b66\u4e60\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u7f13\u51b2\u533a\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u548c\u8bad\u7ec3\u65f6\u95f4\u589e\u52a0\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u7f13\u51b2\u533a\u3002", "method": "\u5206\u6790\u5e76\u5e94\u7528\u540e\u9a8cOOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u66ff\u4ee3\u57fa\u4e8e\u7f13\u51b2\u533a\u7684OOD\u68c0\u6d4b\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u5904\u7406\u672a\u77e5\u7c7b\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4e0e\u7f13\u51b2\u533a\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u540e\u9a8cOOD\u68c0\u6d4b\u65b9\u6cd5\u4e3a\u5f00\u653e\u4e16\u754c\u7c7b\u589e\u91cf\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23448", "pdf": "https://arxiv.org/pdf/2505.23448", "abs": "https://arxiv.org/abs/2505.23448", "authors": ["Pirzada Suhail", "Rehna Afroz", "Amit Sethi"], "title": "Network Inversion for Uncertainty-Aware Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Out-of-distribution (OOD) detection and uncertainty estimation (UE) are\ncritical components for building safe machine learning systems, especially in\nreal-world scenarios where unexpected inputs are inevitable. In this work, we\npropose a novel framework that combines network inversion with classifier\ntraining to simultaneously address both OOD detection and uncertainty\nestimation. For a standard n-class classification task, we extend the\nclassifier to an (n+1)-class model by introducing a \"garbage\" class, initially\npopulated with random gaussian noise to represent outlier inputs. After each\ntraining epoch, we use network inversion to reconstruct input images\ncorresponding to all output classes that initially appear as noisy and\nincoherent and are therefore excluded to the garbage class for retraining the\nclassifier. This cycle of training, inversion, and exclusion continues\niteratively till the inverted samples begin to resemble the in-distribution\ndata more closely, suggesting that the classifier has learned to carve out\nmeaningful decision boundaries while sanitising the class manifolds by pushing\nOOD content into the garbage class. During inference, this training scheme\nenables the model to effectively detect and reject OOD samples by classifying\nthem into the garbage class. Furthermore, the confidence scores associated with\neach prediction can be used to estimate uncertainty for both in-distribution\nand OOD inputs. Our approach is scalable, interpretable, and does not require\naccess to external OOD datasets or post-hoc calibration techniques while\nproviding a unified solution to the dual challenges of OOD detection and\nuncertainty estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7f51\u7edc\u53cd\u8f6c\u548c\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u89e3\u51b3OOD\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u6784\u5efa\u5b89\u5168\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u9700\u8981\u6709\u6548\u5904\u7406\u610f\u5916\u8f93\u5165\uff0cOOD\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u662f\u5173\u952e\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u5783\u573e\u201d\u7c7b\u5e76\u8fed\u4ee3\u8bad\u7ec3\u3001\u53cd\u8f6c\u548c\u6392\u9664\uff0c\u4f18\u5316\u5206\u7c7b\u5668\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u68c0\u6d4bOOD\u6837\u672c\u5e76\u5c06\u5176\u5206\u7c7b\u5230\u5783\u573e\u7c7b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8OOD\u6570\u636e\u6216\u540e\u6821\u51c6\uff0c\u4e3aOOD\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.23465", "pdf": "https://arxiv.org/pdf/2505.23465", "abs": "https://arxiv.org/abs/2505.23465", "authors": ["Zi-An Wang", "Shihao Zou", "Shiyao Yu", "Mingyuan Zhang", "Chao Dong"], "title": "Semantics-Aware Human Motion Generation from Audio Instructions", "categories": ["cs.SD", "cs.CV"], "comment": null, "summary": "Recent advances in interactive technologies have highlighted the prominence\nof audio signals for semantic encoding. This paper explores a new task, where\naudio signals are used as conditioning inputs to generate motions that align\nwith the semantics of the audio. Unlike text-based interactions, audio provides\na more natural and intuitive communication method. However, existing methods\ntypically focus on matching motions with music or speech rhythms, which often\nresults in a weak connection between the semantics of the audio and generated\nmotions. We propose an end-to-end framework using a masked generative\ntransformer, enhanced by a memory-retrieval attention module to handle sparse\nand lengthy audio inputs. Additionally, we enrich existing datasets by\nconverting descriptions into conversational style and generating corresponding\naudio with varied speaker identities. Experiments demonstrate the effectiveness\nand efficiency of the proposed framework, demonstrating that audio instructions\ncan convey semantics similar to text while providing more practical and\nuser-friendly interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u97f3\u9891\u4fe1\u53f7\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u751f\u6210\u4e0e\u97f3\u9891\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u63a9\u7801\u751f\u6210\u53d8\u538b\u5668\u548c\u8bb0\u5fc6\u68c0\u7d22\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u97f3\u9891\u4fe1\u53f7\u6bd4\u6587\u672c\u66f4\u81ea\u7136\u76f4\u89c2\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5339\u914d\u97f3\u4e50\u6216\u8bed\u97f3\u8282\u594f\uff0c\u5bfc\u81f4\u97f3\u9891\u8bed\u4e49\u4e0e\u751f\u6210\u8fd0\u52a8\u5173\u8054\u8f83\u5f31\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7ed3\u5408\u63a9\u7801\u751f\u6210\u53d8\u538b\u5668\u548c\u8bb0\u5fc6\u68c0\u7d22\u6ce8\u610f\u529b\u6a21\u5757\u5904\u7406\u7a00\u758f\u957f\u97f3\u9891\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4e30\u5bcc\u6570\u636e\u96c6\u589e\u5f3a\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u9ad8\u6548\u6709\u6548\uff0c\u97f3\u9891\u6307\u4ee4\u80fd\u4f20\u8fbe\u7c7b\u4f3c\u6587\u672c\u7684\u8bed\u4e49\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5b9e\u7528\u548c\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u3002", "conclusion": "\u97f3\u9891\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\u80fd\u6709\u6548\u751f\u6210\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\uff0c\u4e3a\u4ea4\u4e92\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.23606", "pdf": "https://arxiv.org/pdf/2505.23606", "abs": "https://arxiv.org/abs/2505.23606", "authors": ["Qingyu Shi", "Jinbin Bai", "Zhuoran Zhao", "Wenhao Chai", "Kaidong Yu", "Jianzong Wu", "Shuangyong Song", "Yunhai Tong", "Xiangtai Li", "Xuelong Li", "Shuicheng Yan"], "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model", "categories": ["cs.LG", "cs.CV"], "comment": "The code and model are available at\n  https://github.com/M-E-AGI-Lab/Muddit", "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.", "AI": {"tldr": "Muddit\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u79bb\u6563\u6269\u6563Transformer\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u9aa8\u5e72\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u5feb\u901f\u5e76\u884c\u751f\u6210\u6587\u672c\u548c\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u81ea\u56de\u5f52\u7edf\u4e00\u6a21\u578b\u63a8\u7406\u6162\u548c\u975e\u81ea\u56de\u5f52\u7edf\u4e00\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u79bb\u6563\u6269\u6563\u4f5c\u4e3a\u7edf\u4e00\u751f\u6210\u4efb\u52a1\u7684\u9ad8\u6548\u9aa8\u5e72\u3002", "method": "\u63d0\u51faMuddit\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6587\u672c\u5230\u56fe\u50cf\u9aa8\u5e72\u548c\u8f7b\u91cf\u7ea7\u6587\u672c\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5e76\u884c\u751f\u6210\u3002", "result": "Muddit\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u66f4\u5927\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u79bb\u6563\u6269\u6563\u7684\u6f5c\u529b\u3002", "conclusion": "\u79bb\u6563\u6269\u6563\u7ed3\u5408\u5f3a\u89c6\u89c9\u5148\u9a8c\uff0c\u53ef\u4f5c\u4e3a\u7edf\u4e00\u751f\u6210\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u9aa8\u5e72\u3002"}}
{"id": "2505.23612", "pdf": "https://arxiv.org/pdf/2505.23612", "abs": "https://arxiv.org/abs/2505.23612", "authors": ["Jianbo Zhao", "Taiyu Ban", "Xiyang Wang", "Qibin Zhou", "Hangning Zhou", "Zhihao Liu", "Mu Yang", "Lei Liu", "Bin Li"], "title": "Autoregressive Meta-Actions for Unified Controllable Trajectory Generation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Controllable trajectory generation guided by high-level semantic decisions,\ntermed meta-actions, is crucial for autonomous driving systems. A significant\nlimitation of existing frameworks is their reliance on invariant meta-actions\nassigned over fixed future time intervals, causing temporal misalignment with\nthe actual behavior trajectories. This misalignment leads to irrelevant\nassociations between the prescribed meta-actions and the resulting\ntrajectories, disrupting task coherence and limiting model performance. To\naddress this challenge, we introduce Autoregressive Meta-Actions, an approach\nintegrated into autoregressive trajectory generation frameworks that provides a\nunified and precise definition for meta-action-conditioned trajectory\nprediction. Specifically, We decompose traditional long-interval meta-actions\ninto frame-level meta-actions, enabling a sequential interplay between\nautoregressive meta-action prediction and meta-action-conditioned trajectory\ngeneration. This decomposition ensures strict alignment between each trajectory\nsegment and its corresponding meta-action, achieving a consistent and unified\ntask formulation across the entire trajectory span and significantly reducing\ncomplexity. Moreover, we propose a staged pre-training process to decouple the\nlearning of basic motion dynamics from the integration of high-level decision\ncontrol, which offers flexibility, stability, and modularity. Experimental\nresults validate our framework's effectiveness, demonstrating improved\ntrajectory adaptivity and responsiveness to dynamic decision-making scenarios.\nWe provide the video document and dataset, which are available at\nhttps://arma-traj.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5143\u52a8\u4f5c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u5143\u52a8\u4f5c\u4e0e\u8f68\u8ff9\u65f6\u95f4\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u957f\u95f4\u9694\u5143\u52a8\u4f5c\u4e3a\u5e27\u7ea7\u5143\u52a8\u4f5c\uff0c\u5b9e\u73b0\u4e86\u8f68\u8ff9\u751f\u6210\u4e0e\u51b3\u7b56\u7684\u4e25\u683c\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u65f6\u95f4\u95f4\u9694\u7684\u5143\u52a8\u4f5c\uff0c\u5bfc\u81f4\u5143\u52a8\u4f5c\u4e0e\u5b9e\u9645\u8f68\u8ff9\u65f6\u95f4\u4e0d\u5bf9\u9f50\uff0c\u5f71\u54cd\u4efb\u52a1\u8fde\u8d2f\u6027\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u5143\u52a8\u4f5c\u65b9\u6cd5\uff0c\u5c06\u957f\u95f4\u9694\u5143\u52a8\u5206\u89e3\u4e3a\u5e27\u7ea7\u5143\u52a8\u4f5c\uff0c\u7ed3\u5408\u81ea\u56de\u5f52\u8f68\u8ff9\u751f\u6210\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u5206\u9636\u6bb5\u9884\u8bad\u7ec3\u5206\u79bb\u57fa\u7840\u8fd0\u52a8\u52a8\u529b\u5b66\u4e0e\u9ad8\u5c42\u51b3\u7b56\u63a7\u5236\u7684\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8f68\u8ff9\u7684\u81ea\u9002\u5e94\u6027\u548c\u5bf9\u52a8\u6001\u51b3\u7b56\u573a\u666f\u7684\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e25\u683c\u5bf9\u9f50\u5143\u52a8\u4f5c\u4e0e\u8f68\u8ff9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u6027\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2505.23625", "pdf": "https://arxiv.org/pdf/2505.23625", "abs": "https://arxiv.org/abs/2505.23625", "authors": ["Chao Huang", "Yuesheng Ma", "Junxuan Huang", "Susan Liang", "Yunlong Tang", "Jing Bi", "Wenqiang Liu", "Nima Mesgarani", "Chenliang Xu"], "title": "ZeroSep: Separate Anything in Audio with Zero Training", "categories": ["cs.SD", "cs.CV"], "comment": "Project page: https://wikichao.github.io/ZeroSep/", "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faZeroSep\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5f15\u5bfc\u97f3\u9891\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u96f6\u6837\u672c\u97f3\u9891\u6e90\u5206\u79bb\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u6cdb\u5316\u5230\u73b0\u5b9e\u590d\u6742\u58f0\u5b66\u573a\u666f\uff0c\u53d7\u751f\u6210\u57fa\u7840\u6a21\u578b\u542f\u53d1\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u662f\u5426\u53ef\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u6df7\u5408\u97f3\u9891\u53cd\u8f6c\u5230\u6269\u6563\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u6062\u590d\u5355\u4e2a\u6e90\u4fe1\u53f7\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "result": "ZeroSep\u5728\u591a\u4e2a\u5206\u79bb\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6587\u672c\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u53ef\u6210\u529f\u7528\u4e8e\u96f6\u6837\u672c\u97f3\u9891\u6e90\u5206\u79bb\uff0c\u652f\u6301\u5f00\u653e\u96c6\u573a\u666f\u3002"}}
{"id": "2505.23651", "pdf": "https://arxiv.org/pdf/2505.23651", "abs": "https://arxiv.org/abs/2505.23651", "authors": ["Juncheol Shin", "Minsang Seok", "Seonggon Kim", "Eunhyeok Park"], "title": "Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025. Code: https://github.com/ewsn1593/HDRQ", "summary": "Model merging has emerged as a powerful technique for combining task-specific\nweights, achieving superior performance in multi-target domain adaptation.\nHowever, when applied to practical scenarios, such as quantized models, new\nchallenges arise. In practical scenarios, quantization is often applied to\ntarget-specific data, but this process restricts the domain of interest and\nintroduces discretization effects, making model merging highly non-trivial. In\nthis study, we analyze the impact of quantization on model merging through the\nlens of error barriers. Leveraging these insights, we propose a novel\npost-training quantization, HDRQ - Hessian and distant regularizing\nquantization - that is designed to consider model merging for multi-target\ndomain adaptation. Our approach ensures that the quantization process incurs\nminimal deviation from the source pre-trained model while flattening the loss\nsurface to facilitate smooth model merging. To our knowledge, this is the first\nstudy on this challenge, and extensive experiments confirm its effectiveness.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u91cf\u5316\u5bf9\u6a21\u578b\u5408\u5e76\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5HDRQ\uff0c\u4ee5\u652f\u6301\u591a\u76ee\u6807\u57df\u9002\u5e94\u7684\u6a21\u578b\u5408\u5e76\u3002", "motivation": "\u91cf\u5316\u5728\u76ee\u6807\u7279\u5b9a\u6570\u636e\u4e0a\u7684\u5e94\u7528\u9650\u5236\u4e86\u5174\u8da3\u57df\u5e76\u5f15\u5165\u79bb\u6563\u5316\u6548\u5e94\uff0c\u4f7f\u6a21\u578b\u5408\u5e76\u53d8\u5f97\u590d\u6742\u3002", "method": "\u901a\u8fc7\u8bef\u5dee\u5c4f\u969c\u5206\u6790\u91cf\u5316\u5f71\u54cd\uff0c\u63d0\u51faHDRQ\u65b9\u6cd5\uff0c\u7ed3\u5408Hessian\u548c\u8fdc\u8ddd\u79bb\u6b63\u5219\u5316\u91cf\u5316\uff0c\u786e\u4fdd\u91cf\u5316\u8fc7\u7a0b\u6700\u5c0f\u5316\u504f\u79bb\u6e90\u9884\u8bad\u7ec3\u6a21\u578b\u5e76\u5e73\u6ed1\u635f\u5931\u8868\u9762\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eHDRQ\u5728\u591a\u76ee\u6807\u57df\u9002\u5e94\u4e2d\u7684\u6a21\u578b\u5408\u5e76\u6548\u679c\u663e\u8457\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u9488\u5bf9\u91cf\u5316\u6a21\u578b\u5408\u5e76\u7684\u7814\u7a76\uff0cHDRQ\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2505.23692", "pdf": "https://arxiv.org/pdf/2505.23692", "abs": "https://arxiv.org/abs/2505.23692", "authors": ["Jingyun Yang", "Isabella Huang", "Brandon Vu", "Max Bajracharya", "Rika Antonova", "Jeannette Bohg"], "title": "Mobi-$\u03c0$: Mobilizing Your Robot Learning Policy", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project website: https://mobipi.github.io/", "summary": "Learned visuomotor policies are capable of performing increasingly complex\nmanipulation tasks. However, most of these policies are trained on data\ncollected from limited robot positions and camera viewpoints. This leads to\npoor generalization to novel robot positions, which limits the use of these\npolicies on mobile platforms, especially for precise tasks like pressing\nbuttons or turning faucets. In this work, we formulate the policy mobilization\nproblem: find a mobile robot base pose in a novel environment that is in\ndistribution with respect to a manipulation policy trained on a limited set of\ncamera viewpoints. Compared to retraining the policy itself to be more robust\nto unseen robot base pose initializations, policy mobilization decouples\nnavigation from manipulation and thus does not require additional\ndemonstrations. Crucially, this problem formulation complements existing\nefforts to improve manipulation policy robustness to novel viewpoints and\nremains compatible with them. To study policy mobilization, we introduce the\nMobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty\nof mobilizing a given policy, (2) a suite of simulated mobile manipulation\ntasks based on RoboCasa to evaluate policy mobilization, (3) visualization\ntools for analysis, and (4) several baseline methods. We also propose a novel\napproach that bridges navigation and manipulation by optimizing the robot's\nbase pose to align with an in-distribution base pose for a learned policy. Our\napproach utilizes 3D Gaussian Splatting for novel view synthesis, a score\nfunction to evaluate pose suitability, and sampling-based optimization to\nidentify optimal robot poses. We show that our approach outperforms baselines\nin both simulation and real-world environments, demonstrating its effectiveness\nfor policy mobilization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u65b0\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u673a\u5668\u4eba\u57fa\u5ea7\u59ff\u6001\u4f7f\u5176\u7b26\u5408\u7b56\u7565\u8bad\u7ec3\u65f6\u7684\u5206\u5e03\uff0c\u4ece\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u8bad\u7ec3\u65f6\u53d7\u9650\u4e8e\u6709\u9650\u7684\u673a\u5668\u4eba\u4f4d\u7f6e\u548c\u6444\u50cf\u5934\u89c6\u89d2\uff0c\u5bfc\u81f4\u5728\u65b0\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5c24\u5176\u662f\u5bf9\u7cbe\u786e\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86Mobi-\u03c0\u6846\u67b6\uff0c\u5305\u62ec\u91cf\u5316\u7b56\u7565\u6cdb\u5316\u96be\u5ea6\u7684\u6307\u6807\u3001\u6a21\u62df\u4efb\u52a1\u3001\u53ef\u89c6\u5316\u5de5\u5177\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6563\u5c04\u548c\u91c7\u6837\u4f18\u5316\u7684\u57fa\u5ea7\u59ff\u6001\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u653f\u7b56\u52a8\u5458\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u57fa\u5ea7\u59ff\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5728\u65b0\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u4e0e\u73b0\u6709\u63d0\u5347\u7b56\u7565\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u517c\u5bb9\u3002"}}
{"id": "2505.23751", "pdf": "https://arxiv.org/pdf/2505.23751", "abs": "https://arxiv.org/abs/2505.23751", "authors": ["Declan Kutscher", "David M. Chan", "Yutong Bai", "Trevor Darrell", "Ritwik Gupta"], "title": "REOrdering Patches Improves Vision Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Sequence models such as transformers require inputs to be represented as\none-dimensional sequences. In vision, this typically involves flattening images\nusing a fixed row-major (raster-scan) order. While full self-attention is\npermutation-equivariant, modern long-sequence transformers increasingly rely on\narchitectural approximations that break this invariance and introduce\nsensitivity to patch ordering. We show that patch order significantly affects\nmodel performance in such settings, with simple alternatives like column-major\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\npropose REOrder, a two-stage framework for discovering task-optimal patch\norderings. First, we derive an information-theoretic prior by evaluating the\ncompressibility of various patch sequences. Then, we learn a policy over\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\napproach enables efficient learning in a combinatorial permutation space.\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\n3.01% and Functional Map of the World by 13.35%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faREOrder\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u5757\u987a\u5e8f\u63d0\u5347Transformer\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5bf9\u56fe\u50cf\u5757\u987a\u5e8f\u654f\u611f\uff0c\u56fa\u5b9a\u987a\u5e8f\u53ef\u80fd\u5f71\u54cd\u6027\u80fd\uff0c\u9700\u63a2\u7d22\u4efb\u52a1\u6700\u4f18\u987a\u5e8f\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u4fe1\u606f\u8bba\u8bc4\u4f30\u5757\u5e8f\u5217\u538b\u7f29\u6027\uff1b2\uff09\u7528REINFORCE\u4f18\u5316Plackett-Luce\u7b56\u7565\u5b66\u4e60\u6700\u4f18\u987a\u5e8f\u3002", "result": "\u5728ImageNet-1K\u548cFunctional Map of the World\u4e0a\uff0cREOrder\u5206\u522b\u63d0\u5347\u51c6\u786e\u73873.01%\u548c13.35%\u3002", "conclusion": "REOrder\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u56fe\u50cf\u5757\u987a\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5e8f\u5217\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
