{"id": "2504.18586", "pdf": "https://arxiv.org/pdf/2504.18586", "abs": "https://arxiv.org/abs/2504.18586", "authors": ["Leo Thomas Ramos", "Angel D. Sappa"], "title": "A Decade of You Only Look Once (YOLO) for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "This review marks the tenth anniversary of You Only Look Once (YOLO), one of\nthe most influential frameworks in real-time object detection. Over the past\ndecade, YOLO has evolved from a streamlined detector into a diverse family of\narchitectures characterized by efficient design, modular scalability, and\ncross-domain adaptability. The paper presents a technical overview of the main\nversions, highlights key architectural trends, and surveys the principal\napplication areas in which YOLO has been adopted. It also addresses evaluation\npractices, ethical considerations, and potential future directions for the\nframework's continued development. The analysis aims to provide a comprehensive\nand critical perspective on YOLO's trajectory and ongoing transformation."}
{"id": "2504.18589", "pdf": "https://arxiv.org/pdf/2504.18589", "abs": "https://arxiv.org/abs/2504.18589", "authors": ["Zhikai Wang", "Jiashuo Sun", "Wenqi Zhang", "Zhiqiang Hu", "Xin Li", "Fan Wang", "Deli Zhao"], "title": "Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency", "categories": ["cs.CV"], "comment": "Home page: https://alibaba-damo-academy.github.io/VCBench/", "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly enhanced their ability to integrate visual and linguistic\ninformation, achieving near-human proficiency in tasks like object recognition,\ncaptioning, and visual question answering. However, current benchmarks\ntypically focus on knowledge-centric evaluations that assess domain-specific\nexpertise, often neglecting the core ability to reason about fundamental\nmathematical elements and visual concepts. We identify a gap in evaluating\nelementary-level math problems, which rely on explicit visual\ndependencies-requiring models to discern, integrate, and reason across multiple\nimages while incorporating commonsense knowledge, all of which are crucial for\nadvancing toward broader AGI capabilities. To address this gap, we introduce\nVCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with\nexplicit visual dependencies. VCBENCH includes 1,720 problems across six\ncognitive domains, featuring 6,697 images (averaging 3.9 per question) to\nensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,\nrevealing substantial performance disparities, with even the top models unable\nto exceed 50% accuracy. Our findings highlight the ongoing challenges in\nvisual-mathematical integration and suggest avenues for future LVLM\nadvancements."}
{"id": "2504.18666", "pdf": "https://arxiv.org/pdf/2504.18666", "abs": "https://arxiv.org/abs/2504.18666", "authors": ["David Aparco-Cardenas", "Jancarlo F. Gomes", "Alexandre X. Falcão", "Pedro J. de Rezende"], "title": "Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning", "categories": ["cs.CV", "68T07", "I.4.10; I.5.1"], "comment": "Submitted to Journal of the Brazilian Computer Society (JBCS)\n  [https://journals-sol.sbc.org.br]", "summary": "A major challenge that prevents the training of DL models is the limited\navailability of accurately labeled data. This shortcoming is highlighted in\nareas where data annotation becomes a time-consuming and error-prone task. In\nthis regard, SSL tackles this challenge by capitalizing on scarce labeled and\nabundant unlabeled data; however, SoTA methods typically depend on pre-trained\nfeatures and large validation sets to learn effective representations for\nclassification tasks. In addition, the reduced set of labeled data is often\nrandomly sampled, neglecting the selection of more informative samples. Here,\nwe present active-DeepFA, a method that effectively combines CL,\nteacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN\narchitectures for image classification in scenarios of scarcity of labeled and\nabundance of unlabeled data. It integrates DeepFA into a co-training setup that\nimplements two cooperative networks to mitigate confirmation bias from\npseudo-labels. The method starts with a reduced set of labeled samples by\nwarming up the networks with supervised CL. Afterward and at regular epoch\nintervals, label propagation is performed on the 2D projections of the\nnetworks' deep features. Next, the most reliable pseudo-labels are exchanged\nbetween networks in a cross-training fashion, while the most meaningful samples\nare annotated and added into the labeled set. The networks independently\nminimize an objective loss function comprising supervised contrastive,\nsupervised and semi-supervised loss components, enhancing the representations\ntowards image classification. Our approach is evaluated on three challenging\nbiological image datasets using only 5% of labeled samples, improving baselines\nand outperforming six other SoTA methods. In addition, it reduces annotation\neffort by achieving comparable results to those of its counterparts with only\n3% of labeled data."}
{"id": "2504.18684", "pdf": "https://arxiv.org/pdf/2504.18684", "abs": "https://arxiv.org/abs/2504.18684", "authors": ["Nader Zantout", "Haochen Zhang", "Pujith Kachana", "Jinkai Qiu", "Ji Zhang", "Wenshan Wang"], "title": "SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "7 pages, 6 figures, submitted to IROS 2025", "summary": "Interpreting object-referential language and grounding objects in 3D with\nspatial relations and attributes is essential for robots operating alongside\nhumans. However, this task is often challenging due to the diversity of scenes,\nlarge number of fine-grained objects, and complex free-form nature of language\nreferences. Furthermore, in the 3D domain, obtaining large amounts of natural\nlanguage training data is difficult. Thus, it is important for methods to learn\nfrom little data and zero-shot generalize to new environments. To address these\nchallenges, we propose SORT3D, an approach that utilizes rich object attributes\nfrom 2D data and merges a heuristics-based spatial reasoning toolbox with the\nability of large language models (LLMs) to perform sequential reasoning.\nImportantly, our method does not require text-to-3D data for training and can\nbe applied zero-shot to unseen environments. We show that SORT3D achieves\nstate-of-the-art performance on complex view-dependent grounding tasks on two\nbenchmarks. We also implement the pipeline to run real-time on an autonomous\nvehicle and demonstrate that our approach can be used for object-goal\nnavigation on previously unseen real-world environments. All source code for\nthe system pipeline is publicly released at https://github.com/nzantout/SORT3D ."}
{"id": "2504.18560", "pdf": "https://arxiv.org/pdf/2504.18560", "abs": "https://arxiv.org/abs/2504.18560", "authors": ["Alessio Buscemi", "Cédric Lothritz", "Sergio Morales", "Marcos Gomez-Vazquez", "Robert Clarisó", "Jordi Cabot", "German Castignani"], "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited impressive natural language\nprocessing capabilities but often perpetuate social biases inherent in their\ntraining data. To address this, we introduce MultiLingual Augmented Bias\nTesting (MLA-BiTe), a framework that improves prior bias evaluation methods by\nenabling systematic multilingual bias testing. MLA-BiTe leverages automated\ntranslation and paraphrasing techniques to support comprehensive assessments\nacross diverse linguistic settings. In this study, we evaluate the\neffectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six\nlanguages -- including two low-resource languages -- focusing on seven\nsensitive categories of discrimination."}
{"id": "2504.18768", "pdf": "https://arxiv.org/pdf/2504.18768", "abs": "https://arxiv.org/abs/2504.18768", "authors": ["Letian Huang", "Dongwei Ye", "Jialin Dan", "Chengzhi Tao", "Huiwen Liu", "Kun Zhou", "Bo Ren", "Yuanqi Li", "Yanwen Guo", "Jie Guo"], "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": "accepted by SIGGRAPH 2025;\n  https://letianhuang.github.io/transparentgs/", "summary": "The emergence of neural and Gaussian-based radiance field methods has led to\nconsiderable advancements in novel view synthesis and 3D object reconstruction.\nNonetheless, specular reflection and refraction continue to pose significant\nchallenges due to the instability and incorrect overfitting of radiance fields\nto high-frequency light variations. Currently, even 3D Gaussian Splatting\n(3D-GS), as a powerful and efficient tool, falls short in recovering\ntransparent objects with nearby contents due to the existence of apparent\nsecondary ray effects. To address this issue, we propose TransparentGS, a fast\ninverse rendering pipeline for transparent objects based on 3D-GS. The main\ncontributions are three-fold. Firstly, an efficient representation of\ntransparent objects, transparent Gaussian primitives, is designed to enable\nspecular refraction through a deferred refraction strategy. Secondly, we\nleverage Gaussian light field probes (GaussProbe) to encode both ambient light\nand nearby contents in a unified framework. Thirdly, a depth-based iterative\nprobes query (IterQuery) algorithm is proposed to reduce the parallax errors in\nour probe-based framework. Experiments demonstrate the speed and accuracy of\nour approach in recovering transparent objects from complex environments, as\nwell as several applications in computer graphics and vision."}
{"id": "2504.18689", "pdf": "https://arxiv.org/pdf/2504.18689", "abs": "https://arxiv.org/abs/2504.18689", "authors": ["Apoorva Beedu", "Irfan Essa"], "title": "HierSum: A Global and Local Attention Mechanism for Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Video summarization creates an abridged version (i.e., a summary) that\nprovides a quick overview of the video while retaining pertinent information.\nIn this work, we focus on summarizing instructional videos and propose a method\nfor breaking down a video into meaningful segments, each corresponding to\nessential steps in the video. We propose \\textbf{HierSum}, a hierarchical\napproach that integrates fine-grained local cues from subtitles with global\ncontextual information provided by video-level instructions. Our approach\nutilizes the ``most replayed\" statistic as a supervisory signal to identify\ncritical segments, thereby improving the effectiveness of the summary. We\nevaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow\ntest set, and show that HierSum consistently outperforms existing methods in\nkey metrics such as F1-score and rank correlation. We also curate a new\nmulti-modal dataset using WikiHow and EHow videos and associated articles\ncontaining step-by-step instructions. Through extensive ablation studies, we\ndemonstrate that training on this dataset significantly enhances summarization\non the target datasets."}
{"id": "2504.18639", "pdf": "https://arxiv.org/pdf/2504.18639", "abs": "https://arxiv.org/abs/2504.18639", "authors": ["Passant Elchafei", "Mervet Abu-Elkheir"], "title": "Span-Level Hallucination Detection for LLM-Generated Answers", "categories": ["cs.CL"], "comment": null, "summary": "Detecting spans of hallucination in LLM-generated answers is crucial for\nimproving factual consistency. This paper presents a span-level hallucination\ndetection framework for the SemEval-2025 Shared Task, focusing on English and\nArabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose\nthe answer into atomic roles, which are then compared with a retrieved\nreference context obtained via question-based LLM prompting. Using a\nDeBERTa-based textual entailment model, we evaluate each role semantic\nalignment with the retrieved context. The entailment scores are further refined\nthrough token-level confidence measures derived from output logits, and the\ncombined scores are used to detect hallucinated spans. Experiments on the\nMu-SHROOM dataset demonstrate competitive performance. Additionally,\nhallucinated spans have been verified through fact-checking by prompting GPT-4\nand LLaMA. Our findings contribute to improving hallucination detection in\nLLM-generated responses."}
{"id": "2504.18989", "pdf": "https://arxiv.org/pdf/2504.18989", "abs": "https://arxiv.org/abs/2504.18989", "authors": ["Gal Almog", "Ariel Shamir", "Ohad Fried"], "title": "REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to Eurographics 2025. Project page:\n  https://reed-vae.github.io/", "summary": "While latent diffusion models achieve impressive image editing results, their\napplication to iterative editing of the same image is severely restricted. When\ntrying to apply consecutive edit operations using current models, they\naccumulate artifacts and noise due to repeated transitions between pixel and\nlatent spaces. Some methods have attempted to address this limitation by\nperforming the entire edit chain within the latent space, sacrificing\nflexibility by supporting only a limited, predetermined set of diffusion\nediting operations. We present a RE-encode decode (REED) training scheme for\nvariational autoencoders (VAEs), which promotes image quality preservation even\nafter many iterations. Our work enables multi-method iterative image editing:\nusers can perform a variety of iterative edit operations, with each operation\nbuilding on the output of the previous one using both diffusion-based\noperations and conventional editing techniques. We demonstrate the advantage of\nREED-VAE across a range of image editing scenarios, including text-based and\nmask-based editing frameworks. In addition, we show how REED-VAE enhances the\noverall editability of images, increasing the likelihood of successful and\nprecise edit operations. We hope that this work will serve as a benchmark for\nthe newly introduced task of multi-method image editing. Our code and models\nwill be available at https://github.com/galmog/REED-VAE"}
{"id": "2504.18738", "pdf": "https://arxiv.org/pdf/2504.18738", "abs": "https://arxiv.org/abs/2504.18738", "authors": ["Ranjan Sapkota", "Konstantinos I Roumeliotis", "Rahul Harsha Cheppally", "Marco Flores Calero", "Manoj Karkee"], "title": "A Review of 3D Object Detection with Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This review provides a systematic analysis of comprehensive survey of 3D\nobject detection with vision-language models(VLMs) , a rapidly advancing area\nat the intersection of 3D vision and multimodal AI. By examining over 100\nresearch papers, we provide the first systematic analysis dedicated to 3D\nobject detection with vision-language models. We begin by outlining the unique\nchallenges of 3D object detection with vision-language models, emphasizing\ndifferences from 2D detection in spatial reasoning and data complexity.\nTraditional approaches using point clouds and voxel grids are compared to\nmodern vision-language frameworks like CLIP and 3D LLMs, which enable\nopen-vocabulary detection and zero-shot generalization. We review key\narchitectures, pretraining strategies, and prompt engineering methods that\nalign textual and 3D features for effective 3D object detection with\nvision-language models. Visualization examples and evaluation benchmarks are\ndiscussed to illustrate performance and behavior. Finally, we highlight current\nchallenges, such as limited 3D-language datasets and computational demands, and\npropose future research directions to advance 3D object detection with\nvision-language models. >Object Detection, Vision-Language Models, Agents,\nVLMs, LLMs, AI"}
{"id": "2504.18673", "pdf": "https://arxiv.org/pdf/2504.18673", "abs": "https://arxiv.org/abs/2504.18673", "authors": ["Jiayi Li", "Yingfan Zhou", "Pranav Narayanan Venkit", "Halima Binte Islam", "Sneha Arya", "Shomir Wilson", "Sarah Rajtmajer"], "title": "Can Third-parties Read Our Emotions?", "categories": ["cs.CL"], "comment": null, "summary": "Natural Language Processing tasks that aim to infer an author's private\nstates, e.g., emotions and opinions, from their written text, typically rely on\ndatasets annotated by third-party annotators. However, the assumption that\nthird-party annotators can accurately capture authors' private states remains\nlargely unexamined. In this study, we present human subjects experiments on\nemotion recognition tasks that directly compare third-party annotations with\nfirst-party (author-provided) emotion labels. Our findings reveal significant\nlimitations in third-party annotations-whether provided by human annotators or\nlarge language models (LLMs)-in faithfully representing authors' private\nstates. However, LLMs outperform human annotators nearly across the board. We\nfurther explore methods to improve third-party annotation quality. We find that\ndemographic similarity between first-party authors and third-party human\nannotators enhances annotation performance. While incorporating first-party\ndemographic information into prompts leads to a marginal but statistically\nsignificant improvement in LLMs' performance. We introduce a framework for\nevaluating the limitations of third-party annotations and call for refined\nannotation practices to accurately represent and model authors' private states."}
{"id": "2504.19163", "pdf": "https://arxiv.org/pdf/2504.19163", "abs": "https://arxiv.org/abs/2504.19163", "authors": ["Zhimin Fan", "Chen Wang", "Yiming Wang", "Boxuan Li", "Yuxuan Guo", "Ling-Qi Yan", "Yanwen Guo", "Jie Guo"], "title": "Bernstein Bounds for Caustics", "categories": ["cs.GR"], "comment": "ACM Transactions on Graphics (Proceedings of SIGGRAPH 2025)", "summary": "Systematically simulating specular light transport requires an exhaustive\nsearch for primitive tuples containing admissible paths. Given the extreme\ninefficiency of enumerating all combinations, we propose to significantly\nreduce the search domain by sampling such tuples. The challenge is to design\nproper sampling probabilities that keep the noise level controllable. Our key\ninsight is that by bounding the range of irradiance contributed by each\nprimitive tuple at a given position, we can sample a subset of primitive tuples\nwith potentially high contributions. Although low-contribution tuples are\nassigned a negligible probability, the overall variance remains low. Therefore,\nwe derive vertex position and irradiance bounds for each primitive tuple,\nintroducing a bounding property of rational functions on the Bernstein basis.\nWhen formulating position and irradiance expressions into rational functions,\nwe handle non-rational components through remainder variables to maintain\nvalidity. Finally, we carefully design the sampling probabilities by optimizing\nthe upper bound of the variance, expressed only using the position and\nirradiance bound. The proposed primitive sampling is intrinsically unbiased. It\ncan be seamlessly combined with various unbiased and biased root-finding\ntechniques within a local primitive domain. Extensive evaluations show that our\nmethod enables fast and reliable rendering of complex caustic effects."}
{"id": "2504.18746", "pdf": "https://arxiv.org/pdf/2504.18746", "abs": "https://arxiv.org/abs/2504.18746", "authors": ["Brian K. S. Isaac-Medina", "Toby P. Breckon"], "title": "Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures, 2 tables, LatinX in AI CVPR 2025 Workshop", "summary": "Deep neural networks have demonstrated great generalization capabilities for\ntasks whose training and test sets are drawn from the same distribution.\nNevertheless, out-of-distribution (OOD) detection remains a challenging task\nthat has received significant attention in recent years. Specifically, OOD\ndetection refers to the detection of instances that do not belong to the\ntraining distribution, while still having good performance on the\nin-distribution task (e.g., classification or object detection). Recent work\nhas focused on generating synthetic outliers and using them to train an outlier\ndetector, generally achieving improved OOD detection than traditional OOD\nmethods. In this regard, outliers can be generated either in feature or pixel\nspace. Feature space driven methods have shown strong performance on both the\nclassification and object detection tasks, at the expense that the\nvisualization of training outliers remains unknown, making further analysis on\nOOD failure modes challenging. On the other hand, pixel space outlier\ngeneration techniques enabled by diffusion models have been used for image\nclassification using, providing improved OOD detection performance and outlier\nvisualization, although their adaption to the object detection task is as yet\nunexplored. We therefore introduce Dream-Box, a method that provides a link to\nobject-wise outlier generation in the pixel space for OOD detection.\nSpecifically, we use diffusion models to generate object-wise outliers that are\nused to train an object detector for an in-distribution task and OOD detection.\nOur method achieves comparable performance to previous traditional methods\nwhile being the first technique to provide concrete visualization of generated\nOOD objects."}
{"id": "2504.18715", "pdf": "https://arxiv.org/pdf/2504.18715", "abs": "https://arxiv.org/abs/2504.18715", "authors": ["Tuochao Chen", "Qirui Wang", "Runlin He", "Shyam Gollakota"], "title": "Spatial Speech Translation: Translating Across Space With Binaural Hearables", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted by CHI2025", "summary": "Imagine being in a crowded space where people speak a different language and\nhaving hearables that transform the auditory space into your native language,\nwhile preserving the spatial cues for all speakers. We introduce spatial speech\ntranslation, a novel concept for hearables that translate speakers in the\nwearer's environment, while maintaining the direction and unique voice\ncharacteristics of each speaker in the binaural output. To achieve this, we\ntackle several technical challenges spanning blind source separation,\nlocalization, real-time expressive translation, and binaural rendering to\npreserve the speaker directions in the translated audio, while achieving\nreal-time inference on the Apple M2 silicon. Our proof-of-concept evaluation\nwith a prototype binaural headset shows that, unlike existing models, which\nfail in the presence of interference, we achieve a BLEU score of up to 22.01\nwhen translating between languages, despite strong interference from other\nspeakers in the environment. User studies further confirm the system's\neffectiveness in spatially rendering the translated speech in previously unseen\nreal-world reverberant environments. Taking a step back, this work marks the\nfirst step towards integrating spatial perception into speech translation."}
{"id": "2504.19174", "pdf": "https://arxiv.org/pdf/2504.19174", "abs": "https://arxiv.org/abs/2504.19174", "authors": ["Xueqi Ma", "Yilin Liu", "Tianlong Gao", "Qirui Huang", "Hui Huang"], "title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "We introduce CLR-Wire, a novel framework for 3D curve-based wireframe\ngeneration that integrates geometry and topology into a unified Continuous\nLatent Representation. Unlike conventional methods that decouple vertices,\nedges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along\nwith their topological connectivity into a continuous and fixed-length latent\nspace using an attention-driven variational autoencoder (VAE). This unified\napproach facilitates joint learning and generation of both geometry and\ntopology. To generate wireframes, we employ a flow matching model to\nprogressively map Gaussian noise to these latents, which are subsequently\ndecoded into complete 3D wireframes. Our method provides fine-grained modeling\nof complex shapes and irregular topologies, and supports both unconditional\ngeneration and generation conditioned on point cloud or image inputs.\nExperimental results demonstrate that, compared with state-of-the-art\ngenerative approaches, our method achieves substantial improvements in\naccuracy, novelty, and diversity, offering an efficient and comprehensive\nsolution for CAD design, geometric reconstruction, and 3D content creation."}
{"id": "2504.18756", "pdf": "https://arxiv.org/pdf/2504.18756", "abs": "https://arxiv.org/abs/2504.18756", "authors": ["Rezowan Shuvo", "M S Mekala", "Eyad Elyan"], "title": "Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos", "categories": ["cs.CV"], "comment": null, "summary": "Understanding actions within surgical workflows is essential for evaluating\npost-operative outcomes. However, capturing long sequences of actions performed\nin surgical settings poses challenges, as individual surgeons have their unique\napproaches shaped by their expertise, leading to significant variability. To\ntackle this complex problem, we focused on segmentation with precise\nboundaries, a demanding task due to the inherent variability in action\ndurations and the subtle transitions often observed in untrimmed videos. These\ntransitions, marked by ambiguous starting and ending points, complicate the\nsegmentation process. Traditional models, such as MS-TCN, which depend on large\nreceptive fields, frequently face challenges of over-segmentation (resulting in\nfragmented segments) or under-segmentation (merging distinct actions). Both of\nthese issues negatively impact the quality of segmentation. To overcome these\nchallenges, we present the Multi-Stage Boundary-Aware Transformer Network\n(MSBATN) with hierarchical sliding window attention, designed to enhance action\nsegmentation. Our proposed approach incorporates a novel unified loss function\nthat treats action classification and boundary detection as distinct yet\ninterdependent tasks. Unlike traditional binary boundary detection methods, our\nboundary voting mechanism accurately identifies start and end points by\nleveraging contextual information. Extensive experiments using three\nchallenging surgical datasets demonstrate the superior performance of the\nproposed method, achieving state-of-the-art results in F1 scores at thresholds\nof 25% and 50%, while also delivering comparable performance in other metrics."}
{"id": "2504.18718", "pdf": "https://arxiv.org/pdf/2504.18718", "abs": "https://arxiv.org/abs/2504.18718", "authors": ["Lauren Levine", "Junghyun Min", "Amir Zeldes"], "title": "Building UD Cairo for Old English in the Classroom", "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "In this paper we present a sample treebank for Old English based on the UD\nCairo sentences, collected and annotated as part of a classroom curriculum in\nHistorical Linguistics. To collect the data, a sample of 20 sentences\nillustrating a range of syntactic constructions in the world's languages, we\nemploy a combination of LLM prompting and searches in authentic Old English\ndata. For annotation we assigned sentences to multiple students with limited\nprior exposure to UD, whose annotations we compare and adjudicate. Our results\nsuggest that while current LLM outputs in Old English do not reflect authentic\nsyntax, this can be mitigated by post-editing, and that although beginner\nannotators do not possess enough background to complete the task perfectly,\ntaken together they can produce good results and learn from the experience. We\nalso conduct preliminary parsing experiments using Modern English training\ndata, and find that although performance on Old English is poor, parsing on\nannotated features (lemma, hyperlemma, gloss) leads to improved performance."}
{"id": "2504.19189", "pdf": "https://arxiv.org/pdf/2504.19189", "abs": "https://arxiv.org/abs/2504.19189", "authors": ["Lei Zhong", "Chuan Guo", "Yiming Xie", "Jiawei Wang", "Changjian Li"], "title": "Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://zhongleilz.github.io/Sketch2Anim/", "summary": "Storyboarding is widely used for creating 3D animations. Animators use the 2D\nsketches in storyboards as references to craft the desired 3D animations\nthrough a trial-and-error process. The traditional approach requires\nexceptional expertise and is both labor-intensive and time-consuming.\nConsequently, there is a high demand for automated methods that can directly\ntranslate 2D storyboard sketches into 3D animations. This task is\nunder-explored to date and inspired by the significant advancements of motion\ndiffusion models, we propose to address it from the perspective of conditional\nmotion synthesis. We thus present Sketch2Anim, composed of two key modules for\nsketch constraint understanding and motion generation. Specifically, due to the\nlarge domain gap between the 2D sketch and 3D motion, instead of directly\nconditioning on 2D inputs, we design a 3D conditional motion generator that\nsimultaneously leverages 3D keyposes, joint trajectories, and action words, to\nachieve precise and fine-grained motion control. Then, we invent a neural\nmapper dedicated to aligning user-provided 2D sketches with their corresponding\n3D keyposes and trajectories in a shared embedding space, enabling, for the\nfirst time, direct 2D control of motion generation. Our approach successfully\ntransfers storyboards into high-quality 3D motions and inherently supports\ndirect 3D animation editing, thanks to the flexibility of our multi-conditional\nmotion generator. Comprehensive experiments and evaluations, and a user\nperceptual study demonstrate the effectiveness of our approach."}
{"id": "2504.18770", "pdf": "https://arxiv.org/pdf/2504.18770", "abs": "https://arxiv.org/abs/2504.18770", "authors": ["Manuel Weber", "Carly Beneke"], "title": "PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "11 pages, 13 figures, Published at ICLR 2025 - Machine Learning for\n  Remote Sensing (ML4RS) Workshop", "summary": "We propose PyViT-FUSE, a foundation model for earth observation data\nexplicitly designed to handle multi-modal imagery by learning to fuse an\narbitrary number of mixed-resolution input bands into a single representation\nthrough an attention mechanism. The learned patch tokens are further processed\nby a stack of vision transformers with a novel pyramidal structure. We train\nthe model on a globally sampled dataset in a self-supervised manner, leveraging\ncore concepts of the SwAV algorithm. We show the interpretability of the fusion\nmechanism by visualization of the attention scores and the models applicability\nto downstream tasks."}
{"id": "2504.18736", "pdf": "https://arxiv.org/pdf/2504.18736", "abs": "https://arxiv.org/abs/2504.18736", "authors": ["Jianyou Wang", "Weili Cao", "Kaicheng Wang", "Xiaoyue Wang", "Ashish Dalvi", "Gino Prasad", "Qishan Liang", "Hsuan-lin Her", "Ming Wang", "Qin Yang", "Gene W. Yeo", "David E. Neal", "Maxim Khan", "Christopher D. Rosin", "Ramamohan Paturi", "Leon Bergen"], "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers", "categories": ["cs.CL"], "comment": null, "summary": "We study the task of automatically finding evidence relevant to hypotheses in\nbiomedical papers. Finding relevant evidence is an important step when\nresearchers investigate scientific hypotheses. We introduce EvidenceBench to\nmeasure models performance on this task, which is created by a novel pipeline\nthat consists of hypothesis generation and sentence-by-sentence annotation of\nbiomedical papers for relevant evidence, completely guided by and faithfully\nfollowing existing human experts judgment. We demonstrate the pipeline's\nvalidity and accuracy with multiple sets of human-expert annotations. We\nevaluated a diverse set of language models and retrieval systems on the\nbenchmark and found that model performances still fall significantly short of\nthe expert level on this task. To show the scalability of our proposed\npipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated\npapers with hypotheses to facilitate model training and development. Both\ndatasets are available at https://github.com/EvidenceBench/EvidenceBench"}
{"id": "2504.19718", "pdf": "https://arxiv.org/pdf/2504.19718", "abs": "https://arxiv.org/abs/2504.19718", "authors": ["Victoria Yue Chen", "Daoye Wang", "Stephan Garbin", "Sebastian Winberg", "Timo Bolkart", "Thabo Beeler"], "title": "Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation", "categories": ["cs.GR", "cs.CV"], "comment": "4 pages, 4 figures, to be published in Eurographics 2025 as a short\n  paper", "summary": "Face registration deforms a template mesh to closely fit a 3D face scan, the\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\naccessories), because the optimized template-to-scan distance pulls the\ntemplate mesh towards the noisy scan surface. Improving registration quality\nrequires a clean separation of skin and non-skin regions on the scan mesh.\nExisting image-based (2D) or scan-based (3D) segmentation methods however\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\nand they cannot account for scan inaccuracies or scan-image misalignment, while\nscan-based methods suffer from lower spatial resolution compared to images. In\nthis work, we introduce a novel method that accurately separates skin from\nnon-skin geometry on 3D human head scans. For this, our method extracts\nfeatures from multi-view images using a frozen image foundation model and\naggregates these features in 3D. These lifted 2D features are then fused with\n3D geometric features extracted from the scan mesh, to then predict a\nsegmentation mask directly on the scan mesh. We show that our segmentations\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\nmodel generalizes well to real data."}
{"id": "2504.18773", "pdf": "https://arxiv.org/pdf/2504.18773", "abs": "https://arxiv.org/abs/2504.18773", "authors": ["Zhiheng Tu", "Xinjian Huang", "Yong He", "Ruiyang Zhou", "Bo Du", "Weitao Wu"], "title": "Depth as Points: Center Point-based Depth Estimation", "categories": ["cs.CV"], "comment": "Depth Esitimation, Key-points, Virtual Datasets, Autonomous Driving", "summary": "The perception of vehicles and pedestrians in urban scenarios is crucial for\nautonomous driving. This process typically involves complicated data\ncollection, imposes high computational and hardware demands. To address these\nlimitations, we first develop a highly efficient method for generating virtual\ndatasets, which enables the creation of task- and scenario-specific datasets in\na short time. Leveraging this method, we construct the virtual depth estimation\ndataset VirDepth, a large-scale, multi-task autonomous driving dataset.\nSubsequently, we propose CenterDepth, a lightweight architecture for monocular\ndepth estimation that ensures high operational efficiency and exhibits superior\nperformance in depth estimation tasks with highly imbalanced height-scale\ndistributions. CenterDepth integrates global semantic information through the\ninnovative Center FC-CRFs algorithm, aggregates multi-scale features based on\nobject key points, and enables detection-based depth estimation of targets.\nExperiments demonstrate that our proposed method achieves superior performance\nin terms of both computational speed and prediction accuracy."}
{"id": "2504.18762", "pdf": "https://arxiv.org/pdf/2504.18762", "abs": "https://arxiv.org/abs/2504.18762", "authors": ["Ojasw Upadhyay", "Abishek Saravankumar", "Ayman Ismail"], "title": "SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Large Language Models (LLMs) are powerful but often require extensive\nfine-tuning and large datasets for specialized domains like law.\nGeneral-purpose pre-training may not capture legal nuances, and acquiring\nsufficient legal data is challenging. We introduce SynLexLM, a novel approach\nto efficiently pre-train a legal LLM. Our method employs curriculum learning,\nprogressing from simple to complex legal texts and queries, combined with\nsynthetic data augmentation using models like Gemini Pro to address data\nscarcity. We aim to achieve improved performance on legal benchmarks\n(BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned\nversions. Preliminary work involves generating synthetic QA pairs reflecting\nlegal reasoning. This work aims to enhance legal document analysis and research\ntools, potentially democratizing access to advanced legal AI."}
{"id": "2504.19401", "pdf": "https://arxiv.org/pdf/2504.19401", "abs": "https://arxiv.org/abs/2504.19401", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Li Zhang", "Rong Wang"], "title": "Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning", "categories": ["physics.med-ph", "cs.CV", "cs.GR", "eess.IV", "J.3; I.3.8"], "comment": "35 pages, 9 figures", "summary": "Background: Coronary artery bypass grafting (CABG) planning requires advanced\nspatial visualization and consideration of coronary artery depth,\ncalcification, and pericardial adhesions. Objective: To develop and evaluate a\ndynamic cardiovascular holographic visualization tool for preoperative CABG\nplanning. Methods: Using 4D cardiac computed tomography angiography data from\n14 CABG candidates, we developed a semi-automated workflow for time-resolved\nsegmentation of cardiac structures, epicardial adipose tissue (EAT), and\ncoronary arteries with calcium scoring. The workflow incorporated methods for\ncardiac segmentation, coronary calcification quantification, visualization of\ncoronary depth within EAT, and pericardial adhesion assessment through motion\nanalysis. Dynamic cardiovascular holograms were displayed using the Looking\nGlass platform. Thirteen cardiac surgeons evaluated the tool using a Likert\nscale. Additionally, pericardial adhesion scores from holograms of 21 patients\n(including seven undergoing secondary cardiac surgeries) were compared with\nintraoperative findings. Results: Surgeons rated the visualization tool highly\nfor preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based\npericardial adhesion scoring strongly correlated with intraoperative findings\n(r=0.786, P<0.001). Conclusion: This study establishes a visualization\nframework for CABG planning that produces clinically relevant dynamic holograms\nfrom patient-specific data, with clinical feedback confirming its effectiveness\nfor preoperative planning."}
{"id": "2504.18781", "pdf": "https://arxiv.org/pdf/2504.18781", "abs": "https://arxiv.org/abs/2504.18781", "authors": ["Hassan Wasswa", "Timothy Lynar", "Aziida Nanyonga", "Hussein Abbass"], "title": "IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the demonstrated effectiveness of transformer models in NLP, and\nimage and video classification, the available tools for extracting features\nfrom captured IoT network flow packets fail to capture sequential patterns in\naddition to the absence of spatial patterns consequently limiting transformer\nmodel application. This work introduces a novel preprocessing method to adapt\ntransformer models, the vision transformer (ViT) in particular, for IoT botnet\nattack detection using network flow packets. The approach involves feature\nextraction from .pcap files and transforming each instance into a 1-channel 2D\nimage shape, enabling ViT-based classification. Also, the ViT model was\nenhanced to allow use any classifier besides Multilayer Perceptron (MLP) that\nwas deployed in the initial ViT paper. Models including the conventional feed\nforward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM)\ndemonstrated competitive performance in terms of precision, recall, and\nF1-score for multiclass-based attack detection when evaluated on two IoT attack\ndatasets."}
{"id": "2504.18805", "pdf": "https://arxiv.org/pdf/2504.18805", "abs": "https://arxiv.org/abs/2504.18805", "authors": ["Jong Inn Park", "Maanas Taneja", "Qianwen Wang", "Dongyeop Kang"], "title": "Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page: https://minnesotanlp.github.io/scitalk-project-page/", "summary": "Generating engaging, accurate short-form videos from scientific papers is\nchallenging due to content complexity and the gap between expert authors and\nreaders. Existing end-to-end methods often suffer from factual inaccuracies and\nvisual artifacts, limiting their utility for scientific dissemination. To\naddress these issues, we propose SciTalk, a novel multi-LLM agentic framework,\ngrounding videos in various sources, such as text, figures, visual styles, and\navatars. Inspired by content creators' workflows, SciTalk uses specialized\nagents for content summarization, visual scene planning, and text and layout\nediting, and incorporates an iterative feedback mechanism where video agents\nsimulate user roles to give feedback on generated videos from previous\niterations and refine generation prompts. Experimental evaluations show that\nSciTalk outperforms simple prompting methods in generating scientifically\naccurate and engaging content over the refined loop of video generation.\nAlthough preliminary results are still not yet matching human creators'\nquality, our framework provides valuable insights into the challenges and\nbenefits of feedback-driven video generation. Our code, data, and generated\nvideos will be publicly available."}
{"id": "2504.19740", "pdf": "https://arxiv.org/pdf/2504.19740", "abs": "https://arxiv.org/abs/2504.19740", "authors": ["Yonghui Zhai", "Yang Zhang", "Minghao Shang", "Lihua Pang", "Yaxin Ren"], "title": "Graph Fourier Transformer with Structure-Frequency Information", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Graph Transformers (GTs) have shown advantages in numerous graph structure\ntasks but their self-attention mechanism ignores the generalization bias of\ngraphs, with existing methods mainly compensating for this bias from aspects\nlike position encoding, attention bias and relative distance yet still having\nsub-optimal performance and being insufficient by only considering the\nstructural perspective of generalization bias. To address this, this paper\nproposes Grafourierformer, which innovatively combines GT with inductive bias\ncontaining Frequency-Structure information by applying Graph Fourier Transform\nto the Attention Matrix: specifically, eigenvalues from the Graph Laplacian\nmatrix are used to construct an Eigenvalue matrix mask (reflecting node\npositions and structural relationships with neighboring nodes to enable\nconsideration of node range structural characteristics and focus on local graph\ndetails), and inverse Fourier transform is employed to extract node\nhigh-frequency and low-frequency features, calculate low-frequency and\nhigh-frequency energy, and construct a node frequency-energy matrix to filter\nthe eigenvalue matrix mask, allowing attention heads to incorporate both graph\nstructural information and node frequency information optimization, adaptively\ndistinguish global trends from local details, and effectively suppress\nredundant information interference. Extensive experiments on various benchmarks\nshow Grafourierformer consistently outperforms GNN and GT-based models in graph\nclassification and node classification tasks, with ablation experiments further\nvalidating the effectiveness and necessity of the method. Codes are available\nat https://github.com/Arichibald/Grafourierformer.git"}
{"id": "2504.18782", "pdf": "https://arxiv.org/pdf/2504.18782", "abs": "https://arxiv.org/abs/2504.18782", "authors": ["Hang Yu", "Jiahao Wen", "Zhedong Zheng"], "title": "CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Text-based person retrieval aims to identify specific individuals within an\nimage database using textual descriptions. Due to the high cost of annotation\nand privacy protection, researchers resort to synthesized data for the paradigm\nof pretraining and fine-tuning. However, these generated data often exhibit\ndomain biases in both images and textual annotations, which largely compromise\nthe scalability of the pre-trained model. Therefore, we introduce a\ndomain-agnostic pretraining framework based on Cross-modality Adaptive\nMeta-Learning (CAMeL) to enhance the model generalization capability during\npretraining to facilitate the subsequent downstream tasks. In particular, we\ndevelop a series of tasks that reflect the diversity and complexity of\nreal-world scenarios, and introduce a dynamic error sample memory unit to\nmemorize the history for errors encountered within multiple tasks. To further\nensure multi-task adaptation, we also adopt an adaptive dual-speed update\nstrategy, balancing fast adaptation to new tasks and slow weight updates for\nhistorical tasks. Albeit simple, our proposed model not only surpasses existing\nstate-of-the-art methods on real-world benchmarks, including CUHK-PEDES,\nICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in\nhandling biased synthetic images and noisy text annotations. Our code is\navailable at https://github.com/Jahawn-Wen/CAMeL-reID."}
{"id": "2504.18838", "pdf": "https://arxiv.org/pdf/2504.18838", "abs": "https://arxiv.org/abs/2504.18838", "authors": ["Yixin Cao", "Shibo Hong", "Xinze Li", "Jiahao Ying", "Yubo Ma", "Haiyuan Liang", "Yantao Liu", "Zijun Yao", "Xiaozhi Wang", "Dan Huang", "Wenxuan Zhang", "Lifu Huang", "Muhao Chen", "Lei Hou", "Qianru Sun", "Xingjun Ma", "Zuxuan Wu", "Min-Yen Kan", "David Lo", "Qi Zhang", "Heng Ji", "Jing Jiang", "Juanzi Li", "Aixin Sun", "Xuanjing Huang", "Tat-Seng Chua", "Yu-Gang Jiang"], "title": "Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are advancing at an amazing speed and have\nbecome indispensable across academia, industry, and daily applications. To keep\npace with the status quo, this survey probes the core challenges that the rise\nof LLMs poses for evaluation. We identify and analyze two pivotal transitions:\n(i) from task-specific to capability-based evaluation, which reorganizes\nbenchmarks around core competencies such as knowledge, reasoning, instruction\nfollowing, multi-modal understanding, and safety; and (ii) from manual to\nautomated evaluation, encompassing dynamic dataset curation and\n\"LLM-as-a-judge\" scoring.\n  Yet, even with these transitions, a crucial obstacle persists: the evaluation\ngeneralization issue. Bounded test sets cannot scale alongside models whose\nabilities grow seemingly without limit. We will dissect this issue, along with\nthe core challenges of the above two transitions, from the perspectives of\nmethods, datasets, evaluators, and metrics. Due to the fast evolving of this\nfield, we will maintain a living GitHub repository (links are in each section)\nto crowd-source updates and corrections, and warmly invite contributors and\ncollaborators."}
{"id": "2504.18800", "pdf": "https://arxiv.org/pdf/2504.18800", "abs": "https://arxiv.org/abs/2504.18800", "authors": ["Ryo Takizawa", "Satoshi Kodera", "Tempei Kabayama", "Ryo Matsuoka", "Yuta Ando", "Yuto Nakamura", "Haruki Settai", "Norihiko Takeda"], "title": "Video CLIP Model for Multi-View Echocardiography Interpretation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Echocardiography involves recording videos of the heart using ultrasound,\nenabling clinicians to evaluate its condition. Recent advances in large-scale\nvision-language models (VLMs) have garnered attention for automating the\ninterpretation of echocardiographic videos. However, most existing VLMs\nproposed for medical interpretation thus far rely on single-frame (i.e., image)\ninputs. Consequently, these image-based models often exhibit lower diagnostic\naccuracy for conditions identifiable through cardiac motion. Moreover,\nechocardiographic videos are recorded from various views that depend on the\ndirection of ultrasound emission, and certain views are more suitable than\nothers for interpreting specific conditions. Incorporating multiple views could\npotentially yield further improvements in accuracy. In this study, we developed\na video-language model that takes five different views and full video sequences\nas input, training it on pairs of echocardiographic videos and clinical reports\nfrom 60,747 cases. Our experiments demonstrate that this expanded approach\nachieves higher interpretation accuracy than models trained with only\nsingle-view videos or with still images."}
{"id": "2504.18839", "pdf": "https://arxiv.org/pdf/2504.18839", "abs": "https://arxiv.org/abs/2504.18839", "authors": ["Abdellah Ghassel", "Xianzhi Li", "Xiaodan Zhu"], "title": "Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are rapidly changing various domains. However,\ntheir capabilities in handling conversational breakdowns still require an\nin-depth exploration. This paper addresses the challenge of detecting and\nmitigating dialogue breakdowns within LLM-driven conversational systems. While\npowerful models from OpenAI and Anthropic excel in many dialogue tasks, they\ncan still produce incoherent or contradictory responses, commonly referred to\nas breakdowns, which undermine user trust. To tackle this, we propose an\napproach that combines specialized fine-tuning with advanced prompting\nstrategies, including few-shot learning, chain-of-thought reasoning, and\nanalogical prompting. In particular, we fine-tune a small 8B model and\ndemonstrate its robust classification and calibration capabilities in English\nand Japanese dialogue. We also validate its generalization on the BETOLD\ndataset, achieving a 7\\% accuracy improvement over its base model. Furthermore,\nwe introduce a real-time deployment architecture that selectively escalates\nsuspicious responses to more resource-intensive frontier models only when\nbreakdowns are detected, significantly cutting operational expenses and energy\nconsumption. Experimental results show our method surpasses prior\nstate-of-the-art specialized classifiers while also narrowing performance gaps\nbetween smaller open-source models and large proprietary ones. Our approach\noffers a scalable solution for robust conversational AI in high-impact domains\nby combining efficiency, interpretability, and reliability."}
{"id": "2504.18810", "pdf": "https://arxiv.org/pdf/2504.18810", "abs": "https://arxiv.org/abs/2504.18810", "authors": ["Yifan Xie", "Fei Ma", "Yi Bin", "Ying He", "Fei Yu"], "title": "Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 7 figures", "summary": "Talking face video generation with arbitrary speech audio is a significant\nchallenge within the realm of digital human technology. The previous studies\nhave emphasized the significance of audio-lip synchronization and visual\nquality. Currently, limited attention has been given to the learning of visual\nuncertainty, which creates several issues in existing systems, including\ninconsistent visual quality and unreliable performance across different input\nconditions. To address the problem, we propose a Joint Uncertainty Learning\nNetwork (JULNet) for high-quality talking face video generation, which\nincorporates a representation of uncertainty that is directly related to visual\nerror. Specifically, we first design an uncertainty module to individually\npredict the error map and uncertainty map after obtaining the generated image.\nThe error map represents the difference between the generated image and the\nground truth image, while the uncertainty map is used to predict the\nprobability of incorrect estimates. Furthermore, to match the uncertainty\ndistribution with the error distribution through a KL divergence term, we\nintroduce a histogram technique to approximate the distributions. By jointly\noptimizing error and uncertainty, the performance and robustness of our model\ncan be enhanced. Extensive experiments demonstrate that our method achieves\nsuperior high-fidelity and audio-lip synchronization in talking face video\ngeneration compared to previous methods."}
{"id": "2504.18851", "pdf": "https://arxiv.org/pdf/2504.18851", "abs": "https://arxiv.org/abs/2504.18851", "authors": ["Hayley Ross", "Ameya Sunil Mahabaleshwarkar", "Yoshi Suhara"], "title": "When2Call: When (not) to Call Tools", "categories": ["cs.CL"], "comment": "NAACL 2025", "summary": "Leveraging external tools is a key feature for modern Language Models (LMs)\nto expand their capabilities and integrate them into existing systems. However,\nexisting benchmarks primarily focus on the accuracy of tool calling -- whether\nthe correct tool is called with the correct parameters -- and less on\nevaluating when LMs should (not) call tools. We develop a new benchmark,\nWhen2Call, which evaluates tool-calling decision-making: when to generate a\ntool call, when to ask follow-up questions and when to admit the question can't\nbe answered with the tools provided. We find that state-of-the-art tool-calling\nLMs show significant room for improvement on When2Call, indicating the\nimportance of this benchmark. We also develop a training set for When2Call and\nleverage the multiple-choice nature of the benchmark to develop a preference\noptimization training regime, which shows considerably more improvement than\ntraditional fine-tuning. We release the benchmark and training data as well as\nevaluation scripts at https://github.com/NVIDIA/When2Call."}
{"id": "2504.18856", "pdf": "https://arxiv.org/pdf/2504.18856", "abs": "https://arxiv.org/abs/2504.18856", "authors": ["Shahad Albastaki", "Anabia Sohail", "Iyyakutti Iyappan Ganapathi", "Basit Alawode", "Asim Khan", "Sajid Javed", "Naoufel Werghi", "Mohammed Bennamoun", "Arif Mahmood"], "title": "Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation", "categories": ["cs.CV"], "comment": null, "summary": "In Computational Pathology (CPath), the introduction of Vision-Language\nModels (VLMs) has opened new avenues for research, focusing primarily on\naligning image-text pairs at a single magnification level. However, this\napproach might not be sufficient for tasks like cancer subtype classification,\ntissue phenotyping, and survival analysis due to the limited level of detail\nthat a single-resolution image can provide. Addressing this, we propose a novel\nmulti-resolution paradigm leveraging Whole Slide Images (WSIs) to extract\nhistology patches at multiple resolutions and generate corresponding textual\ndescriptions through advanced CPath VLM. We introduce visual-textual alignment\nat multiple resolutions as well as cross-resolution alignment to establish more\neffective text-guided visual representations. Cross-resolution alignment using\na multimodal encoder enhances the model's ability to capture context from\nmultiple resolutions in histology images. Our model aims to capture a broader\nrange of information, supported by novel loss functions, enriches feature\nrepresentation, improves discriminative ability, and enhances generalization\nacross different resolutions. Pre-trained on a comprehensive TCGA dataset with\n34 million image-language pairs at various resolutions, our fine-tuned model\noutperforms state-of-the-art (SOTA) counterparts across multiple datasets and\ntasks, demonstrating its effectiveness in CPath. The code is available on\nGitHub at: https://github.com/BasitAlawode/MR-PLIP"}
{"id": "2504.18857", "pdf": "https://arxiv.org/pdf/2504.18857", "abs": "https://arxiv.org/abs/2504.18857", "authors": ["Yi Lu", "Wanxu Zhao", "Xin Zhou", "Chenxin An", "Chenglong Wang", "Shuo Li", "Yuming Yang", "Jun Zhao", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often struggle to process and generate coherent\ncontext when the number of input tokens exceeds the pre-trained length. Recent\nadvancements in long-context extension have significantly expanded the context\nwindow of LLMs but require expensive overhead to train the large-scale models\nwith longer context. In this work, we propose Dimension-Wise Positional\nEmbeddings Manipulation (DPE), a training-free framework to extrapolate the\ncontext window of LLMs by diving into RoPE's different hidden dimensions.\nInstead of manipulating all dimensions equally, DPE detects the effective\nlength for every dimension and finds the key dimensions for context extension.\nWe reuse the original position indices with their embeddings from the\npre-trained model and manipulate the key dimensions' position indices to their\nmost effective lengths. In this way, DPE adjusts the pre-trained models with\nminimal modifications while ensuring that each dimension reaches its optimal\nstate for extrapolation. DPE significantly surpasses well-known baselines such\nas YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of\n128k tokens without continual training and integrates seamlessly with Flash\nAttention 2. In addition to its impressive extrapolation capability, DPE also\ndramatically improves the models' performance within training length, such as\nLlama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When\ncompared with commercial models, Llama 3.1 70B with DPE even achieves better\nperformance than GPT-4-128K."}
{"id": "2504.18864", "pdf": "https://arxiv.org/pdf/2504.18864", "abs": "https://arxiv.org/abs/2504.18864", "authors": ["Yunzhong Zhang", "Bo Xiong", "You Zhou", "Changqing Su", "Zhen Cheng", "Zhaofei Yu", "Xun Cao", "Tiejun Huang"], "title": "Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras", "categories": ["cs.CV"], "comment": null, "summary": "The need for accurate and non-intrusive flow measurement methods has led to\nthe widespread adoption of Particle Image Velocimetry (PIV), a powerful\ndiagnostic tool in fluid motion estimation. This study investigates the\ntremendous potential of spike cameras (a type of ultra-high-speed,\nhigh-dynamic-range camera) in PIV. We propose a deep learning framework, Spike\nImaging Velocimetry (SIV), designed specifically for highly turbulent and\nintricate flow fields. To aggregate motion features from the spike stream while\nminimizing information loss, we incorporate a Detail-Preserving Hierarchical\nTransform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to\nextract contextual features from highly complex fluid flows. Furthermore, we\npresent a spike-based PIV dataset, Particle Scenes with Spike and Displacement\n(PSSD), which provides labeled data for three challenging fluid dynamics\nscenarios. Our proposed method achieves superior performance compared to\nexisting baseline methods on PSSD. The datasets and our implementation of SIV\nare open-sourced in the supplementary materials."}
{"id": "2504.18872", "pdf": "https://arxiv.org/pdf/2504.18872", "abs": "https://arxiv.org/abs/2504.18872", "authors": ["Alexandra Abbas", "Nora Petrova", "Helios Ael Lyons", "Natalia Perez-Campanero"], "title": "Latent Adversarial Training Improves the Representation of Refusal", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent work has shown that language models' refusal behavior is primarily\nencoded in a single direction in their latent space, making it vulnerable to\ntargeted attacks. Although Latent Adversarial Training (LAT) attempts to\nimprove robustness by introducing noise during training, a key question\nremains: How does this noise-based training affect the underlying\nrepresentation of refusal behavior? Understanding this encoding is crucial for\nevaluating LAT's effectiveness and limitations, just as the discovery of linear\nrefusal directions revealed vulnerabilities in traditional supervised safety\nfine-tuning (SSFT).\n  Through the analysis of Llama 2 7B, we examine how LAT reorganizes the\nrefusal behavior in the model's latent space compared to SSFT and embedding\nspace adversarial training (AT). By computing activation differences between\nharmful and harmless instruction pairs and applying Singular Value\nDecomposition (SVD), we find that LAT significantly alters the refusal\nrepresentation, concentrating it in the first two SVD components which explain\napproximately 75 percent of the activation differences variance - significantly\nhigher than in reference models. This concentrated representation leads to more\neffective and transferable refusal vectors for ablation attacks: LAT models\nshow improved robustness when attacked with vectors from reference models but\nbecome more vulnerable to self-generated vectors compared to SSFT and AT. Our\nfindings suggest that LAT's training perturbations enable a more comprehensive\nrepresentation of refusal behavior, highlighting both its potential strengths\nand vulnerabilities for improving model safety."}
{"id": "2504.18866", "pdf": "https://arxiv.org/pdf/2504.18866", "abs": "https://arxiv.org/abs/2504.18866", "authors": ["Jiaxu Leng", "Zhanjie Wu", "Mingpi Tan", "Mengjingcheng Mo", "Jiankang Zheng", "Qingqing Li", "Ji Gan", "Xinbo Gao"], "title": "PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Existing weakly supervised video violence detection (VVD) methods primarily\nrely on Euclidean representation learning, which often struggles to distinguish\nvisually similar yet semantically distinct events due to limited hierarchical\nmodeling and insufficient ambiguous training samples. To address this\nchallenge, we propose PiercingEye, a novel dual-space learning framework that\nsynergizes Euclidean and hyperbolic geometries to enhance discriminative\nfeature representation. Specifically, PiercingEye introduces a layer-sensitive\nhyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to\nprogressively model event hierarchies, and a cross-space attention mechanism to\nfacilitate complementary feature interactions between Euclidean and hyperbolic\nspaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage\nlarge language models to generate logic-guided ambiguous event descriptions,\nenabling explicit supervision through a hyperbolic vision-language contrastive\nloss that prioritizes high-confusion samples via dynamic similarity-aware\nweighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks\ndemonstrate that PiercingEye achieves state-of-the-art performance, with\nparticularly strong results on a newly curated ambiguous event subset,\nvalidating its superior capability in fine-grained violence detection."}
{"id": "2504.18884", "pdf": "https://arxiv.org/pdf/2504.18884", "abs": "https://arxiv.org/abs/2504.18884", "authors": ["Junichiro Niimi"], "title": "A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "This manuscript has been accepted for the 30th International\n  Conference on Natural Language & Information Systems (NLDB 2025). The final\n  version will appear in the Springer LNCS proceedings. arXiv admin note: text\n  overlap with arXiv:2407.13069", "summary": "With the advance of large language models (LLMs), LLMs have been utilized for\nthe various tasks. However, the issues of variability and reproducibility of\nresults from each trial of LLMs have been largely overlooked in existing\nliterature while actual human annotation uses majority voting to resolve\ndisagreements among annotators. Therefore, this study introduces the\nstraightforward ensemble strategy to a sentiment analysis using LLMs. As the\nresults, we demonstrate that the ensemble of multiple inference using\nmedium-sized LLMs produces more robust and accurate results than using a large\nmodel with a single attempt with reducing RMSE by 18.6%."}
{"id": "2504.18870", "pdf": "https://arxiv.org/pdf/2504.18870", "abs": "https://arxiv.org/abs/2504.18870", "authors": ["Guodong Sun", "Mingjing Li", "Dingjie Liu", "Mingxuan Liu", "Bo Wu", "Yang Zhang"], "title": "WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "To appear in IEEE TIM", "summary": "As an essential component of logistics automation, the automated loading\nsystem is becoming a critical technology for enhancing operational efficiency\nand safety. Precise automatic positioning of the truck compartment, which\nserves as the loading area, is the primary step in automated loading. However,\nexisting methods have difficulty adapting to truck compartments of various\nsizes, do not establish a unified coordinate system for LiDAR and mobile\nmanipulators, and often exhibit reliability issues in cluttered environments.\nTo address these limitations, our study focuses on achieving precise automatic\npositioning of key points in large, medium, and small fence-style truck\ncompartments in cluttered scenarios. We propose an innovative wide\nfield-of-view 3-D LiDAR vehicle compartment automatic localization system. For\nvehicles of various sizes, this system leverages the LiDAR to generate\nhigh-density point clouds within an extensive field-of-view range. By\nincorporating parking area constraints, our vehicle point cloud segmentation\nmethod more effectively segments vehicle point clouds within the scene. Our\ncompartment key point positioning algorithm utilizes the geometric features of\nthe compartments to accurately locate the corner points, providing stackable\nspatial regions. Extensive experiments on our collected data and public\ndatasets demonstrate that this system offers reliable positioning accuracy and\nreduced computational resource consumption, leading to its application and\npromotion in relevant fields."}
{"id": "2504.18938", "pdf": "https://arxiv.org/pdf/2504.18938", "abs": "https://arxiv.org/abs/2504.18938", "authors": ["Junhong Liang", "Yu Zhou"], "title": "MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens\nin sentences. While Large Language Models (LLMs) have shown remarkable success\nin identifying and rectifying potential errors, they often struggle with\nmaintaining consistent output lengths and adapting to domain-specific\ncorrections. Furthermore, existing CSC task impose rigid constraints requiring\ninput and output lengths to be identical, limiting their applicability. In this\nwork, we extend traditional CSC to variable-length correction scenarios,\nincluding Chinese Splitting Error Correction (CSEC) and ASR N-best Error\nCorrection. To address domain adaptation and length consistency, we propose\nMTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection\nmechanism. Our approach constructs a retrieval database from domain-specific\ntraining data and dictionaries, fine-tuning retrievers to optimize performance\nfor error-containing inputs. Additionally, we introduce a multi-source\ncombination strategy with iterative length reflection to ensure output length\nfidelity. Experiments across diverse domain datasets demonstrate that our\nmethod significantly outperforms current approaches in correction quality,\nparticularly in handling domain-specific and variable-length error correction\ntasks."}
{"id": "2504.18886", "pdf": "https://arxiv.org/pdf/2504.18886", "abs": "https://arxiv.org/abs/2504.18886", "authors": ["Simone Maurizio La Cava", "Roberto Casula", "Sara Concas", "Giulia Orrù", "Ruben Tolosana", "Martin Drahansky", "Julian Fierrez", "Gian Luca Marcialis"], "title": "Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to the limits and characteristics of the different application\nscenarios. In this study, we investigate how multiple state-of-the-art 3DFR\nalgorithms can be used to generate a better representation of subjects, with\nthe final goal of improving the performance of face recognition systems in\nchallenging uncontrolled scenarios. We also explore how different parametric\nand non-parametric score-level fusion methods can exploit the unique strengths\nof multiple 3DFR algorithms to enhance biometric recognition robustness. With\nthis goal, we propose a comprehensive analysis of several face recognition\nsystems across diverse conditions, such as varying distances and camera setups,\nintra-dataset and cross-dataset, to assess the robustness of the proposed\nensemble method. The results demonstrate that the distinct information provided\nby different 3DFR algorithms can alleviate the problem of generalizing over\nmultiple application scenarios. In addition, the present study highlights the\npotential of advanced fusion strategies to enhance the reliability of\n3DFR-based face recognition systems, providing the research community with key\ninsights to exploit them in real-world applications effectively. Although the\nexperiments are carried out in a specific face verification setup, our proposed\nfusion-based 3DFR methods may be applied to other tasks around face biometrics\nthat are not strictly related to identity recognition."}
{"id": "2504.18942", "pdf": "https://arxiv.org/pdf/2504.18942", "abs": "https://arxiv.org/abs/2504.18942", "authors": ["Debarati Das", "Khanh Chi Le", "Ritik Sachin Parkar", "Karin De Langis", "Brendan Madson", "Chad M. Berryman", "Robin M. Willis", "Daniel H. Moses", "Brett McDonnell", "Daniel Schwarcz", "Dongyeop Kang"], "title": "LawFlow : Collecting and Simulating Lawyers' Thought Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "submitted to COLM 2025", "summary": "Legal practitioners, particularly those early in their careers, face complex,\nhigh-stakes tasks that require adaptive, context-sensitive reasoning. While AI\nholds promise in supporting legal work, current datasets and models are\nnarrowly focused on isolated subtasks and fail to capture the end-to-end\ndecision-making required in real-world practice. To address this gap, we\nintroduce LawFlow, a dataset of complete end-to-end legal workflows collected\nfrom trained law students, grounded in real-world business entity formation\nscenarios. Unlike prior datasets focused on input-output pairs or linear chains\nof thought, LawFlow captures dynamic, modular, and iterative reasoning\nprocesses that reflect the ambiguity, revision, and client-adaptive strategies\nof legal practice. Using LawFlow, we compare human and LLM-generated workflows,\nrevealing systematic differences in structure, reasoning flexibility, and plan\nexecution. Human workflows tend to be modular and adaptive, while LLM workflows\nare more sequential, exhaustive, and less sensitive to downstream implications.\nOur findings also suggest that legal professionals prefer AI to carry out\nsupportive roles, such as brainstorming, identifying blind spots, and surfacing\nalternatives, rather than executing complex workflows end-to-end. Building on\nthese findings, we propose a set of design suggestions, rooted in empirical\nobservations, that align AI assistance with human goals of clarity,\ncompleteness, creativity, and efficiency, through hybrid planning, adaptive\nexecution, and decision-point support. Our results highlight both the current\nlimitations of LLMs in supporting complex legal workflows and opportunities for\ndeveloping more collaborative, reasoning-aware legal AI systems. All data and\ncode are available on our project page\n(https://minnesotanlp.github.io/LawFlow-website/)."}
{"id": "2504.18906", "pdf": "https://arxiv.org/pdf/2504.18906", "abs": "https://arxiv.org/abs/2504.18906", "authors": ["Yufeng Wu", "Xin Liao", "Baowei Wang", "Han Fang", "Xiaoshuai Wu", "Guiling Wang"], "title": "Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Unauthorized screen capturing and dissemination pose severe security threats\nsuch as data leakage and information theft. Several studies propose robust\nwatermarking methods to track the copyright of Screen-Camera (SC) images,\nfacilitating post-hoc certification against infringement. These techniques\ntypically employ heuristic mathematical modeling or supervised neural network\nfitting as the noise layer, to enhance watermarking robustness against SC.\nHowever, both strategies cannot fundamentally achieve an effective\napproximation of SC noise. Mathematical simulation suffers from biased\napproximations due to the incomplete decomposition of the noise and the absence\nof interdependence among the noise components. Supervised networks require\npaired data to train the noise-fitting model, and it is difficult for the model\nto learn all the features of the noise. To address the above issues, we propose\nSimulation-to-Real (S2R). Specifically, an unsupervised noise layer employs\nunpaired data to learn the discrepancy between the modeling simulated noise\ndistribution and the real-world SC noise distribution, rather than directly\nlearning the mapping from sharp images to real-world images. Learning this\ntransformation from simulation to reality is inherently simpler, as it\nprimarily involves bridging the gap in noise distributions, instead of the\ncomplex task of reconstructing fine-grained image details. Extensive\nexperimental results validate the efficacy of the proposed method,\ndemonstrating superior watermark robustness and generalization compared to\nthose of state-of-the-art methods."}
{"id": "2504.18992", "pdf": "https://arxiv.org/pdf/2504.18992", "abs": "https://arxiv.org/abs/2504.18992", "authors": ["Sanwoo Lee", "Jiahao Liu", "Qifan Wang", "Jingang Wang", "Xunliang Cai", "Yunfang Wu"], "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The fine-tuning of pre-trained language models has resulted in the widespread\navailability of task-specific models. Model merging offers an efficient way to\ncreate multi-task models by combining these fine-tuned models at the parameter\nlevel, without the need for training data or joint training on multiple\ndatasets. Existing merging approaches typically involve scaling the parameters\nmodel-wise or integrating parameter importance parameter-wise. Both approaches\nexhibit their own weaknesses, leading to a notable performance gap compared to\nmulti-task fine-tuning. In this paper, we unify these seemingly distinct\nstrategies into a more general merging framework, and introduce Dynamic\nFisher-weighted Merging (DF-Merge). Specifically, candidate models are\nassociated with a set of coefficients that linearly scale their fine-tuned\nparameters. Bayesian optimization is applied to dynamically adjust these\ncoefficients, aiming to maximize overall performance on validation sets. Each\niteration of this process integrates parameter importance based on the Fisher\ninformation conditioned by the coefficients. Experimental results show that\nDF-Merge outperforms strong baselines across models of different sizes and a\nvariety of tasks. Our analysis shows that the effectiveness of DF-Merge arises\nfrom the unified view of merging and that near-optimal performance is\nachievable in a few iterations, even with minimal validation data."}
{"id": "2504.18910", "pdf": "https://arxiv.org/pdf/2504.18910", "abs": "https://arxiv.org/abs/2504.18910", "authors": ["Ali Nazari", "Mohsen Ebrahimi Moghaddam", "Omidreza Borzoei"], "title": "Kinship Verification through a Forest Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early methods used face representations in kinship verification, which are\nless accurate than joint representations of parents' and children's facial\nimages learned from scratch. We propose an approach featuring graph neural\nnetwork concepts to utilize face representations and have comparable results to\njoint representation algorithms. Moreover, we designed the structure of the\nclassification module and introduced a new combination of losses to engage the\ncenter loss gradually in training our network. Additionally, we conducted\nexperiments on KinFaceW-I and II, demonstrating the effectiveness of our\napproach. We achieved the best result on KinFaceW-II, an average improvement of\nnearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The\ncode is available at https://github.com/ali-nazari/Kinship-Verification"}
{"id": "2504.19019", "pdf": "https://arxiv.org/pdf/2504.19019", "abs": "https://arxiv.org/abs/2504.19019", "authors": ["Mohammad Akbar-Tajari", "Mohammad Taher Pilehvar", "Mohammad Mahmoody"], "title": "Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "19 pages, 1 figure, 6 tables", "summary": "The challenge of ensuring Large Language Models (LLMs) align with societal\nstandards is of increasing interest, as these models are still prone to\nadversarial jailbreaks that bypass their safety mechanisms. Identifying these\nvulnerabilities is crucial for enhancing the robustness of LLMs against such\nexploits. We propose Graph of ATtacks (GoAT), a method for generating\nadversarial prompts to test the robustness of LLM alignment using the Graph of\nThoughts framework [Besta et al., 2024]. GoAT excels at generating highly\neffective jailbreak prompts with fewer queries to the victim model than\nstate-of-the-art attacks, achieving up to five times better jailbreak success\nrate against robust models like Llama. Notably, GoAT creates high-quality,\nhuman-readable prompts without requiring access to the targeted model's\nparameters, making it a black-box attack. Unlike approaches constrained by\ntree-based reasoning, GoAT's reasoning is based on a more intricate graph\nstructure. By making simultaneous attack paths aware of each other's progress,\nthis dynamic framework allows a deeper integration and refinement of reasoning\npaths, significantly enhancing the collaborative exploration of adversarial\nvulnerabilities in LLMs. At a technical level, GoAT starts with a graph\nstructure and iteratively refines it by combining and improving thoughts,\nenabling synergy between different thought paths. The code for our\nimplementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks."}
{"id": "2504.18959", "pdf": "https://arxiv.org/pdf/2504.18959", "abs": "https://arxiv.org/abs/2504.18959", "authors": ["Kamirul Kamirul", "Odysseas Pappas", "Alin Achim"], "title": "R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals", "categories": ["cs.CV"], "comment": "Submitted to IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "summary": "We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in\nSynthetic Aperture Radar (SAR) images that leverages sparse learnable proposals\nenriched with background contextual information, termed background-aware\nproposals (BAPs). The adoption of sparse proposals streamlines the pipeline by\neliminating the need for proposal generators and post-processing for\noverlapping predictions. The proposed BAPs enrich object representation by\nintegrating ship and background features, allowing the model to learn their\ncontextual relationships for more accurate distinction of ships in complex\nenvironments. To complement BAPs, we propose Dual-Context Pooling (DCP), a\nnovel strategy that jointly extracts ship and background features in a single\nunified operation. This unified design improves efficiency by eliminating\nredundant computation inherent in separate pooling. Moreover, by ensuring that\nship and background features are pooled from the same feature map level, DCP\nprovides aligned features that improve contextual relationship learning.\nFinally, as a core component of contextual relationship learning in R-Sparse\nR-CNN, we design a dedicated transformer-based Interaction Module. This module\ninteracts pooled ship and background features with corresponding proposal\nfeatures and models their relationships. Experimental results show that\nR-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art\nmodels by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore\ndatasets, respectively. These results demonstrate the effectiveness and\ncompetitiveness of R-Sparse R-CNN as a robust framework for oriented ship\ndetection in SAR imagery. The code is available at:\nwww.github.com/ka-mirul/R-Sparse-R-CNN."}
{"id": "2504.19021", "pdf": "https://arxiv.org/pdf/2504.19021", "abs": "https://arxiv.org/abs/2504.19021", "authors": ["Zhyar Rzgar K Rostam", "Gábor Kertész"], "title": "Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure, 8 tables", "summary": "Efficient text classification is essential for handling the increasing volume\nof academic publications. This study explores the use of pre-trained language\nmodels (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on\nthe Web of Science (WoS-46985) dataset for scientific text classification. To\nenhance performance, we augment the dataset by executing seven targeted queries\nin the WoS database, retrieving 1,000 articles per category aligned with\nWoS-46985's main classes. PLMs predict labels for this unlabeled data, and a\nhard-voting strategy combines predictions for improved accuracy and confidence.\nFine-tuning on the expanded dataset with dynamic learning rates and early\nstopping significantly boosts classification accuracy, especially in\nspecialized domains. Domain-specific models like SciBERT and BioBERT\nconsistently outperform general-purpose models such as BERT. These findings\nunderscore the efficacy of dataset augmentation, inference-driven label\nprediction, hard-voting, and fine-tuning techniques in creating robust and\nscalable solutions for automated academic text classification."}
{"id": "2504.18977", "pdf": "https://arxiv.org/pdf/2504.18977", "abs": "https://arxiv.org/abs/2504.18977", "authors": ["Ihsan Ullah", "Alfredo Petrosino"], "title": "3DPyranet Features Fusion for Spatio-temporal Feature Learning", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural network (CNN) slides a kernel over the whole image to\nproduce an output map. This kernel scheme reduces the number of parameters with\nrespect to a fully connected neural network (NN). While CNN has proven to be an\neffective model in recognition of handwritten characters and traffic signal\nsign boards, etc. recently, its deep variants have proven to be effective in\nsimilar as well as more challenging applications like object, scene and action\nrecognition. Deep CNN add more layers and kernels to the classical CNN,\nincreasing the number of parameters, and partly reducing the main advantage of\nCNN which is less parameters. In this paper, a 3D pyramidal neural network\ncalled 3DPyraNet and a discriminative approach for spatio-temporal feature\nlearning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a\nnew weighting scheme which learns features from both spatial and temporal\ndimensions analyzing multiple adjacent frames and keeping a biological\nplausible structure. It keeps the spatial topology of the input image and\npresents fewer parameters and lower computational and memory costs compared to\nboth fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features\nmaps of the highest layer of the learned network, fuse them in a single vector,\nand provide it as input in such a way to a linear-SVM classifier that enhances\nthe recognition of human actions and dynamic scenes from the videos.\nEncouraging results are reported with 3DPyraNet in real-world environments,\nespecially in the presence of camera induced motion. Further, 3DPyraNet-F\nclearly outperforms the state-of-the-art on three benchmark datasets and shows\ncomparable result for the fourth."}
{"id": "2504.19024", "pdf": "https://arxiv.org/pdf/2504.19024", "abs": "https://arxiv.org/abs/2504.19024", "authors": ["Jiabin Fan", "Guoqing Luo", "Michael Bowling", "Lili Mou"], "title": "KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "We propose a novel k-step return estimation method (called KETCHUP) for\nReinforcement Learning(RL)-based knowledge distillation (KD) in text generation\ntasks. Our idea is to induce a K-step return by using the Bellman Optimality\nEquation for multiple steps. Theoretical analysis shows that this K-step\nformulation reduces the variance of the gradient estimates, thus leading to\nimproved RL optimization especially when the student model size is large.\nEmpirical evaluation on three text generation tasks demonstrates that our\napproach yields superior performance in both standard task metrics and large\nlanguage model (LLM)-based evaluation. These results suggest that our K-step\nreturn induction offers a promising direction for enhancing RL-based KD in LLM\nresearch."}
{"id": "2504.18983", "pdf": "https://arxiv.org/pdf/2504.18983", "abs": "https://arxiv.org/abs/2504.18983", "authors": ["Xuyin Qi", "Zeyu Zhang", "Canxuan Gang", "Hao Zhang", "Lei Zhang", "Zhiwei Zhang", "Yang Zhao"], "title": "MediAug: Exploring Visual Augmentation in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is essential in medical imaging for improving\nclassification accuracy, lesion detection, and organ segmentation under limited\ndata conditions. However, two significant challenges remain. First, a\npronounced domain gap between natural photographs and medical images can\ndistort critical disease features. Second, augmentation studies in medical\nimaging are fragmented and limited to single tasks or architectures, leaving\nthe benefits of advanced mix-based strategies unclear. To address these\nchallenges, we propose a unified evaluation framework with six mix-based\naugmentation methods integrated with both convolutional and transformer\nbackbones on brain tumour MRI and eye disease fundus datasets. Our\ncontributions are threefold. (1) We introduce MediAug, a comprehensive and\nreproducible benchmark for advanced data augmentation in medical imaging. (2)\nWe systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix\nwith ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive\nexperiments that MixUp yields the greatest improvement on the brain tumor\nclassification task for ResNet-50 with 79.19% accuracy and SnapMix yields the\ngreatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the\ngreatest improvement on the eye disease classification task for ResNet-50 with\n91.60% accuracy and CutMix yields the greatest improvement for ViT-B with\n97.94% accuracy. Code will be available at\nhttps://github.com/AIGeeksGroup/MediAug."}
{"id": "2504.19044", "pdf": "https://arxiv.org/pdf/2504.19044", "abs": "https://arxiv.org/abs/2504.19044", "authors": ["Di Wu", "Yibin Lei", "Christof Monz"], "title": "Calibrating Translation Decoding with Quality Estimation on LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Neural machine translation (NMT) systems typically employ maximum a\nposteriori (MAP) decoding to select the highest-scoring translation from the\ndistribution mass. However, recent evidence highlights the inadequacy of MAP\ndecoding, often resulting in low-quality or even pathological hypotheses -- the\ndecoding objective is not aligned with real-world translation quality. This\npaper proposes calibrating hypothesis likelihoods with translation quality from\na distribution view by directly optimizing their Pearson correlation -- thereby\nenhancing the effectiveness of translation decoding. With our method,\ntranslation on large language models (LLMs) improves substantially after\nlimited training (2K instances per direction). This improvement is orthogonal\nto those achieved through supervised fine-tuning, leading to substantial gains\nacross a broad range of metrics and human evaluations -- even when applied to\ntop-performing translation-specialized LLMs fine-tuned on high-quality\ntranslation data, such as Tower, or when compared to recent preference\noptimization methods, like CPO. Moreover, the calibrated translation likelihood\ncan directly serve as a strong proxy for translation quality, closely\napproximating or even surpassing some state-of-the-art translation quality\nestimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates\nthat calibration enhances the effectiveness of MAP decoding, thereby enabling\ngreater efficiency in real-world deployment. The resulting state-of-the-art\ntranslation model, which covers 10 languages, along with the accompanying code\nand human evaluation data, has been released to the community:\nhttps://github.com/moore3930/calibrating-llm-mt."}
{"id": "2504.19032", "pdf": "https://arxiv.org/pdf/2504.19032", "abs": "https://arxiv.org/abs/2504.19032", "authors": ["Niaz Ahmad", "Youngmoon Lee", "Guanghui Wang"], "title": "VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce VISUALCENT, a unified human pose and instance segmentation\nframework to address generalizability and scalability limitations to multi\nperson visual human analysis. VISUALCENT leverages centroid based bottom up\nkeypoint detection paradigm and uses Keypoint Heatmap incorporating Disk\nRepresentation and KeyCentroid to identify the optimal keypoint coordinates.\nFor the unified segmentation task, an explicit keypoint is defined as a dynamic\ncentroid called MaskCentroid to swiftly cluster pixels to specific human\ninstance during rapid changes in human body movement or significantly occluded\nenvironment. Experimental results on COCO and OCHuman datasets demonstrate\nVISUALCENTs accuracy and real time performance advantages, outperforming\nexisting methods in mAP scores and execution frame rate per second. The\nimplementation is available on the project page."}
{"id": "2504.19061", "pdf": "https://arxiv.org/pdf/2504.19061", "abs": "https://arxiv.org/abs/2504.19061", "authors": ["Anindya Bijoy Das", "Shibbir Ahmed", "Shahnewaz Karim Sakib"], "title": "Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Clinical summarization is crucial in healthcare as it distills complex\nmedical data into digestible information, enhancing patient understanding and\ncare management. Large language models (LLMs) have shown significant potential\nin automating and improving the accuracy of such summarizations due to their\nadvanced natural language understanding capabilities. These models are\nparticularly applicable in the context of summarizing medical/clinical texts,\nwhere precise and concise information transfer is essential. In this paper, we\ninvestigate the effectiveness of open-source LLMs in extracting key events from\ndischarge reports, such as reasons for hospital admission, significant\nin-hospital events, and critical follow-up actions. In addition, we also assess\nthe prevalence of various types of hallucinations in the summaries produced by\nthese models. Detecting hallucinations is vital as it directly influences the\nreliability of the information, potentially affecting patient care and\ntreatment outcomes. We conduct comprehensive numerical simulations to\nrigorously evaluate the performance of these models, further probing the\naccuracy and fidelity of the extracted content in clinical summarization."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19066", "pdf": "https://arxiv.org/pdf/2504.19066", "abs": "https://arxiv.org/abs/2504.19066", "authors": ["Deeksha Varshney", "Keane Ong", "Rui Mao", "Erik Cambria", "Gianmarco Mengaldo"], "title": "ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Accurate assessments of extreme weather events are vital for research and\npolicy, yet localized and granular data remain scarce in many parts of the\nworld. This data gap limits our ability to analyze potential outcomes and\nimplications of extreme weather events, hindering effective decision-making.\nLarge Language Models (LLMs) can process vast amounts of unstructured text\ndata, extract meaningful insights, and generate detailed assessments by\nsynthesizing information from multiple sources. Furthermore, LLMs can\nseamlessly transfer their general language understanding to smaller models,\nenabling these models to retain key knowledge while being fine-tuned for\nspecific tasks. In this paper, we propose Extreme Weather Reasoning-Aware\nAlignment (EWRA), a method that enhances small language models (SLMs) by\nincorporating structured reasoning paths derived from LLMs, and\nExtremeWeatherNews, a large dataset of extreme weather event-related news\narticles. EWRA and ExtremeWeatherNews together form the overall framework,\nClimaEmpact, that focuses on addressing three critical extreme-weather tasks:\ncategorization of tangible vulnerabilities/impacts, topic labeling, and emotion\nanalysis. By aligning SLMs with advanced reasoning strategies on\nExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for\nSLM alignment), EWRA improves the SLMs' ability to generate well-grounded and\ndomain-specific responses for extreme weather analytics. Our results show that\nthe approach proposed guides SLMs to output domain-aligned responses,\nsurpassing the performance of task-specific models and offering enhanced\nreal-world applicability for extreme weather analytics."}
{"id": "2504.19074", "pdf": "https://arxiv.org/pdf/2504.19074", "abs": "https://arxiv.org/abs/2504.19074", "authors": ["Anyong Qin", "Chaoqi Yuan", "Qiang Li", "Feng Yang", "Tiecheng Song", "Chenqiang Gao"], "title": "Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype", "categories": ["cs.CV", "cs.LG"], "comment": "5 pages, 2 figures. IEEE Geoscience and Remote Sensing Letters (2025)", "summary": "Convolutional neural networks (CNNs) are effective for hyperspectral image\n(HSI) classification, but their 3D convolutional structures introduce high\ncomputational costs and limited generalization in few-shot scenarios. Domain\nshifts caused by sensor differences and environmental variations further hinder\ncross-dataset adaptability. Metric-based few-shot learning (FSL) prototype\nnetworks mitigate this problem, yet their performance is sensitive to prototype\nquality, especially with limited samples. To overcome these challenges, a\ndual-branch residual network that integrates spatial and spectral features via\nparallel branches is proposed in this letter. Additionally, more robust refined\nprototypes are obtained through a regulation term. Furthermore, a kernel\nprobability matching strategy aligns source and target domain features,\nalleviating domain shift. Experiments on four publicly available HSI datasets\nillustrate that the proposal achieves superior performance compared to other\nmethods."}
{"id": "2504.19070", "pdf": "https://arxiv.org/pdf/2504.19070", "abs": "https://arxiv.org/abs/2504.19070", "authors": ["Sakshi Singh", "Abhinav Prakash", "Aakriti Shah", "Chaitanya Sachdeva", "Sanjana Dumpala"], "title": "Sample-Efficient Language Model for Hinglish Conversational AI", "categories": ["cs.CL", "I.2.7; I.2.6; H.5.2"], "comment": "5 pages, 2 tables, 2 figures", "summary": "This paper presents our process for developing a sample-efficient language\nmodel for a conversational Hinglish chatbot. Hinglish, a code-mixed language\nthat combines Hindi and English, presents a unique computational challenge due\nto inconsistent spelling, lack of standardization, and limited quality of\nconversational data. This work evaluates multiple pre-trained cross-lingual\nlanguage models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning\ntechniques to improve performance on Hinglish conversational tasks. The\nproposed approach integrates synthetically generated dialogues with insights\nfrom existing Hinglish datasets to address data scarcity. Experimental results\ndemonstrate that models with fewer parameters, when appropriately fine-tuned on\nhigh-quality code-mixed data, can achieve competitive performance for Hinglish\nconversation generation while maintaining computational efficiency."}
{"id": "2504.19075", "pdf": "https://arxiv.org/pdf/2504.19075", "abs": "https://arxiv.org/abs/2504.19075", "authors": ["Qiuhui Chen", "Jintao Wang", "Gang Wang", "Yi Hong"], "title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance."}
{"id": "2504.19095", "pdf": "https://arxiv.org/pdf/2504.19095", "abs": "https://arxiv.org/abs/2504.19095", "authors": ["Jikai Wang", "Juntao Li", "Lijun Wu", "Min Zhang"], "title": "Efficient Reasoning for LLMs through Speculative Chain-of-Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have\nrecently attracted widespread attention due to their impressive task-solving\nabilities. However, the enormous model size and the generation of lengthy\nthought chains introduce significant reasoning costs and response latency.\nExisting methods for efficient reasoning mainly focus on reducing the number of\nmodel parameters or shortening the chain-of-thought length. In this paper, we\nintroduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency\nfrom another perspective by accelerated average reasoning speed through large\nand small model collaboration. SCoT conducts thought-level drafting using a\nlightweight draft model. Then it selects the best CoT draft and corrects the\nerror cases with the target model. The proposed thinking behavior alignment\nimproves the efficiency of drafting and the draft selection strategy maintains\nthe prediction accuracy for complex problems. Experimental results on GSM8K,\nMATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces\nreasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while\nachieving near-target-model-level performance. Our code is available at\nhttps://github.com/Jikai0Wang/Speculative_CoT."}
{"id": "2504.19077", "pdf": "https://arxiv.org/pdf/2504.19077", "abs": "https://arxiv.org/abs/2504.19077", "authors": ["Mitchell Goff", "Greg Hogan", "George Hotz", "Armand du Parc Locmaria", "Kacper Raczy", "Harald Schäfer", "Adeeb Shihadeh", "Weixing Zhang", "Yassine Yousfi"], "title": "Learning to Drive from a World Model", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Most self-driving systems rely on hand-coded perception outputs and\nengineered driving rules. Learning directly from human driving data with an\nend-to-end method can allow for a training architecture that is simpler and\nscales well with compute and data.\n  In this work, we propose an end-to-end training architecture that uses real\ndriving data to train a driving policy in an on-policy simulator. We show two\ndifferent methods of simulation, one with reprojective simulation and one with\na learned world model. We show that both methods can be used to train a policy\nthat learns driving behavior without any hand-coded driving rules. We evaluate\nthe performance of these policies in a closed-loop simulation and when deployed\nin a real-world advanced driver-assistance system."}
{"id": "2504.19101", "pdf": "https://arxiv.org/pdf/2504.19101", "abs": "https://arxiv.org/abs/2504.19101", "authors": ["Qianren Mao", "Qili Zhang", "Hanwen Hao", "Zhentao Han", "Runhua Xu", "Weifeng Jiang", "Qi Hu", "Zhijun Chen", "Tyler Zhou", "Bo Li", "Yangqiu Song", "Jin Dong", "Jianxin Li", "Philip S. Yu"], "title": "Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising\nsolution for enhancing the accuracy and credibility of Large Language Models\n(LLMs), particularly in Question & Answer tasks. This is achieved by\nincorporating proprietary and private data from integrated databases. However,\nprivate RAG systems face significant challenges due to the scarcity of private\ndomain data and critical data privacy issues. These obstacles impede the\ndeployment of private RAG systems, as developing privacy-preserving RAG systems\nrequires a delicate balance between data security and data availability. To\naddress these challenges, we regard federated learning (FL) as a highly\npromising technology for privacy-preserving RAG services. We propose a novel\nframework called Federated Retrieval-Augmented Generation (FedE4RAG). This\nframework facilitates collaborative training of client-side RAG retrieval\nmodels. The parameters of these models are aggregated and distributed on a\ncentral-server, ensuring data privacy without direct sharing of raw data. In\nFedE4RAG, knowledge distillation is employed for communication between the\nserver and client models. This technique improves the generalization of local\nRAG retrievers during the federated learning process. Additionally, we apply\nhomomorphic encryption within federated learning to safeguard model parameters\nand mitigate concerns related to data leakage. Extensive experiments conducted\non the real-world dataset have validated the effectiveness of FedE4RAG. The\nresults demonstrate that our proposed framework can markedly enhance the\nperformance of private RAG systems while maintaining robust data privacy\nprotection."}
{"id": "2504.19080", "pdf": "https://arxiv.org/pdf/2504.19080", "abs": "https://arxiv.org/abs/2504.19080", "authors": ["Zhenkai Qin", "Jiaquan Liang", "Qiao Fang"], "title": "MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Attention mechanisms have significantly advanced deep learning by enhancing\nfeature representation through selective focus. However, existing approaches\noften independently model channel importance and spatial saliency, overlooking\ntheir inherent interdependence and limiting their effectiveness. To address\nthis limitation, we propose MIA-Mind, a lightweight and modular\nMultidimensional Interactive Attention Mechanism, built upon the MindSpore\nframework. MIA-Mind jointly models spatial and channel features through a\nunified cross-attentive fusion strategy, enabling fine-grained feature\nrecalibration with minimal computational overhead. Extensive experiments are\nconducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an\naccuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on\nCIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the\nversatility, lightweight design, and generalization ability of MIA-Mind across\nheterogeneous tasks. Future work will explore the extension of MIA-Mind to\nlarge-scale datasets, the development of ada,ptive attention fusion strategies,\nand distributed deployment to further enhance scalability and robustness."}
{"id": "2504.19110", "pdf": "https://arxiv.org/pdf/2504.19110", "abs": "https://arxiv.org/abs/2504.19110", "authors": ["Huajian Xin", "Luming Li", "Xiaoran Jin", "Jacques Fleuriot", "Wenda Li"], "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."}
{"id": "2504.19086", "pdf": "https://arxiv.org/pdf/2504.19086", "abs": "https://arxiv.org/abs/2504.19086", "authors": ["Xiaoran Xu", "Jiangang Yang", "Wenyue Chong", "Wenhui Shi", "Shichu Sun", "Jing Xing", "Jian Liu"], "title": "Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object\ndetector on a single source domain while generalizing well to diverse unseen\ntarget domains, making it suitable for multimedia applications that involve\nvarious domain shifts, such as intelligent video surveillance and VR/AR\ntechnologies. With the success of large-scale Vision-Language Models, recent\nS-DGOD approaches exploit pre-trained vision-language knowledge to guide\ninvariant feature learning across visual domains. However, the utilized\nknowledge remains at a coarse-grained level~(e.g., the textual description of\nadverse weather paired with the image) and serves as an implicit regularization\nfor guidance, struggling to learn accurate region- and object-level features in\nvarying domains. In this work, we propose a new cross-modal feature learning\nmethod, which can capture generalized and discriminative regional features for\nS-DGOD tasks. The core of our method is the mechanism of Cross-modal and\nRegion-aware Feature Interaction, which simultaneously learns both inter-modal\nand intra-modal regional invariance through dynamic interactions between\nfine-grained textual and visual features. Moreover, we design a simple but\neffective strategy called Cross-domain Proposal Refining and Mixing, which\naligns the position of region proposals across multiple domains and diversifies\nthem, enhancing the localization ability of detectors in unseen scenarios. Our\nmethod achieves new state-of-the-art results on S-DGOD benchmark datasets, with\nimprovements of +8.8\\%~mPC on Cityscapes-C and +7.9\\%~mPC on DWD over\nbaselines, demonstrating its efficacy."}
{"id": "2504.19162", "pdf": "https://arxiv.org/pdf/2504.19162", "abs": "https://arxiv.org/abs/2504.19162", "authors": ["Jiaqi Chen", "Bang Zhang", "Ruotian Ma", "Peisong Wang", "Xiaodan Liang", "Zhaopeng Tu", "Xiaolong Li", "Kwan-Yee K. Wong"], "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project: https://chen-judge.github.io/SPC/", "summary": "Evaluating the step-by-step reliability of large language model (LLM)\nreasoning, such as Chain-of-Thought, remains challenging due to the difficulty\nand cost of obtaining high-quality step-level supervision. In this paper, we\nintroduce Self-Play Critic (SPC), a novel approach where a critic model evolves\nits ability to assess reasoning steps through adversarial self-play games,\neliminating the need for manual step-level annotation. SPC involves fine-tuning\ntwo copies of a base model to play two roles, namely a \"sneaky generator\" that\ndeliberately produces erroneous steps designed to be difficult to detect, and a\n\"critic\" that analyzes the correctness of reasoning steps. These two models\nengage in an adversarial game in which the generator aims to fool the critic,\nwhile the critic model seeks to identify the generator's errors. Using\nreinforcement learning based on the game outcomes, the models iteratively\nimprove; the winner of each confrontation receives a positive reward and the\nloser receives a negative reward, driving continuous self-evolution.\nExperiments on three reasoning process benchmarks (ProcessBench, PRM800K,\nDeltaBench) demonstrate that our SPC progressively enhances its error detection\ncapabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and\nsurpasses strong baselines, including distilled R1 model. Furthermore, applying\nSPC to guide the test-time search of diverse LLMs significantly improves their\nmathematical reasoning performance on MATH500 and AIME2024, outperforming\nstate-of-the-art process reward models."}
{"id": "2504.19115", "pdf": "https://arxiv.org/pdf/2504.19115", "abs": "https://arxiv.org/abs/2504.19115", "authors": ["Jiaqi Peng", "Tai Wang", "Jiangmiao Pang", "Yuan Shen"], "title": "Towards Latency-Aware 3D Streaming Perception for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although existing 3D perception algorithms have demonstrated significant\nimprovements in performance, their deployment on edge devices continues to\nencounter critical challenges due to substantial runtime latency. We propose a\nnew benchmark tailored for online evaluation by considering runtime latency.\nBased on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP)\nframework that addresses the latency issue through two primary components: 1)\nlatency-aware history integration, which extends query propagation into a\ncontinuous process, ensuring the integration of historical feature regardless\nof varying latency; 2) latency-aware predictive detection, a module that\ncompensates the detection results with the predicted trajectory and the\nposterior accessed latency. By incorporating the latency-aware mechanism, our\nmethod shows generalization across various latency levels, achieving an online\nperformance that closely aligns with 80\\% of its offline evaluation on the\nJetson AGX Orin without any acceleration techniques."}
{"id": "2504.19191", "pdf": "https://arxiv.org/pdf/2504.19191", "abs": "https://arxiv.org/abs/2504.19191", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "WuNeng: Hybrid State with Attention", "categories": ["cs.CL"], "comment": null, "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."}
{"id": "2504.19124", "pdf": "https://arxiv.org/pdf/2504.19124", "abs": "https://arxiv.org/abs/2504.19124", "authors": ["Zhongxuan Li"], "title": "Blind Source Separation Based on Sparsity", "categories": ["cs.CV"], "comment": null, "summary": "Blind source separation (BSS) is a key technique in array processing and data\nanalysis, aiming to recover unknown sources from observed mixtures without\nknowledge of the mixing matrix. Classical independent component analysis (ICA)\nmethods rely on the assumption that sources are mutually independent. To\naddress limitations of ICA, sparsity-based methods have been introduced, which\ndecompose source signals sparsely in a predefined dictionary. Morphological\nComponent Analysis (MCA), based on sparse representation theory, assumes that a\nsignal is a linear combination of components with distinct geometries, each\nsparsely representable in one dictionary and not in others. This approach has\nrecently been applied to BSS with promising results.\n  This report reviews key approaches derived from classical ICA and explores\nsparsity-based methods for BSS. It introduces the theory of sparse\nrepresentation and decomposition, followed by a block coordinate relaxation MCA\nalgorithm, whose variants are used in Multichannel MCA (MMCA) and Generalized\nMCA (GMCA). A local dictionary learning method using K-SVD is then presented.\nFinally, we propose an improved algorithm, SAC+BK-SVD, which enhances K-SVD by\nlearning a block-sparsifying dictionary that clusters and updates similar atoms\nin blocks.\n  The implementation includes experiments on image segmentation and blind image\nsource separation using the discussed techniques. We also compare the proposed\nblock-sparse dictionary learning algorithm with K-SVD. Simulation results\ndemonstrate that our method yields improved blind image separation quality."}
{"id": "2504.19209", "pdf": "https://arxiv.org/pdf/2504.19209", "abs": "https://arxiv.org/abs/2504.19209", "authors": ["Elisabeth Fittschen", "Bella Xia", "Leib Celnik", "Paul Dilley", "Tom Lippincott"], "title": "Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora", "categories": ["cs.CL", "cs.LG"], "comment": "Under review", "summary": "We measure the effects of several implementation choices for the Dynamic\nEmbedded Topic Model, as applied to five distinct diachronic corpora, with the\ngoal of isolating important decisions for its use and further development. We\nidentify priorities that will maximize utility in applied scholarship,\nincluding the practical scalability of vocabulary size to best exploit the\nstrengths of embedded representations, and more flexible modeling of intervals\nto accommodate the uneven temporal distributions of historical writing. Of\nsimilar importance, we find performance is not significantly or consistently\naffected by several aspects that otherwise limit the model's application or\nmight consume the resources of a grid search."}
{"id": "2504.19127", "pdf": "https://arxiv.org/pdf/2504.19127", "abs": "https://arxiv.org/abs/2504.19127", "authors": ["Jialang Lu", "Huayu Zhao", "Huiyu Zhai", "Xingxing Yang", "Shini Han"], "title": "DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ICMR 2025 Main track. Code is available at\n  https://github.com/Wenyuzhy/DeepSPG", "summary": "There has long been a belief that high-level semantics learning can benefit\nvarious downstream computer vision tasks. However, in the low-light image\nenhancement (LLIE) community, existing methods learn a brutal mapping between\nlow-light and normal-light domains without considering the semantic information\nof different regions, especially in those extremely dark regions that suffer\nfrom severe information loss. To address this issue, we propose a new deep\nsemantic prior-guided framework (DeepSPG) based on Retinex image decomposition\nfor LLIE to explore informative semantic knowledge via a pre-trained semantic\nsegmentation model and multimodal learning. Notably, we incorporate both\nimage-level semantic prior and text-level semantic prior and thus formulate a\nmultimodal learning framework with combinatorial deep semantic prior guidance\nfor LLIE. Specifically, we incorporate semantic knowledge to guide the\nenhancement process via three designs: an image-level semantic prior guidance\nby leveraging hierarchical semantic features from a pre-trained semantic\nsegmentation model; a text-level semantic prior guidance by integrating natural\nlanguage semantic constraints via a pre-trained vision-language model; a\nmulti-scale semantic-aware structure that facilitates effective semantic\nfeature incorporation. Eventually, our proposed DeepSPG demonstrates superior\nperformance compared to state-of-the-art methods across five benchmark\ndatasets. The implementation details and code are publicly available at\nhttps://github.com/Wenyuzhy/DeepSPG."}
{"id": "2504.19254", "pdf": "https://arxiv.org/pdf/2504.19254", "abs": "https://arxiv.org/abs/2504.19254", "authors": ["Dylan Bouchard", "Mohit Singh Chauhan"], "title": "Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "UQLM repository: https://github.com/cvs-health/uqlm", "summary": "Hallucinations are a persistent problem with Large Language Models (LLMs). As\nthese models become increasingly used in high-stakes domains, such as\nhealthcare and finance, the need for effective hallucination detection is\ncrucial. To this end, we propose a versatile framework for zero-resource\nhallucination detection that practitioners can apply to real-world use cases.\nTo achieve this, we adapt a variety of existing uncertainty quantification (UQ)\ntechniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge,\ntransforming them as necessary into standardized response-level confidence\nscores ranging from 0 to 1. To enhance flexibility, we introduce a tunable\nensemble approach that incorporates any combination of the individual\nconfidence scores. This approach enables practitioners to optimize the ensemble\nfor a specific use case for improved performance. To streamline implementation,\nthe full suite of scorers is offered in this paper's companion Python toolkit,\nUQLM. To evaluate the performance of the various scorers, we conduct an\nextensive set of experiments using several LLM question-answering benchmarks.\nWe find that our tunable ensemble typically surpasses its individual components\nand outperforms existing hallucination detection methods. Our results\ndemonstrate the benefits of customized hallucination detection strategies for\nimproving the accuracy and reliability of LLMs."}
{"id": "2504.19136", "pdf": "https://arxiv.org/pdf/2504.19136", "abs": "https://arxiv.org/abs/2504.19136", "authors": ["Huiling Zheng", "Xian Zhong", "Bin Liu", "Yi Xiao", "Bihan Wen", "Xiaofeng Li"], "title": "PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 8 figures", "summary": "The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover\nclassification remains challenging due to modality heterogeneity and the\nunderutilization of spectral complementarity. Existing methods often fail to\ndecouple shared structural features from modality-specific radiometric\nattributes, leading to feature conflicts and information loss. To address this\nissue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework\nthat separates phase (modality-shared) and amplitude (modality-specific)\ncomponents in the Fourier domain. Specifically, PAD consists of two key\ncomponents: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase\nfeatures through convolution-guided scaling to enhance geometric consistency,\nand 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates\nhigh-frequency details and low-frequency structures using frequency-adaptive\nmultilayer perceptrons. This approach leverages SAR's sensitivity to\nmorphological features and RGB's spectral richness. Extensive experiments on\nWHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our\nwork establishes a new paradigm for physics-aware multi-modal fusion in remote\nsensing. The code will be available at https://github.com/RanFeng2/PAD."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19161", "pdf": "https://arxiv.org/pdf/2504.19161", "abs": "https://arxiv.org/abs/2504.19161", "authors": ["Zheng Fang", "Kangjun Liu", "Ke Chen", "Qingyu Liu", "Jianguo Zhang", "Lingyang Song", "Yaowei Wang"], "title": "RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\\textpertenthousand Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "The task of radio map estimation aims to generate a dense representation of\nelectromagnetic spectrum quantities, such as the received signal strength at\neach grid point within a geographic region, based on measurements from a subset\nof spatially distributed nodes (represented as pixels). Recently, deep vision\nmodels such as the U-Net have been adapted to radio map estimation, whose\neffectiveness can be guaranteed with sufficient spatial observations (typically\n0.01% to 1% of pixels) in each map, to model local dependency of observed\nsignal power. However, such a setting of sufficient measurements can be less\npractical in real-world scenarios, where extreme sparsity in spatial sampling\ncan be widely encountered. To address this challenge, we propose RadioFormer, a\nnovel multiple-granularity transformer designed to handle the constraints posed\nby spatial sparse observations. Our RadioFormer, through a dual-stream\nself-attention (DSA) module, can respectively discover the correlation of\npixel-wise observed signal power and also learn patch-wise buildings'\ngeometries in a style of multiple granularities, which are integrated into\nmulti-scale representations of radio maps by a cross stream cross-attention\n(CCA) module. Extensive experiments on the public RadioMapSeer dataset\ndemonstrate that RadioFormer outperforms state-of-the-art methods in radio map\nestimation while maintaining the lowest computational cost. Furthermore, the\nproposed approach exhibits exceptional generalization capabilities and robust\nzero-shot performance, underscoring its potential to advance radio map\nestimation in a more practical setting with very limited observation nodes."}
{"id": "2504.19298", "pdf": "https://arxiv.org/pdf/2504.19298", "abs": "https://arxiv.org/abs/2504.19298", "authors": ["Hanyu Lai", "Junjie Gao", "Xiao Liu", "Yifan Xu", "Shudan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "AndroidGen: Building an Android Language Agent under Data Scarcity", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have opened up a world of possibilities for various NLP\ntasks, sparking optimism for the future. Despite their potential, LLMs have yet\nto be widely used as agents on real mobile devices. The main challenge is the\nneed for high-quality data sources. Time constraints and labor intensity often\nhinder human annotation. On the other hand, existing LLMs exhibit inadequate\ncompletion rates and need a robust data filtration strategy. Given these\nchallenges, we develop a framework called AndroidGen to enhance the\ncapabilities of LLM-based agents under data scarcity. In addition, we leverage\nAndroidGen to collect trajectories given human tasks and train open-source LLMs\non these trajectories to develop an open-source mobile agent without manually\nlabeled trajectories. We extensively evaluate AndroidGen with AndroidWorld,\nAitW, and various popular applications, demonstrating its improvements and\nrevealing potential areas for future improvement. Code, model, and data are\navailable at https://github.com/THUDM/AndroidGen."}
{"id": "2504.19165", "pdf": "https://arxiv.org/pdf/2504.19165", "abs": "https://arxiv.org/abs/2504.19165", "authors": ["Yuan Li", "Ziqian Bai", "Feitong Tan", "Zhaopeng Cui", "Sean Fanello", "Yinda Zhang"], "title": "IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos", "categories": ["cs.CV"], "comment": "CVPR2025; project page:\n  https://y-u-a-n-l-i.github.io/projects/IM-Portrait/", "summary": "We propose a novel 3D-aware diffusion-based method for generating\nphotorealistic talking head videos directly from a single identity image and\nexplicit control signals (e.g., expressions). Our method generates Multiplane\nImages (MPIs) that ensure geometric consistency, making them ideal for\nimmersive viewing experiences like binocular videos for VR headsets. Unlike\nexisting methods that often require a separate stage or joint optimization to\nreconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach\ndirectly generates the final output through a single denoising process,\neliminating the need for post-processing steps to render novel views\nefficiently. To effectively learn from monocular videos, we introduce a\ntraining mechanism that reconstructs the output MPI randomly in either the\ntarget or the reference camera space. This approach enables the model to\nsimultaneously learn sharp image details and underlying 3D information.\nExtensive experiments demonstrate the effectiveness of our method, which\nachieves competitive avatar quality and novel-view rendering capabilities, even\nwithout explicit 3D reconstruction or high-quality multi-view training data."}
{"id": "2504.19314", "pdf": "https://arxiv.org/pdf/2504.19314", "abs": "https://arxiv.org/abs/2504.19314", "authors": ["Peilin Zhou", "Bruce Leon", "Xiang Ying", "Can Zhang", "Yifan Shao", "Qichen Ye", "Dading Chong", "Zhiling Jin", "Chenxuan Xie", "Meng Cao", "Yuxin Gu", "Sixin Hong", "Jing Ren", "Jian Chen", "Chao Liu", "Yining Hua"], "title": "BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese", "categories": ["cs.CL"], "comment": "Under Review", "summary": "As large language models (LLMs) evolve into tool-using agents, the ability to\nbrowse the web in real-time has become a critical yardstick for measuring their\nreasoning and retrieval competence. Existing benchmarks such as BrowseComp\nconcentrate on English and overlook the linguistic, infrastructural, and\ncensorship-related complexities of other major information ecosystems -- most\nnotably Chinese. To address this gap, we introduce BrowseComp-ZH, a\nhigh-difficulty benchmark purpose-built to comprehensively evaluate LLM agents\non the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning\n11 diverse domains. Each question is reverse-engineered from a short,\nobjective, and easily verifiable answer (e.g., a date, number, or proper noun).\nA two-stage quality control protocol is applied to strive for high question\ndifficulty and answer uniqueness. We benchmark over 20 state-of-the-art\nlanguage models and agentic search systems on our proposed BrowseComp-ZH.\nDespite their strong conversational and retrieval capabilities, most models\nstruggle severely: a large number achieve accuracy rates below 10%, and only a\nhandful exceed 20%. Even the best-performing system, OpenAI's DeepResearch,\nreaches just 42.9%. These results demonstrate the considerable difficulty of\nBrowseComp-ZH, where success demands not only effective retrieval strategies,\nbut also sophisticated reasoning and information reconciliation -- capabilities\nthat current models still struggle to master. Our dataset, construction\nguidelines, and benchmark results have been publicly released at\nhttps://github.com/PALIN2018/BrowseComp-ZH."}
{"id": "2504.19183", "pdf": "https://arxiv.org/pdf/2504.19183", "abs": "https://arxiv.org/abs/2504.19183", "authors": ["Mi Zheng", "Guanglei Yang", "Zitong Huang", "Zhenhua Guo", "Kevin Han", "Wangmeng Zuo"], "title": "Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "With the emergence of transformer-based architectures and large language\nmodels (LLMs), the accuracy of road scene perception has substantially\nadvanced. Nonetheless, current road scene segmentation approaches are\npredominantly trained on closed-set data, resulting in insufficient detection\ncapabilities for out-of-distribution (OOD) objects. To overcome this\nlimitation, road anomaly detection methods have been proposed. However,\nexisting methods primarily depend on image inpainting and OOD distribution\ndetection techniques, facing two critical issues: (1) inadequate consideration\nof the objectiveness attributes of anomalous regions, causing incomplete\nsegmentation when anomalous objects share similarities with known classes, and\n(2) insufficient attention to environmental constraints, leading to the\ndetection of anomalies irrelevant to autonomous driving tasks. In this paper,\nwe propose a novel framework termed Segmenting Objectiveness and Task-Awareness\n(SOTA) for autonomous driving scenes. Specifically, SOTA enhances the\nsegmentation of objectiveness through a Semantic Fusion Block (SFB) and filters\nanomalies irrelevant to road navigation tasks using a Scene-understanding\nGuided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on\nmultiple benchmark datasets, including Fishyscapes Lost and Found,\nSegment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA\nconsistently improves OOD detection performance across diverse detectors,\nachieving robust and accurate segmentation outcomes."}
{"id": "2504.19333", "pdf": "https://arxiv.org/pdf/2504.19333", "abs": "https://arxiv.org/abs/2504.19333", "authors": ["James O' Neill", "Santhosh Subramanian", "Eric Lin", "Vaikkunth Mugunthan"], "title": "Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The trend towards large language models (LLMs) for guardrailing against\nundesired behaviors is increasing and has shown promise for censoring user\ninputs. However, increased latency, memory consumption, hosting expenses and\nnon-structured outputs can make their use prohibitive.\n  In this work, we show that task-specific data generation can lead to\nfine-tuned classifiers that significantly outperform current state of the art\n(SoTA) while being orders of magnitude smaller. Secondly, we show that using a\nsingle model, \\texttt{MultiTaskGuard}, that is pretrained on a large\nsynthetically generated dataset with unique task instructions further improves\ngeneralization. Thirdly, our most performant models, \\texttt{UniGuard}, are\nfound using our proposed search-based model merging approach that finds an\noptimal set of parameters to combine single-policy models and multi-policy\nguardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,\nour efficient guardrail classifiers improve over the best performing SoTA\npublicly available LLMs and 3$^{\\text{rd}}$ party guardrail APIs in detecting\nunsafe and safe behaviors by an average F1 score improvement of \\textbf{29.92}\npoints over Aegis-LlamaGuard and \\textbf{21.62} over \\texttt{gpt-4o},\nrespectively. Lastly, our guardrail synthetic data generation process that uses\ncustom task-specific guardrail poli"}
{"id": "2504.19186", "pdf": "https://arxiv.org/pdf/2504.19186", "abs": "https://arxiv.org/abs/2504.19186", "authors": ["Zhangshuo Qi", "Luqi Cheng", "Zijie Zhou", "Guangming Xiong"], "title": "LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "In autonomous driving, place recognition is critical for global localization\nin GPS-denied environments. LiDAR and radar-based place recognition methods\nhave garnered increasing attention, as LiDAR provides precise ranging, whereas\nradar excels in adverse weather resilience. However, effectively leveraging\nLiDAR-radar fusion for place recognition remains challenging. The noisy and\nsparse nature of radar data limits its potential to further improve recognition\naccuracy. In addition, heterogeneous radar configurations complicate the\ndevelopment of unified cross-modality fusion frameworks. In this paper, we\npropose LRFusionPR, which improves recognition accuracy and robustness by\nfusing LiDAR with either single-chip or scanning radar. Technically, a\ndual-branch network is proposed to fuse different modalities within the unified\npolar coordinate bird's eye view (BEV) representation. In the fusion branch,\ncross-attention is utilized to perform cross-modality feature interactions. The\nknowledge from the fusion branch is simultaneously transferred to the\ndistillation branch, which takes radar as its only input to further improve the\nrobustness. Ultimately, the descriptors from both branches are concatenated,\nproducing the multimodal global descriptor for place retrieval. Extensive\nevaluations on multiple datasets demonstrate that our LRFusionPR achieves\naccurate place recognition, while maintaining robustness under varying weather\nconditions. Our open-source code will be released at\nhttps://github.com/QiZS-BIT/LRFusionPR."}
{"id": "2504.19339", "pdf": "https://arxiv.org/pdf/2504.19339", "abs": "https://arxiv.org/abs/2504.19339", "authors": ["Dongqi Liu", "Xi Yu", "Vera Demberg", "Mirella Lapata"], "title": "Explanatory Summarization with Discourse-Driven Planning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by the Transactions of the Association for Computational\n  Linguistics (TACL)", "summary": "Lay summaries for scientific documents typically include explanations to help\nreaders grasp sophisticated concepts or arguments. However, current automatic\nsummarization methods do not explicitly model explanations, which makes it\ndifficult to align the proportion of explanatory content with human-written\nsummaries. In this paper, we present a plan-based approach that leverages\ndiscourse frameworks to organize summary generation and guide explanatory\nsentences by prompting responses to the plan. Specifically, we propose two\ndiscourse-driven planning strategies, where the plan is conditioned as part of\nthe input or part of the output prefix, respectively. Empirical experiments on\nthree lay summarization datasets show that our approach outperforms existing\nstate-of-the-art methods in terms of summary quality, and it enhances model\nrobustness, controllability, and mitigates hallucination."}
{"id": "2504.19198", "pdf": "https://arxiv.org/pdf/2504.19198", "abs": "https://arxiv.org/abs/2504.19198", "authors": ["Lingtao Peng", "Liheng Bian"], "title": "Adaptive Dual-domain Learning for Underwater Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by AAAI 2025", "summary": "Recently, learning-based Underwater Image Enhancement (UIE) methods have\ndemonstrated promising performance. However, existing learning-based methods\nstill face two challenges. 1) They rarely consider the inconsistent degradation\nlevels in different spatial regions and spectral bands simultaneously. 2) They\ntreat all regions equally, ignoring that the regions with high-frequency\ndetails are more difficult to reconstruct. To address these challenges, we\npropose a novel UIE method based on spatial-spectral dual-domain adaptive\nlearning, termed SS-UIE. Specifically, we first introduce a spatial-wise\nMulti-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise\nSelf-Attention (SWSA) module, both with linear complexity, and combine them in\nparallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the\nglobal receptive field of MCSS and SWSA, SS-block can effectively model the\ndegradation levels of different spatial regions and spectral bands, thereby\nenabling degradation level-based dual-domain adaptive UIE. By stacking multiple\nSS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss\n(FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the\nmodel's attention on the regions with high-frequency details. Extensive\nexperiments validate that the SS-UIE technique outperforms state-of-the-art UIE\nmethods while requiring cheaper computational and memory costs."}
{"id": "2504.19395", "pdf": "https://arxiv.org/pdf/2504.19395", "abs": "https://arxiv.org/abs/2504.19395", "authors": ["Zhouxiang Fang", "Aayush Mishra", "Muhan Gao", "Anqi Liu", "Daniel Khashabi"], "title": "ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recent works have suggested that In-Context Learning (ICL) operates in dual\nmodes, i.e. task retrieval (remember learned patterns from pre-training) and\ntask learning (inference-time ``learning'' from demonstrations). However,\ndisentangling these the two modes remains a challenging goal. We introduce ICL\nCIPHERS, a class of task reformulations based on substitution ciphers borrowed\nfrom classic cryptography. In this approach, a subset of tokens in the\nin-context inputs are substituted with other (irrelevant) tokens, rendering\nEnglish sentences less comprehensible to human eye. However, by design, there\nis a latent, fixed pattern to this substitution, making it reversible. This\nbijective (reversible) cipher ensures that the task remains a well-defined task\nin some abstract sense, despite the transformations. It is a curious question\nif LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires\ndeciphering the latent cipher. We show that LLMs are better at solving ICL\nCIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline,\nproviding a novel approach to quantify ``learning'' in ICL. While this gap is\nsmall, it is consistent across the board on four datasets and six models.\nFinally, we examine LLMs' internal representations and identify evidence in\ntheir ability to decode the ciphered inputs."}
{"id": "2504.19210", "pdf": "https://arxiv.org/pdf/2504.19210", "abs": "https://arxiv.org/abs/2504.19210", "authors": ["Yuming Zhao", "Qijian Zhang", "Junhui Hou", "Jiazhi Xia", "Wenping Wang", "Ying He"], "title": "FlexPara: Flexible Neural Surface Parameterization", "categories": ["cs.CV"], "comment": null, "summary": "Surface parameterization is a fundamental geometry processing task, laying\nthe foundations for the visual presentation of 3D assets and numerous\ndownstream shape analysis scenarios. Conventional parameterization approaches\ndemand high-quality mesh triangulation and are restricted to certain simple\ntopologies unless additional surface cutting and decomposition are provided. In\npractice, the optimal configurations (e.g., type of parameterization domains,\ndistribution of cutting seams, number of mapping charts) may vary drastically\nwith different surface structures and task characteristics, thus requiring more\nflexible and controllable processing pipelines. To this end, this paper\nintroduces FlexPara, an unsupervised neural optimization framework to achieve\nboth global and multi-chart surface parameterizations by establishing\npoint-wise mappings between 3D surface points and adaptively-deformed 2D UV\ncoordinates. We ingeniously design and combine a series of\ngeometrically-interpretable sub-networks, with specific functionalities of\ncutting, deforming, unwrapping, and wrapping, to construct a bi-directional\ncycle mapping framework for global parameterization without the need for\nmanually specified cutting seams. Furthermore, we construct a multi-chart\nparameterization framework with adaptively-learned chart assignment. Extensive\nexperiments demonstrate the universality, superiority, and inspiring potential\nof our neural surface parameterization paradigm. The code will be publicly\navailable at https://github.com/AidenZhao/FlexPara"}
{"id": "2504.19406", "pdf": "https://arxiv.org/pdf/2504.19406", "abs": "https://arxiv.org/abs/2504.19406", "authors": ["Mengxia Yu", "Bang Nguyen", "Olivia Zino", "Meng Jiang"], "title": "Context Selection and Rewriting for Video-based EducationalQuestion Generation", "categories": ["cs.CL"], "comment": null, "summary": "Educational question generation (EQG) is a crucial component of intelligent\neducational systems, significantly aiding self-assessment, active learning, and\npersonalized education. While EQG systems have emerged, existing datasets\ntypically rely on predefined, carefully edited texts, failing to represent\nreal-world classroom content, including lecture speech with a set of\ncomplementary slides. To bridge this gap, we collect a dataset of educational\nquestions based on lectures from real-world classrooms. On this realistic\ndataset, we find that current methods for EQG struggle with accurately\ngenerating questions from educational videos, particularly in aligning with\nspecific timestamps and target answers. Common challenges include selecting\ninformative contexts from extensive transcripts and ensuring generated\nquestions meaningfully incorporate the target answer. To address the\nchallenges, we introduce a novel framework utilizing large language models for\ndynamically selecting and rewriting contexts based on target timestamps and\nanswers. First, our framework selects contexts from both lecture transcripts\nand video keyframes based on answer relevance and temporal proximity. Then, we\nintegrate the contexts selected from both modalities and rewrite them into\nanswer-containing knowledge statements, to enhance the logical connection\nbetween the contexts and the desired answer. This approach significantly\nimproves the quality and relevance of the generated questions. Our dataset and\ncode are released in https://github.com/mengxiayu/COSER."}
{"id": "2504.19212", "pdf": "https://arxiv.org/pdf/2504.19212", "abs": "https://arxiv.org/abs/2504.19212", "authors": ["Tuan Nguyen", "Naseem Khan", "Issa Khalil"], "title": "CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The rapid evolution of deepfake technology, particularly in\ninstruction-guided image editing, threatens the integrity of digital images by\nenabling subtle, context-aware manipulations. Generated conditionally from real\nimages and textual prompts, these edits are often imperceptible to both humans\nand existing detection systems, revealing significant limitations in current\ndefenses. We propose a novel multimodal capsule network, CapsFake, designed to\ndetect such deepfake image edits by integrating low-level capsules from visual,\ntextual, and frequency-domain modalities. High-level capsules, predicted\nthrough a competitive routing mechanism, dynamically aggregate local features\nto identify manipulated regions with precision. Evaluated on diverse datasets,\nincluding MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits,\nCapsFake outperforms state-of-the-art methods by up to 20% in detection\naccuracy. Ablation studies validate its robustness, achieving detection rates\nabove 94% under natural perturbations and 96% against adversarial attacks, with\nexcellent generalization to unseen editing scenarios. This approach establishes\na powerful framework for countering sophisticated image manipulations."}
{"id": "2504.19413", "pdf": "https://arxiv.org/pdf/2504.19413", "abs": "https://arxiv.org/abs/2504.19413", "authors": ["Prateek Chhikara", "Dev Khant", "Saket Aryan", "Taranjeet Singh", "Deshraj Yadav"], "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents."}
{"id": "2504.19223", "pdf": "https://arxiv.org/pdf/2504.19223", "abs": "https://arxiv.org/abs/2504.19223", "authors": ["Alexander Baumann", "Leonardo Ayala", "Silvia Seidlitz", "Jan Sellner", "Alexander Studier-Fischer", "Berkin Özdemir", "Lena Maier-Hein", "Slobodan Ilic"], "title": "CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Spectral imaging offers promising applications across diverse domains,\nincluding medicine and urban scene understanding, and is already established as\na critical modality in remote sensing. However, variability in channel\ndimensionality and captured wavelengths among spectral cameras impede the\ndevelopment of AI-driven methodologies, leading to camera-specific models with\nlimited generalizability and inadequate cross-camera applicability. To address\nthis bottleneck, we introduce $\\textbf{CARL}$, a model for\n$\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation\n$\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging\nmodalities. To enable the conversion of a spectral image with any channel\ndimensionality to a camera-agnostic embedding, we introduce wavelength\npositional encoding and a self-attention-cross-attention mechanism to compress\nspectral information into learned query representations. Spectral-spatial\npre-training is achieved with a novel spectral self-supervised JEPA-inspired\nstrategy tailored to CARL. Large-scale experiments across the domains of\nmedical imaging, autonomous driving, and satellite imaging demonstrate our\nmodel's unique robustness to spectral heterogeneity, outperforming on datasets\nwith simulated and real-world cross-camera spectral variations. The scalability\nand versatility of the proposed approach position our model as a backbone for\nfuture spectral foundation models."}
{"id": "2504.19436", "pdf": "https://arxiv.org/pdf/2504.19436", "abs": "https://arxiv.org/abs/2504.19436", "authors": ["Jacky He", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang", "Hongye Zheng", "Xiaokai Wang"], "title": "Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper focuses on the dynamic optimization of the Retrieval-Augmented\nGeneration (RAG) architecture. It proposes a state-aware dynamic knowledge\nretrieval mechanism to enhance semantic understanding and knowledge scheduling\nefficiency in large language models for open-domain question answering and\ncomplex generation tasks. The method introduces a multi-level perceptive\nretrieval vector construction strategy and a differentiable document matching\npath. These components enable end-to-end joint training and collaborative\noptimization of the retrieval and generation modules. This effectively\naddresses the limitations of static RAG structures in context adaptation and\nknowledge access. Experiments are conducted on the Natural Questions dataset.\nThe proposed structure is thoroughly evaluated across different large models,\nincluding GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments\nfrom multiple perspectives confirm the significant improvements in BLEU and\nROUGE-L scores. The approach also demonstrates stronger robustness and\ngeneration consistency in tasks involving semantic ambiguity and multi-document\nfusion. These results highlight its broad application potential and practical\nvalue in building high-quality language generation systems."}
{"id": "2504.19227", "pdf": "https://arxiv.org/pdf/2504.19227", "abs": "https://arxiv.org/abs/2504.19227", "authors": ["Shalini Maiti", "Lourdes Agapito", "Benjamin Graham"], "title": "Unsupervised 2D-3D lifting of non-rigid objects using local constraints", "categories": ["cs.CV"], "comment": null, "summary": "For non-rigid objects, predicting the 3D shape from 2D keypoint observations\nis ill-posed due to occlusions, and the need to disentangle changes in\nviewpoint and changes in shape. This challenge has often been addressed by\nembedding low-rank constraints into specialized models. These models can be\nhard to train, as they depend on finding a canonical way of aligning\nobservations, before they can learn detailed geometry. These constraints have\nlimited the reconstruction quality. We show that generic, high capacity models,\ntrained with an unsupervised loss, allow for more accurate predicted shapes. In\nparticular, applying low-rank constraints to localized subsets of the full\nshape allows the high capacity to be suitably constrained. We reduce the\nstate-of-the-art reconstruction error on the S-Up3D dataset by over 70%."}
{"id": "2504.19445", "pdf": "https://arxiv.org/pdf/2504.19445", "abs": "https://arxiv.org/abs/2504.19445", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Wei Wang"], "title": "Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in tasks such as\npsychological text analysis and decision-making in automated workflows.\nHowever, their reliability remains a concern due to potential biases inherited\nfrom their training process. In this study, we examine how different response\nformat: binary versus continuous, may systematically influence LLMs' judgments.\nIn a value statement judgments task and a text sentiment analysis task, we\nprompted LLMs to simulate human responses and tested both formats across\nseveral models, including both open-source and commercial models. Our findings\nrevealed a consistent negative bias: LLMs were more likely to deliver\n\"negative\" judgments in binary formats compared to continuous ones. Control\nexperiments further revealed that this pattern holds across both tasks. Our\nresults highlight the importance of considering response format when applying\nLLMs to decision tasks, as small changes in task design can introduce\nsystematic biases."}
{"id": "2504.19244", "pdf": "https://arxiv.org/pdf/2504.19244", "abs": "https://arxiv.org/abs/2504.19244", "authors": ["De Cheng", "Lingfeng He", "Nannan Wang", "Dingwen Zhang", "Xinbo Gao"], "title": "Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID", "categories": ["cs.CV"], "comment": "Accepted by IJCV 2025", "summary": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to\nmatch pedestrian images of the same individual across different modalities\nwithout human annotations for model learning. Previous methods unify\npseudo-labels of cross-modality images through label association algorithms and\nthen design contrastive learning framework for global feature learning.\nHowever, these methods overlook the cross-modality variations in feature\nrepresentation and pseudo-label distributions brought by fine-grained patterns.\nThis insight results in insufficient modality-shared learning when only global\nfeatures are optimized. To address this issue, we propose a Semantic-Aligned\nLearning with Collaborative Refinement (SALCR) framework, which builds up\noptimization objective for specific fine-grained patterns emphasized by each\nmodality, thereby achieving complementary alignment between the label\ndistributions of different modalities. Specifically, we first introduce a Dual\nAssociation with Global Learning (DAGI) module to unify the pseudo-labels of\ncross-modality instances in a bi-directional manner. Afterward, a Fine-Grained\nSemantic-Aligned Learning (FGSAL) module is carried out to explore part-level\nsemantic-aligned patterns emphasized by each modality from cross-modality\ninstances. Optimization objective is then formulated based on the\nsemantic-aligned features and their corresponding label space. To alleviate the\nside-effects arising from noisy pseudo-labels, we propose a Global-Part\nCollaborative Refinement (GPCR) module to mine reliable positive sample sets\nfor the global and part features dynamically and optimize the inter-instance\nrelationships. Extensive experiments demonstrate the effectiveness of the\nproposed method, which achieves superior performances to state-of-the-art\nmethods. Our code is available at\n\\href{https://github.com/FranklinLingfeng/code-for-SALCR}."}
{"id": "2504.19457", "pdf": "https://arxiv.org/pdf/2504.19457", "abs": "https://arxiv.org/abs/2504.19457", "authors": ["Siyi Liu", "Kishaloy Halder", "Zheng Qi", "Wei Xiao", "Nikolaos Pappas", "Phu Mon Htut", "Neha Anna John", "Yassine Benajiba", "Dan Roth"], "title": "Towards Long Context Hallucination Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious tasks. However, they are prone to contextual hallucination, generating\ninformation that is either unsubstantiated or contradictory to the given\ncontext. Although many studies have investigated contextual hallucinations in\nLLMs, addressing them in long-context inputs remains an open problem. In this\nwork, we take an initial step toward solving this problem by constructing a\ndataset specifically designed for long-context hallucination detection.\nFurthermore, we propose a novel architecture that enables pre-trained encoder\nmodels, such as BERT, to process long contexts and effectively detect\ncontextual hallucinations through a decomposition and aggregation mechanism.\nOur experimental results show that the proposed architecture significantly\noutperforms previous models of similar size as well as LLM-based models across\nvarious metrics, while providing substantially faster inference."}
{"id": "2504.19249", "pdf": "https://arxiv.org/pdf/2504.19249", "abs": "https://arxiv.org/abs/2504.19249", "authors": ["Loc Phuc Truong Nguyen", "Hung Truong Thanh Nguyen", "Hung Cao"], "title": "ODExAI: A Comprehensive Object Detection Explainable AI Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) techniques for interpreting object\ndetection models remain in an early stage, with no established standards for\nsystematic evaluation. This absence of consensus hinders both the comparative\nanalysis of methods and the informed selection of suitable approaches. To\naddress this gap, we introduce the Object Detection Explainable AI Evaluation\n(ODExAI), a comprehensive framework designed to assess XAI methods in object\ndetection based on three core dimensions: localization accuracy, faithfulness\nto model behavior, and computational complexity. We benchmark a set of XAI\nmethods across two widely used object detectors (YOLOX and Faster R-CNN) and\nstandard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that\nregion-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%)\nand high model faithfulness (OA = 0.863), though with substantial computational\noverhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME)\nachieve superior localization (PG = 96.13%) and significantly lower runtime\n(Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These\nfindings demonstrate critical trade-offs among existing XAI approaches and\nreinforce the need for task-specific evaluation when deploying them in object\ndetection pipelines. Our implementation and evaluation benchmarks are publicly\navailable at: https://github.com/Analytics-Everywhere-Lab/odexai."}
{"id": "2504.19467", "pdf": "https://arxiv.org/pdf/2504.19467", "abs": "https://arxiv.org/abs/2504.19467", "authors": ["Jiageng Wu", "Bowen Gu", "Ren Zhou", "Kevin Xie", "Doug Snyder", "Yixing Jiang", "Valentina Carducci", "Richard Wyss", "Rishi J Desai", "Emily Alsentzer", "Leo Anthony Celi", "Adam Rodman", "Sebastian Schneeweiss", "Jonathan H. Chen", "Santiago Romero-Brufau", "Kueiyu Joshua Lin", "Jie Yang"], "title": "BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) hold great promise for medical applications and\nare evolving rapidly, with new models being released at an accelerated pace.\nHowever, current evaluations of LLMs in clinical contexts remain limited. Most\nexisting benchmarks rely on medical exam-style questions or PubMed-derived\ntext, failing to capture the complexity of real-world electronic health record\n(EHR) data. Others focus narrowly on specific application scenarios, limiting\ntheir generalizability across broader clinical use. To address this gap, we\npresent BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks\nsourced from real-world clinical data sources across nine languages. We\nsystematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1,\nGPT-4o, Gemini, and Llama 4) under various inference strategies. With a total\nof 13,572 experiments, our results reveal substantial performance variation\nacross model sizes, languages, natural language processing tasks, and clinical\nspecialties. Notably, we demonstrate that open-source LLMs can achieve\nperformance comparable to proprietary models, while medically fine-tuned LLMs\nbased on older architectures often underperform versus updated general-purpose\nmodels. The BRIDGE and its corresponding leaderboard serve as a foundational\nresource and a unique reference for the development and evaluation of new LLMs\nin real-world clinical text understanding."}
{"id": "2504.19256", "pdf": "https://arxiv.org/pdf/2504.19256", "abs": "https://arxiv.org/abs/2504.19256", "authors": ["Songsong Xiong", "Hamidreza Kasaei"], "title": "LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In human-centered environments such as restaurants, homes, and warehouses,\nrobots often face challenges in accurately recognizing 3D objects. These\nchallenges stem from the complexity and variability of these environments,\nincluding diverse object shapes. In this paper, we propose a novel Lightweight\nMulti-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to\nenhance 3D object recognition in robotic applications. Our approach leverages\nthe Globally Entropy-based Embeddings Fusion (GEEF) method to integrate\nmulti-views efficiently. The LM-MCVT architecture incorporates pre- and\nmid-level convolutional encoders and local and global transformers to enhance\nfeature extraction and recognition accuracy. We evaluate our method on the\nsynthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using\na four-view setup, surpassing existing state-of-the-art methods. To further\nvalidate its effectiveness, we conduct 5-fold cross-validation on the\nreal-world OmniObject3D dataset using the same configuration. Results\nconsistently show superior performance, demonstrating the method's robustness\nin 3D object recognition across synthetic and real-world 3D data."}
{"id": "2504.19472", "pdf": "https://arxiv.org/pdf/2504.19472", "abs": "https://arxiv.org/abs/2504.19472", "authors": ["Siyi Liu", "Dan Roth"], "title": "Conflicts in Texts: Data, Implications and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "As NLP models become increasingly integrated into real-world applications, it\nbecomes clear that there is a need to address the fact that models often rely\non and generate conflicting information. Conflicts could reflect the complexity\nof situations, changes that need to be explained and dealt with, difficulties\nin data annotation, and mistakes in generated outputs. In all cases,\ndisregarding the conflicts in data could result in undesired behaviors of\nmodels and undermine NLP models' reliability and trustworthiness. This survey\ncategorizes these conflicts into three key areas: (1) natural texts on the web,\nwhere factual inconsistencies, subjective biases, and multiple perspectives\nintroduce contradictions; (2) human-annotated data, where annotator\ndisagreements, mistakes, and societal biases impact model training; and (3)\nmodel interactions, where hallucinations and knowledge conflicts emerge during\ndeployment. While prior work has addressed some of these conflicts in\nisolation, we unify them under the broader concept of conflicting information,\nanalyze their implications, and discuss mitigation strategies. We highlight key\nchallenges and future directions for developing conflict-aware NLP systems that\ncan reason over and reconcile conflicting information more effectively."}
{"id": "2504.19258", "pdf": "https://arxiv.org/pdf/2504.19258", "abs": "https://arxiv.org/abs/2504.19258", "authors": ["Shuhao Kang", "Martin Y. Liao", "Yan Xia", "Olaf Wysocki", "Boris Jutzi", "Daniel Cremers"], "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion", "categories": ["cs.CV", "cs.RO"], "comment": "Technical report. 15 pages, 9 figures", "summary": "LiDAR place recognition is a critical capability for autonomous navigation\nand cross-modal localization in large-scale outdoor environments. Existing\napproaches predominantly depend on pre-built 3D dense maps or aerial imagery,\nwhich impose significant storage overhead and lack real-time adaptability. In\nthis paper, we propose OPAL, a novel network for LiDAR place recognition that\nleverages OpenStreetMap as a lightweight and up-to-date prior. Our key\ninnovation lies in bridging the domain disparity between sparse LiDAR scans and\nstructured OSM data through two carefully designed components: a cross-modal\nvisibility mask that identifies maximal observable regions from both modalities\nto guide feature learning, and an adaptive radial fusion module that\ndynamically consolidates multiscale radial features into discriminative global\ndescriptors. Extensive experiments on the augmented KITTI and KITTI-360\ndatasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m\nthreshold for top-1 retrieved matches while operating at 12x faster inference\nspeeds compared to state-of-the-art approaches. Code and datasets are publicly\navailable at: https://github.com/WHU-USI3DV/OPAL ."}
{"id": "2504.19556", "pdf": "https://arxiv.org/pdf/2504.19556", "abs": "https://arxiv.org/abs/2504.19556", "authors": ["Kristen Sussman", "Daniel Carter"], "title": "Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment", "categories": ["cs.CL", "cs.HC", "J.4; K.4.0; I.2.7"], "comment": "5 pages, 3 figures, Companion Proceedings of the ACM Web Conference\n  2025", "summary": "Given the subtle human-like effects of large language models on linguistic\npatterns, this study examines shifts in language over time to detect the impact\nof AI-mediated communication (AI- MC) on social media. We compare a replicated\ndataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the\nsame period in 2024, all of which mention Donald Trump during election periods.\nUsing a combination of Flesch-Kincaid readability and polarity scores, we\nanalyze changes in text complexity and sentiment. Our findings reveal a\nsignificant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift\nfrom predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more\npositive expressions (28.6% to 45.9%). These findings suggest not only an\nincreasing presence of AI in social media communication but also its impact on\nlanguage and emotional expression patterns."}
{"id": "2504.19261", "pdf": "https://arxiv.org/pdf/2504.19261", "abs": "https://arxiv.org/abs/2504.19261", "authors": ["Xiaofeng Jin", "Yan Fang", "Matteo Frosi", "Jianfei Ge", "Jiangjian Xiao", "Matteo Matteucci"], "title": "Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting", "categories": ["cs.CV", "65D18, 68U05", "I.3.7; I.4.8"], "comment": "8 pages,8 figures", "summary": "Scene view synthesis, which generates novel views from limited perspectives,\nis increasingly vital for applications like virtual reality, augmented reality,\nand robotics. Unlike object-based tasks, such as generating 360{\\deg} views of\na car, scene view synthesis handles entire environments where non-uniform\nobservations pose unique challenges for stable rendering quality. To address\nthis issue, we propose a novel approach: renderability field-guided gaussian\nsplatting (RF-GS). This method quantifies input inhomogeneity through a\nrenderability field, guiding pseudo-view sampling to enhanced visual\nconsistency. To ensure the quality of wide-baseline pseudo-views, we train an\nimage restoration model to map point projections to visible-light styles.\nAdditionally, our validated hybrid data optimization strategy effectively fuses\ninformation of pseudo-view angles and source view textures. Comparative\nexperiments on simulated and real-world data show that our method outperforms\nexisting approaches in rendering stability."}
{"id": "2504.19565", "pdf": "https://arxiv.org/pdf/2504.19565", "abs": "https://arxiv.org/abs/2504.19565", "authors": ["Meng Xiao", "Xunxin Cai", "Chengrui Wang", "Yuanchun Zhou"], "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "22 pages, Large Language Model, Agentic AI, Dataset Distillation,\n  Multi-agent Collaboration", "summary": "The rapid progress of large language models (LLMs) in biomedical research has\nunderscored the limitations of existing open-source annotated scientific\ncorpora, which are often insufficient in quantity and quality. Addressing the\nchallenge posed by the complex hierarchy of biomedical knowledge, we propose a\nknowledge-driven, multi-agent framework for scientific corpus distillation\ntailored for LLM training in the biomedical domain. Central to our approach is\na collaborative multi-agent architecture, where specialized agents, each guided\nby the Medical Subject Headings (MeSH) hierarchy, work in concert to\nautonomously extract, synthesize, and self-evaluate high-quality textual data\nfrom vast scientific literature. These agents collectively generate and refine\ndomain-specific question-answer pairs, ensuring comprehensive coverage and\nconsistency with biomedical ontologies while minimizing manual involvement.\nExtensive experimental results show that language models trained on our\nmulti-agent distilled datasets achieve notable improvements in biomedical\nquestion-answering tasks, outperforming both strong life sciences LLM baselines\nand advanced proprietary models. Notably, our AI-Ready dataset enables\nLlama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger\nscale. Detailed ablation studies and case analyses further validate the\neffectiveness and synergy of each agent within the framework, highlighting the\npotential of multi-agent collaboration in biomedical LLM training."}
{"id": "2504.19266", "pdf": "https://arxiv.org/pdf/2504.19266", "abs": "https://arxiv.org/abs/2504.19266", "authors": ["Xiaofeng Jin", "Matteo Frosi", "Matteo Matteucci"], "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System", "categories": ["cs.CV", "68T45, 68U05", "I.2.10; I.4.8"], "comment": "8 pages, 9 figures", "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."}
{"id": "2504.19590", "pdf": "https://arxiv.org/pdf/2504.19590", "abs": "https://arxiv.org/abs/2504.19590", "authors": ["Israa Alsiyat"], "title": "Arabic Metaphor Sentiment Classification Using Semantic Information", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1]\nusing newly designed automatic tools for sentiment classification for AMC based\non semantic tags. The tool incorporates semantic emotional tags for sentiment\nclassification. I evaluate the tool using standard methods, which are F-score,\nrecall, and precision. The method is to show the impact of Arabic online\nmetaphors on sentiment through the newly designed tools. To the best of our\nknowledge, this is the first approach to conduct sentiment classification for\nArabic metaphors using semantic tags to find the impact of the metaphor."}
{"id": "2504.19270", "pdf": "https://arxiv.org/pdf/2504.19270", "abs": "https://arxiv.org/abs/2504.19270", "authors": ["Chamin Hewa Koneputugodage", "Yizhak Ben-Shabat", "Sameera Ramasinghe", "Stephen Gould"], "title": "VI3NR: Variance Informed Initialization for Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Implicit Neural Representations (INRs) are a versatile and powerful tool for\nencoding various forms of data, including images, videos, sound, and 3D shapes.\nA critical factor in the success of INRs is the initialization of the network,\nwhich can significantly impact the convergence and accuracy of the learned\nmodel. Unfortunately, commonly used neural network initializations are not\nwidely applicable for many activation functions, especially those used by INRs.\nIn this paper, we improve upon previous initialization methods by deriving an\ninitialization that has stable variance across layers, and applies to any\nactivation function. We show that this generalizes many previous initialization\nmethods, and has even better stability for well studied activations. We also\nshow that our initialization leads to improved results with INR activation\nfunctions in multiple signal modalities. Our approach is particularly effective\nfor Gaussian INRs, where we demonstrate that the theory of our initialization\nmatches with task performance in multiple experiments, allowing us to achieve\nimprovements in image, audio, and 3D surface reconstruction."}
{"id": "2504.19606", "pdf": "https://arxiv.org/pdf/2504.19606", "abs": "https://arxiv.org/abs/2504.19606", "authors": ["Hieu-Dai Tran", "Duc-Vu Nguyen", "Ngan Luu-Thuy Nguyen"], "title": "Coreference Resolution for Vietnamese Narrative Texts", "categories": ["cs.CL"], "comment": "Accepted at PACLIC 2024", "summary": "Coreference resolution is a vital task in natural language processing (NLP)\nthat involves identifying and linking different expressions in a text that\nrefer to the same entity. This task is particularly challenging for Vietnamese,\na low-resource language with limited annotated datasets. To address these\nchallenges, we developed a comprehensive annotated dataset using narrative\ntexts from VnExpress, a widely-read Vietnamese online news platform. We\nestablished detailed guidelines for annotating entities, focusing on ensuring\nconsistency and accuracy. Additionally, we evaluated the performance of large\nlanguage models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset.\nOur results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in\nterms of both accuracy and response consistency, making it a more reliable tool\nfor coreference resolution in Vietnamese."}
{"id": "2504.19271", "pdf": "https://arxiv.org/pdf/2504.19271", "abs": "https://arxiv.org/abs/2504.19271", "authors": ["Athul M. Mathew", "Arshad Ali Khan", "Thariq Khalid", "Faroq AL-Tam", "Riad Souissi"], "title": "Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection", "categories": ["cs.CV"], "comment": "accepted at NeurIPS 2023 Gaze Meets ML Workshop", "summary": "Gaze target detection (GTD) is the task of predicting where a person in an\nimage is looking. This is a challenging task, as it requires the ability to\nunderstand the relationship between the person's head, body, and eyes, as well\nas the surrounding environment. In this paper, we propose a novel method for\nGTD that fuses multiple pieces of information extracted from an image. First,\nwe project the 2D image into a 3D representation using monocular depth\nestimation. We then extract a depth-infused saliency module map, which\nhighlights the most salient (\\textit{attention-grabbing}) regions in image for\nthe subject in consideration. We also extract face and depth modalities from\nthe image, and finally fuse all the extracted modalities to identify the gaze\ntarget. We quantitatively evaluated our method, including the ablation analysis\non three publicly available datasets, namely VideoAttentionTarget, GazeFollow\nand GOO-Real, and showed that it outperforms other state-of-the-art methods.\nThis suggests that our method is a promising new approach for GTD."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19279", "pdf": "https://arxiv.org/pdf/2504.19279", "abs": "https://arxiv.org/abs/2504.19279", "authors": ["Vita V. Vlasova", "Vladimir G. Kuzmin", "Maria S. Varetsa", "Natalia A. Ibragimova", "Oleg Y. Rogov", "Elena V. Lyapuntsova"], "title": "Optimal Hyperspectral Undersampling Strategy for Satellite Imaging", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Hyperspectral image (HSI) classification presents significant challenges due\nto the high dimensionality, spectral redundancy, and limited labeled data\ntypically available in real-world applications. To address these issues and\noptimize classification performance, we propose a novel band selection strategy\nknown as Iterative Wavelet-based Gradient Sampling (IWGS). This method\nincrementally selects the most informative spectral bands by analyzing\ngradients within the wavelet-transformed domain, enabling efficient and\ntargeted dimensionality reduction. Unlike traditional selection methods, IWGS\nleverages the multi-resolution properties of wavelets to better capture subtle\nspectral variations relevant for classification. The iterative nature of the\napproach ensures that redundant or noisy bands are systematically excluded\nwhile maximizing the retention of discriminative features. We conduct\ncomprehensive experiments on two widely-used benchmark HSI datasets: Houston\n2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms\nstate-of-the-art band selection and classification techniques in terms of both\naccuracy and computational efficiency. These improvements make our method\nespecially suitable for deployment in edge devices or other\nresource-constrained environments, where memory and processing power are\nlimited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian\nPines for selected classes, confirming its effectiveness and generalizability\nacross different HSI scenarios."}
{"id": "2504.19645", "pdf": "https://arxiv.org/pdf/2504.19645", "abs": "https://arxiv.org/abs/2504.19645", "authors": ["Shadan Shukr Sabr", "Nazira Sabr Mustafa", "Talar Sabah Omar", "Salah Hwayyiz Rasool", "Nawzad Anwer Omer", "Darya Sabir Hamad", "Hemin Abdulhameed Shams", "Omer Mahmood Kareem", "Rozhan Noori Abdullah", "Khabat Atar Abdullah", "Mahabad Azad Mohammad", "Haneen Al-Raghefy", "Safar M. Asaad", "Sara Jamal Mohammed", "Twana Saeed Ali", "Fazil Shawrow", "Halgurd S. Maghdid"], "title": "A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks", "categories": ["cs.CL", "cs.AI", "K.5; K.7; J.7"], "comment": "25 pages, 4 figures, 2 tables", "summary": "- The field of natural language processing (NLP) has dramatically expanded\nwithin the last decade. Many human-being applications are conducted daily via\nNLP tasks, starting from machine translation, speech recognition, text\ngeneration and recommendations, Part-of-Speech tagging (POS), and Named-Entity\nRecognition (NER). However, low-resourced languages, such as the\nCentral-Kurdish language (CKL), mainly remain unexamined due to shortage of\nnecessary resources to support their development. The POS tagging task is the\nbase of other NLP tasks; for example, the POS tag set has been used to\nstandardized languages to provide the relationship between words among the\nsentences, followed by machine translation and text recommendation.\nSpecifically, for the CKL, most of the utilized or provided POS tagsets are\nneither standardized nor comprehensive. To this end, this study presented an\naccurate and comprehensive POS tagset for the CKL to provide better performance\nof the Kurdish NLP tasks. The article also collected most of the POS tags from\ndifferent studies as well as from Kurdish linguistic experts to standardized\npart-of-speech tags. The proposed POS tagset is designed to annotate a large\nCKL corpus and support Kurdish NLP tasks. The initial investigations of this\nstudy via comparison with the Universal Dependencies framework for standard\nlanguages, show that the proposed POS tagset can streamline or correct\nsentences more accurately for Kurdish NLP tasks."}
{"id": "2504.19289", "pdf": "https://arxiv.org/pdf/2504.19289", "abs": "https://arxiv.org/abs/2504.19289", "authors": ["Alexandra Malyugina", "Guoxi Huang", "Eduardo Ruiz", "Benjamin Leslie", "Nantheera Anantrasirichai"], "title": "Marine Snow Removal Using Internally Generated Pseudo Ground Truth", "categories": ["cs.CV"], "comment": null, "summary": "Underwater videos often suffer from degraded quality due to light absorption,\nscattering, and various noise sources. Among these, marine snow, which is\nsuspended organic particles appearing as bright spots or noise, significantly\nimpacts machine vision tasks, particularly those involving feature matching.\nExisting methods for removing marine snow are ineffective due to the lack of\npaired training data. To address this challenge, this paper proposes a novel\nenhancement framework that introduces a new approach for generating paired\ndatasets from raw underwater videos. The resulting dataset consists of paired\nimages of generated snowy and snow, free underwater videos, enabling supervised\ntraining for video enhancement. We describe the dataset creation process,\nhighlight its key characteristics, and demonstrate its effectiveness in\nenhancing underwater image restoration in the absence of ground truth."}
{"id": "2504.19669", "pdf": "https://arxiv.org/pdf/2504.19669", "abs": "https://arxiv.org/abs/2504.19669", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song"], "title": "Multimodal Conditioned Diffusive Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion models achieve remarkable success in processing images and text,\nand have been extended to special domains such as time series forecasting\n(TSF). Existing diffusion-based approaches for TSF primarily focus on modeling\nsingle-modality numerical sequences, overlooking the rich multimodal\ninformation in time series data. To effectively leverage such information for\nprediction, we propose a multimodal conditioned diffusion model for TSF,\nnamely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for\ntime series modeling, especially for forecasting. Specifically, Timestamps are\ncombined with time series to establish temporal and semantic correlations among\ndifferent data points when aggregating information along the temporal\ndimension. Texts serve as supplementary descriptions of time series' history,\nand adaptively aligned with data points as well as dynamically controlled in a\nclassifier-free manner. Extensive experiments on real-world benchmark datasets\nacross eight domains demonstrate that the proposed MCD-TSF model achieves\nstate-of-the-art performance."}
{"id": "2504.19295", "pdf": "https://arxiv.org/pdf/2504.19295", "abs": "https://arxiv.org/abs/2504.19295", "authors": ["Kangbiao Shi", "Yixu Feng", "Tao Hu", "Yu Cao", "Peng Wu", "Yijin Liang", "Yanning Zhang", "Qingsen Yan"], "title": "FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "The advent of Deep Neural Networks (DNNs) has driven remarkable progress in\nlow-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and\nTransformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive\nresults. Recent efforts have sought to leverage the complementary strengths of\nthese paradigms, offering promising solutions to enhance performance across\nvarying degradation scenarios. However, existing fusion strategies are hindered\nby challenges such as parameter explosion, optimization instability, and\nfeature misalignment, limiting further improvements. To overcome these issues,\nwe introduce FusionNet, a novel multi-model linear fusion framework that\noperates in parallel to effectively capture global and local features across\ndiverse color spaces. By incorporating a linear fusion strategy underpinned by\nHilbert space theoretical guarantees, FusionNet mitigates network collapse and\nreduces excessive training costs. Our method achieved 1st place in the CVPR2025\nNTIRE Low Light Enhancement Challenge. Extensive experiments conducted on\nsynthetic and real-world benchmark datasets demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods in terms of both\nquantitative and qualitative results, delivering robust enhancement under\ndiverse low-light conditions."}
{"id": "2504.19675", "pdf": "https://arxiv.org/pdf/2504.19675", "abs": "https://arxiv.org/abs/2504.19675", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "title": "Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.IR", "cs.LG", "I.2.7"], "comment": "6 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects),\nwhich focussed on subject indexing using large language models (LLMs). The task\nrequired creating subject predictions for bibliographic records from the\nbilingual TIBKAT database using the GND subject vocabulary. Our approach\ncombines traditional natural language processing and machine learning\ntechniques implemented in the Annif toolkit with innovative LLM-based methods\nfor translation and synthetic data generation, and merging predictions from\nmonolingual models. The system ranked first in the all-subjects category and\nsecond in the tib-core-subjects category in the quantitative evaluation, and\nfourth in qualitative evaluations. These findings demonstrate the potential of\ncombining traditional XMTC algorithms with modern LLM techniques to improve the\naccuracy and efficiency of subject indexing in multilingual contexts."}
{"id": "2504.19300", "pdf": "https://arxiv.org/pdf/2504.19300", "abs": "https://arxiv.org/abs/2504.19300", "authors": ["Ni Yao", "Xiangyu Liu", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Chengyang Li", "Fubao Zhu", "Weihua Zhou", "Chen Zhao"], "title": "Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography", "categories": ["cs.CV"], "comment": "31 pages, 12 figures", "summary": "Coronary artery disease (CAD) remains a leading cause of mortality worldwide,\nrequiring accurate segmentation and stenosis detection using Coronary Computed\nTomography angiography (CCTA). Existing methods struggle with challenges such\nas low contrast, morphological variability and small vessel segmentation. To\naddress these limitations, we propose the Myocardial Region-guided Feature\nAggregation Net, a novel U-shaped dual-encoder architecture that integrates\nanatomical prior knowledge to enhance robustness in coronary artery\nsegmentation. Our framework incorporates three key innovations: (1) a\nMyocardial Region-guided Module that directs attention to coronary regions via\nmyocardial contour expansion and multi-scale feature fusion, (2) a Residual\nFeature Extraction Encoding Module that combines parallel spatial channel\nattention with residual blocks to enhance local-global feature discrimination,\nand (3) a Multi-scale Feature Fusion Module for adaptive aggregation of\nhierarchical vascular features. Additionally, Monte Carlo dropout f quantifies\nprediction uncertainty, supporting clinical interpretability. For stenosis\ndetection, a morphology-based centerline extraction algorithm separates the\nvascular tree into anatomical branches, enabling cross-sectional area\nquantification and stenosis grading. The superiority of MGFA-Net was\ndemonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an\nHD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for\nstenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis\npipeline provides automated, clinically interpretable CAD assessment, bridging\ndeep learning with anatomical prior knowledge for precision medicine. Our code\nis publicly available at http://github.com/chenzhao2023/MGFA_CCTA"}
{"id": "2504.19720", "pdf": "https://arxiv.org/pdf/2504.19720", "abs": "https://arxiv.org/abs/2504.19720", "authors": ["Ranran Zhen", "Juntao Li", "Yixin Ji", "Zhenlin Yang", "Tong Liu", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": "work in progress;11 pages of main paper with 7 main figures, overall\n  20 pages", "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable\nprogress, evolving into sophisticated and versatile tools widely adopted across\nvarious domains and applications. However, the substantial memory overhead\ncaused by their vast number of parameters, combined with the high computational\ndemands of the attention mechanism, poses significant challenges in achieving\nlow latency and high throughput for LLM inference services. Recent\nadvancements, driven by groundbreaking research, have significantly accelerated\nprogress in this field. This paper provides a comprehensive survey of these\nmethods, covering fundamental instance-level approaches, in-depth cluster-level\nstrategies, emerging scenario directions, and other miscellaneous but important\nareas. At the instance level, we review model placement, request scheduling,\ndecoding length prediction, storage management, and the disaggregation\nparadigm. At the cluster level, we explore GPU cluster deployment,\nmulti-instance load balancing, and cloud service solutions. For emerging\nscenarios, we organize the discussion around specific tasks, modules, and\nauxiliary methods. To ensure a holistic overview, we also highlight several\nniche yet critical areas. Finally, we outline potential research directions to\nfurther advance the field of LLM inference serving."}
{"id": "2504.19327", "pdf": "https://arxiv.org/pdf/2504.19327", "abs": "https://arxiv.org/abs/2504.19327", "authors": ["Moulik Choraria", "Xinbo Wu", "Akhil Bhimaraju", "Nitesh Sekhar", "Yue Wu", "Xu Zhang", "Prateek Singhal", "Lav R. Varshney"], "title": "Platonic Grounding for Efficient Multimodal Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The hyperscaling of data and parameter count in Transformer-based models is\nyielding diminishing performance improvement, especially when weighed against\ntraining costs. Such plateauing indicates the importance of methods for more\nefficient finetuning and inference, while retaining similar performance. This\nis especially relevant for multimodal learning paradigms, where inference costs\nof processing multimodal tokens can determine the model's practical viability.\nAt the same time, research on representations and mechanistic interpretability\nhas improved our understanding of the inner workings of Transformer-based\nmodels; one such line of work reveals an implicit alignment in the deeper\nlayers of pretrained models, across modalities. Taking inspiration from this,\nwe motivate and propose a simple modification to existing multimodal frameworks\nthat rely on aligning pretrained models. We demonstrate that our approach\nmaintains and, in some cases, even improves performance of baseline methods\nwhile achieving significant gains in both training and inference-time compute.\nOur work also has implications for combining pretrained models into larger\nsystems efficiently."}
{"id": "2504.19734", "pdf": "https://arxiv.org/pdf/2504.19734", "abs": "https://arxiv.org/abs/2504.19734", "authors": ["Ying Na", "Shihui Feng"], "title": "LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Dialogue data has been a key source for understanding learning processes,\noffering critical insights into how students engage in collaborative\ndiscussions and how these interactions shape their knowledge construction. The\nadvent of Large Language Models (LLMs) has introduced promising opportunities\nfor advancing qualitative research, particularly in the automated coding of\ndialogue data. However, the inherent contextual complexity of dialogue presents\nunique challenges for these models, especially in understanding and\ninterpreting complex contextual information. This study addresses these\nchallenges by developing a novel LLM-assisted automated coding approach for\ndialogue data. The novelty of our proposed framework is threefold: 1) We\npredict the code for an utterance based on dialogue-specific characteristics --\ncommunicative acts and communicative events -- using separate prompts following\nthe role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs\nincluding GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We\nleveraged the interrelation between events and acts to implement consistency\nchecking using GPT-4o. In particular, our contextual consistency checking\nprovided a substantial accuracy improvement. We also found the accuracy of act\npredictions was consistently higher than that of event predictions. This study\ncontributes a new methodological framework for enhancing the precision of\nautomated coding of dialogue data as well as offers a scalable solution for\naddressing the contextual challenges inherent in dialogue analysis."}
{"id": "2504.19334", "pdf": "https://arxiv.org/pdf/2504.19334", "abs": "https://arxiv.org/abs/2504.19334", "authors": ["Sidharth Rai", "Aryan Dalal", "Riley Slichter", "Ajay Sharda"], "title": "Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time", "categories": ["cs.CV"], "comment": null, "summary": "Effective seed sowing in precision agriculture is hindered by challenges such\nas residue accumulation, low soil temperatures, and hair pinning (crop residue\npushed in the trench by furrow opener), which obstruct optimal trench\nformation. Row cleaners are employed to mitigate these issues, but there is a\nlack of quantitative methods to assess trench cleanliness. In this study, a\nnovel computer vision-based method was developed to evaluate row cleaner\nperformance. Multiple air seeders were equipped with a video acquisition system\nto capture trench conditions after row cleaner operation, enabling an effective\ncomparison of the performance of each row cleaner. The captured data were used\nto develop a segmentation model that analyzed key elements such as soil, straw,\nand machinery. Using the results from the segmentation model, an objective\nmethod was developed to quantify row cleaner performance. The results\ndemonstrated the potential of this method to improve row cleaner selection and\nenhance seeding efficiency in precision agriculture."}
{"id": "2504.19759", "pdf": "https://arxiv.org/pdf/2504.19759", "abs": "https://arxiv.org/abs/2504.19759", "authors": ["Huichi Zhou", "Zehao Xu", "Munan Zhao", "Kaihong Li", "Yiqiang Li", "Hongtao Wang"], "title": "Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs", "categories": ["cs.CL"], "comment": "5 pages, 2 figures", "summary": "In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB)\nto evaluate the moral reasoning abilities of large language models (LLMs)\nacross five typologically diverse languages and three levels of contextual\ncomplexity: sentence, paragraph, and document. Our results show moral reasoning\nperformance degrades with increasing context complexity, particularly for\nlow-resource languages such as Vietnamese. We further fine-tune the open-source\nLLaMA-3-8B model using curated monolingual data for alignment and poisoning.\nSurprisingly, low-resource languages have a stronger impact on multilingual\nreasoning than high-resource ones, highlighting their critical role in\nmultilingual NLP."}
{"id": "2504.19347", "pdf": "https://arxiv.org/pdf/2504.19347", "abs": "https://arxiv.org/abs/2504.19347", "authors": ["Rayson Laroca", "Marcelo dos Santos", "David Menotti"], "title": "Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation", "categories": ["cs.CV"], "comment": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "Detecting small drones, often indistinguishable from birds, is crucial for\nmodern surveillance. This work introduces a drone detection methodology built\nupon the medium-sized YOLOv11 object detection model. To enhance its\nperformance on small targets, we implemented a multi-scale approach in which\nthe input image is processed both as a whole and in segmented parts, with\nsubsequent prediction aggregation. We also utilized a copy-paste data\naugmentation technique to enrich the training dataset with diverse drone and\nbird examples. Finally, we implemented a post-processing technique that\nleverages frame-to-frame consistency to mitigate missed detections. The\nproposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird\nDetection Grand Challenge, held at the 2025 International Joint Conference on\nNeural Networks (IJCNN), showcasing its capability to detect drones in complex\nenvironments effectively."}
{"id": "2504.19811", "pdf": "https://arxiv.org/pdf/2504.19811", "abs": "https://arxiv.org/abs/2504.19811", "authors": ["Takuya Tamura", "Taro Yano", "Masafumi Enomoto", "Masafumi Oyamada"], "title": "Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance", "categories": ["cs.CL"], "comment": null, "summary": "Accurately forecasting the performance of Large Language Models (LLMs) before\nextensive fine-tuning or merging can substantially reduce both computational\nexpense and development time. Although prior approaches like scaling laws\naccount for global factors such as parameter size or training tokens, they\noften overlook explicit lineage relationships - i.e., which models are derived\nor merged from which parents. In this work, we propose a novel\nLineage-Regularized Matrix Factorization (LRMF) framework that encodes\nancestral ties among LLMs via a graph Laplacian regularizer. By leveraging\nmulti-hop parent-child connections, LRMF consistently outperforms conventional\nmatrix factorization and collaborative filtering methods in both instance-level\nand benchmark-level performance prediction. Our large-scale study includes\n2,934 publicly available Hugging Face models and 21,000+ instances across 6\nmajor benchmarks, showing that lineage constraints yield up to 7-10 percentage\npoints higher correlation with actual performance compared to baselines.\nMoreover, LRMF effectively addresses the cold-start problem, providing accurate\nestimates for newly derived or merged models even with minimal data. This\nlineage-guided strategy thus offers a resource-efficient way to inform\nhyperparameter tuning, data selection, and model combination in modern LLM\ndevelopment."}
{"id": "2504.19357", "pdf": "https://arxiv.org/pdf/2504.19357", "abs": "https://arxiv.org/abs/2504.19357", "authors": ["Jiahao Lu", "Chong Yin", "Silvia Ingala", "Kenny Erleben", "Michael Bachmann Nielsen", "Sune Darkner"], "title": "MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Lung cancer, a leading cause of cancer-related deaths globally, emphasises\nthe importance of early detection for better patient outcomes. Pulmonary\nnodules, often early indicators of lung cancer, necessitate accurate, timely\ndiagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many\nexisting systems struggle providing clear, comprehensive explanations,\nespecially with limited labelled data. This study introduces MERA, a Multimodal\nand Multiscale self-Explanatory model designed for lung nodule diagnosis with\nconsiderably Reduced Annotation requirements. MERA integrates unsupervised and\nweakly supervised learning strategies (self-supervised learning techniques and\nVision Transformer architecture for unsupervised feature extraction) and a\nhierarchical prediction mechanism leveraging sparse annotations via\nsemi-supervised active learning in the learned latent space. MERA explains its\ndecisions on multiple levels: model-level global explanations via semantic\nlatent space clustering, instance-level case-based explanations showing similar\ninstances, local visual explanations via attention maps, and concept\nexplanations using critical nodule attributes. Evaluations on the public LIDC\ndataset show MERA's superior diagnostic accuracy and self-explainability. With\nonly 1% annotated samples, MERA achieves diagnostic accuracy comparable to or\nexceeding state-of-the-art methods requiring full annotation. The model's\ninherent design delivers comprehensive, robust, multilevel explanations aligned\nclosely with clinical practice, enhancing trustworthiness and transparency.\nDemonstrated viability of unsupervised and weakly supervised learning lowers\nthe barrier to deploying diagnostic AI in broader medical domains. Our complete\ncode is open-source available: https://github.com/diku-dk/credanno."}
{"id": "2504.19850", "pdf": "https://arxiv.org/pdf/2504.19850", "abs": "https://arxiv.org/abs/2504.19850", "authors": ["Kyo Gerrits", "Ana Guerberof-Arenas"], "title": "To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This article presents the results of a pilot study involving the reception of\na fictional short story translated from English into Dutch under four\nconditions: machine translation (MT), post-editing (PE), human translation (HT)\nand original source text (ST). The aim is to understand how creativity and\nerrors in different translation modalities affect readers, specifically\nregarding cognitive load. Eight participants filled in a questionnaire, read a\nstory using an eye-tracker, and conducted a retrospective think-aloud (RTA)\ninterview. The results show that units of creative potential (UCP) increase\ncognitive load and that this effect is highest for HT and lowest for MT; no\neffect of error was observed. Triangulating the data with RTAs leads us to\nhypothesize that the higher cognitive load in UCPs is linked to increases in\nreader enjoyment and immersion. The effect of translation creativity on\ncognitive load in different translation modalities at word-level is novel and\nopens up new avenues for further research. All the code and data are available\nat https://github.com/INCREC/Pilot_to_MT_or_not_to_MT"}
{"id": "2504.19370", "pdf": "https://arxiv.org/pdf/2504.19370", "abs": "https://arxiv.org/abs/2504.19370", "authors": ["Jean-Rémy Conti", "Stéphan Clémençon"], "title": "Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "comment": "Accepted at both the AFME and RegML Workshops at NeurIPS 2024. A\n  preliminary version has been accepted for publication by Springer Nature, in\n  the context of the ICPR 2024 conference", "summary": "The urging societal demand for fair AI systems has put pressure on the\nresearch community to develop predictive models that are not only globally\naccurate but also meet new fairness criteria, reflecting the lack of disparate\nmistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender,\nethnicity, age). In particular, the variability of the errors made by certain\nFacial Recognition (FR) systems across specific segments of the population\ncompromises the deployment of the latter, and was judged unacceptable by\nregulatory authorities. Designing fair FR systems is a very challenging\nproblem, mainly due to the complex and functional nature of the performance\nmeasure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the\nhuge heterogeneity of the face image datasets usually available for training.\nIn this paper, we propose a novel post-processing approach to improve the\nfairness of pre-trained FR models by optimizing a regression loss which acts on\ncentroid-based scores. Beyond the computational advantages of the method, we\npresent numerical experiments providing strong empirical evidence of the gain\nin fairness and of the ability to preserve global accuracy."}
{"id": "2504.19856", "pdf": "https://arxiv.org/pdf/2504.19856", "abs": "https://arxiv.org/abs/2504.19856", "authors": ["Anastasia Zhukova", "Christian E. Matt", "Terry Ruas", "Bela Gipp"], "title": "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language", "categories": ["cs.CL"], "comment": null, "summary": "Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique\nthat further trains a language model (LM) on its pretraining task, e.g.,\nlanguage masking. Although popular, it requires a significant corpus of\ndomain-related data, which is difficult to obtain for specific domains in\nlanguages other than English, such as the process industry in the German\nlanguage. This paper introduces an efficient approach called ICL-augmented\npretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest\nneighbors (kNN) to augment target data with domain-related and in-domain texts,\nsignificantly reducing GPU time while maintaining strong model performance. Our\nresults show that this approach performs better than traditional DAPT by 3.5 of\nthe average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times\nless computing time, providing a cost-effective solution for industries with\nlimited computational capacity. The findings highlight the broader\napplicability of this framework to other low-resource industries, making\nNLP-based solutions more accessible and feasible in production environments."}
{"id": "2504.19390", "pdf": "https://arxiv.org/pdf/2504.19390", "abs": "https://arxiv.org/abs/2504.19390", "authors": ["Jakub Zadrożny", "Hakan Bilen"], "title": "HumMorph: Generalized Dynamic Human Neural Fields from Few Views", "categories": ["cs.CV"], "comment": "Project page: https://jakubzadrozny.github.io/hummorph", "summary": "We introduce HumMorph, a novel generalized approach to free-viewpoint\nrendering of dynamic human bodies with explicit pose control. HumMorph renders\na human actor in any specified pose given a few observed views (starting from\njust one) in arbitrary poses. Our method enables fast inference as it relies\nonly on feed-forward passes through the model. We first construct a coarse\nrepresentation of the actor in the canonical T-pose, which combines visual\nfeatures from individual partial observations and fills missing information\nusing learned prior knowledge. The coarse representation is complemented by\nfine-grained pixel-aligned features extracted directly from the observed views,\nwhich provide high-resolution appearance information. We show that HumMorph is\ncompetitive with the state-of-the-art when only a single input view is\navailable, however, we achieve results with significantly better visual quality\ngiven just 2 monocular observations. Moreover, previous generalized methods\nassume access to accurate body shape and pose parameters obtained using\nsynchronized multi-camera setups. In contrast, we consider a more practical\nscenario where these body parameters are noisily estimated directly from the\nobserved views. Our experimental results demonstrate that our architecture is\nmore robust to errors in the noisy parameters and clearly outperforms the state\nof the art in this setting."}
{"id": "2504.19867", "pdf": "https://arxiv.org/pdf/2504.19867", "abs": "https://arxiv.org/abs/2504.19867", "authors": ["Ke Hong", "Lufang Chen", "Zhong Wang", "Xiuhong Li", "Qiuli Mao", "Jianping Ma", "Chao Xiong", "Guanyu Wu", "Buhe Han", "Guohao Dai", "Yun Liang", "Yu Wang"], "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "18 pages, 16 figures", "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."}
{"id": "2504.19398", "pdf": "https://arxiv.org/pdf/2504.19398", "abs": "https://arxiv.org/abs/2504.19398", "authors": ["Shuo Wang", "Weili Shi", "Shuai Yang", "Jiahao Cui", "Qinwei Guo"], "title": "Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture", "categories": ["cs.CV", "I.4.9; I.2.10; J.3; I.4.8; I.5.4"], "comment": "28 pages, 13 figures", "summary": "This paper presents a dynamic arthroscopic navigation system based on\nmulti-level memory architecture for anterior cruciate ligament (ACL)\nreconstruction surgery. The system extends our previously proposed markerless\nnavigation method from static image matching to dynamic video sequence\ntracking. By integrating the Atkinson-Shiffrin memory model's three-level\narchitecture (sensory memory, working memory, and long-term memory), our system\nmaintains continuous tracking of the femoral condyle throughout the surgical\nprocedure, providing stable navigation support even in complex situations\ninvolving viewpoint changes, instrument occlusion, and tissue deformation.\nUnlike existing methods, our system operates in real-time on standard\narthroscopic equipment without requiring additional tracking hardware,\nachieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold\nimprovement over our previous static system. For extended sequences (1000\nframes), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels,\ncompared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of\napproximately 45 percent. For medium-length sequences (500 frames) and short\nsequences (100 frames), the system achieved approximately 35 percent and 19\npercent accuracy improvements, respectively. Experimental results demonstrate\nthe system overcomes limitations of traditional static matching methods,\nproviding new technical support for improving surgical precision in ACL\nreconstruction."}
{"id": "2504.19898", "pdf": "https://arxiv.org/pdf/2504.19898", "abs": "https://arxiv.org/abs/2504.19898", "authors": ["Mingqian He", "Fei Zhao", "Chonggang Lu", "Ziyan Liu", "Yue Wang", "Haofu Qian"], "title": "GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets", "categories": ["cs.CL"], "comment": null, "summary": "As a fundamental task in machine learning, text classification plays a\ncrucial role in many areas. With the rapid scaling of Large Language Models\n(LLMs), particularly through reinforcement learning (RL), there is a growing\nneed for more capable discriminators. Consequently, advances in classification\nare becoming increasingly vital for enhancing the overall capabilities of LLMs.\nTraditional discriminative methods map text to labels but overlook LLMs'\nintrinsic generative strengths. Generative classification addresses this by\nprompting the model to directly output labels. However, existing studies still\nrely on simple SFT alone, seldom probing the interplay between training and\ninference prompts, and no work has systematically leveraged RL for generative\ntext classifiers and unified SFT, RL, and inference-time prompting in one\nframework. We bridge this gap with GenCLS++, a framework that jointly optimizes\nSFT and RL while systematically exploring five high-level strategy\ndimensions-in-context learning variants, category definitions, explicit\nuncertainty labels, semantically irrelevant numeric labels, and\nperplexity-based decoding-during both training and inference. After an SFT\n\"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable\nextra gains. Across seven datasets, GenCLS++ achieves an average accuracy\nimprovement of 3.46% relative to the naive SFT baseline; on public datasets,\nthis improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that\nbenefit from explicit thinking processes, we find that classification tasks\nperform better without such reasoning steps. These insights into the role of\nexplicit reasoning provide valuable guidance for future LLM applications."}
{"id": "2504.19402", "pdf": "https://arxiv.org/pdf/2504.19402", "abs": "https://arxiv.org/abs/2504.19402", "authors": ["Khoa Tuan Nguyen", "Francesca Tozzi", "Wouter Willaert", "Joris Vankerschaver", "Nikdokht Rashidian", "Wesley De Neve"], "title": "Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "While the availability of open 3D medical shape datasets is increasing,\noffering substantial benefits to the research community, we have found that\nmany of these datasets are, unfortunately, disorganized and contain artifacts.\nThese issues limit the development and training of robust models, particularly\nfor accurate 3D reconstruction tasks. In this paper, we examine the current\nstate of available 3D liver shape datasets and propose a solution using\ndiffusion models combined with implicit neural representations (INRs) to\naugment and expand existing datasets. Our approach utilizes the generative\ncapabilities of diffusion models to create realistic, diverse 3D liver shapes,\ncapturing a wide range of anatomical variations and addressing the problem of\ndata scarcity. Experimental results indicate that our method enhances dataset\ndiversity, providing a scalable solution to improve the accuracy and\nreliability of 3D liver reconstruction and generation in medical applications.\nFinally, we suggest that diffusion models can also be applied to other\ndownstream tasks in 3D medical imaging."}
{"id": "2504.19940", "pdf": "https://arxiv.org/pdf/2504.19940", "abs": "https://arxiv.org/abs/2504.19940", "authors": ["Luigia Costabile", "Gian Marco Orlando", "Valerio La Gatta", "Vincenzo Moscato"], "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "The growing spread of online misinformation has created an urgent need for\nscalable, reliable fact-checking solutions. Crowdsourced fact-checking - where\nnon-experts evaluate claim veracity - offers a cost-effective alternative to\nexpert verification, despite concerns about variability in quality and bias.\nEncouraged by promising results in certain contexts, major platforms such as X\n(formerly Twitter), Facebook, and Instagram have begun shifting from\ncentralized moderation to decentralized, crowd-based approaches.\n  In parallel, advances in Large Language Models (LLMs) have shown strong\nperformance across core fact-checking tasks, including claim detection and\nevidence evaluation. However, their potential role in crowdsourced workflows\nremains unexplored. This paper investigates whether LLM-powered generative\nagents - autonomous entities that emulate human behavior and decision-making -\ncan meaningfully contribute to fact-checking tasks traditionally reserved for\nhuman crowds. Using the protocol of La Barbera et al. (2024), we simulate\ncrowds of generative agents with diverse demographic and ideological profiles.\nAgents retrieve evidence, assess claims along multiple quality dimensions, and\nissue final veracity judgments.\n  Our results show that agent crowds outperform human crowds in truthfulness\nclassification, exhibit higher internal consistency, and show reduced\nsusceptibility to social and cognitive biases. Compared to humans, agents rely\nmore systematically on informative criteria such as Accuracy, Precision, and\nInformativeness, suggesting a more structured decision-making process. Overall,\nour findings highlight the potential of generative agents as scalable,\nconsistent, and less biased contributors to crowd-based fact-checking systems."}
{"id": "2504.19414", "pdf": "https://arxiv.org/pdf/2504.19414", "abs": "https://arxiv.org/abs/2504.19414", "authors": ["Sehyeong Jo", "Gangjae Jang", "Haesol Park"], "title": "GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability", "categories": ["cs.CV"], "comment": null, "summary": "The Vision Transformer (ViT) has made significant advancements in computer\nvision, utilizing self-attention mechanisms to achieve state-of-the-art\nperformance across various tasks, including image classification, object\ndetection, and segmentation. Its architectural flexibility and capabilities\nhave made it a preferred choice among researchers and practitioners. However,\nthe intricate multi-head attention mechanism of ViT presents significant\nchallenges to interpretability, as the underlying prediction process remains\nopaque. A critical limitation arises from an observation commonly noted in\ntransformer architectures: \"Not all attention heads are equally meaningful.\"\nOverlooking the relative importance of specific heads highlights the\nlimitations of existing interpretability methods. To address these challenges,\nwe introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel\nmethod that quantifies the importance of each attention head using\ngradient-based scores. These scores are normalized to derive a weighted\naggregate attention score, effectively capturing the relative contributions of\nindividual heads. GMAR clarifies the role of each head in the prediction\nprocess, enabling more precise interpretability at the head level. Experimental\nresults demonstrate that GMAR consistently outperforms traditional attention\nrollout techniques. This work provides a practical contribution to\ntransformer-based architectures, establishing a robust framework for enhancing\nthe interpretability of Vision Transformer models."}
{"id": "2504.19982", "pdf": "https://arxiv.org/pdf/2504.19982", "abs": "https://arxiv.org/abs/2504.19982", "authors": ["Emre Can Acikgoz", "Carl Guo", "Suvodip Dey", "Akul Datta", "Takyoung Kim", "Gokhan Tur", "Dilek Hakkani-Tür"], "title": "TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Task-oriented dialogue (TOD) systems are experiencing a revolution driven by\nLarge Language Models (LLMs), yet the evaluation methodologies for these\nsystems remain insufficient for their growing sophistication. While traditional\nautomatic metrics effectively assessed earlier modular systems, they focus\nsolely on the dialogue level and cannot detect critical intermediate errors\nthat can arise during user-agent interactions. In this paper, we introduce\nTD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework\nthat unifies fine-grained turn-level analysis with holistic dialogue-level\ncomparisons. At turn level, we evaluate each response along three TOD-specific\ndimensions: conversation cohesion, backend knowledge consistency, and policy\ncompliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons\nto provide a measure of dialogue-level quality. Through experiments on MultiWOZ\n2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the\nconversational errors that conventional metrics miss. Furthermore, TD-EVAL\nexhibits better alignment with human judgments than traditional and LLM-based\nmetrics. These findings demonstrate that TD-EVAL introduces a new paradigm for\nTOD system evaluation, efficiently assessing both turn and system levels with a\nplug-and-play framework for future research."}
{"id": "2504.19417", "pdf": "https://arxiv.org/pdf/2504.19417", "abs": "https://arxiv.org/abs/2504.19417", "authors": ["Dehao Yuan", "Cornelia Fermüller"], "title": "A Real-Time Event-Based Normal Flow Estimator", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a real-time, asynchronous, event-based normal flow\nestimator. It follows the same algorithm as Learning Normal Flow Directly From\nEvent Neighborhoods, but with a more optimized implementation. The original\nmethod treats event slices as 3D point clouds, encodes each event's local\ngeometry into a fixed-length vector, and uses a multi-layer perceptron to\npredict normal flow. It constructs representations by multiplying an adjacency\nmatrix with a feature matrix, resulting in quadratic time complexity with\nrespect to the number of events. In contrast, we leverage the fact that event\ncoordinates are integers and reformulate the representation step as a pooling\noperation. This achieves the same effect as the adjacency matrix but with much\nlower computational cost. As a result, our method supports real-time normal\nflow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and\nruns at 4 million normal flows per second on an RTX 3070, or 6 million per\nsecond on an RTX A5000. We release the CUDA implementation along with a Python\ninterface at https://github.com/dhyuan99/VecKM_flow_cpp."}
{"id": "2504.20000", "pdf": "https://arxiv.org/pdf/2504.20000", "abs": "https://arxiv.org/abs/2504.20000", "authors": ["Rishika Sen", "Sujoy Roychowdhury", "Sumit Soman", "H. G. Ranjani", "Srikhetra Mohanty"], "title": "Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "comment": "10 pages, 4 figures, 3 tables", "summary": "Knowledge Distillation (KD) is one of the approaches to reduce the size of\nLarge Language Models (LLMs). A LLM with smaller number of model parameters\n(student) is trained to mimic the performance of a LLM of a larger size\n(teacher model) on a specific task. For domain-specific tasks, it is not clear\nif teacher or student model, or both, must be considered for domain adaptation.\nIn this work, we study this problem from perspective of telecom domain\nQuestion-Answering (QA) task. We systematically experiment with Supervised\nFine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to\nKD. We design experiments to study the impact of vocabulary (same and\ndifferent) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the\ndistilled model. Multi-faceted evaluation of the distillation using 14\ndifferent metrics (N-gram, embedding and LLM-based metrics) is considered.\nExperimental results show that SFT of teacher improves performance of distilled\nmodel when both models have same vocabulary, irrespective of algorithm and\nmetrics. Overall, SFT of both teacher and student results in better performance\nacross all metrics, although the statistical significance of the same depends\non the vocabulary of the teacher models."}
{"id": "2504.19432", "pdf": "https://arxiv.org/pdf/2504.19432", "abs": "https://arxiv.org/abs/2504.19432", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Wangmeng Zuo", "Yanfeng Gu"], "title": "EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Satellite imagery and maps, as two fundamental data modalities in remote\nsensing, offer direct observations of the Earth's surface and\nhuman-interpretable geographic abstractions, respectively. The task of\nbidirectional translation between satellite images and maps (BSMT) holds\nsignificant potential for applications in urban planning and disaster response.\nHowever, this task presents two major challenges: first, the absence of precise\npixel-wise alignment between the two modalities substantially complicates the\ntranslation process; second, it requires achieving both high-level abstraction\nof geographic features and high-quality visual synthesis, which further\nelevates the technical complexity. To address these limitations, we introduce\nEarthMapper, a novel autoregressive framework for controllable bidirectional\nsatellite-map translation. EarthMapper employs geographic coordinate embeddings\nto anchor generation, ensuring region-specific adaptability, and leverages\nmulti-scale feature alignment within a geo-conditioned joint scale\nautoregression (GJSA) process to unify bidirectional translation in a single\ntraining cycle. A semantic infusion (SI) mechanism is introduced to enhance\nfeature-level consistency, while a key point adaptive guidance (KPAG) mechanism\nis proposed to dynamically balance diversity and precision during inference. We\nfurther contribute CNSatMap, a large-scale dataset comprising 302,132 precisely\naligned satellite-map pairs across 38 Chinese cities, enabling robust\nbenchmarking. Extensive experiments on CNSatMap and the New York dataset\ndemonstrate EarthMapper's superior performance, achieving significant\nimprovements in visual realism, semantic consistency, and structural fidelity\nover state-of-the-art methods. Additionally, EarthMapper excels in zero-shot\ntasks like in-painting, out-painting and coordinate-conditional generation,\nunderscoring its versatility."}
{"id": "2504.20013", "pdf": "https://arxiv.org/pdf/2504.20013", "abs": "https://arxiv.org/abs/2504.20013", "authors": ["Beizhe Hu", "Qiang Sheng", "Juan Cao", "Yang Li", "Danding Wang"], "title": "LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": "ACM SIGIR 2025 Full Paper", "summary": "Online fake news moderation now faces a new challenge brought by the\nmalicious use of large language models (LLMs) in fake news production. Though\nexisting works have shown LLM-generated fake news is hard to detect from an\nindividual aspect, it remains underexplored how its large-scale release will\nimpact the news ecosystem. In this study, we develop a simulation pipeline and\na dataset with ~56k generated news of diverse types to investigate the effects\nof LLM-generated fake news within neural news recommendation systems. Our\nfindings expose a truth decay phenomenon, where real news is gradually losing\nits advantageous position in news ranking against fake news as LLM-generated\nnews is involved in news recommendation. We further provide an explanation\nabout why truth decay occurs from a familiarity perspective and show the\npositive correlation between perplexity and news ranking. Finally, we discuss\nthe threats of LLM-generated fake news and provide possible countermeasures. We\nurge stakeholders to address this emerging challenge to preserve the integrity\nof news ecosystems."}
{"id": "2504.19443", "pdf": "https://arxiv.org/pdf/2504.19443", "abs": "https://arxiv.org/abs/2504.19443", "authors": ["Yejin Jeong", "Donghun Lee"], "title": "CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 2 figures", "summary": "Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders\nworldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence\n(KL) grading system is widely used to assess KOA severity. However, its high\ninter-observer variability and subjectivity hinder diagnostic consistency. To\naddress these limitations, automated diagnostic techniques using deep learning\nhave been actively explored in recent years. In this study, we propose a\nCLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of\nKOA grade prediction. To achieve this, we introduce a learning approach that\nintegrates image and text information and incorporate Symmetry Loss and\nConsistency Loss to ensure prediction consistency between the original and\nflipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\\% on KOA\nseverity prediction task, and ablation studies show that CLIP-KOA has 2.36\\%\nimprovement in accuracy over the standard CLIP model due to our contribution.\nThis study shows a novel direction for data-driven medical prediction not only\nto improve reliability of fine-grained diagnosis and but also to explore\nmultimodal methods for medical image analysis. Our code is available at\nhttps://github.com/anonymized-link."}
{"id": "2504.20022", "pdf": "https://arxiv.org/pdf/2504.20022", "abs": "https://arxiv.org/abs/2504.20022", "authors": ["Pritika Rohera", "Chaitrali Ginimav", "Gayatri Sawant", "Raviraj Joshi"], "title": "Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have demonstrated significant\neffectiveness across various languages, particularly in high-resource languages\nsuch as English. However, their performance in terms of factual accuracy across\nother low-resource languages, especially Indic languages, remains an area of\ninvestigation. In this study, we assess the factual accuracy of LLMs - GPT-4o,\nGemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in\nEnglish and Indic languages using the IndicQuest dataset, which contains\nquestion-answer pairs in English and 19 Indic languages. By asking the same\nquestions in English and their respective Indic translations, we analyze\nwhether the models are more reliable for regional context questions in Indic\nlanguages or when operating in English. Our findings reveal that LLMs often\nperform better in English, even for questions rooted in Indic contexts.\nNotably, we observe a higher tendency for hallucination in responses generated\nin low-resource Indic languages, highlighting challenges in the multilingual\nunderstanding capabilities of current LLMs."}
{"id": "2504.19455", "pdf": "https://arxiv.org/pdf/2504.19455", "abs": "https://arxiv.org/abs/2504.19455", "authors": ["Yuki Hirakawa", "Ryotaro Shimizu"], "title": "Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Constructing dataset for fashion style recognition is challenging due to the\ninherent subjectivity and ambiguity of style concepts. Recent advances in\ntext-to-image models have facilitated generative data augmentation by\nsynthesizing images from labeled data, yet existing methods based solely on\nclass names or reference captions often fail to balance visual diversity and\nstyle consistency. In this work, we propose \\textbf{Masked Language Prompting\n(MLP)}, a novel prompting strategy that masks selected words in a reference\ncaption and leverages large language models to generate diverse yet\nsemantically coherent completions. This approach preserves the structural\nsemantics of the original caption while introducing attribute-level variations\naligned with the intended style, enabling style-consistent and diverse image\ngeneration without fine-tuning. Experimental results on the FashionStyle14\ndataset demonstrate that our MLP-based augmentation consistently outperforms\nclass-name and caption-based baselines, validating its effectiveness for\nfashion style recognition under limited supervision."}
{"id": "2504.20039", "pdf": "https://arxiv.org/pdf/2504.20039", "abs": "https://arxiv.org/abs/2504.20039", "authors": ["Roman Garipov", "Fedor Velikonivtsev", "Ruslan Svirschevski", "Vage Egiazarian", "Max Ryabinin"], "title": "AutoJudge: Judge Decoding Without Manual Annotation", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint, Work in progress", "summary": "We introduce AutoJudge, a framework that accelerates large language model\n(LLM) inference with task-specific lossy speculative decoding. Instead of\nmatching the original model output distribution token-by-token, we identify\nwhich of the generated tokens affect the downstream quality of the generated\nresponse, relaxing the guarantee so that the \"unimportant\" tokens can be\ngenerated faster. Our approach relies on a semi-greedy search algorithm to test\nwhich of the mismatches between target and draft model should be corrected to\npreserve quality, and which ones may be skipped. We then train a lightweight\nclassifier based on existing LLM embeddings to predict, at inference time,\nwhich mismatching tokens can be safely accepted without compromising the final\nanswer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B\n(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more\naccepted tokens per verification cycle with under 1% degradation in answer\naccuracy compared to standard speculative decoding and over 2x with small loss\nin accuracy. When applied to the LiveCodeBench benchmark, our approach\nautomatically detects other, programming-specific important tokens and shows\nsimilar speedups, demonstrating its ability to generalize across tasks."}
{"id": "2504.19475", "pdf": "https://arxiv.org/pdf/2504.19475", "abs": "https://arxiv.org/abs/2504.19475", "authors": ["Sonia Joseph", "Praneet Suresh", "Lorenz Hufe", "Edward Stevinson", "Robert Graham", "Yash Vadi", "Danilo Bzdok", "Sebastian Lapuschkin", "Lee Sharkey", "Blake Aaron Richards"], "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop", "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."}
{"id": "2504.18596", "pdf": "https://arxiv.org/pdf/2504.18596", "abs": "https://arxiv.org/abs/2504.18596", "authors": ["Anantha Sharma", "Swetha Devabhaktuni", "Eklove Mohan"], "title": "Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "math.PR"], "comment": "18 pages, 8 figures, 5 tables", "summary": "This paper explores the strategic use of modern synthetic data generation and\nadvanced data perturbation techniques to enhance security, maintain analytical\nutility, and improve operational efficiency when managing large datasets, with\na particular focus on the Banking, Financial Services, and Insurance (BFSI)\nsector. We contrast these advanced methods encompassing generative models like\nGANs, sophisticated context-aware PII transformation, configurable statistical\nperturbation, and differential privacy with traditional anonymization\napproaches.\n  The goal is to create realistic, privacy-preserving datasets that retain high\nutility for complex machine learning tasks and analytics, a critical need in\nthe data-sensitive industries like BFSI, Healthcare, Retail, and\nTelecommunications. We discuss how these modern techniques potentially offer\nsignificant improvements in balancing privacy preservation while maintaining\ndata utility compared to older methods. Furthermore, we examine the potential\nfor operational gains, such as reduced overhead and accelerated analytics, by\nusing these privacy-enhanced datasets. We also explore key use cases where\nthese methods can mitigate regulatory risks and enable scalable, data-driven\ninnovation without compromising sensitive customer information."}
{"id": "2504.19478", "pdf": "https://arxiv.org/pdf/2504.19478", "abs": "https://arxiv.org/abs/2504.19478", "authors": ["Weitao Feng", "Hang Zhou", "Jing Liao", "Li Cheng", "Wenbo Zhou"], "title": "CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel approach for indoor scene synthesis, which learns to\narrange decomposed cuboid primitives to represent 3D objects within a scene.\nUnlike conventional methods that use bounding boxes to determine the placement\nand scale of 3D objects, our approach leverages cuboids as a straightforward\nyet highly effective alternative for modeling objects. This allows for compact\nscene generation while minimizing object intersections. Our approach, coined\nCasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive\nmodel to sequentially arrange cuboids, producing physically plausible scenes.\nBy applying rejection sampling during the fine-tuning stage to filter out\nscenes with object collisions, our model further reduces intersections and\nenhances scene quality. Additionally, we introduce a refined dataset,\n3DFRONT-NC, which eliminates significant noise presented in the original\ndataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our\ndataset demonstrate that our approach consistently outperforms the\nstate-of-the-art methods, enhancing the realism of generated scenes, and\nproviding a promising direction for 3D scene synthesis."}
{"id": "2504.18748", "pdf": "https://arxiv.org/pdf/2504.18748", "abs": "https://arxiv.org/abs/2504.18748", "authors": ["Kaustubh D. Dhole", "Nikhita Vedula", "Saar Kuzi", "Giuseppe Castellucci", "Eugene Agichtein", "Shervin Malmasi"], "title": "Generative Product Recommendations for Implicit Superlative Queries", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "In Recommender Systems, users often seek the best products through indirect,\nvague, or under-specified queries, such as \"best shoes for trail running\". Such\nqueries, also referred to as implicit superlative queries, pose a significant\nchallenge for standard retrieval and ranking systems as they lack an explicit\nmention of attributes and require identifying and reasoning over complex\nfactors. We investigate how Large Language Models (LLMs) can generate implicit\nattributes for ranking as well as reason over them to improve product\nrecommendations for such queries. As a first step, we propose a novel\nfour-point schema for annotating the best product candidates for superlative\nqueries called SUPERB, paired with LLM-based product annotations. We then\nempirically evaluate several existing retrieval and ranking approaches on our\nnew dataset, providing insights and discussing their integration into\nreal-world e-commerce production systems."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.18919", "pdf": "https://arxiv.org/pdf/2504.18919", "abs": "https://arxiv.org/abs/2504.18919", "authors": ["Andrew M. Bean", "Rebecca Payne", "Guy Parsons", "Hannah Rose Kirk", "Juan Ciro", "Rafael Mosquera", "Sara Hincapié Monsalve", "Aruna S. Ekanayaka", "Lionel Tarassenko", "Luc Rocher", "Adam Mahdi"], "title": "Clinical knowledge in LLMs does not translate to human interactions", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "52 pages, 4 figures", "summary": "Global healthcare providers are exploring use of large language models (LLMs)\nto provide medical advice to the public. LLMs now achieve nearly perfect scores\non medical licensing exams, but this does not necessarily translate to accurate\nperformance in real-world settings. We tested if LLMs can assist members of the\npublic in identifying underlying conditions and choosing a course of action\n(disposition) in ten medical scenarios in a controlled study with 1,298\nparticipants. Participants were randomly assigned to receive assistance from an\nLLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested\nalone, LLMs complete the scenarios accurately, correctly identifying conditions\nin 94.9% of cases and disposition in 56.3% on average. However, participants\nusing the same LLMs identified relevant conditions in less than 34.5% of cases\nand disposition in less than 44.2%, both no better than the control group. We\nidentify user interactions as a challenge to the deployment of LLMs for medical\nadvice. Standard benchmarks for medical knowledge and simulated patient\ninteractions do not predict the failures we find with human participants.\nMoving forward, we recommend systematic human user testing to evaluate\ninteractive capabilities prior to public deployments in healthcare."}
{"id": "2504.19506", "pdf": "https://arxiv.org/pdf/2504.19506", "abs": "https://arxiv.org/abs/2504.19506", "authors": ["Xinyang Li", "Chengjie Yi", "Jiawei Lai", "Mingbao Lin", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao"], "title": "SynergyAmodal: Deocclude Anything with Text Control", "categories": ["cs.CV"], "comment": "17 pages", "summary": "Image deocclusion (or amodal completion) aims to recover the invisible\nregions (\\ie, shape and appearance) of occluded instances in images. Despite\nrecent advances, the scarcity of high-quality data that balances diversity,\nplausibility, and fidelity remains a major obstacle. To address this challenge,\nwe identify three critical elements: leveraging in-the-wild image data for\ndiversity, incorporating human expertise for plausibility, and utilizing\ngenerative priors for fidelity. We propose SynergyAmodal, a novel framework for\nco-synthesizing in-the-wild amodal datasets with comprehensive shape and\nappearance annotations, which integrates these elements through a tripartite\ndata-human-model collaboration. First, we design an occlusion-grounded\nself-supervised learning algorithm to harness the diversity of in-the-wild\nimage data, fine-tuning an inpainting diffusion model into a partial completion\ndiffusion model. Second, we establish a co-synthesis pipeline to iteratively\nfilter, refine, select, and annotate the initial deocclusion results of the\npartial completion diffusion model, ensuring plausibility and fidelity through\nhuman expert guidance and prior model constraints. This pipeline generates a\nhigh-quality paired amodal dataset with extensive category and scale diversity,\ncomprising approximately 16K pairs. Finally, we train a full completion\ndiffusion model on the synthesized dataset, incorporating text prompts as\nconditioning signals. Extensive experiments demonstrate the effectiveness of\nour framework in achieving zero-shot generalization and textual\ncontrollability. Our code, dataset, and models will be made publicly available\nat https://github.com/imlixinyang/SynergyAmodal."}
{"id": "2504.18988", "pdf": "https://arxiv.org/pdf/2504.18988", "abs": "https://arxiv.org/abs/2504.18988", "authors": ["Saramsh Gautam", "Mahmood Jasim"], "title": "LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings", "categories": ["cs.HC", "cs.CL", "H.5.3"], "comment": "19 pages, 4 figures. Multimodal system design and evaluation study", "summary": "Collaborative research often includes contributors with varied perspectives\nfrom diverse linguistic backgrounds. However, English as a Second Language\n(ESL) researchers often struggle to communicate during meetings in English and\ncomprehend discussions, leading to limited contribution. To investigate these\nchallenges, we surveyed 64 ESL researchers who frequently collaborate in\nmultilingual teams and identified four key design goals around participation,\ncomprehension, documentation, and feedback. Guided by these design goals, we\ndeveloped LINC, a multimodal Language INdependent Collaboration system with two\ncomponents: a real-time module for multilingual communication during meetings\nand a post-meeting dashboard for discussion analysis. We evaluated the system\nthrough a two-phased study with six triads of multilingual teams. We found that\nusing LINC, participants benefited from communicating in their preferred\nlanguage, recalled and reviewed actionable insights, and prepared for upcoming\nmeetings effectively. We discuss external factors that impact multilingual\nmeeting participation beyond language preferences and the implications of\nmultimodal systems in facilitating meetings in hybrid multilingual\ncollaborative settings beyond research."}
{"id": "2504.19514", "pdf": "https://arxiv.org/pdf/2504.19514", "abs": "https://arxiv.org/abs/2504.19514", "authors": ["Rong Gao", "Xin Liu", "Zhuozhao Hu", "Bohao Xing", "Baiqiang Xia", "Zitong Yu", "Heikki Kälviäinen"], "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Figure skating, known as the \"Art on Ice,\" is among the most artistic sports,\nchallenging to understand due to its blend of technical elements (like jumps\nand spins) and overall artistic expression. Existing figure skating datasets\nmainly focus on single tasks, such as action recognition or scoring, lacking\ncomprehensive annotations for both technical and artistic evaluation. Current\nsports research is largely centered on ball games, with limited relevance to\nartistic sports like figure skating. To address this, we introduce FSAnno, a\nlarge-scale dataset advancing artistic sports understanding through figure\nskating. FSAnno includes an open-access training and test dataset, alongside a\nbenchmark dataset, FSBench, for fair model evaluation. FSBench consists of\nFSBench-Text, with multiple-choice questions and explanations, and\nFSBench-Motion, containing multimodal data and Question and Answer (QA) pairs,\nsupporting tasks from technical analysis to performance commentary. Initial\ntests on FSBench reveal significant limitations in existing models'\nunderstanding of artistic sports. We hope FSBench will become a key tool for\nevaluating and enhancing model comprehension of figure skating."}
{"id": "2504.19056", "pdf": "https://arxiv.org/pdf/2504.19056", "abs": "https://arxiv.org/abs/2504.19056", "authors": ["Mohammad Mahdi Abootorabi", "Omid Ghahroodi", "Pardis Sadat Zahraei", "Hossein Behzadasl", "Alireza Mirrokni", "Mobina Salimipanah", "Arash Rasouli", "Bahar Behzadipour", "Sara Azarnoush", "Benyamin Maleki", "Erfan Sadraiye", "Kiarash Kiani Feriz", "Mahdi Teymouri Nahad", "Ali Moghadasi", "Abolfazl Eshagh Abianeh", "Nizi Nazar", "Hamid R. Rabiee", "Mahdieh Soleymani Baghshah", "Meisam Ahmadi", "Ehsaneddin Asgari"], "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "50 main pages, 30 pages appendix, 21 figures, 8 tables, GitHub\n  Repository:\n  https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey", "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent\nbreakthroughs in foundation and diffusion models have reduced the time and cost\nof producing animated content. Characters are central animation components,\ninvolving motion, emotions, gestures, and facial expressions. The pace and\nbreadth of advances in recent months make it difficult to maintain a coherent\nview of the field, motivating the need for an integrative review. Unlike\nearlier overviews that treat avatars, gestures, or facial animation in\nisolation, this survey offers a single, comprehensive perspective on all the\nmain generative AI applications for character animation. We begin by examining\nthe state-of-the-art in facial animation, expression rendering, image\nsynthesis, avatar creation, gesture modeling, motion synthesis, object\ngeneration, and texture synthesis. We highlight leading research, practical\ndeployments, commonly used datasets, and emerging trends for each area. To\nsupport newcomers, we also provide a comprehensive background section that\nintroduces foundational models and evaluation metrics, equipping readers with\nthe knowledge needed to enter the field. We discuss open challenges and map\nfuture research directions, providing a roadmap to advance AI-driven\ncharacter-animation technologies. This survey is intended as a resource for\nresearchers and developers entering the field of generative AI animation or\nadjacent fields. Resources are available at:\nhttps://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey."}
{"id": "2504.19524", "pdf": "https://arxiv.org/pdf/2504.19524", "abs": "https://arxiv.org/abs/2504.19524", "authors": ["Peijian Zeng", "Feiyan Pang", "Zhanbo Wang", "Aimin Yang"], "title": "LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Industrial Anomaly Detection (IAD) is critical for ensuring product quality\nby identifying defects. Traditional methods such as feature embedding and\nreconstruction-based approaches require large datasets and struggle with\nscalability. Existing vision-language models (VLMs) and Multimodal Large\nLanguage Models (MLLMs) address some limitations but rely on mask annotations,\nleading to high implementation costs and false positives. Additionally,\nindustrial datasets like MVTec-AD and VisA suffer from severe class imbalance,\nwith defect samples constituting only 23.8% and 11.1% of total data\nrespectively. To address these challenges, we propose a reward function that\ndynamically prioritizes rare defect patterns during training to handle class\nimbalance. We also introduce a mask-free reasoning framework using Chain of\nThought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms,\nenabling anomaly detection directly from raw images without annotated masks.\nThis approach generates interpretable step-by-step explanations for defect\nlocalization. Our method achieves state-of-the-art performance, outperforming\nprior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating\nmask dependency and reducing costs while providing explainable outputs, this\nwork advances industrial anomaly detection and supports scalable quality\ncontrol in manufacturing. Code to reproduce the experiment is available at\nhttps://github.com/LilaKen/LR-IAD."}
{"id": "2504.19062", "pdf": "https://arxiv.org/pdf/2504.19062", "abs": "https://arxiv.org/abs/2504.19062", "authors": ["Yu Zhang", "Wenxiang Guo", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Jingyu Lu", "Rongjie Huang", "Ruiyuan Zhang", "Zhiqing Hong", "Ziyue Jiang", "Zhou Zhao"], "title": "Versatile Framework for Song Generation with Prompt-based Control", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Song generation focuses on producing controllable high-quality songs based on\nvarious prompts. However, existing methods struggle to generate vocals and\naccompaniments with prompt-based control and proper alignment. Additionally,\nthey fall short in supporting various tasks. To address these challenges, we\nintroduce VersBand, a multi-task song generation framework for synthesizing\nhigh-quality, aligned songs with prompt-based control. VersBand comprises these\nprimary models: 1) VocalBand, a decoupled model, leverages the flow-matching\nmethod for generating singing styles, pitches, and mel-spectrograms, allowing\nfast, high-quality vocal generation with style control. 2) AccompBand, a\nflow-based transformer model, incorporates the Band-MOE, selecting suitable\nexperts for enhanced quality, alignment, and control. This model allows for\ngenerating controllable, high-quality accompaniments aligned with vocals. 3)\nTwo generation models, LyricBand for lyrics and MelodyBand for melodies,\ncontribute to the comprehensive multi-task song generation system, allowing for\nextensive control based on multiple prompts. Experimental results demonstrate\nthat VersBand performs better over baseline models across multiple song\ngeneration tasks using objective and subjective metrics. Audio samples are\navailable at https://VersBand.github.io."}
{"id": "2504.19529", "pdf": "https://arxiv.org/pdf/2504.19529", "abs": "https://arxiv.org/abs/2504.19529", "authors": ["Guobiao Li", "Lei Tan", "Yuliang Xue", "Gaozhi Liu", "Zhenxing Qian", "Sheng Li", "Xinpeng Zhang"], "title": "Adversarial Shallow Watermarking", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 12 figures", "summary": "Recent advances in digital watermarking make use of deep neural networks for\nmessage embedding and extraction. They typically follow the ``encoder-noise\nlayer-decoder''-based architecture. By deliberately establishing a\ndifferentiable noise layer to simulate the distortion of the watermarked\nsignal, they jointly train the deep encoder and decoder to fit the noise layer\nto guarantee robustness. As a result, they are usually weak against unknown\ndistortions that are not used in their training pipeline. In this paper, we\npropose a novel watermarking framework to resist unknown distortions, namely\nAdversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder\nthat is randomly parameterized and designed to be insensitive to distortions\nfor watermarking extraction. During the watermark embedding, ASW freezes the\nshallow decoder and adversarially optimizes a host image until its updated\nversion (i.e., the watermarked image) stably triggers the shallow decoder to\noutput the watermark message. During the watermark extraction, it accurately\nrecovers the message from the watermarked image by leveraging the insensitive\nnature of the shallow decoder against arbitrary distortions. Our ASW is\ntraining-free, encoder-free, and noise layer-free. Experiments indicate that\nthe watermarked images created by ASW have strong robustness against various\nunknown distortions. Compared to the existing ``encoder-noise layer-decoder''\napproaches, ASW achieves comparable results on known distortions and better\nrobustness on unknown distortions."}
{"id": "2504.19188", "pdf": "https://arxiv.org/pdf/2504.19188", "abs": "https://arxiv.org/abs/2504.19188", "authors": ["Jianlong Chen", "Chao Li", "Yang Yuan", "Andrew C Yao"], "title": "Hierarchical Attention Generates Better Proofs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "comment": "15 pages with 3 figures", "summary": "Large language models (LLMs) have shown promise in formal theorem proving,\nbut their token-level processing often fails to capture the inherent\nhierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical\nAttention}, a regularization method that aligns LLMs' attention mechanisms with\nmathematical reasoning structures. Our approach establishes a five-level\nhierarchy from foundational elements to high-level concepts, ensuring\nstructured information flow in proof generation. Experiments demonstrate that\nour method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on\nProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively.\nThe code is available at https://github.com/Car-pe/HAGBP."}
{"id": "2504.19545", "pdf": "https://arxiv.org/pdf/2504.19545", "abs": "https://arxiv.org/abs/2504.19545", "authors": ["Zezeng Li", "Zhihui Qi", "Weimin Wang", "Ziliang Wang", "Junyi Duan", "Na Lei"], "title": "Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Quad meshes are essential in geometric modeling and computational mechanics.\nAlthough learning-based methods for triangle mesh demonstrate considerable\nadvancements, quad mesh generation remains less explored due to the challenge\nof ensuring coplanarity, convexity, and quad-only meshes. In this paper, we\npresent Point2Quad, the first learning-based method for quad-only mesh\ngeneration from point clouds. The key idea is learning to identify quad mesh\nwith fused pointwise and facewise features. Specifically, Point2Quad begins\nwith a k-NN-based candidate generation considering the coplanarity and\nsquareness. Then, two encoders are followed to extract geometric and\ntopological features that address the challenge of quad-related constraints,\nespecially by combining in-depth quadrilaterals-specific characteristics.\nSubsequently, the extracted features are fused to train the classifier with a\ndesigned compound loss. The final results are derived after the refinement by a\nquad-specific post-processing. Extensive experiments on both clear and noise\ndata demonstrate the effectiveness and superiority of Point2Quad, compared to\nbaseline methods under comprehensive metrics."}
{"id": "2504.19276", "pdf": "https://arxiv.org/pdf/2504.19276", "abs": "https://arxiv.org/abs/2504.19276", "authors": ["Yiyang Zhou", "Zhaoyang Wang", "Tianle Wang", "Shangyu Xing", "Peng Xia", "Bo Li", "Kaiyuan Zheng", "Zijian Zhang", "Zhaorun Chen", "Wenhao Zheng", "Xuchao Zhang", "Chetan Bansal", "Weitong Zhang", "Ying Wei", "Mohit Bansal", "Huaxiu Yao"], "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "High-quality preference data is essential for aligning foundation models with\nhuman values through preference learning. However, manual annotation of such\ndata is often time-consuming and costly. Recent methods often adopt a\nself-rewarding approach, where the target model generates and annotates its own\npreference data, but this can lead to inaccuracies since the reward model\nshares weights with the target model, thereby amplifying inherent biases. To\naddress these issues, we propose Anyprefer, a framework designed to synthesize\nhigh-quality preference data for aligning the target model. Anyprefer frames\nthe data synthesis process as a cooperative two-player Markov Game, where the\ntarget model and the judge model collaborate together. Here, a series of\nexternal tools are introduced to assist the judge model in accurately rewarding\nthe target model's responses, mitigating biases in the rewarding process. In\naddition, a feedback mechanism is introduced to optimize prompts for both\nmodels, enhancing collaboration and improving data quality. The synthesized\ndata is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K\nhigh-quality preference pairs. Extensive experiments show that Anyprefer\nsignificantly improves model alignment performance across four main\napplications, covering 21 datasets, achieving average improvements of 18.55% in\nfive natural language generation datasets, 3.66% in nine vision-language\nunderstanding datasets, 30.05% in three medical image analysis datasets, and\n16.00% in four visuo-motor control tasks."}
{"id": "2504.19546", "pdf": "https://arxiv.org/pdf/2504.19546", "abs": "https://arxiv.org/abs/2504.19546", "authors": ["Tong Xiao", "Qunming Wang", "Ping Lu", "Tenghai Huang", "Xiaohua Tong", "Peter M. Atkinson"], "title": "Crowd Detection Using Very-Fine-Resolution Satellite Imagery", "categories": ["cs.CV"], "comment": "17 pages, 12 figures, 5 tables", "summary": "Accurate crowd detection (CD) is critical for public safety and historical\npattern analysis, yet existing methods relying on ground and aerial imagery\nsuffer from limited spatio-temporal coverage. The development of\nvery-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial\nresolution) provides unprecedented opportunities for large-scale crowd activity\nanalysis, but it has never been considered for this task. To address this gap,\nwe proposed CrowdSat-Net, a novel point-based convolutional neural network,\nwhich features two innovative components: Dual-Context Progressive Attention\nNetwork (DCPAN) to improve feature representation of individuals by aggregating\nscene context and local individual characteristics, and High-Frequency Guided\nDeformable Upsampler (HFGDU) that recovers high-frequency information during\nupsampling through frequency-domain guided deformable convolutions. To validate\nthe effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR\nsatellite imagery dataset designed specifically for CD tasks, comprising over\n120k manually labeled individuals from multi-source satellite platforms\n(Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the\nexperiments, CrowdSat-Net was compared with five state-of-the-art point-based\nCD methods (originally designed for ground or aerial imagery) using CrowdSat\nand achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing\nthe second-best method by 1.71% and 2.42%, respectively. Moreover, extensive\nablation experiments validated the importance of the DCPAN and HFGDU modules.\nFurthermore, cross-regional evaluation further demonstrated the spatial\ngeneralizability of CrowdSat-Net. This research advances CD capability by\nproviding both a newly developed network architecture for CD and a pioneering\nbenchmark dataset to facilitate future CD development."}
{"id": "2504.19444", "pdf": "https://arxiv.org/pdf/2504.19444", "abs": "https://arxiv.org/abs/2504.19444", "authors": ["Kang Yang", "Xinjun Mao", "Shangwen Wang", "Yanlin Wang", "Tanghaoran Zhang", "Bo Lin", "Yihao Qin", "Zhang Zhang", "Yao Lu", "Kamal Al-Sabahi"], "title": "Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks", "categories": ["cs.SE", "cs.CL"], "comment": "Awarded the ACM SIGSOFT Distinguished Paper Award in ICPC 2025", "summary": "Pre-trained code models rely heavily on high-quality pre-training data,\nparticularly human-written reference comments that bridge code and natural\nlanguage. However, these comments often become outdated as software evolves,\ndegrading model performance. Large language models (LLMs) excel at generating\nhigh-quality code comments. We investigate whether replacing human-written\ncomments with LLM-generated ones improves pre-training datasets. Since standard\nmetrics cannot assess reference comment quality, we propose two novel\nreference-free evaluation tasks: code-comment inconsistency detection and\nsemantic code search. Results show that LLM-generated comments are more\nsemantically consistent with code than human-written ones, as confirmed by\nmanual evaluation. Leveraging this finding, we rebuild the CodeSearchNet\ndataset with LLM-generated comments and re-pre-train CodeT5. Evaluations\ndemonstrate that models trained on LLM-enhanced data outperform those using\noriginal human comments in code summarization, generation, and translation\ntasks. This work validates rebuilding pre-training datasets with LLMs to\nadvance code intelligence, challenging the traditional reliance on human\nreference comments."}
{"id": "2504.19549", "pdf": "https://arxiv.org/pdf/2504.19549", "abs": "https://arxiv.org/abs/2504.19549", "authors": ["Deng Li", "Bohao Xing", "Xin Liu", "Baiqiang Xia", "Bihan Wen", "Heikki Kälviäinen"], "title": "DEEMO: De-identity Multimodal Emotion Recognition and Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Most existing\napproaches rely heavily on identity-sensitive information, such as facial\nexpressions and speech, which raises concerns about personal privacy. To\naddress this, we introduce the De-identity Multimodal Emotion Recognition and\nReasoning (DEEMO), a novel task designed to enable emotion understanding using\nde-identified video and audio inputs. The DEEMO dataset consists of two\nsubsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body\nLanguage (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion\nRecognition and Reasoning using identity-free cues. This design supports\nemotion understanding without compromising identity privacy. In addition, we\npropose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates\nde-identified audio, video, and textual information to enhance both emotion\nrecognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves\nstate-of-the-art performance on both tasks, outperforming existing MLLMs by a\nsignificant margin, achieving 74.49% accuracy and 74.45% F1-score in\nde-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap\nin de-identity emotion reasoning. Our work contributes to ethical AI by\nadvancing privacy-preserving emotion understanding and promoting responsible\naffective computing."}
{"id": "2504.19458", "pdf": "https://arxiv.org/pdf/2504.19458", "abs": "https://arxiv.org/abs/2504.19458", "authors": ["Taoyu Su", "Jiawei Sheng", "Duohe Ma", "Xiaodong Li", "Juwei Yue", "Mengxiao Song", "Yingkai Tang", "Tingwen Liu"], "title": "Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective", "categories": ["cs.MM", "cs.CL", "cs.IR"], "comment": "Accepted by SIGIR 2025, 11 pages, 10 figures, 4 tables,", "summary": "Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from\ndifferent Multi-Modal Knowledge Graphs (MMKGs), a critical information\nretrieval task. Existing studies have explored various fusion paradigms and\nconsistency constraints to improve the alignment of equivalent entities, while\noverlooking that the visual modality may not always contribute positively.\nEmpirically, entities with low-similarity images usually generate\nunsatisfactory performance, highlighting the limitation of overly relying on\nvisual features. We believe the model can be biased toward the visual modality,\nleading to a shortcut image-matching task. To address this, we propose a\ncounterfactual debiasing framework for MMEA, termed CDMEA, which investigates\nvisual modality bias from a causal perspective. Our approach aims to leverage\nboth visual and graph modalities to enhance MMEA while suppressing the direct\ncausal effect of the visual modality on model predictions. By estimating the\nTotal Effect (TE) of both modalities and excluding the Natural Direct Effect\n(NDE) of the visual modality, we ensure that the model predicts based on the\nTotal Indirect Effect (TIE), effectively utilizing both modalities and reducing\nvisual modality bias. Extensive experiments on 9 benchmark datasets show that\nCDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,\nhigh-noise, and low-resource data scenarios."}
{"id": "2504.19557", "pdf": "https://arxiv.org/pdf/2504.19557", "abs": "https://arxiv.org/abs/2504.19557", "authors": ["Mohammad Altillawi", "Fengyi Shen", "Liudi Yang", "Sai Manoj Prakhya", "Ziyuan Liu"], "title": "CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes", "categories": ["cs.CV"], "comment": "Accepted in 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "summary": "Current point-based approaches encounter limitations in scalability and\nrendering quality when using large 3D point cloud maps because using them\ndirectly for novel view synthesis (NVS) leads to degraded visualizations. We\nidentify the primary issue behind these low-quality renderings as a visibility\nmismatch between geometry and appearance, stemming from using these two\nmodalities together. To address this problem, we present CE-NPBG, a new\napproach for novel view synthesis (NVS) in large-scale autonomous driving\nscenes. Our method is a neural point-based technique that leverages two\nmodalities: posed images (cameras) and synchronized raw 3D point clouds\n(LiDAR). We first employ a connectivity relationship graph between appearance\nand geometry, which retrieves points from a large 3D point cloud map observed\nfrom the current camera perspective and uses them for rendering. By leveraging\nthis connectivity, our method significantly improves rendering quality and\nenhances run-time and scalability by using only a small subset of points from\nthe large 3D point cloud map. Our approach associates neural descriptors with\nthe points and uses them to synthesize views. To enhance the encoding of these\ndescriptors and elevate rendering quality, we propose a joint adversarial and\npoint rasterization training. During training, we pair an image-synthesizer\nnetwork with a multi-resolution discriminator. At inference, we decouple them\nand use the image-synthesizer to generate novel views. We also integrate our\nproposal into the recent 3D Gaussian Splatting work to highlight its benefits\nfor improved rendering and scalability."}
{"id": "2504.19483", "pdf": "https://arxiv.org/pdf/2504.19483", "abs": "https://arxiv.org/abs/2504.19483", "authors": ["Bertram Højer", "Oliver Jarvis", "Stefan Heinrich"], "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Has been accepted at \"The Thirteenth International Conference on\n  Learning Representations (ICLR 2025)\" Link to publication:\n  https://openreview.net/forum?id=IssPhpUsKt", "summary": "Recent advancements in large language models (LLMs) have resulted in\nincreasingly anthropomorphic language concerning the ability of LLMs to reason.\nWhether reasoning in LLMs should be understood to be inherently different is,\nhowever, widely debated. We propose utilizing a representation engineering\napproach wherein model activations are read from the residual stream of an LLM\nwhen processing a reasoning task. The activations are used to derive a control\nvector that is applied to the model as an inference-time intervention,\nmodulating the representational space of the model, to improve performance on\nthe specified task. We publish the code for deriving control vectors and\nanalyzing model representations. The method allows us to improve performance on\nreasoning benchmarks and assess how control vectors influence the final logit\ndistribution of a model via metrics such as KL divergence and entropy. We apply\ncontrol vectors to Mistral-7B-Instruct and a range of Pythia models on an\ninductive, a deductive and mathematical reasoning task. We show that an LLM\ncan, to a certain degree, be controlled to improve its perceived reasoning\nability by modulating activations. The intervention is dependent upon the\nability to reliably extract the model's typical state when correctly solving a\ntask. Our results suggest that reasoning performance can be modulated in the\nsame manner as other information-processing tasks performed by LLMs and\ndemonstrate that we are capable of improving performance on specific tasks via\na simple intervention on the residual stream with no additional training."}
{"id": "2504.19572", "pdf": "https://arxiv.org/pdf/2504.19572", "abs": "https://arxiv.org/abs/2504.19572", "authors": ["Peter Hönig", "Matthias Hirschmanner", "Markus Vincze"], "title": "Category-Level and Open-Set Object Pose Estimation for Robotics", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "Object pose estimation enables a variety of tasks in computer vision and\nrobotics, including scene understanding and robotic grasping. The complexity of\na pose estimation task depends on the unknown variables related to the target\nobject. While instance-level methods already excel for opaque and Lambertian\nobjects, category-level and open-set methods, where texture, shape, and size\nare partially or entirely unknown, still struggle with these basic material\nproperties. Since texture is unknown in these scenarios, it cannot be used for\ndisambiguating object symmetries, another core challenge of 6D object pose\nestimation. The complexity of estimating 6D poses with such a manifold of\nunknowns led to various datasets, accuracy metrics, and algorithmic solutions.\nThis paper compares datasets, accuracy metrics, and algorithms for solving 6D\npose estimation on the category-level. Based on this comparison, we analyze how\nto bridge category-level and open-set object pose estimation to reach\ngeneralization and provide actionable recommendations."}
{"id": "2504.19500", "pdf": "https://arxiv.org/pdf/2504.19500", "abs": "https://arxiv.org/abs/2504.19500", "authors": ["Yan Wang", "Baoxiong Jia", "Ziyu Zhu", "Siyuan Huang"], "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "CVPR 2025", "summary": "Open-vocabulary 3D scene understanding is pivotal for enhancing physical\nintelligence, as it enables embodied agents to interpret and interact\ndynamically within real-world environments. This paper introduces MPEC, a novel\nMasked Point-Entity Contrastive learning method for open-vocabulary 3D semantic\nsegmentation that leverages both 3D entity-language alignment and point-entity\nconsistency across different point cloud views to foster entity-specific\nfeature representations. Our method improves semantic discrimination and\nenhances the differentiation of unique instances, achieving state-of-the-art\nresults on ScanNet for open-vocabulary 3D semantic segmentation and\ndemonstrating superior zero-shot scene understanding capabilities. Extensive\nfine-tuning experiments on 8 datasets, spanning from low-level perception to\nhigh-level reasoning tasks, showcase the potential of learned 3D features,\ndriving consistent performance gains across varied 3D scene understanding\ntasks. Project website: https://mpec-3d.github.io/"}
{"id": "2504.19574", "pdf": "https://arxiv.org/pdf/2504.19574", "abs": "https://arxiv.org/abs/2504.19574", "authors": ["Seongmin Hwang", "Daeyoung Han", "Moongu Jeon"], "title": "DG-DETR: Toward Domain Generalized Detection Transformer", "categories": ["cs.CV"], "comment": "Under Review", "summary": "End-to-end Transformer-based detectors (DETRs) have demonstrated strong\ndetection performance. However, domain generalization (DG) research has\nprimarily focused on convolutional neural network (CNN)-based detectors, while\npaying little attention to enhancing the robustness of DETRs. In this letter,\nwe introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple,\neffective, and plug-and-play method that improves out-of-distribution (OOD)\nrobustness for DETRs. Specifically, we propose a novel domain-agnostic query\nselection strategy that removes domain-induced biases from object queries via\northogonal projection onto the instance-specific style space. Additionally, we\nleverage a wavelet decomposition to disentangle features into domain-invariant\nand domain-specific components, enabling synthesis of diverse latent styles\nwhile preserving the semantic features of objects. Experimental results\nvalidate the effectiveness of DG-DETR. Our code is available at\nhttps://github.com/sminhwang/DG-DETR."}
{"id": "2504.19519", "pdf": "https://arxiv.org/pdf/2504.19519", "abs": "https://arxiv.org/abs/2504.19519", "authors": ["Ke Hong", "Xiuhong Li", "Minxu Liu", "Qiuli Mao", "Tianqi Wu", "Zixiao Huang", "Lufang Chen", "Zhong Wang", "Yichong Zhang", "Zhenhua Zhu", "Guohao Dai", "Yu Wang"], "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation", "categories": ["cs.DC", "cs.CL", "cs.LG"], "comment": "17 pages, 11 figures, 4 tables", "summary": "Generative models have achieved remarkable success across various\napplications, driving the demand for multi-GPU computing. Inter-GPU\ncommunication becomes a bottleneck in multi-GPU computing systems, particularly\non consumer-grade GPUs. By exploiting concurrent hardware execution,\noverlapping computation and communication latency is an effective technique for\nmitigating the communication overhead. We identify that an efficient and\nadaptable overlapping design should satisfy (1) tile-wise overlapping to\nmaximize the overlapping opportunity, (2) interference-free computation to\nmaintain the original computational performance, and (3) communication\nagnosticism to reduce the development burden against varying communication\nprimitives. Nevertheless, current designs fail to simultaneously optimize for\nall of those features.\n  To address the issue, we propose FlashOverlap, a lightweight design\ncharacterized by tile-wise overlapping, interference-free computation, and\ncommunication agnosticism. FlashOverlap utilizes a novel signaling mechanism to\nidentify tile-wise data dependency without interrupting the computation\nprocess, and reorders data to contiguous addresses, enabling communication by\nsimply calling NCCL APIs. Experiments show that such a lightweight design\nachieves up to 1.65x speedup, outperforming existing works in most cases."}
{"id": "2504.19581", "pdf": "https://arxiv.org/pdf/2504.19581", "abs": "https://arxiv.org/abs/2504.19581", "authors": ["Chengzhi Wu", "Yuxin Wan", "Hao Fu", "Julius Pfrommer", "Zeyun Zhong", "Junwei Zheng", "Jiaming Zhang", "Jürgen Beyerer"], "title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity", "categories": ["cs.CV"], "comment": null, "summary": "Driven by the increasing demand for accurate and efficient representation of\n3D data in various domains, point cloud sampling has emerged as a pivotal\nresearch topic in 3D computer vision. Recently, learning-to-sample methods have\ngarnered growing interest from the community, particularly for their ability to\nbe jointly trained with downstream tasks. However, previous learning-based\nsampling methods either lead to unrecognizable sampling patterns by generating\na new point cloud or biased sampled results by focusing excessively on sharp\nedge details. Moreover, they all overlook the natural variations in point\ndistribution across different shapes, applying a similar sampling strategy to\nall point clouds. In this paper, we propose a Sparse Attention Map and\nBin-based Learning method (termed SAMBLE) to learn shape-specific sampling\nstrategies for point cloud shapes. SAMBLE effectively achieves an improved\nbalance between sampling edge points for local details and preserving\nuniformity in the global shape, resulting in superior performance across\nmultiple common point cloud downstream tasks, even in scenarios with few-point\nsampling."}
{"id": "2504.19583", "pdf": "https://arxiv.org/pdf/2504.19583", "abs": "https://arxiv.org/abs/2504.19583", "authors": ["Hanlu Zhang", "Yumeng Ma", "Shuo Wang", "Guiran Liu", "Binrong Zhu"], "title": "Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "This paper proposes a parameter collaborative optimization algorithm for\nlarge language models, enhanced with graph spectral analysis. The goal is to\nimprove both fine-tuning efficiency and structural awareness during training.\nIn the proposed method, the parameters of a pre-trained language model are\ntreated as nodes in a graph. A weighted graph is constructed, and Laplacian\nspectral decomposition is applied to enable frequency-domain modeling and\nstructural representation of the parameter space. Based on this structure, a\njoint loss function is designed. It combines the task loss with a spectral\nregularization term to facilitate collaborative updates among parameters. In\naddition, a spectral filtering mechanism is introduced during the optimization\nphase. This mechanism adjusts gradients in a structure-aware manner, enhancing\nthe model's training stability and convergence behavior. The method is\nevaluated on multiple tasks, including traditional fine-tuning comparisons,\nfew-shot generalization tests, and convergence speed analysis. In all settings,\nthe proposed approach demonstrates superior performance. The experimental\nresults confirm that the spectral collaborative optimization framework\neffectively reduces parameter perturbations and improves fine-tuning quality\nwhile preserving overall model performance. This work contributes significantly\nto the field of artificial intelligence by advancing parameter-efficient\ntraining methodologies for large-scale models, reinforcing the importance of\nstructural signal processing in deep learning optimization, and offering a\nrobust, generalizable framework for enhancing language model adaptability and\nperformance."}
{"id": "2504.19584", "pdf": "https://arxiv.org/pdf/2504.19584", "abs": "https://arxiv.org/abs/2504.19584", "authors": ["Sangmin Kim", "Seunguk Do", "Jaesik Park"], "title": "ShowMak3r: Compositional TV Show Reconstruction", "categories": ["cs.CV"], "comment": "Project page : https://nstar1125.github.io/showmak3r", "summary": "Reconstructing dynamic radiance fields from video clips is challenging,\nespecially when entertainment videos like TV shows are given. Many challenges\nmake the reconstruction difficult due to (1) actors occluding with each other\nand having diverse facial expressions, (2) cluttered stages, and (3) small\nbaseline views or sudden shot changes. To address these issues, we present\nShowMak3r, a comprehensive reconstruction pipeline that allows the editing of\nscenes like how video clips are made in a production control room. In\nShowMak3r, a 3DLocator module locates recovered actors on the stage using depth\nprior and estimates unseen human poses via interpolation. The proposed\nShotMatcher module then tracks the actors under shot changes. Furthermore,\nShowMak3r introduces a face-fitting network that dynamically recovers the\nactors' expressions. Experiments on Sitcoms3D dataset show that our pipeline\ncan reassemble TV show scenes with new cameras at different timestamps. We also\ndemonstrate that ShowMak3r enables interesting applications such as synthetic\nshot-making, actor relocation, insertion, deletion, and pose manipulation.\nProject page : https://nstar1125.github.io/showmak3r"}
{"id": "2504.19730", "pdf": "https://arxiv.org/pdf/2504.19730", "abs": "https://arxiv.org/abs/2504.19730", "authors": ["Wenhan Mu", "Ling Xu", "Shuren Pei", "Le Mi", "Huichi Zhou"], "title": "Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge", "categories": ["cs.SE", "cs.CL"], "comment": "25 pages, 6 figures", "summary": "The widespread adoption of code language models in software engineering tasks\nhas exposed vulnerabilities to adversarial attacks, especially the identifier\nsubstitution attacks. Although existing identifier substitution attackers\ndemonstrate high success rates, they often produce adversarial examples with\nunnatural code patterns. In this paper, we systematically assess the quality of\nadversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80%\nof adversarial examples generated by state-of-the-art identifier substitution\nattackers (e.g., ALERT) are actually detectable. Based on this insight, we\npropose EP-Shield, a unified framework for evaluating and purifying identifier\nsubstitution attacks via naturalness-aware reasoning. Specifically, we first\nevaluate the naturalness of code and identify the perturbed adversarial code,\nthen purify it so that the victim model can restore correct prediction.\nExtensive experiments demonstrate the superiority of EP-Shield over adversarial\nfine-tuning (up to 83.36% improvement) and its lightweight design 7B\nparameters) with GPT-4-level performance."}
{"id": "2504.19589", "pdf": "https://arxiv.org/pdf/2504.19589", "abs": "https://arxiv.org/abs/2504.19589", "authors": ["Daniele Rege Cambrin", "Luca Colomba", "Paolo Garza"], "title": "Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted in IEEE Journal of Selected Topics in Applied Earth\n  Observations and Remote Sensing", "summary": "In crisis management and remote sensing, image segmentation plays a crucial\nrole, enabling tasks like disaster response and emergency planning by analyzing\nvisual data. Neural networks are able to analyze satellite acquisitions and\ndetermine which areas were affected by a catastrophic event. The problem in\ntheir development in this context is the data scarcity and the lack of\nextensive benchmark datasets, limiting the capabilities of training large\nneural network models. In this paper, we propose a novel methodology, namely\nMagnifier, to improve segmentation performance with limited data availability.\nThe Magnifier methodology is applicable to any existing encoder-decoder\narchitecture, as it extends a model by merging information at different\ncontextual levels through a dual-encoder approach: a local and global encoder.\nMagnifier analyzes the input data twice using the dual-encoder approach. In\nparticular, the local and global encoders extract information from the same\ninput at different granularities. This allows Magnifier to extract more\ninformation than the other approaches given the same set of input images.\nMagnifier improves the quality of the results of +2.65% on average IoU while\nleading to a restrained increase in terms of the number of trainable parameters\ncompared to the original model. We evaluated our proposed approach with\nstate-of-the-art burned area segmentation models, demonstrating, on average,\ncomparable or better performances in less than half of the GFLOPs."}
{"id": "2504.19754", "pdf": "https://arxiv.org/pdf/2504.19754", "abs": "https://arxiv.org/abs/2504.19754", "authors": ["Carlo Merola", "Jaspinder Singh"], "title": "Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "13 pages, 2 figures, Second Workshop on Knowledge-Enhanced\n  Information Retrieval, ECIR 2025", "summary": "Retrieval-augmented generation (RAG) has become a transformative approach for\nenhancing large language models (LLMs) by grounding their outputs in external\nknowledge sources. Yet, a critical question persists: how can vast volumes of\nexternal knowledge be managed effectively within the input constraints of LLMs?\nTraditional methods address this by chunking external documents into smaller,\nfixed-size segments. While this approach alleviates input limitations, it often\nfragments context, resulting in incomplete retrieval and diminished coherence\nin generation. To overcome these shortcomings, two advanced techniques, late\nchunking and contextual retrieval, have been introduced, both aiming to\npreserve global context. Despite their potential, their comparative strengths\nand limitations remain unclear. This study presents a rigorous analysis of late\nchunking and contextual retrieval, evaluating their effectiveness and\nefficiency in optimizing RAG systems. Our results indicate that contextual\nretrieval preserves semantic coherence more effectively but requires greater\ncomputational resources. In contrast, late chunking offers higher efficiency\nbut tends to sacrifice relevance and completeness."}
{"id": "2504.19592", "pdf": "https://arxiv.org/pdf/2504.19592", "abs": "https://arxiv.org/abs/2504.19592", "authors": ["Roman Malashin", "Daniil Ilyukhin"], "title": "Neural network task specialization via domain constraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a concept of neural network specialization via\ntask-specific domain constraining, aimed at enhancing network performance on\ndata subspace in which the network operates. The study presents experiments on\ntraining specialists for image classification and object detection tasks. The\nresults demonstrate that specialization can enhance a generalist's accuracy\neven without additional data or changing training regimes: solely by\nconstraining class label space in which the network performs. Theoretical and\nexperimental analyses indicate that effective specialization requires modifying\ntraditional fine-tuning methods and constraining data space to semantically\ncoherent subsets. The specialist extraction phase before tuning the network is\nproposed for maximal performance gains. We also provide analysis of the\nevolution of the feature space during specialization. This study paves way to\nfuture research for developing more advanced dynamically configurable image\nanalysis systems, where computations depend on the specific input.\nAdditionally, the proposed methods can help improve system performance in\nscenarios where certain data domains should be excluded from consideration of\nthe generalist network."}
{"id": "2504.19598", "pdf": "https://arxiv.org/pdf/2504.19598", "abs": "https://arxiv.org/abs/2504.19598", "authors": ["Dou Quan", "Rufan Zhou", "Shuang Wang", "Ning Huyan", "Dong Zhao", "Yunan Li", "Licheng Jiao"], "title": "Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promising performances in remote sensing\nimage change detection (CD). However, existing methods usually train a\ndataset-specific deep network for each dataset. Due to the significant\ndifferences in the data distribution and labeling between various datasets, the\ntrained dataset-specific deep network has poor generalization performances on\nother datasets. To solve this problem, this paper proposes a change adapter\nnetwork (CANet) for a more universal and generalized CD. CANet contains\ndataset-shared and dataset-specific learning modules. The former explores the\ndiscriminative features of images, and the latter designs a lightweight adapter\nmodel, to deal with the characteristics of different datasets in data\ndistribution and labeling. The lightweight adapter can quickly generalize the\ndeep network for new CD tasks with a small computation cost. Specifically, this\npaper proposes an interesting change region mask (ICM) in the adapter, which\ncan adaptively focus on interested change objects and decrease the influence of\nlabeling differences in various datasets. Moreover, CANet adopts a unique batch\nnormalization layer for each dataset to deal with data distribution\ndifferences. Compared with existing deep learning methods, CANet can achieve\nsatisfactory CD performances on various datasets simultaneously. Experimental\nresults on several public datasets have verified the effectiveness and\nadvantages of the proposed CANet on CD. CANet has a stronger generalization\nability, smaller training costs (merely updating 4.1%-7.7% parameters), and\nbetter performances under limited training datasets than other deep learning\nmethods, which also can be flexibly inserted with existing deep models."}
{"id": "2504.19600", "pdf": "https://arxiv.org/pdf/2504.19600", "abs": "https://arxiv.org/abs/2504.19600", "authors": ["Pengfei Zhang", "Shouqing Jia"], "title": "Image Generation Method Based on Heat Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image\ngeneration without adversarial training, but they process images as a whole.\nSince adjacent pixels are highly likely to belong to the same object, we\npropose the Heat Diffusion Model (HDM) to further preserve image details and\ngenerate more realistic images. HDM is a model that incorporates pixel-level\noperations while maintaining the same training process as DDPM. In HDM, the\ndiscrete form of the two-dimensional heat equation is integrated into the\ndiffusion and generation formulas of DDPM, enabling the model to compute\nrelationships between neighboring pixels during image processing. Our\nexperiments demonstrate that HDM can generate higher-quality samples compared\nto models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion\nModels (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN)."}
{"id": "2504.19614", "pdf": "https://arxiv.org/pdf/2504.19614", "abs": "https://arxiv.org/abs/2504.19614", "authors": ["Junpeng Jiang", "Gangyi Hong", "Miao Zhang", "Hengtong Hu", "Kun Zhan", "Rui Shao", "Liqiang Nie"], "title": "DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Collecting multi-view driving scenario videos to enhance the performance of\n3D visual perception tasks presents significant challenges and incurs\nsubstantial costs, making generative models for realistic data an appealing\nalternative. Yet, the videos generated by recent works suffer from poor quality\nand spatiotemporal consistency, undermining their utility in advancing\nperception tasks under driving scenarios. To address this gap, we propose DiVE,\na diffusion transformer-based generative framework meticulously engineered to\nproduce high-fidelity, temporally coherent, and cross-view consistent\nmulti-view videos, aligning seamlessly with bird's-eye view layouts and textual\ndescriptions. DiVE leverages a unified cross-attention and a SketchFormer to\nexert precise control over multimodal data, while incorporating a view-inflated\nattention mechanism that adds no extra parameters, thereby guaranteeing\nconsistency across views. Despite these advancements, synthesizing\nhigh-resolution videos under multimodal constraints introduces dual challenges:\ninvestigating the optimal classifier-free guidance coniguration under intricate\nmulti-condition inputs and mitigating excessive computational latency in\nhigh-resolution rendering--both of which remain underexplored in prior\nresearches. To resolve these limitations, we introduce two innovations:\nMulti-Control Auxiliary Branch Distillation, which streamlines multi-condition\nCFG selection while circumventing high computational overhead, and Resolution\nProgressive Sampling, a training-free acceleration strategy that staggers\nresolution scaling to reduce high latency due to high resolution. These\ninnovations collectively achieve a 2.62x speedup with minimal quality\ndegradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance\nin multi-view video generation, yielding photorealistic outputs with\nexceptional temporal and cross-view coherence."}
{"id": "2504.19634", "pdf": "https://arxiv.org/pdf/2504.19634", "abs": "https://arxiv.org/abs/2504.19634", "authors": ["Yechan Kim", "DongHo Yoon", "SooYeon Kim", "Moongu Jeon"], "title": "NSegment : Noisy Segment Improves Remote Sensing Image Segmentation", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Labeling errors in remote sensing (RS) image segmentation datasets often\nremain implicit and subtle due to ambiguous class boundaries, mixed pixels,\nshadows, complex terrain features, and subjective annotator bias. Furthermore,\nthe scarcity of annotated RS data due to high image acquisition and labeling\ncosts complicates training noise-robust models. While sophisticated mechanisms\nsuch as label selection or noise correction might address this issue, they tend\nto increase training time and add implementation complexity. In this letter, we\npropose NSegment-a simple yet effective data augmentation solution to mitigate\nthis issue. Unlike traditional methods, it applies elastic transformations only\nto segmentation labels, varying deformation intensity per sample in each\ntraining epoch to address annotation inconsistencies. Experimental results\ndemonstrate that our approach improves the performance of RS image segmentation\non various state-of-the-art models."}
{"id": "2504.19637", "pdf": "https://arxiv.org/pdf/2504.19637", "abs": "https://arxiv.org/abs/2504.19637", "authors": ["Junlong Ren", "Gangjian Zhang", "Yu Hu", "Jian Shu", "Hao Wang"], "title": "Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video\nthat is partially relevant to the text query. The primary challenge in PRVR\narises from the semantic asymmetry between textual and visual modalities, as\nvideos often contain substantial content irrelevant to the query. Existing\nmethods coarsely align paired videos and text queries to construct the semantic\nspace, neglecting the critical cross-modal dual nature inherent in this task:\ninter-sample correlation and intra-sample redundancy. To this end, we propose a\nnovel PRVR framework to systematically exploit these two characteristics. Our\nframework consists of three core modules. First, the Inter Correlation\nEnhancement (ICE) module captures inter-sample correlation by identifying\nsemantically similar yet unpaired text queries and video moments, combining\nthem to form pseudo-positive pairs for more robust semantic space construction.\nSecond, the Intra Redundancy Mining (IRM) module mitigates intra-sample\nredundancy by mining redundant video moment features and treating them as hard\nnegative samples, thereby encouraging the model to learn more discriminative\nrepresentations. Finally, to reinforce these modules, we introduce the Temporal\nCoherence Prediction (TCP) module, which enhances feature discrimination by\ntraining the model to predict the original temporal order of randomly shuffled\nvideo frames and moments. Extensive experiments on three datasets demonstrate\nthe superiority of our approach compared to previous methods, achieving\nstate-of-the-art results."}
{"id": "2504.19643", "pdf": "https://arxiv.org/pdf/2504.19643", "abs": "https://arxiv.org/abs/2504.19643", "authors": ["Pin-Chi Pan", "Soo-Chang Pei"], "title": "BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, and 11 tables", "summary": "Underwater instance segmentation is challenging due to adverse visual\nconditions such as light attenuation, scattering, and color distortion, which\ndegrade model performance. In this work, we propose BARIS-Decoder\n(Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that\nenhances segmentation accuracy through feature refinement. To address\nunderwater degradations, we introduce the Environmental Robust Adapter (ERA),\nwhich efficiently models underwater degradation patterns while reducing\ntrainable parameters by over 90\\% compared to full fine-tuning. The integration\nof BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves\nstate-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B\nbackbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the\neffectiveness of BARIS-ERA in advancing underwater instance segmentation,\nproviding a robust and efficient solution."}
{"id": "2504.19646", "pdf": "https://arxiv.org/pdf/2504.19646", "abs": "https://arxiv.org/abs/2504.19646", "authors": ["Anjith George", "Sebastien Marcel"], "title": "xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Heterogeneous Face Recognition (HFR) addresses the challenge of matching face\nimages across different sensing modalities, such as thermal to visible or\nnear-infrared to visible, expanding the applicability of face recognition\nsystems in real-world, unconstrained environments. While recent HFR methods\nhave shown promising results, many rely on computation-intensive architectures,\nlimiting their practicality for deployment on resource-constrained edge\ndevices. In this work, we present a lightweight yet effective HFR framework by\nadapting a hybrid CNN-Transformer architecture originally designed for face\nrecognition. Our approach enables efficient end-to-end training with minimal\npaired heterogeneous data while preserving strong performance on standard RGB\nface recognition tasks. This makes it a compelling solution for both\nhomogeneous and heterogeneous scenarios. Extensive experiments across multiple\nchallenging HFR and face recognition benchmarks demonstrate that our method\nconsistently outperforms state-of-the-art approaches while maintaining a low\ncomputational overhead."}
{"id": "2504.19682", "pdf": "https://arxiv.org/pdf/2504.19682", "abs": "https://arxiv.org/abs/2504.19682", "authors": ["Nikolaos Chaidos", "Angeliki Dimitriou", "Nikolaos Spanos", "Athanasios Voulodimos", "Giorgos Stamou"], "title": "Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 3 figures, accepted for presentation at\n  xAI-World-Conference 2025, code is available at\n  https://github.com/nickhaidos/Vision-GNNs-Explainer", "summary": "Graph Neural Networks (GNNs) have emerged as an efficient alternative to\nconvolutional approaches for vision tasks such as image classification,\nleveraging patch-based representations instead of raw pixels. These methods\nconstruct graphs where image patches serve as nodes, and edges are established\nbased on patch similarity or classification relevance. Despite their\nefficiency, the explainability of GNN-based vision models remains\nunderexplored, even though graphs are naturally interpretable. In this work, we\nanalyze the semantic consistency of the graphs formed at different layers of\nGNN-based image classifiers, focusing on how well they preserve object\nstructures and meaningful relationships. A comprehensive analysis is presented\nby quantifying the extent to which inter-layer graph connections reflect\nsemantic similarity and spatial coherence. Explanations from standard and\nadversarial settings are also compared to assess whether they reflect the\nclassifiers' robustness. Additionally, we visualize the flow of information\nacross layers through heatmap-based visualization techniques, thereby\nhighlighting the models' explainability. Our findings demonstrate that the\ndecision-making processes of these models can be effectively explained, while\nalso revealing that their reasoning does not necessarily align with human\nperception, especially in deeper layers."}
{"id": "2504.19684", "pdf": "https://arxiv.org/pdf/2504.19684", "abs": "https://arxiv.org/abs/2504.19684", "authors": ["Anush Lakshman Sivaraman", "Kojo Adu-Gyamfi", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate weather classification from low-quality traffic camera imagery\nremains a challenging task, particularly under adverse nighttime conditions. In\nthis study, we propose a scalable framework that combines generative domain\nadaptation with efficient contrastive learning to enhance classification\nperformance. Using CycleGAN-based domain translation, we improve the quality of\nnighttime images, enabling better feature extraction by downstream models.\nWhile the baseline EVA-02 model employing CLIP-based contrastive loss achieves\nan overall accuracy of 96.55\\%, it exhibits a significant performance gap\nbetween daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP\nwith the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive\noverall accuracy of 94.00\\%, with substantial improvements in nighttime\nperformance (85.90\\% accuracy). The combination of Vision-SigLIP-2,\nText-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime\naccuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN\nmaintains the highest overall accuracy (97.01\\%) and per-class accuracies.\nThese findings demonstrate the potential of combining domain adaptation and\nefficient contrastive learning to build practical, resource-efficient weather\nclassification systems for intelligent transportation infrastructure."}
{"id": "2504.19687", "pdf": "https://arxiv.org/pdf/2504.19687", "abs": "https://arxiv.org/abs/2504.19687", "authors": ["Baoshun Shi", "Bing Chen", "Shaolei Zhang", "Huazhu Fu", "Zhanli Hu"], "title": "Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR", "categories": ["cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it\nwill potentially degrade image quality, even yields metal artifacts at the case\nof metallic implants. For simultaneous LDCT reconstruction and metal artifact\nreduction (LDMAR), existing deep learning-based efforts face two main\nlimitations: i) the network design neglects multi-scale and within-scale\ninformation; ii) training a distinct model for each dose necessitates\nsignificant storage space for multiple doses. To fill these gaps, we propose a\nprompt guiding multi-scale adaptive sparse representation-driven network,\nabbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet\ninspired from multi-scale sparsifying frames, and it can simultaneously employ\nwithin-scale characteristics and cross-scale complementarity owing to an\nelaborated prompt guiding scale-adaptive threshold generator (PSATG) and a\nbuilt multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively\ncapture multiple contextual information to generate more faithful thresholds,\nachieved by fusing features from local, regional, and global levels.\nFurthermore, we elaborate a model interpretable dual domain LDMAR framework\ncalled PDuMSRNet, and train single model with a prompt guiding strategy for\nmultiple dose levels. We build a prompt guiding module, whose input contains\ndose level, metal mask and input instance, to provide various guiding\ninformation, allowing a single model to accommodate various CT dose settings.\nExtensive experiments at various dose levels demonstrate that the proposed\nmethods outperform the state-of-the-art LDMAR methods."}
{"id": "2504.19695", "pdf": "https://arxiv.org/pdf/2504.19695", "abs": "https://arxiv.org/abs/2504.19695", "authors": ["Lucas Morin", "Gerhard Ingmar Meijer", "Valéry Weber", "Luc Van Gool", "Peter W. J. Staar"], "title": "SubGrapher: Visual Fingerprinting of Chemical Structures", "categories": ["cs.CV"], "comment": null, "summary": "Automatic extraction of chemical structures from scientific literature plays\na crucial role in accelerating research across fields ranging from drug\ndiscovery to materials science. Patent documents, in particular, contain\nmolecular information in visual form, which is often inaccessible through\ntraditional text-based searches. In this work, we introduce SubGrapher, a\nmethod for the visual fingerprinting of chemical structure images. Unlike\nconventional Optical Chemical Structure Recognition (OCSR) models that attempt\nto reconstruct full molecular graphs, SubGrapher focuses on extracting\nmolecular fingerprints directly from chemical structure images. Using\nlearning-based instance segmentation, SubGrapher identifies functional groups\nand carbon backbones, constructing a substructure-based fingerprint that\nenables chemical structure retrieval. Our approach is evaluated against\nstate-of-the-art OCSR and fingerprinting methods, demonstrating superior\nretrieval performance and robustness across diverse molecular depictions. The\ndataset, models, and code will be made publicly available."}
{"id": "2504.19706", "pdf": "https://arxiv.org/pdf/2504.19706", "abs": "https://arxiv.org/abs/2504.19706", "authors": ["Song Xia", "Yi Yu", "Henghui Ding", "Wenhan Yang", "Shifei Liu", "Alex C. Kot", "Xudong Jiang"], "title": "Open-set Anomaly Segmentation in Complex Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Precise segmentation of out-of-distribution (OoD) objects, herein referred to\nas anomalies, is crucial for the reliable deployment of semantic segmentation\nmodels in open-set, safety-critical applications, such as autonomous driving.\nCurrent anomalous segmentation benchmarks predominantly focus on favorable\nweather conditions, resulting in untrustworthy evaluations that overlook the\nrisks posed by diverse meteorological conditions in open-set environments, such\nas low illumination, dense fog, and heavy rain. To bridge this gap, this paper\nintroduces the ComsAmy, a challenging benchmark specifically designed for\nopen-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide\nspectrum of adverse weather conditions, dynamic driving environments, and\ndiverse anomaly types to comprehensively evaluate the model performance in\nrealistic open-world scenarios. Our extensive evaluation of several\nstate-of-the-art anomalous segmentation models reveals that existing methods\ndemonstrate significant deficiencies in such challenging scenarios,\nhighlighting their serious safety risks for real-world deployment. To solve\nthat, we propose a novel energy-entropy learning (EEL) strategy that integrates\nthe complementary information from energy and entropy to bolster the robustness\nof anomaly segmentation under complex open-world environments. Additionally, a\ndiffusion-based anomalous training data synthesizer is proposed to generate\ndiverse and high-quality anomalous images to enhance the existing copy-paste\ntraining data synthesizer. Extensive experimental results on both public and\nComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer\nwith energy and entropy learning (DiffEEL) serves as an effective and\ngeneralizable plug-and-play method to enhance existing models, yielding an\naverage improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in\n$\\rm{FPR}_{95}$."}
{"id": "2504.19719", "pdf": "https://arxiv.org/pdf/2504.19719", "abs": "https://arxiv.org/abs/2504.19719", "authors": ["Lukas Folkman", "Quynh LK Vo", "Colin Johnston", "Bela Stantic", "Kylie A Pitt"], "title": "A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms", "categories": ["cs.CV"], "comment": null, "summary": "The increasing demand for aquaculture production necessitates the development\nof innovative, intelligent tools to effectively monitor and manage fish health\nand welfare. While non-invasive video monitoring has become a common practice\nin finfish aquaculture, existing intelligent monitoring methods predominantly\nfocus on assessing body condition or fish swimming patterns and are often\ndeveloped and evaluated in controlled tank environments, without demonstrating\ntheir applicability to real-world aquaculture settings in open sea farms. This\nunderscores the necessity for methods that can monitor physiological traits\ndirectly within the production environment of sea fish farms. To this end, we\nhave developed a computer vision method for monitoring ventilation rates of\nAtlantic salmon (Salmo salar), which was specifically designed for videos\nrecorded in the production environment of commercial sea fish farms using the\nexisting infrastructure. Our approach uses a fish head detection model, which\nclassifies the mouth state as either open or closed using a convolutional\nneural network. This is followed with multiple object tracking to create\ntemporal sequences of fish swimming across the field of view of the underwater\nvideo camera to estimate ventilation rates. The method demonstrated high\nefficiency, achieving a Pearson correlation coefficient of 0.82 between ground\ntruth and predicted ventilation rates in a test set of 100 fish collected\nindependently of the training data. By accurately identifying pens where fish\nexhibit signs of respiratory distress, our method offers broad applicability\nand the potential to transform fish health and welfare monitoring in finfish\naquaculture."}
{"id": "2504.19722", "pdf": "https://arxiv.org/pdf/2504.19722", "abs": "https://arxiv.org/abs/2504.19722", "authors": ["Rupert Polley", "Nikolai Polley", "Dominik Heid", "Marc Heinrich", "Sven Ochs", "J. Marius Zöllner"], "title": "The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IEEE Intelligent Vehicles Symposium (IV 2025). Dataset\n  link: https://url.fzi.de/ATLAS", "summary": "Traffic light perception is an essential component of the camera-based\nperception system for autonomous vehicles, enabling accurate detection and\ninterpretation of traffic lights to ensure safe navigation through complex\nurban environments. In this work, we propose a modularized perception framework\nthat integrates state-of-the-art detection models with a novel real-time\nassociation and decision framework, enabling seamless deployment into an\nautonomous driving stack. To address the limitations of existing public\ndatasets, we introduce the ATLAS dataset, which provides comprehensive\nannotations of traffic light states and pictograms across diverse environmental\nconditions and camera setups. This dataset is publicly available at\nhttps://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art\ntraffic light detection architectures on ATLAS, demonstrating significant\nperformance improvements in both accuracy and robustness. Finally, we evaluate\nthe framework in real-world scenarios by deploying it in an autonomous vehicle\nto make decisions at traffic light-controlled intersections, highlighting its\nreliability and effectiveness for real-time operation."}
{"id": "2504.19724", "pdf": "https://arxiv.org/pdf/2504.19724", "abs": "https://arxiv.org/abs/2504.19724", "authors": ["Haofan Wang", "Yujia Xu", "Yimeng Li", "Junchen Li", "Chaowei Zhang", "Jing Wang", "Kejia Yang", "Zhibo Chen"], "title": "RepText: Rendering Visual Text via Replicating", "categories": ["cs.CV"], "comment": "Technical Report. https://reptext.github.io/", "summary": "Although contemporary text-to-image generation models have achieved\nremarkable breakthroughs in producing visually appealing images, their capacity\nto generate precise and flexible typographic elements, especially non-Latin\nalphabets, remains constrained. To address these limitations, we start from an\nnaive assumption that text understanding is only a sufficient condition for\ntext rendering, but not a necessary condition. Based on this, we present\nRepText, which aims to empower pre-trained monolingual text-to-image generation\nmodels with the ability to accurately render, or more precisely, replicate,\nmultilingual visual text in user-specified fonts, without the need to really\nunderstand them. Specifically, we adopt the setting from ControlNet and\nadditionally integrate language agnostic glyph and position of rendered text to\nenable generating harmonized visual text, allowing users to customize text\ncontent, font and position on their needs. To improve accuracy, a text\nperceptual loss is employed along with the diffusion loss. Furthermore, to\nstabilize rendering process, at the inference phase, we directly initialize\nwith noisy glyph latent instead of random initialization, and adopt region\nmasks to restrict the feature injection to only the text region to avoid\ndistortion of the background. We conducted extensive experiments to verify the\neffectiveness of our RepText relative to existing works, our approach\noutperforms existing open-source methods and achieves comparable results to\nnative multi-language closed-source models. To be more fair, we also\nexhaustively discuss its limitations in the end."}
{"id": "2504.19735", "pdf": "https://arxiv.org/pdf/2504.19735", "abs": "https://arxiv.org/abs/2504.19735", "authors": ["Rustam Tagiew", "Prasannavenkatesh Balaji"], "title": "Measuring Train Driver Performance as Key to Approval of Driverless Trains", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, abstract accepted by IAVVC 2025, full paper to be\n  submitted to IAVVC 2025", "summary": "Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation\n(EU) No. 402/2013 allow a simplified approach for the safety approval of\ncomputer vision systems for driverless trains, if they have 'similar' functions\nand interfaces as the replaced human driver. The human driver is not replaced\none-to-one by a technical system - only a limited set of cognitive functions\nare replaced. However, performance in the most challenging function, obstacle\ndetection, is difficult to quantify due to the deficiency of published\nmeasurement results. This article summarizes the data published so far. This\narticle also goes a long way to remedy this situation by providing a new public\nand anonymized dataset of 711 train driver performance measurements from\ncontrolled experiments. The measurements are made for different speeds,\nobstacle sizes, train protection systems and obstacle color contrasts\nrespectively. The measured values are reaction time and distance to the\nobstacle. The goal of this paper is an unbiased and exhaustive description of\nthe presented dataset for research, standardization and regulation. Further\nproject related information including the dataset and source code is available\nat https://atosense-02371c.usercontent.opencode.de/"}
{"id": "2504.19737", "pdf": "https://arxiv.org/pdf/2504.19737", "abs": "https://arxiv.org/abs/2504.19737", "authors": ["Abhishek Kuriyal", "Elliot Vincent", "Mathieu Aubry", "Loic Landrieu"], "title": "CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis", "categories": ["cs.CV"], "comment": "CVPR 2025 EarthVision Workshop", "summary": "Global variations in terrain appearance raise a major challenge for satellite\nimage analysis, leading to poor model performance when training on locations\nthat differ from those encountered at test time. This remains true even with\nrecent large global datasets. To address this challenge, we propose a novel\ndomain-generalization framework for satellite images. Instead of trying to\nlearn a single generalizable model, we train one expert model per training\ndomain, while learning experts' similarity and encouraging similar experts to\nbe consistent. A model selection module then identifies the most suitable\nexperts for a given test sample and aggregates their predictions. Experiments\non four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent\ngains over existing domain generalization and adaptation methods. Our code is\npublicly available at https://github.com/Abhishek19009/CoDEx."}
{"id": "2504.19739", "pdf": "https://arxiv.org/pdf/2504.19739", "abs": "https://arxiv.org/abs/2504.19739", "authors": ["Muzammil Behzad", "Guoying Zhao"], "title": "Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce AffectVLM, a vision-language model designed to\nintegrate multiviews for a semantically rich and visually comprehensive\nunderstanding of facial emotions from 3D/4D data. To effectively capture visual\nfeatures, we propose a joint representation learning framework paired with a\nnovel gradient-friendly loss function that accelerates model convergence\ntowards optimal feature representation. Additionally, we introduce augmented\ntextual prompts to enhance the model's linguistic capabilities and employ mixed\nview augmentation to expand the visual dataset. We also develop a Streamlit app\nfor a real-time interactive inference and enable the model for distributed\nlearning. Extensive experiments validate the superior performance of AffectVLM\nacross multiple benchmarks."}
{"id": "2504.19742", "pdf": "https://arxiv.org/pdf/2504.19742", "abs": "https://arxiv.org/abs/2504.19742", "authors": ["Valerie Zermatten", "Javiera Castillo-Navarro", "Pallavi Jain", "Devis Tuia", "Diego Marcos"], "title": "EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia", "categories": ["cs.CV"], "comment": "Accepted at EarthVision 2025 (CVPRW 2025)", "summary": "The presence of species provides key insights into the ecological properties\nof a location such as land cover, climatic conditions or even soil properties.\nWe propose a method to predict such ecological properties directly from remote\nsensing (RS) images by aligning them with species habitat descriptions. We\nintroduce the EcoWikiRS dataset, consisting of high-resolution aerial images,\nthe corresponding geolocated species observations, and, for each species, the\ntextual descriptions of their habitat from Wikipedia. EcoWikiRS offers a\nscalable way of supervision for RS vision language models (RS-VLMs) for\necology. This is a setting with weak and noisy supervision, where, for\ninstance, some text may describe properties that are specific only to part of\nthe species' niche or is irrelevant to a specific image. We tackle this by\nproposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model\non the task of ecosystem zero-shot classification by following the habitat\ndefinitions from the European Nature Information System (EUNIS). Our results\nshow that our approach helps in understanding RS images in a more ecologically\nmeaningful manner. The code and the dataset are available at\nhttps://github.com/eceo-epfl/EcoWikiRS."}
{"id": "2504.19749", "pdf": "https://arxiv.org/pdf/2504.19749", "abs": "https://arxiv.org/abs/2504.19749", "authors": ["Zhimin Liao", "Ping Wei", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy and scene flow offer a detailed and dynamic representation of 3D\nscene. Recognizing the sparsity and complexity of 3D space, previous\nvision-centric methods have employed implicit learning-based approaches to\nmodel spatial and temporal information. However, these approaches struggle to\ncapture local details and diminish the model's spatial discriminative ability.\nTo address these challenges, we propose a novel explicit state-based modeling\nmethod designed to leverage the occupied state to renovate the 3D features.\nSpecifically, we propose a sparse occlusion-aware attention mechanism,\nintegrated with a cascade refinement strategy, which accurately renovates 3D\nfeatures with the guidance of occupied state information. Additionally, we\nintroduce a novel method for modeling long-term dynamic interactions, which\nreduces computational costs and preserves spatial information. Compared to the\nprevious state-of-the-art methods, our efficient explicit renovation strategy\nnot only delivers superior performance in terms of RayIoU and mAVE for\noccupancy and scene flow prediction but also markedly reduces GPU memory usage\nduring training, bringing it down to 8.7GB. Our code is available on\nhttps://github.com/lzzzzzm/STCOcc"}
{"id": "2504.19755", "pdf": "https://arxiv.org/pdf/2504.19755", "abs": "https://arxiv.org/abs/2504.19755", "authors": ["Kapil Kashyap", "Sean Fargose", "Chrisil Dabre", "Fatema Dolaria", "Nilesh Patil", "Aniket Kore"], "title": "Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Liver cirrhosis is an insidious condition involving the substitution of\nnormal liver tissue with fibrous scar tissue and causing major health\ncomplications. The conventional method of diagnosis using liver biopsy is\ninvasive and, therefore, inconvenient for use in regular screening. In this\npaper,we present a hybrid model that combines machine learning techniques with\nclinical data and ultrasoundscans to improve liver fibrosis and cirrhosis\ndetection accuracy is presented. The model integrates fixed blood test\nprobabilities with deep learning model predictions (DenseNet-201) for\nultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The\nfindings establish the viability of the combined model in enhancing diagnosis\naccuracy and supporting early intervention in liver disease care."}
{"id": "2504.19819", "pdf": "https://arxiv.org/pdf/2504.19819", "abs": "https://arxiv.org/abs/2504.19819", "authors": ["Hoang Chuong Nguyen", "Wei Mao", "Jose M. Alvarez", "Miaomiao Liu"], "title": "Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video", "categories": ["cs.CV"], "comment": null, "summary": "Neural Radiance Fields (NeRF) has demonstrated its superior capability to\nrepresent 3D geometry but require accurately precomputed camera poses during\ntraining. To mitigate this requirement, existing methods jointly optimize\ncamera poses and NeRF often relying on good pose initialisation or depth\npriors. However, these approaches struggle in challenging scenarios, such as\nlarge rotations, as they map each camera to a world coordinate system. We\npropose a novel method that eliminates prior dependencies by modeling\ncontinuous camera motions as time-dependent angular velocity and velocity.\nRelative motions between cameras are learned first via velocity integration,\nwhile camera poses can be obtained by aggregating such relative motions up to a\nworld coordinate system defined at a single time step within the video.\nSpecifically, accurate continuous camera movements are learned through a\ntime-dependent NeRF, which captures local scene geometry and motion by training\nfrom neighboring frames for each time step. The learned motions enable\nfine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D\nand Scannet show our approach achieves superior camera pose and depth\nestimation and comparable novel-view synthesis performance compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/HoangChuongNguyen/cope-nerf."}
{"id": "2504.19824", "pdf": "https://arxiv.org/pdf/2504.19824", "abs": "https://arxiv.org/abs/2504.19824", "authors": ["Mohamed Hassan", "Mohammad Wasil", "Sebastian Houben"], "title": "Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning (CL) approaches have gained great recognition as a very\nsuccessful subset of self-supervised learning (SSL) methods. SSL enables\nlearning from unlabeled data, a crucial step in the advancement of deep\nlearning, particularly in computer vision (CV), given the plethora of unlabeled\nimage data. CL works by comparing different random augmentations (e.g.,\ndifferent crops) of the same image, thus achieving self-labeling. Nevertheless,\nrandomly augmenting images and especially random cropping can result in an\nimage that is semantically very distant from the original and therefore leads\nto false labeling, hence undermining the efficacy of the methods. In this\nresearch, two novel parameterized cropping methods are introduced that increase\nthe robustness of self-labeling and consequently increase the efficacy. The\nresults show that the use of these methods significantly improves the accuracy\nof the model by between 2.7\\% and 12.4\\% on the downstream task of classifying\nCIFAR-10, depending on the crop size compared to that of the non-parameterized\nrandom cropping method."}
{"id": "2504.19828", "pdf": "https://arxiv.org/pdf/2504.19828", "abs": "https://arxiv.org/abs/2504.19828", "authors": ["Zhiming Hu", "Daniel Haeufle", "Syn Schmitt", "Andreas Bulling"], "title": "HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination", "categories": ["cs.CV"], "comment": "Accepted at SIGGRAPH 2025, link:\n  https://zhiminghu.net/hu25_hoigaze.html", "summary": "We present HOIGaze - a novel learning-based approach for gaze estimation\nduring hand-object interactions (HOI) in extended reality (XR). HOIGaze\naddresses the challenging HOI setting by building on one key insight: The eye,\nhand, and head movements are closely coordinated during HOIs and this\ncoordination can be exploited to identify samples that are most useful for gaze\nestimator training - as such, effectively denoising the training data. This\ndenoising approach is in stark contrast to previous gaze estimation methods\nthat treated all training samples as equal. Specifically, we propose: 1) a\nnovel hierarchical framework that first recognises the hand currently visually\nattended to and then estimates gaze direction based on the attended hand; 2) a\nnew gaze estimator that uses cross-modal Transformers to fuse head and\nhand-object features extracted using a convolutional neural network and a\nspatio-temporal graph convolutional network; and 3) a novel eye-head\ncoordination loss that upgrades training samples belonging to the coordinated\neye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin\n(ADT) datasets and show that it significantly outperforms state-of-the-art\nmethods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in\nmean angular error. To demonstrate the potential of our method, we further\nreport significant performance improvements for the sample downstream task of\neye-based activity recognition on ADT. Taken together, our results underline\nthe significant information content available in eye-hand-head coordination\nand, as such, open up an exciting new direction for learning-based gaze\nestimation."}
{"id": "2504.19834", "pdf": "https://arxiv.org/pdf/2504.19834", "abs": "https://arxiv.org/abs/2504.19834", "authors": ["Xiaoyu Liu", "Mingshuai Yao", "Yabo Zhang", "Xianhui Lin", "Peiran Ren", "Xiaoming Li", "Ming Liu", "Wangmeng Zuo"], "title": "AnimateAnywhere: Rouse the Background in Human Image Animation", "categories": ["cs.CV"], "comment": null, "summary": "Human image animation aims to generate human videos of given characters and\nbackgrounds that adhere to the desired pose sequence. However, existing methods\nfocus more on human actions while neglecting the generation of background,\nwhich typically leads to static results or inharmonious movements. The\ncommunity has explored camera pose-guided animation tasks, yet preparing the\ncamera trajectory is impractical for most entertainment applications and\nordinary users. As a remedy, we present an AnimateAnywhere framework, rousing\nthe background in human image animation without requirements on camera\ntrajectories. In particular, based on our key insight that the movement of the\nhuman body often reflects the motion of the background, we introduce a\nbackground motion learner (BML) to learn background motions from human pose\nsequences. To encourage the model to learn more accurate cross-frame\ncorrespondences, we further deploy an epipolar constraint on the 3D attention\nmap. Specifically, the mask used to suppress geometrically unreasonable\nattention is carefully constructed by combining an epipolar mask and the\ncurrent 3D attention map. Extensive experiments demonstrate that our\nAnimateAnywhere effectively learns the background motion from human pose\nsequences, achieving state-of-the-art performance in generating human animation\nresults with vivid and realistic backgrounds. The source code and model will be\navailable at https://github.com/liuxiaoyu1104/AnimateAnywhere."}
{"id": "2504.19839", "pdf": "https://arxiv.org/pdf/2504.19839", "abs": "https://arxiv.org/abs/2504.19839", "authors": ["Yulong Guo", "Zilun Zhang", "Yongheng Shang", "Tiancheng Zhao", "Shuiguang Deng", "Yingchun Yang", "Jianwei Yin"], "title": "SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation", "categories": ["cs.CV"], "comment": "None", "summary": "The long-tail problem presents a significant challenge to the advancement of\nsemantic segmentation in ultra-high-resolution (UHR) satellite imagery. While\nprevious efforts in UHR semantic segmentation have largely focused on\nmulti-branch network architectures that emphasize multi-scale feature\nextraction and fusion, they have often overlooked the importance of addressing\nthe long-tail issue. In contrast to prior UHR methods that focused on\nindependent feature extraction, we emphasize data augmentation and multimodal\nfeature fusion to alleviate the long-tail problem. In this paper, we introduce\nSRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our\napproach addresses the long-tail class distribution by incorporating a\nmulti-scale cropping technique alongside a data augmentation strategy based on\nsemantic reordering and resampling. To further enhance model performance, we\npropose a multimodal fusion-based general representation knowledge injection\nmethod, which, for the first time, fuses text and visual features without the\nneed for individual region text descriptions, extracting more robust features.\nExtensive experiments on the URUR, GID, and FBP datasets demonstrate that our\nmethod improves mIoU by 3.33\\%, 0.66\\%, and 0.98\\%, respectively, achieving\nstate-of-the-art performance. Code is available at:\nhttps://github.com/BinSpa/SRMF.git."}
{"id": "2504.19847", "pdf": "https://arxiv.org/pdf/2504.19847", "abs": "https://arxiv.org/abs/2504.19847", "authors": ["Juhan Park", "Kyungjae Lee", "Hyung Jin Chang", "Jungchan Cho"], "title": "Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we introduce Segmentation to Human-Object Interaction\n(\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates\nsegmentation-based vision foundation models with the human-object interaction\ntask, distinguished from traditional detection-based Human-Object Interaction\n(HOI) methods. Our approach enhances HOI detection by not only predicting the\nstandard triplets but also introducing quadruplets, which extend HOI triplets\nby including segmentation masks for human-object pairs. More specifically,\nSeg2HOI inherits the properties of the vision foundation model (e.g.,\npromptable and interactive mechanisms) and incorporates a decoder that applies\nthese attributes to HOI task. Despite training only for HOI, without additional\ntraining mechanisms for these properties, the framework demonstrates that such\nfeatures still operate efficiently. Extensive experiments on two public\nbenchmark datasets demonstrate that Seg2HOI achieves performance comparable to\nstate-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that\nSeg2HOI can generate HOI quadruplets and interactive HOI segmentation from\nnovel text and visual prompts that were not used during training, making it\nversatile for a wide range of applications by leveraging this flexibility."}
{"id": "2504.19860", "pdf": "https://arxiv.org/pdf/2504.19860", "abs": "https://arxiv.org/abs/2504.19860", "authors": ["Chenhan Jiang", "Yihan Zeng", "Hang Xu", "Dit-Yan Yeung"], "title": "CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) has achieved remarkable success in\ntext-to-3D content generation. However, SDS-based methods struggle to maintain\nsemantic fidelity for user prompts, particularly when involving multiple\nobjects with intricate interactions. While existing approaches often address 3D\nconsistency through multiview diffusion model fine-tuning on 3D datasets, this\nstrategy inadvertently exacerbates text-3D alignment degradation. The\nlimitation stems from SDS's inherent accumulation of view-independent biases\nduring optimization, which progressively diverges from the ideal text alignment\ndirection. To alleviate this limitation, we propose a novel SDS objective,\ndubbed as Textual Coherent Score Distillation (TCSD), which integrates\nalignment feedback from multimodal large language models (MLLMs). Our TCSD\nleverages cross-modal understanding capabilities of MLLMs to assess and guide\nthe text-3D correspondence during the optimization. We further develop\n3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text\nalignment in 3D generations. Additionally, we introduce an LLM-layout\ninitialization that significantly accelerates optimization convergence through\nsemantic-aware spatial configuration. Comprehensive evaluations demonstrate\nthat our framework, CoherenDream, establishes state-of-the-art performance in\ntext-aligned 3D generation across multiple benchmarks, including T$^3$Bench and\nTIFA subset. Qualitative results showcase the superior performance of\nCoherenDream in preserving textual consistency and semantic interactions. As\nthe first study to incorporate MLLMs into SDS optimization, we also conduct\nextensive ablation studies to explore optimal MLLM adaptations for 3D\ngeneration tasks."}
{"id": "2504.19863", "pdf": "https://arxiv.org/pdf/2504.19863", "abs": "https://arxiv.org/abs/2504.19863", "authors": ["Daniel Kienzle", "Robin Schön", "Rainer Lienhart", "Shin'Ichi Satoh"], "title": "Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "To be published in 2025 IEEE/CVF International Conference on Computer\n  Vision and Pattern Recognition Workshops (CVPRW)", "summary": "Analyzing a player's technique in table tennis requires knowledge of the\nball's 3D trajectory and spin. While, the spin is not directly observable in\nstandard broadcasting videos, we show that it can be inferred from the ball's\ntrajectory in the video. We present a novel method to infer the initial spin\nand 3D trajectory from the corresponding 2D trajectory in a video. Without\nground truth labels for broadcast videos, we train a neural network solely on\nsynthetic data. Due to the choice of our input data representation, physically\ncorrect synthetic training data, and using targeted augmentations, the network\nnaturally generalizes to real data. Notably, these simple techniques are\nsufficient to achieve generalization. No real data at all is required for\ntraining. To the best of our knowledge, we are the first to present a method\nfor spin and trajectory prediction in simple monocular broadcast videos,\nachieving an accuracy of 92.0% in spin classification and a 2D reprojection\nerror of 0.19% of the image diagonal."}
{"id": "2504.19876", "pdf": "https://arxiv.org/pdf/2504.19876", "abs": "https://arxiv.org/abs/2504.19876", "authors": ["Mamadou Keita", "Wassim Hamidouche", "Hessen Bougueffa Eutamene", "Abdelmalik Taleb-Ahmed", "Abdenour Hadid"], "title": "DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "This paper introduces DeeCLIP, a novel framework for detecting AI-generated\nimages using CLIP-ViT and fusion learning. Despite significant advancements in\ngenerative models capable of creating highly photorealistic images, existing\ndetection methods often struggle to generalize across different models and are\nhighly sensitive to minor perturbations. To address these challenges, DeeCLIP\nincorporates DeeFuser, a fusion module that combines high-level and low-level\nfeatures, improving robustness against degradations such as compression and\nblurring. Additionally, we apply triplet loss to refine the embedding space,\nenhancing the model's ability to distinguish between real and synthetic\ncontent. To further enable lightweight adaptation while preserving pre-trained\nknowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation\n(LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot\nlearning without sacrificing generalization. Trained exclusively on 4-class\nProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets\ncomposed of generative adversarial network (GAN) and diffusion models. Despite\nhaving fewer trainable parameters, DeeCLIP outperforms existing methods,\ndemonstrating superior robustness against various generative models and\nreal-world distortions. The code is publicly available at\nhttps://github.com/Mamadou-Keita/DeeCLIP for research purposes."}
{"id": "2504.19881", "pdf": "https://arxiv.org/pdf/2504.19881", "abs": "https://arxiv.org/abs/2504.19881", "authors": ["Claire Warwick", "Andrew Beresford", "Soazig Casteau", "Hubert P. H. Shum", "Dan Smith", "Francis Xiatian Zhang"], "title": "Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK", "categories": ["cs.CV"], "comment": null, "summary": "The following paper describes a collaborative project involving researchers\nat Durham University, and professionals at the Bowes Museum, Barnard Castle,\nCounty Durham, UK, during which we used fixed and mobile eye tracking to\nunderstand how visitors view art. Our study took place during summer 2024 and\nbuilds on work presented at DH2017 (Bailey-Ross et al., 2017). Our\ninterdisciplinary team included researchers from digital humanities,\npsychology, art history and computer science, working in collaboration with\nprofessionals from the museum. We used fixed and mobile eye tracking to\nunderstand how museum visitors view art in a physical gallery setting. This\nresearch will enable us to make recommendations about how the Museum's\ncollections could be more effectively displayed, encouraging visitors to engage\nwith them more fully."}
{"id": "2504.19882", "pdf": "https://arxiv.org/pdf/2504.19882", "abs": "https://arxiv.org/abs/2504.19882", "authors": ["Runhui Zhang", "Sijin Zhou", "Zhuang Qi"], "title": "Federated Out-of-Distribution Generalization: A Causal Augmentation View", "categories": ["cs.CV"], "comment": "IJCNN 2025 Accepted", "summary": "Federated learning aims to collaboratively model by integrating multi-source\ninformation to obtain a model that can generalize across all client data.\nExisting methods often leverage knowledge distillation or data augmentation to\nmitigate the negative impact of data bias across clients. However, the limited\nperformance of teacher models on out-of-distribution samples and the inherent\nquality gap between augmented and original data hinder their effectiveness and\nthey typically fail to leverage the advantages of incorporating rich contextual\ninformation. To address these limitations, this paper proposes a Federated\nCausal Augmentation method, termed FedCAug, which employs causality-inspired\ndata augmentation to break the spurious correlation between attributes and\ncategories. Specifically, it designs a causal region localization module to\naccurately identify and decouple the background and objects in the image,\nproviding rich contextual information for causal data augmentation.\nAdditionally, it designs a causality-inspired data augmentation module that\nintegrates causal features and within-client context to generate counterfactual\nsamples. This significantly enhances data diversity, and the entire process\ndoes not require any information sharing between clients, thereby contributing\nto the protection of data privacy. Extensive experiments conducted on three\ndatasets reveal that FedCAug markedly reduces the model's reliance on\nbackground to predict sample labels, achieving superior performance compared to\nstate-of-the-art methods."}
{"id": "2504.19888", "pdf": "https://arxiv.org/pdf/2504.19888", "abs": "https://arxiv.org/abs/2504.19888", "authors": ["Han Chen", "Anne L. Martel"], "title": "Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: The scarcity of high-quality curated labeled medical training data\nremains one of the major limitations in applying artificial intelligence (AI)\nsystems to breast cancer diagnosis. Deep models for mammogram analysis and mass\n(or micro-calcification) detection require training with a large volume of\nlabeled images, which are often expensive and time-consuming to collect. To\nreduce this challenge, we proposed a novel method that leverages\nself-supervised learning (SSL) and a deep hybrid model, named \\textbf{HybMNet},\nwhich combines local self-attention and fine-grained feature extraction to\nenhance breast cancer detection on screening mammograms.\n  Approach: Our method employs a two-stage learning process: (1) SSL\nPretraining: We utilize EsViT, a SSL technique, to pretrain a Swin Transformer\n(Swin-T) using a limited set of mammograms. The pretrained Swin-T then serves\nas the backbone for the downstream task. (2) Downstream Training: The proposed\nHybMNet combines the Swin-T backbone with a CNN-based network and a novel\nfusion strategy. The Swin-T employs local self-attention to identify\ninformative patch regions from the high-resolution mammogram, while the\nCNN-based network extracts fine-grained local features from the selected\npatches. A fusion module then integrates global and local information from both\nnetworks to generate robust predictions. The HybMNet is trained end-to-end,\nwith the loss function combining the outputs of the Swin-T and CNN modules to\noptimize feature extraction and classification performance.\n  Results: The proposed method was evaluated for its ability to detect breast\ncancer by distinguishing between benign (normal) and malignant mammograms.\nLeveraging SSL pretraining and the HybMNet model, it achieved AUC of 0.864 (95%\nCI: 0.852, 0.875) on the CMMD dataset and 0.889 (95% CI: 0.875, 0.903) on the\nINbreast dataset, highlighting its effectiveness."}
{"id": "2504.19894", "pdf": "https://arxiv.org/pdf/2504.19894", "abs": "https://arxiv.org/abs/2504.19894", "authors": ["Quynh Phung", "Long Mai", "Fabian David Caba Heilbron", "Feng Liu", "Jia-Bin Huang", "Cusuh Ham"], "title": "CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition", "categories": ["cs.CV"], "comment": "link website: https://cinevers.github.io/", "summary": "We present CineVerse, a novel framework for the task of cinematic scene\ncomposition. Similar to traditional multi-shot generation, our task emphasizes\nthe need for consistency and continuity across frames. However, our task also\nfocuses on addressing challenges inherent to filmmaking, such as multiple\ncharacters, complex interactions, and visual cinematic effects. In order to\nlearn to generate such content, we first create the CineVerse dataset. We use\nthis dataset to train our proposed two-stage approach. First, we prompt a large\nlanguage model (LLM) with task-specific instructions to take in a high-level\nscene description and generate a detailed plan for the overall setting and\ncharacters, as well as the individual shots. Then, we fine-tune a text-to-image\ngeneration model to synthesize high-quality visual keyframes. Experimental\nresults demonstrate that CineVerse yields promising improvements in generating\nvisually coherent and contextually rich movie scenes, paving the way for\nfurther exploration in cinematic video synthesis."}
{"id": "2504.19900", "pdf": "https://arxiv.org/pdf/2504.19900", "abs": "https://arxiv.org/abs/2504.19900", "authors": ["Han Chen", "Anne L. Martel"], "title": "Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate detection of breast cancer from high-resolution mammograms is\ncrucial for early diagnosis and effective treatment planning. Previous studies\nhave shown the potential of using single-view mammograms for breast cancer\ndetection. However, incorporating multi-view data can provide more\ncomprehensive insights. Multi-view classification, especially in medical\nimaging, presents unique challenges, particularly when dealing with\nlarge-scale, high-resolution data. In this work, we propose a novel Multi-view\nVisual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening\nmammograms. We first pretrain a robust single-view classification model on\nhigh-resolution mammograms and then innovatively adapt multi-view feature\nlearning into a task-specific prompt tuning process. This technique selectively\ntunes a minimal set of trainable parameters (7\\%) while retaining the\nrobustness of the pre-trained single-view model, enabling efficient integration\nof multi-view data without the need for aggressive downsampling. Our approach\noffers an efficient alternative to traditional feature fusion methods,\nproviding a more robust, scalable, and efficient solution for high-resolution\nmammogram analysis. Experimental results on a large multi-institution dataset\ndemonstrate that our method outperforms conventional approaches while\nmaintaining detection efficiency, achieving an AUROC of 0.852 for\ndistinguishing between Benign, DCIS, and Invasive classes. This work highlights\nthe potential of MVPT-NET for medical imaging tasks and provides a scalable\nsolution for integrating multi-view data in breast cancer detection."}
{"id": "2504.19918", "pdf": "https://arxiv.org/pdf/2504.19918", "abs": "https://arxiv.org/abs/2504.19918", "authors": ["Hugo Georgenthum", "Cristian Cosentino", "Fabrizio Marozzo", "Pietro Liò"], "title": "Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The automatic summarization of surgical videos is essential for enhancing\nprocedural documentation, supporting surgical training, and facilitating\npost-operative analysis. This paper presents a novel method at the intersection\nof artificial intelligence and medicine, aiming to develop machine learning\nmodels with direct real-world applications in surgical contexts. We propose a\nmulti-modal framework that leverages recent advancements in computer vision and\nlarge language models to generate comprehensive video summaries. % The approach\nis structured in three key stages. First, surgical videos are divided into\nclips, and visual features are extracted at the frame level using visual\ntransformers. This step focuses on detecting tools, tissues, organs, and\nsurgical actions. Second, the extracted features are transformed into\nframe-level captions via large language models. These are then combined with\ntemporal features, captured using a ViViT-based encoder, to produce clip-level\nsummaries that reflect the broader context of each video segment. Finally, the\nclip-level descriptions are aggregated into a full surgical report using a\ndedicated LLM tailored for the summarization task. % We evaluate our method on\nthe CholecT50 dataset, using instrument and action annotations from 50\nlaparoscopic videos. The results show strong performance, achieving 96\\%\nprecision in tool detection and a BERT score of 0.74 for temporal context\nsummarization. This work contributes to the advancement of AI-assisted tools\nfor surgical reporting, offering a step toward more intelligent and reliable\nclinical documentation."}
{"id": "2504.19935", "pdf": "https://arxiv.org/pdf/2504.19935", "abs": "https://arxiv.org/abs/2504.19935", "authors": ["Xiem HoangVan", "Hieu Bui Minh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model", "categories": ["cs.CV"], "comment": null, "summary": "The latest video coding standard H.266/VVC has shown its great improvement in\nterms of compression performance when compared to its predecessor HEVC\nstandard. Though VVC was implemented with many advanced techniques, it still\nmet the same challenges as its predecessor due to the need for even higher\nperceptual quality demand at the decoder side as well as the compression\nperformance at the encoder side. The advancement of Artificial Intelligence\n(AI) technology, notably the deep learning-based video quality enhancement\nmethods, was shown to be a promising approach to improving the perceptual\nquality experience. In this paper, we propose a novel Omniscient video quality\nenhancement Network for VVC compressed Videos. The Omniscient Network for\ncompressed video quality enhancement was originally designed for HEVC\ncompressed videos in which not only the spatial-temporal features but also\ncross-frequencies information were employed to augment the visual quality.\nInspired by this work, we propose a modification of the OVQE model and\nintegrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder\narchitecture. As assessed in a rich set of test conditions, the proposed\nOVQE-VVC solution is able to achieve significant PSNR improvement, notably\naround 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec.\nThis also corresponds to around 19.6% of bitrate saving while keeping a similar\nquality observation."}
{"id": "2504.19938", "pdf": "https://arxiv.org/pdf/2504.19938", "abs": "https://arxiv.org/abs/2504.19938", "authors": ["Yunfei Wan", "Jianheng Liu", "Jiarong Lin", "Fu Zhang"], "title": "Mesh-Learner: Texturing Mesh with Spherical Harmonics", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this paper, we present a 3D reconstruction and rendering framework termed\nMesh-Learner that is natively compatible with traditional rasterization\npipelines. It integrates mesh and spherical harmonic (SH) texture (i.e.,\ntexture filled with SH coefficients) into the learning process to learn each\nmesh s view-dependent radiance end-to-end. Images are rendered by interpolating\nsurrounding SH Texels at each pixel s sampling point using a novel\ninterpolation method. Conversely, gradients from each pixel are back-propagated\nto the related SH Texels in SH textures. Mesh-Learner exploits graphic features\nof rasterization pipeline (texture sampling, deferred rendering) to render,\nwhich makes Mesh-Learner naturally compatible with tools (e.g., Blender) and\ntasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for\nrobotics) that are based on rasterization pipelines. Our system can train vast,\nunlimited scenes because we transfer only the SH textures within the frustum to\nthe GPU for training. At other times, the SH textures are stored in CPU RAM,\nwhich results in moderate GPU memory usage. The rendering results on\ninterpolation and extrapolation sequences in the Replica and FAST-LIVO2\ndatasets achieve state-of-the-art performance compared to existing\nstate-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To\nbenefit the society, the code will be available at\nhttps://github.com/hku-mars/Mesh-Learner."}
{"id": "2504.19970", "pdf": "https://arxiv.org/pdf/2504.19970", "abs": "https://arxiv.org/abs/2504.19970", "authors": ["Narges Rashvand", "Ghazal Alinezhad Noghre", "Armin Danesh Pazho", "Babak Rahimi Ardabili", "Hamed Tabkhi"], "title": "Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose", "categories": ["cs.CV"], "comment": null, "summary": "Shoplifting remains a costly issue for the retail sector, but traditional\nsurveillance systems, which are mostly based on human monitoring, are still\nlargely ineffective, with only about 2% of shoplifters being arrested. Existing\nAI-based approaches rely on pixel-level video analysis which raises privacy\nconcerns, is sensitive to environmental variations, and demands significant\ncomputational resources. To address these limitations, we introduce Shopformer,\na novel transformer-based model that detects shoplifting by analyzing pose\nsequences rather than raw video. We propose a custom tokenization strategy that\nconverts pose sequences into compact embeddings for efficient transformer\nprocessing. To the best of our knowledge, this is the first pose-sequence-based\ntransformer model for shoplifting detection. Evaluated on real-world pose data,\nour method outperforms state-of-the-art anomaly detection models, offering a\nprivacy-preserving, and scalable solution for real-time retail surveillance.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Shopformer."}
{"id": "2504.19991", "pdf": "https://arxiv.org/pdf/2504.19991", "abs": "https://arxiv.org/abs/2504.19991", "authors": ["Ioannis Kontogiorgakis", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Dimitra A. Loka", "Christos Noulas", "Alexandros Tsitouras", "Charalampos Kontoes"], "title": "Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Effective weed management is crucial for improving agricultural productivity,\nas weeds compete with crops for vital resources like nutrients and water.\nAccurate maps of weed management methods are essential for policymakers to\nassess farmer practices, evaluate impacts on vegetation health, biodiversity,\nand climate, as well as ensure compliance with policies and subsidies. However,\nmonitoring weed management methods is challenging as commonly rely on on-ground\nfield surveys, which are often costly, time-consuming and subject to delays. In\norder to tackle this problem, we leverage Earth Observation (EO) data and\nMachine Learning (ML). Specifically, we developed an ML approach for mapping\nfour distinct weed management methods (Mowing, Tillage, Chemical-spraying, and\nNo practice) in orchards using satellite image time series (SITS) data from two\ndifferent sources: Sentinel-2 (S2) and PlanetScope (PS). The findings\ndemonstrate the potential of ML-driven remote sensing to enhance the efficiency\nand accuracy of weed management mapping in orchards."}
{"id": "2504.19996", "pdf": "https://arxiv.org/pdf/2504.19996", "abs": "https://arxiv.org/abs/2504.19996", "authors": ["Andreas Kalogeras", "Dimitrios Bormpoudakis", "Iason Tsardanidis", "Dimitra A. Loka", "Charalampos Kontoes"], "title": "Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The widespread use of Exogenous Organic Matter in agriculture necessitates\nmonitoring to assess its effects on soil and crop health. This study evaluates\noptical Sentinel-2 satellite imagery for detecting digestate application, a\npractice that enhances soil fertility but poses environmental risks like\nmicroplastic contamination and nitrogen losses. In the first instance,\nSentinel-2 satellite image time series (SITS) analysis of specific indices\n(EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after\napplication on the soils of four different crop types in Thessaly, Greece.\nFurthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient\nBoosting and a Feed-Forward Neural Network), were used to investigate digestate\npresence detection, achieving F1-scores up to 0.85. The findings highlight the\npotential of combining remote sensing and ML for scalable and cost-effective\nmonitoring of EOM applications, supporting precision agriculture and\nsustainability."}
{"id": "2504.20024", "pdf": "https://arxiv.org/pdf/2504.20024", "abs": "https://arxiv.org/abs/2504.20024", "authors": ["Wufei Ma", "Yu-Cheng Chou", "Qihao Liu", "Xingrui Wang", "Celso de Melo", "Jieneng Chen", "Jianwen Xie", "Alan Yuille"], "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "categories": ["cs.CV"], "comment": "Project page: https://spatial-reasoner.github.io", "summary": "Recent studies in 3D spatial reasoning explore data-driven approaches and\nachieve enhanced spatial reasoning performance with reinforcement learning\n(RL). However, these methods typically perform spatial reasoning in an implicit\nmanner, and it remains underexplored whether the acquired 3D knowledge\ngeneralizes to unseen question types at any stage of the training. In this work\nwe introduce SpatialReasoner, a novel large vision-language model (LVLM) that\naddress 3D spatial reasoning with explicit 3D representations shared between\nstages -- 3D perception, computation, and reasoning. Explicit 3D\nrepresentations provide a coherent interface that supports advanced 3D spatial\nreasoning and enable us to study the factual errors made by LVLMs. Results show\nthat our SpatialReasoner achieve improved performance on a variety of spatial\nreasoning benchmarks and generalizes better when evaluating on novel 3D spatial\nreasoning questions. Our study bridges the 3D parsing capabilities of prior\nvisual foundation models with the powerful reasoning abilities of large\nlanguage models, opening new directions for 3D spatial reasoning."}
{"id": "2504.20026", "pdf": "https://arxiv.org/pdf/2504.20026", "abs": "https://arxiv.org/abs/2504.20026", "authors": ["Zhengqin Li", "Dilin Wang", "Ka Chen", "Zhaoyang Lv", "Thu Nguyen-Phuoc", "Milim Lee", "Jia-Bin Huang", "Lei Xiao", "Cheng Zhang", "Yufeng Zhu", "Carl S. Marshall", "Yufeng Ren", "Richard Newcombe", "Zhao Dong"], "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "We present Large Inverse Rendering Model (LIRM), a transformer architecture\nthat jointly reconstructs high-quality shape, materials, and radiance fields\nwith view-dependent effects in less than a second. Our model builds upon the\nrecent Large Reconstruction Models (LRMs) that achieve state-of-the-art\nsparse-view reconstruction quality. However, existing LRMs struggle to\nreconstruct unseen parts accurately and cannot recover glossy appearance or\ngenerate relightable 3D contents that can be consumed by standard Graphics\nengines. To address these limitations, we make three key technical\ncontributions to build a more practical multi-view 3D reconstruction framework.\nFirst, we introduce an update model that allows us to progressively add more\ninput views to improve our reconstruction. Second, we propose a hexa-plane\nneural SDF representation to better recover detailed textures, geometry and\nmaterial parameters. Third, we develop a novel neural directional-embedding\nmechanism to handle view-dependent effects. Trained on a large-scale shape and\nmaterial dataset with a tailored coarse-to-fine training scheme, our model\nachieves compelling results. It compares favorably to optimization-based\ndense-view inverse rendering methods in terms of geometry and relighting\naccuracy, while requiring only a fraction of the inference time."}
{"id": "2504.20032", "pdf": "https://arxiv.org/pdf/2504.20032", "abs": "https://arxiv.org/abs/2504.20032", "authors": ["Kai Ye", "Haidi Tang", "Bowen Liu", "Pingyang Dai", "Liujuan Cao", "Rongrong Ji"], "title": "More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV", "categories": ["cs.CV"], "comment": null, "summary": "Applications of unmanned aerial vehicle (UAV) in logistics, agricultural\nautomation, urban management, and emergency response are highly dependent on\noriented object detection (OOD) to enhance visual perception. Although existing\ndatasets for OOD in UAV provide valuable resources, they are often designed for\nspecific downstream tasks.Consequently, they exhibit limited generalization\nperformance in real flight scenarios and fail to thoroughly demonstrate\nalgorithm effectiveness in practical environments. To bridge this critical gap,\nwe introduce CODrone, a comprehensive oriented object detection dataset for\nUAVs that accurately reflects real-world conditions. It also serves as a new\nbenchmark designed to align with downstream task requirements, ensuring greater\napplicability and robustness in UAV-based OOD.Based on application\nrequirements, we identify four key limitations in current UAV OOD datasets-low\nimage resolution, limited object categories, single-view imaging, and\nrestricted flight altitudes-and propose corresponding improvements to enhance\ntheir applicability and robustness.Furthermore, CODrone contains a broad\nspectrum of annotated images collected from multiple cities under various\nlighting conditions, enhancing the realism of the benchmark. To rigorously\nevaluate CODrone as a new benchmark and gain deeper insights into the novel\nchallenges it presents, we conduct a series of experiments based on 22\nclassical or SOTA methods.Our evaluation not only assesses the effectiveness of\nCODrone in real-world scenarios but also highlights key bottlenecks and\nopportunities to advance OOD in UAV applications.Overall, CODrone fills the\ndata gap in OOD from UAV perspective and provides a benchmark with enhanced\ngeneralization capability, better aligning with practical applications and\nfuture algorithm development."}
{"id": "2504.20033", "pdf": "https://arxiv.org/pdf/2504.20033", "abs": "https://arxiv.org/abs/2504.20033", "authors": ["Sara Yavari", "Jacob Furst"], "title": "Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images", "categories": ["cs.CV", "I.2.6; I.2.10"], "comment": "15 Pages, 3 Figures, 3 Tables, 1 Algorithm, This paper will be\n  updated", "summary": "This paper proposes an Incremental Learning (IL) approach to enhance the\naccuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)\nMRI medical images prostate cancer detection using the PI-CAI dataset. We used\nmultiple health centers' artificial intelligence and radiology data, focused on\ndifferent tasks that looked at prostate cancer detection using MRI (PI-CAI). We\nutilized Knowledge Distillation (KD), as it employs generated images from past\ntasks to guide the training of models for subsequent tasks. The approach\nyielded improved performance and faster convergence of the models. To\ndemonstrate the versatility and robustness of our approach, we evaluated it on\nthe PI-CAI dataset, a diverse set of medical imaging modalities including OCT\nand PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our\nresults indicate that KD can be a promising technique for IL in medical image\nanalysis in which data is sourced from individual health centers and the\nstorage of large datasets is not feasible. By using generated images from prior\ntasks, our method enables the model to retain and apply previously acquired\nknowledge without direct access to the original data."}
{"id": "2504.20040", "pdf": "https://arxiv.org/pdf/2504.20040", "abs": "https://arxiv.org/abs/2504.20040", "authors": ["Zador Pataki", "Paul-Edouard Sarlin", "Johannes L. Schönberger", "Marc Pollefeys"], "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025", "summary": "While Structure-from-Motion (SfM) has seen much progress over the years,\nstate-of-the-art systems are prone to failure when facing extreme viewpoint\nchanges in low-overlap, low-parallax or high-symmetry scenarios. Because\ncapturing images that avoid these pitfalls is challenging, this severely limits\nthe wider use of SfM, especially by non-expert users. We overcome these\nlimitations by augmenting the classical SfM paradigm with monocular depth and\nnormal priors inferred by deep neural networks. Thanks to a tight integration\nof monocular and multi-view constraints, our approach significantly outperforms\nexisting ones under extreme viewpoint changes, while maintaining strong\nperformance in standard conditions. We also show that monocular priors can help\nreject faulty associations due to symmetries, which is a long-standing problem\nfor SfM. This makes our approach the first capable of reliably reconstructing\nchallenging indoor environments from few images. Through principled uncertainty\npropagation, it is robust to errors in the priors, can handle priors inferred\nby different models with little tuning, and will thus easily benefit from\nfuture progress in monocular depth and normal estimation. Our code is publicly\navailable at https://github.com/cvg/mpsfm."}
{"id": "2504.20041", "pdf": "https://arxiv.org/pdf/2504.20041", "abs": "https://arxiv.org/abs/2504.20041", "authors": ["Yibin Yan", "Jilan Xu", "Shangzhe Di", "Yikun Liu", "Yudi Shi", "Qirui Chen", "Zeqian Li", "Yifei Huang", "Weidi Xie"], "title": "Learning Streaming Video Representation via Multitask Training", "categories": ["cs.CV"], "comment": "Technical Report. Project Page:\n  https://go2heart.github.io/streamformer", "summary": "Understanding continuous video streams plays a fundamental role in real-time\napplications including embodied AI and autonomous driving. Unlike offline video\nunderstanding, streaming video understanding requires the ability to process\nvideo streams frame by frame, preserve historical information, and make\nlow-latency decisions.To address these challenges, our main contributions are\nthree-fold. (i) We develop a novel streaming video backbone, termed as\nStreamFormer, by incorporating causal temporal attention into a pre-trained\nvision transformer. This enables efficient streaming video processing while\nmaintaining image representation capability.(ii) To train StreamFormer, we\npropose to unify diverse spatial-temporal video understanding tasks within a\nmultitask visual-language alignment framework. Hence, StreamFormer learns\nglobal semantics, temporal dynamics, and fine-grained spatial relationships\nsimultaneously. (iii) We conduct extensive experiments on online action\ndetection, online video instance segmentation, and video question answering.\nStreamFormer achieves competitive results while maintaining efficiency,\ndemonstrating its potential for real-time applications."}
{"id": "2504.20042", "pdf": "https://arxiv.org/pdf/2504.20042", "abs": "https://arxiv.org/abs/2504.20042", "authors": ["Yu-Ju Tsai", "Brian Price", "Qing Liu", "Luis Figueroa", "Daniil Pakhomov", "Zhihong Ding", "Scott Cohen", "Ming-Hsuan Yang"], "title": "CompleteMe: Reference-based Human Image Completion", "categories": ["cs.CV"], "comment": "Project page: https://liagm.github.io/CompleteMe/", "summary": "Recent methods for human image completion can reconstruct plausible body\nshapes but often fail to preserve unique details, such as specific clothing\npatterns or distinctive accessories, without explicit reference images. Even\nstate-of-the-art reference-based inpainting approaches struggle to accurately\ncapture and integrate fine-grained details from reference images. To address\nthis limitation, we propose CompleteMe, a novel reference-based human image\ncompletion framework. CompleteMe employs a dual U-Net architecture combined\nwith a Region-focused Attention (RFA) Block, which explicitly guides the\nmodel's attention toward relevant regions in reference images. This approach\neffectively captures fine details and ensures accurate semantic correspondence,\nsignificantly improving the fidelity and consistency of completed images.\nAdditionally, we introduce a challenging benchmark specifically designed for\nevaluating reference-based human image completion tasks. Extensive experiments\ndemonstrate that our proposed method achieves superior visual quality and\nsemantic consistency compared to existing techniques. Project page:\nhttps://liagm.github.io/CompleteMe/"}
{"id": "2504.18540", "pdf": "https://arxiv.org/pdf/2504.18540", "abs": "https://arxiv.org/abs/2504.18540", "authors": ["Gonçalo Hora de Carvalho"], "title": "Exploring Visual Complaints through a test battery in Acquired Brain Injury Patients: A Detailed Analysis of the DiaNAH Dataset", "categories": ["q-bio.NC", "cs.CV", "q-bio.QM"], "comment": null, "summary": "This study investigated visual impairment complaints in a sample of 948\nAcquired Brain Injury (ABI) patients using the DiaNAH dataset, emphasizing\nadvanced machine learning techniques for managing missing data. Patients\ncompleted a CVS questionnaire capturing eight types of visual symptoms,\nincluding blurred vision and altered contrast perception. Due to incomplete\ndata, 181 patients were excluded, resulting in an analytical subset of 767\nindividuals. To address the challenge of missing data, an automated machine\nlearning (AutoML) approach was employed for data imputation, preserving the\ndistributional characteristics of the original dataset. Patients were grouped\naccording to singular and combined complaint clusters derived from the 40,320\npotential combinations identified through the CVS questionnaire. A linear\ncorrelation analysis revealed minimal to no direct relationship between\npatient-reported visual complaints and standard visual perceptual function\ntests. This study represents an initial systematic attempt to understand the\ncomplex relationship between subjective visual complaints and objective visual\nperceptual assessments in ABI patients. Given the limitations of sample size\nand variability, further studies with larger populations are recommended to\nrobustly explore these complaint clusters and their implications for visual\nperception following brain injury."}
{"id": "2504.18547", "pdf": "https://arxiv.org/pdf/2504.18547", "abs": "https://arxiv.org/abs/2504.18547", "authors": ["Ching-Yi Lin", "Sahil Shah"], "title": "Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "comment": "4 pages + references, 5 figures, 2 tables in IEEE double column\n  conference template", "summary": "Pre-trained vision transformers have achieved remarkable performance across\nvarious visual tasks but suffer from expensive computational and memory costs.\nWhile model quantization reduces memory usage by lowering precision, these\nmodels still incur significant computational overhead due to the dequantization\nbefore matrix operations. In this work, we analyze the computation graph and\npropose an integerization process based on operation reordering. Specifically,\nthe process delays dequantization until after matrix operations. This enables\nintegerized matrix multiplication and linear module by directly processing the\nquantized input. To validate our approach, we synthesize the self-attention\nmodule of ViT on a systolic array-based hardware. Experimental results show\nthat our low-bit inference reduces per-PE power consumption for linear layer\nand matrix multiplication, bridging the gap between quantized models and\nefficient inference."}
{"id": "2504.18549", "pdf": "https://arxiv.org/pdf/2504.18549", "abs": "https://arxiv.org/abs/2504.18549", "authors": ["Boyuan Peng", "Jiaju Chen", "Yiwei Zhang", "Cuiyi Peng", "Junyang Li", "Jiaming Deng", "Peiwu Qin"], "title": "Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The growing burden of myopia and retinal diseases necessitates more\naccessible and efficient eye screening solutions. This study presents a\ncompact, dual-function optical device that integrates fundus photography and\nrefractive error detection into a unified platform. The system features a\ncoaxial optical design using dichroic mirrors to separate wavelength-dependent\nimaging paths, enabling simultaneous alignment of fundus and refraction\nmodules. A Dense-U-Net-based algorithm with customized loss functions is\nemployed for accurate pupil segmentation, facilitating automated alignment and\nfocusing. Experimental evaluations demonstrate the system's capability to\nachieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and\nreliable refractive estimation with a mean absolute error below 5%. Despite\nlimitations due to commercial lens components, the proposed framework offers a\npromising solution for rapid, intelligent, and scalable ophthalmic screening,\nparticularly suitable for community health settings."}
{"id": "2504.18563", "pdf": "https://arxiv.org/pdf/2504.18563", "abs": "https://arxiv.org/abs/2504.18563", "authors": ["Abha Jha", "Ashwath Vaithinathan Aravindan", "Matthew Salaway", "Atharva Sandeep Bhide", "Duygu Nur Yaldiz"], "title": "Backdoor Defense in Diffusion Models via Spatial Attention Unlearning", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image diffusion models are increasingly vulnerable to backdoor\nattacks, where malicious modifications to the training data cause the model to\ngenerate unintended outputs when specific triggers are present. While\nclassification models have seen extensive development of defense mechanisms,\ngenerative models remain largely unprotected due to their high-dimensional\noutput space, which complicates the detection and mitigation of subtle\nperturbations. Defense strategies for diffusion models, in particular, remain\nunder-explored. In this work, we propose Spatial Attention Unlearning (SAU), a\nnovel technique for mitigating backdoor attacks in diffusion models. SAU\nleverages latent space manipulation and spatial attention mechanisms to isolate\nand remove the latent representation of backdoor triggers, ensuring precise and\nefficient removal of malicious effects. We evaluate SAU across various types of\nbackdoor attacks, including pixel-based and style-based triggers, and\ndemonstrate its effectiveness in achieving 100% trigger removal accuracy.\nFurthermore, SAU achieves a CLIP score of 0.7023, outperforming existing\nmethods while preserving the model's ability to generate high-quality,\nsemantically aligned images. Our results show that SAU is a robust, scalable,\nand practical solution for securing text-to-image diffusion models against\nbackdoor attacks."}
{"id": "2504.18591", "pdf": "https://arxiv.org/pdf/2504.18591", "abs": "https://arxiv.org/abs/2504.18591", "authors": ["Giovanni Catalani", "Michael Bauerheim", "Frédéric Tost", "Xavier Bertrand", "Joseph Morlier"], "title": "Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in Neural Fields have enabled powerful,\ndiscretization-invariant methods for learning neural operators that approximate\nsolutions of Partial Differential Equations (PDEs) on general geometries.\nBuilding on these developments, we introduce enf2enf, an encoder--decoder\nmethodology for predicting steady-state Partial Differential Equations with\nnon-parameterized geometric variability, based on recently proposed Equivariant\nNeural Field architectures. In enf2enf, input geometries are encoded into\nlatent point cloud embeddings that inherently preserve geometric grounding and\ncapture local phenomena. The resulting representations are then combined with\nglobal parameters and directly decoded into continuous output fields, thus\nefficiently modeling the coupling between geometry and physics. By leveraging\nthe inductive biases of locality and translation invariance, our approach is\nable to capture fine-scale physical features as well as complex shape\nvariations, thereby enhancing generalization and physical compliance. Extensive\nexperiments on a high-fidelity aerodynamic dataset, a hyper-elastic material\nbenchmark, and multi-element airfoil geometries, demonstrate that the proposed\nmodel achieves superior or competitive performance compared to state-of-the-art\ngraph based, operator learning, and neural field methods. Notably, our method\nsupports real time inference and zero-shot super-resolution, enabling efficient\ntraining on low-resolution meshes while maintaining high accuracy on full-scale\ndiscretizations."}
{"id": "2504.18624", "pdf": "https://arxiv.org/pdf/2504.18624", "abs": "https://arxiv.org/abs/2504.18624", "authors": ["Ali SaraerToosi", "Avery Broderick"], "title": "Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*", "categories": ["astro-ph.HE", "astro-ph.IM", "cs.CV", "cs.LG", "85A99, 35Q75, 65C60, 62F15", "I.2.6; G.1.10; I.4.10"], "comment": "26 pages, 9 figures, 2 tables, submitted to ApJ", "summary": "The Event Horizon Telescope (EHT) enables the exploration of black hole\naccretion flows at event-horizon scales. Fitting ray-traced physical models to\nEHT observations requires the generation of synthetic images, a task that is\ncomputationally demanding. This study leverages \\alinet, a generative machine\nlearning model, to efficiently produce radiatively inefficient accretion flow\n(RIAF) images as a function of the specified physical parameters. \\alinet has\npreviously been shown to be able to interpolate black hole images and their\nassociated physical parameters after training on a computationally tractable\nset of library images. We utilize this model to estimate the uncertainty\nintroduced by a number of anticipated unmodeled physical effects, including\ninterstellar scattering and intrinsic source variability. We then use this to\ncalibrate physical parameter estimates and their associated uncertainties from\nRIAF model fits to mock EHT data via a library of general relativistic\nmagnetohydrodynamics models."}
{"id": "2504.18768", "pdf": "https://arxiv.org/pdf/2504.18768", "abs": "https://arxiv.org/abs/2504.18768", "authors": ["Letian Huang", "Dongwei Ye", "Jialin Dan", "Chengzhi Tao", "Huiwen Liu", "Kun Zhou", "Bo Ren", "Yuanqi Li", "Yanwen Guo", "Jie Guo"], "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": "accepted by SIGGRAPH 2025;\n  https://letianhuang.github.io/transparentgs/", "summary": "The emergence of neural and Gaussian-based radiance field methods has led to\nconsiderable advancements in novel view synthesis and 3D object reconstruction.\nNonetheless, specular reflection and refraction continue to pose significant\nchallenges due to the instability and incorrect overfitting of radiance fields\nto high-frequency light variations. Currently, even 3D Gaussian Splatting\n(3D-GS), as a powerful and efficient tool, falls short in recovering\ntransparent objects with nearby contents due to the existence of apparent\nsecondary ray effects. To address this issue, we propose TransparentGS, a fast\ninverse rendering pipeline for transparent objects based on 3D-GS. The main\ncontributions are three-fold. Firstly, an efficient representation of\ntransparent objects, transparent Gaussian primitives, is designed to enable\nspecular refraction through a deferred refraction strategy. Secondly, we\nleverage Gaussian light field probes (GaussProbe) to encode both ambient light\nand nearby contents in a unified framework. Thirdly, a depth-based iterative\nprobes query (IterQuery) algorithm is proposed to reduce the parallax errors in\nour probe-based framework. Experiments demonstrate the speed and accuracy of\nour approach in recovering transparent objects from complex environments, as\nwell as several applications in computer graphics and vision."}
{"id": "2504.18802", "pdf": "https://arxiv.org/pdf/2504.18802", "abs": "https://arxiv.org/abs/2504.18802", "authors": ["Xiren Zhou", "Shikang Liu", "Xinyu Yan", "Yizhan Fan", "Xiangyu Wang", "Yu Kang", "Jian Cheng", "Huanhuan Chen"], "title": "Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Urban roads and infrastructure, vital to city operations, face growing\nthreats from subsurface anomalies like cracks and cavities. Ground Penetrating\nRadar (GPR) effectively visualizes underground conditions employing\nelectromagnetic (EM) waves; however, accurate anomaly detection via GPR remains\nchallenging due to limited labeled data, varying subsurface conditions, and\nindistinct target boundaries. Although visually image-like, GPR data\nfundamentally represent EM waves, with variations within and between waves\ncritical for identifying anomalies. Addressing these, we propose the\nReservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework\nexploiting both visual discernibility and wave-changing properties of GPR data.\nRes-SAM initially identifies apparent candidate anomaly regions given minimal\nprompts, and further refines them by analyzing anomaly-induced changing\ninformation within and between EM waves in local GPR data, enabling precise and\ncomplete anomaly region extraction and category determination. Real-world\nexperiments demonstrate that Res-SAM achieves high detection accuracy (>85%)\nand outperforms state-of-the-art. Notably, Res-SAM requires only minimal\naccessible non-target data, avoids intensive training, and incorporates simple\nhuman interaction to enhance reliability. Our research provides a scalable,\nresource-efficient solution for rapid subsurface anomaly detection across\ndiverse environments, improving urban safety monitoring while reducing manual\neffort and computational cost."}
{"id": "2504.18829", "pdf": "https://arxiv.org/pdf/2504.18829", "abs": "https://arxiv.org/abs/2504.18829", "authors": ["Jiayi Chen", "Yubin Ke", "Lin Peng", "He Wang"], "title": "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by Robotics: Science and Systems (RSS 2025)", "summary": "Generalizable dexterous grasping with suitable grasp types is a fundamental\nskill for intelligent robots. Developing such skills requires a large-scale and\nhigh-quality dataset that covers numerous grasp types (i.e., at least those\ncategorized by the GRASP taxonomy), but collecting such data is extremely\nchallenging. Existing automatic grasp synthesis methods are often limited to\nspecific grasp types or object categories, hindering scalability. This work\nproposes an efficient pipeline capable of synthesizing contact-rich,\npenetration-free, and physically plausible grasps for any grasp type, object,\nand articulated hand. Starting from a single human-annotated template for each\nhand and grasp type, our pipeline tackles the complicated synthesis problem\nwith two stages: optimize the object to fit the hand template first, and then\nlocally refine the hand to fit the object in simulation. To validate the\nsynthesized grasps, we introduce a contact-aware control strategy that allows\nthe hand to apply the appropriate force at each contact point to the object.\nThose validated grasps can also be used as new grasp templates to facilitate\nfuture synthesis. Experiments show that our method significantly outperforms\nprevious type-unaware grasp synthesis baselines in simulation. Using our\nalgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,\ncovering 31 grasp types in the GRASP taxonomy. Finally, we train a\ntype-conditional generative model that successfully performs the desired grasp\ntype from single-view object point clouds, achieving an 82.3% success rate in\nreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy."}
{"id": "2504.18954", "pdf": "https://arxiv.org/pdf/2504.18954", "abs": "https://arxiv.org/abs/2504.18954", "authors": ["Marco Mezzina", "Pieter De Backer", "Tom Vercauteren", "Matthew Blaschko", "Alexandre Mottrie", "Tinne Tuytelaars"], "title": "Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial\nIntelligence (AI) to segment the surgical workflow into its key events,\nfunctioning as a building block for efficient video review, surgical education\nas well as skill assessment. Previous research has focused on short and linear\nsurgical procedures and has not explored if temporal context influences\nexperts' ability to better classify surgical phases. This research addresses\nthese gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly\nnon-linear procedure. Methods: Urologists of varying expertise were grouped and\ntasked to indicate the surgical phase for RAPN on both single frames and video\nsnippets using a custom-made web platform. Participants reported their\nconfidence levels and the visual landmarks used in their decision-making. AI\narchitectures without and with temporal context as trained and benchmarked on\nthe Cholec80 dataset were subsequently trained on this RAPN dataset. Results:\nVideo snippets and presence of specific visual landmarks improved phase\nclassification accuracy across all groups. Surgeons displayed high confidence\nin their classifications and outperformed novices, who struggled discriminating\nphases. The performance of the AI models is comparable to the surgeons in the\nsurvey, with improvements when temporal context was incorporated in both cases.\nConclusion: SPR is an inherently complex task for expert surgeons and computer\nvision, where both perform equally well when given the same context.\nPerformance increases when temporal information is provided. Surgical tools and\norgans form the key landmarks for human interpretation and are expected to\nshape the future of automated SPR."}
{"id": "2504.18989", "pdf": "https://arxiv.org/pdf/2504.18989", "abs": "https://arxiv.org/abs/2504.18989", "authors": ["Gal Almog", "Ariel Shamir", "Ohad Fried"], "title": "REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to Eurographics 2025. Project page:\n  https://reed-vae.github.io/", "summary": "While latent diffusion models achieve impressive image editing results, their\napplication to iterative editing of the same image is severely restricted. When\ntrying to apply consecutive edit operations using current models, they\naccumulate artifacts and noise due to repeated transitions between pixel and\nlatent spaces. Some methods have attempted to address this limitation by\nperforming the entire edit chain within the latent space, sacrificing\nflexibility by supporting only a limited, predetermined set of diffusion\nediting operations. We present a RE-encode decode (REED) training scheme for\nvariational autoencoders (VAEs), which promotes image quality preservation even\nafter many iterations. Our work enables multi-method iterative image editing:\nusers can perform a variety of iterative edit operations, with each operation\nbuilding on the output of the previous one using both diffusion-based\noperations and conventional editing techniques. We demonstrate the advantage of\nREED-VAE across a range of image editing scenarios, including text-based and\nmask-based editing frameworks. In addition, we show how REED-VAE enhances the\noverall editability of images, increasing the likelihood of successful and\nprecise edit operations. We hope that this work will serve as a benchmark for\nthe newly introduced task of multi-method image editing. Our code and models\nwill be available at https://github.com/galmog/REED-VAE"}
{"id": "2504.19002", "pdf": "https://arxiv.org/pdf/2504.19002", "abs": "https://arxiv.org/abs/2504.19002", "authors": ["Delun Lai", "Yeyubei Zhang", "Yunchong Liu", "Chaojie Li", "Huadong Mo"], "title": "Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "6 pages, 4 figures", "summary": "This paper introduces a novel deep learning-based multimodal fusion\narchitecture aimed at enhancing the perception capabilities of autonomous\nnavigation robots in complex environments. By utilizing innovative feature\nextraction modules, adaptive fusion strategies, and time-series modeling\nmechanisms, the system effectively integrates RGB images and LiDAR data. The\nkey contributions of this work are as follows: a. the design of a lightweight\nfeature extraction network to enhance feature representation; b. the\ndevelopment of an adaptive weighted cross-modal fusion strategy to improve\nsystem robustness; and c. the incorporation of time-series information modeling\nto boost dynamic scene perception accuracy. Experimental results on the KITTI\ndataset demonstrate that the proposed approach increases navigation and\npositioning accuracy by 3.5% and 2.2%, respectively, while maintaining\nreal-time performance. This work provides a novel solution for autonomous robot\nnavigation in complex environments."}
{"id": "2504.19174", "pdf": "https://arxiv.org/pdf/2504.19174", "abs": "https://arxiv.org/abs/2504.19174", "authors": ["Xueqi Ma", "Yilin Liu", "Tianlong Gao", "Qirui Huang", "Hui Huang"], "title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH 2025", "summary": "We introduce CLR-Wire, a novel framework for 3D curve-based wireframe\ngeneration that integrates geometry and topology into a unified Continuous\nLatent Representation. Unlike conventional methods that decouple vertices,\nedges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along\nwith their topological connectivity into a continuous and fixed-length latent\nspace using an attention-driven variational autoencoder (VAE). This unified\napproach facilitates joint learning and generation of both geometry and\ntopology. To generate wireframes, we employ a flow matching model to\nprogressively map Gaussian noise to these latents, which are subsequently\ndecoded into complete 3D wireframes. Our method provides fine-grained modeling\nof complex shapes and irregular topologies, and supports both unconditional\ngeneration and generation conditioned on point cloud or image inputs.\nExperimental results demonstrate that, compared with state-of-the-art\ngenerative approaches, our method achieves substantial improvements in\naccuracy, novelty, and diversity, offering an efficient and comprehensive\nsolution for CAD design, geometric reconstruction, and 3D content creation."}
{"id": "2504.19189", "pdf": "https://arxiv.org/pdf/2504.19189", "abs": "https://arxiv.org/abs/2504.19189", "authors": ["Lei Zhong", "Chuan Guo", "Yiming Xie", "Jiawei Wang", "Changjian Li"], "title": "Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://zhongleilz.github.io/Sketch2Anim/", "summary": "Storyboarding is widely used for creating 3D animations. Animators use the 2D\nsketches in storyboards as references to craft the desired 3D animations\nthrough a trial-and-error process. The traditional approach requires\nexceptional expertise and is both labor-intensive and time-consuming.\nConsequently, there is a high demand for automated methods that can directly\ntranslate 2D storyboard sketches into 3D animations. This task is\nunder-explored to date and inspired by the significant advancements of motion\ndiffusion models, we propose to address it from the perspective of conditional\nmotion synthesis. We thus present Sketch2Anim, composed of two key modules for\nsketch constraint understanding and motion generation. Specifically, due to the\nlarge domain gap between the 2D sketch and 3D motion, instead of directly\nconditioning on 2D inputs, we design a 3D conditional motion generator that\nsimultaneously leverages 3D keyposes, joint trajectories, and action words, to\nachieve precise and fine-grained motion control. Then, we invent a neural\nmapper dedicated to aligning user-provided 2D sketches with their corresponding\n3D keyposes and trajectories in a shared embedding space, enabling, for the\nfirst time, direct 2D control of motion generation. Our approach successfully\ntransfers storyboards into high-quality 3D motions and inherently supports\ndirect 3D animation editing, thanks to the flexibility of our multi-conditional\nmotion generator. Comprehensive experiments and evaluations, and a user\nperceptual study demonstrate the effectiveness of our approach."}
{"id": "2504.19200", "pdf": "https://arxiv.org/pdf/2504.19200", "abs": "https://arxiv.org/abs/2504.19200", "authors": ["Tristan Manchester", "Adam Anders", "Julio Spadotto", "Hannah Eccleston", "William Beavan", "Hugues Arcis", "Brian J. Connolly"], "title": "Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": null, "summary": "In situ synchrotron X-ray computed tomography enables dynamic material\nstudies, but automated segmentation remains challenging due to complex imaging\nartefacts and limited training data. We present a methodology for deep\nlearning-based segmentation by transforming high-quality ex situ laboratory\ndata to train models for binary segmentation of in situ synchrotron data,\ndemonstrated through copper oxide dissolution studies. Using a modified\nSegFormer architecture, our approach achieves high segmentation performance on\nunseen data while reducing processing time from hours to seconds per 3D\ndataset. The method maintains consistent performance over significant\nmorphological changes during experiments, despite training only on static\nspecimens. This methodology can be readily applied to diverse materials\nsystems, accelerating the analysis of time-resolved tomographic data across\nscientific disciplines."}
{"id": "2504.19203", "pdf": "https://arxiv.org/pdf/2504.19203", "abs": "https://arxiv.org/abs/2504.19203", "authors": ["Ehsan Karami", "Hamid Soltanian-Zadeh"], "title": "Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Knee osteoarthritis (KOA) is a common joint disease that causes pain and\nmobility issues. While MRI-based deep learning models have demonstrated\nsuperior performance in predicting total knee replacement (TKR) and disease\nprogression, their generalizability remains challenging, particularly when\napplied to imaging data from different sources. In this study, we have shown\nthat replacing batch normalization with instance normalization, using data\naugmentation, and applying contrastive loss improves model generalization in a\nbaseline deep learning model for knee osteoarthritis (KOA) prediction. We\ntrained and evaluated our model using MRI data from the Osteoarthritis\nInitiative (OAI) database, considering sagittal fat-suppressed\nintermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain\nand sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state\n(DESS) images as the target domain. The results demonstrate a statistically\nsignificant improvement in classification accuracy across both domains, with\nour approach outperforming the baseline model."}
{"id": "2504.19253", "pdf": "https://arxiv.org/pdf/2504.19253", "abs": "https://arxiv.org/abs/2504.19253", "authors": ["Taoyi Wang", "Lijian Wang", "Yihan Lin", "Mingtao Ou", "Yuguo Chen", "Xinglong Ji", "Rong Zhao"], "title": "Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 8 figures, 1 table, conference", "summary": "Perception systems in robotics encounter significant challenges in high-speed\nand dynamic conditions when relying on traditional cameras, where motion blur\ncan compromise spatial feature integrity and task performance. Brain-inspired\nvision sensors (BVS) have recently gained attention as an alternative, offering\nhigh temporal resolution with reduced bandwidth and power requirements. Here,\nwe present the first quantitative evaluation framework for two representative\nclasses of BVSs in variable-speed robotic sensing, including event-based vision\nsensors (EVS) that detect asynchronous temporal contrasts, and the\nprimitive-based sensor Tianmouc that employs a complementary mechanism to\nencode both spatiotemporal changes and intensity. A unified testing protocol is\nestablished, including crosssensor calibrations, standardized testing\nplatforms, and quality metrics to address differences in data modality. From an\nimaging standpoint, we evaluate the effects of sensor non-idealities, such as\nmotion-induced distortion, on the capture of structural information. For\nfunctional benchmarking, we examine task performance in corner detection and\nmotion estimation under different rotational speeds. Results indicate that EVS\nperforms well in highspeed, sparse scenarios and in modestly fast, complex\nscenes, but exhibits performance limitations in high-speed, cluttered settings\ndue to pixel-level bandwidth variations and event rate saturation. In\ncomparison, Tianmouc demonstrates consistent performance across sparse and\ncomplex scenarios at various speeds, supported by its global, precise,\nhigh-speed spatiotemporal gradient samplings. These findings offer valuable\ninsights into the applicationdependent suitability of BVS technologies and\nsupport further advancement in this area."}
{"id": "2504.19267", "pdf": "https://arxiv.org/pdf/2504.19267", "abs": "https://arxiv.org/abs/2504.19267", "authors": ["Mohamed Gado", "Towhid Taliee", "Muhammad Memon", "Dmitry Ignatov", "Radu Timofte"], "title": "VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Visual storytelling is an interdisciplinary field combining computer vision\nand natural language processing to generate cohesive narratives from sequences\nof images. This paper presents a novel approach that leverages recent\nadvancements in multimodal models, specifically adapting transformer-based\narchitectures and large multimodal models, for the visual storytelling task.\nLeveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT\nmodel produces visually grounded, contextually appropriate narratives. We\naddress the limitations of traditional evaluation metrics, such as BLEU,\nMETEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we\nutilize RoViST and GROOVIST, novel reference-free metrics designed to assess\nvisual storytelling, focusing on visual grounding, coherence, and\nnon-redundancy. These metrics provide a more nuanced evaluation of narrative\nquality, aligning closely with human judgment."}
{"id": "2504.19362", "pdf": "https://arxiv.org/pdf/2504.19362", "abs": "https://arxiv.org/abs/2504.19362", "authors": ["Yunxuan Wang", "Ray Yin", "Yumei Tan", "Hao Chen", "Haiying Xia"], "title": "Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by IJCNN 2025", "summary": "Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one\nof the primary causes of vision loss among retinal vascular diseases. Deep\nlearning methods have been extensively applied in the grading of diabetic\nretinopathy (DR). However, their performance declines significantly when\napplied to data outside the training distribution due to domain shifts. Domain\ngeneralization (DG) has emerged as a solution to this challenge. However, most\nexisting DG methods overlook lesion-specific features, resulting in\ninsufficient accuracy. In this paper, we propose a novel approach that enhances\nexisting DG methods by incorporating structural priors, inspired by the\nobservation that DR grading is heavily dependent on vessel and lesion\nstructures. We introduce Low-rank Adaptive Structural Priors (LoASP), a\nplug-and-play framework designed for seamless integration with existing DG\nmodels. LoASP improves generalization by learning adaptive structural\nrepresentations that are finely tuned to the complexities of DR diagnosis.\nExtensive experiments on eight diverse datasets validate its effectiveness in\nboth single-source and multi-source domain scenarios. Furthermore,\nvisualizations reveal that the learned structural priors intuitively align with\nthe intricate architecture of the vessels and lesions, providing compelling\ninsights into their interpretability and diagnostic relevance."}
{"id": "2504.19401", "pdf": "https://arxiv.org/pdf/2504.19401", "abs": "https://arxiv.org/abs/2504.19401", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Li Zhang", "Rong Wang"], "title": "Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning", "categories": ["physics.med-ph", "cs.CV", "cs.GR", "eess.IV", "J.3; I.3.8"], "comment": "35 pages, 9 figures", "summary": "Background: Coronary artery bypass grafting (CABG) planning requires advanced\nspatial visualization and consideration of coronary artery depth,\ncalcification, and pericardial adhesions. Objective: To develop and evaluate a\ndynamic cardiovascular holographic visualization tool for preoperative CABG\nplanning. Methods: Using 4D cardiac computed tomography angiography data from\n14 CABG candidates, we developed a semi-automated workflow for time-resolved\nsegmentation of cardiac structures, epicardial adipose tissue (EAT), and\ncoronary arteries with calcium scoring. The workflow incorporated methods for\ncardiac segmentation, coronary calcification quantification, visualization of\ncoronary depth within EAT, and pericardial adhesion assessment through motion\nanalysis. Dynamic cardiovascular holograms were displayed using the Looking\nGlass platform. Thirteen cardiac surgeons evaluated the tool using a Likert\nscale. Additionally, pericardial adhesion scores from holograms of 21 patients\n(including seven undergoing secondary cardiac surgeries) were compared with\nintraoperative findings. Results: Surgeons rated the visualization tool highly\nfor preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based\npericardial adhesion scoring strongly correlated with intraoperative findings\n(r=0.786, P<0.001). Conclusion: This study establishes a visualization\nframework for CABG planning that produces clinically relevant dynamic holograms\nfrom patient-specific data, with clinical feedback confirming its effectiveness\nfor preoperative planning."}
{"id": "2504.19408", "pdf": "https://arxiv.org/pdf/2504.19408", "abs": "https://arxiv.org/abs/2504.19408", "authors": ["Maitreya Sonawane", "Sumit Mamtani"], "title": "UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting", "categories": ["cs.LG", "cs.CV", "eess.SP"], "comment": null, "summary": "Making accurate weather predictions can be particularly challenging for\nlocalized storms or events that evolve on hourly timescales, such as\nthunderstorms. Hence, our goal for the project was to model Weather Nowcasting\nfor making highly localized and accurate predictions that apply to the\nimmediate future replacing the current numerical weather models and data\nassimilation systems with Deep Learning approaches. A significant advantage of\nmachine learning is that inference is computationally cheap given an\nalready-trained model, allowing forecasts that are nearly instantaneous and in\nthe native high resolution of the input data. In this work we developed a novel\nmethod that employs Transformer-based machine learning models to forecast\nprecipitation. This approach works by leveraging axial attention mechanisms to\nlearn complex patterns and dynamics from time series frames. Moreover, it is a\ngeneric framework and can be applied to univariate and multivariate time series\ndata, as well as time series embeddings data. This paper represents an initial\nresearch on the dataset used in the domain of next frame prediciton, and hence,\nwe demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67,\nSSIM = 0.9943) used for the given dataset using UNet with Axial Transformer."}
{"id": "2504.19438", "pdf": "https://arxiv.org/pdf/2504.19438", "abs": "https://arxiv.org/abs/2504.19438", "authors": ["Lingrui Zhang", "Liang Guo", "Xiao An", "Feng Lin", "Binlong Zheng", "Jiankun Wang", "Zhirui Li"], "title": "Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 7 figures", "summary": "Lumbar disc herniation (LDH) is a common musculoskeletal disease that\nrequires magnetic resonance imaging (MRI) for effective clinical management.\nHowever, the interpretation of MRI images heavily relies on the expertise of\nradiologists, leading to delayed diagnosis and high costs for training\nphysicians. Therefore, this paper proposes an innovative automated LDH\nclassification framework. To address these key issues, the framework utilizes\nT1-weighted and T2-weighted MRI images from 205 people. The framework extracts\nclinically actionable LDH features and generates standardized diagnostic\noutputs by leveraging data augmentation and channel and spatial attention\nmechanisms. These outputs can help physicians make confident and time-effective\ncare decisions when needed. The proposed framework achieves an area under the\nreceiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of\n0.9486 for LDH detection. The experimental results demonstrate the performance\nof the proposed framework. Our framework only requires a small number of\ndatasets for training to demonstrate high diagnostic accuracy. This is expected\nto be a solution to enhance the LDH detection capabilities of primary\nhospitals."}
{"id": "2504.19595", "pdf": "https://arxiv.org/pdf/2504.19595", "abs": "https://arxiv.org/abs/2504.19595", "authors": ["Pietro Bongini", "Sara Mandelli", "Andrea Montibeller", "Mirko Casu", "Orazio Pontorno", "Claudio Ragaglia", "Luca Zanchetta", "Mattia Aquilina", "Taiba Majid Wani", "Luca Guarnera", "Benedetta Tondi", "Paolo Bestagini", "Irene Amerini", "Francesco Denatale", "Sebastiano Battiato", "Mauro Barni"], "title": "WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Synthetic image source attribution is an open challenge, with an increasing\nnumber of image generators being released yearly. The complexity and the sheer\nnumber of available generative techniques, as well as the scarcity of\nhigh-quality open source datasets of diverse nature for this task, make\ntraining and benchmarking synthetic image source attribution models very\nchallenging. WILD is a new in-the-Wild Image Linkage Dataset designed to\nprovide a powerful training and benchmarking tool for synthetic image\nattribution models. The dataset is built out of a closed set of 10 popular\ncommercial generators, which constitutes the training base of attribution\nmodels, and an open set of 10 additional generators, simulating a real-world\nin-the-wild scenario. Each generator is represented by 1,000 images, for a\ntotal of 10,000 images in the closed set and 10,000 images in the open set.\nHalf of the images are post-processed with a wide range of operators. WILD\nallows benchmarking attribution models in a wide range of tasks, including\nclosed and open set identification and verification, and robust attribution\nwith respect to post-processing and adversarial attacks. Models trained on WILD\nare expected to benefit from the challenging scenario represented by the\ndataset itself. Moreover, an assessment of seven baseline methodologies on\nclosed and open set attribution is presented, including robustness tests with\nrespect to post-processing."}
{"id": "2504.19627", "pdf": "https://arxiv.org/pdf/2504.19627", "abs": "https://arxiv.org/abs/2504.19627", "authors": ["Run Luo", "Renke Shan", "Longze Chen", "Ziqiang Liu", "Lu Wang", "Min Yang", "Xiaobo Xia"], "title": "VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "VCM", "summary": "Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like\nembodied intelligence due to their strong vision-language reasoning abilities.\nHowever, current LVLMs process entire images at the token level, which is\ninefficient compared to humans who analyze information and generate content at\nthe conceptual level, extracting relevant visual concepts with minimal effort.\nThis inefficiency, stemming from the lack of a visual concept model, limits\nLVLMs' usability in real-world applications. To address this, we propose VCM,\nan end-to-end self-supervised visual concept modeling framework. VCM leverages\nimplicit contrastive learning across multiple sampled instances and\nvision-language fine-tuning to construct a visual concept model without\nrequiring costly concept-level annotations. Our results show that VCM\nsignificantly reduces computational costs (e.g., 85\\% fewer FLOPs for\nLLaVA-1.5-7B) while maintaining strong performance across diverse image\nunderstanding tasks. Moreover, VCM enhances visual encoders' capabilities in\nclassic visual concept perception tasks. Extensive quantitative and qualitative\nexperiments validate the effectiveness and efficiency of VCM."}
{"id": "2504.19718", "pdf": "https://arxiv.org/pdf/2504.19718", "abs": "https://arxiv.org/abs/2504.19718", "authors": ["Victoria Yue Chen", "Daoye Wang", "Stephan Garbin", "Sebastian Winberg", "Timo Bolkart", "Thabo Beeler"], "title": "Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation", "categories": ["cs.GR", "cs.CV"], "comment": "4 pages, 4 figures, to be published in Eurographics 2025 as a short\n  paper", "summary": "Face registration deforms a template mesh to closely fit a 3D face scan, the\nquality of which commonly degrades in non-skin regions (e.g., hair, beard,\naccessories), because the optimized template-to-scan distance pulls the\ntemplate mesh towards the noisy scan surface. Improving registration quality\nrequires a clean separation of skin and non-skin regions on the scan mesh.\nExisting image-based (2D) or scan-based (3D) segmentation methods however\nperform poorly. Image-based segmentation outputs multi-view inconsistent masks,\nand they cannot account for scan inaccuracies or scan-image misalignment, while\nscan-based methods suffer from lower spatial resolution compared to images. In\nthis work, we introduce a novel method that accurately separates skin from\nnon-skin geometry on 3D human head scans. For this, our method extracts\nfeatures from multi-view images using a frozen image foundation model and\naggregates these features in 3D. These lifted 2D features are then fused with\n3D geometric features extracted from the scan mesh, to then predict a\nsegmentation mask directly on the scan mesh. We show that our segmentations\nimprove the registration accuracy over pure 2D or 3D segmentation methods by\n8.89% and 14.3%, respectively. Although trained only on synthetic data, our\nmodel generalizes well to real data."}
{"id": "2504.19779", "pdf": "https://arxiv.org/pdf/2504.19779", "abs": "https://arxiv.org/abs/2504.19779", "authors": ["Claudia Drygala", "Hanno Gottschalk", "Thomas Kruse", "Ségolène Martin", "Annika Mütze"], "title": "Learning Brenier Potentials with Convex Generative Adversarial Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Brenier proved that under certain conditions on a source and a target\nprobability measure there exists a strictly convex function such that its\ngradient is a transport map from the source to the target distribution. This\nfunction is called the Brenier potential. Furthermore, detailed information on\nthe H\\\"older regularity of the Brenier potential is available. In this work we\ndevelop the statistical learning theory of generative adversarial neural\nnetworks that learn the Brenier potential. As by the transformation of\ndensities formula, the density of the generated measure depends on the second\nderivative of the Brenier potential, we develop the universal approximation\ntheory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$\nthat combines the favorable approximation properties of H\\\"older functions with\na Lipschitz continuous density. In order to assure the convexity of such\ngeneral networks, we introduce an adversarial training procedure for a\npotential function represented by the ReCU networks that combines the classical\ndiscriminator cross entropy loss with a penalty term that enforces (strict)\nconvexity. We give a detailed decomposition of learning errors and show that\nfor a suitable high penalty parameter all networks chosen in the adversarial\nmin-max optimization problem are strictly convex. This is further exploited to\nprove the consistency of the learning procedure for (slowly) expanding network\ncapacity. We also implement the described learning algorithm and apply it to a\nnumber of standard test cases from Gaussian mixture to image data as target\ndistributions. As predicted in theory, we observe that the convexity loss\nbecomes inactive during the training process and the potentials represented by\nthe neural networks have learned convexity."}
{"id": "2504.19822", "pdf": "https://arxiv.org/pdf/2504.19822", "abs": "https://arxiv.org/abs/2504.19822", "authors": ["Minjong Cheon"], "title": "Mjölnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "Recent advances in AI-based weather forecasting models, such as FourCastNet,\nPangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep\nlearning to emulate complex atmospheric dynamics. Building on this momentum, we\npropose Mj\\\"olnir, a novel deep learning-based framework for global lightning\nflash density parameterization. Trained on ERA5 atmospheric predictors and\nWorld Wide Lightning Location Network (WWLLN) observations at a daily temporal\nresolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear\nmapping between large-scale environmental conditions and lightning activity.\nThe model architecture is based on the InceptionNeXt backbone with SENet, and a\nmulti-task learning strategy to simultaneously predict lightning occurrence and\nmagnitude. Extensive evaluations yield that Mollnir accurately reproduces the\nglobal distribution, seasonal variability, and regional characteristics of\nlightning activity, achieving a global Pearson correlation coefficient of 0.96\nfor annual mean fields. These results suggest that Mj\\\"olnir serves not only as\nan effective data-driven global lightning parameterization but also as a\npromising AI-based scheme for next-generation Earth system models (AI-ESMs)."}
{"id": "2504.19854", "pdf": "https://arxiv.org/pdf/2504.19854", "abs": "https://arxiv.org/abs/2504.19854", "authors": ["Chia-Yu Hung", "Qi Sun", "Pengfei Hong", "Amir Zadeh", "Chuan Li", "U-Xuan Tan", "Navonil Majumder", "Soujanya Poria"], "title": "NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing Visual-Language-Action (VLA) models have shown promising performance\nin zero-shot scenarios, demonstrating impressive task execution and reasoning\ncapabilities. However, a significant challenge arises from the limitations of\nvisual encoding, which can result in failures during tasks such as object\ngrasping. Moreover, these models typically suffer from high computational\noverhead due to their large sizes, often exceeding 7B parameters. While these\nmodels excel in reasoning and task planning, the substantial computational\noverhead they incur makes them impractical for real-time robotic environments,\nwhere speed and efficiency are paramount. To address the limitations of\nexisting VLA models, we propose NORA, a 3B-parameter model designed to reduce\ncomputational overhead while maintaining strong task performance. NORA adopts\nthe Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior\nvisual-semantic understanding to enhance visual reasoning and action grounding.\nAdditionally, our \\model{} is trained on 970k real-world robot demonstrations\nand equipped with the FAST+ tokenizer for efficient action sequence generation.\nExperimental results demonstrate that NORA outperforms existing large-scale VLA\nmodels, achieving better task performance with significantly reduced\ncomputational overhead, making it a more practical solution for real-time\nrobotic autonomy."}
{"id": "2504.19930", "pdf": "https://arxiv.org/pdf/2504.19930", "abs": "https://arxiv.org/abs/2504.19930", "authors": ["Thanuja Uruththirakodeeswaran", "Harald Becher", "Michelle Noga", "Lawrence H. Le", "Pierre Boulanger", "Jonathan Windram", "Kumaradevan Punithakumar"], "title": "Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The perfect alignment of 3D echocardiographic images captured from various\nangles has improved image quality and broadened the field of view. This study\nproposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid\nregistration of transthoracic echocardiographic images with significant and\nlimited overlap taken from apical window that is robust to the noise and\nintensity variation in ultrasound images. The algorithm estimates the\ntranslational and rotational components of the rigid transform through an\niterative process and requires an initial approximation of the rotation and\ntranslation limits. We perform registration in two ways: the image-based\nregistration computes the transform to align the end-diastolic frame of the\napical nonstandard image to the apical standard image and applies the same\ntransform to all frames of the cardiac cycle, whereas the mask-based\nregistration approach uses the binary masks of the left ventricle in the same\nway. The SMC and exhaustive search (EX) algorithms were evaluated for 4D\ntemporal sequences recorded from 7 volunteers who participated in a study\nconducted at the Mazankowski Alberta Heart Institute. The evaluations\ndemonstrate that the mask-based approach of the accelerated SMC yielded a Dice\nscore value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup\ncompared to the CPU version of the SMC algorithm."}
{"id": "2504.19937", "pdf": "https://arxiv.org/pdf/2504.19937", "abs": "https://arxiv.org/abs/2504.19937", "authors": ["Sima Soltanpour", "Rachel Utama", "Arnold Chang", "Md Taufiq Nasseef", "Dan Madularu", "Praveen Kulkarni", "Craig Ferris", "Chris Joslin"], "title": "SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Skull stripping is a common preprocessing step that is often performed\nmanually in Magnetic Resonance Imaging (MRI) pipelines, including functional\nMRI (fMRI). This manual process is time-consuming and operator dependent.\nAutomating this process is challenging for preclinical data due to variations\nin brain geometry, resolution, and tissue contrast. While existing methods for\nMRI skull stripping exist, they often struggle with the low resolution and\nvarying slice sizes in preclinical fMRI data. This study proposes a novel\nmethod called SST-DUNet, that integrates a dense UNet-based architecture with a\nfeature extractor based on Smart Swin Transformer (SST) for fMRI skull\nstripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module\nin SST is adapted to replace the mask-based module in the Swin Transformer\n(ST), enabling the learning of distinct channel-wise features while focusing on\nrelevant dependencies within brain structures. This modification allows the\nmodel to better handle the complexities of fMRI skull stripping, such as low\nresolution and variable slice sizes. To address the issue of class imbalance in\npreclinical data, a combined loss function using Focal and Dice loss is\nutilized. The model was trained on rat fMRI images and evaluated across three\nin-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%.\nThe fMRI results obtained through automatic skull stripping using the SST-DUNet\nmodel closely align with those from manual skull stripping for both seed-based\nand independent component analyses. These results indicate that the SST-DUNet\ncan effectively substitute manual brain extraction in rat fMRI analysis."}
{"id": "2504.20007", "pdf": "https://arxiv.org/pdf/2504.20007", "abs": "https://arxiv.org/abs/2504.20007", "authors": ["Anita Srbinovska", "Angela Srbinovska", "Vivek Senthil", "Adrian Martin", "John McCluskey", "Ernest Fokoué"], "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, and 1 table", "summary": "This paper proposes a novel interdisciplinary framework for analyzing police\nbody-worn camera (BWC) footage from the Rochester Police Department (RPD) using\nadvanced artificial intelligence (AI) and statistical machine learning (ML)\ntechniques. Our goal is to detect, classify, and analyze patterns of\ninteraction between police officers and civilians to identify key behavioral\ndynamics, such as respect, disrespect, escalation, and de-escalation. We apply\nmultimodal data analysis by integrating video, audio, and natural language\nprocessing (NLP) techniques to extract meaningful insights from BWC footage. We\npresent our methodology, computational techniques, and findings, outlining a\npractical approach for law enforcement while advancing the frontiers of\nknowledge discovery from police BWC data."}
