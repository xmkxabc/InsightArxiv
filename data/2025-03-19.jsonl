{"id": "2503.13531", "pdf": "https://arxiv.org/pdf/2503.13531", "abs": "https://arxiv.org/abs/2503.13531", "authors": ["Jin Kim", "Byunghwee Lee", "Taekho You", "Jinhyuk Yun"], "title": "Context-aware Multimodal AI Reveals Hidden Pathways in Five Centuries of Art Evolution", "categories": ["cs.CV", "cs.AI", "cs.CY", "cs.LG"], "comment": "30 pages, 4 figures. Some example paintings are blurred to avoid\n  potential copyright violations", "summary": "The rise of multimodal generative AI is transforming the intersection of\ntechnology and art, offering deeper insights into large-scale artwork. Although\nits creative capabilities have been widely explored, its potential to represent\nartwork in latent spaces remains underexamined. We use cutting-edge generative\nAI, specifically Stable Diffusion, to analyze 500 years of Western paintings by\nextracting two types of latent information with the model: formal aspects\n(e.g., colors) and contextual aspects (e.g., subject). Our findings reveal that\ncontextual information differentiates between artistic periods, styles, and\nindividual artists more successfully than formal elements. Additionally, using\ncontextual keywords extracted from paintings, we show how artistic expression\nevolves alongside societal changes. Our generative experiment, infusing\nprospective contexts into historical artworks, successfully reproduces the\nevolutionary trajectory of artworks, highlighting the significance of mutual\ninteraction between society and art. This study demonstrates how multimodal AI\nexpands traditional formal analysis by integrating temporal, cultural, and\nhistorical contexts."}
{"id": "2503.13576", "pdf": "https://arxiv.org/pdf/2503.13576", "abs": "https://arxiv.org/abs/2503.13576", "authors": ["Ziqiang Li", "Jun Li", "Lizhi Xiong", "Zhangjie Fu", "Zechao Li"], "title": "A Comprehensive Survey on Visual Concept Mining in Text-to-image Diffusion Models", "categories": ["cs.CV"], "comment": "Under review", "summary": "Text-to-image diffusion models have made significant advancements in\ngenerating high-quality, diverse images from text prompts. However, the\ninherent limitations of textual signals often prevent these models from fully\ncapturing specific concepts, thereby reducing their controllability. To address\nthis issue, several approaches have incorporated personalization techniques,\nutilizing reference images to mine visual concept representations that\ncomplement textual inputs and enhance the controllability of text-to-image\ndiffusion models. Despite these advances, a comprehensive, systematic\nexploration of visual concept mining remains limited. In this paper, we\ncategorize existing research into four key areas: Concept Learning, Concept\nErasing, Concept Decomposition, and Concept Combination. This classification\nprovides valuable insights into the foundational principles of Visual Concept\nMining (VCM) techniques. Additionally, we identify key challenges and propose\nfuture research directions to propel this important and interesting field\nforward."}
{"id": "2503.13587", "pdf": "https://arxiv.org/pdf/2503.13587", "abs": "https://arxiv.org/abs/2503.13587", "authors": ["Dingkang Liang", "Dingyuan Zhang", "Xin Zhou", "Sifan Tu", "Tianrui Feng", "Xiaofan Li", "Yumeng Zhang", "Mingyang Du", "Xiao Tan", "Xiang Bai"], "title": "Seeing the Future, Perceiving the Future: A Unified Driving World Model for Future Generation and Perception", "categories": ["cs.CV"], "comment": "The project page is at https://github.com/dk-liang/UniFuture", "summary": "We present UniFuture, a simple yet effective driving world model that\nseamlessly integrates future scene generation and perception within a single\nframework. Unlike existing models focusing solely on pixel-level future\nprediction or geometric reasoning, our approach jointly models future\nappearance (i.e., RGB image) and geometry (i.e., depth), ensuring coherent\npredictions. Specifically, during the training, we first introduce a\nDual-Latent Sharing scheme, which transfers image and depth sequence in a\nshared latent space, allowing both modalities to benefit from shared feature\nlearning. Additionally, we propose a Multi-scale Latent Interaction mechanism,\nwhich facilitates bidirectional refinement between image and depth features at\nmultiple spatial scales, effectively enhancing geometry consistency and\nperceptual alignment. During testing, our UniFuture can easily predict\nhigh-consistency future image-depth pairs by only using the current image as\ninput. Extensive experiments on the nuScenes dataset demonstrate that UniFuture\noutperforms specialized models on future generation and perception tasks,\nhighlighting the advantages of a unified, structurally-aware world model. The\nproject page is at https://github.com/dk-liang/UniFuture."}
{"id": "2503.13617", "pdf": "https://arxiv.org/pdf/2503.13617", "abs": "https://arxiv.org/abs/2503.13617", "authors": ["Hao Li", "Yubin Xiao", "Ke Liang", "Mengzhu Wang", "Long Lan", "Kenli Li", "Xinwang Liu"], "title": "Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization", "categories": ["cs.CV"], "comment": "26 pages, 10 figures", "summary": "Single Domain Generalization (SDG) aims to train models with consistent\nperformance across diverse scenarios using data from a single source. While\nusing latent diffusion models (LDMs) show promise in augmenting limited source\ndata, we demonstrate that directly using synthetic data can be detrimental due\nto significant feature distribution discrepancies between synthetic and real\ntarget domains, leading to performance degradation. To address this issue, we\npropose Discriminative Domain Reassembly and Soft-Fusion (DRSF), a training\nframework leveraging synthetic data to improve model generalization. We employ\nLDMs to produce diverse pseudo-target domain samples and introduce two key\nmodules to handle distribution bias. First, Discriminative Feature Decoupling\nand Reassembly (DFDR) module uses entropy-guided attention to recalibrate\nchannel-level features, suppressing synthetic noise while preserving semantic\nconsistency. Second, Multi-pseudo-domain Soft Fusion (MDSF) module uses\nadversarial training with latent-space feature interpolation, creating\ncontinuous feature transitions between domains. Extensive SDG experiments on\nobject detection and semantic segmentation tasks demonstrate that DRSF achieves\nsubstantial performance gains with only marginal computational overhead.\nNotably, DRSF's plug-and-play architecture enables seamless integration with\nunsupervised domain adaptation paradigms, underscoring its broad applicability\nin addressing diverse and real-world domain challenges."}
{"id": "2503.13646", "pdf": "https://arxiv.org/pdf/2503.13646", "abs": "https://arxiv.org/abs/2503.13646", "authors": ["Chiara Plizzari", "Alessio Tonioni", "Yongqin Xian", "Achin Kulshrestha", "Federico Tombari"], "title": "Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Dataset and code are available at\n  https://github.com/google-research-datasets/egotempo.git", "summary": "Understanding fine-grained temporal dynamics is crucial in egocentric videos,\nwhere continuous streams capture frequent, close-up interactions with objects.\nIn this work, we bring to light that current egocentric video\nquestion-answering datasets often include questions that can be answered using\nonly few frames or commonsense reasoning, without being necessarily grounded in\nthe actual video. Our analysis shows that state-of-the-art Multi-Modal Large\nLanguage Models (MLLMs) on these benchmarks achieve remarkably high performance\nusing just text or a single frame as input. To address these limitations, we\nintroduce EgoTempo, a dataset specifically designed to evaluate temporal\nunderstanding in the egocentric domain. EgoTempo emphasizes tasks that require\nintegrating information across the entire video, ensuring that models would\nneed to rely on temporal patterns rather than static cues or pre-existing\nknowledge. Extensive experiments on EgoTempo show that current MLLMs still fall\nshort in temporal reasoning on egocentric videos, and thus we hope EgoTempo\nwill catalyze new research in the field and inspire models that better capture\nthe complexity of temporal dynamics. Dataset and code are available at\nhttps://github.com/google-research-datasets/egotempo.git."}
{"id": "2503.13652", "pdf": "https://arxiv.org/pdf/2503.13652", "abs": "https://arxiv.org/abs/2503.13652", "authors": ["Maan Qraitem", "Piotr Teterwak", "Kate Saenko", "Bryan A. Plummer"], "title": "Web Artifact Attacks Disrupt Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) (e.g., CLIP, LLaVA) are trained on large-scale,\nlightly curated web datasets, leading them to learn unintended correlations\nbetween semantic concepts and unrelated visual signals. These associations\ndegrade model accuracy by causing predictions to rely on incidental patterns\nrather than genuine visual understanding. Prior work has weaponized these\ncorrelations as an attack vector to manipulate model predictions, such as\ninserting a deceiving class text onto the image in a typographic attack. These\nattacks succeed due to VLMs' text-heavy bias-a result of captions that echo\nvisible words rather than describing content. However, this attack has focused\nsolely on text that matches the target class exactly, overlooking a broader\nrange of correlations, including non-matching text and graphical symbols, which\narise from the abundance of branding content in web-scale data. To address this\ngap, we introduce artifact-based attacks: a novel class of manipulations that\nmislead models using both non-matching text and graphical elements. Unlike\ntypographic attacks, these artifacts are not predefined, making them harder to\ndefend against but also more challenging to find. We address this by framing\nartifact attacks as a search problem and demonstrate their effectiveness across\nfive datasets, with some artifacts reinforcing each other to reach 100% attack\nsuccess rates. These attacks transfer across models with up to 90%\neffectiveness, making it possible to attack unseen models. To defend against\nthese attacks, we extend prior work's artifact aware prompting to the graphical\nsetting. We see a moderate reduction of success rates of up to 15% relative to\nstandard prompts, suggesting a promising direction for enhancing model\nrobustness."}
{"id": "2503.13684", "pdf": "https://arxiv.org/pdf/2503.13684", "abs": "https://arxiv.org/abs/2503.13684", "authors": ["Minghan Li", "Chenxi Xie", "Yichen Wu", "Lei Zhang", "Mengyu Wang"], "title": "FiVE: A Fine-grained Video Editing Benchmark for Evaluating Emerging Diffusion and Rectified Flow Models", "categories": ["cs.CV"], "comment": "24 pages, 14 figures, 16 tables", "summary": "Numerous text-to-video (T2V) editing methods have emerged recently, but the\nlack of a standardized benchmark for fair evaluation has led to inconsistent\nclaims and an inability to assess model sensitivity to hyperparameters.\nFine-grained video editing is crucial for enabling precise, object-level\nmodifications while maintaining context and temporal consistency. To address\nthis, we introduce FiVE, a Fine-grained Video Editing Benchmark for evaluating\nemerging diffusion and rectified flow models. Our benchmark includes 74\nreal-world videos and 26 generated videos, featuring 6 fine-grained editing\ntypes, 420 object-level editing prompt pairs, and their corresponding masks.\nAdditionally, we adapt the latest rectified flow (RF) T2V generation models,\nPyramid-Flow and Wan2.1, by introducing FlowEdit, resulting in training-free\nand inversion-free video editing models Pyramid-Edit and Wan-Edit. We evaluate\nfive diffusion-based and two RF-based editing methods on our FiVE benchmark\nusing 15 metrics, covering background preservation, text-video similarity,\ntemporal consistency, video quality, and runtime. To further enhance\nobject-level evaluation, we introduce FiVE-Acc, a novel metric leveraging\nVision-Language Models (VLMs) to assess the success of fine-grained video\nediting. Experimental results demonstrate that RF-based editing significantly\noutperforms diffusion-based methods, with Wan-Edit achieving the best overall\nperformance and exhibiting the least sensitivity to hyperparameters. More video\ndemo available on the anonymous website:\nhttps://sites.google.com/view/five-benchmark"}
{"id": "2503.13693", "pdf": "https://arxiv.org/pdf/2503.13693", "abs": "https://arxiv.org/abs/2503.13693", "authors": ["Eitan Shaar", "Ariel Shaulov", "Gal Chechik", "Lior Wolf"], "title": "Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds", "categories": ["cs.CV"], "comment": null, "summary": "In the domain of audio-visual event perception, which focuses on the temporal\nlocalization and classification of events across distinct modalities (audio and\nvisual), existing approaches are constrained by the vocabulary available in\ntheir training data. This limitation significantly impedes their capacity to\ngeneralize to novel, unseen event categories. Furthermore, the annotation\nprocess for this task is labor-intensive, requiring extensive manual labeling\nacross modalities and temporal segments, limiting the scalability of current\nmethods. Current state-of-the-art models ignore the shifts in event\ndistributions over time, reducing their ability to adjust to changing video\ndynamics. Additionally, previous methods rely on late fusion to combine audio\nand visual information. While straightforward, this approach results in a\nsignificant loss of multimodal interactions. To address these challenges, we\npropose Audio-Visual Adaptive Video Analysis ($\\text{AV}^2\\text{A}$), a\nmodel-agnostic approach that requires no further training and integrates a\nscore-level fusion technique to retain richer multimodal interactions.\n$\\text{AV}^2\\text{A}$ also includes a within-video label shift algorithm,\nleveraging input video data and predictions from prior frames to dynamically\nadjust event distributions for subsequent frames. Moreover, we present the\nfirst training-free, open-vocabulary baseline for audio-visual event\nperception, demonstrating that $\\text{AV}^2\\text{A}$ achieves substantial\nimprovements over naive training-free baselines. We demonstrate the\neffectiveness of $\\text{AV}^2\\text{A}$ on both zero-shot and weakly-supervised\nstate-of-the-art methods, achieving notable improvements in performance metrics\nover existing approaches."}
{"id": "2503.13707", "pdf": "https://arxiv.org/pdf/2503.13707", "abs": "https://arxiv.org/abs/2503.13707", "authors": ["Saket Gurukar", "Asim Kadav"], "title": "Long-VMNet: Accelerating Long-Form Video Understanding via Fixed Memory", "categories": ["cs.CV"], "comment": null, "summary": "Long-form video understanding is essential for various applications such as\nvideo retrieval, summarizing, and question answering. Yet, traditional\napproaches demand substantial computing power and are often bottlenecked by GPU\nmemory. To tackle this issue, we present Long-Video Memory Network, Long-VMNet,\na novel video understanding method that employs a fixed-size memory\nrepresentation to store discriminative patches sampled from the input video.\nLong-VMNet achieves improved efficiency by leveraging a neural sampler that\nidentifies discriminative tokens. Additionally, Long-VMNet only needs one scan\nthrough the video, greatly boosting efficiency. Our results on the Rest-ADL\ndataset demonstrate an 18x -- 75x improvement in inference times for long-form\nvideo retrieval and answering questions, with a competitive predictive\nperformance."}
{"id": "2503.13710", "pdf": "https://arxiv.org/pdf/2503.13710", "abs": "https://arxiv.org/abs/2503.13710", "authors": ["Iryna Repinetska", "Anna Hilsmann", "Peter Eisert"], "title": "Improving Geometric Consistency for 360-Degree Neural Radiance Fields in Indoor Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Photo-realistic rendering and novel view synthesis play a crucial role in\nhuman-computer interaction tasks, from gaming to path planning. Neural Radiance\nFields (NeRFs) model scenes as continuous volumetric functions and achieve\nremarkable rendering quality. However, NeRFs often struggle in large,\nlow-textured areas, producing cloudy artifacts known as ''floaters'' that\nreduce scene realism, especially in indoor environments with featureless\narchitectural surfaces like walls, ceilings, and floors. To overcome this\nlimitation, prior work has integrated geometric constraints into the NeRF\npipeline, typically leveraging depth information derived from Structure from\nMotion or Multi-View Stereo. Yet, conventional RGB-feature correspondence\nmethods face challenges in accurately estimating depth in textureless regions,\nleading to unreliable constraints. This challenge is further complicated in\n360-degree ''inside-out'' views, where sparse visual overlap between adjacent\nimages further hinders depth estimation. In order to address these issues, we\npropose an efficient and robust method for computing dense depth priors,\nspecifically tailored for large low-textured architectural surfaces in indoor\nenvironments. We introduce a novel depth loss function to enhance rendering\nquality in these challenging, low-feature regions, while complementary\ndepth-patch regularization further refines depth consistency across other\nareas. Experiments with Instant-NGP on two synthetic 360-degree indoor scenes\ndemonstrate improved visual fidelity with our method compared to standard\nphotometric loss and Mean Squared Error depth supervision."}
{"id": "2503.13721", "pdf": "https://arxiv.org/pdf/2503.13721", "abs": "https://arxiv.org/abs/2503.13721", "authors": ["Zhenlong Yuan", "Zhidong Yang", "Yujun Cai", "Kuangxin Wu", "Mufan Liu", "Dapeng Zhang", "Hao Jiang", "Zhaoxin Li", "Zhaoqi Wang"], "title": "SED-MVS: Segmentation-Driven and Edge-Aligned Deformation Multi-View Stereo with Depth Restoration and Occlusion Constraint", "categories": ["cs.CV"], "comment": null, "summary": "Recently, patch-deformation methods have exhibited significant effectiveness\nin multi-view stereo owing to the deformable and expandable patches in\nreconstructing textureless areas. However, such methods primarily emphasize\nbroadening the receptive field in textureless areas, while neglecting\ndeformation instability caused by easily overlooked edge-skipping, potentially\nleading to matching distortions. To address this, we propose SED-MVS, which\nadopts panoptic segmentation and multi-trajectory diffusion strategy for\nsegmentation-driven and edge-aligned patch deformation. Specifically, to\nprevent unanticipated edge-skipping, we first employ SAM2 for panoptic\nsegmentation as depth-edge guidance to guide patch deformation, followed by\nmulti-trajectory diffusion strategy to ensure patches are comprehensively\naligned with depth edges. Moreover, to avoid potential inaccuracy of random\ninitialization, we combine both sparse points from LoFTR and monocular depth\nmap from DepthAnything V2 to restore reliable and realistic depth map for\ninitialization and supervised guidance. Finally, we integrate segmentation\nimage with monocular depth map to exploit inter-instance occlusion\nrelationship, then further regard them as occlusion map to implement two\ndistinct edge constraint, thereby facilitating occlusion-aware patch\ndeformation. Extensive results on ETH3D, Tanks & Temples, BlendedMVS and\nStrecha datasets validate the state-of-the-art performance and robust\ngeneralization capability of our proposed method."}
{"id": "2503.13724", "pdf": "https://arxiv.org/pdf/2503.13724", "abs": "https://arxiv.org/abs/2503.13724", "authors": ["Shristi Das Biswas", "Efstathia Soufleri", "Arani Roy", "Kaushik Roy"], "title": "Towards Scalable Modeling of Compressed Videos for Efficient Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Training robust deep video representations has proven to be computationally\nchallenging due to substantial decoding overheads, the enormous size of raw\nvideo streams, and their inherent high temporal redundancy. Different from\nexisting schemes, operating exclusively in the compressed video domain and\nexploiting all freely available modalities, i.e., I-frames, and P-frames\n(motion vectors and residuals) offers a compute-efficient alternative. Existing\nmethods approach this task as a naive multi-modality problem, ignoring the\ntemporal correlation and implicit sparsity across P-frames for modeling\nstronger shared representations for videos of the same action, making training\nand generalization easier. By revisiting the high-level design of dominant\nvideo understanding backbones, we increase inference speed by a factor of $56$\nwhile retaining similar performance. For this, we propose a hybrid end-to-end\nframework that factorizes learning across three key concepts to reduce\ninference cost by $330\\times$ versus prior art: First, a specially designed\ndual-encoder scheme with efficient Spiking Temporal Modulators to minimize\nlatency while retaining cross-domain feature aggregation. Second, a unified\ntransformer model to capture inter-modal dependencies using global\nself-attention to enhance I-frame -- P-frame contextual interactions. Third, a\nMulti-Modal Mixer Block to model rich representations from the joint\nspatiotemporal token embeddings. Experiments show that our method results in a\nlightweight architecture achieving state-of-the-art video recognition\nperformance on UCF-101, HMDB-51, K-400, K-600 and SS-v2 datasets with favorable\ncosts ($0.73$J/V) and fast inference ($16$V/s). Our observations bring new\ninsights into practical design choices for efficient next-generation\nspatiotemporal learners. Code is available."}
{"id": "2503.13730", "pdf": "https://arxiv.org/pdf/2503.13730", "abs": "https://arxiv.org/abs/2503.13730", "authors": ["Forouzan Fallah", "Maitreya Patel", "Agneet Chatterjee", "Vlad I. Morariu", "Chitta Baral", "Yezhou Yang"], "title": "TextInVision: Text and Prompt Complexity Driven Visual Text Generation Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Generating images with embedded text is crucial for the automatic production\nof visual and multimodal documents, such as educational materials and\nadvertisements. However, existing diffusion-based text-to-image models often\nstruggle to accurately embed text within images, facing challenges in spelling\naccuracy, contextual relevance, and visual coherence. Evaluating the ability of\nsuch models to embed text within a generated image is complicated due to the\nlack of comprehensive benchmarks. In this work, we introduce TextInVision, a\nlarge-scale, text and prompt complexity driven benchmark designed to evaluate\nthe ability of diffusion models to effectively integrate visual text into\nimages. We crafted a diverse set of prompts and texts that consider various\nattributes and text characteristics. Additionally, we prepared an image dataset\nto test Variational Autoencoder (VAE) models across different character\nrepresentations, highlighting that VAE architectures can also pose challenges\nin text generation within diffusion frameworks. Through extensive analysis of\nmultiple models, we identify common errors and highlight issues such as\nspelling inaccuracies and contextual mismatches. By pinpointing the failure\npoints across different prompts and texts, our research lays the foundation for\nfuture advancements in AI-generated multimodal content."}
{"id": "2503.13739", "pdf": "https://arxiv.org/pdf/2503.13739", "abs": "https://arxiv.org/abs/2503.13739", "authors": ["Keqi Chen", "Vinkle Srivastav", "Didier Mutter", "Nicolas Padoy"], "title": "Learning from Synchronization: Self-Supervised Uncalibrated Multi-View Person Association in Challenging Scenes", "categories": ["cs.CV"], "comment": "Accepted for CVPR 2025. Code:\n  https://github.com/CAMMA-public/Self-MVA", "summary": "Multi-view person association is a fundamental step towards multi-view\nanalysis of human activities. Although the person re-identification features\nhave been proven effective, they become unreliable in challenging scenes where\npersons share similar appearances. Therefore, cross-view geometric constraints\nare required for a more robust association. However, most existing approaches\nare either fully-supervised using ground-truth identity labels or require\ncalibrated camera parameters that are hard to obtain. In this work, we\ninvestigate the potential of learning from synchronization, and propose a\nself-supervised uncalibrated multi-view person association approach, Self-MVA,\nwithout using any annotations. Specifically, we propose a self-supervised\nlearning framework, consisting of an encoder-decoder model and a\nself-supervised pretext task, cross-view image synchronization, which aims to\ndistinguish whether two images from different views are captured at the same\ntime. The model encodes each person's unified geometric and appearance\nfeatures, and we train it by utilizing synchronization labels for supervision\nafter applying Hungarian matching to bridge the gap between instance-wise and\nimage-wise distances. To further reduce the solution space, we propose two\ntypes of self-supervised linear constraints: multi-view re-projection and\npairwise edge association. Extensive experiments on three challenging public\nbenchmark datasets (WILDTRACK, MVOR, and SOLDIERS) show that our approach\nachieves state-of-the-art results, surpassing existing unsupervised and\nfully-supervised approaches. Code is available at\nhttps://github.com/CAMMA-public/Self-MVA."}
{"id": "2503.13740", "pdf": "https://arxiv.org/pdf/2503.13740", "abs": "https://arxiv.org/abs/2503.13740", "authors": ["Yuxuan Jiang", "Chengxi Zeng", "Siyue Teng", "Fan Zhang", "Xiaoqing Zhu", "Joel Sole", "David Bull"], "title": "C2D-ISR: Optimizing Attention-based Image Super-resolution from Continuous to Discrete Scales", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, attention mechanisms have been exploited in single image\nsuper-resolution (SISR), achieving impressive reconstruction results. However,\nthese advancements are still limited by the reliance on simple training\nstrategies and network architectures designed for discrete up-sampling scales,\nwhich hinder the model's ability to effectively capture information across\nmultiple scales. To address these limitations, we propose a novel framework,\n\\textbf{C2D-ISR}, for optimizing attention-based image super-resolution models\nfrom both performance and complexity perspectives. Our approach is based on a\ntwo-stage training methodology and a hierarchical encoding mechanism. The new\ntraining methodology involves continuous-scale training for discrete scale\nmodels, enabling the learning of inter-scale correlations and multi-scale\nfeature representation. In addition, we generalize the hierarchical encoding\nmechanism with existing attention-based network structures, which can achieve\nimproved spatial feature fusion, cross-scale information aggregation, and more\nimportantly, much faster inference. We have evaluated the C2D-ISR framework\nbased on three efficient attention-based backbones, SwinIR-L, SRFormer-L and\nMambaIRv2-L, and demonstrated significant improvements over the other existing\noptimization framework, HiT, in terms of super-resolution performance (up to\n0.2dB) and computational complexity reduction (up to 11%). The source code will\nbe made publicly available at www.github.com."}
{"id": "2503.13743", "pdf": "https://arxiv.org/pdf/2503.13743", "abs": "https://arxiv.org/abs/2503.13743", "authors": ["Johannes Meier", "Louis Inchingolo", "Oussema Dhaouadi", "Yan Xia", "Jacques Kaiser", "Daniel Cremers"], "title": "MonoCT: Overcoming Monocular 3D Detection Domain Shift with Consistent Teacher Models", "categories": ["cs.CV"], "comment": "ICRA2025", "summary": "We tackle the problem of monocular 3D object detection across different\nsensors, environments, and camera setups. In this paper, we introduce a novel\nunsupervised domain adaptation approach, MonoCT, that generates highly accurate\npseudo labels for self-supervision. Inspired by our observation that accurate\ndepth estimation is critical to mitigating domain shifts, MonoCT introduces a\nnovel Generalized Depth Enhancement (GDE) module with an ensemble concept to\nimprove depth estimation accuracy. Moreover, we introduce a novel Pseudo Label\nScoring (PLS) module by exploring inner-model consistency measurement and a\nDiversity Maximization (DM) strategy to further generate high-quality pseudo\nlabels for self-training. Extensive experiments on six benchmarks show that\nMonoCT outperforms existing SOTA domain adaptation methods by large margins\n(~21% minimum for AP Mod.) and generalizes well to car, traffic camera and\ndrone views."}
{"id": "2503.13745", "pdf": "https://arxiv.org/pdf/2503.13745", "abs": "https://arxiv.org/abs/2503.13745", "authors": ["Ali Mollaahmadi Dehaghi", "Hossein KhademSohi", "Reza Razavi", "Steve Drew", "Mohammad Moshirpour"], "title": "FedVSR: Towards Model-Agnostic Federated Learning in Video Super-Resolution", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Video Super-Resolution (VSR) reconstructs high-resolution videos from\nlow-resolution inputs to restore fine details and improve visual clarity. While\ndeep learning-based VSR methods achieve impressive results, their centralized\nnature raises serious privacy concerns, particularly in applications with\nstrict privacy requirements. Federated Learning (FL) offers an alternative\napproach, but existing FL methods struggle with low-level vision tasks, leading\nto suboptimal reconstructions. To address this, we propose FedVSR1, a novel,\narchitecture-independent, and stateless FL framework for VSR. Our approach\nintroduces a lightweight loss term that improves local optimization and guides\nglobal aggregation with minimal computational overhead. To the best of our\nknowledge, this is the first attempt at federated VSR. Extensive experiments\nshow that FedVSR outperforms general FL methods by an average of 0.85 dB in\nPSNR, highlighting its effectiveness. The code is available at:\nhttps://github.com/alimd94/FedVSR"}
{"id": "2503.13756", "pdf": "https://arxiv.org/pdf/2503.13756", "abs": "https://arxiv.org/abs/2503.13756", "authors": ["Yunpeng Shi", "Amit Singer", "Eric J. Verbeke"], "title": "Fast alignment of heterogeneous images in sliced Wasserstein distance", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Many applications of computer vision rely on the alignment of similar but\nnon-identical images. We present a fast algorithm for aligning heterogeneous\nimages based on optimal transport. Our approach combines the speed of fast\nFourier methods with the robustness of sliced probability metrics and allows us\nto efficiently compute the alignment between two $L \\times L$ images using the\nsliced 2-Wasserstein distance in $O(L^2 \\log L)$ operations. We show that our\nmethod is robust to translations, rotations and deformations in the images."}
{"id": "2503.13769", "pdf": "https://arxiv.org/pdf/2503.13769", "abs": "https://arxiv.org/abs/2503.13769", "authors": ["Kartik Thakral", "Tamar Glaser", "Tal Hassner", "Mayank Vatsa", "Richa Singh"], "title": "Continual Unlearning for Foundational Text-to-Image Models without Generalization Erosion", "categories": ["cs.CV"], "comment": "Under submission in T-PAMI", "summary": "How can we effectively unlearn selected concepts from pre-trained generative\nfoundation models without resorting to extensive retraining? This research\nintroduces `continual unlearning', a novel paradigm that enables the targeted\nremoval of multiple specific concepts from foundational generative models,\nincrementally. We propose Decremental Unlearning without Generalization Erosion\n(DUGE) algorithm which selectively unlearns the generation of undesired\nconcepts while preserving the generation of related, non-targeted concepts and\nalleviating generalization erosion. For this, DUGE targets three losses: a\ncross-attention loss that steers the focus towards images devoid of the target\nconcept; a prior-preservation loss that safeguards knowledge related to\nnon-target concepts; and a regularization loss that prevents the model from\nsuffering from generalization erosion. Experimental results demonstrate the\nability of the proposed approach to exclude certain concepts without\ncompromising the overall integrity and performance of the model. This offers a\npragmatic solution for refining generative models, adeptly handling the\nintricacies of model training and concept management lowering the risks of\ncopyright infringement, personal or licensed material misuse, and replication\nof distinctive artistic styles. Importantly, it maintains the non-targeted\nconcepts, thereby safeguarding the model's core capabilities and effectiveness."}
{"id": "2503.13777", "pdf": "https://arxiv.org/pdf/2503.13777", "abs": "https://arxiv.org/abs/2503.13777", "authors": ["Xuyang Fang", "Sion Hannuna", "Neill Campbell"], "title": "8-Calves Image dataset", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "We introduce the 8-Calves dataset, a benchmark for evaluating object\ndetection and identity classification in occlusion-rich, temporally consistent\nenvironments. The dataset comprises a 1-hour video (67,760 frames) of eight\nHolstein Friesian calves in a barn, with ground truth bounding boxes and\nidentities, alongside 900 static frames for detection tasks. Each calf exhibits\na unique coat pattern, enabling precise identity distinction.\n  For cow detection, we fine-tuned 28 models (25 YOLO variants, 3 transformers)\non 600 frames, testing on the full video. Results reveal smaller YOLO models\n(e.g., YOLOV9c) outperform larger counterparts despite potential bias from a\nYOLOv8m-based labeling pipeline. For identity classification, embeddings from\n23 pretrained vision models (ResNet, ConvNextV2, ViTs) were evaluated via\nlinear classifiers and KNN. Modern architectures like ConvNextV2 excelled,\nwhile larger models frequently overfit, highlighting inefficiencies in scaling.\n  Key findings include: (1) Minimal, targeted augmentations (e.g., rotation)\noutperform complex strategies on simpler datasets; (2) Pretraining strategies\n(e.g., BEiT, DinoV2) significantly boost identity recognition; (3) Temporal\ncontinuity and natural motion patterns offer unique challenges absent in\nsynthetic or domain-specific benchmarks. The dataset's controlled design and\nextended sequences (1 hour vs. prior 10-minute benchmarks) make it a pragmatic\ntool for stress-testing occlusion handling, temporal consistency, and\nefficiency.\n  The link to the dataset is https://github.com/tonyFang04/8-calves."}
{"id": "2503.13778", "pdf": "https://arxiv.org/pdf/2503.13778", "abs": "https://arxiv.org/abs/2503.13778", "authors": ["Dmitrii Usenko", "David Helman", "Chen Giladi"], "title": "Using 3D reconstruction from image motion to predict total leaf area in dwarf tomato plants", "categories": ["cs.CV", "cs.AI"], "comment": "24 pages, 11 figures, submitted to Computers and Electronics in\n  Agriculture", "summary": "Accurate estimation of total leaf area (TLA) is crucial for evaluating plant\ngrowth, photosynthetic activity, and transpiration. However, it remains\nchallenging for bushy plants like dwarf tomatoes due to their complex canopies.\nTraditional methods are often labor-intensive, damaging to plants, or limited\nin capturing canopy complexity. This study evaluated a non-destructive method\ncombining sequential 3D reconstructions from RGB images and machine learning to\nestimate TLA for three dwarf tomato cultivars: Mohamed, Hahms Gelbe Topftomate,\nand Red Robin -- grown under controlled greenhouse conditions. Two experiments\n(spring-summer and autumn-winter) included 73 plants, yielding 418 TLA\nmeasurements via an \"onion\" approach. High-resolution videos were recorded, and\n500 frames per plant were used for 3D reconstruction. Point clouds were\nprocessed using four algorithms (Alpha Shape, Marching Cubes, Poisson's, Ball\nPivoting), and meshes were evaluated with seven regression models:\nMultivariable Linear Regression, Lasso Regression, Ridge Regression, Elastic\nNet Regression, Random Forest, Extreme Gradient Boosting, and Multilayer\nPerceptron. The Alpha Shape reconstruction ($\\alpha = 3$) with Extreme Gradient\nBoosting achieved the best performance ($R^2 = 0.80$, $MAE = 489 cm^2$).\nCross-experiment validation showed robust results ($R^2 = 0.56$, $MAE = 579\ncm^2$). Feature importance analysis identified height, width, and surface area\nas key predictors. This scalable, automated TLA estimation method is suited for\nurban farming and precision agriculture, offering applications in automated\npruning, resource efficiency, and sustainable food production. The approach\ndemonstrated robustness across variable environmental conditions and canopy\nstructures."}
{"id": "2503.13792", "pdf": "https://arxiv.org/pdf/2503.13792", "abs": "https://arxiv.org/abs/2503.13792", "authors": ["Xinyu Tian", "Shu Zou", "Zhaoyuan Yang", "Jing Zhang"], "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "The evolution of Large Vision-Language Models (LVLMs) has progressed from\nsingle to multi-image reasoning. Despite this advancement, our findings\nindicate that LVLMs struggle to robustly utilize information across multiple\nimages, with predictions significantly affected by the alteration of image\npositions. To further explore this issue, we introduce Position-wise Question\nAnswering (PQA), a meticulously designed task to quantify reasoning\ncapabilities at each position. Our analysis reveals a pronounced position bias\nin LVLMs: open-source models excel in reasoning with images positioned later\nbut underperform with those in the middle or at the beginning, while\nproprietary models show improved comprehension for images at the beginning and\nend but struggle with those in the middle. Motivated by this, we propose SoFt\nAttention (SoFA), a simple, training-free approach that mitigates this bias by\nemploying linear interpolation between inter-image causal attention and\nbidirectional counterparts. Experimental results demonstrate that SoFA reduces\nposition bias and enhances the reasoning performance of existing LVLMs."}
{"id": "2503.13794", "pdf": "https://arxiv.org/pdf/2503.13794", "abs": "https://arxiv.org/abs/2503.13794", "authors": ["Yang Zhou", "Shiyu Zhao", "Yuxiao Chen", "Zhenting Wang", "Dimitris N. Metaxas"], "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large foundation models trained on large-scale visual-text data can\nsignificantly enhance Open Vocabulary Object Detection (OVD) through data\ngeneration. However, this may lead to biased synthetic data and overfitting to\nspecific configurations. It can sidestep biases of manually curated data\ngeneration by directly leveraging hidden states of Large Language Models\n(LLMs), which is surprisingly rarely explored. This paper presents a systematic\nmethod to enhance visual grounding by utilizing decoder layers of the LLM of a\nMLLM. We introduce a zero-initialized cross-attention adapter to enable\nefficient knowledge transfer from LLMs to object detectors, an new approach\ncalled LED (LLM Enhanced Open-Vocabulary Object Detection). We demonstrate that\nintermediate hidden states from early LLM layers retain strong spatial-semantic\ncorrelations that are beneficial to grounding tasks. Experiments show that our\nadaptation strategy significantly enhances the performance on complex free-form\ntext queries while remaining the same on plain categories. With our adaptation,\nQwen2-0.5B with Swin-T as the vision encoder improves GroundingDINO by 2.33% on\nOmnilabel, at the overhead of 8.7% more GFLOPs. Qwen2-0.5B with a larger vision\nencoder can further boost the performance by 6.22%. We further validate our\ndesign by ablating on varied adapter architectures, sizes of LLMs, and which\nlayers to add adaptation."}
{"id": "2503.13799", "pdf": "https://arxiv.org/pdf/2503.13799", "abs": "https://arxiv.org/abs/2503.13799", "authors": ["Liangrui Pan", "Xiaoyu Li", "Yutao Dou", "Qiya Song", "Jiadi Luo", "Qingchun Liang", "Shaoliang Peng"], "title": "SMILE: a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spread through air spaces (STAS) represents a newly identified aggressive\npattern in lung cancer, which is known to be associated with adverse prognostic\nfactors and complex pathological features. Pathologists currently rely on time\nconsuming manual assessments, which are highly subjective and prone to\nvariation. This highlights the urgent need for automated and precise diag\nnostic solutions. 2,970 lung cancer tissue slides are comprised from multiple\ncenters, re-diagnosed them, and constructed and publicly released three lung\ncancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS\ndatasets provide corresponding pathological feature diagnoses and related\nclinical data. To address the bias, sparse and heterogeneous nature of STAS, we\npropose an scale-aware multiple instance learning(SMILE) method for STAS\ndiagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,\nthe SMILE can adaptively adjust high attention instances, reducing\nover-reliance on local regions and promoting consistent detection of STAS\nlesions. Extensive experiments show that SMILE achieved competitive diagnostic\nresults on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC\nandTCGA,respectively, surpassing clinical average AUC. The 11 open baseline\nresults are the first to be established for STAS research, laying the\nfoundation for the future expansion, interpretability, and clinical integration\nof computational pathology technologies. The datasets and code are available at\nhttps://anonymous.4open.science/r/IJCAI25-1DA1."}
{"id": "2503.13805", "pdf": "https://arxiv.org/pdf/2503.13805", "abs": "https://arxiv.org/abs/2503.13805", "authors": ["Muhammad Ahtesham", "Xin Zhong"], "title": "Text-Guided Image Invariant Feature Learning for Robust Image Watermarking", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Ensuring robustness in image watermarking is crucial for and maintaining\ncontent integrity under diverse transformations. Recent self-supervised\nlearning (SSL) approaches, such as DINO, have been leveraged for watermarking\nbut primarily focus on general feature representation rather than explicitly\nlearning invariant features. In this work, we propose a novel text-guided\ninvariant feature learning framework for robust image watermarking. Our\napproach leverages CLIP's multimodal capabilities, using text embeddings as\nstable semantic anchors to enforce feature invariance under distortions. We\nevaluate the proposed method across multiple datasets, demonstrating superior\nrobustness against various image transformations. Compared to state-of-the-art\nSSL methods, our model achieves higher cosine similarity in feature consistency\ntests and outperforms existing watermarking schemes in extraction accuracy\nunder severe distortions. These results highlight the efficacy of our method in\nlearning invariant representations tailored for robust deep learning-based\nwatermarking."}
{"id": "2503.13806", "pdf": "https://arxiv.org/pdf/2503.13806", "abs": "https://arxiv.org/abs/2503.13806", "authors": ["Wenjie Zhang", "Ziyang Zhang", "Mengnan He", "Jiancheng Ye"], "title": "Organ-aware Multi-scale Medical Image Segmentation Using Text Prompt Engineering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate segmentation is essential for effective treatment planning and\ndisease monitoring. Existing medical image segmentation methods predominantly\nrely on uni-modal visual inputs, such as images or videos, requiring\nlabor-intensive manual annotations. Additionally, medical imaging techniques\ncapture multiple intertwined organs within a single scan, further complicating\nsegmentation accuracy. To address these challenges, MedSAM, a large-scale\nmedical segmentation model based on the Segment Anything Model (SAM), was\ndeveloped to enhance segmentation accuracy by integrating image features with\nuser-provided prompts. While MedSAM has demonstrated strong performance across\nvarious medical segmentation tasks, it primarily relies on geometric prompts\n(e.g., points and bounding boxes) and lacks support for text-based prompts,\nwhich could help specify subtle or ambiguous anatomical structures. To overcome\nthese limitations, we propose the Organ-aware Multi-scale Text-guided Medical\nImage Segmentation Model (OMT-SAM) for multi-organ segmentation. Our approach\nintroduces CLIP encoders as a novel image-text prompt encoder, operating with\nthe geometric prompt encoder to provide informative contextual guidance. We\npair descriptive textual prompts with corresponding images, processing them\nthrough pre-trained CLIP encoders and a cross-attention mechanism to generate\nfused image-text embeddings. Additionally, we extract multi-scale visual\nfeatures from MedSAM, capturing fine-grained anatomical details at different\nlevels of granularity. We evaluate OMT-SAM on the FLARE 2021 dataset,\nbenchmarking its performance against existing segmentation methods. Empirical\nresults demonstrate that OMT-SAM achieves a mean Dice Similarity Coefficient of\n0.937, outperforming MedSAM (0.893) and other segmentation models, highlighting\nits superior capability in handling complex medical image segmentation tasks."}
{"id": "2503.13814", "pdf": "https://arxiv.org/pdf/2503.13814", "abs": "https://arxiv.org/abs/2503.13814", "authors": ["Jinping Wang", "Weiwei Song", "Hao Chen", "Jinchang Ren", "Huimin Zhao"], "title": "FusDreamer: Label-efficient Remote Sensing World Model for Multimodal Data Classification", "categories": ["cs.CV"], "comment": null, "summary": "World models significantly enhance hierarchical understanding, improving data\nintegration and learning efficiency. To explore the potential of the world\nmodel in the remote sensing (RS) field, this paper proposes a label-efficient\nremote sensing world model for multimodal data fusion (FusDreamer). The\nFusDreamer uses the world model as a unified representation container to\nabstract common and high-level knowledge, promoting interactions across\ndifferent types of data, \\emph{i.e.}, hyperspectral (HSI), light detection and\nranging (LiDAR), and text data. Initially, a new latent diffusion fusion and\nmultimodal generation paradigm (LaMG) is utilized for its exceptional\ninformation integration and detail retention capabilities. Subsequently, an\nopen-world knowledge-guided consistency projection (OK-CP) module incorporates\nprompt representations for visually described objects and aligns\nlanguage-visual features through contrastive learning. In this way, the domain\ngap can be bridged by fine-tuning the pre-trained world models with limited\nsamples. Finally, an end-to-end multitask combinatorial optimization (MuCO)\nstrategy can capture slight feature bias and constrain the diffusion process in\na collaboratively learnable direction. Experiments conducted on four typical\ndatasets indicate the effectiveness and advantages of the proposed FusDreamer.\nThe corresponding code will be released at\nhttps://github.com/Cimy-wang/FusDreamer."}
{"id": "2503.13816", "pdf": "https://arxiv.org/pdf/2503.13816", "abs": "https://arxiv.org/abs/2503.13816", "authors": ["Zhixuan Liu", "Haokun Zhu", "Rui Chen", "Jonathan Francis", "Soonmin Hwang", "Ji Zhang", "Jean Oh"], "title": "MOSAIC: Generating Consistent, Privacy-Preserving Scenes from Multiple Depth Views in Multi-Room Environments", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a novel diffusion-based approach for generating\nprivacy-preserving digital twins of multi-room indoor environments from depth\nimages only. Central to our approach is a novel Multi-view Overlapped Scene\nAlignment with Implicit Consistency (MOSAIC) model that explicitly considers\ncross-view dependencies within the same scene in the probabilistic sense.\nMOSAIC operates through a novel inference-time optimization that avoids error\naccumulation common in sequential or single-room constraint in panorama-based\napproaches. MOSAIC scales to complex scenes with zero extra training and\nprovably reduces the variance during denoising processes when more overlapping\nviews are added, leading to improved generation quality. Experiments show that\nMOSAIC outperforms state-of-the-art baselines on image fidelity metrics in\nreconstructing complex multi-room environments. Project page is available at:\nhttps://mosaic-cmubig.github.io"}
{"id": "2503.13821", "pdf": "https://arxiv.org/pdf/2503.13821", "abs": "https://arxiv.org/abs/2503.13821", "authors": ["Chi Hsuan Wu", "Kumar Ashutosh", "Kristen Grauman"], "title": "Stitch-a-Recipe: Video Demonstration from Multistep Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "When obtaining visual illustrations from text descriptions, today's methods\ntake a description with-a single text context caption, or an action\ndescription-and retrieve or generate the matching visual context. However,\nprior work does not permit visual illustration of multistep descriptions, e.g.\na cooking recipe composed of multiple steps. Furthermore, simply handling each\nstep description in isolation would result in an incoherent demonstration. We\npropose Stitch-a-Recipe, a novel retrieval-based method to assemble a video\ndemonstration from a multistep description. The resulting video contains clips,\npossibly from different sources, that accurately reflect all the step\ndescriptions, while being visually coherent. We formulate a training pipeline\nthat creates large-scale weakly supervised data containing diverse and novel\nrecipes and injects hard negatives that promote both correctness and coherence.\nValidated on in-the-wild instructional videos, Stitch-a-Recipe achieves\nstate-of-the-art performance, with quantitative gains up to 24% as well as\ndramatic wins in a human preference study."}
{"id": "2503.13828", "pdf": "https://arxiv.org/pdf/2503.13828", "abs": "https://arxiv.org/abs/2503.13828", "authors": ["Chunlei Li", "Yilei Shi", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Unsupervised anomaly detection using deep learning has garnered significant\nresearch attention due to its broad applicability, particularly in medical\nimaging where labeled anomalous data are scarce. While earlier approaches\nleverage generative models like autoencoders and generative adversarial\nnetworks (GANs), they often fall short due to overgeneralization. Recent\nmethods explore various strategies, including memory banks, normalizing flows,\nself-supervised learning, and knowledge distillation, to enhance\ndiscrimination. Among these, knowledge distillation, particularly reverse\ndistillation, has shown promise. Following this paradigm, we propose a novel\nscale-aware contrastive reverse distillation model that addresses two key\nlimitations of existing reverse distillation methods: insufficient feature\ndiscriminability and inability to handle anomaly scale variations.\nSpecifically, we introduce a contrastive student-teacher learning approach to\nderive more discriminative representations by generating and exploring\nout-of-normal distributions. Further, we design a scale adaptation mechanism to\nsoftly weight contrastive distillation losses at different scales to account\nfor the scale variation issue. Extensive experiments on benchmark datasets\ndemonstrate state-of-the-art performance, validating the efficacy of the\nproposed method. Code is available at https://github.com/MedAITech/SCRD4AD."}
{"id": "2503.13834", "pdf": "https://arxiv.org/pdf/2503.13834", "abs": "https://arxiv.org/abs/2503.13834", "authors": ["JuneHyoung Kwon", "MiHyeon Kim", "Eunju Lee", "Juhwan Choi", "YoungBin Kim"], "title": "See-Saw Modality Balance: See Gradient, and Sew Impaired Vision-Language Balance to Mitigate Dominant Modality Bias", "categories": ["cs.CV"], "comment": "Accepted to NAACL 2025 Main", "summary": "Vision-language (VL) models have demonstrated strong performance across\nvarious tasks. However, these models often rely on a specific modality for\npredictions, leading to \"dominant modality bias.'' This bias significantly\nhurts performance, especially when one modality is impaired. In this study, we\nanalyze model behavior under dominant modality bias and theoretically show that\nunaligned gradients or differences in gradient magnitudes prevent balanced\nconvergence of the loss. Based on these findings, we propose a novel framework,\nBalGrad to mitigate dominant modality bias. Our approach includes\ninter-modality gradient reweighting, adjusting the gradient of KL divergence\nbased on each modality's contribution, and inter-task gradient projection to\nalign task directions in a non-conflicting manner. Experiments on UPMC\nFood-101, Hateful Memes, and MM-IMDb datasets confirm that BalGrad effectively\nalleviates over-reliance on specific modalities when making predictions."}
{"id": "2503.13836", "pdf": "https://arxiv.org/pdf/2503.13836", "abs": "https://arxiv.org/abs/2503.13836", "authors": ["Seokhyeon Hong", "Chaelin Kim", "Serin Yoon", "Junghyun Nam", "Sihun Cha", "Junyong Noh"], "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "CVPR 2025; Project page\n  https://seokhyeonhong.github.io/projects/salad/", "summary": "Text-driven motion generation has advanced significantly with the rise of\ndenoising diffusion models. However, previous methods often oversimplify\nrepresentations for the skeletal joints, temporal frames, and textual words,\nlimiting their ability to fully capture the information within each modality\nand their interactions. Moreover, when using pre-trained models for downstream\ntasks, such as editing, they typically require additional efforts, including\nmanual interventions, optimization, or fine-tuning. In this paper, we introduce\na skeleton-aware latent diffusion (SALAD), a model that explicitly captures the\nintricate inter-relationships between joints, frames, and words. Furthermore,\nby leveraging cross-attention maps produced during the generation process, we\nenable attention-based zero-shot text-driven motion editing using a pre-trained\nSALAD model, requiring no additional user input beyond text prompts. Our\napproach significantly outperforms previous methods in terms of text-motion\nalignment without compromising generation quality, and demonstrates practical\nversatility by providing diverse editing capabilities beyond generation. Code\nis available at project page."}
{"id": "2503.13847", "pdf": "https://arxiv.org/pdf/2503.13847", "abs": "https://arxiv.org/abs/2503.13847", "authors": ["Monika Shah", "Somdeb Sarkhel", "Deepak Venugopal"], "title": "Disentangling Fine-Tuning from Pre-Training in Visual Captioning with Hybrid Markov Logic", "categories": ["cs.CV", "cs.AI"], "comment": "2024 IEEE International Conference on Big Data (BigData), 10 pages", "summary": "Multimodal systems have highly complex processing pipelines and are\npretrained over large datasets before being fine-tuned for specific tasks such\nas visual captioning. However, it becomes hard to disentangle what the model\nlearns during the fine-tuning process from what it already knows due to its\npretraining. In this work, we learn a probabilistic model using Hybrid Markov\nLogic Networks (HMLNs) over the training examples by relating symbolic\nknowledge (extracted from the caption) with visual features (extracted from the\nimage). For a generated caption, we quantify the influence of training examples\nbased on the HMLN distribution using probabilistic inference. We evaluate two\ntypes of inference procedures on the MSCOCO dataset for different types of\ncaptioning models. Our results show that for BLIP2 (a model that uses a LLM),\nthe fine-tuning may have smaller influence on the knowledge the model has\nacquired since it may have more general knowledge to perform visual captioning\nas compared to models that do not use a LLM"}
{"id": "2503.13858", "pdf": "https://arxiv.org/pdf/2503.13858", "abs": "https://arxiv.org/abs/2503.13858", "authors": ["Hongyu Ke", "Jack Morris", "Kentaro Oguchi", "Xiaofei Cao", "Yongkang Liu", "Haoxin Wang", "Yi Ding"], "title": "MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "3D visual perception tasks, such as 3D detection from multi-camera images,\nare essential components of autonomous driving and assistance systems. However,\ndesigning computationally efficient methods remains a significant challenge. In\nthis paper, we propose a Mamba-based framework called MamBEV, which learns\nunified Bird's Eye View (BEV) representations using linear spatio-temporal\nSSM-based attention. This approach supports multiple 3D perception tasks with\nsignificantly improved computational and memory efficiency. Furthermore, we\nintroduce SSM based cross-attention, analogous to standard cross attention,\nwhere BEV query representations can interact with relevant image features.\nExtensive experiments demonstrate MamBEV's promising performance across diverse\nvisual perception metrics, highlighting its advantages in input scaling\nefficiency compared to existing benchmark models."}
{"id": "2503.13859", "pdf": "https://arxiv.org/pdf/2503.13859", "abs": "https://arxiv.org/abs/2503.13859", "authors": ["Jinseok Bae", "Inwoo Hwang", "Young Yoon Lee", "Ziyu Guo", "Joseph Liu", "Yizhak Ben-Shabat", "Young Min Kim", "Mubbasir Kapadia"], "title": "Less is More: Improving Motion Diffusion Models with Sparse Keyframes", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in motion diffusion models have led to remarkable progress in\ndiverse motion generation tasks, including text-to-motion synthesis. However,\nexisting approaches represent motions as dense frame sequences, requiring the\nmodel to process redundant or less informative frames. The processing of dense\nanimation frames imposes significant training complexity, especially when\nlearning intricate distributions of large motion datasets even with modern\nneural architectures. This severely limits the performance of generative motion\nmodels for downstream tasks. Inspired by professional animators who mainly\nfocus on sparse keyframes, we propose a novel diffusion framework explicitly\ndesigned around sparse and geometrically meaningful keyframes. Our method\nreduces computation by masking non-keyframes and efficiently interpolating\nmissing frames. We dynamically refine the keyframe mask during inference to\nprioritize informative frames in later diffusion steps. Extensive experiments\nshow that our approach consistently outperforms state-of-the-art methods in\ntext alignment and motion realism, while also effectively maintaining high\nperformance at significantly fewer diffusion steps. We further validate the\nrobustness of our framework by using it as a generative prior and adapting it\nto different downstream tasks. Source code and pre-trained models will be\nreleased upon acceptance."}
{"id": "2503.13861", "pdf": "https://arxiv.org/pdf/2503.13861", "abs": "https://arxiv.org/abs/2503.13861", "authors": ["Yujin Wang", "Quanfeng Liu", "Zhengxin Jiang", "Tianyi Wang", "Junfeng Jiao", "Hongqing Chu", "Bingzhao Gao", "Hong Chen"], "title": "RAD: Retrieval-Augmented Decision-Making of Meta-Actions with Vision-Language Models in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurately understanding and deciding high-level meta-actions is essential\nfor ensuring reliable and safe autonomous driving systems. While\nvision-language models (VLMs) have shown significant potential in various\nautonomous driving tasks, they often suffer from limitations such as inadequate\nspatial perception and hallucination, reducing their effectiveness in complex\nautonomous driving scenarios. To address these challenges, we propose a\nretrieval-augmented decision-making (RAD) framework, a novel architecture\ndesigned to enhance VLMs' capabilities to reliably generate meta-actions in\nautonomous driving scenes. RAD leverages a retrieval-augmented generation (RAG)\npipeline to dynamically improve decision accuracy through a three-stage process\nconsisting of the embedding flow, retrieving flow, and generating flow.\nAdditionally, we fine-tune VLMs on a specifically curated dataset derived from\nthe NuScenes dataset to enhance their spatial perception and bird's-eye view\nimage comprehension capabilities. Extensive experimental evaluations on the\ncurated NuScenes-based dataset demonstrate that RAD outperforms baseline\nmethods across key evaluation metrics, including match accuracy, and F1 score,\nand self-defined overall score, highlighting its effectiveness in improving\nmeta-action decision-making for autonomous driving tasks."}
{"id": "2503.13862", "pdf": "https://arxiv.org/pdf/2503.13862", "abs": "https://arxiv.org/abs/2503.13862", "authors": ["Jiaqi Yang", "Wenting Chen", "Xiaohan Xing", "Sean He", "Xiaoling Luo", "Xinheng Lyu", "Linlin Shen", "Guoping Qiu"], "title": "HySurvPred: Multimodal Hyperbolic Embedding with Angle-Aware Hierarchical Contrastive Learning and Uncertainty Constraints for Survival Prediction", "categories": ["cs.CV", "cs.LG"], "comment": "submitted to IJCAI2025", "summary": "Multimodal learning that integrates histopathology images and genomic data\nholds great promise for cancer survival prediction. However, existing methods\nface key limitations: 1) They rely on multimodal mapping and metrics in\nEuclidean space, which cannot fully capture the hierarchical structures in\nhistopathology (among patches from different resolutions) and genomics data\n(from genes to pathways). 2) They discretize survival time into independent\nrisk intervals, which ignores its continuous and ordinal nature and fails to\nachieve effective optimization. 3) They treat censorship as a binary indicator,\nexcluding censored samples from model optimization and not making full use of\nthem. To address these challenges, we propose HySurvPred, a novel framework for\nsurvival prediction that integrates three key modules: Multimodal Hyperbolic\nMapping (MHM), Angle-aware Ranking-based Contrastive Loss (ARCL) and\nCensor-Conditioned Uncertainty Constraint (CUC). Instead of relying on\nEuclidean space, we design the MHM module to explore the inherent hierarchical\nstructures within each modality in hyperbolic space. To better integrate\nmultimodal features in hyperbolic space, we introduce the ARCL module, which\nuses ranking-based contrastive learning to preserve the ordinal nature of\nsurvival time, along with the CUC module to fully explore the censored data.\nExtensive experiments demonstrate that our method outperforms state-of-the-art\nmethods on five benchmark datasets. The source code is to be released."}
{"id": "2503.13869", "pdf": "https://arxiv.org/pdf/2503.13869", "abs": "https://arxiv.org/abs/2503.13869", "authors": ["Jinge Ma", "Jiangpeng He", "Fengqing Zhu"], "title": "Robust3D-CIL: Robust Class-Incremental Learning for 3D Perception", "categories": ["cs.CV"], "comment": "16 pages, 7 figures", "summary": "3D perception plays a crucial role in real-world applications such as\nautonomous driving, robotics, and AR/VR. In practical scenarios, 3D perception\nmodels must continuously adapt to new data and emerging object categories, but\nretraining from scratch incurs prohibitive costs. Therefore, adopting\nclass-incremental learning (CIL) becomes particularly essential. However,\nreal-world 3D point cloud data often include corrupted samples, which poses\nsignificant challenges for existing CIL methods and leads to more severe\nforgetting on corrupted data. To address these challenges, we consider the\nscenario in which a CIL model can be updated using point clouds with unknown\ncorruption to better simulate real-world conditions. Inspired by Farthest Point\nSampling, we propose a novel exemplar selection strategy that effectively\npreserves intra-class diversity when selecting replay exemplars, mitigating\nforgetting induced by data corruption. Furthermore, we introduce a point cloud\ndownsampling-based replay method to utilize the limited replay buffer memory\nmore efficiently, thereby further enhancing the model's continual learning\nability. Extensive experiments demonstrate that our method improves the\nperformance of replay-based CIL baselines by 2% to 11%, proving its\neffectiveness and promising potential for real-world 3D applications."}
{"id": "2503.13881", "pdf": "https://arxiv.org/pdf/2503.13881", "abs": "https://arxiv.org/abs/2503.13881", "authors": ["Donggon Jang", "Yucheol Cho", "Suin Lee", "Taehyeon Kim", "Dae-Shik Kim"], "title": "MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation", "categories": ["cs.CV"], "comment": "ICLR 2025, Code and dataset are available at\n  \\url{https://github.com/jdg900/MMR}", "summary": "The fusion of Large Language Models with vision models is pioneering new\npossibilities in user-interactive vision-language tasks. A notable application\nis reasoning segmentation, where models generate pixel-level segmentation masks\nby comprehending implicit meanings in human instructions. However, seamless\nhuman-AI interaction demands more than just object-level recognition; it\nrequires understanding both objects and the functions of their detailed parts,\nparticularly in multi-target scenarios. For example, when instructing a robot\nto \\textit{turn on the TV\"}, there could be various ways to accomplish this\ncommand. Recognizing multiple objects capable of turning on the TV, such as the\nTV itself or a remote control (multi-target), provides more flexible options\nand aids in finding the optimized scenario. Furthermore, understanding specific\nparts of these objects, like the TV's button or the remote's button\n(part-level), is important for completing the action. Unfortunately, current\nreasoning segmentation datasets predominantly focus on a single target\nobject-level reasoning, which limits the detailed recognition of an object's\nparts in multi-target contexts. To address this gap, we construct a large-scale\ndataset called Multi-target and Multi-granularity Reasoning (MMR). MMR\ncomprises 194K complex and implicit instructions that consider multi-target,\nobject-level, and part-level aspects, based on pre-existing image-mask sets.\nThis dataset supports diverse and context-aware interactions by hierarchically\nproviding object and part information. Moreover, we propose a straightforward\nyet effective framework for multi-target, object-level, and part-level\nreasoning segmentation. Experimental results on MMR show that the proposed\nmethod can reason effectively in multi-target and multi-granularity scenarios,\nwhile the existing reasoning segmentation model still has room for improvement."}
{"id": "2503.13883", "pdf": "https://arxiv.org/pdf/2503.13883", "abs": "https://arxiv.org/abs/2503.13883", "authors": ["Ziyu Lin", "Yunfan Wu", "Yuhang Ma", "Junzhou Chen", "Ronghui Zhang", "Jiaming Wu", "Guodong Yin", "Liang Lin"], "title": "YOLO-LLTS: Real-Time Low-Light Traffic Sign Detection via Prior-Guided Enhancement and Multi-Branch Feature Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Detecting traffic signs effectively under low-light conditions remains a\nsignificant challenge. To address this issue, we propose YOLO-LLTS, an\nend-to-end real-time traffic sign detection algorithm specifically designed for\nlow-light environments. Firstly, we introduce the High-Resolution Feature Map\nfor Small Object Detection (HRFM-TOD) module to address indistinct small-object\nfeatures in low-light scenarios. By leveraging high-resolution feature maps,\nHRFM-TOD effectively mitigates the feature dilution problem encountered in\nconventional PANet frameworks, thereby enhancing both detection accuracy and\ninference speed. Secondly, we develop the Multi-branch Feature Interaction\nAttention (MFIA) module, which facilitates deep feature interaction across\nmultiple receptive fields in both channel and spatial dimensions, significantly\nimproving the model's information extraction capabilities. Finally, we propose\nthe Prior-Guided Enhancement Module (PGFE) to tackle common image quality\nchallenges in low-light environments, such as noise, low contrast, and\nblurriness. This module employs prior knowledge to enrich image details and\nenhance visibility, substantially boosting detection performance. To support\nthis research, we construct a novel dataset, the Chinese Nighttime Traffic Sign\nSample Set (CNTSSS), covering diverse nighttime scenarios, including urban,\nhighway, and rural environments under varying weather conditions. Experimental\nevaluations demonstrate that YOLO-LLTS achieves state-of-the-art performance,\noutperforming the previous best methods by 2.7% mAP50 and 1.6% mAP50:95 on\nTT100K-night, 1.3% mAP50 and 1.9% mAP50:95 on CNTSSS, and achieving superior\nresults on the CCTSDB2021 dataset. Moreover, deployment experiments on edge\ndevices confirm the real-time applicability and effectiveness of our proposed\napproach."}
{"id": "2503.13891", "pdf": "https://arxiv.org/pdf/2503.13891", "abs": "https://arxiv.org/abs/2503.13891", "authors": ["Xiaoying Xing", "Chia-Wen Kuo", "Li Fuxin", "Yulei Niu", "Fan Chen", "Ming Li", "Ying Wu", "Longyin Wen", "Sijie Zhu"], "title": "Where do Large Vision-Language Models Look at when Answering Questions?", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown promising performance in\nvision-language understanding and reasoning tasks. However, their visual\nunderstanding behaviors remain underexplored. A fundamental question arises: to\nwhat extent do LVLMs rely on visual input, and which image regions contribute\nto their responses? It is non-trivial to interpret the free-form generation of\nLVLMs due to their complicated visual architecture (e.g., multiple encoders and\nmulti-resolution) and variable-length outputs. In this paper, we extend\nexisting heatmap visualization methods (e.g., iGOS++) to support LVLMs for\nopen-ended visual question answering. We propose a method to select visually\nrelevant tokens that reflect the relevance between generated answers and input\nimage. Furthermore, we conduct a comprehensive analysis of state-of-the-art\nLVLMs on benchmarks designed to require visual information to answer. Our\nfindings offer several insights into LVLM behavior, including the relationship\nbetween focus region and answer correctness, differences in visual attention\nacross architectures, and the impact of LLM scale on visual understanding. The\ncode and data are available at\nhttps://github.com/bytedance/LVLM_Interpretation."}
{"id": "2503.13895", "pdf": "https://arxiv.org/pdf/2503.13895", "abs": "https://arxiv.org/abs/2503.13895", "authors": ["Xinliang Zhang", "Lei Zhu", "Shuang Zeng", "Hangzhou He", "Ourui Fu", "Zhengjian Yao", "Zhaoheng Xie", "Yanye Lu"], "title": "Exploiting Inherent Class Label: Towards Robust Scribble Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Scribble-based weakly supervised semantic segmentation leverages only a few\nannotated pixels as labels to train a segmentation model, presenting\nsignificant potential for reducing the human labor involved in the annotation\nprocess. This approach faces two primary challenges: first, the sparsity of\nscribble annotations can lead to inconsistent predictions due to limited\nsupervision; second, the variability in scribble annotations, reflecting\ndiffering human annotator preferences, can prevent the model from consistently\ncapturing the discriminative regions of objects, potentially leading to\nunstable predictions. To address these issues, we propose a holistic framework,\nthe class-driven scribble promotion network, for robust scribble-supervised\nsemantic segmentation. This framework not only utilizes the provided scribble\nannotations but also leverages their associated class labels to generate\nreliable pseudo-labels. Within the network, we introduce a localization\nrectification module to mitigate noisy labels and a distance perception module\nto identify reliable regions surrounding scribble annotations and\npseudo-labels. In addition, we introduce new large-scale benchmarks,\nScribbleCOCO and ScribbleCityscapes, accompanied by a scribble simulation\nalgorithm that enables evaluation across varying scribble styles. Our method\ndemonstrates competitive performance in both accuracy and robustness,\nunderscoring its superiority over existing approaches. The datasets and the\ncodes will be made publicly available."}
{"id": "2503.13903", "pdf": "https://arxiv.org/pdf/2503.13903", "abs": "https://arxiv.org/abs/2503.13903", "authors": ["Qiang Qi", "Xiao Wang"], "title": "TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI2025", "summary": "Video object detection has made significant progress in recent years thanks\nto convolutional neural networks (CNNs) and vision transformers (ViTs).\nTypically, CNNs excel at capturing local features but struggle to model global\nrepresentations. Conversely, ViTs are adept at capturing long-range global\nfeatures but face challenges in representing local feature details.\nOff-the-shelf video object detection methods solely rely on CNNs or ViTs to\nconduct feature aggregation, which hampers their capability to simultaneously\nleverage global and local information, thereby resulting in limited detection\nperformance. In this paper, we propose a Transformer-GraphFormer Blender\nNetwork (TGBFormer) for video object detection, with three key technical\nimprovements to fully exploit the advantages of transformers and graph\nconvolutional networks while compensating for their limitations. First, we\ndevelop a spatial-temporal transformer module to aggregate global contextual\ninformation, constituting global representations with long-range feature\ndependencies. Second, we introduce a spatial-temporal GraphFormer module that\nutilizes local spatial and temporal relationships to aggregate features,\ngenerating new local representations that are complementary to the transformer\noutputs. Third, we design a global-local feature blender module to adaptively\ncouple transformer-based global representations and GraphFormer-based local\nrepresentations. Extensive experiments demonstrate that our TGBFormer\nestablishes new state-of-the-art results on the ImageNet VID dataset.\nParticularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS\non a single Tesla A100 GPU."}
{"id": "2503.13906", "pdf": "https://arxiv.org/pdf/2503.13906", "abs": "https://arxiv.org/abs/2503.13906", "authors": ["Yuhao Qiu", "Shuyan Bai", "Tingfa Xu", "Peifu Liu", "Haolin Qin", "Jianan Li"], "title": "HSOD-BIT-V2: A New Challenging Benchmarkfor Hyperspectral Salient Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2025", "summary": "Salient Object Detection (SOD) is crucial in computer vision, yet RGB-based\nmethods face limitations in challenging scenes, such as small objects and\nsimilar color features. Hyperspectral images provide a promising solution for\nmore accurate Hyperspectral Salient Object Detection (HSOD) by abundant\nspectral information, while HSOD methods are hindered by the lack of extensive\nand available datasets. In this context, we introduce HSOD-BIT-V2, the largest\nand most challenging HSOD benchmark dataset to date. Five distinct challenges\nfocusing on small objects and foreground-background similarity are designed to\nemphasize spectral advantages and real-world complexity. To tackle these\nchallenges, we propose Hyper-HRNet, a high-resolution HSOD network. Hyper-HRNet\neffectively extracts, integrates, and preserves effective spectral information\nwhile reducing dimensionality by capturing the self-similar spectral features.\nAdditionally, it conveys fine details and precisely locates object contours by\nincorporating comprehensive global information and detailed object saliency\nrepresentations. Experimental analysis demonstrates that Hyper-HRNet\noutperforms existing models, especially in challenging scenarios."}
{"id": "2503.13914", "pdf": "https://arxiv.org/pdf/2503.13914", "abs": "https://arxiv.org/abs/2503.13914", "authors": ["Barza Nisar", "Steven L. Waslander"], "title": "PSA-SSL: Pose and Size-aware Self-Supervised Learning on LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) on 3D point clouds has the potential to learn\nfeature representations that can transfer to diverse sensors and multiple\ndownstream perception tasks. However, recent SSL approaches fail to define\npretext tasks that retain geometric information such as object pose and scale,\nwhich can be detrimental to the performance of downstream localization and\ngeometry-sensitive 3D scene understanding tasks, such as 3D semantic\nsegmentation and 3D object detection. We propose PSA-SSL, a novel extension to\npoint cloud SSL that learns object pose and size-aware (PSA) features. Our\napproach defines a self-supervised bounding box regression pretext task, which\nretains object pose and size information. Furthermore, we incorporate LiDAR\nbeam pattern augmentation on input point clouds, which encourages learning\nsensor-agnostic features. Our experiments demonstrate that with a single\npretrained model, our light-weight yet effective extensions achieve significant\nimprovements on 3D semantic segmentation with limited labels across popular\nautonomous driving datasets (Waymo, nuScenes, SemanticKITTI). Moreover, our\napproach outperforms other state-of-the-art SSL methods on 3D semantic\nsegmentation (using up to 10 times less labels), as well as on 3D object\ndetection. Our code will be released on https://github.com/TRAILab/PSA-SSL."}
{"id": "2503.13915", "pdf": "https://arxiv.org/pdf/2503.13915", "abs": "https://arxiv.org/abs/2503.13915", "authors": ["Dongkwan Lee", "Kyomin Hwang", "Nojun Kwak"], "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "We address the problem of semi-supervised domain generalization (SSDG), where\nthe distributions of train and test data differ, and only a small amount of\nlabeled data along with a larger amount of unlabeled data are available during\ntraining. Existing SSDG methods that leverage only the unlabeled samples for\nwhich the model's predictions are highly confident (confident-unlabeled\nsamples), limit the full utilization of the available unlabeled data. To the\nbest of our knowledge, we are the first to explore a method for incorporating\nthe unconfident-unlabeled samples that were previously disregarded in SSDG\nsetting. To this end, we propose UPCSC to utilize these unconfident-unlabeled\nsamples in SSDG that consists of two modules: 1) Unlabeled Proxy-based\nContrastive learning (UPC) module, treating unconfident-unlabeled samples as\nadditional negative pairs and 2) Surrogate Class learning (SC) module,\ngenerating positive pairs for unconfident-unlabeled samples using their\nconfusing class set. These modules are plug-and-play and do not require any\ndomain labels, which can be easily integrated into existing approaches.\nExperiments on four widely used SSDG benchmarks demonstrate that our approach\nconsistently improves performance when attached to baselines and outperforms\ncompeting plug-and-play methods. We also analyze the role of our method in\nSSDG, showing that it enhances class-level discriminability and mitigates\ndomain gaps. The code is available at https://github.com/dongkwani/UPCSC."}
{"id": "2503.13926", "pdf": "https://arxiv.org/pdf/2503.13926", "abs": "https://arxiv.org/abs/2503.13926", "authors": ["Huan Ren", "Wenfei Yang", "Xiang Liu", "Shifeng Zhang", "Tianzhu Zhang"], "title": "Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025. Project page is available at\n  https://renhuan1999.github.io/SpherePose", "summary": "Category-level object pose estimation aims to determine the pose and size of\nnovel objects in specific categories. Existing correspondence-based approaches\ntypically adopt point-based representations to establish the correspondences\nbetween primitive observed points and normalized object coordinates. However,\ndue to the inherent shape-dependence of canonical coordinates, these methods\nsuffer from semantic incoherence across diverse object shapes. To resolve this\nissue, we innovatively leverage the sphere as a shared proxy shape of objects\nto learn shape-independent transformation via spherical representations. Based\non this insight, we introduce a novel architecture called SpherePose, which\nyields precise correspondence prediction through three core designs. Firstly,\nWe endow the point-wise feature extraction with SO(3)-invariance, which\nfacilitates robust mapping between camera coordinate space and object\ncoordinate space regardless of rotation transformation. Secondly, the spherical\nattention mechanism is designed to propagate and integrate features among\nspherical anchors from a comprehensive perspective, thus mitigating the\ninterference of noise and incomplete point cloud. Lastly, a hyperbolic\ncorrespondence loss function is designed to distinguish subtle distinctions,\nwhich can promote the precision of correspondence prediction. Experimental\nresults on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior\nperformance of our method, verifying the effectiveness of spherical\nrepresentations and architectural innovations."}
{"id": "2503.13935", "pdf": "https://arxiv.org/pdf/2503.13935", "abs": "https://arxiv.org/abs/2503.13935", "authors": ["Bowen Yuan", "Yuxia Fu", "Zijian Wang", "Yadan Luo", "Zi Huang"], "title": "SCORE: Soft Label Compression-Centric Dataset Condensation via Coding Rate Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Dataset Condensation (DC) aims to obtain a condensed dataset that allows\nmodels trained on the condensed dataset to achieve performance comparable to\nthose trained on the full dataset. Recent DC approaches increasingly focus on\nencoding knowledge into realistic images with soft labeling, for their\nscalability to ImageNet-scale datasets and strong capability of cross-domain\ngeneralization. However, this strong performance comes at a substantial storage\ncost which could significantly exceed the storage cost of the original dataset.\nWe argue that the three key properties to alleviate this performance-storage\ndilemma are informativeness, discriminativeness, and compressibility of the\ncondensed data. Towards this end, this paper proposes a \\textbf{S}oft label\ncompression-centric dataset condensation framework using \\textbf{CO}ding\n\\textbf{R}at\\textbf{E} (SCORE). SCORE formulates dataset condensation as a\nmin-max optimization problem, which aims to balance the three key properties\nfrom an information-theoretic perspective. In particular, we theoretically\ndemonstrate that our coding rate-inspired objective function is submodular, and\nits optimization naturally enforces low-rank structure in the soft label set\ncorresponding to each condensed data. Extensive experiments on large-scale\ndatasets, including ImageNet-1K and Tiny-ImageNet, demonstrate that SCORE\noutperforms existing methods in most cases. Even with 30$\\times$ compression of\nsoft labels, performance decreases by only 5.5\\% and 2.7\\% for ImageNet-1K with\nIPC 10 and 50, respectively. Code will be released upon paper acceptance."}
{"id": "2503.13938", "pdf": "https://arxiv.org/pdf/2503.13938", "abs": "https://arxiv.org/abs/2503.13938", "authors": ["Qingyao Xu", "Siheng Chen", "Guang Chen", "Yanfeng Wang", "Ya Zhang"], "title": "ChatBEV: A Visual Language Model that Understands BEV Maps", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traffic scene understanding is essential for intelligent transportation\nsystems and autonomous driving, ensuring safe and efficient vehicle operation.\nWhile recent advancements in VLMs have shown promise for holistic scene\nunderstanding, the application of VLMs to traffic scenarios, particularly using\nBEV maps, remains under explored. Existing methods often suffer from limited\ntask design and narrow data amount, hindering comprehensive scene\nunderstanding. To address these challenges, we introduce ChatBEV-QA, a novel\nBEV VQA benchmark contains over 137k questions, designed to encompass a wide\nrange of scene understanding tasks, including global scene understanding,\nvehicle-lane interactions, and vehicle-vehicle interactions. This benchmark is\nconstructed using an novel data collection pipeline that generates scalable and\ninformative VQA data for BEV maps. We further fine-tune a specialized\nvision-language model ChatBEV, enabling it to interpret diverse question\nprompts and extract relevant context-aware information from BEV maps.\nAdditionally, we propose a language-driven traffic scene generation pipeline,\nwhere ChatBEV facilitates map understanding and text-aligned navigation\nguidance, significantly enhancing the generation of realistic and consistent\ntraffic scenarios. The dataset, code and the fine-tuned model will be released."}
{"id": "2503.13939", "pdf": "https://arxiv.org/pdf/2503.13939", "abs": "https://arxiv.org/abs/2503.13939", "authors": ["Yuxiang Lai", "Jike Zhong", "Ming Li", "Shitian Zhao", "Xiaofeng Yang"], "title": "Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have advanced reasoning in natural scenes, but\ntheir role in medical imaging remains underexplored. Medical reasoning tasks\ndemand robust image analysis and well-justified answers, posing challenges due\nto the complexity of medical images. Transparency and trustworthiness are\nessential for clinical adoption and regulatory compliance. We introduce Med-R1,\na framework exploring reinforcement learning (RL) to enhance VLMs'\ngeneralizability and trustworthiness in medical reasoning. Leveraging the\nDeepSeek strategy, we employ Group Relative Policy Optimization (GRPO) to guide\nreasoning paths via reward signals. Unlike supervised fine-tuning (SFT), which\noften overfits and lacks generalization, RL fosters robust and diverse\nreasoning. Med-R1 is evaluated across eight medical imaging modalities: CT,\nMRI, Ultrasound, Dermoscopy, Fundus Photography, Optical Coherence Tomography\n(OCT), Microscopy, and X-ray Imaging. Compared to its base model, Qwen2-VL-2B,\nMed-R1 achieves a 29.94% accuracy improvement and outperforms Qwen2-VL-72B,\nwhich has 36 times more parameters. Testing across five question types-modality\nrecognition, anatomy identification, disease diagnosis, lesion grading, and\nbiological attribute analysis Med-R1 demonstrates superior generalization,\nexceeding Qwen2-VL-2B by 32.06% and surpassing Qwen2-VL-72B in question-type\ngeneralization. These findings show that RL improves medical reasoning and\nenables parameter-efficient models to outperform significantly larger ones.\nWith interpretable reasoning outputs, Med-R1 represents a promising step toward\ngeneralizable, trustworthy, and clinically viable medical VLMs."}
{"id": "2503.13940", "pdf": "https://arxiv.org/pdf/2503.13940", "abs": "https://arxiv.org/abs/2503.13940", "authors": ["Hang Zhao", "Hongru Li", "Dongfang Xu", "Shenghui Song", "Khaled B. Letaief"], "title": "Multi-Modal Self-Supervised Semantic Communication", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Semantic communication is emerging as a promising paradigm that focuses on\nthe extraction and transmission of semantic meanings using deep learning\ntechniques. While current research primarily addresses the reduction of\nsemantic communication overhead, it often overlooks the training phase, which\ncan incur significant communication costs in dynamic wireless environments. To\naddress this challenge, we propose a multi-modal semantic communication system\nthat leverages multi-modal self-supervised learning to enhance task-agnostic\nfeature extraction. The proposed approach employs self-supervised learning\nduring the pre-training phase to extract task-agnostic semantic features,\nfollowed by supervised fine-tuning for downstream tasks. This dual-phase\nstrategy effectively captures both modality-invariant and modality-specific\nfeatures while minimizing training-related communication overhead. Experimental\nresults on the NYU Depth V2 dataset demonstrate that the proposed method\nsignificantly reduces training-related communication overhead while maintaining\nor exceeding the performance of existing supervised learning approaches. The\nfindings underscore the advantages of multi-modal self-supervised learning in\nsemantic communication, paving the way for more efficient and scalable edge\ninference systems."}
{"id": "2503.13945", "pdf": "https://arxiv.org/pdf/2503.13945", "abs": "https://arxiv.org/abs/2503.13945", "authors": ["Long Tang", "Dengpan Ye", "Sirun Chen", "Xiuwen Shi", "Yunna Lv", "Ziyi Liu"], "title": "Make the Most of Everything: Further Considerations on Disrupting Diffusion-based Customization", "categories": ["cs.CV"], "comment": null, "summary": "The fine-tuning technique for text-to-image diffusion models facilitates\nimage customization but risks privacy breaches and opinion manipulation.\nCurrent research focuses on prompt- or image-level adversarial attacks for\nanti-customization, yet it overlooks the correlation between these two levels\nand the relationship between internal modules and inputs. This hinders\nanti-customization performance in practical threat scenarios. We propose Dual\nAnti-Diffusion (DADiff), a two-stage adversarial attack targeting diffusion\ncustomization, which, for the first time, integrates the adversarial\nprompt-level attack into the generation process of image-level adversarial\nexamples. In stage 1, we generate prompt-level adversarial vectors to guide the\nsubsequent image-level attack. In stage 2, besides conducting the end-to-end\nattack on the UNet model, we disrupt its self- and cross-attention modules,\naiming to break the correlations between image pixels and align the\ncross-attention results computed using instance prompts and adversarial prompt\nvectors within the images. Furthermore, we introduce a local random timestep\ngradient ensemble strategy, which updates adversarial perturbations by\nintegrating random gradients from multiple segmented timesets. Experimental\nresults on various mainstream facial datasets demonstrate 10%-30% improvements\nin cross-prompt, keyword mismatch, cross-model, and cross-mechanism\nanti-customization with DADiff compared to existing methods."}
{"id": "2503.13946", "pdf": "https://arxiv.org/pdf/2503.13946", "abs": "https://arxiv.org/abs/2503.13946", "authors": ["Kang Yang", "Tianci Bu", "Lantao Li", "Chunxu Li", "Yongcai Wang", "Deying Li"], "title": "Is Discretization Fusion All You Need for Collaborative Perception?", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception in multi-agent system enhances overall perceptual\ncapabilities by facilitating the exchange of complementary information among\nagents. Current mainstream collaborative perception methods rely on discretized\nfeature maps to conduct fusion, which however, lacks flexibility in extracting\nand transmitting the informative features and can hardly focus on the\ninformative features during fusion. To address these problems, this paper\nproposes a novel Anchor-Centric paradigm for Collaborative Object detection\n(ACCO). It avoids grid precision issues and allows more flexible and efficient\nanchor-centric communication and fusion. ACCO is composed by three main\ncomponents: (1) Anchor featuring block (AFB) that targets to generate anchor\nproposals and projects prepared anchor queries to image features. (2) Anchor\nconfidence generator (ACG) is designed to minimize communication by selecting\nonly the features in the confident anchors to transmit. (3) A local-global\nfusion module, in which local fusion is anchor alignment-based fusion (LAAF)\nand global fusion is conducted by spatial-aware cross-attention (SACA). LAAF\nand SACA run in multi-layers, so agents conduct anchor-centric fusion\niteratively to adjust the anchor proposals. Comprehensive experiments are\nconducted to evaluate ACCO on OPV2V and Dair-V2X datasets, which demonstrate\nACCO's superiority in reducing the communication volume, and in improving the\nperception range and detection performances. Code can be found at:\n\\href{https://github.com/sidiangongyuan/ACCO}{https://github.com/sidiangongyuan/ACCO}."}
{"id": "2503.13947", "pdf": "https://arxiv.org/pdf/2503.13947", "abs": "https://arxiv.org/abs/2503.13947", "authors": ["Sayak Nag", "Udita Ghosh", "Sarosij Bose", "Calvin-Khang Ta", "Jiachen Li", "Amit K Roy Chowdhury"], "title": "Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Scene Graph Generation (SGG) aims to represent visual scenes by identifying\nobjects and their pairwise relationships, providing a structured understanding\nof image content. However, inherent challenges like long-tailed class\ndistributions and prediction variability necessitate uncertainty quantification\nin SGG for its practical viability. In this paper, we introduce a novel\nConformal Prediction (CP) based framework, adaptive to any existing SGG method,\nfor quantifying their predictive uncertainty by constructing well-calibrated\nprediction sets over their generated scene graphs. These scene graph prediction\nsets are designed to achieve statistically rigorous coverage guarantees.\nAdditionally, to ensure these prediction sets contain the most practically\ninterpretable scene graphs, we design an effective MLLM-based post-processing\nstrategy for selecting the most visually and semantically plausible scene\ngraphs within these prediction sets. We show that our proposed approach can\nproduce diverse possible scene graphs from an image, assess the reliability of\nSGG methods, and improve overall SGG performance."}
{"id": "2503.13948", "pdf": "https://arxiv.org/pdf/2503.13948", "abs": "https://arxiv.org/abs/2503.13948", "authors": ["Mufan Liu", "Qi Yang", "He Huang", "Wenjie Huang", "Zhenlong Yuan", "Zhu Li", "Yiling Xu"], "title": "Light4GS: Lightweight Compact 4D Gaussian Splatting Generation via Context Model", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as an efficient and high-fidelity\nparadigm for novel view synthesis. To adapt 3DGS for dynamic content,\ndeformable 3DGS incorporates temporally deformable primitives with learnable\nlatent embeddings to capture complex motions. Despite its impressive\nperformance, the high-dimensional embeddings and vast number of primitives lead\nto substantial storage requirements. In this paper, we introduce a\n\\textbf{Light}weight \\textbf{4}D\\textbf{GS} framework, called Light4GS, that\nemploys significance pruning with a deep context model to provide a lightweight\nstorage-efficient dynamic 3DGS representation. The proposed Light4GS is based\non 4DGS that is a typical representation of deformable 3DGS. Specifically, our\nframework is built upon two core components: (1) a spatio-temporal significance\npruning strategy that eliminates over 64\\% of the deformable primitives,\nfollowed by an entropy-constrained spherical harmonics compression applied to\nthe remainder; and (2) a deep context model that integrates intra- and\ninter-prediction with hyperprior into a coarse-to-fine context structure to\nenable efficient multiscale latent embedding compression. Our approach achieves\nover 120x compression and increases rendering FPS up to 20\\% compared to the\nbaseline 4DGS, and also superior to frame-wise state-of-the-art 3DGS\ncompression methods, revealing the effectiveness of our Light4GS in terms of\nboth intra- and inter-prediction methods without sacrificing rendering quality."}
{"id": "2503.13951", "pdf": "https://arxiv.org/pdf/2503.13951", "abs": "https://arxiv.org/abs/2503.13951", "authors": ["Lili Yang", "Mengshuai Chang", "Xiao Guo", "Yuxin Feng", "Yiwen Mei", "Caicong Wu"], "title": "FrustumFusionNets: A Three-Dimensional Object Detection Network Based on Tractor Road Scene", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "To address the issues of the existing frustum-based methods' underutilization\nof image information in road three-dimensional object detection as well as the\nlack of research on agricultural scenes, we constructed an object detection\ndataset using an 80-line Light Detection And Ranging (LiDAR) and a camera in a\ncomplex tractor road scene and proposed a new network called FrustumFusionNets\n(FFNets). Initially, we utilize the results of image-based two-dimensional\nobject detection to narrow down the search region in the three-dimensional\nspace of the point cloud. Next, we introduce a Gaussian mask to enhance the\npoint cloud information. Then, we extract the features from the frustum point\ncloud and the crop image using the point cloud feature extraction pipeline and\nthe image feature extraction pipeline, respectively. Finally, we concatenate\nand fuse the data features from both modalities to achieve three-dimensional\nobject detection. Experiments demonstrate that on the constructed test set of\ntractor road data, the FrustumFusionNetv2 achieves 82.28% and 95.68% accuracy\nin the three-dimensional object detection of the two main road objects, cars\nand people, respectively. This performance is 1.83% and 2.33% better than the\noriginal model. It offers a hybrid fusion-based multi-object, high-precision,\nreal-time three-dimensional object detection technique for unmanned\nagricultural machines in tractor road scenarios. On the Karlsruhe Institute of\nTechnology and Toyota Technological Institute (KITTI) Benchmark Suite\nvalidation set, the FrustumFusionNetv2 also demonstrates significant\nsuperiority in detecting road pedestrian objects compared with other\nfrustum-based three-dimensional object detection methods."}
{"id": "2503.13952", "pdf": "https://arxiv.org/pdf/2503.13952", "abs": "https://arxiv.org/abs/2503.13952", "authors": ["Xinqing Li", "Ruiqi Song", "Qingyu Xie", "Ye Wu", "Nanxin Zeng", "Yunfeng Ai"], "title": "SimWorld: A Unified Benchmark for Simulator-Conditioned Scene Generation via World Model", "categories": ["cs.CV", "I.4.8; I.2.10"], "comment": "8 pages, 4 figures", "summary": "With the rapid advancement of autonomous driving technology, a lack of data\nhas become a major obstacle to enhancing perception model accuracy. Researchers\nare now exploring controllable data generation using world models to diversify\ndatasets. However, previous work has been limited to studying image generation\nquality on specific public datasets. There is still relatively little research\non how to build data generation engines for real-world application scenes to\nachieve large-scale data generation for challenging scenes. In this paper, a\nsimulator-conditioned scene generation engine based on world model is proposed.\nBy constructing a simulation system consistent with real-world scenes,\nsimulation data and labels, which serve as the conditions for data generation\nin the world model, for any scenes can be collected. It is a novel data\ngeneration pipeline by combining the powerful scene simulation capabilities of\nthe simulation engine with the robust data generation capabilities of the world\nmodel. In addition, a benchmark with proportionally constructed virtual and\nreal data, is provided for exploring the capabilities of world models in\nreal-world scenes. Quantitative results show that these generated images\nsignificantly improve downstream perception models performance. Finally, we\nexplored the generative performance of the world model in urban autonomous\ndriving scenarios. All the data and code will be available at\nhttps://github.com/Li-Zn-H/SimWorld."}
{"id": "2503.13956", "pdf": "https://arxiv.org/pdf/2503.13956", "abs": "https://arxiv.org/abs/2503.13956", "authors": ["Yixuan Li", "Changli Tang", "Jimin Zhuang", "Yudong Yang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "Improving LLM Video Understanding with 16 Frames Per Second", "categories": ["cs.CV"], "comment": null, "summary": "Human vision is dynamic and continuous. However, in video understanding with\nmultimodal large language models (LLMs), existing methods primarily rely on\nstatic features extracted from images sampled at a fixed low frame rate of\nframe-per-second (FPS) $\\leqslant$2, leading to critical visual information\nloss. In this paper, we introduce F-16, the first multimodal LLM designed for\nhigh-frame-rate video understanding. By increasing the frame rate to 16 FPS and\ncompressing visual tokens within each 1-second clip, F-16 efficiently captures\ndynamic visual features while preserving key semantic information. Experimental\nresults demonstrate that higher frame rates considerably enhance video\nunderstanding across multiple benchmarks, providing a new approach to improving\nvideo LLMs beyond scaling model size or training data. F-16 achieves\nstate-of-the-art performance among 7-billion-parameter video LLMs on both\ngeneral and fine-grained video understanding benchmarks, such as Video-MME and\nTemporalBench. Furthermore, F-16 excels in complex spatiotemporal tasks,\nincluding high-speed sports analysis (\\textit{e.g.}, basketball, football,\ngymnastics, and diving), outperforming SOTA proprietary visual models like\nGPT-4o and Gemini-1.5-pro. Additionally, we introduce a novel decoding method\nfor F-16 that enables highly efficient low-frame-rate inference without\nrequiring model retraining. Upon acceptance, we will release the source code,\nmodel checkpoints, and data."}
{"id": "2503.13957", "pdf": "https://arxiv.org/pdf/2503.13957", "abs": "https://arxiv.org/abs/2503.13957", "authors": ["Mu Chen", "Liulei Li", "Wenguan Wang", "Yi Yang"], "title": "DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation", "categories": ["cs.CV"], "comment": "CVPR 2025, Code: https://github.com/kagawa588/DiffVsgg", "summary": "Top-leading solutions for Video Scene Graph Generation (VSGG) typically adopt\nan offline pipeline. Though demonstrating promising performance, they remain\nunable to handle real-time video streams and consume large GPU memory.\nMoreover, these approaches fall short in temporal reasoning, merely aggregating\nframe-level predictions over a temporal context. In response, we introduce\nDIFFVSGG, an online VSGG solution that frames this task as an iterative scene\ngraph update problem. Drawing inspiration from Latent Diffusion Models (LDMs)\nwhich generate images via denoising a latent feature embedding, we unify the\ndecoding of object classification, bounding box regression, and graph\ngeneration three tasks using one shared feature embedding. Then, given an\nembedding containing unified features of object pairs, we conduct a step-wise\nDenoising on it within LDMs, so as to deliver a clean embedding which clearly\nindicates the relationships between objects. This embedding then serves as the\ninput to task-specific heads for object classification, scene graph generation,\netc. DIFFVSGG further facilitates continuous temporal reasoning, where\npredictions for subsequent frames leverage results of past frames as the\nconditional inputs of LDMs, to guide the reverse diffusion process for current\nframes. Extensive experiments on three setups of Action Genome demonstrate the\nsuperiority of DIFFVSGG."}
{"id": "2503.13962", "pdf": "https://arxiv.org/pdf/2503.13962", "abs": "https://arxiv.org/abs/2503.13962", "authors": ["Chengze Jiang", "Zhuangzhuang Wang", "Minjing Dong", "Jie Gui"], "title": "Survey of Adversarial Robustness in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\nperformance in artificial intelligence by facilitating integrated understanding\nacross diverse modalities, including text, images, video, audio, and speech.\nHowever, their deployment in real-world applications raises significant\nconcerns about adversarial vulnerabilities that could compromise their safety\nand reliability. Unlike unimodal models, MLLMs face unique challenges due to\nthe interdependencies among modalities, making them susceptible to\nmodality-specific threats and cross-modal adversarial manipulations. This paper\nreviews the adversarial robustness of MLLMs, covering different modalities. We\nbegin with an overview of MLLMs and a taxonomy of adversarial attacks tailored\nto each modality. Next, we review key datasets and evaluation metrics used to\nassess the robustness of MLLMs. After that, we provide an in-depth review of\nattacks targeting MLLMs across different modalities. Our survey also identifies\ncritical challenges and suggests promising future research directions."}
{"id": "2503.13966", "pdf": "https://arxiv.org/pdf/2503.13966", "abs": "https://arxiv.org/abs/2503.13966", "authors": ["Siqi Zhang", "Yanyuan Qiao", "Qunbo Wang", "Longteng Guo", "Zhihua Wei", "Jing Liu"], "title": "FlexVLN: Flexible Adaptation for Diverse Vision-and-Language Navigation Tasks", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The aspiration of the Vision-and-Language Navigation (VLN) task has long been\nto develop an embodied agent with robust adaptability, capable of seamlessly\ntransferring its navigation capabilities across various tasks. Despite\nremarkable advancements in recent years, most methods necessitate\ndataset-specific training, thereby lacking the capability to generalize across\ndiverse datasets encompassing distinct types of instructions. Large language\nmodels (LLMs) have demonstrated exceptional reasoning and generalization\nabilities, exhibiting immense potential in robot action planning. In this\npaper, we propose FlexVLN, an innovative hierarchical approach to VLN that\nintegrates the fundamental navigation ability of a supervised-learning-based\nInstruction Follower with the robust generalization ability of the LLM Planner,\nenabling effective generalization across diverse VLN datasets. Moreover, a\nverification mechanism and a multi-model integration mechanism are proposed to\nmitigate potential hallucinations by the LLM Planner and enhance execution\naccuracy of the Instruction Follower. We take REVERIE, SOON, and CVDN-target as\nout-of-domain datasets for assessing generalization ability. The generalization\nperformance of FlexVLN surpasses that of all the previous methods to a large\nextent."}
{"id": "2503.13969", "pdf": "https://arxiv.org/pdf/2503.13969", "abs": "https://arxiv.org/abs/2503.13969", "authors": ["HaoBin Qin", "Jiale Fang", "Keisuke Fujii"], "title": "SoccerSynth Field: enhancing field detection with synthetic data from virtual soccer simulator", "categories": ["cs.CV"], "comment": null, "summary": "Field detection in team sports is an essential task in sports video analysis.\nHowever, collecting large-scale and diverse real-world datasets for training\ndetection models is often cost and time-consuming. Synthetic datasets, which\nallow controlled variability in lighting, textures, and camera angles, will be\na promising alternative for addressing these problems. This study addresses the\nchallenges of high costs and difficulties in collecting real-world datasets by\ninvestigating the effectiveness of pretraining models using synthetic datasets.\nIn this paper, we propose the effectiveness of using a synthetic dataset\n(SoccerSynth-Field) for soccer field detection. A synthetic soccer field\ndataset was created to pretrain models, and the performance of these models was\ncompared with models trained on real-world datasets. The results demonstrate\nthat models pretrained on the synthetic dataset exhibit superior performance in\ndetecting soccer fields. This highlights the effectiveness of synthetic data in\nenhancing model robustness and accuracy, offering a cost-effective and scalable\nsolution for advancing detection tasks in sports field detection."}
{"id": "2503.13982", "pdf": "https://arxiv.org/pdf/2503.13982", "abs": "https://arxiv.org/abs/2503.13982", "authors": ["Huy-Hoang Bui", "Bach-Thuan Bui", "Quang-Vinh Tran", "Yasuyuki Fujii", "Joo-Ho Lee"], "title": "A-SCoRe: Attention-based Scene Coordinate Regression for wide-ranging scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Visual localization is considered to be one of the crucial parts in many\nrobotic and vision systems. While state-of-the art methods that relies on\nfeature matching have proven to be accurate for visual localization, its\nrequirements for storage and compute are burdens. Scene coordinate regression\n(SCR) is an alternative approach that remove the barrier for storage by\nlearning to map 2D pixels to 3D scene coordinates. Most popular SCR use\nConvolutional Neural Network (CNN) to extract 2D descriptor, which we would\nargue that it miss the spatial relationship between pixels. Inspired by the\nsuccess of vision transformer architecture, we present a new SCR architecture,\ncalled A-ScoRe, an Attention-based model which leverage attention on descriptor\nmap level to produce meaningful and high-semantic 2D descriptors. Since the\noperation is performed on descriptor map, our model can work with multiple data\nmodality whether it is a dense or sparse from depth-map, SLAM to\nStructure-from-Motion (SfM). This versatility allows A-SCoRe to operate in\ndifferent kind of environments, conditions and achieve the level of flexibility\nthat is important for mobile robots. Results show our methods achieve\ncomparable performance with State-of-the-art methods on multiple benchmark\nwhile being light-weighted and much more flexible. Code and pre-trained models\nare public in our repository: https://github.com/ais-lab/A-SCoRe."}
{"id": "2503.13983", "pdf": "https://arxiv.org/pdf/2503.13983", "abs": "https://arxiv.org/abs/2503.13983", "authors": ["Jiankang Wang", "Zhihan zhang", "Zhihang Liu", "Yang Li", "Jiannan Ge", "Hongtao Xie", "Yongdong Zhang"], "title": "SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal Video Grounding Capability", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have made remarkable progress in\neither temporal or spatial localization. However, they struggle to perform\nspatio-temporal video grounding. This limitation stems from two major\nchallenges. Firstly, it is difficult to extract accurate spatio-temporal\ninformation of each frame in the video. Secondly, the substantial number of\nvisual tokens makes it challenging to precisely map visual tokens of each frame\nto their corresponding spatial coordinates. To address these issues, we\nintroduce SpaceVLLM, a MLLM endowed with spatio-temporal video grounding\ncapability. Specifically, we adopt a set of interleaved Spatio-Temporal Aware\nQueries to capture temporal perception and dynamic spatial information.\nMoreover, we propose a Query-Guided Space Decoder to establish a corresponding\nconnection between the queries and spatial coordinates. Additionally, due to\nthe lack of spatio-temporal datasets, we construct the Unified Spatio-Temporal\nGrounding (Uni-STG) dataset, comprising 480K instances across three tasks. This\ndataset fully exploits the potential of MLLM to simultaneously facilitate\nlocalization in both temporal and spatial dimensions. Extensive experiments\ndemonstrate that SpaceVLLM achieves the state-of-the-art performance across 11\nbenchmarks covering temporal, spatial, spatio-temporal and video understanding\ntasks, highlighting the effectiveness of our approach. Our code, datasets and\nmodel will be released."}
{"id": "2503.13985", "pdf": "https://arxiv.org/pdf/2503.13985", "abs": "https://arxiv.org/abs/2503.13985", "authors": ["Jaewoo Song", "Daemin Park", "Kanghyun Baek", "Sangyub Lee", "Jooyoung Choi", "Eunji Kim", "Sungroh Yoon"], "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Developing effective visual inspection models remains challenging due to the\nscarcity of defect data. While image generation models have been used to\nsynthesize defect images, producing highly realistic defects remains difficult.\nWe propose DefectFill, a novel method for realistic defect generation that\nrequires only a few reference defect images. It leverages a fine-tuned\ninpainting diffusion model, optimized with our custom loss functions\nincorporating defect, object, and attention terms. It enables precise capture\nof detailed, localized defect features and their seamless integration into\ndefect-free objects. Additionally, our Low-Fidelity Selection method further\nenhances the defect sample quality. Experiments show that DefectFill generates\nhigh-quality defect images, enabling visual inspection models to achieve\nstate-of-the-art performance on the MVTec AD dataset."}
{"id": "2503.13989", "pdf": "https://arxiv.org/pdf/2503.13989", "abs": "https://arxiv.org/abs/2503.13989", "authors": ["Zixuan Zheng", "Yilei Shi", "Chunlei Li", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Rethinking Cell Counting Methods: Decoupling Counting and Localization", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Cell counting in microscopy images is vital in medicine and biology but\nextremely tedious and time-consuming to perform manually. While automated\nmethods have advanced in recent years, state-of-the-art approaches tend to\nincreasingly complex model designs. In this paper, we propose a conceptually\nsimple yet effective decoupled learning scheme for automated cell counting,\nconsisting of separate counter and localizer networks. In contrast to jointly\nlearning counting and density map estimation, we show that decoupling these\nobjectives surprisingly improves results. The counter operates on intermediate\nfeature maps rather than pixel space to leverage global context and produce\ncount estimates, while also generating coarse density maps. The localizer then\nreconstructs high-resolution density maps that precisely localize individual\ncells, conditional on the original images and coarse density maps from the\ncounter. Besides, to boost counting accuracy, we further introduce a global\nmessage passing module to integrate cross-region patterns. Extensive\nexperiments on four datasets demonstrate that our approach, despite its\nsimplicity, challenges common practice and achieves state-of-the-art\nperformance by significant margins. Our key insight is that decoupled learning\nalleviates the need to learn counting on high-resolution density maps directly,\nallowing the model to focus on global features critical for accurate estimates.\nCode is available at https://github.com/MedAITech/DCL."}
{"id": "2503.13991", "pdf": "https://arxiv.org/pdf/2503.13991", "abs": "https://arxiv.org/abs/2503.13991", "authors": ["Bo Peng", "Jintao Chen", "Mufeng Yao", "Chenhao Zhang", "Jianghui Zhang", "Mingmin Chi", "Jiang Tao"], "title": "GraphTEN: Graph Enhanced Texture Encoding Network", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10; I.4.7"], "comment": "6 pages, 7 figures, conference paper", "summary": "Texture recognition is a fundamental problem in computer vision and pattern\nrecognition. Recent progress leverages feature aggregation into discriminative\ndescriptions based on convolutional neural networks (CNNs). However, modeling\nnon-local context relations through visual primitives remains challenging due\nto the variability and randomness of texture primitives in spatial\ndistributions. In this paper, we propose a graph-enhanced texture encoding\nnetwork (GraphTEN) designed to capture both local and global features of\ntexture primitives. GraphTEN models global associations through fully connected\ngraphs and captures cross-scale dependencies of texture primitives via\nbipartite graphs. Additionally, we introduce a patch encoding module that\nutilizes a codebook to achieve an orderless representation of texture by\nencoding multi-scale patch features into a unified feature space. The proposed\nGraphTEN achieves superior performance compared to state-of-the-art methods\nacross five publicly available datasets."}
{"id": "2503.13999", "pdf": "https://arxiv.org/pdf/2503.13999", "abs": "https://arxiv.org/abs/2503.13999", "authors": ["Mohaddeseh Chegini", "Ali Mahloojifar"], "title": "BI-RADS prediction of mammographic masses using uncertainty information extracted from a Bayesian Deep Learning model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The BI_RADS score is a probabilistic reporting tool used by radiologists to\nexpress the level of uncertainty in predicting breast cancer based on some\nmorphological features in mammography images. There is a significant\nvariability in describing masses which sometimes leads to BI_RADS\nmisclassification. Using a BI_RADS prediction system is required to support the\nfinal radiologist decisions. In this study, the uncertainty information\nextracted by a Bayesian deep learning model is utilized to predict the BI_RADS\nscore. The investigation results based on the pathology information demonstrate\nthat the f1-scores of the predictions of the radiologist are 42.86%, 48.33% and\n48.28%, meanwhile, the f1-scores of the model performance are 73.33%, 59.60%\nand 59.26% in the BI_RADS 2, 3 and 5 dataset samples, respectively. Also, the\nmodel can distinguish malignant from benign samples in the BI_RADS 0 category\nof the used dataset with an accuracy of 75.86% and correctly identify all\nmalignant samples as BI_RADS 5. The Grad-CAM visualization shows the model pays\nattention to the morphological features of the lesions. Therefore, this study\nshows the uncertainty-aware Bayesian Deep Learning model can report his\nuncertainty about the malignancy of a lesion based on morphological features,\nlike a radiologist."}
{"id": "2503.14001", "pdf": "https://arxiv.org/pdf/2503.14001", "abs": "https://arxiv.org/abs/2503.14001", "authors": ["Yi Xiao", "Qiannan Han", "Guiping Liang", "Hongyan Zhang", "Song Wang", "Zhihao Xu", "Weican Wan", "Chuang Li", "Guitao Jiang", "Wenbo Xiao"], "title": "Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body Dimensions and Weight", "categories": ["cs.CV"], "comment": null, "summary": "Accurate body dimension and weight measurements are critical for optimizing\npoultry management, health assessment, and economic efficiency. This study\nintroduces an innovative deep learning-based model leveraging multimodal\ndata-2D RGB images from different views, depth images, and 3D point clouds-for\nthe non-invasive estimation of duck body dimensions and weight. A dataset of\n1,023 Linwu ducks, comprising over 5,000 samples with diverse postures and\nconditions, was collected to support model training. The proposed method\ninnovatively employs PointNet++ to extract key feature points from point\nclouds, extracts and computes corresponding 3D geometric features, and fuses\nthem with multi-view convolutional 2D features. A Transformer encoder is then\nutilized to capture long-range dependencies and refine feature interactions,\nthereby enhancing prediction robustness. The model achieved a mean absolute\npercentage error (MAPE) of 6.33% and an R2 of 0.953 across eight morphometric\nparameters, demonstrating strong predictive capability. Unlike conventional\nmanual measurements, the proposed model enables high-precision estimation while\neliminating the necessity for physical handling, thereby reducing animal stress\nand broadening its application scope. This study marks the first application of\ndeep learning techniques to poultry body dimension and weight estimation,\nproviding a valuable reference for the intelligent and precise management of\nthe livestock industry with far-reaching practical significance."}
{"id": "2503.14002", "pdf": "https://arxiv.org/pdf/2503.14002", "abs": "https://arxiv.org/abs/2503.14002", "authors": ["Damian Boborzi", "Phillip Mueller", "Jonas Emrich", "Dominik Schmid", "Sebastian Mueller", "Lars Mikelsons"], "title": "MeshFleet: Filtered and Annotated 3D Vehicle Dataset for Domain Specific Generative Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have recently made remarkable progress in the field of 3D\nobjects. However, their practical application in fields like engineering\nremains limited since they fail to deliver the accuracy, quality, and\ncontrollability needed for domain-specific tasks. Fine-tuning large generative\nmodels is a promising perspective for making these models available in these\nfields. Creating high-quality, domain-specific 3D datasets is crucial for\nfine-tuning large generative models, yet the data filtering and annotation\nprocess remains a significant bottleneck. We present MeshFleet, a filtered and\nannotated 3D vehicle dataset extracted from Objaverse-XL, the most extensive\npublicly available collection of 3D objects. Our approach proposes a pipeline\nfor automated data filtering based on a quality classifier. This classifier is\ntrained on a manually labeled subset of Objaverse, incorporating DINOv2 and\nSigLIP embeddings, refined through caption-based analysis and uncertainty\nestimation. We demonstrate the efficacy of our filtering method through a\ncomparative analysis against caption and image aesthetic score-based techniques\nand fine-tuning experiments with SV3D, highlighting the importance of targeted\ndata selection for domain-specific 3D generative modeling."}
{"id": "2503.14012", "pdf": "https://arxiv.org/pdf/2503.14012", "abs": "https://arxiv.org/abs/2503.14012", "authors": ["Wei Lu", "Si-Bao Chen", "Hui-Dong Li", "Qing-Ling Shu", "Chris H. Q. Ding", "Jin Tang", "Bin Luo"], "title": "LEGNet: Lightweight Edge-Gaussian Driven Network for Low-Quality Remote Sensing Image Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 5 figures. Remote Sensing Image Object Detection", "summary": "Remote sensing object detection (RSOD) faces formidable challenges in complex\nvisual environments. Aerial and satellite images inherently suffer from\nlimitations such as low spatial resolution, sensor noise, blurred objects,\nlow-light degradation, and partial occlusions. These degradation factors\ncollectively compromise the feature discriminability in detection models,\nresulting in three key issues: (1) reduced contrast that hampers\nforeground-background separation, (2) structural discontinuities in edge\nrepresentations, and (3) ambiguous feature responses caused by variations in\nillumination. These collectively weaken model robustness and deployment\nfeasibility. To address these challenges, we propose LEGNet, a lightweight\nnetwork that incorporates a novel edge-Gaussian aggregation (EGA) module\nspecifically designed for low-quality remote sensing images. Our key innovation\nlies in the synergistic integration of Scharr operator-based edge priors with\nuncertainty-aware Gaussian modeling: (a) The orientation-aware Scharr filters\npreserve high-frequency edge details with rotational invariance; (b) The\nuncertainty-aware Gaussian layers probabilistically refine low-confidence\nfeatures through variance estimation. This design enables precision enhancement\nwhile maintaining architectural simplicity. Comprehensive evaluations across\nfour RSOD benchmarks (DOTA-v1.0, v1.5, DIOR-R, FAIR1M-v1.0) and a UAV-view\ndataset (VisDrone2019) demonstrate significant improvements. LEGNet achieves\nstate-of-the-art performance across five benchmark datasets while ensuring\ncomputational efficiency, making it well-suited for deployment on\nresource-constrained edge devices in real-world remote sensing applications.\nThe code is available at https://github.com/lwCVer/LEGNet."}
{"id": "2503.14013", "pdf": "https://arxiv.org/pdf/2503.14013", "abs": "https://arxiv.org/abs/2503.14013", "authors": ["Pengcheng Zhou", "Lantian Zhang", "Wei Li"], "title": "Boosting Semi-Supervised Medical Image Segmentation via Masked Image Consistency and Discrepancy Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semi-supervised learning is of great significance in medical image\nsegmentation by exploiting unlabeled data. Among its strategies, the\nco-training framework is prominent. However, previous co-training studies\npredominantly concentrate on network initialization variances and pseudo-label\ngeneration, while overlooking the equilibrium between information interchange\nand model diversity preservation. In this paper, we propose the Masked Image\nConsistency and Discrepancy Learning (MICD) framework with three key modules.\nThe Masked Cross Pseudo Consistency (MCPC) module enriches context perception\nand small sample learning via pseudo-labeling across masked-input branches. The\nCross Feature Consistency (CFC) module fortifies information exchange and model\nrobustness by ensuring decoder feature consistency. The Cross Model Discrepancy\n(CMD) module utilizes EMA teacher networks to oversee outputs and preserve\nbranch diversity. Together, these modules address existing limitations by\nfocusing on fine-grained local information and maintaining diversity in a\nheterogeneous framework. Experiments on two public medical image datasets, AMOS\nand Synapse, demonstrate that our approach outperforms state-of-the-art\nmethods."}
{"id": "2503.14021", "pdf": "https://arxiv.org/pdf/2503.14021", "abs": "https://arxiv.org/abs/2503.14021", "authors": ["Ziwei Wang", "Weizhi Chen", "Leyang Yang", "Sheng Zhou", "Shengchu Zhao", "Hanbei Zhan", "Jiongchao Jin", "Liangcheng Li", "Zirui Shao", "Jiajun Bu"], "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "Paper accepted to CVPR 2025", "summary": "Graphical user interface (GUI) has become integral to modern society, making\nit crucial to be understood for human-centric systems. However, unlike natural\nimages or documents, GUIs comprise artificially designed graphical elements\narranged to convey specific semantic meanings. Current multi-modal large\nlanguage models (MLLMs) already proficient in processing graphical and textual\ncomponents suffer from hurdles in GUI understanding due to the lack of explicit\nspatial structure modeling. Moreover, obtaining high-quality spatial structure\ndata is challenging due to privacy issues and noisy environments. To address\nthese challenges, we present MP-GUI, a specially designed MLLM for GUI\nunderstanding. MP-GUI features three precisely specialized perceivers to\nextract graphical, textual, and spatial modalities from the screen as\nGUI-tailored visual clues, with spatial structure refinement strategy and\nadaptively combined via a fusion gate to meet the specific preferences of\ndifferent GUI understanding tasks. To cope with the scarcity of training data,\nwe also introduce a pipeline for automatically data collecting. Extensive\nexperiments demonstrate that MP-GUI achieves impressive results on various GUI\nunderstanding tasks with limited data."}
{"id": "2503.14029", "pdf": "https://arxiv.org/pdf/2503.14029", "abs": "https://arxiv.org/abs/2503.14029", "authors": ["Runsong Zhu", "Shi Qiu", "Zhengzhe Liu", "Ka-Hei Hui", "Qianyi Wu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2025. The code is publicly available at this https URL\n  (https://github.com/Runsong123/Unified-Lift)", "summary": "Lifting multi-view 2D instance segmentation to a radiance field has proven to\nbe effective to enhance 3D understanding. Existing methods rely on direct\nmatching for end-to-end lifting, yielding inferior results; or employ a\ntwo-stage solution constrained by complex pre- or post-processing. In this\nwork, we design a new end-to-end object-aware lifting approach, named\nUnified-Lift that provides accurate 3D segmentation based on the 3D Gaussian\nrepresentation. To start, we augment each Gaussian point with an additional\nGaussian-level feature learned using a contrastive loss to encode instance\ninformation. Importantly, we introduce a learnable object-level codebook to\naccount for individual objects in the scene for an explicit object-level\nunderstanding and associate the encoded object-level features with the\nGaussian-level point features for segmentation predictions. While promising,\nachieving effective codebook learning is non-trivial and a naive solution leads\nto degraded performance. Therefore, we formulate the association learning\nmodule and the noisy label filtering module for effective and robust codebook\nlearning. We conduct experiments on three benchmarks: LERF-Masked, Replica, and\nMessy Rooms datasets. Both qualitative and quantitative results manifest that\nour Unified-Lift clearly outperforms existing methods in terms of segmentation\nquality and time efficiency. The code is publicly available at\n\\href{https://github.com/Runsong123/Unified-Lift}{https://github.com/Runsong123/Unified-Lift}."}
{"id": "2503.14035", "pdf": "https://arxiv.org/pdf/2503.14035", "abs": "https://arxiv.org/abs/2503.14035", "authors": ["Seung Woo Ko", "Joopyo Hong", "Suyoung Kim", "Seungjai Bang", "Sungzoon Cho", "Nojun Kwak", "Hyung-Sin Kim", "Joonseok Lee"], "title": "A Revisit to the Decoder for Camouflaged Object Detection", "categories": ["cs.CV"], "comment": "Published in BMVC 2024, 13 pages, 7 figures (Appendix: 5 pages, 2\n  figures)", "summary": "Camouflaged object detection (COD) aims to generate a fine-grained\nsegmentation map of camouflaged objects hidden in their background. Due to the\nhidden nature of camouflaged objects, it is essential for the decoder to be\ntailored to effectively extract proper features of camouflaged objects and\nextra-carefully generate their complex boundaries. In this paper, we propose a\nnovel architecture that augments the prevalent decoding strategy in COD with\nEnrich Decoder and Retouch Decoder, which help to generate a fine-grained\nsegmentation map. Specifically, the Enrich Decoder amplifies the channels of\nfeatures that are important for COD using channel-wise attention. Retouch\nDecoder further refines the segmentation maps by spatially attending to\nimportant pixels, such as the boundary regions. With extensive experiments, we\ndemonstrate that ENTO shows superior performance using various encoders, with\nthe two novel components playing their unique roles that are mutually\ncomplementary."}
{"id": "2503.14037", "pdf": "https://arxiv.org/pdf/2503.14037", "abs": "https://arxiv.org/abs/2503.14037", "authors": ["Cong Wang", "Jinshan Pan", "Liyan Wang", "Wei Wang"], "title": "Intra and Inter Parser-Prompted Transformers for Effective Image Restoration", "categories": ["cs.CV"], "comment": "This version is accepted by the Association for the Advancement of\n  Artificial Intelligence (AAAI-25)", "summary": "We propose Intra and Inter Parser-Prompted Transformers (PPTformer) that\nexplore useful features from visual foundation models for image restoration.\nSpecifically, PPTformer contains two parts: an Image Restoration Network\n(IRNet) for restoring images from degraded observations and a Parser-Prompted\nFeature Generation Network (PPFGNet) for providing IRNet with reliable parser\ninformation to boost restoration. To enhance the integration of the parser\nwithin IRNet, we propose Intra Parser-Prompted Attention (IntraPPA) and Inter\nParser-Prompted Attention (InterPPA) to implicitly and explicitly learn useful\nparser features to facilitate restoration. The IntraPPA re-considers cross\nattention between parser and restoration features, enabling implicit perception\nof the parser from a long-range and intra-layer perspective. Conversely, the\nInterPPA initially fuses restoration features with those of the parser,\nfollowed by formulating these fused features within an attention mechanism to\nexplicitly perceive parser information. Further, we propose a parser-prompted\nfeed-forward network to guide restoration within pixel-wise gating modulation.\nExperimental results show that PPTformer achieves state-of-the-art performance\non image deraining, defocus deblurring, desnowing, and low-light enhancement."}
{"id": "2503.14064", "pdf": "https://arxiv.org/pdf/2503.14064", "abs": "https://arxiv.org/abs/2503.14064", "authors": ["Xinhao Xiang", "Xiao Liu", "Zizhong Li", "Zhuosheng Liu", "Jiawei Zhang"], "title": "AIGVE-Tool: AI-Generated Video Evaluation Toolkit with Multifaceted Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement in AI-generated video synthesis has led to a growth\ndemand for standardized and effective evaluation metrics. Existing metrics lack\na unified framework for systematically categorizing methodologies, limiting a\nholistic understanding of the evaluation landscape. Additionally, fragmented\nimplementations and the absence of standardized interfaces lead to redundant\nprocessing overhead. Furthermore, many prior approaches are constrained by\ndataset-specific dependencies, limiting their applicability across diverse\nvideo domains. To address these challenges, we introduce AIGVE-Tool\n(AI-Generated Video Evaluation Toolkit), a unified framework that provides a\nstructured and extensible evaluation pipeline for a comprehensive AI-generated\nvideo evaluation. Organized within a novel five-category taxonomy, AIGVE-Tool\nintegrates multiple evaluation methodologies while allowing flexible\ncustomization through a modular configuration system. Additionally, we propose\nAIGVE-Bench, a large-scale benchmark dataset created with five SOTA video\ngeneration models based on hand-crafted instructions and prompts. This dataset\nsystematically evaluates various video generation models across nine critical\nquality dimensions. Extensive experiments demonstrate the effectiveness of\nAIGVE-Tool in providing standardized and reliable evaluation results,\nhighlighting specific strengths and limitations of current models and\nfacilitating the advancements of next-generation AI-generated video techniques."}
{"id": "2503.14070", "pdf": "https://arxiv.org/pdf/2503.14070", "abs": "https://arxiv.org/abs/2503.14070", "authors": ["Yang Ye", "Junliang Guo", "Haoyu Wu", "Tianyu He", "Tim Pearce", "Tabish Rashid", "Katja Hofmann", "Jiang Bian"], "title": "Fast Autoregressive Video Generation with Diagonal Decoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive Transformer models have demonstrated impressive performance in\nvideo generation, but their sequential token-by-token decoding process poses a\nmajor bottleneck, particularly for long videos represented by tens of thousands\nof tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free\ninference acceleration algorithm for autoregressively pre-trained models that\nexploits spatial and temporal correlations in videos. Our method generates\ntokens along diagonal paths in the spatial-temporal token grid, enabling\nparallel decoding within each frame as well as partially overlapping across\nconsecutive frames. The proposed algorithm is versatile and adaptive to various\ngenerative models and tasks, while providing flexible control over the\ntrade-off between inference speed and visual quality. Furthermore, we propose a\ncost-effective finetuning strategy that aligns the attention patterns of the\nmodel with our decoding order, further mitigating the training-inference gap on\nsmall-scale models. Experiments on multiple autoregressive video generation\nmodels and datasets demonstrate that DiagD achieves up to $10\\times$ speedup\ncompared to naive sequential decoding, while maintaining comparable visual\nfidelity."}
{"id": "2503.14074", "pdf": "https://arxiv.org/pdf/2503.14074", "abs": "https://arxiv.org/abs/2503.14074", "authors": ["Shengping Zhang", "Xiaoyu Han", "Weigang Zhang", "Xiangyuan Lan", "Hongxun Yao", "Qingming Huang"], "title": "Limb-Aware Virtual Try-On Network with Progressive Clothing Warping", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM). The code is\n  available at https://github.com/aipixel/PL-VTONv2", "summary": "Image-based virtual try-on aims to transfer an in-shop clothing image to a\nperson image. Most existing methods adopt a single global deformation to\nperform clothing warping directly, which lacks fine-grained modeling of in-shop\nclothing and leads to distorted clothing appearance. In addition, existing\nmethods usually fail to generate limb details well because they are limited by\nthe used clothing-agnostic person representation without referring to the limb\ntextures of the person image. To address these problems, we propose Limb-aware\nVirtual Try-on Network named PL-VTON, which performs fine-grained clothing\nwarping progressively and generates high-quality try-on results with realistic\nlimb details. Specifically, we present Progressive Clothing Warping (PCW) that\nexplicitly models the location and size of in-shop clothing and utilizes a\ntwo-stage alignment strategy to progressively align the in-shop clothing with\nthe human body. Moreover, a novel gravity-aware loss that considers the fit of\nthe person wearing clothing is adopted to better handle the clothing edges.\nThen, we design Person Parsing Estimator (PPE) with a non-limb target parsing\nmap to semantically divide the person into various regions, which provides\nstructural constraints on the human body and therefore alleviates texture\nbleeding between clothing and body regions. Finally, we introduce Limb-aware\nTexture Fusion (LTF) that focuses on generating realistic details in limb\nregions, where a coarse try-on result is first generated by fusing the warped\nclothing image with the person image, then limb textures are further fused with\nthe coarse result under limb-aware guidance to refine limb details. Extensive\nexperiments demonstrate that our PL-VTON outperforms the state-of-the-art\nmethods both qualitatively and quantitatively."}
{"id": "2503.14075", "pdf": "https://arxiv.org/pdf/2503.14075", "abs": "https://arxiv.org/abs/2503.14075", "authors": ["Zhenwei Shao", "Mingyang Wang", "Zhou Yu", "Wenwen Pan", "Yan Yang", "Tao Wei", "Hongyuan Zhang", "Ning Mao", "Wei Chen", "Jun Yu"], "title": "Growing a Twig to Accelerate Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "17 pages, 8 figures", "summary": "Large vision-language models (VLMs) have demonstrated remarkable capabilities\nin open-world multimodal understanding, yet their high computational overheads\npose great challenges for practical deployment. Some recent works have proposed\nmethods to accelerate VLMs by pruning redundant visual tokens guided by the\nattention maps of VLM's early layers. Despite the success of these token\npruning methods, they still suffer from two major shortcomings: (i)\nconsiderable accuracy drop due to insensitive attention signals in early\nlayers, and (ii) limited speedup when generating long responses (e.g., 30\ntokens). To address the limitations above, we present TwigVLM -- a simple and\ngeneral architecture by growing a lightweight twig upon an early layer of the\nbase VLM. Compared with most existing VLM acceleration methods purely based on\nvisual token pruning, our TwigVLM not only achieves better accuracy retention\nby employing a twig-guided token pruning (TTP) strategy, but also yields higher\ngeneration speed by utilizing a self-speculative decoding (SSD) strategy.\nTaking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM\npreserves 96% of the original performance after pruning 88.9% of visual tokens\nand achieves 154% speedup in generating long responses, delivering\nsignificantly better performance in terms of both accuracy and speed over the\nstate-of-the-art VLM acceleration methods. Code will be made publicly\navailable."}
{"id": "2503.14097", "pdf": "https://arxiv.org/pdf/2503.14097", "abs": "https://arxiv.org/abs/2503.14097", "authors": ["Weihong Chen", "Xuemiao Xu", "Haoxin Yang", "Yi Xie", "Peng Xiao", "Cheng Xu", "Huaidong Zhang", "Pheng-Ann Heng"], "title": "SCJD: Sparse Correlation and Joint Distillation for Efficient 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but\nsuffer from computational overhead and slow inference, while knowledge\ndistillation methods fail to address spatial relationships between joints and\ntemporal correlations in multi-frame inputs. In this paper, we propose Sparse\nCorrelation and Joint Distillation (SCJD), a novel framework that balances\nefficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input\nSequence Downsampling to reduce redundancy in student network inputs while\npreserving inter-frame correlations. For effective knowledge transfer, we\npropose Dynamic Joint Spatial Attention Distillation, which includes Dynamic\nJoint Embedding Distillation to enhance the student's feature representation\nusing the teacher's multi-frame context feature, and Adjacent Joint Attention\nDistillation to improve the student network's focus on adjacent joint\nrelationships for better spatial understanding. Additionally, Temporal\nConsistency Distillation aligns the temporal correlations between teacher and\nstudent networks through upsampling and global supervision. Extensive\nexperiments demonstrate that SCJD achieves state-of-the-art performance. Code\nis available at https://github.com/wileychan/SCJD."}
{"id": "2503.14106", "pdf": "https://arxiv.org/pdf/2503.14106", "abs": "https://arxiv.org/abs/2503.14106", "authors": ["Jef Jonkers", "Frank Coopman", "Luc Duchateau", "Glenn Van Wallendael", "Sofie Van Hoecke"], "title": "Reliable uncertainty quantification for 2D/3D anatomical landmark localization using multi-output conformal prediction", "categories": ["cs.CV", "cs.AI", "stat.ML"], "comment": "33 pages, 10 figures", "summary": "Automatic anatomical landmark localization in medical imaging requires not\njust accurate predictions but reliable uncertainty quantification for effective\nclinical decision support. Current uncertainty quantification approaches often\nfall short, particularly when combined with normality assumptions,\nsystematically underestimating total predictive uncertainty. This paper\nintroduces conformal prediction as a framework for reliable uncertainty\nquantification in anatomical landmark localization, addressing a critical gap\nin automatic landmark localization. We present two novel approaches\nguaranteeing finite-sample validity for multi-output prediction: Multi-output\nRegression-as-Classification Conformal Prediction (M-R2CCP) and its variant\nMulti-output Regression to Classification Conformal Prediction set to Region\n(M-R2C2R). Unlike conventional methods that produce axis-aligned\nhyperrectangular or ellipsoidal regions, our approaches generate flexible,\nnon-convex prediction regions that better capture the underlying uncertainty\nstructure of landmark predictions. Through extensive empirical evaluation\nacross multiple 2D and 3D datasets, we demonstrate that our methods\nconsistently outperform existing multi-output conformal prediction approaches\nin both validity and efficiency. This work represents a significant advancement\nin reliable uncertainty estimation for anatomical landmark localization,\nproviding clinicians with trustworthy confidence measures for their diagnoses.\nWhile developed for medical imaging, these methods show promise for broader\napplications in multi-output regression problems."}
{"id": "2503.14109", "pdf": "https://arxiv.org/pdf/2503.14109", "abs": "https://arxiv.org/abs/2503.14109", "authors": ["Nicolas Gonthier"], "title": "Operational Change Detection for Geographical Information: Overview and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint under review", "summary": "Rapid evolution of territories due to climate change and human impact\nrequires prompt and effective updates to geospatial databases maintained by the\nNational Mapping Agency. This paper presents a comprehensive overview of change\ndetection methods tailored for the operational updating of large-scale\ngeographic databases. This review first outlines the fundamental definition of\nchange, emphasizing its multifaceted nature, from temporal to semantic\ncharacterization. It categorizes automatic change detection methods into four\nmain families: rule-based, statistical, machine learning, and simulation\nmethods. The strengths, limitations, and applicability of every family are\ndiscussed in the context of various input data. Then, key applications for\nNational Mapping Agencies are identified, particularly the optimization of\ngeospatial database updating, change-based phenomena, and dynamics monitoring.\nFinally, the paper highlights the current challenges for leveraging change\ndetection such as the variability of change definition, the missing of relevant\nlarge-scale datasets, the diversity of input data, the unstudied no-change\ndetection, the human in the loop integration and the operational constraints.\nThe discussion underscores the necessity for ongoing innovation in change\ndetection techniques to address the future needs of geographic information\nsystems for national mapping agencies."}
{"id": "2503.14111", "pdf": "https://arxiv.org/pdf/2503.14111", "abs": "https://arxiv.org/abs/2503.14111", "authors": ["Egor Kuznetsov", "Kirill Aistov", "Maxim Koroteev"], "title": "Towards properties of adversarial image perturbations", "categories": ["cs.CV"], "comment": "13 pages, 40 figures", "summary": "Using stochastic gradient approach we study the properties of adversarial\nperturbations resulting in noticeable growth of VMAF image quality metric. The\nstructure of the perturbations is investigated depending on the acceptable PSNR\nvalues and based on the Fourier power spectrum computations for the\nperturbations. It is demonstrated that moderate variation of image brightness\n($\\sim 10$ pixel units in a restricted region of an image can result in VMAF\ngrowth by $\\sim 60\\%$). Unlike some other methods demonstrating similar VMAF\ngrowth, the subjective quality of an image remains almost unchanged. It is also\nshown that the adversarial perturbations may demonstrate approximately linear\ndependence of perturbation amplitudes on the image brightness. The\nperturbations are studied based on the direct VMAF optimization in PyTorch. The\nsignificant discrepancies between the metric values and subjective judgements\nare also demonstrated when image restoration from noise is carried out using\nthe same direct VMAF optimization."}
{"id": "2503.14112", "pdf": "https://arxiv.org/pdf/2503.14112", "abs": "https://arxiv.org/abs/2503.14112", "authors": ["Guodong Ding", "Rongyu Chen", "Angela Yao"], "title": "Condensing Action Segmentation Datasets via Generative Network Inversion", "categories": ["cs.CV"], "comment": "10 pages, 3 figures, 5 tables, Accepted to CVPR2025", "summary": "This work presents the first condensation approach for procedural video\ndatasets used in temporal action segmentation. We propose a condensation\nframework that leverages generative prior learned from the dataset and network\ninversion to condense data into compact latent codes with significant storage\nreduced across temporal and channel aspects. Orthogonally, we propose sampling\ndiverse and representative action sequences to minimize video-wise redundancy.\nOur evaluation on standard benchmarks demonstrates consistent effectiveness in\ncondensing TAS datasets and achieving competitive performances. Specifically,\non the Breakfast dataset, our approach reduces storage by over 500$\\times$\nwhile retaining 83% of the performance compared to training with the full\ndataset. Furthermore, when applied to a downstream incremental learning task,\nit yields superior performance compared to the state-of-the-art."}
{"id": "2503.14129", "pdf": "https://arxiv.org/pdf/2503.14129", "abs": "https://arxiv.org/abs/2503.14129", "authors": ["Subhadeep Koley", "Tapas Kumar Dutta", "Aneeshan Sain", "Pinaki Nath Chowdhury", "Ayan Kumar Bhunia", "Yi-Zhe Song"], "title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025. Project page available at\n  https://subhadeepkoley.github.io/SketchFusion/", "summary": "While foundation models have revolutionised computer vision, their\neffectiveness for sketch understanding remains limited by the unique challenges\nof abstract, sparse visual inputs. Through systematic analysis, we uncover two\nfundamental limitations: Stable Diffusion (SD) struggles to extract meaningful\nfeatures from abstract sketches (unlike its success with photos), and exhibits\na pronounced frequency-domain bias that suppresses essential low-frequency\ncomponents needed for sketch understanding. Rather than costly retraining, we\naddress these limitations by strategically combining SD with CLIP, whose strong\nsemantic understanding naturally compensates for SD's spatial-frequency biases.\nBy dynamically injecting CLIP features into SD's denoising process and\nadaptively aggregating features across semantic levels, our method achieves\nstate-of-the-art performance in sketch retrieval (+3.35%), recognition\n(+1.06%), segmentation (+29.42%), and correspondence learning (+21.22%),\ndemonstrating the first truly universal sketch feature representation in the\nera of foundation models."}
{"id": "2503.14138", "pdf": "https://arxiv.org/pdf/2503.14138", "abs": "https://arxiv.org/abs/2503.14138", "authors": ["Siddharth D Jaiswal", "Sagnik Basu", "Sandipan Sikdar", "Animesh Mukherjee"], "title": "Exploring Disparity-Accuracy Trade-offs in Face Recognition Systems: The Role of Datasets, Architectures, and Loss Functions", "categories": ["cs.CV", "cs.AI", "cs.CY"], "comment": "This work has been accepted for publication at AAAI ICWSM 2025", "summary": "Automated Face Recognition Systems (FRSs), developed using deep learning\nmodels, are deployed worldwide for identity verification and facial attribute\nanalysis. The performance of these models is determined by a complex\ninterdependence among the model architecture, optimization/loss function and\ndatasets. Although FRSs have surpassed human-level accuracy, they continue to\nbe disparate against certain demographics. Due to the ubiquity of applications,\nit is extremely important to understand the impact of the three components --\nmodel architecture, loss function and face image dataset on the\naccuracy-disparity trade-off to design better, unbiased platforms. In this\nwork, we perform an in-depth analysis of three FRSs for the task of gender\nprediction, with various architectural modifications resulting in ten\ndeep-learning models coupled with four loss functions and benchmark them on\nseven face datasets across 266 evaluation configurations. Our results show that\nall three components have an individual as well as a combined impact on both\naccuracy and disparity. We identify that datasets have an inherent property\nthat causes them to perform similarly across models, independent of the choice\nof loss functions. Moreover, the choice of dataset determines the model's\nperceived bias -- the same model reports bias in opposite directions for three\ngender-balanced datasets of ``in-the-wild'' face images of popular individuals.\nStudying the facial embeddings shows that the models are unable to generalize a\nuniform definition of what constitutes a ``female face'' as opposed to a ``male\nface'', due to dataset diversity. We provide recommendations to model\ndevelopers on using our study as a blueprint for model development and\nsubsequent deployment."}
{"id": "2503.14140", "pdf": "https://arxiv.org/pdf/2503.14140", "abs": "https://arxiv.org/abs/2503.14140", "authors": ["Zining Wang", "Tongkun Guan", "Pei Fu", "Chen Duan", "Qianyi Jiang", "Zhentao Guo", "Shan Guo", "Junfeng Luo", "Wei Shen", "Xiaokang Yang"], "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension\nto document understanding, i.e., they endow large language models with visual\ncomprehension capabilities; however, how to design a suitable image-text\npre-training task for bridging the visual and language modality in\ndocument-level MLLMs remains underexplored. In this study, we introduce a novel\nvisual-language alignment method that casts the key issue as a Visual Question\nAnswering with Mask generation (VQAMask) task, optimizing two tasks\nsimultaneously: VQA-based text parsing and mask generation. The former allows\nthe model to implicitly align images and text at the semantic level. The latter\nintroduces an additional mask generator (discarded during inference) to\nexplicitly ensure alignment between visual texts within images and their\ncorresponding image regions at a spatially-aware level. Together, they can\nprevent model hallucinations when parsing visual text and effectively promote\nspatially-aware feature representation learning. To support the proposed\nVQAMask task, we construct a comprehensive image-mask generation pipeline and\nprovide a large-scale dataset with 6M data (MTMask6M). Subsequently, we\ndemonstrate that introducing the proposed mask generation task yields\ncompetitive document-level understanding performance. Leveraging the proposed\nVQAMask, we introduce Marten, a training-efficient MLLM tailored for\ndocument-level understanding. Extensive experiments show that our Marten\nconsistently achieves significant improvements among 8B-MLLMs in\ndocument-centric tasks. Code and datasets are available at\nhttps://github.com/PriNing/Marten."}
{"id": "2503.14150", "pdf": "https://arxiv.org/pdf/2503.14150", "abs": "https://arxiv.org/abs/2503.14150", "authors": ["Yihang Zhou", "Ruige Kong", "Zhengsen Xu", "Linlin Xu", "Sibo Cheng"], "title": "Comparative and Interpretative Analysis of CNN and Transformer Models in Predicting Wildfire Spread Using Remote Sensing Data", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Facing the escalating threat of global wildfires, numerous computer vision\ntechniques using remote sensing data have been applied in this area. However,\nthe selection of deep learning methods for wildfire prediction remains\nuncertain due to the lack of comparative analysis in a quantitative and\nexplainable manner, crucial for improving prevention measures and refining\nmodels. This study aims to thoroughly compare the performance, efficiency, and\nexplainability of four prevalent deep learning architectures: Autoencoder,\nResNet, UNet, and Transformer-based Swin-UNet. Employing a real-world dataset\nthat includes nearly a decade of remote sensing data from California, U.S.,\nthese models predict the spread of wildfires for the following day. Through\ndetailed quantitative comparison analysis, we discovered that Transformer-based\nSwin-UNet and UNet generally outperform Autoencoder and ResNet, particularly\ndue to the advanced attention mechanisms in Transformer-based Swin-UNet and the\nefficient use of skip connections in both UNet and Transformer-based Swin-UNet,\nwhich contribute to superior predictive accuracy and model interpretability.\nThen we applied XAI techniques on all four models, this not only enhances the\nclarity and trustworthiness of models but also promotes focused improvements in\nwildfire prediction capabilities. The XAI analysis reveals that UNet and\nTransformer-based Swin-UNet are able to focus on critical features such as\n'Previous Fire Mask', 'Drought', and 'Vegetation' more effectively than the\nother two models, while also maintaining balanced attention to the remaining\nfeatures, leading to their superior performance. The insights from our thorough\ncomparative analysis offer substantial implications for future model design and\nalso provide guidance for model selection in different scenarios."}
{"id": "2503.14151", "pdf": "https://arxiv.org/pdf/2503.14151", "abs": "https://arxiv.org/abs/2503.14151", "authors": ["Yong Zhong", "Zhuoyi Yang", "Jiayan Teng", "Xiaotao Gu", "Chongxuan Li"], "title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Concat-ID, a unified framework for identity-preserving video\ngeneration. Concat-ID employs Variational Autoencoders to extract image\nfeatures, which are concatenated with video latents along the sequence\ndimension, leveraging solely 3D self-attention mechanisms without the need for\nadditional modules. A novel cross-video pairing strategy and a multi-stage\ntraining regimen are introduced to balance identity consistency and facial\neditability while enhancing video naturalness. Extensive experiments\ndemonstrate Concat-ID's superiority over existing methods in both single and\nmulti-identity generation, as well as its seamless scalability to multi-subject\nscenarios, including virtual try-on and background-controllable generation.\nConcat-ID establishes a new benchmark for identity-preserving video synthesis,\nproviding a versatile and scalable solution for a wide range of applications."}
{"id": "2503.14154", "pdf": "https://arxiv.org/pdf/2503.14154", "abs": "https://arxiv.org/abs/2503.14154", "authors": ["Zhang Chen", "Shuai Wan", "Siyu Ren", "Fuzheng Yang", "Mengting Yu", "Junhui Hou"], "title": "RBFIM: Perceptual Quality Assessment for Compressed Point Clouds Using Radial Basis Function Interpolation", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": null, "summary": "One of the main challenges in point cloud compression (PCC) is how to\nevaluate the perceived distortion so that the codec can be optimized for\nperceptual quality. Current standard practices in PCC highlight a primary\nissue: while single-feature metrics are widely used to assess compression\ndistortion, the classic method of searching point-to-point nearest neighbors\nfrequently fails to adequately build precise correspondences between point\nclouds, resulting in an ineffective capture of human perceptual features. To\novercome the related limitations, we propose a novel assessment method called\nRBFIM, utilizing radial basis function (RBF) interpolation to convert discrete\npoint features into a continuous feature function for the distorted point\ncloud. By substituting the geometry coordinates of the original point cloud\ninto the feature function, we obtain the bijective sets of point features. This\nenables an establishment of precise corresponding features between distorted\nand original point clouds and significantly improves the accuracy of quality\nassessments. Moreover, this method avoids the complexity caused by\nbidirectional searches. Extensive experiments on multiple subjective quality\ndatasets of compressed point clouds demonstrate that our RBFIM excels in\naddressing human perception tasks, thereby providing robust support for PCC\noptimization efforts."}
{"id": "2503.14161", "pdf": "https://arxiv.org/pdf/2503.14161", "abs": "https://arxiv.org/abs/2503.14161", "authors": ["Yiqi Zhu", "Ziyue Wang", "Can Zhang", "Peng Li", "Yang Liu"], "title": "CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have recently witnessed significant progress in\nvisual comprehension. As the permitting length of image context grows, VLMs can\nnow comprehend a broader range of views and spaces. Current benchmarks provide\ninsightful analysis of VLMs in tasks involving complex visual instructions\nfollowing, multi-image understanding and spatial reasoning. However, they\nusually focus on spatially irrelevant images or discrete images captured from\nvaried viewpoints. The compositional characteristic of images captured from a\nstatic viewpoint remains underestimated. We term this characteristic as\nContinuous Space Perception. When observing a scene from a static viewpoint\nwhile shifting orientations, it produces a series of spatially continuous\nimages, enabling the reconstruction of the entire space. In this paper, we\npresent CoSpace, a multi-image visual understanding benchmark designed to\nassess the Continuous Space perception ability for VLMs. CoSpace contains 2,918\nimages and 1,626 question-answer pairs, covering seven types of tasks. We\nconduct evaluation across 19 proprietary and open-source VLMs. Results reveal\nthat there exist pitfalls on the continuous space perception ability for most\nof the evaluated models, including proprietary ones. Interestingly, we find\nthat the main discrepancy between open-source and proprietary models lies not\nin accuracy but in the consistency of responses. We believe that enhancing the\nability of continuous space perception is essential for VLMs to perform\neffectively in real-world tasks and encourage further research to advance this\ncapability."}
{"id": "2503.14171", "pdf": "https://arxiv.org/pdf/2503.14171", "abs": "https://arxiv.org/abs/2503.14171", "authors": ["Simon Niedermayr", "Christoph Neuhauser Rüdiger Westermann"], "title": "Lightweight Gradient-Aware Upscaling of 3D Gaussian Splatting Images", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "We introduce an image upscaling technique tailored for 3D Gaussian Splatting\n(3DGS) on lightweight GPUs. Compared to 3DGS, it achieves significantly higher\nrendering speeds and reduces artifacts commonly observed in 3DGS\nreconstructions. Our technique upscales low-resolution 3DGS renderings with a\nmarginal increase in cost by directly leveraging the analytical image gradients\nof Gaussians for gradient-based bicubic spline interpolation. The technique is\nagnostic to the specific 3DGS implementation, achieving novel view synthesis at\nrates 3x-4x higher than the baseline implementation. Through extensive\nexperiments on multiple datasets, we showcase the performance improvements and\nhigh reconstruction fidelity attainable with gradient-aware upscaling of 3DGS\nimages. We further demonstrate the integration of gradient-aware upscaling into\nthe gradient-based optimization of a 3DGS model and analyze its effects on\nreconstruction quality and performance."}
{"id": "2503.14198", "pdf": "https://arxiv.org/pdf/2503.14198", "abs": "https://arxiv.org/abs/2503.14198", "authors": ["Junjin Xiao", "Qing Zhang", "Yonewei Nie", "Lei Zhu", "Wei-Shi Zheng"], "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "This paper presents RoGSplat, a novel approach for synthesizing high-fidelity\nnovel views of unseen human from sparse multi-view images, while requiring no\ncumbersome per-subject optimization. Unlike previous methods that typically\nstruggle with sparse views with few overlappings and are less effective in\nreconstructing complex human geometry, the proposed method enables robust\nreconstruction in such challenging conditions. Our key idea is to lift SMPL\nvertices to dense and reliable 3D prior points representing accurate human body\ngeometry, and then regress human Gaussian parameters based on the points. To\naccount for possible misalignment between SMPL model and images, we propose to\npredict image-aligned 3D prior points by leveraging both pixel-level features\nand voxel-level features, from which we regress the coarse Gaussians. To\nenhance the ability to capture high-frequency details, we further render depth\nmaps from the coarse 3D Gaussians to help regress fine-grained pixel-wise\nGaussians. Experiments on several benchmark datasets demonstrate that our\nmethod outperforms state-of-the-art methods in novel view synthesis and\ncross-dataset generalization. Our code is available at\nhttps://github.com/iSEE-Laboratory/RoGSplat."}
{"id": "2503.14209", "pdf": "https://arxiv.org/pdf/2503.14209", "abs": "https://arxiv.org/abs/2503.14209", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "AI-Driven Diabetic Retinopathy Diagnosis Enhancement through Image Processing and Salp Swarm Algorithm-Optimized Ensemble Network", "categories": ["cs.CV"], "comment": null, "summary": "Diabetic retinopathy is a leading cause of blindness in diabetic patients and\nearly detection plays a crucial role in preventing vision loss. Traditional\ndiagnostic methods are often time-consuming and prone to errors. The emergence\nof deep learning techniques has provided innovative solutions to improve\ndiagnostic efficiency. However, single deep learning models frequently face\nissues related to extracting key features from complex retinal images. To\nhandle this problem, we present an effective ensemble method for DR diagnosis\ncomprising four main phases: image pre-processing, selection of backbone\npre-trained models, feature enhancement, and optimization. Our methodology\ninitiates with the pre-processing phase, where we apply CLAHE to enhance image\ncontrast and Gamma correction is then used to adjust the brightness for better\nfeature recognition. We then apply Discrete Wavelet Transform (DWT) for image\nfusion by combining multi-resolution details to create a richer dataset. Then,\nwe selected three pre-trained models with the best performance named\nDenseNet169, MobileNetV1, and Xception for diverse feature extraction. To\nfurther improve feature extraction, an improved residual block is integrated\ninto each model. Finally, the predictions from these base models are then\naggregated using weighted ensemble approach, with the weights optimized by\nusing Salp Swarm Algorithm (SSA).SSA intelligently explores the weight space\nand finds the optimal configuration of base architectures to maximize the\nperformance of the ensemble model. The proposed model is evaluated on the\nmulticlass Kaggle APTOS 2019 dataset and obtained 88.52% accuracy."}
{"id": "2503.14219", "pdf": "https://arxiv.org/pdf/2503.14219", "abs": "https://arxiv.org/abs/2503.14219", "authors": ["Yizhou Li", "Yusuke Monno", "Masatoshi Okutomi", "Yuuichi Tanaka", "Seiichi Kataoka", "Teruaki Kosiba"], "title": "Segmentation-Guided Neural Radiance Fields for Novel Street View Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": "Presented at VISAPP2025. Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/NVS/index.html", "summary": "Recent advances in Neural Radiance Fields (NeRF) have shown great potential\nin 3D reconstruction and novel view synthesis, particularly for indoor and\nsmall-scale scenes. However, extending NeRF to large-scale outdoor environments\npresents challenges such as transient objects, sparse cameras and textures, and\nvarying lighting conditions. In this paper, we propose a segmentation-guided\nenhancement to NeRF for outdoor street scenes, focusing on complex urban\nenvironments. Our approach extends ZipNeRF and utilizes Grounded SAM for\nsegmentation mask generation, enabling effective handling of transient objects,\nmodeling of the sky, and regularization of the ground. We also introduce\nappearance embeddings to adapt to inconsistent lighting across view sequences.\nExperimental results demonstrate that our method outperforms the baseline\nZipNeRF, improving novel view synthesis quality with fewer artifacts and\nsharper details."}
{"id": "2503.14228", "pdf": "https://arxiv.org/pdf/2503.14228", "abs": "https://arxiv.org/abs/2503.14228", "authors": ["Nobuhiko Wakai", "Satoshi Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Panoramic Distortion-Aware Tokenization for Person Detection and Localization Using Transformers in Overhead Fisheye Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person detection methods are used widely in applications including visual\nsurveillance, pedestrian detection, and robotics. However, accurate detection\nof persons from overhead fisheye images remains an open challenge because of\nfactors including person rotation and small-sized persons. To address the\nperson rotation problem, we convert the fisheye images into panoramic images.\nFor smaller people, we focused on the geometry of the panoramas. Conventional\ndetection methods tend to focus on larger people because these larger people\nyield large significant areas for feature maps. In equirectangular panoramic\nimages, we find that a person's height decreases linearly near the top of the\nimages. Using this finding, we leverage the significance values and aggregate\ntokens that are sorted based on these values to balance the significant areas.\nIn this leveraging process, we introduce panoramic distortion-aware\ntokenization. This tokenization procedure divides a panoramic image using\nself-similarity figures that enable determination of optimal divisions without\ngaps, and we leverage the maximum significant values in each tile of token\ngroups to preserve the significant areas of smaller people. To achieve higher\ndetection accuracy, we propose a person detection and localization method that\ncombines panoramic-image remapping and the tokenization procedure. Extensive\nexperiments demonstrated that our method outperforms conventional methods when\napplied to large-scale datasets."}
{"id": "2503.14231", "pdf": "https://arxiv.org/pdf/2503.14231", "abs": "https://arxiv.org/abs/2503.14231", "authors": ["Ziyao Ling", "Giovanni Delnevo", "Paola Salomoni", "Silvia Mirri"], "title": "Multi-task Learning for Identification of Porcelain in Song and Yuan Dynasties", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Chinese porcelain holds immense historical and cultural value, making its\naccurate classification essential for archaeological research and cultural\nheritage preservation. Traditional classification methods rely heavily on\nexpert analysis, which is time-consuming, subjective, and difficult to scale.\nThis paper explores the application of DL and transfer learning techniques to\nautomate the classification of porcelain artifacts across four key attributes:\ndynasty, glaze, ware, and type. We evaluate four Convolutional Neural Networks\n(CNNs) - ResNet50, MobileNetV2, VGG16, and InceptionV3 - comparing their\nperformance with and without pre-trained weights. Our results demonstrate that\ntransfer learning significantly enhances classification accuracy, particularly\nfor complex tasks like type classification, where models trained from scratch\nexhibit lower performance. MobileNetV2 and ResNet50 consistently achieve high\naccuracy and robustness across all tasks, while VGG16 struggles with more\ndiverse classifications. We further discuss the impact of dataset limitations\nand propose future directions, including domain-specific pre-training,\nintegration of attention mechanisms, explainable AI methods, and generalization\nto other cultural artifacts."}
{"id": "2503.14232", "pdf": "https://arxiv.org/pdf/2503.14232", "abs": "https://arxiv.org/abs/2503.14232", "authors": ["Yuyang Xue", "Edward Moroshko", "Feng Chen", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-to-Image diffusion models can produce undesirable content that\nnecessitates concept erasure techniques. However, existing methods struggle\nwith under-erasure, leaving residual traces of targeted concepts, or\nover-erasure, mistakenly eliminating unrelated but visually similar concepts.\nTo address these limitations, we introduce CRCE, a novel concept erasure\nframework that leverages Large Language Models to identify both semantically\nrelated concepts that should be erased alongside the target and distinct\nconcepts that should be preserved. By explicitly modeling coreferential and\nretained concepts semantically, CRCE enables more precise concept removal,\nwithout unintended erasure. Experiments demonstrate that CRCE outperforms\nexisting methods on diverse erasure tasks."}
{"id": "2503.14237", "pdf": "https://arxiv.org/pdf/2503.14237", "abs": "https://arxiv.org/abs/2503.14237", "authors": ["Chenting Wang", "Kunchang Li", "Tianxiang Jiang", "Xiangyu Zeng", "Yi Wang", "Limin Wang"], "title": "Make Your Training Flexible: Towards Deployment-Efficient Video Models", "categories": ["cs.CV"], "comment": null, "summary": "Popular video training methods mainly operate on a fixed number of tokens\nsampled from a predetermined spatiotemporal grid, resulting in sub-optimal\naccuracy-computation trade-offs due to inherent video redundancy. They also\nlack adaptability to varying computational budgets for downstream tasks,\nhindering applications of the most competitive model in real-world scenes. We\nthus propose a new test setting, Token Optimization, for maximized input\ninformation across budgets, which optimizes the size-limited set of input\ntokens through token selection from more suitably sampled videos. To this end,\nwe propose a novel augmentation tool termed Flux. By making the sampling grid\nflexible and leveraging token selection, it is easily adopted in most popular\nvideo training frameworks, boosting model robustness with nearly no additional\ncost. We integrate Flux in large-scale video pre-training, and the resulting\nFluxViT establishes new state-of-the-art results across extensive tasks at\nstandard costs. Notably, with 1/4 tokens only, it can still match the\nperformance of previous state-of-the-art models with Token Optimization,\nyielding nearly 90\\% savings. All models and data are available at\nhttps://github.com/OpenGVLab/FluxViT."}
{"id": "2503.14244", "pdf": "https://arxiv.org/pdf/2503.14244", "abs": "https://arxiv.org/abs/2503.14244", "authors": ["Fedor Zolotarev", "Tuomas Eerola", "Tomi Kauppi"], "title": "Deep Unsupervised Segmentation of Log Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "In sawmills, it is essential to accurately measure the raw material, i.e.\nwooden logs, to optimise the sawing process. Earlier studies have shown that\naccurate predictions of the inner structure of the logs can be obtained using\njust surface point clouds produced by a laser scanner. This provides a\ncost-efficient and fast alternative to the X-ray CT-based measurement devices.\nThe essential steps in analysing log point clouds is segmentation, as it forms\nthe basis for finding the fine surface details that provide the cues about the\ninner structure of the log. We propose a novel Point Transformer-based point\ncloud segmentation technique that learns to find the points belonging to the\nlog surface in unsupervised manner. This is obtained using a loss function that\nutilises the geometrical properties of a cylinder while taking into account the\nshape variation common in timber logs. We demonstrate the accuracy of the\nmethod on wooden logs, but the approach could be utilised also on other\ncylindrical objects."}
{"id": "2503.14272", "pdf": "https://arxiv.org/pdf/2503.14272", "abs": "https://arxiv.org/abs/2503.14272", "authors": ["Runyi Li", "Bin Chen", "Jian Zhang", "Radu Timofte"], "title": "CTSR: Controllable Fidelity-Realness Trade-off Distillation for Real-World Image Super Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Real-world image super-resolution is a critical image processing task, where\ntwo key evaluation criteria are the fidelity to the original image and the\nvisual realness of the generated results. Although existing methods based on\ndiffusion models excel in visual realness by leveraging strong priors, they\noften struggle to achieve an effective balance between fidelity and realness.\nIn our preliminary experiments, we observe that a linear combination of\nmultiple models outperforms individual models, motivating us to harness the\nstrengths of different models for a more effective trade-off. Based on this\ninsight, we propose a distillation-based approach that leverages the geometric\ndecomposition of both fidelity and realness, alongside the performance\nadvantages of multiple teacher models, to strike a more balanced trade-off.\nFurthermore, we explore the controllability of this trade-off, enabling a\nflexible and adjustable super-resolution process, which we call CTSR\n(Controllable Trade-off Super-Resolution). Experiments conducted on several\nreal-world image super-resolution benchmarks demonstrate that our method\nsurpasses existing state-of-the-art approaches, achieving superior performance\nacross both fidelity and realness metrics."}
{"id": "2503.14273", "pdf": "https://arxiv.org/pdf/2503.14273", "abs": "https://arxiv.org/abs/2503.14273", "authors": ["Matthew J. Allen", "Harry J. F. Owen", "Stuart W. D. Grieve", "Emily R. Lines"], "title": "Manual Labelling Artificially Inflates Deep Learning-Based Segmentation Performance on Closed Canopy: Validation Using TLS", "categories": ["cs.CV", "cs.AI", "I.4; I.4.6; I.4.8; I.4.9; I.5; I.5.4"], "comment": "17 pages, 3 figures", "summary": "Monitoring forest dynamics at an individual tree scale is essential for\naccurately assessing ecosystem responses to climate change, yet traditional\nmethods relying on field-based forest inventories are labor-intensive and\nlimited in spatial coverage. Advances in remote sensing using drone-acquired\nRGB imagery combined with deep learning models have promised precise individual\ntree crown (ITC) segmentation; however, existing methods are frequently\nvalidated against human-annotated images, lacking rigorous independent ground\ntruth. In this study, we generate high-fidelity validation labels from\nco-located Terrestrial Laser Scanning (TLS) data for drone imagery of mixed\nunmanaged boreal and Mediterranean forests. We evaluate the performance of two\nwidely used deep learning ITC segmentation models - DeepForest (RetinaNet) and\nDetectree2 (Mask R-CNN) - on these data, and compare to performance on further\nMediterranean forest data labelled manually. When validated against TLS-derived\nground truth from Mediterranean forests, model performance decreased\nsignificantly compared to assessment based on hand-labelled from an\necologically similar site (AP50: 0.094 vs. 0.670). Restricting evaluation to\nonly canopy trees shrank this gap considerably (Canopy AP50: 0.365), although\nperformance was still far lower than on similar hand-labelled data. Models also\nperformed poorly on boreal forest data (AP50: 0.142), although again increasing\nwhen evaluated on canopy trees only (Canopy AP50: 0.308). Both models showed\nvery poor localisation accuracy at stricter IoU thresholds, even when\nrestricted to canopy trees (Max AP75: 0.051). Similar results have been\nobserved in studies using aerial LiDAR data, suggesting fundamental limitations\nin aerial-based segmentation approaches in closed canopy forests."}
{"id": "2503.14274", "pdf": "https://arxiv.org/pdf/2503.14274", "abs": "https://arxiv.org/abs/2503.14274", "authors": ["Glenn Grubert", "Florian Barthel", "Anna Hilsmann", "Peter Eisert"], "title": "Improving Adaptive Density Control for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become one of the most influential works in\nthe past year. Due to its efficient and high-quality novel view synthesis\ncapabilities, it has been widely adopted in many research fields and\napplications. Nevertheless, 3DGS still faces challenges to properly manage the\nnumber of Gaussian primitives that are used during scene reconstruction.\nFollowing the adaptive density control (ADC) mechanism of 3D Gaussian\nSplatting, new Gaussians in under-reconstructed regions are created, while\nGaussians that do not contribute to the rendering quality are pruned. We\nobserve that those criteria for densifying and pruning Gaussians can sometimes\nlead to worse rendering by introducing artifacts. We especially observe\nunder-reconstructed background or overfitted foreground regions. To encounter\nboth problems, we propose three new improvements to the adaptive density\ncontrol mechanism. Those include a correction for the scene extent calculation\nthat does not only rely on camera positions, an exponentially ascending\ngradient threshold to improve training convergence, and significance-aware\npruning strategy to avoid background artifacts. With these adaptions, we show\nthat the rendering quality improves while using the same number of Gaussians\nprimitives. Furthermore, with our improvements, the training converges\nconsiderably faster, allowing for more than twice as fast training times while\nyielding better quality than 3DGS. Finally, our contributions are easily\ncompatible with most existing derivative works of 3DGS making them relevant for\nfuture works."}
{"id": "2503.14275", "pdf": "https://arxiv.org/pdf/2503.14275", "abs": "https://arxiv.org/abs/2503.14275", "authors": ["Jiang Qin", "Senmao Li", "Alexandra Gomez-Villa", "Shiqi Yang", "Yaxing Wang", "Kai Wang", "Joost van de Weijer"], "title": "Free-Lunch Color-Texture Disentanglement for Stylized Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Text-to-Image (T2I) diffusion models have transformed\nimage generation, enabling significant progress in stylized generation using\nonly a few style reference images. However, current diffusion-based methods\nstruggle with fine-grained style customization due to challenges in controlling\nmultiple style attributes, such as color and texture. This paper introduces the\nfirst tuning-free approach to achieve free-lunch color-texture disentanglement\nin stylized T2I generation, addressing the need for independently controlled\nstyle elements for the Disentangled Stylized Image Generation (DisIG) problem.\nOur approach leverages the Image-Prompt Additivity property in the CLIP image\nembedding space to develop techniques for separating and extracting\nColor-Texture Embeddings (CTE) from individual color and texture reference\nimages. To ensure that the color palette of the generated image aligns closely\nwith the color reference, we apply a whitening and coloring transformation to\nenhance color consistency. Additionally, to prevent texture loss due to the\nsignal-leak bias inherent in diffusion training, we introduce a noise term that\npreserves textural fidelity during the Regularized Whitening and Coloring\nTransformation (RegWCT). Through these methods, our Style Attributes\nDisentanglement approach (SADis) delivers a more precise and customizable\nsolution for stylized image generation. Experiments on images from the WikiArt\nand StyleDrop datasets demonstrate that, both qualitatively and quantitatively,\nSADis surpasses state-of-the-art stylization methods in the DisIG task."}
{"id": "2503.14277", "pdf": "https://arxiv.org/pdf/2503.14277", "abs": "https://arxiv.org/abs/2503.14277", "authors": ["Fedor Zolotarev", "Borek Reich", "Tuomas Eerola", "Tomi Kauppi", "Pavel Zemcik"], "title": "Towards synthetic generation of realistic wooden logs", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a novel method to synthetically generate realistic\n3D representations of wooden logs. Efficient sawmilling heavily relies on\naccurate measurement of logs and the distribution of knots inside them.\nComputed Tomography (CT) can be used to obtain accurate information about the\nknots but is often not feasible in a sawmill environment. A promising\nalternative is to utilize surface measurements and machine learning techniques\nto predict the inner structure of the logs. However, obtaining enough training\ndata remains a challenge. We focus mainly on two aspects of log generation: the\nmodeling of knot growth inside the tree, and the realistic synthesis of the\nsurface including the regions, where the knots reach the surface. This results\nin the first log synthesis approach capable of generating both the internal\nknot and external surface structures of wood. We demonstrate that the proposed\nmathematical log model accurately fits to real data obtained from CT scans and\nenables the generation of realistic logs."}
{"id": "2503.14295", "pdf": "https://arxiv.org/pdf/2503.14295", "abs": "https://arxiv.org/abs/2503.14295", "authors": ["Baiqin Wang", "Xiangyu Zhu", "Fan Shen", "Hao Xu", "Zhen Lei"], "title": "PC-Talk: Precise Facial Animation Control for Audio-Driven Talking Face Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in audio-driven talking face generation have made great\nprogress in lip synchronization. However, current methods often lack sufficient\ncontrol over facial animation such as speaking style and emotional expression,\nresulting in uniform outputs. In this paper, we focus on improving two key\nfactors: lip-audio alignment and emotion control, to enhance the diversity and\nuser-friendliness of talking videos. Lip-audio alignment control focuses on\nelements like speaking style and the scale of lip movements, whereas emotion\ncontrol is centered on generating realistic emotional expressions, allowing for\nmodifications in multiple attributes such as intensity. To achieve precise\ncontrol of facial animation, we propose a novel framework, PC-Talk, which\nenables lip-audio alignment and emotion control through implicit keypoint\ndeformations. First, our lip-audio alignment control module facilitates precise\nediting of speaking styles at the word level and adjusts lip movement scales to\nsimulate varying vocal loudness levels, maintaining lip synchronization with\nthe audio. Second, our emotion control module generates vivid emotional facial\nfeatures with pure emotional deformation. This module also enables the fine\nmodification of intensity and the combination of multiple emotions across\ndifferent facial regions. Our method demonstrates outstanding control\ncapabilities and achieves state-of-the-art performance on both HDTF and MEAD\ndatasets in extensive experiments."}
{"id": "2503.14324", "pdf": "https://arxiv.org/pdf/2503.14324", "abs": "https://arxiv.org/abs/2503.14324", "authors": ["Wei Song", "Yuran Wang", "Zijia Song", "Yadong Li", "Haoze Sun", "Weipeng Chen", "Zenan Zhou", "Jianhua Xu", "Jiaqi Wang", "Kaicheng Yu"], "title": "DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The differing representation spaces required for visual understanding and\ngeneration pose a challenge in unifying them within the autoregressive paradigm\nof large language models. A vision tokenizer trained for reconstruction excels\nat capturing low-level perceptual details, making it well-suited for visual\ngeneration but lacking high-level semantic representations for understanding\ntasks. Conversely, a vision encoder trained via contrastive learning aligns\nwell with language but struggles to decode back into the pixel space for\ngeneration tasks. To bridge this gap, we propose DualToken, a method that\nunifies representations for both understanding and generation within a single\ntokenizer. However, directly integrating reconstruction and semantic objectives\nin a single tokenizer creates conflicts, leading to degraded performance in\nboth reconstruction quality and semantic performance. Instead of forcing a\nsingle codebook to handle both semantic and perceptual information, DualToken\ndisentangles them by introducing separate codebooks for high and low-level\nfeatures, effectively transforming their inherent conflict into a synergistic\nrelationship. As a result, DualToken achieves state-of-the-art performance in\nboth reconstruction and semantic tasks while demonstrating remarkable\neffectiveness in downstream MLLM understanding and generation tasks. Notably,\nwe also show that DualToken, as a unified tokenizer, surpasses the naive\ncombination of two distinct types vision encoders, providing superior\nperformance within a unified MLLM."}
{"id": "2503.14325", "pdf": "https://arxiv.org/pdf/2503.14325", "abs": "https://arxiv.org/abs/2503.14325", "authors": ["Yu Cheng", "Fajie Yuan"], "title": "LeanVAE: An Ultra-Efficient Reconstruction VAE for Video Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recent advances in Latent Video Diffusion Models (LVDMs) have revolutionized\nvideo generation by leveraging Video Variational Autoencoders (Video VAEs) to\ncompress intricate video data into a compact latent space.However, as LVDM\ntraining scales, the computational overhead of Video VAEs becomes a critical\nbottleneck, particularly for encoding high-resolution videos. To address this,\nwe propose LeanVAE, a novel and ultra-efficient Video VAE framework that\nintroduces two key innovations: (1) a lightweight architecture based on a\nNeighborhood-Aware Feedforward (NAF) module and non-overlapping patch\noperations, drastically reducing computational cost, and (2) the integration of\nwavelet transforms and compressed sensing techniques to enhance reconstruction\nquality. Extensive experiments validate LeanVAE's superiority in video\nreconstruction and generation, particularly in enhancing efficiency over\nexisting Video VAEs.Our model offers up to 50x fewer FLOPs and 44x faster\ninference speed while maintaining competitive reconstruction quality, providing\ninsights for scalable, efficient video generation.Our models and code are\navailable at https://github.com/westlake-repl/LeanVAE."}
{"id": "2503.14329", "pdf": "https://arxiv.org/pdf/2503.14329", "abs": "https://arxiv.org/abs/2503.14329", "authors": ["Yufei Zhu", "Yiming Zhong", "Zemin Yang", "Peishan Cong", "Jingyi Yu", "Xinge Zhu", "Yuexin Ma"], "title": "EvolvingGrasp: Evolutionary Grasp Generation via Efficient Preference Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Dexterous robotic hands often struggle to generalize effectively in complex\nenvironments due to the limitations of models trained on low-diversity data.\nHowever, the real world presents an inherently unbounded range of scenarios,\nmaking it impractical to account for every possible variation. A natural\nsolution is to enable robots learning from experience in complex environments,\nan approach akin to evolution, where systems improve through continuous\nfeedback, learning from both failures and successes, and iterating toward\noptimal performance. Motivated by this, we propose EvolvingGrasp, an\nevolutionary grasp generation method that continuously enhances grasping\nperformance through efficient preference alignment. Specifically, we introduce\nHandpose wise Preference Optimization (HPO), which allows the model to\ncontinuously align with preferences from both positive and negative feedback\nwhile progressively refining its grasping strategies. To further enhance\nefficiency and reliability during online adjustments, we incorporate a\nPhysics-aware Consistency Model within HPO, which accelerates inference,\nreduces the number of timesteps needed for preference finetuning, and ensures\nphysical plausibility throughout the process. Extensive experiments across four\nbenchmark datasets demonstrate state of the art performance of our method in\ngrasp success rate and sampling efficiency. Our results validate that\nEvolvingGrasp enables evolutionary grasp generation, ensuring robust,\nphysically feasible, and preference-aligned grasping in both simulation and\nreal scenarios."}
{"id": "2503.14346", "pdf": "https://arxiv.org/pdf/2503.14346", "abs": "https://arxiv.org/abs/2503.14346", "authors": ["X. Anadón", "Javier Rodríguez-Puigvert", "J. M. M. Montiel"], "title": "3D Densification for Multi-Map Monocular VSLAM in Endoscopy", "categories": ["cs.CV"], "comment": null, "summary": "Multi-map Sparse Monocular visual Simultaneous Localization and Mapping\napplied to monocular endoscopic sequences has proven efficient to robustly\nrecover tracking after the frequent losses in endoscopy due to motion blur,\ntemporal occlusion, tools interaction or water jets. The sparse multi-maps are\nadequate for robust camera localization, however they are very poor for\nenvironment representation, they are noisy, with a high percentage of\ninaccurately reconstructed 3D points, including significant outliers, and more\nimportantly with an unacceptable low density for clinical applications.\n  We propose a method to remove outliers and densify the maps of the state of\nthe art for sparse endoscopy multi-map CudaSIFT-SLAM. The NN LightDepth for\nup-to-scale depth dense predictions are aligned with the sparse CudaSIFT\nsubmaps by means of the robust to spurious LMedS. Our system mitigates the\ninherent scale ambiguity in monocular depth estimation while filtering\noutliers, leading to reliable densified 3D maps.\n  We provide experimental evidence of accurate densified maps 4.15 mm RMS\naccuracy at affordable computing time in the C3VD phantom colon dataset. We\nreport qualitative results on the real colonoscopy from the Endomapper dataset."}
{"id": "2503.14350", "pdf": "https://arxiv.org/pdf/2503.14350", "abs": "https://arxiv.org/abs/2503.14350", "authors": ["Shoubin Yu", "Difan Liu", "Ziqiao Ma", "Yicong Hong", "Yang Zhou", "Hao Tan", "Joyce Chai", "Mohit Bansal"], "title": "VEGGIE: Instructional Editing and Reasoning Video Concepts with Grounded Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "First three authors contributed equally. Project page:\n  https://veggie-gen.github.io/", "summary": "Recent video diffusion models have enhanced video editing, but it remains\nchallenging to handle instructional editing and diverse tasks (e.g., adding,\nremoving, changing) within a unified framework. In this paper, we introduce\nVEGGIE, a Video Editor with Grounded Generation from Instructions, a simple\nend-to-end framework that unifies video concept editing, grounding, and\nreasoning based on diverse user instructions. Specifically, given a video and\ntext query, VEGGIE first utilizes an MLLM to interpret user intentions in\ninstructions and ground them to the video contexts, generating frame-specific\ngrounded task queries for pixel-space responses. A diffusion model then renders\nthese plans and generates edited videos that align with user intent. To support\ndiverse tasks and complex instructions, we employ a curriculum learning\nstrategy: first aligning the MLLM and video diffusion model with large-scale\ninstructional image editing data, followed by end-to-end fine-tuning on\nhigh-quality multitask video data. Additionally, we introduce a novel data\nsynthesis pipeline to generate paired instructional video editing data for\nmodel training. It transforms static image data into diverse, high-quality\nvideo editing samples by leveraging Image-to-Video models to inject dynamics.\nVEGGIE shows strong performance in instructional video editing with different\nediting skills, outperforming the best instructional baseline as a versatile\nmodel, while other models struggle with multi-tasking. VEGGIE also excels in\nvideo object grounding and reasoning segmentation, where other baselines fail.\nWe further reveal how the multiple tasks help each other and highlight\npromising applications like zero-shot multimodal instructional and in-context\nvideo editing."}
{"id": "2503.14355", "pdf": "https://arxiv.org/pdf/2503.14355", "abs": "https://arxiv.org/abs/2503.14355", "authors": ["Runqi Meng", "Sifan Song", "Pengfei Jin", "Yujin Oh", "Lin Teng", "Yulin Wang", "Yiqun Sun", "Ling Chen", "Xiang Li", "Quanzheng Li", "Ning Guo", "Dinggang Shen"], "title": "MAST-Pro: Dynamic Mixture-of-Experts for Adaptive Segmentation of Pan-Tumors with Knowledge-Driven Prompts", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Accurate tumor segmentation is crucial for cancer diagnosis and treatment.\nWhile foundation models have advanced general-purpose segmentation, existing\nmethods still struggle with: (1) limited incorporation of medical priors, (2)\nimbalance between generic and tumor-specific features, and (3) high\ncomputational costs for clinical adaptation. To address these challenges, we\npropose MAST-Pro (Mixture-of-experts for Adaptive Segmentation of pan-Tumors\nwith knowledge-driven Prompts), a novel framework that integrates dynamic\nMixture-of-Experts (D-MoE) and knowledge-driven prompts for pan-tumor\nsegmentation. Specifically, text and anatomical prompts provide domain-specific\npriors, guiding tumor representation learning, while D-MoE dynamically selects\nexperts to balance generic and tumor-specific feature learning, improving\nsegmentation accuracy across diverse tumor types. To enhance efficiency, we\nemploy Parameter-Efficient Fine-Tuning (PEFT), optimizing MAST-Pro with\nsignificantly reduced computational overhead. Experiments on multi-anatomical\ntumor datasets demonstrate that MAST-Pro outperforms state-of-the-art\napproaches, achieving up to a 5.20% improvement in average DSC while reducing\ntrainable parameters by 91.04%, without compromising accuracy."}
{"id": "2503.14358", "pdf": "https://arxiv.org/pdf/2503.14358", "abs": "https://arxiv.org/abs/2503.14358", "authors": ["Chao Wang", "Giulio Franzese", "Alessandro Finamore", "Pietro Michiardi"], "title": "RFMI: Estimating Mutual Information on Rectified Flow for Text-to-Image Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "to appear at ICLR 2025 Workshop on Deep Generative Model in Machine\n  Learning: Theory, Principle and Efficacy", "summary": "Rectified Flow (RF) models trained with a Flow matching framework have\nachieved state-of-the-art performance on Text-to-Image (T2I) conditional\ngeneration. Yet, multiple benchmarks show that synthetic images can still\nsuffer from poor alignment with the prompt, i.e., images show wrong attribute\nbinding, subject positioning, numeracy, etc. While the literature offers many\nmethods to improve T2I alignment, they all consider only Diffusion Models, and\nrequire auxiliary datasets, scoring models, and linguistic analysis of the\nprompt. In this paper we aim to address these gaps. First, we introduce RFMI, a\nnovel Mutual Information (MI) estimator for RF models that uses the pre-trained\nmodel itself for the MI estimation. Then, we investigate a self-supervised\nfine-tuning approach for T2I alignment based on RFMI that does not require\nauxiliary information other than the pre-trained model itself. Specifically, a\nfine-tuning set is constructed by selecting synthetic images generated from the\npre-trained RF model and having high point-wise MI between images and prompts.\nOur experiments on MI estimation benchmarks demonstrate the validity of RFMI,\nand empirical fine-tuning on SD3.5-Medium confirms the effectiveness of RFMI\nfor improving T2I alignment while maintaining image quality."}
{"id": "2503.14359", "pdf": "https://arxiv.org/pdf/2503.14359", "abs": "https://arxiv.org/abs/2503.14359", "authors": ["Zhengxian Yang", "Shi Pan", "Shengqi Wang", "Haoxiang Wang", "Li Lin", "Guanjun Li", "Zhengqi Wen", "Borong Lin", "Jianhua Tao", "Tao Yu"], "title": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "User engagement is greatly enhanced by fully immersive multi-modal\nexperiences that combine visual and auditory stimuli. Consequently, the next\nfrontier in VR/AR technologies lies in immersive volumetric videos with\ncomplete scene capture, large 6-DoF interaction space, multi-modal feedback,\nand high resolution & frame-rate contents. To stimulate the reconstruction of\nimmersive volumetric videos, we introduce ImViD, a multi-view, multi-modal\ndataset featuring complete space-oriented data capture and various\nindoor/outdoor scenarios. Our capture rig supports multi-view video-audio\ncapture while on the move, a capability absent in existing datasets,\nsignificantly enhancing the completeness, flexibility, and efficiency of data\ncapture.\n  The captured multi-view videos (with synchronized audios) are in 5K\nresolution at 60FPS, lasting from 1-5 minutes, and include rich\nforeground-background elements, and complex dynamics. We benchmark existing\nmethods using our dataset and establish a base pipeline for constructing\nimmersive volumetric videos from multi-view audiovisual inputs for 6-DoF\nmulti-modal immersive VR experiences. The benchmark and the reconstruction and\ninteraction results demonstrate the effectiveness of our dataset and baseline\nmethod, which we believe will stimulate future research on immersive volumetric\nvideo production."}
{"id": "2503.14378", "pdf": "https://arxiv.org/pdf/2503.14378", "abs": "https://arxiv.org/abs/2503.14378", "authors": ["Zechen Bai", "Hai Ci", "Mike Zheng Shou"], "title": "Impossible Videos", "categories": ["cs.CV"], "comment": "26 pages", "summary": "Synthetic videos nowadays is widely used to complement data scarcity and\ndiversity of real-world videos. Current synthetic datasets primarily replicate\nreal-world scenarios, leaving impossible, counterfactual and anti-reality video\nconcepts underexplored. This work aims to answer two questions: 1) Can today's\nvideo generation models effectively follow prompts to create impossible video\ncontent? 2) Are today's video understanding models good enough for\nunderstanding impossible videos? To this end, we introduce IPV-Bench, a novel\nbenchmark designed to evaluate and foster progress in video understanding and\ngeneration. IPV-Bench is underpinned by a comprehensive taxonomy, encompassing\n4 domains, 14 categories. It features diverse scenes that defy physical,\nbiological, geographical, or social laws. Based on the taxonomy, a prompt suite\nis constructed to evaluate video generation models, challenging their prompt\nfollowing and creativity capabilities. In addition, a video benchmark is\ncurated to assess Video-LLMs on their ability of understanding impossible\nvideos, which particularly requires reasoning on temporal dynamics and world\nknowledge. Comprehensive evaluations reveal limitations and insights for future\ndirections of video models, paving the way for next-generation video models."}
{"id": "2503.14402", "pdf": "https://arxiv.org/pdf/2503.14402", "abs": "https://arxiv.org/abs/2503.14402", "authors": ["Lisha Li", "Jingwen Hou", "Weide Liu", "Yuming Fang", "Jiebin Yan"], "title": "Diffusion-based Facial Aesthetics Enhancement with 3D Structure Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Facial Aesthetics Enhancement (FAE) aims to improve facial attractiveness by\nadjusting the structure and appearance of a facial image while preserving its\nidentity as much as possible. Most existing methods adopted deep feature-based\nor score-based guidance for generation models to conduct FAE. Although these\nmethods achieved promising results, they potentially produced excessively\nbeautified results with lower identity consistency or insufficiently improved\nfacial attractiveness. To enhance facial aesthetics with less loss of identity,\nwe propose the Nearest Neighbor Structure Guidance based on Diffusion\n(NNSG-Diffusion), a diffusion-based FAE method that beautifies a 2D facial\nimage with 3D structure guidance. Specifically, we propose to extract FAE\nguidance from a nearest neighbor reference face. To allow for less change of\nfacial structures in the FAE process, a 3D face model is recovered by referring\nto both the matched 2D reference face and the 2D input face, so that the depth\nand contour guidance can be extracted from the 3D face model. Then the depth\nand contour clues can provide effective guidance to Stable Diffusion with\nControlNet for FAE. Extensive experiments demonstrate that our method is\nsuperior to previous relevant methods in enhancing facial aesthetics while\npreserving facial identity."}
{"id": "2503.14405", "pdf": "https://arxiv.org/pdf/2503.14405", "abs": "https://arxiv.org/abs/2503.14405", "authors": ["Mert Bulent Sariyildiz", "Philippe Weinzaepfel", "Thomas Lucas", "Pau de Jorge", "Diane Larlus", "Yannis Kalantidis"], "title": "DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR-2025. Project page:\n  https://europe.naverlabs.com/dune", "summary": "Recent multi-teacher distillation methods have unified the encoders of\nmultiple foundation models into a single encoder, achieving competitive\nperformance on core vision tasks like classification, segmentation, and depth\nestimation. This led us to ask: Could similar success be achieved when the pool\nof teachers also includes vision models specialized in diverse tasks across\nboth 2D and 3D perception? In this paper, we define and investigate the problem\nof heterogeneous teacher distillation, or co-distillation, a challenging\nmulti-teacher distillation scenario where teacher models vary significantly in\nboth (a) their design objectives and (b) the data they were trained on. We\nexplore data-sharing strategies and teacher-specific encoding, and introduce\nDUNE, a single encoder excelling in 2D vision, 3D understanding, and 3D human\nperception. Our model achieves performance comparable to that of its larger\nteachers, sometimes even outperforming them, on their respective tasks.\nNotably, DUNE surpasses MASt3R in Map-free Visual Relocalization with a much\nsmaller encoder."}
{"id": "2503.14421", "pdf": "https://arxiv.org/pdf/2503.14421", "abs": "https://arxiv.org/abs/2503.14421", "authors": ["Vlad Hondru", "Eduard Hogea", "Darian Onchis", "Radu Tudor Ionescu"], "title": "ExDDV: A New Dataset for Explainable Deepfake Detection in Video", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": null, "summary": "The ever growing realism and quality of generated videos makes it\nincreasingly harder for humans to spot deepfake content, who need to rely more\nand more on automatic deepfake detectors. However, deepfake detectors are also\nprone to errors, and their decisions are not explainable, leaving humans\nvulnerable to deepfake-based fraud and misinformation. To this end, we\nintroduce ExDDV, the first dataset and benchmark for Explainable Deepfake\nDetection in Video. ExDDV comprises around 5.4K real and deepfake videos that\nare manually annotated with text descriptions (to explain the artifacts) and\nclicks (to point out the artifacts). We evaluate a number of vision-language\nmodels on ExDDV, performing experiments with various fine-tuning and in-context\nlearning strategies. Our results show that text and click supervision are both\nrequired to develop robust explainable models for deepfake videos, which are\nable to localize and describe the observed artifacts. Our novel dataset and\ncode to reproduce the results are available at\nhttps://github.com/vladhondru25/ExDDV."}
{"id": "2503.14428", "pdf": "https://arxiv.org/pdf/2503.14428", "abs": "https://arxiv.org/abs/2503.14428", "authors": ["Hongyu Zhang", "Yufan Deng", "Shenghai Yuan", "Peng Jin", "Zesen Cheng", "Yian Zhao", "Chang Liu", "Jie Chen"], "title": "MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project webpage: https://hong-yu-zhang.github.io/MagicComp-Page/", "summary": "Text-to-video (T2V) generation has made significant strides with diffusion\nmodels. However, existing methods still struggle with accurately binding\nattributes, determining spatial relationships, and capturing complex action\ninteractions between multiple subjects. To address these limitations, we\npropose MagicComp, a training-free method that enhances compositional T2V\ngeneration through dual-phase refinement. Specifically, (1) During the\nConditioning Stage: We introduce the Semantic Anchor Disambiguation to\nreinforces subject-specific semantics and resolve inter-subject ambiguity by\nprogressively injecting the directional vectors of semantic anchors into\noriginal text embedding; (2) During the Denoising Stage: We propose Dynamic\nLayout Fusion Attention, which integrates grounding priors and model-adaptive\nspatial perception to flexibly bind subjects to their spatiotemporal regions\nthrough masked attention modulation. Furthermore, MagicComp is a model-agnostic\nand versatile approach, which can be seamlessly integrated into existing T2V\narchitectures. Extensive experiments on T2V-CompBench and VBench demonstrate\nthat MagicComp outperforms state-of-the-art methods, highlighting its potential\nfor applications such as complex prompt-based and trajectory-controllable video\ngeneration. Project page: https://hong-yu-zhang.github.io/MagicComp-Page/."}
{"id": "2503.14430", "pdf": "https://arxiv.org/pdf/2503.14430", "abs": "https://arxiv.org/abs/2503.14430", "authors": ["Zefeng Qian", "Chongyang Zhang", "Yifei Huang", "Gang Wang", "Jiangyong Ying"], "title": "Joint Image-Instance Spatial-Temporal Attention for Few-shot Action Recognition", "categories": ["cs.CV"], "comment": "Accepted by Computer Vision and Image Understanding", "summary": "Few-shot Action Recognition (FSAR) constitutes a crucial challenge in\ncomputer vision, entailing the recognition of actions from a limited set of\nexamples. Recent approaches mainly focus on employing image-level features to\nconstruct temporal dependencies and generate prototypes for each action\ncategory. However, a considerable number of these methods utilize mainly\nimage-level features that incorporate background noise and focus insufficiently\non real foreground (action-related instances), thereby compromising the\nrecognition capability, particularly in the few-shot scenario. To tackle this\nissue, we propose a novel joint Image-Instance level Spatial-temporal attention\napproach (I2ST) for Few-shot Action Recognition. The core concept of I2ST is to\nperceive the action-related instances and integrate them with image features\nvia spatial-temporal attention. Specifically, I2ST consists of two key\ncomponents: Action-related Instance Perception and Joint Image-Instance\nSpatial-temporal Attention. Given the basic representations from the feature\nextractor, the Action-related Instance Perception is introduced to perceive\naction-related instances under the guidance of a text-guided segmentation\nmodel. Subsequently, the Joint Image-Instance Spatial-temporal Attention is\nused to construct the feature dependency between instances and images..."}
{"id": "2503.14445", "pdf": "https://arxiv.org/pdf/2503.14445", "abs": "https://arxiv.org/abs/2503.14445", "authors": ["Stanislaw Szymanowicz", "Jason Y. Zhang", "Pratul Srinivasan", "Ruiqi Gao", "Arthur Brussee", "Aleksander Holynski", "Ricardo Martin-Brualla", "Jonathan T. Barron", "Philipp Henzler"], "title": "Bolt3D: Generating 3D Scenes in Seconds", "categories": ["cs.CV"], "comment": "Project page: https://szymanowiczs.github.io/bolt3d", "summary": "We present a latent diffusion model for fast feed-forward 3D scene\ngeneration. Given one or more images, our model Bolt3D directly samples a 3D\nscene representation in less than seven seconds on a single GPU. We achieve\nthis by leveraging powerful and scalable existing 2D diffusion network\narchitectures to produce consistent high-fidelity 3D scene representations. To\ntrain this model, we create a large-scale multiview-consistent dataset of 3D\ngeometry and appearance by applying state-of-the-art dense 3D reconstruction\ntechniques to existing multiview image datasets. Compared to prior multiview\ngenerative models that require per-scene optimization for 3D reconstruction,\nBolt3D reduces the inference cost by a factor of up to 300 times."}
{"id": "2503.14463", "pdf": "https://arxiv.org/pdf/2503.14463", "abs": "https://arxiv.org/abs/2503.14463", "authors": ["Yucheng Mao", "Boyang Wang", "Nilesh Kulkarni", "Jeong Joon Park"], "title": "SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "The computer vision community has developed numerous techniques for digitally\nrestoring true scene information from single-view degraded photographs, an\nimportant yet extremely ill-posed task. In this work, we tackle image\nrestoration from a different perspective by jointly denoising multiple\nphotographs of the same scene. Our core hypothesis is that degraded images\ncapturing a shared scene contain complementary information that, when combined,\nbetter constrains the restoration problem. To this end, we implement a powerful\nmulti-view diffusion model that jointly generates uncorrupted views by\nextracting rich information from multi-view relationships. Our experiments show\nthat our multi-view approach outperforms existing single-view image and even\nvideo-based methods on image deblurring and super-resolution tasks. Critically,\nour model is trained to output 3D consistent images, making it a promising tool\nfor applications requiring robust multi-view integration, such as 3D\nreconstruction or pose estimation."}
{"id": "2503.14478", "pdf": "https://arxiv.org/pdf/2503.14478", "abs": "https://arxiv.org/abs/2503.14478", "authors": ["Xinyu Fang", "Zhijian Chen", "Kai Lan", "Shengyuan Ding", "Yingji Liang", "Xiangyu Zhao", "Farong Wen", "Zicheng Zhang", "Guofeng Zhang", "Haodong Duan", "Kai Chen", "Dahua Lin"], "title": "Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM", "categories": ["cs.CV"], "comment": "Evaluation Code and dataset see\n  https://github.com/open-compass/Creation-MMBench", "summary": "Creativity is a fundamental aspect of intelligence, involving the ability to\ngenerate novel and appropriate solutions across diverse contexts. While Large\nLanguage Models (LLMs) have been extensively evaluated for their creative\ncapabilities, the assessment of Multimodal Large Language Models (MLLMs) in\nthis domain remains largely unexplored. To address this gap, we introduce\nCreation-MMBench, a multimodal benchmark specifically designed to evaluate the\ncreative capabilities of MLLMs in real-world, image-based tasks. The benchmark\ncomprises 765 test cases spanning 51 fine-grained tasks. To ensure rigorous\nevaluation, we define instance-specific evaluation criteria for each test case,\nguiding the assessment of both general response quality and factual consistency\nwith visual inputs. Experimental results reveal that current open-source MLLMs\nsignificantly underperform compared to proprietary models in creative tasks.\nFurthermore, our analysis demonstrates that visual fine-tuning can negatively\nimpact the base LLM's creative abilities. Creation-MMBench provides valuable\ninsights for advancing MLLM creativity and establishes a foundation for future\nimprovements in multimodal generative intelligence. Full data and evaluation\ncode is released on https://github.com/open-compass/Creation-MMBench."}
{"id": "2503.14482", "pdf": "https://arxiv.org/pdf/2503.14482", "abs": "https://arxiv.org/abs/2503.14482", "authors": ["Yulin Pan", "Xiangteng He", "Chaojie Mao", "Zhen Han", "Zeyinzi Jiang", "Jingfeng Zhang", "Yu Liu"], "title": "ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing", "categories": ["cs.CV"], "comment": "17 pages", "summary": "Image generation has witnessed significant advancements in the past few\nyears. However, evaluating the performance of image generation models remains a\nformidable challenge. In this paper, we propose ICE-Bench, a unified and\ncomprehensive benchmark designed to rigorously assess image generation models.\nIts comprehensiveness could be summarized in the following key features: (1)\nCoarse-to-Fine Tasks: We systematically deconstruct image generation into four\ntask categories: No-ref/Ref Image Creating/Editing, based on the presence or\nabsence of source images and reference images. And further decompose them into\n31 fine-grained tasks covering a broad spectrum of image generation\nrequirements, culminating in a comprehensive benchmark. (2) Multi-dimensional\nMetrics: The evaluation framework assesses image generation capabilities across\n6 dimensions: aesthetic quality, imaging quality, prompt following, source\nconsistency, reference consistency, and controllability. 11 metrics are\nintroduced to support the multi-dimensional evaluation. Notably, we introduce\nVLLM-QA, an innovative metric designed to assess the success of image editing\nby leveraging large models. (3) Hybrid Data: The data comes from real scenes\nand virtual generation, which effectively improves data diversity and\nalleviates the bias problem in model evaluation. Through ICE-Bench, we conduct\na thorough analysis of existing generation models, revealing both the\nchallenging nature of our benchmark and the gap between current model\ncapabilities and real-world generation requirements. To foster further\nadvancements in the field, we will open-source ICE-Bench, including its\ndataset, evaluation code, and models, thereby providing a valuable resource for\nthe research community."}
{"id": "2503.14483", "pdf": "https://arxiv.org/pdf/2503.14483", "abs": "https://arxiv.org/abs/2503.14483", "authors": ["Haoyu Guo", "He Zhu", "Sida Peng", "Haotong Lin", "Yunzhi Yan", "Tao Xie", "Wenguan Wang", "Xiaowei Zhou", "Hujun Bao"], "title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page: https://zju3dv.github.io/murre/", "summary": "In this paper, we present a new method for multi-view geometric\nreconstruction. In recent years, large vision models have rapidly developed,\nperforming excellently across various tasks and demonstrating remarkable\ngeneralization capabilities. Some works use large vision models for monocular\ndepth estimation, which have been applied to facilitate multi-view\nreconstruction tasks in an indirect manner. Due to the ambiguity of the\nmonocular depth estimation task, the estimated depth values are usually not\naccurate enough, limiting their utility in aiding multi-view reconstruction. We\npropose to incorporate SfM information, a strong multi-view prior, into the\ndepth estimation process, thus enhancing the quality of depth prediction and\nenabling their direct application in multi-view geometric reconstruction.\nExperimental results on public real-world datasets show that our method\nsignificantly improves the quality of depth estimation compared to previous\nmonocular depth estimation works. Additionally, we evaluate the reconstruction\nquality of our approach in various types of scenes including indoor,\nstreetscape, and aerial views, surpassing state-of-the-art MVS methods. The\ncode and supplementary materials are available at\nhttps://zju3dv.github.io/murre/ ."}
{"id": "2503.14487", "pdf": "https://arxiv.org/pdf/2503.14487", "abs": "https://arxiv.org/abs/2503.14487", "authors": ["Minglei Shi", "Ziyang Yuan", "Haotian Yang", "Xintao Wang", "Mingwu Zheng", "Xin Tao", "Wenliang Zhao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://shiml20.github.io/DiffMoE/", "summary": "Diffusion models have demonstrated remarkable success in various image\ngeneration tasks, but their performance is often limited by the uniform\nprocessing of inputs across varying conditions and noise levels. To address\nthis limitation, we propose a novel approach that leverages the inherent\nheterogeneity of the diffusion process. Our method, DiffMoE, introduces a\nbatch-level global token pool that enables experts to access global token\ndistributions during training, promoting specialized expert behavior. To\nunleash the full potential of the diffusion process, DiffMoE incorporates a\ncapacity predictor that dynamically allocates computational resources based on\nnoise levels and sample complexity. Through comprehensive evaluation, DiffMoE\nachieves state-of-the-art performance among diffusion models on ImageNet\nbenchmark, substantially outperforming both dense architectures with 3x\nactivated parameters and existing MoE approaches while maintaining 1x activated\nparameters. The effectiveness of our approach extends beyond class-conditional\ngeneration to more challenging tasks such as text-to-image generation,\ndemonstrating its broad applicability across different diffusion model\napplications. Project Page: https://shiml20.github.io/DiffMoE/"}
{"id": "2503.14489", "pdf": "https://arxiv.org/pdf/2503.14489", "abs": "https://arxiv.org/abs/2503.14489", "authors": ["Jensen", "Zhou", "Hang Gao", "Vikram Voleti", "Aaryaman Vasishta", "Chun-Han Yao", "Mark Boss", "Philip Torr", "Christian Rupprecht", "Varun Jampani"], "title": "Stable Virtual Camera: Generative View Synthesis with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present Stable Virtual Camera (Seva), a generalist diffusion model that\ncreates novel views of a scene, given any number of input views and target\ncameras. Existing works struggle to generate either large viewpoint changes or\ntemporally smooth samples, while relying on specific task configurations. Our\napproach overcomes these limitations through simple model design, optimized\ntraining recipe, and flexible sampling strategy that generalize across view\nsynthesis tasks at test time. As a result, our samples maintain high\nconsistency without requiring additional 3D representation-based distillation,\nthus streamlining view synthesis in the wild. Furthermore, we show that our\nmethod can generate high-quality videos lasting up to half a minute with\nseamless loop closure. Extensive benchmarking demonstrates that Seva\noutperforms existing methods across different datasets and settings."}
{"id": "2503.14492", "pdf": "https://arxiv.org/pdf/2503.14492", "abs": "https://arxiv.org/abs/2503.14492", "authors": ["NVIDIA", ":", "Hassan Abu Alhaija", "Jose Alvarez", "Maciej Bala", "Tiffany Cai", "Tianshi Cao", "Liz Cha", "Joshua Chen", "Mike Chen", "Francesco Ferroni", "Sanja Fidler", "Dieter Fox", "Yunhao Ge", "Jinwei Gu", "Ali Hassani", "Michael Isaev", "Pooya Jannaty", "Shiyi Lan", "Tobias Lasser", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Fabio Ramos", "Xuanchi Ren", "Tianchang Shen", "Shitao Tang", "Ting-Chun Wang", "Jay Wu", "Jiashu Xu", "Stella Xu", "Kevin Xie", "Yuchong Ye", "Xiaodong Yang", "Xiaohui Zeng", "Yu Zeng"], "title": "Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "We introduce Cosmos-Transfer, a conditional world generation model that can\ngenerate world simulations based on multiple spatial control inputs of various\nmodalities such as segmentation, depth, and edge. In the design, the spatial\nconditional scheme is adaptive and customizable. It allows weighting different\nconditional inputs differently at different spatial locations. This enables\nhighly controllable world generation and finds use in various world-to-world\ntransfer use cases, including Sim2Real. We conduct extensive evaluations to\nanalyze the proposed model and demonstrate its applications for Physical AI,\nincluding robotics Sim2Real and autonomous vehicle data enrichment. We further\ndemonstrate an inference scaling strategy to achieve real-time world generation\nwith an NVIDIA GB200 NVL72 rack. To help accelerate research development in the\nfield, we open-source our models and code at\nhttps://github.com/nvidia-cosmos/cosmos-transfer1."}
{"id": "2503.14493", "pdf": "https://arxiv.org/pdf/2503.14493", "abs": "https://arxiv.org/abs/2503.14493", "authors": ["Chuxin Wang", "Wenfei Yang", "Xiang Liu", "Tianzhu Zhang"], "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR 2025", "summary": "DETR-based methods, which use multi-layer transformer decoders to refine\nobject queries iteratively, have shown promising performance in 3D indoor\nobject detection. However, the scene point features in the transformer decoder\nremain fixed, leading to minimal contributions from later decoder layers,\nthereby limiting performance improvement. Recently, State Space Models (SSM)\nhave shown efficient context modeling ability with linear complexity through\niterative interactions between system states and inputs. Inspired by SSMs, we\npropose a new 3D object DEtection paradigm with an interactive STate space\nmodel (DEST). In the interactive SSM, we design a novel state-dependent SSM\nparameterization method that enables system states to effectively serve as\nqueries in 3D indoor detection tasks. In addition, we introduce four key\ndesigns tailored to the characteristics of point cloud and SSM: The\nserialization and bidirectional scanning strategies enable bidirectional\nfeature interaction among scene points within the SSM. The inter-state\nattention mechanism models the relationships between state points, while the\ngated feed-forward network enhances inter-channel correlations. To the best of\nour knowledge, this is the first method to model queries as system states and\nscene points as system inputs, which can simultaneously update scene point\nfeatures and query features with linear complexity. Extensive experiments on\ntwo challenging datasets demonstrate the effectiveness of our DEST-based\nmethod. Our method improves the GroupFree baseline in terms of AP50 on ScanNet\nV2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our\nmethod sets a new SOTA on the ScanNetV2 and SUN RGB-D datasets."}
{"id": "2503.14494", "pdf": "https://arxiv.org/pdf/2503.14494", "abs": "https://arxiv.org/abs/2503.14494", "authors": ["Inkyu Shin", "Chenglin Yang", "Liang-Chieh Chen"], "title": "Deeply Supervised Flow-Based Generative Models", "categories": ["cs.CV"], "comment": "Project website at https://deepflow-project.github.io/", "summary": "Flow based generative models have charted an impressive path across multiple\nvisual generation tasks by adhering to a simple principle: learning velocity\nrepresentations of a linear interpolant. However, we observe that training\nvelocity solely from the final layer output underutilizes the rich inter layer\nrepresentations, potentially impeding model convergence. To address this\nlimitation, we introduce DeepFlow, a novel framework that enhances velocity\nrepresentation through inter layer communication. DeepFlow partitions\ntransformer layers into balanced branches with deep supervision and inserts a\nlightweight Velocity Refiner with Acceleration (VeRA) block between adjacent\nbranches, which aligns the intermediate velocity features within transformer\nblocks. Powered by the improved deep supervision via the internal velocity\nalignment, DeepFlow converges 8 times faster on ImageNet with equivalent\nperformance and further reduces FID by 2.6 while halving training time compared\nto previous flow based models without a classifier free guidance. DeepFlow also\noutperforms baselines in text to image generation tasks, as evidenced by\nevaluations on MSCOCO and zero shot GenEval."}
{"id": "2503.14498", "pdf": "https://arxiv.org/pdf/2503.14498", "abs": "https://arxiv.org/abs/2503.14498", "authors": ["Ayesha Ishaq", "Jean Lahoud", "Fahad Shahbaz Khan", "Salman Khan", "Hisham Cholakkal", "Rao Muhammad Anwer"], "title": "Tracking Meets Large Multimodal Models for Driving Scenario Understanding", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 8 figures, Github:\n  https://github.com/mbzuai-oryx/TrackingMeetsLMM", "summary": "Large Multimodal Models (LMMs) have recently gained prominence in autonomous\ndriving research, showcasing promising capabilities across various emerging\nbenchmarks. LMMs specifically designed for this domain have demonstrated\neffective perception, planning, and prediction skills. However, many of these\nmethods underutilize 3D spatial and temporal elements, relying mainly on image\ndata. As a result, their effectiveness in dynamic driving environments is\nlimited. We propose to integrate tracking information as an additional input to\nrecover 3D spatial and temporal details that are not effectively captured in\nthe images. We introduce a novel approach for embedding this tracking\ninformation into LMMs to enhance their spatiotemporal understanding of driving\nscenarios. By incorporating 3D tracking data through a track encoder, we enrich\nvisual queries with crucial spatial and temporal cues while avoiding the\ncomputational overhead associated with processing lengthy video sequences or\nextensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain\nthe tracking encoder to provide LMMs with additional contextual information,\nsignificantly improving their performance in perception, planning, and\nprediction tasks for autonomous driving. Experimental results demonstrate the\neffectiveness of our approach, with a gain of 9.5% in accuracy, an increase of\n7.04 points in the ChatGPT score, and 9.4% increase in the overall score over\nbaseline models on DriveLM-nuScenes benchmark, along with a 3.7% final score\nimprovement on DriveLM-CARLA. Our code is available at\nhttps://github.com/mbzuai-oryx/TrackingMeetsLMM"}
{"id": "2503.14500", "pdf": "https://arxiv.org/pdf/2503.14500", "abs": "https://arxiv.org/abs/2503.14500", "authors": ["Gihan Jayatilaka", "Abhinav Shrivastava", "Matthew Gwilliam"], "title": "Utilization of Neighbor Information for Image Classification with Different Levels of Supervision", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages, 16 figures, 7 tables", "summary": "We propose to bridge the gap between semi-supervised and unsupervised image\nrecognition with a flexible method that performs well for both generalized\ncategory discovery (GCD) and image clustering. Despite the overlap in\nmotivation between these tasks, the methods themselves are restricted to a\nsingle task -- GCD methods are reliant on the labeled portion of the data, and\ndeep image clustering methods have no built-in way to leverage the labels\nefficiently. We connect the two regimes with an innovative approach that\nUtilizes Neighbor Information for Classification (UNIC) both in the\nunsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art\nclustering methods already rely heavily on nearest neighbors. We improve on\ntheir results substantially in two parts, first with a sampling and cleaning\nstrategy where we identify accurate positive and negative neighbors, and\nsecondly by finetuning the backbone with clustering losses computed by sampling\nboth types of neighbors. We then adapt this pipeline to GCD by utilizing the\nlabelled images as ground truth neighbors. Our method yields state-of-the-art\nresults for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%\nImageNet-100, +5% CUB, +2% SCars, +4% Aircraft)."}
{"id": "2503.14501", "pdf": "https://arxiv.org/pdf/2503.14501", "abs": "https://arxiv.org/abs/2503.14501", "authors": ["Qiaowei Miao", "Kehan Li", "Jinsheng Quan", "Zhiyuan Min", "Shaojie Ma", "Yichao Xu", "Yi Yang", "Yawei Luo"], "title": "Advances in 4D Generation: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Generative artificial intelligence has witnessed remarkable advancements\nacross multiple domains in recent years. Building on the successes of 2D and 3D\ncontent generation, 4D generation, which incorporates the temporal dimension\ninto generative tasks, has emerged as a burgeoning yet rapidly evolving\nresearch area. This paper presents a comprehensive survey of this emerging\nfield, systematically examining its theoretical foundations, key methodologies,\nand practical applications, with the aim of providing readers with a holistic\nunderstanding of the current state and future potential of 4D generation. We\nbegin by introducing the core concepts of 4D data representations, encompassing\nboth structured and unstructured formats, and their implications for generative\ntasks. Building upon this foundation, we delve into the enabling technologies\nthat drive 4D generation, including advancements in spatiotemporal modeling,\nneural representations, and generative frameworks. We further review recent\nstudies that employ diverse control mechanisms and representation strategies\nfor generating 4D outputs, categorizing these approaches and summarizing their\nresearch trajectories. In addition, we explore the wide-ranging applications of\n4D generation techniques, spanning dynamic object modeling, scene generation,\ndigital human synthesis, 4D content editing, and autonomous driving. Finally,\nwe analyze the key challenges inherent to 4D generation, such as data\navailability, computational efficiency, and spatiotemporal consistency, and\npropose promising directions for future research. Our code is publicly\navailable at:\n\\href{https://github.com/MiaoQiaowei/Awesome-4D}{https://github.com/MiaoQiaowei/Awesome-4D}."}
{"id": "2503.14503", "pdf": "https://arxiv.org/pdf/2503.14503", "abs": "https://arxiv.org/abs/2503.14503", "authors": ["Kangfu Mei", "Hossein Talebi", "Mojtaba Ardakani", "Vishal M. Patel", "Peyman Milanfar", "Mauricio Delbracio"], "title": "The Power of Context: How Multimodality Improves Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "accepted by CVPR2025", "summary": "Single-image super-resolution (SISR) remains challenging due to the inherent\ndifficulty of recovering fine-grained details and preserving perceptual quality\nfrom low-resolution inputs. Existing methods often rely on limited image\npriors, leading to suboptimal results. We propose a novel approach that\nleverages the rich contextual information available in multiple modalities --\nincluding depth, segmentation, edges, and text prompts -- to learn a powerful\ngenerative prior for SISR within a diffusion model framework. We introduce a\nflexible network architecture that effectively fuses multimodal information,\naccommodating an arbitrary number of input modalities without requiring\nsignificant modifications to the diffusion process. Crucially, we mitigate\nhallucinations, often introduced by text prompts, by using spatial information\nfrom other modalities to guide regional text-based conditioning. Each\nmodality's guidance strength can also be controlled independently, allowing\nsteering outputs toward different directions, such as increasing bokeh through\ndepth or adjusting object prominence via segmentation. Extensive experiments\ndemonstrate that our model surpasses state-of-the-art generative SISR methods,\nachieving superior visual quality and fidelity. See project page at\nhttps://mmsr.kfmei.com/."}
{"id": "2503.14504", "pdf": "https://arxiv.org/pdf/2503.14504", "abs": "https://arxiv.org/abs/2503.14504", "authors": ["Tao Yu", "Yi-Fan Zhang†", "Chaoyou Fu", "Junkang Wu", "Jinda Lu", "Kun Wang", "Xingyu Lu", "Yunhang Shen", "Guibin Zhang", "Dingjie Song", "Yibo Yan", "Tianlong Xu", "Qingsong Wen", "Zhang Zhang", "Yan Huang", "Liang Wang", "Tieniu Tan"], "title": "Aligning Multimodal LLM with Human Preference: A Survey", "categories": ["cs.CV"], "comment": "https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment", "summary": "Large language models (LLMs) can handle a wide variety of general tasks with\nsimple prompts, without the need for task-specific training. Multimodal Large\nLanguage Models (MLLMs), built upon LLMs, have demonstrated impressive\npotential in tackling complex tasks involving visual, auditory, and textual\ndata. However, critical issues related to truthfulness, safety, o1-like\nreasoning, and alignment with human preference remain insufficiently addressed.\nThis gap has spurred the emergence of various alignment algorithms, each\ntargeting different application scenarios and optimization goals. Recent\nstudies have shown that alignment algorithms are a powerful approach to\nresolving the aforementioned challenges. In this paper, we aim to provide a\ncomprehensive and systematic review of alignment algorithms for MLLMs.\nSpecifically, we explore four key aspects: (1) the application scenarios\ncovered by alignment algorithms, including general image understanding,\nmulti-image, video, and audio, and extended multimodal applications; (2) the\ncore factors in constructing alignment datasets, including data sources, model\nresponses, and preference annotations; (3) the benchmarks used to evaluate\nalignment algorithms; and (4) a discussion of potential future directions for\nthe development of alignment algorithms. This work seeks to help researchers\norganize current advancements in the field and inspire better alignment\nmethods. The project page of this paper is available at\nhttps://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment."}
{"id": "2503.14505", "pdf": "https://arxiv.org/pdf/2503.14505", "abs": "https://arxiv.org/abs/2503.14505", "authors": ["Susung Hong", "Ira Kemelmacher-Shlizerman", "Brian Curless", "Steven M. Seitz"], "title": "MusicInfuser: Making Video Diffusion Listen and Dance", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://susunghong.github.io/MusicInfuser", "summary": "We introduce MusicInfuser, an approach for generating high-quality dance\nvideos that are synchronized to a specified music track. Rather than attempting\nto design and train a new multimodal audio-video model, we show how existing\nvideo diffusion models can be adapted to align with musical inputs by\nintroducing lightweight music-video cross-attention and a low-rank adapter.\nUnlike prior work requiring motion capture data, our approach fine-tunes only\non dance videos. MusicInfuser achieves high-quality music-driven video\ngeneration while preserving the flexibility and generative capabilities of the\nunderlying models. We introduce an evaluation framework using Video-LLMs to\nassess multiple dimensions of dance generation quality. The project page and\ncode are available at https://susunghong.github.io/MusicInfuser."}
{"id": "2409.13661", "pdf": "https://arxiv.org/pdf/2409.13661", "abs": "https://arxiv.org/abs/2409.13661", "authors": ["Luciano Baresi", "Davide Yi Xian Hu", "Andrea Stocco", "Paolo Tonella"], "title": "Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models", "categories": ["cs.SE", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted for publication at the 47th International Conference on\n  Software Engineering (ICSE 2025). This research was partially supported by\n  project EMELIOT, funded by MUR under the PRIN 2020 program (n. 2020W3A5FY),\n  by the Bavarian Ministry of Economic Affairs, Regional Development and\n  Energy, by the TUM Global Incentive Fund, and by the EU Project Sec4AI4Sec\n  (n. 101120393)", "summary": "Simulation-based testing is widely used to assess the reliability of\nAutonomous Driving Systems (ADS), but its effectiveness is limited by the\noperational design domain (ODD) conditions available in such simulators. To\naddress this limitation, in this work, we explore the integration of generative\nartificial intelligence techniques with physics-based simulators to enhance ADS\nsystem-level testing. Our study evaluates the effectiveness and computational\noverhead of three generative strategies based on diffusion models, namely\ninstruction-editing, inpainting, and inpainting with refinement. Specifically,\nwe assess these techniques' capabilities to produce augmented\nsimulator-generated images of driving scenarios representing new ODDs. We\nemploy a novel automated detector for invalid inputs based on semantic\nsegmentation to ensure semantic preservation and realism of the neural\ngenerated images. We then perform system-level testing to evaluate the ADS's\ngeneralization ability to newly synthesized ODDs. Our findings show that\ndiffusion models help increase the ODD coverage for system-level testing of\nADS. Our automated semantic validator achieved a percentage of false positives\nas low as 3%, retaining the correctness and quality of the generated images for\ntesting. Our approach successfully identified new ADS system failures before\nreal-world testing."}
{"id": "2503.12828", "pdf": "https://arxiv.org/pdf/2503.12828", "abs": "https://arxiv.org/abs/2503.12828", "authors": ["Quang Trung Truong", "Wong Yuk Kwan", "Duc Thanh Nguyen", "Binh-Son Hua", "Sai-Kit Yeung"], "title": "AUTV: Creating Underwater Video Datasets with Pixel-wise Annotations", "categories": ["cs.CE", "cs.CV"], "comment": "under review", "summary": "Underwater video analysis, hampered by the dynamic marine environment and\ncamera motion, remains a challenging task in computer vision. Existing\ntraining-free video generation techniques, learning motion dynamics on the\nframe-by-frame basis, often produce poor results with noticeable motion\ninterruptions and misaligments. To address these issues, we propose AUTV, a\nframework for synthesizing marine video data with pixel-wise annotations. We\ndemonstrate the effectiveness of this framework by constructing two video\ndatasets, namely UTV, a real-world dataset comprising 2,000 video-text pairs,\nand SUTV, a synthetic video dataset including 10,000 videos with segmentation\nmasks for marine objects. UTV provides diverse underwater videos with\ncomprehensive annotations including appearance, texture, camera intrinsics,\nlighting, and animal behavior. SUTV can be used to improve underwater\ndownstream tasks, which are demonstrated in video inpainting and video object\nsegmentation."}
{"id": "2503.13469", "pdf": "https://arxiv.org/pdf/2503.13469", "abs": "https://arxiv.org/abs/2503.13469", "authors": ["Ivan Sviridov", "Konstantin Egorov"], "title": "Conditional Electrocardiogram Generation Using Hierarchical Variational Autoencoders", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "10 pages, 6 figures, 7 tables", "summary": "Cardiovascular diseases (CVDs) are disorders impacting the heart and\ncirculatory system. These disorders are the foremost and continuously\nescalating cause of mortality worldwide. One of the main tasks when working\nwith CVDs is analyzing and identifying pathologies on a 12-lead\nelectrocardiogram (ECG) with a standard 10-second duration. Using machine\nlearning (ML) in automatic ECG analysis increases CVD diagnostics'\navailability, speed, and accuracy. However, the most significant difficulty in\ndeveloping ML models is obtaining a sufficient training dataset. Due to the\nlimitations of medical data usage, such as expensiveness, errors, the ambiguity\nof labels, imbalance of classes, and privacy issues, utilizing synthetic\nsamples depending on specific pathologies bypasses these restrictions and\nimproves algorithm quality. Existing solutions for the conditional generation\nof ECG signals are mainly built on Generative Adversarial Networks (GANs), and\nonly a few papers consider the architectures based on Variational Autoencoders\n(VAEs), showing comparable results in recent works. This paper proposes the\npublicly available conditional Nouveau VAE model for ECG signal generation\n(cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We\nprovide an extensive comparison of the proposed model on various practical\ndownstream tasks, including transfer learning scenarios showing an area under\nthe receiver operating characteristic (AUROC) increase up to 2% surpassing\nGAN-like competitors."}
{"id": "2503.13470", "pdf": "https://arxiv.org/pdf/2503.13470", "abs": "https://arxiv.org/abs/2503.13470", "authors": ["Mohammod N. I. Suvon", "Shuo Zhou", "Prasun C. Tripathi", "Wenrui Fan", "Samer Alabed", "Bishesh Khanal", "Venet Osmani", "Andrew J. Swift", "Chen", "Chen", "Haiping Lu"], "title": "Multimodal Lead-Specific Modeling of ECG for Low-Cost Pulmonary Hypertension Assessment", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": null, "summary": "Pulmonary hypertension (PH) is frequently underdiagnosed in low- and\nmiddle-income countries (LMICs) primarily due to the scarcity of advanced\ndiagnostic tools. Several studies in PH have applied machine learning to\nlow-cost diagnostic tools like 12-lead ECG (12L-ECG), but they mainly focus on\nareas with limited resources, overlooking areas with no diagnostic tools, such\nas rural primary healthcare in LMICs. Recent studies have shown the\neffectiveness of 6-lead ECG (6L-ECG), as a cheaper and portable alternative in\ndetecting various cardiac conditions, but its clinical value for PH detection\nis not well proved. Furthermore, existing methods treat 12L-/6L-ECG as a single\nmodality, capturing only shared features while overlooking lead-specific\nfeatures essential for identifying complex cardiac hemodynamic changes. In this\npaper, we propose Lead-Specific Electrocardiogram Multimodal Variational\nAutoencoder (LS-EMVAE), a model pre-trained on large-population 12L-ECG data\nand fine-tuned on task-specific data (12L-ECG or 6L-ECG). LS-EMVAE models each\n12L-ECG lead as a separate modality and introduces a hierarchical expert\ncomposition using Mixture and Product of Experts for adaptive latent feature\nfusion between lead-specific and shared features. Unlike existing approaches,\nLS-EMVAE makes better predictions on both 12L-ECG and 6L-ECG at inference,\nmaking it an equitable solution for areas with limited or no diagnostic tools.\nWe pre-trained LS-EMVAE on 800,000 publicly available 12L-ECG samples and\nfine-tuned it for two tasks: 1) PH detection and 2) phenotyping\npre-/post-capillary PH, on in-house datasets of 892 and 691 subjects across\n12L-ECG and 6L-ECG settings. Extensive experiments show that LS-EMVAE\noutperforms existing baselines in both ECG settings, while 6L-ECG achieves\nperformance comparable to 12L-ECG, unlocking its potential for global PH\nscreening in areas without diagnostic tools."}
{"id": "2503.13473", "pdf": "https://arxiv.org/pdf/2503.13473", "abs": "https://arxiv.org/abs/2503.13473", "authors": ["Jisoo Hong", "Youngjin Jung", "Jihwan Bae", "Seungho Song", "Sung-Woo Kang"], "title": "Robust Detection of Extremely Thin Lines Using 0.2mm Piano Wire", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "This study developed an algorithm capable of detecting a reference line (a\n0.2 mm thick piano wire) to accurately determine the position of an automated\ninstallation robot within an elevator shaft. A total of 3,245 images were\ncollected from the experimental tower of H Company, the leading elevator\nmanufacturer in South Korea, and the detection performance was evaluated using\nfour experimental approaches (GCH, GSCH, GECH, FCH). During the initial image\nprocessing stage, Gaussian blurring, sharpening filter, embossing filter, and\nFourier Transform were applied, followed by Canny Edge Detection and Hough\nTransform. Notably, the method was developed to accurately extract the\nreference line by averaging the x-coordinates of the lines detected through the\nHough Transform. This approach enabled the detection of the 0.2 mm thick piano\nwire with high accuracy, even in the presence of noise and other interfering\nfactors (e.g., concrete cracks inside the elevator shaft or safety bars for\nfilming equipment). The experimental results showed that Experiment 4 (FCH),\nwhich utilized Fourier Transform in the preprocessing stage, achieved the\nhighest detection rate for the LtoL, LtoR, and RtoL datasets. Experiment\n2(GSCH), which applied Gaussian blurring and a sharpening filter, demonstrated\nsuperior detection performance on the RtoR dataset. This study proposes a\nreference line detection algorithm that enables precise position calculation\nand control of automated robots in elevator shaft installation. Moreover, the\ndeveloped method shows potential for applicability even in confined working\nspaces. Future work aims to develop a line detection algorithm equipped with\nmachine learning-based hyperparameter tuning capabilities."}
{"id": "2503.13477", "pdf": "https://arxiv.org/pdf/2503.13477", "abs": "https://arxiv.org/abs/2503.13477", "authors": ["Ryan Banks", "Vishal Thengane", "María Eugenia Guerrero", "Nelly Maria García-Madueño", "Yunpeng Li", "Hongying Tang", "Akhilanand Chaurasia"], "title": "Periodontal Bone Loss Analysis via Keypoint Detection With Heuristic Post-Processing", "categories": ["q-bio.TO", "cs.AI", "cs.CV", "I.2.1; I.2.10; J.3"], "comment": "31 pages, 7 tables, 5 figures, 3 equations, journal paper submitted\n  to Computers in Biology and Medicine", "summary": "Calculating percentage bone loss is a critical test for periodontal disease\nstaging but is sometimes imprecise and time consuming when manually calculated.\nThis study evaluates the application of a deep learning keypoint and object\ndetection model, YOLOv8-pose, for the automatic identification of localised\nperiodontal bone loss landmarks, conditions and staging. YOLOv8-pose was\nfine-tuned on 193 annotated periapical radiographs. We propose a keypoint\ndetection metric, Percentage of Relative Correct Keypoints (PRCK), which\nnormalises the metric to the average tooth size of teeth in the image. We\npropose a heuristic post-processing module that adjusts certain keypoint\npredictions to align with the edge of the related tooth, using a supporting\ninstance segmentation model trained on an open source auxiliary dataset. The\nmodel can sufficiently detect bone loss keypoints, tooth boxes, and alveolar\nridge resorption, but has insufficient performance at detecting detached\nperiodontal ligament and furcation involvement. The model with post-processing\ndemonstrated a PRCK 0.25 of 0.726 and PRCK 0.05 of 0.401 for keypoint\ndetection, mAP 0.5 of 0.715 for tooth object detection, mesial dice score of\n0.593 for periodontal staging, and dice score of 0.280 for furcation\ninvolvement. Our annotation methodology provides a stage agnostic approach to\nperiodontal disease detection, by ensuring most keypoints are present for each\ntooth in the image, allowing small imbalanced datasets. Our PRCK metric allows\naccurate evaluation of keypoints in dental domains. Our post-processing module\nadjusts predicted keypoints correctly but is dependent on a minimum quality of\nprediction by the pose detection and segmentation models. Code: https://\nanonymous.4open.science/r/Bone-Loss-Keypoint-Detection-Code. Dataset:\nhttps://bit.ly/4hJ3aE7."}
{"id": "2503.13504", "pdf": "https://arxiv.org/pdf/2503.13504", "abs": "https://arxiv.org/abs/2503.13504", "authors": ["Rujia Wang", "Xiangbo Gao", "Hao Xiang", "Runsheng Xu", "Zhengzhong Tu"], "title": "CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative Perception", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative perception enhances each agent perceptual\ncapabilities by sharing sensing information to cooperatively perform robot\nperception tasks. This approach has proven effective in addressing challenges\nsuch as sensor deficiencies, occlusions, and long-range perception. However,\nexisting representative collaborative perception systems transmit intermediate\nfeature maps, such as bird-eye view (BEV) representations, which contain a\nsignificant amount of non-critical information, leading to high communication\nbandwidth requirements. To enhance communication efficiency while preserving\nperception capability, we introduce CoCMT, an object-query-based collaboration\nframework that optimizes communication bandwidth by selectively extracting and\ntransmitting essential features. Within CoCMT, we introduce the Efficient Query\nTransformer (EQFormer) to effectively fuse multi-agent object queries and\nimplement a synergistic deep supervision to enhance the positive reinforcement\nbetween stages, leading to improved overall performance. Experiments on OPV2V\nand V2V4Real datasets show CoCMT outperforms state-of-the-art methods while\ndrastically reducing communication needs. On V2V4Real, our model (Top-50 object\nqueries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods,\nwhile improving AP70 by 1.1 percent. This efficiency breakthrough enables\npractical collaborative perception deployment in bandwidth-constrained\nenvironments without sacrificing detection accuracy."}
{"id": "2503.13555", "pdf": "https://arxiv.org/pdf/2503.13555", "abs": "https://arxiv.org/abs/2503.13555", "authors": ["Zhe Wang", "Aladine Chetouani", "Rachid Jennane"], "title": "Feasibility study for reconstruction of knee MRI from one corresponding X-ray via CNN", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Generally, X-ray, as an inexpensive and popular medical imaging technique, is\nwidely chosen by medical practitioners. With the development of medical\ntechnology, Magnetic Resonance Imaging (MRI), an advanced medical imaging\ntechnique, has already become a supplementary diagnostic option for the\ndiagnosis of KOA. We propose in this paper a deep-learning-based approach for\ngenerating MRI from one corresponding X-ray. Our method uses the hidden\nvariables of a Convolutional Auto-Encoder (CAE) model, trained for\nreconstructing X-ray image, as inputs of a generator model to provide 3D MRI."}
{"id": "2503.13560", "pdf": "https://arxiv.org/pdf/2503.13560", "abs": "https://arxiv.org/abs/2503.13560", "authors": ["Zhaodong Wu", "Qiaochu Zhao", "Ming Hu", "Yulong Li", "Haochen Xue", "Kang Dang", "Zhengyong Jiang", "Angelos Stefanidis", "Qiufeng Wang", "Imran Razzak", "Zongyuan Ge", "Junjun He", "Yu Qiao", "Zhong Zheng", "Feilong Tang", "Jionglong Su"], "title": "MSWAL: 3D Multi-class Segmentation of Whole Abdominal Lesions Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the significantly increasing incidence and prevalence of abdominal\ndiseases, there is a need to embrace greater use of new innovations and\ntechnology for the diagnosis and treatment of patients. Although deep-learning\nmethods have notably been developed to assist radiologists in diagnosing\nabdominal diseases, existing models have the restricted ability to segment\ncommon lesions in the abdomen due to missing annotations for typical abdominal\npathologies in their training datasets. To address the limitation, we introduce\nMSWAL, the first 3D Multi-class Segmentation of the Whole Abdominal Lesions\ndataset, which broadens the coverage of various common lesion types, such as\ngallstones, kidney stones, liver tumors, kidney tumors, pancreatic cancer,\nliver cysts, and kidney cysts. With CT scans collected from 694 patients\n(191,417 slices) of different genders across various scanning phases, MSWAL\ndemonstrates strong robustness and generalizability. The transfer learning\nexperiment from MSWAL to two public datasets, LiTS and KiTS, effectively\ndemonstrates consistent improvements, with Dice Similarity Coefficient (DSC)\nincrease of 3.00% for liver tumors and 0.89% for kidney tumors, demonstrating\nthat the comprehensive annotations and diverse lesion types in MSWAL facilitate\neffective learning across different domains and data distributions.\nFurthermore, we propose Inception nnU-Net, a novel segmentation framework that\neffectively integrates an Inception module with the nnU-Net architecture to\nextract information from different receptive fields, achieving significant\nenhancement in both voxel-level DSC and region-level F1 compared to the\ncutting-edge public algorithms on MSWAL. Our dataset will be released after\nbeing accepted, and the code is publicly released at\nhttps://github.com/tiuxuxsh76075/MSWAL-."}
{"id": "2503.13573", "pdf": "https://arxiv.org/pdf/2503.13573", "abs": "https://arxiv.org/abs/2503.13573", "authors": ["Moises Diaz", "Miguel A. Ferrer", "Juan M. Gil", "Rafael Rodriguez", "Peirong Zhang", "Lianwen Jin"], "title": "Online Signature Verification based on the Lagrange formulation with 2D and 3D robotic models", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Online Signature Verification commonly relies on function-based features,\nsuch as time-sampled horizontal and vertical coordinates, as well as the\npressure exerted by the writer, obtained through a digitizer. Although\ninferring additional information about the writers arm pose, kinematics, and\ndynamics based on digitizer data can be useful, it constitutes a challenge. In\nthis paper, we tackle this challenge by proposing a new set of features based\non the dynamics of online signatures. These new features are inferred through a\nLagrangian formulation, obtaining the sequences of generalized coordinates and\ntorques for 2D and 3D robotic arm models. By combining kinematic and dynamic\nrobotic features, our results demonstrate their significant effectiveness for\nonline automatic signature verification and achieving state-of-the-art results\nwhen integrated into deep learning models."}
{"id": "2503.13581", "pdf": "https://arxiv.org/pdf/2503.13581", "abs": "https://arxiv.org/abs/2503.13581", "authors": ["Beatrice Brown-Mulry", "Rohan Satya Isaac", "Sang Hyup Lee", "Ambika Seth", "KyungJee Min", "Theo Dapamede", "Frank Li", "Aawez Mansuri", "MinJae Woo", "Christian Allison Fauria-Robinson", "Bhavna Paryani", "Judy Wawira Gichoya", "Hari Trivedi"], "title": "Subgroup Performance of a Commercial Digital Breast Tomosynthesis Model for Breast Cancer Detection", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages, 7 figures (plus 7 figures in supplement), 3 tables (plus 1\n  table in supplement)", "summary": "While research has established the potential of AI models for mammography to\nimprove breast cancer screening outcomes, there have not been any detailed\nsubgroup evaluations performed to assess the strengths and weaknesses of\ncommercial models for digital breast tomosynthesis (DBT) imaging. This study\npresents a granular evaluation of the Lunit INSIGHT DBT model on a large\nretrospective cohort of 163,449 screening mammography exams from the Emory\nBreast Imaging Dataset (EMBED). Model performance was evaluated in a binary\ncontext with various negative exam types (162,081 exams) compared against\nscreen detected cancers (1,368 exams) as the positive class. The analysis was\nstratified across demographic, imaging, and pathologic subgroups to identify\npotential disparities. The model achieved an overall AUC of 0.91 (95% CI:\n0.90-0.92) with a precision of 0.08 (95% CI: 0.08-0.08), and a recall of 0.73\n(95% CI: 0.71-0.76). Performance was found to be robust across demographics,\nbut cases with non-invasive cancers (AUC: 0.85, 95% CI: 0.83-0.87),\ncalcifications (AUC: 0.80, 95% CI: 0.78-0.82), and dense breast tissue (AUC:\n0.90, 95% CI: 0.88-0.91) were associated with significantly lower performance\ncompared to other groups. These results highlight the need for detailed\nevaluation of model characteristics and vigilance in considering adoption of\nnew tools for clinical deployment."}
{"id": "2503.13588", "pdf": "https://arxiv.org/pdf/2503.13588", "abs": "https://arxiv.org/abs/2503.13588", "authors": ["Shiran Yuan", "Hao Zhao"], "title": "Next-Scale Autoregressive Models are Zero-Shot Single-Image Object View Synthesizers", "categories": ["cs.GR", "cs.CV"], "comment": "Full codebase, training set, and eval benchmark at\n  https://github.com/Shiran-Yuan/ArchonView", "summary": "Methods based on diffusion backbones have recently revolutionized novel view\nsynthesis (NVS). However, those models require pretrained 2D diffusion\ncheckpoints (e.g., Stable Diffusion) as the basis for geometrical priors. Since\nsuch checkpoints require exorbitant amounts of data and compute to train, this\ngreatly limits the scalability of diffusion-based NVS models. We present\nNext-Scale Autoregression Conditioned by View (ArchonView), a method that\nsignificantly exceeds state-of-the-art methods despite being trained from\nscratch with 3D rendering data only and no 2D pretraining. We achieve this by\nincorporating both global (pose-augmented semantics) and local (multi-scale\nhierarchical encodings) conditioning into a backbone based on the next-scale\nautoregression paradigm. Our model also exhibits robust performance even for\ndifficult camera poses where previous methods fail, and is several times faster\nin inference speed compared to diffusion. We experimentally verify that\nperformance scales with model and dataset size, and conduct extensive\ndemonstration of our method's synthesis quality across several tasks. Our code\nis open-sourced at https://github.com/Shiran-Yuan/ArchonView."}
{"id": "2503.13623", "pdf": "https://arxiv.org/pdf/2503.13623", "abs": "https://arxiv.org/abs/2503.13623", "authors": ["Sai Vijay Kumar Surineela", "Prathyusha Kanakamalla", "Harigovind Harikumar", "Tomojit Ghosh"], "title": "A Convex formulation for linear discriminant analysis", "categories": ["cs.LG", "cs.CV"], "comment": "Total pages 29 including references, six figures, seven tables.\n  Submitted to an Elsevier journal", "summary": "We present a supervised dimensionality reduction technique called Convex\nLinear Discriminant Analysis (ConvexLDA). The proposed model optimizes a\nmulti-objective cost function by balancing two complementary terms. The first\nterm pulls the samples of a class towards its centroid by minimizing a sample's\ndistance from its class-centroid in low dimensional space. The second term\npushes the classes far apart by maximizing their hyperellipsoid scattering\nvolume via the logarithm of the determinant (\\textit{log det}) of the outer\nproduct matrix formed by the low-dimensional class-centroids. Using the\nnegative of the \\textit{log det}, we pose the final cost as a minimization\nproblem, which balances the two terms using a hyper-parameter $\\lambda$. We\ndemonstrate that the cost function is convex. Unlike Fisher LDA, the proposed\nmethod doesn't require to compute the inverse of a matrix, hence avoiding any\nill-conditioned problem where data dimension is very high, e.g. RNA-seq data.\nConvexLDA doesn't require pair-wise distance calculation, making it faster and\nmore easily scalable. Moreover, the convex nature of the cost function ensures\nglobal optimality, enhancing the reliability of the learned embedding. Our\nexperimental evaluation demonstrates that ConvexLDA outperforms several popular\nlinear discriminant analysis (LDA)-based methods on a range of high-dimensional\nbiological data, image data sets, etc."}
{"id": "2503.13896", "pdf": "https://arxiv.org/pdf/2503.13896", "abs": "https://arxiv.org/abs/2503.13896", "authors": ["Yi Yang", "Xuran Zhao", "H. Charles Zhao", "Shumin Yuan", "Samuel M. Bateman", "Tiffany A. Huang", "Chris Beall", "Will Maddern"], "title": "Evaluating Global Geo-alignment for Precision Learned Autonomous Vehicle Localization using Aerial Data", "categories": ["cs.RO", "cs.CV", "I.2.9"], "comment": "8 pages, 7 figures, accepted by International Conference on Robotics\n  and Automation (ICRA) 2025", "summary": "Recently there has been growing interest in the use of aerial and satellite\nmap data for autonomous vehicles, primarily due to its potential for\nsignificant cost reduction and enhanced scalability. Despite the advantages,\naerial data also comes with challenges such as a sensor-modality gap and a\nviewpoint difference gap. Learned localization methods have shown promise for\novercoming these challenges to provide precise metric localization for\nautonomous vehicles. Most learned localization methods rely on coarsely aligned\nground truth, or implicit consistency-based methods to learn the localization\ntask -- however, in this paper we find that improving the alignment between\naerial data and autonomous vehicle sensor data at training time is critical to\nthe performance of a learning-based localization system. We compare two data\nalignment methods using a factor graph framework and, using these methods, we\nthen evaluate the effects of closely aligned ground truth on learned\nlocalization accuracy through ablation studies. Finally, we evaluate a learned\nlocalization system using the data alignment methods on a comprehensive\n(1600km) autonomous vehicle dataset and demonstrate localization error below\n0.3m and 0.5$^{\\circ}$ sufficient for autonomous vehicle applications."}
{"id": "2503.13928", "pdf": "https://arxiv.org/pdf/2503.13928", "abs": "https://arxiv.org/abs/2503.13928", "authors": ["Santanu Roy", "Ashvath Suresh", "Archit Gupta", "Shubhi Tiwari", "Palak Sahu", "Prashant Adhikari", "Yuvraj S. Shekhawat"], "title": "Fibonacci-Net: A Lightweight CNN model for Automatic Brain Tumor Classification", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This research proposes a very lightweight model \"Fibonacci-Net\" along with a\nnovel pooling technique, for automatic brain tumor classification from\nimbalanced Magnetic Resonance Imaging (MRI) datasets. Automatic brain tumor\ndetection from MRI dataset has garnered significant attention in the research\ncommunity, since the inception of Convolutional Neural Network (CNN) models.\nHowever, the performance of conventional CNN models is hindered due to class\nimbalance problems. The novelties of this work are as follows: (I) A\nlightweight CNN model is proposed in which the number of filters in different\nconvolutional layers is chosen according to the numbers of Fibonacci series.\n(II) In the last two blocks of the proposed model, depth-wise separable\nconvolution (DWSC) layers are employed to considerably reduce the computational\ncomplexity of the model. (III) Two parallel concatenations (or, skip\nconnections) are deployed from 2nd to 4th, and 3rd to 5th convolutional block\nin the proposed Fibonacci-Net. This skip connection encompasses a novel\nAverage-2Max pooling layer that produces two stacks of convoluted output,\nhaving a bit different statistics. Therefore, this parallel concatenation block\nworks as an efficient feature augmenter inside the model, thus, automatically\nalleviating the class imbalance problem to a certain extent. For validity\npurpose, we have implemented the proposed framework on three MRI datasets which\nare highly class-imbalanced. (a) The first dataset has four classes, i.e.,\nglioma tumor, meningioma tumor, pituitary tumor, and no-tumor. (b) Second and\nthird MRI datasets have 15 and 44 classes respectively. Experimental results\nreveal that, after employing the proposed Fibonacci-Net we have achieved 96.2%\naccuracy, 97.17% precision, 95.9% recall, 96.5% F1 score, and 99.9% specificity\non the most challenging ``44-classes MRI dataset''."}
{"id": "2503.13961", "pdf": "https://arxiv.org/pdf/2503.13961", "abs": "https://arxiv.org/abs/2503.13961", "authors": ["Minye Wu", "Haizhao Dai", "Kaixin Yao", "Tinne Tuytelaars", "Jingyi Yu"], "title": "BG-Triangle: Bézier Gaussian Triangle for 3D Vectorization and Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Differentiable rendering enables efficient optimization by allowing gradients\nto be computed through the rendering process, facilitating 3D reconstruction,\ninverse rendering and neural scene representation learning. To ensure\ndifferentiability, existing solutions approximate or re-formulate traditional\nrendering operations using smooth, probabilistic proxies such as volumes or\nGaussian primitives. Consequently, they struggle to preserve sharp edges due to\nthe lack of explicit boundary definitions. We present a novel hybrid\nrepresentation, B\\'ezier Gaussian Triangle (BG-Triangle), that combines\nB\\'ezier triangle-based vector graphics primitives with Gaussian-based\nprobabilistic models, to maintain accurate shape modeling while conducting\nresolution-independent differentiable rendering. We present a robust and\neffective discontinuity-aware rendering technique to reduce uncertainties at\nobject boundaries. We also employ an adaptive densification and pruning scheme\nfor efficient training while reliably handling level-of-detail (LoD)\nvariations. Experiments show that BG-Triangle achieves comparable rendering\nquality as 3DGS but with superior boundary preservation. More importantly,\nBG-Triangle uses a much smaller number of primitives than its alternatives,\nshowcasing the benefits of vectorized graphics primitives and the potential to\nbridge the gap between classic and emerging representations."}
{"id": "2503.13987", "pdf": "https://arxiv.org/pdf/2503.13987", "abs": "https://arxiv.org/abs/2503.13987", "authors": ["Yaxiong Chen", "Yujie Wang", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Striving for Simplicity: Simple Yet Effective Prior-Aware Pseudo-Labeling for Semi-Supervised Ultrasound Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2024", "summary": "Medical ultrasound imaging is ubiquitous, but manual analysis struggles to\nkeep pace. Automated segmentation can help but requires large labeled datasets,\nwhich are scarce. Semi-supervised learning leveraging both unlabeled and\nlimited labeled data is a promising approach. State-of-the-art methods use\nconsistency regularization or pseudo-labeling but grow increasingly complex.\nWithout sufficient labels, these models often latch onto artifacts or allow\nanatomically implausible segmentations. In this paper, we present a simple yet\neffective pseudo-labeling method with an adversarially learned shape prior to\nregularize segmentations. Specifically, we devise an encoder-twin-decoder\nnetwork where the shape prior acts as an implicit shape model, penalizing\nanatomically implausible but not ground-truth-deviating predictions. Without\nbells and whistles, our simple approach achieves state-of-the-art performance\non two benchmarks under different partition protocols. We provide a strong\nbaseline for future semi-supervised medical image segmentation. Code is\navailable at https://github.com/WUTCM-Lab/Shape-Prior-Semi-Seg."}
{"id": "2503.13994", "pdf": "https://arxiv.org/pdf/2503.13994", "abs": "https://arxiv.org/abs/2503.13994", "authors": ["Kaixin Shen", "Ruijie Quan", "Jiaxu Miao", "Jun Xiao", "Yi Yang"], "title": "TarPro: Targeted Protection against Malicious Image Editing", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "The rapid advancement of image editing techniques has raised concerns about\ntheir misuse for generating Not-Safe-for-Work (NSFW) content. This necessitates\na targeted protection mechanism that blocks malicious edits while preserving\nnormal editability. However, existing protection methods fail to achieve this\nbalance, as they indiscriminately disrupt all edits while still allowing some\nharmful content to be generated. To address this, we propose TarPro, a targeted\nprotection framework that prevents malicious edits while maintaining benign\nmodifications. TarPro achieves this through a semantic-aware constraint that\nonly disrupts malicious content and a lightweight perturbation generator that\nproduces a more stable, imperceptible, and robust perturbation for image\nprotection. Extensive experiments demonstrate that TarPro surpasses existing\nmethods, achieving a high protection efficacy while ensuring minimal impact on\nnormal edits. Our results highlight TarPro as a practical solution for secure\nand controlled image editing."}
{"id": "2503.14024", "pdf": "https://arxiv.org/pdf/2503.14024", "abs": "https://arxiv.org/abs/2503.14024", "authors": ["Pingting Hao", "Kunpeng Liu", "Wanfu Gao"], "title": "Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label Feature Selection", "categories": ["cs.LG", "cs.CV"], "comment": "9 pages,5 figures, accept in AAAI 25", "summary": "In recent years, multi-view multi-label learning (MVML) has gained popularity\ndue to its close resemblance to real-world scenarios. However, the challenge of\nselecting informative features to ensure both performance and efficiency\nremains a significant question in MVML. Existing methods often extract\ninformation separately from the consistency part and the complementary part,\nwhich may result in noise due to unclear segmentation. In this paper, we\npropose a unified model constructed from the perspective of global-view\nreconstruction. Additionally, while feature selection methods can discern the\nimportance of features, they typically overlook the uncertainty of samples,\nwhich is prevalent in realistic scenarios. To address this, we incorporate the\nperception of sample uncertainty during the reconstruction process to enhance\ntrustworthiness. Thus, the global-view is reconstructed through the graph\nstructure between samples, sample confidence, and the view relationship. The\naccurate mapping is established between the reconstructed view and the label\nmatrix. Experimental results demonstrate the superior performance of our method\non multi-view datasets."}
{"id": "2503.14034", "pdf": "https://arxiv.org/pdf/2503.14034", "abs": "https://arxiv.org/abs/2503.14034", "authors": ["Xi Shen", "Julian Gamboa", "Tabassom Hamidfar", "Shamima Mitu", "Selim M. Shahriar"], "title": "Shift, Scale and Rotation Invariant Multiple Object Detection using Balanced Joint Transform Correlator", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The Polar Mellin Transform (PMT) is a well-known technique that converts\nimages into shift, scale and rotation invariant signatures for object detection\nusing opto-electronic correlators. However, this technique cannot be properly\napplied when there are multiple targets in a single input. Here, we propose a\nSegmented PMT (SPMT) that extends this methodology for cases where multiple\nobjects are present within the same frame. Simulations show that this SPMT can\nbe integrated into an opto-electronic joint transform correlator to create a\ncorrelation system capable of detecting multiple objects simultaneously,\npresenting robust detection capabilities across various transformation\nconditions, with remarkable discrimination between matching and non-matching\ntargets."}
{"id": "2503.14040", "pdf": "https://arxiv.org/pdf/2503.14040", "abs": "https://arxiv.org/abs/2503.14040", "authors": ["Binjie Liu", "Lina Liu", "Sanyi Zhang", "Songen Gu", "Yihao Zhi", "Tianyi Zhu", "Lei Yang", "Long Ye"], "title": "MAG: Multi-Modal Aligned Autoregressive Co-Speech Gesture Generation without Vector Quantization", "categories": ["cs.GR", "cs.CV", "cs.SD"], "comment": null, "summary": "This work focuses on full-body co-speech gesture generation. Existing methods\ntypically employ an autoregressive model accompanied by vector-quantized tokens\nfor gesture generation, which results in information loss and compromises the\nrealism of the generated gestures. To address this, inspired by the natural\ncontinuity of real-world human motion, we propose MAG, a novel multi-modal\naligned framework for high-quality and diverse co-speech gesture synthesis\nwithout relying on discrete tokenization. Specifically, (1) we introduce a\nmotion-text-audio-aligned variational autoencoder (MTA-VAE), which leverages\npre-trained WavCaps' text and audio embeddings to enhance both semantic and\nrhythmic alignment with motion, ultimately producing more realistic gestures.\n(2) Building on this, we propose a multimodal masked autoregressive model\n(MMAG) that enables autoregressive modeling in continuous motion embeddings\nthrough diffusion without vector quantization. To further ensure multi-modal\nconsistency, MMAG incorporates a hybrid granularity audio-text fusion block,\nwhich serves as conditioning for diffusion process. Extensive experiments on\ntwo benchmark datasets demonstrate that MAG achieves stateof-the-art\nperformance both quantitatively and qualitatively, producing highly realistic\nand diverse co-speech gestures.The code will be released to facilitate future\nresearch."}
{"id": "2503.14051", "pdf": "https://arxiv.org/pdf/2503.14051", "abs": "https://arxiv.org/abs/2503.14051", "authors": ["Tianshu Wu", "Jiyao Zhang", "Shiqian Liang", "Zhengxiao Han", "Hao Dong"], "title": "Foundation Feature-Driven Online End-Effector Pose Estimation: A Marker-Free and Learning-Free Approach", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate transformation estimation between camera space and robot space is\nessential. Traditional methods using markers for hand-eye calibration require\noffline image collection, limiting their suitability for online\nself-calibration. Recent learning-based robot pose estimation methods, while\nadvancing online calibration, struggle with cross-robot generalization and\nrequire the robot to be fully visible. This work proposes a Foundation\nfeature-driven online End-Effector Pose Estimation (FEEPE) algorithm,\ncharacterized by its training-free and cross end-effector generalization\ncapabilities. Inspired by the zero-shot generalization capabilities of\nfoundation models, FEEPE leverages pre-trained visual features to estimate\n2D-3D correspondences derived from the CAD model and target image, enabling 6D\npose estimation via the PnP algorithm. To resolve ambiguities from partial\nobservations and symmetry, a multi-historical key frame enhanced pose\noptimization algorithm is introduced, utilizing temporal information for\nimproved accuracy. Compared to traditional hand-eye calibration, FEEPE enables\nmarker-free online calibration. Unlike robot pose estimation, it generalizes\nacross robots and end-effectors in a training-free manner. Extensive\nexperiments demonstrate its superior flexibility, generalization, and\nperformance."}
{"id": "2503.14094", "pdf": "https://arxiv.org/pdf/2503.14094", "abs": "https://arxiv.org/abs/2503.14094", "authors": ["Roman Denkin", "Orcun Goksel"], "title": "Image-Based Metrics in Ultrasound for Estimation of Global Speed-of-Sound", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "Accurate speed-of-sound (SoS) estimation is crucial for ultrasound image\nformation, yet conventional systems often rely on an assumed value for imaging.\nWhile several methods exist for SoS estimation, they typically depend on\ncomplex physical models of acoustic propagation. We propose to leverage\nconventional image analysis techniques and metrics, as a novel and simple\napproach to estimate tissue SoS. We study eleven metrics in three categories\nfor assessing image quality, image similarity and multi-frame variation, by\ntesting them in numerical simulations and phantom experiments. Among\nsingle-frame image quality metrics, conventional Focus and our proposed\nSmoothed Threshold Tenengrad metrics achieved satisfactory accuracy, however\nonly when applied to compounded images. Image quality metrics were largely\nsurpassed by various image comparison metrics, which exhibited errors\nconsistently under 8 m/s even applied to a single pair of images. Particularly,\nMean Square Error is a computationally efficient alternative for global\nestimation. Mutual Information and Correlation are found to be robust to\nprocessing small image segments, making them suitable, e.g., for multi-layer\nSoS estimation. The above metrics do not require access to raw channel data as\nthey can operate on post-beamformed data, and in the case of image quality\nmetrics they can operate on B-mode images, given that the beamforming SoS can\nbe controlled for beamforming using a multitude of values. These image analysis\nbased SoS estimation methods offer a computationally efficient and\ndata-accessible alternative to conventional physics-based methods, with\npotential extensions to layered or local SoS imaging."}
{"id": "2503.14182", "pdf": "https://arxiv.org/pdf/2503.14182", "abs": "https://arxiv.org/abs/2503.14182", "authors": ["Bozhou Zhang", "Nan Song", "Xin Jin", "Li Zhang"], "title": "Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning", "categories": ["cs.RO", "cs.CV"], "comment": "CVPR 2025", "summary": "End-to-end autonomous driving unifies tasks in a differentiable framework,\nenabling planning-oriented optimization and attracting growing attention.\nCurrent methods aggregate historical information either through dense\nhistorical bird's-eye-view (BEV) features or by querying a sparse memory bank,\nfollowing paradigms inherited from detection. However, we argue that these\nparadigms either omit historical information in motion planning or fail to\nalign with its multi-step nature, which requires predicting or planning\nmultiple future time steps. In line with the philosophy of future is a\ncontinuation of past, we propose BridgeAD, which reformulates motion and\nplanning queries as multi-step queries to differentiate the queries for each\nfuture time step. This design enables the effective use of historical\nprediction and planning by applying them to the appropriate parts of the\nend-to-end system based on the time steps, which improves both perception and\nmotion planning. Specifically, historical queries for the current frame are\ncombined with perception, while queries for future frames are integrated with\nmotion planning. In this way, we bridge the gap between past and future by\naggregating historical insights at every time step, enhancing the overall\ncoherence and accuracy of the end-to-end autonomous driving pipeline. Extensive\nexperiments on the nuScenes dataset in both open-loop and closed-loop settings\ndemonstrate that BridgeAD achieves state-of-the-art performance."}
{"id": "2503.14189", "pdf": "https://arxiv.org/pdf/2503.14189", "abs": "https://arxiv.org/abs/2503.14189", "authors": ["Yongqi Li", "Lu Yang", "Jian Wang", "Runyang You", "Wenjie Li", "Liqiang Nie"], "title": "Towards Harmless Multimodal Assistants with Blind Preference Optimization", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in multimodal understanding, reasoning, and interaction. Given the\nextensive applications of MLLMs, the associated safety issues have become\nincreasingly critical. Due to the effectiveness of preference optimization in\naligning MLLMs with human preferences, there is an urgent need for\nsafety-related preference data for MLLMs. To address this, we construct the\nMMSafe-PO preference dataset towards harmless multimodal assistants, featuring\nmultimodal instructions, the conversational format, and ranked paired responses\nfrom human feedback. We also identify two insightful observations: modality\nco-defense and modality cheating, which illustrate that MLLMs possess a certain\nlevel of inherent defense while still presenting unique safety challenges.\nBased on these observations, we propose the Blind Preference Optimization (BPO)\napproach. Comprehensive experiments on three benchmarks show that BPO\neffectively enhances the safety capabilities of MLLMs. Notably, BPO\nsignificantly improves the safety rate of the base MLLM by 45.0%, outperforming\nthe DPO approach. Additionally, applying BPO to the MMSafe-PO dataset greatly\nreduces the base MLLM's unsafe rate on other safety benchmarks (14.5% on\nMM-SafetyBench and 82.9% on HarmEval, demonstrating the effectiveness and\nrobustness of both the dataset and the approach. We release code and data at\nhttps://lu-yang666.github.io/MMsafe-PO-Web/."}
{"id": "2503.14195", "pdf": "https://arxiv.org/pdf/2503.14195", "abs": "https://arxiv.org/abs/2503.14195", "authors": ["Rui Cao", "Wei Tu", "Dongsheng Chen", "Wenyu Zhang"], "title": "Mapping Urban Villages in China: Progress and Challenges", "categories": ["cs.DB", "cs.CV"], "comment": "Updated review at\n  https://github.com/rui-research/urban-village-review", "summary": "The shift toward high-quality urbanization has brought increased attention to\nthe issue of \"urban villages\", which has become a prominent social problem in\nChina. However, there is a lack of available geospatial data on urban villages,\nmaking it crucial to prioritize urban village mapping. In order to assess the\ncurrent progress in urban village mapping and identify challenges and future\ndirections, we have conducted a comprehensive review, which to the best of our\nknowledge is the first of its kind in this field. Our review begins by\nproviding a clear context for urban villages and elaborating the method for\nliterature review, then summarizes the study areas, data sources, and\napproaches used for urban village mapping in China. We also address the\nchallenges and future directions for further research. Through thorough\ninvestigation, we find that current studies only cover very limited study areas\nand periods and lack sufficient investigation into the scalability,\ntransferability, and interpretability of identification approaches due to the\nchallenges in concept fuzziness and variances, spatial heterogeneity and\nvariances of urban villages, and data availability. Future research can\ncomplement and further the current research in the following potential\ndirections in order to achieve large-area mapping across the whole nation..."}
{"id": "2503.14229", "pdf": "https://arxiv.org/pdf/2503.14229", "abs": "https://arxiv.org/abs/2503.14229", "authors": ["Yifei Dong", "Fengyi Wu", "Qi He", "Heng Li", "Minghan Li", "Zebang Cheng", "Yuxuan Zhou", "Jingdong Sun", "Qi Dai", "Zhi-Qi Cheng", "Alexander G Hauptmann"], "title": "HA-VLN: A Benchmark for Human-Aware Navigation in Discrete-Continuous Environments with Dynamic Multi-Human Interactions, Real-World Validation, and an Open Leaderboard", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "27 pages, website: https://ha-vln-project.vercel.app/", "summary": "Vision-and-Language Navigation (VLN) systems often focus on either discrete\n(panoramic) or continuous (free-motion) paradigms alone, overlooking the\ncomplexities of human-populated, dynamic environments. We introduce a unified\nHuman-Aware VLN (HA-VLN) benchmark that merges these paradigms under explicit\nsocial-awareness constraints. Our contributions include: 1. A standardized task\ndefinition that balances discrete-continuous navigation with personal-space\nrequirements; 2. An enhanced human motion dataset (HAPS 2.0) and upgraded\nsimulators capturing realistic multi-human interactions, outdoor contexts, and\nrefined motion-language alignment; 3. Extensive benchmarking on 16,844\nhuman-centric instructions, revealing how multi-human dynamics and partial\nobservability pose substantial challenges for leading VLN agents; 4. Real-world\nrobot tests validating sim-to-real transfer in crowded indoor spaces; and 5. A\npublic leaderboard supporting transparent comparisons across discrete and\ncontinuous tasks. Empirical results show improved navigation success and fewer\ncollisions when social context is integrated, underscoring the need for\nhuman-centric design. By releasing all datasets, simulators, agent code, and\nevaluation tools, we aim to advance safer, more capable, and socially\nresponsible VLN research."}
{"id": "2503.14304", "pdf": "https://arxiv.org/pdf/2503.14304", "abs": "https://arxiv.org/abs/2503.14304", "authors": ["Yuheng Li", "Mingzhe Hu", "Richard L. J. Qiu", "Maria Thor", "Andre Williams", "Deborah Marshall", "Xiaofeng Yang"], "title": "RoMedFormer: A Rotary-Embedding Transformer Foundation Model for 3D Genito-Pelvic Structure Segmentation in MRI and CT", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Deep learning-based segmentation of genito-pelvic structures in MRI and CT is\ncrucial for applications such as radiation therapy, surgical planning, and\ndisease diagnosis. However, existing segmentation models often struggle with\ngeneralizability across imaging modalities, and anatomical variations. In this\nwork, we propose RoMedFormer, a rotary-embedding transformer-based foundation\nmodel designed for 3D female genito-pelvic structure segmentation in both MRI\nand CT. RoMedFormer leverages self-supervised learning and rotary positional\nembeddings to enhance spatial feature representation and capture long-range\ndependencies in 3D medical data. We pre-train our model using a diverse dataset\nof 3D MRI and CT scans and fine-tune it for downstream segmentation tasks.\nExperimental results demonstrate that RoMedFormer achieves superior performance\nsegmenting genito-pelvic organs. Our findings highlight the potential of\ntransformer-based architectures in medical image segmentation and pave the way\nfor more transferable segmentation frameworks."}
{"id": "2503.14331", "pdf": "https://arxiv.org/pdf/2503.14331", "abs": "https://arxiv.org/abs/2503.14331", "authors": ["Johannes Huemer", "Markus Murschitz", "Matthias Schörghuber", "Lukas Reisinger", "Thomas Kadiofsky", "Christoph Weidinger", "Mario Niedermeyer", "Benedikt Widy", "Marcel Zeilinger", "Csaba Beleznai", "Tobias Glück", "Andreas Kugi", "Patrik Zips"], "title": "ADAPT: An Autonomous Forklift for Construction Site Operation", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Efficient material logistics play a critical role in controlling costs and\nschedules in the construction industry. However, manual material handling\nremains prone to inefficiencies, delays, and safety risks. Autonomous forklifts\noffer a promising solution to streamline on-site logistics, reducing reliance\non human operators and mitigating labor shortages. This paper presents the\ndevelopment and evaluation of the Autonomous Dynamic All-terrain Pallet\nTransporter (ADAPT), a fully autonomous off-road forklift designed for\nconstruction environments. Unlike structured warehouse settings, construction\nsites pose significant challenges, including dynamic obstacles, unstructured\nterrain, and varying weather conditions. To address these challenges, our\nsystem integrates AI-driven perception techniques with traditional approaches\nfor decision making, planning, and control, enabling reliable operation in\ncomplex environments. We validate the system through extensive real-world\ntesting, comparing its long-term performance against an experienced human\noperator across various weather conditions. We also provide a comprehensive\nanalysis of challenges and key lessons learned, contributing to the advancement\nof autonomous heavy machinery. Our findings demonstrate that autonomous outdoor\nforklifts can operate near human-level performance, offering a viable path\ntoward safer and more efficient construction logistics."}
{"id": "2503.14343", "pdf": "https://arxiv.org/pdf/2503.14343", "abs": "https://arxiv.org/abs/2503.14343", "authors": ["Yali Bi", "Enyu Che", "Yinan Chen", "Yuanpeng He", "Jingwei Qu"], "title": "Multi-Prototype Embedding Refinement for Semi-Supervised Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation aims to identify anatomical structures at the\nvoxel-level. Segmentation accuracy relies on distinguishing voxel differences.\nCompared to advancements achieved in studies of the inter-class variance, the\nintra-class variance receives less attention. Moreover, traditional linear\nclassifiers, limited by a single learnable weight per class, struggle to\ncapture this finer distinction. To address the above challenges, we propose a\nMulti-Prototype-based Embedding Refinement method for semi-supervised medical\nimage segmentation. Specifically, we design a multi-prototype-based\nclassification strategy, rethinking the segmentation from the perspective of\nstructural relationships between voxel embeddings. The intra-class variations\nare explored by clustering voxels along the distribution of multiple prototypes\nin each class. Next, we introduce a consistency constraint to alleviate the\nlimitation of linear classifiers. This constraint integrates different\nclassification granularities from a linear classifier and the proposed\nprototype-based classifier. In the thorough evaluation on two popular\nbenchmarks, our method achieves superior performance compared with\nstate-of-the-art methods. Code is available at\nhttps://github.com/Briley-byl123/MPER."}
{"id": "2503.14354", "pdf": "https://arxiv.org/pdf/2503.14354", "abs": "https://arxiv.org/abs/2503.14354", "authors": ["Omkar Kokane", "Gopal Raut", "Salim Ullah", "Mukul Lokhande", "Adam Teman", "Akash Kumar", "Santosh Kumar Vishvakarma"], "title": "Retrospective: A CORDIC Based Configurable Activation Function for NN Applications", "categories": ["cs.AR", "cs.AI", "cs.CV", "cs.ET", "eess.IV"], "comment": null, "summary": "A CORDIC-based configuration for the design of Activation Functions (AF) was\npreviously suggested to accelerate ASIC hardware design for\nresource-constrained systems by providing functional reconfigurability. Since\nits introduction, this new approach for neural network acceleration has gained\nwidespread popularity, influencing numerous designs for activation functions in\nboth academic and commercial AI processors. In this retrospective analysis, we\nexplore the foundational aspects of this initiative, summarize key developments\nover recent years, and introduce the DA-VINCI AF tailored for the evolving\nneeds of AI applications. This new generation of dynamically configurable and\nprecision-adjustable activation function cores promise greater adaptability for\na range of activation functions in AI workloads, including Swish, SoftMax,\nSeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously\npresented design has been optimized for MAC, Sigmoid, and Tanh functionalities\nand incorporated into ReLU AFs, culminating in an accumulative NEURIC compute\nunit. These enhancements position NEURIC as a fundamental component in the\nresource-efficient vector engine for the realization of AI accelerators that\nfocus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results\n(QoR) of 98.5%."}
{"id": "2503.14377", "pdf": "https://arxiv.org/pdf/2503.14377", "abs": "https://arxiv.org/abs/2503.14377", "authors": ["Negin Baghbanzadeh", "Adibvafa Fallahpour", "Yasaman Parhizkar", "Franklin Ogidi", "Shuvendu Roy", "Sajad Ashkezari", "Vahid Reza Khazaie", "Michael Colacci", "Ali Etemad", "Arash Afkanpour", "Elham Dolatabadi"], "title": "Advancing Medical Representation Learning Through High-Quality Data", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Despite the growing scale of medical Vision-Language datasets, the impact of\ndataset quality on model performance remains under-explored. We introduce\nOpen-PMC, a high-quality medical dataset from PubMed Central, containing 2.2\nmillion image-text pairs, enriched with image modality annotations, subfigures,\nand summarized in-text references. Notably, the in-text references provide\nricher medical context, extending beyond the abstract information typically\nfound in captions. Through extensive experiments, we benchmark Open-PMC against\nlarger datasets across retrieval and zero-shot classification tasks. Our\nresults show that dataset quality-not just size-drives significant performance\ngains. We complement our benchmark with an in-depth analysis of feature\nrepresentation. Our findings highlight the crucial role of data curation\nquality in advancing multimodal medical AI. We release Open-PMC, along with the\ntrained models and our codebase."}
{"id": "2503.14395", "pdf": "https://arxiv.org/pdf/2503.14395", "abs": "https://arxiv.org/abs/2503.14395", "authors": ["Jing Wang", "Ruirui Liu", "Yu Lei", "Michael J. Baine", "Tian Liu", "Yang Lei"], "title": "Weakly Supervised Spatial Implicit Neural Representation Learning for 3D MRI-Ultrasound Deformable Image Registration in HDR Prostate Brachytherapy", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Purpose: Accurate 3D MRI-ultrasound (US) deformable registration is critical\nfor real-time guidance in high-dose-rate (HDR) prostate brachytherapy. We\npresent a weakly supervised spatial implicit neural representation (SINR)\nmethod to address modality differences and pelvic anatomy challenges.\n  Methods: The framework uses sparse surface supervision from MRI/US\nsegmentations instead of dense intensity matching. SINR models deformations as\ncontinuous spatial functions, with patient-specific surface priors guiding a\nstationary velocity field for biologically plausible deformations. Validation\nincluded 20 public Prostate-MRI-US-Biopsy cases and 10 institutional HDR cases,\nevaluated via Dice similarity coefficient (DSC), mean surface distance (MSD),\nand 95% Hausdorff distance (HD95).\n  Results: The proposed method achieved robust registration. For the public\ndataset, prostate DSC was $0.93 \\pm 0.05$, MSD $0.87 \\pm 0.10$ mm, and HD95\n$1.58 \\pm 0.37$ mm. For the institutional dataset, prostate CTV achieved DSC\n$0.88 \\pm 0.09$, MSD $1.21 \\pm 0.38$ mm, and HD95 $2.09 \\pm 1.48$ mm. Bladder\nand rectum performance was lower due to ultrasound's limited field of view.\nVisual assessments confirmed accurate alignment with minimal discrepancies.\n  Conclusion: This study introduces a novel weakly supervised SINR-based\napproach for 3D MRI-US deformable registration. By leveraging sparse surface\nsupervision and spatial priors, it achieves accurate, robust, and\ncomputationally efficient registration, enhancing real-time image guidance in\nHDR prostate brachytherapy and improving treatment precision."}
{"id": "2503.14475", "pdf": "https://arxiv.org/pdf/2503.14475", "abs": "https://arxiv.org/abs/2503.14475", "authors": ["Umar Farooq", "Jean-Yves Guillemaut", "Adrian Hilton", "Marco Volino"], "title": "Optimized 3D Gaussian Splatting using Coarse-to-Fine Image Frequency Modulation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "The field of Novel View Synthesis has been revolutionized by 3D Gaussian\nSplatting (3DGS), which enables high-quality scene reconstruction that can be\nrendered in real-time. 3DGS-based techniques typically suffer from high GPU\nmemory and disk storage requirements which limits their practical application\non consumer-grade devices. We propose Opti3DGS, a novel frequency-modulated\ncoarse-to-fine optimization framework that aims to minimize the number of\nGaussian primitives used to represent a scene, thus reducing memory and storage\ndemands. Opti3DGS leverages image frequency modulation, initially enforcing a\ncoarse scene representation and progressively refining it by modulating\nfrequency details in the training images. On the baseline 3DGS, we demonstrate\nan average reduction of 62% in Gaussians, a 40% reduction in the training GPU\nmemory requirements and a 20% reduction in optimization time without\nsacrificing the visual quality. Furthermore, we show that our method integrates\nseamlessly with many 3DGS-based techniques, consistently reducing the number of\nGaussian primitives while maintaining, and often improving, visual quality.\nAdditionally, Opti3DGS inherently produces a level-of-detail scene\nrepresentation at no extra cost, a natural byproduct of the optimization\npipeline. Results and code will be made publicly available."}
{"id": "2503.14485", "pdf": "https://arxiv.org/pdf/2503.14485", "abs": "https://arxiv.org/abs/2503.14485", "authors": ["Yiqun Mei", "Mingming He", "Li Ma", "Julien Philip", "Wenqi Xian", "David M George", "Xueming Yu", "Gabriel Dedic", "Ahmet Levent Taşel", "Ning Yu", "Vishal M. Patel", "Paul Debevec"], "title": "Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset", "categories": ["cs.GR", "cs.CV"], "comment": "CVPR 2025", "summary": "Video portrait relighting remains challenging because the results need to be\nboth photorealistic and temporally stable. This typically requires a strong\nmodel design that can capture complex facial reflections as well as intensive\ntraining on a high-quality paired video dataset, such as dynamic\none-light-at-a-time (OLAT). In this work, we introduce Lux Post Facto, a novel\nportrait video relighting method that produces both photorealistic and\ntemporally consistent lighting effects. From the model side, we design a new\nconditional video diffusion model built upon state-of-the-art pre-trained video\ndiffusion model, alongside a new lighting injection mechanism to enable precise\ncontrol. This way we leverage strong spatial and temporal generative capability\nto generate plausible solutions to the ill-posed relighting problem. Our\ntechnique uses a hybrid dataset consisting of static expression OLAT data and\nin-the-wild portrait performance videos to jointly learn relighting and\ntemporal modeling. This avoids the need to acquire paired video data in\ndifferent lighting conditions. Our extensive experiments show that our model\nproduces state-of-the-art results both in terms of photorealism and temporal\nconsistency."}
