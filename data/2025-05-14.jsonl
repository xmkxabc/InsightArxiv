{"id": "2505.07843", "pdf": "https://arxiv.org/pdf/2505.07843", "abs": "https://arxiv.org/abs/2505.07843", "authors": ["HsiaoYuan Hsu", "Yuxin Peng"], "title": "PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation", "categories": ["cs.GR", "cs.LG"], "comment": "Accepted to CVPR 2025. Code and dataset are available at\n  https://thekinsley.github.io/PosterO/", "summary": "In poster design, content-aware layout generation is crucial for\nautomatically arranging visual-textual elements on the given image. With\nlimited training data, existing work focused on image-centric enhancement.\nHowever, this neglects the diversity of layouts and fails to cope with\nshape-variant elements or diverse design intents in generalized settings. To\nthis end, we proposed a layout-centric approach that leverages layout knowledge\nimplicit in large language models (LLMs) to create posters for omnifarious\npurposes, hence the name PosterO. Specifically, it structures layouts from\ndatasets as trees in SVG language by universal shape, design intent\nvectorization, and hierarchical node representation. Then, it applies LLMs\nduring inference to predict new layout trees by in-context learning with\nintent-aligned example selection. After layout trees are generated, we can\nseamlessly realize them into poster designs by editing the chat with LLMs.\nExtensive experimental results have demonstrated that PosterO can generate\nvisually appealing layouts for given images, achieving new state-of-the-art\nperformance across various benchmarks. To further explore PosterO's abilities\nunder the generalized settings, we built PStylish7, the first dataset with\nmulti-purpose posters and various-shaped elements, further offering a\nchallenging test for advanced research."}
{"id": "2505.07887", "pdf": "https://arxiv.org/pdf/2505.07887", "abs": "https://arxiv.org/abs/2505.07887", "authors": ["Songyin Wu", "Zhaoyang Lv", "Yufeng Zhu", "Duncan Frost", "Zhengqin Li", "Ling-Qi Yan", "Carl Ren", "Richard Newcombe", "Zhao Dong"], "title": "Monocular Online Reconstruction with Enhanced Detail Preservation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose an online 3D Gaussian-based dense mapping framework for\nphotorealistic details reconstruction from a monocular image stream. Our\napproach addresses two key challenges in monocular online reconstruction:\ndistributing Gaussians without relying on depth maps and ensuring both local\nand global consistency in the reconstructed maps. To achieve this, we introduce\ntwo key modules: the Hierarchical Gaussian Management Module for effective\nGaussian distribution and the Global Consistency Optimization Module for\nmaintaining alignment and coherence at all scales. In addition, we present the\nMulti-level Occupancy Hash Voxels (MOHV), a structure that regularizes\nGaussians for capturing details across multiple levels of granularity. MOHV\nensures accurate reconstruction of both fine and coarse geometries and\ntextures, preserving intricate details while maintaining overall structural\nintegrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our\nframework achieves superior reconstruction quality with high computational\nefficiency. Moreover, it integrates seamlessly with various tracking systems,\nensuring generality and scalability."}
{"id": "2505.08239", "pdf": "https://arxiv.org/pdf/2505.08239", "abs": "https://arxiv.org/abs/2505.08239", "authors": ["Yizhi Wang", "Mingrui Zhao", "Ali Mahdavi-Amiri", "Hao Zhang"], "title": "ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We introduce adaptive view planning to multi-view synthesis, aiming to\nimprove both occlusion revelation and 3D consistency for single-view 3D\nreconstruction. Instead of generating an unordered set of views independently\nor simultaneously, we generate a sequence of views, leveraging temporal\nconsistency to enhance 3D coherence. Most importantly, our view sequence is not\ndetermined by a pre-determined camera setup. Instead, we compute an adaptive\ncamera trajectory (ACT), specifically, an orbit of camera views, which\nmaximizes the visibility of occluded regions of the 3D object to be\nreconstructed. Once the best orbit is found, we feed it to a video diffusion\nmodel to generate novel views around the orbit, which in turn, are passed to a\nmulti-view 3D reconstruction model to obtain the final reconstruction. Our\nmulti-view synthesis pipeline is quite efficient since it involves no run-time\ntraining/optimization, only forward inferences by applying the pre-trained\nmodels for occlusion analysis and multi-view synthesis. Our method predicts\ncamera trajectories that reveal occlusions effectively and produce consistent\nnovel views, significantly improving 3D reconstruction over SOTA on the unseen\nGSO dataset, both quantitatively and qualitatively."}
{"id": "2505.08293", "pdf": "https://arxiv.org/pdf/2505.08293", "abs": "https://arxiv.org/abs/2505.08293", "authors": ["Zhizhuo Yin", "Yuk Hang Tsui", "Pan Hui"], "title": "M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.SD", "eess.AS", "I.3.6"], "comment": "9 Pages, 4 figures, submitted to NIPS 2025", "summary": "Generating full-body human gestures encompassing face, body, hands, and\nglobal movements from audio is a valuable yet challenging task in virtual\navatar creation. Previous systems focused on tokenizing the human gestures\nframewisely and predicting the tokens of each frame from the input audio.\nHowever, one observation is that the number of frames required for a complete\nexpressive human gesture, defined as granularity, varies among different human\ngesture patterns. Existing systems fail to model these gesture patterns due to\nthe fixed granularity of their gesture tokens. To solve this problem, we\npropose a novel framework named Multi-Granular Gesture Generator (M3G) for\naudio-driven holistic gesture generation. In M3G, we propose a novel\nMulti-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct\nmotion sequences from different temporal granularities. Subsequently, we\nproposed a multi-granular token predictor that extracts multi-granular\ninformation from audio and predicts the corresponding motion tokens. Then M3G\nreconstructs the human gestures from the predicted tokens using the MGVQ-VAE.\nBoth objective and subjective experiments demonstrate that our proposed M3G\nframework outperforms the state-of-the-art methods in terms of generating\nnatural and expressive full-body human gestures."}
{"id": "2505.07984", "pdf": "https://arxiv.org/pdf/2505.07984", "abs": "https://arxiv.org/abs/2505.07984", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to JSTARS on April 2, 2025. Code and dataset will be\n  available upon acceptance", "summary": "Remarkable capabilities in understanding and generating text-image content\nhave been demonstrated by recent advancements in multimodal large language\nmodels (MLLMs). However, their effectiveness in specialized\ndomains-particularly those requiring resource-efficient and domain-specific\nadaptations-has remained limited. In this work, a lightweight multimodal\nlanguage model termed MilChat is introduced, specifically adapted to analyze\nremote sensing imagery in secluded areas, including challenging missile launch\nsites. A new dataset, MilData, was compiled by verifying hundreds of aerial\nimages through expert review, and subtle military installations were\nhighlighted via detailed captions. Supervised fine-tuning on a 2B-parameter\nopen-source MLLM with chain-of-thought (CoT) reasoning annotations was\nperformed, enabling more accurate and interpretable explanations. Additionally,\nGroup Relative Policy Optimization (GRPO) was leveraged to enhance the model's\nability to detect critical domain-specific cues-such as defensive layouts and\nkey military structures-while minimizing false positives on civilian scenes.\nThrough empirical evaluations, it has been shown that MilChat significantly\noutperforms both larger, general-purpose multimodal models and existing remote\nsensing-adapted approaches on open-ended captioning and classification metrics.\nOver 80% recall and 98% precision were achieved on the newly proposed MilData\nbenchmark, underscoring the potency of targeted fine-tuning and reinforcement\nlearning in specialized real-world applications."}
{"id": "2505.07831", "pdf": "https://arxiv.org/pdf/2505.07831", "abs": "https://arxiv.org/abs/2505.07831", "authors": ["Michael Pichat", "William Pogrund", "Paloma Pichat", "Judicael Poumay", "Armanouche Gasparian", "Samuel Demarchi", "Martin Corbet", "Alois Georgeon", "Michael Veillet-Guillem"], "title": "Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The polysemantic nature of synthetic neurons in artificial intelligence\nlanguage models is currently understood as the result of a necessary\nsuperposition of distributed features within the latent space. We propose an\nalternative approach, geometrically defining a neuron in layer n as a\ncategorical vector space with a non-orthogonal basis, composed of categorical\nsub-dimensions extracted from preceding neurons in layer n-1. This categorical\nvector space is structured by the activation space of each neuron and enables,\nvia an intra-neuronal attention process, the identification and utilization of\na critical categorical zone for the efficiency of the language model - more\nhomogeneous and located at the intersection of these different categorical\nsub-dimensions."}
{"id": "2505.08666", "pdf": "https://arxiv.org/pdf/2505.08666", "abs": "https://arxiv.org/abs/2505.08666", "authors": ["Marco Maida", "Alberto Crescini", "Marco Perronet", "Elena Camuffo"], "title": "Claycode: Stylable and Deformable 2D Scannable Codes", "categories": ["cs.GR", "cs.CG", "cs.CV", "cs.HC", "I.3.0; I.3.5; I.3.6; E.4"], "comment": null, "summary": "This paper introduces Claycode, a novel 2D scannable code designed for\nextensive stylization and deformation. Unlike traditional matrix-based codes\n(e.g., QR codes), Claycodes encode their message in a tree structure. During\nthe encoding process, bits are mapped into a topology tree, which is then\ndepicted as a nesting of color regions drawn within the boundaries of a target\npolygon shape. When decoding, Claycodes are extracted and interpreted in\nreal-time from a camera stream. We detail the end-to-end pipeline and show that\nClaycodes allow for extensive stylization without compromising their\nfunctionality. We then empirically demonstrate Claycode's high tolerance to\nheavy deformations, outperforming traditional 2D scannable codes in scenarios\nwhere they typically fail."}
{"id": "2505.07998", "pdf": "https://arxiv.org/pdf/2505.07998", "abs": "https://arxiv.org/abs/2505.07998", "authors": ["Max Peter Ronecker", "Matthew Foutter", "Amine Elhafsi", "Daniele Gammelli", "Ihor Barakaiev", "Marco Pavone", "Daniel Watzenig"], "title": "Vision Foundation Model Embedding-Based Semantic Anomaly Detection", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for the Workshop \"Safely Leveraging Vision-Language\n  Foundation Models in Robotics: Challenges and Opportunities\" at ICRA 2025", "summary": "Semantic anomalies are contextually invalid or unusual combinations of\nfamiliar visual elements that can cause undefined behavior and failures in\nsystem-level reasoning for autonomous systems. This work explores semantic\nanomaly detection by leveraging the semantic priors of state-of-the-art vision\nfoundation models, operating directly on the image. We propose a framework that\ncompares local vision embeddings from runtime images to a database of nominal\nscenarios in which the autonomous system is deemed safe and performant. In this\nwork, we consider two variants of the proposed framework: one using raw\ngrid-based embeddings, and another leveraging instance segmentation for\nobject-centric representations. To further improve robustness, we introduce a\nsimple filtering mechanism to suppress false positives. Our evaluations on\nCARLA-simulated anomalies show that the instance-based method with filtering\nachieves performance comparable to GPT-4o, while providing precise anomaly\nlocalization. These results highlight the potential utility of vision\nembeddings from foundation models for real-time anomaly detection in autonomous\nsystems."}
{"id": "2505.07850", "pdf": "https://arxiv.org/pdf/2505.07850", "abs": "https://arxiv.org/abs/2505.07850", "authors": ["Pranav Narayanan Venkit", "Jiayi Li", "Yingfan Zhou", "Sarah Rajtmajer", "Shomir Wilson"], "title": "A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As LLMs (large language models) are increasingly used to generate synthetic\npersonas particularly in data-limited domains such as health, privacy, and HCI,\nit becomes necessary to understand how these narratives represent identity,\nespecially that of minority communities. In this paper, we audit synthetic\npersonas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the\nlens of representational harm, focusing specifically on racial identity. Using\na mixed methods approach combining close reading, lexical analysis, and a\nparameterized creativity framework, we compare 1512 LLM generated personas to\nhuman-authored responses. Our findings reveal that LLMs disproportionately\nforeground racial markers, overproduce culturally coded language, and construct\npersonas that are syntactically elaborate yet narratively reductive. These\npatterns result in a range of sociotechnical harms, including stereotyping,\nexoticism, erasure, and benevolent bias, that are often obfuscated by\nsuperficially positive narrations. We formalize this phenomenon as algorithmic\nothering, where minoritized identities are rendered hypervisible but less\nauthentic. Based on these findings, we offer design recommendations for\nnarrative-aware evaluation metrics and community-centered validation protocols\nfor synthetic identity generation."}
{"id": "2505.08686", "pdf": "https://arxiv.org/pdf/2505.08686", "abs": "https://arxiv.org/abs/2505.08686", "authors": ["Changqi He", "Shuhan Zhang", "Liguo Zhang", "Jiajun Miao"], "title": "CAD-Coder:Text-Guided CAD Files Code Generation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D\nmodels of real-world products. Traditional CAD typically relies on hand-drawing\nby experts or modifications of existing library files, which doesn't allow for\nrapid personalization. With the emergence of generative artificial\nintelligence, convenient and efficient personalized CAD generation has become\npossible. However, existing generative methods typically produce outputs that\nlack interactive editability and geometric annotations, limiting their\npractical applications in manufacturing. To enable interactive generative CAD,\nwe propose CAD-Coder, a framework that transforms natural language instructions\ninto CAD script codes, which can be executed in Python environments to generate\nhuman-editable CAD files (.Dxf). To facilitate the generation of editable CAD\nsketches with annotation information, we construct a comprehensive dataset\ncomprising 29,130 Dxf files with their corresponding script codes, where each\nsketch preserves both editability and geometric annotations. We evaluate\nCAD-Coder on various 2D/3D CAD generation tasks against existing methods,\ndemonstrating superior interactive capabilities while uniquely providing\neditable sketches with geometric annotations."}
{"id": "2505.08013", "pdf": "https://arxiv.org/pdf/2505.08013", "abs": "https://arxiv.org/abs/2505.08013", "authors": ["Gonglin Chen", "Tianwen Fu", "Haiwei Chen", "Wenbin Teng", "Hanyuan Xiao", "Yajie Zhao"], "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer", "categories": ["cs.CV"], "comment": null, "summary": "As a core step in structure-from-motion and SLAM, robust feature detection\nand description under challenging scenarios such as significant viewpoint\nchanges remain unresolved despite their ubiquity. While recent works have\nidentified the importance of local features in modeling geometric\ntransformations, these methods fail to learn the visual cues present in\nlong-range relationships. We present Robust Deformable Detector (RDD), a novel\nand robust keypoint detector/descriptor leveraging the deformable transformer,\nwhich captures global context and geometric invariance through deformable\nself-attention mechanisms. Specifically, we observed that deformable attention\nfocuses on key locations, effectively reducing the search space complexity and\nmodeling the geometric invariance. Furthermore, we collected an Air-to-Ground\ndataset for training in addition to the standard MegaDepth dataset. Our\nproposed method outperforms all state-of-the-art keypoint detection/description\nmethods in sparse matching tasks and is also capable of semi-dense matching. To\nensure comprehensive evaluation, we introduce two challenging benchmarks: one\nemphasizing large viewpoint and scale variations, and the other being an\nAir-to-Ground benchmark -- an evaluation setting that has recently gaining\npopularity for 3D reconstruction across different altitudes."}
{"id": "2505.07852", "pdf": "https://arxiv.org/pdf/2505.07852", "abs": "https://arxiv.org/abs/2505.07852", "authors": ["Ali Senol", "Garima Agrawal", "Huan Liu"], "title": "Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Detecting fake interactions in digital communication platforms remains a\nchallenging and insufficiently addressed problem. These interactions may appear\nas harmless spam or escalate into sophisticated scam attempts, making it\ndifficult to flag malicious intent early. Traditional detection methods often\nrely on static anomaly detection techniques that fail to adapt to dynamic\nconversational shifts. One key limitation is the misinterpretation of benign\ntopic transitions referred to as concept drift as fraudulent behavior, leading\nto either false alarms or missed threats. We propose a two stage detection\nframework that first identifies suspicious conversations using a tailored\nensemble classification model. To improve the reliability of detection, we\nincorporate a concept drift analysis step using a One Class Drift Detector\n(OCDD) to isolate conversational shifts within flagged dialogues. When drift is\ndetected, a large language model (LLM) assesses whether the shift indicates\nfraudulent manipulation or a legitimate topic change. In cases where no drift\nis found, the behavior is inferred to be spam like. We validate our framework\nusing a dataset of social engineering chat scenarios and demonstrate its\npractical advantages in improving both accuracy and interpretability for real\ntime fraud detection. To contextualize the trade offs, we compare our modular\napproach against a Dual LLM baseline that performs detection and judgment using\ndifferent language models."}
{"id": "2505.08137", "pdf": "https://arxiv.org/pdf/2505.08137", "abs": "https://arxiv.org/abs/2505.08137", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Siew-Kei Lam", "Tuan Ngo"], "title": "Large Language Models for Computer-Aided Design: A Survey", "categories": ["cs.LG", "cs.CL", "cs.GR", "cs.MM"], "comment": null, "summary": "Large Language Models (LLMs) have seen rapid advancements in recent years,\nwith models like ChatGPT and DeepSeek, showcasing their remarkable capabilities\nacross diverse domains. While substantial research has been conducted on LLMs\nin various fields, a comprehensive review focusing on their integration with\nComputer-Aided Design (CAD) remains notably absent. CAD is the industry\nstandard for 3D modeling and plays a vital role in the design and development\nof products across different industries. As the complexity of modern designs\nincreases, the potential for LLMs to enhance and streamline CAD workflows\npresents an exciting frontier. This article presents the first systematic\nsurvey exploring the intersection of LLMs and CAD. We begin by outlining the\nindustrial significance of CAD, highlighting the need for AI-driven innovation.\nNext, we provide a detailed overview of the foundation of LLMs. We also examine\nboth closed-source LLMs as well as publicly available models. The core of this\nreview focuses on the various applications of LLMs in CAD, providing a taxonomy\nof six key areas where these models are making considerable impact. Finally, we\npropose several promising future directions for further advancements, which\noffer vast opportunities for innovation and are poised to shape the future of\nCAD technology. Github:\nhttps://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy"}
{"id": "2505.08084", "pdf": "https://arxiv.org/pdf/2505.08084", "abs": "https://arxiv.org/abs/2505.08084", "authors": ["Yu Cheng", "Arushi Goel", "Hakan Bilen"], "title": "Visually Interpretable Subtask Reasoning for Visual Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Answering complex visual questions like `Which red furniture can be used for\nsitting?' requires multi-step reasoning, including object recognition,\nattribute filtering, and relational understanding. Recent work improves\ninterpretability in multimodal large language models (MLLMs) by decomposing\ntasks into sub-task programs, but these methods are computationally expensive\nand less accurate due to poor adaptation to target data. To address this, we\nintroduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a\nsubtask-driven training framework that enhances both interpretability and\nreasoning by generating textual and visual explanations within MLLMs. Instead\nof relying on external models, VISTAR fine-tunes MLLMs to produce structured\nSubtask-of-Thought rationales (step-by-step reasoning sequences). Experiments\non two benchmarks show that VISTAR consistently improves reasoning accuracy\nwhile maintaining interpretability. Our code and dataset will be available at\nhttps://github.com/ChengJade/VISTAR."}
{"id": "2505.07853", "pdf": "https://arxiv.org/pdf/2505.07853", "abs": "https://arxiv.org/abs/2505.07853", "authors": ["Hao Zhen", "Jidong J. Yang"], "title": "CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 7 figures", "summary": "Road crashes claim over 1.3 million lives annually worldwide and incur global\neconomic losses exceeding \\$1.8 trillion. Such profound societal and financial\nimpacts underscore the urgent need for road safety research that uncovers crash\nmechanisms and delivers actionable insights. Conventional statistical models\nand tree ensemble approaches typically rely on structured crash data,\noverlooking contextual nuances and struggling to capture complex relationships\nand underlying semantics. Moreover, these approaches tend to incur significant\ninformation loss, particularly in narrative elements related to multi-vehicle\ninteractions, crash progression, and rare event characteristics. This study\npresents CrashSage, a novel Large Language Model (LLM)-centered framework\ndesigned to advance crash analysis and modeling through four key innovations.\nFirst, we introduce a tabular-to-text transformation strategy paired with\nrelational data integration schema, enabling the conversion of raw,\nheterogeneous crash data into enriched, structured textual narratives that\nretain essential structural and relational context. Second, we apply\ncontext-aware data augmentation using a base LLM model to improve narrative\ncoherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B\nmodel for crash severity inference, demonstrating superior performance over\nbaseline approaches, including zero-shot, zero-shot with chain-of-thought\nprompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini,\nLLaMA3-70B). Finally, we employ a gradient-based explainability technique to\nelucidate model decisions at both the individual crash level and across broader\nrisk factor dimensions. This interpretability mechanism enhances transparency\nand enables targeted road safety interventions by providing deeper insights\ninto the most influential factors."}
{"id": "2505.08086", "pdf": "https://arxiv.org/pdf/2505.08086", "abs": "https://arxiv.org/abs/2505.08086", "authors": ["Ramin Mousa", "Ehsan Matbooe", "Hakimeh Khojasteh", "Amirali Bengari", "Mohammadmahdi Vahediahmar"], "title": "Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)", "categories": ["cs.CV"], "comment": null, "summary": "The effective diagnosis of acute and hard-to-heal wounds is crucial for wound\ncare practitioners to provide effective patient care. Poor clinical outcomes\nare often linked to infection, peripheral vascular disease, and increasing\nwound depth, which collectively exacerbate these comorbidities. However,\ndiagnostic tools based on Artificial Intelligence (AI) speed up the\ninterpretation of medical images and improve early detection of disease. In\nthis article, we propose a multi-modal AI model based on transfer learning\n(TL), which combines two state-of-the-art architectures, Xception and GMRNN,\nfor wound classification. The multi-modal network is developed by concatenating\nthe features extracted by a transfer learning algorithm and location features\nto classify the wound types of diabetic, pressure, surgical, and venous ulcers.\nThe proposed method is comprehensively compared with deep neural networks (DNN)\nfor medical image analysis. The experimental results demonstrate a notable\nwound-class classifications (containing only diabetic, pressure, surgical, and\nvenous) vary from 78.77 to 100\\% in various experiments. The results presented\nin this study showcase the exceptional accuracy of the proposed methodology in\naccurately classifying the most commonly occurring wound types using wound\nimages and their corresponding locations."}
{"id": "2505.07856", "pdf": "https://arxiv.org/pdf/2505.07856", "abs": "https://arxiv.org/abs/2505.07856", "authors": ["Paweł Walkowiak", "Marek Klonowski", "Marcin Oleksy", "Arkadiusz Janz"], "title": "Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Various techniques are used in the generation of adversarial examples,\nincluding methods such as TextBugger which introduce minor, hardly visible\nperturbations to words leading to changes in model behaviour. Another class of\ntechniques involves substituting words with their synonyms in a way that\npreserves the text's meaning but alters its predicted class, with TextFooler\nbeing a prominent example of such attacks. Most adversarial example generation\nmethods are developed and evaluated primarily on non-inflectional languages,\ntypically English. In this work, we evaluate and explain how adversarial\nattacks perform in inflectional languages. To explain the impact of inflection\non model behaviour and its robustness under attack, we designed a novel\nprotocol inspired by mechanistic interpretability, based on Edge Attribution\nPatching (EAP) method. The proposed evaluation protocol relies on parallel\ntask-specific corpora that include both inflected and syncretic variants of\ntexts in two languages -- Polish and English. To analyse the models and explain\nthe relationship between inflection and adversarial robustness, we create a new\nbenchmark based on task-oriented dataset MultiEmo, enabling the identification\nof mechanistic inflection-related elements of circuits within the model and\nanalyse their behaviour under attack."}
{"id": "2505.08101", "pdf": "https://arxiv.org/pdf/2505.08101", "abs": "https://arxiv.org/abs/2505.08101", "authors": ["Luu Tung Hai", "Thinh D. Le", "Zhicheng Ding", "Qing Tian", "Truong-Son Hy"], "title": "Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud processing has gained significant attention due to its critical\nrole in applications such as autonomous driving and 3D object recognition.\nHowever, deploying high-performance models like Point Transformer V3 in\nresource-constrained environments remains challenging due to their high\ncomputational and memory demands. This work introduces a novel distillation\nframework that leverages topology-aware representations and gradient-guided\nknowledge distillation to effectively transfer knowledge from a high-capacity\nteacher to a lightweight student model. Our approach captures the underlying\ngeometric structures of point clouds while selectively guiding the student\nmodel's learning process through gradient-based feature alignment. Experimental\nresults in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the\nproposed method achieves competitive performance, with an approximately 16x\nreduction in model size and a nearly 1.9x decrease in inference time compared\nto its teacher model. Notably, on NuScenes, our method achieves\nstate-of-the-art performance among knowledge distillation techniques trained\nsolely on LiDAR data, surpassing prior knowledge distillation baselines in\nsegmentation performance. Our implementation is available publicly at:\n  https://github.com/HySonLab/PointDistill"}
{"id": "2505.07857", "pdf": "https://arxiv.org/pdf/2505.07857", "abs": "https://arxiv.org/abs/2505.07857", "authors": ["Faiza Hassan", "Summra Saleem", "Kashif Javed", "Muhammad Nabeel Asim", "Abdur Rehman", "Andreas Dengel"], "title": "Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages, 10 figures(including 6 graphs)", "summary": "Multifarious intent detection predictors are developed for different\nlanguages, including English, Chinese and French, however, the field remains\nunderdeveloped for Urdu, the 10th most spoken language. In the realm of\nwell-known languages, intent detection predictors utilize the strategy of\nfew-shot learning and prediction of unseen classes based on the model training\non seen classes. However, Urdu language lacks few-shot strategy based intent\ndetection predictors and traditional predictors are focused on prediction of\nthe same classes which models have seen in the train set. To empower Urdu\nlanguage specific intent detection, this introduces a unique contrastive\nlearning approach that leverages unlabeled Urdu data to re-train pre-trained\nlanguage models. This re-training empowers LLMs representation learning for the\ndownstream intent detection task. Finally, it reaps the combined potential of\npre-trained LLMs and the prototype-informed attention mechanism to create a\ncomprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm\nof proposed predictive pipeline, it explores the potential of 6 distinct\nlanguage models and 13 distinct similarity computation methods. The proposed\nframework is evaluated on 2 public benchmark datasets, namely ATIS encompassing\n5836 samples and Web Queries having 8519 samples. Across ATIS dataset under\n4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and\n98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score,\nrespectively. In an additional case study on the Web Queries dataset under same\nclasses train and test set settings, LLMPIA outperformed state-of-the-art\npredictor by 53.55% F1-Score."}
{"id": "2505.08111", "pdf": "https://arxiv.org/pdf/2505.08111", "abs": "https://arxiv.org/abs/2505.08111", "authors": ["Olivier Papillon", "Rafik Goubran", "James Green", "Julien Larivière-Chartier", "Caitlin Higginson", "Frank Knoefel", "Rébecca Robillard"], "title": "Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors", "categories": ["cs.CV"], "comment": "Conference publication submitted to IEEE I2MTC 2025", "summary": "Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of\nmonitoring patients during sleep. We focus on four-way sleep position\nclassification using data collected from a PSM placed under a mattress in a\nsleep clinic. Sleep positions can affect sleep quality and the prevalence of\nsleep disorders, such as apnea. Measurements were performed on patients with\nsuspected sleep disorders referred for assessments at a sleep clinic. Training\ndeep learning models can be challenging in clinical settings due to the need\nfor large amounts of labeled data. To overcome the shortage of labeled training\ndata, we utilize transfer learning to adapt pre-trained deep learning models to\naccurately estimate sleep positions from a low-resolution PSM dataset collected\nin a polysomnography sleep lab. Our approach leverages Vision Transformer\nmodels pre-trained on ImageNet using masked autoencoding (ViTMAE) and a\npre-trained model for human pose estimation (ViTPose). These approaches\noutperform previous work from PSM-based sleep pose classification using deep\nlearning (TCN) as well as traditional machine learning models (SVM, XGBoost,\nRandom Forest) that use engineered features. We evaluate the performance of\nsleep position classification from 112 nights of patient recordings and\nvalidate it on a higher resolution 13-patient dataset. Despite the challenges\nof differentiating between sleep positions from low-resolution PSM data, our\napproach shows promise for real-world deployment in clinical settings"}
{"id": "2505.07858", "pdf": "https://arxiv.org/pdf/2505.07858", "abs": "https://arxiv.org/abs/2505.07858", "authors": ["Siyuan Yan", "Mo Zhu", "Guo-qing Jiang", "Jianfei Wang", "Jiaxing Chen", "Wentai Zhang", "Xiang Liao", "Xiao Cui", "Chen Zhang", "Zhuoran Song", "Ran Zhu"], "title": "Scaling Laws for Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "The escalating demand for efficient decoding in large language models (LLMs)\nis particularly critical for reasoning-intensive architectures like OpenAI-o3\nand DeepSeek-R1, which depend on extended chain-of-thought reasoning. This\nstudy investigates speculative decoding techniques through dense LLM\narchitectures to establish foundational insights for accelerating reasoning\ntasks. While speculative decoding methods leveraging parallel\ndraft-verification cycles have emerged as promising acceleration techniques,\nthe scaling laws governing decoding efficiency remain under-explored compared\nto conventional backbone LLMs developed through Pretraining->SFT->RLHF training\nparadigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2\nand 1.3) governing draft model acceptance rate (or decoding speed) across three\ndimensions: pretraining token volume, draft model capacity, and decoding batch\nsize. Building on these laws, we achieve Scylla, which coordinates\nmulti-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical\nvalidation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and\n0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on\nsummarization and QA tasks (Figure 2). Industrial inference engine deployments\ndemonstrate 2X decoding throughput improvements over EAGLE2 (Table 5),\nvalidating the transformative potential of systematic scaling for efficient LLM\ninference. Code will be released later."}
{"id": "2505.08117", "pdf": "https://arxiv.org/pdf/2505.08117", "abs": "https://arxiv.org/abs/2505.08117", "authors": ["Thomas Manzini", "Priyankari Perali", "Jayesh Tripathi", "Robin Murphy"], "title": "Now you see it, Now you don't: Damage Label Agreement in Drone & Satellite Post-Disaster Imagery", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 3 tables. Appearing at ACM FAccT'25", "summary": "This paper audits damage labels derived from coincident satellite and drone\naerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey,\nfinding 29.02% label disagreement and significantly different distributions\nbetween the two sources, which presents risks and potential harms during the\ndeployment of machine learning damage assessment systems. Currently, there is\nno known study of label agreement between drone and satellite imagery for\nbuilding damage assessment. The only prior work that could be used to infer if\nsuch imagery-derived labels agree is limited by differing damage label schemas,\nmisaligned building locations, and low data quantities. This work overcomes\nthese limitations by comparing damage labels using the same damage label\nschemas and building locations from three hurricanes, with the 15,814 buildings\nrepresenting 19.05 times more buildings considered than the most relevant prior\nwork. The analysis finds satellite-derived labels significantly under-report\ndamage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and\nsatellite- and drone-derived labels represent significantly different\ndistributions (p<5.1x10^-175). This indicates that computer vision and machine\nlearning (CV/ML) models trained on at least one of these distributions will\nmisrepresent actual conditions, as the differing satellite and drone-derived\ndistributions cannot simultaneously represent the distribution of actual\nconditions in a scene. This potential misrepresentation poses ethical risks and\npotential societal harm if not managed. To reduce the risk of future societal\nharms, this paper offers four recommendations to improve reliability and\ntransparency to decisio-makers when deploying CV/ML damage assessment systems\nin practice"}
{"id": "2505.07859", "pdf": "https://arxiv.org/pdf/2505.07859", "abs": "https://arxiv.org/abs/2505.07859", "authors": ["Daniel Franzen", "Jan Disselhoff", "David Hartmann"], "title": "Boosting Performance on ARC is a Matter of Perspective", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures, 5 tables", "summary": "The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge\nfor large language models (LLMs), exposing limitations in their abstract\nreasoning abilities. In this work, we leverage task-specific data augmentations\nthroughout the training, generation, and scoring phases, and employ a\ndepth-first search algorithm to generate diverse, high-probability candidate\nsolutions. Furthermore, we utilize the LLM not only as a generator but also as\na scorer, using its output probabilities to select the most promising\nsolutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the\npublic ARC-AGI evaluation set, demonstrating state-of-the-art performance among\npublicly available approaches. While concurrent closed-source work has reported\nhigher scores, our method distinguishes itself through its transparency,\nreproducibility, and remarkably low inference cost, averaging only around 2ct\nper task on readily available hardware (we assume a price of 36ct/hour for a\nNvidia 4090 GPU)."}
{"id": "2505.08123", "pdf": "https://arxiv.org/pdf/2505.08123", "abs": "https://arxiv.org/abs/2505.08123", "authors": ["Qing Wu", "Hongjiang Wei", "Jingyi Yu", "S. Kevin Zhou", "Yuyao Zhang"], "title": "JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages", "summary": "Multi-material decomposition (MMD) enables quantitative reconstruction of\ntissue compositions in the human body, supporting a wide range of clinical\napplications. However, traditional MMD typically requires spectral CT scanners\nand pre-measured X-ray energy spectra, significantly limiting clinical\napplicability. To this end, various methods have been developed to perform MMD\nusing conventional (i.e., single-energy, SE) CT systems, commonly referred to\nas SEMMD. Despite promising progress, most SEMMD methods follow a two-step\nimage decomposition pipeline, which first reconstructs monochromatic CT images\nusing algorithms such as FBP, and then performs decomposition on these images.\nThe initial reconstruction step, however, neglects the energy-dependent\nattenuation of human tissues, introducing severe nonlinear beam hardening\nartifacts and noise into the subsequent decomposition. This paper proposes\nJSover, a fundamentally reformulated one-step SEMMD framework that jointly\nreconstructs multi-material compositions and estimates the energy spectrum\ndirectly from SECT projections. By explicitly incorporating physics-informed\nspectral priors into the SEMMD process, JSover accurately simulates a virtual\nspectral CT system from SE acquisitions, thereby improving the reliability and\naccuracy of decomposition. Furthermore, we introduce implicit neural\nrepresentation (INR) as an unsupervised deep learning solver for representing\nthe underlying material maps. The inductive bias of INR toward continuous image\npatterns constrains the solution space and further enhances estimation quality.\nExtensive experiments on both simulated and real CT datasets show that JSover\noutperforms state-of-the-art SEMMD methods in accuracy and computational\nefficiency."}
{"id": "2505.07861", "pdf": "https://arxiv.org/pdf/2505.07861", "abs": "https://arxiv.org/abs/2505.07861", "authors": ["Harry Dong", "Bilge Acun", "Beidi Chen", "Yuejie Chi"], "title": "Scalable LLM Math Reasoning Acceleration with Low-rank Distillation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Due to long generations, large language model (LLM) math reasoning demands\nsignificant computational resources and time. While many existing efficient\ninference methods have been developed with excellent performance preservation\non language tasks, they often severely degrade math performance. In this paper,\nwe propose Caprese, a low-cost distillation method to recover lost capabilities\nfrom deploying efficient inference methods, focused primarily in feedforward\nblocks. With original weights unperturbed, roughly 1% of additional parameters,\nand only 20K synthetic training samples, we are able to recover much if not all\nof the math capabilities lost from efficient inference for thinking LLMs and\nwithout harm to language tasks for instruct LLMs. Moreover, Caprese slashes the\nnumber of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and\nintegrates cleanly into existing model layers to reduce latency (>11% reduction\nto generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity."}
{"id": "2505.08124", "pdf": "https://arxiv.org/pdf/2505.08124", "abs": "https://arxiv.org/abs/2505.08124", "authors": ["Laszlo Szilagyi", "Francis Engelmann", "Jeannette Bohg"], "title": "SLAG: Scalable Language-Augmented Gaussian Splatting", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Language-augmented scene representations hold great promise for large-scale\nrobotics applications such as search-and-rescue, smart cities, and mining. Many\nof these scenarios are time-sensitive, requiring rapid scene encoding while\nalso being data-intensive, necessitating scalable solutions. Deploying these\nrepresentations on robots with limited computational resources further adds to\nthe challenge. To address this, we introduce SLAG, a multi-GPU framework for\nlanguage-augmented Gaussian splatting that enhances the speed and scalability\nof embedding large scenes. Our method integrates 2D visual-language model\nfeatures into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG\neliminates the need for a loss function to compute per-Gaussian language\nembeddings. Instead, it derives embeddings from 3D Gaussian scene parameters\nvia a normalized weighted average, enabling highly parallelized scene encoding.\nAdditionally, we introduce a vector database for efficient embedding storage\nand retrieval. Our experiments show that SLAG achieves an 18 times speedup in\nembedding computation on a 16-GPU setup compared to OpenGaussian, while\npreserving embedding quality on the ScanNet and LERF datasets. For more\ndetails, visit our project website: https://slag-project.github.io/."}
{"id": "2505.07862", "pdf": "https://arxiv.org/pdf/2505.07862", "abs": "https://arxiv.org/abs/2505.07862", "authors": ["Andrew Kiruluta", "Eric Lundy", "Priscilla Burity"], "title": "Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition", "categories": ["cs.CL"], "comment": null, "summary": "Existing sequence to sequence models for structured language tasks rely\nheavily on the dot product self attention mechanism, which incurs quadratic\ncomplexity in both computation and memory for input length N. We introduce the\nGraph Wavelet Transformer (GWT), a novel architecture that replaces this\nbottleneck with a learnable, multi scale wavelet transform defined over an\nexplicit graph Laplacian derived from syntactic or semantic parses. Our\nanalysis shows that multi scale spectral decomposition offers an interpretable,\nefficient, and expressive alternative to quadratic self attention for graph\nstructured sequence modeling."}
{"id": "2505.08126", "pdf": "https://arxiv.org/pdf/2505.08126", "abs": "https://arxiv.org/abs/2505.08126", "authors": ["Angus Apps", "Ziwei Wang", "Vladimir Perejogin", "Timothy Molloy", "Robert Mahony"], "title": "Asynchronous Multi-Object Tracking with an Event Camera", "categories": ["cs.CV"], "comment": "7 pages, 5 figures, published in IEEE International Conference on\n  Robotics and Automation (ICRA), 2025", "summary": "Events cameras are ideal sensors for enabling robots to detect and track\nobjects in highly dynamic environments due to their low latency output, high\ntemporal resolution, and high dynamic range. In this paper, we present the\nAsynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and\ntracking multiple objects by processing individual raw events asynchronously.\nAEMOT detects salient event blob features by identifying regions of consistent\noptical flow using a novel Field of Active Flow Directions built from the\nSurface of Active Events. Detected features are tracked as candidate objects\nusing the recently proposed Asynchronous Event Blob (AEB) tracker in order to\nconstruct small intensity patches of each candidate object. A novel learnt\nvalidation stage promotes or discards candidate objects based on classification\nof their intensity patches, with promoted objects having their position,\nvelocity, size, and orientation estimated at their event rate. We evaluate\nAEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with\nprecision and recall performance exceeding that of alternative event-based\ndetection and tracking algorithms by over 37%. Source code and the labelled\nevent Bee Swarm Dataset will be open sourced"}
{"id": "2505.07863", "pdf": "https://arxiv.org/pdf/2505.07863", "abs": "https://arxiv.org/abs/2505.07863", "authors": ["Ziliang Wang", "Xiaohong Zhang", "Ze Shi Li", "Meng Yan"], "title": "QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Accurate prediction of Quality of Service (QoS) metrics is fundamental for\nselecting and managing cloud based services. Traditional QoS models rely on\nmanual feature engineering and yield only point estimates, offering no insight\ninto the confidence of their predictions. In this paper, we propose QoSBERT,\nthe first framework that reformulates QoS prediction as a semantic regression\ntask based on pre trained language models. Unlike previous approaches relying\non sparse numerical features, QoSBERT automatically encodes user service\nmetadata into natural language descriptions, enabling deep semantic\nunderstanding. Furthermore, we integrate a Monte Carlo Dropout based\nuncertainty estimation module, allowing for trustworthy and risk-aware service\nquality prediction, which is crucial yet underexplored in existing QoS models.\nQoSBERT applies attentive pooling over contextualized embeddings and a\nlightweight multilayer perceptron regressor, fine tuned jointly to minimize\nabsolute error. We further exploit the resulting uncertainty estimates to\nselect high quality training samples, improving robustness in low resource\nsettings. On standard QoS benchmark datasets, QoSBERT achieves an average\nreduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and\n6.9% in MAE for throughput prediction compared to the strongest baselines,\nwhile providing well calibrated confidence intervals for robust and trustworthy\nservice quality estimation. Our approach not only advances the accuracy of\nservice quality prediction but also delivers reliable uncertainty\nquantification, paving the way for more trustworthy, data driven service\nselection and optimization."}
{"id": "2505.08170", "pdf": "https://arxiv.org/pdf/2505.08170", "abs": "https://arxiv.org/abs/2505.08170", "authors": ["Zeeshan Hayder", "Ali Cheraghian", "Lars Petersson", "Mehrtash Harandi"], "title": "MoKD: Multi-Task Optimization for Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Compact models can be effectively trained through Knowledge Distillation\n(KD), a technique that transfers knowledge from larger, high-performing teacher\nmodels. Two key challenges in Knowledge Distillation (KD) are: 1) balancing\nlearning from the teacher's guidance and the task objective, and 2) handling\nthe disparity in knowledge representation between teacher and student models.\nTo address these, we propose Multi-Task Optimization for Knowledge Distillation\n(MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where\ntask-specific and distillation gradients are misaligned, and b) Gradient\nDominance, where one objective's gradient dominates, causing imbalance. MoKD\nreformulates KD as a multi-objective optimization problem, enabling better\nbalance between objectives. Additionally, it introduces a subspace learning\nframework to project feature representations into a high-dimensional space,\nimproving knowledge transfer. Our MoKD is demonstrated to outperform existing\nmethods through extensive experiments on image classification using the\nImageNet-1K dataset and object detection using the COCO dataset, achieving\nstate-of-the-art performance with greater efficiency. To the best of our\nknowledge, MoKD models also achieve state-of-the-art performance compared to\nmodels trained from scratch."}
{"id": "2505.07870", "pdf": "https://arxiv.org/pdf/2505.07870", "abs": "https://arxiv.org/abs/2505.07870", "authors": ["Suavis Giramata", "Madhusudan Srinivasan", "Venkat Naidu Gudivada", "Upulee Kanewala"], "title": "Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in various\napplications, raising critical concerns about fairness and potential biases in\ntheir outputs. This paper explores the prioritization of metamorphic relations\n(MRs) in metamorphic testing as a strategy to efficiently detect fairness\nissues within LLMs. Given the exponential growth of possible test cases,\nexhaustive testing is impractical; therefore, prioritizing MRs based on their\neffectiveness in detecting fairness violations is crucial. We apply a sentence\ndiversity-based approach to compute and rank MRs to optimize fault detection.\nExperimental results demonstrate that our proposed prioritization approach\nimproves fault detection rates by 22% compared to random prioritization and 12%\ncompared to distance-based prioritization, while reducing the time to the first\nfailure by 15% and 8%, respectively. Furthermore, our approach performs within\n5% of fault-based prioritization in effectiveness, while significantly reducing\nthe computational cost associated with fault labeling. These results validate\nthe effectiveness of diversity-based MR prioritization in enhancing fairness\ntesting for LLMs."}
{"id": "2505.08173", "pdf": "https://arxiv.org/pdf/2505.08173", "abs": "https://arxiv.org/abs/2505.08173", "authors": ["Xiaoshuo Yan", "Zhaochuan Li", "Lei Meng", "Zhuang Qi", "Wei Wu", "Zixuan Li", "Xiangxu Meng"], "title": "Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Causal inference has emerged as a promising approach to mitigate long-tail\nclassification by handling the biases introduced by class imbalance. However,\nalong with the change of advanced backbone models from Convolutional Neural\nNetworks (CNNs) to Visual Transformers (ViT), existing causal models may not\nachieve an expected performance gain. This paper investigates the influence of\nexisting causal models on CNNs and ViT variants, highlighting that ViT's global\nfeature representation makes it hard for causal methods to model associations\nbetween fine-grained features and predictions, which leads to difficulties in\nclassifying tail classes with similar visual appearance. To address these\nissues, this paper proposes TSCNet, a two-stage causal modeling method to\ndiscover fine-grained causal associations through multi-scale causal\ninterventions. Specifically, in the hierarchical causal representation learning\nstage (HCRL), it decouples the background and objects, applying backdoor\ninterventions at both the patch and feature level to prevent model from using\nclass-irrelevant areas to infer labels which enhances fine-grained causal\nrepresentation. In the counterfactual logits bias calibration stage (CLBC), it\nrefines the optimization of model's decision boundary by adaptive constructing\ncounterfactual balanced data distribution to remove the spurious associations\nin the logits caused by data distribution. Extensive experiments conducted on\nvarious long-tail benchmarks demonstrate that the proposed TSCNet can eliminate\nmultiple biases introduced by data imbalance, which outperforms existing\nmethods."}
{"id": "2505.07871", "pdf": "https://arxiv.org/pdf/2505.07871", "abs": "https://arxiv.org/abs/2505.07871", "authors": ["A M Muntasir Rahman", "Ajim Uddin", "Guiling \"Grace\" Wang"], "title": "Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Financial sentiment analysis (FSA) presents unique challenges to LLMs that\nsurpass those in typical sentiment analysis due to the nuanced language used in\nfinancial contexts. The prowess of these models is often undermined by the\ninherent subjectivity of sentiment classifications in existing benchmark\ndatasets like Financial Phrasebank. These datasets typically feature undefined\nsentiment classes that reflect the highly individualized perspectives of\nannotators, leading to significant variability in annotations. This variability\nresults in an unfair expectation for LLMs during benchmarking, where they are\ntasked to conjecture the subjective viewpoints of human annotators without\nsufficient context. In this paper, we introduce the Annotators' Instruction\nAssisted Prompt, a novel evaluation prompt designed to redefine the task\ndefinition of FSA for LLMs. By integrating detailed task instructions\noriginally intended for human annotators into the LLMs' prompt framework, AIAP\naims to standardize the understanding of sentiment across both human and\nmachine interpretations, providing a fair and context-rich foundation for\nsentiment analysis. We utilize a new dataset, WSBS, derived from the\nWallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM\nperformance by aligning machine operations with the refined task definitions.\nExperimental results demonstrate that AIAP enhances LLM performance\nsignificantly, with improvements up to 9.08. This context-aware approach not\nonly yields incremental gains in performance but also introduces an innovative\nsentiment-indexing method utilizing model confidence scores. This method\nenhances stock price prediction models and extracts more value from the\nfinancial sentiment analysis, underscoring the significance of WSB as a\ncritical source of financial text. Our research offers insights into both\nimproving FSA through better evaluation methods."}
{"id": "2505.08178", "pdf": "https://arxiv.org/pdf/2505.08178", "abs": "https://arxiv.org/abs/2505.08178", "authors": ["Ziteng Liu", "Dongdong He", "Chenghong Zhang", "Wenpeng Gao", "Yili Fu"], "title": "Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images", "categories": ["cs.CV"], "comment": null, "summary": "Occlusion and the scarcity of labeled surgical data are significant\nchallenges in disparity estimation for stereo laparoscopic images. To address\nthese issues, this study proposes a Depth Guided Occlusion-Aware Disparity\nRefinement Network (DGORNet), which refines disparity maps by leveraging\nmonocular depth information unaffected by occlusion. A Position Embedding (PE)\nmodule is introduced to provide explicit spatial context, enhancing the\nnetwork's ability to localize and refine features. Furthermore, we introduce an\nOptical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal\ncontinuity across video frames to improve robustness in dynamic surgical\nscenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms\nstate-of-the-art methods in terms of End-Point Error (EPE) and Root Mean\nSquared Error (RMSE), particularly in occlusion and texture-less regions.\nAblation studies confirm the contributions of the Position Embedding and\nOptical Flow Difference Loss, highlighting their roles in improving spatial and\ntemporal consistency. These results underscore DGORNet's effectiveness in\nenhancing disparity estimation for laparoscopic surgery, offering a practical\nsolution to challenges in disparity estimation and data limitations."}
{"id": "2505.07874", "pdf": "https://arxiv.org/pdf/2505.07874", "abs": "https://arxiv.org/abs/2505.07874", "authors": ["Yu Wang", "Runxi Yu", "Zhongyuan Wang", "Jing He"], "title": "The Sound of Populism: Distinct Linguistic Features Across Populist Variants", "categories": ["cs.CL"], "comment": null, "summary": "This study explores the sound of populism by integrating the classic\nLinguistic Inquiry and Word Count (LIWC) features, which capture the emotional\nand stylistic tones of language, with a fine-tuned RoBERTa model, a\nstate-of-the-art context-aware language model trained to detect nuanced\nexpressions of populism. This approach allows us to uncover the auditory\ndimensions of political rhetoric in U.S. presidential inaugural and State of\nthe Union addresses. We examine how four key populist dimensions (i.e.,\nleft-wing, right-wing, anti-elitism, and people-centrism) manifest in the\nlinguistic markers of speech, drawing attention to both commonalities and\ndistinct tonal shifts across these variants. Our findings reveal that populist\nrhetoric consistently features a direct, assertive ``sound\" that forges a\nconnection with ``the people'' and constructs a charismatic leadership persona.\nHowever, this sound is not simply informal but strategically calibrated.\nNotably, right-wing populism and people-centrism exhibit a more emotionally\ncharged discourse, resonating with themes of identity, grievance, and crisis,\nin contrast to the relatively restrained emotional tones of left-wing and\nanti-elitist expressions."}
{"id": "2505.08190", "pdf": "https://arxiv.org/pdf/2505.08190", "abs": "https://arxiv.org/abs/2505.08190", "authors": ["Lhuqita Fazry", "Valentino Vito"], "title": "Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Raindrop removal is a challenging task in image processing. Removing\nraindrops while relying solely on a single image further increases the\ndifficulty of the task. Common approaches include the detection of raindrop\nregions in the image, followed by performing a background restoration process\nconditioned on those regions. While various methods can be applied for the\ndetection step, the most common architecture used for background restoration is\nthe Generative Adversarial Network (GAN). Recent advances in the use of\ndiffusion models have led to state-of-the-art image inpainting techniques. In\nthis paper, we introduce a novel technique for raindrop removal from a single\nimage using diffusion-based image inpainting."}
{"id": "2505.07883", "pdf": "https://arxiv.org/pdf/2505.07883", "abs": "https://arxiv.org/abs/2505.07883", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rational decision-making under uncertainty requires coherent degrees of\nbelief in events. However, event probabilities generated by Large Language\nModels (LLMs) have been shown to exhibit incoherence, violating the axioms of\nprobability theory. This raises the question of whether coherent event\nprobabilities can be recovered from the embeddings used by the models. If so,\nthose derived probabilities could be used as more accurate estimates in events\ninvolving uncertainty. To explore this question, we propose enforcing axiomatic\nconstraints, such as the additive rule of probability theory, in the latent\nspace learned by an extended variational autoencoder (VAE) applied to LLM\nembeddings. This approach enables event probabilities to naturally emerge in\nthe latent space as the VAE learns to both reconstruct the original embeddings\nand predict the embeddings of semantically related events. We evaluate our\nmethod on complementary events (i.e., event A and its complement, event not-A),\nwhere the true probabilities of the two events must sum to 1. Experiment\nresults on open-weight language models demonstrate that probabilities recovered\nfrom embeddings exhibit greater coherence than those directly reported by the\ncorresponding models and align closely with the true probabilities."}
{"id": "2505.08196", "pdf": "https://arxiv.org/pdf/2505.08196", "abs": "https://arxiv.org/abs/2505.08196", "authors": ["He Huang", "Qi Yang", "Mufan Liu", "Yiling Xu", "Zhu Li"], "title": "ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from\na canonical space to target frames, which overlooks redundancy among adjacent\nGaussian primitives and results in suboptimal performance. To address this\nlimitation, we propose Anchor-Driven Deformable and Compressed Gaussian\nSplatting (ADC-GS), a compact and efficient representation for dynamic scene\nreconstruction. Specifically, ADC-GS organizes Gaussian primitives into an\nanchor-based structure within the canonical space, enhanced by a temporal\nsignificance-based anchor refinement strategy. To reduce deformation\nredundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that\ncaptures motions at varying granularities. Moreover, a rate-distortion\noptimization is adopted to achieve an optimal balance between bitrate\nconsumption and representation fidelity. Experimental results demonstrate that\nADC-GS outperforms the per-Gaussian deformation approaches in rendering speed\nby 300%-800% while achieving state-of-the-art storage efficiency without\ncompromising rendering quality. The code is released at\nhttps://github.com/H-Huang774/ADC-GS.git."}
{"id": "2505.07884", "pdf": "https://arxiv.org/pdf/2505.07884", "abs": "https://arxiv.org/abs/2505.07884", "authors": ["S. E Emedem", "I. E Onyenwe", "E. G Onyedinma"], "title": "Development of a WAZOBIA-Named Entity Recognition System", "categories": ["cs.CL", "cs.HC", "cs.IR", "cs.LG"], "comment": "6 pages, 3 figures, 1 table", "summary": "Named Entity Recognition NER is very crucial for various natural language\nprocessing applications, including information extraction, machine translation,\nand sentiment analysis. Despite the ever-increasing interest in African\nlanguages within computational linguistics, existing NER systems focus mainly\non English, European, and a few other global languages, leaving a significant\ngap for under-resourced languages. This research presents the development of a\nWAZOBIA-NER system tailored for the three most prominent Nigerian languages:\nHausa, Yoruba, and Igbo. This research begins with a comprehensive compilation\nof annotated datasets for each language, addressing data scarcity and\nlinguistic diversity challenges. Exploring the state-of-the-art machine\nlearning technique, Conditional Random Fields (CRF) and deep learning models\nsuch as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder\nRepresentation from Transformers (Bert) and fine-tune with a Recurrent Neural\nNetwork (RNN), the study evaluates the effectiveness of these approaches in\nrecognizing three entities: persons, organizations, and locations. The system\nutilizes optical character recognition (OCR) technology to convert textual\nimages into machine-readable text, thereby enabling the Wazobia system to\naccept both input text and textual images for extraction purposes. The system\nachieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in\nF1-score, and 0.9301 in accuracy. The model's evaluation was conducted across\nthree languages, with precision, recall, F1-score, and accuracy as key\nassessment metrics. The Wazobia-NER system demonstrates that it is feasible to\nbuild robust NER tools for under-resourced African languages using current NLP\nframeworks and transfer learning."}
{"id": "2505.08197", "pdf": "https://arxiv.org/pdf/2505.08197", "abs": "https://arxiv.org/abs/2505.08197", "authors": ["Junxian Duan", "Jiyang Guang", "Wenkui Yang", "Ran He"], "title": "Visual Watermarking in the Era of Diffusion Models: Advances and Challenges", "categories": ["cs.CV"], "comment": null, "summary": "As generative artificial intelligence technologies like Stable Diffusion\nadvance, visual content becomes more vulnerable to misuse, raising concerns\nabout copyright infringement. Visual watermarks serve as effective protection\nmechanisms, asserting ownership and deterring unauthorized use. Traditional\ndeepfake detection methods often rely on passive techniques that struggle with\nsophisticated manipulations. In contrast, diffusion models enhance detection\naccuracy by allowing for the effective learning of features, enabling the\nembedding of imperceptible and robust watermarks. We analyze the strengths and\nchallenges of watermark techniques related to diffusion models, focusing on\ntheir robustness and application in watermark generation. By exploring the\nintegration of advanced diffusion models and watermarking security, we aim to\nadvance the discourse on preserving watermark robustness against evolving\nforgery threats. It emphasizes the critical importance of developing innovative\nsolutions to protect digital content and ensure the preservation of ownership\nrights in the era of generative AI."}
{"id": "2505.07886", "pdf": "https://arxiv.org/pdf/2505.07886", "abs": "https://arxiv.org/abs/2505.07886", "authors": ["Chun-Pai Yang", "Kan Zheng", "Shou-De Lin"], "title": "PLHF: Prompt Optimization with Few-Shot Human Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic prompt optimization frameworks are developed to obtain suitable\nprompts for large language models (LLMs) with respect to desired output quality\nmetrics. Although existing approaches can handle conventional tasks such as\nfixed-solution question answering, defining the metric becomes complicated when\nthe output quality cannot be easily assessed by comparisons with standard\ngolden samples. Consequently, optimizing the prompts effectively and\nefficiently without a clear metric becomes a critical challenge. To address the\nissue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman\n\"F\"eedback), a few-shot prompt optimization framework inspired by the\nwell-known RLHF technique. Different from naive strategies, PLHF employs a\nspecific evaluator module acting as the metric to estimate the output quality.\nPLHF requires only a single round of human feedback to complete the entire\nprompt optimization process. Empirical results on both public and industrial\ndatasets show that PLHF outperforms prior output grading strategies for LLM\nprompt optimizations."}
{"id": "2505.08228", "pdf": "https://arxiv.org/pdf/2505.08228", "abs": "https://arxiv.org/abs/2505.08228", "authors": ["Unai Gurbindo", "Axel Brando", "Jaume Abella", "Caroline König"], "title": "Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.10; I.4.8; I.5.1"], "comment": "8 pages, 5 figures. Accepted at the International Joint Conference on\n  Neural Networks (IJCNN) 2025 (to appear)", "summary": "Enhancing the robustness of object detection systems under adverse weather\nconditions is crucial for the advancement of autonomous driving technology.\nThis study presents a novel approach leveraging the diffusion model Instruct\nPix2Pix to develop prompting methodologies that generate realistic datasets\nwith weather-based augmentations aiming to mitigate the impact of adverse\nweather on the perception capabilities of state-of-the-art object detection\nmodels, including Faster R-CNN and YOLOv10. Experiments were conducted in two\nenvironments, in the CARLA simulator where an initial evaluation of the\nproposed data augmentation was provided, and then on the real-world image data\nsets BDD100K and ACDC demonstrating the effectiveness of the approach in real\nenvironments.\n  The key contributions of this work are twofold: (1) identifying and\nquantifying the performance gap in object detection models under challenging\nweather conditions, and (2) demonstrating how tailored data augmentation\nstrategies can significantly enhance the robustness of these models. This\nresearch establishes a solid foundation for improving the reliability of\nperception systems in demanding environmental scenarios, and provides a pathway\nfor future advancements in autonomous driving."}
{"id": "2505.07888", "pdf": "https://arxiv.org/pdf/2505.07888", "abs": "https://arxiv.org/abs/2505.07888", "authors": ["Yusen Wu", "Xiaotie Deng"], "title": "Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper addresses the challenge in long-text style transfer using\nzero-shot learning of large language models (LLMs), proposing a hierarchical\nframework that combines sentence-level stylistic adaptation with\nparagraph-level structural coherence. We argue that in the process of effective\nparagraph-style transfer, to preserve the consistency of original syntactic and\nsemantic information, it is essential to perform style transfer not only at the\nsentence level but also to incorporate paragraph-level semantic considerations,\nwhile ensuring structural coherence across inter-sentential relationships. Our\nproposed framework, ZeroStylus, operates through two systematic phases:\nhierarchical template acquisition from reference texts and template-guided\ngeneration with multi-granular matching. The framework dynamically constructs\nsentence and paragraph template repositories, enabling context-aware\ntransformations while preserving inter-sentence logical relationships.\nExperimental evaluations demonstrate significant improvements over baseline\nmethods, with structured rewriting achieving 6.90 average score compared to\n6.70 for direct prompting approaches in tri-axial metrics assessing style\nconsistency, content preservation, and expression quality. Ablation studies\nvalidate the necessity of both template hierarchies during style transfer,\nshowing higher content preservation win rate against sentence-only approaches\nthrough paragraph-level structural encoding, as well as direct prompting method\nthrough sentence-level pattern extraction and matching. The results establish\nnew capabilities for coherent long-text style transfer without requiring\nparallel corpora or LLM fine-tuning."}
{"id": "2505.08231", "pdf": "https://arxiv.org/pdf/2505.08231", "abs": "https://arxiv.org/abs/2505.08231", "authors": ["Yu Zhang", "Fengyuan Liu", "Juan Lyu", "Yi Wei", "Changdong Yu"], "title": "HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective", "categories": ["cs.CV"], "comment": "This paper has been accepted to ICME 2025", "summary": "In the realm of intelligent maritime navigation, object detection from a\nshipborne perspective is paramount. Despite the criticality, the paucity of\nmaritime-specific data impedes the deployment of sophisticated visual\nperception techniques, akin to those utilized in autonomous vehicular systems,\nwithin the maritime context. To bridge this gap, we introduce Navigation12, a\nnovel dataset annotated for 12 object categories under diverse maritime\nenvironments and weather conditions. Based upon this dataset, we propose\nHMPNet, a lightweight architecture tailored for shipborne object detection.\nHMPNet incorporates a hierarchical dynamic modulation backbone to bolster\nfeature aggregation and expression, complemented by a matrix cascading\npoly-scale neck and a polymerization weight sharing detector, facilitating\nefficient multi-scale feature aggregation. Empirical evaluations indicate that\nHMPNet surpasses current state-of-the-art methods in terms of both accuracy and\ncomputational efficiency, realizing a 3.3% improvement in mean Average\nPrecision over YOLOv11n, the prevailing model, and reducing parameters by 23%."}
{"id": "2505.07889", "pdf": "https://arxiv.org/pdf/2505.07889", "abs": "https://arxiv.org/abs/2505.07889", "authors": ["Yuyang Liu", "Liuzhenghao Lv", "Xiancheng Zhang", "Li Yuan", "Yonghong Tian"], "title": "BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Biological protocols are fundamental to reproducible and safe life science\nresearch. While LLMs excel on general tasks, their systematic evaluation on\nthese highly specialized, accuracy-critical, and inherently procedural texts\nremains limited. In this work, we present BioProBench, the first large-scale,\nintegrated multi-task benchmark for biological protocol understanding and\nreasoning. While limited benchmarks have touched upon specific aspects like\nprotocol QA, BioProBench provides a comprehensive suite of five core tasks:\nProtocol Question Answering, Step Ordering, Error Correction, Protocol\nGeneration, and Protocol Reasoning, enabling a holistic evaluation of LLMs on\nprocedural biological texts. Built upon 27K original protocols, it yields\nnearly 556K high-quality structured instances. We evaluate 12 mainstream\nopen/closed-source LLMs on BioProBench. Experimental results reveal that while\ntop models preform well on surface understanding tasks, struggle significantly\nwith deep reasoning and structured generation tasks like ordering and\ngeneration. Furthermore, model comparisons reveal diverse performance: certain\nopen-source models approach closed-source levels on some tasks, yet\nbio-specific small models lag behind general LLMs, indicating limitations on\ncomplex procedural content. Overall, our findings underscore that procedural\nreasoning within biological protocols represents a significant challenge for\ncurrent LLMs. BioProBench serves as a standardized framework to diagnose these\nspecific limitations and guide the development of AI systems better equipped\nfor safely automating complex scientific procedures. The code and data are\navailable at: https://github.com/YuyangSunshine/bioprotocolbench and\nhttps://huggingface.co/datasets/GreatCaptainNemo/BioProBench."}
{"id": "2505.08233", "pdf": "https://arxiv.org/pdf/2505.08233", "abs": "https://arxiv.org/abs/2505.08233", "authors": ["Santhoshkumar Peddi", "Soham Bandyopadhyay", "Debasis Samanta"], "title": "G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents G-MSGINet, a unified and efficient framework for robust\ncontactless fingerprint recognition that jointly performs minutiae localization\nand identity embedding directly from raw input images. Existing approaches rely\non multi-branch architectures, orientation labels, or complex preprocessing\nsteps, which limit scalability and generalization across real-world acquisition\nscenarios. In contrast, the proposed architecture introduces the GMSGI layer, a\nnovel computational module that integrates grouped pixel-level involution,\ndynamic multi-scale kernel generation, and graph-based relational modelling\ninto a single processing unit. Stacked GMSGI layers progressively refine both\nlocal minutiae-sensitive features and global topological representations\nthrough end-to-end optimization. The architecture eliminates explicit\norientation supervision and adapts graph connectivity directly from learned\nkernel descriptors, thereby capturing meaningful structural relationships among\nfingerprint regions without fixed heuristics. Extensive experiments on three\nbenchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that\nG-MSGINet consistently achieves minutiae F1-scores in the range of\n$0.83\\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%,\nwhile maintaining an Equal Error Rate (EER) as low as 0.5%. These results\ncorrespond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1\naccuracy when compared to prior methods, using only 0.38 million parameters and\n6.63 giga floating-point operations, which represents up to ten times fewer\nparameters than competitive baselines. This highlights the scalability and\neffectiveness of G-MSGINet in real-world contactless biometric recognition\nscenarios."}
{"id": "2505.07890", "pdf": "https://arxiv.org/pdf/2505.07890", "abs": "https://arxiv.org/abs/2505.07890", "authors": ["Kutay Ertürk", "Furkan Altınışık", "İrem Sarıaltın", "Ömer Nezih Gerek"], "title": "TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks", "categories": ["cs.CL", "eess.IV"], "comment": null, "summary": "This study presents TSLFormer, a light and robust word-level Turkish Sign\nLanguage (TSL) recognition model that treats sign gestures as ordered,\nstring-like language. Instead of using raw RGB or depth videos, our method only\nworks with 3D joint positions - articulation points - extracted using Google's\nMediapipe library, which focuses on the hand and torso skeletal locations. This\ncreates efficient input dimensionality reduction while preserving important\nsemantic gesture information.\n  Our approach revisits sign language recognition as sequence-to-sequence\ntranslation, inspired by the linguistic nature of sign languages and the\nsuccess of transformers in natural language processing. Since TSLFormer uses\nthe self-attention mechanism, it effectively captures temporal co-occurrence\nwithin gesture sequences and highlights meaningful motion patterns as words\nunfold.\n  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different\nwords, TSLFormer achieves competitive performance with minimal computational\ncost. These results show that joint-based input is sufficient for enabling\nreal-time, mobile, and assistive communication systems for hearing-impaired\nindividuals."}
{"id": "2505.08234", "pdf": "https://arxiv.org/pdf/2505.08234", "abs": "https://arxiv.org/abs/2505.08234", "authors": ["Krti Tallam", "John Kevin Cava", "Caleb Geniesse", "N. Benjamin Erichson", "Michael W. Mahoney"], "title": "Removing Watermarks with Partial Regeneration using Semantic Information", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged\nas a primary line of defense for copyright and provenance. The newest\nwatermarking schemes embed semantic signals - content-aware patterns that are\ndesigned to survive common image manipulations - yet their true robustness\nagainst adaptive adversaries remains under-explored. We expose a previously\nunreported vulnerability and introduce SemanticRegen, a three-stage, label-free\nattack that erases state-of-the-art semantic and invisible watermarks while\nleaving an image's apparent meaning intact. Our pipeline (i) uses a\nvision-language model to obtain fine-grained captions, (ii) extracts foreground\nmasks with zero-shot segmentation, and (iii) inpaints only the background via\nan LLM-guided diffusion model, thereby preserving salient objects and style\ncues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing,\nStegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat\nthe semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy\nbelow 0.75 for the remaining schemes, all while maintaining high perceptual\nquality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM)\nto quantify fidelity within foreground regions, showing that our attack\nachieves up to 12 percent higher mSSIM than prior diffusion-based attackers.\nThese results highlight an urgent gap between current watermark defenses and\nthe capabilities of adaptive, semantics-aware adversaries, underscoring the\nneed for watermarking algorithms that are resilient to content-preserving\nregenerative attacks."}
{"id": "2505.07891", "pdf": "https://arxiv.org/pdf/2505.07891", "abs": "https://arxiv.org/abs/2505.07891", "authors": ["Ching Nam Hang", "Pei-Duo Yu", "Chee Wei Tan"], "title": "TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the age of social media, the rapid spread of misinformation and rumors has\nled to the emergence of infodemics, where false information poses a significant\nthreat to society. To combat this issue, we introduce TrumorGPT , a novel\ngenerative artificial intelligence solution designed for fact-checking in the\nhealth domain. TrumorGPT aims to distinguish \"trumors\", which are\nhealth-related rumors that turn out to be true, providing a crucial tool in\ndifferentiating between mere speculation and verified facts. This framework\nleverages a large language model (LLM) with few-shot learning for semantic\nhealth knowledge graph construction and semantic reasoning. TrumorGPT\nincorporates graph-based retrieval-augmented generation (GraphRAG) to address\nthe hallucination issue common in LLMs and the limitations of static training\ndata. GraphRAG involves accessing and utilizing information from regularly\nupdated semantic health knowledge graphs that consist of the latest medical\nnews and health information, ensuring that fact-checking by TrumorGPT is based\non the most recent data. Evaluating with extensive healthcare datasets,\nTrumorGPT demonstrates superior performance in fact-checking for public health\nclaims. Its ability to effectively conduct fact-checking across various\nplatforms marks a critical step forward in the fight against health-related\nmisinformation, enhancing trust and accuracy in the digital information age."}
{"id": "2505.08235", "pdf": "https://arxiv.org/pdf/2505.08235", "abs": "https://arxiv.org/abs/2505.08235", "authors": ["Hanle Zheng", "Xujie Han", "Zegang Peng", "Shangbin Zhang", "Guangxun Du", "Zhuo Zou", "Xilin Wang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation", "categories": ["cs.CV"], "comment": null, "summary": "Video Frame Interpolation (VFI) is a fundamental yet challenging task in\ncomputer vision, particularly under conditions involving large motion,\nocclusion, and lighting variation. Recent advancements in event cameras have\nopened up new opportunities for addressing these challenges. While existing\nevent-based VFI methods have succeeded in recovering large and complex motions\nby leveraging handcrafted intermediate representations such as optical flow,\nthese designs often compromise high-fidelity image reconstruction under subtle\nmotion scenarios due to their reliance on explicit motion modeling. Meanwhile,\ndiffusion models provide a promising alternative for VFI by reconstructing\nframes through a denoising process, eliminating the need for explicit motion\nestimation or warping operations. In this work, we propose EventDiff, a unified\nand efficient event-based diffusion model framework for VFI. EventDiff features\na novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight\nSpatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic\nevent streams with static frames. Unlike previous event-based VFI methods,\nEventDiff performs interpolation directly in the latent space via a denoising\ndiffusion process, making it more robust across diverse and challenging VFI\nscenarios. Through a two-stage training strategy that first pretrains the HAE\nand then jointly optimizes it with the diffusion model, our method achieves\nstate-of-the-art performance across multiple synthetic and real-world event VFI\ndatasets. The proposed method outperforms existing state-of-the-art event-based\nVFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior\nperformance in SNU-FILM tasks with multiple difficulty levels. Compared to the\nemerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR\ngain on Vimeo90K-Triplet and 4.24X faster inference."}
{"id": "2505.07897", "pdf": "https://arxiv.org/pdf/2505.07897", "abs": "https://arxiv.org/abs/2505.07897", "authors": ["Stefano Rando", "Luca Romani", "Alessio Sampieri", "Yuta Kyuragi", "Luca Franco", "Fabio Galasso", "Tatsunori Hashimoto", "John Yang"], "title": "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Context lengths for models have grown rapidly, from thousands to millions of\ntokens in just a few years. The extreme context sizes of modern long-context\nmodels have made it difficult to construct realistic long-context benchmarks --\nnot only due to the cost of collecting million-context tasks but also in\nidentifying realistic scenarios that require significant contexts. We identify\ncode comprehension and repair as a natural testbed and challenge task for\nlong-context models and introduce LongCodeBench (LCB), a benchmark to test LLM\ncoding abilities in long-context scenarios. Our benchmark tests both the\ncomprehension and repair capabilities of LCLMs in realistic and important\nsettings by drawing from real-world GitHub issues and constructing QA\n(LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the\ncomplexity of our benchmark, enabling us to evaluate models across different\nscales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model.\nWe find that long-context remains a weakness for all models, with performance\ndrops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for\nQwen2.5."}
{"id": "2505.08242", "pdf": "https://arxiv.org/pdf/2505.08242", "abs": "https://arxiv.org/abs/2505.08242", "authors": ["Aidar Amangeldi", "Vladislav Yarovenko", "Angsar Taigonyrov"], "title": "Congenital Heart Disease recognition using Deep Learning/Transformer models", "categories": ["cs.CV"], "comment": null, "summary": "Congenital Heart Disease (CHD) remains a leading cause of infant morbidity\nand mortality, yet non-invasive screening methods often yield false negatives.\nDeep learning models, with their ability to automatically extract features, can\nassist doctors in detecting CHD more effectively. In this work, we investigate\nthe use of dual-modality (sound and image) deep learning methods for CHD\ndiagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72%\naccuracy on the DICOM Chest X-ray dataset."}
{"id": "2505.07899", "pdf": "https://arxiv.org/pdf/2505.07899", "abs": "https://arxiv.org/abs/2505.07899", "authors": ["Ding Cao", "Yuchen Cai", "Rongxi Guo", "Xuesong He", "Guiquan Liu"], "title": "DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sequential knowledge editing techniques aim to continuously update the\nknowledge in large language models at a low cost, preventing the models from\ngenerating outdated or incorrect information. However, existing sequential\nediting methods suffer from a significant decline in editing success rates\nafter long-term editing. Through theoretical analysis and experiments, we\nidentify that as the number of edits increases, the model's output increasingly\ndeviates from the desired target, leading to a drop in editing success rates.\nWe refer to this issue as the accumulation of superimposed noise problem. To\naddress this, we identify the factors contributing to this deviation and\npropose DeltaEdit, a novel method that optimizes update parameters through a\ndynamic orthogonal constraints strategy, effectively reducing interference\nbetween edits to mitigate deviation. Experimental results demonstrate that\nDeltaEdit significantly outperforms existing methods in edit success rates and\nthe retention of generalization capabilities, ensuring stable and reliable\nmodel performance even under extensive sequential editing."}
{"id": "2505.08246", "pdf": "https://arxiv.org/pdf/2505.08246", "abs": "https://arxiv.org/abs/2505.08246", "authors": ["Jonathan Brokman", "Amit Giloni", "Omer Hofman", "Roman Vainshtein", "Hisashi Kojima", "Guy Gilboa"], "title": "Identifying Memorization of Diffusion Models through p-Laplace Analysis", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": "To be published in SSVM 2025 (proceedings of the 10th International\n  Conference on Scale Space and Variational Methods in Computer Vision)", "summary": "Diffusion models, today's leading image generative models, estimate the score\nfunction, i.e. the gradient of the log probability of (perturbed) data samples,\nwithout direct access to the underlying probability distribution. This work\ninvestigates whether the estimated score function can be leveraged to compute\nhigher-order differentials, namely p-Laplace operators. We show here these\noperators can be employed to identify memorized training data. We propose a\nnumerical p-Laplace approximation based on the learned score functions, showing\nits effectiveness in identifying key features of the probability landscape. We\nanalyze the structured case of Gaussian mixture models, and demonstrate the\nresults carry-over to image generative models, where memorization\nidentification based on the p-Laplace operator is performed for the first time."}
{"id": "2505.07903", "pdf": "https://arxiv.org/pdf/2505.07903", "abs": "https://arxiv.org/abs/2505.07903", "authors": ["Zeyang Sha", "Shiwen Cui", "Weiqiang Wang"], "title": "SEM: Reinforcement Learning for Search-Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models(LLMs) have demonstrated their\ncapabilities not only in reasoning but also in invoking external tools,\nparticularly search engines. However, teaching models to discern when to invoke\nsearch and when to rely on their internal knowledge remains a significant\nchallenge. Existing reinforcement learning approaches often lead to redundant\nsearch behaviors, resulting in inefficiencies and over-cost. In this paper, we\npropose SEM, a novel post-training reinforcement learning framework that\nexplicitly trains LLMs to optimize search usage. By constructing a balanced\ndataset combining MuSiQue and MMLU, we create scenarios where the model must\nlearn to distinguish between questions it can answer directly and those\nrequiring external retrieval. We design a structured reasoning template and\nemploy Group Relative Policy Optimization(GRPO) to post-train the model's\nsearch behaviors. Our reward function encourages accurate answering without\nunnecessary search while promoting effective retrieval when needed.\nExperimental results demonstrate that our method significantly reduces\nredundant search operations while maintaining or improving answer accuracy\nacross multiple challenging benchmarks. This framework advances the model's\nreasoning efficiency and extends its capability to judiciously leverage\nexternal knowledge."}
{"id": "2505.08259", "pdf": "https://arxiv.org/pdf/2505.08259", "abs": "https://arxiv.org/abs/2505.08259", "authors": ["Aidar Amangeldi", "Angsar Taigonyrov", "Muhammad Huzaid Jawad", "Chinedu Emmanuel Mbonu"], "title": "CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets", "categories": ["cs.CV"], "comment": null, "summary": "This study evaluates the trade-offs between convolutional and\ntransformer-based architectures on both medical and general-purpose image\nclassification benchmarks. We use ResNet-18 as our baseline and introduce a\nfine-tuning strategy applied to four Vision Transformer variants (Tiny, Small,\nBase, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce\ninference latency and model complexity with acceptable accuracy degradation.\nThrough systematic hyperparameter variations, we demonstrate that appropriately\nfine-tuned Vision Transformers can match or exceed the baseline's performance,\nachieve faster inference, and operate with fewer parameters, highlighting their\nviability for deployment in resource-constrained environments."}
{"id": "2505.07920", "pdf": "https://arxiv.org/pdf/2505.07920", "abs": "https://arxiv.org/abs/2505.07920", "authors": ["Daoze Zhang", "Zhijian Bao", "Sihang Du", "Zhiyi Zhao", "Kuangling Zhang", "Dezheng Bao", "Yang Yang"], "title": "Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2 figures, 5 tables", "summary": "Peer review is a critical component of scientific progress in the fields like\nAI, but the rapid increase in submission volume has strained the reviewing\nsystem, which inevitably leads to reviewer shortages and declines review\nquality. Besides the growing research popularity, another key factor in this\noverload is the repeated resubmission of substandard manuscripts, largely due\nto the lack of effective tools for authors to self-evaluate their work before\nsubmission. Large Language Models (LLMs) show great promise in assisting both\nauthors and reviewers, and their performance is fundamentally limited by the\nquality of the peer review data. However, existing peer review datasets face\nthree major limitations: (1) limited data diversity, (2) inconsistent and\nlow-quality data due to the use of revised rather than initial submissions, and\n(3) insufficient support for tasks involving rebuttal and reviewer-author\ninteractions. To address these challenges, we introduce the largest\nconsistency-ensured peer review and rebuttal dataset named Re^2, which\ncomprises 19,926 initial submissions, 70,668 review comments, and 53,818\nrebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the\nrebuttal and discussion stage is framed as a multi-turn conversation paradigm\nto support both traditional static review tasks and dynamic interactive LLM\nassistants, providing more practical guidance for authors to refine their\nmanuscripts and helping alleviate the growing review burden. Our data and code\nare available in https://anonymous.4open.science/r/ReviewBench_anon/."}
{"id": "2505.08260", "pdf": "https://arxiv.org/pdf/2505.08260", "abs": "https://arxiv.org/abs/2505.08260", "authors": ["Chunming Li", "Shidong Wang", "Haofeng Zhang"], "title": "Few-shot Novel Category Discovery", "categories": ["cs.CV"], "comment": null, "summary": "The recently proposed Novel Category Discovery (NCD) adapt paradigm of\ntransductive learning hinders its application in more real-world scenarios. In\nfact, few labeled data in part of new categories can well alleviate this\nburden, which coincides with the ease that people can label few of new category\ndata. Therefore, this paper presents a new setting in which a trained agent is\nable to flexibly switch between the tasks of identifying examples of known\n(labelled) classes and clustering novel (completely unlabeled) classes as the\nnumber of query examples increases by leveraging knowledge learned from only a\nfew (handful) support examples. Drawing inspiration from the discovery of novel\ncategories using prior-based clustering algorithms, we introduce a novel\nframework that further relaxes its assumptions to the real-world open set level\nby unifying the concept of model adaptability in few-shot learning. We refer to\nthis setting as Few-Shot Novel Category Discovery (FSNCD) and propose\nSemi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means\nClustering (UKC) to examine the model's reasoning capabilities. Extensive\nexperiments and detailed analysis on five commonly used datasets demonstrate\nthat our methods can achieve leading performance levels across different task\nsettings and scenarios."}
{"id": "2505.07968", "pdf": "https://arxiv.org/pdf/2505.07968", "abs": "https://arxiv.org/abs/2505.07968", "authors": ["Weiyi Wu", "Xinwen Xu", "Chongyang Gao", "Xingjian Diao", "Siting Li", "Lucas A. Salas", "Jiang Gui"], "title": "Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have great potential in the field of health\ncare, yet they face great challenges in adapting to rapidly evolving medical\nknowledge. This can lead to outdated or contradictory treatment suggestions.\nThis study investigated how LLMs respond to evolving clinical guidelines,\nfocusing on concept drift and internal inconsistencies. We developed the\nDriftMedQA benchmark to simulate guideline evolution and assessed the temporal\nreliability of various LLMs. Our evaluation of seven state-of-the-art models\nacross 4,290 scenarios demonstrated difficulties in rejecting outdated\nrecommendations and frequently endorsing conflicting guidance. Additionally, we\nexplored two mitigation strategies: Retrieval-Augmented Generation and\npreference fine-tuning via Direct Preference Optimization. While each method\nimproved model performance, their combination led to the most consistent and\nreliable results. These findings underscore the need to improve LLM robustness\nto temporal shifts to ensure more dependable applications in clinical practice."}
{"id": "2505.08266", "pdf": "https://arxiv.org/pdf/2505.08266", "abs": "https://arxiv.org/abs/2505.08266", "authors": ["Yanbin Wei", "Xuehao Wang", "Zhan Zhuang", "Yang Chen", "Shuhao Chen", "Yulong Zhang", "Yu Zhang", "James Kwok"], "title": "Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Message-passing graph neural networks (MPNNs) and structural features (SFs)\nare cornerstones for the link prediction task. However, as a common and\nintuitive mode of understanding, the potential of visual perception has been\noverlooked in the MPNN community. For the first time, we equip MPNNs with\nvision structural awareness by proposing an effective framework called Graph\nVision Network (GVN), along with a more efficient variant (E-GVN). Extensive\nempirical results demonstrate that with the proposed frameworks, GVN\nconsistently benefits from the vision enhancement across seven link prediction\ndatasets, including challenging large-scale graphs. Such improvements are\ncompatible with existing state-of-the-art (SOTA) methods and GVNs achieve new\nSOTA results, thereby underscoring a promising novel direction for link\nprediction."}
{"id": "2505.07980", "pdf": "https://arxiv.org/pdf/2505.07980", "abs": "https://arxiv.org/abs/2505.07980", "authors": ["Fupei Guo", "Achintha Wijesinghe", "Songyang Zhang", "Zhi Ding"], "title": "Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration", "categories": ["cs.CL", "C.2.1; I.4.8"], "comment": null, "summary": "Semantic communications represent a new paradigm of next-generation\nnetworking that shifts bit-wise data delivery to conveying the semantic\nmeanings for bandwidth efficiency. To effectively accommodate various potential\ndownstream tasks at the receiver side, one should adaptively convey the most\ncritical semantic information. This work presents a novel task-adaptive\nsemantic communication framework based on diffusion models that is capable of\ndynamically adjusting the semantic message delivery according to various\ndownstream tasks. Specifically, we initialize the transmission of a\ndeep-compressed general semantic representation from the transmitter to enable\ndiffusion-based coarse data reconstruction at the receiver. The receiver\nidentifies the task-specific demands and generates textual prompts as feedback.\nIntegrated with the attention mechanism, the transmitter updates the semantic\ntransmission with more details to better align with the objectives of the\nintended receivers. Our test results demonstrate the efficacy of the proposed\nmethod in adaptively preserving critical task-relevant information for semantic\ncommunications while preserving high compression efficiency."}
{"id": "2505.08273", "pdf": "https://arxiv.org/pdf/2505.08273", "abs": "https://arxiv.org/abs/2505.08273", "authors": ["Nibir Chandra Mandal", "Oishee Bintey Hoque", "Abhijin Adiga", "Samarth Swarup", "Mandy Wilson", "Lu Feng", "Yangfeng Ji", "Miaomiao Zhang", "Geoffrey Fox", "Madhav Marathe"], "title": "IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping", "categories": ["cs.CV"], "comment": null, "summary": "We introduce IrrMap, the first large-scale dataset (1.1 million patches) for\nirrigation method mapping across regions. IrrMap consists of multi-resolution\nsatellite imagery from LandSat and Sentinel, along with key auxiliary data such\nas crop type, land use, and vegetation indices. The dataset spans 1,687,899\nfarms and 14,117,330 acres across multiple western U.S. states from 2013 to\n2023, providing a rich and diverse foundation for irrigation analysis and\nensuring geospatial alignment and quality control. The dataset is ML-ready,\nwith standardized 224x224 GeoTIFF patches, the multiple input modalities,\ncarefully chosen train-test-split data, and accompanying dataloaders for\nseamless deep learning model training andbenchmarking in irrigation mapping.\nThe dataset is also accompanied by a complete pipeline for dataset generation,\nenabling researchers to extend IrrMap to new regions for irrigation data\ncollection or adapt it with minimal effort for other similar applications in\nagricultural and geospatial analysis. We also analyze the irrigation method\ndistribution across crop groups, spatial irrigation patterns (using Shannon\ndiversity indices), and irrigated area variations for both LandSat and\nSentinel, providing insights into regional and resolution-based differences. To\npromote further exploration, we openly release IrrMap, along with the derived\ndatasets, benchmark models, and pipeline code, through a GitHub repository:\nhttps://github.com/Nibir088/IrrMap and Data repository:\nhttps://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and\nimplementation details."}
{"id": "2505.08004", "pdf": "https://arxiv.org/pdf/2505.08004", "abs": "https://arxiv.org/abs/2505.08004", "authors": ["Haneh Rhel", "Dmitri Roussinov"], "title": "Large Language Models and Arabic Content: A Review", "categories": ["cs.CL", "cs.AI"], "comment": "Original language: English This paper has been submitted to the First\n  International Conference on Artificial Intelligence and Generative AI\n  (FICAILY 2025), and it has been accepted for presentation at FICAILY on\n  9-10/July 2025 and for publication in the Springer Nature. Number of pages:\n  16 Publication status Accepted/In press - 7 Apr 2025\n  https://www.gena-ai-libya2025.com/", "summary": "Over the past three years, the rapid advancement of Large Language Models\n(LLMs) has had a profound impact on multiple areas of Artificial Intelligence\n(AI), particularly in Natural Language Processing (NLP) across diverse\nlanguages, including Arabic. Although Arabic is considered one of the most\nwidely spoken languages across 27 countries in the Arabic world and used as a\nsecond language in some other non-Arabic countries as well, there is still a\nscarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face\nvarious challenges due to the complexities of the Arabic language, including\nits rich morphology, intricate structure, and diverse writing standards, among\nother factors. Researchers have been actively addressing these challenges,\ndemonstrating that pre-trained Large Language Models (LLMs) trained on\nmultilingual corpora achieve significant success in various Arabic NLP tasks.\nThis study provides an overview of using large language models (LLMs) for the\nArabic language, highlighting early pre-trained Arabic Language models across\nvarious NLP applications and their ability to handle diverse Arabic content\ntasks and dialects. It also provides an overview of how techniques like\nfinetuning and prompt engineering can enhance the performance of these models.\nAdditionally, the study summarizes common Arabic benchmarks and datasets while\npresenting our observations on the persistent upward trend in the adoption of\nLLMs."}
{"id": "2505.08281", "pdf": "https://arxiv.org/pdf/2505.08281", "abs": "https://arxiv.org/abs/2505.08281", "authors": ["Anle Ke", "Xu Zhang", "Tong Chen", "Ming Lu", "Chao Zhou", "Jiawen Gu", "Zhan Ma"], "title": "Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Existing multimodal large model-based image compression frameworks often rely\non a fragmented integration of semantic retrieval, latent compression, and\ngenerative models, resulting in suboptimal performance in both reconstruction\nfidelity and coding efficiency. To address these challenges, we propose a\nresidual-guided ultra lowrate image compression named ResULIC, which\nincorporates residual signals into both semantic retrieval and the\ndiffusion-based generation process. Specifically, we introduce Semantic\nResidual Coding (SRC) to capture the semantic disparity between the original\nimage and its compressed latent representation. A perceptual fidelity optimizer\nis further applied for superior reconstruction quality. Additionally, we\npresent the Compression-aware Diffusion Model (CDM), which establishes an\noptimal alignment between bitrates and diffusion time steps, improving\ncompression-reconstruction synergy. Extensive experiments demonstrate the\neffectiveness of ResULIC, achieving superior objective and subjective\nperformance compared to state-of-the-art diffusion-based methods with - 80.7%,\n-66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at\nhttps: //njuvision.github.io/ResULIC/."}
{"id": "2505.08037", "pdf": "https://arxiv.org/pdf/2505.08037", "abs": "https://arxiv.org/abs/2505.08037", "authors": ["Yutong Liu", "Feng Xiao", "Ziyue Zhang", "Yongbin Yu", "Cheng Huang", "Fan Gao", "Xiangxiang Wang", "Ma-bao Ban", "Manping Fan", "Thupten Tsering", "Cheng Huang", "Gadeng Luosang", "Renzeng Duojie", "Nyima Tashi"], "title": "TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation", "categories": ["cs.CL", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Multi-level Tibetan spelling correction addresses errors at both the\ncharacter and syllable levels within a unified model. Existing methods focus\nmainly on single-level correction and lack effective integration of both\nlevels. Moreover, there are no open-source datasets or augmentation methods\ntailored for this task in Tibetan. To tackle this, we propose a data\naugmentation approach using unlabeled text to generate multi-level corruptions,\nand introduce TiSpell, a semi-masked model capable of correcting both\ncharacter- and syllable-level errors. Although syllable-level correction is\nmore challenging due to its reliance on global context, our semi-masked\nstrategy simplifies this process. We synthesize nine types of corruptions on\nclean sentences to create a robust training set. Experiments on both simulated\nand real-world data demonstrate that TiSpell, trained on our dataset,\noutperforms baseline models and matches the performance of state-of-the-art\napproaches, confirming its effectiveness."}
{"id": "2505.08284", "pdf": "https://arxiv.org/pdf/2505.08284", "abs": "https://arxiv.org/abs/2505.08284", "authors": ["Honna Shinichi", "Akira Matsui"], "title": "Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Artwork research has long relied on human sensibility and subjective\njudgment, but recent developments in machine learning have enabled the\nquantitative assessment of features that humans could not discover. In Western\npaintings, comprehensive analyses have been conducted from various perspectives\nin conjunction with large databases, but such extensive analysis has not been\nsufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a\ntraditional Japanese art form, as a case study of Eastern paintings, and\nconduct a quantitative analysis of creativity in works of art using 11,000\nhigh-resolution images. This involves using the concept of calculating\ncreativity from networks to analyze both the creativity of the artwork and that\nof the artists. As a result, In terms of Ukiyo-e as a whole, it was found that\nthe creativity of its appearance has declined with the maturation of culture,\nbut in terms of style, it has become more segmented with the maturation of\nculture and has maintained a high level of creativity. This not only provides\nnew insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved\nwithin the ongoing cultural history, playing a culturally significant role in\nthe analysis of Eastern art."}
{"id": "2505.08054", "pdf": "https://arxiv.org/pdf/2505.08054", "abs": "https://arxiv.org/abs/2505.08054", "authors": ["Zhehao Zhang", "Weijie Xu", "Fanyou Wu", "Chandan K. Reddy"], "title": "FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Safety alignment approaches in large language models (LLMs) often lead to the\nover-refusal of benign queries, significantly diminishing their utility in\nsensitive scenarios. To address this challenge, we introduce FalseReject, a\ncomprehensive resource containing 16k seemingly toxic queries accompanied by\nstructured responses across 44 safety-related categories. We propose a\ngraph-informed adversarial multi-agent interaction framework to generate\ndiverse and complex prompts, while structuring responses with explicit\nreasoning to aid models in accurately distinguishing safe from unsafe contexts.\nFalseReject includes training datasets tailored for both standard\ninstruction-tuned models and reasoning-oriented models, as well as a\nhuman-annotated benchmark test set. Our extensive benchmarking on 29\nstate-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges.\nEmpirical results demonstrate that supervised finetuning with FalseReject\nsubstantially reduces unnecessary refusals without compromising overall safety\nor general language capabilities."}
{"id": "2505.08294", "pdf": "https://arxiv.org/pdf/2505.08294", "abs": "https://arxiv.org/abs/2505.08294", "authors": ["Jian Wang", "Baoyuan Wu", "Li Liu", "Qingshan Liu"], "title": "FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units", "categories": ["cs.CV"], "comment": null, "summary": "The rapid evolution of generative AI has increased the threat of realistic\naudio-visual deepfakes, demanding robust detection methods. Existing solutions\nprimarily address unimodal (audio or visual) forgeries but struggle with\nmultimodal manipulations due to inadequate handling of heterogeneous modality\nfeatures and poor generalization across datasets. To this end, we propose a\nnovel framework called FauForensics by introducing biologically invariant\nfacial action units (FAUs), which is a quantitative descriptor of facial muscle\nactivity linked to emotion physiology. It serves as forgery-resistant\nrepresentations that reduce domain dependency while capturing subtle dynamics\noften disrupted in synthetic content. Besides, instead of comparing entire\nvideo clips as in prior works, our method computes fine-grained frame-wise\naudiovisual similarities via a dedicated fusion module augmented with learnable\ncross-modal queries. It dynamically aligns temporal-spatial lip-audio\nrelationships while mitigating multi-modal feature heterogeneity issues.\nExperiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance\nand superior cross-dataset generalizability with up to an average of 4.83\\%\nthan existing methods."}
{"id": "2505.08058", "pdf": "https://arxiv.org/pdf/2505.08058", "abs": "https://arxiv.org/abs/2505.08058", "authors": ["Chris Forrester", "Octavia Sulea"], "title": "HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method", "categories": ["cs.CL"], "comment": null, "summary": "Compute optimization using token reduction of LLM prompts is an emerging task\nin the fields of NLP and next generation, agentic AI. In this white paper, we\nintroduce a novel (patent pending) text representation scheme and a\nfirst-of-its-kind word-level semantic compression of paragraphs that can lead\nto over 90\\% token reduction, while retaining high semantic similarity to the\nsource text. We explain how this novel compression technique can be lossless\nand how the detail granularity is controllable. We discuss benchmark results\nover open source data (i.e. Bram Stoker's Dracula available through Project\nGutenberg) and show how our results hold at the paragraph level, across\nmultiple genres and models."}
{"id": "2505.08302", "pdf": "https://arxiv.org/pdf/2505.08302", "abs": "https://arxiv.org/abs/2505.08302", "authors": ["Oishee Bintey Hoque", "Nibir Chandra Mandal", "Abhijin Adiga", "Samarth Swarup", "Sayjro Kossi Nouwakpo", "Amanda Wilson", "Madhav Marathe"], "title": "Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing", "categories": ["cs.CV"], "comment": "Full version of the paper will be appearing at the Proceedings of the\n  Thirty-Third International Joint Conference on Artificial Intelligence\n  (IJCAI-25), Special Track on AI for Good", "summary": "Accurate mapping of irrigation methods is crucial for sustainable\nagricultural practices and food systems. However, existing models that rely\nsolely on spectral features from satellite imagery are ineffective due to the\ncomplexity of agricultural landscapes and limited training data, making this a\nchallenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a\nnovel Swin-Transformer based approach that uses (i) a specialized projection\nmatrix to encode crop to irrigation probability, (ii) a spatial attention map\nto identify agricultural lands from non-agricultural lands, (iii)\nbi-directional cross-attention to focus complementary information from\ndifferent modalities, and (iv) a weighted ensemble for combining predictions\nfrom images and crop information. Our experimentation on five states in the US\nshows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU)\nimprovement for hard-to-classify drip irrigation. In addition, we propose a\ntwo-phase transfer learning approach to enhance cross-state irrigation mapping,\nachieving a 51% IoU boost in a state with limited labeled data. The ability to\nachieve baseline performance with only 40% of the training data highlights its\nefficiency, reducing the dependency on extensive manual labeling efforts and\nmaking large-scale, automated irrigation mapping more feasible and\ncost-effective."}
{"id": "2505.08106", "pdf": "https://arxiv.org/pdf/2505.08106", "abs": "https://arxiv.org/abs/2505.08106", "authors": ["Jiashen", "Du", "Jesse Yao", "Allen Liu", "Zhekai Zhang"], "title": "Are LLMs complicated ethical dilemma analyzers?", "categories": ["cs.CL", "cs.AI"], "comment": "CS194-280 Advanced LLM Agents project. Project page:\n  https://github.com/ALT-JS/ethicaLLM", "summary": "One open question in the study of Large Language Models (LLMs) is whether\nthey can emulate human ethical reasoning and act as believable proxies for\nhuman judgment. To investigate this, we introduce a benchmark dataset\ncomprising 196 real-world ethical dilemmas and expert opinions, each segmented\ninto five structured components: Introduction, Key Factors, Historical\nTheoretical Perspectives, Resolution Strategies, and Key Takeaways. We also\ncollect non-expert human responses for comparison, limited to the Key Factors\nsection due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini,\nClaude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric\nframework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine\nsimilarity, and Universal Sentence Encoder similarity. Metric weights are\ncomputed through an inversion-based ranking alignment and pairwise AHP\nanalysis, enabling fine-grained comparison of model outputs to expert\nresponses. Our results show that LLMs generally outperform non-expert humans in\nlexical and structural alignment, with GPT-4o-mini performing most consistently\nacross all sections. However, all models struggle with historical grounding and\nproposing nuanced resolution strategies, which require contextual abstraction.\nHuman responses, while less structured, occasionally achieve comparable\nsemantic similarity, suggesting intuitive moral reasoning. These findings\nhighlight both the strengths and current limitations of LLMs in ethical\ndecision-making."}
{"id": "2505.08324", "pdf": "https://arxiv.org/pdf/2505.08324", "abs": "https://arxiv.org/abs/2505.08324", "authors": ["Elena Morotti"], "title": "An incremental algorithm for non-convex AI-enhanced medical image processing", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "Solving non-convex regularized inverse problems is challenging due to their\ncomplex optimization landscapes and multiple local minima. However, these\nmodels remain widely studied as they often yield high-quality, task-oriented\nsolutions, particularly in medical imaging, where the goal is to enhance\nclinically relevant features rather than merely minimizing global error. We\npropose incDG, a hybrid framework that integrates deep learning with\nincremental model-based optimization to efficiently approximate the\n$\\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess\nstrategy, incDG exploits a deep neural network to generate effective\ninitializations for a non-convex variational solver, which refines the\nreconstruction through regularized incremental iterations. This design combines\nthe efficiency of Artificial Intelligence (AI) tools with the theoretical\nguarantees of model-based optimization, ensuring robustness and stability. We\nvalidate incDG on TpV-regularized optimization tasks, demonstrating its\neffectiveness in medical image deblurring and tomographic reconstruction across\ndiverse datasets, including synthetic images, brain CT slices, and\nchest-abdomen scans. Results show that incDG outperforms both conventional\niterative solvers and deep learning-based methods, achieving superior accuracy\nand stability. Moreover, we confirm that training incDG without ground truth\ndoes not significantly degrade performance, making it a practical and powerful\ntool for solving non-convex inverse problems in imaging and beyond."}
{"id": "2505.08120", "pdf": "https://arxiv.org/pdf/2505.08120", "abs": "https://arxiv.org/abs/2505.08120", "authors": ["Mingjian Jiang", "Yangjun Ruan", "Luis Lastras", "Pavan Kapanipathi", "Tatsunori Hashimoto"], "title": "Putting It All into Context: Simplifying Agents with LCLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in language model (LM) agents have demonstrated significant\npotential for automating complex real-world tasks. To make progress on these\ndifficult tasks, LM agent architectures have become increasingly complex, often\nincorporating multi-step retrieval tools, multiple agents, and scaffolding\nadapted to the underlying LM. In this work, we investigate whether all of this\ncomplexity is necessary, or if parts of these scaffolds can be removed on\nchallenging tasks like SWE-bench. We show that in the case of SWE-bench, simply\nputting the entire environment into the context of a long context language\nmodel (LCLM) and properly prompting the model makes it competitive with\ncarefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model\nwithout any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable\nwith approaches using carefully tuned agent scaffolds (32%). While the\nunscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic\narchitectures, we demonstrate that the more capable Gemini-2.5-Pro using the\nsame unscaffolded approach directly attains a 50.8% solve rate. Additionally, a\ntwo-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a\ncompetitive 48.6% solve rate."}
{"id": "2505.08336", "pdf": "https://arxiv.org/pdf/2505.08336", "abs": "https://arxiv.org/abs/2505.08336", "authors": ["Xue Cui", "Vincent Gbouna Zakka", "Minhyun Lee"], "title": "A computer vision-based model for occupancy detection using low-resolution thermal images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Occupancy plays an essential role in influencing the energy consumption and\noperation of heating, ventilation, and air conditioning (HVAC) systems.\nTraditional HVAC typically operate on fixed schedules without considering\noccupancy. Advanced occupant-centric control (OCC) adopted occupancy status in\nregulating HVAC operations. RGB images combined with computer vision (CV)\ntechniques are widely used for occupancy detection, however, the detailed\nfacial and body features they capture raise significant privacy concerns.\nLow-resolution thermal images offer a non-invasive solution that mitigates\nprivacy issues. The study developed an occupancy detection model utilizing\nlow-resolution thermal images and CV techniques, where transfer learning was\napplied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The\ndeveloped model ultimately achieved satisfactory performance, with precision,\nrecall, mAP50, and mAP50 values approaching 1.000. The contributions of this\nmodel lie not only in mitigating privacy concerns but also in reducing\ncomputing resource demands."}
{"id": "2505.08130", "pdf": "https://arxiv.org/pdf/2505.08130", "abs": "https://arxiv.org/abs/2505.08130", "authors": ["Mingxu Tao", "Bowen Tang", "Mingxuan Ma", "Yining Zhang", "Hourun Li", "Feifan Wen", "Hao Ma", "Jia Yang"], "title": "ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in NAACL 2025 Demo Track", "summary": "The rise of Large Language Models~(LLMs) revolutionizes information\nretrieval, allowing users to obtain required answers through complex\ninstructions within conversations. However, publicly available services remain\ninadequate in addressing the needs of faculty and students to search\ncampus-specific information. It is primarily due to the LLM's lack of\ndomain-specific knowledge and the limitation of search engines in supporting\nmultilingual and timely scenarios. To tackle these challenges, we introduce\nALOHA, a multilingual agent enhanced by hierarchical retrieval for university\norientation. We also integrate external APIs into the front-end interface to\nprovide interactive service. The human evaluation and case study show our\nproposed system has strong capabilities to yield correct, timely, and\nuser-friendly responses to the queries in multiple languages, surpassing\ncommercial chatbots and search engines. The system has been deployed and has\nprovided service for more than 12,000 people."}
{"id": "2505.08349", "pdf": "https://arxiv.org/pdf/2505.08349", "abs": "https://arxiv.org/abs/2505.08349", "authors": ["Ruixiao Shi", "Fu Feng", "Yucheng Xie", "Jing Wang", "Xin Geng"], "title": "FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-domain few-shot learning (CD-FSL) requires models to generalize from\nlimited labeled samples under significant distribution shifts. While recent\nmethods enhance adaptability through lightweight task-specific modules, they\noperate solely in the spatial domain and overlook frequency-specific variations\nthat are often critical for robust transfer. We observe that spatially similar\nimages across domains can differ substantially in their spectral\nrepresentations, with low and high frequencies capturing complementary semantic\ninformation at coarse and fine levels. This indicates that uniform spatial\nadaptation may overlook these spectral distinctions, thus constraining\ngeneralization. To address this, we introduce Frequency Adaptation and\nDiversion (FAD), a frequency-aware framework that explicitly models and\nmodulates spectral components. At its core is the Frequency Diversion Adapter,\nwhich transforms intermediate features into the frequency domain using the\ndiscrete Fourier transform (DFT), partitions them into low, mid, and\nhigh-frequency bands via radial masks, and reconstructs each band using inverse\nDFT (IDFT). Each frequency band is then adapted using a dedicated convolutional\nbranch with a kernel size tailored to its spectral scale, enabling targeted and\ndisentangled adaptation across frequencies. Extensive experiments on the\nMeta-Dataset benchmark demonstrate that FAD consistently outperforms\nstate-of-the-art methods on both seen and unseen domains, validating the\nutility of frequency-domain representations and band-wise adaptation for\nimproving generalization in CD-FSL."}
{"id": "2505.08167", "pdf": "https://arxiv.org/pdf/2505.08167", "abs": "https://arxiv.org/abs/2505.08167", "authors": ["Ruilin Liu", "Zhixiao Zhao", "Jieqiong Li", "Chang Liu", "Dongbo Wang"], "title": "Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "The rapid development of large language models (LLMs) has provided\nsignificant support and opportunities for the advancement of domain-specific\nLLMs. However, fine-tuning these large models using Intangible Cultural\nHeritage (ICH) data inevitably faces challenges such as bias, incorrect\nknowledge inheritance, and catastrophic forgetting. To address these issues, we\npropose a novel training method that integrates a bidirectional chains of\nthought and a reward mechanism. This method is built upon ICH-Qwen, a large\nlanguage model specifically designed for the field of intangible cultural\nheritage. The proposed method enables the model to not only perform forward\nreasoning but also enhances the accuracy of the generated answers by utilizing\nreverse questioning and reverse reasoning to activate the model's latent\nknowledge. Additionally, a reward mechanism is introduced during training to\noptimize the decision-making process. This mechanism improves the quality of\nthe model's outputs through structural and content evaluations with different\nweighting schemes. We conduct comparative experiments on ICH-Qwen, with results\ndemonstrating that our method outperforms 0-shot, step-by-step reasoning,\nknowledge distillation, and question augmentation methods in terms of accuracy,\nBleu-4, and Rouge-L scores on the question-answering task. Furthermore, the\npaper highlights the effectiveness of combining the bidirectional chains of\nthought and reward mechanism through ablation experiments. In addition, a\nseries of generalizability experiments are conducted, with results showing that\nthe proposed method yields improvements on various domain-specific datasets and\nadvanced models in areas such as Finance, Wikidata, and StrategyQA. This\ndemonstrates that the method is adaptable to multiple domains and provides a\nvaluable approach for model training in future applications across diverse\nfields."}
{"id": "2505.08350", "pdf": "https://arxiv.org/pdf/2505.08350", "abs": "https://arxiv.org/abs/2505.08350", "authors": ["Bo Wang", "Haoyang Huang", "Zhiyin Lu", "Fengyuan Liu", "Guoqing Ma", "Jianlong Yuan", "Yuan Zhang", "Nan Duan"], "title": "STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces StoryAnchors, a unified framework for generating\nhigh-quality, multi-scene story frames with strong temporal consistency. The\nframework employs a bidirectional story generator that integrates both past and\nfuture contexts to ensure temporal consistency, character continuity, and\nsmooth scene transitions throughout the narrative. Specific conditions are\nintroduced to distinguish story frame generation from standard video synthesis,\nfacilitating greater scene diversity and enhancing narrative richness. To\nfurther improve generation quality, StoryAnchors integrates Multi-Event Story\nFrame Labeling and Progressive Story Frame Training, enabling the model to\ncapture both overarching narrative flow and event-level dynamics. This approach\nsupports the creation of editable and expandable story frames, allowing for\nmanual modifications and the generation of longer, more complex sequences.\nExtensive experiments show that StoryAnchors outperforms existing open-source\nmodels in key areas such as consistency, narrative coherence, and scene\ndiversity. Its performance in narrative consistency and story richness is also\non par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of\nstory-driven frame generation, offering a scalable, flexible, and highly\neditable foundation for future research."}
{"id": "2505.08168", "pdf": "https://arxiv.org/pdf/2505.08168", "abs": "https://arxiv.org/abs/2505.08168", "authors": ["Yuxiang Wang", "Xiao Yan", "Shiyu Jin", "Quanqing Xu", "Chuang Hu", "Yuanyuan Zhu", "Bo Du", "Jia Wu", "Jiawei Jiang"], "title": "Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-attributed graph (TAG) provides a text description for each graph node,\nand few- and zero-shot node classification on TAGs have many applications in\nfields such as academia and social networks. Existing work utilizes various\ngraph-based augmentation techniques to train the node and text embeddings,\nwhile text-based augmentations are largely unexplored. In this paper, we\npropose Text Semantics Augmentation (TSA) to improve accuracy by introducing\nmore text semantic supervision signals. Specifically, we design two\naugmentation techniques, i.e., positive semantics matching and negative\nsemantics contrast, to provide more reference texts for each graph node or text\ndescription. Positive semantic matching retrieves texts with similar embeddings\nto match with a graph node. Negative semantic contrast adds a negative prompt\nto construct a text description with the opposite semantics, which is\ncontrasted with the original node and text. We evaluate TSA on 5 datasets and\ncompare with 13 state-of-the-art baselines. The results show that TSA\nconsistently outperforms all baselines, and its accuracy improvements over the\nbest-performing baseline are usually over 5%."}
{"id": "2505.08423", "pdf": "https://arxiv.org/pdf/2505.08423", "abs": "https://arxiv.org/abs/2505.08423", "authors": ["Sadaf Gulshad", "Abdullah Aldahlawi Thakaa"], "title": "DArFace: Deformation Aware Robustness for Low Quality Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial recognition systems have achieved remarkable success by leveraging\ndeep neural networks, advanced loss functions, and large-scale datasets.\nHowever, their performance often deteriorates in real-world scenarios involving\nlow-quality facial images. Such degradations, common in surveillance footage or\nstandoff imaging include low resolution, motion blur, and various distortions,\nresulting in a substantial domain gap from the high-quality data typically used\nduring training. While existing approaches attempt to address robustness by\nmodifying network architectures or modeling global spatial transformations,\nthey frequently overlook local, non-rigid deformations that are inherently\npresent in real-world settings. In this work, we introduce DArFace, a\nDeformation-Aware robust Face recognition framework that enhances robustness to\nsuch degradations without requiring paired high- and low-quality training\nsamples. Our method adversarially integrates both global transformations (e.g.,\nrotation, translation) and local elastic deformations during training to\nsimulate realistic low-quality conditions. Moreover, we introduce a contrastive\nobjective to enforce identity consistency across different deformed views.\nExtensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and\nIJB-C demonstrate that DArFace surpasses state-of-the-art methods, with\nsignificant gains attributed to the inclusion of local deformation modeling."}
{"id": "2505.08200", "pdf": "https://arxiv.org/pdf/2505.08200", "abs": "https://arxiv.org/abs/2505.08200", "authors": ["Artem Shelmanov", "Ekaterina Fadeeva", "Akim Tsvigun", "Ivan Tsvigun", "Zhuohan Xie", "Igor Kiselev", "Nico Daheim", "Caiqi Zhang", "Artem Vazhentsev", "Mrinmaya Sachan", "Preslav Nakov", "Timothy Baldwin"], "title": "A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have the tendency to hallucinate, i.e., to\nsporadically generate false or fabricated information. This presents a major\nchallenge, as hallucinations often appear highly convincing and users generally\nlack the tools to detect them. Uncertainty quantification (UQ) provides a\nframework for assessing the reliability of model outputs, aiding in the\nidentification of potential hallucinations. In this work, we introduce\npre-trained UQ heads: supervised auxiliary modules for LLMs that substantially\nenhance their ability to capture uncertainty compared to unsupervised UQ\nmethods. Their strong performance stems from the powerful Transformer\narchitecture in their design and informative features derived from LLM\nattention maps. Experimental evaluation shows that these heads are highly\nrobust and achieve state-of-the-art performance in claim-level hallucination\ndetection across both in-domain and out-of-domain prompts. Moreover, these\nmodules demonstrate strong generalization to languages they were not explicitly\ntrained on. We pre-train a collection of UQ heads for popular LLM series,\nincluding Mistral, Llama, and Gemma 2. We publicly release both the code and\nthe pre-trained heads."}
{"id": "2505.08426", "pdf": "https://arxiv.org/pdf/2505.08426", "abs": "https://arxiv.org/abs/2505.08426", "authors": ["Franko Šikić", "Donik Vršnak", "Sven Lončarić"], "title": "DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Unconstrained gaze estimation is the process of determining where a subject\nis directing their visual attention in uncontrolled environments. Gaze\nestimation systems are important for a myriad of tasks such as driver\ndistraction monitoring, exam proctoring, accessibility features in modern\nsoftware, etc. However, these systems face challenges in real-world scenarios,\npartially due to the low resolution of in-the-wild images and partially due to\ninsufficient modeling of head-eye interactions in current state-of-the-art\n(SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based\nmethod that advances gaze prediction through super-resolution (SR) and a dual\nhead-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone\nprocesses eye and multiscale SR head images, while the proposed DHECA module\nenables bidirectional feature refinement between the extracted visual features\nthrough cross-attention mechanisms. Furthermore, we identified critical\nannotation errors in one of the most diverse and widely used gaze estimation\ndatasets, Gaze360, and rectified the mislabeled data. Performance evaluation on\nGaze360 and GFIE datasets demonstrates superior within-dataset performance of\nthe proposed method, reducing angular error (AE) by 0.48{\\deg} (Gaze360) and\n2.95{\\deg} (GFIE) in static configurations, and 0.59{\\deg} (Gaze360) and\n3.00{\\deg} (GFIE) in temporal settings compared to prior SOTA methods.\nCross-dataset testing shows improvements in AE of more than 1.53{\\deg}\n(Gaze360) and 3.99{\\deg} (GFIE) in both static and temporal settings,\nvalidating the robust generalization properties of our approach."}
{"id": "2505.08245", "pdf": "https://arxiv.org/pdf/2505.08245", "abs": "https://arxiv.org/abs/2505.08245", "authors": ["Haoran Ye", "Jing Jin", "Yuhang Xie", "Xin Zhang", "Guojie Song"], "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "63 pages, 482 references", "summary": "The rapid advancement of large language models (LLMs) has outpaced\ntraditional evaluation methodologies. It presents novel challenges, such as\nmeasuring human-like psychological constructs, navigating beyond static and\ntask-specific benchmarks, and establishing human-centered evaluation. These\nchallenges intersect with Psychometrics, the science of quantifying the\nintangible aspects of human psychology, such as personality, values, and\nintelligence. This survey introduces and synthesizes an emerging\ninterdisciplinary field of LLM Psychometrics, which leverages psychometric\ninstruments, theories, and principles to evaluate, understand, and enhance\nLLMs. We systematically explore the role of Psychometrics in shaping\nbenchmarking principles, broadening evaluation scopes, refining methodologies,\nvalidating results, and advancing LLM capabilities. This paper integrates\ndiverse perspectives to provide a structured framework for researchers across\ndisciplines, enabling a more comprehensive understanding of this nascent field.\nUltimately, we aim to provide actionable insights for developing future\nevaluation paradigms that align with human-level AI and promote the advancement\nof human-centered AI systems for societal benefit. A curated repository of LLM\npsychometric resources is available at\nhttps://github.com/valuebyte-ai/Awesome-LLM-Psychometrics."}
{"id": "2505.08429", "pdf": "https://arxiv.org/pdf/2505.08429", "abs": "https://arxiv.org/abs/2505.08429", "authors": ["Yukiyasu Kamitani", "Misato Tanaka", "Ken Shirakawa"], "title": "Visual Image Reconstruction from Brain Activity via Latent Representation", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Visual image reconstruction, the decoding of perceptual content from brain\nactivity into images, has advanced significantly with the integration of deep\nneural networks (DNNs) and generative models. This review traces the field's\nevolution from early classification approaches to sophisticated reconstructions\nthat capture detailed, subjective visual experiences, emphasizing the roles of\nhierarchical latent representations, compositional strategies, and modular\narchitectures. Despite notable progress, challenges remain, such as achieving\ntrue zero-shot generalization for unseen images and accurately modeling the\ncomplex, subjective aspects of perception. We discuss the need for diverse\ndatasets, refined evaluation metrics aligned with human perceptual judgments,\nand compositional representations that strengthen model robustness and\ngeneralizability. Ethical issues, including privacy, consent, and potential\nmisuse, are underscored as critical considerations for responsible development.\nVisual image reconstruction offers promising insights into neural coding and\nenables new psychological measurements of visual experiences, with applications\nspanning clinical diagnostics and brain-machine interfaces."}
{"id": "2505.08261", "pdf": "https://arxiv.org/pdf/2505.08261", "abs": "https://arxiv.org/abs/2505.08261", "authors": ["Rishabh Agrawal", "Himanshu Kumar"], "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."}
{"id": "2505.08437", "pdf": "https://arxiv.org/pdf/2505.08437", "abs": "https://arxiv.org/abs/2505.08437", "authors": ["Wenkui Yang", "Zhida Zhang", "Xiaoqiang Zhou", "Junxian Duan", "Jie Cao"], "title": "TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection", "categories": ["cs.CV"], "comment": "Accepted to PRCV 2024", "summary": "The emergence and popularity of facial deepfake methods spur the vigorous\ndevelopment of deepfake datasets and facial forgery detection, which to some\nextent alleviates the security concerns about facial-related artificial\nintelligence technologies. However, when it comes to human body forgery, there\nhas been a persistent lack of datasets and detection methods, due to the later\ninception and complexity of human body generation methods. To mitigate this\nissue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale\ndiffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic\nframes, specifically tailored for body forgery detection. TT-DF offers a wide\nvariety of forgery methods, involving multiple advanced human image animation\nmodels utilized for manipulation, two generative configurations based on the\ndisentanglement of identity and pose information, as well as different\ncompressed versions. The aim is to simulate any potential unseen forged data in\nthe wild as comprehensively as possible, and we also furnish a benchmark on\nTT-DF. Additionally, we propose an adapted body forgery detection model,\nTemporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal\ninconsistencies and optical flow distribution differences between natural data\nand forged data. Our experiments demonstrate that TOF-Net achieves favorable\nperformance on TT-DF, outperforming current state-of-the-art extendable facial\nforgery detection models. For our TT-DF dataset, please refer to\nhttps://github.com/HashTAG00002/TT-DF."}
{"id": "2505.08303", "pdf": "https://arxiv.org/pdf/2505.08303", "abs": "https://arxiv.org/abs/2505.08303", "authors": ["Ziyu Zhou", "Yihang Wu", "Jingyuan Yang", "Zhan Xiao", "Rongjun Li"], "title": "Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow", "categories": ["cs.CL"], "comment": null, "summary": "Black-Box prompt optimization methods have emerged as a promising strategy\nfor refining input prompts to better align large language models (LLMs),\nthereby enhancing their task performance. Although these methods have\ndemonstrated encouraging results, most studies and experiments have primarily\nfocused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g.,\nGPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with\nDeepSeek V3 (671B), it remains an open question whether these black-box\noptimization techniques will continue to yield significant performance\nimprovements for models of such scale. In response to this, we select three\nwell-known black-box optimization methods and evaluate them on large-scale LLMs\n(DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The\nresults show that these black-box prompt optimization methods offer only\nlimited improvements on these large-scale LLMs. Furthermore, we hypothesize\nthat the scale of the model is the primary factor contributing to the limited\nbenefits observed. To explore this hypothesis, we conducted experiments on LLMs\nof varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an\ninverse scaling law, wherein the effectiveness of black-box optimization\nmethods diminished as the model size increased."}
{"id": "2505.08438", "pdf": "https://arxiv.org/pdf/2505.08438", "abs": "https://arxiv.org/abs/2505.08438", "authors": ["Chuanzhi Xu", "Haoxian Zhou", "Langyi Chen", "Haodong Chen", "Ying Zhou", "Vera Chung", "Qiang Qu"], "title": "A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 12 figures, 11 tables", "summary": "Event cameras have emerged as promising sensors for 3D reconstruction due to\ntheir ability to capture per-pixel brightness changes asynchronously. Unlike\nconventional frame-based cameras, they produce sparse and temporally rich data\nstreams, which enable more accurate 3D reconstruction and open up the\npossibility of performing reconstruction in extreme environments such as\nhigh-speed motion, low light, or high dynamic range scenes. In this survey, we\nprovide the first comprehensive review focused exclusively on 3D reconstruction\nusing event cameras. The survey categorises existing works into three major\ntypes based on input modality - stereo, monocular, and multimodal systems, and\nfurther classifies them by reconstruction approach, including geometry-based,\ndeep learning-based, and recent neural rendering techniques such as Neural\nRadiance Fields and 3D Gaussian Splatting. Methods with a similar research\nfocus were organised chronologically into the most subdivided groups. We also\nsummarise public datasets relevant to event-based 3D reconstruction. Finally,\nwe highlight current research limitations in data availability, evaluation,\nrepresentation, and dynamic scene handling, and outline promising future\nresearch directions. This survey aims to serve as a comprehensive reference and\na roadmap for future developments in event-driven 3D reconstruction."}
{"id": "2505.08311", "pdf": "https://arxiv.org/pdf/2505.08311", "abs": "https://arxiv.org/abs/2505.08311", "authors": ["Yunjie Ji", "Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Han Zhao", "Xiangang Li"], "title": "AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale", "categories": ["cs.CL"], "comment": null, "summary": "We present AM-Thinking-v1, a 32B dense language model that advances the\nfrontier of reasoning, embodying the collaborative spirit of open-source\ninnovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts\n(MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves\nimpressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on\nLiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities\namong open-source models of similar scale.\n  Built entirely from the open-source Qwen2.5-32B base model and publicly\navailable queries, AM-Thinking-v1 leverages a meticulously crafted\npost-training pipeline - combining supervised fine-tuning and reinforcement\nlearning - to deliver exceptional reasoning capabilities. This work\ndemonstrates that the open-source community can achieve high performance at the\n32B scale, a practical sweet spot for deployment and fine-tuning. By striking a\nbalance between top-tier performance and real-world usability, we hope\nAM-Thinking-v1 inspires further collaborative efforts to harness mid-scale\nmodels, pushing reasoning boundaries while keeping accessibility at the core of\ninnovation. We have open-sourced our model on\n\\href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}."}
{"id": "2505.08455", "pdf": "https://arxiv.org/pdf/2505.08455", "abs": "https://arxiv.org/abs/2505.08455", "authors": ["Pritam Sarkar", "Ali Etemad"], "title": "VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in video understanding, the capabilities of Large\nVideo Language Models (LVLMs) to perform video-based causal reasoning remains\nunderexplored, largely due to the absence of relevant and dedicated benchmarks\nfor evaluating causal reasoning in visually grounded and goal-driven settings.\nTo fill this gap, we introduce a novel benchmark named Video-based long-form\nCausal Reasoning (VCRBench). We create VCRBench using procedural videos of\nsimple everyday activities, where the steps are deliberately shuffled with each\nclip capturing a key causal event, to test whether LVLMs can identify, reason\nabout, and correctly sequence the events needed to accomplish a specific goal.\nMoreover, the benchmark is carefully designed to prevent LVLMs from exploiting\nlinguistic shortcuts, as seen in multiple-choice or binary QA formats, while\nalso avoiding the challenges associated with evaluating open-ended QA. Our\nevaluation of state-of-the-art LVLMs on VCRBench suggests that these models\nstruggle with video-based long-form causal reasoning, primarily due to their\ndifficulty in modeling long-range causal dependencies directly from visual\nobservations. As a simple step toward enabling such capabilities, we propose\nRecognition-Reasoning Decomposition (RRD), a modular approach that breaks\nvideo-based causal reasoning into two sub-tasks of video recognition and causal\nreasoning. Our experiments on VCRBench show that RRD significantly boosts\naccuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis\nreveals interesting insights, for instance, that LVLMs primarily rely on\nlanguage knowledge for complex video-based long-form causal reasoning tasks."}
{"id": "2505.08348", "pdf": "https://arxiv.org/pdf/2505.08348", "abs": "https://arxiv.org/abs/2505.08348", "authors": ["Yize Zhao", "Christos Thrampoulidis"], "title": "On the Geometry of Semantics in Next-token Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Modern language models demonstrate a remarkable ability to capture linguistic\nmeaning despite being trained solely through next-token prediction (NTP). We\ninvestigate how this conceptually simple training objective leads models to\nextract and encode latent semantic and grammatical concepts. Our analysis\nreveals that NTP optimization implicitly guides models to encode concepts via\nsingular value decomposition (SVD) factors of a centered data-sparsity matrix\nthat captures next-word co-occurrence patterns. While the model never\nexplicitly constructs this matrix, learned word and context embeddings\neffectively factor it to capture linguistic structure. We find that the most\nimportant SVD factors are learned first during training, motivating the use of\nspectral clustering of embeddings to identify human-interpretable semantics,\nincluding both classical k-means and a new orthant-based method directly\nmotivated by our interpretation of concepts. Overall, our work bridges\ndistributional semantics, neural collapse geometry, and neural network training\ndynamics, providing insights into how NTP's implicit biases shape the emergence\nof meaning representations in language models."}
{"id": "2505.08517", "pdf": "https://arxiv.org/pdf/2505.08517", "abs": "https://arxiv.org/abs/2505.08517", "authors": ["Yifan Li", "Alan W Pang", "Jo Woon Chong"], "title": "A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Inhalation injuries face a challenge in clinical diagnosis and grading due to\nthe limitations of traditional methods, such as Abbreviated Injury Score (AIS),\nwhich rely on subjective assessments and show weak correlations with clinical\noutcomes. This study introduces a novel deep learning-based framework for\ngrading inhalation injuries using bronchoscopy images with the duration of\nmechanical ventilation as an objective metric. To address the scarcity of\nmedical imaging data, we propose enhanced StarGAN, a generative model that\nintegrates Patch Loss and SSIM Loss to improve synthetic images' quality and\nclinical relevance. The augmented dataset generated by enhanced StarGAN\nsignificantly improved classification performance when evaluated using the Swin\nTransformer, achieving an accuracy of 77.78%, an 11.11% improvement over the\noriginal dataset. Image quality was assessed using the Fr\\'echet Inception\nDistance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06,\noutperforming baseline models. Burn surgeons confirmed the realism and clinical\nrelevance of the generated images, particularly the preservation of bronchial\nstructures and color distribution. These results highlight the potential of\nenhanced StarGAN in addressing data limitations and improving classification\naccuracy for inhalation injury grading."}
{"id": "2505.08351", "pdf": "https://arxiv.org/pdf/2505.08351", "abs": "https://arxiv.org/abs/2505.08351", "authors": ["Mina Almasi", "Ross Deans Kristensen-McLachlan"], "title": "Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring", "categories": ["cs.CL"], "comment": null, "summary": "This paper investigates the potentials of Large Language Models (LLMs) as\nadaptive tutors in the context of second-language learning. In particular, we\nevaluate whether system prompting can reliably constrain LLMs to generate only\ntext appropriate to the student's competence level. We simulate full\nteacher-student dialogues in Spanish using instruction-tuned, open-source LLMs\nranging in size from 7B to 12B parameters. Dialogues are generated by having an\nLLM alternate between tutor and student roles with separate chat histories. The\noutput from the tutor model is then used to evaluate the effectiveness of\nCEFR-based prompting to control text difficulty across three proficiency levels\n(A1, B1, C1). Our findings suggest that while system prompting can be used to\nconstrain model outputs, prompting alone is too brittle for sustained,\nlong-term interactional contexts - a phenomenon we term alignment drift. Our\nresults provide insights into the feasibility of LLMs for personalized,\nproficiency-aligned adaptive tutors and provide a scalable method for low-cost\nevaluation of model performance without human participants."}
{"id": "2505.08524", "pdf": "https://arxiv.org/pdf/2505.08524", "abs": "https://arxiv.org/abs/2505.08524", "authors": ["Pratibha Kumari", "Daniel Reisenbüchler", "Afshin Bozorgpour", "Nadine S. Schaadt", "Friedrich Feuerhake", "Dorit Merhof"], "title": "Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis", "categories": ["cs.CV", "cs.ET"], "comment": null, "summary": "Whole slide image (WSI) classification has emerged as a powerful tool in\ncomputational pathology, but remains constrained by domain shifts, e.g., due to\ndifferent organs, diseases, or institution-specific variations. To address this\nchallenge, we propose an Attention-based Generative Latent Replay Continual\nLearning framework (AGLR-CL), in a multiple instance learning (MIL) setup for\ndomain incremental WSI classification. Our method employs Gaussian Mixture\nModels (GMMs) to synthesize WSI representations and patch count distributions,\npreserving knowledge of past domains without explicitly storing original data.\nA novel attention-based filtering step focuses on the most salient patch\nembeddings, ensuring high-quality synthetic samples. This privacy-aware\nstrategy obviates the need for replay buffers and outperforms other buffer-free\ncounterparts while matching the performance of buffer-based solutions. We\nvalidate AGLR-CL on clinically relevant biomarker detection and molecular\nstatus prediction across multiple public datasets with diverse centers, organs,\nand patient cohorts. Experimental results confirm its ability to retain prior\nknowledge and adapt to new domains, offering an effective, privacy-preserving\navenue for domain incremental continual learning in WSI classification."}
{"id": "2505.08389", "pdf": "https://arxiv.org/pdf/2505.08389", "abs": "https://arxiv.org/abs/2505.08389", "authors": ["Rahmatullah Musawi", "Sheng Lu"], "title": "Towards Contamination Resistant Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of large language models (LLMs) has transformed the\nlandscape of natural language processing. Evaluating LLMs properly is crucial\nfor understanding their potential and addressing concerns such as safety.\nHowever, LLM evaluation is confronted by various factors, among which\ncontamination stands out as a key issue that undermines the reliability of\nevaluations. In this work, we introduce the concept of contamination resistance\nto address this challenge. We propose a benchmark based on Caesar ciphers\n(e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an\nexcellent example of a contamination resistant benchmark. We test this\nbenchmark on widely used LLMs under various settings, and we find that these\nmodels struggle with this benchmark when contamination is controlled. Our\nfindings reveal issues in current LLMs and raise important questions regarding\ntheir true capabilities. Our work contributes to the development of\ncontamination resistant benchmarks, enabling more rigorous LLM evaluation and\noffering insights into the true capabilities and limitations of LLMs."}
{"id": "2505.08525", "pdf": "https://arxiv.org/pdf/2505.08525", "abs": "https://arxiv.org/abs/2505.08525", "authors": ["Yiqi Chen", "Ganghai Huang", "Sheng Zhang", "Jianglin Dai"], "title": "Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular topological structures (e.g., fissures and\nvasculature) is critical in various fields to guarantee dependable downstream\nquantitative analysis and modeling. However, in dense prediction tasks such as\nsemantic segmentation and super-resolution, conventional upsampling operators\ncannot accommodate the slenderness of tubular structures and the curvature of\nmorphology. This paper introduces a dynamic snake upsampling operators and a\nboundary-skeleton weighted loss tailored for topological tubular structures.\nSpecifically, we design a snake upsampling operators based on an adaptive\nsampling domain, which dynamically adjusts the sampling stride according to the\nfeature map and selects a set of subpixel sampling points along the serpentine\npath, enabling more accurate subpixel-level feature recovery for tubular\nstructures. Meanwhile, we propose a skeleton-to-boundary increasing weighted\nloss that trades off main body and boundary weight allocation based on mask\nclass ratio and distance field, preserving main body overlap while enhancing\nfocus on target topological continuity and boundary alignment precision.\nExperiments across various domain datasets and backbone networks show that this\nplug-and-play dynamic snake upsampling operator and boundary-skeleton weighted\nloss boost both pixel-wise segmentation accuracy and topological consistency of\nresults."}
{"id": "2505.08392", "pdf": "https://arxiv.org/pdf/2505.08392", "abs": "https://arxiv.org/abs/2505.08392", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "title": "Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models leverage Chain-of-Thought (CoT) prompting for complex\ntasks, but their reasoning traces are often excessively verbose and\ninefficient, leading to significant computational costs and latency. Current\nCoT compression techniques typically rely on generic importance metrics and\nstatic compression rates, which may inadvertently remove functionally critical\ntokens or fail to adapt to varying reasoning complexity. To overcome these\nlimitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic\nCoT compression via supervised fine-tuning. This approach introduces two\nsynergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric\naccurately identifying functionally relevant tokens by measuring the gradient\ninfluence of their intermediate representations on the final answer loss, and\n(2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the\ncompression rate based on runtime model uncertainty while ensuring local\ncoherence through an adaptive N-token constraint. To our knowledge, this is the\nfirst work unifying a goal-oriented, gradient-based importance metric with\ndynamic, uncertainty-aware skipping for CoT compression. Trained on compressed\nMATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization\nacross diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It\nachieves substantial efficiency gains - reducing CoT token counts by over 45%\non average and delivering 1.6-2.0 times inference speedups - while maintaining\nhigh reasoning accuracy. Notably, it significantly outperforms existing\nbaselines by preserving accuracy even at high effective compression rates,\nadvancing the state of the art in the CoT reasoning efficiency-accuracy\ntrade-off."}
{"id": "2505.08527", "pdf": "https://arxiv.org/pdf/2505.08527", "abs": "https://arxiv.org/abs/2505.08527", "authors": ["Zheang Huai", "Hui Tang", "Yi Li", "Zhuangzhuang Chen", "Xiaomeng Li"], "title": "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting", "categories": ["cs.CV"], "comment": null, "summary": "Source-free domain adaptation (SFDA) for segmentation aims at adapting a\nmodel trained in the source domain to perform well in the target domain with\nonly the source model and unlabeled target data.Inspired by the recent success\nof Segment Anything Model (SAM) which exhibits the generality of segmenting\nimages of various modalities and in different domains given human-annotated\nprompts like bounding boxes or points, we for the first time explore the\npotentials of Segment Anything Model for SFDA via automatedly finding an\naccurate bounding box prompt. We find that the bounding boxes directly\ngenerated with existing SFDA approaches are defective due to the domain gap.To\ntackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting\napproach to search for the box prompt. Specifically, the source model is first\ntrained in a feature aggregation phase, which not only preliminarily adapts the\nsource model to the target domain but also builds a feature distribution\nwell-prepared for box prompt search. In the second phase, based on two feature\ndistribution observations, we gradually expand the box prompt with the guidance\nof the target model feature and the SAM feature to handle the class-wise\nclustered target features and the class-wise dispersed target features,\nrespectively. To remove the potentially enlarged false positive regions caused\nby the over-confident prediction of the target model, the refined pseudo-labels\nproduced by SAM are further postprocessed based on connectivity analysis.\nExperiments on 3D and 2D datasets indicate that our approach yields superior\nperformance compared to conventional methods. Code is available at\nhttps://github.com/zheangh/DFG."}
{"id": "2505.08402", "pdf": "https://arxiv.org/pdf/2505.08402", "abs": "https://arxiv.org/abs/2505.08402", "authors": ["Aiyao He", "Sijia Cui", "Shuai Xu", "Yanna Wang", "Bo Xu"], "title": "TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers", "categories": ["cs.CL"], "comment": "Accepted to ICONIP 2024", "summary": "Recently, large language models(LLMs) have played an increasingly important\nrole in solving a wide range of NLP tasks, leveraging their capabilities of\nnatural language understanding and generating. Integration with external tools\nfurther enhances LLMs' effectiveness, providing more precise, timely, and\nspecialized responses. However, LLMs still encounter difficulties with\nnon-executable actions and improper actions, which are primarily attributed to\nincorrect parameters. The process of generating parameters by LLMs is confined\nto the tool level, employing the coarse-grained strategy without considering\nthe different difficulties of various tools. To address this issue, we propose\nTUMS, a novel framework designed to enhance the tool-use capabilities of LLMs\nby transforming tool-level processing into parameter-level processing.\nSpecifically, our framework consists of four key components: (1) an intent\nrecognizer that identifies the user's intent to help LLMs better understand the\ntask; (2) a task decomposer that breaks down complex tasks into simpler\nsubtasks, each involving a tool call; (3) a subtask processor equipped with\nmulti-structure handlers to generate accurate parameters; and (4) an executor.\nOur empirical studies have evidenced the effectiveness and efficiency of the\nTUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on\neasy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key\ncontribution of each part with ablation experiments, offering more insights and\nstimulating future research on Tool-augmented LLMs."}
{"id": "2505.08537", "pdf": "https://arxiv.org/pdf/2505.08537", "abs": "https://arxiv.org/abs/2505.08537", "authors": ["Mohamed Lamine Mekhalfi", "Paul Chippendale", "Fabio Poiesi", "Samuele Bonecher", "Gilberto Osler", "Nicola Zancanella"], "title": "The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning", "categories": ["cs.CV"], "comment": null, "summary": "This research investigates the application of computer vision for rapid,\naccurate, and non-invasive food quality assessment, focusing on the novel\nchallenge of real-time raspberry grading into five distinct classes within an\nindustrial environment as the fruits move along a conveyor belt. To address\nthis, a dedicated dataset of raspberries, namely RaspGrade, was acquired and\nmeticulously annotated. Instance segmentation experiments revealed that\naccurate fruit-level masks can be obtained; however, the classification of\ncertain raspberry grades presents challenges due to color similarities and\nocclusion, while others are more readily distinguishable based on color. The\nacquired and annotated RaspGrade dataset is accessible on HuggingFace at:\nhttps://huggingface.co/datasets/FBK-TeV/RaspGrade."}
{"id": "2505.08435", "pdf": "https://arxiv.org/pdf/2505.08435", "abs": "https://arxiv.org/abs/2505.08435", "authors": ["Mehran Sarmadi", "Morteza Alikhani", "Erfan Zinvandi", "Zahra Pourbahman"], "title": "Hakim: Farsi Text Embedding Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in text embedding have significantly improved natural\nlanguage understanding across many languages, yet Persian remains notably\nunderrepresented in large-scale embedding research. In this paper, we present\nHakim, a novel state-of-the-art Persian text embedding model that achieves a\n8.5% performance improvement over existing approaches on the FaMTEB benchmark,\noutperforming all previously developed Persian language models. As part of this\nwork, we introduce three new datasets - Corpesia, Pairsia-sup, and\nPairsia-unsup - to support supervised and unsupervised training scenarios.\nAdditionally, Hakim is designed for applications in chatbots and\nretrieval-augmented generation (RAG) systems, particularly addressing retrieval\ntasks that require incorporating message history within these systems. We also\npropose a new baseline model built on the BERT architecture. Our language model\nconsistently achieves higher accuracy across various Persian NLP tasks, while\nthe RetroMAE-based model proves particularly effective for textual information\nretrieval applications. Together, these contributions establish a new\nfoundation for advancing Persian language understanding."}
{"id": "2505.08552", "pdf": "https://arxiv.org/pdf/2505.08552", "abs": "https://arxiv.org/abs/2505.08552", "authors": ["Haroon Wahab", "Hassan Ugail", "Irfan Mehmood"], "title": "DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent proliferation of generative AI tools for visual content\ncreation-particularly in the context of visual artworks-has raised serious\nconcerns about copyright infringement and forgery. The large-scale datasets\nused to train these models often contain a mixture of copyrighted and\nnon-copyrighted artworks. Given the tendency of generative models to memorize\ntraining patterns, they are susceptible to varying degrees of copyright\nviolation. Building on the recently proposed DeepfakeArt Challenge benchmark,\nthis work introduces DFA-CON, a contrastive learning framework designed to\ndetect copyright-infringing or forged AI-generated art. DFA-CON learns a\ndiscriminative representation space, posing affinity among original artworks\nand their forged counterparts within a contrastive learning framework. The\nmodel is trained across multiple attack types, including inpainting, style\ntransfer, adversarial perturbation, and cutmix. Evaluation results demonstrate\nrobust detection performance across most attack types, outperforming recent\npretrained foundation models. Code and model checkpoints will be released\npublicly upon acceptance."}
{"id": "2505.08439", "pdf": "https://arxiv.org/pdf/2505.08439", "abs": "https://arxiv.org/abs/2505.08439", "authors": ["Matteo Marulli", "Glauco Panattoni", "Marco Bertini"], "title": "A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court", "categories": ["cs.CL"], "comment": "51 pages", "summary": "Topic modeling in Italian legal research is hindered by the lack of public\ndatasets, limiting the analysis of legal themes in Supreme Court judgments. To\naddress this, we developed a document processing pipeline that produces an\nanonymized dataset optimized for topic modeling.\n  The pipeline integrates document layout analysis (YOLOv8x), optical character\nrecognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964\nand a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and\nthe text recognizer (TrOCR) obtained a character error rate of 0.0047 and a\nword error rate of 0.0248. Compared to OCR-only methods, our dataset improved\ntopic modeling with a diversity score of 0.6198 and a coherence score of\n0.6638.\n  We applied BERTopic to extract topics and used large language models to\ngenerate labels and summaries. Outputs were evaluated against domain expert\ninterpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for\nlabeling and 0.9130 for summarization."}
{"id": "2505.08561", "pdf": "https://arxiv.org/pdf/2505.08561", "abs": "https://arxiv.org/abs/2505.08561", "authors": ["Ayush K. Rai", "Kyle Min", "Tarun Krishna", "Feiyan Hu", "Alan F. Smeaton", "Noel E. O'Connor"], "title": "Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection", "categories": ["cs.CV"], "comment": null, "summary": "Masked video modeling~(MVM) has emerged as a highly effective pre-training\nstrategy for visual foundation models, whereby the model reconstructs masked\nspatiotemporal tokens using information from visible tokens. However, a key\nchallenge in such approaches lies in selecting an appropriate masking strategy.\nPrevious studies have explored predefined masking techniques, including random\nand tube-based masking, as well as approaches that leverage key motion priors,\noptical flow and semantic cues from externally pre-trained models. In this\nwork, we introduce a novel and generalizable Trajectory-Aware Adaptive Token\nSampler (TATS), which models the motion dynamics of tokens and can be\nseamlessly integrated into the masked autoencoder (MAE) framework to select\nmotion-centric tokens in videos. Additionally, we propose a unified training\nstrategy that enables joint optimization of both MAE and TATS from scratch\nusing Proximal Policy Optimization (PPO). We show that our model allows for\naggressive masking without compromising performance on the downstream task of\naction recognition while also ensuring that the pre-training remains memory\nefficient. Extensive experiments of the proposed approach across four\nbenchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51,\ndemonstrate the effectiveness, transferability, generalization, and efficiency\nof our work compared to other state-of-the-art methods."}
{"id": "2505.08450", "pdf": "https://arxiv.org/pdf/2505.08450", "abs": "https://arxiv.org/abs/2505.08450", "authors": ["Kazuki Hayashi", "Hidetaka Kamigaito", "Shinya Kouda", "Taro Watanabe"], "title": "IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a way to complement the\nin-context knowledge of Large Language Models (LLMs) by integrating external\ndocuments. However, real-world applications demand not only accuracy but also\ninterpretability. While dense retrieval methods provide high accuracy, they\nlack interpretability; conversely, sparse retrieval methods offer transparency\nbut often fail to capture the full intent of queries due to their reliance on\nkeyword matching. To address these issues, we introduce IterKey, an LLM-driven\niterative keyword generation framework that enhances RAG via sparse retrieval.\nIterKey consists of three LLM-driven stages: generating keywords for retrieval,\ngenerating answers based on retrieved documents, and validating the answers. If\nvalidation fails, the process iteratively repeats with refined keywords. Across\nfour QA tasks, experimental results show that IterKey achieves 5% to 20%\naccuracy improvements over BM25-based RAG and simple baselines. Its performance\nis comparable to dense retrieval-based RAG and prior iterative query refinement\nmethods using dense models. In summary, IterKey is a novel BM25-based approach\nleveraging LLMs to iteratively refine RAG, effectively balancing accuracy with\ninterpretability."}
{"id": "2505.08568", "pdf": "https://arxiv.org/pdf/2505.08568", "abs": "https://arxiv.org/abs/2505.08568", "authors": ["Xiao Ni", "Carsten Kuehnel", "Xiaoyi Jiang"], "title": "Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections", "categories": ["cs.CV"], "comment": null, "summary": "Rapid advances in deep learning for computer vision have driven the adoption\nof RGB camera-based adaptive traffic light systems to improve traffic safety\nand pedestrian comfort. However, these systems often overlook the needs of\npeople with mobility restrictions. Moreover, the use of RGB cameras presents\nsignificant challenges, including limited detection performance under adverse\nweather or low-visibility conditions, as well as heightened privacy concerns.\nTo address these issues, we propose a fully automated, thermal detector-based\ntraffic light system that dynamically adjusts signal durations for individuals\nwith walking impairments or mobility burden and triggers the auditory signal\nfor visually impaired individuals, thereby advancing towards barrier-free\nintersection for all users. To this end, we build the thermal dataset for\npeople with mobility restrictions (TD4PWMR), designed to capture diverse\npedestrian scenarios, particularly focusing on individuals with mobility aids\nor mobility burden under varying environmental conditions, such as different\nlighting, weather, and crowded urban settings. While thermal imaging offers\nadvantages in terms of privacy and robustness to adverse conditions, it also\nintroduces inherent hurdles for object detection due to its lack of color and\nfine texture details and generally lower resolution of thermal images. To\novercome these limitations, we develop YOLO-Thermal, a novel variant of the\nYOLO architecture that integrates advanced feature extraction and attention\nmechanisms for enhanced detection accuracy and robustness in thermal imaging.\nExperiments demonstrate that the proposed thermal detector outperforms existing\ndetectors, while the proposed traffic light system effectively enhances\nbarrier-free intersection. The source codes and dataset are available at\nhttps://github.com/leon2014dresden/YOLO-THERMAL."}
{"id": "2505.08463", "pdf": "https://arxiv.org/pdf/2505.08463", "abs": "https://arxiv.org/abs/2505.08463", "authors": ["Fujun Zhang", "XiangDong Su"], "title": "RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm\nin applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs\nstill struggle with the discrepancies between the representation obtained from\nthe PLMs' encoder and the optimal input to the PLMs' decoder. This paper\ntackles this challenge by learning to calibrate the representation of PLMs in\nthe latent space. In the proposed representation calibration method (RepCali),\nwe integrate a specific calibration block to the latent space after the encoder\nand use the calibrated output as the decoder input. The merits of the proposed\nRepCali include its universality to all PLMs with encoder-decoder\narchitectures, its plug-and-play nature, and ease of implementation. Extensive\nexperiments on 25 PLM-based models across 8 tasks (including both English and\nChinese datasets) demonstrate that the proposed RepCali offers desirable\nenhancements to PLMs (including LLMs) and significantly improves the\nperformance of downstream tasks. Comparison experiments across 4 benchmark\ntasks indicate that RepCali is superior to the representative fine-tuning\nbaselines."}
{"id": "2505.08581", "pdf": "https://arxiv.org/pdf/2505.08581", "abs": "https://arxiv.org/abs/2505.08581", "authors": ["Haofeng Liu", "Mingqi Gao", "Xuxiao Luo", "Ziyue Wang", "Guanyi Qin", "Junde Wu", "Yueming Jin"], "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking", "categories": ["cs.CV", "eess.IV", "q-bio.TO"], "comment": "Early accepted by MICCAI 2025", "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2."}
{"id": "2505.08464", "pdf": "https://arxiv.org/pdf/2505.08464", "abs": "https://arxiv.org/abs/2505.08464", "authors": ["Lata Pangtey", "Anukriti Bhatnagar", "Shubhi Bansal", "Shahid Shafi Dar", "Nagendra Kumar"], "title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Stance detection is essential for understanding subjective content across\nvarious platforms such as social media, news articles, and online reviews.\nRecent advances in Large Language Models (LLMs) have revolutionized stance\ndetection by introducing novel capabilities in contextual understanding,\ncross-domain generalization, and multimodal analysis. Despite these\nprogressions, existing surveys often lack comprehensive coverage of approaches\nthat specifically leverage LLMs for stance detection. To bridge this critical\ngap, our review article conducts a systematic analysis of stance detection,\ncomprehensively examining recent advancements of LLMs transforming the field,\nincluding foundational concepts, methodologies, datasets, applications, and\nemerging challenges. We present a novel taxonomy for LLM-based stance detection\napproaches, structured along three key dimensions: 1) learning methods,\nincluding supervised, unsupervised, few-shot, and zero-shot; 2) data\nmodalities, such as unimodal, multimodal, and hybrid; and 3) target\nrelationships, encompassing in-target, cross-target, and multi-target\nscenarios. Furthermore, we discuss the evaluation techniques and analyze\nbenchmark datasets and performance trends, highlighting the strengths and\nlimitations of different architectures. Key applications in misinformation\ndetection, political analysis, public health monitoring, and social media\nmoderation are discussed. Finally, we identify critical challenges such as\nimplicit stance expression, cultural biases, and computational constraints,\nwhile outlining promising future directions, including explainable stance\nreasoning, low-resource adaptation, and real-time deployment frameworks. Our\nsurvey highlights emerging trends, open challenges, and future directions to\nguide researchers and practitioners in developing next-generation stance\ndetection systems powered by large language models."}
{"id": "2505.08585", "pdf": "https://arxiv.org/pdf/2505.08585", "abs": "https://arxiv.org/abs/2505.08585", "authors": ["Jorge Quesada", "Chen Zhou", "Prithwijit Chowdhury", "Mohammad Alotaibi", "Ahmad Mustafa", "Yusufjon Kumamnov", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior", "categories": ["cs.CV"], "comment": null, "summary": "Machine learning has taken a critical role in seismic interpretation\nworkflows, especially in fault delineation tasks. However, despite the recent\nproliferation of pretrained models and synthetic datasets, the field still\nlacks a systematic understanding of the generalizability limits of these models\nacross seismic data representing a variety of geologic, acquisition and\nprocessing settings. Distributional shifts between different data sources,\nlimitations in fine-tuning strategies and labeled data accessibility, and\ninconsistent evaluation protocols all represent major roadblocks in the\ndeployment of reliable and robust models in real-world exploration settings. In\nthis paper, we present the first large-scale benchmarking study explicitly\ndesigned to provide answers and guidelines for domain shift strategies in\nseismic interpretation. Our benchmark encompasses over $200$ models trained and\nevaluated on three heterogeneous datasets (synthetic and real data) including\nFaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining,\nfine-tuning, and joint training strategies under varying degrees of domain\nshift. Our analysis highlights the fragility of current fine-tuning practices,\nthe emergence of catastrophic forgetting, and the challenges of interpreting\nperformance in a systematic manner. We establish a robust experimental baseline\nto provide insights into the tradeoffs inherent to current fault delineation\nworkflows, and shed light on directions for developing more generalizable,\ninterpretable and effective machine learning models for seismic interpretation.\nThe insights and analyses reported provide a set of guidelines on the\ndeployment of fault delineation models within seismic interpretation workflows."}
{"id": "2505.08468", "pdf": "https://arxiv.org/pdf/2505.08468", "abs": "https://arxiv.org/abs/2505.08468", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Ahmed Masry", "Mizanur Rahman", "Amran Bhuiyan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025 Industry Track", "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist."}
{"id": "2505.08586", "pdf": "https://arxiv.org/pdf/2505.08586", "abs": "https://arxiv.org/abs/2505.08586", "authors": ["Libo Huang", "Zhulin An", "Chuanguang Yang", "Boyu Diao", "Fei Wang", "Yan Zeng", "Zhifeng Hao", "Yongjun Xu"], "title": "PrePrompt: Predictive prompting for class incremental learning", "categories": ["cs.CV", "I.5.4"], "comment": "16 pages, 29 figures, conference", "summary": "Class Incremental Learning (CIL) based on pre-trained models offers a\npromising direction for open-world continual learning. Existing methods\ntypically rely on correlation-based strategies, where an image's classification\nfeature is used as a query to retrieve the most related key prompts and select\nthe corresponding value prompts for training. However, these approaches face an\ninherent limitation: fitting the entire feature space of all tasks with only a\nfew trainable prompts is fundamentally challenging. We propose Predictive\nPrompting (PrePrompt), a novel CIL framework that circumvents correlation-based\nlimitations by leveraging pre-trained models' natural classification ability to\npredict task-specific prompts. Specifically, PrePrompt decomposes CIL into a\ntwo-stage prediction framework: task-specific prompt prediction followed by\nlabel prediction. While theoretically appealing, this framework risks bias\ntoward recent classes due to missing historical data for older classifier\ncalibration. PrePrompt then mitigates this by incorporating feature\ntranslation, dynamically balancing stability and plasticity. Experiments across\nmultiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art\nprompt-based CIL methods. The code will be released upon acceptance."}
{"id": "2505.08498", "pdf": "https://arxiv.org/pdf/2505.08498", "abs": "https://arxiv.org/abs/2505.08498", "authors": ["Takumi Shibata", "Yuichi Miyamura"], "title": "LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled zero-shot\nautomated essay scoring (AES), providing a promising way to reduce the cost and\neffort of essay scoring in comparison with manual grading. However, most\nexisting zero-shot approaches rely on LLMs to directly generate absolute\nscores, which often diverge from human evaluations owing to model biases and\ninconsistent scoring. To address these limitations, we propose LLM-based\nComparative Essay Scoring (LCES), a method that formulates AES as a pairwise\ncomparison task. Specifically, we instruct LLMs to judge which of two essays is\nbetter, collect many such comparisons, and convert them into continuous scores.\nConsidering that the number of possible comparisons grows quadratically with\nthe number of essays, we improve scalability by employing RankNet to\nefficiently transform LLM preferences into scalar scores. Experiments using AES\nbenchmark datasets show that LCES outperforms conventional zero-shot methods in\naccuracy while maintaining computational efficiency. Moreover, LCES is robust\nacross different LLM backbones, highlighting its applicability to real-world\nzero-shot AES."}
{"id": "2505.08589", "pdf": "https://arxiv.org/pdf/2505.08589", "abs": "https://arxiv.org/abs/2505.08589", "authors": ["Barak Pinkovich", "Boaz Matalon", "Ehud Rivlin", "Hector Rotstein"], "title": "MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI)\ndataset comprising 2525 images taken by a drone flying over dense urban\nenvironments. MESSI is unique in two main features. First, it contains images\nfrom various altitudes, allowing us to investigate the effect of depth on\nsemantic segmentation. Second, it includes images taken from several different\nurban regions (at different altitudes). This is important since the variety\ncovers the visual richness captured by a drone's 3D flight, performing\nhorizontal and vertical maneuvers. MESSI contains images annotated with\nlocation, orientation, and the camera's intrinsic parameters and can be used to\ntrain a deep neural network for semantic segmentation or other applications of\ninterest (e.g., localization, navigation, and tracking). This paper describes\nthe dataset and provides annotation details. It also explains how semantic\nsegmentation was performed using several neural network models and shows\nseveral relevant statistics. MESSI will be published in the public domain to\nserve as an evaluation benchmark for semantic segmentation using images\ncaptured by a drone or similar vehicle flying over a dense urban environment."}
{"id": "2505.08504", "pdf": "https://arxiv.org/pdf/2505.08504", "abs": "https://arxiv.org/abs/2505.08504", "authors": ["Jeongwoo Kang", "Maximin Coavoux", "Cédric Lopez", "Didier Schwab"], "title": "Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding", "categories": ["cs.CL"], "comment": "published at Insights from Negative Results in NLP (workshop EMNLP\n  2025)", "summary": "Sequence-to-sequence models are widely used to train Abstract Meaning\nRepresentation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR\ngraphs have to be linearized into a one-line text format. While Penman encoding\nis typically used for this purpose, we argue that it has limitations: (1) for\ndeep graphs, some closely related nodes are located far apart in the linearized\ntext (2) Penman's tree-based encoding necessitates inverse roles to handle node\nre-entrancy, doubling the number of relation types to predict. To address these\nissues, we propose a triple-based linearization method and compare its\nefficiency with Penman linearization. Although triples are well suited to\nrepresent a graph, our results suggest room for improvement in triple encoding\nto better compete with Penman's concise and explicit representation of a nested\ngraph structure."}
{"id": "2505.08601", "pdf": "https://arxiv.org/pdf/2505.08601", "abs": "https://arxiv.org/abs/2505.08601", "authors": ["Jinchi Zhu", "Zhou Zhao", "Hailong Lei", "Xiaoguang Wang", "Jialiang Lu", "Jing Li", "Qianqian Tang", "Jiachen Shen", "Gui-Song Xia", "Bo Du", "Yongchao Xu"], "title": "Rejoining fragmented ancient bamboo slips with physics-driven deep learning", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "comment": null, "summary": "Bamboo slips are a crucial medium for recording ancient civilizations in East\nAsia, and offers invaluable archaeological insights for reconstructing the Silk\nRoad, studying material culture exchanges, and global history. However, many\nexcavated bamboo slips have been fragmented into thousands of irregular pieces,\nmaking their rejoining a vital yet challenging step for understanding their\ncontent. Here we introduce WisePanda, a physics-driven deep learning framework\ndesigned to rejoin fragmented bamboo slips. Based on the physics of fracture\nand material deterioration, WisePanda automatically generates synthetic\ntraining data that captures the physical properties of bamboo fragmentations.\nThis approach enables the training of a matching network without requiring\nmanually paired samples, providing ranked suggestions to facilitate the\nrejoining process. Compared to the leading curve matching method, WisePanda\nincreases Top-50 matching accuracy from 36\\% to 52\\%. Archaeologists using\nWisePanda have experienced substantial efficiency improvements (approximately\n20 times faster) when rejoining fragmented bamboo slips. This research\ndemonstrates that incorporating physical principles into deep learning models\ncan significantly enhance their performance, transforming how archaeologists\nrestore and study fragmented artifacts. WisePanda provides a new paradigm for\naddressing data scarcity in ancient artifact restoration through physics-driven\nmachine learning."}
{"id": "2505.08546", "pdf": "https://arxiv.org/pdf/2505.08546", "abs": "https://arxiv.org/abs/2505.08546", "authors": ["Chiara Manna", "Afra Alishahi", "Frédéric Blain", "Eva Vanmassenhove"], "title": "Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "While gender bias in modern Neural Machine Translation (NMT) systems has\nreceived much attention, traditional evaluation metrics do not to fully capture\nthe extent to which these systems integrate contextual gender cues. We propose\na novel evaluation metric called Minimal Pair Accuracy (MPA), which measures\nthe reliance of models on gender cues for gender disambiguation. MPA is\ndesigned to go beyond surface-level gender accuracy metrics by focusing on\nwhether models adapt to gender cues in minimal pairs -- sentence pairs that\ndiffer solely in the gendered pronoun, namely the explicit indicator of the\ntarget's entity gender in the source language (EN). We evaluate a number of NMT\nmodels on the English-Italian (EN--IT) language pair using this metric, we show\nthat they ignore available gender cues in most cases in favor of (statistical)\nstereotypical gender interpretation. We further show that in anti-stereotypical\ncases, these models tend to more consistently take masculine gender cues into\naccount while ignoring the feminine cues. Furthermore, we analyze the attention\nhead weights in the encoder component and show that while all models encode\ngender information to some extent, masculine cues elicit a more diffused\nresponse compared to the more concentrated and specialized responses to\nfeminine gender cues."}
{"id": "2505.08604", "pdf": "https://arxiv.org/pdf/2505.08604", "abs": "https://arxiv.org/abs/2505.08604", "authors": ["Yu-Jen Chen", "Xueyang Li", "Yiyu Shi", "Tsung-Yi Ho"], "title": "Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Out-of-distribution (OOD) detection is essential for ensuring the reliability\nof deep learning models in medical imaging applications. This work is motivated\nby the observation that class activation maps (CAMs) for in-distribution (ID)\ndata typically emphasize regions that are highly relevant to the model's\npredictions, whereas OOD data often lacks such focused activations. By masking\ninput images with inverted CAMs, the feature representations of ID data undergo\nmore substantial changes compared to those of OOD data, offering a robust\ncriterion for differentiation. In this paper, we introduce a novel unsupervised\nOOD detection framework, Multi-Exit Class Activation Map (MECAM), which\nleverages multi-exit CAMs and feature masking. By utilizing mult-exit networks\nthat combine CAMs from varying resolutions and depths, our method captures both\nglobal and local feature representations, thereby enhancing the robustness of\nOOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and\nPathMNIST, and test its performance against three medical OOD datasets, RSNA\nPneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN.\nComprehensive comparisons with state-of-the-art OOD detection methods validate\nthe effectiveness of our approach. Our findings emphasize the potential of\nmulti-exit networks and feature masking for advancing unsupervised OOD\ndetection in medical imaging, paving the way for more reliable and\ninterpretable models in clinical practice."}
{"id": "2505.08588", "pdf": "https://arxiv.org/pdf/2505.08588", "abs": "https://arxiv.org/abs/2505.08588", "authors": ["Yumou Wei", "Paulo Carvalho", "John Stamper"], "title": "Small but Significant: On the Promise of Small Language Models for Accessible AIED", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "This vision paper advocates using small language models (e.g., Phi-2)\n  in AI for education (AIED)", "summary": "GPT has become nearly synonymous with large language models (LLMs), an\nincreasingly popular term in AIED proceedings. A simple keyword-based search\nreveals that 61% of the 76 long and short papers presented at AIED 2024\ndescribe novel solutions using LLMs to address some of the long-standing\nchallenges in education, and 43% specifically mention GPT. Although LLMs\npioneered by GPT create exciting opportunities to strengthen the impact of AI\non education, we argue that the field's predominant focus on GPT and other\nresource-intensive LLMs (with more than 10B parameters) risks neglecting the\npotential impact that small language models (SLMs) can make in providing\nresource-constrained institutions with equitable and affordable access to\nhigh-quality AI tools. Supported by positive results on knowledge component\n(KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as\nPhi-2 can produce an effective solution without elaborate prompting strategies.\nHence, we call for more attention to developing SLM-based AIED approaches."}
{"id": "2505.08605", "pdf": "https://arxiv.org/pdf/2505.08605", "abs": "https://arxiv.org/abs/2505.08605", "authors": ["Zhe Li", "Hadrien Reynaud", "Bernhard Kainz"], "title": "Leveraging Multi-Modal Information to Enhance Dataset Distillation", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Dataset distillation aims to create a compact and highly representative\nsynthetic dataset that preserves the knowledge of a larger real dataset. While\nexisting methods primarily focus on optimizing visual representations,\nincorporating additional modalities and refining object-level information can\nsignificantly improve the quality of distilled datasets. In this work, we\nintroduce two key enhancements to dataset distillation: caption-guided\nsupervision and object-centric masking. To integrate textual information, we\npropose two strategies for leveraging caption features: the feature\nconcatenation, where caption embeddings are fused with visual features at the\nclassification stage, and caption matching, which introduces a caption-based\nalignment loss during training to ensure semantic coherence between real and\nsynthetic data. Additionally, we apply segmentation masks to isolate target\nobjects and remove background distractions, introducing two loss functions\ndesigned for object-centric learning: masked feature alignment loss and masked\ngradient matching loss. Comprehensive evaluations demonstrate that integrating\ncaption-based guidance and object-centric masking enhances dataset\ndistillation, leading to synthetic datasets that achieve superior performance\non downstream tasks."}
{"id": "2505.08590", "pdf": "https://arxiv.org/pdf/2505.08590", "abs": "https://arxiv.org/abs/2505.08590", "authors": ["Hussien Al-Asi", "Jordan P Reynolds", "Shweta Agarwal", "Bryan J Dangott", "Aziza Nassar", "Zeynettin Akkus"], "title": "Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models", "categories": ["cs.CL", "q-bio.QM"], "comment": null, "summary": "Advancements in artificial intelligence (AI) are transforming pathology by\nintegrat-ing large language models (LLMs) with retrieval-augmented generation\n(RAG) and domain-specific foundation models. This study explores the\napplication of RAG-enhanced LLMs coupled with pathology foundation models for\nthyroid cytology diagnosis, addressing challenges in cytological\ninterpretation, standardization, and diagnostic accuracy. By leveraging a\ncurated knowledge base, RAG facilitates dy-namic retrieval of relevant case\nstudies, diagnostic criteria, and expert interpreta-tion, improving the\ncontextual understanding of LLMs. Meanwhile, pathology foun-dation models,\ntrained on high-resolution pathology images, refine feature extrac-tion and\nclassification capabilities. The fusion of these AI-driven approaches en-hances\ndiagnostic consistency, reduces variability, and supports pathologists in\ndis-tinguishing benign from malignant thyroid lesions. Our results demonstrate\nthat integrating RAG with pathology-specific LLMs significantly improves\ndiagnostic efficiency and interpretability, paving the way for AI-assisted\nthyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for\ncorrect prediction of surgi-cal pathology diagnosis from thyroid cytology\nsamples."}
{"id": "2505.08607", "pdf": "https://arxiv.org/pdf/2505.08607", "abs": "https://arxiv.org/abs/2505.08607", "authors": ["Yuran Wang", "Yingping Liang", "Ying Fu"], "title": "Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World", "categories": ["cs.CV"], "comment": null, "summary": "Stereo matching methods rely on dense pixel-wise ground truth labels, which\nare laborious to obtain, especially for real-world datasets. The scarcity of\nlabeled data and domain gaps between synthetic and real-world images also pose\nnotable challenges. In this paper, we propose a novel framework,\n\\textbf{BooSTer}, that leverages both vision foundation models and large-scale\nmixed image sources, including synthetic, real, and single-view images. First,\nto fully unleash the potential of large-scale single-view images, we design a\ndata generation strategy combining monocular depth estimation and diffusion\nmodels to generate dense stereo matching data from single-view images. Second,\nto tackle sparse labels in real-world datasets, we transfer knowledge from\nmonocular depth estimation models, using pseudo-mono depth labels and a dynamic\nscale- and shift-invariant loss for additional supervision. Furthermore, we\nincorporate vision foundation model as an encoder to extract robust and\ntransferable features, boosting accuracy and generalization. Extensive\nexperiments on benchmark datasets demonstrate the effectiveness of our\napproach, achieving significant improvements in accuracy over existing methods,\nparticularly in scenarios with limited labeled data and domain shifts."}
{"id": "2505.08600", "pdf": "https://arxiv.org/pdf/2505.08600", "abs": "https://arxiv.org/abs/2505.08600", "authors": ["Danying Ge", "Jianhua Gao", "Qizhi Jiang", "Yifei Feng", "Weixing Ji"], "title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding", "categories": ["cs.CL", "I.2.7"], "comment": "10 pages, 10 figures, 2 tables", "summary": "Speculative decoding, which combines a draft model with a target model, has\nemerged as an effective approach to accelerate large language model (LLM)\ninference. However, existing methods often face a trade-off between the\nacceptance rate and decoding speed in downstream tasks due to the limited\ncapacity of the draft model, making it difficult to ensure efficiency across\ndiverse tasks. To address this problem, we propose a speculative decoding\nalgorithm tailored for downstream task optimization. It includes an automatic\ntask partitioning and assigning method, which automatically categorizes\ndownstream tasks into different sub-tasks and assigns them to a set of\nheterogeneous draft models. Each draft model is aligned with the target model\nusing task-specific data, thereby enhancing the consistency of inference\nresults. In addition, our proposed method incorporates an online lightweight\nprompt classifier to dynamically route prompts to the appropriate draft model.\nExperimental results demonstrate that the proposed method improves draft\naccuracy by 6% to 50% over vanilla speculative decoding, while achieving a\nspeedup of 1.10x to 2.64x in LLM inference."}
{"id": "2505.08614", "pdf": "https://arxiv.org/pdf/2505.08614", "abs": "https://arxiv.org/abs/2505.08614", "authors": ["Ziyuan He", "Zhiqing Guo", "Liejun Wang", "Gaobo Yang", "Yunfeng Diao", "Dan Ma"], "title": "WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Deepfake technology poses increasing risks such as privacy invasion and\nidentity theft. To address these threats, we propose WaveGuard, a proactive\nwatermarking framework that enhances robustness and imperceptibility via\nfrequency-domain embedding and graph-based structural consistency.\nSpecifically, we embed watermarks into high-frequency sub-bands using Dual-Tree\nComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph\nNeural Network (SC-GNN) to preserve visual quality. We also design an attention\nmodule to refine embedding precision. Experimental results on face swap and\nreenactment tasks demonstrate that WaveGuard outperforms state-of-the-art\nmethods in both robustness and visual quality. Code is available at\nhttps://github.com/vpsg-research/WaveGuard."}
{"id": "2505.08651", "pdf": "https://arxiv.org/pdf/2505.08651", "abs": "https://arxiv.org/abs/2505.08651", "authors": ["Chen Wu", "Yin Song"], "title": "Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 6 figures, ACL 2025 (Industry Track)", "summary": "We present MegaBeam-Mistral-7B, a language model that supports 512K-token\ncontext length. Our work addresses practical limitations in long-context\ntraining, supporting real-world tasks such as compliance monitoring and\nverification. Evaluated on three long-context benchmarks, our 7B-parameter\nmodel demonstrates superior in-context learning performance on HELMET and\nrobust retrieval and tracing capability on RULER. It is currently the only open\nmodel to achieve competitive long-range reasoning on BABILong at 512K context\nlength without RAG or targeted fine-tuning. Released as fully open source under\nthe Apache 2.0 license, the model has been downloaded over 100,000 times on\nHugging Face. Model available at:\nhttps://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k"}
{"id": "2505.08617", "pdf": "https://arxiv.org/pdf/2505.08617", "abs": "https://arxiv.org/abs/2505.08617", "authors": ["Zhaochen Su", "Linjie Li", "Mingyang Song", "Yunzhuo Hao", "Zhengyuan Yang", "Jun Zhang", "Guanjie Chen", "Jiawei Gu", "Juntao Li", "Xiaoye Qu", "Yu Cheng"], "title": "OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "While humans can flexibly leverage interactive visual cognition for complex\nproblem-solving, enabling Large Vision-Language Models (LVLMs) to learn\nsimilarly adaptive behaviors with visual tools remains challenging. A\nsignificant hurdle is the current lack of standardized infrastructure, which\nhinders integrating diverse tools, generating rich interaction data, and\ntraining robust agents effectively. To address these gaps, we introduce\nOpenThinkIMG, the first open-source, comprehensive end-to-end framework for\ntool-augmented LVLMs. It features standardized vision tool interfaces, scalable\ntrajectory generation for policy initialization, and a flexible training\nenvironment. Furthermore, considering supervised fine-tuning (SFT) on static\ndemonstrations offers limited policy generalization for dynamic tool\ninvocation, we propose a novel reinforcement learning (RL) framework V-ToolRL\nto train LVLMs to learn adaptive policies for invoking external vision tools.\nV-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies\nby directly optimizing for task success using feedback from tool interactions.\nWe empirically validate V-ToolRL on challenging chart reasoning tasks. Our\nRL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its\nSFT-initialized counterpart (+28.83 points) and surpasses established\nsupervised tool-learning baselines like Taco and CogCom by an average of +12.7\npoints. Notably, it also surpasses prominent closed-source models like GPT-4.1\nby +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational\nframework for advancing dynamic, tool-augmented visual reasoning, helping the\ncommunity develop AI agents that can genuinely \"think with images\"."}
{"id": "2505.08662", "pdf": "https://arxiv.org/pdf/2505.08662", "abs": "https://arxiv.org/abs/2505.08662", "authors": ["Marcus Buckmann", "Quynh Anh Nguyen", "Edward Hill"], "title": "Revealing economic facts: LLMs know more than they say", "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC", "I.2.7"], "comment": "34 pages, 17 figures", "summary": "We investigate whether the hidden states of large language models (LLMs) can\nbe used to estimate and impute economic and financial statistics. Focusing on\ncounty-level (e.g. unemployment) and firm-level (e.g. total assets) variables,\nwe show that a simple linear model trained on the hidden states of open-source\nLLMs outperforms the models' text outputs. This suggests that hidden states\ncapture richer economic information than the responses of the LLMs reveal\ndirectly. A learning curve analysis indicates that only a few dozen labelled\nexamples are sufficient for training. We also propose a transfer learning\nmethod that improves estimation accuracy without requiring any labelled data\nfor the target variable. Finally, we demonstrate the practical utility of\nhidden-state representations in super-resolution and data imputation tasks."}
{"id": "2505.08644", "pdf": "https://arxiv.org/pdf/2505.08644", "abs": "https://arxiv.org/abs/2505.08644", "authors": ["Holly Dinkel", "Marcel Büsching", "Alberta Longhini", "Brian Coltin", "Trey Smith", "Danica Kragic", "Mårten Björkman", "Timothy Bretl"], "title": "DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting", "categories": ["cs.CV", "cs.RO"], "comment": "5 pages, 2 figures, presented at the 2025 5th Workshop: Reflections\n  on Representations and Manipulating Deformable Objects at the IEEE\n  International Conference on Robotics and Automation. RMDO workshop\n  (https://deformable-workshop.github.io/icra2025/)", "summary": "This work presents DLO-Splatting, an algorithm for estimating the 3D shape of\nDeformable Linear Objects (DLOs) from multi-view RGB images and gripper state\ninformation through prediction-update filtering. The DLO-Splatting algorithm\nuses a position-based dynamics model with shape smoothness and rigidity\ndampening corrections to predict the object shape. Optimization with a 3D\nGaussian Splatting-based rendering loss iteratively renders and refines the\nprediction to align it with the visual observations in the update step. Initial\nexperiments demonstrate promising results in a knot tying scenario, which is\nchallenging for existing vision-only methods."}
{"id": "2505.08690", "pdf": "https://arxiv.org/pdf/2505.08690", "abs": "https://arxiv.org/abs/2505.08690", "authors": ["Sheng Liang", "Hang Lv", "Zhihao Wen", "Yaxiong Wu", "Yongyue Zhang", "Hao Wang", "Yong Liu"], "title": "Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation", "categories": ["cs.CL", "I.2.7"], "comment": "15 pages, 3 figures", "summary": "Event extraction (EE) is a fundamental task in natural language processing\n(NLP) that involves identifying and extracting event information from\nunstructured text. Effective EE in real-world scenarios requires two key steps:\nselecting appropriate schemas from hundreds of candidates and executing the\nextraction process. Existing research exhibits two critical gaps: (1) the rigid\nschema fixation in existing pipeline systems, and (2) the absence of benchmarks\nfor evaluating joint schema matching and extraction. Although large language\nmodels (LLMs) offer potential solutions, their schema hallucination tendencies\nand context window limitations pose challenges for practical deployment. In\nresponse, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel\nparadigm combining schema paraphrasing with schema retrieval-augmented\ngeneration. ASEE adeptly retrieves paraphrased schemas and accurately generates\ntargeted structures. To facilitate rigorous evaluation, we construct the\nMulti-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which\nsystematically consolidates 12 datasets across diverse domains, complexity\nlevels, and language settings. Extensive evaluations on MD-SEE show that our\nproposed ASEE demonstrates strong adaptability across various scenarios,\nsignificantly improving the accuracy of event extraction."}
{"id": "2505.08665", "pdf": "https://arxiv.org/pdf/2505.08665", "abs": "https://arxiv.org/abs/2505.08665", "authors": ["Edoardo Bianchi", "Antonio Liotta"], "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment."}
{"id": "2505.08734", "pdf": "https://arxiv.org/pdf/2505.08734", "abs": "https://arxiv.org/abs/2505.08734", "authors": ["Ben Yao", "Qiuchi Li", "Yazhou Zhang", "Siyu Yang", "Bohan Zhang", "Prayag Tiwari", "Jing Qin"], "title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 10 figures, 16 tables", "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues."}
{"id": "2505.08685", "pdf": "https://arxiv.org/pdf/2505.08685", "abs": "https://arxiv.org/abs/2505.08685", "authors": ["Meritxell Riera-Marin", "Sikha O K", "Julia Rodriguez-Comas", "Matthias Stefan May", "Zhaohong Pan", "Xiang Zhou", "Xiaokun Liang", "Franciskus Xaverius Erick", "Andrea Prenner", "Cedric Hemon", "Valentin Boussot", "Jean-Louis Dillenseger", "Jean-Claude Nunes", "Abdul Qayyum", "Moona Mazher", "Steven A Niederer", "Kaisar Kushibar", "Carlos Martin-Isla", "Petia Radeva", "Karim Lekadir", "Theodore Barfoot", "Luis C. Garcia Peraza Herrera", "Ben Glocker", "Tom Vercauteren", "Lucas Gago", "Justin Englemann", "Joy-Marie Kleiss", "Anton Aubanell", "Andreu Antolin", "Javier Garcia-Lopez", "Miguel A. Gonzalez Ballester", "Adrian Galdran"], "title": "Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results", "categories": ["cs.CV"], "comment": "This challenge was hosted in MICCAI 2024", "summary": "Deep learning (DL) has become the dominant approach for medical image\nsegmentation, yet ensuring the reliability and clinical applicability of these\nmodels requires addressing key challenges such as annotation variability,\ncalibration, and uncertainty estimation. This is why we created the Calibration\nand Uncertainty for multiRater Volume Assessment in multiorgan Segmentation\n(CURVAS), which highlights the critical role of multiple annotators in\nestablishing a more comprehensive ground truth, emphasizing that segmentation\nis inherently subjective and that leveraging inter-annotator variability is\nessential for robust model evaluation. Seven teams participated in the\nchallenge, submitting a variety of DL models evaluated using metrics such as\nDice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and\nContinuous Ranked Probability Score (CRPS). By incorporating consensus and\ndissensus ground truth, we assess how DL models handle uncertainty and whether\ntheir confidence estimates align with true segmentation performance. Our\nfindings reinforce the importance of well-calibrated models, as better\ncalibration is strongly correlated with the quality of the results.\nFurthermore, we demonstrate that segmentation models trained on diverse\ndatasets and enriched with pre-trained knowledge exhibit greater robustness,\nparticularly in cases deviating from standard anatomical structures. Notably,\nthe best-performing models achieved high DSC and well-calibrated uncertainty\nestimates. This work underscores the need for multi-annotator ground truth,\nthorough calibration assessments, and uncertainty-aware evaluations to develop\ntrustworthy and clinically reliable DL-based medical image segmentation models."}
{"id": "2505.08739", "pdf": "https://arxiv.org/pdf/2505.08739", "abs": "https://arxiv.org/abs/2505.08739", "authors": ["Xiaoliang Luo", "Xinyi Xu", "Michael Ramscar", "Bradley C. Love"], "title": "Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies", "categories": ["cs.CL"], "comment": null, "summary": "Can autoregressive large language models (LLMs) learn consistent probability\ndistributions when trained on sequences in different token orders? We prove\nformally that for any well-defined probability distribution, sequence\nperplexity is invariant under any factorization, including forward, backward,\nor arbitrary permutations. This result establishes a rigorous theoretical\nfoundation for studying how LLMs learn from data and defines principled\nprotocols for empirical evaluation. Applying these protocols, we show that\nprior studies examining ordering effects suffer from critical methodological\nflaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted\norders on scientific text. We find systematic deviations from theoretical\ninvariance across all orderings with arbitrary permutations strongly deviating\nfrom both forward and backward models, which largely (but not completely)\nagreed with one another. Deviations were traceable to differences in\nself-attention, reflecting positional and locality biases in processing. Our\ntheoretical and empirical results provide novel avenues for understanding\npositional biases in LLMs and suggest methods for detecting when LLMs'\nprobability distributions are inconsistent and therefore untrustworthy."}
{"id": "2505.08695", "pdf": "https://arxiv.org/pdf/2505.08695", "abs": "https://arxiv.org/abs/2505.08695", "authors": ["Zhanjie Zhang", "Quanwei Zhang", "Junsheng Luan", "Mengyuan Yang", "Yun Wang", "Lei Zhao"], "title": "SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model", "categories": ["cs.CV"], "comment": "Accepted by Neural Networks", "summary": "Given an arbitrary content and style image, arbitrary style transfer aims to\nrender a new stylized\n  image which preserves the content image's structure and possesses the style\nimage's style. Existing\n  arbitrary style transfer methods are based on either small models or\npre-trained large-scale models.\n  The small model-based methods fail to generate high-quality stylized images,\nbringing artifacts and\n  disharmonious patterns. The pre-trained large-scale model-based methods can\ngenerate high-quality\n  stylized images but struggle to preserve the content structure and cost long\ninference time. To this\n  end, we propose a new framework, called SPAST, to generate high-quality\nstylized images with\n  less inference time. Specifically, we design a novel Local-global Window Size\nStylization Module\n  (LGWSSM)tofuse style features into content features. Besides, we introduce a\nnovel style prior loss,\n  which can dig out the style priors from a pre-trained large-scale model into\nthe SPAST and motivate\n  the SPAST to generate high-quality stylized images with short inference\ntime.We conduct abundant\n  experiments to verify that our proposed method can generate high-quality\nstylized images and less\n  inference time compared with the SOTA arbitrary style transfer methods."}
{"id": "2505.08750", "pdf": "https://arxiv.org/pdf/2505.08750", "abs": "https://arxiv.org/abs/2505.08750", "authors": ["Yanxi Zhang", "Xin Cong", "Zhong Zhang", "Xiao Liu", "Dongyan Zhao", "Yesai Wu"], "title": "AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Actual causality (AC), a fundamental aspect of causal reasoning (CR), is\nresponsible for attribution and responsibility assignment in real-world\nscenarios. However, existing LLM-based methods lack grounding in formal AC\ntheory, resulting in limited interpretability. Therefore, we propose AC-Reason,\na semi-formal reasoning framework that identifies causally relevant events\nwithin an AC scenario, infers the values of their formal causal factors (e.g.,\nsufficiency, necessity, and normality), and answers AC queries via a\ntheory-guided algorithm with explanations. While AC-Reason does not explicitly\nconstruct a causal graph, it operates over variables in the underlying causal\nstructure to support principled reasoning. To enable comprehensive evaluation,\nwe introduce AC-Bench, a new benchmark built upon and substantially extending\nBig-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully\nannotated samples, each with detailed reasoning steps and focuses solely on\nactual causation. The case study shows that synthesized samples in AC-Bench\npresent greater challenges for LLMs. Extensive experiments on BBH-CJ and\nAC-Bench show that AC-Reason consistently improves LLM performance over\nbaselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy\nof 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 +\nAC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further\nenables fine-grained analysis of reasoning faithfulness, revealing that only\nQwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful\nreasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation\nstudy proves that integrating AC theory into LLMs is highly effective, with the\nproposed algorithm contributing the most significant performance gains."}
{"id": "2505.08705", "pdf": "https://arxiv.org/pdf/2505.08705", "abs": "https://arxiv.org/abs/2505.08705", "authors": ["Yanru An", "Ling Gui", "Qiang Hu", "Chunlei Cai", "Tianxiao Ye", "Xiaoyun Zhang", "Yanfeng Wang"], "title": "Controllable Image Colorization with Instance-aware Texts and Masks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, the application of deep learning in image colorization has received\nwidespread attention. The maturation of diffusion models has further advanced\nthe development of image colorization models. However, current mainstream image\ncolorization models still face issues such as color bleeding and color binding\nerrors, and cannot colorize images at the instance level. In this paper, we\npropose a diffusion-based colorization method MT-Color to achieve precise\ninstance-aware colorization with use-provided guidance. To tackle color\nbleeding issue, we design a pixel-level mask attention mechanism that\nintegrates latent features and conditional gray image features through\ncross-attention. We use segmentation masks to construct cross-attention masks,\npreventing pixel information from exchanging between different instances. We\nalso introduce an instance mask and text guidance module that extracts instance\nmasks and text representations of each instance, which are then fused with\nlatent features through self-attention, utilizing instance masks to form\nself-attention masks to prevent instance texts from guiding the colorization of\nother areas, thus mitigating color binding errors. Furthermore, we apply a\nmulti-instance sampling strategy, which involves sampling each instance region\nseparately and then fusing the results. Additionally, we have created a\nspecialized dataset for instance-level colorization tasks, GPT-color, by\nleveraging large visual language models on existing image datasets. Qualitative\nand quantitative experiments show that our model and dataset outperform\nprevious methods and datasets."}
{"id": "2505.08751", "pdf": "https://arxiv.org/pdf/2505.08751", "abs": "https://arxiv.org/abs/2505.08751", "authors": ["Saurabh Dash", "Yiyang Nan", "John Dang", "Arash Ahmadian", "Shivalika Singh", "Madeline Smith", "Bharat Venkitesh", "Vlad Shmyhlo", "Viraat Aryabumi", "Walter Beller-Morales", "Jeremy Pekmez", "Jason Ozuzu", "Pierre Richemond", "Acyr Locatelli", "Nick Frosst", "Phil Blunsom", "Aidan Gomez", "Ivan Zhang", "Marzieh Fadaee", "Manoj Govindassamy", "Sudip Roy", "Matthias Gallé", "Beyza Ermis", "Ahmet Üstün", "Sara Hooker"], "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance."}
{"id": "2505.08723", "pdf": "https://arxiv.org/pdf/2505.08723", "abs": "https://arxiv.org/abs/2505.08723", "authors": ["Xiaolei Qin", "Di Wang", "Jing Zhang", "Fengxiang Wang", "Xin Su", "Bo Du", "Liangpei Zhang"], "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series", "categories": ["cs.CV"], "comment": null, "summary": "Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo."}
{"id": "2505.08775", "pdf": "https://arxiv.org/pdf/2505.08775", "abs": "https://arxiv.org/abs/2505.08775", "authors": ["Rahul K. Arora", "Jason Wei", "Rebecca Soskin Hicks", "Preston Bowman", "Joaquin Quiñonero-Candela", "Foivos Tsimpourlas", "Michael Sharman", "Meghan Shah", "Andrea Vallone", "Alex Beutel", "Johannes Heidecke", "Karan Singhal"], "title": "HealthBench: Evaluating Large Language Models Towards Improved Human Health", "categories": ["cs.CL"], "comment": "Blog: https://openai.com/index/healthbench/ Code:\n  https://github.com/openai/simple-evals", "summary": "We present HealthBench, an open-source benchmark measuring the performance\nand safety of large language models in healthcare. HealthBench consists of\n5,000 multi-turn conversations between a model and an individual user or\nhealthcare professional. Responses are evaluated using conversation-specific\nrubrics created by 262 physicians. Unlike previous multiple-choice or\nshort-answer benchmarks, HealthBench enables realistic, open-ended evaluation\nthrough 48,562 unique rubric criteria spanning several health contexts (e.g.,\nemergencies, transforming clinical data, global health) and behavioral\ndimensions (e.g., accuracy, instruction following, communication). HealthBench\nperformance over the last two years reflects steady initial progress (compare\nGPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3\nscores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms\nGPT-4o and is 25 times cheaper. We additionally release two HealthBench\nvariations: HealthBench Consensus, which includes 34 particularly important\ndimensions of model behavior validated via physician consensus, and HealthBench\nHard, where the current top score is 32%. We hope that HealthBench grounds\nprogress towards model development and applications that benefit human health."}
{"id": "2505.08725", "pdf": "https://arxiv.org/pdf/2505.08725", "abs": "https://arxiv.org/abs/2505.08725", "authors": ["Zongchuang Zhao", "Haoyu Fu", "Dingkang Liang", "Xin Zhou", "Dingyuan Zhang", "Hongwei Xie", "Bing Wang", "Xiang Bai"], "title": "Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving", "categories": ["cs.CV"], "comment": "The dataset and code will be released at\n  https://github.com/zc-zhao/DriveMonkey", "summary": "The Large Visual-Language Models (LVLMs) have significantly advanced image\nunderstanding. Their comprehension and reasoning capabilities enable promising\napplications in autonomous driving scenarios. However, existing research\ntypically focuses on front-view perspectives and partial objects within scenes,\nstruggling to achieve comprehensive scene understanding. Meanwhile, existing\nLVLMs suffer from the lack of mapping relationship between 2D and 3D and\ninsufficient integration of 3D object localization and instruction\nunderstanding. To tackle these limitations, we first introduce NuInteract, a\nlarge-scale dataset with over 1.5M multi-view image language pairs spanning\ndense scene captions and diverse interactive tasks. Furthermore, we propose\nDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs\nwith a spatial processor using a series of learnable queries. The spatial\nprocessor, designed as a plug-and-play component, can be initialized with\npre-trained 3D detectors to improve 3D perception. Our experiments show that\nDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable\nimprovement on the 3D visual grounding task. The dataset and code will be\nreleased at https://github.com/zc-zhao/DriveMonkey."}
{"id": "2505.07864", "pdf": "https://arxiv.org/pdf/2505.07864", "abs": "https://arxiv.org/abs/2505.07864", "authors": ["Takamitsu Omasa", "Ryo Koshihara", "Masumi Morishige"], "title": "Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "11 pages, 1 figures,", "summary": "Flowcharts are indispensable tools in software design and business-process\nanalysis, yet current vision-language models (VLMs) frequently misinterpret the\ndirectional arrows and graph topology that set these diagrams apart from\nnatural images. We introduce a seven-stage pipeline grouped into three broader\nprocesses: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical\ncharacter recognition (OCR) to extract node text; and (3) construction of a\nstructured prompt that guides the VLMs. Tested on a 90-question benchmark\ndistilled from 30 annotated flowcharts, the method raises overall accuracy from\n80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The\ngain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);\nbranch-result questions improve more modestly, and before-step questions remain\ndifficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same\ntrends, reinforcing the advantage of explicit arrow encoding. Limitations\ninclude dependence on detector and OCR precision, the small evaluation set, and\nresidual errors at nodes with multiple incoming edges. Future work will enlarge\nthe benchmark with synthetic and handwritten flowcharts and assess the approach\non Business Process Model and Notation (BPMN) and Unified Modeling Language\n(UML)."}
{"id": "2505.08747", "pdf": "https://arxiv.org/pdf/2505.08747", "abs": "https://arxiv.org/abs/2505.08747", "authors": ["Huiyan Qi", "Bin Zhu", "Chong-Wah Ngo", "Jingjing Chen", "Ee-Peng Lim"], "title": "Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication in ACM International Conference on\n  Multimedia Retrieval 2025", "summary": "Nutrition estimation is an important component of promoting healthy eating\nand mitigating diet-related health risks. Despite advances in tasks such as\nfood classification and ingredient recognition, progress in nutrition\nestimation is limited due to the lack of datasets with nutritional annotations.\nTo address this issue, we introduce FastFood, a dataset with 84,446 images\nacross 908 fast food categories, featuring ingredient and nutritional\nannotations. In addition, we propose a new model-agnostic Visual-Ingredient\nFeature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating\nvisual and ingredient features. Ingredient robustness is improved through\nsynonym replacement and resampling strategies during training. The\ningredient-aware visual feature fusion module combines ingredient features and\nvisual representation to achieve accurate nutritional prediction. During\ntesting, ingredient predictions are refined using large multimodal models by\ndata augmentation and majority voting. Our experiments on both FastFood and\nNutrition5k datasets validate the effectiveness of our proposed method built in\ndifferent backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the\nimportance of ingredient information in nutrition estimation.\nhttps://huiyanqi.github.io/fastfood-nutrition-estimation/."}
{"id": "2505.07865", "pdf": "https://arxiv.org/pdf/2505.07865", "abs": "https://arxiv.org/abs/2505.07865", "authors": ["Fan Zhang", "Tianyu Liu", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Donghao Zhou", "Yefeng Zheng", "Kun Wang", "Xian Wu", "Pheng-Ann Heng"], "title": "CellVerse: Do Large Language Models Really Understand Cell Biology?", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "q-bio.CB"], "comment": null, "summary": "Recent studies have demonstrated the feasibility of modeling single-cell data\nas natural languages and the potential of leveraging powerful large language\nmodels (LLMs) for understanding cell biology. However, a comprehensive\nevaluation of LLMs' performance on language-driven single-cell analysis tasks\nstill remains unexplored. Motivated by this challenge, we introduce CellVerse,\na unified language-centric question-answering benchmark that integrates four\ntypes of single-cell multi-omics data and encompasses three hierarchical levels\nof single-cell analysis tasks: cell type annotation (cell-level), drug response\nprediction (drug-level), and perturbation analysis (gene-level). Going beyond\nthis, we systematically evaluate the performance across 14 open-source and\nclosed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the\nexperimental results reveal: (1) Existing specialist models (C2S-Pythia) fail\nto make reasonable decisions across all sub-tasks within CellVerse, while\ngeneralist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit\npreliminary understanding capabilities within the realm of cell biology. (2)\nThe performance of current LLMs falls short of expectations and has substantial\nroom for improvement. Notably, in the widely studied drug response prediction\ntask, none of the evaluated LLMs demonstrate significant performance\nimprovement over random guessing. CellVerse offers the first large-scale\nempirical demonstration that significant challenges still remain in applying\nLLMs to cell biology. By introducing CellVerse, we lay the foundation for\nadvancing cell biology through natural languages and hope this paradigm could\nfacilitate next-generation single-cell analysis."}
{"id": "2505.08765", "pdf": "https://arxiv.org/pdf/2505.08765", "abs": "https://arxiv.org/abs/2505.08765", "authors": ["Yatai Ji", "Zhengqiu Zhu", "Yong Zhao", "Beidan Liu", "Chen Gao", "Yihao Zhao", "Sihang Qiu", "Yue Hu", "Quanjun Yin", "Yong Li"], "title": "Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial Visual Object Search (AVOS) tasks in urban environments require\nUnmanned Aerial Vehicles (UAVs) to autonomously search for and identify target\nobjects using visual and textual cues without external guidance. Existing\napproaches struggle in complex urban environments due to redundant semantic\nprocessing, similar object distinction, and the exploration-exploitation\ndilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,\nthe first benchmark dataset for autonomous search of common urban objects. This\ndataset comprises 2,420 tasks across six object categories with varying\ndifficulty levels, enabling comprehensive evaluation of UAV agents' search\ncapabilities. To solve the AVOS tasks, we also propose PRPSearcher\n(Perception-Reasoning-Planning Searcher), a novel agentic method powered by\nmulti-modal large language models (MLLMs) that mimics human three-tier\ncognition. Specifically, PRPSearcher constructs three specialized maps: an\nobject-centric dynamic semantic map enhancing spatial perception, a 3D\ncognitive map based on semantic attraction values for target reasoning, and a\n3D uncertainty map for balanced exploration-exploitation search. Also, our\napproach incorporates a denoising mechanism to mitigate interference from\nsimilar objects and utilizes an Inspiration Promote Thought (IPT) prompting\nmechanism for adaptive action planning. Experimental results on CityAVOS\ndemonstrate that PRPSearcher surpasses existing baselines in both success rate\nand search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and\n-46.40% NE). While promising, the performance gap compared to humans highlights\nthe need for better semantic reasoning and spatial exploration capabilities in\nAVOS tasks. This work establishes a foundation for future advances in embodied\ntarget search. Dataset and source code are available at\nhttps://anonymous.4open.science/r/CityAVOS-3DF8."}
{"id": "2505.07902", "pdf": "https://arxiv.org/pdf/2505.07902", "abs": "https://arxiv.org/abs/2505.07902", "authors": ["Ruikun Hou", "Babette Bühler", "Tim Fütterer", "Efe Bozkir", "Peter Gerjets", "Ulrich Trautwein", "Enkelejda Kasneci"], "title": "Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "The 18th International Conference on Educational Data Mining (EDM\n  2025)", "summary": "Classroom discourse is an essential vehicle through which teaching and\nlearning take place. Assessing different characteristics of discursive\npractices and linking them to student learning achievement enhances the\nunderstanding of teaching quality. Traditional assessments rely on manual\ncoding of classroom observation protocols, which is time-consuming and costly.\nDespite many studies utilizing AI techniques to analyze classroom discourse at\nthe utterance level, investigations into the evaluation of discursive practices\nthroughout an entire lesson segment remain limited. To address this gap, our\nstudy proposes a novel text-centered multimodal fusion architecture to assess\nthe quality of three discourse components grounded in the Global Teaching\nInSights (GTI) observation protocol: Nature of Discourse, Questioning, and\nExplanations. First, we employ attention mechanisms to capture inter- and\nintra-modal interactions from transcript, audio, and video streams. Second, a\nmulti-task learning approach is adopted to jointly predict the quality scores\nof the three components. Third, we formulate the task as an ordinal\nclassification problem to account for rating level order. The effectiveness of\nthese designed elements is demonstrated through an ablation study on the GTI\nGermany dataset containing 92 videotaped math lessons. Our results highlight\nthe dominant role of text modality in approaching this task. Integrating\nacoustic features enhances the model's consistency with human ratings,\nachieving an overall Quadratic Weighted Kappa score of 0.384, comparable to\nhuman inter-rater reliability (0.326). Our study lays the groundwork for the\nfuture development of automated discourse quality assessment to support teacher\nprofessional development through timely feedback on multidimensional discourse\npractices."}
{"id": "2505.07840", "pdf": "https://arxiv.org/pdf/2505.07840", "abs": "https://arxiv.org/abs/2505.07840", "authors": ["Alavikunhu Panthakkan", "S M Anzar", "K. Sherin", "Saeed Al Mansoori", "Hussain Al-Ahmad"], "title": "Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Precision farming relies on accurate vegetation monitoring to enhance crop\nproductivity and promote sustainable agricultural practices. This study\npresents a comprehensive evaluation of UAV-based imaging for vegetation health\nassessment in a palm tree cultivation region in Dubai. By comparing\nmultispectral and RGB image data, we demonstrate that RGBbased vegetation\nindices offer performance comparable to more expensive multispectral indices,\nproviding a cost-effective alternative for large-scale agricultural monitoring.\nUsing UAVs equipped with multispectral sensors, indices such as NDVI and SAVI\nwere computed to categorize vegetation into healthy, moderate, and stressed\nconditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered\nsimilar results in vegetation classification and stress detection. Our findings\nhighlight the practical benefits of integrating RGB imagery into precision\nfarming, reducing operational costs while maintaining accuracy in plant health\nmonitoring. This research underscores the potential of UAVbased RGB imaging as\na powerful tool for precision agriculture, enabling broader adoption of\ndata-driven decision-making in crop management. By leveraging the strengths of\nboth multispectral and RGB imaging, this work advances the state of UAV\napplications in agriculture, paving the way for more efficient and scalable\nfarming solutions."}
{"id": "2505.07908", "pdf": "https://arxiv.org/pdf/2505.07908", "abs": "https://arxiv.org/abs/2505.07908", "authors": ["Karahan Sarıtaş", "Çağatay Yıldız"], "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport."}
{"id": "2505.07851", "pdf": "https://arxiv.org/pdf/2505.07851", "abs": "https://arxiv.org/abs/2505.07851", "authors": ["Jaeyoung Huh", "Ankur Kapoor", "Young-Ho Kim"], "title": "Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Intra-cardiac Echocardiography (ICE) plays a crucial role in\nElectrophysiology (EP) and Structural Heart Disease (SHD) interventions by\nproviding high-resolution, real-time imaging of cardiac structures. However,\nexisting navigation methods rely on electromagnetic (EM) tracking, which is\nsusceptible to interference and position drift, or require manual adjustments\nbased on operator expertise. To overcome these limitations, we propose a novel\nanatomy-aware pose estimation system that determines the ICE catheter position\nand orientation solely from ICE images, eliminating the need for external\ntracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep\nlearning model, which captures spatial relationships between ICE images and\nanatomical structures. The model is trained on a clinically acquired dataset of\n851 subjects, including ICE images paired with position and orientation labels\nnormalized to the left atrium (LA) mesh. ICE images are patchified into 16x16\nembeddings and processed through a transformer network, where a [CLS] token\nindependently predicts position and orientation via separate linear layers. The\nmodel is optimized using a Mean Squared Error (MSE) loss function, balancing\npositional and orientational accuracy. Experimental results demonstrate an\naverage positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98\ndeg, 10.47 deg) across x, y, and z axes, confirming the model accuracy.\nQualitative assessments further validate alignment between predicted and target\nviews within 3D cardiac meshes. This AI-driven system enhances procedural\nefficiency, reduces operator workload, and enables real-time ICE catheter\nlocalization for tracking-free procedures. The proposed method can function\nindependently or complement existing mapping systems like CARTO, offering a\ntransformative approach to ICE-guided interventions."}
{"id": "2505.07912", "pdf": "https://arxiv.org/pdf/2505.07912", "abs": "https://arxiv.org/abs/2505.07912", "authors": ["Tim Wittenborg", "Constantin Sebastian Tremel", "Niklas Stehr", "Oliver Karras", "Markus Stocker", "Sören Auer"], "title": "SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts", "categories": ["cs.DL", "cs.CL", "cs.MM"], "comment": "18 pages, 10 figures, submitted to TPDL 2025", "summary": "Democratic societies need accessible, reliable information. Videos and\nPodcasts have established themselves as the medium of choice for civic\ndissemination, but also as carriers of misinformation. The emerging Science\nCommunication Knowledge Infrastructure (SciCom KI) curating non-textual media\nis still fragmented and not adequately equipped to scale against the content\nflood. Our work sets out to support the SciCom KI with a central, collaborative\nplatform, the SciCom Wiki, to facilitate FAIR (findable, accessible,\ninteroperable, reusable) media representation and the fact-checking of their\ncontent, particularly for videos and podcasts. Building an open-source service\nsystem centered around Wikibase, we survey requirements from 53 stakeholders,\nrefine these in 11 interviews, and evaluate our prototype based on these\nrequirements with another 14 participants. To address the most requested\nfeature, fact-checking, we developed a neurosymbolic computational\nfact-checking approach, converting heterogenous media into knowledge graphs.\nThis increases machine-readability and allows comparing statements against\nequally represented ground-truth. Our computational fact-checking tool was\niteratively evaluated through 10 expert interviews, a public user survey with\n43 participants verified the necessity and usability of our tool. Overall, our\nfindings identified several needs to systematically support the SciCom KI. The\nSciCom Wiki, as a FAIR digital library complementing our neurosymbolic\ncomputational fact-checking framework, was found suitable to address the raised\nrequirements. Further, we identified that the SciCom KI is severely\nunderdeveloped regarding FAIR knowledge and related systems facilitating its\ncollaborative creation and curation. Our system can provide a central knowledge\nnode, yet a collaborative effort is required to scale against the imminent\n(mis-)information flood."}
{"id": "2505.07864", "pdf": "https://arxiv.org/pdf/2505.07864", "abs": "https://arxiv.org/abs/2505.07864", "authors": ["Takamitsu Omasa", "Ryo Koshihara", "Masumi Morishige"], "title": "Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "11 pages, 1 figures,", "summary": "Flowcharts are indispensable tools in software design and business-process\nanalysis, yet current vision-language models (VLMs) frequently misinterpret the\ndirectional arrows and graph topology that set these diagrams apart from\nnatural images. We introduce a seven-stage pipeline grouped into three broader\nprocesses: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical\ncharacter recognition (OCR) to extract node text; and (3) construction of a\nstructured prompt that guides the VLMs. Tested on a 90-question benchmark\ndistilled from 30 annotated flowcharts, the method raises overall accuracy from\n80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The\ngain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp);\nbranch-result questions improve more modestly, and before-step questions remain\ndifficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same\ntrends, reinforcing the advantage of explicit arrow encoding. Limitations\ninclude dependence on detector and OCR precision, the small evaluation set, and\nresidual errors at nodes with multiple incoming edges. Future work will enlarge\nthe benchmark with synthetic and handwritten flowcharts and assess the approach\non Business Process Model and Notation (BPMN) and Unified Modeling Language\n(UML)."}
{"id": "2505.08052", "pdf": "https://arxiv.org/pdf/2505.08052", "abs": "https://arxiv.org/abs/2505.08052", "authors": ["Kourosh Shahnazari", "Seyed Moein Ayyoubzadeh"], "title": "NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This study formalizes a computational model to simulate classical Persian\npoets' dynamics of influence through constructing a multi-dimensional\nsimilarity network. Using a rigorously curated dataset based on Ganjoor's\ncorpus, we draw upon semantic, lexical, stylistic, thematic, and metrical\nfeatures to demarcate each poet's corpus. Each is contained within weighted\nsimilarity matrices, which are then appended to generate an aggregate graph\nshowing poet-to-poet influence. Further network investigation is carried out to\nidentify key poets, style hubs, and bridging poets by calculating degree,\ncloseness, betweenness, eigenvector, and Katz centrality measures. Further, for\ntypological insight, we use the Louvain community detection algorithm to\ndemarcate clusters of poets sharing both style and theme coherence, which\ncorrespond closely to acknowledged schools of literature like Sabk-e Hindi,\nSabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a\nnew data-driven view of Persian literature distinguished between canonical\nsignificance and interextual influence, thus highlighting relatively\nlesser-known figures who hold great structural significance. Combining\ncomputational linguistics with literary study, this paper produces an\ninterpretable and scalable model for poetic tradition, enabling retrospective\nreflection as well as forward-looking research within digital humanities."}
{"id": "2505.07866", "pdf": "https://arxiv.org/pdf/2505.07866", "abs": "https://arxiv.org/abs/2505.07866", "authors": ["Abdullah", "Tao Huang", "Ickjai Lee", "Euijoon Ahn"], "title": "Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "pages 36, 6 figures", "summary": "The diffusion model has recently emerged as a potent approach in computer\nvision, demonstrating remarkable performances in the field of generative\nartificial intelligence. Capable of producing high-quality synthetic images,\ndiffusion models have been successfully applied across a range of applications.\nHowever, a significant challenge remains with the high computational cost\nassociated with training and generating these models. This study focuses on the\nefficiency and inference time of diffusion-based generative models,\nhighlighting their applications in both natural and medical imaging. We present\nthe most recent advances in diffusion models by categorizing them into three\nkey models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent\nDiffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play\na crucial role in medical imaging, where producing fast, reliable, and\nhigh-quality medical images is essential for accurate analysis of abnormalities\nand disease diagnosis. We first investigate the general framework of DDPM, LDM,\nand WDM and discuss the computational complexity gap filled by these models in\nnatural and medical imaging. We then discuss the current limitations of these\nmodels as well as the opportunities and future research directions in medical\nimaging."}
{"id": "2505.08080", "pdf": "https://arxiv.org/pdf/2505.08080", "abs": "https://arxiv.org/abs/2505.08080", "authors": ["Dong Shu", "Xuansheng Wu", "Haiyan Zhao", "Mengnan Du", "Ninghao Liu"], "title": "Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 3 figures", "summary": "Sparse Autoencoders (SAEs) have recently emerged as powerful tools for\ninterpreting and steering the internal representations of large language models\n(LLMs). However, conventional approaches to analyzing SAEs typically rely\nsolely on input-side activations, without considering the causal influence\nbetween each latent feature and the model's output. This work is built on two\nkey hypotheses: (1) activated latents do not contribute equally to the\nconstruction of the model's output, and (2) only latents with high causal\ninfluence are effective for model steering. To validate these hypotheses, we\npropose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method\nthat identifies the most influential latents by incorporating output-side\ngradient information."}
{"id": "2505.07879", "pdf": "https://arxiv.org/pdf/2505.07879", "abs": "https://arxiv.org/abs/2505.07879", "authors": ["Wei Yang", "Jingjing Fu", "Rui Wang", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "19 pages, 6 figures, 17 tables", "summary": "Vision-language retrieval-augmented generation (RAG) has become an effective\napproach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which\nrequires external knowledge beyond the visual content presented in images. The\neffectiveness of Vision-language RAG systems hinges on multimodal retrieval,\nwhich is inherently challenging due to the diverse modalities and knowledge\ngranularities in both queries and knowledge bases. Existing methods have not\nfully tapped into the potential interplay between these elements. We propose a\nmultimodal RAG system featuring a coarse-to-fine, multi-step retrieval that\nharmonizes multiple granularities and modalities to enhance efficacy. Our\nsystem begins with a broad initial search aligning knowledge granularity for\ncross-modal retrieval, followed by a multimodal fusion reranking to capture the\nnuanced multimodal information for top entity selection. A text reranker then\nfilters out the most relevant fine-grained section for augmented generation.\nExtensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our\nmethod achieves state-of-the-art retrieval performance and highly competitive\nanswering results, underscoring its effectiveness in advancing KB-VQA systems."}
{"id": "2505.08137", "pdf": "https://arxiv.org/pdf/2505.08137", "abs": "https://arxiv.org/abs/2505.08137", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Siew-Kei Lam", "Tuan Ngo"], "title": "Large Language Models for Computer-Aided Design: A Survey", "categories": ["cs.LG", "cs.CL", "cs.GR", "cs.MM"], "comment": null, "summary": "Large Language Models (LLMs) have seen rapid advancements in recent years,\nwith models like ChatGPT and DeepSeek, showcasing their remarkable capabilities\nacross diverse domains. While substantial research has been conducted on LLMs\nin various fields, a comprehensive review focusing on their integration with\nComputer-Aided Design (CAD) remains notably absent. CAD is the industry\nstandard for 3D modeling and plays a vital role in the design and development\nof products across different industries. As the complexity of modern designs\nincreases, the potential for LLMs to enhance and streamline CAD workflows\npresents an exciting frontier. This article presents the first systematic\nsurvey exploring the intersection of LLMs and CAD. We begin by outlining the\nindustrial significance of CAD, highlighting the need for AI-driven innovation.\nNext, we provide a detailed overview of the foundation of LLMs. We also examine\nboth closed-source LLMs as well as publicly available models. The core of this\nreview focuses on the various applications of LLMs in CAD, providing a taxonomy\nof six key areas where these models are making considerable impact. Finally, we\npropose several promising future directions for further advancements, which\noffer vast opportunities for innovation and are poised to shape the future of\nCAD technology. Github:\nhttps://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy"}
{"id": "2505.07887", "pdf": "https://arxiv.org/pdf/2505.07887", "abs": "https://arxiv.org/abs/2505.07887", "authors": ["Songyin Wu", "Zhaoyang Lv", "Yufeng Zhu", "Duncan Frost", "Zhengqin Li", "Ling-Qi Yan", "Carl Ren", "Richard Newcombe", "Zhao Dong"], "title": "Monocular Online Reconstruction with Enhanced Detail Preservation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose an online 3D Gaussian-based dense mapping framework for\nphotorealistic details reconstruction from a monocular image stream. Our\napproach addresses two key challenges in monocular online reconstruction:\ndistributing Gaussians without relying on depth maps and ensuring both local\nand global consistency in the reconstructed maps. To achieve this, we introduce\ntwo key modules: the Hierarchical Gaussian Management Module for effective\nGaussian distribution and the Global Consistency Optimization Module for\nmaintaining alignment and coherence at all scales. In addition, we present the\nMulti-level Occupancy Hash Voxels (MOHV), a structure that regularizes\nGaussians for capturing details across multiple levels of granularity. MOHV\nensures accurate reconstruction of both fine and coarse geometries and\ntextures, preserving intricate details while maintaining overall structural\nintegrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our\nframework achieves superior reconstruction quality with high computational\nefficiency. Moreover, it integrates seamlessly with various tracking systems,\nensuring generality and scalability."}
{"id": "2505.08148", "pdf": "https://arxiv.org/pdf/2505.08148", "abs": "https://arxiv.org/abs/2505.08148", "authors": ["Sunday Oyinlola Ogundoyin", "Muhammad Ikram", "Hassan Jameel Asghar", "Benjamin Zi Hao Zhao", "Dali Kaafar"], "title": "A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Millions of users leverage generative pretrained transformer (GPT)-based\nlanguage models developed by leading model providers for a wide range of tasks.\nTo support enhanced user interaction and customization, many platforms-such as\nOpenAI-now enable developers to create and publish tailored model instances,\nknown as custom GPTs, via dedicated repositories or application stores. These\ncustom GPTs empower users to browse and interact with specialized applications\ndesigned to meet specific needs. However, as custom GPTs see growing adoption,\nconcerns regarding their security vulnerabilities have intensified. Existing\nresearch on these vulnerabilities remains largely theoretical, often lacking\nempirical, large-scale, and statistically rigorous assessments of associated\nrisks.\n  In this study, we analyze 14,904 custom GPTs to assess their susceptibility\nto seven exploitable threats, such as roleplay-based attacks, system prompt\nleakage, phishing content generation, and malicious code synthesis, across\nvarious categories and popularity tiers within the OpenAI marketplace. We\nintroduce a multi-metric ranking system to examine the relationship between a\ncustom GPT's popularity and its associated security risks.\n  Our findings reveal that over 95% of custom GPTs lack adequate security\nprotections. The most prevalent vulnerabilities include roleplay-based\nvulnerabilities (96.51%), system prompt leakage (92.20%), and phishing\n(91.22%). Furthermore, we demonstrate that OpenAI's foundational models exhibit\ninherent security weaknesses, which are often inherited or amplified in custom\nGPTs. These results highlight the urgent need for enhanced security measures\nand stricter content moderation to ensure the safe deployment of GPT-based\napplications."}
{"id": "2505.07906", "pdf": "https://arxiv.org/pdf/2505.07906", "abs": "https://arxiv.org/abs/2505.07906", "authors": ["Geunho Choi", "Changhwan Lee", "Jieun Kim", "Insoo Ye", "Keeyoung Jung", "Inchul Park"], "title": "Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors", "categories": ["cond-mat.mtrl-sci", "cs.CV", "cs.LG"], "comment": "37 pages, 10 figures", "summary": "Microstructure often dictates materials performance, yet it is rarely treated\nas an explicit design variable because microstructure is hard to quantify,\npredict, and optimize. Here, we introduce an image centric, closed-loop\nframework that makes microstructural morphology into a controllable objective\nand demonstrate its use case with Li- and Mn-rich layered oxide cathode\nprecursors. This work presents an integrated, AI driven framework for the\npredictive design and optimization of lithium-ion battery cathode precursor\nsynthesis. This framework integrates a diffusion-based image generation model,\na quantitative image analysis pipeline, and a particle swarm optimization (PSO)\nalgorithm. By extracting key morphological descriptors such as texture,\nsphericity, and median particle size (D50) from SEM images, the platform\naccurately predicts SEM like morphologies resulting from specific\ncoprecipitation conditions, including reaction time-, solution concentration-,\nand pH-dependent structural changes. Optimization then pinpoints synthesis\nparameters that yield user defined target morphologies, as experimentally\nvalidated by the close agreement between predicted and synthesized structures.\nThis framework offers a practical strategy for data driven materials design,\nenabling both forward prediction and inverse design of synthesis conditions and\npaving the way toward autonomous, image guided microstructure engineering."}
{"id": "2505.08203", "pdf": "https://arxiv.org/pdf/2505.08203", "abs": "https://arxiv.org/abs/2505.08203", "authors": ["Li Zhang"], "title": "Not that Groove: Zero-Shot Symbolic Music Editing", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Most work in AI music generation focused on audio, which has seen limited use\nin the music production industry due to its rigidity. To maximize flexibility\nwhile assuming only textual instructions from producers, we are among the first\nto tackle symbolic music editing. We circumvent the known challenge of lack of\nlabeled data by proving that LLMs with zero-shot prompting can effectively edit\ndrum grooves. The recipe of success is a creatively designed format that\ninterfaces LLMs and music, while we facilitate evaluation by providing an\nevaluation dataset with annotated unit tests that highly aligns with musicians'\njudgment."}
{"id": "2505.07908", "pdf": "https://arxiv.org/pdf/2505.07908", "abs": "https://arxiv.org/abs/2505.07908", "authors": ["Karahan Sarıtaş", "Çağatay Yıldız"], "title": "A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "In this reproduction study, we revisit recent claims that self-attention\nimplements kernel principal component analysis (KPCA) (Teo et al., 2024),\npositing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix\nof the keys, and (ii) that self-attention projects queries onto the principal\ncomponent axes of the key matrix $K$ in a feature space. Our analysis reveals\nthree critical inconsistencies: (1) No alignment exists between learned\nself-attention value vectors and what is proposed in the KPCA perspective, with\naverage similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA\n(Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating\nnegligible correspondence; (2) Reported decreases in reconstruction loss\n$J_\\text{proj}$, arguably justifying the claim that the self-attention\nminimizes the projection error of KPCA, are misinterpreted, as the quantities\ninvolved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix\neigenvalue statistics, introduced to justify that $V$ captures the eigenvector\nof the gram matrix, are irreproducible without undocumented\nimplementation-specific adjustments. Across 10 transformer architectures, we\nconclude that the KPCA interpretation of self-attention lacks empirical\nsupport."}
{"id": "2505.08445", "pdf": "https://arxiv.org/pdf/2505.08445", "abs": "https://arxiv.org/abs/2505.08445", "authors": ["Adel Ammar", "Anis Koubaa", "Omer Nacar", "Wadii Boulila"], "title": "Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models achieve high task performance yet often hallucinate or\nrely on outdated knowledge. Retrieval-augmented generation (RAG) addresses\nthese gaps by coupling generation with external search. We analyse how\nhyperparameters influence speed and quality in RAG systems, covering Chroma and\nFaiss vector stores, chunking policies, cross-encoder re-ranking, and\ntemperature, and we evaluate six metrics: faithfulness, answer correctness,\nanswer relevancy, context precision, context recall, and answer similarity.\nChroma processes queries 13% faster, whereas Faiss yields higher retrieval\nprecision, revealing a clear speed-accuracy trade-off. Naive fixed-length\nchunking with small windows and minimal overlap outperforms semantic\nsegmentation while remaining the quickest option. Re-ranking provides modest\ngains in retrieval quality yet increases runtime by roughly a factor of 5, so\nits usefulness depends on latency constraints. These results help practitioners\nbalance computational cost and accuracy when tuning RAG systems for\ntransparent, up-to-date responses. Finally, we re-evaluate the top\nconfigurations with a corrective RAG workflow and show that their advantages\npersist when the model can iteratively request additional evidence. We obtain a\nnear-perfect context precision (99%), which demonstrates that RAG systems can\nachieve extremely high retrieval accuracy with the right combination of\nhyperparameters, with significant implications for applications where retrieval\nquality directly impacts downstream task performance, such as clinical decision\nsupport in healthcare."}
{"id": "2505.08082", "pdf": "https://arxiv.org/pdf/2505.08082", "abs": "https://arxiv.org/abs/2505.08082", "authors": ["Yuting Cai", "Shaohuai Liu", "Chao Tian", "Le Xie"], "title": "Fréchet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "Generative artificial intelligence (AI) models in smart grids have advanced\nsignificantly in recent years due to their ability to generate large amounts of\nsynthetic data, which would otherwise be difficult to obtain in the real world\ndue to confidentiality constraints. A key challenge in utilizing such synthetic\ndata is how to assess the data quality produced from such generative models.\nTraditional Euclidean distance-based metrics only reflect pair-wise relations\nbetween two individual samples, and could fail in evaluating quality\ndifferences between groups of synthetic datasets. In this work, we propose a\nnovel metric based on the Fr\\'{e}chet Distance (FD) estimated between two\ndatasets in a learned feature space. The proposed method evaluates the quality\nof generation from a distributional perspective. Empirical results demonstrate\nthe superiority of the proposed metric across timescales and models, enhancing\nthe reliability of data-driven decision-making in smart grid operations."}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622", "abs": "https://arxiv.org/abs/2505.08622", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."}
{"id": "2505.08163", "pdf": "https://arxiv.org/pdf/2505.08163", "abs": "https://arxiv.org/abs/2505.08163", "authors": ["Andrew Cart", "Shaohu Zhang", "Melanie Escue", "Xugui Zhou", "Haitao Zhao", "Prashanth BusiReddyGari", "Beiyu Lin", "Shuang Li"], "title": "Decoding Neighborhood Environments with Large Language Models", "categories": ["cs.AI", "cs.CV"], "comment": "8 pages", "summary": "Neighborhood environments include physical and environmental conditions such\nas housing quality, roads, and sidewalks, which significantly influence human\nhealth and well-being. Traditional methods for assessing these environments,\nincluding field surveys and geographic information systems (GIS), are\nresource-intensive and challenging to evaluate neighborhood environments at\nscale. Although machine learning offers potential for automated analysis, the\nlaborious process of labeling training data and the lack of accessible models\nhinder scalability. This study explores the feasibility of large language\nmodels (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood\nenvironments (e.g., sidewalk and powerline) at scale. We train a robust\nYOLOv11-based model, which achieves an average accuracy of 99.13% in detecting\nsix environmental indicators, including streetlight, sidewalk, powerline,\napartment, single-lane road, and multilane road. We then evaluate four LLMs,\nincluding ChatGPT, Gemini, Claude, and Grok, to assess their feasibility,\nrobustness, and limitations in identifying these indicators, with a focus on\nthe impact of prompting strategies and fine-tuning. We apply majority voting\nwith the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs\ncould be a useful tool to decode the neighborhood environment without any\ntraining effort."}
{"id": "2505.08638", "pdf": "https://arxiv.org/pdf/2505.08638", "abs": "https://arxiv.org/abs/2505.08638", "authors": ["Darshan Deshpande", "Varun Gangal", "Hersh Mehta", "Jitin Krishnan", "Anand Kannappan", "Rebecca Qian"], "title": "TRAIL: Trace Reasoning and Agentic Issue Localization", "categories": ["cs.AI", "cs.CL"], "comment": "Dataset link: https://huggingface.co/datasets/PatronusAI/TRAIL", "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows."}
{"id": "2505.08191", "pdf": "https://arxiv.org/pdf/2505.08191", "abs": "https://arxiv.org/abs/2505.08191", "authors": ["Yipu Zhang", "Jiawei Liang", "Jian Peng", "Jiang Xu", "Wei Zhang"], "title": "SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices", "categories": ["cs.AR", "cs.CV"], "comment": "Accepted by DATE 2025", "summary": "Neural rendering has gained prominence for its high-quality output, which is\ncrucial for AR/VR applications. However, its large voxel grid data size and\nirregular access patterns challenge real-time processing on edge devices. While\nprevious works have focused on improving data locality, they have not\nadequately addressed the issue of large voxel grid sizes, which necessitate\nfrequent off-chip memory access and substantial on-chip memory. This paper\nintroduces SpNeRF, a software-hardware co-design solution tailored for sparse\nvolumetric neural rendering. We first identify memory-bound rendering\ninefficiencies and analyze the inherent sparsity in the voxel grid data of\nneural rendering. To enhance efficiency, we propose novel preprocessing and\nonline decoding steps, reducing the memory size for voxel grid. The\npreprocessing step employs hash mapping to support irregular data access while\nmaintaining a minimal memory size. The online decoding step enables efficient\non-chip sparse voxel grid processing, incorporating bitmap masking to mitigate\nPSNR loss caused by hash collisions. To further optimize performance, we design\na dedicated hardware architecture supporting our sparse voxel grid processing\ntechnique. Experimental results demonstrate that SpNeRF achieves an average\n21.07$\\times$ reduction in memory size while maintaining comparable PSNR\nlevels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and\nNeuRex.Edge, our design achieves speedups of 95.1$\\times$, 63.5$\\times$,\n1.5$\\times$ and 10.3$\\times$, and improves energy efficiency by 625.6$\\times$,\n529.1$\\times$, 4$\\times$, and 4.4$\\times$, respectively."}
{"id": "2505.08704", "pdf": "https://arxiv.org/pdf/2505.08704", "abs": "https://arxiv.org/abs/2505.08704", "authors": ["K M Sajjadul Islam", "Ayesha Siddika Nipu", "Jiawei Wu", "Praveen Madiraju"], "title": "LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs", "categories": ["cs.AI", "cs.CL"], "comment": "IEEE 26th International Conference on Information Reuse and\n  Integration for Data Science (IRI 2025), San Jose, CA, USA", "summary": "Electronic Health Records (EHRs) are digital records of patient information,\noften containing unstructured clinical text. Named Entity Recognition (NER) is\nessential in EHRs for extracting key medical entities like problems, tests, and\ntreatments to support downstream clinical applications. This paper explores\nprompt-based medical entity recognition using large language models (LLMs),\nspecifically GPT-4o and DeepSeek-R1, guided by various prompt engineering\ntechniques, including zero-shot, few-shot, and an ensemble approach. Among all\nstrategies, GPT-4o with prompt ensemble achieved the highest classification\nperformance with an F1-score of 0.95 and recall of 0.98, outperforming\nDeepSeek-R1 on the task. The ensemble method improved reliability by\naggregating outputs through embedding-based similarity and majority voting."}
{"id": "2505.08239", "pdf": "https://arxiv.org/pdf/2505.08239", "abs": "https://arxiv.org/abs/2505.08239", "authors": ["Yizhi Wang", "Mingrui Zhao", "Ali Mahdavi-Amiri", "Hao Zhang"], "title": "ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We introduce adaptive view planning to multi-view synthesis, aiming to\nimprove both occlusion revelation and 3D consistency for single-view 3D\nreconstruction. Instead of generating an unordered set of views independently\nor simultaneously, we generate a sequence of views, leveraging temporal\nconsistency to enhance 3D coherence. Most importantly, our view sequence is not\ndetermined by a pre-determined camera setup. Instead, we compute an adaptive\ncamera trajectory (ACT), specifically, an orbit of camera views, which\nmaximizes the visibility of occluded regions of the 3D object to be\nreconstructed. Once the best orbit is found, we feed it to a video diffusion\nmodel to generate novel views around the orbit, which in turn, are passed to a\nmulti-view 3D reconstruction model to obtain the final reconstruction. Our\nmulti-view synthesis pipeline is quite efficient since it involves no run-time\ntraining/optimization, only forward inferences by applying the pre-trained\nmodels for occlusion analysis and multi-view synthesis. Our method predicts\ncamera trajectories that reveal occlusions effectively and produce consistent\nnovel views, significantly improving 3D reconstruction over SOTA on the unseen\nGSO dataset, both quantitatively and qualitatively."}
{"id": "2505.08727", "pdf": "https://arxiv.org/pdf/2505.08727", "abs": "https://arxiv.org/abs/2505.08727", "authors": ["Fangyuan Yu"], "title": "Memorization-Compression Cycles Improve Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "comment": "12 pages, 6 figures", "summary": "We prove theoretically that generalization improves not only through data\nscaling but also by compressing internal representations. To operationalize\nthis insight, we introduce the Information Bottleneck Language Modeling (IBLM)\nobjective, which reframes language modeling as a constrained optimization\nproblem: minimizing representation entropy subject to optimal prediction\nperformance. Empirically, we observe an emergent memorization-compression cycle\nduring LLM pretraining, evidenced by oscillation positive/negative gradient\nalignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of\nrepresentation entropy. This pattern closely mirrors the predictive-compressive\ntrade-off prescribed by IBLM and also parallels the biological alternation\nbetween awake learning and sleep consolidation. Motivated by this observation,\nwe propose Gated Phase Transition (GAPT), a training algorithm that adaptively\nswitches between memorization and compression phases. When applied to GPT-2\npretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves\ncross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining\ntask on arithmetic multiplication. In a setting designed to simulate\ncatastrophic forgetting, GAPT reduces interference by compressing and\nseparating representations, achieving a 97% improvement in separation -\nparalleling the functional role of sleep consolidation."}
{"id": "2505.08247", "pdf": "https://arxiv.org/pdf/2505.08247", "abs": "https://arxiv.org/abs/2505.08247", "authors": ["Midi Wan", "Pengfei Li", "Yizhuo Liang", "Di Wu", "Yushan Pan", "Guangzhen Zhu", "Hao Wang"], "title": "Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image synthesis plays a crucial role in providing anatomically\naccurate images for diagnosis and treatment. Hallux valgus, which affects\napproximately 19% of the global population, requires frequent weight-bearing\nX-rays for assessment, placing additional strain on both patients and\nhealthcare providers. Existing X-ray models often struggle to balance image\nfidelity, skeletal consistency, and physical constraints, particularly in\ndiffusion-based methods that lack skeletal guidance. We propose the\nSkeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a\nfoot evaluation method utilizing skeletal landmarks. SCCDM incorporates\nmulti-scale feature extraction and attention mechanisms, improving the\nStructural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise\nRatio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves\nan average score of 0.85, demonstrating strong clinical applicability. The code\nis available at https://github.com/midisec/SCCDM."}
{"id": "2505.08783", "pdf": "https://arxiv.org/pdf/2505.08783", "abs": "https://arxiv.org/abs/2505.08783", "authors": ["Shanda Li", "Tanya Marwah", "Junhong Shen", "Weiwei Sun", "Andrej Risteski", "Yiming Yang", "Ameet Talwalkar"], "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": null, "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE."}
{"id": "2505.08255", "pdf": "https://arxiv.org/pdf/2505.08255", "abs": "https://arxiv.org/abs/2505.08255", "authors": ["Shuaiwei Yuan", "Junyu Dong", "Yuezun Li"], "title": "Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted", "categories": ["cs.CR", "cs.CV"], "comment": "CVPR 2025", "summary": "With the advancement of AI generative techniques, Deepfake faces have become\nincredibly realistic and nearly indistinguishable to the human eye. To counter\nthis, Deepfake detectors have been developed as reliable tools for assessing\nface authenticity. These detectors are typically developed on Deep Neural\nNetworks (DNNs) and trained using third-party datasets. However, this protocol\nraises a new security risk that can seriously undermine the trustfulness of\nDeepfake detectors: Once the third-party data providers insert poisoned\n(corrupted) data maliciously, Deepfake detectors trained on these datasets will\nbe injected ``backdoors'' that cause abnormal behavior when presented with\nsamples containing specific triggers. This is a practical concern, as\nthird-party providers may distribute or sell these triggers to malicious users,\nallowing them to manipulate detector performance and escape accountability.\n  This paper investigates this risk in depth and describes a solution to\nstealthily infect Deepfake detectors. Specifically, we develop a trigger\ngenerator, that can synthesize passcode-controlled, semantic-suppression,\nadaptive, and invisible trigger patterns, ensuring both the stealthiness and\neffectiveness of these triggers. Then we discuss two poisoning scenarios,\ndirty-label poisoning and clean-label poisoning, to accomplish the injection of\nbackdoors. Extensive experiments demonstrate the effectiveness, stealthiness,\nand practicality of our method compared to several baselines."}
{"id": "2505.08283", "pdf": "https://arxiv.org/pdf/2505.08283", "abs": "https://arxiv.org/abs/2505.08283", "authors": ["Jueqing Lu", "Yuanyuan Qi", "Xiaohao Yang", "Shujie Zhou", "Lan Du"], "title": "Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal learning enhances deep learning models by enabling them to\nperceive and understand information from multiple data modalities, such as\nvisual and textual inputs. However, most existing approaches assume the\navailability of all modalities, an assumption that often fails in real-world\napplications. Recent works have introduced learnable missing-case-aware prompts\nto mitigate performance degradation caused by missing modalities while reducing\nthe need for extensive model fine-tuning. Building upon the effectiveness of\nmissing-case-aware handling for missing modalities, we propose a novel\ndecoupled prototype-based output head, which leverages missing-case-aware\nclass-wise prototypes tailored for each individual modality. This approach\ndynamically adapts to different missing modality scenarios and can be\nseamlessly integrated with existing prompt-based methods. Extensive experiments\ndemonstrate that our proposed output head significantly improves performance\nacross a wide range of missing-modality scenarios and varying missing rates."}
{"id": "2505.08293", "pdf": "https://arxiv.org/pdf/2505.08293", "abs": "https://arxiv.org/abs/2505.08293", "authors": ["Zhizhuo Yin", "Yuk Hang Tsui", "Pan Hui"], "title": "M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.SD", "eess.AS", "I.3.6"], "comment": "9 Pages, 4 figures, submitted to NIPS 2025", "summary": "Generating full-body human gestures encompassing face, body, hands, and\nglobal movements from audio is a valuable yet challenging task in virtual\navatar creation. Previous systems focused on tokenizing the human gestures\nframewisely and predicting the tokens of each frame from the input audio.\nHowever, one observation is that the number of frames required for a complete\nexpressive human gesture, defined as granularity, varies among different human\ngesture patterns. Existing systems fail to model these gesture patterns due to\nthe fixed granularity of their gesture tokens. To solve this problem, we\npropose a novel framework named Multi-Granular Gesture Generator (M3G) for\naudio-driven holistic gesture generation. In M3G, we propose a novel\nMulti-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct\nmotion sequences from different temporal granularities. Subsequently, we\nproposed a multi-granular token predictor that extracts multi-granular\ninformation from audio and predicts the corresponding motion tokens. Then M3G\nreconstructs the human gestures from the predicted tokens using the MGVQ-VAE.\nBoth objective and subjective experiments demonstrate that our proposed M3G\nframework outperforms the state-of-the-art methods in terms of generating\nnatural and expressive full-body human gestures."}
{"id": "2505.08299", "pdf": "https://arxiv.org/pdf/2505.08299", "abs": "https://arxiv.org/abs/2505.08299", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "State-space models (SSMs), particularly the Mamba architecture, have emerged\nas powerful alternatives to Transformers for sequence modeling, offering\nlinear-time complexity and competitive performance across diverse tasks.\nHowever, their large parameter counts pose significant challenges for\ndeployment in resource-constrained environments. We propose a novel\nunstructured pruning framework tailored for Mamba models that achieves up to\n70\\% parameter reduction while retaining over 95\\% of the original performance.\nOur approach integrates three key innovations: (1) a gradient-aware magnitude\npruning technique that combines weight magnitude and gradient information to\nidentify less critical parameters, (2) an iterative pruning schedule that\ngradually increases sparsity to maintain model stability, and (3) a global\npruning strategy that optimizes parameter allocation across the entire model.\nThrough extensive experiments on WikiText-103, Long Range Arena, and ETT\ntime-series benchmarks, we demonstrate significant efficiency gains with\nminimal performance degradation. Our analysis of pruning effects on Mamba's\ncomponents reveals critical insights into the architecture's redundancy and\nrobustness, enabling practical deployment in resource-constrained settings\nwhile broadening Mamba's applicability."}
{"id": "2505.08316", "pdf": "https://arxiv.org/pdf/2505.08316", "abs": "https://arxiv.org/abs/2505.08316", "authors": ["Dazhong Rong", "Hao Dong", "Xing Gao", "Jiyu Wei", "Di Hong", "Yaoyao Hao", "Qinming He", "Yueming Wang"], "title": "Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity", "categories": ["cs.CE", "cs.CV"], "comment": "This paper has been accepted for full publication at CogSci 2025\n  (https://cognitivesciencesociety.org/cogsci-2025/)", "summary": "Based on the concept that ventral visual stream (VVS) mainly functions for\nobject recognition, current unsupervised task-driven methods model VVS by\ncontrastive learning, and have achieved good brain similarity. However, we\nbelieve functions of VVS extend beyond just object recognition. In this paper,\nwe introduce an additional function involving VVS, named relative position (RP)\nprediction. We first theoretically explain contrastive learning may be unable\nto yield the model capability of RP prediction. Motivated by this, we\nsubsequently integrate RP learning with contrastive learning, and propose a new\nunsupervised task-driven method to model VVS, which is more inline with\nbiological reality. We conduct extensive experiments, demonstrating that: (i)\nour method significantly improves downstream performance of object recognition\nwhile enhancing RP predictivity; (ii) RP predictivity generally improves the\nmodel brain similarity. Our results provide strong evidence for the involvement\nof VVS in location perception (especially RP prediction) from a computational\nperspective."}
{"id": "2505.08414", "pdf": "https://arxiv.org/pdf/2505.08414", "abs": "https://arxiv.org/abs/2505.08414", "authors": ["Zhi Da Soh", "Yang Bai", "Kai Yu", "Yang Zhou", "Xiaofeng Lei", "Sahil Thakur", "Zann Lee", "Lee Ching Linette Phang", "Qingsheng Peng", "Can Can Xue", "Rachel Shujuan Chong", "Quan V. Hoang", "Lavanya Raghavan", "Yih Chung Tham", "Charumathi Sabanayagam", "Wei-Chi Wu", "Ming-Chih Ho", "Jiangnan He", "Preeti Gupta", "Ecosse Lamoureux", "Seang Mei Saw", "Vinay Nangia", "Songhomitra Panda-Jonas", "Jie Xu", "Ya Xing Wang", "Xinxing Xu", "Jost B. Jonas", "Tien Yin Wong", "Rick Siow Mong Goh", "Yong Liu", "Ching-Yu Cheng"], "title": "An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current deep learning models are mostly task specific and lack a\nuser-friendly interface to operate. We present Meta-EyeFM, a multi-function\nfoundation model that integrates a large language model (LLM) with vision\nfoundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a\nrouting mechanism to enable accurate task-specific analysis based on text\nqueries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and\nsystemic diseases, differentiate ocular disease severity, and identify common\nocular signs. The model achieved 100% accuracy in routing fundus images to\nappropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection,\n$\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification.\nMeta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o\nLMMs in detecting various eye diseases and comparable to an ophthalmologist.\nThis system offers enhanced usability and diagnostic performance, making it a\nvaluable decision support tool for primary eye care or an online LLM for fundus\nevaluation."}
{"id": "2505.08430", "pdf": "https://arxiv.org/pdf/2505.08430", "abs": "https://arxiv.org/abs/2505.08430", "authors": ["Lei Su"], "title": "GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Tertiary lymphoid structures (TLS) are organized clusters of immune cells,\nwhose maturity and area can be quantified in whole slide image (WSI) for\nvarious prognostic tasks. Existing methods for assessing these characteristics\ntypically rely on cell proxy tasks and require additional post-processing\nsteps. In this work, We focus on a novel task-TLS Semantic Segmentation\n(TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in\nan end-to-end manner. Due to the extensive scale of WSI and patch-based\nsegmentation strategies, TLS-SS necessitates integrating from neighboring\npatches to guide target patch (target) segmentation. Previous techniques often\nemploy on multi-resolution approaches, constraining the capacity to leverage\nthe broader neighboring context while tend to preserve coarse-grained\ninformation. To address this, we propose a GNN-based Neighboring Context\nAggregation Framework (GNCAF), which progressively aggregates multi-hop\nneighboring context from the target and employs a self-attention mechanism to\nguide the segmentation of the target. GNCAF can be integrated with various\nsegmentation models to enhance their ability to perceive contextual information\noutside of the patch. We build two TLS-SS datasets, called TCGA-COAD and\nINHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly\navailable. Experiments on these datasets demonstrate the superiority of GNCAF,\nachieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU,\nrespectively. Additionally, we also validate the task scalability of GNCAF on\nsegmentation of lymph node metastases."}
{"id": "2505.08468", "pdf": "https://arxiv.org/pdf/2505.08468", "abs": "https://arxiv.org/abs/2505.08468", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Ahmed Masry", "Mizanur Rahman", "Amran Bhuiyan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025 Industry Track", "summary": "Charts are ubiquitous as they help people understand and reason with data.\nRecently, various downstream tasks, such as chart question answering,\nchart2text, and fact-checking, have emerged. Large Vision-Language Models\n(LVLMs) show promise in tackling these tasks, but their evaluation is costly\nand time-consuming, limiting real-world deployment. While using LVLMs as judges\nto assess the chart comprehension capabilities of other LVLMs could streamline\nevaluation processes, challenges like proprietary datasets, restricted access\nto powerful models, and evaluation costs hinder their adoption in industrial\nsettings. To this end, we present a comprehensive evaluation of 13 open-source\nLVLMs as judges for diverse chart comprehension and reasoning tasks. We design\nboth pairwise and pointwise evaluation tasks covering criteria like factual\ncorrectness, informativeness, and relevancy. Additionally, we analyze LVLM\njudges based on format adherence, positional consistency, length bias, and\ninstruction-following. We focus on cost-effective LVLMs (<10B parameters)\nsuitable for both research and commercial use, following a standardized\nevaluation protocol and rubric to measure the LVLM judge's accuracy.\nExperimental results reveal notable variability: while some open LVLM judges\nachieve GPT-4-level evaluation performance (about 80% agreement with GPT-4\njudgments), others struggle (below ~10% agreement). Our findings highlight that\nstate-of-the-art open-source LVLMs can serve as cost-effective automatic\nevaluators for chart-related tasks, though biases such as positional preference\nand length bias persist."}
{"id": "2505.08528", "pdf": "https://arxiv.org/pdf/2505.08528", "abs": "https://arxiv.org/abs/2505.08528", "authors": ["Minsu Kim", "Seong-Hyeon Hwang", "Steven Euijong Whang"], "title": "GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "In the context of continual learning, acquiring new knowledge while\nmaintaining previous knowledge presents a significant challenge. Existing\nmethods often use experience replay techniques that store a small portion of\nprevious task data for training. In experience replay approaches, data\naugmentation has emerged as a promising strategy to further improve the model\nperformance by mixing limited previous task data with sufficient current task\ndata. However, we theoretically and empirically analyze that training with\nmixed samples from random sample pairs may harm the knowledge of previous tasks\nand cause greater catastrophic forgetting. We then propose GradMix, a robust\ndata augmentation method specifically designed for mitigating catastrophic\nforgetting in class-incremental learning. GradMix performs gradient-based\nselective mixup using a class-based criterion that mixes only samples from\nhelpful class pairs and not from detrimental class pairs for reducing\ncatastrophic forgetting. Our experiments on various real datasets show that\nGradMix outperforms data augmentation baselines in accuracy by minimizing the\nforgetting of previous knowledge."}
{"id": "2505.08616", "pdf": "https://arxiv.org/pdf/2505.08616", "abs": "https://arxiv.org/abs/2505.08616", "authors": ["Yifan Li", "Myeongjun Kim", "Yanjing Jin", "Peter Ho", "Jo Woon Chong"], "title": "A portable diagnosis model for Keratoconus using a smartphone", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Keratoconus (KC) is a progressive corneal disorder characterized by localized\nthinning and protrusion, leading to visual distortion. While Placido disc-based\ntopography remains a standard in clinical diagnostics, its dependence on\nspecialized equipment limits accessibility. In this paper, we propose a\nportable, smartphone-based diagnostic framework that captures corneal\nreflections of a Placido disc displayed on a phone screen and applies a\ntwo-stage detection pipeline, then validate on 3D-printed emulated eyeball\nmodels that simulate normal, moderate, and severe KC stages based on anterior\nchamber depth (ACD). The first step of the two-stage detection pipeline is\nclassifying different stages of KC with features including height and width of\nextracted reflections using weighted support vector machine (WSVM). It achieves\na maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple\nsmartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16\nPro. For the second step, we visualize the KC-affected protrusion regions on\nthe corneas with color maps based on inter-disc distance, that provides an\nintuitive representation of disease severity and localization. Moreover, we\nvalidate the ability of the extracted features to differentiate between KC\nstages with ANOVA and Omega Squared, with significant p-values (e.g., $p <\n10^{-6}$) and large effect sizes ($\\\\omega^2$ up to 0.8398) among classes."}
{"id": "2505.08622", "pdf": "https://arxiv.org/pdf/2505.08622", "abs": "https://arxiv.org/abs/2505.08622", "authors": ["Donghoon Kim", "Minji Bae", "Kyuhong Shim", "Byonghyo Shim"], "title": "Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "ICLR 2025", "summary": "Text-to-image generative models like DALL-E and Stable Diffusion have\nrevolutionized visual content creation across various applications, including\nadvertising, personalized media, and design prototyping. However, crafting\neffective textual prompts to guide these models remains challenging, often\nrequiring extensive trial and error. Existing prompt inversion approaches, such\nas soft and hard prompt techniques, are not so effective due to the limited\ninterpretability and incoherent prompt generation. To address these issues, we\npropose Visually Guided Decoding (VGD), a gradient-free approach that leverages\nlarge language models (LLMs) and CLIP-based guidance to generate coherent and\nsemantically aligned prompts. In essence, VGD utilizes the robust text\ngeneration capabilities of LLMs to produce human-readable prompts. Further, by\nemploying CLIP scores to ensure alignment with user-specified visual concepts,\nVGD enhances the interpretability, generalization, and flexibility of prompt\ngeneration without the need for additional training. Our experiments\ndemonstrate that VGD outperforms existing prompt inversion techniques in\ngenerating understandable and contextually relevant prompts, facilitating more\nintuitive and controllable interactions with text-to-image models."}
{"id": "2505.08666", "pdf": "https://arxiv.org/pdf/2505.08666", "abs": "https://arxiv.org/abs/2505.08666", "authors": ["Marco Maida", "Alberto Crescini", "Marco Perronet", "Elena Camuffo"], "title": "Claycode: Stylable and Deformable 2D Scannable Codes", "categories": ["cs.GR", "cs.CG", "cs.CV", "cs.HC", "I.3.0; I.3.5; I.3.6; E.4"], "comment": null, "summary": "This paper introduces Claycode, a novel 2D scannable code designed for\nextensive stylization and deformation. Unlike traditional matrix-based codes\n(e.g., QR codes), Claycodes encode their message in a tree structure. During\nthe encoding process, bits are mapped into a topology tree, which is then\ndepicted as a nesting of color regions drawn within the boundaries of a target\npolygon shape. When decoding, Claycodes are extracted and interpreted in\nreal-time from a camera stream. We detail the end-to-end pipeline and show that\nClaycodes allow for extensive stylization without compromising their\nfunctionality. We then empirically demonstrate Claycode's high tolerance to\nheavy deformations, outperforming traditional 2D scannable codes in scenarios\nwhere they typically fail."}
{"id": "2505.08686", "pdf": "https://arxiv.org/pdf/2505.08686", "abs": "https://arxiv.org/abs/2505.08686", "authors": ["Changqi He", "Shuhan Zhang", "Liguo Zhang", "Jiajun Miao"], "title": "CAD-Coder:Text-Guided CAD Files Code Generation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D\nmodels of real-world products. Traditional CAD typically relies on hand-drawing\nby experts or modifications of existing library files, which doesn't allow for\nrapid personalization. With the emergence of generative artificial\nintelligence, convenient and efficient personalized CAD generation has become\npossible. However, existing generative methods typically produce outputs that\nlack interactive editability and geometric annotations, limiting their\npractical applications in manufacturing. To enable interactive generative CAD,\nwe propose CAD-Coder, a framework that transforms natural language instructions\ninto CAD script codes, which can be executed in Python environments to generate\nhuman-editable CAD files (.Dxf). To facilitate the generation of editable CAD\nsketches with annotation information, we construct a comprehensive dataset\ncomprising 29,130 Dxf files with their corresponding script codes, where each\nsketch preserves both editability and geometric annotations. We evaluate\nCAD-Coder on various 2D/3D CAD generation tasks against existing methods,\ndemonstrating superior interactive capabilities while uniquely providing\neditable sketches with geometric annotations."}
{"id": "2505.08693", "pdf": "https://arxiv.org/pdf/2505.08693", "abs": "https://arxiv.org/abs/2505.08693", "authors": ["Badhan Kumar Das", "Ajay Singh", "Gengyan Zhao", "Han Liu", "Thomas J. Re", "Dorin Comaniciu", "Eli Gibson", "Andreas Maier"], "title": "VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages", "summary": "Self-supervised pretrain techniques have been widely used to improve the\ndownstream tasks' performance. However, real-world magnetic resonance (MR)\nstudies usually consist of different sets of contrasts due to different\nacquisition protocols, which poses challenges for the current deep learning\nmethods on large-scale pretrain and different downstream tasks with different\ninput requirements, since these methods typically require a fixed set of input\nmodalities or, contrasts. To address this challenge, we propose variable-input\nViT (VIViT), a transformer-based framework designed for self-supervised\npretraining and segmentation finetuning for variable contrasts in each study.\nWith this ability, our approach can maximize the data availability in pretrain,\nand can transfer the learned knowledge from pretrain to downstream tasks\ndespite variations in input requirements. We validate our method on brain\ninfarct and brain tumor segmentation, where our method outperforms current CNN\nand ViT-based models with a mean Dice score of 0.624 and 0.883 respectively.\nThese results highlight the efficacy of our design for better adaptability and\nperformance on tasks with real-world heterogeneous MR data."}
{"id": "2505.08751", "pdf": "https://arxiv.org/pdf/2505.08751", "abs": "https://arxiv.org/abs/2505.08751", "authors": ["Saurabh Dash", "Yiyang Nan", "John Dang", "Arash Ahmadian", "Shivalika Singh", "Madeline Smith", "Bharat Venkitesh", "Vlad Shmyhlo", "Viraat Aryabumi", "Walter Beller-Morales", "Jeremy Pekmez", "Jason Ozuzu", "Pierre Richemond", "Acyr Locatelli", "Nick Frosst", "Phil Blunsom", "Aidan Gomez", "Ivan Zhang", "Marzieh Fadaee", "Manoj Govindassamy", "Sudip Roy", "Matthias Gallé", "Beyza Ermis", "Ahmet Üstün", "Sara Hooker"], "title": "Aya Vision: Advancing the Frontier of Multilingual Multimodality", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Building multimodal language models is fundamentally challenging: it requires\naligning vision and language modalities, curating high-quality instruction\ndata, and avoiding the degradation of existing text-only capabilities once\nvision is introduced. These difficulties are further magnified in the\nmultilingual setting, where the need for multimodal data in different languages\nexacerbates existing data scarcity, machine translation often distorts meaning,\nand catastrophic forgetting is more pronounced. To address the aforementioned\nchallenges, we introduce novel techniques spanning both data and modeling.\nFirst, we develop a synthetic annotation framework that curates high-quality,\ndiverse multilingual multimodal instruction data, enabling Aya Vision models to\nproduce natural, human-preferred responses to multimodal inputs across many\nlanguages. Complementing this, we propose a cross-modal model merging technique\nthat mitigates catastrophic forgetting, effectively preserving text-only\ncapabilities while simultaneously enhancing multimodal generative performance.\nAya-Vision-8B achieves best-in-class performance compared to strong multimodal\nmodels such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger\nLlama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which\noutperforms models more than twice its size, such as Molmo-72B and\nLLaMA-3.2-90B-Vision. Our work advances multilingual progress on the\nmulti-modal frontier, and provides insights into techniques that effectively\nbend the need for compute while delivering extremely high performance."}
{"id": "2505.08787", "pdf": "https://arxiv.org/pdf/2505.08787", "abs": "https://arxiv.org/abs/2505.08787", "authors": ["Hanjung Kim", "Jaehyun Kang", "Hyolim Kang", "Meedeum Cho", "Seon Joo Kim", "Youngwoon Lee"], "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations", "categories": ["cs.RO", "cs.CV"], "comment": "Project Page: https://kimhanjung.github.io/UniSkill/", "summary": "Mimicry is a fundamental learning mechanism in humans, enabling individuals\nto learn new tasks by observing and imitating experts. However, applying this\nability to robots presents significant challenges due to the inherent\ndifferences between human and robot embodiments in both their visual appearance\nand physical capabilities. While previous methods bridge this gap using\ncross-embodiment datasets with shared scenes and tasks, collecting such aligned\ndata between humans and robots at scale is not trivial. In this paper, we\npropose UniSkill, a novel framework that learns embodiment-agnostic skill\nrepresentations from large-scale cross-embodiment video data without any\nlabels, enabling skills extracted from human video prompts to effectively\ntransfer to robot policies trained only on robot data. Our experiments in both\nsimulation and real-world environments show that our cross-embodiment skills\nsuccessfully guide robots in selecting appropriate actions, even with unseen\nvideo prompts. The project website can be found at:\nhttps://kimhanjung.github.io/UniSkill."}
