{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974", "abs": "https://arxiv.org/abs/2504.17974", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages."}
{"id": "2504.17993", "pdf": "https://arxiv.org/pdf/2504.17993", "abs": "https://arxiv.org/abs/2504.17993", "authors": ["Brihi Joshi", "Xiang Ren", "Swabha Swayamdipta", "Rik Koncel-Kedziorski", "Tim Paek"], "title": "Improving LLM Personas via Rationalization with Psychological Scaffolds", "categories": ["cs.CL"], "comment": null, "summary": "Language models prompted with a user description or persona can predict a\nuser's preferences and opinions, but existing approaches to building personas\n-- based solely on a user's demographic attributes and/or prior judgments --\nfail to capture the underlying reasoning behind said user judgments. We\nintroduce PB&J (Psychology of Behavior and Judgments), a framework that\nimproves LLM personas by incorporating rationales of why a user might make\nspecific judgments. These rationales are LLM-generated, and aim to reason about\na user's behavior on the basis of their experiences, personality traits or\nbeliefs. This is done using psychological scaffolds -- structured frameworks\ngrounded in theories such as the Big 5 Personality Traits and Primal World\nBeliefs -- that help provide structure to the generated rationales. Experiments\non public opinion and movie preference prediction tasks demonstrate that LLM\npersonas augmented with PB&J rationales consistently outperform methods using\nonly a user's demographics and/or judgments. Additionally, LLM personas\nconstructed using scaffolds describing user beliefs perform competitively with\nthose using human-written rationales."}
{"id": "2504.18012", "pdf": "https://arxiv.org/pdf/2504.18012", "abs": "https://arxiv.org/abs/2504.18012", "authors": ["Zhuang Yu", "Shiliang Sun", "Jing Zhao", "Tengfei Song", "Hao Yang"], "title": "Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Machine Translation (MMT) aims to improve translation quality by\nleveraging auxiliary modalities such as images alongside textual input. While\nrecent advances in large-scale pre-trained language and vision models have\nsignificantly benefited unimodal natural language processing tasks, their\neffectiveness and role in MMT remain underexplored. In this work, we conduct a\nsystematic study on the impact of pre-trained encoders and decoders in\nmultimodal translation models. Specifically, we analyze how different training\nstrategies, from training from scratch to using pre-trained and partially\nfrozen components, affect translation performance under a unified MMT\nframework. Experiments are carried out on the Multi30K and CoMMuTE dataset\nacross English-German and English-French translation tasks. Our results reveal\nthat pre-training plays a crucial yet asymmetrical role in multimodal settings:\npre-trained decoders consistently yield more fluent and accurate outputs, while\npre-trained encoders show varied effects depending on the quality of\nvisual-text alignment. Furthermore, we provide insights into the interplay\nbetween modality fusion and pre-trained components, offering guidance for\nfuture architecture design in multimodal translation systems."}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS."}
{"id": "2504.18041", "pdf": "https://arxiv.org/pdf/2504.18041", "abs": "https://arxiv.org/abs/2504.18041", "authors": ["Bang An", "Shiyue Zhang", "Mark Dredze"], "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025", "summary": "Efforts to ensure the safety of large language models (LLMs) include safety\nfine-tuning, evaluation, and red teaming. However, despite the widespread use\nof the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses\non standard LLMs, which means we know little about how RAG use cases change a\nmodel's safety profile. We conduct a detailed comparative analysis of RAG and\nnon-RAG frameworks with eleven LLMs. We find that RAG can make models less safe\nand change their safety profile. We explore the causes of this change and find\nthat even combinations of safe models with safe documents can cause unsafe\ngenerations. In addition, we evaluate some existing red teaming methods for RAG\nsettings and show that they are less effective than when used for non-RAG\nsettings. Our work highlights the need for safety research and red-teaming\nmethods specifically tailored for RAG LLMs."}
{"id": "2504.18001", "pdf": "https://arxiv.org/pdf/2504.18001", "abs": "https://arxiv.org/abs/2504.18001", "authors": ["Daniel Zavorotny", "Qi Wu", "David Bauer", "Kwan-Liu Ma"], "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for Interactive Visualization of Tera-Scale Data", "categories": ["cs.GR"], "comment": "11 pages, 11 figures, EGPGV25", "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."}
{"id": "2504.17804", "pdf": "https://arxiv.org/pdf/2504.17804", "abs": "https://arxiv.org/abs/2504.17804", "authors": ["Andrew Kiruluta"], "title": "Spectral Dictionary Learning for Generative Image Modeling", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel spectral generative model for image synthesis that departs\nradically from the common variational, adversarial, and diffusion paradigms. In\nour approach, images, after being flattened into one-dimensional signals, are\nreconstructed as linear combinations of a set of learned spectral basis\nfunctions, where each basis is explicitly parameterized in terms of frequency,\nphase, and amplitude. The model jointly learns a global spectral dictionary\nwith time-varying modulations and per-image mixing coefficients that quantify\nthe contributions of each spectral component. Subsequently, a simple\nprobabilistic model is fitted to these mixing coefficients, enabling the\ndeterministic generation of new images by sampling from the latent space. This\nframework leverages deterministic dictionary learning, offering a highly\ninterpretable and physically meaningful representation compared to methods\nrelying on stochastic inference or adversarial training. Moreover, the\nincorporation of frequency-domain loss functions, computed via the short-time\nFourier transform (STFT), ensures that the synthesized images capture both\nglobal structure and fine-grained spectral details, such as texture and edge\ninformation. Experimental evaluations on the CIFAR-10 benchmark demonstrate\nthat our approach not only achieves competitive performance in terms of\nreconstruction quality and perceptual fidelity but also offers improved\ntraining stability and computational efficiency. This new type of generative\nmodel opens up promising avenues for controlled synthesis, as the learned\nspectral dictionary affords a direct handle on the intrinsic frequency content\nof the images, thus providing enhanced interpretability and potential for novel\napplications in image manipulation and analysis."}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053", "abs": "https://arxiv.org/abs/2504.18053", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM."}
{"id": "2504.18380", "pdf": "https://arxiv.org/pdf/2504.18380", "abs": "https://arxiv.org/abs/2504.18380", "authors": ["Steven Häsler", "Philipp Ackermann"], "title": "Spatial Reasoner: A 3D Inference Pipeline for XR Applications", "categories": ["cs.SE", "cs.AI", "cs.GR", "cs.HC", "spatial computing, extended reality, knowledge representation,\n  spatial reasoning"], "comment": "11 pages, preprint of ICVARS 2025 paper", "summary": "Modern extended reality XR systems provide rich analysis of image data and\nfusion of sensor input and demand AR/VR applications that can reason about 3D\nscenes in a semantic manner. We present a spatial reasoning framework that\nbridges geometric facts with symbolic predicates and relations to handle key\ntasks such as determining how 3D objects are arranged among each other ('on',\n'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box\nrepresentations, enhanced by a comprehensive set of spatial predicates, ranging\nfrom topology and connectivity to directionality and orientation, expressed in\na formalism related to natural language. The derived predicates form a spatial\nknowledge graph and, in combination with a pipeline-based inference model,\nenable spatial queries and dynamic rule evaluation. Implementations for client-\nand server-side processing demonstrate the framework's capability to\nefficiently translate geometric data into actionable knowledge, ensuring\nscalable and technology-independent spatial reasoning in complex 3D\nenvironments. The Spatial Reasoner framework is fostering the creation of\nspatial ontologies, and seamlessly integrates with and therefore enriches\nmachine learning, natural language processing, and rule systems in XR\napplications."}
{"id": "2504.17810", "pdf": "https://arxiv.org/pdf/2504.17810", "abs": "https://arxiv.org/abs/2504.17810", "authors": ["Yuxin Yao", "Yan Zhang", "Zhening Huang", "Joan Lasenby"], "title": "SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos", "categories": ["cs.CV", "eess.IV"], "comment": "10 pages, 4 figures, Accepted by CVPR workshop", "summary": "Dynamic videos with small baseline motions are ubiquitous in daily life,\nespecially on social media. However, these videos present a challenge to\nexisting pose estimation frameworks due to ambiguous features, drift\naccumulation, and insufficient triangulation constraints. Gaussian splatting,\nwhich maintains an explicit representation for scenes, provides a reliable\nnovel view rasterization when the viewpoint change is small. Inspired by this,\nwe propose SmallGS, a camera pose estimation framework that is specifically\ndesigned for small-baseline videos. SmallGS optimizes sequential camera poses\nusing Gaussian splatting, which reconstructs the scene from the first frame in\neach video segment to provide a stable reference for the rest. The temporal\nconsistency of Gaussian splatting within limited viewpoint differences reduced\nthe requirement of sufficient depth variations in traditional camera pose\nestimation. We further incorporate pretrained robust visual features, e.g.\nDINOv2, into Gaussian splatting, where high-dimensional feature map rendering\nenhances the robustness of camera pose estimation. By freezing the Gaussian\nsplatting and optimizing camera viewpoints based on rasterized features,\nSmallGS effectively learns camera poses without requiring explicit feature\ncorrespondences or strong parallax motion. We verify the effectiveness of\nSmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves\nimpressive accuracy in camera pose estimation compared to MonST3R and\nDORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at:\nhttps://yuxinyao620.github.io/SmallGS"}
{"id": "2504.18058", "pdf": "https://arxiv.org/pdf/2504.18058", "abs": "https://arxiv.org/abs/2504.18058", "authors": ["Sijia Cheng", "Wen-Yu Chang", "Yun-Nung Chen"], "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by IWSDS 2025", "summary": "The integration of dialogue agents into the sales domain requires a deep\nunderstanding of how these systems interact with users possessing diverse\npersonas. This study explores the influence of user personas, defined using the\nMyers-Briggs Type Indicator (MBTI), on the interaction quality and performance\nof sales-oriented dialogue agents. Through large-scale testing and analysis, we\nassess the pre-trained agent's effectiveness, adaptability, and personalization\ncapabilities across a wide range of MBTI-defined user types. Our findings\nreveal significant patterns in interaction dynamics, task completion rates, and\ndialogue naturalness, underscoring the future potential for dialogue agents to\nrefine their strategies to better align with varying personality traits. This\nwork not only provides actionable insights for building more adaptive and\nuser-centric conversational systems in the sales domain but also contributes\nbroadly to the field by releasing persona-defined user simulators. These\nsimulators, unconstrained by domain, offer valuable tools for future research\nand demonstrate the potential for scaling personalized dialogue systems across\ndiverse applications."}
{"id": "2504.17812", "pdf": "https://arxiv.org/pdf/2504.17812", "abs": "https://arxiv.org/abs/2504.17812", "authors": ["Sara Sabour"], "title": "Object Learning and Robust 3D Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "PhD Thesis", "summary": "In this thesis we discuss architectural designs and training methods for a\nneural network to have the ability of dissecting an image into objects of\ninterest without supervision. The main challenge in 2D unsupervised object\nsegmentation is distinguishing between foreground objects of interest and\nbackground. FlowCapsules uses motion as a cue for the objects of interest in 2D\nscenarios. The last part of this thesis focuses on 3D applications where the\ngoal is detecting and removal of the object of interest from the input images.\nIn these tasks, we leverage the geometric consistency of scenes in 3D to detect\nthe inconsistent dynamic objects. Our transient object masks are then used for\ndesigning robust optimization kernels to improve 3D modelling in a casual\ncapture setup. One of our goals in this thesis is to show the merits of\nunsupervised object based approaches in computer vision. Furthermore, we\nsuggest possible directions for defining objects of interest or foreground\nobjects without requiring supervision. Our hope is to motivate and excite the\ncommunity into further exploring explicit object representations in image\nunderstanding tasks."}
{"id": "2504.18070", "pdf": "https://arxiv.org/pdf/2504.18070", "abs": "https://arxiv.org/abs/2504.18070", "authors": ["Jingjin Wang"], "title": "PropRAG: Guiding Retrieval with Beam Search over Proposition Paths", "categories": ["cs.CL", "cs.AI"], "comment": "Code and data to be released at:\n  https://github.com/ReLink-Inc/PropRAG", "summary": "Retrieval Augmented Generation (RAG) has become the standard non-parametric\napproach for equipping Large Language Models (LLMs) with up-to-date knowledge\nand mitigating catastrophic forgetting common in continual learning. However,\nstandard RAG, relying on independent passage retrieval, fails to capture the\ninterconnected nature of human memory crucial for complex reasoning\n(associativity) and contextual understanding (sense-making). While structured\nRAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples,\nthe inherent context loss limits fidelity. We introduce PropRAG, a framework\nleveraging contextually rich propositions and a novel beam search algorithm\nover proposition paths to explicitly discover multi-step reasoning chains.\nCrucially, PropRAG's online retrieval process operates entirely without\ninvoking generative LLMs, relying instead on efficient graph traversal and\npre-computed embeddings. This avoids online LLM inference costs and potential\ninconsistencies during evidence gathering. LLMs are used effectively offline\nfor high-quality proposition extraction and post-retrieval for answer\ngeneration. PropRAG achieves state-of-the-art zero-shot Recall@5 results on\nPopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside\ntop F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through\nricher representation and explicit, LLM-free online path finding, PropRAG\nadvances non-parametric continual learning."}
{"id": "2504.17813", "pdf": "https://arxiv.org/pdf/2504.17813", "abs": "https://arxiv.org/abs/2504.17813", "authors": ["Dileepa Pitawela", "Gustavo Carneiro", "Hsiang-Ting Chen"], "title": "CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025", "summary": "In ordinal classification, misclassifying neighboring ranks is common, yet\nthe consequences of these errors are not the same. For example, misclassifying\nbenign tumor categories is less consequential, compared to an error at the\npre-cancerous to cancerous threshold, which could profoundly influence\ntreatment choices. Despite this, existing ordinal classification methods do not\naccount for the varying importance of these margins, treating all neighboring\nclasses as equally significant. To address this limitation, we propose CLOC, a\nnew margin-based contrastive learning method for ordinal classification that\nlearns an ordered representation based on the optimization of multiple margins\nwith a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision\nboundaries across key adjacent categories, facilitating smooth transitions\nbetween classes and reducing the risk of overfitting to biases present in the\ntraining data. We provide empirical discussion regarding the properties of MMNP\nand show experimental results on five real-world image datasets (Adience,\nHistorical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic\nRetinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset\nsimulating clinical decision bias. Our results demonstrate that CLOC\noutperforms existing ordinal classification methods and show the\ninterpretability and controllability of CLOC in learning meaningful, ordered\nrepresentations that align with clinical and practical needs."}
{"id": "2504.18080", "pdf": "https://arxiv.org/pdf/2504.18080", "abs": "https://arxiv.org/abs/2504.18080", "authors": ["Wataru Kawakami", "Keita Suzuki", "Junichiro Iwasawa"], "title": "Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) show potential in medicine, yet clinical\nadoption is hindered by concerns over factual accuracy, language-specific\nlimitations (e.g., Japanese), and critically, their reliability when required\nto generate reasoning explanations -- a prerequisite for trust. This paper\nintroduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the\nJapanese medical domain to achieve both high accuracy and stable reasoning. We\nemploy a two-stage fine-tuning process on the Qwen2.5-72B base model: first,\nContinued Pretraining (CPT) on a comprehensive Japanese medical corpus instills\ndeep domain knowledge. Second, Reasoning Preference Optimization (RPO), a\npreference-based method, enhances the generation of reliable reasoning pathways\nwhile preserving high answer accuracy. Evaluations on the Japanese Medical\nLicensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves\nstate-of-the-art performance (0.868 accuracy), surpassing strong proprietary\nmodels like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which\nexhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively\non IgakuQA) when prompted for explanations, our model maintains its high\naccuracy (0.868) under such conditions. This highlights RPO's effectiveness in\nstabilizing reasoning generation. This work underscores the importance of\noptimizing for reliable explanations alongside accuracy. We release the\nPreferred-MedLLM-Qwen-72B model weights to foster research into trustworthy\nLLMs for specialized, high-stakes applications."}
{"id": "2504.17815", "pdf": "https://arxiv.org/pdf/2504.17815", "abs": "https://arxiv.org/abs/2504.17815", "authors": ["Mingxuan Cui", "Qing Guo", "Yuyi Wang", "Hongkai Yu", "Di Lin", "Qin Zou", "Ming-Ming Cheng", "Xi Li"], "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning", "categories": ["cs.CV"], "comment": "14 pages, 12 figures, ICCV", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D\nrepresentation for novel view synthesis. This paper extends 3DGS capabilities\nto inpainting, where masked objects in a scene are replaced with new contents\nthat blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D\nGaussian inpainting (3DGI) is challenging in effectively leveraging\ncomplementary visual and semantic cues from multiple input views, as occluded\nareas in one view may be visible in others. To address this, we propose a\nmethod that measures the visibility uncertainties of 3D points across different\ninput views and uses them to guide 3DGI in utilizing complementary visual cues.\nWe also employ uncertainties to learn a semantic concept of scene without the\nmasked object and use a diffusion model to fill masked objects in input images\nbased on the learned concept. Finally, we build a novel 3DGI framework, VISTA,\nby integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl\nlearning. VISTA generates high-quality 3DGS models capable of synthesizing\nartifact-free and naturally inpainted novel views. Furthermore, our approach\nextends to handling dynamic distractors arising from temporal object changes,\nenhancing its versatility in diverse scene reconstruction scenarios. We\ndemonstrate the superior performance of our method over state-of-the-art\ntechniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10\ndiverse static 3D inpainting scenes, and an underwater 3D inpainting dataset\nderived from UTB180, including fast-moving fish as inpainting targets."}
{"id": "2504.18085", "pdf": "https://arxiv.org/pdf/2504.18085", "abs": "https://arxiv.org/abs/2504.18085", "authors": ["Muhammad Mubashar", "Shireen Kudukkil Manchingal", "Fabio Cuzzolin"], "title": "Random-Set Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "16 pages, 6 figures", "summary": "Large Language Models (LLMs) are known to produce very high-quality tests and\nresponses to our queries. But how much can we trust this generated text? In\nthis paper, we study the problem of uncertainty quantification in LLMs. We\npropose a novel Random-Set Large Language Model (RSLLM) approach which predicts\nfinite random sets (belief functions) over the token space, rather than\nprobability vectors as in classical LLMs. In order to allow so efficiently, we\nalso present a methodology based on hierarchical clustering to extract and use\na budget of \"focal\" subsets of tokens upon which the belief prediction is\ndefined, rather than using all possible collections of tokens, making the\nmethod scalable yet effective. RS-LLMs encode the epistemic uncertainty induced\nin their generation process by the size and diversity of its training set via\nthe size of the credal sets associated with the predicted belief functions. The\nproposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b,\nMistral-7b and Phi-2 models and is shown to outperform the standard model in\nboth datasets in terms of correctness of answer while also showing potential in\nestimating the second level uncertainty in its predictions and providing the\ncapability to detect when its hallucinating."}
{"id": "2504.17816", "pdf": "https://arxiv.org/pdf/2504.17816", "abs": "https://arxiv.org/abs/2504.17816", "authors": ["Daneul Kim", "Jingxu Zhang", "Wonjoon Jin", "Sunghyun Cho", "Qi Dai", "Jaesik Park", "Chong Luo"], "title": "Subject-driven Video Generation via Disentangled Identity and Motion", "categories": ["cs.CV", "eess.IV"], "comment": "Project Page :\n  https://carpedkm.github.io/projects/disentangled_sub/index.html", "summary": "We propose to train a subject-driven customized video generation model\nthrough decoupling the subject-specific learning from temporal dynamics in\nzero-shot without additional tuning. A traditional method for video\ncustomization that is tuning-free often relies on large, annotated video\ndatasets, which are computationally expensive and require extensive annotation.\nIn contrast to the previous approach, we introduce the use of an image\ncustomization dataset directly on training video customization models,\nfactorizing the video customization into two folds: (1) identity injection\nthrough image customization dataset and (2) temporal modeling preservation with\na small set of unannotated videos through the image-to-video training method.\nAdditionally, we employ random image token dropping with randomized image\ninitialization during image-to-video fine-tuning to mitigate the copy-and-paste\nissue. To further enhance learning, we introduce stochastic switching during\njoint optimization of subject-specific and temporal features, mitigating\ncatastrophic forgetting. Our method achieves strong subject consistency and\nscalability, outperforming existing video customization models in zero-shot\nsettings, demonstrating the effectiveness of our framework."}
{"id": "2504.18104", "pdf": "https://arxiv.org/pdf/2504.18104", "abs": "https://arxiv.org/abs/2504.18104", "authors": ["Yinglong Yu", "Hao Shen", "Zhengyi Lyu", "Qi He"], "title": "Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In response to the growing problem of misinformation in the context of\nglobalization and informatization, this paper proposes a classification method\nfor fact-check-worthiness estimation based on prompt tuning. We construct a\nmodel for fact-check-worthiness estimation at the methodological level using\nprompt tuning. By applying designed prompt templates to large language models,\nwe establish in-context learning and leverage prompt tuning technology to\nimprove the accuracy of determining whether claims have fact-check-worthiness,\nparticularly when dealing with limited or unlabeled data. Through extensive\nexperiments on public datasets, we demonstrate that the proposed method\nsurpasses or matches multiple baseline methods in the classification task of\nfact-check-worthiness estimation assessment, including classical pre-trained\nmodels such as BERT, as well as recent popular large models like GPT-3.5 and\nGPT-4. Experiments show that the prompt tuning-based method proposed in this\nstudy exhibits certain advantages in evaluation metrics such as F1 score and\naccuracy, thereby effectively validating its effectiveness and advancement in\nthe task of fact-check-worthiness estimation."}
{"id": "2504.17817", "pdf": "https://arxiv.org/pdf/2504.17817", "abs": "https://arxiv.org/abs/2504.17817", "authors": ["Alexandre Cardaillac", "Donald G. Dansereau"], "title": "Learning Underwater Active Perception in Simulation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "When employing underwater vehicles for the autonomous inspection of assets,\nit is crucial to consider and assess the water conditions. Indeed, they have a\nsignificant impact on the visibility, which also affects robotic operations.\nTurbidity can jeopardise the whole mission as it may prevent correct visual\ndocumentation of the inspected structures. Previous works have introduced\nmethods to adapt to turbidity and backscattering, however, they also include\nmanoeuvring and setup constraints. We propose a simple yet efficient approach\nto enable high-quality image acquisition of assets in a broad range of water\nconditions. This active perception framework includes a multi-layer perceptron\n(MLP) trained to predict image quality given a distance to a target and\nartificial light intensity. We generated a large synthetic dataset including\nten water types with different levels of turbidity and backscattering. For\nthis, we modified the modelling software Blender to better account for the\nunderwater light propagation properties. We validated the approach in\nsimulation and showed significant improvements in visual coverage and quality\nof imagery compared to traditional approaches. The project code is available on\nour project page at https://roboticimaging.org/Projects/ActiveUW/."}
{"id": "2504.18106", "pdf": "https://arxiv.org/pdf/2504.18106", "abs": "https://arxiv.org/abs/2504.18106", "authors": ["Yinglong Yu", "Zhaopu Yao", "Fang Yuan"], "title": "Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering", "categories": ["cs.CL"], "comment": null, "summary": "This study analyzes Chinese and English media reports on the Paris Olympics\nusing topic modeling, Large Language Model (LLM) prompt engineering, and corpus\nphraseology methods to explore similarities and differences in discourse\nconstruction and attitudinal meanings. Common topics include the opening\nceremony, athlete performance, and sponsorship brands. Chinese media focus on\nspecific sports, sports spirit, doping controversies, and new technologies,\nwhile English media focus on female athletes, medal wins, and eligibility\ncontroversies. Chinese reports show more frequent prepositional co-occurrences\nand positive semantic prosody in describing the opening ceremony and sports\nspirit. English reports exhibit positive semantic prosody when covering female\nathletes but negative prosody in predicting opening ceremony reactions and\ndiscussing women's boxing controversies."}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821", "abs": "https://arxiv.org/abs/2504.17821", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics."}
{"id": "2504.18114", "pdf": "https://arxiv.org/pdf/2504.18114", "abs": "https://arxiv.org/abs/2504.18114", "authors": ["Atharva Kulkarni", "Yuan Zhang", "Joel Ruben Antony Moniz", "Xiou Ge", "Bo-Hsiang Tseng", "Dhivya Piraviperumal", "Swabha Swayamdipta", "Hong Yu"], "title": "Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Hallucinations pose a significant obstacle to the reliability and widespread\nadoption of language models, yet their accurate measurement remains a\npersistent challenge. While many task- and domain-specific metrics have been\nproposed to assess faithfulness and factuality concerns, the robustness and\ngeneralization of these metrics are still untested. In this paper, we conduct a\nlarge-scale empirical evaluation of 6 diverse sets of hallucination detection\nmetrics across 4 datasets, 37 language models from 5 families, and 5 decoding\nmethods. Our extensive investigation reveals concerning gaps in current\nhallucination evaluation: metrics often fail to align with human judgments,\ntake an overtly myopic view of the problem, and show inconsistent gains with\nparameter scaling. Encouragingly, LLM-based evaluation, particularly with\nGPT-4, yields the best overall results, and mode-seeking decoding methods seem\nto reduce hallucinations, especially in knowledge-grounded settings. These\nfindings underscore the need for more robust metrics to understand and quantify\nhallucinations, and better strategies to mitigate them."}
{"id": "2504.17822", "pdf": "https://arxiv.org/pdf/2504.17822", "abs": "https://arxiv.org/abs/2504.17822", "authors": ["Wenwen Li", "Chia-Yu Hsu", "Sizhe Wang", "Zhining Gu", "Yili Yang", "Brendan M. Rogers", "Anna Liljedahl"], "title": "A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost\nlandforms with significant environmental impacts. Mapping these RTS is crucial\nbecause their appearance serves as a clear indication of permafrost thaw.\nHowever, their small scale compared to other landform features, vague\nboundaries, and spatiotemporal variation pose significant challenges for\naccurate detection. In this paper, we employed a state-of-the-art deep learning\nmodel, the Cascade Mask R-CNN with a multi-scale vision transformer-based\nbackbone, to delineate RTS features across the Arctic. Two new strategies were\nintroduced to optimize multimodal learning and enhance the model's predictive\nperformance: (1) a feature-level, residual cross-modality attention fusion\nstrategy, which effectively integrates feature maps from multiple modalities to\ncapture complementary information and improve the model's ability to understand\ncomplex patterns and relationships within the data; (2) pre-trained unimodal\nlearning followed by multimodal fine-tuning to alleviate high computing demand\nwhile achieving strong model performance. Experimental results demonstrated\nthat our approach outperformed existing models adopting data-level fusion,\nfeature-level convolutional fusion, and various attention fusion strategies,\nproviding valuable insights into the efficient utilization of multimodal data\nfor RTS mapping. This research contributes to our understanding of permafrost\nlandforms and their environmental implications."}
{"id": "2504.18128", "pdf": "https://arxiv.org/pdf/2504.18128", "abs": "https://arxiv.org/abs/2504.18128", "authors": ["Tatsunori Tanaka", "Fi Zheng", "Kai Sato", "Zhifeng Li", "Yuanyun Zhang", "Shi Li"], "title": "Temporal Entailment Pretraining for Clinical Language Models over EHR Data", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Clinical language models have achieved strong performance on downstream tasks\nby pretraining on domain specific corpora such as discharge summaries and\nmedical notes. However, most approaches treat the electronic health record as a\nstatic document, neglecting the temporally-evolving and causally entwined\nnature of patient trajectories. In this paper, we introduce a novel temporal\nentailment pretraining objective for language models in the clinical domain.\nOur method formulates EHR segments as temporally ordered sentence pairs and\ntrains the model to determine whether a later state is entailed by,\ncontradictory to, or neutral with respect to an earlier state. Through this\ntemporally structured pretraining task, models learn to perform latent clinical\nreasoning over time, improving their ability to generalize across forecasting\nand diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and\ndemonstrate state of the art results on temporal clinical QA, early warning\nprediction, and disease progression modeling."}
{"id": "2504.17825", "pdf": "https://arxiv.org/pdf/2504.17825", "abs": "https://arxiv.org/abs/2504.17825", "authors": ["Dehong Kong", "Fan Li", "Zhixin Wang", "Jiaqi Xu", "Renjing Pei", "Wenbo Li", "WenQi Ren"], "title": "Dual Prompting Image Restoration with Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR2025", "summary": "Recent state-of-the-art image restoration methods mostly adopt latent\ndiffusion models with U-Net backbones, yet still facing challenges in achieving\nhigh-quality restoration due to their limited capabilities. Diffusion\ntransformers (DiTs), like SD3, are emerging as a promising alternative because\nof their better quality with scalability. In this paper, we introduce DPIR\n(Dual Prompting Image Restoration), a novel image restoration method that\neffectivly extracts conditional information of low-quality images from multiple\nperspectives. Specifically, DPIR consits of two branches: a low-quality image\nconditioning branch and a dual prompting control branch. The first branch\nutilizes a lightweight module to incorporate image priors into the DiT with\nhigh efficiency. More importantly, we believe that in image restoration,\ntextual description alone cannot fully capture its rich visual characteristics.\nTherefore, a dual prompting module is designed to provide DiT with additional\nvisual cues, capturing both global context and local appearance. The extracted\nglobal-local visual prompts as extra conditional control, alongside textual\nprompts to form dual prompts, greatly enhance the quality of the restoration.\nExtensive experimental results demonstrate that DPIR delivers superior image\nrestoration performance."}
{"id": "2504.18142", "pdf": "https://arxiv.org/pdf/2504.18142", "abs": "https://arxiv.org/abs/2504.18142", "authors": ["Fida Ullah", "Muhammad Ahmad", "Muhammad Tayyab Zamir", "Muhammad Arif", "Grigori sidorov", "Edgardo Manuel Felipe Riverón", "Alexander Gelbukh"], "title": "EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) plays a pivotal role in various Natural\nLanguage Processing (NLP) tasks by identifying and classifying named entities\n(NEs) from unstructured data into predefined categories such as person,\norganization, location, date, and time. While extensive research exists for\nhigh-resource languages and general domains, NER in Urdu particularly within\ndomain-specific contexts like education remains significantly underexplored.\nThis is Due to lack of annotated datasets for educational content which limits\nthe ability of existing models to accurately identify entities such as academic\nroles, course names, and institutional terms, underscoring the urgent need for\ntargeted resources in this domain. To the best of our knowledge, no dataset\nexists in the domain of the Urdu language for this purpose. To achieve this\nobjective this study makes three key contributions. Firstly, we created a\nmanually annotated dataset in the education domain, named EDU-NER-2025, which\ncontains 13 unique most important entities related to education domain. Second,\nwe describe our annotation process and guidelines in detail and discuss the\nchallenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed\nkey linguistic challenges, such as morphological complexity and ambiguity,\nwhich are prevalent in formal Urdu texts."}
{"id": "2504.17826", "pdf": "https://arxiv.org/pdf/2504.17826", "abs": "https://arxiv.org/abs/2504.17826", "authors": ["Kaicheng Pang", "Xingxing Zou", "Waikeung Wong"], "title": "FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fashion styling and personalized recommendations are pivotal in modern\nretail, contributing substantial economic value in the fashion industry. With\nthe advent of vision-language models (VLM), new opportunities have emerged to\nenhance retailing through natural language and visual interactions. This work\nproposes FashionM3, a multimodal, multitask, and multiround fashion assistant,\nbuilt upon a VLM fine-tuned for fashion-specific tasks. It helps users discover\nsatisfying outfits by offering multiple capabilities including personalized\nrecommendation, alternative suggestion, product image generation, and virtual\ntry-on simulation. Fine-tuned on the novel FashionRec dataset, comprising\n331,124 multimodal dialogue samples across basic, personalized, and alternative\nrecommendation tasks, FashionM3 delivers contextually personalized suggestions\nwith iterative refinement through multiround interactions. Quantitative and\nqualitative evaluations, alongside user studies, demonstrate FashionM3's\nsuperior performance in recommendation effectiveness and practical value as a\nfashion assistant."}
{"id": "2504.18180", "pdf": "https://arxiv.org/pdf/2504.18180", "abs": "https://arxiv.org/abs/2504.18180", "authors": ["Þórir Hrafn Harðarson", "Hrafn Loftsson", "Stefán Ólafsson"], "title": "Aligning Language Models for Icelandic Legal Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at NoDaLiDa 2025", "summary": "The integration of language models in the legal domain holds considerable\npromise for streamlining processes and improving efficiency in managing\nextensive workloads. However, the specialized terminology, nuanced language,\nand formal style of legal texts can present substantial challenges. This study\nexamines whether preference-based training techniques, specifically\nReinforcement Learning from Human Feedback and Direct Preference Optimization,\ncan enhance models' performance in generating Icelandic legal summaries that\nalign with domain-specific language standards and user preferences. We compare\nmodels fine-tuned with preference training to those using conventional\nsupervised learning. Results indicate that preference training improves the\nlegal accuracy of generated summaries over standard fine-tuning but does not\nsignificantly enhance the overall quality of Icelandic language usage.\nDiscrepancies between automated metrics and human evaluations further\nunderscore the importance of qualitative assessment in developing language\nmodels for the legal domain."}
{"id": "2504.17828", "pdf": "https://arxiv.org/pdf/2504.17828", "abs": "https://arxiv.org/abs/2504.17828", "authors": ["Bozheng Li", "Yongliang Wu", "Yi Lu", "Jiashuo Yu", "Licheng Tang", "Jiawang Cao", "Wenqing Zhu", "Yuyang Sun", "Jay Wu", "Wenbo Zhu"], "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR2025", "summary": "Widely shared videos on the internet are often edited. Recently, although\nVideo Large Language Models (Vid-LLMs) have made great progress in general\nvideo understanding tasks, their capabilities in video editing understanding\n(VEU) tasks remain unexplored. To address this gap, in this paper, we introduce\nVEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark\nthat categorizes video editing components across various dimensions, from\nintra-frame features like shot size to inter-shot attributes such as cut types\nand transitions. Unlike previous video editing understanding benchmarks that\nfocus mainly on editing element classification, VEU-Bench encompasses 19\nfine-grained tasks across three stages: recognition, reasoning, and judging. To\nenhance the annotation of VEU automatically, we built an annotation pipeline\nintegrated with an ontology-based knowledge base. Through extensive experiments\nwith 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs\nface significant challenges in VEU tasks, with some performing worse than\nrandom choice. To alleviate this issue, we develop Oscars, a VEU expert model\nfine-tuned on the curated VEU-Bench dataset. It outperforms existing\nopen-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves\nperformance comparable to commercial models like GPT-4o. We also demonstrate\nthat incorporating VEU data significantly enhances the performance of Vid-LLMs\non general video understanding benchmarks, with an average improvement of 8.3%\nacross nine reasoning tasks."}
{"id": "2504.18221", "pdf": "https://arxiv.org/pdf/2504.18221", "abs": "https://arxiv.org/abs/2504.18221", "authors": ["Shuxiang Du", "Ana Guerberof Arenas", "Antonio Toral", "Kyo Gerrits", "Josep Marco Borillo"], "title": "Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish", "categories": ["cs.CL"], "comment": "This paper has been accepted to the MT Summit 2025 to be held in\n  Geneva on June 23-27 2025", "summary": "This study examines the variability of Chat-GPT machine translation (MT)\noutputs across six different configurations in four languages,with a focus on\ncreativity in a literary text. We evaluate GPT translations in different text\ngranularity levels, temperature settings and prompting strategies with a\nCreativity Score formula. We found that prompting ChatGPT with a minimal\ninstruction yields the best creative translations, with \"Translate the\nfollowing text into [TG] creatively\" at the temperature of 1.0 outperforming\nother configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless,\nChatGPT consistently underperforms compared to human translation (HT)."}
{"id": "2504.17829", "pdf": "https://arxiv.org/pdf/2504.17829", "abs": "https://arxiv.org/abs/2504.17829", "authors": ["Vlad Vasilescu", "Ana Neacsu", "Daniela Faur"], "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single-image dehazing is an important topic in remote sensing applications,\nenhancing the quality of acquired images and increasing object detection\nprecision. However, the reliability of such structures has not been\nsufficiently analyzed, which poses them to the risk of imperceptible\nperturbations that can significantly hinder their performance. In this work, we\nshow that state-of-the-art image-to-image dehazing transformers are susceptible\nto adversarial noise, with even 1 pixel change being able to decrease the PSNR\nby as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies\naimed at increasing the robustness of pre-trained transformers. Our methods\nresults in comparable clean performance, while significantly increasing the\nprotection against adversarial data. We further present their applicability in\ntwo remote sensing scenarios, showcasing their robust behavior for\nout-of-distribution data. The source code for adversarial fine-tuning and\nattack algorithms can be found at github.com/Vladimirescu/RobustDehazing."}
{"id": "2504.18225", "pdf": "https://arxiv.org/pdf/2504.18225", "abs": "https://arxiv.org/abs/2504.18225", "authors": ["Pierre-Carl Langlais", "Pavel Chizhov", "Mattia Nee", "Carlos Rosas Hinostroza", "Matthieu Delsart", "Irène Girard", "Othman Hicheur", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "title": "Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a new generation of small reasoning models for RAG, search, and\nsource summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a\nlarge synthetic dataset emulating the retrieval of a wide variety of\nmultilingual open sources from the Common Corpus. They provide native support\nfor citation and grounding with literal quotes and reintegrate multiple\nfeatures associated with RAG workflows, such as query routing, query\nreformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B\noutperform SLMs below 4 billion parameters on standardized RAG benchmarks\n(HotPotQA, 2wiki) and are competitive with popular larger models, including\nQwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date\nmaintaining consistent RAG performance across leading European languages and\nensuring systematic reference grounding for statements. Due to their size and\nease of deployment on constrained infrastructure and higher factuality by\ndesign, the models unlock a range of new use cases for generative AI."}
{"id": "2504.17892", "pdf": "https://arxiv.org/pdf/2504.17892", "abs": "https://arxiv.org/abs/2504.17892", "authors": ["Yasmine Omri", "Parth Shroff", "Thierry Tambe"], "title": "Token Sequence Compression for Efficient Multimodal Computing", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems."}
{"id": "2504.18246", "pdf": "https://arxiv.org/pdf/2504.18246", "abs": "https://arxiv.org/abs/2504.18246", "authors": ["Ritesh Goru", "Shanay Mehta", "Prateek Jain"], "title": "Efficient Single-Pass Training for Multi-Turn Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 3 figures", "summary": "Training Large Language Models ( LLMs) to generate explicit reasoning before\nthey produce an answer has been shown to improve their performance across\nvarious tasks such as mathematics and coding. However, fine-tuning LLMs on\nmulti-turn reasoning datasets presents a unique challenge: LLMs must generate\nreasoning tokens that are excluded from subsequent inputs to the LLM. This\ndiscrepancy prevents us from processing an entire conversation in a single\nforward pass-an optimization readily available when we fine-tune on a\nmulti-turn non-reasoning dataset. This paper proposes a novel approach that\novercomes this limitation through response token duplication and a custom\nattention mask that enforces appropriate visibility constraints. Our approach\nsignificantly reduces the training time and allows efficient fine-tuning on\nmulti-turn reasoning datasets."}
{"id": "2504.17894", "pdf": "https://arxiv.org/pdf/2504.17894", "abs": "https://arxiv.org/abs/2504.17894", "authors": ["Aniruddha Bala", "Rohit Chowdhury", "Rohan Jaiswal", "Siddharth Roheda"], "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques."}
{"id": "2504.18260", "pdf": "https://arxiv.org/pdf/2504.18260", "abs": "https://arxiv.org/abs/2504.18260", "authors": ["Guanqun Bi", "Zhuang Chen", "Zhoufu Liu", "Hongkai Wang", "Xiyao Xiao", "Yuqiang Xie", "Wen Zhang", "Yongkang Huang", "Yuxuan Chen", "Libiao Peng", "Yi Feng", "Minlie Huang"], "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment", "categories": ["cs.CL"], "comment": "In progress", "summary": "Automating structured clinical interviews could revolutionize mental\nhealthcare accessibility, yet existing large language models (LLMs) approaches\nfail to align with psychiatric diagnostic protocols. We present MAGI, the first\nframework that transforms the gold-standard Mini International Neuropsychiatric\nInterview (MINI) into automatic computational workflows through coordinated\nmulti-agent collaboration. MAGI dynamically navigates clinical logic via four\nspecialized agents: 1) an interview tree guided navigation agent adhering to\nthe MINI's branching structure, 2) an adaptive question agent blending\ndiagnostic probing, explaining, and empathy, 3) a judgment agent validating\nwhether the response from participants meet the node, and 4) a diagnosis Agent\ngenerating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map\nsymptoms to clinical criteria. Experimental results on 1,002 real-world\nparticipants covering depression, generalized anxiety, social anxiety and\nsuicide shows that MAGI advances LLM- assisted mental health assessment by\ncombining clinical rigor, conversational adaptability, and explainable\nreasoning."}
{"id": "2504.17902", "pdf": "https://arxiv.org/pdf/2504.17902", "abs": "https://arxiv.org/abs/2504.17902", "authors": ["Girish A. Koushik", "Diptesh Kanojia", "Helen Treharne", "Aditya Joshi"], "title": "CAMU: Context Augmentation for Meme Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Under review at ACM MM 2025", "summary": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task."}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269", "abs": "https://arxiv.org/abs/2504.18269", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance."}
{"id": "2504.17935", "pdf": "https://arxiv.org/pdf/2504.17935", "abs": "https://arxiv.org/abs/2504.17935", "authors": ["H. Martin Gillis", "Ming Hill", "Paul Hollensen", "Alan Fine", "Thomas Trappenberg"], "title": "Masked strategies for images with small objects", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The hematology analytics used for detection and classification of small blood\ncomponents is a significant challenge. In particular, when objects exists as\nsmall pixel-sized entities in a large context of similar objects. Deep learning\napproaches using supervised models with pre-trained weights, such as residual\nnetworks and vision transformers have demonstrated success for many\napplications. Unfortunately, when applied to images outside the domain of\nlearned representations, these methods often result with less than acceptable\nperformance. A strategy to overcome this can be achieved by using\nself-supervised models, where representations are learned and weights are then\napplied for downstream applications. Recently, masked autoencoders have proven\nto be effective to obtain representations that captures global context\ninformation. By masking regions of an image and having the model learn to\nreconstruct both the masked and non-masked regions, weights can be used for\nvarious applications. However, if the sizes of the objects in images are less\nthan the size of the mask, the global context information is lost, making it\nalmost impossible to reconstruct the image. In this study, we investigated the\neffect of mask ratios and patch sizes for blood components using a MAE to\nobtain learned ViT encoder representations. We then applied the encoder weights\nto train a U-Net Transformer for semantic segmentation to obtain both local and\nglobal contextual information. Our experimental results demonstrates that both\nsmaller mask ratios and patch sizes improve the reconstruction of images using\na MAE. We also show the results of semantic segmentation with and without\npre-trained weights, where smaller-sized blood components benefited with\npre-training. Overall, our proposed method offers an efficient and effective\nstrategy for the segmentation and classification of small objects."}
{"id": "2504.18346", "pdf": "https://arxiv.org/pdf/2504.18346", "abs": "https://arxiv.org/abs/2504.18346", "authors": ["Toghrul Abbasli", "Kentaroh Toyoda", "Yuan Wang", "Leon Witt", "Muhammad Asif Ali", "Yukai Miao", "Dan Li", "Qingsong Wei"], "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been transformative across many domains.\nHowever, hallucination -- confidently outputting incorrect information --\nremains one of the leading challenges for LLMs. This raises the question of how\nto accurately assess and quantify the uncertainty of LLMs. Extensive literature\non traditional models has explored Uncertainty Quantification (UQ) to measure\nuncertainty and employed calibration techniques to address the misalignment\nbetween uncertainty and accuracy. While some of these methods have been adapted\nfor LLMs, the literature lacks an in-depth analysis of their effectiveness and\ndoes not offer a comprehensive benchmark to enable insightful comparison among\nexisting solutions. In this work, we fill this gap via a systematic survey of\nrepresentative prior works on UQ and calibration for LLMs and introduce a\nrigorous benchmark. Using two widely used reliability datasets, we empirically\nevaluate six related methods, which justify the significant findings of our\nreview. Finally, we provide outlooks for key future directions and outline open\nchallenges. To the best of our knowledge, this survey is the first dedicated\nstudy to review the calibration methods and relevant metrics for LLMs."}
{"id": "2504.17990", "pdf": "https://arxiv.org/pdf/2504.17990", "abs": "https://arxiv.org/abs/2504.17990", "authors": ["Yabing Wang", "Zhuotao Tian", "Qingpei Guo", "Zheng Qin", "Sanping Zhou", "Ming Yang", "Le Wang"], "title": "From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Composed Image Retrieval (CIR) is a challenging multimodal task that\nretrieves a target image based on a reference image and accompanying\nmodification text. Due to the high cost of annotating CIR triplet datasets,\nzero-shot (ZS) CIR has gained traction as a promising alternative. Existing\nstudies mainly focus on projection-based methods, which map an image to a\nsingle pseudo-word token. However, these methods face three critical\nchallenges: (1) insufficient pseudo-word token representation capacity, (2)\ndiscrepancies between training and inference phases, and (3) reliance on\nlarge-scale synthetic data. To address these issues, we propose a two-stage\nframework where the training is accomplished from mapping to composing. In the\nfirst stage, we enhance image-to-pseudo-word token learning by introducing a\nvisual semantic injection module and a soft text alignment objective, enabling\nthe token to capture richer and fine-grained image information. In the second\nstage, we optimize the text encoder using a small amount of synthetic triplet\ndata, enabling it to effectively extract compositional semantics by combining\npseudo-word tokens with modification text for accurate target image retrieval.\nThe strong visual-to-pseudo mapping established in the first stage provides a\nsolid foundation for the second stage, making our approach compatible with both\nhigh- and low-quality synthetic data, and capable of achieving significant\nperformance gains with only a small amount of synthetic data. Extensive\nexperiments were conducted on three public datasets, achieving superior\nperformance compared to existing approaches."}
{"id": "2504.18373", "pdf": "https://arxiv.org/pdf/2504.18373", "abs": "https://arxiv.org/abs/2504.18373", "authors": ["Lei Shen", "Xiaoyu Shen"], "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, multi-agent frameworks powered by large language models\n(LLMs) have advanced rapidly. Despite this progress, there is still a notable\nabsence of benchmark datasets specifically tailored to evaluate their\nperformance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset\naimed at evaluating LLM-based multi-agent frameworks in the context of\nintelligent personal assistants. Auto-SLURP extends the original SLURP dataset\n-- initially developed for natural language understanding tasks -- by\nrelabeling the data and integrating simulated servers and external services.\nThis enhancement enables a comprehensive end-to-end evaluation pipeline,\ncovering language understanding, task execution, and response generation. Our\nexperiments demonstrate that Auto-SLURP presents a significant challenge for\ncurrent state-of-the-art frameworks, highlighting that truly reliable and\nintelligent multi-agent personal assistants remain a work in progress. The\ndataset and related code are available at\nhttps://github.com/lorashen/Auto-SLURP/."}
{"id": "2504.17991", "pdf": "https://arxiv.org/pdf/2504.17991", "abs": "https://arxiv.org/abs/2504.17991", "authors": ["Zheng Qin", "Le Wang", "Yabing Wang", "Sanping Zhou", "Gang Hua", "Wei Tang"], "title": "RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Recent image-goal navigation (ImageNav) methods learn a perception-action\npolicy by separately capturing semantic features of the goal and egocentric\nimages, then passing them to a policy network. However, challenges remain: (1)\nSemantic features often fail to provide accurate directional information,\nleading to superfluous actions, and (2) performance drops significantly when\nviewpoint inconsistencies arise between training and application. To address\nthese challenges, we propose RSRNav, a simple yet effective method that reasons\nspatial relationships between the goal and current observations as navigation\nguidance. Specifically, we model the spatial relationship by constructing\ncorrelations between the goal and current observations, which are then passed\nto the policy network for action prediction. These correlations are\nprogressively refined using fine-grained cross-correlation and direction-aware\ncorrelation for more precise navigation. Extensive evaluation of RSRNav on\nthree benchmark datasets demonstrates superior navigation performance,\nparticularly in the \"user-matched goal\" setting, highlighting its potential for\nreal-world applications."}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376", "abs": "https://arxiv.org/abs/2504.18376", "authors": ["Pablo Miralles-González", "Javier Huertas-Tato", "Alejandro Martín", "David Camacho"], "title": "Pushing the boundary on Natural Language Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality."}
{"id": "2504.17996", "pdf": "https://arxiv.org/pdf/2504.17996", "abs": "https://arxiv.org/abs/2504.17996", "authors": ["Yuanbing Ouyang", "Yizhuo Liang", "Qingpeng Li", "Xinfei Guo", "Yiming Luo", "Di Wu", "Hao Wang", "Yushan Pan"], "title": "Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) excel in semantic segmentation but demand\nsignificant computation, posing challenges for deployment on\nresource-constrained devices. Existing token pruning methods often overlook\nfundamental visual data characteristics. This study introduces 'LVTP', a\nprogressive token pruning framework guided by multi-scale Tsallis entropy and\nlow-level visual features with twice clustering. It integrates high-level\nsemantics and basic visual attributes for precise segmentation. A novel dynamic\nscoring mechanism using multi-scale Tsallis entropy weighting overcomes\nlimitations of traditional single-parameter entropy. The framework also\nincorporates low-level feature analysis to preserve critical edge information\nwhile optimizing computational cost. As a plug-and-play module, it requires no\narchitectural changes or additional training. Evaluations across multiple\ndatasets show 20%-45% computational reductions with negligible performance\nloss, outperforming existing methods in balancing cost and accuracy, especially\nin complex edge regions."}
{"id": "2504.18386", "pdf": "https://arxiv.org/pdf/2504.18386", "abs": "https://arxiv.org/abs/2504.18386", "authors": ["Amir Zeldes", "Nina Speransky", "Nicholas Wagner", "Caroline T. Schroeder"], "title": "A UD Treebank for Bohairic Coptic", "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advances in digital resources for other Coptic dialects,\nespecially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk,\nlate Byzantine Egypt, and the contemporary language of the Coptic Church,\nremains critically under-resourced. This paper presents and evaluates the first\nsyntactically annotated corpus of Bohairic Coptic, sampling data from a range\nof works, including Biblical text, saints' lives and Christian ascetic writing.\nWe also explore some of the main differences we observe compared to the\nexisting UD treebank of Sahidic Coptic, the classical dialect of the language,\nand conduct joint and cross-dialect parsing experiments, revealing the unique\nnature of Bohairic as a related, but distinct variety from the more often\nstudied Sahidic."}
{"id": "2504.18020", "pdf": "https://arxiv.org/pdf/2504.18020", "abs": "https://arxiv.org/abs/2504.18020", "authors": ["Guyue Hu", "Siyuan Song", "Yukun Kang", "Zhu Yin", "Gangming Zhao", "Chenglong Li", "Jin Tang"], "title": "Federated Client-tailored Adapter for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation in X-ray images is beneficial for computer-aided\ndiagnosis and lesion localization. Existing methods mainly fall into a\ncentralized learning paradigm, which is inapplicable in the practical medical\nscenario that only has access to distributed data islands. Federated Learning\nhas the potential to offer a distributed solution but struggles with heavy\ntraining instability due to client-wise domain heterogeneity (including\ndistribution diversity and class imbalance). In this paper, we propose a novel\nFederated Client-tailored Adapter (FCA) framework for medical image\nsegmentation, which achieves stable and client-tailored adaptive segmentation\nwithout sharing sensitive local data. Specifically, the federated adapter stirs\nuniversal knowledge in off-the-shelf medical foundation models to stabilize the\nfederated training process. In addition, we develop two client-tailored\nfederated updating strategies that adaptively decompose the adapter into common\nand individual components, then globally and independently update the parameter\ngroups associated with common client-invariant and individual client-specific\nunits, respectively. They further stabilize the heterogeneous federated\nlearning process and realize optimal client-tailored instead of sub-optimal\nglobal-compromised segmentation models. Extensive experiments on three\nlarge-scale datasets demonstrate the effectiveness and superiority of the\nproposed FCA framework for federated medical segmentation."}
{"id": "2504.18406", "pdf": "https://arxiv.org/pdf/2504.18406", "abs": "https://arxiv.org/abs/2504.18406", "authors": ["Yusen Zhang", "Wenliang Zheng", "Aashrith Madasu", "Peng Shi", "Ryo Kamoi", "Hao Zhou", "Zhuoyang Zou", "Shu Zhao", "Sarkar Snigdha Sarathi Das", "Vipul Gupta", "Xiaoxin Lu", "Nan Zhang", "Ranran Haoran Zhang", "Avitej Iyer", "Renze Lou", "Wenpeng Yin", "Rui Zhang"], "title": "HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "High-resolution image (HRI) understanding aims to process images with a large\nnumber of pixels, such as pathological images and agricultural aerial images,\nboth of which can exceed 1 million pixels. Vision Large Language Models (VLMs)\ncan allegedly handle HRIs, however, there is a lack of a comprehensive\nbenchmark for VLMs to evaluate HRI understanding. To address this gap, we\nintroduce HRScene, a novel unified benchmark for HRI understanding with rich\nscenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic\ndatasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$\n26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,\ncovering 25 scenarios, ranging from microscopic to radiology images, street\nviews, long-range pictures, and telescope images. It includes HRIs of\nreal-world objects, scanned documents, and composite multi-image. The two\ndiagnostic evaluation datasets are synthesized by combining the target image\nwith the gold answer and distracting images in different orders, assessing how\nwell models utilize regions in HRI. We conduct extensive experiments involving\n28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show\nthat current VLMs achieve an average accuracy of around 50% on real-world\ntasks, revealing significant gaps in HRI understanding. Results on synthetic\ndatasets reveal that VLMs struggle to effectively utilize HRI regions, showing\nsignificant Regional Divergence and lost-in-middle, shedding light on future\nresearch."}
{"id": "2504.18025", "pdf": "https://arxiv.org/pdf/2504.18025", "abs": "https://arxiv.org/abs/2504.18025", "authors": ["Shuanglin Yan", "Neng Dong", "Shuang Li", "Rui Yan", "Hao Tang", "Jing Qin"], "title": "ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Visible-Infrared Person Re-identification (VIReID) aims to match visible and\ninfrared pedestrian images, but the modality differences and the complexity of\nidentity features make it challenging. Existing methods rely solely on identity\nlabel supervision, which makes it difficult to fully extract high-level\nsemantic information. Recently, vision-language pre-trained models have been\nintroduced to VIReID, enhancing semantic information modeling by generating\ntextual descriptions. However, such methods do not explicitly model body shape\nfeatures, which are crucial for cross-modal matching. To address this, we\npropose an effective Body Shape-aware Textual Alignment (BSaTa) framework that\nexplicitly models and utilizes body shape information to improve VIReID\nperformance. Specifically, we design a Body Shape Textual Alignment (BSTA)\nmodule that extracts body shape information using a human parsing model and\nconverts it into structured text representations via CLIP. We also design a\nText-Visual Consistency Regularizer (TVCR) to ensure alignment between body\nshape textual representations and visual body shape features. Furthermore, we\nintroduce a Shape-aware Representation Learning (SRL) mechanism that combines\nMulti-text Supervision and Distribution Consistency Constraints to guide the\nvisual encoder to learn modality-invariant and discriminative identity\nfeatures, thus enhancing modality invariance. Experimental results demonstrate\nthat our method achieves superior performance on the SYSU-MM01 and RegDB\ndatasets, validating its effectiveness."}
{"id": "2504.18412", "pdf": "https://arxiv.org/pdf/2504.18412", "abs": "https://arxiv.org/abs/2504.18412", "authors": ["Jared Moore", "Declan Grabb", "William Agnew", "Kevin Klyman", "Stevie Chancellor", "Desmond C. Ong", "Nick Haber"], "title": "Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers", "categories": ["cs.CL"], "comment": null, "summary": "Should a large language model (LLM) be used as a therapist? In this paper, we\ninvestigate the use of LLMs to *replace* mental health providers, a use case\npromoted in the tech startup and research space. We conduct a mapping review of\ntherapy guides used by major medical institutions to identify crucial aspects\nof therapeutic relationships, such as the importance of a therapeutic alliance\nbetween therapist and client. We then assess the ability of LLMs to reproduce\nand adhere to these aspects of therapeutic relationships by conducting several\nexperiments investigating the responses of current LLMs, such as `gpt-4o`.\nContrary to best practices in the medical community, LLMs 1) express stigma\ntoward those with mental health conditions and 2) respond inappropriately to\ncertain common (and critical) conditions in naturalistic therapy settings --\ne.g., LLMs encourage clients' delusional thinking, likely due to their\nsycophancy. This occurs even with larger and newer LLMs, indicating that\ncurrent safety practices may not address these gaps. Furthermore, we note\nfoundational and practical barriers to the adoption of LLMs as therapists, such\nas that a therapeutic alliance requires human characteristics (e.g., identity\nand stakes). For these reasons, we conclude that LLMs should not replace\ntherapists, and we discuss alternative roles for LLMs in clinical therapy."}
{"id": "2504.18027", "pdf": "https://arxiv.org/pdf/2504.18027", "abs": "https://arxiv.org/abs/2504.18027", "authors": ["Zezhou Chen", "Zhaoxiang Liu", "Kai Wang", "Kohou Wang", "Shiguo Lian"], "title": "A Large Vision-Language Model based Environment Perception System for Visually Impaired People", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted by IROS2024(9 pages, 8 figures)", "summary": "It is a challenging task for visually impaired people to perceive their\nsurrounding environment due to the complexity of the natural scenes. Their\npersonal and social activities are thus highly limited. This paper introduces a\nLarge Vision-Language Model(LVLM) based environment perception system which\nhelps them to better understand the surrounding environment, by capturing the\ncurrent scene they face with a wearable device, and then letting them retrieve\nthe analysis results through the device. The visually impaired people could\nacquire a global description of the scene by long pressing the screen to\nactivate the LVLM output, retrieve the categories of the objects in the scene\nresulting from a segmentation model by tapping or swiping the screen, and get a\ndetailed description of the objects they are interested in by double-tapping\nthe screen. To help visually impaired people more accurately perceive the\nworld, this paper proposes incorporating the segmentation result of the RGB\nimage as external knowledge into the input of LVLM to reduce the LVLM's\nhallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the\nsystem could provide a more accurate description of the scene compared to\nQwen-VL-Chat, exploratory experiments show that the system helps visually\nimpaired people to perceive the surrounding environment effectively."}
{"id": "2504.18415", "pdf": "https://arxiv.org/pdf/2504.18415", "abs": "https://arxiv.org/abs/2504.18415", "authors": ["Hongyu Wang", "Shuming Ma", "Furu Wei"], "title": "BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by\nactivation outliers, which complicate quantization to low bit-widths. We\nintroduce BitNet v2, a novel framework enabling native 4-bit activation\nquantization for 1-bit LLMs. To tackle outliers in attention and feed-forward\nnetwork activations, we propose H-BitLinear, a module applying an online\nHadamard transformation prior to activation quantization. This transformation\nsmooths sharp activation distributions into more Gaussian-like forms, suitable\nfor low-bit representation. Experiments show BitNet v2 trained from scratch\nwith 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2\nachieves minimal performance degradation when trained with native 4-bit\nactivations, significantly reducing memory footprint and computational cost for\nbatched inference."}
{"id": "2504.18032", "pdf": "https://arxiv.org/pdf/2504.18032", "abs": "https://arxiv.org/abs/2504.18032", "authors": ["Chen Chen", "Daochang Liu", "Mubarak Shah", "Chang Xu"], "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. Project page:\n  https://chenchen-usyd.github.io/PRSS-Project-Page/", "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ncreating images highly aligned with user prompts, yet their proclivity for\nmemorizing training set images has sparked concerns about the originality of\nthe generated images and privacy issues, potentially leading to legal\ncomplications for both model owners and users, particularly when the memorized\nimages contain proprietary content. Although methods to mitigate these issues\nhave been suggested, enhancing privacy often results in a significant decrease\nin the utility of the outputs, as indicated by text-alignment scores. To bridge\nthe research gap, we introduce a novel method, PRSS, which refines the\nclassifier-free guidance approach in diffusion models by integrating prompt\nre-anchoring (PR) to improve privacy and incorporating semantic prompt search\n(SS) to enhance utility. Extensive experiments across various privacy levels\ndemonstrate that our approach consistently improves the privacy-utility\ntrade-off, establishing a new state-of-the-art."}
{"id": "2504.18428", "pdf": "https://arxiv.org/pdf/2504.18428", "abs": "https://arxiv.org/abs/2504.18428", "authors": ["Yiming Wang", "Pei Zhang", "Jialong Tang", "Haoran Wei", "Baosong Yang", "Rui Wang", "Chenshu Sun", "Feitong Sun", "Jiran Zhang", "Junxuan Wu", "Qiqian Cang", "Yichang Zhang", "Fei Huang", "Junyang Lin", "Fei Huang", "Jingren Zhou"], "title": "PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce PolyMath, a multilingual mathematical reasoning\nbenchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our\nbenchmark ensures difficulty comprehensiveness, language diversity, and\nhigh-quality translation, making it a highly discriminative multilingual\nmathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive\nevaluation for advanced LLMs and find that even Deepseek-R1-671B and\nQwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30%\naccuracy under the highest level. From a language perspective, our benchmark\nreveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning\nperformance varies widely across languages for current LLMs; (2) Input-output\nlanguage consistency is low in reasoning LLMs and may be correlated with\nperformance; (3) The thinking length differs significantly by language for\ncurrent LLMs. Additionally, we demonstrate that controlling the output language\nin the instructions has the potential to affect reasoning performance,\nespecially for some low-resource languages, suggesting a promising direction\nfor improving multilingual capabilities in LLMs."}
{"id": "2504.18040", "pdf": "https://arxiv.org/pdf/2504.18040", "abs": "https://arxiv.org/abs/2504.18040", "authors": ["Xiaoyi Liu", "Hao Tang"], "title": "Cabbage: A Differential Growth Framework for Open Surfaces", "categories": ["cs.CV"], "comment": null, "summary": "We propose Cabbage, a differential growth framework to model buckling\nbehavior in 3D open surfaces found in nature-like the curling of flower petals.\nCabbage creates high-quality triangular meshes free of self-intersection.\nCabbage-Shell is driven by edge subdivision which differentially increases\ndiscretization resolution. Shell forces expands the surface, generating\nbuckling over time. Feature-aware smoothing and remeshing ensures mesh quality.\nCorrective collision effectively prevents self-collision even in tight spaces.\nWe additionally provide Cabbage-Collision, and approximate alternative,\nfollowed by CAD-ready surface generation. Cabbage is the first open-source\neffort with this calibre and robustness, outperforming SOTA methods in its\nmorphological expressiveness, mesh quality, and stably generates large, complex\npatterns over hundreds of simulation steps. It is a source not only of\ncomputational modeling, digital fabrication, education, but also high-quality,\nannotated data for geometry processing and shape analysis."}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458", "abs": "https://arxiv.org/abs/2504.18458", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy."}
{"id": "2504.18046", "pdf": "https://arxiv.org/pdf/2504.18046", "abs": "https://arxiv.org/abs/2504.18046", "authors": ["Guohao Huo", "Zibo Lin", "Zitong Wang", "Ruiting Dai", "Hao Tang"], "title": "DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Ophthalmic diseases pose a significant global health challenge, yet\ntraditional diagnosis methods and existing single-eye deep learning approaches\noften fail to account for binocular pathological correlations. To address this,\nwe propose DMS-Net, a dual-modal multi-scale Siamese network for binocular\nfundus image classification. Our framework leverages weight-shared Siamese\nResNet-152 backbones to extract deep semantic features from paired fundus\nimages. To tackle challenges such as lesion boundary ambiguity and scattered\npathological distributions, we introduce a Multi-Scale Context-Aware Module\n(MSCAM) that integrates adaptive pooling and attention mechanisms for\nmulti-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion\n(DMFF) module enhances cross-modal interaction through spatial-semantic\nrecalibration and bidirectional attention, effectively combining global context\nand local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves\nstate-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8%\nCohen's kappa, demonstrating superior capability in detecting symmetric\npathologies and advancing clinical decision-making for ocular diseases."}
{"id": "2504.18474", "pdf": "https://arxiv.org/pdf/2504.18474", "abs": "https://arxiv.org/abs/2504.18474", "authors": ["James D. Finch", "Yasasvi Josyula", "Jinho D. Choi"], "title": "Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions", "categories": ["cs.CL"], "comment": "Accepted (B) to TACL 2025", "summary": "In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is\nessential for automatically identifying key information slots from dialogue\ndata without manual intervention. This paper presents a novel state-of-the-art\n(SoTA) approach that formulates SSI as a text generation task, where a language\nmodel incrementally constructs and refines a slot schema over a stream of\ndialogue data. To develop this approach, we present a fully automatic LLM-based\nTOD simulation method that creates data with high-quality state labels for\nnovel task domains. Furthermore, we identify issues in SSI evaluation due to\ndata leakage and poor metric alignment with human judgment. We resolve these by\ncreating new evaluation data using our simulation method with human guidance\nand correction, as well as designing improved evaluation metrics. These\ncontributions establish a foundation for future SSI research and advance the\nSoTA in dialogue understanding and system development."}
{"id": "2504.18049", "pdf": "https://arxiv.org/pdf/2504.18049", "abs": "https://arxiv.org/abs/2504.18049", "authors": ["Xin Li", "Wenhui Zhu", "Peijie Qiu", "Oana M. Dumitrascu", "Amal Youssef", "Yalin Wang"], "title": "A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the field of medical imaging, the advent of deep learning, especially the\napplication of convolutional neural networks (CNNs) has revolutionized the\nanalysis and interpretation of medical images. Nevertheless, deep learning\nmethods usually rely on large amounts of labeled data. In medical imaging\nresearch, the acquisition of high-quality labels is both expensive and\ndifficult. The introduction of Vision Transformers (ViT) and self-supervised\nlearning provides a pre-training strategy that utilizes abundant unlabeled\ndata, effectively alleviating the label acquisition challenge while broadening\nthe breadth of data utilization. However, ViT's high computational density and\nsubstantial demand for computing power, coupled with the lack of localization\ncharacteristics of its operations on image patches, limit its efficiency and\napplicability in many application scenarios. In this study, we employ\nnn-MobileNet, a lightweight CNN framework, to implement a BERT-style\nself-supervised learning approach. We pre-train the network on the unlabeled\nretinal fundus images from the UK Biobank to improve downstream application\nperformance. We validate the results of the pre-trained model on Alzheimer's\ndisease (AD), Parkinson's disease (PD), and various retinal diseases\nidentification. The results show that our approach can significantly improve\nperformance in the downstream tasks. In summary, this study combines the\nbenefits of CNNs with the capabilities of advanced self-supervised learning in\nhandling large-scale unlabeled data, demonstrating the potential of CNNs in the\npresence of label scarcity."}
{"id": "2504.18483", "pdf": "https://arxiv.org/pdf/2504.18483", "abs": "https://arxiv.org/abs/2504.18483", "authors": ["Leandra Fichtel", "Maximilian Spliethöver", "Eyke Hüllermeier", "Patricia Jimenez", "Nils Klowait", "Stefan Kopp", "Axel-Cyrille Ngonga Ngomo", "Amelie Robrecht", "Ingrid Scharlau", "Lutz Terfloth", "Anna-Lisa Vollmer", "Henning Wachsmuth"], "title": "Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues", "categories": ["cs.CL"], "comment": "Submitted to the SIGDial Conference 2025", "summary": "The ability to generate explanations that are understood by explainees is the\nquintessence of explainable artificial intelligence. Since understanding\ndepends on the explainee's background and needs, recent research has focused on\nco-constructive explanation dialogues, where the explainer continuously\nmonitors the explainee's understanding and adapts explanations dynamically. We\ninvestigate the ability of large language models (LLMs) to engage as explainers\nin co-constructive explanation dialogues. In particular, we present a user\nstudy in which explainees interact with LLMs, of which some have been\ninstructed to explain a predefined topic co-constructively. We evaluate the\nexplainees' understanding before and after the dialogue, as well as their\nperception of the LLMs' co-constructive behavior. Our results indicate that\ncurrent LLMs show some co-constructive behaviors, such as asking verification\nquestions, that foster the explainees' engagement and can improve understanding\nof a topic. However, their ability to effectively monitor the current\nunderstanding and scaffold the explanations accordingly remains limited."}
{"id": "2504.18059", "pdf": "https://arxiv.org/pdf/2504.18059", "abs": "https://arxiv.org/abs/2504.18059", "authors": ["Prachi Garg", "Joseph K J", "Vineeth N Balasubramanian", "Necati Cihan Camgoz", "Chengde Wan", "Kenrick Kin", "Weiguang Si", "Shugao Ma", "Fernando De La Torre"], "title": "POET: Prompt Offset Tuning for Continual Human Action Adaptation", "categories": ["cs.CV"], "comment": "ECCV 2024 (Oral), webpage\n  https://humansensinglab.github.io/POET-continual-action-recognition/", "summary": "As extended reality (XR) is redefining how users interact with computing\ndevices, research in human action recognition is gaining prominence. Typically,\nmodels deployed on immersive computing devices are static and limited to their\ndefault set of classes. The goal of our research is to provide users and\ndevelopers with the capability to personalize their experience by adding new\naction classes to their device models continually. Importantly, a user should\nbe able to add new classes in a low-shot and efficient manner, while this\nprocess should not require storing or replaying any of user's sensitive\ntraining data. We formalize this problem as privacy-aware few-shot continual\naction recognition. Towards this end, we propose POET: Prompt-Offset Tuning.\nWhile existing prompt tuning approaches have shown great promise for continual\nlearning of image, text, and video modalities; they demand access to\nextensively pretrained transformers. Breaking away from this assumption, POET\ndemonstrates the efficacy of prompt tuning a significantly lightweight\nbackbone, pretrained exclusively on the base class data. We propose a novel\nspatio-temporal learnable prompt offset tuning approach, and are the first to\napply such prompt tuning to Graph Neural Networks. We contribute two new\nbenchmarks for our new problem setting in human action recognition: (i) NTU\nRGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand\ngesture recognition. We find that POET consistently outperforms comprehensive\nbenchmarks. Source code at\nhttps://github.com/humansensinglab/POET-continual-action-recognition."}
{"id": "2504.18535", "pdf": "https://arxiv.org/pdf/2504.18535", "abs": "https://arxiv.org/abs/2504.18535", "authors": ["Gwen Yidou Weng", "Benjie Wang", "Guy Van den Broeck"], "title": "TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LMs) advance, there is an increasing need to\ncontrol their outputs to align with human values (e.g., detoxification) or\ndesired attributes (e.g., personalization, topic). However, autoregressive\nmodels focus on next-token predictions and struggle with global properties that\nrequire looking ahead. Existing solutions either tune or post-train LMs for\neach new attribute - expensive and inflexible - or approximate the Expected\nAttribute Probability (EAP) of future sequences by sampling or training, which\nis slow and unreliable for rare attributes. We introduce TRACE (Tractable\nProbabilistic Reasoning for Adaptable Controllable gEneration), a novel\nframework that efficiently computes EAP and adapts to new attributes through\ntractable probabilistic reasoning and lightweight control. TRACE distills a\nHidden Markov Model (HMM) from an LM and pairs it with a small classifier to\nestimate attribute probabilities, enabling exact EAP computation over the HMM's\npredicted futures. This EAP is then used to reweigh the LM's next-token\nprobabilities for globally compliant continuations. Empirically, TRACE achieves\nstate-of-the-art results in detoxification with only 10% decoding overhead,\nadapts to 76 low-resource personalized LLMs within seconds, and seamlessly\nextends to composite attributes."}
{"id": "2504.18068", "pdf": "https://arxiv.org/pdf/2504.18068", "abs": "https://arxiv.org/abs/2504.18068", "authors": ["Zhuohao Yan", "Shaoquan Feng", "Xingxing Li", "Yuxuan Zhou", "Chunxi Xia", "Shengyu Li"], "title": "S3MOT: Monocular 3D Object Tracking with Selective State Space Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and reliable multi-object tracking (MOT) in 3D space is essential\nfor advancing robotics and computer vision applications. However, it remains a\nsignificant challenge in monocular setups due to the difficulty of mining 3D\nspatiotemporal associations from 2D video streams. In this work, we present\nthree innovative techniques to enhance the fusion and exploitation of\nheterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State\nSpace Model (HSSM), a novel data association mechanism that compresses\ncontextual tracking cues across multiple paths, enabling efficient and\ncomprehensive assignment decisions with linear complexity. HSSM features a\nglobal receptive field and dynamic weights, in contrast to traditional linear\nassignment algorithms that rely on hand-crafted association costs. (2) We\npropose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI\npooling by directly using dense feature maps for contrastive learning, thus\nimproving object re-identification accuracy under challenging conditions such\nas varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation\nthrough VeloSSM, an encoder-decoder architecture that models temporal\ndependencies in velocity to capture motion dynamics, overcoming the limitations\nof frame-based 3D inference. Experiments on the KITTI public test benchmark\ndemonstrate the effectiveness of our method, achieving a new state-of-the-art\nperformance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best\nby significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness\nand efficiency for monocular 3D MOT tasks. The code and models are available at\nhttps://github.com/bytepioneerX/s3mot."}
{"id": "2504.17821", "pdf": "https://arxiv.org/pdf/2504.17821", "abs": "https://arxiv.org/abs/2504.17821", "authors": ["Xinyu Chen", "Yunxin Li", "Haoyuan Shi", "Baotian Hu", "Wenhan Luo", "Yaowei Wang", "Min Zhang"], "title": "VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Assessing the video comprehension capabilities of multimodal AI systems can\neffectively measure their understanding and reasoning abilities. Most video\nevaluation benchmarks are limited to a single language, typically English, and\npredominantly feature videos rooted in Western cultural contexts. In this\npaper, we present VideoVista-CulturalLingo, the first video evaluation\nbenchmark designed to bridge cultural, linguistic, and domain divide in video\ncomprehension. Our work differs from existing benchmarks in the following ways:\n1) Cultural diversity, incorporating cultures from China, North America, and\nEurope; 2) Multi-linguistics, with questions presented in Chinese and\nEnglish-two of the most widely spoken languages; and 3) Broad domain, featuring\nvideos sourced from hundreds of human-created domains. VideoVista-CulturalLingo\ncontains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent\nopen-source or proprietary video large models. From the experiment results, we\nobserve that: 1) Existing models perform worse on Chinese-centric questions\nthan Western-centric ones, particularly those related to Chinese history; 2)\nCurrent open-source models still exhibit limitations in temporal understanding,\nespecially in the Event Localization task, achieving a maximum score of only\n45.2%; 3) Mainstream models demonstrate strong performance in general\nscientific questions, while open-source models demonstrate weak performance in\nmathematics."}
{"id": "2504.18087", "pdf": "https://arxiv.org/pdf/2504.18087", "abs": "https://arxiv.org/abs/2504.18087", "authors": ["Weipeng Tan", "Chuming Lin", "Chengming Xu", "FeiFan Xu", "Xiaobin Hu", "Xiaozhong Ji", "Junwei Zhu", "Chengjie Wang", "Yanwei Fu"], "title": "Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2409.03270", "summary": "Recent advances in Talking Head Generation (THG) have achieved impressive lip\nsynchronization and visual quality through diffusion models; yet existing\nmethods struggle to generate emotionally expressive portraits while preserving\nspeaker identity. We identify three critical limitations in current emotional\ntalking head generation: insufficient utilization of audio's inherent emotional\ncues, identity leakage in emotion representations, and isolated learning of\nemotion correlations. To address these challenges, we propose a novel framework\ndubbed as DICE-Talk, following the idea of disentangling identity with emotion,\nand then cooperating emotions with similar characteristics. First, we develop a\ndisentangled emotion embedder that jointly models audio-visual emotional cues\nthrough cross-modal attention, representing emotions as identity-agnostic\nGaussian distributions. Second, we introduce a correlation-enhanced emotion\nconditioning module with learnable Emotion Banks that explicitly capture\ninter-emotion relationships through vector quantization and attention-based\nfeature aggregation. Third, we design an emotion discrimination objective that\nenforces affective consistency during the diffusion process through\nlatent-space classification. Extensive experiments on MEAD and HDTF datasets\ndemonstrate our method's superiority, outperforming state-of-the-art approaches\nin emotion accuracy while maintaining competitive lip-sync performance.\nQualitative results and user studies further confirm our method's ability to\ngenerate identity-preserving portraits with rich, correlated emotional\nexpressions that naturally adapt to unseen identities."}
{"id": "2504.17884", "pdf": "https://arxiv.org/pdf/2504.17884", "abs": "https://arxiv.org/abs/2504.17884", "authors": ["Yongkang Li", "Panagiotis Eustratiadis", "Simon Lupart", "Evangelos Kanoulas"], "title": "Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "This paper has been accepted as a full paper at SIGIR 2025 and will\n  be presented orally", "summary": "This paper concerns corpus poisoning attacks in dense information retrieval,\nwhere an adversary attempts to compromise the ranking performance of a search\nalgorithm by injecting a small number of maliciously generated documents into\nthe corpus. Our work addresses two limitations in the current literature.\nFirst, attacks that perform adversarial gradient-based word substitution search\ndo so in the discrete lexical space, while retrieval itself happens in the\ncontinuous embedding space. We thus propose an optimization method that\noperates in the embedding space directly. Specifically, we train a perturbation\nmodel with the objective of maintaining the geometric distance between the\noriginal and adversarial document embeddings, while also maximizing the\ntoken-level dissimilarity between the original and adversarial documents.\nSecond, it is common for related work to have a strong assumption that the\nadversary has prior knowledge about the queries. In this paper, we focus on a\nmore challenging variant of the problem where the adversary assumes no prior\nknowledge about the query distribution (hence, unsupervised). Our core\ncontribution is an adversarial corpus attack that is fast and effective. We\npresent comprehensive experimental results on both in- and out-of-domain\ndatasets, focusing on two related tasks: a top-1 attack and a corpus poisoning\nattack. We consider attacks under both a white-box and a black-box setting.\nNotably, our method can generate successful adversarial examples in under two\nminutes per target document; four times faster compared to the fastest\ngradient-based word substitution methods in the literature with the same\nhardware. Furthermore, our adversarial generation method generates text that is\nmore likely to occur under the distribution of natural text (low perplexity),\nand is therefore more difficult to detect."}
{"id": "2504.18112", "pdf": "https://arxiv.org/pdf/2504.18112", "abs": "https://arxiv.org/abs/2504.18112", "authors": ["Deepak Ghimire", "Byoungjun Kim", "Donghoon Kim", "SungHwan Jeong"], "title": "Study on Real-Time Road Surface Reconstruction Using Stereo Vision", "categories": ["cs.CV"], "comment": "Stereo Vision, Efficient CNN, Pruning, Optimization. 2025 Intelligent\n  Information and Control Conference (IICC 2025), Jeonju, Korea", "summary": "Road surface reconstruction plays a crucial role in autonomous driving,\nproviding essential information for safe and smooth navigation. This paper\nenhances the RoadBEV [1] framework for real-time inference on edge devices by\noptimizing both efficiency and accuracy. To achieve this, we proposed to apply\nIsomorphic Global Structured Pruning to the stereo feature extraction backbone,\nreducing network complexity while maintaining performance. Additionally, the\nhead network is redesigned with an optimized hourglass structure, dynamic\nattention heads, reduced feature channels, mixed precision inference, and\nefficient probability volume computation. Our approach improves inference speed\nwhile achieving lower reconstruction error, making it well-suited for real-time\nroad surface reconstruction in autonomous driving."}
{"id": "2504.17892", "pdf": "https://arxiv.org/pdf/2504.17892", "abs": "https://arxiv.org/abs/2504.17892", "authors": ["Yasmine Omri", "Parth Shroff", "Thierry Tambe"], "title": "Token Sequence Compression for Efficient Multimodal Computing", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The exponential growth of Large Multimodal Models (LMMs) has driven\nadvancements in cross-modal reasoning but at significant computational costs.\nIn this work, we focus on visual language models. We highlight the redundancy\nand inefficiency in current vision encoders, and seek to construct an adaptive\ncompression method for multimodal data. In this work, we characterize a panoply\nof visual token selection and merging approaches through both benchmarking and\nqualitative analysis. In particular, we demonstrate that simple cluster-level\ntoken aggregation outperforms prior state-of-the-art works in token selection\nand merging, including merging at the vision encoder level and attention-based\napproaches. We underline the redundancy in current vision encoders, and shed\nlight on several puzzling trends regarding principles of visual token selection\nthrough cross-modal attention visualizations. This work is a first effort\ntowards more effective encoding and processing of high-dimensional data, and\npaves the way for more scalable and sustainable multimodal systems."}
{"id": "2504.18127", "pdf": "https://arxiv.org/pdf/2504.18127", "abs": "https://arxiv.org/abs/2504.18127", "authors": ["Jingfan Yang", "Hu Gao", "Ying Zhang", "Depeng Dang"], "title": "Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network", "categories": ["cs.CV"], "comment": null, "summary": "Spacecraft image super-resolution seeks to enhance low-resolution spacecraft\nimages into high-resolution ones. Although existing arbitrary-scale\nsuper-resolution methods perform well on general images, they tend to overlook\nthe difference in features between the spacecraft core region and the large\nblack space background, introducing irrelevant noise. In this paper, we propose\na salient region-guided spacecraft image arbitrary-scale super-resolution\nnetwork (SGSASR), which uses features from the spacecraft core salient regions\nto guide latent modulation and achieve arbitrary-scale super-resolution.\nSpecifically, we design a spacecraft core region recognition block (SCRRB) that\nidentifies the core salient regions in spacecraft images using a pre-trained\nsaliency detection model. Furthermore, we present an adaptive-weighted feature\nfusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft\ncore region features with general image features by dynamic weight parameter to\nenhance the response of the core salient regions. Experimental results\ndemonstrate that the proposed SGSASR outperforms state-of-the-art approaches."}
{"id": "2504.17902", "pdf": "https://arxiv.org/pdf/2504.17902", "abs": "https://arxiv.org/abs/2504.17902", "authors": ["Girish A. Koushik", "Diptesh Kanojia", "Helen Treharne", "Aditya Joshi"], "title": "CAMU: Context Augmentation for Meme Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Under review at ACM MM 2025", "summary": "Social media memes are a challenging domain for hate detection because they\nintertwine visual and textual cues into culturally nuanced messages. We\nintroduce a novel framework, CAMU, which leverages large vision-language models\nto generate more descriptive captions, a caption-scoring neural network to\nemphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's\ntext encoder for an improved multimodal understanding of memes. Experiments on\npublicly available hateful meme datasets show that simple projection layer\nfine-tuning yields modest gains, whereas selectively tuning deeper text encoder\nlayers significantly boosts performance on all evaluation metrics. Moreover,\nour approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful\nMemes dataset, at par with the existing SoTA framework while being much more\nefficient, offering practical advantages in real-world scenarios that rely on\nfixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the\nMultiOFF dataset for offensive meme identification, demonstrating its\ngeneralisability. Additional analyses on benign confounders reveal that robust\nvisual grounding and nuanced text representations are crucial for reliable hate\nand offence detection. We will publicly release CAMU along with the resultant\nmodels for further research.\n  Disclaimer: This paper includes references to potentially disturbing,\nhateful, or offensive content due to the nature of the task."}
{"id": "2504.18136", "pdf": "https://arxiv.org/pdf/2504.18136", "abs": "https://arxiv.org/abs/2504.18136", "authors": ["Liugang Lu", "Dabin He", "Congxiang Liu", "Zhixiang Deng"], "title": "MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer\nvision technologies, object detection from UAV perspectives has emerged as a\nprominent research area. However, challenges for detection brought by the\nextremely small proportion of target pixels, significant scale variations of\nobjects, and complex background information in UAV images have greatly limited\nthe practical applications of UAV. To address these challenges, we propose a\nnovel object detection network Multi-scale Context Aggregation and\nScale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11.\nFirstly, to tackle the difficulty of detecting small objects in UAV images, we\ndesign a Multi-scale Feature Aggregation Module (MFAM), which significantly\nimproves the detection accuracy of small objects through parallel multi-scale\nconvolutions and feature fusion. Secondly, to mitigate the interference of\nbackground noise, we propose an Improved Efficient Multi-scale Attention Module\n(IEMA), which enhances the focus on target regions through feature grouping,\nparallel sub-networks, and cross-spatial learning. Thirdly, we introduce a\nDimension-Aware Selective Integration Module (DASI), which further enhances\nmulti-scale feature fusion capabilities by adaptively weighting and fusing\nlow-dimensional features and high-dimensional features. Finally, we conducted\nextensive performance evaluations of our proposed method on the VisDrone2019\ndataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in\nmAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set.\nRemarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only\napproximately 60% of its parameters and 65% of its computational cost.\nFurthermore, comparative experiments with state-of-the-art detectors confirm\nthat MASF-YOLO-s maintains a clear competitive advantage in both detection\naccuracy and model efficiency."}
{"id": "2504.17934", "pdf": "https://arxiv.org/pdf/2504.17934", "abs": "https://arxiv.org/abs/2504.17934", "authors": ["Chaoran Chen", "Zhiping Zhang", "Ibrahim Khalilov", "Bingcan Guo", "Simret A Gebreegziabher", "Yanfang Ye", "Ziang Xiao", "Yaxing Yao", "Tianshi Li", "Toby Jia-Jun Li"], "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents", "categories": ["cs.HC", "cs.CL", "cs.CR"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has revolutionized Graphical User\nInterface (GUI) automation through LLM-powered GUI agents, yet their ability to\nprocess sensitive data with limited human oversight raises significant privacy\nand security risks. This position paper identifies three key risks of GUI\nagents and examines how they differ from traditional GUI automation and general\nautonomous agents. Despite these risks, existing evaluations focus primarily on\nperformance, leaving privacy and security assessments largely unexplored. We\nreview current evaluation metrics for both GUI and general LLM agents and\noutline five key challenges in integrating human evaluators for GUI agent\nassessments. To address these gaps, we advocate for a human-centered evaluation\nframework that incorporates risk assessments, enhances user awareness through\nin-context consent, and embeds privacy and security considerations into GUI\nagent design and evaluation."}
{"id": "2504.18152", "pdf": "https://arxiv.org/pdf/2504.18152", "abs": "https://arxiv.org/abs/2504.18152", "authors": ["Yi-Xing Peng", "Qize Yang", "Yu-Ming Tang", "Shenghao Fu", "Kun-Yu Lin", "Xihan Wei", "Wei-Shi Zheng"], "title": "ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained understanding of human actions and poses in videos is essential\nfor human-centric AI applications. In this work, we introduce ActionArt, a\nfine-grained video-caption dataset designed to advance research in\nhuman-centric multimodal understanding. Our dataset comprises thousands of\nvideos capturing a broad spectrum of human actions, human-object interactions,\nand diverse scenarios, each accompanied by detailed annotations that\nmeticulously label every limb movement. We develop eight sub-tasks to evaluate\nthe fine-grained understanding capabilities of existing large multimodal models\nacross different dimensions. Experimental results indicate that, while current\nlarge multimodal models perform commendably on various tasks, they often fall\nshort in achieving fine-grained understanding. We attribute this limitation to\nthe scarcity of meticulously annotated data, which is both costly and difficult\nto scale manually. Since manual annotations are costly and hard to scale, we\npropose proxy tasks to enhance the model perception ability in both spatial and\ntemporal dimensions. These proxy tasks are carefully crafted to be driven by\ndata automatically generated from existing MLLMs, thereby reducing the reliance\non costly manual labels. Experimental results show that the proposed proxy\ntasks significantly narrow the gap toward the performance achieved with\nmanually annotated fine-grained data."}
{"id": "2504.17950", "pdf": "https://arxiv.org/pdf/2504.17950", "abs": "https://arxiv.org/abs/2504.17950", "authors": ["Isadora White", "Kolby Nottingham", "Ayush Maniar", "Max Robinson", "Hansen Lillemark", "Mehul Maheshwari", "Lianhui Qin", "Prithviraj Ammanabrolu"], "title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning", "categories": ["cs.MA", "cs.CL"], "comment": "9 pages of main paper with 6 main figures, overall 28 pages", "summary": "Collaboration is ubiquitous and essential in day-to-day life -- from\nexchanging ideas, to delegating tasks, to generating plans together. This work\nstudies how LLMs can adaptively collaborate to perform complex embodied\nreasoning tasks. To this end we introduce MINDcraft, an easily extensible\nplatform built to enable LLM agents to control characters in the open-world\ngame of Minecraft; and MineCollab, a benchmark to test the different dimensions\nof embodied and collaborative reasoning. An experimental study finds that the\nprimary bottleneck in collaborating effectively for current state-of-the-art\nagents is efficient natural language communication, with agent performance\ndropping as much as 15% when they are required to communicate detailed task\ncompletion plans. We conclude that existing LLM agents are ill-optimized for\nmulti-agent collaboration, especially in embodied scenarios, and highlight the\nneed to employ methods beyond in-context and imitation learning. Our website\ncan be found here: https://mindcraft-minecollab.github.io/"}
{"id": "2504.18158", "pdf": "https://arxiv.org/pdf/2504.18158", "abs": "https://arxiv.org/abs/2504.18158", "authors": ["Jiahao Zhang", "Bowen Wang", "Hong Liu", "Liangzhi Li", "Yuta Nakashima", "Hajime Nagahara"], "title": "E-InMeMo: Enhanced Prompting for Visual In-Context Learning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Large-scale models trained on extensive datasets have become the standard due\nto their strong generalizability across diverse tasks. In-context learning\n(ICL), widely used in natural language processing, leverages these models by\nproviding task-specific prompts without modifying their parameters. This\nparadigm is increasingly being adapted for computer vision, where models\nreceive an input-output image pair, known as an in-context pair, alongside a\nquery image to illustrate the desired output. However, the success of visual\nICL largely hinges on the quality of these prompts. To address this, we propose\nEnhanced Instruct Me More (E-InMeMo), a novel approach that incorporates\nlearnable perturbations into in-context pairs to optimize prompting. Through\nextensive experiments on standard vision tasks, E-InMeMo demonstrates superior\nperformance over existing state-of-the-art methods. Notably, it improves mIoU\nscores by 7.99 for foreground segmentation and by 17.04 for single object\ndetection when compared to the baseline without learnable prompts. These\nresults highlight E-InMeMo as a lightweight yet effective strategy for\nenhancing visual ICL. Code is publicly available at:\nhttps://github.com/Jackieam/E-InMeMo"}
{"id": "2504.18024", "pdf": "https://arxiv.org/pdf/2504.18024", "abs": "https://arxiv.org/abs/2504.18024", "authors": ["Yiwei Zha"], "title": "SMARTFinRAG: Interactive Modularized Financial RAG Benchmark", "categories": ["cs.CE", "cs.CL", "cs.IR"], "comment": "For open source github repo, see\n  https://github.com/JonathanZha47/SMARTFinRAG", "summary": "Financial sectors are rapidly adopting language model technologies, yet\nevaluating specialized RAG systems in this domain remains challenging. This\npaper introduces SMARTFinRAG, addressing three critical gaps in financial RAG\nassessment: (1) a fully modular architecture where components can be\ndynamically interchanged during runtime; (2) a document-centric evaluation\nparadigm generating domain-specific QA pairs from newly ingested financial\ndocuments; and (3) an intuitive interface bridging research-implementation\ndivides. Our evaluation quantifies both retrieval efficacy and response\nquality, revealing significant performance variations across configurations.\nThe platform's open-source architecture supports transparent, reproducible\nresearch while addressing practical deployment challenges faced by financial\ninstitutions implementing RAG systems."}
{"id": "2504.18165", "pdf": "https://arxiv.org/pdf/2504.18165", "abs": "https://arxiv.org/abs/2504.18165", "authors": ["Michel Gokan Khan", "Renan Guarese", "Fabian Johnson", "Xi Vincent Wang", "Anders Bergman", "Benjamin Edvinsson", "Mario Romero", "Jérémy Vachier", "Jan Kronqvist"], "title": "PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning\nframework that combines camera and sensory data with 3D Gaussian Splatting and\ncomputer vision models for digital twinning, object tracking, and Key\nPerformance Indicators (KPIs) extraction in industrial production lines. By\nutilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam\noffers a semi-automated approach to object tracking and spatial mapping,\nenabling digital twins that capture real-time KPIs such as availability,\nperformance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts\nin the production line. We validate the effectiveness of PerfCam through a\npractical deployment within realistic test production lines in the\npharmaceutical industry and contribute an openly published dataset to support\nfurther research and development in the field. The results demonstrate\nPerfCam's ability to deliver actionable insights through its precise digital\ntwin capabilities, underscoring its value as an effective tool for developing\nusable digital twins in smart manufacturing environments and extracting\noperational analytics."}
{"id": "2504.18099", "pdf": "https://arxiv.org/pdf/2504.18099", "abs": "https://arxiv.org/abs/2504.18099", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak", "Elizabeth Sherly"], "title": "Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "10 pages with 8 figures. This paper presented in an international\n  Conference", "summary": "Speech production is a complex sequential process which involve the\ncoordination of various articulatory features. Among them tongue being a highly\nversatile active articulator responsible for shaping airflow to produce\ntargeted speech sounds that are intellectual, clear, and distinct. This paper\npresents a novel approach for predicting tongue and lip articulatory features\ninvolved in a given speech acoustics using a stacked Bidirectional Long\nShort-Term Memory (BiLSTM) architecture, combined with a one-dimensional\nConvolutional Neural Network (CNN) for post-processing with fixed weights\ninitialization. The proposed network is trained with two datasets consisting of\nsimultaneously recorded speech and Electromagnetic Articulography (EMA)\ndatasets, each introducing variations in terms of geographical origin,\nlinguistic characteristics, phonetic diversity, and recording equipment. The\nperformance of the model is assessed in Speaker Dependent (SD), Speaker\nIndependent (SI), corpus dependent (CD) and cross corpus (CC) modes.\nExperimental results indicate that the proposed model with fixed weights\napproach outperformed the adaptive weights initialization with in relatively\nminimal number of training epochs. These findings contribute to the development\nof robust and efficient models for articulatory feature prediction, paving the\nway for advancements in speech production research and applications."}
{"id": "2504.18179", "pdf": "https://arxiv.org/pdf/2504.18179", "abs": "https://arxiv.org/abs/2504.18179", "authors": ["Lovro Sindicic", "Ivica Kopriva"], "title": "Label-independent hyperparameter-free self-supervised single-view deep subspace clustering", "categories": ["cs.CV", "cs.LG", "68", "I.5.3; I.4.6; I.4.10"], "comment": "35 pages; 1 figure; 10 Tables", "summary": "Deep subspace clustering (DSC) algorithms face several challenges that hinder\ntheir widespread adoption across variois application domains. First, clustering\nquality is typically assessed using only the encoder's output layer,\ndisregarding valuable information present in the intermediate layers. Second,\nmost DSC approaches treat representation learning and subspace clustering as\nindependent tasks, limiting their effectiveness. Third, they assume the\navailability of a held-out dataset for hyperparameter tuning, which is often\nimpractical in real-world scenarios. Fourth, learning termination is commonly\nbased on clustering error monitoring, requiring external labels. Finally, their\nperformance often depends on post-processing techniques that rely on labeled\ndata. To address this limitations, we introduce a novel single-view DSC\napproach that: (i) minimizes a layer-wise self expression loss using a joint\nrepresentation matrix; (ii) optimizes a subspace-structured norm to enhance\nclustering quality; (iii) employs a multi-stage sequential learning framework,\nconsisting of pre-training and fine-tuning, enabling the use of multiple\nregularization terms without hyperparameter tuning; (iv) incorporates a\nrelative error-based self-stopping mechanism to terminate training without\nlabels; and (v) retains a fixed number of leading coefficients in the learned\nrepresentation matrix based on prior knowledge. We evaluate the proposed method\non six datasets representing faces, digits, and objects. The results show that\nour method outperforms most linear SC algorithms with careffulyl tuned\nhyperparameters while maintaining competitive performance with the best\nperforming linear appoaches."}
{"id": "2504.18333", "pdf": "https://arxiv.org/pdf/2504.18333", "abs": "https://arxiv.org/abs/2504.18333", "authors": ["Narek Maloyan", "Dmitry Namiot"], "title": "Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "LLM as judge systems used to assess text quality code correctness and\nargument strength are vulnerable to prompt injection attacks. We introduce a\nframework that separates content author attacks from system prompt attacks and\nevaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3\nOpus on four tasks with various defenses using fifty prompts per condition.\nAttacks achieved up to seventy three point eight percent success smaller models\nproved more vulnerable and transferability ranged from fifty point five to\nsixty two point six percent. Our results contrast with Universal Prompt\nInjection and AdvPrompter We recommend multi model committees and comparative\nscoring and release all code and datasets"}
{"id": "2504.18190", "pdf": "https://arxiv.org/pdf/2504.18190", "abs": "https://arxiv.org/abs/2504.18190", "authors": ["Brunó B. Englert", "Tommie Kerssies", "Gijs Dubbelman"], "title": "What is the Added Value of UDA in the VFM Era?", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised Domain Adaptation (UDA) can improve a perception model's\ngeneralization to an unlabeled target domain starting from a labeled source\ndomain. UDA using Vision Foundation Models (VFMs) with synthetic source data\ncan achieve generalization performance comparable to fully-supervised learning\nwith real target data. However, because VFMs have strong generalization from\ntheir pre-training, more straightforward, source-only fine-tuning can also\nperform well on the target. As data scenarios used in academic research are not\nnecessarily representative for real-world applications, it is currently unclear\n(a) how UDA behaves with more representative and diverse data and (b) if\nsource-only fine-tuning of VFMs can perform equally well in these scenarios.\nOur research aims to close these gaps and, similar to previous studies, we\nfocus on semantic segmentation as a representative perception task. We assess\nUDA for synth-to-real and real-to-real use cases with different source and\ntarget data combinations. We also investigate the effect of using a small\namount of labeled target data in UDA. We clarify that while these scenarios are\nmore realistic, they are not necessarily more challenging. Our results show\nthat, when using stronger synthetic source data, UDA's improvement over\nsource-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using\nmore diverse real source data, UDA has no added value. However, UDA\ngeneralization is always higher in all synthetic data scenarios than\nsource-only fine-tuning and, when including only 1/16 of Cityscapes labels,\nsynthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU\nas a fully-supervised model using all labels. Considering the mixed results, we\ndiscuss how UDA can best support robust autonomous driving at scale."}
{"id": "2504.18425", "pdf": "https://arxiv.org/pdf/2504.18425", "abs": "https://arxiv.org/abs/2504.18425", "authors": ["KimiTeam", "Ding Ding", "Zeqian Ju", "Yichong Leng", "Songxiang Liu", "Tong Liu", "Zeyu Shang", "Kai Shen", "Wei Song", "Xu Tan", "Heyi Tang", "Zhengtao Wang", "Chu Wei", "Yifei Xin", "Xinran Xu", "Jianwei Yu", "Yutao Zhang", "Xinyu Zhou", "Y. Charles", "Jun Chen", "Yanru Chen", "Yulun Du", "Weiran He", "Zhenxing Hu", "Guokun Lai", "Qingcheng Li", "Yangyang Liu", "Weidong Sun", "Jianzhou Wang", "Yuzhi Wang", "Yuefeng Wu", "Yuxin Wu", "Dongchao Yang", "Hao Yang", "Ying Yang", "Zhilin Yang", "Aoxiong Yin", "Ruibin Yuan", "Yutong Zhang", "Zaida Zhou"], "title": "Kimi-Audio Technical Report", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "We present Kimi-Audio, an open-source audio foundation model that excels in\naudio understanding, generation, and conversation. We detail the practices in\nbuilding Kimi-Audio, including model architecture, data curation, training\nrecipe, inference deployment, and evaluation. Specifically, we leverage a\n12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous\nfeatures as input and discrete tokens as output, and develop a chunk-wise\nstreaming detokenizer based on flow matching. We curate a pre-training dataset\nthat consists of more than 13 million hours of audio data covering a wide range\nof modalities including speech, sound, and music, and build a pipeline to\nconstruct high-quality and diverse post-training data. Initialized from a\npre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text\ndata with several carefully designed tasks, and then fine-tuned to support a\ndiverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio\nachieves state-of-the-art performance on a range of audio benchmarks including\nspeech recognition, audio understanding, audio question answering, and speech\nconversation. We release the codes, model checkpoints, as well as the\nevaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio."}
{"id": "2504.18201", "pdf": "https://arxiv.org/pdf/2504.18201", "abs": "https://arxiv.org/abs/2504.18201", "authors": ["Yin Tang", "Jiankai Li", "Hongyu Yang", "Xuan Dong", "Lifeng Fan", "Weixin Li"], "title": "Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In an era where social media platforms abound, individuals frequently share\nimages that offer insights into their intents and interests, impacting\nindividual life quality and societal stability. Traditional computer vision\ntasks, such as object detection and semantic segmentation, focus on concrete\nvisual representations, while intent recognition relies more on implicit visual\nclues. This poses challenges due to the wide variation and subjectivity of such\nclues, compounded by the problem of intra-class variety in conveying abstract\nconcepts, e.g. \"enjoy life\". Existing methods seek to solve the problem by\nmanually designing representative features or building prototypes for each\nclass from global features. However, these methods still struggle to deal with\nthe large visual diversity of each intent category. In this paper, we introduce\na novel approach named Multi-grained Compositional visual Clue Learning (MCCL)\nto address these challenges for image intent recognition. Our method leverages\nthe systematic compositionality of human cognition by breaking down intent\nrecognition into visual clue composition and integrating multi-grained\nfeatures. We adopt class-specific prototypes to alleviate data imbalance. We\ntreat intent recognition as a multi-label classification problem, using a graph\nconvolutional network to infuse prior knowledge through label embedding\ncorrelations. Demonstrated by a state-of-the-art performance on the Intentonomy\nand MDID datasets, our approach advances the accuracy of existing methods while\nalso possessing good interpretability. Our work provides an attempt for future\nexplorations in understanding complex and miscellaneous forms of human\nexpression."}
{"id": "2504.18203", "pdf": "https://arxiv.org/pdf/2504.18203", "abs": "https://arxiv.org/abs/2504.18203", "authors": ["Raul David Dominguez Sanchez", "Xavier Diaz Ortiz", "Xingcheng Zhou", "Max Peter Ronecker", "Michael Karner", "Daniel Watzenig", "Alois Knoll"], "title": "LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for the Data-Driven Learning for Intelligent Vehicle\n  Applications Workshop at the 36th IEEE Intelligent Vehicles Symposium (IV)\n  2025", "summary": "Railway systems, particularly in Germany, require high levels of automation\nto address legacy infrastructure challenges and increase train traffic safely.\nA key component of automation is robust long-range perception, essential for\nearly hazard detection, such as obstacles at level crossings or pedestrians on\ntracks. Unlike automotive systems with braking distances of ~70 meters, trains\nrequire perception ranges exceeding 1 km. This paper presents an\ndeep-learning-based approach for long-range 3D object detection tailored for\nautonomous trains. The method relies solely on monocular images, inspired by\nthe Faraway-Frustum approach, and incorporates LiDAR data during training to\nimprove depth estimation. The proposed pipeline consists of four key modules:\n(1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation\nnetwork, and (3-4) dedicated short- and long-range 3D detection heads.\nEvaluations on the OSDaR23 dataset demonstrate the effectiveness of the\napproach in detecting objects up to 250 meters. Results highlight its potential\nfor railway automation and outline areas for future improvement."}
{"id": "2504.18204", "pdf": "https://arxiv.org/pdf/2504.18204", "abs": "https://arxiv.org/abs/2504.18204", "authors": ["Kun Li", "Jianhui Wang", "Yangfan He", "Xinyuan Song", "Ruoyu Wang", "Hongyang He", "Wenxin Zhang", "Jiaqi Chen", "Keqin Li", "Sida Li", "Miao Zhang", "Tianyu Shi", "Xueqian Wang"], "title": "Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17660", "summary": "Generative AI has significantly changed industries by enabling text-driven\nimage generation, yet challenges remain in achieving high-resolution outputs\nthat align with fine-grained user preferences. Consequently, multi-round\ninteractions are necessary to ensure the generated images meet expectations.\nPrevious methods enhanced prompts via reward feedback but did not optimize over\na multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation\n(VCA) framework incorporating human-in-the-loop feedback, leveraging a\nwell-trained reward model aligned with human preferences. Using a diverse\nmulti-turn dialogue dataset, our framework applies multiple reward functions,\nsuch as diversity, consistency, and preference feedback, while fine-tuning the\ndiffusion model through LoRA, thus optimizing image generation based on user\ninput. We also construct multi-round dialogue datasets of prompts and image\npairs aligned with user intent. Experiments demonstrate that our method\noutperforms state-of-the-art baselines, significantly improving image\nconsistency and alignment with user intent. Our approach consistently surpasses\ncompeting models in user satisfaction, especially in multi-turn dialogue\nscenarios."}
{"id": "2504.18213", "pdf": "https://arxiv.org/pdf/2504.18213", "abs": "https://arxiv.org/abs/2504.18213", "authors": ["Nicolas Münger", "Max Peter Ronecker", "Xavier Diaz", "Michael Karner", "Daniel Watzenig", "Jan Skaloud"], "title": "A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes", "categories": ["cs.CV"], "comment": "Accepted at the 28th Computer Vision Winter Workshop 2025", "summary": "LiDAR-based semantic segmentation is critical for autonomous trains,\nrequiring accurate predictions across varying distances. This paper introduces\ntwo targeted data augmentation methods designed to improve segmentation\nperformance on the railway-specific OSDaR23 dataset. The person instance\npasting method enhances segmentation of pedestrians at distant ranges by\ninjecting realistic variations into the dataset. The track sparsification\nmethod redistributes point density in LiDAR scans, improving track segmentation\nat far distances with minimal impact on close-range accuracy. Both methods are\nevaluated using a state-of-the-art 3D semantic segmentation network,\ndemonstrating significant improvements in distant-range performance while\nmaintaining robustness in close-range predictions. We establish the first 3D\nsemantic segmentation benchmark for OSDaR23, demonstrating the potential of\ndata-centric approaches to address railway-specific challenges in autonomous\ntrain perception."}
{"id": "2504.18215", "pdf": "https://arxiv.org/pdf/2504.18215", "abs": "https://arxiv.org/abs/2504.18215", "authors": ["Nanjie Yao", "Gangjian Zhang", "Wenhao Shen", "Jian Shu", "Hao Wang"], "title": "Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating", "categories": ["cs.CV"], "comment": null, "summary": "Monocular 3D clothed human reconstruction aims to create a complete 3D avatar\nfrom a single image. To tackle the human geometry lacking in one RGB image,\ncurrent methods typically resort to a preceding model for an explicit geometric\nrepresentation. For the reconstruction itself, focus is on modeling both it and\nthe input image. This routine is constrained by the preceding model, and\noverlooks the integrity of the reconstruction task. To address this, this paper\nintroduces a novel paradigm that treats human reconstruction as a holistic\nprocess, utilizing an end-to-end network for direct prediction from 2D image to\n3D avatar, eliminating any explicit intermediate geometry display. Based on\nthis, we further propose a novel reconstruction framework consisting of two\ncore components: the Anatomy Shaping Extraction module, which captures implicit\nshape features taking into account the specialty of human anatomy, and the\nTwins Negotiating Reconstruction U-Net, which enhances reconstruction through\nfeature interaction between two U-Nets of different modalities. Moreover, we\npropose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to\nbolster model performance in more complex case input. Extensive experiments on\ntwo test sets and many in-the-wild cases show the superiority of our method\nover SOTA methods. Our demos can be found in :\nhttps://e2e3dgsrecon.github.io/e2e3dgsrecon/."}
{"id": "2504.18233", "pdf": "https://arxiv.org/pdf/2504.18233", "abs": "https://arxiv.org/abs/2504.18233", "authors": ["Wenxiang Gua", "Lin Qia"], "title": "Dense Geometry Supervision for Underwater Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "The field of monocular depth estimation is continually evolving with the\nadvent of numerous innovative models and extensions. However, research on\nmonocular depth estimation methods specifically for underwater scenes remains\nlimited, compounded by a scarcity of relevant data and methodological support.\nThis paper proposes a novel approach to address the existing challenges in\ncurrent monocular depth estimation methods for underwater environments. We\nconstruct an economically efficient dataset suitable for underwater scenarios\nby employing multi-view depth estimation to generate supervisory signals and\ncorresponding enhanced underwater images. we introduces a texture-depth fusion\nmodule, designed according to the underwater optical imaging principles, which\naims to effectively exploit and integrate depth information from texture cues.\nExperimental results on the FLSea dataset demonstrate that our approach\nsignificantly improves the accuracy and adaptability of models in underwater\nsettings. This work offers a cost-effective solution for monocular underwater\ndepth estimation and holds considerable promise for practical applications."}
{"id": "2504.18235", "pdf": "https://arxiv.org/pdf/2504.18235", "abs": "https://arxiv.org/abs/2504.18235", "authors": ["Andreas Ziegler", "David Joseph", "Thomas Gossard", "Emil Moldovan", "Andreas Zell"], "title": "BiasBench: A reproducible benchmark for tuning the biases of event cameras", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to CVPR 2025 Workshop on Event-based Vision", "summary": "Event-based cameras are bio-inspired sensors that detect light changes\nasynchronously for each pixel. They are increasingly used in fields like\ncomputer vision and robotics because of several advantages over traditional\nframe-based cameras, such as high temporal resolution, low latency, and high\ndynamic range. As with any camera, the output's quality depends on how well the\ncamera's settings, called biases for event-based cameras, are configured. While\nframe-based cameras have advanced automatic configuration algorithms, there are\nvery few such tools for tuning these biases. A systematic testing framework\nwould require observing the same scene with different biases, which is tricky\nsince event cameras only generate events when there is movement. Event\nsimulators exist, but since biases heavily depend on the electrical circuit and\nthe pixel design, available simulators are not well suited for bias tuning. To\nallow reproducibility, we present BiasBench, a novel event dataset containing\nmultiple scenes with settings sampled in a grid-like pattern. We present three\ndifferent scenes, each with a quality metric of the downstream application.\nAdditionally, we present a novel, RL-based method to facilitate online bias\nadjustments."}
{"id": "2504.18249", "pdf": "https://arxiv.org/pdf/2504.18249", "abs": "https://arxiv.org/abs/2504.18249", "authors": ["Qinyu Chen", "Chang Gao", "Min Liu", "Daniele Perrone", "Yan Ru Pei", "Zuowen Wang", "Zhuo Zou", "Shihang Tan", "Tao Han", "Guorui Lu", "Zhen Xu", "Junyuan Ding", "Ziteng Wang", "Zongwei Wu", "Han Han", "Yuliang Wu", "Jinze Chen", "Wei Zhai", "Yang Cao", "Zheng-jun Zha", "Nuwan Bandara", "Thivya Kandappu", "Archan Misra", "Xiaopeng Lin", "Hongxiang Huang", "Hongwei Ren", "Bojun Cheng", "Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This survey serves as a review for the 2025 Event-Based Eye Tracking\nChallenge organized as part of the 2025 CVPR event-based vision workshop. This\nchallenge focuses on the task of predicting the pupil center by processing\nevent camera recorded eye movement. We review and summarize the innovative\nmethods from teams rank the top in the challenge to advance future event-based\neye tracking research. In each method, accuracy, model size, and number of\noperations are reported. In this survey, we also discuss event-based eye\ntracking from the perspective of hardware design."}
{"id": "2504.18256", "pdf": "https://arxiv.org/pdf/2504.18256", "abs": "https://arxiv.org/abs/2504.18256", "authors": ["Elena Plekhanova", "Damien Robert", "Johannes Dollinger", "Emilia Arens", "Philipp Brun", "Jan Dirk Wegner", "Niklaus Zimmermann"], "title": "SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology", "categories": ["cs.CV"], "comment": "CVPR 2025, EarthVision workshop", "summary": "With the exacerbation of the biodiversity and climate crises, macroecological\npursuits such as global biodiversity mapping become more urgent. Remote sensing\noffers a wealth of Earth observation data for ecological studies, but the\nscarcity of labeled datasets remains a major challenge. Recently,\nself-supervised learning has enabled learning representations from unlabeled\ndata, triggering the development of pretrained geospatial models with\ngeneralizable features. However, these models are often trained on datasets\nbiased toward areas of high human activity, leaving entire ecological regions\nunderrepresented. Additionally, while some datasets attempt to address\nseasonality through multi-date imagery, they typically follow calendar seasons\nrather than local phenological cycles. To better capture vegetation seasonality\nat a global scale, we propose a simple phenology-informed sampling strategy and\nintroduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we\ntrain an existing model with a season-contrastive objective. We compare\nrepresentations learned from SSL4Eco against other datasets on diverse\necological downstream tasks and demonstrate that our straightforward sampling\nmethod consistently improves representation quality, highlighting the\nimportance of dataset construction. The model pretrained on SSL4Eco reaches\nstate of the art performance on 7 out of 8 downstream tasks spanning\n(multi-label) classification and regression. We release our code, data, and\nmodel weights to support macroecological and computer vision research at\nhttps://github.com/PlekhanovaElena/ssl4eco."}
{"id": "2504.18283", "pdf": "https://arxiv.org/pdf/2504.18283", "abs": "https://arxiv.org/abs/2504.18283", "authors": ["Minjae Kang", "Martim Brandão"], "title": "Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "comment": "Originally submitted to CVPR 2025 on 2024-11-15 with paper ID 15808", "summary": "Recent audio-visual generative models have made substantial progress in\ngenerating images from audio. However, existing approaches focus on generating\nimages from single-class audio and fail to generate images from mixed audio. To\naddress this, we propose an Audio-Visual Generation and Separation model\n(AV-GAS) for generating images from soundscapes (mixed audio containing\nmultiple classes). Our contribution is threefold: First, we propose a new\nchallenge in the audio-visual generation task, which is to generate an image\ngiven a multi-class audio input, and we propose a method that solves this task\nusing an audio-visual separator. Second, we introduce a new audio-visual\nseparation task, which involves generating separate images for each class\npresent in a mixed audio input. Lastly, we propose new evaluation metrics for\nthe audio-visual generation task: Class Representation Score (CRS) and a\nmodified R@K. Our model is trained and evaluated on the VGGSound dataset. We\nshow that our method outperforms the state-of-the-art, achieving 7% higher CRS\nand 4% higher R@2* in generating plausible images with mixed audio."}
{"id": "2504.18286", "pdf": "https://arxiv.org/pdf/2504.18286", "abs": "https://arxiv.org/abs/2504.18286", "authors": ["Christian Pionzewski", "Rebecca Rademacher", "Jérôme Rutinowski", "Antonia Ponikarov", "Stephan Matzke", "Tim Chilla", "Pia Schreynemackers", "Alice Kirchheim"], "title": "Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.10; I.4.9"], "comment": "Published in: 2024 International Conference on Machine Learning and\n  Applications (ICMLA), IEEE. 6 pages, 3 figures", "summary": "This contribution explores the impact of synthetic training data usage and\nthe prediction of material wear and aging in the context of re-identification.\nDifferent experimental setups and gallery set expanding strategies are tested,\nanalyzing their impact on performance over time for aging re-identification\nsubjects. Using a continuously updating gallery, we were able to increase our\nmean Rank-1 accuracy by 24%, as material aging was taken into account step by\nstep. In addition, using models trained with 10% artificial training data,\nRank-1 accuracy could be increased by up to 13%, in comparison to a model\ntrained on only real-world data, significantly boosting generalized performance\non hold-out data. Finally, this work introduces a novel, open-source\nre-identification dataset, pallet-block-2696. This dataset contains 2,696\nimages of Euro pallets, taken over a period of 4 months. During this time,\nnatural aging processes occurred and some of the pallets were damaged during\ntheir usage. These wear and tear processes significantly changed the appearance\nof the pallets, providing a dataset that can be used to generate synthetically\naged pallets or other wooden materials."}
{"id": "2504.18317", "pdf": "https://arxiv.org/pdf/2504.18317", "abs": "https://arxiv.org/abs/2504.18317", "authors": ["Zhengru Fang", "Zhenghao Liu", "Jingjing Wang", "Senkang Hu", "Yu Guo", "Yiqin Deng", "Yuguang Fang"], "title": "Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy", "categories": ["cs.CV", "cs.NI"], "comment": "Code and dataset will be made publicly available:\n  https://github.com/fangzr/TOC-Edge-Aerial", "summary": "To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles\n(UAVs) localization in urban areas where global positioning system (GPS)\nsignals are unavailable. Vision-based methods offer a viable alternative but\nface severe bandwidth, memory and processing constraints on lightweight UAVs.\nInspired by mammalian spatial cognition, we propose a task-oriented\ncommunication framework, where UAVs equipped with multi-camera systems extract\ncompact multi-view features and offload localization tasks to edge servers. We\nintroduce the Orthogonally-constrained Variational Information Bottleneck\nencoder (O-VIB), which incorporates automatic relevance determination (ARD) to\nprune non-informative features while enforcing orthogonality to minimize\nredundancy. This enables efficient and accurate localization with minimal\ntransmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows\nthat O-VIB achieves high-precision localization under stringent bandwidth\nbudgets. Code and dataset will be made publicly available:\ngithub.com/fangzr/TOC-Edge-Aerial."}
{"id": "2504.18318", "pdf": "https://arxiv.org/pdf/2504.18318", "abs": "https://arxiv.org/abs/2504.18318", "authors": ["Yunze Deng", "Haijun Xiong", "Bin Feng", "Xinggang Wang", "Wenyu Liu"], "title": "STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-4D generation is rapidly developing and widely applied in various\nscenarios. However, existing methods often fail to incorporate adequate\nspatio-temporal modeling and prompt alignment within a unified framework,\nresulting in temporal inconsistencies, geometric distortions, or low-quality 4D\ncontent that deviates from the provided texts. Therefore, we propose STP4D, a\nnovel approach that aims to integrate comprehensive spatio-temporal-prompt\nconsistency modeling for high-quality text-to-4D generation. Specifically,\nSTP4D employs three carefully designed modules: Time-varying Prompt Embedding,\nGeometric Information Enhancement, and Temporal Extension Deformation, which\ncollaborate to accomplish this goal. Furthermore, STP4D is among the first\nmethods to exploit the Diffusion model to generate 4D Gaussians, combining the\nfine-grained modeling capabilities and the real-time rendering process of 4DGS\nwith the rapid inference speed of the Diffusion model. Extensive experiments\ndemonstrate that STP4D excels in generating high-fidelity 4D content with\nexceptional efficiency (approximately 4.6s per asset), surpassing existing\nmethods in both quality and speed."}
{"id": "2504.18325", "pdf": "https://arxiv.org/pdf/2504.18325", "abs": "https://arxiv.org/abs/2504.18325", "authors": ["Dongxin Lyu", "Han Huang", "Cheng Tan", "Zimu Li"], "title": "Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation", "categories": ["cs.CV"], "comment": "Submitting to ICCV2025", "summary": "Monocular 3D lane detection is challenging due to the difficulty in capturing\ndepth information from single-camera images. A common strategy involves\ntransforming front-view (FV) images into bird's-eye-view (BEV) space through\ninverse perspective mapping (IPM), facilitating lane detection using BEV\nfeatures. However, IPM's flat-ground assumption and loss of contextual\ninformation lead to inaccuracies in reconstructing 3D information, especially\nheight. In this paper, we introduce a BEV-based framework to address these\nlimitations and improve 3D lane detection accuracy. Our approach incorporates a\nHierarchical Depth-Aware Head that provides multi-scale depth features,\nmitigating the flat-ground assumption by enhancing spatial awareness across\nvarying depths. Additionally, we leverage Depth Prior Distillation to transfer\nsemantic depth knowledge from a teacher model, capturing richer structural and\ncontextual information for complex lane structures. To further refine lane\ncontinuity and ensure smooth lane reconstruction, we introduce a Conditional\nRandom Field module that enforces spatial coherence in lane predictions.\nExtensive experiments validate that our method achieves state-of-the-art\nperformance in terms of z-axis error and outperforms other methods in the field\nin overall performance. The code is released at:\nhttps://anonymous.4open.science/r/Depth3DLane-DCDD."}
{"id": "2504.18332", "pdf": "https://arxiv.org/pdf/2504.18332", "abs": "https://arxiv.org/abs/2504.18332", "authors": ["Shuting Zhao", "Linxin Bai", "Liangjing Shao", "Ye Zhang", "Xinrong Chen"], "title": "SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations", "categories": ["cs.CV", "cs.HC", "68U05"], "comment": "9 pages, 6 figures, conference ICMR 2025", "summary": "The growing applications of AR/VR increase the demand for real-time full-body\npose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint\nsignals from the head and hands, reconstructing a full-body pose remains\nchallenging due to the unconstrained lower body. Recent advancements often rely\non conventional neural networks and generative models to improve performance in\nthis task, such as Transformers and diffusion models. However, these approaches\nstruggle to strike a balance between achieving precise pose reconstruction and\nmaintaining fast inference speed. To overcome these challenges, a lightweight\nand efficient model, SSD-Poser, is designed for robust full-body motion\nestimation from sparse observations. SSD-Poser incorporates a well-designed\nhybrid encoder, State Space Attention Encoders, to adapt the state space\nduality to complex motion poses and enable real-time realistic pose\nreconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate\njitter caused by variable-frequency motion signals, remarkably enhancing the\nmotion smoothness. Comprehensive experiments on the AMASS dataset demonstrate\nthat SSD-Poser achieves exceptional accuracy and computational efficiency,\nshowing outstanding inference efficiency compared to state-of-the-art methods."}
{"id": "2504.18348", "pdf": "https://arxiv.org/pdf/2504.18348", "abs": "https://arxiv.org/abs/2504.18348", "authors": ["Fengchun Liu. Tong Zhang", "Chunying Zhang"], "title": "TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "For deep learning-based image steganography frameworks, in order to ensure\nthe invisibility and recoverability of the information embedding, the loss\nfunction usually contains several losses such as embedding loss, recovery loss\nand steganalysis loss. In previous research works, fixed loss weights are\nusually chosen for training optimization, and this setting is not linked to the\nimportance of the steganography task itself and the training process. In this\npaper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for\nbalancing multinomial losses in deep learning image steganography algorithms.\nTSCL consists of two phases: a priori curriculum control and loss dynamics\ncontrol. The first phase firstly focuses the model on learning the information\nembedding of the original image by controlling the loss weights in the\nmulti-party adversarial training; secondly, it makes the model shift its\nlearning focus to improving the decoding accuracy; and finally, it makes the\nmodel learn to generate a steganographic image that is resistant to\nsteganalysis. In the second stage, the learning speed of each training task is\nevaluated by calculating the loss drop of the before and after iteration rounds\nto balance the learning of each task. Experimental results on three large\npublic datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL\nstrategy improves the quality of steganography, decoding accuracy and security."}
{"id": "2504.18349", "pdf": "https://arxiv.org/pdf/2504.18349", "abs": "https://arxiv.org/abs/2504.18349", "authors": ["Hongyu Zhu", "Sichu Liang", "Wenwen Wang", "Boheng Li", "Tongxin Yuan", "Fangqi Li", "ShiLin Wang", "Zhuosheng Zhang"], "title": "Revisiting Data Auditing in Large Vision-Language Models", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "With the surge of large language models (LLMs), Large Vision-Language Models\n(VLMs)--which integrate vision encoders with LLMs for accurate visual\ngrounding--have shown great potential in tasks like generalist agents and\nrobotic control. However, VLMs are typically trained on massive web-scraped\nimages, raising concerns over copyright infringement and privacy violations,\nand making data auditing increasingly urgent. Membership inference (MI), which\ndetermines whether a sample was used in training, has emerged as a key auditing\ntechnique, with promising results on open-source VLMs like LLaVA (AUC > 80%).\nIn this work, we revisit these advances and uncover a critical issue: current\nMI benchmarks suffer from distribution shifts between member and non-member\nimages, introducing shortcut cues that inflate MI performance. We further\nanalyze the nature of these shifts and propose a principled metric based on\noptimal transport to quantify the distribution discrepancy. To evaluate MI in\nrealistic settings, we construct new benchmarks with i.i.d. member and\nnon-member images. Existing MI methods fail under these unbiased conditions,\nperforming only marginally better than chance. Further, we explore the\ntheoretical upper bound of MI by probing the Bayes Optimality within the VLM's\nembedding space and find the irreducible error rate remains high. Despite this\npessimistic outlook, we analyze why MI for VLMs is particularly challenging and\nidentify three practical scenarios--fine-tuning, access to ground-truth texts,\nand set-based inference--where auditing becomes feasible. Our study presents a\nsystematic view of the limits and opportunities of MI for VLMs, providing\nguidance for future efforts in trustworthy data auditing."}
{"id": "2504.18355", "pdf": "https://arxiv.org/pdf/2504.18355", "abs": "https://arxiv.org/abs/2504.18355", "authors": ["Maximilian Xiling Li", "Korbinian Rudolf", "Nils Blank", "Rudolf Lioutikov"], "title": "Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Robotic agents need to understand how to interact with objects in their\nenvironment, both autonomously and during human-robot interactions. Affordance\ndetection on 3D point clouds, which identifies object regions that allow\nspecific interactions, has traditionally relied on deep learning models like\nPointNet++, DGCNN, or PointTransformerV3. However, these models operate as\nblack boxes, offering no insight into their decision-making processes.\nPrototypical Learning methods, such as ProtoPNet, provide an interpretable\nalternative to black-box models by employing a \"this looks like that\"\ncase-based reasoning approach. However, they have been primarily applied to\nimage-based tasks. In this work, we apply prototypical learning to models for\naffordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet\nbenchmark dataset show that prototypical models achieve competitive performance\nwith state-of-the-art black-box models and offer inherent interpretability.\nThis makes prototypical models a promising candidate for human-robot\ninteraction scenarios that require increased trust and safety."}
{"id": "2504.18361", "pdf": "https://arxiv.org/pdf/2504.18361", "abs": "https://arxiv.org/abs/2504.18361", "authors": ["Haozhen Yan", "Yan Hong", "Jiahui Zhan", "Yikun Ji", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Recent advancements in image manipulation have achieved unprecedented\nprogress in generating photorealistic content, but also simultaneously\neliminating barriers to arbitrary manipulation and editing, raising concerns\nabout multimedia authenticity and cybersecurity. However, existing Image\nManipulation Detection and Localization (IMDL) methodologies predominantly\nfocus on splicing or copy-move forgeries, lacking dedicated benchmarks for\ninpainting-based manipulations. To bridge this gap, we present COCOInpaint, a\ncomprehensive benchmark specifically designed for inpainting detection, with\nthree key contributions: 1) High-quality inpainting samples generated by six\nstate-of-the-art inpainting models, 2) Diverse generation scenarios enabled by\nfour mask generation strategies with optional text guidance, and 3) Large-scale\ncoverage with 258,266 inpainted images with rich semantic diversity. Our\nbenchmark is constructed to emphasize intrinsic inconsistencies between\ninpainted and authentic regions, rather than superficial semantic artifacts\nsuch as object shapes. We establish a rigorous evaluation protocol using three\nstandard metrics to assess existing IMDL approaches. The dataset will be made\npublicly available to facilitate future research in this area."}
{"id": "2504.18391", "pdf": "https://arxiv.org/pdf/2504.18391", "abs": "https://arxiv.org/abs/2504.18391", "authors": ["Tiankai Hang", "Jianmin Bao", "Fangyun Wei", "Dong Chen"], "title": "Fast Autoregressive Models for Continuous Latent Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autoregressive models have demonstrated remarkable success in sequential data\ngeneration, particularly in NLP, but their extension to continuous-domain image\ngeneration presents significant challenges. Recent work, the masked\nautoregressive model (MAR), bypasses quantization by modeling per-token\ndistributions in continuous spaces using a diffusion head but suffers from slow\ninference due to the high computational cost of the iterative denoising\nprocess. To address this, we propose the Fast AutoRegressive model (FAR), a\nnovel framework that replaces MAR's diffusion head with a lightweight shortcut\nhead, enabling efficient few-step sampling while preserving autoregressive\nprinciples. Additionally, FAR seamlessly integrates with causal Transformers,\nextending them from discrete to continuous token generation without requiring\narchitectural modifications. Experiments demonstrate that FAR achieves\n$2.3\\times$ faster inference than MAR while maintaining competitive FID and IS\nscores. This work establishes the first efficient autoregressive paradigm for\nhigh-fidelity continuous-space image generation, bridging the critical gap\nbetween quality and scalability in visual autoregressive modeling."}
{"id": "2504.18397", "pdf": "https://arxiv.org/pdf/2504.18397", "abs": "https://arxiv.org/abs/2504.18397", "authors": ["Kesen Zhao", "Beier Zhu", "Qianru Sun", "Hanwang Zhang"], "title": "Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning greatly improves the interpretability and\nproblem-solving abilities of multimodal large language models (MLLMs). However,\nexisting approaches are focused on text CoT, limiting their ability to leverage\nvisual cues. Visual CoT remains underexplored, and the only work is based on\nsupervised fine-tuning (SFT) that relies on extensive labeled bounding-box data\nand is hard to generalize to unseen cases. In this paper, we introduce\nUnsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT\nreasoning via preference optimization. UV-CoT performs preference comparisons\nbetween model-generated bounding boxes (one is preferred and the other is\ndis-preferred), eliminating the need for bounding-box annotations. We get such\npreference data by introducing an automatic data generation pipeline. Given an\nimage, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using\na template prompt and then answers the question using each bounded region as\ninput. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these\nrankings serve as supervision to train the target MLLM with UV-CoT by\nminimizing negative log-likelihood losses. By emulating human\nperception--identifying key regions and reasoning based on them--UV-CoT can\nimprove visual comprehension, particularly in spatial reasoning tasks where\ntextual descriptions alone fall short. Our experiments on six datasets\ndemonstrate the superiority of UV-CoT, compared to the state-of-the-art textual\nand visual CoT methods. Our zero-shot testing on four unseen datasets shows the\nstrong generalization of UV-CoT. The code is available in\nhttps://github.com/kesenzhao/UV-CoT."}
{"id": "2504.18419", "pdf": "https://arxiv.org/pdf/2504.18419", "abs": "https://arxiv.org/abs/2504.18419", "authors": ["Carlo Sgaravatti", "Roberto Basla", "Riccardo Pieroni", "Matteo Corno", "Sergio M. Savaresi", "Luca Magri", "Giacomo Boracchi"], "title": "A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a new way to detect 3D objects from multimodal inputs, leveraging\nboth LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an\nRGB detection network and a 3D LiDAR detector. We exploit late fusion\nprinciples to reduce LiDAR False Positives, matching LiDAR detections with RGB\nones by projecting the LiDAR bounding boxes on the image. We rely on cascade\nfusion principles to recover LiDAR False Negatives leveraging epipolar\nconstraints and frustums generated by RGB detections of separate views. Our\nsolution can be plugged on top of any underlying single-modal detectors,\nenabling a flexible training process that can take advantage of pre-trained\nLiDAR and RGB detectors, or train the two branches separately. We evaluate our\nresults on the KITTI object detection benchmark, showing significant\nperformance improvements, especially for the detection of Pedestrians and\nCyclists."}
{"id": "2504.18424", "pdf": "https://arxiv.org/pdf/2504.18424", "abs": "https://arxiv.org/abs/2504.18424", "authors": ["Rui Li", "Biao Zhang", "Zhenyu Li", "Federico Tombari", "Peter Wonka"], "title": "LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning", "categories": ["cs.CV"], "comment": "Project page: https://ruili3.github.io/lari", "summary": "We present layered ray intersections (LaRI), a new method for unseen geometry\nreasoning from a single image. Unlike conventional depth estimation that is\nlimited to the visible surface, LaRI models multiple surfaces intersected by\nthe camera rays using layered point maps. Benefiting from the compact and\nlayered representation, LaRI enables complete, efficient, and view-aligned\ngeometric reasoning to unify object- and scene-level tasks. We further propose\nto predict the ray stopping index, which identifies valid intersecting pixels\nand layers from LaRI's output. We build a complete training data generation\npipeline for synthetic and real-world data, including 3D objects and scenes,\nwith necessary data cleaning steps and coordination between rendering engines.\nAs a generic method, LaRI's performance is validated in two scenarios: It\nyields comparable object-level results to the recent large generative model\nusing 4% of its training data and 17% of its parameters. Meanwhile, it achieves\nscene-level occluded geometry reasoning in only one feed-forward."}
{"id": "2504.18447", "pdf": "https://arxiv.org/pdf/2504.18447", "abs": "https://arxiv.org/abs/2504.18447", "authors": ["Ryo Yamaki", "Shintaro Shiba", "Guillermo Gallego", "Yoshimitsu Aoki"], "title": "Iterative Event-based Motion Segmentation by Variational Contrast Maximization", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "11 pages, 9 figures, 3 tables, CVPR Workshop 2025", "summary": "Event cameras provide rich signals that are suitable for motion estimation\nsince they respond to changes in the scene. As any visual changes in the scene\nproduce event data, it is paramount to classify the data into different motions\n(i.e., motion segmentation), which is useful for various tasks such as object\ndetection and visual servoing. We propose an iterative motion segmentation\nmethod, by classifying events into background (e.g., dominant motion\nhypothesis) and foreground (independent motion residuals), thus extending the\nContrast Maximization framework. Experimental results demonstrate that the\nproposed method successfully classifies event clusters both for public and\nself-recorded datasets, producing sharp, motion-compensated edge-like images.\nThe proposed method achieves state-of-the-art accuracy on moving object\ndetection benchmarks with an improvement of over 30%, and demonstrates its\npossibility of applying to more complex and noisy real-world scenes. We hope\nthis work broadens the sensitivity of Contrast Maximization with respect to\nboth motion parameters and input events, thus contributing to theoretical\nadvancements in event-based motion segmentation estimation.\nhttps://github.com/aoki-media-lab/event_based_segmentation_vcmax"}
{"id": "2504.18448", "pdf": "https://arxiv.org/pdf/2504.18448", "abs": "https://arxiv.org/abs/2504.18448", "authors": ["Haotian Dong", "Xin Wang", "Di Lin", "Yipeng Wu", "Qin Chen", "Ruonan Liu", "Kairui Yang", "Ping Li", "Qing Guo"], "title": "NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "High-quality video generation is crucial for many fields, including the film\nindustry and autonomous driving. However, generating videos with spatiotemporal\nconsistencies remains challenging. Current methods typically utilize attention\nmechanisms or modify noise to achieve consistent videos, neglecting global\nspatiotemporal information that could help ensure spatial and temporal\nconsistency during video generation. In this paper, we propose the\nNoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame\nNoise Collaboration, and Joint Denoising, to enhance spatiotemporal\nconsistencies in video generation. In multi-level noise decomposition, we first\ndecompose initial noises into scene-level foreground/background noises,\ncapturing distinct motion properties to model multi-view foreground/background\nvariations. Furthermore, each scene-level noise is further decomposed into\nindividual-level shared and residual components. The shared noise preserves\nconsistency, while the residual component maintains diversity. In multi-frame\nnoise collaboration, we introduce an inter-view spatiotemporal collaboration\nmatrix and an intra-view impact collaboration matrix , which captures mutual\ncross-view effects and historical cross-frame impacts to enhance video quality.\nThe joint denoising contains two parallel denoising U-Nets to remove each\nscene-level noise, mutually enhancing video generation. We evaluate our\nNoiseController on public datasets focusing on video generation and downstream\ntasks, demonstrating its state-of-the-art performance."}
{"id": "2504.18468", "pdf": "https://arxiv.org/pdf/2504.18468", "abs": "https://arxiv.org/abs/2504.18468", "authors": ["Georgios Kouros", "Minye Wu", "Tinne Tuytelaars"], "title": "RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects", "categories": ["cs.CV"], "comment": null, "summary": "We introduce RGS-DR, a novel inverse rendering method for reconstructing and\nrendering glossy and reflective objects with support for flexible relighting\nand scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian\nSplatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D\nGaussian surfel representation to accurately estimate geometry and surface\nnormals, an essential property for high-quality inverse rendering. Our approach\nexplicitly models geometric and material properties through learnable\nprimitives rasterized into a deferred shading pipeline, effectively reducing\nrendering artifacts and preserving sharp reflections. By employing a\nmulti-level cube mipmap, RGS-DR accurately approximates environment lighting\nintegrals, facilitating high-quality reconstruction and relighting. A residual\npass with spherical-mipmap-based directional encoding further refines the\nappearance modeling. Experiments demonstrate that RGS-DR achieves high-quality\nreconstruction and rendering quality for shiny objects, often outperforming\nreconstruction-exclusive state-of-the-art methods incapable of relighting."}
{"id": "2504.18490", "pdf": "https://arxiv.org/pdf/2504.18490", "abs": "https://arxiv.org/abs/2504.18490", "authors": ["Andrews Danyo", "Anthony Dontoh", "Armstrong Aboah"], "title": "An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images", "categories": ["cs.CV"], "comment": null, "summary": "Accurately predicting the Pavement Condition Index (PCI), a measure of\nroadway conditions, from pavement images is crucial for infrastructure\nmaintenance. This study proposes an enhanced version of the Residual Network\n(ResNet50) architecture, integrated with a Convolutional Block Attention Module\n(CBAM), to predict PCI directly from pavement images without additional\nannotations. By incorporating CBAM, the model autonomously prioritizes critical\nfeatures within the images, improving prediction accuracy. Compared to the\noriginal baseline ResNet50 and DenseNet161 architectures, the enhanced\nResNet50-CBAM model achieved a significantly lower mean absolute percentage\nerror (MAPE) of 58.16%, compared to the baseline models that achieved 70.76%\nand 65.48% respectively. These results highlight the potential of using\nattention mechanisms to refine feature extraction, ultimately enabling more\naccurate and efficient assessments of pavement conditions. This study\nemphasizes the importance of targeted feature refinement in advancing automated\npavement analysis through attention mechanisms."}
{"id": "2504.18509", "pdf": "https://arxiv.org/pdf/2504.18509", "abs": "https://arxiv.org/abs/2504.18509", "authors": ["Shivam Duggal", "Yushi Hu", "Oscar Michel", "Aniruddha Kembhavi", "William T. Freeman", "Noah A. Smith", "Ranjay Krishna", "Antonio Torralba", "Ali Farhadi", "Wei-Chiu Ma"], "title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation", "categories": ["cs.CV"], "comment": "CVPR 2025. Project page and codes: https://eval3d.github.io/", "summary": "Despite the unprecedented progress in the field of 3D generation, current\nsystems still often fail to produce high-quality 3D assets that are visually\nappealing and geometrically and semantically consistent across multiple\nviewpoints. To effectively assess the quality of the generated 3D data, there\nis a need for a reliable 3D evaluation tool. Unfortunately, existing 3D\nevaluation metrics often overlook the geometric quality of generated assets or\nmerely rely on black-box multimodal large language models for coarse\nassessment. In this paper, we introduce Eval3D, a fine-grained, interpretable\nevaluation tool that can faithfully evaluate the quality of generated 3D assets\nbased on various distinct yet complementary criteria. Our key observation is\nthat many desired properties of 3D generation, such as semantic and geometric\nconsistency, can be effectively captured by measuring the consistency among\nvarious foundation models and tools. We thus leverage a diverse set of models\nand tools as probes to evaluate the inconsistency of generated 3D assets across\ndifferent aspects. Compared to prior work, Eval3D provides pixel-wise\nmeasurement, enables accurate 3D spatial feedback, and aligns more closely with\nhuman judgments. We comprehensively evaluate existing 3D generation models\nusing Eval3D and highlight the limitations and challenges of current models."}
{"id": "2504.18510", "pdf": "https://arxiv.org/pdf/2504.18510", "abs": "https://arxiv.org/abs/2504.18510", "authors": ["Patrick Müller", "Alexander Braun", "Margret Keuper"], "title": "Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models", "categories": ["cs.CV"], "comment": "v1.0", "summary": "Deep neural networks (DNNs) have proven to be successful in various computer\nvision applications such that models even infer in safety-critical situations.\nTherefore, vision models have to behave in a robust way to disturbances such as\nnoise or blur. While seminal benchmarks exist to evaluate model robustness to\ndiverse corruptions, blur is often approximated in an overly simplistic way to\nmodel defocus, while ignoring the different blur kernel shapes that result from\noptical systems. To study model robustness against realistic optical blur\neffects, this paper proposes two datasets of blur corruptions, which we denote\nOpticsBench and LensCorruptions. OpticsBench examines primary aberrations such\nas coma, defocus, and astigmatism, i.e. aberrations that can be represented by\nvarying a single parameter of Zernike polynomials. To go beyond the principled\nbut synthetic setting of primary aberrations, LensCorruptions samples linear\ncombinations in the vector space spanned by Zernike polynomials, corresponding\nto 100 real lenses. Evaluations for image classification and object detection\non ImageNet and MSCOCO show that for a variety of different pre-trained models,\nthe performance on OpticsBench and LensCorruptions varies significantly,\nindicating the need to consider realistic image corruptions to evaluate a\nmodel's robustness against blur."}
{"id": "2504.18521", "pdf": "https://arxiv.org/pdf/2504.18521", "abs": "https://arxiv.org/abs/2504.18521", "authors": ["Shintaro Shiba", "Quan Kong", "Norimasa Kobori"], "title": "E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization", "categories": ["cs.CV", "cs.RO", "eess.SP"], "comment": "10 pages, 9 figures, 5 tables, CVPRW on EventVision 2025", "summary": "Optical communication using modulated LEDs (e.g., visible light\ncommunication) is an emerging application for event cameras, thanks to their\nhigh spatio-temporal resolutions. Event cameras can be used simply to decode\nthe LED signals and also to localize the camera relative to the LED marker\npositions. However, there is no public dataset to benchmark the decoding and\nlocalization in various real-world settings. We present, to the best of our\nknowledge, the first public dataset that consists of an event camera, a frame\ncamera, and ground-truth poses that are precisely synchronized with hardware\ntriggers. It provides various camera motions with various sensitivities in\ndifferent scene brightness settings, both indoor and outdoor. Furthermore, we\npropose a novel method of localization that leverages the Contrast Maximization\nframework for motion estimation and compensation. The detailed analysis and\nexperimental results demonstrate the advantages of LED-based localization with\nevents over the conventional AR-marker--based one with frames, as well as the\nefficacy of the proposed method in localization. We hope that the proposed\ndataset serves as a future benchmark for both motion-related classical computer\nvision tasks and LED marker decoding tasks simultaneously, paving the way to\nbroadening applications of event cameras on mobile devices.\nhttps://woven-visionai.github.io/evlc-dataset"}
{"id": "2504.18524", "pdf": "https://arxiv.org/pdf/2504.18524", "abs": "https://arxiv.org/abs/2504.18524", "authors": ["Fengjia Zhang", "Samrudhdhi B. Rangrej", "Tristan Aumentado-Armstrong", "Afsaneh Fazly", "Alex Levinshtein"], "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution (SR), a classical inverse problem in computer vision, is\ninherently ill-posed, inducing a distribution of plausible solutions for every\ninput. However, the desired result is not simply the expectation of this\ndistribution, which is the blurry image obtained by minimizing pixelwise error,\nbut rather the sample with the highest image quality. A variety of techniques,\nfrom perceptual metrics to adversarial losses, are employed to this end. In\nthis work, we explore an alternative: utilizing powerful non-reference image\nquality assessment (NR-IQA) models in the SR context. We begin with a\ncomprehensive analysis of NR-IQA metrics on human-derived SR data, identifying\nboth the accuracy (human alignment) and complementarity of different metrics.\nThen, we explore two methods of applying NR-IQA models to SR learning: (i)\naltering data sampling, by building on an existing multi-ground-truth SR\nframework, and (ii) directly optimizing a differentiable quality score. Our\nresults demonstrate a more human-centric perception-distortion tradeoff,\nfocusing less on non-perceptual pixel-wise distortion, instead improving the\nbalance between perceptual fidelity and human-tuned NR-IQA measures."}
{"id": "2504.17819", "pdf": "https://arxiv.org/pdf/2504.17819", "abs": "https://arxiv.org/abs/2504.17819", "authors": ["Mohaddeseh Chegini", "Ali Mahloojifar"], "title": "A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of\ndiseases. The development of CADs by leveraging third generation neural\nnetwork, namely, Spiking Neural Network (SNN), is essential to utilize of the\nbenefits of SNNs, such as their event_driven processing, parallelism, low power\nconsumption, and the ability to process sparse temporal_spatial information.\nHowever, Deep SNN as a deep learning model faces challenges with unreliability.\nTo deal with unreliability challenges due to inability to quantify the\nuncertainty of the predictions, we proposed a deep Bayesian Convolutional\nSpiking Neural Network based_CADs with uncertainty_aware module. In this study,\nthe Monte Carlo Dropout method as Bayesian approximation is used as an\nuncertainty quantification method. This method was applied to several medical\nimage classification tasks. Our experimental results demonstrate that our\nproposed model is accurate and reliable and will be a proper alternative to\nconventional deep learning for medical image classification."}
{"id": "2504.17865", "pdf": "https://arxiv.org/pdf/2504.17865", "abs": "https://arxiv.org/abs/2504.17865", "authors": ["Charles J. Carver", "Hadleigh Schwartz", "Toma Itagaki", "Zachary Englhardt", "Kechen Liu", "Megan Graciela Nauli Manik", "Chun-Cheng Chang", "Vikram Iyer", "Brian Plancher", "Xia Zhou"], "title": "Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 7 figures, submitted to IROS 2025", "summary": "We present Phaser, a flexible system that directs narrow-beam laser light to\nmoving robots for concurrent wireless power delivery and communication. We\ndesign a semi-automatic calibration procedure to enable fusion of\nstereo-vision-based 3D robot tracking with high-power beam steering, and a\nlow-power optical communication scheme that reuses the laser light as a data\nchannel. We fabricate a Phaser prototype using off-the-shelf hardware and\nevaluate its performance with battery-free autonomous robots. Phaser delivers\noptical power densities of over 110 mW/cm$^2$ and error-free data to mobile\nrobots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\\% less\ncurrent than Bluetooth Low Energy). We demonstrate Phaser fully powering\ngram-scale battery-free robots to nearly 2x higher speeds than prior work while\nsimultaneously controlling them to navigate around obstacles and along paths.\nCode, an open-source design guide, and a demonstration video of Phaser is\navailable at https://mobilex.cs.columbia.edu/phaser."}
{"id": "2504.17898", "pdf": "https://arxiv.org/pdf/2504.17898", "abs": "https://arxiv.org/abs/2504.17898", "authors": ["David Wang", "Derek Goh", "Jiale Zhang"], "title": "Material Identification Via RFID For Smart Shopping", "categories": ["eess.SP", "cs.CV", "J.0; J.7; B.0"], "comment": "5 pages, 7 figures", "summary": "Cashierless stores rely on computer vision and RFID tags to associate\nshoppers with items, but concealed items placed in backpacks, pockets, or bags\ncreate challenges for theft prevention. We introduce a system that turns\nexisting RFID tagged items into material sensors by exploiting how different\ncontainers attenuate and scatter RF signals. Using RSSI and phase angle, we\ntrained a neural network to classify seven common containers. In a simulated\nretail environment, the model achieves 89% accuracy with one second samples and\n74% accuracy from single reads. Incorporating distance measurements, our system\nachieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at\naisle or doorway choke points, the system can flag suspicious events in real\ntime, prompting camera screening or staff intervention. By combining material\nidentification with computer vision tracking, our system provides proactive\nloss prevention for cashierless retail while utilizing existing infrastructure."}
{"id": "2504.17943", "pdf": "https://arxiv.org/pdf/2504.17943", "abs": "https://arxiv.org/abs/2504.17943", "authors": ["Mingsi Liao", "Gota Morota", "Ye Bi", "Rebecca R. Cockrum"], "title": "Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis", "categories": ["eess.IV", "cs.CV"], "comment": "Published on Animals, 18 March 2025", "summary": "Monitoring calf body weight (BW) before weaning is essential for assessing\ngrowth, feed efficiency, health, and weaning readiness. However, labor, time,\nand facility constraints limit BW collection. Additionally, Holstein calf coat\npatterns complicate image-based BW estimation, and few studies have explored\nnon-contact measurements taken at early time points for predicting later BW.\nThe objectives of this study were to (1) develop deep learning-based\nsegmentation models for extracting calf body metrics, (2) compare deep learning\nsegmentation with threshold-based methods, and (3) evaluate BW prediction using\nsingle-time-point cross-validation with linear regression (LR) and extreme\ngradient boosting (XGBoost) and multiple-time-point cross-validation with LR,\nXGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63)\nand Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves\nbeing weighed manually. Results showed that You Only Look Once version 8\n(YOLOv8) deep learning segmentation (intersection over union = 0.98)\noutperformed threshold-based methods (0.89). In single-time-point\ncross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean\nabsolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate\nlongitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight\nthe potential of deep learning for automated BW prediction, enhancing farm\nmanagement."}
{"id": "2504.17945", "pdf": "https://arxiv.org/pdf/2504.17945", "abs": "https://arxiv.org/abs/2504.17945", "authors": ["Bastien C. Baluyot", "Marta Varela", "Chen Qin"], "title": "Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 3 figures, 3 tables", "summary": "Accurate myocardial image registration is essential for cardiac strain\nanalysis and disease diagnosis. However, spectral bias in neural networks\nimpedes modeling high-frequency deformations, producing inaccurate,\nbiomechanically implausible results, particularly in pathological data. This\npaper addresses spectral bias in physics-informed neural networks (PINNs) by\nintegrating Fourier Feature mappings and introducing modulation strategies into\na PINN framework. Experiments on two distinct datasets demonstrate that the\nproposed methods enhance the PINN's ability to capture complex, high-frequency\ndeformations in cardiomyopathies, achieving superior registration accuracy\nwhile maintaining biomechanical plausibility - thus providing a foundation for\nscalable cardiac image registration and generalization across multiple patients\nand pathologies."}
{"id": "2504.17954", "pdf": "https://arxiv.org/pdf/2504.17954", "abs": "https://arxiv.org/abs/2504.17954", "authors": ["Kaiyuan Tang", "Siyuan Yao", "Chaoli Wang"], "title": "iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)", "summary": "In volume visualization, users can interactively explore the\nthree-dimensional data by specifying color and opacity mappings in the transfer\nfunction (TF) or adjusting lighting parameters, facilitating meaningful\ninterpretation of the underlying structure. However, rendering large-scale\nvolumes demands powerful GPUs and high-speed memory access for real-time\nperformance. While existing novel view synthesis (NVS) methods offer faster\nrendering speeds with lower hardware requirements, the visible parts of a\nreconstructed scene are fixed and constrained by preset TF settings,\nsignificantly limiting user exploration. This paper introduces inverse volume\nrendering via Gaussian splatting (iVR-GS), an innovative NVS method that\nreduces the rendering cost while enabling scene editing for interactive volume\nexploration. Specifically, we compose multiple iVR-GS models associated with\nbasic TFs covering disjoint visible parts to make the entire volumetric scene\nvisible. Each basic model contains a collection of 3D editable Gaussians, where\neach Gaussian is a 3D spatial point that supports real-time scene rendering and\nediting. We demonstrate the superior reconstruction quality and composability\nof iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on\nvarious volume datasets. The code is available at\nhttps://github.com/TouKaienn/iVR-GS."}
{"id": "2504.18015", "pdf": "https://arxiv.org/pdf/2504.18015", "abs": "https://arxiv.org/abs/2504.18015", "authors": ["Hanrui Wang", "Shuo Wang", "Chun-Shien Lu", "Isao Echizen"], "title": "Diffusion-Driven Universal Model Inversion Attack for Face Recognition", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": null, "summary": "Facial recognition technology poses significant privacy risks, as it relies\non biometric data that is inherently sensitive and immutable if compromised. To\nmitigate these concerns, face recognition systems convert raw images into\nembeddings, traditionally considered privacy-preserving. However, model\ninversion attacks pose a significant privacy threat by reconstructing these\nprivate facial images, making them a crucial tool for evaluating the privacy\nrisks of face recognition systems. Existing methods usually require training\nindividual generators for each target model, a computationally expensive\nprocess. In this paper, we propose DiffUMI, a training-free diffusion-driven\nuniversal model inversion attack for face recognition systems. DiffUMI is the\nfirst approach to apply a diffusion model for unconditional image generation in\nmodel inversion. Unlike other methods, DiffUMI is universal, eliminating the\nneed for training target-specific generators. It operates within a fixed\nframework and pretrained diffusion model while seamlessly adapting to diverse\ntarget identities and models. DiffUMI breaches privacy-preserving face\nrecognition systems with state-of-the-art success, demonstrating that an\nunconditional diffusion model, coupled with optimized adversarial search,\nenables efficient and high-fidelity facial reconstruction. Additionally, we\nintroduce a novel application of out-of-domain detection (OODD), marking the\nfirst use of model inversion to distinguish non-face inputs from face inputs\nbased solely on embeddings."}
{"id": "2504.18053", "pdf": "https://arxiv.org/pdf/2504.18053", "abs": "https://arxiv.org/abs/2504.18053", "authors": ["Jianyu Liu", "Hangyu Guo", "Ranjie Duan", "Xingyuan Bu", "Yancheng He", "Shilong Li", "Hui Huang", "Jiaheng Liu", "Yucheng Wang", "Chenchen Jing", "Xingwei Qu", "Xiao Zhang", "Yingshui Tan", "Yanan Wu", "Jihao Gu", "Yangguang Li", "Jianke Zhu"], "title": "DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": "[NAACL 2025] The first four authors contribute equally, 23 pages,\n  repo at https://github.com/Kizna1ver/DREAM", "summary": "Multimodal Large Language Models (MLLMs) pose unique safety challenges due to\ntheir integration of visual and textual data, thereby introducing new\ndimensions of potential attacks and complex risk combinations. In this paper,\nwe begin with a detailed analysis aimed at disentangling risks through\nstep-by-step reasoning within multimodal inputs. We find that systematic\nmultimodal risk disentanglement substantially enhances the risk awareness of\nMLLMs. Via leveraging the strong discriminative abilities of multimodal risk\ndisentanglement, we further introduce \\textbf{DREAM}\n(\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety\n\\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety\nalignment in MLLMs through supervised fine-tuning and iterative Reinforcement\nLearning from AI Feedback (RLAIF). Experimental results show that DREAM\nsignificantly boosts safety during both inference and training phases without\ncompromising performance on normal tasks (namely oversafety), achieving a\n16.17\\% improvement in the SIUO safe\\&effective score compared to GPT-4V. The\ndata and code are available at https://github.com/Kizna1ver/DREAM."}
{"id": "2504.18067", "pdf": "https://arxiv.org/pdf/2504.18067", "abs": "https://arxiv.org/abs/2504.18067", "authors": ["Chuyu Wang", "Huiting Deng", "Dong Liu"], "title": "Physics-Driven Neural Compensation For Electrical Impedance Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Electrical Impedance Tomography (EIT) provides a non-invasive, portable\nimaging modality with significant potential in medical and industrial\napplications. Despite its advantages, EIT encounters two primary challenges:\nthe ill-posed nature of its inverse problem and the spatially variable,\nlocation-dependent sensitivity distribution. Traditional model-based methods\nmitigate ill-posedness through regularization but overlook sensitivity\nvariability, while supervised deep learning approaches require extensive\ntraining data and lack generalization. Recent developments in neural fields\nhave introduced implicit regularization techniques for image reconstruction,\nbut these methods typically neglect the physical principles underlying EIT,\nthus limiting their effectiveness. In this study, we propose PhyNC\n(Physics-driven Neural Compensation), an unsupervised deep learning framework\nthat incorporates the physical principles of EIT. PhyNC addresses both the\nill-posed inverse problem and the sensitivity distribution by dynamically\nallocating neural representational capacity to regions with lower sensitivity,\nensuring accurate and balanced conductivity reconstructions. Extensive\nevaluations on both simulated and experimental data demonstrate that PhyNC\noutperforms existing methods in terms of detail preservation and artifact\nresistance, particularly in low-sensitivity regions. Our approach enhances the\nrobustness of EIT reconstructions and provides a flexible framework that can be\nadapted to other imaging modalities with similar challenges."}
{"id": "2504.18207", "pdf": "https://arxiv.org/pdf/2504.18207", "abs": "https://arxiv.org/abs/2504.18207", "authors": ["Simon Lucey"], "title": "Gradient Descent as a Shrinkage Operator for Spectral Bias", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We generalize the connection between activation function and spline\nregression/smoothing and characterize how this choice may influence spectral\nbias within a 1D shallow network. We then demonstrate how gradient descent (GD)\ncan be reinterpreted as a shrinkage operator that masks the singular values of\na neural network's Jacobian. Viewed this way, GD implicitly selects the number\nof frequency components to retain, thereby controlling the spectral bias. An\nexplicit relationship is proposed between the choice of GD hyperparameters\n(learning rate & number of iterations) and bandwidth (the number of active\ncomponents). GD regularization is shown to be effective only with monotonic\nactivation functions. Finally, we highlight the utility of non-monotonic\nactivation functions (sinc, Gaussian) as iteration-efficient surrogates for\nspectral bias."}
{"id": "2504.18268", "pdf": "https://arxiv.org/pdf/2504.18268", "abs": "https://arxiv.org/abs/2504.18268", "authors": ["Ana Matoso", "Catarina Passarinho", "Marta P. Loureiro", "José Maria Moreira", "Patrícia Figueiredo", "Rita G. Nunes"], "title": "Towards a deep learning approach for classifying treatment response in glioblastomas", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Glioblastomas are the most aggressive type of glioma, having a 5-year\nsurvival rate of 6.9%. Treatment typically involves surgery, followed by\nradiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI)\nscans to monitor disease progression. To assess treatment response,\nradiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to\ncategorize the tumor into one of four labels based on imaging and clinical\nfeatures: complete response, partial response, stable disease, and progressive\ndisease. This assessment is very complex and time-consuming. Since deep\nlearning (DL) has been widely used to tackle classification problems, this work\naimed to implement the first DL pipeline for the classification of RANO\ncriteria based on two consecutive MRI acquisitions. The models were trained and\ntested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction\nof input images, 2) different combinations of modalities, 3) different model\narchitectures, 4) different pretraining tasks, and 5) adding clinical data. The\npipeline that achieved the best performance used a Densenet264 considering only\nT1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR)\nimages as input without any pretraining. A median Balanced Accuracy of 50.96%\nwas achieved. Additionally, explainability methods were applied. Using Saliency\nMaps, the tumor region was often successfully highlighted. In contrast,\nGrad-CAM typically failed to highlight the tumor region, with some exceptions\nobserved in the Complete Response and Progressive Disease classes, where it\neffectively identified the tumor region. These results set a benchmark for\nfuture studies on glioblastoma treatment response assessment based on the RANO\ncriteria while emphasizing the heterogeneity of factors that might play a role\nwhen assessing the tumor's response to treatment."}
{"id": "2504.18269", "pdf": "https://arxiv.org/pdf/2504.18269", "abs": "https://arxiv.org/abs/2504.18269", "authors": ["Shintaro Ozaki", "Kazuki Hayashi", "Yusuke Sakai", "Jingun Kwon", "Hidetaka Kamigaito", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "title": "TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under review", "summary": "Generating images from prompts containing specific entities requires models\nto retain as much entity-specific knowledge as possible. However, fully\nmemorizing such knowledge is impractical due to the vast number of entities and\ntheir continuous emergence. To address this, we propose Text-based Intelligent\nGeneration with Entity prompt Refinement (TextTIGER), which augments knowledge\non entities included in the prompts and then summarizes the augmented\ndescriptions using Large Language Models (LLMs) to mitigate performance\ndegradation from longer inputs. To evaluate our method, we introduce WiT-Cub\n(WiT with Captions and Uncomplicated Background-explanations), a dataset\ncomprising captions, images, and an entity list. Experiments on four image\ngeneration models and five LLMs show that TextTIGER improves image generation\nperformance in standard metrics (IS, FID, and CLIPScore) compared to\ncaption-only prompts. Additionally, multiple annotators' evaluation confirms\nthat the summarized descriptions are more informative, validating LLMs' ability\nto generate concise yet rich descriptions. These findings demonstrate that\nrefining prompts with augmented and summarized entity-related descriptions\nenhances image generation capabilities. The code and dataset will be available\nupon acceptance."}
{"id": "2504.18323", "pdf": "https://arxiv.org/pdf/2504.18323", "abs": "https://arxiv.org/abs/2504.18323", "authors": ["Yangyang Xu", "Kexin Li", "Li Yang", "You-Wei Wen"], "title": "Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation", "categories": ["math.NA", "cs.CV", "cs.LG", "cs.NA", "65K10, 15A69", "I.4.5; G.1.6"], "comment": "12 pages, 6 figures, 3 tables", "summary": "Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique\nfor decomposing multi-dimensional data into a low-rank tensor and an outlier\ntensor, yet existing methods relying on sparse outlier assumptions often fail\nunder structured corruptions. In this paper, we propose a self-guided data\naugmentation approach that employs adaptive weighting to suppress outlier\ninfluence, reformulating the original TRPCA problem into a standard Tensor\nPrincipal Component Analysis (TPCA) problem. The proposed model involves an\noptimization-driven weighting scheme that dynamically identifies and\ndownweights outlier contributions during tensor augmentation. We develop an\nefficient proximal block coordinate descent algorithm with closed-form updates\nto solve the resulting optimization problem, ensuring computational efficiency.\nTheoretical convergence is guaranteed through a framework combining block\ncoordinate descent with majorization-minimization principles. Numerical\nexperiments on synthetic and real-world datasets, including face recovery,\nbackground subtraction, and hyperspectral denoising, demonstrate that our\nmethod effectively handles various corruption patterns. The results show the\nimprovements in both accuracy and computational efficiency compared to\nstate-of-the-art methods."}
{"id": "2504.18344", "pdf": "https://arxiv.org/pdf/2504.18344", "abs": "https://arxiv.org/abs/2504.18344", "authors": ["Kristine Sørensen", "Oscar Camara", "Ole de Backer", "Klaus Kofoed", "Rasmus Paulsen"], "title": "NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation is often considered as the task of labelling each\npixel or voxel as being inside or outside a given anatomy. Processing the\nimages at their original size and resolution often result in insuperable memory\nrequirements, but downsampling the images leads to a loss of important details.\nInstead of aiming to represent a smooth and continuous surface in a binary\nvoxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF)\ndirectly from the image. The small memory requirements of NUDF allow for high\nresolution processing, while the continuous nature of the distance field allows\nus to create high resolution 3D mesh models of shapes of any topology (i.e.\nopen surfaces). We evaluate our method on the task of left atrial appendage\n(LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex\nand highly variable shape, being thus difficult to represent with traditional\nsegmentation methods using discrete labelmaps. With our proposed method, we are\nable to predict 3D mesh models that capture the details of the LAA and achieve\naccuracy in the order of the voxel spacing in the CT images."}
{"id": "2504.18398", "pdf": "https://arxiv.org/pdf/2504.18398", "abs": "https://arxiv.org/abs/2504.18398", "authors": ["Xinmin Feng", "Zhuoyuan Li", "Li Li", "Dong Liu", "Feng Wu"], "title": "Partition Map-Based Fast Block Partitioning for VVC Inter Coding", "categories": ["eess.IV", "cs.CV"], "comment": "23 pages, 26 figures. Project page:\n  https://github.com/ustc-ivclab/IPM", "summary": "Among the new techniques of Versatile Video Coding (VVC), the quadtree with\nnested multi-type tree (QT+MTT) block structure yields significant coding gains\nby providing more flexible block partitioning patterns. However, the recursive\npartition search in the VVC encoder increases the encoder complexity\nsubstantially. To address this issue, we propose a partition map-based\nalgorithm to pursue fast block partitioning in inter coding. Based on our\nprevious work on partition map-based methods for intra coding, we analyze the\ncharacteristics of VVC inter coding, and thus improve the partition map by\nincorporating an MTT mask for early termination. Next, we develop a neural\nnetwork that uses both spatial and temporal features to predict the partition\nmap. It consists of several special designs including stacked top-down and\nbottom-up processing, quantization parameter modulation layers, and\npartitioning-adaptive warping. Furthermore, we present a dual-threshold\ndecision scheme to achieve a fine-grained trade-off between complexity\nreduction and rate-distortion (RD) performance loss. The experimental results\ndemonstrate that the proposed method achieves an average 51.30% encoding time\nsaving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access\nconfiguration."}
{"id": "2504.18400", "pdf": "https://arxiv.org/pdf/2504.18400", "abs": "https://arxiv.org/abs/2504.18400", "authors": ["Yui Lo", "Yuqian Chen", "Dongnan Liu", "Leo Zekelman", "Jarrett Rushmore", "Yogesh Rathi", "Nikos Makris", "Alexandra J. Golby", "Fan Zhang", "Weidong Cai", "Lauren J. O'Donnell"], "title": "A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "21 pages, 3 figures, 6 tables", "summary": "Shape measures have emerged as promising descriptors of white matter\ntractography, offering complementary insights into anatomical variability and\nassociations with cognitive and clinical phenotypes. However, conventional\nmethods for computing shape measures are computationally expensive and\ntime-consuming for large-scale datasets due to reliance on voxel-based\nrepresentations. We propose Tract2Shape, a novel multimodal deep learning\nframework that leverages geometric (point cloud) and scalar (tabular) features\nto predict ten white matter tractography shape measures. To enhance model\nefficiency, we utilize a dimensionality reduction algorithm for the model to\npredict five primary shape components. The model is trained and evaluated on\ntwo independently acquired datasets, the HCP-YA dataset, and the PPMI dataset.\nWe evaluate the performance of Tract2Shape by training and testing it on the\nHCP-YA dataset and comparing the results with state-of-the-art models. To\nfurther assess its robustness and generalization ability, we also test\nTract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep\nlearning models across all ten shape measures, achieving the highest average\nPearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows\nthat both multimodal input and PCA contribute to performance gains. On the\nunseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low\nnMSE, demonstrating strong generalizability in cross-dataset evaluation.\nTract2Shape enables fast, accurate, and generalizable prediction of white\nmatter shape measures from tractography data, supporting scalable analysis\nacross datasets. This framework lays a promising foundation for future\nlarge-scale white matter shape analysis."}
{"id": "2504.18405", "pdf": "https://arxiv.org/pdf/2504.18405", "abs": "https://arxiv.org/abs/2504.18405", "authors": ["Jens Hooge", "Gerard Sanroma-Guell", "Faidra Stavropoulou", "Alexander Ullmann", "Gesine Knobloch", "Mark Klemens", "Carola Schmidt", "Sabine Weckbach", "Andreas Bolz"], "title": "HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a\ncrucial role in the detection and characterization of focal liver lesions, with\nthe hepatobiliary phase (HBP) providing essential diagnostic information.\nHowever, acquiring HBP images requires prolonged scan times, which may\ncompromise patient comfort and scanner throughput. In this study, we propose a\ndeep learning based approach for synthesizing HBP images from earlier contrast\nphases (precontrast and transitional) and compare three generative models: a\nperceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion\nprobabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from\ndiverse clinical settings and introduced a contrast evolution score (CES) to\nassess training data quality, enhancing model performance. Quantitative\nevaluation using pixel-wise and perceptual metrics, combined with qualitative\nassessment through blinded radiologist reviews, showed that pGAN achieved the\nbest quantitative performance but introduced heterogeneous contrast in\nout-of-distribution cases. In contrast, the U-Net produced consistent liver\nenhancement with fewer artifacts, while DDPM underperformed due to limited\npreservation of fine structural details. These findings demonstrate the\nfeasibility of synthetic HBP image generation as a means to reduce scan time\nwithout compromising diagnostic utility, highlighting the clinical potential of\ndeep learning for dynamic contrast enhancement in liver MRI. A project demo is\navailable at: https://jhooge.github.io/hepatogen"}
{"id": "2504.18442", "pdf": "https://arxiv.org/pdf/2504.18442", "abs": "https://arxiv.org/abs/2504.18442", "authors": ["Yue Li", "Pulkit Khandelwal", "Long Xie", "Laura E. M. Wisse", "Nidhi Mundada", "Christopher A. Brown", "Emily McGrew", "Amanda Denning", "Sandhitsu R. Das", "David A. Wolk", "Paul A. Yushkevich"], "title": "Nearly isotropic segmentation for medial temporal lobe subregions in multi-modality MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Morphometry of medial temporal lobe (MTL) subregions in brain MRI is\nsensitive biomarker to Alzheimers Disease and other related conditions. While\nT2-weighted (T2w) MRI with high in-plane resolution is widely used to segment\nhippocampal subfields due to its higher contrast in hippocampus, its lower\nout-of-plane resolution reduces the accuracy of subregion thickness\nmeasurements. To address this issue, we developed a nearly isotropic\nsegmentation pipeline that incorporates image and label upsampling and\nhigh-resolution segmentation in T2w MRI. First, a high-resolution atlas was\ncreated based on an existing anisotropic atlas derived from 29 individuals.\nBoth T1-weighted and T2w images in the atlas were upsampled from their original\nresolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local\nmeans approach. Manual segmentations within the atlas were also upsampled to\nmatch this resolution using a UNet-based neural network, which was trained on a\ncohort consisting of both high-resolution ex vivo and low-resolution\nanisotropic in vivo MRI with manual segmentations. Second, a multi-modality\ndeep learning-based segmentation model was trained within this nearly isotropic\natlas. Finally, experiments showed the nearly isotropic subregion segmentation\nimproved the accuracy of cortical thickness as an imaging biomarker for\nneurodegeneration in T2w MRI."}
{"id": "2504.18458", "pdf": "https://arxiv.org/pdf/2504.18458", "abs": "https://arxiv.org/abs/2504.18458", "authors": ["Wenyi Xiao", "Leilei Gan", "Weilong Dai", "Wanggui He", "Ziwei Huang", "Haoyuan Li", "Fangxun Shu", "Zhelun Yu", "Peng Zhang", "Hao Jiang", "Fei Wu"], "title": "Fast-Slow Thinking for Large Vision-Language Model Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "16 pages, 5 figures, and 12 tables", "summary": "Recent advances in large vision-language models (LVLMs) have revealed an\n\\textit{overthinking} phenomenon, where models generate verbose reasoning\nacross all tasks regardless of questions. To address this issue, we present\n\\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework\nthat dynamically adapts reasoning depth based on question characteristics.\nThrough empirical analysis, we establish the feasibility of fast-slow thinking\nin LVLMs by investigating how response length and data distribution affect\nperformance. We develop FAST-GRPO with three components: model-based metrics\nfor question characterization, an adaptive thinking reward mechanism, and\ndifficulty-aware KL regularization. Experiments across seven reasoning\nbenchmarks demonstrate that FAST achieves state-of-the-art accuracy with over\n10\\% relative improvement compared to the base model, while reducing token\nusage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively\nbalancing reasoning length and accuracy."}
{"id": "2504.18520", "pdf": "https://arxiv.org/pdf/2504.18520", "abs": "https://arxiv.org/abs/2504.18520", "authors": ["Jiahao Huang", "Fanwen Wang", "Pedro F. Ferreira", "Haosen Zhang", "Yinzhe Wu", "Zhifan Gao", "Lei Zhu", "Angelica I. Aviles-Rivero", "Carola-Bibiane Schonlieb", "Andrew D. Scott", "Zohya Khalique", "Maria Dwornik", "Ramyah Rajakulasingam", "Ranil De Silva", "Dudley J. Pennell", "Guang Yang", "Sonia Nielles-Vallespin"], "title": "RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor Cardiac MRI with Semantic-Aware Refinement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cardiac diffusion tensor imaging (DTI) offers unique insights into\ncardiomyocyte arrangements, bridging the gap between microscopic and\nmacroscopic cardiac function. However, its clinical utility is limited by\ntechnical challenges, including a low signal-to-noise ratio, aliasing\nartefacts, and the need for accurate quantitative fidelity. To address these\nlimitations, we introduce RSFR (Reconstruction, Segmentation, Fusion &\nRefinement), a novel framework for cardiac diffusion-weighted image\nreconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot\nsemantic priors via the Segment Anything Model and a robust Vision Mamba-based\nreconstruction backbone. Our framework integrates semantic features effectively\nto mitigate artefacts and enhance fidelity, achieving state-of-the-art\nreconstruction quality and accurate DT parameter estimation under high\nundersampling rates. Extensive experiments and ablation studies demonstrate the\nsuperior performance of RSFR compared to existing methods, highlighting its\nrobustness, scalability, and potential for clinical translation in quantitative\ncardiac DTI."}
