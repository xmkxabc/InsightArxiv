{"id": "2503.15617", "pdf": "https://arxiv.org/pdf/2503.15617", "abs": "https://arxiv.org/abs/2503.15617", "authors": ["Masud Ahmed", "Zahid Hasan", "Syed Arefinul Haque", "Abu Zaher Md Faridee", "Sanjay Purushotham", "Suya You", "Nirmalya Roy"], "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u503c\u5d4c\u5165\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u91cf\u5316\u5d4c\u5165\u65b9\u6cd5\u3002", "motivation": "\u5206\u6790\u8868\u660e\uff0c\u4f7f\u7528\u91cf\u5316\u5d4c\u5165\uff08\u5982VQ-VAE\uff09\u7684\u81ea\u52a8\u7f16\u7801\u5668\u5728\u5206\u5272\u63a9\u7801\u4e0a\u7684\u51c6\u786e\u7387\u6bd4\u8fde\u7eed\u503c\u5d4c\u5165\uff08\u5982KL-VAE\uff09\u4f4e8%\u3002", "method": "\u901a\u8fc7\u5c06\u8bed\u4e49\u63a9\u7801\u751f\u6210\u91cd\u65b0\u8868\u8ff0\u4e3a\u8fde\u7eed\u7684\u56fe\u50cf\u5230\u5d4c\u5165\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u5f15\u5bfc\u7684\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u5b66\u4e60\u8fde\u7eed\u7684\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08\u5982Cityscapes\u548c\u57df\u8f6c\u79fb\u53d8\u4f53\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5206\u5e03\u8f6c\u79fb\uff08\u5982\u6076\u52a3\u5929\u6c14\u548c\u89c6\u89d2\u53d8\u5316\uff09\u4e0b\u5177\u6709\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6297\u566a\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u57df\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u566a\u58f0\u548c\u56fe\u50cf\u53d8\u5316\u4e0b\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\u3002"}}
{"id": "2503.15621", "pdf": "https://arxiv.org/pdf/2503.15621", "abs": "https://arxiv.org/abs/2503.15621", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE.", "AI": {"task": "\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u89c6\u89c9\u9aa8\u5e72\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f15\u5165LLaVA-MORE\u6a21\u578b\u5bb6\u65cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5c06\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u5230\u6570\u5341\u4ebf\u53c2\u6570\uff0c\u4f46\u6a21\u578b\u5927\u5c0f\u3001\u67b6\u6784\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u4e0d\u4e00\u81f4\u6027\u963b\u788d\u4e86\u76f4\u63a5\u6bd4\u8f83\u3002", "method": "\u5f15\u5165LLaVA-MORE\u6a21\u578b\u5bb6\u65cf\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u534f\u8bae\uff0c\u7cfb\u7edf\u5206\u6790\u5c0f\u89c4\u6a21\u548c\u4e2d\u7b49\u89c4\u6a21\u7684LLMs\uff08\u5982Phi-4\u3001LLaMA-3.1\u548cGemma-2\uff09\u4ee5\u53ca\u591a\u79cd\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982CLIP\u3001DINOv2\u3001SigLIP\u548cSigLIP2\uff09\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u8bbe\u8ba1\u66f4\u6709\u6548\u7684MLLMs\u7684\u89c1\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fbf\u4e8e\u76f4\u63a5\u6bd4\u8f83\u548c\u6307\u5bfc\u672a\u6765\u6a21\u578b\u5f00\u53d1\u3002", "conclusion": "LLaVA-MORE\u6a21\u578b\u5bb6\u65cf\u53ca\u5176\u7edf\u4e00\u7684\u8bad\u7ec3\u534f\u8bae\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2503.15625", "pdf": "https://arxiv.org/pdf/2503.15625", "abs": "https://arxiv.org/abs/2503.15625", "authors": ["Matthew Massey", "Abdullah-Al-Zubaer Imran"], "title": "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https://github.com/masseygeo/earthscape.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u8bc4\u4f30EarthScape\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5730\u8868\u5730\u8d28\u5236\u56fe\u548c\u5730\u7403\u8868\u9762\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7684\u5730\u8d28\u5236\u56fe\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u5e76\u5f15\u5165\u4e86\u6f5c\u5728\u7684\u504f\u5dee\u3002", "method": "\u96c6\u6210\u9ad8\u5206\u8fa8\u7387\u822a\u7a7aRGB\u548c\u8fd1\u7ea2\u5916\uff08NIR\uff09\u5f71\u50cf\u3001\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u3001\u591a\u5c3a\u5ea6DEM\u884d\u751f\u7684\u5730\u5f62\u7279\u5f81\u4ee5\u53ca\u6c34\u6587\u548c\u57fa\u7840\u8bbe\u65bd\u77e2\u91cf\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e03\u4e2a\u4e0d\u540c\u5730\u8868\u5730\u8d28\u7c7b\u522b\u7684\u8be6\u7ec6\u6ce8\u91ca\u3002", "result": "\u5efa\u7acb\u4e86\u4f7f\u7528\u4e0d\u540c\u7a7a\u95f4\u6a21\u6001\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86EarthScape\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "EarthScape\u586b\u8865\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u5730\u7403\u79d1\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u3001\u5730\u7406\u7a7a\u95f4\u5206\u6790\u548c\u5730\u8d28\u5236\u56fe\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2503.15633", "pdf": "https://arxiv.org/pdf/2503.15633", "abs": "https://arxiv.org/abs/2503.15633", "authors": ["Am\u00e9lie Royer", "Moritz B\u00f6hle", "Gabriel de Marmiesse", "Laurent Mazar\u00e9", "Neil Zeghidour", "Alexandre D\u00e9fossez", "Patrick P\u00e9rez"], "title": "Vision-Speech Models: Teaching Speech Models to Converse about Images", "categories": ["cs.CV"], "comment": null, "summary": "The recent successes of Vision-Language models raise the question of how to\nequivalently imbue a pretrained speech model with vision understanding, an\nimportant milestone towards building a multimodal speech model able to freely\nconverse about images. Building such a conversational Vision-Speech model\nbrings its unique challenges: (i) paired image-speech datasets are much scarcer\nthan their image-text counterparts, (ii) ensuring real-time latency at\ninference is crucial thus bringing compute and memory constraints, and (iii)\nthe model should preserve prosodic features (e.g., speaker tone) which cannot\nbe inferred from text alone. In this work, we introduce MoshiVis, augmenting a\nrecent dialogue speech LLM, Moshi, with visual inputs through lightweight\nadaptation modules. An additional dynamic gating mechanism enables the model to\nmore easily switch between the visual inputs and unrelated conversation topics.\nTo reduce training costs, we design a simple one-stage, parameter-efficient\nfine-tuning pipeline in which we leverage a mixture of image-text (i.e.,\n\"speechless\") and image-speech samples. We evaluate the model on downstream\nvisual understanding tasks with both audio and text prompts, and report\nqualitative samples of interactions with MoshiVis. Our inference code will be\nmade available, as well as the image-speech data used for audio evaluation.", "AI": {"task": "\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u6a21\u578b\u4e0e\u89c6\u89c9\u7406\u89e3\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u81ea\u7531\u8ba8\u8bba\u56fe\u50cf\u7684\u591a\u6a21\u6001\u8bed\u97f3\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u6784\u5efa\u5bf9\u8bdd\u5f0f\u89c6\u89c9-\u8bed\u97f3\u6a21\u578b\u7684\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u914d\u5bf9\u56fe\u50cf-\u8bed\u97f3\u6570\u636e\u7a00\u7f3a\u3001\u63a8\u7406\u65f6\u5b9e\u65f6\u5ef6\u8fdf\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9650\u5236\uff0c\u4ee5\u53ca\u4fdd\u7559\u65e0\u6cd5\u4ec5\u4ece\u6587\u672c\u63a8\u65ad\u7684\u97f5\u5f8b\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\u589e\u5f3a\u6700\u8fd1\u7684\u5bf9\u8bdd\u8bed\u97f3LLM Moshi\uff0c\u5f15\u5165\u52a8\u6001\u95e8\u63a7\u673a\u5236\u4ee5\u5728\u89c6\u89c9\u8f93\u5165\u548c\u65e0\u5173\u5bf9\u8bdd\u4e3b\u9898\u4e4b\u95f4\u5207\u6362\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u4e00\u9636\u6bb5\u3001\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7ba1\u9053\u3002", "result": "\u5728\u4e0b\u6e38\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u62a5\u544a\u4e0eMoshiVis\u4ea4\u4e92\u7684\u5b9a\u6027\u6837\u672c\u3002", "conclusion": "MoshiVis\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u548c\u52a8\u6001\u95e8\u63a7\u673a\u5236\u6210\u529f\u5730\u5c06\u89c6\u89c9\u8f93\u5165\u96c6\u6210\u5230\u8bed\u97f3\u6a21\u578b\u4e2d\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u6210\u672c\u5e76\u4fdd\u7559\u4e86\u97f5\u5f8b\u7279\u5f81\u3002"}}
{"id": "2503.15639", "pdf": "https://arxiv.org/pdf/2503.15639", "abs": "https://arxiv.org/abs/2503.15639", "authors": ["Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal", "Cheng-Lin Liu"], "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u573a\u666f\u6587\u672c\u8bc6\u522b\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u4ee3\u573a\u666f\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u5927\u578b\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6587\u672c\u8bc6\u522b\u5668\u7684\u4f18\u52bf\uff0c\u5f15\u5165\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5206\u5272\u9636\u6bb5\uff0c\u7ec6\u5316\u5019\u9009\u6587\u672c\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u548c\u8bcd\u6c47\u8bc4\u4f30\u751f\u6210\u6700\u7ec8\u5f97\u5206\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u7684\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u76f8\u5f53\uff0c\u4f46\u6240\u9700\u8d44\u6e90\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u6d88\u8017\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2503.15647", "pdf": "https://arxiv.org/pdf/2503.15647", "abs": "https://arxiv.org/abs/2503.15647", "authors": ["Jumanh Atoum", "Garrison L. H. Johnston", "Nabil Simaan", "Jie Ying Wu"], "title": "Multi-Modal Gesture Recognition from Video and Surgical Tool Pose Information via Motion Invariants", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recognizing surgical gestures in real-time is a stepping stone towards\nautomated activity recognition, skill assessment, intra-operative assistance,\nand eventually surgical automation. The current robotic surgical systems\nprovide us with rich multi-modal data such as video and kinematics. While some\nrecent works in multi-modal neural networks learn the relationships between\nvision and kinematics data, current approaches treat kinematics information as\nindependent signals, with no underlying relation between tool-tip poses.\nHowever, instrument poses are geometrically related, and the underlying\ngeometry can aid neural networks in learning gesture representation. Therefore,\nwe propose combining motion invariant measures (curvature and torsion) with\nvision and kinematics data using a relational graph network to capture the\nunderlying relations between different data streams. We show that gesture\nrecognition improves when combining invariant signals with tool position,\nachieving 90.3\\% frame-wise accuracy on the JIGSAWS suturing dataset. Our\nresults show that motion invariant signals coupled with position are better\nrepresentations of gesture motion compared to traditional position and\nquaternion representations. Our results highlight the need for geometric-aware\nmodeling of kinematics for gesture recognition.", "AI": {"task": "\u5b9e\u65f6\u8bc6\u522b\u624b\u672f\u624b\u52bf\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5316\u6d3b\u52a8\u8bc6\u522b\u3001\u6280\u80fd\u8bc4\u4f30\u3001\u672f\u4e2d\u8f85\u52a9\u548c\u6700\u7ec8\u7684\u624b\u672f\u81ea\u52a8\u5316\u3002", "motivation": "\u5f53\u524d\u7684\u624b\u672f\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u89c6\u9891\u548c\u8fd0\u52a8\u5b66\u6570\u636e\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c06\u8fd0\u52a8\u5b66\u4fe1\u606f\u89c6\u4e3a\u72ec\u7acb\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u5de5\u5177\u5c16\u7aef\u59ff\u6001\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fd0\u52a8\u4e0d\u53d8\u5ea6\u91cf\uff08\u66f2\u7387\u548c\u626d\u8f6c\uff09\u4e0e\u89c6\u89c9\u548c\u8fd0\u52a8\u5b66\u6570\u636e\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5173\u7cfb\u56fe\u7f51\u7edc\u6355\u6349\u4e0d\u540c\u6570\u636e\u6d41\u4e4b\u95f4\u7684\u6f5c\u5728\u5173\u7cfb\u3002", "result": "\u5728JIGSAWS\u7f1d\u5408\u6570\u636e\u96c6\u4e0a\uff0c\u7ed3\u5408\u4e0d\u53d8\u4fe1\u53f7\u4e0e\u5de5\u5177\u4f4d\u7f6e\u7684\u624b\u52bf\u8bc6\u522b\u5e27\u51c6\u786e\u7387\u8fbe\u5230\u4e8690.3%\u3002", "conclusion": "\u8fd0\u52a8\u4e0d\u53d8\u4fe1\u53f7\u4e0e\u4f4d\u7f6e\u7ed3\u5408\u7684\u624b\u52bf\u8fd0\u52a8\u8868\u793a\u4f18\u4e8e\u4f20\u7edf\u7684\u4f4d\u7f6e\u548c\u56db\u5143\u6570\u8868\u793a\uff0c\u5f3a\u8c03\u4e86\u8fd0\u52a8\u5b66\u51e0\u4f55\u611f\u77e5\u5efa\u6a21\u5728\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2503.15653", "pdf": "https://arxiv.org/pdf/2503.15653", "abs": "https://arxiv.org/abs/2503.15653", "authors": ["Miguel Ure\u00f1a Pliego", "Rub\u00e9n Mart\u00ednez Mar\u00edn", "Nianfang Shi", "Takeru Shibayama", "Ulrich Leth", "Miguel Marchamalo Sacrist\u00e1n"], "title": "Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna", "categories": ["cs.CV"], "comment": "Preprint", "summary": "This study explores the integration of machine learning into urban aerial\nimage analysis, with a focus on identifying infrastructure surfaces for cars\nand pedestrians and analyzing historical trends. It emphasizes the transition\nfrom convolutional architectures to transformer-based pre-trained models,\nunderscoring their potential in global geospatial analysis. A workflow is\npresented for automatically generating geospatial datasets, enabling the\ncreation of semantic segmentation datasets from various sources, including\nWMS/WMTS links, vectorial cartography, and OpenStreetMap (OSM) overpass-turbo\nrequests. The developed code allows a fast dataset generation process for\ntraining machine learning models using openly available data without manual\nlabelling. Using aerial imagery and vectorial data from the respective\ngeographical offices of Madrid and Vienna, two datasets were generated for car\nand pedestrian surface detection. A transformer-based model was trained and\nevaluated for each city, demonstrating good accuracy values. The historical\ntrend analysis involved applying the trained model to earlier images predating\nthe availability of vectorial data 10 to 20 years, successfully identifying\ntemporal trends in infrastructure for pedestrians and cars across different\ncity areas. This technique is applicable for municipal governments to gather\nvaluable data at a minimal cost.", "AI": {"task": "\u63a2\u7d22\u5c06\u673a\u5668\u5b66\u4e60\u96c6\u6210\u5230\u57ce\u5e02\u822a\u7a7a\u56fe\u50cf\u5206\u6790\u4e2d\uff0c\u91cd\u70b9\u8bc6\u522b\u6c7d\u8f66\u548c\u884c\u4eba\u7684\u57fa\u7840\u8bbe\u65bd\u8868\u9762\u5e76\u5206\u6790\u5386\u53f2\u8d8b\u52bf\u3002", "motivation": "\u5f3a\u8c03\u4ece\u5377\u79ef\u67b6\u6784\u5411\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8f6c\u53d8\uff0c\u7a81\u51fa\u5176\u5728\u5168\u7403\u5730\u7406\u7a7a\u95f4\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u80fd\u591f\u4ece\u5404\u79cd\u6765\u6e90\u521b\u5efa\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u62ecWMS/WMTS\u94fe\u63a5\u3001\u77e2\u91cf\u5236\u56fe\u548cOpenStreetMap (OSM) overpass-turbo\u8bf7\u6c42\u3002", "result": "\u751f\u6210\u4e86\u4e24\u4e2a\u7528\u4e8e\u6c7d\u8f66\u548c\u884c\u4eba\u8868\u9762\u68c0\u6d4b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u51c6\u786e\u6027\u3002\u5386\u53f2\u8d8b\u52bf\u5206\u6790\u6210\u529f\u8bc6\u522b\u4e86\u4e0d\u540c\u57ce\u5e02\u533a\u57df\u4e2d\u884c\u4eba\u548c\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u7684\u65f6\u95f4\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u6280\u672f\u9002\u7528\u4e8e\u5e02\u653f\u653f\u5e9c\u4ee5\u6700\u4f4e\u6210\u672c\u6536\u96c6\u6709\u4ef7\u503c\u7684\u6570\u636e\u3002"}}
{"id": "2503.15661", "pdf": "https://arxiv.org/pdf/2503.15661", "abs": "https://arxiv.org/abs/2503.15661", "authors": ["Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Juan A. Rodriguez", "Montek Kalsi", "Rabiul Awal", "Nicolas Chapados", "M. Tamer \u00d6zsu", "Aishwarya Agrawal", "David Vazquez", "Christopher Pal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.", "AI": {"task": "\u4ecb\u7ecdUI-Vision\uff0c\u4e00\u4e2a\u7528\u4e8e\u5728\u771f\u5b9e\u684c\u9762\u73af\u5883\u4e2d\u8bc4\u4f30\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u7efc\u5408\u6027\u3001\u8bb8\u53ef\u5bbd\u677e\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5728\u7ebf\u73af\u5883\uff0c\u800c\u684c\u9762\u73af\u5883\u5bf9\u4e8e\u8bb8\u591a\u4e13\u4e1a\u548c\u65e5\u5e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u6536\u96c6\u6311\u6218\u548c\u8bb8\u53ef\u95ee\u9898\uff0c\u684c\u9762\u73af\u5883\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "UI-Vision\u63d0\u4f9b\u4e86\u5bc6\u96c6\u3001\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u6f14\u793a\u6ce8\u91ca\uff0c\u5305\u62ec\u8fb9\u754c\u6846\u3001UI\u6807\u7b7e\u548c\u52a8\u4f5c\u8f68\u8ff9\uff08\u70b9\u51fb\u3001\u62d6\u52a8\u548c\u952e\u76d8\u8f93\u5165\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u4ece\u7ec6\u5230\u7c97\u7c92\u5ea6\u7684\u4efb\u52a1\uff1a\u5143\u7d20\u5b9a\u4f4d\u3001\u5e03\u5c40\u5b9a\u4f4d\u548c\u52a8\u4f5c\u9884\u6d4b\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u6700\u5148\u8fdb\u6a21\u578b\uff08\u5982UI-TARS-72B\uff09\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u7406\u89e3\u4e13\u4e1a\u8f6f\u4ef6\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u6267\u884c\u590d\u6742\u52a8\u4f5c\uff08\u5982\u62d6\u653e\uff09\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90UI-Vision\uff0c\u65e8\u5728\u63a8\u52a8\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u4ee3\u7406\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u684c\u9762\u4efb\u52a1\u3002"}}
{"id": "2503.15666", "pdf": "https://arxiv.org/pdf/2503.15666", "abs": "https://arxiv.org/abs/2503.15666", "authors": ["Kyle Vedder"], "title": "Toward Scalable, Flexible Scene Flow for Point Clouds", "categories": ["cs.CV"], "comment": "PhD Thesis", "summary": "Scene flow estimation is the task of describing 3D motion between temporally\nsuccessive observations. This thesis aims to build the foundation for building\nscene flow estimators with two important properties: they are scalable, i.e.\nthey improve with access to more data and computation, and they are flexible,\ni.e. they work out-of-the-box in a variety of domains and on a variety of\nmotion patterns without requiring significant hyperparameter tuning.\n  In this dissertation we present several concrete contributions towards this.\nIn Chapter 1 we contextualize scene flow and its prior methods. In Chapter 2 we\npresent a blueprint to build and scale feedforward scene flow estimators\nwithout requiring expensive human annotations via large scale distillation from\npseudolabels provided by strong unsupervised test-time optimization methods. In\nChapter 3 we introduce a benchmark to better measure estimate quality across\ndiverse object types, better bringing into focus what we care about and expect\nfrom scene flow estimators, and use this benchmark to host a public challenge\nthat produced significant progress. In Chapter 4 we present a state-of-the-art\nunsupervised scene flow estimator that introduces a new, full sequence problem\nformulation and exhibits great promise in adjacent domains like 3D point\ntracking. Finally, in Chapter 5 I philosophize about what's next for scene flow\nand its potential future broader impacts.", "AI": {"task": "\u63cf\u8ff0\u65f6\u95f4\u4e0a\u8fde\u7eed\u89c2\u6d4b\u4e4b\u95f4\u76843D\u8fd0\u52a8\u3002", "motivation": "\u6784\u5efa\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u7684\u573a\u666f\u6d41\u4f30\u8ba1\u5668\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5404\u79cd\u9886\u57df\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e2d\u65e0\u9700\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u6574\u5373\u53ef\u5de5\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u51e0\u79cd\u5177\u4f53\u8d21\u732e\uff0c\u5305\u62ec\u901a\u8fc7\u5927\u89c4\u6a21\u84b8\u998f\u4ece\u5f3a\u65e0\u76d1\u7763\u6d4b\u8bd5\u65f6\u95f4\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u7684\u4f2a\u6807\u7b7e\u6784\u5efa\u548c\u6269\u5c55\u524d\u9988\u573a\u666f\u6d41\u4f30\u8ba1\u5668\uff0c\u5f15\u5165\u57fa\u51c6\u4ee5\u66f4\u597d\u5730\u8861\u91cf\u4f30\u8ba1\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5168\u5e8f\u5217\u95ee\u9898\u516c\u5f0f\u7684\u65e0\u76d1\u7763\u573a\u666f\u6d41\u4f30\u8ba1\u5668\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u573a\u666f\u6d41\u4f30\u8ba1\u5668\uff0c\u5e76\u5728\u76f8\u90bb\u9886\u57df\u59823D\u70b9\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "\u573a\u666f\u6d41\u4f30\u8ba1\u5668\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\uff0c\u672a\u6765\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2503.15667", "pdf": "https://arxiv.org/pdf/2503.15667", "abs": "https://arxiv.org/abs/2503.15667", "authors": ["Yuming Gu", "Phong Tran", "Yujian Zheng", "Hongyi Xu", "Heyuan Li", "Adilbek Karmanov", "Hao Li"], "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis", "categories": ["cs.CV"], "comment": "Page:https://freedomgu.github.io/DiffPortrait360\n  Code:https://github.com/FreedomGu/DiffPortrait360/", "summary": "Generating high-quality 360-degree views of human heads from single-view\nimages is essential for enabling accessible immersive telepresence applications\nand scalable personalized content creation. While cutting-edge methods for full\nhead generation are limited to modeling realistic human heads, the latest\ndiffusion-based approaches for style-omniscient head synthesis can produce only\nfrontal views and struggle with view consistency, preventing their conversion\ninto true 3D models for rendering from arbitrary angles. We introduce a novel\napproach that generates fully consistent 360-degree head views, accommodating\nhuman, stylized, and anthropomorphic forms, including accessories like glasses\nand hats. Our method builds on the DiffPortrait3D framework, incorporating a\ncustom ControlNet for back-of-head detail generation and a dual appearance\nmodule to ensure global front-back consistency. By training on continuous view\nsequences and integrating a back reference image, our approach achieves robust,\nlocally continuous view synthesis. Our model can be used to produce\nhigh-quality neural radiance fields (NeRFs) for real-time, free-viewpoint\nrendering, outperforming state-of-the-art methods in object synthesis and\n360-degree head generation for very challenging input portraits.", "AI": {"task": "\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5355\u89c6\u89d2\u56fe\u50cf\u5230360\u5ea6\u4eba\u7c7b\u5934\u90e8\u89c6\u56fe\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u53ef\u8bbf\u95ee\u7684\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u5448\u73b0\u5e94\u7528\u548c\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u5185\u5bb9\u521b\u4f5c\u3002", "method": "\u57fa\u4e8eDiffPortrait3D\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49ControlNet\u7528\u4e8e\u540e\u8111\u7ec6\u8282\u751f\u6210\u548c\u53cc\u5916\u89c2\u6a21\u5757\u4ee5\u786e\u4fdd\u5168\u5c40\u524d\u540e\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u795e\u7ecf\u8425\u8f90\u5c04\u573a\uff08NeRFs\uff09\uff0c\u7528\u4e8e\u5b9e\u65f6\u81ea\u7531\u89c6\u89d2\u6e32\u67d3\uff0c\u5728\u5bf9\u8c61\u5408\u6210\u548c360\u5ea6\u5934\u90e8\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210360\u5ea6\u5934\u90e8\u89c6\u56fe\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u4eba\u7c7b\u3001\u98ce\u683c\u5316\u548c\u62df\u4eba\u5316\u5f62\u5f0f\uff0c\u5305\u62ec\u773c\u955c\u548c\u5e3d\u5b50\u7b49\u914d\u9970\u3002"}}
{"id": "2503.15620", "pdf": "https://arxiv.org/pdf/2503.15620", "abs": "https://arxiv.org/abs/2503.15620", "authors": ["Austin Xu", "Srijan Bansal", "Yifei Ming", "Semih Yavuz", "Shafiq Joty"], "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 13 figures, 6 tables", "summary": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u7684\u4e0a\u4e0b\u6587\u8bc4\u4f30\u57fa\u51c6ContextualJudgeBench\u3002", "motivation": "\u73b0\u6709\u7684LLM-as-judge\u8303\u5f0f\u901a\u5e38\u5728\u975e\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6458\u8981\u7b49\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2000\u4e2a\u6311\u6218\u6027\u54cd\u5e94\u5bf9\u7684\u57fa\u51c6ContextualJudgeBench\uff0c\u91c7\u7528\u591a\u7ba1\u9f50\u4e0b\u7684\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u73b0\u6709\u7684\u4eba\u7c7b\u6ce8\u91ca\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u6270\u52a8\u3002", "result": "\u572811\u4e2a\u8bc4\u4f30\u6a21\u578b\u548c9\u4e2a\u901a\u7528\u6a21\u578b\u4e0a\u7684\u7efc\u5408\u7814\u7a76\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ca\u5176\u8bc4\u4f30\u6807\u51c6\u5bf9\u5373\u4f7f\u662f\u76ee\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f8b\u5982\uff0c\u8868\u73b0\u6700\u597d\u7684\u6a21\u578bOpenAI\u7684o1\u4ec5\u8fbe\u523055%\u7684\u4e00\u81f4\u51c6\u786e\u6027\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bc4\u4f30\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u73b0\u6709\u7684\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002"}}
{"id": "2503.15671", "pdf": "https://arxiv.org/pdf/2503.15671", "abs": "https://arxiv.org/abs/2503.15671", "authors": ["Arindam Dutta", "Meng Zheng", "Zhongpai Gao", "Benjamin Planche", "Anwesha Choudhuri", "Terrence Chen", "Amit K. Roy-Chowdhury", "Ziyan Wu"], "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing clothed humans from a single image is a fundamental task in\ncomputer vision with wide-ranging applications. Although existing monocular\nclothed human reconstruction solutions have shown promising results, they often\nrely on the assumption that the human subject is in an occlusion-free\nenvironment. Thus, when encountering in-the-wild occluded images, these\nalgorithms produce multiview inconsistent and fragmented reconstructions.\nAdditionally, most algorithms for monocular 3D human reconstruction leverage\ngeometric priors such as SMPL annotations for training and inference, which are\nextremely challenging to acquire in real-world applications. To address these\nlimitations, we propose CHROME: Clothed Human Reconstruction with\nOcclusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel\npipeline designed to reconstruct occlusion-resilient 3D humans with multiview\nconsistency from a single occluded image, without requiring either ground-truth\ngeometric prior annotations or 3D supervision. Specifically, CHROME leverages a\nmultiview diffusion model to first synthesize occlusion-free human images from\nthe occluded input, compatible with off-the-shelf pose control to explicitly\nenforce cross-view consistency during synthesis. A 3D reconstruction model is\nthen trained to predict a set of 3D Gaussians conditioned on both the occluded\ninput and synthesized views, aligning cross-view details to produce a cohesive\nand accurate 3D representation. CHROME achieves significant improvements in\nterms of both novel view synthesis (upto 3 db PSNR) and geometric\nreconstruction under challenging conditions.", "AI": {"task": "\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u7a7f\u8863\u670d\u7684\u4eba\u4f53", "motivation": "\u73b0\u6709\u7684\u5355\u76ee\u7a7f\u8863\u670d\u4eba\u4f53\u91cd\u5efa\u65b9\u6cd5\u5728\u65e0\u906e\u6321\u73af\u5883\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u906e\u6321\u56fe\u50cf\u65f6\uff0c\u4f1a\u4ea7\u751f\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u548c\u788e\u7247\u5316\u7684\u91cd\u5efa\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u96be\u4ee5\u83b7\u53d6\u7684\u51e0\u4f55\u5148\u9a8c\uff08\u5982SMPL\u6ce8\u91ca\uff09\u3002", "method": "\u63d0\u51fa\u4e86CHROME\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u89c6\u89d2\u6269\u6563\u6a21\u578b\u4ece\u906e\u6321\u8f93\u5165\u4e2d\u5408\u6210\u65e0\u906e\u6321\u7684\u4eba\u4f53\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408\u73b0\u6210\u7684\u59ff\u6001\u63a7\u5236\u6765\u663e\u5f0f\u5730\u5f3a\u5236\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u3002\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a3D\u91cd\u5efa\u6a21\u578b\uff0c\u6839\u636e\u906e\u6321\u8f93\u5165\u548c\u5408\u6210\u89c6\u56fe\u9884\u6d4b\u4e00\u7ec43D\u9ad8\u65af\u5206\u5e03\uff0c\u4ee5\u751f\u6210\u4e00\u81f4\u4e14\u51c6\u786e\u76843D\u8868\u793a\u3002", "result": "CHROME\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6761\u4ef6\u4e0b\uff0c\u5728\u65b0\u89c6\u89d2\u5408\u6210\uff08\u6700\u9ad83 dB PSNR\uff09\u548c\u51e0\u4f55\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "CHROME\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u9700\u8981\u51e0\u4f55\u5148\u9a8c\u6ce8\u91ca\u62163D\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u5355\u5f20\u906e\u6321\u56fe\u50cf\u4e2d\u91cd\u5efa\u51fa\u5177\u6709\u906e\u6321\u6062\u590d\u80fd\u529b\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u76843D\u4eba\u4f53\u3002"}}
{"id": "2503.15664", "pdf": "https://arxiv.org/pdf/2503.15664", "abs": "https://arxiv.org/abs/2503.15664", "authors": ["Hisashi Johno", "Yuki Johno", "Akitomo Amakawa", "Junichi Sato", "Ryota Tozuka", "Atsushi Komaba", "Hiroaki Watanabe", "Hiroki Watanabe", "Chihiro Goto", "Hiroyuki Morisaka", "Hiroshi Onishi", "Kazunori Nakamoto"], "title": "Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "11 pages, 6 figures, 2 tables, 6 supplementary files", "summary": "Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM\nwith REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained\n80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally,\nREK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification.", "AI": {"task": "\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u5728\u4e0d\u540c\u764c\u75c7\u5206\u671f\u4e2d\u7684\u6548\u7528\uff0c\u7279\u522b\u662f\u5728\u80f0\u817a\u764c\u5206\u671f\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u533a\u5206RAG\u7684\u5f71\u54cd\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u764c\u75c7\u4e2d\u7684\u6548\u7528\uff0c\u6bd4\u8f83\u4e86NotebookLM\u4e0e\u5176\u5185\u90e8LLM\uff08Gemini 2.0 Flash\uff09\u5728\u80f0\u817a\u764c\u5206\u671f\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u65e5\u672c\u80f0\u817a\u764c\u5206\u671f\u6307\u5357\u4f5c\u4e3a\u53ef\u9760\u5916\u90e8\u77e5\u8bc6\uff08REK\uff09\uff0c\u6bd4\u8f83\u4e86\u4e09\u7ec4\uff1aREK+/RAG+\uff08NotebookLM\u4e0eREK\uff09\u3001REK+/RAG-\uff08Gemini 2.0 Flash\u4e0eREK\uff09\u548cREK-/RAG-\uff08Gemini 2.0 Flash\u65e0REK\uff09\uff0c\u5728\u57fa\u4e8eCT\u53d1\u73b0\u7684100\u4e2a\u865a\u6784\u80f0\u817a\u764c\u75c5\u4f8b\u4e2d\u8fdb\u884c\u5206\u671f\u3002", "result": "REK+/RAG+\u7684\u5206\u671f\u51c6\u786e\u7387\u4e3a70%\uff0c\u4f18\u4e8eREK+/RAG-\uff0838%\uff09\u548cREK-/RAG-\uff0835%\uff09\u3002\u5728TNM\u5206\u7c7b\u4e2d\uff0cREK+/RAG+\u7684\u51c6\u786e\u7387\u4e3a80%\uff0c\u8d85\u8fc7REK+/RAG-\uff0855%\uff09\u548cREK-/RAG-\uff0850%\uff09\u3002\u6b64\u5916\uff0cREK+/RAG+\u660e\u786e\u5c55\u793a\u4e86\u68c0\u7d22\u5230\u7684REK\u6458\u5f55\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u4e3a92%\u3002", "conclusion": "NotebookLM\uff0c\u4e00\u79cdRAG-LLM\uff0c\u5728\u80f0\u817a\u764c\u5206\u671f\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5176\u5185\u90e8LLM\uff08Gemini 2.0 Flash\uff09\uff0c\u8868\u660eRAG\u53ef\u80fd\u63d0\u9ad8LLM\u7684\u5206\u671f\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u5176\u68c0\u7d22\u548c\u5c55\u793aREK\u6458\u5f55\u7684\u80fd\u529b\u4e3a\u533b\u751f\u63d0\u4f9b\u4e86\u900f\u660e\u5ea6\uff0c\u7a81\u663e\u4e86\u5176\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u5206\u7c7b\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2503.15672", "pdf": "https://arxiv.org/pdf/2503.15672", "abs": "https://arxiv.org/abs/2503.15672", "authors": ["William Ljungbergh", "Adam Lilja", "Adam Tonderski. Arvid Laveno Ling", "Carl Lindstr\u00f6m", "Willem Verbeke", "Junsheng Fu", "Christoffer Petersson", "Lars Hammarstrand", "Michael Felsberg"], "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https://research.zenseact.com/publications/gasp/.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u51e0\u4f55\u548c\u8bed\u4e49\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5GASP\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u73af\u5883\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u751f\u6210\u5927\u91cf\u65f6\u7a7a\u6570\u636e\uff0c\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u5b66\u4e60\u73af\u5883\u7684\u51e0\u4f55\u548c\u8bed\u4e49\u7ed3\u6784\u53ca\u5176\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u65f6\u7a7a\u70b9\u7684\uff081\uff09\u4e00\u822c\u5360\u7528\u7387\uff0c\uff082\uff09\u81ea\u6211\u5360\u7528\u7387\uff0c\u4ee5\u53ca\uff083\uff09\u4ece\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u9ad8\u7ea7\u7279\u5f81\uff0c\u5b66\u4e60\u7edf\u4e00\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86GASP\uff0c\u5c55\u793a\u4e86\u5728\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u3001\u5728\u7ebf\u5730\u56fe\u548c\u81ea\u8f66\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8fde\u7eed\u76844D\u51e0\u4f55\u548c\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2503.15718", "pdf": "https://arxiv.org/pdf/2503.15718", "abs": "https://arxiv.org/abs/2503.15718", "authors": ["Mathilde Aguiar", "Pierre Zweigenbaum", "Nona Naderi"], "title": "Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View", "categories": ["cs.CL"], "comment": null, "summary": "Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories.", "AI": {"task": "\u7814\u7a76\u60a3\u8005\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5176\u533b\u7597\u6863\u6848\u4ee5\u786e\u5b9a\u662f\u5426\u7b26\u5408\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u7684\u60c5\u51b5\u3002", "motivation": "\u901a\u8fc7\u5728\u7ebf\u62db\u52df\u76f4\u63a5\u5411\u60a3\u8005\u63a8\u5e7f\u4e34\u5e8a\u8bd5\u9a8c\uff0c\u53ef\u80fd\u66f4\u6709\u6548\u5730\u63a5\u89e6\u5230\u60a3\u8005\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1NLI4PR\uff0c\u901a\u8fc7\u6539\u7f16TREC 2022\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u96c6\uff0c\u624b\u52a8\u91cd\u65b0\u8868\u8ff0\u60a3\u8005\u7684\u533b\u7597\u6863\u6848\uff0c\u5e76\u4f7f\u7528\u76f8\u5173\u7684\u4e34\u5e8a\u8bd5\u9a8c\u62a5\u544a\u3002", "result": "\u4f7f\u7528\u60a3\u8005\u8bed\u8a00\u65f6\uff0cF1\u5f97\u5206\u572856.5\u523071.8\u4e4b\u95f4\uff0c\u800c\u4f7f\u7528\u533b\u5b66\u8bed\u8a00\u65f6\uff0cF1\u5f97\u5206\u572864.7\u523073.1\u4e4b\u95f4\u3002", "conclusion": "\u4f7f\u7528\u60a3\u8005\u8bed\u8a00\u65f6\uff0c\u6027\u80fd\u635f\u5931\u8f83\u5c0f\uff0c\u8868\u660e\u4ee5\u60a3\u8005\u4e3a\u8d77\u70b9\u53ef\u4ee5\u5e2e\u52a9\u62db\u52df\u4e34\u5e8a\u8bd5\u9a8c\u60a3\u8005\u3002"}}
{"id": "2503.15676", "pdf": "https://arxiv.org/pdf/2503.15676", "abs": "https://arxiv.org/abs/2503.15676", "authors": ["C\u00e9dric Vincent", "Taehyoung Kim", "Henri Mee\u00df"], "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation from RGB cameras is essential to the perception of\nautonomous flying vehicles. The stability of predictions through the captured\nvideos is paramount to their reliability and, by extension, to the\ntrustworthiness of the agents. In this paper, we propose a lightweight video\nsemantic segmentation approach-suited to onboard real-time inference-achieving\nhigh temporal consistency on aerial data through Semantic Similarity\nPropagation across frames. SSP temporally propagates the predictions of an\nefficient image segmentation model with global registration alignment to\ncompensate for camera movements. It combines the current estimation and the\nprior prediction with linear interpolation using weights computed from the\nfeatures similarities of the two frames. Because data availability is a\nchallenge in this domain, we propose a consistency-aware Knowledge Distillation\ntraining procedure for sparsely labeled datasets with few annotations. Using a\nlarge image segmentation model as a teacher to train the efficient SSP, we\nleverage the strong correlations between labeled and unlabeled frames in the\nsame training videos to obtain high-quality supervision on all frames. KD-SSP\nobtains a significant temporal consistency increase over the base image\nsegmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,\nwith higher accuracy and comparable inference speed. On these aerial datasets,\nKD-SSP provides a superior segmentation quality and inference speed trade-off\nthan other video methods proposed for general applications and shows\nconsiderably higher consistency. The code will be made publicly available upon\nacceptance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u9891\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u8f7d\u5b9e\u65f6\u63a8\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u4f20\u64ad\u5b9e\u73b0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "RGB\u76f8\u673a\u7684\u8bed\u4e49\u5206\u5272\u5bf9\u4e8e\u81ea\u4e3b\u98de\u884c\u5668\u7684\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u9884\u6d4b\u7684\u7a33\u5b9a\u6027\u76f4\u63a5\u5f71\u54cd\u5176\u53ef\u9760\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u76f8\u4f3c\u6027\u4f20\u64ad\uff08SSP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u914d\u51c6\u5bf9\u9f50\u8865\u507f\u76f8\u673a\u8fd0\u52a8\uff0c\u7ed3\u5408\u5f53\u524d\u4f30\u8ba1\u548c\u5148\u9a8c\u9884\u6d4b\u8fdb\u884c\u7ebf\u6027\u63d2\u503c\u3002\u5e76\u63d0\u51fa\u4e00\u81f4\u6027\u611f\u77e5\u7684\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6807\u6ce8\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "KD-SSP\u5728UAVid\u548cRuralScapes\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8612.5%\u548c6.7%\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u76f8\u5f53\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "KD-SSP\u5728\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u4f18\u4e8e\u5176\u4ed6\u89c6\u9891\u65b9\u6cd5\u7684\u5206\u5272\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u7684\u6743\u8861\uff0c\u5e76\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2503.15737", "pdf": "https://arxiv.org/pdf/2503.15737", "abs": "https://arxiv.org/abs/2503.15737", "authors": ["Heming Zhang", "Wenyu Li", "Di Huang", "Yinjie Tang", "Yixin Chen", "Philip Payne", "Fuhai Li"], "title": "KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKoGNER\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u77e5\u8bc6\u56fe\u8c31\u84b8\u998f\u96c6\u6210\u5230NER\u6a21\u578b\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60NER\u6a21\u578b\u5728\u9886\u57df\u6cdb\u5316\u548c\u6570\u636e\u7a00\u758f\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5b9e\u4f53\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "method": "KoGNER\u91c7\u7528\u4e24\u6b65\u8fc7\u7a0b\uff1a1) \u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u5916\u90e8\u77e5\u8bc6\u6e90\u84b8\u998f\u4e3a\u8f7b\u91cf\u7ea7\u8868\u793a\uff0c\u4ee5\u4fbf\u4e0eNER\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff1b2) \u5b9e\u4f53\u611f\u77e5\u589e\u5f3a\uff0c\u5c06\u5bcc\u542b\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u76f4\u63a5\u96c6\u6210\u5230GNN\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7406\u89e3\u548c\u8868\u793a\u5b9e\u4f53\u5173\u7cfb\u7684\u80fd\u529b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKoGNER\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u5fae\u8c03\u7684NER\u6a21\u578b\u548cLLMs\u3002", "conclusion": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u606f\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8NER\u7684\u51c6\u786e\u6027\uff0cKoGNER\u4e3a\u77e5\u8bc6\u611f\u77e5NLP\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2503.15683", "pdf": "https://arxiv.org/pdf/2503.15683", "abs": "https://arxiv.org/abs/2503.15683", "authors": ["Benidir Yanis", "Gonthier Nicolas", "Mallet Clement"], "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Bi-temporal change detection at scale based on Very High Resolution (VHR)\nimages is crucial for Earth monitoring. This remains poorly addressed so far:\nmethods either require large volumes of annotated data (semantic case), or are\nlimited to restricted datasets (binary set-ups). Most approaches do not exhibit\nthe versatility required for temporal and spatial adaptation: simplicity in\narchitecture design and pretraining on realistic and comprehensive datasets.\nSynthetic datasets are the key solution but still fail to handle complex and\ndiverse scenes. In this paper, we present HySCDG a generative pipeline for\ncreating a large hybrid semantic change detection dataset that contains both\nreal VHR images and inpainted ones, along with land cover semantic map at both\ndates and the change map. Being semantically and spatially guided, HySCDG\ngenerates realistic images, leading to a comprehensive and hybrid\ntransfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection\ncases (binary and semantic), from zero-shot to mixed and sequential training,\nand also under low data regime training. Experiments demonstrate that\npretraining on our hybrid dataset leads to a significant performance boost,\noutperforming SyntheWorld, a fully synthetic dataset, in every configuration.\nAll codes, models, and data are available here:\n$\\href{https://yb23.github.io/projects/cywd/}{https://yb23.github.io/projects/cywd/}$.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5927\u89c4\u6a21\u6df7\u5408\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u6570\u636e\u96c6HySCDG\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u53cc\u65f6\u76f8\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u65f6\u7a7a\u9002\u5e94\u6027\u3002\u5408\u6210\u6570\u636e\u96c6\u867d\u7136\u662f\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86HySCDG\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u771f\u5b9e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u4fee\u590d\u56fe\u50cf\uff0c\u751f\u6210\u5305\u542b\u571f\u5730\u8986\u76d6\u8bed\u4e49\u56fe\u548c\u53d8\u5316\u56fe\u7684\u6df7\u5408\u6570\u636e\u96c6FSC-180k\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u6240\u6709\u914d\u7f6e\u4e0b\u5747\u4f18\u4e8e\u5b8c\u5168\u5408\u6210\u6570\u636e\u96c6SyntheWorld\u3002", "conclusion": "HySCDG\u751f\u6210\u7684\u6df7\u5408\u6570\u636e\u96c6FSC-180k\u5728\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2503.15768", "pdf": "https://arxiv.org/pdf/2503.15768", "abs": "https://arxiv.org/abs/2503.15768", "authors": ["Alexandra DeLucia", "Mark Dredze"], "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box.", "AI": {"task": "\u81ea\u52a8\u603b\u7ed3\u591a\u7bc7\u6587\u6863\u4e2d\u7684\u4fe1\u606f\uff0c\u4ece\u65b0\u95fb\u6587\u7ae0\u5230\u591a\u8bf4\u8bdd\u8005\u7684\u5bf9\u8bdd\u3002", "motivation": "\u8bc4\u4f30\u591a\u6587\u6863\u6458\u8981\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u3001\u9886\u57df\u548c\u7ef4\u5ea6\uff08\u53c2\u8003\u76f8\u4f3c\u6027\u3001\u8d28\u91cf\u548c\u4e8b\u5b9e\u6027\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u6a21\u578b\u5728\u4e00\u4e2a\u9886\u57df\u8bad\u7ec3\u7684\u6a21\u578b\u4e3a\u4f55\u5728\u53e6\u4e00\u4e2a\u9886\u57df\uff08\u65b0\u95fb\u3001\u79d1\u5b66\u548c\u5bf9\u8bdd\uff09\u7684\u96f6\u6837\u672c\u9886\u57df\u8f6c\u79fb\u8bbe\u7f6e\u4e2d\u5931\u8d25\u3002", "method": "\u5c06\u5f53\u524dMDS\u6a21\u578b\u7684\u8bad\u7ec3\u65b9\u6cd5\u5206\u4e3a\u56db\u7c7b\uff1a\u7aef\u5230\u7aef\u7279\u6b8a\u9884\u8bad\u7ec3\uff08\u201c\u76f4\u63a5\u201d\uff09\u3001\u5206\u5757\u540e\u603b\u7ed3\u3001\u63d0\u53d6\u540e\u603b\u7ed3\u548c\u4f7f\u7528GPT\u98ce\u683c\u6a21\u578b\u7684\u63a8\u7406\u3002", "result": "\u5b9a\u4e49\u4e86\u9886\u57df\u8f6c\u79fb\u201c\u5931\u8d25\u201d\u4e3a\u4e8b\u5b9e\u6027\u4e0b\u964d\u3001\u4e0e\u76ee\u6807\u7684\u504f\u5dee\u66f4\u5927\u4ee5\u53ca\u6458\u8981\u8d28\u91cf\u666e\u904d\u4e0b\u964d\u3002", "conclusion": "\u9664\u4e86\u63a2\u7d22MDS\u6a21\u578b\u7684\u9886\u57df\u8f6c\u79fb\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u76f4\u63a5\u5e94\u7528\u6d41\u884c\u7684\u6458\u8981\u6307\u6807\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898\u3002"}}
{"id": "2503.15686", "pdf": "https://arxiv.org/pdf/2503.15686", "abs": "https://arxiv.org/abs/2503.15686", "authors": ["Jiaqi Liu", "Jichao Zahng", "Paolo Rota", "Nicu Sebe"], "title": "Multi-focal Conditioned Latent Diffusion for Person Image Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted", "summary": "The Latent Diffusion Model (LDM) has demonstrated strong capabilities in\nhigh-resolution image generation and has been widely employed for Pose-Guided\nPerson Image Synthesis (PGPIS), yielding promising results. However, the\ncompression process of LDM often results in the deterioration of details,\nparticularly in sensitive areas such as facial features and clothing textures.\nIn this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD)\nmethod to address these limitations by conditioning the model on disentangled,\npose-invariant features from these sensitive regions. Our approach utilizes a\nmulti-focal condition aggregation module, which effectively integrates facial\nidentity and texture-specific information, enhancing the model's ability to\nproduce appearance realistic and identity-consistent images. Our method\ndemonstrates consistent identity and appearance generation on the DeepFashion\ndataset and enables flexible person image editing due to its generation\nconsistency. The code is available at https://github.com/jqliu09/mcld.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u7126\u70b9\u6761\u4ef6\u6f5c\u5728\u6269\u6563\uff08MCLD\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u5728\u7ec6\u8282\u9000\u5316\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u6f5c\u5728\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u538b\u7f29\u8fc7\u7a0b\u4e2d\u4f1a\u5bfc\u81f4\u7ec6\u8282\u9000\u5316\uff0c\u7279\u522b\u662f\u5728\u9762\u90e8\u7279\u5f81\u548c\u670d\u88c5\u7eb9\u7406\u7b49\u654f\u611f\u533a\u57df\u3002", "method": "\u901a\u8fc7\u5728\u591a\u7126\u70b9\u6761\u4ef6\u805a\u5408\u6a21\u5757\u4e2d\u5229\u7528\u89e3\u8026\u7684\u3001\u59ff\u6001\u4e0d\u53d8\u7684\u7279\u5f81\u6765\u589e\u5f3a\u6a21\u578b\u751f\u6210\u5916\u89c2\u903c\u771f\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u56fe\u50cf\u7684\u80fd\u529b\u3002", "result": "\u5728DeepFashion\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u8eab\u4efd\u548c\u5916\u89c2\u751f\u6210\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u4eba\u7269\u56fe\u50cf\u7f16\u8f91\u3002", "conclusion": "\u63d0\u51fa\u7684MCLD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LDM\u5728\u7ec6\u8282\u9000\u5316\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2503.15783", "pdf": "https://arxiv.org/pdf/2503.15783", "abs": "https://arxiv.org/abs/2503.15783", "authors": ["Tsunehiko Tanaka", "Edgar Simo-Serra"], "title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone.", "AI": {"task": "\u751f\u6210\u6e38\u620f\u63cf\u8ff0\u8bed\u8a00\uff08GDL\uff09\u7684\u6e38\u620f\u63cf\u8ff0\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u518d\u73b0\u6e38\u620f\u63cf\u8ff0\u7684\u6e38\u620f\u7279\u5f81\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684LLMs\u5fae\u8c03\u65b9\u6cd5\uff08RLGDG\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u6cd5\u5956\u52b1\u548c\u6982\u5ff5\u5956\u52b1\u6765\u540c\u65f6\u63d0\u9ad8\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u5bf9\u6e38\u620f\u6982\u5ff5\u7684\u5fe0\u5b9e\u5ea6\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5373\u5728\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4e4b\u540e\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528SFT\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u65b9\u6cd5\u5728\u751f\u6210\u6e38\u620f\u63cf\u8ff0\u6587\u672c\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2503.15697", "pdf": "https://arxiv.org/pdf/2503.15697", "abs": "https://arxiv.org/abs/2503.15697", "authors": ["Panagiota Moraiti", "Efstathios Karypidis"], "title": "Technical Report for the 5th CLVision Challenge at CVPR: Addressing the Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution", "categories": ["cs.CV"], "comment": null, "summary": "This paper outlines our approach to the 5th CLVision challenge at CVPR, which\naddresses the Class-Incremental with Repetition (CIR) scenario. In contrast to\ntraditional class incremental learning, this novel setting introduces unique\nchallenges and research opportunities, particularly through the integration of\nunlabeled data into the training process. In the CIR scenario, encountered\nclasses may reappear in later learning experiences, and each experience may\ninvolve only a subset of the overall class distribution. Additionally, the\nunlabeled data provided during training may include instances of unseen\nclasses, or irrelevant classes which should be ignored. Our approach focuses on\nretaining previously learned knowledge by utilizing knowledge distillation and\npseudo-labeling techniques. The key characteristic of our method is the\nexploitation of unlabeled data during training, in order to maintain optimal\nperformance on instances of previously encountered categories and reduce the\ndetrimental effects of catastrophic forgetting. Our method achieves an average\naccuracy of 16.68\\% during the pre-selection phase and 21.19% during the final\nevaluation phase, outperforming the baseline accuracy of 9.39%. We provide the\nimplementation code at\nhttps://github.com/panagiotamoraiti/continual-learning-challenge-2024 .", "AI": {"task": "\u89e3\u51b3CVPR\u7b2c5\u5c4aCLVision\u6311\u6218\u4e2d\u7684\u7c7b\u589e\u91cf\u91cd\u590d\uff08CIR\uff09\u573a\u666f\u95ee\u9898\u3002", "motivation": "CIR\u573a\u666f\u4e0e\u4f20\u7edf\u7c7b\u589e\u91cf\u5b66\u4e60\u4e0d\u540c\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u6311\u6218\u548c\u7814\u7a76\u673a\u4f1a\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6574\u5408\u672a\u6807\u8bb0\u6570\u636e\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u548c\u4f2a\u6807\u7b7e\u6280\u672f\u6765\u4fdd\u7559\u5148\u524d\u5b66\u5230\u7684\u77e5\u8bc6\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5728\u9884\u9009\u9636\u6bb5\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a16.68%\uff0c\u5728\u6700\u7ec8\u8bc4\u4f30\u9636\u6bb5\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a21.19%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u51c6\u786e\u73879.39%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u5728\u4fdd\u6301\u5148\u524d\u9047\u5230\u7c7b\u522b\u7684\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u51cf\u5c11\u4e86\u707e\u96be\u6027\u9057\u5fd8\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2503.15837", "pdf": "https://arxiv.org/pdf/2503.15837", "abs": "https://arxiv.org/abs/2503.15837", "authors": ["Shangqing Zhao", "Yuhao Zhou", "Yupei Ren", "Zhe Chen", "Chenghao Jia", "Fang Zhe", "Zhaogaung Long", "Shu Liu", "Man Lan"], "title": "F\u00f9x\u00ec: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation", "categories": ["cs.CL", "cs.AI"], "comment": "working in progress", "summary": "Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u5178\u4e2d\u6587\u6587\u672c\u5904\u7406\u4e2d\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u9009\u62e9\u9898\u8bc4\u4f30\u7406\u89e3\u80fd\u529b\uff0c\u800c\u5728\u53e4\u5178\u4e2d\u6587\u751f\u6210\u80fd\u529b\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u5f15\u5165F`ux`i\u57fa\u51c6\uff0c\u6db5\u76d621\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5305\u62ec\u8bd7\u6b4c\u521b\u4f5c\u548c\u5bf9\u8054\u5b8c\u6210\uff0c\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u548c\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u5728\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u6df1\u539a\u6587\u5316\u77e5\u8bc6\u548c\u53e4\u5178\u683c\u5f0f\u7684\u4efb\u52a1\u4e0a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u53e4\u4ee3\u4e2d\u6587\u6587\u672c\u5904\u7406\u7684\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2503.15699", "pdf": "https://arxiv.org/pdf/2503.15699", "abs": "https://arxiv.org/abs/2503.15699", "authors": ["Neehar Kondapaneni", "Oisin Mac Aodha", "Pietro Perona"], "title": "Representational Similarity via Interpretable Visual Concepts", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "32 pages, 5 Figures, 16 Supplemental Figures, ICLR 2025", "summary": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness.", "AI": {"task": "\u6bd4\u8f83\u4e24\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u5dee\u5f02\u3002", "motivation": "\u6d4b\u91cf\u6df1\u5ea6\u7f51\u7edc\u7684\u76f8\u4f3c\u6027\u662f\u4e00\u4e2a\u957f\u671f\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u63d0\u4f9b\u4e00\u4e2a\u5355\u4e00\u7684\u6570\u5b57\u6765\u8861\u91cf\u4e24\u4e2a\u7f51\u7edc\u5728\u67d0\u4e00\u5c42\u7684\u76f8\u4f3c\u6027\uff0c\u4f46\u65e0\u6cd5\u89e3\u91ca\u5b83\u4eec\u76f8\u4f3c\u6216\u4e0d\u540c\u7684\u539f\u56e0\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8868\u793a\u76f8\u4f3c\u6027\u65b9\u6cd5\uff08RSVC\uff09\u6765\u6bd4\u8f83\u4e24\u4e2a\u7f51\u7edc\uff0c\u5e76\u4f7f\u7528RSVC\u53d1\u73b0\u4e24\u4e2a\u6a21\u578b\u4e4b\u95f4\u5171\u4eab\u548c\u72ec\u7279\u7684\u89c6\u89c9\u6982\u5ff5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6a21\u578b\u5dee\u5f02\u7684\u67d0\u4e9b\u65b9\u9762\u53ef\u4ee5\u5f52\u56e0\u4e8e\u4e00\u4e2a\u6a21\u578b\u53d1\u73b0\u7684\u72ec\u7279\u6982\u5ff5\uff0c\u800c\u8fd9\u4e9b\u6982\u5ff5\u5728\u53e6\u4e00\u4e2a\u6a21\u578b\u4e2d\u672a\u80fd\u5f88\u597d\u5730\u8868\u793a\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u4e0d\u540c\u89c6\u89c9\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u534f\u8bae\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86RSVC\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2503.15850", "pdf": "https://arxiv.org/pdf/2503.15850", "abs": "https://arxiv.org/abs/2503.15850", "authors": ["Xiaoou Liu", "Tiejin Chen", "Longchao Da", "Chacha Chen", "Zhen Lin", "Hua Wei"], "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7b\u57fa\u4e8e\u8ba1\u7b97\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u7ef4\u5ea6\uff08\u8f93\u5165\u3001\u63a8\u7406\u3001\u53c2\u6570\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff09\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u53ef\u9760\u6027\u662f\u4e00\u4e2a\u4e3b\u8981\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u7ecf\u5e38\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u4e0d\u6b63\u786e\u7684\u54cd\u5e94\u3002\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u901a\u8fc7\u4f30\u8ba1\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u6765\u589e\u5f3a\u53ef\u4fe1\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u98ce\u9669\u7f13\u89e3\u548c\u9009\u62e9\u6027\u9884\u6d4b\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684UQ\u65b9\u6cd5\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\u548c\u89e3\u7801\u4e0d\u4e00\u81f4\u6027\u800c\u96be\u4ee5\u5e94\u7528\u4e8eLLMs\u3002\u6b64\u5916\uff0cLLMs\u5f15\u5165\u4e86\u72ec\u7279\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5982\u8f93\u5165\u6a21\u7cca\u6027\u3001\u63a8\u7406\u8def\u5f84\u5206\u6b67\u548c\u89e3\u7801\u968f\u673a\u6027\uff0c\u8fd9\u4e9b\u8d85\u51fa\u4e86\u7ecf\u5178\u7684\u5076\u7136\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u8bc4\u4f30\u73b0\u6709\u6280\u672f\uff0c\u8bc4\u4f30\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u8bc6\u522b\u5f00\u653e\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u5f3a\u8c03\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684UQ\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u9c81\u68d2\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2503.15708", "pdf": "https://arxiv.org/pdf/2503.15708", "abs": "https://arxiv.org/abs/2503.15708", "authors": ["Sam Narimani", "Solveig Roth Hoff", "Kathinka Dahli Kurz", "Kjell-Inge Gjesdal", "Jurgen Geisler", "Endre Grovik"], "title": "Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Purpose: Segmentation of the breast lesion in dynamic contrast-enhanced\nmagnetic resonance imaging (DCE-MRI) is an essential step to accurately\ndiagnose and plan treatment and monitor progress. This study aims to highlight\nthe impact of breast region segmentation (BRS) on deep learning-based breast\nlesion segmentation (BLS) in breast DCE-MRI.\n  Methods Using the Stavanger Dataset containing primarily 59 DCE-MRI scans and\nUNet++ as deep learning models, four different process were conducted to\ncompare effect of BRS on BLS. These four approaches included the whole volume\nwithout BRS and with BRS, BRS with the selected lesion slices and lastly\noptimal volume with BRS. Preprocessing methods like augmentation and\noversampling were used to enhance the small dataset, data shape uniformity and\nimprove model performance. Optimal volume size were investigated by a precise\nprocess to ensure that all lesions existed in slices. To evaluate the model, a\nhybrid loss function including dice, focal and cross entropy along with 5-fold\ncross validation method were used and lastly a test dataset which was randomly\nsplit used to evaluate the model performance on unseen data for each of four\nmentioned approaches.\n  Results Results demonstrate that using BRS considerably improved model\nperformance and validation. Significant improvement in last approach -- optimal\nvolume with BRS -- compared to the approach without BRS counting around 50\npercent demonstrating how effective BRS has been in BLS. Moreover, huge\nimprovement in energy consumption, decreasing up to 450 percent, introduces a\ngreen solution toward a more environmentally sustainable approach for future\nwork on large dataset.", "AI": {"task": "\u7814\u7a76\u4e73\u817a\u533a\u57df\u5206\u5272\uff08BRS\uff09\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e73\u817a\u75c5\u53d8\u5206\u5272\uff08BLS\uff09\u5728\u4e73\u817a\u52a8\u6001\u5bf9\u6bd4\u589e\u5f3a\u78c1\u5171\u632f\u6210\u50cf\uff08DCE-MRI\uff09\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u51c6\u786e\u5206\u5272\u4e73\u817a\u75c5\u53d8\u662f\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u548c\u8fdb\u5c55\u76d1\u6d4b\u7684\u5173\u952e\u6b65\u9aa4\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8BRS\u5bf9BLS\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5305\u542b59\u4e2aDCE-MRI\u626b\u63cf\u7684Stavanger\u6570\u636e\u96c6\u548cUNet++\u6a21\u578b\uff0c\u8fdb\u884c\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u5904\u7406\u6765\u6bd4\u8f83BRS\u5bf9BLS\u7684\u5f71\u54cd\u3002\u9884\u5904\u7406\u65b9\u6cd5\u5305\u62ec\u6570\u636e\u589e\u5f3a\u548c\u8fc7\u91c7\u6837\uff0c\u4ee5\u63d0\u9ad8\u5c0f\u6570\u636e\u96c6\u7684\u6027\u80fd\u548c\u5f62\u72b6\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u7cbe\u786e\u8fc7\u7a0b\u786e\u5b9a\u6700\u4f73\u4f53\u79ef\u5927\u5c0f\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\u548c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528BRS\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u9a8c\u8bc1\u6548\u679c\u3002\u6700\u4f73\u4f53\u79ef\u4e0eBRS\u7684\u65b9\u6cd5\u76f8\u6bd4\u6ca1\u6709BRS\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7ea650%\uff0c\u5e76\u4e14\u5728\u80fd\u8017\u65b9\u9762\u4e5f\u6709\u663e\u8457\u6539\u5584\uff0c\u51cf\u5c11\u4e86450%\u3002", "conclusion": "BRS\u5728BLS\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u51cf\u5c11\u4e86\u80fd\u8017\uff0c\u4e3a\u672a\u6765\u5728\u5927\u6570\u636e\u96c6\u4e0a\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u66f4\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2503.15879", "pdf": "https://arxiv.org/pdf/2503.15879", "abs": "https://arxiv.org/abs/2503.15879", "authors": ["DongGeon Lee", "Ahjeong Park", "Hyeri Lee", "Hyeonseo Nam", "Yunho Maeng"], "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to NAACL 2025 SRW", "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https://github.com/TeamNLP/Typed-RAG}{https://github.com/TeamNLP/Typed-RAG}.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u578b\u611f\u77e5\u7684\u591a\u65b9\u9762\u5206\u89e3\u6846\u67b6Typed-RAG\uff0c\u7528\u4e8e\u975e\u4e8b\u5b9e\u6027\u95ee\u7b54\uff08NFQA\uff09\u3002", "motivation": "\u975e\u4e8b\u5b9e\u6027\u95ee\u7b54\u7531\u4e8e\u5176\u5f00\u653e\u6027\u3001\u591a\u6837\u7684\u610f\u56fe\u548c\u591a\u65b9\u9762\u7684\u63a8\u7406\u9700\u6c42\uff0c\u4f7f\u5f97\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u3002", "method": "Typed-RAG\u5c06\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\u5206\u7c7b\u4e3a\u4e0d\u540c\u7684\u7c7b\u578b\uff08\u5982\u8fa9\u8bba\u3001\u7ecf\u9a8c\u548c\u6bd4\u8f83\uff09\uff0c\u5e76\u5e94\u7528\u57fa\u4e8e\u65b9\u9762\u7684\u5206\u89e3\u6765\u4f18\u5316\u68c0\u7d22\u548c\u751f\u6210\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTyped-RAG\u5728\u57fa\u51c6\u6570\u636e\u96c6Wiki-NFQA\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u7c7b\u578b\u611f\u77e5\u5206\u89e3\u5728NFQA\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Typed-RAG\u901a\u8fc7\u7c7b\u578b\u611f\u77e5\u7684\u591a\u65b9\u9762\u5206\u89e3\uff0c\u751f\u6210\u4e86\u66f4\u5177\u4fe1\u606f\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u56de\u7b54\uff0c\u6709\u6548\u63d0\u5347\u4e86\u975e\u4e8b\u5b9e\u6027\u95ee\u7b54\u7684\u6548\u679c\u3002"}}
{"id": "2503.15712", "pdf": "https://arxiv.org/pdf/2503.15712", "abs": "https://arxiv.org/abs/2503.15712", "authors": ["Weiwen Hu", "Niccol\u00f2 Parodi", "Marcus Zepp", "Ingo Feldmann", "Oliver Schreer", "Peter Eisert"], "title": "SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints", "categories": ["cs.CV"], "comment": "In Proceedings of the 20th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (2025)", "summary": "Open-vocabulary segmentation, powered by large visual-language models like\nCLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined\nby the dataset, enabling zero-shot understanding across diverse scenes.\nExtending these capabilities to 3D segmentation introduces challenges, as\nCLIP's image-based embeddings often lack the geometric detail necessary for 3D\nscene segmentation. Recent methods tend to address this by introducing\nadditional segmentation models or replacing CLIP with variations trained on\nsegmentation data, which lead to redundancy or loss on CLIP's general language\ncapabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based\nzero-shot 3D segmentation approach that leverages geometric priors. We\nintegrate geometric primitives derived from the 3D scene into NeRF training to\nproduce primitive-wise CLIP features, avoiding the ambiguity of point-wise\nfeatures. Additionally, we propose a primitive-based merging mechanism enhanced\nwith affinity scores. Without relying on additional segmentation models, our\nmethod further explores CLIP's capability for 3D segmentation and achieves\nnotable improvements over original LERF.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeRF\u7684\u96f6\u6837\u672c3D\u5206\u5272\u65b9\u6cd5SPNeRF\uff0c\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u8fdb\u884c3D\u573a\u666f\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCLIP\u76842D\u5206\u5272\u65b9\u6cd5\u57283D\u5206\u5272\u4e2d\u5b58\u5728\u51e0\u4f55\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5f15\u5165\u5197\u4f59\u6216\u4e27\u5931CLIP\u7684\u901a\u7528\u8bed\u8a00\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5c063D\u573a\u666f\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\u6574\u5408\u5230NeRF\u8bad\u7ec3\u4e2d\uff0c\u751f\u6210\u57fa\u5143\u7ea7\u522b\u7684CLIP\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u57fa\u5143\u7684\u5408\u5e76\u673a\u5236\u3002", "result": "\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u5206\u5272\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u5206\u5272\u7684\u6027\u80fd\u3002", "conclusion": "SPNeRF\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86CLIP\u7684\u80fd\u529b\u8fdb\u884c3D\u5206\u5272\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2503.15888", "pdf": "https://arxiv.org/pdf/2503.15888", "abs": "https://arxiv.org/abs/2503.15888", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Yilong Xu", "Junfeng Fang", "Lingrui Mei", "Xueqi Cheng"], "title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u53c2\u6570\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u4f9d\u8d56\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\uff0c\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u68c0\u7d22\u4fe1\u606f\u4e0d\u53ef\u9760\u6216\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u8fc7\u65f6\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51faCK-PLUG\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u77e5\u8bc6\u4e00\u81f4\u6027\u5ea6\u91cf\u2014\u2014\u7f6e\u4fe1\u589e\u76ca\uff08Confidence Gain\uff09\uff0c\u68c0\u6d4b\u77e5\u8bc6\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u5177\u6709\u8d1f\u7f6e\u4fe1\u589e\u76ca\u7684token\u7684\u6982\u7387\u5206\u5e03\u6765\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u77e5\u8bc6\u504f\u597d\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCK-PLUG\u80fd\u591f\u5728\u53cd\u4e8b\u5b9eRAG\u573a\u666f\u4e2d\u663e\u8457\u8c03\u8282\u77e5\u8bc6\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6d41\u7545\u6027\u548c\u77e5\u8bc6\u51c6\u786e\u6027\u3002\u4f8b\u5982\uff0c\u5728Llama3-8B\u4e0a\uff0cRAG\u54cd\u5e94\u7684\u8bb0\u5fc6\u53ec\u56de\u7387\uff08MR\uff09\u53ef\u4ee5\u57289.9%-71.9%\u7684\u8303\u56f4\u5185\u8c03\u6574\uff0c\u800c\u57fa\u7ebf\u4e3a42.1%\u3002", "conclusion": "CK-PLUG\u652f\u6301\u57fa\u4e8e\u6a21\u578b\u5bf9\u5185\u90e8\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u5728\u5404\u79cd\u901a\u7528RAG\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2503.15731", "pdf": "https://arxiv.org/pdf/2503.15731", "abs": "https://arxiv.org/abs/2503.15731", "authors": ["Yuqing Zhang", "Qi Han", "Ligeng Wang", "Kai Cheng", "Bo Wang", "Kun Zhan"], "title": "Graph-Weighted Contrastive Learning for Semi-Supervised Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "Journal of Electronic Imaging, 2025", "summary": "Most existing graph-based semi-supervised hyperspectral image classification\nmethods rely on superpixel partitioning techniques. However, they suffer from\nmisclassification of certain pixels due to inaccuracies in superpixel\nboundaries, \\ie, the initial inaccuracies in superpixel partitioning limit\noverall classification performance. In this paper, we propose a novel\ngraph-weighted contrastive learning approach that avoids the use of superpixel\npartitioning and directly employs neural networks to learn hyperspectral image\nrepresentation. Furthermore, while many approaches require all graph nodes to\nbe available during training, our approach supports mini-batch training by\nprocessing only a subset of nodes at a time, reducing computational complexity\nand improving generalization to unseen nodes. Experimental results on three\nwidely-used datasets demonstrate the effectiveness of the proposed approach\ncompared to baselines relying on superpixel partitioning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u534a\u76d1\u7763\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8d85\u50cf\u7d20\u5206\u5272\u6280\u672f\uff0c\u4f46\u7531\u4e8e\u8d85\u50cf\u7d20\u8fb9\u754c\u7684\u4e0d\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u67d0\u4e9b\u50cf\u7d20\u7684\u8bef\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u6574\u4f53\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u907f\u514d\u4f7f\u7528\u8d85\u50cf\u7d20\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u9ad8\u5149\u8c31\u56fe\u50cf\u8868\u793a\uff0c\u5e76\u652f\u6301\u5c0f\u6279\u91cf\u8bad\u7ec3\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u5bf9\u672a\u89c1\u8282\u70b9\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f9d\u8d56\u8d85\u50cf\u7d20\u5206\u5272\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u907f\u514d\u4e86\u8d85\u50cf\u7d20\u5206\u5272\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2503.15904", "pdf": "https://arxiv.org/pdf/2503.15904", "abs": "https://arxiv.org/abs/2503.15904", "authors": ["Evan Chen", "Run-Jun Zhan", "Yan-Bai Lin", "Hung-Hsuan Chen"], "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u7279\u522b\u662f\u5176\u5728\u804c\u4e1a\u53d9\u8ff0\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u9769\u547d\u6027\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u53cd\u6620\u6216\u653e\u5927\u793e\u4f1a\u504f\u89c1\u7684\u95ee\u9898\u4ecd\u7136\u5b58\u5728\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7531\u5f62\u5f0f\u7684\u8bb2\u6545\u4e8b\u6765\u63ed\u793a\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u7ed3\u6784\u5316\u573a\u666f\u6216\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u3002", "result": "\u7cfb\u7edf\u5206\u6790\u663e\u793a\uff0c\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u5728\u804c\u4e1a\u53d9\u8ff0\u4e2d\u5973\u6027\u89d2\u8272\u7684\u6bd4\u4f8b\u8fc7\u9ad8\u3002\u6b64\u5916\uff0cLLM\u751f\u6210\u7684\u804c\u4e1a\u6027\u522b\u6392\u540d\u66f4\u63a5\u8fd1\u4eba\u7c7b\u523b\u677f\u5370\u8c61\uff0c\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u52b3\u52a8\u529b\u7edf\u8ba1\u6570\u636e\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u9700\u8981\u5e73\u8861\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\uff0c\u540c\u65f6\u907f\u514d\u5f3a\u5316\u65b0\u7684\u523b\u677f\u5370\u8c61\u3002"}}
{"id": "2503.15742", "pdf": "https://arxiv.org/pdf/2503.15742", "abs": "https://arxiv.org/abs/2503.15742", "authors": ["Sarosij Bose", "Arindam Dutta", "Sayak Nag", "Junge Zhang", "Jiachen Li", "Konstantinos Karydis", "Amit K. Roy Chowdhury"], "title": "Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes", "categories": ["cs.CV"], "comment": "13 pages, 7 figures", "summary": "Reconstructing 3D scenes from a single image is a fundamentally ill-posed\ntask due to the severely under-constrained nature of the problem. Consequently,\nwhen the scene is rendered from novel camera views, existing single image to 3D\nreconstruction methods render incoherent and blurry views. This problem is\nexacerbated when the unseen regions are far away from the input camera. In this\nwork, we address these inherent limitations in existing single image-to-3D\nscene feedforward networks. To alleviate the poor performance due to\ninsufficient information beyond the input image's view, we leverage a strong\ngenerative prior in the form of a pre-trained latent video diffusion model, for\niterative refinement of a coarse scene represented by optimizable Gaussian\nparameters. To ensure that the style and texture of the generated images align\nwith that of the input image, we incorporate on-the-fly Fourier-style transfer\nbetween the generated images and the input image. Additionally, we design a\nsemantic uncertainty quantification module that calculates the per-pixel\nentropy and yields uncertainty maps used to guide the refinement process from\nthe most confident pixels while discarding the remaining highly uncertain ones.\nWe conduct extensive experiments on real-world scene datasets, including\nin-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach\ncan provide more realistic and high-fidelity novel view synthesis results\ncompared to existing state-of-the-art methods.", "AI": {"task": "\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa3D\u573a\u666f\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u5f20\u56fe\u50cf\u52303D\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u751f\u6210\u65b0\u89c6\u89d2\u65f6\u5f80\u5f80\u5448\u73b0\u4e0d\u8fde\u8d2f\u548c\u6a21\u7cca\u7684\u89c6\u56fe\uff0c\u5c24\u5176\u662f\u5728\u8f93\u5165\u76f8\u673a\u89c6\u89d2\u4e4b\u5916\u7684\u533a\u57df\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\uff0c\u901a\u8fc7\u53ef\u4f18\u5316\u7684\u9ad8\u65af\u53c2\u6570\u5bf9\u7c97\u7cd9\u573a\u666f\u8fdb\u884c\u8fed\u4ee3\u7ec6\u5316\uff0c\u5e76\u7ed3\u5408\u5373\u65f6\u5085\u91cc\u53f6\u98ce\u683c\u8fc1\u79fb\u548c\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6a21\u5757\u6765\u6307\u5bfc\u7ec6\u5316\u8fc7\u7a0b\u3002", "result": "\u5728RealEstate-10K\u548cKITTI-v2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u771f\u5b9e\u548c\u9ad8\u4fdd\u771f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u751f\u6210\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6a21\u5757\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5355\u5f20\u56fe\u50cf\u52303D\u573a\u666f\u91cd\u5efa\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u65b0\u89c6\u89d2\u65f6\u7684\u8868\u73b0\u3002"}}
{"id": "2503.15924", "pdf": "https://arxiv.org/pdf/2503.15924", "abs": "https://arxiv.org/abs/2503.15924", "authors": ["Peiyi Lin", "Fukai Zhang", "Kai Niu", "Hao Fu"], "title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u52a8\u6001\u8fc7\u6ee4\u8f93\u5165\u6570\u636e\u4ee5\u51cf\u5c11\u5197\u4f59\u6570\u636e\u3002", "motivation": "\u5728\u7279\u5b9a\u9886\u57df\u80cc\u666f\u4e0b\uff0c\u4fdd\u6301\u6570\u636e\u8d28\u91cf\u548c\u7ba1\u7406\u7cfb\u7edf\u7ea6\u675f\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u5c0f\u578b\u4ee3\u7406\u6a21\u578b\u8fdb\u884c\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u9ad8\u6548\u8fc7\u6ee4\uff0c\u5e76\u66f4\u65b0\u4ee3\u7406\u4ee5\u786e\u4fdd\u8fc7\u6ee4\u6807\u51c6\u4e0e\u90e8\u7f72\u6a21\u578b\u7684\u6f14\u53d8\u72b6\u6001\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u533b\u7597\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u7cfb\u7edf\uff0c\u51cf\u5c11\u4e8666.7%\u7684\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5904\u7406\u4e86\u589e\u91cf\u83b7\u53d6\u7684\u6570\u636e\u548c\u5206\u5e03\u53d8\u5316\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2503.15761", "pdf": "https://arxiv.org/pdf/2503.15761", "abs": "https://arxiv.org/abs/2503.15761", "authors": ["Mir Mohammad Khaleghi", "Mehran Safayani", "Abdolreza Mirzaei"], "title": "GraPLUS: Graph-based Placement Using Semantics for Image Composition", "categories": ["cs.CV"], "comment": "17 pages, 3 figures, 6 tables", "summary": "We present GraPLUS (Graph-based Placement Using Semantics), a novel framework\nfor plausible object placement in images that leverages scene graphs and large\nlanguage models. Our approach uniquely combines graph-structured scene\nrepresentation with semantic understanding to determine contextually\nappropriate object positions. The framework employs GPT-2 to transform\ncategorical node and edge labels into rich semantic embeddings that capture\nboth definitional characteristics and typical spatial contexts, enabling\nnuanced understanding of object relationships and placement patterns. GraPLUS\nachieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA\ndataset, outperforming state-of-the-art methods by 8.1% while maintaining\ncompetitive visual quality. In human evaluation studies involving 964 samples\nassessed by 19 participants, our method was preferred in 52.1% of cases,\nsignificantly outperforming previous approaches. The framework's key\ninnovations include: (i) leveraging pre-trained scene graph models that\ntransfer knowledge from other domains, (ii) edge-aware graph neural networks\nthat process scene semantics through structured relationships, (iii) a\ncross-modal attention mechanism that aligns categorical embeddings with\nenhanced scene features, and (iv) a multiobjective training strategy\nincorporating semantic consistency constraints.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u548c\u8bed\u4e49\u7406\u89e3\u7684\u65b0\u6846\u67b6GraPLUS\uff0c\u7528\u4e8e\u56fe\u50cf\u4e2d\u5408\u7406\u7684\u7269\u4f53\u653e\u7f6e\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u56fe\u7ed3\u6784\u7684\u573a\u666f\u8868\u793a\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u786e\u5b9a\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u7269\u4f53\u4f4d\u7f6e\u3002", "method": "\u5229\u7528GPT-2\u5c06\u5206\u7c7b\u8282\u70b9\u548c\u8fb9\u6807\u7b7e\u8f6c\u6362\u4e3a\u4e30\u5bcc\u7684\u8bed\u4e49\u5d4c\u5165\uff0c\u6355\u6349\u5b9a\u4e49\u7279\u5f81\u548c\u5178\u578b\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u7406\u89e3\u7269\u4f53\u5173\u7cfb\u548c\u653e\u7f6e\u6a21\u5f0f\u3002", "result": "\u5728OPA\u6570\u636e\u96c6\u4e0a\uff0cGraPLUS\u5b9e\u73b0\u4e8692.1%\u7684\u653e\u7f6e\u51c6\u786e\u7387\u548c28.83\u7684FID\u5206\u6570\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd58.1%\u3002\u5728\u4eba\u7c7b\u8bc4\u4f30\u7814\u7a76\u4e2d\uff0c52.1%\u7684\u60c5\u51b5\u4e0b\u88ab\u4f18\u5148\u9009\u62e9\u3002", "conclusion": "GraPLUS\u6846\u67b6\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u573a\u666f\u56fe\u6a21\u578b\u3001\u8fb9\u7f18\u611f\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u76ee\u6807\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u4f53\u653e\u7f6e\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2503.15944", "pdf": "https://arxiv.org/pdf/2503.15944", "abs": "https://arxiv.org/abs/2503.15944", "authors": ["Jinyi Liu", "Yan Zheng", "Rong Cheng", "Qiyu Wu", "Wei Guo", "Fei Ni", "Hebin Liang", "Yifu Yuan", "Hangyu Mao", "Fuzheng Zhang", "Jianye Hao"], "title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aAtomic Reasoner (AR)\u7684\u8ba4\u77e5\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u788e\u7247\u5316\u601d\u7ef4\u6d41\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u65b9\u9762\u5b58\u5728\u601d\u7ef4\u6d41\u788e\u7247\u5316\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u63a8\u7406\u80fd\u529b\u3002", "method": "AR\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u539f\u5b50\u8ba4\u77e5\u5355\u5143\uff0c\u5e76\u91c7\u7528\u8ba4\u77e5\u8def\u7531\u673a\u5236\u52a8\u6001\u6784\u5efa\u63a8\u7406\u8868\u793a\u548c\u7f16\u6392\u63a8\u7406\u8def\u5f84\uff0c\u5b9e\u73b0\u9010\u6b65\u3001\u7ed3\u6784\u5316\u7684\u8ba4\u77e5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAR\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u5728\u8bed\u8a00\u903b\u8f91\u8c1c\u9898\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "AR\u6709\u6548\u589e\u5f3a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u903b\u8f91\u63a8\u7406\u548c\u6df1\u601d\u719f\u8651\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2503.15763", "pdf": "https://arxiv.org/pdf/2503.15763", "abs": "https://arxiv.org/abs/2503.15763", "authors": ["Huan Lei"], "title": "OffsetOPT: Explicit Surface Reconstruction without Normals", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Neural surface reconstruction has been dominated by implicit representations\nwith marching cubes for explicit surface extraction. However, those methods\ntypically require high-quality normals for accurate reconstruction. We propose\nOffsetOPT, a method that reconstructs explicit surfaces directly from 3D point\nclouds and eliminates the need for point normals. The approach comprises two\nstages: first, we train a neural network to predict surface triangles based on\nlocal point geometry, given uniformly distributed training point clouds. Next,\nwe apply the frozen network to reconstruct surfaces from unseen point clouds by\noptimizing a per-point offset to maximize the accuracy of triangle predictions.\nCompared to state-of-the-art methods, OffsetOPT not only excels at\nreconstructing overall surfaces but also significantly preserves sharp surface\nfeatures. We demonstrate its accuracy on popular benchmarks, including\nsmall-scale shapes and large-scale open surfaces.", "AI": {"task": "\u4ece3D\u70b9\u4e91\u76f4\u63a5\u91cd\u5efa\u663e\u5f0f\u8868\u9762\uff0c\u6d88\u9664\u5bf9\u70b9\u6cd5\u7ebf\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6cd5\u7ebf\u6765\u8fdb\u884c\u51c6\u786e\u91cd\u5efa\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51faOffsetOPT\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u57fa\u4e8e\u5c40\u90e8\u70b9\u51e0\u4f55\u9884\u6d4b\u8868\u9762\u4e09\u89d2\u5f62\uff0c\u7136\u540e\u5728\u672a\u89c1\u8fc7\u7684\u70b9\u4e91\u4e0a\u5e94\u7528\u51bb\u7ed3\u7f51\u7edc\uff0c\u901a\u8fc7\u4f18\u5316\u6bcf\u70b9\u504f\u79fb\u6765\u6700\u5927\u5316\u4e09\u89d2\u5f62\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "OffsetOPT\u5728\u91cd\u5efa\u6574\u4f53\u8868\u9762\u548c\u663e\u8457\u4fdd\u7559\u5c16\u9510\u8868\u9762\u7279\u5f81\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OffsetOPT\u5728\u5305\u62ec\u5c0f\u5c3a\u5ea6\u5f62\u72b6\u548c\u5927\u5c3a\u5ea6\u5f00\u653e\u8868\u9762\u5728\u5185\u7684\u6d41\u884c\u57fa\u51c6\u4e0a\u5c55\u793a\u4e86\u5176\u51c6\u786e\u6027\u3002"}}
{"id": "2503.15952", "pdf": "https://arxiv.org/pdf/2503.15952", "abs": "https://arxiv.org/abs/2503.15952", "authors": ["Chen Li", "Nazhou Liu", "Kai Yang"], "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning", "categories": ["cs.CL"], "comment": "This is an unfinished version and will be updated. We aim to share\n  some findings", "summary": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5373\u81ea\u9002\u5e94\u7ec4\u7b56\u7565\u4f18\u5316\uff08AGPO\uff09\u3002", "motivation": "\u53d1\u73b0\u73b0\u6709\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u6539\u8fdb\uff1a\u4e00\u79cd\u4fee\u8ba2\u7684\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u96f6\u65b9\u5dee\u60c5\u51b5\uff1b\u4e00\u79cd\u57fa\u4e8e\u957f\u5ea6\u7684\u5956\u52b1\uff0c\u6fc0\u52b1\u6a21\u578b\u907f\u514d\u8fc7\u5ea6\u601d\u8003\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u4f7f\u7528\u7684token\u6570\u91cf\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u81ea\u9002\u5e94\u7ec4\u7b56\u7565\u4f18\u5316\uff08AGPO\uff09\u5728\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2503.15778", "pdf": "https://arxiv.org/pdf/2503.15778", "abs": "https://arxiv.org/abs/2503.15778", "authors": ["Boshra Khalili", "Andrew W. Smyth"], "title": "AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In autonomous driving, open-ended question answering often suffers from\nunreliable evaluations because freeform responses require either complex\nmetrics or subjective human judgment. To address this challenge, we introduce\nAutoDrive-QA, an automatic pipeline that converts existing driving QA datasets\n(including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice\nquestion (MCQ) format. This benchmark systematically assesses perception,\nprediction, and planning tasks, providing a standardized and objective\nevaluation framework. AutoDrive-QA employs an automated pipeline that leverages\nlarge language models (LLMs) to generate high-quality, contextually relevant\ndistractors based on domain-specific error patterns commonly found in\nautonomous driving scenarios. To evaluate both general capabilities and\ngeneralization performance, we test the benchmark on three public datasets and\nconduct zero-shot experiments on an unseen dataset. The zero-shot evaluations\nreveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in\nPerception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that\nwhile all models excel in Perception, they struggle in Prediction.\nConsequently, AutoDrive-QA establishes a rigorous, unbiased standard for\nintegrating and evaluating different vision-language models across various\nautonomous driving datasets, thereby improving generalization in this field. We\nrelease all the codes in the AutoDrive-QA GitHub Repository.", "AI": {"task": "\u5c06\u73b0\u6709\u7684\u9a7e\u9a76\u95ee\u7b54\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u591a\u9009\u9898\u683c\u5f0f\uff0c\u4ee5\u63d0\u4f9b\u6807\u51c6\u5316\u548c\u5ba2\u89c2\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f00\u653e\u5f0f\u95ee\u7b54\u8bc4\u4f30\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u81ea\u7531\u5f62\u5f0f\u7684\u56de\u7b54\u9700\u8981\u590d\u6742\u7684\u5ea6\u91cf\u6807\u51c6\u6216\u4e3b\u89c2\u7684\u4eba\u7c7b\u5224\u65ad\u3002", "method": "\u5f15\u5165AutoDrive-QA\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u7ba1\u9053\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u5e72\u6270\u9879\uff0c\u57fa\u4e8e\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5e38\u89c1\u7684\u9886\u57df\u7279\u5b9a\u9519\u8bef\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u57fa\u51c6\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u5b9e\u9a8c\u3002GPT-4V\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\u4ee569.57%\u7684\u51c6\u786e\u7387\u9886\u5148\uff0c\u5176\u4e2d\u611f\u77e5\u4efb\u52a1\u8fbe\u523074.94%\uff0c\u9884\u6d4b\u4efb\u52a1\u8fbe\u523065.33%\uff0c\u89c4\u5212\u4efb\u52a1\u8fbe\u523068.45%\u3002", "conclusion": "AutoDrive-QA\u4e3a\u6574\u5408\u548c\u8bc4\u4f30\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u3001\u65e0\u504f\u7684\u6807\u51c6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8be5\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2503.15979", "pdf": "https://arxiv.org/pdf/2503.15979", "abs": "https://arxiv.org/abs/2503.15979", "authors": ["Navneet Agarwal", "Kairit Sirts"], "title": "Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, there has been growing interest in studying cognitive\ndistortions and emotional appraisals from both computational and psychological\nperspectives. Despite considerable similarities between emotional reappraisal\nand cognitive reframing as emotion regulation techniques, these concepts have\nlargely been examined in isolation. This research explores the relationship\nbetween cognitive distortions and emotional appraisal dimensions, examining\ntheir potential connections and relevance for future interdisciplinary studies.\nUnder this pretext, we conduct an exploratory computational study, aimed at\ninvestigating the relationship between cognitive distortion and emotional\nappraisals. We show that the patterns of statistically significant\nrelationships between cognitive distortions and appraisal dimensions vary\nacross different distortion categories, giving rise to distinct appraisal\nprofiles for individual distortion classes. Additionally, we analyze the impact\nof cognitive restructuring on appraisal dimensions, exemplifying the emotion\nregulation aspect of cognitive restructuring.", "AI": {"task": "\u63a2\u7d22\u8ba4\u77e5\u626d\u66f2\u4e0e\u60c5\u611f\u8bc4\u4f30\u7ef4\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u8ba4\u77e5\u91cd\u6784\u5bf9\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u60c5\u611f\u91cd\u8bc4\u548c\u8ba4\u77e5\u91cd\u6784\u4f5c\u4e3a\u60c5\u7eea\u8c03\u8282\u6280\u672f\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46\u8fd9\u4e9b\u6982\u5ff5\u5927\u591a\u88ab\u5b64\u7acb\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u53ca\u5176\u5bf9\u672a\u6765\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u63a2\u7d22\u6027\u8ba1\u7b97\u7814\u7a76\uff0c\u5206\u6790\u8ba4\u77e5\u626d\u66f2\u4e0e\u60c5\u611f\u8bc4\u4f30\u7ef4\u5ea6\u4e4b\u95f4\u7684\u7edf\u8ba1\u663e\u8457\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u8ba4\u77e5\u91cd\u6784\u5bf9\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u626d\u66f2\u7c7b\u522b\u4e0e\u8bc4\u4f30\u7ef4\u5ea6\u4e4b\u95f4\u7684\u7edf\u8ba1\u663e\u8457\u5173\u7cfb\u6a21\u5f0f\u5404\u4e0d\u76f8\u540c\uff0c\u5f62\u6210\u4e86\u4e0d\u540c\u626d\u66f2\u7c7b\u522b\u7684\u72ec\u7279\u8bc4\u4f30\u7279\u5f81\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5c55\u793a\u4e86\u8ba4\u77e5\u91cd\u6784\u5bf9\u8bc4\u4f30\u7ef4\u5ea6\u7684\u5f71\u54cd\uff0c\u4f53\u73b0\u4e86\u8ba4\u77e5\u91cd\u6784\u5728\u60c5\u7eea\u8c03\u8282\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u8ba4\u77e5\u626d\u66f2\u4e0e\u60c5\u611f\u8bc4\u4f30\u7ef4\u5ea6\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u4e86\u8ba4\u77e5\u91cd\u6784\u5728\u60c5\u7eea\u8c03\u8282\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2503.15784", "pdf": "https://arxiv.org/pdf/2503.15784", "abs": "https://arxiv.org/abs/2503.15784", "authors": ["Parham Saremi", "Amar Kumar", "Mohammed Mohammed", "Zahra TehraniNasab", "Tal Arbel"], "title": "RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Foundation Models (VLFM) have shown a tremendous increase in\nperformance in terms of generating high-resolution, photorealistic natural\nimages. While VLFMs show a rich understanding of semantic content across\nmodalities, they often struggle with fine-grained alignment tasks that require\nprecise correspondence between image regions and textual descriptions a\nlimitation in medical imaging, where accurate localization and detection of\nclinical features are essential for diagnosis and analysis. To address this\nissue, we propose a multi-stage architecture where a pre-trained VLFM provides\na cursory semantic understanding, while a reinforcement learning (RL) algorithm\nrefines the alignment through an iterative process that optimizes for\nunderstanding semantic context. The reward signal is designed to align the\nsemantic information of the text with synthesized images. We demonstrate the\neffectiveness of our method on a medical imaging skin dataset where the\ngenerated images exhibit improved generation quality and alignment with prompt\nover the fine-tuned Stable Diffusion. We also show that the synthesized samples\ncould be used to improve disease classifier performance for underrepresented\nsubgroups through augmentation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u9636\u6bb5\u67b6\u6784\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLFM\uff09\u63d0\u4f9b\u7c97\u7565\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u4f18\u5316\u8bed\u4e49\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4ee5\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u4e2d\u56fe\u50cf\u533a\u57df\u4e0e\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u5e94\u95ee\u9898\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff08VLFM\uff09\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u903c\u771f\u7684\u81ea\u7136\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u56fe\u50cf\u533a\u57df\u4e0e\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u7cbe\u786e\u5bf9\u5e94\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u51c6\u786e\u7684\u5b9a\u4f4d\u548c\u68c0\u6d4b\u4e34\u5e8a\u7279\u5f81\u5bf9\u8bca\u65ad\u548c\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u9636\u6bb5\u67b6\u6784\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684VLFM\u548c\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u4f18\u5316\u8bed\u4e49\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5956\u52b1\u4fe1\u53f7\u8bbe\u8ba1\u4e3a\u4f7f\u6587\u672c\u7684\u8bed\u4e49\u4fe1\u606f\u4e0e\u5408\u6210\u56fe\u50cf\u5bf9\u9f50\u3002", "result": "\u5728\u533b\u5b66\u5f71\u50cf\u76ae\u80a4\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5728\u751f\u6210\u8d28\u91cf\u548c\u4e0e\u63d0\u793a\u7684\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u5fae\u8c03\u7684Stable Diffusion\uff0c\u5e76\u4e14\u5408\u6210\u7684\u6837\u672c\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u75be\u75c5\u5206\u7c7b\u5668\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u5b50\u7ec4\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u6709\u6548\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u9ad8\u4e86\u75be\u75c5\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002"}}
{"id": "2503.15983", "pdf": "https://arxiv.org/pdf/2503.15983", "abs": "https://arxiv.org/abs/2503.15983", "authors": ["Tony Zhang", "Rickard Br\u00e4nnvall"], "title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68Q32 (Secondary)", "I.2.6; I.2.7; I.5.1"], "comment": "7 pages, 2 tables", "summary": "This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks.", "AI": {"task": "\u4f18\u5316\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u538b\u7f29\u6280\u672f\u548c\u4e00\u79cd\u65b0\u9896\u7684\u66ff\u4ee3\u6ce8\u610f\u529b\u673a\u5236\u2014\u2014\u6291\u5236\u6ce8\u610f\u529b\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u66ff\u4ee3\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u8282\u7701\u8ba1\u7b97\u548c\u80fd\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u66fc\u54c8\u987f\u8ddd\u79bb\u548cReLU\u6fc0\u6d3b\u51fd\u6570\u66ff\u4ee3\u4f20\u7edf\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u548csoftmax\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5bf9\u6291\u5236\u673a\u5236\u8fdb\u884c\u8fdb\u4e00\u6b65\u8c03\u6574\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728DistilBERT\u67b6\u6784\u4e0a\u8fdb\u884c\u7684\u77e5\u8bc6\u84b8\u998f\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u6291\u5236Transformer\u6a21\u578b\u5728\u6807\u51c6NLP\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecGLUE\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff09\u4e0a\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u591f\u8282\u7701\u8ba1\u7b97\u548c\u80fd\u6e90\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2503.15800", "pdf": "https://arxiv.org/pdf/2503.15800", "abs": "https://arxiv.org/abs/2503.15800", "authors": ["Jingyun Liu", "Daiqin Yang", "Zhenzhong Chen"], "title": "Frequency Enhancement for Image Demosaicking", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Recovering high-frequency textures in image demosaicking remains a\nchallenging issue. While existing methods introduced elaborate spatial learning\nmethods, they still exhibit limited performance. To address this issue, a\nfrequency enhancement approach is proposed. Based on the frequency analysis of\ncolor filter array (CFA)/demosaicked/ground truth images, we propose Dual-path\nFrequency Enhancement Network (DFENet), which reconstructs RGB images in a\ndivide-and-conquer manner through fourier-domain frequency selection. In\nDFENet, two frequency selectors are employed, each selecting a set of frequency\ncomponents for processing along separate paths. One path focuses on generating\nmissing information through detail refinement in spatial domain, while the\nother aims at suppressing undesirable frequencies with the guidance of CFA\nimages in frequency domain. Multi-level frequency supervision with a stagewise\ntraining strategy is employed to further improve the reconstruction\nperformance. With these designs, the proposed DFENet outperforms other\nstate-of-the-art algorithms on different datasets and demonstrates significant\nadvantages on hard cases. Moreover, to better assess algorithms' ability to\nreconstruct high-frequency textures, a new dataset, LineSet37, is contributed,\nwhich consists of 37 artificially designed and generated images. These images\nfeature complex line patterns and are prone to severe visual artifacts like\ncolor moir\\'e after demosaicking. Experiments on LineSet37 offer a more\ntargeted evaluation of performance on challenging cases. The code and dataset\nare available at https://github.com/VelvetReverie/DFENet-demosaicking.", "AI": {"task": "\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u65b9\u6cd5\u6062\u590d\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u4e2d\u7684\u9ad8\u9891\u7eb9\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\u5728\u6062\u590d\u9ad8\u9891\u7eb9\u7406\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u7387\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u8def\u5f84\u9891\u7387\u589e\u5f3a\u7f51\u7edc\uff08DFENet\uff09\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u57df\u9891\u7387\u9009\u62e9\u4ee5\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u5f0f\u91cd\u5efaRGB\u56fe\u50cf\u3002", "result": "DFENet\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u56f0\u96be\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "DFENet\u901a\u8fc7\u9891\u7387\u589e\u5f3a\u548c\u591a\u7ea7\u9891\u7387\u76d1\u7763\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u53bb\u9a6c\u8d5b\u514b\u7684\u6027\u80fd\uff0c\u5e76\u8d21\u732e\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6LineSet37\u7528\u4e8e\u8bc4\u4f30\u7b97\u6cd5\u5728\u9ad8\u9891\u7eb9\u7406\u91cd\u5efa\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2503.15990", "pdf": "https://arxiv.org/pdf/2503.15990", "abs": "https://arxiv.org/abs/2503.15990", "authors": ["Langming Liu", "Haibin Chen", "Yuhao Wang", "Yujin Yuan", "Shilei Liu", "Wenbo Su", "Xiangyu Zhao", "Bo Zheng"], "title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7535\u5b50\u5546\u52a1\u77e5\u8bc6\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u7531\u4e8eLLMs\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u4e8b\u5b9e\u6027\uff08\u5982\u5e7b\u89c9\uff09\u5bf9\u7528\u6237\u4f53\u9a8c\u548c\u6536\u5165\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faECKGBench\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6807\u51c6\u5316\u5de5\u4f5c\u6d41\u7a0b\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u95ee\u7b54\u8303\u5f0f\u63d0\u9ad8\u8bc4\u4f30\u6548\u7387\u3002", "result": "\u901a\u8fc7ECKGBench\u5bf9\u591a\u4e2a\u5148\u8fdbLLMs\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u5206\u6790\u548c\u89c1\u89e3\u3002", "conclusion": "ECKGBench\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLMs\u5728\u7535\u5b50\u5546\u52a1\u77e5\u8bc6\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u5229\u7528LLMs\u8fdb\u884c\u7535\u5b50\u5546\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2503.15816", "pdf": "https://arxiv.org/pdf/2503.15816", "abs": "https://arxiv.org/abs/2503.15816", "authors": ["Abduljaleel Adejumo", "Faegheh Yeganli", "Clifford Broni-bediako", "Aoran Xiao", "Naoto Yokoya", "Mennatullah Siam"], "title": "A Vision Centric Remote Sensing Benchmark", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "6 PAGES, 7 figures, CVPR", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications.", "AI": {"task": "\u7814\u7a76\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9065\u611f\uff08RS\uff09\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u9065\u611f\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u5f0f\uff08RSMMVP\uff09\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u9065\u611f\u9886\u57df\u7684\u5e94\u7528\u76f8\u5bf9\u8f83\u5c11\u3002\u9065\u611f\u56fe\u50cf\u4e0e\u81ea\u7136\u56fe\u50cf\u4e0d\u540c\uff0c\u5177\u6709\u72ec\u7279\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u5b9a\u4f4d\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u9065\u611f\u591a\u6a21\u6001\u89c6\u89c9\u6a21\u5f0f\uff08RSMMVP\uff09\u57fa\u51c6\uff0c\u8bc4\u4f30CLIP-based MLLMs\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u8bc6\u522bCLIP-blind\u5bf9\uff0c\u5373CLIP-based\u6a21\u578b\u9519\u8bef\u5730\u5c06\u9ad8\u76f8\u4f3c\u5ea6\u5206\u6570\u5206\u914d\u7ed9\u89c6\u89c9\u4e0a\u4e0d\u540c\u7684\u9065\u611f\u56fe\u50cf\u3002", "result": "\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u5728\u9065\u611f\u7279\u5b9a\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u663e\u8457\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8eCLIP-based\u89c6\u89c9\u7f16\u7801\u5f31\u70b9\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9002\u5408\u9065\u611f\u5e94\u7528\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2503.16022", "pdf": "https://arxiv.org/pdf/2503.16022", "abs": "https://arxiv.org/abs/2503.16022", "authors": ["Mario Sanz-Guerrero", "Katharina von der Wense"], "title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to the 6th Workshop on Insights from Negative Results in NLP\n  at NAACL 2025", "summary": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research.", "AI": {"task": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u7ea0\u6b63\u6027\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08CICL\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u6311\u6218\u6027\u7684\u793a\u4f8b\u65f6\u5bb9\u6613\u51fa\u9519\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7ea0\u6b63\u6027\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08CICL\uff09\uff0c\u5c06\u6a21\u578b\u7684\u9519\u8bef\u9884\u6d4b\u4e0e\u771f\u5b9e\u7ea0\u6b63\u4e00\u8d77\u7eb3\u5165\u63d0\u793a\u4e2d\uff0c\u4ee5\u901a\u8fc7\u81ea\u6211\u7ea0\u6b63\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCICL\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u8868\u73b0\u4e0d\u5982\u6807\u51c6ICL\uff0c\u968f\u7740\u63d0\u793a\u4e2d\u7ea0\u6b63\u6bd4\u4f8b\u7684\u589e\u52a0\uff0c\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "CICL\u901a\u8fc7\u7834\u574f\u6a21\u578b\u7684\u4efb\u52a1\u7406\u89e3\u5f15\u5165\u6df7\u6dc6\uff0c\u800c\u4e0d\u662f\u6539\u8fdb\u5176\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u6807\u51c6ICL\u4e2d\u5448\u73b0\u66f4\u96be\u793a\u4f8b\u5e76\u4e0d\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u8868\u660e\u793a\u4f8b\u96be\u5ea6\u672c\u8eab\u53ef\u80fd\u4e0d\u662f\u6709\u6548\u9009\u62e9\u7684\u53ef\u9760\u6807\u51c6\u3002"}}
