{"id": "2506.10020", "title": "From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment", "authors": ["Kyubyung Chae", "Hyunbin Jin", "Taesup Kim"], "summary": "Safely aligning large language models (LLMs) often demands extensive\nhuman-labeled preference data, a process that's both costly and time-consuming.\nWhile synthetic data offers a promising alternative, current methods frequently\nrely on complex iterative prompting or auxiliary models. To address this, we\nintroduce Refusal-Aware Adaptive Injection (RAAI), a straightforward,\ntraining-free, and model-agnostic framework that repurposes LLM attack\ntechniques. RAAI works by detecting internal refusal signals and adaptively\ninjecting predefined phrases to elicit harmful, yet fluent, completions. Our\nexperiments show RAAI effectively jailbreaks LLMs, increasing the harmful\nresponse rate from a baseline of 2.15% to up to 61.04% on average across four\nbenchmarks. Crucially, fine-tuning LLMs with the synthetic data generated by\nRAAI improves model robustness against harmful prompts while preserving general\ncapabilities on standard tasks like MMLU and ARC. This work highlights how LLM\nattack methodologies can be reframed as practical tools for scalable and\ncontrollable safety alignment.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10020v1", "AI": {"title_translation": "从威胁到工具：利用拒绝感知注入攻击实现安全对齐", "tldr": "本文提出RAAI框架，利用LLM攻击技术生成有害但流畅的合成数据，以经济高效地对齐LLM，提高模型对有害提示的鲁棒性。", "motivation": "大型语言模型（LLM）的安全对齐通常需要大量人工标注的偏好数据，这既昂贵又耗时。现有合成数据方法通常依赖复杂的迭代提示或辅助模型，效率低下。", "method": "我们引入了拒绝感知自适应注入（RAAI），这是一个简单、无需训练且与模型无关的框架，它重新利用了LLM攻击技术。RAAI通过检测内部拒绝信号并自适应地注入预定义短语来引出有害但流畅的补全。", "result": "实验表明，RAAI能有效越狱LLM，在四个基准测试中，有害响应率从基线的2.15%平均提高到61.04%。更重要的是，使用RAAI生成的合成数据对LLM进行微调，能提高模型对有害提示的鲁棒性，同时保持在MMLU和ARC等标准任务上的通用能力。", "conclusion": "这项工作强调了LLM攻击方法可以被重新构建为可扩展和可控安全对齐的实用工具。", "translation": "安全对齐大型语言模型（LLM）通常需要大量人工标注的偏好数据，这是一个既昂贵又耗时的过程。虽然合成数据提供了一个有前景的替代方案，但当前的方法通常依赖于复杂的迭代提示或辅助模型。为了解决这个问题，我们引入了拒绝感知自适应注入（RAAI），这是一个简单、无需训练且与模型无关的框架，它重新利用了LLM攻击技术。RAAI通过检测内部拒绝信号并自适应地注入预定义短语来引出有害但流畅的补全。我们的实验表明，RAAI能有效越狱LLM，在四个基准测试中，有害响应率从基线的2.15%平均提高到高达61.04%。至关重要的是，使用RAAI生成的合成数据对LLM进行微调，能提高模型对有害提示的鲁棒性，同时保持在MMLU和ARC等标准任务上的通用能力。这项工作强调了LLM攻击方法可以被重新构建为可扩展和可控安全对齐的实用工具。", "summary": "本文提出了一种名为拒绝感知自适应注入（RAAI）的新框架，旨在解决大型语言模型（LLM）安全对齐中对昂贵的人工标注数据和复杂合成数据方法的依赖。RAAI通过重新利用LLM攻击技术，检测模型内部拒绝信号并自适应注入预设短语，从而生成有害但流畅的合成数据。实验证明，RAAI能显著提高LLM的有害响应率，且使用其生成的合成数据进行微调，可有效增强模型对有害提示的鲁棒性，同时不损害其通用能力。这表明LLM攻击方法可作为实现可扩展和可控安全对齐的实用工具。", "keywords": "LLM安全对齐, 合成数据, 注入攻击, 拒绝感知, RAAI", "comments": "RAAI的创新之处在于将LLM攻击技术逆向利用，变“威胁”为“工具”，从而高效生成用于模型对齐的合成数据，避免了传统方法中高昂的人工成本和复杂的迭代过程。这种方法提供了一种新颖且经济高效的LLM安全对齐途径，具有重要的实践意义和研究价值。"}}
{"id": "2506.10022", "title": "LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges", "authors": ["Haoyang Li", "Huan Gao", "Zhiyuan Zhao", "Zhiyu Lin", "Junyu Gao", "Xuelong Li"], "summary": "The widespread adoption of Large Language Models (LLMs) has heightened\nconcerns about their security, particularly their vulnerability to jailbreak\nattacks that leverage crafted prompts to generate malicious outputs. While\nprior research has been conducted on general security capabilities of LLMs,\ntheir specific susceptibility to jailbreak attacks in code generation remains\nlargely unexplored. To fill this gap, we propose MalwareBench, a benchmark\ndataset containing 3,520 jailbreaking prompts for malicious code-generation,\ndesigned to evaluate LLM robustness against such threats. MalwareBench is based\non 320 manually crafted malicious code generation requirements, covering 11\njailbreak methods and 29 code functionality categories. Experiments show that\nmainstream LLMs exhibit limited ability to reject malicious code-generation\nrequirements, and the combination of multiple jailbreak methods further reduces\nthe model's security capabilities: specifically, the average rejection rate for\nmalicious content is 60.93%, dropping to 39.92% when combined with jailbreak\nattack algorithms. Our work highlights that the code security capabilities of\nLLMs still pose significant challenges.", "comment": "Accepted as ACL 2025 main conference", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10022v1", "AI": {"title_translation": "大型语言模型陷入交火：恶意软件请求和越狱挑战", "tldr": "研究发现大型语言模型在恶意代码生成方面容易受到越狱攻击，其安全性仍面临严峻挑战。", "motivation": "现有研究对LLM的普遍安全能力有所涉猎，但其在代码生成方面对越狱攻击的特定脆弱性仍未被充分探索。本文旨在填补这一空白。", "method": "提出了MalwareBench，一个包含3520个用于恶意代码生成的越狱提示的基准数据集。该数据集基于320个手工制作的恶意代码生成需求，涵盖11种越狱方法和29种代码功能类别。通过该数据集评估了主流LLM对这些威胁的鲁棒性。", "result": "实验表明，主流LLM拒绝恶意代码生成请求的能力有限。结合多种越狱方法会进一步降低模型的安全能力：恶意内容的平均拒绝率为60.93%，当与越狱攻击算法结合时，这一比例下降到39.92%。", "conclusion": "LLM的代码安全能力仍然面临重大挑战。", "translation": "大型语言模型（LLM）的广泛应用加剧了对其安全性的担忧，特别是它们易受利用精心设计的提示生成恶意输出的越狱攻击。虽然先前已经对LLM的通用安全能力进行了研究，但它们在代码生成方面对越狱攻击的特定脆弱性仍未得到充分探索。为了填补这一空白，我们提出了MalwareBench，一个包含3520个用于恶意代码生成的越狱提示的基准数据集，旨在评估LLM抵御此类威胁的鲁棒性。MalwareBench基于320个人工制作的恶意代码生成需求，涵盖11种越狱方法和29种代码功能类别。实验表明，主流LLM拒绝恶意代码生成需求的能力有限，并且结合多种越狱方法会进一步降低模型的安全能力：具体而言，恶意内容的平均拒绝率为60.93%，当与越狱攻击算法结合时，这一比例下降到39.92%。我们的工作强调，LLM的代码安全能力仍然构成重大挑战。", "summary": "本文针对大型语言模型（LLMs）在恶意代码生成方面对越狱攻击的脆弱性进行了研究。作者提出了MalwareBench，一个包含3520个越狱提示的基准数据集，用于评估LLMs抵御此类威胁的鲁棒性。实验结果显示，主流LLMs在拒绝恶意代码生成请求方面的能力有限，且结合多种越狱方法会显著降低其安全性，揭示了LLMs在代码安全方面存在的严峻挑战。", "keywords": "大型语言模型, 越狱攻击, 恶意代码生成, 安全性, MalwareBench", "comments": "这篇论文通过构建一个专门针对恶意代码生成的越狱攻击基准数据集MalwareBench，填补了LLM在代码安全领域研究的空白。其创新之处在于系统性地评估了LLM在生成恶意代码时的脆弱性，并量化了多种越狱方法对模型安全能力的削弱作用。这项工作对于理解和提升LLM在安全敏感应用中的鲁棒性具有重要意义，尤其是在代码生成领域。"}}
{"id": "2506.10024", "title": "Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models", "authors": ["Elena Sofia Ruzzetti", "Giancarlo A. Xompero", "Davide Venditti", "Fabio Massimo Zanzotto"], "summary": "Large Language Models (LLMs) memorize, and thus, among huge amounts of\nuncontrolled data, may memorize Personally Identifiable Information (PII),\nwhich should not be stored and, consequently, not leaked. In this paper, we\nintroduce Private Memorization Editing (PME), an approach for preventing\nprivate data leakage that turns an apparent limitation, that is, the LLMs'\nmemorization ability, into a powerful privacy defense strategy. While attacks\nagainst LLMs have been performed exploiting previous knowledge regarding their\ntraining data, our approach aims to exploit the same kind of knowledge in order\nto make a model more robust. We detect a memorized PII and then mitigate the\nmemorization of PII by editing a model knowledge of its training data. We\nverify that our procedure does not affect the underlying language model while\nmaking it more robust against privacy Training Data Extraction attacks. We\ndemonstrate that PME can effectively reduce the number of leaked PII in a\nnumber of configurations, in some cases even reducing the accuracy of the\nprivacy attacks to zero.", "comment": "To be published at ACL 2025 (Main)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10024v1", "AI": {"title_translation": "私有记忆编辑：将记忆转化为防御以增强大型语言模型中的数据隐私", "tldr": "引入私有记忆编辑（PME），将大型语言模型（LLM）的记忆能力转化为一种隐私防御策略，通过编辑模型对训练数据的知识来防止个人身份信息（PII）泄露。", "motivation": "大型语言模型（LLM）会记忆大量数据，包括个人身份信息（PII），这可能导致PII的存储和泄露问题。", "method": "本文提出私有记忆编辑（PME）方法，旨在将大型语言模型（LLM）的记忆能力转化为一种强大的隐私防御策略。具体而言，该方法通过检测被记忆的个人身份信息（PII），然后编辑模型对训练数据的知识来减轻PII的记忆化。", "result": "PME程序不影响底层语言模型，同时使其对隐私训练数据提取攻击更具鲁棒性。PME可以有效减少泄露的PII数量，在某些情况下甚至将隐私攻击的准确性降低到零。", "conclusion": "私有记忆编辑（PME）能够有效地利用大型语言模型（LLM）的记忆能力，将其转化为一种强大的隐私防御机制，显著降低个人身份信息（PII）泄露风险，并增强模型对抗数据提取攻击的鲁棒性。", "translation": "大型语言模型（LLM）具有记忆能力，因此在海量不受控数据中，可能会记忆个人身份信息（PII），这些信息不应被存储，从而也不应被泄露。本文介绍了私有记忆编辑（PME），这是一种防止私人数据泄露的方法，它将一个明显的局限性，即LLM的记忆能力，转化为一种强大的隐私防御策略。虽然针对LLM的攻击是利用其训练数据的先前知识进行的，但我们的方法旨在利用相同类型的知识来使模型更具鲁棒性。我们检测被记忆的PII，然后通过编辑模型对其训练数据的知识来减轻PII的记忆化。我们验证了我们的程序不会影响底层语言模型，同时使其对隐私训练数据提取攻击更具鲁棒性。我们证明PME可以在多种配置下有效减少泄露的PII数量，在某些情况下甚至将隐私攻击的准确性降低到零。", "summary": "本文提出了一种名为私有记忆编辑（PME）的新方法，旨在解决大型语言模型（LLMs）记忆并可能泄露个人身份信息（PII）的问题。PME将LLMs的记忆能力从一个弱点转化为一种隐私防御策略，通过检测和编辑模型对训练数据中PII的记忆来减轻泄露风险。实验结果表明，PME在不影响模型基础性能的前提下，显著增强了LLMs对抗隐私数据提取攻击的鲁棒性，有效减少了PII泄露，甚至在某些情况下将攻击成功率降至零。", "keywords": "大型语言模型, 隐私, 记忆化, 个人身份信息, 数据隐私", "comments": "这篇论文的创新之处在于它逆向思维，将LLM的“记忆”这一固有特性从隐私泄露的风险源转化为防御机制。这种“化腐朽为神奇”的思路为LLM隐私保护提供了一个新颖且有效的方向。其重要性在于它直接解决了LLM在实际应用中面临的PII泄露这一核心隐私挑战。"}}
{"id": "2506.10025", "title": "Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers", "authors": ["Yuanhaur Chang", "Oren Heller", "Yaniv Shlomo", "Iddo Bar-Noy", "Ella Bokobza", "Michal Grinstein-Weiss", "Ning Zhang"], "summary": "Key decision-makers in small and medium businesses (SMBs) often lack the\nawareness and knowledge to implement cybersecurity measures effectively. To\ngain a deeper understanding of how SMB executives navigate cybersecurity\ndecision-making, we deployed a mixed-method approach, conducting\nsemi-structured interviews (n=21) and online surveys (n=322) with SMB key\ndecision-makers. Using thematic analysis, we revealed SMB decision-makers'\nperceived risks in terms of the digital assets they valued, and found reasons\nfor their choice of defense measures and factors impacting security perception.\nWe employed the situational awareness model to characterize decision-makers\nbased on cybersecurity awareness, identifying those who have comparatively low\nawareness in the fight against adversaries. We further explored the\nrelationship between awareness and business attributes, and constructed a\nholistic structural equation model to understand how awareness can be improved.\nFinally, we proposed interventions to help SMBs overcome potential challenges.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10025v1", "AI": {"title_translation": "注意差距：通过中小型企业关键决策者的态势感知揭示安全障碍", "tldr": "本研究通过混合方法深入分析了中小型企业（SMB）关键决策者在网络安全方面的意识、知识和决策过程，揭示了其面临的安全障碍，并提出了旨在提高意识和克服挑战的干预措施。", "motivation": "中小型企业（SMB）的关键决策者通常缺乏有效实施网络安全措施的意识和知识。本研究旨在深入了解SMB高管如何进行网络安全决策，以弥补这一“差距”。", "method": "采用混合方法：对21位SMB关键决策者进行半结构化访谈，并对322位SMB关键决策者进行在线调查。使用主题分析揭示感知风险、防御措施选择原因和影响安全感知的因素。运用态势感知模型刻画决策者，并构建整体结构方程模型以理解如何提高意识。", "result": "揭示了SMB决策者对其重视的数字资产的感知风险；发现了他们选择防御措施的原因以及影响安全感知的因素；识别出在对抗威胁方面意识相对较低的决策者；探讨了意识与业务属性之间的关系。", "conclusion": "提出了帮助中小型企业克服潜在网络安全挑战的干预措施，以提高其网络安全意识和防御能力。", "translation": "中小型企业（SMB）的关键决策者通常缺乏有效实施网络安全措施的意识和知识。为了更深入地了解SMB高管如何进行网络安全决策，我们部署了混合方法，对SMB关键决策者进行了半结构化访谈（n=21）和在线调查（n=322）。通过主题分析，我们揭示了SMB决策者对其重视的数字资产的感知风险，并找到了他们选择防御措施的原因以及影响安全感知的因素。我们采用态势感知模型来根据网络安全意识对决策者进行特征描述，识别出那些在对抗威胁方面意识相对较低的人。我们进一步探讨了意识与业务属性之间的关系，并构建了一个整体结构方程模型来理解如何提高意识。最后，我们提出了干预措施，以帮助SMB克服潜在的挑战。", "summary": "本研究通过对中小型企业（SMB）关键决策者的混合方法调查（访谈和在线问卷），深入探讨了他们在网络安全决策中存在的意识和知识不足问题。研究运用主题分析揭示了决策者对数字资产的风险感知、防御措施选择原因及影响安全感知的因素，并利用态势感知模型识别出网络安全意识薄弱的群体。论文进一步分析了意识与业务属性的关系，构建了结构方程模型以探索提升意识的途径，并最终提出了旨在帮助SMB克服网络安全障碍的实用干预措施。", "keywords": "中小型企业, 网络安全, 决策者, 态势感知, 风险感知", "comments": "本文创新性地结合了定性和定量研究方法，深入探讨了中小型企业网络安全决策者的意识和行为模式。其重要性在于揭示了SMB在网络安全方面的“差距”，并基于态势感知模型提出了实用的干预措施，为提升SMB的网络安全水平提供了理论和实践指导。研究结果对于网络安全教育、政策制定以及SMB自身的安全实践具有重要指导意义。"}}
{"id": "2506.10540", "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation", "authors": ["Haoyuan Shi", "Yunxin Li", "Xinyu Chen", "Longyue Wang", "Baotian Hu", "Min Zhang"], "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.10540v1", "AI": {"title_translation": "AniMaker：基于MCTS驱动的片段生成实现自动化多智能体动画故事创作", "tldr": "AniMaker是一个多智能体框架，利用MCTS驱动的片段生成和故事感知的片段选择，从文本输入自动创建连贯的多场景动画故事，解决了现有视频生成模型在故事连贯性和效率上的挑战。", "motivation": "尽管视频生成模型快速发展，但生成跨多个场景和角色的连贯故事视频仍然具有挑战性。现有方法常将预生成关键帧刚性转换为固定长度片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着单个低质量片段就可能显著降低整个输出动画的逻辑连贯性和视觉连续性。", "method": "我们提出了AniMaker，一个多智能体框架，它包含：导演智能体（故事板生成）、摄影智能体（视频片段生成）、评审智能体（评估）和后期制作智能体（编辑和画外音）。核心技术是摄影智能体中的MCTS-Gen，一种受蒙特卡洛树搜索启发的策略，用于高效生成高质量片段；以及评审智能体中的AniEval，第一个专为多镜头动画评估设计的框架，它通过考虑每个片段与其前后片段的关系来评估故事一致性、动作完成度和动画特定特征。", "result": "实验证明，AniMaker在VBench和我们提出的AniEval框架等流行指标上实现了卓越的质量，同时显著提高了多候选片段生成的效率。", "conclusion": "AniMaker通过其多智能体框架和创新的MCTS-Gen与AniEval组件，有效地解决了长篇动画故事生成中的连贯性和效率问题，使AI生成的叙事动画更接近生产标准。", "translation": "尽管视频生成模型取得了快速进展，但生成跨多个场景和角色的连贯故事视频仍然具有挑战性。当前方法通常将预生成的关键帧僵化地转换为固定长度的片段，导致叙事脱节和节奏问题。此外，视频生成模型固有的不稳定性意味着即使单个低质量片段也可能显著降低整个输出动画的逻辑连贯性和视觉连续性。为了克服这些障碍，我们引入了AniMaker，一个多智能体框架，能够实现高效的多候选片段生成和故事感知的片段选择，从而仅从文本输入创建全局一致且故事连贯的动画。该框架围绕专门的智能体构建，包括用于故事板生成的导演智能体、用于视频片段生成的摄影智能体、用于评估的评审智能体以及用于编辑和画外音的后期制作智能体。AniMaker方法的核心是两个关键技术组件：摄影智能体中的MCTS-Gen，一种受蒙特卡洛树搜索（MCTS）启发的高效策略，可智能地导航候选空间以生成高潜力片段，同时优化资源使用；以及评审智能体中的AniEval，第一个专门为多镜头动画评估设计的框架，它通过考虑每个片段在其前置和后置片段的上下文来评估故事层级一致性、动作完成度和动画特定特征等关键方面。实验表明，AniMaker在包括VBench和我们提出的AniEval框架在内的流行指标上实现了卓越的质量，同时显著提高了多候选生成的效率，推动了AI生成的叙事动画更接近生产标准。", "summary": "AniMaker是一个创新的多智能体框架，旨在解决当前视频生成模型在创建连贯、多场景动画故事时的挑战。它通过整合导演、摄影、评审和后期制作等专业智能体，并引入MCTS-Gen高效生成高质量视频片段，以及AniEval框架进行故事级一致性评估，从而能从文本输入生成全局一致且故事连贯的动画。实验证明，AniMaker在质量和效率上均优于现有方法，推动了AI动画故事创作达到生产级标准。", "keywords": "动画故事创作, 多智能体系统, 蒙特卡洛树搜索, 视频生成, AniEval", "comments": "该论文的创新点在于提出了一个端到端的多智能体框架AniMaker，解决了长篇动画故事生成中连贯性和效率的核心问题。特别是引入MCTS-Gen优化片段生成和AniEval进行上下文感知的多镜头评估，是其技术亮点，显著提升了AI生成动画的质量和实用性。这对于推动AI在创意内容生成领域的应用具有重要意义。"}}
{"id": "2506.10043", "title": "TrioXpert: An automated incident management framework for microservice system", "authors": ["Yongqian Sun", "Yu Luo", "Xidao Wen", "Yuan Yuan", "Xiaohui Nie", "Shenglin Zhang", "Tong Liu", "Xi Luo"], "summary": "Automated incident management plays a pivotal role in large-scale\nmicroservice systems. However, many existing methods rely solely on\nsingle-modal data (e.g., metrics, logs, and traces) and struggle to\nsimultaneously address multiple downstream tasks, including anomaly detection\n(AD), failure triage (FT), and root cause localization (RCL). Moreover, the\nlack of clear reasoning evidence in current techniques often leads to\ninsufficient interpretability. To address these limitations, we propose\nTrioXpert, an end-to-end incident management framework capable of fully\nleveraging multimodal data. TrioXpert designs three independent data processing\npipelines based on the inherent characteristics of different modalities,\ncomprehensively characterizing the operational status of microservice systems\nfrom both numerical and textual dimensions. It employs a collaborative\nreasoning mechanism using large language models (LLMs) to simultaneously handle\nmultiple tasks while providing clear reasoning evidence to ensure strong\ninterpretability. We conducted extensive evaluations on two popular\nmicroservice system datasets, and the experimental results demonstrate that\nTrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),\nFT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10043v1", "AI": {"title_translation": "TrioXpert: 微服务系统自动化事件管理框架", "tldr": "TrioXpert是一个利用多模态数据和LLM的自动化微服务事件管理框架，在异常检测、故障分类和根因定位方面表现出色。", "motivation": "现有微服务事件管理方法主要依赖单一模态数据，难以同时处理异常检测、故障分类和根因定位等多个下游任务。此外，缺乏清晰的推理证据导致可解释性不足。", "method": "TrioXpert是一个端到端的事件管理框架，充分利用多模态数据。它设计了三个独立的、基于不同模态固有特性的数据处理管道，从数值和文本维度全面表征微服务系统的运行状态。它采用大语言模型（LLMs）的协同推理机制，同时处理多任务并提供清晰的推理证据以确保强可解释性。", "result": "在两个流行的微服务系统数据集上进行了广泛评估，实验结果表明TrioXpert在异常检测（AD）任务中性能提升4.7%至57.7%，在故障分类（FT）任务中提升2.1%至40.6%，在根因定位（RCL）任务中提升1.6%至163.1%。", "conclusion": "TrioXpert通过利用多模态数据和LLM的协同推理机制，有效解决了微服务系统中自动化事件管理面临的单一模态依赖、多任务处理困难和可解释性不足的问题，并在各项任务中取得了显著的性能提升。", "translation": "自动化事件管理在大型微服务系统中扮演着关键角色。然而，许多现有方法仅依赖单一模态数据（例如，指标、日志和追踪），并且难以同时处理多个下游任务，包括异常检测（AD）、故障分类（FT）和根因定位（RCL）。此外，当前技术缺乏清晰的推理证据，常常导致可解释性不足。为了解决这些限制，我们提出了TrioXpert，一个能够充分利用多模态数据的端到端事件管理框架。TrioXpert根据不同模态的固有特性设计了三个独立的数据处理管道，从数值和文本维度全面表征微服务系统的运行状态。它采用大语言模型（LLMs）的协同推理机制，同时处理多个任务，并提供清晰的推理证据以确保强大的可解释性。我们在两个流行的微服务系统数据集上进行了广泛评估，实验结果表明TrioXpert在AD（提升4.7%至57.7%）、FT（提升2.1%至40.6%）和RCL（提升1.6%至163.1%）任务中取得了出色的性能。", "summary": "本论文提出了TrioXpert，一个针对微服务系统的自动化事件管理框架，旨在解决现有方法在处理多模态数据、多任务集成及可解释性方面的不足。TrioXpert通过设计多模态数据处理管道并结合大语言模型进行协同推理，实现了对微服务系统运行状态的全面表征，并能同时执行异常检测、故障分类和根因定位。实验证明，TrioXpert在各项任务中均取得了显著的性能提升。", "keywords": "微服务系统, 事件管理, 多模态数据, 大语言模型, 异常检测", "comments": "TrioXpert的创新点在于其多模态数据融合能力和引入大语言模型进行协同推理，这不仅提高了事件管理的自动化水平和准确性，也增强了结果的可解释性，解决了现有技术中的核心痛点。其在多个任务上的显著性能提升，表明了该框架在实际应用中的巨大潜力。"}}
{"id": "2506.10079", "title": "Cybernetic Marionette: Channeling Collective Agency Through a Wearable Robot in a Live Dancer-Robot Duet", "authors": ["Anup Sathya", "Jiasheng Li", "Zeyu Yan", "Adriane Fang", "Bill Kules", "Jonathan David Martin", "Huaishu Peng"], "summary": "We describe DANCE^2, an interactive dance performance in which audience\nmembers channel their collective agency into a dancer-robot duet by voting on\nthe behavior of a wearable robot affixed to the dancer's body. At key moments\nduring the performance, the audience is invited to either continue the\nchoreography or override it, shaping the unfolding interaction through\nreal-time collective input. While post-performance surveys revealed that\nparticipants felt their choices meaningfully influenced the performance, voting\ndata across four public performances exhibited strikingly consistent patterns.\nThis tension between what audience members do, what they feel, and what\nactually changes highlights a complex interplay between agentive behavior, the\nexperience of agency, and power. We reflect on how choreography, interaction\ndesign, and the structure of the performance mediate this relationship,\noffering a live analogy for algorithmically curated digital systems where\nagency is felt, but not exercised.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10079v1", "AI": {"title_translation": "赛博提线木偶：通过可穿戴机器人在现场舞者-机器人二重奏中引导集体能动性", "tldr": "本文描述了一个名为“DANCE^2”的互动舞蹈表演，观众通过投票控制舞者身上可穿戴机器人的行为，探索集体能动性在人机互动中的体验与实际影响之间的张力，并发现观众感受到的影响与实际变化之间存在差异。", "motivation": "本文旨在探索在互动舞蹈表演中，观众的集体能动性如何通过可穿戴机器人影响舞者-机器人二重奏，并研究能动行为、能动体验与实际改变之间的复杂关系，为算法策划的数字系统中能动性被感受而非实际行使的现象提供现场类比。", "method": "研究描述了一个名为“DANCE^2”的互动舞蹈表演。在表演中，观众被邀请通过投票来决定连接在舞者身上的可穿戴机器人的行为，在关键时刻选择继续现有编舞或覆盖它，从而实时塑造正在展开的互动。", "result": "演出后的调查显示，参与者认为他们的选择有意义地影响了表演。然而，四场公开演出的投票数据却表现出惊人的一致模式。这突显了观众所做、所感受和实际改变之间复杂的张力。", "conclusion": "论文反思了编舞、互动设计和表演结构如何调节能动行为、能动体验与权力之间的关系，并将其作为算法策划数字系统中能动性被感受而非实际行使的现场类比。", "translation": "我们描述了DANCE^2，一个互动舞蹈表演，其中观众通过投票决定固定在舞者身上的可穿戴机器人的行为，从而将他们的集体能动性引导到一个舞者-机器人二重奏中。在表演的关键时刻，观众被邀请选择继续编舞或覆盖它，通过实时集体输入塑造正在展开的互动。虽然表演后的调查显示参与者认为他们的选择有意义地影响了表演，但四场公开演出的投票数据却表现出惊人的一致模式。观众所做、所感受和实际改变之间的这种张力突显了能动行为、能动体验与权力之间复杂的相互作用。我们反思了编舞、互动设计和表演结构如何调节这种关系，为算法策划的数字系统中能动性被感受但未被行使提供了一个现场类比。", "summary": "本文介绍了“DANCE^2”互动舞蹈表演，观众通过投票控制舞者身上可穿戴机器人的行为，引导集体能动性。研究发现，尽管参与者主观上感到其选择对表演有影响，但实际投票模式却高度一致，突显了能动行为、能动体验与实际变化之间的复杂张力。该研究通过现场表演为算法策划的数字系统中“有感无实”的能动性提供了一个类比。", "keywords": "集体能动性, 可穿戴机器人, 互动表演, 人机二重奏, 能动体验", "comments": "这项研究通过创新的现场人机交互表演，深刻探讨了集体能动性在数字时代背景下的复杂性。其独特之处在于通过对比观众的主观感受与客观投票数据，揭示了“能动感”与“实际影响”之间的脱节，为理解算法系统中的用户体验提供了宝贵的洞察和生动的类比。"}}
{"id": "2506.10111", "title": "AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components", "authors": ["Abiodun Ganiyu", "Pranshav Gajjar", "Vijay K Shah"], "summary": "The advent of Open Radio Access Networks (O-RAN) has transformed the\ntelecommunications industry by promoting interoperability, vendor diversity,\nand rapid innovation. However, its disaggregated architecture introduces\ncomplex testing challenges, particularly in validating multi-vendor components\nagainst O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as\nthose provided by Open Testing and Integration Centres (OTICs), rely heavily on\nmanual processes, are fragmented and prone to human error, leading to\ninconsistency and scalability issues. To address these limitations, we present\nAI5GTest -- an AI-powered, specification-aware testing framework designed to\nautomate the validation of O-RAN components. AI5GTest leverages a cooperative\nLarge Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and\nDebug-LLM. Gen-LLM automatically generates expected procedural flows for test\ncases based on 3GPP and O-RAN specifications, while Val-LLM cross-references\nsignaling messages against these flows to validate compliance and detect\ndeviations. If anomalies arise, Debug-LLM performs root cause analysis,\nproviding insight to the failure cause. To enhance transparency and\ntrustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the\nGen-LLM presents top-k relevant official specifications to the tester for\napproval before proceeding with validation. Evaluated using a range of test\ncases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest\ndemonstrates a significant reduction in overall test execution time compared to\ntraditional manual methods, while maintaining high validation accuracy.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10111v1", "AI": {"title_translation": "AI5GTest：AI驱动的5G O-RAN组件规范感知自动化测试与验证", "tldr": "AI5GTest是一个AI驱动的测试框架，利用协作式大型语言模型（LLM）自动化验证5G O-RAN组件，显著减少测试时间并保持高精度。", "motivation": "开放无线接入网络（O-RAN）的分解架构引入了复杂的测试挑战，尤其是在根据O-RAN ALLIANCE和3GPP规范验证多供应商组件时。现有框架（如OTICs）严重依赖手动流程，存在碎片化、易出错、不一致和可扩展性问题。", "method": "AI5GTest是一个AI驱动的规范感知测试框架，用于自动化验证O-RAN组件。它利用一个协作式大型语言模型（LLM）框架，包括Gen-LLM、Val-LLM和Debug-LLM。Gen-LLM根据3GPP和O-RAN规范自动生成测试用例的预期流程；Val-LLM交叉引用信令消息以验证合规性；Debug-LLM在出现异常时进行根本原因分析。为增强透明度和可信度，AI5GTest还整合了人机协作机制，Gen-LLM会在验证前向测试人员展示相关官方规范以供批准。", "result": "AI5GTest在O-RAN TIFG和WG5-IOT测试规范的各种测试用例上进行了评估，与传统手动方法相比，显著减少了总测试执行时间，同时保持了高验证准确性。", "conclusion": "AI5GTest通过自动化和AI驱动的方法，有效解决了O-RAN组件测试的复杂性和效率问题，显著提升了验证流程的效率和准确性。", "translation": "开放无线接入网络（O-RAN）的出现通过促进互操作性、供应商多样性和快速创新，改变了电信行业。然而，其分解架构引入了复杂的测试挑战，尤其是在根据O-RAN ALLIANCE和3GPP规范验证多供应商组件时。现有的框架，例如开放测试和集成中心（OTIC）提供的框架，严重依赖手动流程，碎片化且容易出现人为错误，导致不一致和可扩展性问题。为了解决这些限制，我们提出了AI5GTest——一个由AI驱动、规范感知的测试框架，旨在自动化O-RAN组件的验证。AI5GTest利用一个协作式大型语言模型（LLM）框架，该框架由Gen-LLM、Val-LLM和Debug-LLM组成。Gen-LLM根据3GPP和O-RAN规范自动生成测试用例的预期流程，而Val-LLM将信令消息与这些流程进行交叉引用，以验证合规性并检测偏差。如果出现异常，Debug-LLM会执行根本原因分析，提供故障原因的洞察。为了增强透明度和可信度，AI5GTest整合了人机协作机制，其中Gen-LLM向测试人员展示前k个相关的官方规范以供批准，然后继续进行验证。通过O-RAN TIFG和WG5-IOT测试规范中获得的一系列测试用例进行评估，AI5GTest与传统手动方法相比，显著减少了总测试执行时间，同时保持了高验证准确性。", "summary": "AI5GTest是一个AI驱动的框架，旨在解决开放无线接入网络（O-RAN）组件测试中的复杂性挑战。该框架利用Gen-LLM、Val-LLM和Debug-LLM组成的协作式大型语言模型（LLM）架构，实现基于3GPP和O-RAN规范的自动化测试用例生成、合规性验证和故障根本原因分析。AI5GTest还引入了人机协作机制以提高透明度和信任度。评估结果表明，与传统手动方法相比，AI5GTest显著减少了测试执行时间，并保持了高验证精度。", "keywords": "O-RAN, 自动化测试, 大型语言模型, 5G, 规范感知", "comments": "该论文提出的AI5GTest框架通过结合大型语言模型（LLM）和人机协作机制，为O-RAN组件的自动化测试和验证提供了一个创新解决方案。它解决了现有手动测试方法的效率低下和易错性问题，特别是在处理多供应商组件和复杂规范时。LLM在生成测试流程、验证合规性和进行故障分析方面的应用，展现了AI在电信测试领域的重要潜力。"}}
{"id": "2506.10093", "title": "Leveraging LLMs for Mission Planning in Precision Agriculture", "authors": ["Marcos Abel Zuzuárregui", "Stefano Carpin"], "summary": "Robotics and artificial intelligence hold significant potential for advancing\nprecision agriculture. While robotic systems have been successfully deployed\nfor various tasks, adapting them to perform diverse missions remains\nchallenging, particularly because end users often lack technical expertise. In\nthis paper, we present an end-to-end system that leverages large language\nmodels (LLMs), specifically ChatGPT, to enable users to assign complex data\ncollection tasks to autonomous robots using natural language instructions. To\nenhance reusability, mission plans are encoded using an existing IEEE task\nspecification standard, and are executed on robots via ROS2 nodes that bridge\nhigh-level mission descriptions with existing ROS libraries. Through extensive\nexperiments, we highlight the strengths and limitations of LLMs in this\ncontext, particularly regarding spatial reasoning and solving complex routing\nchallenges, and show how our proposed implementation overcomes them.", "comment": "Published in Proceedings of 2025 International Conference on Robotics\n  and Automation (ICRA)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10093v1", "AI": {"title_translation": "利用大型语言模型进行精准农业任务规划", "tldr": "该研究提出一个利用大型语言模型（如ChatGPT）的端到端系统，让非技术用户能通过自然语言指令为精准农业机器人分配复杂任务，并通过实验展示了其优势和局限性。", "motivation": "精准农业中的机器人系统在执行多样化任务时面临挑战，尤其因为终端用户常缺乏技术专业知识，难以适应和编程机器人。", "method": "本文提出了一个端到端系统，利用大型语言模型（特指ChatGPT）使非技术用户能通过自然语言指令为自主机器人分配复杂数据收集任务。为增强可重用性，任务计划采用现有的IEEE任务规范标准编码，并通过ROS2节点在机器人上执行，这些节点将高级任务描述与现有ROS库连接起来。", "result": "通过大量实验，作者强调了大型语言模型在此背景下的优势和局限性，特别是在空间推理和解决复杂路径规划挑战方面，并展示了所提出的实现如何克服这些局限性。", "conclusion": "该系统成功利用大型语言模型使非技术用户能通过自然语言控制精准农业机器人，有效克服了用户专业知识不足的挑战，并展示了LLMs在此类应用中的潜力和局限性。", "translation": "机器人技术和人工智能在推动精准农业方面具有巨大潜力。尽管机器人系统已成功部署执行各种任务，但使其适应执行多样化任务仍然具有挑战性，特别是因为终端用户通常缺乏技术专业知识。在本文中，我们提出了一个端到端系统，该系统利用大型语言模型（特别是ChatGPT），使用户能够通过自然语言指令为自主机器人分配复杂的数据收集任务。为了增强可重用性，任务计划使用现有的IEEE任务规范标准进行编码，并通过ROS2节点在机器人上执行，这些节点将高级任务描述与现有ROS库连接起来。通过广泛的实验，我们强调了大型语言模型在此背景下的优势和局限性，特别是在空间推理和解决复杂路径规划挑战方面，并展示了我们提出的实现如何克服这些局限性。", "summary": "本文提出一个创新的端到端系统，旨在通过整合大型语言模型（LLMs，如ChatGPT）来简化精准农业中机器人任务的规划。该系统允许非技术用户使用自然语言指令为自主机器人分配复杂的数据收集任务。为了确保任务计划的可重用性，系统采用了IEEE任务规范标准进行编码，并通过ROS2节点实现高层任务描述与现有ROS库的桥接执行。实验结果展示了LLMs在该应用场景中的能力与局限性，尤其是在空间推理和路径规划方面，并验证了所提出的实现方案能有效克服这些挑战。", "keywords": "大型语言模型, 精准农业, 机器人, 任务规划, 自然语言处理", "comments": "这篇论文通过将大型语言模型应用于精准农业中的机器人任务规划，解决了非技术用户操作复杂机器人系统的痛点，具有重要的实际应用价值。其创新点在于利用LLMs的自然语言理解能力，结合标准化的任务编码和ROS2执行，有效降低了机器人部署的门槛。论文也坦诚地指出了LLMs在空间推理和复杂路径规划方面的固有局限性，并展示了工程上的解决方案，这使得研究结果更加可靠和具有指导意义。"}}
{"id": "2506.10005", "title": "Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models", "authors": ["Sridhar S", "Nithin A", "Shakeel Rifath", "Vasantha Raj"], "summary": "Advances in generative artificial intelligence have altered multimedia\ncreation, allowing for automatic cinematic video synthesis from text inputs.\nThis work describes a method for creating 60-second cinematic movies\nincorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for\nnarrative structuring, and a hybrid audio pipeline using gTTS and\nYouTube-sourced music. It uses a five-scene framework, which is augmented by\nlinear frame interpolation, cinematic post-processing (e.g., sharpening), and\naudio-video synchronization to provide professional-quality results. It was\ncreated in a GPU-accelerated Google Colab environment using Python 3.11. It has\na dual-mode Gradio interface (Simple and Advanced), which supports resolutions\nof up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA\nmemory management and error handling ensure reliability. The experiments\ndemonstrate outstanding visual quality, narrative coherence, and efficiency,\nfurthering text-to-video synthesis for creative, educational, and industrial\napplications.", "comment": "10 pages, seven figures about Multimodal Cinematic Video Synthesis\n  Using Text-to-Image and Audio Generation Models", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10005v1", "AI": {"title_translation": "多模态电影视频合成：利用文本到图像和音频生成模型", "tldr": "本文提出了一种利用文本到图像和音频生成模型来创建60秒电影视频的方法，结合了Stable Diffusion、GPT-2、混合音频管道、后期处理和音视频同步，实现了高质量、连贯且高效的电影级视频合成。", "motivation": "随着生成式人工智能在多媒体创作领域的进步，本文旨在实现从文本输入自动合成电影级视频。", "method": "该方法使用一个五场景框架，结合Stable Diffusion进行高保真图像合成，GPT-2进行叙事结构化，以及一个混合音频管道（gTTS和YouTube音乐）。它还包括线性帧插值、电影后期处理（如锐化）和音视频同步，以提供专业质量的结果。系统在GPU加速的Google Colab环境中使用Python 3.11创建，具有双模式Gradio界面，支持高达1024x768的分辨率和15-30 FPS的帧率，并进行了CUDA内存管理和错误处理优化。", "result": "实验证明了出色的视觉质量、叙事连贯性和效率。", "conclusion": "该工作进一步推动了文本到视频合成技术，使其适用于创意、教育和工业应用。", "translation": "生成式人工智能的进步改变了多媒体创作，使得从文本输入自动合成电影视频成为可能。这项工作描述了一种创建60秒电影的方法，该方法结合了Stable Diffusion用于高保真图像合成、GPT-2用于叙事结构化，以及一个使用gTTS和YouTube音乐的混合音频管道。它采用了一个五场景框架，并通过线性帧插值、电影后期处理（例如锐化）和音视频同步进行增强，以提供专业质量的结果。它在GPU加速的Google Colab环境中使用Python 3.11创建。它具有双模式Gradio界面（简单和高级），支持高达1024x768的分辨率和15-30 FPS的帧率。CUDA内存管理和错误处理等优化确保了可靠性。实验证明了出色的视觉质量、叙事连贯性和效率，进一步推动了文本到视频合成技术在创意、教育和工业应用中的发展。", "summary": "本文提出了一种多模态电影视频合成方法，通过集成Stable Diffusion、GPT-2和混合音频管道，实现了从文本输入自动生成60秒电影级视频。该系统采用五场景框架，并结合了帧插值、后期处理和音视频同步，以确保专业级质量。其在Google Colab中实现，并展示了在视觉质量、叙事连贯性和效率方面的显著性能，为文本到视频合成在多种应用领域开辟了新途径。", "keywords": "电影视频合成, 多模态, 文本到图像, 音频生成, Stable Diffusion", "comments": "这项工作通过整合多种先进的生成模型（如Stable Diffusion和GPT-2）和精心设计的处理流程（五场景框架、后期处理、音视频同步），在文本到电影视频合成领域取得了显著进展。其创新性在于多模态融合和对电影级质量的追求。在GPU加速环境下实现并提供用户界面，也体现了其工程实用性。"}}
{"id": "2506.10373", "title": "CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs", "authors": ["Jiajun Hu", "Chetan Choppali Sudarshan", "Vidya A. Chhabria", "Aman Arora"], "summary": "Over the years, the chip industry has consistently developed high-performance\nprocessors to address the increasing demands across diverse applications.\nHowever, the rapid expansion of chip production has significantly increased\ncarbon emissions, raising critical concerns about environmental sustainability.\nWhile researchers have previously modeled the carbon footprint (CFP) at both\nsystem and processor levels, a holistic analysis of sustainability trends\nencompassing the entire chip lifecycle remains lacking. This paper presents\nCarbonSet, a comprehensive dataset integrating sustainability and performance\nmetrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and\nassess the design of next-generation processors. Leveraging this dataset, we\nconducted detailed analysis of flagship processors' sustainability trends over\nthe last decade. This paper further highlights that modern processors are not\nyet sustainably designed, with total carbon emissions increasing more than\n50$\\times$ in the past three years due to the surging demand driven by the AI\nboom. Power efficiency remains a significant concern, while advanced process\nnodes pose new challenges requiring to effectively amortize the dramatically\nincreased manufacturing carbon emissions.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10373v1", "AI": {"title_translation": "CarbonSet：一个分析CPU和GPU可持续性趋势并进行基准测试的数据集", "tldr": "CarbonSet是一个新的数据集，用于分析过去十年CPU和GPU的可持续性趋势。研究发现，由于AI需求，处理器总碳排放量在过去三年增加了50多倍，且现代处理器设计仍不具可持续性。", "motivation": "芯片产业的快速发展导致碳排放显著增加，引发了对环境可持续性的严重担忧。尽管已有研究对系统和处理器层面的碳足迹进行建模，但仍缺乏对整个芯片生命周期可持续性趋势的整体分析。", "method": "本文提出了CarbonSet，一个综合性的数据集，整合了过去十年CPU和GPU的可持续性和性能指标。研究人员利用该数据集对旗舰处理器的可持续性趋势进行了详细分析，旨在对下一代处理器设计进行基准测试和评估。", "result": "分析发现，由于AI需求的激增，过去三年处理器总碳排放量增加了50多倍。功耗效率仍然是一个重大问题，同时先进工艺节点带来了新的挑战，需要有效摊销显著增加的制造碳排放。", "conclusion": "现代处理器尚未实现可持续设计，芯片行业的碳排放问题日益严峻，需要关注功耗效率和制造碳排放的摊销。", "translation": "多年来，芯片行业持续开发高性能处理器，以满足各行各业不断增长的需求。然而，芯片生产的快速扩张显著增加了碳排放，引发了对环境可持续性的严重担忧。尽管研究人员此前已在系统和处理器层面模拟了碳足迹（CFP），但仍缺乏对涵盖整个芯片生命周期的可持续性趋势的整体分析。本文提出了CarbonSet，一个综合性的数据集，整合了过去十年CPU和GPU的可持续性和性能指标。CarbonSet旨在对下一代处理器设计进行基准测试和评估。利用该数据集，我们对过去十年旗舰处理器的可持续性趋势进行了详细分析。本文进一步强调，现代处理器尚未实现可持续设计，由于人工智能（AI）热潮驱动的需求激增，过去三年总碳排放量增加了50多倍。功耗效率仍然是一个重大问题，而先进工艺节点带来了新的挑战，需要有效摊销显著增加的制造碳排放。", "summary": "本文介绍了CarbonSet，一个用于分析CPU和GPU可持续性的综合数据集，涵盖了过去十年的性能和环境指标。研究利用该数据集揭示了芯片行业碳排放的严峻趋势，特别是指出由于AI需求的推动，处理器总碳排放量在三年内增长了50多倍。文章强调，当前处理器设计在可持续性方面存在不足，功耗效率和先进工艺节点带来的制造碳排放摊销是亟待解决的问题。", "keywords": "CarbonSet, 可持续性, 碳排放, CPU, GPU", "comments": "本文创新性地提出了CarbonSet数据集，填补了芯片生命周期可持续性分析的空白。其重要性在于通过数据驱动的方式揭示了芯片产业，尤其是AI发展背景下，日益严峻的碳排放问题。研究结果对未来处理器设计和政策制定具有重要的指导意义，促使行业关注绿色计算和可持续发展。"}}
{"id": "2506.09999", "title": "Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion", "authors": ["Yukun Chen", "Zihuan Qiu", "Fanman Meng", "Hongliang Li", "Linfeng Xu", "Qingbo Wu"], "summary": "Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that\nfocus only on vision and text, this paper explores MCIL across vision, audio\nand text modalities, addressing challenges in integrating complementary\ninformation and mitigating catastrophic forgetting. To tackle these issues, we\npropose an MCIL method based on multimodal pre-trained models. Firstly, a\nMultimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts\n(MoE) structure is introduced to achieve effective incremental fine-tuning for\nAudioCLIP. Secondly, to enhance feature discriminability and generalization, we\npropose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking\nthreshold mechanism and a dynamic feature fusion mechanism, along with a\nstrategy to enhance text diversity. Thirdly, a novel multimodal\nclass-incremental contrastive training loss is proposed to optimize cross-modal\nalignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced\nfor comprehensive assessment. Extensive experiments on three multimodal\ndatasets validate the effectiveness of our method.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.09999v1", "AI": {"title_translation": "利用预训练模型在自适应融合下的多模态类增量学习", "tldr": "本文提出了一种新的多模态类增量学习（MCIL）方法，将视觉、音频和文本模态结合，通过引入基于MoE的特征提取器、自适应音频-视觉融合模块、增强文本多样性策略和新的对比学习损失来解决信息集成和灾难性遗忘问题，并引入了新的评估指标，实验证明了其有效性。", "motivation": "传统的MCIL方法仅关注视觉和文本，而忽略了音频模态。该研究旨在解决在视觉、音频和文本多模态MCIL中集成互补信息和减轻灾难性遗忘的挑战。", "method": "1. 引入了基于专家混合（MoE）结构的多模态增量特征提取器（MIFE），以实现AudioCLIP的有效增量微调。2. 提出了自适应音频-视觉融合模块（AAVFM），包含遮罩阈值机制和动态特征融合机制，并结合了增强文本多样性的策略。3. 提出了一种新颖的多模态类增量对比训练损失，以优化MCIL中的跨模态对齐。4. 引入了两个MCIL特有的评估指标用于综合评估。", "result": "在三个多模态数据集上进行了广泛实验，验证了所提出方法的有效性。", "conclusion": "本文提出的方法通过利用预训练模型、引入创新的模块和损失函数，有效地解决了视觉、音频和文本多模态类增量学习中的信息集成和灾难性遗忘挑战，并通过实验得到了验证。", "translation": "与传统仅关注视觉和文本的多模态类增量学习（MCIL）方法不同，本文探索了跨视觉、音频和文本模态的MCIL，解决了整合互补信息和减轻灾难性遗忘的挑战。为了解决这些问题，我们提出了一种基于多模态预训练模型的MCIL方法。首先，引入了基于专家混合（MoE）结构的多模态增量特征提取器（MIFE），以实现AudioCLIP的有效增量微调。其次，为了增强特征判别性和泛化能力，我们提出了一个自适应音频-视觉融合模块（AAVFM），其中包括遮罩阈值机制和动态特征融合机制，以及增强文本多样性的策略。第三，提出了一种新颖的多模态类增量对比训练损失，以优化MCIL中的跨模态对齐。最后，引入了两个MCIL特有的评估指标用于综合评估。在三个多模态数据集上进行了广泛实验，验证了我们方法的有效性。", "summary": "本文提出了一种用于视觉、音频和文本多模态类增量学习（MCIL）的新方法，旨在解决传统方法仅限于视觉和文本的局限性，并应对信息集成和灾难性遗忘的挑战。该方法利用多模态预训练模型，核心组件包括基于专家混合（MoE）的多模态增量特征提取器（MIFE）用于AudioCLIP的微调，一个带有遮罩阈值和动态融合机制的自适应音频-视觉融合模块（AAVFM）以增强特征，以及一种新的多模态类增量对比训练损失以优化跨模态对齐。此外，还引入了两个MCIL特有的评估指标。在三个多模态数据集上的实验验证了该方法的有效性。", "keywords": "多模态类增量学习, 预训练模型, 自适应融合, 灾难性遗忘, 音频视觉文本", "comments": "本文的创新点在于将多模态类增量学习（MCIL）的范围从传统的视觉和文本扩展到包含音频，这显著增加了问题的复杂性和应用潜力。通过引入基于MoE的特征提取器、自适应融合模块和新颖的对比损失，该方法为解决多模态数据集成和灾难性遗忘提供了全面的解决方案。此外，提出MCIL特有的评估指标也提升了研究的严谨性。"}}
{"id": "2506.10248", "title": "Resilience through Automated Adaptive Configuration for Distribution and Replication", "authors": ["Scott D. Stoller", "Balaji Jayasankar", "Yanhong A. Liu"], "summary": "This paper presents a powerful automated framework for making complex systems\nresilient under failures, by optimized adaptive distribution and replication of\ninterdependent software components across heterogeneous hardware components\nwith widely varying capabilities. A configuration specifies how software is\ndistributed and replicated: which software components to run on each computer,\nwhich software components to replicate, which replication protocols to use,\netc. We present an algorithm that, given a system model and resilience\nrequirements, (1) determines initial configurations of the system that are\nresilient, and (2) generates a reconfiguration policy that determines\nreconfiguration actions to execute in response to failures and recoveries. This\nmodel-finding algorithm is based on state-space exploration and incorporates\npowerful optimizations, including a quotient reduction based on a novel\nequivalence relation between states. We present experimental results from\nsuccessfully applying a prototype implementation of our framework to a model of\nan autonomous driving system.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10248v1", "AI": {"title_translation": "通过自动化自适应配置实现分发和复制的弹性", "tldr": "本文提出了一种自动化框架，通过优化软件组件在异构硬件上的自适应分发和复制，使复杂系统在故障下保持弹性。", "motivation": "为了使复杂系统在面对故障时能够保持弹性，特别是在异构硬件环境下，需要一种有效的方法来自动化地管理和优化软件组件的分布和复制。", "method": "本文提出了一种基于状态空间探索的模型查找算法，该算法能够根据系统模型和弹性需求，确定系统的初始弹性配置，并生成在故障和恢复时执行重配置操作的策略。该算法包含强大的优化，包括基于新颖状态等价关系的商约简。", "result": "作者成功地将该框架的原型实现应用于自动驾驶系统模型，并展示了实验结果。", "conclusion": "通过提出的自动化框架和模型查找算法，可以有效地实现复杂系统在故障下的弹性，并通过自适应配置管理软件组件的分发和复制。", "translation": "本文提出了一种强大的自动化框架，通过对具有广泛不同能力的异构硬件组件上的相互依赖的软件组件进行优化的自适应分发和复制，使复杂系统在故障下具有弹性。配置指定了软件如何分发和复制：每个计算机上运行哪些软件组件，复制哪些软件组件，使用哪些复制协议等。我们提出了一种算法，在给定系统模型和弹性要求的情况下，(1) 确定系统初始的弹性配置，以及 (2) 生成一个重配置策略，该策略确定响应故障和恢复时要执行的重配置操作。这种模型查找算法基于状态空间探索，并结合了强大的优化，包括基于状态之间新颖等价关系的商约简。我们展示了成功将我们框架的原型实现应用于自动驾驶系统模型的实验结果。", "summary": "本文提出了一种自动化框架，旨在通过优化软件组件在异构硬件上的自适应分发和复制，增强复杂系统在故障下的弹性。该框架包含一个模型查找算法，能够根据系统模型和弹性需求，确定初始弹性配置并生成应对故障和恢复的重配置策略。该算法利用状态空间探索和优化的商约简技术。作者通过将原型实现应用于自动驾驶系统模型，验证了其有效性。", "keywords": "自动化配置, 系统弹性, 分布式系统, 复制, 故障恢复", "comments": "本文的创新点在于提出了一个自动化框架，能够为复杂系统在异构环境下提供故障弹性，并通过模型查找算法自动化地生成初始弹性配置和重配置策略。其引入的基于新颖等价关系的商约简优化提升了算法效率。该研究对于需要高可用性和韧性的分布式系统（如自动驾驶）具有重要意义。"}}
{"id": "2506.10377", "title": "Chance and Mass Interpretations of Probabilities in Markov Decision Processes (Extended Version)", "authors": ["Yun Chen Tsai", "Kittiphon Phalakarn", "S. Akshay", "Ichiro Hasuo"], "summary": "Markov decision processes (MDPs) are a popular model for decision-making in\nthe presence of uncertainty. The conventional view of MDPs in verification\ntreats them as state transformers with probabilities defined over sequences of\nstates and with schedulers making random choices. An alternative view,\nespecially well-suited for modeling dynamical systems, defines MDPs as\ndistribution transformers with schedulers distributing probability masses. Our\nmain contribution is a unified semantical framework that accommodates these two\nviews and two new ones. These four semantics of MDPs arise naturally through\nidentifying different sources of randomness in an MDP (namely schedulers,\nconfigurations, and transitions) and providing different ways of interpreting\nthese probabilities (called the chance and mass interpretations). These\nsemantics are systematically unified through a mathematical construct called\nchance-mass (CM) classifier. As another main contribution, we study a\nreachability problem in each of the two new semantics, demonstrating their\nhardness and providing two algorithms for solving them.", "comment": null, "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.10377v1", "AI": {"title_translation": "马尔可夫决策过程（扩展版）中概率的机遇与质量解释", "tldr": "本文提出了一个统一的语义框架，以整合和扩展马尔可夫决策过程（MDPs）中概率的现有解释，并研究了两种新语义下的可达性问题。", "motivation": "传统的马尔可夫决策过程（MDPs）有两种看待概率的方式：一种是将MDPs视为状态转换器，概率定义在状态序列上，调度器做出随机选择；另一种是将其定义为分布转换器，调度器分配概率质量。为了统一这两种观点并引入新的解释，本文旨在提供一个统一的语义框架。", "method": "作者通过识别MDP中不同的随机性来源（调度器、配置和转换）并提供不同的概率解释方式（机遇解释和质量解释），自然地导出了四种MDP语义。这些语义通过一种称为机遇-质量（CM）分类器的数学结构得到系统性统一。此外，作者还研究了两种新语义下的可达性问题，并提供了两种算法来解决它们。", "result": "本文成功提出了一个统一的语义框架，该框架容纳了两种现有观点和两种新观点。这四种MDP语义通过机遇-质量（CM）分类器得到了系统性统一。此外，作者还研究了两种新语义下的可达性问题，证明了它们的难度并提供了相应的解决算法。", "conclusion": "本文的主要贡献在于建立了一个统一的语义框架，能够整合并扩展马尔可夫决策过程（MDPs）中概率的多种解释，并通过机遇-质量（CM）分类器实现了系统性统一。此外，对新语义下的可达性问题的研究也为MDPs的验证和建模提供了新的视角和解决方案。", "translation": "马尔可夫决策过程（MDPs）是处理不确定性决策的流行模型。验证领域中MDPs的传统观点将其视为状态转换器，概率定义在状态序列上，调度器做出随机选择。另一种观点，特别适合建模动态系统，将MDPs定义为分布转换器，调度器分配概率质量。我们的主要贡献是一个统一的语义框架，它容纳了这两种观点以及两种新观点。MDPs的这四种语义自然地产生于识别MDP中不同随机性来源（即调度器、配置和转换）并提供不同的概率解释方式（称为机遇解释和质量解释）。这些语义通过一种称为机遇-质量（CM）分类器的数学结构得到系统性统一。作为另一个主要贡献，我们研究了两种新语义中的可达性问题，证明了它们的难度并提供了两种解决算法。", "summary": "本文提出了一个统一的语义框架，旨在整合和扩展马尔可夫决策过程（MDPs）中概率的现有解释。通过识别MDP中随机性的不同来源（调度器、配置、转换）并提供“机遇”和“质量”两种概率解释，该框架自然地导出了四种MDP语义，并通过“机遇-质量（CM）分类器”实现了系统性统一。此外，文章还研究了两种新语义下的可达性问题，证明了其计算难度并提供了相应的解决算法。", "keywords": "马尔可夫决策过程, 概率解释, 统一语义框架, 机遇-质量分类器, 可达性问题", "comments": "本文的创新之处在于提出了一个统一的语义框架和“机遇-质量（CM）分类器”，成功地整合并扩展了马尔可夫决策过程（MDPs）中概率的多种解释。这种统一性对于理解和分析不同建模范式下的MDPs具有重要意义。通过对新语义下可达性问题的研究，该工作也为MDPs的验证和算法开发提供了新的方向和工具。"}}
{"id": "2506.10130", "title": "A Conjecture on a Fundamental Trade-Off between Certainty and Scope in Symbolic and Generative AI", "authors": ["Luciano Floridi"], "summary": "This article introduces a conjecture that formalises a fundamental trade-off\nbetween provable correctness and broad data-mapping capacity in Artificial\nIntelligence (AI) systems. When an AI system is engineered for deductively\nwatertight guarantees (demonstrable certainty about the error-free nature of\nits outputs) -- as in classical symbolic AI -- its operational domain must be\nnarrowly circumscribed and pre-structured. Conversely, a system that can input\nhigh-dimensional data to produce rich information outputs -- as in contemporary\ngenerative models -- necessarily relinquishes the possibility of zero-error\nperformance, incurring an irreducible risk of errors or misclassification. By\nmaking this previously implicit trade-off explicit and open to rigorous\nverification, the conjecture significantly reframes both engineering ambitions\nand philosophical expectations for AI. After reviewing the historical\nmotivations for this tension, the article states the conjecture in\ninformation-theoretic form and contextualises it within broader debates in\nepistemology, formal verification, and the philosophy of technology. It then\noffers an analysis of its implications and consequences, drawing on notions of\nunderdetermination, prudent epistemic risk, and moral responsibility. The\ndiscussion clarifies how, if correct, the conjecture would help reshape\nevaluation standards, governance frameworks, and hybrid system design. The\nconclusion underscores the importance of eventually proving or refuting the\ninequality for the future of trustworthy AI.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10130v1", "AI": {"title_translation": "符号和生成式AI中确定性与范围之间基本权衡的猜想", "tldr": "人工智能系统中，可证明的正确性与广泛的数据映射能力之间存在基本权衡。", "motivation": "本文旨在形式化人工智能系统中可证明的正确性与广泛数据映射能力之间的一个基本权衡，使这个此前隐含的权衡变得显性化并可供严格验证，从而重塑AI的工程抱负和哲学期望。", "method": "本文提出了一个关于确定性与范围之间基本权衡的猜想，并以信息论形式阐述。它回顾了历史动机，将猜想置于认识论、形式验证和技术哲学的更广泛辩论中，并分析了其在不确定性、审慎认知风险和道德责任背景下的含义和后果。", "result": "本文提出了一个关于AI系统中确定性（可证明的正确性）与范围（广泛数据映射能力）之间存在基本权衡的猜想。该猜想指出，为保证确定性，AI系统操作领域必须狭窄；而能处理高维数据的系统则必然放弃零错误性能。如果该猜想正确，将有助于重塑评估标准、治理框架和混合系统设计。", "conclusion": "结论强调了最终证明或驳斥这个不等式对于未来可信赖AI的重要性。", "translation": "这篇论文提出了一个猜想，形式化了人工智能（AI）系统中可证明的正确性与广泛数据映射能力之间的一个基本权衡。当一个AI系统被设计用于提供演绎上滴水不漏的保证（对其输出无错误的明确确定性）——正如在经典符号AI中那样——其操作领域必须被严格限定和预先结构化。相反，一个能够输入高维数据以产生丰富信息输出的系统——正如在当代生成模型中那样——必然放弃零错误性能的可能性，承担不可避免的错误或误分类风险。通过使这个此前隐含的权衡变得显性化并可供严格验证，该猜想显著重塑了AI的工程抱负和哲学期望。在回顾了这种紧张关系的历史动机之后，文章以信息论形式阐述了该猜想，并将其置于认识论、形式验证和技术哲学的更广泛辩论中。然后，它利用不确定性、审慎认知风险和道德责任等概念，分析了其含义和后果。讨论阐明了如果该猜想正确，它将如何帮助重塑评估标准、治理框架和混合系统设计。结论强调了最终证明或驳斥这个不等式对于未来可信赖AI的重要性。", "summary": "本文提出了一个关于符号AI和生成式AI之间确定性与范围的根本性权衡的猜想。该猜想指出，追求可证明的零错误性能（如符号AI）会限制系统的操作范围，而处理高维数据并产生丰富输出（如生成式AI）则必然伴随着不可避免的错误风险。通过将这一隐含的权衡显性化，该研究旨在重塑AI的工程目标和哲学预期，并探讨其对评估标准、治理和混合系统设计的影响。", "keywords": "人工智能, 符号AI, 生成式AI, 确定性, 范围, 权衡", "comments": "这篇论文通过提出一个关于AI系统确定性与范围之间权衡的猜想，为理解当前AI技术（特别是符号AI和生成式AI）的局限性和潜力提供了一个新颖的理论框架。其创新之处在于将这一普遍存在的现象形式化，并将其置于更广泛的哲学和信息论背景下讨论，对未来AI研究方向、系统设计和治理策略具有重要指导意义。"}}
{"id": "2506.10057", "title": "Inverted Classroom in der Einführungsveranstaltung Programmierung", "authors": ["Ulrich von Zadow", "Natalie Kiesler"], "summary": "Traditionally, the introductory programming course for computer science\nstudents at Nuremberg Tech had been implemented as a combination of lectures\nand exercise sessions. Due to high failure rates in the winter semester\n2023/24, an experimental teaching concept based on the inverted classroom was\nimplemented for one cohort in the winter semester 2024/25. Students had to\nprepare themselves through literature work and activating teaching and learning\nmethods. The course was accompanied by a series of data collections (i.e., a\nTeaching Analysis Poll, two surveys, and a teaching diary) to gain insights\ninto students' learning methods and behaviors. The concept was evaluated\npositively overall, although many detailed opportunities for improvement were\nidentified. In this article, we document the results of the surveys and discuss\nthe implications", "comment": "10 pages, in German language, 4 figures, MINT Symposium 2025", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10057v1", "AI": {"title_translation": "倒置课堂在编程入门课程中的应用", "tldr": "纽伦堡理工学院为解决编程入门课高挂科率问题，实验性地引入了倒置课堂，并获得积极评价，但仍有改进空间。", "motivation": "传统编程入门课程挂科率高，促使学校尝试新的教学概念。", "method": "实验性地将倒置课堂应用于一个班级，学生需通过文献和激活教学法进行预习。通过教学分析投票、两次调查和教学日记收集数据，并评估概念。", "result": "整体评价积极，但识别出许多详细的改进机会。", "conclusion": "倒置课堂概念在编程入门课程中获得了积极评价，但仍有改进空间。", "translation": "传统上，纽伦堡理工学院计算机科学专业的编程入门课程采用讲座和练习相结合的方式。由于2023/24冬季学期的高挂科率，2024/25冬季学期为一个班级实验性地实施了基于倒置课堂的教学概念。学生需要通过文献工作和激活教学学习方法进行自我准备。课程伴随一系列数据收集（即教学分析投票、两次调查和教学日记），以深入了解学生的学习方法和行为。该概念总体上获得了积极评价，尽管也发现了许多详细的改进机会。本文记录了调查结果并讨论了其影响。", "summary": "本文介绍了纽伦堡理工学院为解决编程入门课程高挂科率问题，在2024/25冬季学期对一个班级实施倒置课堂教学实验。该实验要求学生提前通过文献和激活教学法进行准备，并通过多种数据收集方法（教学分析投票、调查、教学日记）来了解学生学习情况。尽管该教学概念获得了总体积极的评价，但仍识别出许多可改进的细节。文章记录了调查结果并讨论了其启示。", "keywords": "倒置课堂, 编程入门, 教学实验, 挂科率, 教学评估", "comments": "这项研究通过在实践中应用倒置课堂来解决实际的教学问题（高挂科率），其创新点在于将理论概念应用于具体课程并进行量化评估。其重要性在于为其他面临类似问题的教育机构提供了实践案例和经验。局限性可能在于实验范围仅限于一个班级，且具体改进点未在摘要中详述。"}}
{"id": "2506.10009", "title": "The Iris File Extension", "authors": ["Ryan Erik Landvater", "Michael David Olp", "Mustafa Yousif", "Ulysses Balis"], "summary": "A modern digital pathology vendor-agnostic binary slide format specifically\ntargeting the unmet need of efficient real-time transfer and display has not\nyet been established. Growing adoption of digital pathology only intensifies\nthe need for an intermediary digital slide format with an emphasis on\nperformance for use between slide servers and image management software or for\ninter-institutional transmission of cases. Although the DICOM standard is a\nwell-established format widely used for long-term storage of both images and\ncritically associated metadata, its inherent limitations on maximum image\ndimensions can impact retrieval speed, particularly when accessing whole slide\nimages using a pyramidal structure of slide viewer applications. Here, we\nintroduce the Iris file extension, a binary file container specification\nexplicitly designed for whole slide image systems that can abstract the file\nstructure outline into memory for immediate tile access. The Iris file\nextension adds modern compression support, a dynamic structure with optional\nfile features, computationally trivial deep file validation and corruption\nrecovery capabilities, and slide annotation support. In addition to the file\nspecification document, we provide source code to allow for (de)serialization\nand validation of a binary stream against the standard and corresponding binary\nbuilds with C++, Python, and JavaScript language bindings. We further provide\nfull encoder and decoder implementation source code, as well as binary builds\n(as part of the separate Iris Codec Community module) with language bindings\nfor C++ and Python to allow for easy integration with existing WSI solutions.\nWe provide the Iris File Extension specification openly to the community in the\nform of a Creative Commons Attribution-No Derivative 4.0 international license.", "comment": "8 pages, 6 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10009v1", "AI": {"title_translation": "Iris文件扩展名", "tldr": "本文介绍了Iris文件扩展名，这是一种新的二进制幻灯片格式，专门用于解决数字病理学中全玻片图像的高效实时传输和显示需求，克服了现有格式的局限性，并提供了开源实现。", "motivation": "现代数字病理学缺乏一种供应商无关的二进制玻片格式，能够高效地进行实时传输和显示。随着数字病理学的日益普及，对一种侧重于性能的中间数字玻片格式的需求变得更加迫切，以用于玻片服务器和图像管理软件之间或机构间的病例传输。现有的DICOM标准虽然用于长期存储，但在最大图像尺寸方面存在固有限制，会影响检索速度，尤其是在访问金字塔结构的全玻片图像时。", "method": "本文引入了Iris文件扩展名，这是一种二进制文件容器规范，专门为全玻片图像系统设计。它能够将文件结构大纲抽象到内存中，以实现即时瓦片访问。Iris文件扩展名支持现代压缩、动态结构（包含可选文件特性）、计算上微不足道的深度文件验证和损坏恢复功能，以及玻片注释支持。此外，作者还提供了允许对二进制流进行（反）序列化和验证的源代码，以及C++、Python和JavaScript语言绑定的相应二进制构建。", "result": "Iris文件扩展名是一种新的二进制文件容器规范，具有以下特点：为全玻片图像系统设计、支持将文件结构抽象到内存以实现即时瓦片访问、支持现代压缩、动态结构（带可选文件特性）、计算上微不足道的深度文件验证和损坏恢复功能、玻片注释支持。论文提供了（反）序列化和验证源代码，以及C++、Python、JavaScript的语言绑定二进制构建，以及完整的编码器和解码器实现源代码（作为单独的Iris Codec Community模块的一部分）。", "conclusion": "Iris文件扩展名通过提供一种开放、高效且功能丰富的二进制玻片格式，解决了数字病理学中全玻片图像传输和显示的关键未满足需求，促进了与现有WSI解决方案的轻松集成和社区协作。", "translation": "现代数字病理学中尚未建立一种专门针对高效实时传输和显示这一未满足需求的、与供应商无关的二进制玻片格式。数字病理学日益普及，这只会加剧对一种中间数字玻片格式的需求，该格式应侧重于性能，用于玻片服务器和图像管理软件之间或机构间的病例传输。尽管DICOM标准是一种成熟的格式，广泛用于图像和关键相关元数据的长期存储，但其固有的最大图像尺寸限制会影响检索速度，尤其是在使用玻片查看器应用程序的金字塔结构访问全玻片图像时。在此，我们介绍了Iris文件扩展名，这是一种二进制文件容器规范，明确为全玻片图像系统设计，能够将文件结构大纲抽象到内存中，以实现即时瓦片访问。Iris文件扩展名增加了现代压缩支持、具有可选文件特性的动态结构、计算上微不足道的深度文件验证和损坏恢复功能，以及玻片注释支持。除了文件规范文档，我们还提供了源代码，允许对二进制流进行（反）序列化和验证，并提供了C++、Python和JavaScript语言绑定的相应二进制构建。我们进一步提供了完整的编码器和解码器实现源代码，以及C++和Python语言绑定的二进制构建（作为单独的Iris Codec Community模块的一部分），以便于与现有WSI解决方案轻松集成。我们以知识共享署名-禁止演绎4.0国际许可的形式向社区开放Iris文件扩展名规范。", "summary": "本文介绍了Iris文件扩展名，这是一种专为数字病理学全玻片图像设计的新型供应商无关二进制文件格式。该格式旨在解决现有解决方案在实时传输和显示方面的效率不足问题，特别是DICOM在处理大规模图像时的局限性。Iris文件扩展名支持高效瓦片访问、现代压缩、动态结构、强大的文件验证和恢复功能以及注释支持。作者提供了该规范的开源实现，包括多种编程语言的（反）序列化、验证、编码器和解码器工具，以促进其在数字病理学领域的广泛应用和集成。", "keywords": "数字病理学, 全玻片图像, 文件格式, Iris文件扩展名, 图像传输", "comments": "Iris文件扩展名的创新之处在于它专门针对数字病理学全玻片图像的实时传输和显示需求，填补了现有DICOM标准在处理大规模图像时效率不足的空白。其供应商无关的特性、对现代压缩和动态结构的支持，以及内置的深度文件验证和损坏恢复能力，都显著提升了其实用性和鲁棒性。此外，提供多语言的开源实现（C++, Python, JavaScript）以及开放许可，极大地降低了集成门槛，有望加速其在数字病理学领域的普及和标准化，具有重要的行业推动作用。"}}
{"id": "2506.10289", "title": "RT-VC: Real-Time Zero-Shot Voice Conversion with Speech Articulatory Coding", "authors": ["Yisi Liu", "Chenyang Wang", "Hanjo Kim", "Raniya Khan", "Gopala Anumanchipalli"], "summary": "Voice conversion has emerged as a pivotal technology in numerous applications\nranging from assistive communication to entertainment. In this paper, we\npresent RT-VC, a zero-shot real-time voice conversion system that delivers\nultra-low latency and high-quality performance. Our approach leverages an\narticulatory feature space to naturally disentangle content and speaker\ncharacteristics, facilitating more robust and interpretable voice\ntransformations. Additionally, the integration of differentiable digital signal\nprocessing (DDSP) enables efficient vocoding directly from articulatory\nfeatures, significantly reducing conversion latency. Experimental evaluations\ndemonstrate that, while maintaining synthesis quality comparable to the current\nstate-of-the-art (SOTA) method, RT-VC achieves a CPU latency of 61.4 ms,\nrepresenting a 13.3\\% reduction in latency.", "comment": "ACL Demo Track 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10289v1", "AI": {"title_translation": "RT-VC：基于语音发音编码的实时零样本语音转换", "tldr": "RT-VC是一个实时零样本语音转换系统，利用发音特征和DDSP实现超低延迟和高质量的语音转换，性能与SOTA相当但延迟更低。", "motivation": "语音转换已成为辅助通信和娱乐等众多应用中的关键技术，但现有系统可能存在延迟或质量问题。本文旨在开发一个实时、低延迟、高质量的零样本语音转换系统。", "method": "RT-VC系统利用发音特征空间自然地解耦内容和说话人特征，以实现更鲁棒和可解释的语音转换。此外，它集成了可微分数字信号处理（DDSP），可以直接从发音特征高效地进行声码器编码，显著降低了转换延迟。", "result": "实验评估表明，RT-VC在保持与当前最先进（SOTA）方法相当的合成质量的同时，实现了61.4毫秒的CPU延迟，延迟降低了13.3%。", "conclusion": "RT-VC成功地实现了实时、零样本的语音转换，具有超低延迟和高质量性能，为语音转换技术带来了显著的改进。", "translation": "语音转换已成为辅助通信到娱乐等众多应用中的关键技术。在本文中，我们提出了RT-VC，一个零样本实时语音转换系统，它提供了超低延迟和高质量的性能。我们的方法利用发音特征空间自然地解耦内容和说话人特征，促进了更鲁棒和可解释的语音转换。此外，可微分数字信号处理（DDSP）的集成使得可以直接从发音特征高效地进行声码器编码，显著降低了转换延迟。实验评估表明，在保持与当前最先进（SOTA）方法相当的合成质量的同时，RT-VC实现了61.4毫秒的CPU延迟，延迟降低了13.3%。", "summary": "本文介绍了RT-VC，一个创新的实时零样本语音转换系统。该系统通过利用发音特征空间来有效分离语音内容和说话人特征，并结合可微分数字信号处理（DDSP）进行高效声码器编码，从而实现了超低延迟和高质量的语音转换。实验结果显示，RT-VC在保持与现有最先进方法相当的合成质量的同时，显著降低了CPU延迟达13.3%。", "keywords": "实时语音转换, 零样本, 发音特征, DDSP, 低延迟", "comments": "RT-VC的创新点在于其结合发音特征空间进行内容与说话人分离，以及集成DDSP进行高效声码器编码，这使其能够实现实时、低延迟的零样本语音转换。其在保持高质量的同时显著降低延迟的成果，对于需要即时响应的语音应用具有重要意义。"}}
{"id": "2506.10339", "title": "New Approximation Guarantees for The Inventory Staggering Problem", "authors": ["Noga Alon", "Danny Segev"], "summary": "Since its inception in the mid-60s, the inventory staggering problem has been\nexplored and exploited in a wide range of application domains, such as\nproduction planning, stock control systems, warehousing, and aerospace/defense\nlogistics. However, even with a rich history of academic focus, we are still\nvery much in the dark when it comes to cornerstone computational questions\naround inventory staggering and to related structural characterizations, with\nour methodological toolbox being severely under-stocked.\n  The central contribution of this paper consists in devising a host of\nalgorithmic techniques and analytical ideas -- some being entirely novel and\nsome leveraging well-studied concepts in combinatorics and number theory -- for\nsurpassing essentially all known approximation guarantees for the inventory\nstaggering problem. In particular, our work demonstrates that numerous\nstructural properties open the door for designing polynomial-time approximation\nschemes, including polynomially-bounded cycle lengths, constantly-many distinct\ntime intervals, so-called nested instances, and pairwise coprime settings.\nThese findings offer substantial improvements over currently available\nconstant-factor approximations and resolve outstanding open questions in their\nrespective contexts. In parallel, we develop new theory around a number of\nyet-uncharted questions, related to the sampling complexity of peak inventory\nestimation as well as to the plausibility of groupwise synchronization.\nInterestingly, we establish the global nature of inventory staggering, proving\nthat there are $n$-item instances where, for every subset of roughly $\\sqrt{n}$\nitems, no policy improves on the worst-possible one by a factor greater than\n$1+\\epsilon$, whereas for the entire instance, there exists a policy that\noutperforms the worst-possible one by a factor of nearly $2$, which is optimal.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10339v1", "AI": {"title_translation": "库存交错问题的新近似保证", "tldr": "本文为库存交错问题提供了新的、更优的近似保证，超越了现有结果，并解决了相关开放问题。", "motivation": "库存交错问题在多个应用领域中被广泛探索，但其核心计算问题和结构特征仍缺乏深入理解，现有方法工具箱严重不足。", "method": "本文设计了一系列算法技术和分析思想，包括新颖的方法以及利用组合学和数论中已有的概念，以超越现有近似保证。具体方法包括利用多项式有界循环长度、常数个不同时间间隔、嵌套实例和成对互素设置等结构特性来设计多项式时间近似方案。同时，还围绕峰值库存估计的采样复杂性和群组同步的合理性等未探索问题开发了新理论。", "result": "本文证明了多种结构特性（如多项式有界循环长度、常数个不同时间间隔、嵌套实例、成对互素设置）可以用于设计多项式时间近似方案，显著改进了现有常数因子近似结果，并解决了各自背景下的突出开放问题。此外，还建立了库存交错问题的全局性质，证明了存在n个项目的实例，其中对于大约√n个项目的每个子集，没有策略能比最差策略提高超过1+ε的因子，而对于整个实例，存在一个策略能比最差策略提高近2倍，这是最优的。", "conclusion": "本文为库存交错问题提供了实质性的近似保证改进，并解决了多个悬而未决的开放问题。研究结果还揭示了库存交错问题的全局性质，表明在某些情况下，全局优化可以显著优于局部优化。", "translation": "自20世纪60年代中期诞生以来，库存交错问题已在生产计划、库存控制系统、仓储以及航空航天/国防物流等广泛应用领域中得到了探索和利用。然而，尽管学术界对其关注已久，但在库存交错的核心计算问题以及相关的结构特征方面，我们仍然知之甚少，我们的方法工具箱严重不足。\n本文的核心贡献在于设计了一系列算法技术和分析思想——其中一些是全新的，另一些则利用了组合学和数论中已有的成熟概念——以超越库存交错问题几乎所有已知的近似保证。特别是，我们的工作表明，许多结构特性为设计多项式时间近似方案打开了大门，包括多项式有界循环长度、常数个不同时间间隔、所谓的嵌套实例以及成对互素设置。这些发现比当前可用的常数因子近似提供了实质性的改进，并解决了各自背景下的突出开放问题。与此同时，我们围绕一些尚未探索的问题发展了新理论，这些问题与峰值库存估计的采样复杂性以及群组同步的合理性有关。有趣的是，我们建立了库存交错问题的全局性质，证明存在n个项目的实例，其中对于大约√n个项目的每个子集，没有策略能比最差策略提高超过1+ε的因子，而对于整个实例，存在一个策略能比最差策略提高近2倍，这是最优的。", "summary": "本文针对库存交错问题，在现有研究不足的背景下，提出了一系列新颖的算法技术和分析思想。通过利用问题的多种结构特性，作者设计了能够实现多项式时间近似的方案，显著超越了当前已知的近似保证，并解决了该领域的多个开放问题。此外，研究还开发了关于峰值库存估计和群组同步的新理论，并揭示了库存交错问题的全局性质，证明了在某些情况下全局策略能达到最优性能。", "keywords": "库存交错问题, 近似算法, 组合学, 数论, 优化", "comments": "本文的创新之处在于其提出了一系列新的算法技术和分析思想，显著提升了库存交错问题的近似保证，超越了现有成果。它不仅解决了该领域长期存在的开放问题，还揭示了问题的全局性质，为未来的研究提供了新的视角。其对组合学和数论的借鉴也显示了跨学科研究的潜力。"}}
{"id": "2506.10019", "title": "A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations", "authors": ["Tian Lan", "Yang-Hao Zhou", "Zi-Ao Ma", "Fanshu Sun", "Rui-Qing Sun", "Junyu Luo", "Rong-Cheng Tu", "Heyan Huang", "Chen Xu", "Zhijing Wu", "Xian-Ling Mao"], "summary": "Recent advances in deep learning have significantly enhanced generative AI\ncapabilities across text, images, and audio. However, automatically evaluating\nthe quality of these generated outputs presents ongoing challenges. Although\nnumerous automatic evaluation methods exist, current research lacks a\nsystematic framework that comprehensively organizes these methods across text,\nvisual, and audio modalities. To address this issue, we present a comprehensive\nreview and a unified taxonomy of automatic evaluation methods for generated\ncontent across all three modalities; We identify five fundamental paradigms\nthat characterize existing evaluation approaches across these domains. Our\nanalysis begins by examining evaluation methods for text generation, where\ntechniques are most mature. We then extend this framework to image and audio\ngeneration, demonstrating its broad applicability. Finally, we discuss\npromising directions for future research in cross-modal evaluation\nmethodologies.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10019v1", "AI": {"title_translation": "文本、视觉和语音生成自动评估方法综述", "tldr": "本文综述了文本、视觉和语音生成内容的自动评估方法，提出了统一的分类法并确定了五种基本范式。", "motivation": "尽管深度学习显著增强了文本、图像和音频领域的生成式AI能力，但自动评估这些生成内容的质量仍面临挑战，且当前研究缺乏一个系统框架来全面组织跨模态的评估方法。", "method": "本文对跨文本、视觉和音频三种模态的生成内容自动评估方法进行了全面综述，并提出了统一的分类法。研究确定了表征现有评估方法的五个基本范式，并首先考察了文本生成评估方法，然后将该框架扩展到图像和音频生成，以展示其广泛适用性。", "result": "本文提出了一个统一的跨模态生成内容自动评估方法分类法，并确定了表征现有评估方法的五个基本范式，展示了所提出框架的广泛适用性。", "conclusion": "本文为跨模态自动评估方法提供了一个统一的分类法，并讨论了未来跨模态评估方法学的研究方向。", "translation": "深度学习的最新进展显著增强了文本、图像和音频领域的生成式AI能力。然而，自动评估这些生成内容的质量仍面临持续挑战。尽管存在大量的自动评估方法，但当前研究缺乏一个能够全面组织跨文本、视觉和音频模态这些方法的系统框架。为了解决这个问题，我们对跨所有三种模态的生成内容自动评估方法进行了全面综述并提出了统一的分类法；我们确定了表征这些领域现有评估方法的五个基本范式。我们的分析首先考察了文本生成评估方法，该领域技术最为成熟。然后，我们将此框架扩展到图像和音频生成，展示了其广泛适用性。最后，我们讨论了跨模态评估方法学未来研究的有前景方向。", "summary": "本文综述了文本、视觉和语音生成内容的自动评估方法。针对当前研究缺乏系统性框架的问题，文章提出了一个统一的分类法，并确定了五种基本评估范式。研究从文本生成评估方法入手，随后将框架扩展至图像和音频生成，展示了其广泛适用性，并讨论了未来跨模态评估方法学的研究方向。", "keywords": "自动评估, 生成式AI, 文本生成, 图像生成, 音频生成", "comments": "这项综述工作非常重要，它为理解和组织生成式AI领域中快速发展的自动评估方法提供了一个急需的系统性框架。其提出的五种基本范式为该领域提供了一个宝贵的概念工具。"}}
{"id": "2506.10027", "title": "Learning-based density-equalizing map", "authors": ["Yanwen Huang", "Lok Ming Lui", "Gary P. T. Choi"], "summary": "Density-equalizing map (DEM) serves as a powerful technique for creating\nshape deformations with the area changes reflecting an underlying density\nfunction. In recent decades, DEM has found widespread applications in fields\nsuch as data visualization, geometry processing, and medical imaging.\nTraditional approaches to DEM primarily rely on iterative numerical solvers for\ndiffusion equations or optimization-based methods that minimize handcrafted\nenergy functionals. However, these conventional techniques often face several\nchallenges: they may suffer from limited accuracy, produce overlapping\nartifacts in extreme cases, and require substantial algorithmic redesign when\nextended from 2D to 3D, due to the derivative-dependent nature of their energy\nformulations. In this work, we propose a novel learning-based\ndensity-equalizing mapping framework (LDEM) using deep neural networks.\nSpecifically, we introduce a loss function that enforces density uniformity and\ngeometric regularity, and utilize a hierarchical approach to predict the\ntransformations at both the coarse and dense levels. Our method demonstrates\nsuperior density-equalizing and bijectivity properties compared to prior\nmethods for a wide range of simple and complex density distributions, and can\nbe easily applied to surface remeshing with different effects. Also, it\ngeneralizes seamlessly from 2D to 3D domains without structural changes to the\nmodel architecture or loss formulation. Altogether, our work opens up new\npossibilities for scalable and robust computation of density-equalizing maps\nfor practical applications.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10027v1", "AI": {"title_translation": "基于学习的密度均衡映射", "tldr": "本文提出了一种名为LDEM的深度学习框架，用于生成密度均衡映射，解决了传统方法的局限性，并实现了2D到3D的无缝扩展。", "motivation": "传统的密度均衡映射（DEM）方法依赖于迭代数值求解器或优化方法，存在准确性有限、可能产生重叠伪影以及从2D扩展到3D时需要大量算法重新设计等挑战，促使本文寻求一种新的解决方案。", "method": "本文提出了一种新颖的基于学习的密度均衡映射框架（LDEM），利用深度神经网络。具体来说，引入了一个强制密度均匀性和几何规律性的损失函数，并采用分层方法在粗粒度和密集级别预测变换。", "result": "与现有方法相比，本文方法在各种简单和复杂密度分布下表现出卓越的密度均衡和双射特性。它可以轻松应用于具有不同效果的表面重新网格化。此外，该方法可以从2D域无缝推广到3D域，无需改变模型架构或损失公式。", "conclusion": "本文提出的基于学习的密度均衡映射框架（LDEM）为实际应用中可扩展且稳健的密度均衡映射计算开辟了新的可能性。", "translation": "密度均衡映射（DEM）是一种强大的技术，用于创建形状变形，其面积变化反映了潜在的密度函数。近几十年来，DEM已广泛应用于数据可视化、几何处理和医学成像等领域。传统的DEM方法主要依赖于扩散方程的迭代数值求解器或最小化手工设计能量泛函的优化方法。然而，这些传统技术常常面临一些挑战：它们可能精度有限，在极端情况下产生重叠伪影，并且由于其能量公式的导数依赖性，从2D扩展到3D时需要大量的算法重新设计。在这项工作中，我们提出了一种新颖的基于学习的密度均衡映射框架（LDEM），使用深度神经网络。具体来说，我们引入了一个强制密度均匀性和几何规律性的损失函数，并利用分层方法在粗粒度和密集级别预测变换。与现有方法相比，我们的方法在各种简单和复杂密度分布下表现出卓越的密度均衡和双射特性，并且可以轻松应用于具有不同效果的表面重新网格化。此外，它可以从2D域无缝推广到3D域，无需改变模型架构或损失公式。总而言之，我们的工作为实际应用中可扩展且稳健的密度均衡映射计算开辟了新的可能性。", "summary": "本文提出了一种名为LDEM的深度学习框架，旨在克服传统密度均衡映射（DEM）方法的局限性，如准确性低、重叠伪影和2D到3D扩展困难。LDEM利用深度神经网络，并通过引入特定的损失函数和分层方法来确保密度均匀性和几何规律性。实验结果表明，该方法在密度均衡和双射性方面优于现有技术，并能无缝地从2D推广到3D，为DEM的实际应用提供了可扩展且稳健的计算方法。", "keywords": "密度均衡映射, 深度学习, 几何处理, 神经网络, 2D到3D泛化", "comments": "这项工作通过引入深度学习方法，为密度均衡映射领域带来了创新。它解决了传统方法在准确性、伪影和维度扩展方面的关键限制，特别是其从2D到3D的无缝泛化能力是一项重要突破，有望推动DEM在多个应用领域的发展。"}}
{"id": "2506.10301", "title": "Towards Understanding Bias in Synthetic Data for Evaluation", "authors": ["Hossein A. Rahmani", "Varsha Ramineni", "Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz"], "summary": "Test collections are crucial for evaluating Information Retrieval (IR)\nsystems. Creating a diverse set of user queries for these collections can be\nchallenging, and obtaining relevance judgments, which indicate how well\nretrieved documents match a query, is often costly and resource-intensive.\nRecently, generating synthetic datasets using Large Language Models (LLMs) has\ngained attention in various applications. While previous work has used LLMs to\ngenerate synthetic queries or documents to improve ranking models, using LLMs\nto create synthetic test collections is still relatively unexplored. Previous\nwork~\\cite{rahmani2024synthetic} showed that synthetic test collections have\nthe potential to be used for system evaluation, however, more analysis is\nneeded to validate this claim. In this paper, we thoroughly investigate the\nreliability of synthetic test collections constructed using LLMs, where LLMs\nare used to generate synthetic queries, labels, or both. In particular, we\nexamine the potential biases that might occur when such test collections are\nused for evaluation. We first empirically show the presence of such bias in\nevaluation results and analyse the effects it might have on system evaluation.\nWe further validate the presence of such bias using a linear mixed-effects\nmodel. Our analysis shows that while the effect of bias present in evaluation\nresults obtained using synthetic test collections could be significant, for\ne.g.~computing absolute system performance, its effect may not be as\nsignificant in comparing relative system performance. Codes and data are\navailable at: https://github.com/rahmanidashti/BiasSyntheticData.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10301v1", "AI": {"title_translation": "理解用于评估的合成数据中的偏差", "tldr": "本文深入探讨了使用大型语言模型（LLMs）生成的合成测试集在信息检索系统评估中可能存在的偏差，并分析了其对绝对和相对系统性能评估的影响。", "motivation": "创建多样化的用户查询和获取相关性判断对于信息检索（IR）系统的评估而言成本高昂且资源密集。尽管大型语言模型（LLMs）在生成合成数据集方面受到关注，但将其用于构建合成测试集仍相对未被充分探索。先前的研究表明合成测试集有潜力用于系统评估，但需要进一步分析来验证其可靠性。", "method": "本文彻底调查了使用LLMs（用于生成合成查询、标签或两者）构建的合成测试集的可靠性，并特别检查了在评估中使用此类测试集时可能出现的潜在偏差。研究首先通过实证方法展示了评估结果中存在的偏差及其对系统评估的影响，然后使用线性混合效应模型进一步验证了这种偏差的存在。", "result": "分析表明，尽管合成测试集在评估结果中存在的偏差对计算绝对系统性能等可能影响显著，但对比较相对系统性能而言，其影响可能不那么显著。", "conclusion": "使用LLMs生成的合成测试集在评估信息检索系统时存在偏差，这种偏差可能显著影响绝对性能评估，但对相对性能比较的影响较小。在利用合成数据进行评估时，需要充分理解和考虑这些偏差。", "translation": "测试集对于评估信息检索（IR）系统至关重要。为这些集合创建多样化的用户查询可能具有挑战性，并且获取表明检索到的文档与查询匹配程度的相关性判断通常成本高昂且资源密集。最近，使用大型语言模型（LLMs）生成合成数据集在各种应用中受到了关注。虽然之前的工作已使用LLMs生成合成查询或文档来改进排名模型，但使用LLMs创建合成测试集仍相对未被充分探索。之前的工作表明合成测试集有潜力用于系统评估，然而，需要更多的分析来验证这一主张。在本文中，我们彻底调查了使用LLMs构建的合成测试集的可靠性，其中LLMs用于生成合成查询、标签或两者。特别是，我们检查了当此类测试集用于评估时可能出现的潜在偏差。我们首先通过实证方法展示了评估结果中存在的这种偏差，并分析了它可能对系统评估产生的影响。我们进一步使用线性混合效应模型验证了这种偏差的存在。我们的分析表明，虽然使用合成测试集获得的评估结果中存在的偏差可能影响显著，例如计算绝对系统性能，但其对比较相对系统性能的影响可能不那么显著。代码和数据可在：https://github.com/rahmanidashti/BiasSyntheticData 获取。", "summary": "本研究深入探讨了使用大型语言模型（LLMs）构建的合成测试集在信息检索系统评估中的可靠性及其潜在偏差。鉴于传统测试集构建成本高昂，合成数据提供了替代方案。文章通过实证分析和线性混合效应模型验证了合成数据中偏差的存在，并发现这种偏差对计算绝对系统性能有显著影响，但对比较相对系统性能的影响较小。这为理解和利用合成数据进行系统评估提供了重要见解。", "keywords": "合成数据, 大型语言模型, 评估偏差, 信息检索, 测试集", "comments": "本文创新性地关注了LLMs生成合成数据用于评估时固有的偏差问题，这是一个非常及时且重要的研究方向。它不仅指出了偏差的存在，还量化了其对不同评估指标（绝对与相对性能）的影响，为未来利用合成数据进行评估提供了实用的指导和警示。这对于确保合成数据评估结果的有效性和可靠性至关重要。"}}
{"id": "2506.10118", "title": "Data-driven balanced truncation for second-order systems with generalized proportional damping", "authors": ["Sean Reiter", "Steffen W. R. Werner"], "summary": "Structured reduced-order modeling is a central component in the\ncomputer-aided design of control systems in which cheap-to-evaluate\nlow-dimensional models with physically meaningful internal structures are\ncomputed. In this work, we develop a new approach for the structured\ndata-driven surrogate modeling of linear dynamical systems described by\nsecond-order time derivatives via balanced truncation model-order reduction.\nThe proposed method is a data-driven reformulation of position-velocity\nbalanced truncation for second-order systems and generalizes the\nquadrature-based balanced truncation for unstructured first-order systems to\nthe second-order case. The computed surrogates encode a generalized\nproportional damping structure, and the damping coefficients are inferred\nsolely from data by minimizing a least-squares error over the coefficients.\nSeveral numerical examples demonstrate the effectiveness of the proposed\nmethod.", "comment": "31 pages, 5 figures, 5 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10118v1", "AI": {"title_translation": "数据驱动的二阶系统平衡截断与广义比例阻尼", "tldr": "开发了一种新的数据驱动方法，用于具有广义比例阻尼的二阶系统的平衡截断模型降阶。", "motivation": "在计算机辅助控制系统设计中，需要计算具有物理意义内部结构的低维模型，以实现高效评估。现有方法可能无法充分处理具有特定阻尼结构的二阶系统，因此需要一种新的结构化数据驱动替代建模方法。", "method": "本文提出了一种新的方法，它是二阶系统位置-速度平衡截断的数据驱动重新表述，并将基于正交的平衡截断从非结构化一阶系统推广到二阶情况。该方法通过最小化系数的最小二乘误差，仅从数据中推断出广义比例阻尼结构中的阻尼系数。", "result": "计算出的替代模型成功编码了广义比例阻尼结构，并且阻尼系数能够仅从数据中推断。多个数值示例证明了所提出方法的有效性。", "conclusion": "该研究成功开发了一种有效的数据驱动方法，能够为具有广义比例阻尼的二阶系统生成结构化降阶模型，从而为控制系统设计提供更高效、物理意义更强的工具。", "translation": "结构化降阶建模是计算机辅助控制系统设计中的核心组成部分，其中计算的是具有物理意义内部结构的、易于评估的低维模型。在这项工作中，我们开发了一种新方法，用于通过平衡截断模型降阶，对由二阶时间导数描述的线性动力系统进行结构化数据驱动的替代建模。所提出的方法是二阶系统位置-速度平衡截断的数据驱动重新表述，并将非结构化一阶系统的基于正交的平衡截断推广到二阶情况。计算出的替代模型编码了广义比例阻尼结构，并且阻尼系数仅通过最小化系数的最小二乘误差从数据中推断出来。几个数值示例证明了所提出方法的有效性。", "summary": "本文提出了一种针对由二阶时间导数描述的线性动力系统的结构化数据驱动替代建模新方法。该方法是对二阶系统位置-速度平衡截断的数据驱动重新表述，并将其推广到二阶情况。其核心在于能够仅从数据中推断出具有广义比例阻尼结构的低维模型，并通过最小化最小二乘误差来确定阻尼系数。数值示例验证了该方法的有效性。", "keywords": "数据驱动, 平衡截断, 二阶系统, 广义比例阻尼, 模型降阶", "comments": "该论文的创新之处在于将数据驱动方法与平衡截断相结合，专门针对具有广义比例阻尼的二阶系统进行模型降阶。这使得生成的低维模型不仅高效，而且保留了重要的物理结构，增强了模型的可解释性。该方法为复杂控制系统的计算机辅助设计提供了新的工具，具有重要的实际应用价值。"}}
{"id": "2506.10097", "title": "Description and Discussion on DCASE 2025 Challenge Task 2: First-shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring", "authors": ["Tomoya Nishida", "Noboru Harada", "Daisuke Niizumi", "Davide Albertini", "Roberto Sannino", "Simone Pradolini", "Filippo Augusti", "Keisuke Imoto", "Kota Dohi", "Harsh Purohit", "Takashi Endo", "Yohei Kawaguchi"], "summary": "This paper introduces the task description for the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2025 Challenge Task 2,\ntitled \"First-shot unsupervised anomalous sound detection (ASD) for machine\ncondition monitoring.\" Building on the DCASE 2024 Challenge Task 2, this task\nis structured as a first-shot problem within a domain generalization framework.\nThe primary objective of the first-shot approach is to facilitate the rapid\ndeployment of ASD systems for new machine types without requiring\nmachine-specific hyperparameter tunings. For DCASE 2025 Challenge Task 2,\nsounds from previously unseen machine types have been collected and provided as\nthe evaluation dataset. Results and analysis of the challenge submissions will\nbe added following the challenge's submission deadline.", "comment": "this article draws heavily from arXiv:2406.07250v1", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10097v1", "AI": {"title_translation": "DCASE 2025挑战任务2：机器状态监测的首发无监督异常声音检测的描述与讨论", "tldr": "本文介绍了DCASE 2025挑战任务2，该任务专注于为机器状态监测提供首发无监督异常声音检测，旨在实现新机器类型的快速部署。", "motivation": "该任务的主要目标是促进异常声音检测（ASD）系统在新机器类型上的快速部署，而无需进行机器特定的超参数调整。", "method": "该任务基于DCASE 2024挑战任务2，被构建为域泛化框架内的首发问题。评估数据集包含来自以前未见过的机器类型的声音。", "result": "挑战提交的结果和分析将在提交截止日期后添加。", "conclusion": "本文介绍了DCASE 2025挑战任务2“机器状态监测的首发无监督异常声音检测”的任务描述。", "translation": "本文介绍了DCASE 2025挑战任务2的任务描述，该任务题为“机器状态监测的首发无监督异常声音检测（ASD）”。该任务建立在DCASE 2024挑战任务2的基础上，被构建为域泛化框架内的首发问题。首发方法的主要目标是促进异常声音检测系统在新机器类型上的快速部署，而无需进行机器特定的超参数调整。对于DCASE 2025挑战任务2，已收集并提供了来自以前未见过的机器类型声音作为评估数据集。挑战提交的结果和分析将在挑战提交截止日期后添加。", "summary": "本文详细介绍了DCASE 2025挑战任务2，该任务名为“机器状态监测的首发无监督异常声音检测”。此任务延续DCASE 2024挑战任务2，旨在解决域泛化框架下的首发问题，核心目标是实现异常声音检测系统在新机器类型上的快速部署，避免繁琐的超参数调优。为此，挑战提供了包含前所未见机器类型声音的评估数据集。", "keywords": "DCASE 2025, 异常声音检测, 机器状态监测, 首发, 无监督", "comments": "这项挑战的重要性在于其“首发”特性，旨在解决实际应用中新机器类型快速部署ASD系统的痛点，减少了传统方法所需的机器特定调优，具有重要的实际应用价值和创新性。"}}
{"id": "2506.10000", "title": "A Survey of Data Compression Algorithms and their Applications", "authors": ["Mohammad Hosseini"], "summary": "Today, with the growing demands of information storage and data transfer,\ndata compression is becoming increasingly important. Data Compression is a\ntechnique which is used to decrease the size of data. This is very useful when\nsome huge files have to be transferred over networks or being stored on a data\nstorage device and the size is more than the capacity of the data storage or\nwould consume so much bandwidth for transmission in a network. With the advent\nof the Internet and mobile devices with limited resources, data compression has\ngained even more importance. It can be effectively used to save both storage\nand bandwidth, thus to decrease download duration. Data compression can be\nachieved by a host of techniques. During this survey, I'm going to thoroughly\ndiscuss some of important data compression algorithms, their performance\nevaluation, and their major applications along with today's issues and recent\nresearch approaches.", "comment": "Network Systems Lab, Simon Fraser University, 2012", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10000v1", "AI": {"title_translation": "数据压缩算法及其应用综述", "tldr": "本综述讨论了数据压缩算法、其性能评估、主要应用、当前问题和最新研究方法，以应对信息存储和数据传输日益增长的需求。", "motivation": "随着信息存储和数据传输需求的增长，数据压缩变得越来越重要。在网络传输或数据存储时，当文件过大导致超出存储容量或消耗过多带宽时，数据压缩非常有用。互联网和资源有限的移动设备的出现，进一步提升了数据压缩的重要性，因为它能有效节省存储空间和带宽，从而缩短下载时间。", "method": "本文将对一些重要的数据压缩算法、它们的性能评估、主要应用以及当前面临的问题和最新的研究方法进行深入探讨和综述。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "如今，随着信息存储和数据传输需求的增长，数据压缩变得越来越重要。数据压缩是一种用于减小数据大小的技术。当一些巨大的文件必须通过网络传输或存储在数据存储设备上，并且其大小超过数据存储容量或在网络传输中会消耗过多带宽时，这非常有用。随着互联网和资源有限的移动设备的出现，数据压缩变得更加重要。它可以有效地用于节省存储和带宽，从而缩短下载时间。数据压缩可以通过多种技术实现。在本次调查中，我将深入讨论一些重要的数据压缩算法、它们的性能评估以及它们的主要应用，以及当今面临的问题和最新的研究方法。", "summary": "本综述旨在探讨数据压缩算法及其在当前信息存储和数据传输需求下的重要性。文章将深入讨论各种数据压缩算法、它们的性能评估、主要应用，并分析当前面临的问题和最新的研究方法，以应对大数据传输和存储的挑战。", "keywords": "数据压缩, 算法, 性能评估, 应用, 综述", "comments": "本文作为一篇综述性文章，其创新性在于系统性地梳理了数据压缩领域的重要算法、性能、应用及研究前沿，对于理解该领域现状具有重要意义。鉴于其综述性质，文章的局限性在于不提供新的实证研究或算法创新。"}}
{"id": "2506.10004", "title": "Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming", "authors": ["Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "summary": "Extended reality (XR) is rapidly advancing, and poised to revolutionize\ncontent creation and consumption. In XR, users integrate various sensory inputs\nto form a cohesive perception of the virtual environment. This survey reviews\nthe state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,\nwe define XR and introduce various XR headsets along with their multimodal\ninteraction methods to provide a foundational understanding. We then analyze XR\ntraffic characteristics to highlight the unique data transmission requirements.\nWe also explore factors that influence the quality of experience in XR systems,\naiming to identify key elements for enhancing user satisfaction. Following\nthis, we present visual attention-based optimization methods for XR streaming\nto improve efficiency and performance. Finally, we examine current applications\nand highlight challenges to provide insights into ongoing and future\ndevelopments of XR.", "comment": "accepted by ACM Transactions on Multimedia Computing, Communications,\n  and Applications", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10004v1", "AI": {"title_translation": "沉浸式多媒体通信：扩展现实流媒体的最新技术", "tldr": "这篇综述探讨了扩展现实（XR）流媒体的最新技术，涵盖了XR定义、设备、流量特性、QoE、优化方法、应用及挑战。", "motivation": "扩展现实（XR）技术正在迅速发展，并有望彻底改变内容创作和消费方式。本综述旨在回顾XR流媒体的最新技术。", "method": "本综述首先定义XR并介绍XR头戴设备及其多模态交互方法以提供基础理解；接着分析XR流量特性以突出独特的数据传输要求；然后探讨影响XR系统体验质量的因素以识别增强用户满意度的关键要素；随后介绍基于视觉注意力的XR流媒体优化方法以提高效率和性能；最后审视当前应用并突出挑战以提供见解。", "result": "本综述提供了对XR基础概念、设备、数据传输要求、体验质量增强要素、基于视觉注意力的优化方法、当前应用以及未来发展挑战的深入理解。", "conclusion": "本综述为XR的持续和未来发展提供了见解，强调了理解其基础、优化技术和应对挑战的重要性。", "translation": "扩展现实（XR）正在迅速发展，并有望彻底改变内容创作和消费。在XR中，用户整合各种感官输入，形成对虚拟环境的连贯感知。本综述回顾了XR流媒体的最新技术，重点关注多种范式。首先，我们定义了XR并介绍了各种XR头戴设备及其多模态交互方法，以提供基础理解。然后，我们分析了XR流量特性，以突出其独特的数据传输要求。我们还探讨了影响XR系统体验质量的因素，旨在确定增强用户满意度的关键要素。在此之后，我们提出了基于视觉注意力的XR流媒体优化方法，以提高效率和性能。最后，我们审视了当前的应用并强调了挑战，以期为XR的持续和未来发展提供见解。", "summary": "本文是一篇关于扩展现实（XR）流媒体技术现状的综述。它首先介绍了XR的基本概念、设备和交互方式，随后分析了XR流量特性和影响用户体验质量的因素。文章还探讨了基于视觉注意力的XR流媒体优化方法，并总结了当前的应用及面临的挑战，旨在为XR的未来发展提供全面的洞察。", "keywords": "扩展现实, XR流媒体, 沉浸式通信, 体验质量, 视觉注意力优化", "comments": "这篇论文是一篇全面的综述性研究，系统地梳理了扩展现实（XR）流媒体领域的关键方面。其价值在于为研究人员和开发者提供了XR技术的基础知识、挑战和潜在优化方向的概览，特别是在提高效率和用户体验方面，对于理解XR通信的当前状态和未来发展具有重要意义。"}}
{"id": "2506.10017", "title": "Design of A* based heuristic algorithm for efficient interdiction in multi-Layer networks", "authors": ["Sukanya Samanta"], "summary": "Intercepting a criminal using limited police resources presents a significant\nchallenge in dynamic crime environments, where the criminal's location\ncontinuously changes over time. The complexity is further heightened by the\nvastness of the transportation network. To tackle this problem, we propose a\nlayered graph representation, in which each time step is associated with a\nduplicate of the transportation network. For any given set of attacker\nstrategies, a near-optimal defender strategy is computed using the A-Star\nheuristic algorithm applied to the layered graph. The defender's goal is to\nmaximize the probability of successful interdiction. We evaluate the\nperformance of the proposed method by comparing it with a Mixed-Integer Linear\nProgramming (MILP) approach used for the defender. The comparison considers\nboth computational efficiency and solution quality. The results demonstrate\nthat our approach effectively addresses the complexity of the problem and\ndelivers high-quality solutions within a short computation time.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.10017v1", "AI": {"title_translation": "基于A*启发式算法的多层网络高效拦截设计", "tldr": "本文提出了一种基于A*启发式算法的方法，利用分层图表示来高效拦截动态犯罪环境中的罪犯，并在计算效率和解决方案质量方面表现出色。", "motivation": "在动态犯罪环境中，罪犯位置不断变化，且交通网络庞大，使用有限警力拦截罪犯面临巨大挑战。", "method": "提出了一种分层图表示方法，每个时间步对应一个交通网络副本。对于给定的攻击者策略，使用A*启发式算法在分层图上计算接近最优的防御者策略，以最大化成功拦截的概率。", "result": "与混合整数线性规划(MILP)方法相比，所提出的方法有效解决了问题的复杂性，并在短时间内提供了高质量的解决方案。", "conclusion": "该方法能够有效应对动态犯罪环境下的拦截挑战，提供高效且高质量的防御策略。", "translation": "使用有限的警力在动态犯罪环境中拦截罪犯是一个重大挑战，因为罪犯的位置会随时间不断变化。交通网络的庞大性进一步增加了复杂性。为了解决这个问题，我们提出了一种分层图表示，其中每个时间步都与一个交通网络的副本相关联。对于任何给定的一组攻击者策略，使用应用于分层图的A*启发式算法计算出接近最优的防御者策略。防御者的目标是最大化成功拦截的概率。我们通过将所提出的方法与用于防御者的混合整数线性规划（MILP）方法进行比较来评估其性能。比较考虑了计算效率和解决方案质量。结果表明，我们的方法有效地解决了问题的复杂性，并在短时间内提供了高质量的解决方案。", "summary": "本文针对动态犯罪环境中有限警力拦截罪犯的挑战，提出了一种基于A*启发式算法的方法。该方法采用分层图表示，将每个时间步映射为网络副本，并在此基础上计算出近似最优的防御策略以最大化拦截成功率。实验结果表明，与传统的MILP方法相比，该方法在计算效率和解决方案质量方面均表现出色，能有效应对复杂问题。", "keywords": "A*启发式算法, 多层网络, 拦截, 动态犯罪环境, 分层图", "comments": "该论文的创新点在于将动态拦截问题建模为分层图，并创造性地应用A*启发式算法来寻找高效的防御策略。这对于资源有限且需快速响应的实际场景（如警务行动）具有重要意义。"}}
{"id": "2506.10028", "title": "Secure Data Access in Cloud Environments Using Quantum Cryptography", "authors": ["S. Vasavi Venkata Lakshmi", "Ziaul Haque Choudhury"], "summary": "Cloud computing has made storing and accessing data easier but keeping it\nsecure is a big challenge nowadays. Traditional methods of ensuring data may\nnot be strong enough in the future when powerful quantum computers become\navailable. To solve this problem, this study uses quantum cryptography to\nprotect data in the cloud environment. Quantum Key Distribution (QKD) creates\nsecure keys by sending information using quantum particles like photons.\nSpecifically, we use the BB84 protocol, a simple and reliable way to make\nsecure keys that cannot be stolen without detection. To protect the data, we\nuse the Quantum One Time pad (QOTP) for encryption and decryption, ensuring the\ndata stays completely private. This study shows how these Quantum methods can\nbe applied in cloud systems to provide a strong defense against hackers, even\nif they have access to quantum computers. The combination of QKD, BB84, and\nQOTP creates a safe and reliable way to keep data secure when it is stored or\nshared in the cloud. Using quantum cryptography, this paper provides a way to\nensure data security now and in the future, making cloud computing safer for\neveryone to store their data securely and safely.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10028v1", "AI": {"title_translation": "在云环境中利用量子密码学实现安全数据访问", "tldr": "本研究提出了一种结合量子密钥分发（QKD）、BB84协议和量子一次性密码本（QOTP）的方法，以在云环境中提供针对量子计算机攻击的强大数据安全保障。", "motivation": "云数据存储和访问面临日益严峻的安全挑战，传统加密方法在未来量子计算机面前可能变得脆弱，因此需要一种更强大的安全解决方案。", "method": "本研究利用量子密码学来保护云环境中的数据。具体方法包括使用量子密钥分发（QKD）技术通过量子粒子（如光子）创建安全密钥，并特别采用了BB84协议来生成不可窃取的安全密钥。数据加密和解密则使用量子一次性密码本（QOTP）来确保数据的完全隐私性。", "result": "本研究展示了如何将量子方法应用于云系统，以提供针对黑客（即使拥有量子计算机）的强大防御。QKD、BB84和QOTP的结合为云中数据的存储和共享提供了一种安全可靠的保护方式。", "conclusion": "通过利用量子密码学，本研究提供了一种确保当前和未来数据安全的方法，使云计算对所有人来说都更安全，可以安全地存储数据。", "translation": "云计算使得数据存储和访问变得更容易，但数据安全如今是一个巨大的挑战。传统的确保数据安全的方法在未来强大的量子计算机出现时可能不够强大。为了解决这个问题，本研究利用量子密码学来保护云环境中的数据。量子密钥分发（QKD）通过使用量子粒子（如光子）发送信息来创建安全密钥。具体来说，我们使用BB84协议，这是一种简单可靠的方式来创建无法在不被发现的情况下被窃取的安全密钥。为了保护数据，我们使用量子一次性密码本（QOTP）进行加密和解密，确保数据完全私密。本研究展示了这些量子方法如何应用于云系统，以提供针对黑客（即使他们能够访问量子计算机）的强大防御。QKD、BB84和QOTP的结合为在云中存储或共享数据时保持数据安全提供了一种安全可靠的方式。通过使用量子密码学，本文提供了一种确保当前和未来数据安全的方法，使云计算对所有人来说都更安全，可以安全地存储他们的数据。", "summary": "本研究旨在解决云环境中数据安全面临的未来量子计算威胁。为此，论文提出了一种基于量子密码学的数据保护方案，该方案结合了量子密钥分发（QKD）、BB84协议用于安全密钥生成，以及量子一次性密码本（QOTP）用于数据加密和解密。研究展示了这些量子方法在云系统中的应用，旨在提供强大的抗量子攻击能力，确保云数据的长期安全和隐私。", "keywords": "量子密码学, 云计算安全, 量子密钥分发, BB84协议, 量子一次性密码本", "comments": "本文提出了一种前瞻性的解决方案，通过引入量子密码学应对未来量子计算对传统加密方法的威胁，特别是在云数据安全领域。其创新点在于将QKD、BB84协议和QOTP结合起来，构建了一个理论上不可破解的数据安全框架。这项研究的重要性在于为云计算的未来安全性提供了潜在方向和技术基础，尤其是在后量子密码时代。然而，实际部署和性能开销可能是未来需要深入研究的局限性。"}}
{"id": "2506.10874", "title": "Higher-Order Uncoupled Learning Dynamics and Nash Equilibrium", "authors": ["Sarah A. Toonsi", "Jeff S. Shamma"], "summary": "We study learnability of mixed-strategy Nash Equilibrium (NE) in general\nfinite games using higher-order replicator dynamics as well as classes of\nhigher-order uncoupled heterogeneous dynamics. In higher-order uncoupled\nlearning dynamics, players have no access to utilities of opponents (uncoupled)\nbut are allowed to use auxiliary states to further process information\n(higher-order). We establish a link between uncoupled learning and feedback\nstabilization with decentralized control. Using this association, we show that\nfor any finite game with an isolated completely mixed-strategy NE, there exist\nhigher-order uncoupled learning dynamics that lead (locally) to that NE. We\nfurther establish the lack of universality of learning dynamics by linking\nlearning to the control theoretic concept of simultaneous stabilization. We\nconstruct two games such that any higher-order dynamics that learn the\ncompletely mixed-strategy NE of one of these games can never learn the\ncompletely mixed-strategy NE of the other. Next, motivated by imposing natural\nrestrictions on allowable learning dynamics, we introduce the Asymptotic Best\nResponse (ABR) property. Dynamics with the ABR property asymptotically learn a\nbest response in environments that are asymptotically stationary. We show that\nthe ABR property relates to an internal stability condition on higher-order\nlearning dynamics. We provide conditions under which NE are compatible with the\nABR property. Finally, we address learnability of mixed-strategy NE in the\nbandit setting using a bandit version of higher-order replicator dynamics.", "comment": null, "cate": "cs.MA", "url": "http://arxiv.org/abs/2506.10874v1", "AI": {"title_translation": "高阶非耦合学习动力学与纳什均衡", "tldr": "研究在高阶非耦合学习动力学下混合策略纳什均衡的可学习性，并探讨其局限性。", "motivation": "研究在一般有限博弈中混合策略纳什均衡（NE）的可学习性，并对允许的学习动力学施加自然限制。", "method": "使用高阶复制器动力学和高阶非耦合异构动力学。建立非耦合学习与分散控制反馈稳定之间的联系。引入渐近最优响应（ABR）特性。在强盗设置中使用高阶复制器动力学的强盗版本。", "result": "对于任何具有孤立完全混合策略NE的有限博弈，存在导致该NE的高阶非耦合学习动力学（局部）。通过将学习与同时稳定性的控制理论概念联系起来，证明了学习动力学缺乏普适性。构建了两个博弈，任何学习其中一个博弈的完全混合策略NE的高阶动力学都不能学习另一个博弈的完全混合策略NE。ABR特性与高阶学习动力学的内部稳定性条件相关。提供了NE与ABR特性兼容的条件。", "conclusion": "高阶非耦合学习动力学可以在局部学习混合策略纳什均衡，但其普适性受到限制。引入的渐近最优响应特性为更自然和稳定的学习动力学提供了条件。", "translation": "我们研究在一般有限博弈中混合策略纳什均衡（NE）的可学习性，使用高阶复制器动力学以及高阶非耦合异构动力学。在高阶非耦合学习动力学中，玩家无法获取对手的效用（非耦合），但被允许使用辅助状态进一步处理信息（高阶）。我们建立了非耦合学习与分散控制反馈稳定之间的联系。利用这种关联，我们表明，对于任何具有孤立完全混合策略NE的有限博弈，存在导致该NE的高阶非耦合学习动力学（局部）。我们通过将学习与同时稳定性的控制理论概念联系起来，进一步证明了学习动力学缺乏普适性。我们构建了两个博弈，任何学习其中一个博弈的完全混合策略NE的高阶动力学都不能学习另一个博弈的完全混合策略NE。接下来，受对允许学习动力学施加自然限制的启发，我们引入了渐近最优响应（ABR）特性。具有ABR特性的动力学在渐近平稳的环境中渐近学习最优响应。我们表明，ABR特性与高阶学习动力学的内部稳定性条件相关。我们提供了NE与ABR特性兼容的条件。最后，我们使用高阶复制器动力学的强盗版本，解决了强盗设置中混合策略NE的可学习性问题。", "summary": "本研究探讨了在高阶非耦合学习动力学下，一般有限博弈中混合策略纳什均衡（NE）的可学习性。论文发现，对于具有孤立完全混合策略NE的博弈，存在能局部收敛到该NE的高阶非耦合学习动力学。然而，通过与同时稳定性概念的联系，研究也揭示了这类学习动力学的普适性不足。此外，论文引入了渐近最优响应（ABR）特性，并探讨了其与高阶学习动力学内部稳定性的关系，为NE与ABR兼容性提供了条件。最后，还讨论了在强盗设置下混合策略NE的可学习性。", "keywords": "纳什均衡, 非耦合学习, 高阶动力学, 复制器动力学, 渐近最优响应", "comments": "本文通过引入高阶非耦合学习动力学，扩展了对纳什均衡可学习性的理解。其创新之处在于将学习动力学与控制理论中的反馈稳定和同时稳定性概念相结合，从而深入揭示了学习的内在机制和局限性。特别是对学习动力学缺乏普适性的证明，为该领域的研究提供了重要的警示。引入渐近最优响应特性也使得所提出的学习动力学更具实际意义。"}}
{"id": "2506.10049", "title": "Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)", "authors": ["Francesco Vinci", "Gyunam Park", "Wil van der Aalst", "Massimiliano de Leoni"], "summary": "Business Process Simulation (BPS) refers to techniques designed to replicate\nthe dynamic behavior of a business process. Many approaches have been proposed\nto automatically discover simulation models from historical event logs,\nreducing the cost and time to manually design them. However, in dynamic\nbusiness environments, organizations continuously refine their processes to\nenhance efficiency, reduce costs, and improve customer satisfaction. Existing\ntechniques to process simulation discovery lack adaptability to real-time\noperational changes. In this paper, we propose a streaming process simulation\ndiscovery technique that integrates Incremental Process Discovery with Online\nMachine Learning methods. This technique prioritizes recent data while\npreserving historical information, ensuring adaptation to evolving process\ndynamics. Experiments conducted on four different event logs demonstrate the\nimportance in simulation of giving more weight to recent data while retaining\nhistorical knowledge. Our technique not only produces more stable simulations\nbut also exhibits robustness in handling concept drift, as highlighted in one\nof the use cases.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10049v1", "AI": {"title_translation": "演进业务流程仿真模型的在线发现（扩展版）", "tldr": "本文提出了一种流式过程仿真发现技术，结合增量过程发现和在线机器学习方法，以适应动态变化的业务流程，并通过实验证明其在处理概念漂移方面的稳定性和鲁棒性。", "motivation": "现有从历史事件日志中自动发现仿真模型的技术缺乏对实时操作变化的适应性，而组织在动态业务环境中不断优化其流程，因此需要一种能适应演进业务流程的在线仿真模型发现方法。", "method": "本文提出了一种流式过程仿真发现技术，该技术将增量过程发现与在线机器学习方法相结合。该技术优先处理最新数据，同时保留历史信息，以确保适应不断演进的过程动态。", "result": "在四种不同事件日志上进行的实验表明，在仿真中，赋予最新数据更多权重同时保留历史知识的重要性。我们的技术不仅产生了更稳定的仿真，而且在处理概念漂移方面也表现出鲁棒性。", "conclusion": "本文提出的流式过程仿真发现技术，通过优先处理最新数据并保留历史信息，能够有效适应动态变化的业务流程，产生更稳定的仿真，并对概念漂移具有鲁棒性。", "translation": "业务流程仿真（BPS）是指旨在复制业务流程动态行为的技术。许多方法已被提出用于从历史事件日志中自动发现仿真模型，从而降低手动设计模型的成本和时间。然而，在动态业务环境中，组织不断完善其流程以提高效率、降低成本和改善客户满意度。现有的过程仿真发现技术缺乏对实时操作变化的适应性。在本文中，我们提出了一种流式过程仿真发现技术，该技术将增量过程发现与在线机器学习方法相结合。该技术优先处理最新数据，同时保留历史信息，确保适应不断演进的过程动态。在四种不同事件日志上进行的实验证明了在仿真中，赋予最新数据更多权重同时保留历史知识的重要性。我们的技术不仅产生了更稳定的仿真，而且在处理概念漂移方面也表现出鲁棒性，正如其中一个用例所强调的。", "summary": "针对现有业务流程仿真（BPS）模型发现技术难以适应动态环境的问题，本文提出了一种流式过程仿真发现技术。该技术结合了增量过程发现和在线机器学习方法，旨在优先处理最新数据同时保留历史信息，以适应不断演进的业务流程。实验结果表明，该方法能够生成更稳定的仿真模型，并有效处理概念漂移。", "keywords": "业务流程仿真, 在线发现, 增量过程发现, 在线机器学习, 概念漂移", "comments": "该论文创新性地将增量过程发现与在线机器学习结合，以解决业务流程动态演进中仿真模型发现的适应性问题。其关注实时数据和概念漂移处理的能力，对于实际业务应用具有重要意义。"}}
{"id": "2506.10164", "title": "Mastery Learning Improves Performance on Complex Tasks on PCP Literacy Test", "authors": ["Chandana Srinivas", "Elif E. Firat", "Robert S. Laramee", "Alark Joshi"], "summary": "Developing literacy with unfamiliar data visualization techniques such as\nParallel Coordinate Plots (PCPs) can be a significant challenge for students.\nWe adopted the Revised Bloom's taxonomy to instruct students on Parallel\nCoordinate Plots (PCPs) using Mastery Learning in the classroom. To evaluate\nMastery Learning's impact, we conducted an intervention in a Data Visualization\ncourse to teach students about PCPs using the Revised Bloom's taxonomy with and\nwithout Mastery Learning. Based on our intervention, we found that while\nstudents in both groups performed similarly on the first two (Remember,\nUnderstand) modules, the students in the Mastery Learning group performed\nbetter on modules that required more advanced thinking (Analyze, Evaluate) and\ndemonstrated a better comprehension of PCPs. We provide all the materials\ndeveloped including the six-module Bloom's Taxonomy PCP literacy (BTPL) test\nfor full reproducibility on our website at\nhttps://vis-graphics.github.io/PCP-Literacy-Test/.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10164v1", "AI": {"title_translation": "精熟学习提高了PCP识字测试中复杂任务的表现", "tldr": "精熟学习能帮助学生在平行坐标图（PCP）的复杂认知任务上表现更好。", "motivation": "学生学习不熟悉的数据可视化技术（如平行坐标图PCPs）存在困难，因此研究旨在探索精熟学习如何帮助学生掌握这些技术。", "method": "研究采用修订版布鲁姆分类法，在数据可视化课程中对学生进行平行坐标图（PCPs）教学，并通过干预实验，比较有无精熟学习对学生表现的影响。", "result": "在初步模块（记忆、理解）上，两组学生表现相似；但在需要更高级思维（分析、评估）的模块上，精熟学习组的学生表现更优，并显示出对PCPs更好的理解。", "conclusion": "精熟学习能有效提高学生在PCP识字测试中复杂任务上的表现和理解能力。", "translation": "开发对平行坐标图（PCPs）等不熟悉数据可视化技术的理解能力对学生来说可能是一个重大挑战。我们采用修订版布鲁姆分类法，在课堂上利用精熟学习教授学生平行坐标图（PCPs）。为了评估精熟学习的影响，我们在数据可视化课程中进行了一项干预，使用修订版布鲁姆分类法，在有和没有精熟学习的情况下教授学生PCPs。根据我们的干预，我们发现，虽然两组学生在前两个模块（记忆、理解）上的表现相似，但精熟学习组的学生在需要更高级思维（分析、评估）的模块上表现更好，并展示了对PCPs更好的理解。我们提供了所有开发的材料，包括六模块布鲁姆分类法PCP识字（BTPL）测试，以便在我们的网站https://vis-graphics.github.io/PCP-Literacy-Test/上实现完全复现。", "summary": "该研究探讨了精熟学习如何提高学生对平行坐标图（PCPs）等复杂数据可视化技术的理解能力。通过在数据可视化课程中进行干预实验，并结合修订版布鲁姆分类法，研究发现精熟学习组的学生在需要高级思维（分析、评估）的PCPs理解任务上表现显著优于对照组，而在基础任务上两组表现相似。这表明精熟学习能有效提升学生在复杂认知任务上的表现。", "keywords": "精熟学习, 平行坐标图, 数据可视化, 布鲁姆分类法, 素养测试", "comments": "该研究的创新之处在于将精熟学习和布鲁姆分类法应用于数据可视化素养的教学，并提供了可复现的教学材料和测试。其重要性在于为提高学生对复杂数据可视化工具的掌握提供了实证支持和教学策略。"}}
{"id": "2506.10651", "title": "Large Language Models-Empowered Wireless Networks: Fundamentals, Architecture, and Challenges", "authors": ["Latif U. Khan", "Maher Guizani", "Sami Muhaidat", "Choong Seon Hong"], "summary": "The rapid advancement of wireless networks has resulted in numerous\nchallenges stemming from their extensive demands for quality of service towards\ninnovative quality of experience metrics (e.g., user-defined metrics in terms\nof sense of physical experience for haptics applications). In the meantime,\nlarge language models (LLMs) emerged as promising solutions for many difficult\nand complex applications/tasks. These lead to a notion of the integration of\nLLMs and wireless networks. However, this integration is challenging and needs\ncareful attention in design. Therefore, in this article, we present a notion of\nrational wireless networks powered by \\emph{telecom LLMs}, namely,\n\\emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case\nstudy of the distributed implementation of LLM-native wireless systems. In the\ncase study, we propose a solution based on double deep Q-learning (DDQN) that\noutperforms existing DDQN solutions. Finally, we provide open challenges.", "comment": null, "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10651v1", "AI": {"title_translation": "大型语言模型赋能的无线网络：基础、架构与挑战", "tldr": "本文探讨了大型语言模型（LLMs）与无线网络的结合，提出了“LLM原生无线系统”的概念，并讨论了其基础、架构和面临的挑战，通过案例研究展示了潜力。", "motivation": "无线网络的快速发展对服务质量和用户体验提出了巨大需求，导致诸多挑战。同时，大型语言模型（LLMs）在解决复杂应用和任务方面展现出巨大潜力，促使研究者考虑将LLMs与无线网络集成以应对现有挑战。", "method": "本文提出了由“电信LLMs”驱动的“LLM原生无线系统”概念，并阐述了其基本原理、愿景。通过一个分布式实现的案例研究，提出了一种基于双深度Q学习（DDQN）的解决方案。", "result": "在案例研究中，所提出的基于双深度Q学习（DDQN）的解决方案优于现有的DDQN解决方案。", "conclusion": "本文提出了LLM原生无线网络这一新颖范式，并指出了其集成所面临的开放性挑战。", "translation": "无线网络的快速发展带来了诸多挑战，这些挑战源于其对创新服务质量体验指标（例如，触觉应用中用户定义的物理体验感知指标）的广泛服务质量需求。与此同时，大型语言模型（LLMs）作为许多困难和复杂应用/任务的有前景的解决方案而出现。这些促成了LLMs与无线网络集成的概念。然而，这种集成具有挑战性，在设计上需要仔细关注。因此，在本文中，我们提出了由“电信LLM”驱动的“LLM原生无线系统”这一理性无线网络概念。我们提供了LLM原生无线系统的基础、愿景和分布式实现案例研究。在案例研究中，我们提出了一种基于双深度Q学习（DDQN）的解决方案，该方案优于现有的DDQN解决方案。最后，我们提出了开放性挑战。", "summary": "本文探讨了大型语言模型（LLMs）与无线网络的结合，以应对无线网络日益增长的服务质量和体验需求。作者提出了“电信LLM”驱动的“LLM原生无线系统”概念，并阐述了其基本原理和愿景。通过一个分布式实现的案例研究，展示了基于双深度Q学习（DDQN）的解决方案如何优于现有方法。文章最后指出了该领域面临的开放性挑战。", "keywords": "大型语言模型, 无线网络, LLM原生无线系统, 双深度Q学习, 挑战", "comments": "本文提出了将大型语言模型（LLMs）引入无线网络的新颖概念，即“LLM原生无线系统”，这在应对无线网络复杂性和优化用户体验方面具有潜在的创新性。通过引入“电信LLM”并展示DDQN在案例研究中的性能提升，为未来无线网络的设计提供了新的思路。其重要性在于开辟了LLM在通信领域应用的新方向，但也指出其集成面临挑战。"}}
{"id": "2506.10098", "title": "Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models", "authors": ["Christian Reichenbächer", "Philipp Rank", "Jochen Hipp", "Oliver Bringmann"], "summary": "This paper presents the first application of Gaussian Mixture Copula Models\nto the statistical modeling of driving scenarios for the safety validation of\nautomated driving systems. Knowledge of the joint probability distribution of\nscenario parameters is essential for scenario-based safety assessment, where\nrisk quantification depends on the likelihood of concrete parameter\ncombinations. Gaussian Mixture Copula Models bring together the multimodal\nexpressivity of Gaussian Mixture Models and the flexibility of copulas,\nenabling separate modeling of marginal distributions and dependencies. We\nbenchmark Gaussian Mixture Copula Models against previously proposed approaches\n- Gaussian Mixture Models and Gaussian Copula Models - using real-world driving\ndata drawn from scenarios defined in United Nations Regulation No. 157. Our\nevaluation across 18 million scenario instances demonstrates that Gaussian\nMixture Copula Models provide a better fit to the data in terms of both\nlikelihood and Sinkhorn distance. These results suggest that Gaussian Mixture\nCopula Models are a compelling foundation for future scenario-based validation\nframeworks.", "comment": "8 pages, 4 figures; This work has been submitted to the IEEE for\n  possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10098v1", "AI": {"title_translation": "使用高斯混合Copula模型估计场景参数的联合概率", "tldr": "本文首次将高斯混合Copula模型应用于自动驾驶场景的统计建模，并证明其在数据拟合方面优于现有方法，为自动驾驶安全验证提供了更精确的统计基础。", "motivation": "在自动驾驶系统安全验证中，基于场景的安全评估需要场景参数的联合概率分布知识，因为风险量化取决于具体参数组合的可能性，而现有方法可能无法充分捕捉其复杂性。", "method": "本文首次将高斯混合Copula模型应用于自动驾驶系统安全验证中的驾驶场景统计建模。该模型结合了高斯混合模型的多模态表达能力和Copula函数的灵活性，能够分别建模边缘分布和依赖关系。研究使用联合国法规No. 157中定义的真实世界驾驶数据，将高斯混合Copula模型与高斯混合模型和高斯Copula模型进行了基准测试。", "result": "在对1800万个场景实例的评估中，高斯混合Copula模型在似然性和Sinkhorn距离方面都提供了更好的数据拟合。", "conclusion": "高斯混合Copula模型是未来基于场景的自动驾驶安全验证框架的一个引人注目的基础。", "translation": "本文首次将高斯混合Copula模型应用于自动驾驶系统安全验证的驾驶场景统计建模。场景参数联合概率分布的知识对于基于场景的安全评估至关重要，其中风险量化取决于具体参数组合的可能性。高斯混合Copula模型结合了高斯混合模型的多模态表达能力和Copula函数的灵活性，能够分别建模边缘分布和依赖关系。我们使用从联合国法规No. 157中定义的场景中提取的真实世界驾驶数据，将高斯混合Copula模型与先前提出的方法——高斯混合模型和高斯Copula模型进行了基准测试。我们对1800万个场景实例的评估表明，高斯混合Copula模型在似然性和Sinkhorn距离方面都提供了更好的数据拟合。这些结果表明，高斯混合Copula模型是未来基于场景的验证框架的一个引人注目的基础。", "summary": "本文首次将高斯混合Copula模型应用于自动驾驶场景的统计建模，以进行安全验证。该模型结合了高斯混合模型的多模态表达能力和Copula的灵活性，能够有效建模场景参数的联合概率分布。通过与现有方法在真实世界驾驶数据上的对比，结果表明高斯混合Copula模型在数据拟合方面表现更优，为未来的自动驾驶安全验证框架奠定了基础。", "keywords": "高斯混合Copula模型, 自动驾驶, 场景建模, 联合概率, 安全验证", "comments": "本文的创新之处在于首次将高斯混合Copula模型应用于自动驾驶场景的联合概率分布估计，解决了现有模型在多模态和依赖关系建模上的局限性。其重要性体现在为自动驾驶系统的风险量化和安全评估提供了更精确的统计基础，对于提升自动驾驶系统的安全性具有重要意义。"}}
{"id": "2506.10082", "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning", "authors": ["Chenjian Gao", "Lihe Ding", "Xin Cai", "Zhanpeng Huang", "Zibin Wang", "Tianfan Xue"], "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.", "comment": "12 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10082v1", "AI": {"title_translation": "LoRA-Edit：通过掩码感知LoRA微调实现可控的首帧引导视频编辑", "tldr": "LoRA-Edit提出了一种基于掩码的LoRA微调方法，用于可控的首帧引导视频编辑，解决了现有方法灵活性不足的问题。", "motivation": "现有基于扩散模型的视频编辑方法通常依赖大规模预训练，限制了特定编辑的灵活性。首帧引导编辑虽能控制首帧，但缺乏对后续帧的灵活性。", "method": "本文提出了LoRA-Edit，一种基于掩码的低秩适应（LoRA）微调方法，用于调整预训练的图像到视频（I2V）模型以进行灵活的视频编辑。该方法在保留背景区域的同时，实现可控的编辑传播，并且不改变模型架构。它通过引入额外的参考（如不同视角或代表性场景状态）作为视觉锚点来指导内容展开。通过掩码驱动的LoRA微调策略，模型从输入视频中学习空间结构和运动线索，从参考图像中获取外观指导。空间掩码实现区域特定学习，确保每个区域从适当的源获取信息。", "result": "实验结果表明，与最先进的方法相比，我们的方法实现了卓越的视频编辑性能。", "conclusion": "LoRA-Edit通过掩码感知LoRA微调和外部参考，提供了一种高效、适应性强且可控的视频编辑解决方案，克服了现有方法的局限性。", "translation": "使用扩散模型进行视频编辑在生成高质量视频编辑方面取得了显著成果。然而，当前方法通常依赖大规模预训练，限制了特定编辑的灵活性。首帧引导编辑提供了对第一帧的控制，但缺乏对后续帧的灵活性。为了解决这个问题，我们提出了一种基于掩码的低秩适应（LoRA）微调方法，该方法调整预训练的图像到视频（I2V）模型以实现灵活的视频编辑。我们的方法保留背景区域，同时实现可控的编辑传播。该解决方案提供高效且适应性强的视频编辑，而无需更改模型架构。为了更好地引导此过程，我们加入了额外的参考，例如备用视角或代表性场景状态，它们作为内容应如何展开的视觉锚点。我们使用掩码驱动的LoRA微调策略解决了控制挑战，该策略将预训练的图像到视频模型适应到编辑上下文中。模型必须从两个不同的来源学习：输入视频提供空间结构和运动线索，而参考图像提供外观指导。空间掩码通过动态调节模型关注的内容来实现区域特定学习，确保每个区域从适当的来源获取信息。实验结果表明，我们的方法与最先进的方法相比，实现了卓越的视频编辑性能。", "summary": "LoRA-Edit提出了一种基于掩码的LoRA微调方法，用于可控的首帧引导视频编辑。它通过适应预训练的图像到视频模型，在保留背景的同时实现可控的编辑传播。该方法利用空间掩码和额外参考（如不同视角或代表性场景状态）来引导区域特定学习，从而从输入视频获取结构和运动信息，从参考图像获取外观指导。实验证明，LoRA-Edit在视频编辑性能上优于现有SOTA方法，提供了高效且灵活的解决方案。", "keywords": "视频编辑, 扩散模型, LoRA, 掩码感知, 首帧引导", "comments": "该论文的创新之处在于利用掩码感知的LoRA微调来适应图像到视频模型，从而在不改变模型架构的情况下实现灵活且可控的视频编辑。引入额外的视觉锚点和用于区域特定学习的空间掩码是其关键区别，解决了之前首帧引导方法的控制局限性。其高效性和适应性是重要的贡献。"}}
{"id": "2506.10441", "title": "EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques", "authors": ["Oğuzhan Canpolat", "Ataberk Olgun", "David Novo", "Oğuz Ergin", "Onur Mutlu"], "summary": "DRAM is a critical component of modern computing systems. Recent works\npropose numerous techniques (that we call DRAM techniques) to enhance\nDRAM-based computing systems' throughput, reliability, and computing\ncapabilities (e.g., in-DRAM bulk data copy). Evaluating the system-wide\nbenefits of DRAM techniques is challenging as they often require modifications\nacross multiple layers of the computing stack. Prior works propose FPGA-based\nplatforms for rapid end-to-end evaluation of DRAM techniques on real DRAM\nchips. Unfortunately, existing platforms fall short in two major aspects: (1)\nthey require deep expertise in hardware description languages, limiting\naccessibility; and (2) they are not designed to accurately model modern\ncomputing systems.\n  We introduce EasyDRAM, an FPGA-based framework for rapid and accurate\nend-to-end evaluation of DRAM techniques on real DRAM chips. EasyDRAM overcomes\nthe main drawbacks of prior FPGA-based platforms with two key ideas. First,\nEasyDRAM removes the need for hardware description language expertise by\nenabling developers to implement DRAM techniques using a high-level language\n(C++). At runtime, EasyDRAM executes the software-defined memory system design\nin a programmable memory controller. Second, EasyDRAM tackles a fundamental\nchallenge in accurately modeling modern systems: real processors typically\noperate at higher clock frequencies than DRAM, a disparity that is difficult to\nreplicate on FPGA platforms. EasyDRAM addresses this challenge by decoupling\nthe processor-DRAM interface and advancing the system state using a novel\ntechnique we call time scaling, which faithfully captures the timing behavior\nof the modeled system.\n  We believe and hope that EasyDRAM will enable innovative ideas in memory\nsystem design to rapidly come to fruition. To aid future research EasyDRAM\nimplementation is open sourced at https://github.com/CMU-SAFARI/EasyDRAM.", "comment": "Extended version of our publication at DSN 2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10441v1", "AI": {"title_translation": "EasyDRAM: 一种基于FPGA的快速准确评估新兴DRAM技术的端到端基础设施", "tldr": "EasyDRAM是一个基于FPGA的平台，它解决了现有平台在评估新兴DRAM技术时在易用性和准确性方面的不足，通过允许C++编程和创新的时间缩放技术。", "motivation": "现有FPGA平台在评估新兴DRAM技术时存在两大缺点：1) 需要深厚的硬件描述语言专业知识，限制了可访问性；2) 无法准确模拟现代计算系统，特别是处理器与DRAM之间的时钟频率差异。", "method": "EasyDRAM是一个基于FPGA的框架，通过两种关键思想克服了现有平台的缺点。首先，它允许开发者使用高级语言（C++）实现DRAM技术，无需硬件描述语言专业知识，并在可编程内存控制器中执行软件定义的内存系统设计。其次，它通过解耦处理器-DRAM接口并采用一种称为“时间缩放”的新颖技术来推进系统状态，从而解决了准确模拟现代系统（即处理器和DRAM之间的时钟频率差异）的挑战，忠实地捕获了建模系统的时序行为。", "result": "Not mentioned in abstract", "conclusion": "EasyDRAM有望使内存系统设计中的创新理念迅速实现。", "translation": "DRAM是现代计算系统的关键组件。最近的工作提出了许多技术（我们称之为DRAM技术）来提高基于DRAM的计算系统的吞吐量、可靠性和计算能力（例如，DRAM内部批量数据复制）。评估DRAM技术的系统范围效益具有挑战性，因为它们通常需要修改计算堆栈的多个层。先前的研究提出了基于FPGA的平台，用于在真实DRAM芯片上快速端到端评估DRAM技术。不幸的是，现有平台在两个主要方面存在不足：（1）它们需要深厚的硬件描述语言专业知识，限制了可访问性；（2）它们并非旨在准确模拟现代计算系统。\n我们引入了EasyDRAM，一个基于FPGA的框架，用于在真实DRAM芯片上快速准确地端到端评估DRAM技术。EasyDRAM通过两个关键思想克服了先前基于FPGA平台的主要缺点。首先，EasyDRAM通过允许开发者使用高级语言（C++）实现DRAM技术，消除了对硬件描述语言专业知识的需求。在运行时，EasyDRAM在可编程内存控制器中执行软件定义的内存系统设计。其次，EasyDRAM解决了准确模拟现代系统中的一个基本挑战：实际处理器通常以高于DRAM的时钟频率运行，这种差异在FPGA平台上难以复制。EasyDRAM通过解耦处理器-DRAM接口并使用我们称为时间缩放的新颖技术推进系统状态来解决这一挑战，该技术忠实地捕获了建模系统的时序行为。\n我们相信并希望EasyDRAM将使内存系统设计中的创新理念迅速实现。为了帮助未来的研究，EasyDRAM的实现已在https://github.com/CMU-SAFARI/EasyDRAM开源。", "summary": "EasyDRAM是一个创新的基于FPGA的框架，旨在解决现有平台在评估新兴DRAM技术时面临的挑战。它通过允许开发者使用高级语言（C++）实现DRAM技术来降低使用门槛，并引入了“时间缩放”技术来准确模拟现代计算系统中处理器与DRAM之间的时钟频率差异。该框架旨在促进内存系统设计的快速创新，并且已开源。", "keywords": "FPGA, DRAM, 内存系统, 评估, 时间缩放", "comments": "EasyDRAM的创新之处在于它显著降低了DRAM技术评估的门槛，通过允许使用高级语言编程，并解决了FPGA平台在模拟现代系统时序上的核心难题。这对于加速内存系统研究和新技术的验证具有重要意义。"}}
{"id": "2506.10014", "title": "NOCL: Node-Oriented Conceptualization LLM for Graph Tasks without Message Passing", "authors": ["Wei Li", "Mengcheng Lan", "Jiaxing Xu", "Yiping Ke"], "summary": "Graphs are essential for modeling complex interactions across domains such as\nsocial networks, biology, and recommendation systems. Traditional Graph Neural\nNetworks, particularly Message Passing Neural Networks (MPNNs), rely heavily on\nsupervised learning, limiting their generalization and applicability in\nlabel-scarce scenarios. Recent self-supervised approaches still require labeled\nfine-tuning, limiting their effectiveness in zero-shot scenarios. Meanwhile,\nLarge Language Models (LLMs) excel in natural language tasks but face\nsignificant challenges when applied to graphs, including preserving reasoning\nabilities, managing extensive token lengths from rich node attributes, and\nbeing limited to textual-attributed graphs (TAGs) and a single level task. To\novercome these limitations, we propose the Node-Oriented Conceptualization LLM\n(NOCL), a novel framework that leverages two core techniques: 1) node\ndescription, which converts heterogeneous node attributes into structured\nnatural language, extending LLM from TAGs to non-TAGs; 2) node concept, which\nencodes node descriptions into compact semantic embeddings using pretrained\nlanguage models, significantly reducing token lengths by up to 93.9% compared\nto directly using node descriptions. Additionally, our NOCL employs graph\nrepresentation descriptors to unify graph tasks at various levels into a\nshared, language-based query format, paving a new direction for Graph\nFoundation Models. Experimental results validate NOCL's competitive supervised\nperformance relative to traditional MPNNs and hybrid LLM-MPNN methods and\ndemonstrate superior generalization in zero-shot settings.", "comment": "10 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1703.00552, arXiv:1403.2844 by other authors", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10014v1", "AI": {"title_translation": "NOCL：用于无需消息传递的图任务的面向节点概念化的LLM", "tldr": "NOCL是一种新的LLM框架，通过节点描述和节点概念，使LLM能够在没有消息传递的情况下处理图任务，并在零样本设置中表现出优越的泛化能力。", "motivation": "传统图神经网络（GNNs），尤其是消息传递神经网络（MPNNs），严重依赖监督学习，导致其泛化能力和在标签稀缺场景中的适用性受限。现有的自监督方法仍需标签微调，限制了它们在零样本场景中的有效性。大型语言模型（LLMs）在处理图数据时面临保留推理能力、管理冗长的标记长度（尤其对于丰富节点属性）以及仅限于文本属性图（TAGs）和单级任务的挑战。", "method": "本文提出了Node-Oriented Conceptualization LLM (NOCL)框架，该框架利用两大核心技术：1) 节点描述：将异构节点属性转换为结构化自然语言，从而将LLM的应用范围从文本属性图（TAGs）扩展到非TAGs。2) 节点概念：使用预训练语言模型将节点描述编码为紧凑的语义嵌入，与直接使用节点描述相比，标记长度显著减少高达93.9%。此外，NOCL还采用图表示描述符，将不同级别的图任务统一为共享的、基于语言的查询格式，旨在为图基础模型开辟新方向。", "result": "实验结果验证了NOCL相对于传统MPNNs和混合LLM-MPNN方法具有竞争性的监督性能，并且在零样本设置中表现出卓越的泛化能力。", "conclusion": "NOCL成功地将LLM应用于图任务，克服了传统GNN和现有LLM在图处理上的局限性，尤其在零样本场景下展现出优异的泛化能力，为图基础模型的发展开辟了新方向。", "translation": "图对于建模社交网络、生物学和推荐系统等领域的复杂交互至关重要。传统的图神经网络，特别是消息传递神经网络（MPNNs），严重依赖监督学习，这限制了它们在标签稀缺场景中的泛化能力和适用性。最近的自监督方法仍然需要带标签的微调，这限制了它们在零样本场景中的有效性。同时，大型语言模型（LLMs）在自然语言任务中表现出色，但在应用于图时面临重大挑战，包括保留推理能力、管理丰富节点属性带来的冗长标记长度，以及仅限于文本属性图（TAGs）和单级任务。为了克服这些限制，我们提出了面向节点概念化的LLM（NOCL），这是一个新颖的框架，它利用了两个核心技术：1）节点描述，将异构节点属性转换为结构化自然语言，将LLM从TAGs扩展到非TAGs；2）节点概念，它使用预训练语言模型将节点描述编码为紧凑的语义嵌入，与直接使用节点描述相比，标记长度显著减少高达93.9%。此外，我们的NOCL采用图表示描述符将各种级别的图任务统一为共享的、基于语言的查询格式，为图基础模型开辟了新方向。实验结果验证了NOCL相对于传统MPNNs和混合LLM-MPNN方法具有竞争性的监督性能，并展示了在零样本设置中卓越的泛化能力。", "summary": "本文提出了NOCL（Node-Oriented Conceptualization LLM），一个无需消息传递的图任务LLM框架。它通过节点描述将异构节点属性转化为自然语言，并利用节点概念技术将描述编码为紧凑嵌入，从而显著减少LLM处理图数据时的标记长度。NOCL还通过图表示描述符统一了不同级别的图任务。实验证明，NOCL在监督学习和零样本泛化方面均表现出色，为图基础模型提供了新方向。", "keywords": "图神经网络, 大型语言模型, 零样本学习, 节点概念化, 无消息传递", "comments": "NOCL的创新之处在于其无需消息传递即可将LLM应用于图任务，这克服了传统GNN在标签稀缺和零样本场景下的局限性。通过节点描述和节点概念，NOCL有效地解决了LLM处理图数据时的标记长度问题，并扩展了LLM处理非文本属性图的能力。其统一图任务的方法也为构建通用的图基础模型奠定了基础，具有重要的研究价值。"}}
{"id": "2506.10356", "title": "Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?", "authors": ["Omid Asudeh", "Sina Mahdipour Saravani", "Gerald Sabin", "Fabrice Rastello", "P Sadayappan"], "summary": "This work evaluates the impact of sparse matrix reordering on the performance\nof sparse matrix-vector multiplication across different multicore CPU\nplatforms. Reordering can significantly enhance performance by optimizing the\nnon-zero element patterns to reduce total data movement and improve the\nload-balancing. We examine how these gains vary over different CPUs for\ndifferent reordering strategies, focusing on both sequential and parallel\nexecution. We address multiple aspects, including appropriate measurement\nmethodology, comparison across different kinds of reordering strategies,\nconsistency across machines, and impact of load imbalance.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10356v1", "AI": {"title_translation": "稀疏矩阵重排序对稀疏矩阵向量乘法有效吗？", "tldr": "本文评估了稀疏矩阵重排序对多核CPU上稀疏矩阵向量乘法性能的影响。", "motivation": "稀疏矩阵重排序可以通过优化非零元素模式来减少总数据移动和改善负载平衡，从而显著提升性能。本文旨在评估这种重排序对不同多核CPU平台上的稀疏矩阵向量乘法性能的影响。", "method": "本文评估了稀疏矩阵重排序对多核CPU上稀疏矩阵向量乘法性能的影响。研究检查了不同CPU上不同重排序策略（包括顺序和并行执行）的性能增益如何变化，并探讨了适当的测量方法、不同重排序策略的比较、机器间的一致性以及负载不平衡的影响。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "这项工作评估了稀疏矩阵重排序对不同多核CPU平台上稀疏矩阵向量乘法性能的影响。重排序可以通过优化非零元素模式来减少总数据移动并改善负载平衡，从而显著提高性能。我们研究了这些增益在不同CPU上，对于不同的重排序策略，包括顺序和并行执行，是如何变化的。我们解决了多个方面，包括适当的测量方法、不同类型重排序策略的比较、机器间的一致性以及负载不平衡的影响。", "summary": "本文评估了稀疏矩阵重排序对多核CPU上稀疏矩阵向量乘法(SpMV)性能的影响。研究指出，重排序能通过优化非零元素分布来减少数据移动和改善负载平衡，从而提高性能。文章探讨了这些性能提升在不同CPU和重排序策略（包括顺序和并行执行）下的变化，并讨论了测量方法、策略比较、跨机器一致性及负载不平衡等关键因素。", "keywords": "稀疏矩阵, 重排序, 矩阵向量乘法, 多核CPU, 性能优化", "comments": "这篇论文的创新点在于它全面评估了稀疏矩阵重排序对多核CPU上稀疏矩阵向量乘法性能的影响，并考虑了多种关键因素，如不同CPU、不同重排序策略、顺序与并行执行、测量方法、跨机器一致性以及负载不平衡。这项工作对于优化高性能计算中的稀疏矩阵操作具有重要意义。"}}
{"id": "2506.10610", "title": "Minimality and computability of languages of G-shifts", "authors": ["Djamel Eddine Amir", "Benjamin Hellouin de Menibus"], "summary": "Motivated by the notion of strong computable type for sets in computable\nanalysis, we define the notion of strong computable type for $G$-shifts, where\n$G$ is a finitely generated group with decidable word problem. A $G$-shift has\nstrong computable type if one can compute its language from the complement of\nits language. We obtain a characterization of $G$-shifts with strong computable\ntype in terms of a notion of minimality with respect to properties with a\nbounded computational complexity. We provide a self-contained direct proof, and\nalso explain how this characterization can be obtained from an existing similar\ncharacterization for sets by Amir and Hoyrup, and discuss its connexions with\nresults by Jeandel on closure spaces. We apply this characterization to several\nclasses of shifts that are minimal with respect to specific properties. This\nprovides a unifying approach that not only generalizes many existing results\nbut also has the potential to yield new findings effortlessly. In contrast to\nthe case of sets, we prove that strong computable type for G-shifts is\npreserved under products. We conclude by discussing some generalizations and\nfuture directions.", "comment": "Accepted to ICALP 2025", "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.10610v1", "AI": {"title_translation": "G-移位语言的最小性和可计算性", "tldr": "本文定义了G-移位的强可计算类型，并将其与具有有界计算复杂度的最小性概念联系起来，证明了其在乘积下的保持性，并提供了一种统一的方法来推广现有结果。", "motivation": "受可计算分析中集合的强可计算类型概念的启发，本文定义了G-移位的强可计算类型。", "method": "本文定义了G-移位的强可计算类型，并通过与具有有界计算复杂度的最小性概念相关联来表征它。提供了直接证明，并讨论了其与现有工作（如Amir和Hoyrup的集合表征以及Jeandel的闭包空间结果）的联系。将此表征应用于几类最小移位。", "result": "获得了G-移位强可计算类型的表征，该表征基于具有有界计算复杂度的最小性概念。证明了与集合情况不同，G-移位的强可计算类型在乘积下得以保持。该方法统一并推广了许多现有结果。", "conclusion": "本文讨论了一些推广和未来的研究方向。", "translation": "受可计算分析中集合的强可计算类型概念的启发，我们定义了G-移位的强可计算类型概念，其中G是具有可判定字问题的有限生成群。如果一个G-移位的语言可以从其语言的补集计算出来，则该G-移位具有强可计算类型。我们通过一个关于具有有界计算复杂度的最小性概念来表征具有强可计算类型的G-移位。我们提供了一个独立的直接证明，并解释了如何从Amir和Hoyrup现有的集合类似表征中获得此表征，并讨论了其与Jeandel关于闭包空间结果的联系。我们将此表征应用于几类在特定性质上是最小的移位。这提供了一种统一的方法，不仅推广了许多现有结果，而且有可能轻松地产生新的发现。与集合的情况相反，我们证明了G-移位的强可计算类型在乘积下得以保持。最后，我们讨论了一些推广和未来的方向。", "summary": "本文定义了G-移位（其中G是具有可判定字问题的有限生成群）的强可计算类型，即其语言可以从其语言的补集计算出来。作者通过一个关于具有有界计算复杂度的最小性概念来表征这种强可计算类型，并提供了一个独立的直接证明，同时解释了其与现有结果的联系。该表征被应用于多类具有特定性质的最小移位，提供了一种统一的方法，能够推广现有结果并潜在地发现新发现。此外，研究还证明了G-移位的强可计算类型在乘积下保持不变，这与集合的情况不同。文章最后讨论了未来的泛化和方向。", "keywords": "G-移位, 强可计算类型, 最小性, 可计算性, 语言", "comments": "这项工作在可计算分析领域为G-移位引入了“强可计算类型”这一新颖概念，并提供了其与最小性的重要表征。其创新之处在于将可计算性与群论中的移位概念结合，并提供了一个统一的框架来推广现有结果。证明了强可计算类型在乘积下保持的性质，这与集合的情况形成对比，突出了G-移位特有的行为。这为该领域未来的研究提供了坚实的基础。"}}
{"id": "2506.10157", "title": "One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence", "authors": ["Michelle M. Li", "Ben Y. Reis", "Adam Rodman", "Tianxi Cai", "Noa Dagan", "Ran D. Balicer", "Joseph Loscalzo", "Isaac S. Kohane", "Marinka Zitnik"], "summary": "Medical foundation models, including language models trained on clinical\nnotes, vision-language models on medical images, and multimodal models on\nelectronic health records, can summarize clinical notes, answer medical\nquestions, and assist in decision-making. Adapting these models to new\npopulations, specialties, or settings typically requires fine-tuning, careful\nprompting, or retrieval from knowledge bases. This can be impractical, and\nlimits their ability to interpret unfamiliar inputs and adjust to clinical\nsituations not represented during training. As a result, models are prone to\ncontextual errors, where predictions appear reasonable but fail to account for\ncritical patient-specific or contextual information. These errors stem from a\nfundamental limitation that current models struggle with: dynamically adjusting\ntheir behavior across evolving contexts of medical care. In this Perspective,\nwe outline a vision for context-switching in medical AI: models that\ndynamically adapt their reasoning without retraining to new specialties,\npopulations, workflows, and clinical roles. We envision context-switching AI to\ndiagnose, manage, and treat a wide range of diseases across specialties and\nregions, and expand access to medical care.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10157v1", "AI": {"title_translation": "一个患者，多种情境：通过情境智能扩展医疗AI", "tldr": "当前医疗AI模型难以动态适应不断变化的医疗情境，导致情境错误。本文提出“情境切换AI”的愿景，旨在无需重新训练即可适应新的专业、人群和工作流程，从而扩展医疗服务的可及性。", "motivation": "当前的医疗基础模型（包括语言模型、视觉-语言模型和多模态模型）在适应新的患者群体、专业或环境时，通常需要进行微调、精心提示或从知识库中检索，这既不切实际也限制了它们解释不熟悉输入和适应训练中未体现的临床情况的能力。这导致模型容易出现情境错误，即预测看似合理但未能考虑关键的患者特定或情境信息。这些错误源于当前模型难以动态调整其行为以适应不断变化的医疗情境的基本局限性。", "method": "本文提出了一种医疗AI中“情境切换”的愿景：即模型能够无需重新训练即可动态调整其推理能力，以适应新的专业、人群、工作流程和临床角色。", "result": "Not mentioned in abstract", "conclusion": "本文设想情境切换AI能够诊断、管理和治疗跨专业和地区的各种疾病，并扩大医疗服务的可及性。", "translation": "医疗基础模型，包括在临床笔记上训练的语言模型、在医学图像上训练的视觉-语言模型以及在电子健康记录上训练的多模态模型，可以总结临床笔记、回答医疗问题并协助决策。使这些模型适应新的人群、专业或设置通常需要微调、精心提示或从知识库中检索。这可能不切实际，并限制了它们解释不熟悉输入和适应训练中未体现的临床情况的能力。因此，模型容易出现情境错误，即预测看似合理但未能考虑关键的患者特定或情境信息。这些错误源于当前模型面临的一个根本限制：难以动态调整其行为以适应不断变化的医疗护理情境。在本视角中，我们概述了医疗AI中情境切换的愿景：模型能够无需重新训练即可动态调整其推理能力，以适应新的专业、人群、工作流程和临床角色。我们设想情境切换AI能够诊断、管理和治疗跨专业和地区的各种疾病，并扩大医疗服务的可及性。", "summary": "当前的医疗基础模型在适应新的临床情境时存在局限性，通常需要微调，导致情境错误。本文提出了一种“情境切换AI”的愿景，旨在使模型无需重新训练即可动态适应不同医疗情境（如专业、人群、工作流程和角色），从而提高诊断、管理和治疗能力，并扩大医疗服务的可及性。", "keywords": "医疗AI, 情境智能, 基础模型, 情境切换, 医疗可及性", "comments": "该论文解决了当前医疗AI的一个关键局限性——缺乏动态情境理解和适应能力。无需重新训练的“情境切换AI”概念是创新且至关重要的，它能使医疗AI安全有效地扩展到多样化的临床现实中，并有可能使先进的医疗见解普及化。"}}
{"id": "2506.10217", "title": "Data-Centric Safety and Ethical Measures for Data and AI Governance", "authors": ["Srija Chakraborty"], "summary": "Datasets play a key role in imparting advanced capabilities to artificial\nintelligence (AI) foundation models that can be adapted to various downstream\ntasks. These downstream applications can introduce both beneficial and harmful\ncapabilities -- resulting in dual use AI foundation models, with various\ntechnical and regulatory approaches to monitor and manage these risks. However,\ndespite the crucial role of datasets, responsible dataset design and ensuring\ndata-centric safety and ethical practices have received less attention. In this\nstudy, we pro-pose responsible dataset design framework that encompasses\nvarious stages in the AI and dataset lifecycle to enhance safety measures and\nreduce the risk of AI misuse due to low quality, unsafe and unethical data\ncontent. This framework is domain agnostic, suitable for adoption for various\napplications and can promote responsible practices in dataset creation, use,\nand sharing to facilitate red teaming, minimize risks, and increase trust in AI\nmodels.", "comment": "Paper accepted and presented at the AAAI 2025 Workshop on Datasets\n  and Evaluators of AI Safety https://sites.google.com/view/datasafe25/home", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10217v1", "AI": {"title_translation": "以数据为中心的数据和人工智能治理安全与伦理措施", "tldr": "本研究提出了一个负责任的数据集设计框架，旨在通过解决低质量、不安全和不道德的数据内容问题，来增强人工智能的安全措施并降低其滥用风险。", "motivation": "尽管数据集在赋予人工智能基础模型先进能力方面发挥着关键作用，但负责任的数据集设计以及确保以数据为中心的安全和伦理实践却较少受到关注。现有方法主要关注技术和监管层面，而忽视了数据本身可能导致人工智能滥用的风险。", "method": "本研究提出了一个负责任的数据集设计框架，该框架涵盖了人工智能和数据集生命周期中的各个阶段。", "result": "该框架旨在增强安全措施，并减少因低质量、不安全和不道德的数据内容而导致的人工智能滥用风险。", "conclusion": "该框架是领域无关的，适用于各种应用，并且可以促进数据集创建、使用和共享中的负责任实践，从而有助于红队测试、最小化风险并增加对人工智能模型的信任。", "translation": "数据集在赋予人工智能（AI）基础模型先进能力方面发挥着关键作用，这些模型可以适应各种下游任务。这些下游应用既可能带来有益的能力，也可能带来有害的能力——从而产生双重用途的AI基础模型，并伴随着各种技术和监管方法来监控和管理这些风险。然而，尽管数据集起着至关重要的作用，但负责任的数据集设计以及确保以数据为中心的安全和伦理实践却较少受到关注。在本研究中，我们提出了一个负责任的数据集设计框架，该框架涵盖了AI和数据集生命周期中的各个阶段，以增强安全措施并降低因低质量、不安全和不道德的数据内容而导致AI滥用的风险。该框架是领域无关的，适用于各种应用，并且可以促进数据集创建、使用和共享中的负责任实践，从而有助于红队测试、最小化风险并增加对AI模型的信任。", "summary": "本研究关注人工智能（AI）基础模型中数据集的关键作用，并指出当前对以数据为中心的安全和伦理实践关注不足。为此，论文提出了一个负责任的数据集设计框架，该框架涵盖AI和数据集生命周期的不同阶段，旨在通过解决低质量、不安全和不道德的数据内容问题，来增强AI安全并降低滥用风险。该框架具有领域无关性，可适用于多种应用，并能促进数据集的负责任创建、使用和共享，以支持红队测试、风险最小化和提升AI模型信任度。", "keywords": "数据集安全, 人工智能治理, 伦理AI, 负责任数据设计, 数据生命周期", "comments": "本文的创新点在于将数据本身作为AI安全和伦理问题的核心，提出了一个以数据为中心的负责任数据集设计框架。这填补了现有研究主要关注模型和监管而忽视数据源头风险的空白。其重要性在于，通过在数据生命周期早期介入，能够从根本上减少AI滥用和风险，提升AI系统的可信度。该框架的普适性和促进负责任实践的潜力是其亮点。"}}
{"id": "2506.10142", "title": "Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective", "authors": ["Minye Shao", "Zeyu Wang", "Haoran Duan", "Yawen Huang", "Bing Zhai", "Shizheng Wang", "Yang Long", "Yefeng Zheng"], "summary": "Precise segmentation of brain tumors, particularly contrast-enhancing regions\nvisible in post-contrast MRI (areas highlighted by contrast agent injection),\nis crucial for accurate clinical diagnosis and treatment planning but remains\nchallenging. However, current methods exhibit notable performance degradation\nin segmenting these enhancing brain tumor areas, largely due to insufficient\nconsideration of MRI-specific tumor features such as complex textures and\ndirectional variations. To address this, we propose the Harmonized Frequency\nFusion Network (HFF-Net), which rethinks brain tumor segmentation from a\nfrequency-domain perspective. To comprehensively characterize tumor regions, we\ndevelop a Frequency Domain Decomposition (FDD) module that separates MRI images\ninto low-frequency components, capturing smooth tumor contours and\nhigh-frequency components, highlighting detailed textures and directional\nedges. To further enhance sensitivity to tumor boundaries, we introduce an\nAdaptive Laplacian Convolution (ALC) module that adaptively emphasizes critical\nhigh-frequency details using dynamically updated convolution kernels. To\neffectively fuse tumor features across multiple scales, we design a Frequency\nDomain Cross-Attention (FDCA) integrating semantic, positional, and\nslice-specific information. We further validate and interpret frequency-domain\nimprovements through visualization, theoretical reasoning, and experimental\nanalyses. Extensive experiments on four public datasets demonstrate that\nHFF-Net achieves an average relative improvement of 4.48\\% (ranging from 2.39\\%\nto 7.72\\%) in the mean Dice scores across the three major subregions, and an\naverage relative improvement of 7.33% (ranging from 5.96% to 8.64%) in the\nsegmentation of contrast-enhancing tumor regions, while maintaining favorable\ncomputational efficiency and clinical applicability. Code:\nhttps://github.com/VinyehShaw/HFF.", "comment": "Accepted by IEEE Transactions on Medical Imaging", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10142v1", "AI": {"title_translation": "从频域角度重新思考脑肿瘤分割", "tldr": "HFF-Net通过引入频域分解、自适应拉普拉斯卷积和频域交叉注意力模块，从频域视角改进了脑肿瘤分割，尤其是在增强区域的分割性能，并保持了计算效率。", "motivation": "脑肿瘤的精确分割对临床诊断和治疗至关重要，但现有方法在分割MRI图像中对比增强的脑肿瘤区域时表现出显著的性能下降，主要原因是未充分考虑MRI特有的肿瘤特征，如复杂的纹理和方向性变化。", "method": "本文提出了协同频域融合网络（HFF-Net），从频域角度重新思考脑肿瘤分割。HFF-Net包含：1. 频域分解（FDD）模块，将MRI图像分解为低频（平滑轮廓）和高频（纹理、边缘）分量。2. 自适应拉普拉斯卷积（ALC）模块，通过动态更新卷积核，自适应地强调关键高频细节以增强肿瘤边界敏感性。3. 频域交叉注意力（FDCA）模块，融合语义、位置和切片特异性信息，有效融合多尺度肿瘤特征。研究还通过可视化、理论推理和实验分析验证并解释了频域改进。", "result": "在四个公共数据集上的大量实验表明，HFF-Net在三个主要子区域的平均Dice得分上实现了4.48%（2.39%至7.72%）的平均相对改进，在对比增强肿瘤区域的分割上实现了7.33%（5.96%至8.64%）的平均相对改进，同时保持了良好的计算效率和临床适用性。", "conclusion": "HFF-Net通过引入频域视角和创新的模块设计，显著提升了脑肿瘤（特别是对比增强区域）的分割精度和效率，为临床诊断和治疗规划提供了更准确的工具。", "translation": "脑肿瘤的精确分割，特别是造影剂注射后MRI可见的对比增强区域（由造影剂突出显示的区域），对准确的临床诊断和治疗计划至关重要，但仍然具有挑战性。然而，当前方法在分割这些增强的脑肿瘤区域时表现出显著的性能下降，这在很大程度上是由于对MRI特有的肿瘤特征，如复杂纹理和方向性变化，考虑不足。为了解决这个问题，我们提出了协同频域融合网络（HFF-Net），它从频域角度重新思考脑肿瘤分割。为了全面表征肿瘤区域，我们开发了一个频域分解（FDD）模块，将MRI图像分离为捕获平滑肿瘤轮廓的低频分量和突出详细纹理和方向边缘的高频分量。为了进一步增强对肿瘤边界的敏感性，我们引入了一个自适应拉普拉斯卷积（ALC）模块，该模块使用动态更新的卷积核自适应地强调关键高频细节。为了有效融合多尺度肿瘤特征，我们设计了一个频域交叉注意力（FDCA），整合了语义、位置和切片特异性信息。我们还通过可视化、理论推理和实验分析进一步验证和解释了频域改进。在四个公共数据集上的大量实验表明，HFF-Net在三个主要子区域的平均Dice得分上实现了4.48%（范围从2.39%到7.72%）的平均相对改进，在对比增强肿瘤区域的分割上实现了7.33%（范围从5.96%到8.64%）的平均相对改进，同时保持了良好的计算效率和临床适用性。代码：https://github.com/VinyehShaw/HFF。", "summary": "本文提出HFF-Net，通过引入频域视角来解决脑肿瘤（特别是对比增强区域）分割的挑战。该网络包含频域分解（FDD）模块用于分离图像频率分量，自适应拉普拉斯卷积（ALC）模块增强边界敏感性，以及频域交叉注意力（FDCA）模块用于多尺度特征融合。实验结果表明，HFF-Net在多个数据集上显著提升了脑肿瘤分割的精度，尤其是在增强区域，并保持了计算效率。", "keywords": "脑肿瘤分割, 频域, 深度学习, MRI, HFF-Net", "comments": "该论文的创新点在于将频域分析引入脑肿瘤分割任务，通过设计FDD、ALC和FDCA等模块，有效地利用了MRI图像的频率特性来捕捉肿瘤的复杂纹理和边界信息。这为解决现有方法在处理MRI特有肿瘤特征时的性能瓶颈提供了新的思路。其提出的HFF-Net在分割精度上取得了显著提升，尤其是在对比增强区域，这对于临床诊断和治疗规划具有重要意义。同时，该方法保持了计算效率，增强了其实用性。"}}
{"id": "2506.10312", "title": "AC/DC: LLM-based Audio Comprehension via Dialogue Continuation", "authors": ["Yusuke Fujita", "Tomoya Mizumoto", "Atsushi Kojima", "Lianbo Liu", "Yui Sudo"], "summary": "We propose an instruction-following audio comprehension model that leverages\nthe dialogue continuation ability of large language models (LLMs). Instead of\ndirectly generating target captions in training data, the proposed method\ntrains a model to produce responses as if the input caption triggered a\ndialogue. This dialogue continuation training mitigates the caption variation\nproblem. Learning to continue a dialogue effectively captures the caption's\nmeaning beyond its surface-level words. As a result, our model enables\nzero-shot instruction-following capability without multitask instruction\ntuning, even trained solely on audio captioning datasets. Experiments on\nAudioCaps, WavCaps, and Clotho datasets with AudioBench audio-scene\nquestion-answering tests demonstrate our model's ability to follow various\nunseen instructions.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10312v1", "AI": {"title_translation": "AC/DC：基于LLM的通过对话延续实现音频理解", "tldr": "提出了一种基于LLM对话延续能力的音频理解模型，通过将音频描述转化为对话响应来解决描述多样性问题，实现了零样本指令遵循能力。", "motivation": "现有音频理解模型在处理描述多样性方面存在问题，且难以实现零样本指令遵循能力。", "method": "提出了一种指令遵循音频理解模型，利用大型语言模型（LLM）的对话延续能力。该方法不直接生成目标描述，而是训练模型生成响应，如同输入描述触发了对话。这种对话延续训练缓解了描述多样性问题，并能捕捉描述的深层含义。", "result": "该模型即使仅在音频描述数据集上训练，也能实现零样本指令遵循能力，无需多任务指令微调。在AudioCaps、WavCaps和Clotho数据集上进行的AudioBench音频场景问答测试表明，该模型能够遵循各种未曾见过的指令。", "conclusion": "通过利用LLM的对话延续能力，本研究提出的模型有效解决了音频描述多样性问题，并实现了在仅使用音频描述数据集训练的情况下，具备零样本指令遵循能力，展现了对各种未知指令的泛化能力。", "translation": "我们提出了一种指令遵循的音频理解模型，该模型利用大型语言模型（LLM）的对话延续能力。所提出的方法不是直接在训练数据中生成目标字幕，而是训练模型生成响应，就像输入字幕触发了一场对话。这种对话延续训练缓解了字幕变异问题。学习延续对话有效地捕捉了字幕超越其表面词汇的含义。因此，我们的模型即使仅在音频字幕数据集上训练，也无需多任务指令微调即可实现零样本指令遵循能力。在AudioCaps、WavCaps和Clotho数据集上进行的AudioBench音频场景问答测试实验证明了我们模型遵循各种未见指令的能力。", "summary": "本文提出了一种名为AC/DC的指令遵循音频理解模型，该模型创新性地利用大型语言模型（LLM）的对话延续能力。不同于传统的直接生成描述，AC/DC将音频理解任务转化为生成对话响应，从而有效缓解了音频描述中的多样性问题，并能深入理解描述的含义。实验证明，该模型即使仅在单一音频描述数据集上训练，也能实现零样本指令遵循能力，并在AudioBench等测试中展现出对未知指令的强大泛化能力。", "keywords": "音频理解, 大型语言模型, 对话延续, 零样本, 指令遵循", "comments": "这篇论文的创新点在于将音频理解任务框架化为LLM的对话延续问题，巧妙地利用了LLM强大的语言理解和生成能力来处理音频描述的多样性。这种方法使得模型无需多任务指令微调即可实现零样本指令遵循，极大地提升了模型的泛化性和实用性。"}}
{"id": "2506.10717", "title": "Structural Parameterizations of $k$-Planarity", "authors": ["Tatsuya Gima", "Yasuaki Kobayashi", "Yuto Okada"], "summary": "The concept of $k$-planarity is extensively studied in the context of Beyond\nPlanarity. A graph is $k$-planar if it admits a drawing in the plane in which\neach edge is crossed at most $k$ times. The local crossing number of a graph is\nthe minimum integer $k$ such that it is $k$-planar. The problem of determining\nwhether an input graph is $1$-planar is known to be NP-complete even for\nnear-planar graphs [Cabello and Mohar, SIAM J. Comput. 2013], that is, the\ngraphs obtained from planar graphs by adding a single edge. Moreover, the local\ncrossing number is hard to approximate within a factor $2 - \\varepsilon$ for\nany $\\varepsilon > 0$ [Urschel and Wellens, IPL 2021]. To address this\ncomputational intractability, Bannister, Cabello, and Eppstein [JGAA 2018]\ninvestigated the parameterized complexity of the case of $k = 1$, particularly\nfocusing on structural parameterizations on input graphs, such as treedepth,\nvertex cover number, and feedback edge number. In this paper, we extend their\napproach by considering the general case $k \\ge 1$ and give (tight)\nparameterized upper and lower bound results. In particular, we strengthen the\naforementioned lower bound results to subclasses of constant-treewidth graphs:\nwe show that testing $1$-planarity is NP-complete even for near-planar graphs\nwith feedback vertex set number at most $3$ and pathwidth at most $4$, and the\nlocal crossing number is hard to approximate within any constant factor for\ngraphs with feedback vertex set number at most $2$.", "comment": "20 pages, 9 figures", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10717v1", "AI": {"title_translation": "k-平面性的结构参数化", "tldr": "本文扩展了k-平面性在一般k值下的结构参数化研究，给出了（紧致的）参数化上下界结果，并强化了对特定图类的计算复杂性下界。", "motivation": "确定图是否为k-平面图以及其局部交叉数的问题在计算上是难以处理的（对于1-平面图是NP完全的，且难以近似）。为了解决这种计算上的难处理性，前人对k=1的情况进行了结构参数化研究，本文旨在将其扩展到一般k≥1的情况。", "method": "本文通过考虑一般情况k≥1，扩展了Bannister、Cabello和Eppstein（2018）的方法，该方法侧重于输入图的结构参数化，如树深、顶点覆盖数和反馈边数。", "result": "本文给出了（紧致的）参数化上下界结果。特别是，将先前关于常数树宽图子类的下界结果进一步加强：证明了即使对于反馈顶点集数至多为3且路径宽度至多为4的近平面图，测试1-平面性仍然是NP完全的；并且对于反馈顶点集数至多为2的图，局部交叉数难以在任何常数因子内近似。", "conclusion": "本文全面研究了k-平面性在一般k值下的结构参数化，揭示了即使在受限图类下，该问题仍具有固有的计算难度。", "translation": "k-平面性概念在“超越平面性”的背景下得到了广泛研究。如果一个图允许在平面上绘制，并且每条边最多被交叉k次，则称其为k-平面图。图的局部交叉数是使其成为k-平面图的最小整数k。即使对于近平面图（即通过添加一条边从平面图获得的图），确定输入图是否为1-平面图的问题已知是NP完全的[Cabello and Mohar, SIAM J. Comput. 2013]。此外，对于任意ε>0，局部交叉数难以在2-ε因子内近似[Urschel and Wellens, IPL 2021]。为了解决这种计算上的难处理性，Bannister、Cabello和Eppstein[JGAA 2018]研究了k=1情况的参数化复杂性，特别是关注输入图的结构参数化，如树深、顶点覆盖数和反馈边数。在本文中，我们通过考虑一般情况k≥1扩展了他们的方法，并给出了（紧致的）参数化上下界结果。特别是，我们加强了上述关于常数树宽图子类的下界结果：我们表明，即使对于反馈顶点集数至多为3且路径宽度至多为4的近平面图，测试1-平面性仍然是NP完全的；并且对于反馈顶点集数至多为2的图，局部交叉数难以在任何常数因子内近似。", "summary": "本文研究了k-平面性在一般k≥1情况下的参数化复杂性，扩展了先前关于结构参数化的工作。它提供了新的参数化上下界，并强化了计算难处理性的结果，表明即使对于具有小反馈顶点集数和路径宽度的近平面图，1-平面性测试仍然是NP完全的，并且对于具有小反馈顶点集数的图，局部交叉数难以近似。", "keywords": "k-平面性, 结构参数化, 参数化复杂性, 局部交叉数, NP完全性", "comments": "本文的创新之处在于将结构参数化方法推广到一般的k-平面性问题，并为更广泛的k值和更受限的图类提供了更紧致的计算复杂性下界。这对于理解图绘制问题中高效算法的内在局限性具有重要意义，尤其是在超越平面性的领域。"}}
{"id": "2506.10055", "title": "TaskCraft: Automated Generation of Agentic Tasks", "authors": ["Dingfeng Shi", "Jingyi Cao", "Qianben Chen", "Weichen Sun", "Weizhen Li", "Hongxuan Lu", "Fangchen Dong", "Tianrui Qin", "King Zhu", "Minghao Yang", "Jian Yang", "Ge Zhang", "Jiaheng Liu", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce \\textsc{TaskCraft}, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10055v1", "AI": {"title_translation": "TaskCraft：代理任务的自动化生成", "tldr": "TaskCraft自动化生成可扩展的代理任务，解决了现有数据和基准的局限性，并提升了模型性能。", "motivation": "现有的指令数据缺乏工具交互，且当前代理基准依赖昂贵的人工标注，限制了其可扩展性。", "method": "本文引入了TaskCraft，一个自动化工作流，通过深度和宽度扩展原子任务来生成难度可扩展、多工具和可验证的代理任务及其执行轨迹。", "result": "经验结果表明，TaskCraft生成的任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。此外，还提出了一个包含约36,000个不同难度任务的大规模合成数据集。", "conclusion": "TaskCraft提供了一种自动化生成代理任务的方法，解决了当前数据和评估的限制，并为未来代理调优和评估研究提供了大规模数据集支持。", "translation": "代理任务，即需要多步骤问题解决、自主性、工具使用和自适应推理的任务，正日益成为自然语言处理和人工智能发展的核心。然而，现有的指令数据缺乏工具交互，当前的代理基准依赖昂贵的人工标注，限制了它们的可扩展性。我们引入了\\textsc{TaskCraft}，一个自动化工作流，用于生成难度可扩展、多工具和可验证的代理任务及其执行轨迹。TaskCraft通过基于深度和基于宽度的扩展来扩展原子任务，以创建结构和层次复杂的挑战。经验结果表明，这些任务改进了生成工作流中的提示优化，并增强了代理基础模型的监督微调。我们提出了一个包含约36,000个不同难度任务的大规模合成数据集，以支持未来对代理调优和评估的研究。", "summary": "本文介绍了TaskCraft，一个自动化生成代理任务的工作流，旨在解决现有指令数据缺乏工具交互和代理基准可扩展性差的问题。TaskCraft通过深度和宽度扩展原子任务来创建复杂的代理挑战，并生成了包含约36,000个任务的大规模合成数据集。实验证明，这些任务能够优化提示生成并改进代理基础模型的微调。", "keywords": "代理任务, 自动化生成, TaskCraft, 数据集, 提示优化", "comments": "TaskCraft的创新之处在于其自动化生成代理任务的能力，解决了当前代理任务数据和基准构建中耗时耗力的问题。其通过深度和宽度扩展生成复杂任务的方法也很有新意，为代理AI的训练和评估提供了可扩展的解决方案。该工作对于推动NLP和AI领域中代理AI的发展具有重要意义。"}}
{"id": "2506.10035", "title": "FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training", "authors": ["Fuhan Cai", "Yong Guo", "Jie Li", "Wenbo Li", "Xiangzhong Fang", "Jian Chen"], "summary": "Recent advancements in text-to-image (T2I) generation have led to the\nemergence of highly expressive models such as diffusion transformers (DiTs),\nexemplified by FLUX. However, their massive parameter sizes lead to slow\ninference, high memory usage, and poor deployability. Existing acceleration\nmethods (e.g., single-step distillation and attention pruning) often suffer\nfrom significant performance degradation and incur substantial training costs.\nTo address these limitations, we propose FastFLUX, an architecture-level\npruning framework designed to enhance the inference efficiency of FLUX. At its\ncore is the Block-wise Replacement with Linear Layers (BRLL) method, which\nreplaces structurally complex residual branches in ResBlocks with lightweight\nlinear layers while preserving the original shortcut connections for stability.\nFurthermore, we introduce Sandwich Training (ST), a localized fine-tuning\nstrategy that leverages LoRA to supervise neighboring blocks, mitigating\nperformance drops caused by structural replacement. Experiments show that our\nFastFLUX maintains high image quality under both qualitative and quantitative\nevaluations, while significantly improving inference speed, even with 20\\% of\nthe hierarchy pruned. Our code will be available soon.", "comment": "14 pages", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10035v1", "AI": {"title_translation": "FastFLUX：通过块级替换和三明治训练剪枝FLUX", "tldr": "FastFLUX是一种针对FLUX扩散模型提出的架构级剪枝框架，通过块级替换和三明治训练，显著提高推理速度并保持图像质量。", "motivation": "现有的文本到图像生成模型（如FLUX）参数量大，导致推理缓慢、内存占用高和部署困难。现有加速方法常导致性能显著下降且训练成本高。", "method": "提出FastFLUX框架，核心是块级线性层替换（BRLL）方法，用轻量级线性层替换残差块中复杂的残差分支，并保留原始捷径连接。同时引入三明治训练（ST），一种利用LoRA监督相邻块的局部微调策略，以减轻性能下降。", "result": "FastFLUX在定性和定量评估下均保持了高图像质量，即使剪枝20%的层级，也显著提高了推理速度。", "conclusion": "FastFLUX通过其创新的剪枝和训练策略，成功解决了大型T2I模型推理效率低下的问题，同时保持了图像生成质量。", "translation": "近期文本到图像（T2I）生成领域的进展催生了像扩散Transformer（DiTs）这样极具表现力的模型，FLUX就是其代表。然而，其庞大的参数量导致推理缓慢、内存占用高以及部署困难。现有的加速方法（例如单步蒸馏和注意力剪枝）通常会遭受显著的性能下降，并产生高昂的训练成本。为了解决这些限制，我们提出了FastFLUX，一个架构级别的剪枝框架，旨在提高FLUX的推理效率。其核心是块级线性层替换（BRLL）方法，该方法用轻量级线性层替换ResBlocks中结构复杂的残差分支，同时保留原始的捷径连接以保持稳定性。此外，我们引入了三明治训练（ST），一种局部微调策略，利用LoRA监督相邻块，从而减轻结构替换导致的性能下降。实验表明，我们的FastFLUX在定性和定量评估下均保持了高图像质量，同时显著提高了推理速度，即使剪枝了20%的层级。我们的代码将很快可用。", "summary": "本文提出了FastFLUX，一个针对大型文本到图像扩散模型FLUX的架构级剪枝框架，旨在提高推理效率。它通过块级线性层替换（BRLL）将复杂的残差分支替换为轻量级线性层，并引入三明治训练（ST）策略，利用LoRA进行局部微调以缓解性能下降。实验证明，FastFLUX在大幅提升推理速度的同时，仍能保持高图像生成质量。", "keywords": "文本到图像生成, 模型剪枝, FLUX, 扩散模型, 推理加速", "comments": "FastFLUX的创新之处在于结合了结构剪枝（BRLL）和局部微调策略（ST），有效解决了大型T2I模型效率低下的问题，同时避免了传统剪枝方法带来的显著性能下降。这种方法对于推动T2I模型在实际应用中的部署具有重要意义。"}}
{"id": "2506.10329", "title": "Context-Adaptive Graph Neural Networks for Next POI Recommendation", "authors": ["Yu Lei", "Limin Shen", "Zhu Sun", "Tiantian He", "Yew-Soon Ong"], "summary": "Next Point-of-Interest (POI) recommendation is a critical task in\nlocation-based services, aiming to predict users' next visits based on their\ncheck-in histories. While many existing methods leverage Graph Neural Networks\n(GNNs) to incorporate collaborative information and improve recommendation\naccuracy, most of them model each type of context using separate graphs,\ntreating different factors in isolation. This limits their ability to model the\nco-influence of multiple contextual factors on user transitions during message\npropagation, resulting in suboptimal attention weights and recommendation\nperformance. Furthermore, they often prioritize sequential components as the\nprimary predictor, potentially undermining the semantic and structural\ninformation encoded in the POI embeddings learned by GNNs. To address these\nlimitations, we propose a Context-Adaptive Graph Neural Networks (CAGNN) for\nnext POI recommendation, which dynamically adjusts attention weights using\nedge-specific contextual factors and enables mutual enhancement between\ngraph-based and sequential components. Specifically, CAGNN introduces (1) a\ncontext-adaptive attention mechanism that jointly incorporates different types\nof contextual factors into the attention computation during graph propagation,\nenabling the model to dynamically capture collaborative and context-dependent\ntransition patterns; (2) a graph-sequential mutual enhancement module, which\naligns the outputs of the graph- and sequential-based modules via the KL\ndivergence, enabling mutual enhancement of both components. Experimental\nresults on three real-world datasets demonstrate that CAGNN consistently\noutperforms state-of-the-art methods. Meanwhile, theoretical guarantees are\nprovided that our context-adaptive attention mechanism improves the\nexpressiveness of POI representations.", "comment": "12 pages, 6 figures", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10329v1", "AI": {"title_translation": "上下文自适应图神经网络用于下一兴趣点推荐", "tldr": "CAGNN通过上下文自适应注意力和图序列互增强，提升了下一POI推荐的性能和POI表示的表达能力。", "motivation": "现有GNN方法在POI推荐中独立处理上下文信息，限制了多上下文因素的共同影响建模，导致注意力权重和推荐性能不佳；并且过度依赖序列组件，忽视了GNN学习到的POI嵌入中的语义和结构信息。", "method": "提出上下文自适应图神经网络（CAGNN），该网络通过边缘特定的上下文因素动态调整注意力权重，并实现基于图和基于序列组件之间的相互增强。具体包括：1) 上下文自适应注意力机制，在图传播过程中联合整合不同类型的上下文因素，动态捕获协作和上下文依赖的转换模式；2) 图-序列互增强模块，通过KL散度对齐图和序列模块的输出，实现两者互增强。", "result": "在三个真实世界数据集上的实验结果表明，CAGNN持续优于最先进的方法。同时，理论上保证了其上下文自适应注意力机制提升了POI表示的表达能力。", "conclusion": "CAGNN通过其创新的上下文自适应注意力和图-序列互增强模块，有效解决了现有方法的局限性，显著提高了下一POI推荐的性能和POI表示的质量。", "translation": "下一兴趣点（POI）推荐是基于位置服务中的一项关键任务，旨在根据用户的签到历史预测其下一次访问。尽管许多现有方法利用图神经网络（GNNs）来整合协作信息并提高推荐准确性，但它们大多数通过独立的图来建模每种类型的上下文，孤立地处理不同的因素。这限制了它们在消息传播过程中建模多个上下文因素对用户转换的共同影响的能力，导致次优的注意力权重和推荐性能。此外，它们通常将序列组件作为主要预测器，这可能会削弱GNNs学习到的POI嵌入中编码的语义和结构信息。为了解决这些局限性，我们提出了一种用于下一POI推荐的上下文自适应图神经网络（CAGNN），它利用边缘特定的上下文因素动态调整注意力权重，并实现基于图和基于序列组件之间的相互增强。具体来说，CAGNN引入了（1）一种上下文自适应注意力机制，在图传播过程中将不同类型的上下文因素联合纳入注意力计算中，使模型能够动态捕获协作和上下文相关的转换模式；（2）一个图-序列互增强模块，通过KL散度对齐图和序列模块的输出，实现两个组件的相互增强。在三个真实世界数据集上的实验结果表明，CAGNN持续优于最先进的方法。同时，理论上保证了我们的上下文自适应注意力机制提高了POI表示的表达能力。", "summary": "本文提出了一种上下文自适应图神经网络（CAGNN）用于下一兴趣点（POI）推荐，旨在解决现有GNN方法在处理多上下文信息时的局限性及过度依赖序列组件的问题。CAGNN引入了上下文自适应注意力机制，该机制能在图传播中联合考虑多种上下文因素，并动态捕捉协作和上下文依赖的转换模式。此外，它还包含一个图-序列互增强模块，通过KL散度对齐图和序列模块的输出，实现两者优势互补。实验证明CAGNN在真实数据集上优于现有SOTA方法，并从理论上证明了其上下文自适应注意力机制能提升POI表示的表达能力。", "keywords": "下一POI推荐, 图神经网络, 上下文自适应, 注意力机制, 序列增强", "comments": "CAGNN的创新点在于其上下文自适应注意力机制，它能够动态整合不同类型的上下文因素，解决了现有方法孤立处理上下文的痛点。此外，图-序列互增强模块的设计也很有意义，它弥补了GNN和序列模型各自的不足，实现了模型整体性能的提升。理论保证POI表示的表达能力也增加了其可信度。"}}
{"id": "2506.10243", "title": "R-PINN: Recovery-type a-posteriori estimator enhanced adaptive PINN", "authors": ["Rongxin Lu", "Jiwei Jia", "Young Ju Lee", "Zheng Lu", "Chensong Zhang"], "summary": "In recent years, with the advancements in machine learning and neural\nnetworks, algorithms using physics-informed neural networks (PINNs) to solve\nPDEs have gained widespread applications. While these algorithms are\nwell-suited for a wide range of equations, they often exhibit suboptimal\nperformance when applied to equations with large local gradients, resulting in\nsubstantial localized errors. To address this issue, this paper proposes an\nadaptive PINN algorithm designed to improve accuracy in such cases. The core\nidea of the algorithm is to adaptively adjust the distribution of collocation\npoints based on the recovery-type a-posterior error of the current numerical\nsolution, enabling a better approximation of the true solution. This approach\nis inspired by the adaptive finite element method. By combining the\nrecovery-type a-posteriori estimator, a gradient-recovery estimator commonly\nused in the adaptive finite element method (FEM) with PINNs, we introduce the\nRecovery-type a-posteriori estimator enhanced adaptive PINN (R-PINN) and\ncompare its performance with a typical adaptive PINN algorithm, FI-PINN. Our\nresults demonstrate that R-PINN achieves faster convergence with fewer adaptive\npoints and significantly outperforms in the cases with multiple regions of\nlarge errors than FI-PINN. Notably, our method is a hybrid numerical approach\nfor solving partial differential equations, integrating adaptive FEM with\nPINNs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10243v1", "AI": {"title_translation": "R-PINN：恢复型后验估计器增强的自适应PINN", "tldr": "R-PINN是一种新的自适应物理信息神经网络（PINN），通过结合恢复型后验误差估计器，显著提高了PINN在处理大梯度方程时的精度和收敛速度。", "motivation": "传统的物理信息神经网络（PINN）在处理具有大局部梯度的方程时表现不佳，导致显著的局部误差。为了解决这一问题，本文提出了一种自适应PINN算法来提高此类情况下的精度。", "method": "本文提出了恢复型后验估计器增强的自适应PINN（R-PINN）算法。该算法的核心思想是基于当前数值解的恢复型后验误差自适应调整配置点的分布。这种方法受到自适应有限元方法的启发，将自适应有限元方法中常用的梯度恢复估计器（恢复型后验估计器）与PINN相结合。", "result": "结果表明，R-PINN以更少的自适应点实现了更快的收敛，并且在存在多个大误差区域的情况下，其性能显著优于典型的自适应PINN算法FI-PINN。", "conclusion": "本文提出的R-PINN通过结合恢复型后验估计器，有效解决了传统PINN在处理大梯度问题时的精度不足问题，展现出更快的收敛速度和更高的准确性，尤其适用于存在多个大误差区域的复杂偏微分方程求解。", "translation": "近年来，随着机器学习和神经网络的进步，使用物理信息神经网络（PINN）求解偏微分方程（PDE）的算法得到了广泛应用。虽然这些算法适用于各种方程，但当应用于具有大局部梯度的方程时，它们通常表现出次优的性能，导致大量的局部误差。为了解决这个问题，本文提出了一种自适应PINN算法，旨在提高此类情况下的精度。该算法的核心思想是根据当前数值解的恢复型后验误差自适应调整配置点的分布，从而更好地逼近真实解。这种方法受到了自适应有限元方法的启发。通过将自适应有限元方法（FEM）中常用的梯度恢复估计器——恢复型后验估计器与PINN相结合，我们引入了恢复型后验估计器增强的自适应PINN（R-PINN），并将其性能与典型的自适应PINN算法FI-PINN进行了比较。我们的结果表明，R-PINN以更少的自适应点实现了更快的收敛，并且在存在多个大误差区域的情况下，其性能显著优于FI-PINN。值得注意的是，我们的方法是一种混合数值方法，用于求解偏微分方程，集成了自适应有限元方法和PINN。", "summary": "本文提出了一种名为R-PINN的自适应物理信息神经网络（PINN）算法，旨在解决传统PINN在处理具有大局部梯度的偏微分方程时存在的精度不足和局部误差问题。R-PINN的核心在于利用恢复型后验误差估计器自适应调整配置点分布，以更好地逼近真实解。该方法受到自适应有限元方法的启发，并将梯度恢复估计器与PINN结合。实验结果表明，R-PINN与FI-PINN相比，能以更少的自适应点实现更快收敛，并在存在多个大误差区域的情况下表现出显著优越的性能。该方法是一种结合自适应有限元方法和PINN的混合数值求解偏微分方程的方法。", "keywords": "PINN, 自适应, 后验估计器, 误差估计, 偏微分方程", "comments": "R-PINN的创新之处在于将自适应有限元方法中的恢复型后验误差估计器引入到物理信息神经网络中，有效解决了PINN在处理复杂梯度区域时精度不足的痛点。这种混合数值方法为偏微分方程的高精度求解提供了新的思路，对于提升PINN在实际工程和科学计算中的应用潜力具有重要意义。"}}
{"id": "2506.10207", "title": "FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification", "authors": ["Jun Bai", "Rajib Rana", "Di Wu", "Youyang Qu", "Xiaohui Tao", "Ji Zhang"], "summary": "Federated Learning (FL) provides a privacy-preserving paradigm for training\naudio classification (AC) models across distributed clients without sharing raw\ndata. However, Federated Audio Classification (FedAC) faces three critical\nchallenges that substantially hinder performance: data heterogeneity, model\nheterogeneity, and data poisoning. While prior works have attempted to address\nthese issues, they are typically treated independently, lacking a unified and\nrobust solution suited to real-world federated audio scenarios. To bridge this\ngap, we propose FedMLAC, a unified mutual learning framework designed to\nsimultaneously tackle these challenges in FedAC. Specifically, FedMLAC\nintroduces a dual-model architecture on each client, comprising a personalized\nlocal AC model and a lightweight, globally shared Plug-in model. Through\nbidirectional knowledge distillation, the Plug-in model enables global\nknowledge transfer while adapting to client-specific data distributions, thus\nsupporting both generalization and personalization. To further enhance\nrobustness against corrupted audio data, we develop a Layer-wise Pruning\nAggregation (LPA) strategy that filters unreliable Plug-in model updates based\non parameter deviations during server-side aggregation. Extensive experiments\non four diverse audio classification benchmarks, spanning both speech and\nnon-speech tasks, demonstrate that FedMLAC consistently outperforms existing\nstate-of-the-art methods in terms of classification accuracy and robustness to\nnoisy data.", "comment": "initial version", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10207v1", "AI": {"title_translation": "FedMLAC：互学习驱动的异构联邦音频分类", "tldr": "FedMLAC是一个统一的互学习框架，通过双模型架构和分层剪枝聚合策略，解决了联邦音频分类中的数据异构性、模型异构性和数据中毒问题，显著优于现有方法。", "motivation": "联邦音频分类（FedAC）面临数据异构性、模型异构性以及数据中毒三大挑战，这些问题严重阻碍了性能。现有工作通常独立处理这些问题，缺乏适用于真实世界联邦音频场景的统一且鲁棒的解决方案。", "method": "我们提出了FedMLAC，一个统一的互学习框架，旨在同时解决FedAC中的上述挑战。具体而言，FedMLAC在每个客户端引入了双模型架构，包括个性化本地AC模型和轻量级、全局共享的插件模型。通过双向知识蒸馏，插件模型实现全局知识迁移，同时适应客户端特定数据分布。为了增强对损坏音频数据的鲁棒性，我们开发了分层剪枝聚合（LPA）策略，该策略根据服务器端聚合期间的参数偏差过滤不可靠的插件模型更新。", "result": "在四个不同的音频分类基准测试（包括语音和非语音任务）上进行的广泛实验表明，FedMLAC在分类准确性和对噪声数据的鲁棒性方面始终优于现有最先进的方法。", "conclusion": "FedMLAC提供了一个统一且鲁棒的解决方案，能够同时解决联邦音频分类中的数据异构性、模型异构性以及数据中毒等关键挑战，从而显著提升性能。", "translation": "联邦学习（FL）提供了一种隐私保护范式，用于在不共享原始数据的情况下，跨分布式客户端训练音频分类（AC）模型。然而，联邦音频分类（FedAC）面临三大关键挑战，严重阻碍了性能：数据异构性、模型异构性以及数据中毒。尽管先前的研究试图解决这些问题，但它们通常是独立处理的，缺乏适用于真实世界联邦音频场景的统一且鲁棒的解决方案。为了弥补这一差距，我们提出了FedMLAC，一个统一的互学习框架，旨在同时解决FedAC中的这些挑战。具体而言，FedMLAC在每个客户端引入了双模型架构，包括个性化本地AC模型和轻量级、全局共享的插件模型。通过双向知识蒸馏，插件模型能够实现全局知识迁移，同时适应客户端特定数据分布，从而支持泛化和个性化。为了进一步增强对损坏音频数据的鲁度，我们开发了一种分层剪枝聚合（LPA）策略，该策略根据服务器端聚合期间的参数偏差过滤不可靠的插件模型更新。在四个不同的音频分类基准测试（包括语音和非语音任务）上进行的广泛实验表明，FedMLAC在分类准确性和对噪声数据的鲁棒性方面始终优于现有最先进的方法。", "summary": "本文提出了FedMLAC，一个用于联邦音频分类（FedAC）的统一互学习框架，旨在同时解决数据异构性、模型异构性和数据中毒三大挑战。FedMLAC在每个客户端采用双模型架构（个性化本地AC模型和全局插件模型），并通过双向知识蒸馏实现知识迁移和个性化。此外，LPA策略用于增强对损坏数据的鲁棒性。实验证明，FedMLAC在准确性和抗噪声能力方面均优于现有最先进方法。", "keywords": "联邦学习, 音频分类, 互学习, 异构性, 数据中毒", "comments": "FedMLAC的创新点在于其统一的框架设计，能够同时解决联邦音频分类中的多重挑战，而非独立处理。双模型架构（本地个性化模型与全局共享插件模型）结合双向知识蒸馏，有效平衡了泛化和个性化需求。此外，LPA策略的引入为联邦学习中的数据中毒问题提供了实用的鲁棒性增强机制。这项工作为实际联邦音频应用提供了更全面、更鲁棒的解决方案。"}}
{"id": "2506.10121", "title": "HiKO: A Hierarchical Framework for Beyond-Second-Order KO Codes", "authors": ["Shubham Srivastava", "Adrish Banerjee"], "summary": "This paper introduces HiKO (Hierarchical Kronecker Operation), a novel\nframework for training high-rate neural error-correcting codes that enables KO\ncodes to outperform Reed-Muller codes beyond second order. To our knowledge,\nthis is the first attempt to extend KO codes beyond second order. While\nconventional KO codes show promising results for low-rate regimes ($r < 2$),\nthey degrade at higher rates -- a critical limitation for practical deployment.\nOur framework incorporates three key innovations: (1) a hierarchical training\nmethodology that decomposes complex high-rate codes into simpler constituent\ncodes for efficient knowledge transfer, (2) enhanced neural architectures with\ndropout regularization and learnable skip connections tailored for the Plotkin\nstructure, and (3) a progressive unfreezing strategy that systematically\ntransitions from pre-trained components to fully optimized integrated codes.\nOur experiments show that HiKO codes consistently outperform traditional\nReed-Muller codes across various configurations, achieving notable performance\nimprovements for third-order ($r = 3$) and fourth-order ($r = 4$) codes.\nAnalysis reveals that HiKO codes successfully approximate Shannon-optimal\nGaussian codebooks while preserving efficient decoding properties. This\nrepresents the first successful extension of KO codes beyond second order,\nopening new possibilities for neural code deployment in high-throughput\ncommunication systems.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10121v1", "AI": {"title_translation": "HiKO：超越二阶KO码的层次化框架", "tldr": "HiKO是一个用于训练高码率神经纠错码的新框架，首次将KO码扩展到二阶以上，通过层次化训练、增强型神经网络架构和渐进式解冻策略，使KO码在高码率下优于Reed-Muller码，并逼近香农最优高斯码本。", "motivation": "传统的KO码在低码率（r < 2）下表现良好，但在高码率下性能下降，这限制了其实际部署。因此，需要一种方法来扩展KO码的性能至高阶，以适应高码率应用。", "method": "该论文提出了HiKO框架，包含三个关键创新点：(1) 分解复杂高码率码为简单组成码的层次化训练方法，以实现高效知识迁移。(2) 针对Plotkin结构，增强了包含dropout正则化和可学习跳跃连接的神经网络架构。(3) 渐进式解冻策略，系统地从预训练组件过渡到完全优化的集成码。", "result": "实验表明，HiKO码在各种配置下持续优于传统的Reed-Muller码，在三阶（r = 3）和四阶（r = 4）码方面取得了显著的性能提升。分析表明，HiKO码成功地逼近了香农最优高斯码本，同时保留了高效的解码特性。", "conclusion": "HiKO框架首次成功地将KO码扩展到二阶以上，为高吞吐量通信系统中神经码的部署开辟了新的可能性。", "translation": "本文介绍了HiKO（分层克罗内克运算），一个用于训练高码率神经纠错码的新颖框架，使KO码能够超越二阶Reed-Muller码。据我们所知，这是首次尝试将KO码扩展到二阶以上。虽然传统的KO码在低码率范围（r < 2）显示出有希望的结果，但它们在高码率下性能下降——这是实际部署的关键限制。我们的框架包含了三个关键创新点：（1）一种分层训练方法，将复杂的高码率码分解为更简单的组成码，以实现高效的知识迁移；（2）针对Plotkin结构，增强了包含dropout正则化和可学习跳跃连接的神经网络架构；（3）一种渐进式解冻策略，系统地从预训练组件过渡到完全优化的集成码。我们的实验表明，HiKO码在各种配置下持续优于传统的Reed-Muller码，在三阶（r = 3）和四阶（r = 4）码方面取得了显著的性能改进。分析揭示，HiKO码成功地逼近了香农最优高斯码本，同时保留了高效的解码特性。这代表了KO码首次成功扩展到二阶以上，为高吞吐量通信系统中神经码的部署开辟了新的可能性。", "summary": "该论文提出了HiKO（分层克罗内克运算），一个用于训练高码率神经纠错码的新颖框架，旨在解决传统KO码在高码率下性能下降的问题。HiKO通过引入分层训练方法、增强型神经网络架构和渐进式解冻策略，首次成功地将KO码扩展到二阶以上。实验证明，HiKO码在三阶和四阶码上显著优于Reed-Muller码，并能逼近香农最优高斯码本，为高吞吐量通信系统中的神经码部署提供了新的方向。", "keywords": "神经纠错码, KO码, 高码率, 层次化框架, 香农最优", "comments": "该论文的创新点在于首次成功地将KO码扩展到二阶以上，并通过HiKO框架解决了其在高码率下的性能瓶颈。其提出的层次化训练、增强型神经网络和渐进式解冻策略是解决此问题的关键。这项工作对于推动神经纠错码在实际高吞吐量通信系统中的应用具有重要意义。"}}
{"id": "2506.10052", "title": "Quantum resources in resource management systems", "authors": ["Iskandar Sitdikov", "M. Emre Sahin", "Utz Bacher", "Aleksander Wennersteen", "Andrew Damin", "Mark Birmingham", "Philippa Rubin", "Stefano Mensa", "Matthieu Moreau", "Aurelien Nober", "Hitomi Takahashi", "Munetaka Ohtani"], "summary": "Quantum computers are beginning to operate in high-performance computing\n(HPC) environments. Quantum can complement classical resources for specific\nworkloads, but their adoption depends on integration into existing HPC\ninfrastructure. Treating quantum devices as first-class resources allows for\nunified scheduling, improved usability, and support for hybrid\nquantum-classical applications. This paper presents the design architecture and\nreference implementation for quantum resources control using existing workload\nmanagement systems. We introduce a suite of plugins for Slurm that enable\nintegration of on-prem and cloud quantum computing resources into existing\nhigh-performance computing centers. The paper details the interface design,\nplugin concept and implementation, operational aspects for heterogeneous\ncompute clusters, as well as considerations for other resource management\nsystems.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10052v1", "AI": {"title_translation": "资源管理系统中的量子资源", "tldr": "论文提出了一个将量子计算资源整合到现有高性能计算（HPC）环境中的架构和实现，特别是通过Slurm插件实现统一调度和管理。", "motivation": "量子计算开始进入HPC环境，但其采用依赖于与现有HPC基础设施的集成，以实现统一调度、提高可用性以及支持混合量子-经典应用。", "method": "本文提出了一种用于量子资源控制的设计架构和参考实现，利用现有工作负载管理系统。具体引入了一套用于Slurm的插件，用于将本地和云量子计算资源集成到现有HPC中心。论文详细介绍了接口设计、插件概念和实现、异构计算集群的操作方面以及对其他资源管理系统的考虑。", "result": "论文提出了一个设计架构和参考实现（Slurm插件套件），展示了将量子资源集成到现有HPC环境的可行性，实现了统一调度和支持混合应用。", "conclusion": "通过将量子设备视为一流资源并集成到现有工作负载管理系统（如Slurm），可以实现量子资源在HPC环境中的统一调度、提高可用性并支持混合量子-经典应用，从而促进量子计算的采用。", "translation": "量子计算机正开始在高性能计算（HPC）环境中运行。量子资源可以补充经典资源以处理特定工作负载，但其采用取决于与现有HPC基础设施的集成。将量子设备视为一流资源可以实现统一调度、提高可用性以及支持混合量子-经典应用程序。本文介绍了使用现有工作负载管理系统控制量子资源的设计架构和参考实现。我们为Slurm引入了一套插件，使得本地和云端量子计算资源能够集成到现有高性能计算中心。本文详细阐述了接口设计、插件概念和实现、异构计算集群的操作方面，以及对其他资源管理系统的考虑。", "summary": "本文提出了一种将量子计算资源无缝集成到现有高性能计算（HPC）环境中的方法。通过将量子设备视为一流资源，并利用现有的工作负载管理系统（如Slurm），实现了统一的资源调度和对混合量子-经典应用的支持。论文详细介绍了其设计架构和一套Slurm插件的参考实现，为HPC中心整合本地和云端量子资源提供了解决方案。", "keywords": "量子计算, 资源管理, 高性能计算, Slurm, 混合计算", "comments": "这篇论文解决了量子计算在高性能计算环境中实际应用的关键挑战，即如何将其有效地集成到现有基础设施中。通过提出基于Slurm的插件方案，它提供了一个实用的、可扩展的解决方案，有助于推动量子和经典计算的混合应用。其创新点在于将量子资源作为“一流资源”进行管理，这对于未来的混合计算范式至关重要。"}}
{"id": "2506.10135", "title": "Inference of Hierarchical Core-Periphery Structure in Temporal Network", "authors": ["Theodore Y. Faust", "Mason A. Porter"], "summary": "Networks can have various types of mesoscale structures. One type of\nmesoscale structure in networks is core-periphery structure, which consists of\ndensely-connected core nodes and sparsely-connected peripheral nodes. The core\nnodes are connected densely to each other and can be connected to the\nperipheral nodes, which are connected sparsely to other nodes. There has been\nmuch research on core-periphery structure in time-independent networks, but few\ncore-periphery detection methods have been developed for time-dependent (i.e.,\n``temporal\") networks. Using a multilayer-network representation of temporal\nnetworks and an inference approach that employs stochastic block models, we\ngeneralize a recent method for detecting hierarchical core-periphery structure\n\\cite{Polanco23} from time-independent networks to temporal networks. In\ncontrast to ``onion-like'' nested core-periphery structures (where each node is\nassigned to a group according to how deeply it is nested in a network's core),\nhierarchical core-periphery structures encompass networks with nested\nstructures, tree-like structures (where any two groups must either be disjoint\nor have one as a strict subset of the other), and general non-nested mesoscale\nstructures (where the group assignments of nodes do not have to be nested in\nany way). To perform statistical inference and thereby identify core-periphery\nstructure, we use a Markov-chain Monte Carlo (MCMC) approach. We illustrate our\nmethod for detecting hierarchical core-periphery structure in two real-world\ntemporal networks, and we briefly discuss the structures that we identify in\nthese networks.", "comment": null, "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.10135v1", "AI": {"title_translation": "时间网络中分层核心-边缘结构的推断", "tldr": "该研究提出了一种新的方法，用于在时间网络中推断分层核心-边缘结构，该方法结合了多层网络表示、随机块模型和马尔可夫链蒙特卡罗方法，以解决现有方法在时间依赖网络中检测核心-边缘结构的不足。", "motivation": "现有研究在时间无关网络的核心-边缘结构方面已有很多进展，但针对时间依赖（即“时间”）网络的核心-边缘检测方法却很少。", "method": "本研究使用时间网络的多层网络表示和采用随机块模型进行推断的方法，将最近用于检测时间无关网络中分层核心-边缘结构的方法推广到时间网络。为进行统计推断并识别核心-边缘结构，研究采用了马尔可夫链蒙特卡罗（MCMC）方法。", "result": "该方法在两个真实世界的时间网络中进行了分层核心-边缘结构检测的说明，并简要讨论了在这些网络中识别出的结构。", "conclusion": "本研究成功地将分层核心-边缘结构检测方法推广到了时间网络，并通过在真实世界网络中的应用展示了其有效性。", "translation": "网络可以具有各种类型的中尺度结构。网络中的一种中尺度结构是核心-边缘结构，它由连接紧密的核心节点和连接稀疏的边缘节点组成。核心节点彼此之间连接紧密，并可以连接到边缘节点，边缘节点与其他节点的连接稀疏。关于时间无关网络中核心-边缘结构的研究很多，但针对时间依赖（即“时间”）网络的核心-边缘检测方法却很少。我们使用时间网络的多层网络表示和采用随机块模型进行推断的方法，将最近用于检测分层核心-边缘结构的方法[Polanco23]从时间无关网络推广到时间网络。与“洋葱状”嵌套核心-边缘结构（其中每个节点根据其在网络核心中的嵌套深度被分配到一个组）不同，分层核心-边缘结构包含具有嵌套结构、树状结构（其中任意两个组必须要么不相交，要么一个严格包含另一个）和一般非嵌套中尺度结构（其中节点的组分配不必以任何方式嵌套）的网络。为进行统计推断并识别核心-边缘结构，我们使用马尔可夫链蒙特卡罗（MCMC）方法。我们在两个真实世界的时间网络中说明了我们检测分层核心-边缘结构的方法，并简要讨论了我们在这些网络中识别出的结构。", "summary": "本论文提出了一种新颖的方法，用于在时间网络中推断分层核心-边缘结构。该方法通过将时间网络表示为多层网络，并结合使用随机块模型和马尔可夫链蒙特卡罗（MCMC）方法进行统计推断，解决了现有核心-边缘检测方法在时间依赖网络中的局限性。与传统的“洋葱状”嵌套结构不同，本方法能够处理更广泛的层次结构，包括嵌套、树状和非嵌套的中尺度结构。研究通过在两个真实世界的时间网络中的应用，验证了该方法的有效性。", "keywords": "时间网络, 核心-边缘结构, 分层结构, 随机块模型, 马尔可夫链蒙特卡罗", "comments": "该论文的创新之处在于将核心-边缘结构检测方法从静态网络推广到时间网络，并进一步考虑了分层结构，这在复杂网络分析中具有重要意义。通过引入多层网络表示和随机块模型，并结合MCMC进行推断，为理解时间演化网络中的动态社团结构提供了新的工具。其重要性在于能够揭示时间网络中更深层次的组织模式，有助于理解各种真实世界系统中节点交互的动态性。"}}
{"id": "2506.10170", "title": "Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot", "authors": ["Anh T. Nguyen", "Ajay Anand", "Michelle J. Johnson"], "summary": "Action observation (AO) therapy is a promising rehabilitative treatment for\nmotor and language function in individuals recovering from neurological\nconditions, such as stroke. This pilot study aimed to investigate the potential\nof humanoid robots to support AO therapy in rehabilitation settings. The brain\nactivity of three healthy right-handed participants was monitored with\nelectroencephalography (EEG) while they observed eight different actions\nperformed by two agents, a human actor and a robot, using their left and right\narms. Their event-related spectral perturbations (ERSPs, changes in the\nspectral power of neural oscillations in response to an event or stimulus,\ncompared to baseline) in sensorimotor regions were analyzed. The single-subject\nanalysis showed variability in ERSP patterns among all participants, including\npower suppression in sensorimotor mu and beta rhythms. One participant showed\nstronger responses to \"robot\" AO conditions than to \"human\" conditions. Strong\nand positive correlations in ERSP across all conditions were observed for\nalmost all participants and channels, implying common cognitive processes or\nneural networks at play in the mirror neuron system during AO. The results\nsupport the feasibility of using EEG to explore differences in neural responses\nto observation of robot- and human-induced actions.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10170v1", "AI": {"title_translation": "探索观察人类演员和仿人机器人执行动作时的脑电图反应", "tldr": "本初步研究使用脑电图（EEG）探索了观察人类和仿人机器人执行动作时的大脑反应，以评估机器人支持动作观察（AO）疗法的潜力，发现个体间存在差异，但结果支持使用脑电图探索机器人和人类动作观察的神经反应差异的可行性。", "motivation": "动作观察（AO）疗法对于神经系统疾病（如中风）患者的运动和语言功能康复是一种有前景的治疗方法。本初步研究旨在探讨仿人机器人在康复环境中支持AO疗法的潜力。", "method": "研究使用脑电图（EEG）监测了三名健康的右撇子参与者的大脑活动，他们观察了人类演员和机器人两种执行者使用左右臂完成的八种不同动作。分析了其感觉运动区域的事件相关频谱扰动（ERSPs）。", "result": "单受试者分析显示所有参与者的ERSP模式存在变异性，包括感觉运动mu和beta节律的功率抑制。一名参与者对“机器人”AO条件的反应强于“人类”条件。几乎所有参与者和通道在所有条件下都观察到ERSP的强正相关，这暗示了在动作观察期间镜像神经元系统中存在共同的认知过程或神经网络。", "conclusion": "研究结果支持使用脑电图探索观察机器人和人类引发的动作时神经反应差异的可行性。", "translation": "动作观察（AO）疗法是一种有前景的康复治疗方法，用于帮助中风等神经系统疾病患者恢复运动和语言功能。这项初步研究旨在探讨仿人机器人在康复环境中支持AO疗法的潜力。研究使用脑电图（EEG）监测了三名健康的右撇子参与者的大脑活动，他们观察了人类演员和机器人两种执行者使用左右臂完成的八种不同动作。分析了他们感觉运动区域的事件相关频谱扰动（ERSPs，即与基线相比，神经振荡频谱功率对事件或刺激的响应变化）。单受试者分析显示所有参与者的ERSP模式存在变异性，包括感觉运动mu和beta节律的功率抑制。一名参与者对“机器人”AO条件的反应强于“人类”条件。几乎所有参与者和通道在所有条件下都观察到ERSP的强正相关，这暗示了在动作观察期间镜像神经元系统中存在共同的认知过程或神经网络。研究结果支持使用脑电图探索观察机器人和人类引发的动作时神经反应差异的可行性。", "summary": "本初步研究旨在探索仿人机器人在动作观察（AO）疗法中的应用潜力，通过脑电图（EEG）监测三名健康参与者在观察人类演员和机器人执行动作时的大脑活动。结果显示个体间事件相关频谱扰动（ERSPs）模式存在差异，包括感觉运动mu和beta节律的功率抑制，且一名参与者对机器人动作观察的反应更强。同时，多数参与者和通道在所有条件下ERSP存在强正相关，表明镜像神经元系统存在共同的神经过程。研究验证了使用EEG探索观察机器人和人类动作时神经反应差异的可行性。", "keywords": "动作观察, 仿人机器人, 脑电图, 康复, 镜像神经元系统", "comments": "这是一项有意义的初步研究，探讨了仿人机器人在康复治疗中的潜在应用，特别是与动作观察疗法结合。其创新点在于将机器人作为动作观察的对象，并使用EEG进行神经反应的探索。然而，由于是初步研究，样本量（三名参与者）非常小，这限制了结果的普遍性。未来的研究需要扩大样本量以验证这些发现，并深入探讨个体差异的原因。"}}
{"id": "2506.10658", "title": "Contrastive Matrix Completion with Denoising and Augmented Graph Views for Robust Recommendation", "authors": ["Narges Nemati", "Mostafa Haghir Chehreghani"], "summary": "Matrix completion is a widely adopted framework in recommender systems, as\npredicting the missing entries in the user-item rating matrix enables a\ncomprehensive understanding of user preferences. However, current graph neural\nnetwork (GNN)-based approaches are highly sensitive to noisy or irrelevant\nedges--due to their inherent message-passing mechanisms--and are prone to\noverfitting, which limits their generalizability. To overcome these challenges,\nwe propose a novel method called Matrix Completion using Contrastive Learning\n(MCCL). Our approach begins by extracting local neighborhood subgraphs for each\ninteraction and subsequently generates two distinct graph representations. The\nfirst representation emphasizes denoising by integrating GNN layers with an\nattention mechanism, while the second is obtained via a graph variational\nautoencoder that aligns the feature distribution with a standard prior. A\nmutual learning loss function is employed during training to gradually\nharmonize these representations, enabling the model to capture common patterns\nand significantly enhance its generalizability. Extensive experiments on\nseveral real-world datasets demonstrate that our approach not only improves the\nnumerical accuracy of the predicted scores--achieving up to a 0.8% improvement\nin RMSE--but also produces superior rankings with improvements of up to 36% in\nranking metrics.", "comment": "30 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10658v1", "AI": {"title_translation": "基于去噪和增强图视图的对比矩阵补全，用于鲁棒推荐", "tldr": "针对推荐系统中基于GNN的矩阵补全方法对噪声敏感且易过拟合的问题，本文提出了一种名为MCCL的新方法。MCCL通过对比学习，结合去噪和增强图视图，显著提高了预测准确性和排名性能。", "motivation": "当前基于图神经网络（GNN）的推荐系统中的矩阵补全方法对噪声或不相关边缘高度敏感，并且容易过拟合，这限制了它们的泛化能力。", "method": "本文提出了一种名为对比学习矩阵补全（MCCL）的新方法。该方法首先为每个交互提取局部邻域子图，然后生成两种不同的图表示。第一种表示通过集成GNN层与注意力机制来强调去噪，而第二种则通过图变分自编码器获得，使特征分布与标准先验对齐。训练过程中采用互学习损失函数逐步协调这些表示，使模型能够捕获共同模式并显著增强其泛化能力。", "result": "在多个真实世界数据集上的广泛实验表明，该方法不仅提高了预测分数的数值准确性（RMSE提高了0.8%），而且在排名指标上产生了更优越的排名（排名指标提高了36%）。", "conclusion": "MCCL方法有效解决了基于GNN的推荐系统中噪声敏感性和过拟合的挑战，从而提高了预测准确性和排名性能，增强了模型的泛化能力。", "translation": "矩阵补全是推荐系统中广泛采用的框架，因为预测用户-物品评分矩阵中的缺失条目可以全面了解用户偏好。然而，当前基于图神经网络（GNN）的方法由于其固有的消息传递机制，对噪声或不相关边缘高度敏感，并且容易过拟合，这限制了它们的泛化能力。为了克服这些挑战，我们提出了一种名为使用对比学习的矩阵补全（MCCL）的新方法。我们的方法首先为每个交互提取局部邻域子图，随后生成两种不同的图表示。第一种表示通过集成GNN层与注意力机制来强调去噪，而第二种则通过图变分自编码器获得，使特征分布与标准先验对齐。训练过程中采用互学习损失函数逐步协调这些表示，使模型能够捕获共同模式并显著增强其泛化能力。在多个真实世界数据集上的广泛实验表明，我们的方法不仅提高了预测分数的数值准确性（RMSE提高了0.8%），而且在排名指标上产生了更优越的排名（排名指标提高了36%）。", "summary": "为解决推荐系统中GNN基矩阵补全方法对噪声敏感及易过拟合的问题，本文提出了一种新颖的对比学习矩阵补全（MCCL）方法。MCCL通过为每个交互生成两种不同的图表示来工作：一种通过GNN与注意力机制进行去噪，另一种通过图变分自编码器进行特征对齐。通过互学习损失函数协调这两种表示，模型能够捕获通用模式并增强泛化能力。实验结果显示，MCCL在预测精度和排名性能上均取得了显著提升。", "keywords": "矩阵补全, 对比学习, 图神经网络, 推荐系统, 去噪", "comments": "该论文的创新点在于其结合了对比学习与双重图视图（去噪和增强）的策略，并通过互学习损失函数来明确解决GNN在推荐系统中面临的鲁棒性和泛化能力问题。这种多方面处理噪声和过拟合的方法是其重要的贡献，为构建更稳健的推荐系统提供了新的思路。"}}
{"id": "2506.10029", "title": "Evaluation empirique de la sécurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vulnérabilités par expérimentations de jailbreaks", "authors": ["Rafaël Nouailles"], "summary": "Large Language models (LLMs) are transforming digital usage, particularly in\ntext generation, image creation, information retrieval and code development.\nChatGPT, launched by OpenAI in November 2022, quickly became a reference,\nprompting the emergence of competitors such as Google's Gemini. However, these\ntechnological advances raise new cybersecurity challenges, including prompt\ninjection attacks, the circumvention of regulatory measures (jailbreaking), the\nspread of misinformation (hallucinations) and risks associated with deep fakes.\nThis paper presents a comparative analysis of the security and alignment levels\nof ChatGPT and Gemini, as well as a taxonomy of jailbreak techniques associated\nwith experiments.", "comment": "in French language", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10029v1", "AI": {"title_translation": "ChatGPT 和 Gemini 的安全性与对齐性实证评估：通过越狱实验进行的漏洞比较分析", "tldr": "本文对 ChatGPT 和 Gemini 的安全性与对齐水平进行比较分析，并探讨越狱技术及其分类。", "motivation": "大型语言模型（LLMs）的快速发展带来了新的网络安全挑战，如提示注入攻击和越狱，因此需要对这些模型的安全性进行评估。", "method": "本文对 ChatGPT 和 Gemini 的安全性与对齐水平进行了比较分析，并结合越狱实验提出了越狱技术分类。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "大型语言模型（LLMs）正在改变数字使用方式，特别是在文本生成、图像创建、信息检索和代码开发方面。OpenAI 于 2022 年 11 月推出的 ChatGPT 迅速成为标杆，促使 Google 的 Gemini 等竞争对手的出现。然而，这些技术进步也带来了新的网络安全挑战，包括提示注入攻击、规避监管措施（越狱）、错误信息传播（幻觉）以及与深度伪造相关的风险。本文对 ChatGPT 和 Gemini 的安全性和对齐水平进行了比较分析，并提出了与实验相关的越狱技术分类。", "summary": "本文对大型语言模型 ChatGPT 和 Gemini 的安全性与对齐性进行了实证比较分析，重点关注其在文本生成、图像创建、信息检索和代码开发等应用中面临的网络安全挑战，如提示注入和越狱，并提出了一种越狱技术分类。", "keywords": "大型语言模型, ChatGPT, Gemini, 安全性, 越狱", "comments": "该研究的重要性在于其对当前主流大型语言模型安全性的及时评估，特别是关注越狱这一新兴且关键的网络安全威胁。通过比较分析和越狱技术分类，为理解和应对 LLM 滥用提供了基础。"}}
{"id": "2506.10051", "title": "The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks", "authors": ["Md Istiak Hossain Shihab", "Christopher Hundhausen", "Ahsun Tariq", "Summit Haque", "Yunhan Qiao", "Brian Mulanda"], "summary": "When graduates of computing degree programs enter the software industry, they\nwill most likely join teams working on legacy code bases developed by people\nother than themselves. In these so-called brownfield software development\nsettings, generative artificial intelligence (GenAI) coding assistants like\nGitHub Copilot are rapidly transforming software development practices, yet the\nimpact of GenAI on student programmers performing brownfield development tasks\nremains underexplored. This paper investigates how GitHub Copilot influences\nundergraduate students' programming performance, behaviors, and understanding\nwhen completing brownfield programming tasks in which they add new code to an\nunfamiliar code base. We conducted a controlled experiment in which 10\nundergraduate computer science students completed highly similar brownfield\ndevelopment tasks with and without Copilot in a legacy web application. Using a\nmixed-methods approach combining performance analysis, behavioral analysis, and\nexit interviews, we found that students completed tasks 35% faster (p < 0.05)\nand made 50% more solution progress p (< 0.05) when using Copilot. Moreover,\nour analysis revealed that, when using Copilot, students spent 11% less time\nmanually writing code (p < 0.05), and 12% less time conducting web searches (p\n< 0.05), providing evidence of a fundamental shift in how they engaged in\nprogramming. In exit interviews, students reported concerns about not\nunderstanding how or why Copilot suggestions work. This research suggests the\nneed for computing educators to develop new pedagogical approaches that\nleverage GenAI assistants' benefits while fostering reflection on how and why\nGenAI suggestions address brownfield programming tasks. Complete study results\nand analysis are presented at https://ghcopilot-icer.github.io/.", "comment": "14 pages, 5 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10051v1", "AI": {"title_translation": "GitHub Copilot 对计算专业学生在棕地编程任务中编程效率、效果和过程的影响", "tldr": "本研究发现GitHub Copilot显著提高了计算专业学生在棕地编程任务中的编程效率和进展，但学生对其建议的理解存在顾虑，这提示教育者需调整教学方法。", "motivation": "软件行业毕业生通常会处理遗留代码库，而生成式AI编码助手（如GitHub Copilot）正在迅速改变软件开发实践。然而，GenAI对学生程序员在“棕地”开发任务中表现的影响尚未得到充分探索。", "method": "研究通过一项对照实验进行，共有10名本科计算机科学学生参与。他们在遗留Web应用程序中，分别使用和不使用GitHub Copilot完成高度相似的棕地开发任务。研究采用混合方法，结合了性能分析、行为分析和离职访谈。", "result": "使用Copilot时，学生完成任务的速度提高了35%（p < 0.05），解决方案进展提高了50%（p < 0.05）。此外，学生手动编写代码的时间减少了11%（p < 0.05），进行网络搜索的时间减少了12%（p < 0.05），这表明他们的编程参与方式发生了根本性转变。然而，学生在离职访谈中表示担忧不理解Copilot建议的工作原理或原因。", "conclusion": "本研究表明，计算教育者需要开发新的教学方法，以利用GenAI助手的优势，同时培养学生对GenAI建议如何以及为何解决棕地编程任务的思考能力。", "translation": "当计算学位项目的毕业生进入软件行业时，他们很可能会加入处理由他人开发的遗留代码库的团队。在这些所谓的“棕地”软件开发环境中，像GitHub Copilot这样的生成式人工智能（GenAI）编码助手正在迅速改变软件开发实践，但GenAI对执行棕地开发任务的学生程序员的影响仍未得到充分探索。本文研究了GitHub Copilot如何影响本科生在完成向不熟悉的代码库添加新代码的棕地编程任务时的编程表现、行为和理解。我们进行了一项对照实验，其中10名本科计算机科学学生在遗留Web应用程序中，分别使用和不使用Copilot完成了高度相似的棕地开发任务。我们采用混合方法，结合了性能分析、行为分析和离职访谈，发现学生在使用Copilot时完成任务的速度提高了35%（p < 0.05），解决方案进展提高了50%（p < 0.05）。此外，我们的分析显示，在使用Copilot时，学生手动编写代码的时间减少了11%（p < 0.05），进行网络搜索的时间减少了12%（p < 0.05），这提供了他们参与编程方式发生根本性转变的证据。在离职访谈中，学生表达了对不理解Copilot建议如何或为何起作用的担忧。这项研究表明，计算教育者需要开发新的教学方法，以利用GenAI助手的优势，同时培养学生对GenAI建议如何以及为何解决棕地编程任务的思考能力。完整的学习结果和分析可在https://ghcopilot-icer.github.io/上查阅。", "summary": "本研究调查了GitHub Copilot对本科生在棕地编程任务中表现、行为和理解的影响。通过一项有10名学生参与的对照实验，结果显示使用Copilot的学生完成任务速度提高35%，解决方案进展提高50%。同时，学生手动编码和网络搜索的时间显著减少。然而，学生也表达了对Copilot建议理解不足的担忧。研究强调了教育者需要开发新的教学方法，以充分利用GenAI工具的优势，同时促进学生对其工作原理的深入理解。", "keywords": "GitHub Copilot, 棕地编程, 学生程序员, 生成式AI, 编程教育", "comments": "本研究通过实验量化了GitHub Copilot在棕地编程任务中对学生效率的积极影响，并揭示了编程方式的转变，具有重要的实践意义。同时，它也指出了一个关键的教学挑战：如何确保学生在使用AI工具的同时，不失去对底层原理的理解和批判性思维能力。研究结果为未来的计算教育改革提供了宝贵的见解。"}}
{"id": "2506.10197", "title": "Intergenerational AI Literacy in Korean Immigrant Families: Interpretive Gatekeeping Meets Convenient Critical Deferment", "authors": ["Jeongone Seo", "Ryan Womack", "Tawfiq Ammari"], "summary": "As artificial intelligence (AI) becomes deeply integrated into family life,\nimmigrant families must navigate unique intergenerational, linguistic, and\ncultural challenges. This study examines how Korean immigrant families in the\nUnited States negotiate the use of AI tools such as ChatGPT and smart\nassistants in their homes. Through 20 semi-structured interviews with parents\nand teens, we identify two key practices that shape their engagement:\ninterpretive gatekeeping, where parents mediate their children's AI use through\na lens of cultural and ethical values, and convenient critical deferment, where\nteens strategically postpone critical evaluation of AI for immediate academic\nand social utility. These intertwined practices challenge conventional,\nskills-based models of AI literacy, revealing it instead as a dynamic and\nrelational practice co-constructed through ongoing family negotiation. We\ncontribute to information science and HCI by offering a new conceptual\nextension for intergenerational AI literacy and providing design implications\nfor more equitable, culturally attuned, and family-centered AI systems.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10197v1", "AI": {"title_translation": "韩国移民家庭中的代际AI素养：解释性把关与便捷的批判性延迟", "tldr": "本研究探讨了韩国移民家庭在AI使用中面临的代际、语言和文化挑战，识别出“解释性把关”和“便捷的批判性延迟”两种关键实践，并挑战了传统的AI素养模型。", "motivation": "随着人工智能（AI）深入家庭生活，移民家庭必须应对独特的代际、语言和文化挑战。本研究旨在考察美国韩裔移民家庭如何协商在家庭中使用AI工具（如ChatGPT和智能助手）。", "method": "通过对20名家长和青少年进行半结构化访谈。", "result": "研究识别出两种关键实践：一是“解释性把关”，即父母通过文化和伦理价值观来中介孩子对AI的使用；二是“便捷的批判性延迟”，即青少年为了即时的学术和社会效用而策略性地推迟对AI的批判性评估。", "conclusion": "这些相互交织的实践挑战了传统的、基于技能的AI素养模型，揭示AI素养是一种通过持续的家庭协商共同构建的动态和关系性实践。研究为代际AI素养提供了新的概念扩展，并为更公平、文化适应性更强、以家庭为中心的AI系统提供了设计启示。", "translation": "人工智能（AI）日益深入家庭生活，移民家庭必须应对独特的代际、语言和文化挑战。本研究考察了美国韩裔移民家庭如何协商在家庭中使用ChatGPT和智能助手等AI工具。通过对20名家长和青少年进行半结构化访谈，我们识别出两种塑造他们参与的关键实践：“解释性把关”，即父母通过文化和伦理价值观来中介孩子对AI的使用；以及“便捷的批判性延迟”，即青少年为了即时的学术和社会效用而策略性地推迟对AI的批判性评估。这些相互交织的实践挑战了传统的、基于技能的AI素养模型，反而揭示AI素养是一种通过持续的家庭协商共同构建的动态和关系性实践。我们通过为代际AI素养提供新的概念扩展，并为更公平、文化适应性更强、以家庭为中心的AI系统提供设计启示，从而为信息科学和人机交互领域做出贡献。", "summary": "本研究探讨了美国韩裔移民家庭如何协商家庭中AI工具的使用，通过20次访谈揭示了两种独特的代际实践：“解释性把关”（父母基于文化伦理中介AI使用）和“便捷的批判性延迟”（青少年为实用性推迟批判性评估）。这些发现挑战了传统AI素养模型，提出AI素养是家庭协商中的动态关系实践，并为设计更具文化适应性的AI系统提供了启示。", "keywords": "代际AI素养, 韩国移民家庭, 解释性把关, 批判性延迟, 家庭协商", "comments": "这项研究创新性地将AI素养置于家庭和文化语境中，特别是移民家庭的代际互动，挑战了以往过于侧重技能的AI素养定义。提出的“解释性把关”和“便捷的批判性延迟”概念深刻揭示了AI在实际家庭场景中的复杂性，对于理解AI的社会影响和设计更人性化的AI系统具有重要意义。"}}
{"id": "2506.10851", "title": "Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers", "authors": ["Adel Chehade", "Edoardo Ragusa", "Paolo Gastaldo", "Rodolfo Zunino"], "summary": "In this paper, we present a practical deep learning (DL) approach for\nenergy-efficient traffic classification (TC) on resource-limited\nmicrocontrollers, which are widely used in IoT-based smart systems and\ncommunication networks. Our objective is to balance accuracy, computational\nefficiency, and real-world deployability. To that end, we develop a lightweight\n1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which\nachieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K\nparameters, a 20.12K maximum tensor size, and 10.08M floating-point operations\n(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies\nranging from 94% to 99%. To enable deployment, the model is quantized to INT8,\nsuffering only a marginal 1-2% accuracy drop relative to its Float32\ncounterpart. We evaluate real-world inference performance on two\nmicrocontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive\nNucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and\n115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,\nrespectively. These results demonstrate the feasibility of on-device encrypted\ntraffic analysis, paving the way for scalable, low-power IoT security\nsolutions.", "comment": "Accepted at IEEE ISCC 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10851v1", "AI": {"title_translation": "能量高效的深度学习在微控制器上的流量分类", "tldr": "本文提出了一种能量高效的轻量级1D-CNN模型，通过硬件感知NAS优化并在微控制器上实现部署，用于流量分类，实现了高精度、低功耗和低延迟，为物联网安全提供了可行方案。", "motivation": "在资源受限的微控制器上实现能量高效的流量分类，以平衡准确性、计算效率和实际部署能力，满足物联网智能系统和通信网络的需求。", "method": "开发了一个轻量级的1D-CNN模型，通过硬件感知神经架构搜索（HW-NAS）进行优化。模型被量化为INT8以实现部署。在ISCX VPN-NonVPN数据集上进行评估，并在两种微控制器上测试实际推理性能。", "result": "优化后的1D-CNN在ISCX VPN-NonVPN数据集上达到96.59%的准确率，仅有88.26K参数、20.12K最大张量大小和10.08M FLOPs。模型在不同流量分类任务上泛化能力强，准确率范围94%至99%。INT8量化后准确率仅下降1-2%。在STM32F746G-DISCO和Nucleo-F401RE微控制器上，推理延迟分别为31.43ms和115.40ms，能耗分别为7.86 mJ和29.10 mJ。", "conclusion": "结果表明在设备上进行加密流量分析的可行性，为可扩展、低功耗的物联网安全解决方案铺平了道路。", "translation": "本文提出了一种实用的深度学习（DL）方法，用于在资源受限的微控制器上进行能量高效的流量分类（TC），这些微控制器广泛应用于基于物联网的智能系统和通信网络中。我们的目标是平衡准确性、计算效率和实际部署能力。为此，我们开发了一个轻量级的1D-CNN，通过硬件感知神经架构搜索（HW-NAS）进行优化，在ISCX VPN-NonVPN数据集上实现了96.59%的准确率，仅有88.26K参数、20.12K最大张量大小和10.08M浮点运算（FLOPs）。此外，它在各种TC任务中具有泛化能力，准确率范围从94%到99%。为了实现部署，该模型被量化为INT8，相对于其Float32版本，准确率仅有1-2%的微小下降。我们在两种微控制器上评估了实际推理性能：高性能的STM32F746G-DISCO和成本敏感的Nucleo-F401RE。部署的模型分别实现了31.43ms和115.40ms的推理延迟，以及每次推理7.86 mJ和29.10 mJ的能耗。这些结果证明了设备上加密流量分析的可行性，为可扩展、低功耗的物联网安全解决方案铺平了道路。", "summary": "本文提出一种在资源受限微控制器上进行能量高效流量分类的深度学习方法。通过硬件感知神经架构搜索优化，开发了一个轻量级1D-CNN模型。该模型在保持高准确率（96.59%）的同时，显著降低了参数量和计算量，并成功量化为INT8，仅有微小精度损失。实验证明，其在实际微控制器上实现了低延迟和低能耗的推理，为物联网安全提供了可行的片上加密流量分析方案。", "keywords": "流量分类, 深度学习, 微控制器, 能量高效, 硬件感知NAS", "comments": "本文的创新点在于结合硬件感知神经架构搜索，为资源受限的微控制器量身定制轻量级1D-CNN模型，实现了高精度、低功耗和低延迟的流量分类。其重要性在于验证了在边缘设备上进行复杂深度学习任务的可行性，尤其是在物联网安全领域，为部署可扩展、低功耗的解决方案提供了坚实基础。"}}
{"id": "2506.10106", "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture", "authors": ["Marcos Abel Zuzuárregui", "Mustafa Melih Toslak", "Stefano Carpin"], "summary": "Artificial intelligence is transforming precision agriculture, offering\nfarmers new tools to streamline their daily operations. While these\ntechnological advances promise increased efficiency, they often introduce\nadditional complexity and steep learning curves that are particularly\nchallenging for non-technical users who must balance tech adoption with\nexisting workloads. In this paper, we present a natural language (NL) robotic\nmission planner that enables non-specialists to control heterogeneous robots\nthrough a common interface. By leveraging large language models (LLMs) and\npredefined primitives, our architecture seamlessly translates human language\ninto intermediate descriptions that can be executed by different robotic\nplatforms. With this system, users can formulate complex agricultural missions\nwithout writing any code. In the work presented in this paper, we extend our\nprevious system tailored for wheeled robot mission planning through a new class\nof experiments involving robotic manipulation and computer vision tasks. Our\nresults demonstrate that the architecture is both general enough to support a\ndiverse set of robots and powerful enough to execute complex mission requests.\nThis work represents a significant step toward making robotic automation in\nprecision agriculture more accessible to non-technical users.", "comment": "Accepted to International Federation of Automatic Control (IFAC)\n  Sensing, Control and Automation Technologies for Agriculture - 8th\n  AGRICONTROL 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10106v1", "AI": {"title_translation": "一体化：基于大型语言模型的精准农业异构任务规划", "tldr": "本文提出了一种基于大型语言模型（LLM）的自然语言机器人任务规划器，使非专业用户能够通过通用接口控制异构机器人，从而简化精准农业中的机器人自动化，并通过实验证明了其通用性和执行复杂任务的能力。", "motivation": "尽管人工智能和机器人技术能提高精准农业的效率，但其引入的复杂性和陡峭的学习曲线对非技术用户构成挑战。目前的系统通常需要专业知识。因此，需要一种方法来使非专业用户能够轻松控制异构机器人，以平衡技术采用与现有工作量。", "method": "本文提出了一种自然语言（NL）机器人任务规划器。该架构利用大型语言模型（LLMs）和预定义原语，将人类语言无缝转换为可由不同机器人平台执行的中间描述。该系统允许用户无需编写任何代码即可制定复杂的农业任务。研究扩展了之前为轮式机器人任务规划量身定制的系统，通过涉及机器人操作和计算机视觉任务的新类别实验来验证其能力。", "result": "实验结果表明，该架构足够通用，可以支持多种机器人，并且足够强大，可以执行复杂的任务请求。这包括了机器人操作和计算机视觉任务的成功执行。", "conclusion": "该工作代表了在使精准农业中的机器人自动化对非技术用户更具可访问性方面迈出了重要一步。该系统证明了LLM在简化异构机器人控制方面的潜力。", "translation": "人工智能正在改变精准农业，为农民提供简化日常操作的新工具。虽然这些技术进步有望提高效率，但它们常常引入额外的复杂性和陡峭的学习曲线，这对必须在技术采用和现有工作量之间取得平衡的非技术用户来说尤其具有挑战性。在本文中，我们提出了一种自然语言（NL）机器人任务规划器，使非专业人员能够通过通用接口控制异构机器人。通过利用大型语言模型（LLMs）和预定义原语，我们的架构可以无缝地将人类语言转换为可由不同机器人平台执行的中间描述。通过这个系统，用户可以制定复杂的农业任务，而无需编写任何代码。在本文中介绍的工作中，我们通过涉及机器人操作和计算机视觉任务的新类别实验，扩展了我们之前为轮式机器人任务规划量身定制的系统。我们的结果表明，该架构既足够通用以支持多种机器人，又足够强大以执行复杂的任务请求。这项工作代表着使精准农业中的机器人自动化对非技术用户更具可访问性方面迈出了重要一步。", "summary": "本文提出了一种基于大型语言模型（LLM）的自然语言机器人任务规划器，旨在解决精准农业中机器人技术对非技术用户的复杂性挑战。该系统利用LLM和预定义原语，将自然语言指令转换为可由不同异构机器人平台执行的中间描述，使用户无需编程即可规划复杂的农业任务。研究通过涉及机器人操作和计算机视觉任务的实验，验证了该架构的通用性和执行复杂任务的能力，从而显著提高了精准农业中机器人自动化的可访问性。", "keywords": "精准农业, 大型语言模型, 异构机器人, 任务规划, 自然语言处理", "comments": "这项工作的创新之处在于利用大型语言模型作为通用接口，使非技术用户能够通过自然语言控制异构机器人，从而降低了精准农业中机器人技术的使用门槛。其重要性在于促进了AI在农业领域的普及应用，解决了传统机器人系统复杂性高、学习曲线陡峭的问题。该方法通过将人类语言转换为可执行的机器人指令，为未来更智能、更易用的农业自动化系统奠定了基础。"}}
{"id": "2506.10084", "title": "DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding", "authors": ["Bin Guo", "John H. L. Hansen"], "summary": "Conventional vision backbones, despite their success, often construct\nfeatures through a largely uniform cascade of operations, offering limited\nexplicit pathways for adaptive, iterative refinement. This raises a compelling\nquestion: can principles from classical search algorithms instill a more\nalgorithmic, structured, and logical processing flow within these networks,\nleading to representations built through more interpretable, perhaps\nreasoning-like decision processes? We introduce DeepTraverse, a novel vision\narchitecture directly inspired by algorithmic search strategies, enabling it to\nlearn features through a process of systematic elucidation and adaptive\nrefinement distinct from conventional approaches. DeepTraverse operationalizes\nthis via two key synergistic components: recursive exploration modules that\nmethodically deepen feature analysis along promising representational paths\nwith parameter sharing for efficiency, and adaptive calibration modules that\ndynamically adjust feature salience based on evolving global context. The\nresulting algorithmic interplay allows DeepTraverse to intelligently construct\nand refine feature patterns. Comprehensive evaluations across a diverse suite\nof image classification benchmarks show that DeepTraverse achieves highly\ncompetitive classification accuracy and robust feature discrimination, often\noutperforming conventional models with similar or larger parameter counts. Our\nwork demonstrates that integrating such algorithmic priors provides a\nprincipled and effective strategy for building more efficient, performant, and\nstructured vision backbones.", "comment": "NeurIPS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10084v1", "AI": {"title_translation": "DeepTraverse：一种受深度优先搜索启发的算法视觉理解网络", "tldr": "DeepTraverse是一种受深度优先搜索启发的视觉网络，通过递归探索和自适应校准模块，实现更具算法性和结构化的特征学习，在图像分类任务中表现优异。", "motivation": "传统的视觉骨干网络在特征构建上缺乏自适应、迭代细化的明确路径，这促使研究者思考是否能将经典搜索算法的原理引入网络，以实现更具算法性、结构化和逻辑性的处理流程，从而构建出更具可解释性的特征表示。", "method": "本研究提出了DeepTraverse，一种受算法搜索策略直接启发的视觉架构。它通过两个关键的协同组件来实现特征学习：递归探索模块（Recursive Exploration Modules），用于沿着有前景的表示路径系统性地深化特征分析，并共享参数以提高效率；以及自适应校准模块（Adaptive Calibration Modules），根据演变的全局上下文动态调整特征显著性。", "result": "在多样的图像分类基准测试中，DeepTraverse取得了极具竞争力的分类精度和鲁棒的特征判别能力，通常优于参数量相似或更大的传统模型。", "conclusion": "本研究表明，整合算法先验知识为构建更高效、高性能和结构化的视觉骨干网络提供了一种有原则且有效的策略。", "translation": "尽管传统的视觉骨干网络取得了成功，但它们通常通过一种大致统一的操作级联来构建特征，为自适应、迭代细化提供了有限的明确路径。这提出了一个引人注目的问题：经典搜索算法的原理能否在这些网络中灌输一种更具算法性、结构化和逻辑性的处理流程，从而产生通过更具可解释性、或许是推理般的决策过程构建的表示？我们引入了DeepTraverse，一种直接受算法搜索策略启发的新型视觉架构，使其能够通过与传统方法不同的系统阐明和自适应细化过程来学习特征。DeepTraverse通过两个关键的协同组件来实现这一点：递归探索模块，通过参数共享高效地沿着有前景的表示路径系统性地深化特征分析；以及自适应校准模块，根据演变的全局上下文动态调整特征显著性。由此产生的算法相互作用使DeepTraverse能够智能地构建和细化特征模式。对各种图像分类基准进行的全面评估表明，DeepTraverse实现了极具竞争力的分类精度和鲁棒的特征判别能力，通常优于参数量相似或更大的传统模型。我们的工作表明，整合此类算法先验知识为构建更高效、高性能和结构化的视觉骨干网络提供了一种有原则且有效的策略。", "summary": "DeepTraverse是一种受深度优先搜索启发的视觉网络，旨在解决传统视觉骨干网络在特征学习中缺乏自适应和迭代细化的问题。该网络通过递归探索模块进行系统性特征深化和自适应校准模块动态调整特征显著性，从而学习更具算法性和结构化的特征。实验证明，DeepTraverse在图像分类任务上取得了与传统模型相当或更优的性能，证明了将算法先验知识融入视觉网络是构建高效且高性能骨干网络的有效策略。", "keywords": "深度学习, 视觉骨干网络, 深度优先搜索, 图像分类, 算法理解", "comments": "DeepTraverse的创新之处在于将经典搜索算法（尤其是深度优先搜索）的原理融入到深度学习的视觉骨干网络中，这为构建更具解释性和推理能力的AI模型提供了新思路。其递归探索和自适应校准模块的设计，使得网络能够以更系统和动态的方式处理特征，有望提升模型的效率和性能。这项工作为未来设计更“智能”和“理解”视觉骨干网络开辟了新的方向。"}}
{"id": "2506.10921", "title": "Towards Zero-Stall Matrix Multiplication on Energy-Efficient RISC-V Clusters for Machine Learning Acceleration", "authors": ["Luca Colagrande", "Lorenzo Leone", "Maximilian Coco", "Andrei Deaconeasa", "Luca Benini"], "summary": "The growing computational demands of machine learning (ML) workloads have\ndriven the design of ML accelerators aiming at an optimal tradeoff between\nefficiency and flexibility. A widely explored architecture for flexible ML\naccelerators is based on clusters of lightweight instruction processors sharing\nmulti-banked L1 memory, augmented with specialized instruction extensions for\nkey ML-related computations, such as matrix multiplication (matmul). However,\ninstruction extensions should be coupled with microarchitectural optimizations\nthat remove inefficiencies due to control flow (loop handling) and memory\naccess, without drastically increasing processor complexity. Moving from a\nstate-of-the-art (SoA) ML accelerator cluster based on RISC-V processors, we\npropose a low-overhead optimized microarchitecture that eliminates these\ninefficiencies almost entirely while retaining programmability. We introduce\n\"zero-overhead loop nests\" to remove control overheads, and a \"zero-conflict\nmemory subsystem\", leveraging a novel double-buffering-aware interconnect, to\neliminate bank conflicts in L1 memory. With these enhancements, we attain\nnear-ideal utilizations between 96.1% and 99.4%, achieving 11% performance and\n8% energy efficiency improvements over the baseline SoA RISC-V cluster. We\ndemonstrate comparable utilizations and performance to a specialized SoA\naccelerator, with only 12% difference in energy efficiency, while providing a\nfully-programmable general-purpose solution supporting a significantly wider\nrange of workloads.", "comment": "7 pages, 5 figures, 2 tables. Accepted at ISLPED 2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10921v1", "AI": {"title_translation": "面向机器学习加速的节能RISC-V集群上零停顿矩阵乘法研究", "tldr": "本文提出了一种优化的RISC-V微架构，通过“零开销循环嵌套”和“零冲突内存子系统”实现近乎理想的处理器利用率，从而显著提升机器学习加速的性能和能效，同时保持可编程性。", "motivation": "机器学习工作负载日益增长的计算需求推动了机器学习加速器的设计，旨在实现效率和灵活性的最佳权衡。现有的基于RISC-V处理器的灵活机器学习加速器集群在控制流（循环处理）和内存访问方面存在效率低下问题，需要微架构优化来消除这些低效率，同时不大幅增加处理器复杂性。", "method": "作者提出了一种低开销的优化微架构，以几乎完全消除现有RISC-V集群的低效率，同时保留可编程性。具体方法包括：引入“零开销循环嵌套”以消除控制开销；利用新型双缓冲感知互连，构建“零冲突内存子系统”以消除L1内存中的bank冲突。", "result": "该增强型微架构实现了96.1%至99.4%的接近理想的利用率，相对于基线SoA RISC-V集群，性能提升了11%，能效提升了8%。它还展示了与专用SoA加速器相当的利用率和性能，能效差异仅为12%，同时提供了一个完全可编程的通用解决方案，支持更广泛的工作负载。", "conclusion": "所提出的可编程通用RISC-V集群微架构有效消除了低效率，实现了高利用率以及与专用加速器相当的显著性能和能效提升，同时支持更广泛的机器学习工作负载。", "translation": "机器学习（ML）工作负载日益增长的计算需求推动了ML加速器的设计，旨在实现效率和灵活性的最佳权衡。一种广泛探索的灵活ML加速器架构基于轻量级指令处理器集群，这些处理器共享多bank L1内存，并辅以用于关键ML相关计算（如矩阵乘法）的专用指令扩展。然而，指令扩展应与微架构优化相结合，以消除由控制流（循环处理）和内存访问引起的低效率，同时不大幅增加处理器复杂性。在现有最先进（SoA）的基于RISC-V处理器的ML加速器集群基础上，我们提出了一种低开销的优化微架构，几乎完全消除了这些低效率，同时保留了可编程性。我们引入了“零开销循环嵌套”以消除控制开销，以及利用新型双缓冲感知互连的“零冲突内存子系统”以消除L1内存中的bank冲突。通过这些增强，我们实现了96.1%至99.4%的接近理想的利用率，相对于基线SoA RISC-V集群，性能提升了11%，能效提升了8%。我们展示了与专用SoA加速器相当的利用率和性能，能效差异仅为12%，同时提供了一个完全可编程的通用解决方案，支持显著更广泛的工作负载。", "summary": "本文提出了一种针对机器学习加速的节能RISC-V集群优化的微架构，旨在实现零停顿的矩阵乘法。通过引入“零开销循环嵌套”以减少控制流开销，以及利用新型双缓冲感知互连构建“零冲突内存子系统”以消除L1内存bank冲突，该设计实现了96.1%至99.4%的近乎理想的处理器利用率。实验结果表明，相对于基线SoA RISC-V集群，其性能提升11%，能效提升8%，且其利用率和性能可与专用加速器媲美，同时保持完全可编程性并支持更广泛的机器学习工作负载。", "keywords": "机器学习加速, RISC-V, 矩阵乘法, 微架构, 能效", "comments": "本文提出了一种创新方法，通过解决RISC-V集群中微架构层面的基本低效率（循环开销和内存冲突），显著提升了灵活机器学习加速器的效率。其“零开销循环嵌套”和“零冲突内存子系统”是关键创新点。这项工作对于开发更通用、更节能的机器学习计算平台具有重要意义，因为它在接近专用硬件性能的同时，保留了可编程性和通用性的关键优势。"}}
{"id": "2506.10044", "title": "Improving the performance of optical inverse design of multilayer thin films using CNN-LSTM tandem neural networks", "authors": ["Uijun Jung", "Deokho Jang", "Sungchul Kim", "Jungho Kim"], "summary": "Optical properties of thin film are greatly influenced by the thickness of\neach layer. Accurately predicting these thicknesses and their corresponding\noptical properties is important in the optical inverse design of thin films.\nHowever, traditional inverse design methods usually demand extensive numerical\nsimulations and optimization procedures, which are time-consuming. In this\npaper, we utilize deep learning for the inverse design of the transmission\nspectra of SiO2/TiO2 multilayer thin films. We implement a tandem neural\nnetwork (TNN), which can solve the one-to-many mapping problem that greatly\ndegrades the performance of deep-learning-based inverse designs. In general,\nthe TNN has been implemented by a back-to-back connection of an inverse neural\nnetwork and a pre-trained forward neural network, both of which have been\nimplemented based on multilayer perceptron (MLP) algorithms. In this paper, we\npropose to use not only MLP, but also convolutional neural network (CNN) or\nlong short-term memory (LSTM) algorithms in the configuration of the TNN. We\nshow that an LSTM-LSTM-based TNN yields the highest accuracy but takes the\nlongest training time among nine configurations of TNNs. We also find that a\nCNN-LSTM-based TNN will be an optimal solution in terms of accuracy and speed\nbecause it could integrate the strengths of the CNN and LSTM algorithms.", "comment": "22 pages, 8 figures, 2 tables, 11 supplementary figures, 7\n  supplementary tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10044v1", "AI": {"title_translation": "使用CNN-LSTM串联神经网络提高多层薄膜光学逆向设计性能", "tldr": "本文提出了一种基于深度学习的串联神经网络（TNN）方法，用于多层薄膜的光学逆向设计，解决了传统方法耗时和深度学习中一对多映射问题。研究表明CNN-LSTM组合在准确性和速度方面表现最佳。", "motivation": "传统薄膜光学逆向设计方法需要大量的数值模拟和优化过程，耗时且效率低下。此外，基于深度学习的逆向设计存在一对多映射问题，会严重降低性能。", "method": "本文利用深度学习进行SiO2/TiO2多层薄膜透射光谱的逆向设计。提出了一种串联神经网络（TNN），通过结合逆向神经网络和预训练的正向神经网络来解决一对多映射问题。研究中尝试了包括多层感知器（MLP）、卷积神经网络（CNN）和长短期记忆网络（LSTM）在内的不同TNN配置。", "result": "在九种TNN配置中，基于LSTM-LSTM的TNN实现了最高的精度，但训练时间最长。基于CNN-LSTM的TNN在精度和速度方面达到了最佳平衡，因为它结合了CNN和LSTM算法的优势。", "conclusion": "CNN-LSTM串联神经网络是多层薄膜光学逆向设计中兼顾精度和速度的最佳解决方案。", "translation": "薄膜的光学特性受各层厚度影响很大。准确预测这些厚度及其对应的光学特性在薄膜光学逆向设计中至关重要。然而，传统的逆向设计方法通常需要大量的数值模拟和优化过程，这非常耗时。在本文中，我们利用深度学习对SiO2/TiO2多层薄膜的透射光谱进行逆向设计。我们实现了一种串联神经网络（TNN），它可以解决严重降低基于深度学习的逆向设计性能的一对多映射问题。通常，TNN是通过逆向神经网络和预训练的正向神经网络的反向连接实现的，两者都基于多层感知器（MLP）算法实现。在本文中，我们提出不仅在TNN的配置中使用MLP，还使用卷积神经网络（CNN）或长短期记忆网络（LSTM）算法。我们发现，在九种TNN配置中，基于LSTM-LSTM的TNN精度最高，但训练时间最长。我们还发现，基于CNN-LSTM的TNN将是精度和速度方面的最佳解决方案，因为它能够整合CNN和LSTM算法的优势。", "summary": "本文提出了一种新颖的深度学习方法，即串联神经网络（TNN），用于多层薄膜的光学逆向设计，以克服传统方法的耗时问题和深度学习中一对多映射的挑战。研究人员探讨了不同神经网络类型（MLP、CNN、LSTM）在TNN配置中的应用。实验结果表明，虽然LSTM-LSTM组合提供了最高的准确性，但其训练时间较长；而CNN-LSTM组合在准确性和速度之间取得了最佳平衡，是光学逆向设计的理想选择。", "keywords": "光学逆向设计, 多层薄膜, 深度学习, 串联神经网络, CNN-LSTM", "comments": "该论文通过引入CNN和LSTM到串联神经网络（TNN）的配置中，有效解决了光学逆向设计中传统方法的效率低下问题和深度学习模型中常见的一对多映射难题。其创新点在于对不同神经网络架构组合的系统性评估，特别指出CNN-LSTM作为兼顾精度和速度的优化方案，这对于实际应用具有重要指导意义。"}}
{"id": "2506.10401", "title": "HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration", "authors": ["Jiaqi Lv", "Xufeng He", "Yanchen Liu", "Xu Dai", "Yang Hu", "Shouyi Yin"], "summary": "The rapid growth of deep learning has driven exponential increases in model\nparameters and computational demands. NVIDIA GPUs and their CUDA-based software\necosystem provide robust support for parallel computing, significantly\nalleviating computational bottlenecks. Meanwhile, due to the cultivation of\nuser programming habits and the high performance of GPUs, the CUDA ecosystem\nhas established a dominant position in the field of parallel software. This\ndominance requires other hardware platforms to support CUDA-based software with\nperformance portability. However, translating CUDA code to other platforms\nposes significant challenges due to differences in parallel programming\nparadigms and hardware architectures. Existing approaches rely on language\nextensions, domain-specific languages (DSLs), or compilers but face limitations\nin workload coverage and generalizability. Moreover, these methods often incur\nsubstantial development costs. Recently, LLMs have demonstrated extraordinary\npotential in various vertical domains, especially in code-related tasks.\nHowever, the performance of existing LLMs in CUDA transpilation, particularly\nfor high-performance code, remains suboptimal. The main reason for this\nlimitation lies in the lack of high-quality training datasets. To address these\nchallenges, we propose a novel framework for generating high-performance CUDA\nand corresponding platform code pairs, leveraging AI compiler and automatic\noptimization technology. We further enhance the framework with a graph-based\ndata augmentation method and introduce HPCTransEval, a benchmark for evaluating\nLLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU\ntranspilation as a case study on leading LLMs. The result demonstrates that our\nframework significantly improves CUDA transpilation, highlighting the potential\nof LLMs to address compatibility challenges within the CUDA ecosystem.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10401v1", "AI": {"title_translation": "HPCTransCompile：一个用于高性能CUDA转译和LLM初步探索的AI编译器生成数据集", "tldr": "该研究提出了一个框架，利用AI编译器生成高质量的CUDA代码转换数据集，以提升LLM在高性能CUDA转译上的表现，从而解决CUDA生态系统的兼容性挑战。", "motivation": "CUDA生态系统在并行软件领域占据主导地位，但其他硬件平台需要支持CUDA软件以实现性能可移植性。将CUDA代码翻译到其他平台面临挑战，因为并行编程范式和硬件架构存在差异。现有方法在工作负载覆盖和通用性方面有限，且开发成本高。LLM在代码相关任务中潜力巨大，但其在高性能CUDA转译方面的表现不佳，主要原因是缺乏高质量的训练数据集。", "method": "提出了一个新颖的框架，利用AI编译器和自动优化技术生成高性能CUDA及其对应平台代码对。通过基于图的数据增强方法进一步增强该框架。引入了HPCTransEval，一个用于评估LLM在CUDA转译性能上的基准。以CUDA-to-CPU转译作为案例研究，对领先的LLM进行了实验。", "result": "该框架显著改善了CUDA转译性能。突出了LLM解决CUDA生态系统内兼容性挑战的潜力。", "conclusion": "LLMs有潜力解决CUDA生态系统内的兼容性挑战，特别是通过高质量的训练数据集生成。", "translation": "深度学习的快速发展推动了模型参数和计算需求的指数级增长。NVIDIA GPU及其基于CUDA的软件生态系统为并行计算提供了强大的支持，显著缓解了计算瓶颈。同时，由于用户编程习惯的培养和GPU的高性能，CUDA生态系统在并行软件领域建立了主导地位。这种主导地位要求其他硬件平台通过性能可移植性来支持基于CUDA的软件。然而，由于并行编程范式和硬件架构的差异，将CUDA代码翻译到其他平台带来了重大挑战。现有方法依赖于语言扩展、领域特定语言（DSL）或编译器，但在工作负载覆盖和通用性方面面临限制。此外，这些方法通常会产生高昂的开发成本。最近，大型语言模型（LLMs）在各个垂直领域，特别是在代码相关任务中，展示了非凡的潜力。然而，现有LLMs在CUDA转译（尤其是高性能代码）方面的性能仍然不理想。造成这一限制的主要原因在于缺乏高质量的训练数据集。为了应对这些挑战，我们提出了一个新颖的框架，利用AI编译器和自动优化技术生成高性能CUDA及其对应平台代码对。我们通过基于图的数据增强方法进一步增强了该框架，并引入了HPCTransEval，一个用于评估LLM在CUDA转译性能上的基准。我们以CUDA到CPU的转译作为案例研究，对领先的LLMs进行了实验。结果表明，我们的框架显著改善了CUDA转译，突出了LLMs解决CUDA生态系统内兼容性挑战的潜力。", "summary": "该论文提出了HPCTransCompile框架，旨在解决高性能CUDA代码向其他平台转译的挑战。鉴于现有方法（如语言扩展、DSL或编译器）的局限性以及LLM在高性能CUDA转译中因缺乏高质量数据集而表现不佳的问题，作者利用AI编译器和自动优化技术生成高性能CUDA及其目标平台代码对。该框架还通过图数据增强方法进行了增强，并引入了HPCTransEval基准来评估LLM性能。实验结果表明，该框架显著提升了CUDA转译效果，验证了LLM在解决CUDA生态系统兼容性问题上的巨大潜力。", "keywords": "CUDA Transpilation, LLM, AI Compiler, High-Performance Computing, Dataset Generation", "comments": "该论文的创新点在于提出了一个利用AI编译器和自动优化技术来自动生成高质量、高性能CUDA转译数据集的框架，有效解决了LLM在这一领域性能不佳的根本原因——数据稀缺。通过引入图数据增强和专用的评估基准HPCTransEval，该研究为LLM在高性能计算领域的应用开辟了新的路径，对于提升CUDA代码的跨平台兼容性和性能可移植性具有重要意义。"}}
{"id": "2506.10876", "title": "Landauer Principle and Thermodynamics of Computation", "authors": ["Pritam Chattopadhyay", "Avijit Misra", "Tanmoy Pandit", "Goutam Paul"], "summary": "According to the Landauer principle, any logically irreversible process\naccompanies entropy production, which results in heat dissipation in the\nenvironment. Erasing of information, one of the primary logically irreversible\nprocesses, has a lower bound on heat dissipated into the environment, called\nthe Landauer bound (LB). However, the practical erasure processes dissipate\nmuch more heat than the LB. Recently, there have been a few experimental\ninvestigations to reach this bound both in the classical and quantum domains.\nThere has also been a spate of activities to enquire about this LB in finite\ntime, with finite-size heat baths, non-Markovian and nonequilibrium environment\nin the quantum regime where the effects of fluctuations and correlation of the\nsystems with the bath can no longer be ignored. This article provides a\ncomprehensive review of the recent progress on the Landauer bound, which serves\nas a fundamental principle in the thermodynamics of computation. We also\nprovide a perspective for future endeavors in these directions. Furthermore, we\nreview the recent exploration toward establishing energetic bounds of a\ncomputational process. We also review the thermodynamic aspects of error\ncorrection, which is an indispensable part of information processing and\ncomputations. In doing so, we briefly discuss the basics of these fields to\nprovide a complete picture.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10876v1", "AI": {"title_translation": "朗道尔原理与计算热力学", "tldr": "本文全面回顾了朗道尔原理、朗道尔界限及其在计算热力学中的最新进展，包括信息擦除、计算能量限制和纠错的热力学方面，并展望了未来方向。", "motivation": "实际的信息擦除过程耗散的能量远超朗道尔界限（LB），这促使了对LB在经典和量子领域中，以及在有限时间、有限尺寸热浴、非马尔可夫和非平衡量子环境下的深入实验和理论研究。本文旨在对这些最新进展进行全面回顾，并探讨计算过程的能量界限和纠错的热力学方面。", "method": "本文通过对朗道尔界限、计算过程的能量界限以及信息处理和计算中纠错的热力学方面进行全面综述，并简要讨论了这些领域的基础知识，以提供一个完整的图景。", "result": "本文全面回顾了朗道尔界限的最新进展，包括在经典和量子领域中达到该界限的实验探索，以及在有限时间、有限尺寸热浴、非马尔可夫和非平衡量子体系中对LB的理论探究。此外，论文还综述了建立计算过程能量界限的最新探索以及纠错的热力学方面。", "conclusion": "本文对作为计算热力学基本原理的朗道尔界限的最新进展进行了全面回顾，并探讨了计算过程的能量界限和纠错的热力学方面，为该领域的未来研究提供了展望。", "translation": "根据朗道尔原理，任何逻辑上不可逆的过程都伴随着熵的产生，这导致了环境中的热耗散。信息擦除作为主要的逻辑不可逆过程之一，其耗散到环境中的热量有一个下限，称为朗道尔界限（LB）。然而，实际的擦除过程耗散的热量远超LB。最近，在经典和量子领域都有一些实验研究试图达到这个界限。在量子体系中，当系统与热浴的涨落和关联效应不可忽略时，在有限时间、有限尺寸热浴、非马尔可夫和非平衡环境下，对LB的探究也出现了一系列活跃的研究。本文全面回顾了朗道尔界限的最新进展，该界限是计算热力学中的一个基本原理。我们还为这些方向的未来努力提供了展望。此外，我们回顾了近期在建立计算过程能量界限方面的探索。我们还回顾了纠错的热力学方面，它是信息处理和计算中不可或缺的一部分。在此过程中，我们简要讨论了这些领域的基础知识，以提供一个完整的图景。", "summary": "这篇综述文章全面涵盖了朗道尔原理及其相关的朗道尔界限（LB），后者是逻辑不可逆计算过程（如信息擦除）中热耗散的基本下限。文章讨论了近期为达到LB所做的实验努力，以及在各种复杂条件（有限时间、有限热浴、量子体系等）下对其行为进行的理论研究。此外，论文还综述了计算过程的能量界限和纠错的热力学方面，旨在为计算热力学领域提供一个完整的图景和未来的展望。", "keywords": "朗道尔原理, 计算热力学, 朗道尔界限, 信息擦除, 纠错", "comments": "这篇论文具有重要意义，因为它对计算热力学中的基石——朗道尔原理及其最新进展进行了及时而全面的综述。它强调了理论界限与实际耗散之间的差距，并涵盖了能量界限和纠错等关键相关主题，为这个快速发展的领域提供了宝贵的见解和未来方向。"}}
{"id": "2506.10179", "title": "Correlation vs causation in Alzheimer's disease: an interpretability-driven study", "authors": ["Hamzah Dabool", "Raghad Mustafa"], "summary": "Understanding the distinction between causation and correlation is critical\nin Alzheimer's disease (AD) research, as it impacts diagnosis, treatment, and\nthe identification of true disease drivers. This experiment investigates the\nrelationships among clinical, cognitive, genetic, and biomarker features using\na combination of correlation analysis, machine learning classification, and\nmodel interpretability techniques. Employing the XGBoost algorithm, we\nidentified key features influencing AD classification, including cognitive\nscores and genetic risk factors. Correlation matrices revealed clusters of\ninterrelated variables, while SHAP (SHapley Additive exPlanations) values\nprovided detailed insights into feature contributions across disease stages.\nOur results highlight that strong correlations do not necessarily imply\ncausation, emphasizing the need for careful interpretation of associative data.\nBy integrating feature importance and interpretability with classical\nstatistical analysis, this work lays groundwork for future causal inference\nstudies aimed at uncovering true pathological mechanisms. Ultimately,\ndistinguishing causal factors from correlated markers can lead to improved\nearly diagnosis and targeted interventions for Alzheimer's disease.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10179v1", "AI": {"title_translation": "阿尔茨海默病中的关联与因果关系：一项可解释性驱动的研究", "tldr": "本研究通过结合相关性分析、机器学习分类和模型可解释性技术，探讨阿尔茨海默病（AD）中临床、认知、遗传和生物标志物特征之间的关系，强调强关联不一定意味着因果关系，旨在为未来的因果推断研究奠定基础。", "motivation": "理解阿尔茨海默病（AD）中因果关系与关联的区别对于诊断、治疗和识别真正的疾病驱动因素至关重要。", "method": "本研究利用相关性分析、机器学习分类（采用XGBoost算法）和模型可解释性技术（如SHAP值）来调查临床、认知、遗传和生物标志物特征之间的关系。", "result": "研究识别出影响AD分类的关键特征，包括认知评分和遗传风险因素。相关矩阵揭示了相互关联变量的簇，SHAP值提供了对特征贡献的详细见解。结果强调强关联不一定意味着因果关系。", "conclusion": "区分阿尔茨海默病中的因果因素与关联标记可以改善早期诊断和靶向干预。本工作通过整合特征重要性、可解释性与经典统计分析，为未来的因果推断研究奠定了基础。", "translation": "理解阿尔茨海默病（AD）中因果关系与关联的区别至关重要，因为它影响诊断、治疗和真实疾病驱动因素的识别。本实验利用相关性分析、机器学习分类和模型可解释性技术的组合，调查了临床、认知、遗传和生物标志物特征之间的关系。我们采用XGBoost算法，识别出影响AD分类的关键特征，包括认知评分和遗传风险因素。相关矩阵揭示了相互关联变量的簇，而SHAP（SHapley Additive exPlanations）值提供了对疾病不同阶段特征贡献的详细见解。我们的结果强调，强关联不一定意味着因果关系，这强调了对关联数据进行仔细解释的必要性。通过将特征重要性和可解释性与经典统计分析相结合，这项工作为未来旨在揭示真实病理机制的因果推断研究奠定了基础。最终，区分因果因素与关联标记可以改善阿尔茨海默病的早期诊断和靶向干预。", "summary": "本研究旨在区分阿尔茨海默病（AD）中的关联与因果关系，以改进诊断和治疗。通过结合相关性分析、XGBoost机器学习分类和SHAP可解释性技术，研究人员调查了临床、认知、遗传和生物标志物特征。结果显示认知评分和遗传风险因素是关键特征，并强调强关联不等于因果关系。这项工作为未来的AD因果推断研究奠定了基础，以期实现更精准的早期诊断和干预。", "keywords": "阿尔茨海默病, 因果关系, 关联, 机器学习, 可解释性", "comments": "这项研究的创新之处在于结合了机器学习的可解释性（SHAP）与传统的统计分析，以更深入地理解阿尔茨海默病中的复杂特征关系，并明确区分了关联与因果的重要性。这对于指导未来的疾病机制研究和开发更有效的干预措施具有重要意义。"}}
{"id": "2506.10272", "title": "Collective Bargaining in the Information Economy Can Address AI-Driven Power Concentration", "authors": ["Nicholas Vincent", "Matthew Prewitt", "Hanlin Li"], "summary": "This position paper argues that there is an urgent need to restructure\nmarkets for the information that goes into AI systems. Specifically, producers\nof information goods (such as journalists, researchers, and creative\nprofessionals) need to be able to collectively bargain with AI product builders\nin order to receive reasonable terms and a sustainable return on the\ninformational value they contribute. We argue that without increased market\ncoordination or collective bargaining on the side of these primary information\nproducers, AI will exacerbate a large-scale \"information market failure\" that\nwill lead not only to undesirable concentration of capital, but also to a\npotential \"ecological collapse\" in the informational commons. On the other\nhand, collective bargaining in the information economy can create market\nfrictions and aligned incentives necessary for a pro-social, sustainable AI\nfuture. We provide concrete actions that can be taken to support a\ncoalition-based approach to achieve this goal. For example, researchers and\ndevelopers can establish technical mechanisms such as federated data management\ntools and explainable data value estimations, to inform and facilitate\ncollective bargaining in the information economy. Additionally, regulatory and\npolicy interventions may be introduced to support trusted data intermediary\norganizations representing guilds or syndicates of information producers.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10272v1", "AI": {"title_translation": "信息经济中的集体谈判可以解决人工智能驱动的权力集中问题", "tldr": "信息生产者需要通过集体谈判来应对AI导致的市场失灵和权力集中，以实现可持续的AI未来。", "motivation": "论文认为迫切需要重组AI系统所需信息的市场。当前信息生产者（如记者、研究人员、创意专业人士）缺乏与AI产品构建者议价的能力，导致信息市场失灵、资本过度集中以及信息公共领域的“生态崩溃”。", "method": "提出通过信息生产者的市场协调或集体谈判来解决问题。具体行动包括：研究人员和开发者建立技术机制，如联邦数据管理工具和可解释的数据价值评估，以促进集体谈判；引入监管和政策干预，支持代表信息生产者行会的受信任数据中介组织。", "result": "Not mentioned in abstract", "conclusion": "信息经济中的集体谈判可以创造市场摩擦和一致的激励机制，从而实现一个亲社会、可持续的AI未来，并能有效应对AI导致的权力集中和市场失灵。", "translation": "这篇立场文件认为，迫切需要重组人工智能系统所需信息的市场。具体而言，信息产品生产者（如记者、研究人员和创意专业人士）需要能够与人工智能产品构建者进行集体谈判，以获得合理的条款和对其所贡献信息价值的可持续回报。我们认为，如果没有这些主要信息生产者的市场协调或集体谈判的增加，人工智能将加剧大规模的“信息市场失灵”，这不仅会导致资本的不良集中，还会导致信息公共领域潜在的“生态崩溃”。另一方面，信息经济中的集体谈判可以创造必要的市场摩擦和一致的激励机制，从而实现一个亲社会、可持续的人工智能未来。我们提供了可以采取的具体行动来支持实现这一目标的基于联盟的方法。例如，研究人员和开发人员可以建立技术机制，如联邦数据管理工具和可解释的数据价值估算，以告知和促进信息经济中的集体谈判。此外，可以引入监管和政策干预，以支持代表信息生产者行会或辛迪加的受信任数据中介组织。", "summary": "这篇立场文件强调了重组人工智能系统信息市场的重要性，认为信息生产者应通过集体谈判与AI产品构建者议价，以确保公平回报并避免AI加剧信息市场失灵、资本集中和信息公共领域的“生态崩溃”。论文提出，信息经济中的集体谈判能促进亲社会、可持续的AI发展，并建议通过技术机制（如联邦数据管理、数据价值评估）和政策干预（支持数据中介组织）来实现这一目标。", "keywords": "集体谈判, 信息经济, 人工智能, 权力集中, 市场失灵", "comments": "该论文的创新之处在于将传统的集体谈判概念应用于信息经济和AI领域，以解决AI发展带来的新型权力集中和市场失灵问题。其重要性在于提出了一个前瞻性的框架，旨在保护信息生产者的权益，并促进AI的可持续、公平发展。论文不仅指出了问题，还提供了具体的策略建议，包括技术和政策层面，具有较强的实践指导意义。"}}
{"id": "2506.10230", "title": "Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation", "authors": ["Emerson P. Grabke", "Masoom A. Haider", "Babak Taati"], "summary": "Latent diffusion models (LDM) could alleviate data scarcity challenges\naffecting machine learning development for medical imaging. However, medical\nLDM training typically relies on performance- or scientific\naccessibility-limiting strategies including a reliance on short-prompt text\nencoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with\nlarge data volumes. We propose a Class-Conditioned Efficient Large Language\nmodel Adapter (CCELLA) to address these limitations. CCELLA is a novel\ndual-head conditioning approach that simultaneously conditions the LDM U-Net\nwith non-medical large language model-encoded text features through\ncross-attention and with pathology classification through the timestep\nembedding. We also propose a joint loss function and a data-efficient LDM\ntraining framework. In combination, these strategies enable\npathology-conditioned LDM training for high-quality medical image synthesis\ngiven limited data volume and human data annotation, improving LDM performance\nand scientific accessibility. Our method achieves a 3D FID score of 0.025 on a\nsize-limited prostate MRI dataset, significantly outperforming a recent\nfoundation model with FID 0.071. When training a classifier for prostate cancer\nprediction, adding synthetic images generated by our method to the training\ndataset improves classifier accuracy from 69% to 74%. Training a classifier\nsolely on our method's synthetic images achieved comparable performance to\ntraining on real images alone.", "comment": "MAH and BT are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10230v1", "AI": {"title_translation": "基于提示引导和预测类别条件化的3D前列腺MRI生成潜在扩散模型", "tldr": "提出CCELLA，一种新颖的双头条件化潜在扩散模型，用于在数据有限的情况下生成高质量的3D前列腺MRI，显著优于现有模型并能提升分类器性能。", "motivation": "医疗图像机器学习开发面临数据稀缺挑战，而现有医疗LDM训练策略（如依赖短提示文本编码器、重用非医疗LDM或需要大量数据微调）限制了性能和科学可及性。", "method": "提出Class-Conditioned Efficient Large Language model Adapter (CCELLA)，这是一种新型双头条件化方法。它通过交叉注意力将非医疗大型语言模型编码的文本特征和通过时间步嵌入将病理分类同时条件化到LDM U-Net。还提出了一个联合损失函数和数据高效的LDM训练框架。", "result": "在有限大小的前列腺MRI数据集上，3D FID得分为0.025，显著优于最近的基础模型（FID 0.071）。将我们方法生成的合成图像添加到前列腺癌预测分类器训练数据集中，分类器准确率从69%提高到74%。仅使用我们方法生成的合成图像训练分类器，其性能与仅使用真实图像训练的性能相当。", "conclusion": "结合这些策略，可以在数据量和人工数据标注有限的情况下，实现病理条件化的LDM训练，用于高质量医疗图像合成，从而提高LDM性能和科学可及性。", "translation": "潜在扩散模型（LDM）可以缓解影响医学成像机器学习开发的数据稀缺挑战。然而，医疗LDM训练通常依赖于限制性能或科学可及性的策略，包括依赖短提示文本编码器、重用非医疗LDM或需要大量数据进行微调。我们提出了一种类别条件化高效大型语言模型适配器（CCELLA）来解决这些限制。CCELLA是一种新颖的双头条件化方法，它通过交叉注意力将非医疗大型语言模型编码的文本特征与通过时间步嵌入的病理分类同时条件化到LDM U-Net。我们还提出了一个联合损失函数和数据高效的LDM训练框架。结合这些策略，可以在数据量和人工数据标注有限的情况下，实现病理条件化的LDM训练，用于高质量医疗图像合成，从而提高LDM性能和科学可及性。我们的方法在大小有限的前列腺MRI数据集上实现了0.025的3D FID分数，显著优于最近的基础模型（FID 0.071）。在训练前列腺癌预测分类器时，将我们方法生成的合成图像添加到训练数据集中，将分类器准确率从69%提高到74%。仅使用我们方法生成的合成图像训练分类器，其性能与仅使用真实图像训练的性能相当。", "summary": "本文提出了一种名为CCELLA的新型双头条件化潜在扩散模型（LDM），旨在解决医疗图像生成中数据稀缺及现有LDM训练策略的局限性。CCELLA通过同时结合大型语言模型编码的文本特征和病理分类信息来引导LDM U-Net，并引入了联合损失函数和数据高效训练框架。实验结果表明，该方法能在数据有限的情况下生成高质量的3D前列腺MRI，其FID分数显著优于现有模型。此外，使用其生成的合成图像可以有效提升前列腺癌分类器的准确性，甚至仅用合成数据训练也能达到与真实数据相当的性能，从而提高了医疗LDM的实用性和可及性。", "keywords": "潜在扩散模型, 医疗图像生成, 前列腺MRI, 类别条件化, 数据稀缺", "comments": "本文的创新点在于提出了CCELLA，一种结合了文本特征和病理分类的双头条件化方法，有效解决了医疗LDM在数据稀缺环境下的训练挑战。其数据高效的训练框架和显著优于基线模型的性能，特别是合成数据对分类器性能的提升以及仅用合成数据训练即可达到真实数据水平的能力，显示出其在医疗图像合成和辅助诊断方面的巨大潜力。这对于缓解医疗领域数据隐私和标注成本高昂的问题具有重要意义。"}}
{"id": "2506.10349", "title": "Joint ASR and Speaker Role Tagging with Serialized Output Training", "authors": ["Anfeng Xu", "Tiantian Feng", "Shrikanth Narayanan"], "summary": "Automatic Speech Recognition systems have made significant progress with\nlarge-scale pre-trained models. However, most current systems focus solely on\ntranscribing the speech without identifying speaker roles, a function that is\ncritical for conversational AI. In this work, we investigate the use of\nserialized output training (SOT) for joint ASR and speaker role tagging. By\naugmenting Whisper with role-specific tokens and fine-tuning it with SOT, we\nenable the model to generate role-aware transcriptions in a single decoding\npass. We compare the SOT approach against a self-supervised previous baseline\nmethod on two real-world conversational datasets. Our findings show that this\napproach achieves more than 10% reduction in multi-talker WER, demonstrating\nits feasibility as a unified model for speaker-role aware speech transcription.", "comment": "Under review", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10349v1", "AI": {"title_translation": "联合ASR和说话人角色标注的序列化输出训练", "tldr": "通过序列化输出训练（SOT）增强Whisper模型，实现联合ASR和说话人角色标注，将多说话人词错率降低10%以上，证明了其在对话式AI中生成角色感知转录的可行性。", "motivation": "当前的自动语音识别（ASR）系统主要专注于语音转录，但缺乏说话人角色识别功能，而这对于对话式AI至关重要。", "method": "本文研究了使用序列化输出训练（SOT）进行联合ASR和说话人角色标注。具体方法是，通过添加角色特定标记来增强Whisper模型，并使用SOT进行微调，使模型能够在单次解码过程中生成角色感知的转录。该方法在两个真实世界对话数据集上与自监督基线方法进行了比较。", "result": "该方法在多说话人词错率（multi-talker WER）上实现了超过10%的降低。", "conclusion": "该研究证明了所提出的序列化输出训练方法作为一种统一模型，在实现说话人角色感知语音转录方面的可行性。", "translation": "自动语音识别系统在大规模预训练模型的帮助下取得了显著进展。然而，当前大多数系统仅专注于语音转录，而没有识别说话人角色，这对于对话式AI来说是一项关键功能。在这项工作中，我们研究了使用序列化输出训练（SOT）进行联合ASR和说话人角色标注。通过使用角色特定标记增强Whisper模型并使用SOT对其进行微调，我们使模型能够在单次解码过程中生成角色感知的转录。我们将SOT方法与一种自监督的现有基线方法在两个真实世界的对话数据集上进行了比较。我们的发现表明，这种方法在多说话人词错率上实现了超过10%的降低，证明了其作为说话人角色感知语音转录统一模型的可行性。", "summary": "本研究提出了一种基于序列化输出训练（SOT）的联合ASR和说话人角色标注方法。通过在Whisper模型中引入角色特定标记并进行微调，该模型能够实现单次解码生成角色感知的语音转录。实验结果表明，该方法在多说话人词错率上取得了超过10%的降低，验证了其作为统一模型在对话式AI中实现说话人角色感知转录的有效性和可行性。", "keywords": "ASR, 说话人角色标注, 序列化输出训练, Whisper, 对话式AI", "comments": "这项工作通过引入序列化输出训练（SOT）来解决现有ASR系统缺乏说话人角色识别的关键限制，这对于对话式AI至关重要。其创新点在于将ASR和说话人角色标注整合到一个单一的解码过程中，提高了效率。超过10%的WER降低是一个显著的改进，表明了该方法的实用价值。该研究为开发更智能、更具上下文感知的对话系统提供了有前景的方向。"}}
{"id": "2506.10845", "title": "Faster CONGEST Approximation Algorithms for Maximum Weighted Independent Set in Sparse Graphs", "authors": ["Salwa Faour", "Fabian Kuhn"], "summary": "The maximum independent set problem is a classic optimization problem that\nhas also been studied quite intensively in the distributed setting. While the\nproblem is hard to approximate in general, there are good approximation\nalgorithms known for several sparse graph families. In this paper, we consider\ndeterministic distributed CONGEST algorithms for the weighted version of the\nproblem in trees and graphs of bounded arboricity.\n  For trees, we prove that the task of deterministically computing a\n$(1-\\epsilon)$-approximate solution to the maximum weight independent set\n(MWIS) problem has a tight $\\Theta(\\log^*(n) / \\epsilon)$ complexity. The lower\nbound already holds on unweighted oriented paths. On the upper bound side, we\nshow that the bound can be achieved even in unrooted trees.\n  For graphs $G=(V,E)$ of arboricity $\\beta>1$, we give two algorithms. If the\nsum of all node weights is $w(V)$, we show that for any $\\epsilon>0$, an\nindependent set of weight at least $(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$ can\nbe computed in $O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$ rounds. This\nresult is obtained by a direct application of the local rounding framework of\nFaour, Ghaffari, Grunau, Kuhn, and Rozho\\v{n} [SODA '23]. We further show that\nfor any $\\epsilon>0$, an independent set of weight at least\n$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$ can be computed in\n$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$ rounds. This\nimproves on a recent result of Gil [OPODIS '23], who showed that a\n$1/\\lfloor(2+\\epsilon)\\beta\\rfloor$-approximation to the MWIS problem can be\ncomputed in $O(\\beta\\cdot\\log n)$ rounds. As an intermediate step, we design an\nalgorithm to compute an independent set of total weight at least\n$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$ in time\n$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$, where $\\Delta$ is\nthe maximum degree of the graph.", "comment": "23 pages", "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.10845v1", "AI": {"title_translation": "稀疏图上最大加权独立集的更快CONGEST近似算法", "tldr": "本文提出了在CONGEST模型下，针对树和有界非循环度图上的最大加权独立集问题的更快确定性近似算法，并给出了紧复杂度界限和改进的近似比。", "motivation": "最大独立集问题是一个经典的优化问题，在分布式环境中得到了广泛研究。尽管该问题通常难以近似，但对于某些稀疏图族已存在良好的近似算法。本文旨在为加权版本的最大独立集问题，在CONGEST模型下，为树和有界非循环度图提供确定性分布式算法。", "method": "1. 对于树，通过确定性计算方法，证明了最大加权独立集(MWIS)问题的$(1-\\epsilon)$-近似解具有紧的$\\Theta(\\log^*(n) / \\epsilon)$复杂度。2. 对于非循环度$\\beta>1$的图，提出了两种算法：第一种算法通过直接应用Faour等人的局部舍入框架；第二种算法改进了Gil的最新结果，并通过一个中间步骤设计了一个算法，用于计算总权重至少为$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$的独立集。", "result": "1. 对于树，确定性计算最大加权独立集(MWIS)的$(1-\\epsilon)$-近似解的时间复杂度为紧的$\\Theta(\\log^*(n) / \\epsilon)$轮，该下界在无权有向路径上成立，上界在无根树上也可达到。2. 对于非循环度$\\beta>1$的图：第一种算法能够在$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$的独立集；第二种算法能够在$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$的独立集，这改进了Gil的最新结果。作为中间步骤，设计了一个算法，能在$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$时间内计算出总权重至少为$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$的独立集。", "conclusion": "本文为CONGEST模型下的加权最大独立集问题在树和有界非循环度图上提供了更快且更优的确定性近似算法。对于树，给出了精确的复杂度界限；对于有界非循环度图，通过新算法改进了现有的近似比和轮复杂度。", "translation": "最大独立集问题是一个经典的优化问题，在分布式环境中也得到了相当深入的研究。虽然该问题在一般情况下很难近似，但对于几种稀疏图族，已存在良好的近似算法。在本文中，我们考虑了在CONGEST模型下，针对树和有界非循环度图上加权版本问题的确定性分布式算法。对于树，我们证明了确定性计算最大加权独立集（MWIS）问题的$(1-\\epsilon)$-近似解具有紧的$\\Theta(\\log^*(n) / \\epsilon)$复杂度。该下界在无权有向路径上已经成立。在上界方面，我们表明即使在无根树中也可以达到该界限。对于非循环度$\\beta>1$的图$G=(V,E)$，我们提供了两种算法。如果所有节点权重的总和为$w(V)$，我们表明对于任何$\\epsilon>0$，可以在$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$的独立集。这个结果是通过直接应用Faour、Ghaffari、Grunau、Kuhn和Rozho\\v{n}的局部舍入框架[SODA '23]获得的。我们进一步表明，对于任何$\\epsilon>0$，可以在$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$轮内计算出权重至少为$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$的独立集。这改进了Gil[OPODIS '23]最近的结果，他表明MWIS问题的$1/\\lfloor(2+\\epsilon)\\beta\\rfloor$-近似解可以在$O(\\beta\\cdot\\log n)$轮内计算。作为中间步骤，我们设计了一种算法，可以在$O(\\log^3(\\Delta)\\cdot\\log(1/\\epsilon)/\\epsilon + \\log^* n)$时间内计算出总权重至少为$(1-\\epsilon)\\cdot\\sum_{v\\in V}\\frac{w(v)}{deg(v)+1}$的独立集，其中$\\Delta$是图的最大度数。", "summary": "本文研究了在CONGEST分布式模型下，针对树和有界非循环度图上的最大加权独立集（MWIS）问题。作者提出了确定性近似算法，并为树上的MWIS问题证明了紧的$\\Theta(\\log^*(n) / \\epsilon)$时间复杂度。对于非循环度图，通过两种新算法，分别实现了$O(\\log^2(\\beta/\\epsilon)/\\epsilon + \\log^* n)$轮内达到$(1-\\epsilon)\\cdot \\frac{w(V)}{4\\beta}$近似比，以及$O(\\log^3(\\beta)\\cdot\\log(1/\\epsilon)/\\epsilon^2 \\cdot\\log n)$轮内达到$(1-\\epsilon)\\cdot\\frac{w(V)}{2\\beta+1}$近似比，后者改进了现有结果。", "keywords": "最大加权独立集, CONGEST, 稀疏图, 近似算法, 分布式算法", "comments": "本文在分布式CONGEST模型下，针对最大加权独立集问题在特定稀疏图（树和有界非循环度图）上的近似算法研究取得了显著进展。其创新之处在于为树上的问题给出了紧的复杂度界限，并为有界非循环度图提供了更快的算法和更好的近似比，特别是通过改进现有框架和结果，提升了该领域的技术水平。"}}
{"id": "2506.10077", "title": "A quantum semantic framework for natural language processing", "authors": ["Christopher J. Agostino", "Quan Le Thien", "Molly Apsel", "Denizhan Pak", "Elina Lesyk", "Ashabari Majumdar"], "summary": "Semantic degeneracy represents a fundamental property of natural language\nthat extends beyond simple polysemy to encompass the combinatorial explosion of\npotential interpretations that emerges as semantic expressions increase in\ncomplexity. Large Language Models (LLMs) and other modern NLP systems face\ninherent limitations precisely because they operate within natural language\nitself, making them subject to the same interpretive constraints imposed by\nsemantic degeneracy. In this work, we argue using Kolmogorov complexity that as\nan expression's complexity grows, the likelihood of any interpreting agent\n(human or LLM-powered AI) recovering the single intended meaning vanishes. This\ncomputational intractability suggests the classical view that linguistic forms\npossess meaning in and of themselves is flawed. We alternatively posit that\nmeaning is instead actualized through an observer-dependent interpretive act.\nTo test this, we conducted a semantic Bell inequality test using diverse LLM\nagents as ``computational cognitive systems'' to interpret ambiguous word pairs\nunder varied contextual settings. Across several independent experiments, we\nfound average CHSH expectation values ranging from 1.2 to 2.8, with several\nruns yielding values (e.g., 2.3-2.4) that significantly violate the classical\nboundary ($|S|\\leq2$). This demonstrates that linguistic interpretation under\nambiguity can exhibit non-classical contextuality, consistent with results from\nhuman cognition experiments. These results inherently imply that classical\nfrequentist-based analytical approaches for natural language are necessarily\nlossy. Instead, we propose that Bayesian-style repeated sampling approaches can\nprovide more practically useful and appropriate characterizations of linguistic\nmeaning in context.", "comment": "12 pages, 2 figures, accepted submission to Quantum AI and NLP 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10077v1", "AI": {"title_translation": "自然语言处理的量子语义框架", "tldr": "本文提出语义简并性是自然语言的根本属性，传统方法处理复杂语义有局限。研究通过语义贝尔不等式测试发现语言解释表现出非经典语境性，并建议采用贝叶斯式重复采样方法。", "motivation": "大语言模型（LLMs）及其他现代NLP系统在处理自然语言时面临固有限制，因为它们受限于语义简并性带来的解释性约束。随着语义表达复杂性的增加，任何解释代理（人类或LLM驱动的AI）恢复单一预期含义的可能性都会消失，这表明传统的语言形式本身具有意义的观点存在缺陷。", "method": "1. 使用柯尔莫哥洛夫复杂度论证，当表达式复杂性增长时，解释代理恢复单一预期含义的可能性会消失。2. 进行语义贝尔不等式测试，使用不同的LLM代理作为“计算认知系统”，在不同上下文设置下解释模糊的词对。", "result": "1. 平均CHSH期望值在1.2到2.8之间。2. 某些运行结果（例如2.3-2.4）显著违反了经典边界（|S|≤2）。3. 表明模糊性下的语言解释可以表现出非经典语境性，与人类认知实验结果一致。4. 暗示基于经典频率学派的自然语言分析方法必然是有损的。", "conclusion": "经典频率学派的自然语言分析方法必然是有损的。相反，贝叶斯式重复采样方法可以提供更实用和合适的语境下语言意义的表征。", "translation": "语义简并性是自然语言的一个基本属性，它超越了简单的多义性，涵盖了随着语义表达复杂性增加而出现的潜在解释的组合爆炸。大语言模型（LLMs）和其他现代NLP系统面临固有限制，正是因为它们在自然语言本身中运行，使其受到语义简并性所施加的相同解释约束。在这项工作中，我们使用柯尔莫哥洛夫复杂度论证，认为随着表达式复杂性的增长，任何解释代理（人类或LLM驱动的AI）恢复单一预期含义的可能性都会消失。这种计算上的难处理性表明，语言形式本身具有意义的经典观点是有缺陷的。我们转而提出，意义是通过依赖于观察者的解释行为来实现的。为了验证这一点，我们使用不同的LLM代理作为“计算认知系统”，在不同的上下文设置下解释模糊的词对，进行了语义贝尔不等式测试。在几次独立的实验中，我们发现平均CHSH期望值在1.2到2.8之间，其中一些运行结果（例如2.3-2.4）显著违反了经典边界（|S|≤2）。这表明模糊性下的语言解释可以表现出非经典语境性，与人类认知实验结果一致。这些结果固有地暗示，基于经典频率学派的自然语言分析方法必然是有损的。相反，我们提出贝叶斯式重复采样方法可以在上下文中提供更实用和合适的语言意义表征。", "summary": "本文探讨了自然语言中“语义简并性”的根本属性，指出大语言模型等传统NLP系统在处理复杂语义时的局限性。作者通过柯尔莫哥洛夫复杂度论证了单一确定意义的解释难度，并提出意义是观察者依赖的解释行为。为验证此点，研究使用LLM代理进行语义贝尔不等式测试，发现语言解释在模糊性下表现出非经典语境性，CHSH值显著违反经典边界。这表明经典频率学派方法有损，并提议采用贝叶斯式重复采样方法来更好地表征语境中的语言意义。", "keywords": "语义简并性, 量子语义, 贝尔不等式, 大语言模型, 自然语言处理", "comments": "本文创新性地将量子力学中的贝尔不等式测试引入自然语言处理领域，并通过LLM代理进行实验，验证了语言解释的非经典语境性，为理解和处理自然语言的复杂性提供了全新的视角。这项工作挑战了传统的语言意义观，并为未来NLP系统提出了基于贝叶斯方法的潜在方向，具有重要的理论和实践意义。"}}
{"id": "2506.10036", "title": "Token Perturbation Guidance for Diffusion Models", "authors": ["Javad Rajabi", "Soroush Mehraban", "Seyedmorteza Sadat", "Babak Taati"], "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2$\\times$ improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance", "comment": "18 pages, 14 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10036v1", "AI": {"title_translation": "扩散模型的Token扰动引导", "tldr": "TPG是一种无需训练、与条件无关的扩散模型引导方法，通过扰动中间token来提升生成质量和对齐，效果媲美CFG。", "motivation": "现有的分类器无关引导（CFG）需要特定训练程序且仅限于条件生成，限制了其应用范围。", "method": "本文提出了Token扰动引导（TPG），这是一种新颖的方法，它将范数保持的扰动矩阵直接应用于扩散网络中的中间token表示，从而提供有效且稳定的引导信号，无需架构更改即可提高生成质量。", "result": "TPG无需训练且与输入条件无关，适用于条件和无条件生成。在SDXL和Stable Diffusion 2.1上的实验表明，TPG在无条件生成方面相对于SDXL基线实现了近2倍的FID改进，同时在提示对齐方面与CFG非常接近。", "conclusion": "TPG是一种通用的、与条件无关的引导方法，为更广泛的扩散模型带来了类似CFG的优势。", "translation": "分类器无关引导（CFG）已成为现代扩散模型中必不可少的组件，用于提高生成质量和与输入条件的对齐。然而，CFG需要特定的训练程序，并且仅限于条件生成。为了解决这些限制，我们提出了Token扰动引导（TPG），这是一种新颖的方法，它将扰动矩阵直接应用于扩散网络中的中间token表示。TPG采用范数保持的洗牌操作，提供有效且稳定的引导信号，无需架构更改即可提高生成质量。因此，TPG无需训练且与输入条件无关，使其易于应用于条件生成和无条件生成。我们进一步分析了TPG提供的引导项，并表明与现有无需训练的引导技术相比，其对采样的影响更接近CFG。在SDXL和Stable Diffusion 2.1上进行的广泛实验表明，TPG在无条件生成方面相对于SDXL基线实现了近2倍的FID改进，同时在提示对齐方面与CFG非常接近。这些结果确立了TPG作为一种通用的、与条件无关的引导方法，为更广泛的扩散模型带来了类似CFG的优势。代码可在https://github.com/TaatiTeam/Token-Perturbation-Guidance 获取。", "summary": "本文提出了一种名为Token扰动引导（TPG）的新型方法，旨在克服现有分类器无关引导（CFG）的局限性。TPG通过对扩散模型中的中间token表示应用范数保持的扰动矩阵来提供稳定的引导信号，从而在不改变模型架构的情况下提升生成质量。该方法无需训练且与输入条件无关，适用于条件和无条件生成。实验结果表明，TPG在无条件生成方面显著优于基线模型，并在提示对齐上媲美CFG，证明其是一种通用且有效的扩散模型引导技术。", "keywords": "Token Perturbation Guidance, Diffusion Models, Classifier-Free Guidance", "comments": "TPG的创新之处在于提出了一种无需训练且与条件无关的扩散模型引导机制，通过直接扰动中间token表示来提升生成质量，有效解决了CFG的局限性。其范数保持的洗牌操作保证了引导的稳定性和有效性，使得该方法具有广泛的应用潜力，能够为更多扩散模型带来CFG级别的性能提升。"}}
{"id": "2506.10346", "title": "An Analysis of Datasets, Metrics and Models in Keyphrase Generation", "authors": ["Florian Boudin", "Akiko Aizawa"], "summary": "Keyphrase generation refers to the task of producing a set of words or\nphrases that summarises the content of a document. Continuous efforts have been\ndedicated to this task over the past few years, spreading across multiple lines\nof research, such as model architectures, data resources, and use-case\nscenarios. Yet, the current state of keyphrase generation remains unknown as\nthere has been no attempt to review and analyse previous work. In this paper,\nwe bridge this gap by presenting an analysis of over 50 research papers on\nkeyphrase generation, offering a comprehensive overview of recent progress,\nlimitations, and open challenges. Our findings highlight several critical\nissues in current evaluation practices, such as the concerning similarity among\ncommonly-used benchmark datasets and inconsistencies in metric calculations\nleading to overestimated performances. Additionally, we address the limited\navailability of pre-trained models by releasing a strong PLM-based model for\nkeyphrase generation as an effort to facilitate future research.", "comment": "GEM^2 paper @ ACL 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10346v1", "AI": {"title_translation": "关键词生成中的数据集、评估指标和模型分析", "tldr": "本文对50多篇关键词生成研究论文进行了分析，揭示了当前评估实践中的关键问题，例如基准数据集的相似性和评估指标计算的不一致性，并发布了一个基于PLM的强大模型以促进未来研究。", "motivation": "由于现有工作缺乏回顾和分析，目前关键词生成领域的研究现状尚不明确，因此需要对该领域的进展、局限性及挑战进行全面分析。", "method": "通过分析超过50篇关于关键词生成的研究论文，本文对该领域的最新进展、局限性以及开放性挑战进行了全面概述。此外，还发布了一个基于PLM的强大模型。", "result": "研究发现当前评估实践中存在多个关键问题，包括常用基准数据集之间的高度相似性，以及评估指标计算中的不一致性导致性能被高估。同时，也指出了预训练模型可用性有限的问题。", "conclusion": "本文通过分析当前关键词生成领域的不足，揭示了评估实践中的缺陷并发布了一个新的预训练模型，旨在弥合研究空白并促进该领域的未来发展。", "translation": "关键词生成是指生成一组总结文档内容的单词或短语的任务。在过去几年中，人们为这项任务持续努力，研究范围涵盖了模型架构、数据资源和用例场景等多个方面。然而，由于尚未有对现有工作进行回顾和分析的尝试，当前关键词生成的状态仍然未知。在本文中，我们通过对50多篇关于关键词生成的研究论文进行分析来弥补这一空白，提供了对最新进展、局限性和开放挑战的全面概述。我们的发现突出了当前评估实践中的几个关键问题，例如常用基准数据集之间令人担忧的相似性，以及评估指标计算中的不一致性导致性能被高估。此外，我们通过发布一个强大的基于PLM的关键词生成模型来解决预训练模型可用性有限的问题，以促进未来的研究。", "summary": "本文对50多篇关于关键词生成的研究论文进行了全面的分析，旨在弥补现有工作缺乏回顾的空白。研究揭示了当前评估实践中的关键问题，包括基准数据集的相似性和评估指标计算的不一致性，这些问题可能导致模型性能被高估。为促进未来研究，论文还发布了一个强大的基于PLM的关键词生成模型，以解决预训练模型稀缺的问题。", "keywords": "关键词生成, 数据集分析, 评估指标, 预训练模型, 文献综述", "comments": "本文作为一篇综述性分析，对关键词生成领域现有的数据集、评估指标和模型进行了系统性的梳理和批判性评估，揭示了当前研究中普遍存在但常被忽视的问题。其创新点在于首次对该领域进行了如此大规模的综合性分析，并提供了一个实用的PLM基线模型，对于规范和引导未来研究方向具有重要意义。"}}
{"id": "2506.10261", "title": "Enhanced randomized Douglas-Rachford method: Improved probabilities and adaptive momentum", "authors": ["Liqi Guo", "Ruike Xiang", "Deren Han", "Jiaxin Xie"], "summary": "Randomized iterative methods have gained recent interest in machine learning\nand signal processing for solving large-scale linear systems. One such example\nis the randomized Douglas-Rachford (RDR) method, which updates the iterate by\nreflecting it through two randomly selected hyperplanes and taking a convex\ncombination with the current point. In this work, we enhance RDR by introducing\nimproved sampling strategies and an adaptive heavy-ball momentum scheme.\nSpecifically, we incorporate without-replacement and volume sampling into RDR,\nand establish stronger convergence guarantees compared to conventional i.i.d.\nsampling. Furthermore, we develop an adaptive momentum mechanism that\ndynamically adjusts step sizes and momentum parameters based on previous\niterates, and prove that the resulting method achieves linear convergence in\nexpectation with improved convergence bounds. Numerical experiments demonstrate\nthat the enhanced RDR method consistently outperforms the original version,\nproviding substantial practical benefits across a range of problem settings.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10261v1", "AI": {"title_translation": "增强型随机Douglas-Rachford方法：改进的概率和自适应动量", "tldr": "本文通过引入改进的采样策略和自适应动量机制，增强了随机Douglas-Rachford (RDR) 方法，并在理论和实验上证明了其更强的收敛性和优越的性能。", "motivation": "随机迭代方法在机器学习和信号处理中解决大规模线性系统方面引起了广泛关注，其中随机Douglas-Rachford (RDR) 方法是一个典型例子。现有方法存在收敛性有待提高的问题，因此需要对其进行增强。", "method": "本文通过引入改进的采样策略和自适应重球动量方案来增强RDR。具体来说，将不放回采样和体积采样结合到RDR中，并开发了一种自适应动量机制，该机制根据之前的迭代动态调整步长和动量参数。", "result": "与传统的i.i.d.采样相比，本文的方法建立了更强的收敛性保证。所提出的方法实现了预期的线性收敛，并具有改进的收敛界。数值实验表明，增强型RDR方法始终优于原始版本，在各种问题设置中提供了显著的实际效益。", "conclusion": "通过引入改进的采样策略和自适应动量机制，增强的随机Douglas-Rachford方法在理论收敛性和实际性能上均优于原始版本，为解决大规模线性系统提供了有效方案。", "translation": "随机迭代方法在机器学习和信号处理中解决大规模线性系统方面引起了最近的兴趣。一个这样的例子是随机Douglas-Rachford (RDR) 方法，它通过反射迭代点通过两个随机选择的超平面并与当前点进行凸组合来更新迭代。在这项工作中，我们通过引入改进的采样策略和自适应重球动量方案来增强RDR。具体来说，我们将不放回采样和体积采样结合到RDR中，并建立了比传统i.i.d.采样更强的收敛性保证。此外，我们开发了一种自适应动量机制，该机制根据之前的迭代动态调整步长和动量参数，并证明由此产生的方法在预期中实现了线性收敛，并具有改进的收敛界。数值实验表明，增强型RDR方法始终优于原始版本，在各种问题设置中提供了显著的实际效益。", "summary": "本文提出了一种增强型的随机Douglas-Rachford (RDR) 方法，旨在解决大规模线性系统。通过引入不放回采样、体积采样等改进的采样策略，并结合一种能动态调整参数的自适应重球动量机制，新方法在理论上取得了比传统RDR方法更强的收敛性保证和更优的收敛界。数值实验也验证了其在实际应用中超越原始RDR方法的优越性能。", "keywords": "随机Douglas-Rachford, 自适应动量, 采样策略, 线性收敛, 大规模线性系统", "comments": "这项工作在随机优化领域具有重要意义，它通过结合先进的采样技术和自适应动量机制，显著提升了Douglas-Rachford方法的性能。这种结合不仅在理论上提供了更强的收敛保证，而且在实践中也展现出优越的效率，对于处理大规模优化问题具有重要的应用价值。"}}
{"id": "2506.10225", "title": "Fine-Grained control over Music Generation with Activation Steering", "authors": ["Dipanshu Panda", "Jayden Koshy Joe", "Harshith M R", "Swathi Narashiman", "Pranay Mathur", "Anish Veerakumar", "Aniruddh Krishna", "Keerthiharan A"], "summary": "We present a method for fine-grained control over music generation through\ninference-time interventions on an autoregressive generative music transformer\ncalled MusicGen. Our approach enables timbre transfer, style transfer, and\ngenre fusion by steering the residual stream using weights of linear probes\ntrained on it, or by steering the attention layer activations in a similar\nmanner. We observe that modelling this as a regression task provides improved\nperformance, hypothesizing that the mean-squared-error better preserve\nmeaningful directional information in the activation space. Combined with the\nglobal conditioning offered by text prompts in MusicGen, our method provides\nboth global and local control over music generation. Audio samples illustrating\nour method are available at our demo page.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10225v1", "AI": {"title_translation": "激活引导的音乐生成细粒度控制", "tldr": "该研究提出了一种通过激活引导（对残差流或注意力层进行干预）在MusicGen模型中实现音乐生成细粒度控制的方法，可用于音色、风格迁移和流派融合。", "motivation": "旨在实现对音乐生成过程的细粒度控制，以超越现有模型的全局控制能力，实现音色、风格和流派的特定调整。", "method": "该方法通过在推理时对自回归生成音乐Transformer模型MusicGen进行干预实现。具体而言，通过使用在其上训练的线性探针的权重引导残差流，或以类似方式引导注意力层激活，从而实现音色迁移、风格迁移和流派融合。研究还发现将其建模为回归任务能提供更好的性能，因为均方误差能更好地保留激活空间中有意义的方向信息。", "result": "该方法成功实现了音色迁移、风格迁移和流派融合。结合MusicGen中的文本提示提供的全局条件，该方法为音乐生成提供了全局和局部双重控制。", "conclusion": "通过在MusicGen中引入激活引导的推理时干预，可以实现对音乐生成过程的细粒度控制，从而支持音色、风格和流派的复杂操作，并提供了全局与局部相结合的控制能力。", "translation": "我们提出了一种通过对自回归生成音乐Transformer模型MusicGen进行推理时干预，实现音乐生成细粒度控制的方法。我们的方法通过使用在其上训练的线性探针的权重引导残差流，或以类似方式引导注意力层激活，从而实现音色迁移、风格迁移和流派融合。我们观察到，将其建模为回归任务能提供更好的性能，并假设均方误差能更好地保留激活空间中有意义的方向信息。结合MusicGen中文本提示提供的全局条件，我们的方法为音乐生成提供了全局和局部双重控制。展示我们方法的音频样本可在我们的演示页面获取。", "summary": "本文提出了一种名为“激活引导”的新方法，用于在自回归音乐生成Transformer模型MusicGen中实现细粒度控制。该方法通过在推理时干预模型的残差流或注意力层激活，利用线性探针的权重引导，从而实现音色迁移、风格迁移和流派融合。研究发现将此过程建模为回归任务能提高性能。结合MusicGen原有的文本提示全局控制，该方法提供了对音乐生成过程的局部和全局双重控制能力。", "keywords": "音乐生成, 细粒度控制, 激活引导, MusicGen, Transformer", "comments": "这篇论文的创新点在于提出了“激活引导”这一概念，通过直接干预Transformer模型的内部激活空间，实现了对音乐生成过程前所未有的细粒度控制。这种方法超越了传统的文本提示全局控制，允许用户精确调整音乐的特定属性，如音色、风格和流派，具有重要的实际应用潜力。将控制任务建模为回归问题并利用MSE来保留方向信息也是一个有趣的见解。"}}
{"id": "2506.10166", "title": "DeepPolar+: Breaking the BER-BLER Trade-off with Self-Attention and SMART (SNR-MAtched Redundancy Technique) decoding", "authors": ["Shubham Srivastava", "Adrish Banerjee"], "summary": "DeepPolar codes have recently emerged as a promising approach for channel\ncoding, demonstrating superior bit error rate (BER) performance compared to\nconventional polar codes. Despite their excellent BER characteristics, these\ncodes exhibit suboptimal block error rate (BLER) performance, creating a\nfundamental BER-BLER trade-off that severely limits their practical deployment\nin communication systems. This paper introduces DeepPolar+, an enhanced neural\npolar coding framework that systematically eliminates this BER-BLER trade-off\nby simultaneously improving BLER performance while maintaining the superior BER\ncharacteristics of DeepPolar codes. Our approach achieves this breakthrough\nthrough three key innovations: (1) an attention-enhanced decoder architecture\nthat leverages multi-head self-attention mechanisms to capture complex\ndependencies between bit positions, (2) a structured loss function that jointly\noptimizes for both bit-level accuracy and block-level reliability, and (3) an\nadaptive SNR-Matched Redundancy Technique (SMART) for decoding DeepPolar+ code\n(DP+SMART decoder) that combines specialized models with CRC verification for\nrobust performance across diverse channel conditions. For a (256,37) code\nconfiguration, DeepPolar+ demonstrates notable improvements in both BER and\nBLER performance compared to conventional successive cancellation decoding and\nDeepPolar, while achieving remarkably faster convergence through improved\narchitecture and optimization strategies. The DeepPolar+SMART variant further\namplifies these dual improvements, delivering significant gains in both error\nrate metrics over existing approaches. DeepPolar+ effectively bridges the gap\nbetween theoretical potential and practical implementation of neural polar\ncodes, offering a viable path forward for next-generation error correction\nsystems.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10166v1", "AI": {"title_translation": "DeepPolar+：通过自注意力机制和SMART（信噪比匹配冗余技术）解码打破BER-BLER权衡", "tldr": "DeepPolar+ 通过引入自注意力机制、结构化损失函数和SMART解码，显著改善了DeepPolar码的块错误率（BLER）性能，同时保持了优异的比特错误率（BER），从而解决了BER-BLER权衡问题，并加速了收敛。", "motivation": "DeepPolar码虽然在比特错误率（BER）方面表现优异，但在块错误率（BLER）方面表现不佳，存在BER-BLER权衡，严重限制了其在通信系统中的实际部署。", "method": "本文引入了DeepPolar+，一个增强的神经极化编码框架，通过三个关键创新解决BER-BLER权衡问题：1) 引入多头自注意力机制的注意力增强解码器架构；2) 联合优化比特级精度和块级可靠性的结构化损失函数；3) 用于DeepPolar+码解码的自适应SNR匹配冗余技术（SMART），结合专用模型和CRC验证。", "result": "对于(256,37)码配置，DeepPolar+在BER和BLER性能上均优于传统逐次抵消解码和DeepPolar；同时通过改进架构和优化策略实现了更快的收敛。DeepPolar+SMART变体进一步提升了错误率指标。", "conclusion": "DeepPolar+有效弥合了神经极化码在理论潜力与实际实现之间的差距，为下一代纠错系统提供了可行的前进路径。", "translation": "DeepPolar码最近已成为一种有前景的信道编码方法，与传统极化码相比，展现出卓越的比特错误率（BER）性能。尽管其BER特性优异，但这些码表现出次优的块错误率（BLER）性能，这造成了一个根本性的BER-BLER权衡，严重限制了它们在通信系统中的实际部署。本文引入了DeepPolar+，一个增强的神经极化编码框架，通过同时提高BLER性能并保持DeepPolar码卓越的BER特性，系统地消除了这种BER-BLER权衡。我们的方法通过三项关键创新实现了这一突破：(1) 一个注意力增强的解码器架构，利用多头自注意力机制捕获比特位置之间复杂的依赖关系；(2) 一个结构化损失函数，联合优化比特级精度和块级可靠性；(3) 一种用于DeepPolar+码解码的自适应信噪比匹配冗余技术（SMART）（DP+SMART解码器），结合专用模型和CRC验证，以在不同信道条件下实现鲁棒性能。对于(256,37)码配置，与传统逐次抵消解码和DeepPolar相比，DeepPolar+在BER和BLER性能方面均表现出显著改进，同时通过改进的架构和优化策略实现了显著更快的收敛。DeepPolar+SMART变体进一步放大了这些双重改进，在现有方法上在错误率指标方面取得了显著增益。DeepPolar+有效弥合了神经极化码在理论潜力与实际实现之间的差距，为下一代纠错系统提供了可行的前进路径。", "summary": "本文介绍了DeepPolar+，一种增强的神经极化编码框架，旨在解决DeepPolar码在BER-BLER性能上的权衡问题。通过引入自注意力增强的解码器架构、结构化损失函数以及自适应SMART解码技术，DeepPolar+显著提升了块错误率（BLER）性能，同时保持了优异的比特错误率（BER），并加速了收敛。实验结果表明，DeepPolar+及其SMART变体在关键错误率指标上均优于现有方法，为神经极化码的实际应用铺平了道路。", "keywords": "DeepPolar+, 极化码, 神经网络, 自注意力, BER-BLER权衡", "comments": "本文通过引入自注意力机制、结构化损失函数和自适应冗余技术，创新性地解决了神经极化码在BER和BLER之间的核心权衡问题。其提出的DeepPolar+框架显著提升了码的实际可用性，为下一代通信系统中的错误纠正提供了重要进展。特别是SMART技术的引入，考虑了实际信道条件下的鲁棒性，具有较高的工程实践价值。"}}
{"id": "2506.10117", "title": "A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild", "authors": ["Klim Kireev", "Ana-Maria Creţu", "Raphael Meier", "Sarah Adel Bargal", "Elissa Redmiles", "Carmela Troncoso"], "summary": "Platforms and the law regulate digital content depicting minors (defined as\nindividuals under 18 years of age) differently from other types of content.\nGiven the sheer amount of content that needs to be assessed, machine\nlearning-based automation tools are commonly used to detect content depicting\nminors. To our knowledge, no dataset or benchmark currently exists for\ndetecting these identification methods in a multi-modal environment. To fill\nthis gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an\nimage-caption dataset aimed at benchmarking tools that detect depictions of\nminors. Our dataset is richer than previous child image datasets, containing\nimages of children in a variety of contexts, including fictional depictions and\npartially visible bodies. ICCWD contains 10,000 image-caption pairs manually\nlabeled to indicate the presence or absence of a child in the image. To\ndemonstrate the possible utility of our dataset, we use it to benchmark three\ndifferent detectors, including a commercial age estimation system applied to\nimages. Our results suggest that child detection is a challenging task, with\nthe best method achieving a 75.3% true positive rate. We hope the release of\nour dataset will aid in the design of better minor detection methods in a wide\nrange of scenarios.", "comment": "14 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10117v1", "AI": {"title_translation": "野外儿童检测的手动标注图像-标题数据集", "tldr": "发布了一个新的手动标注图像-标题数据集ICCWD，用于基准测试检测图像中未成年人的工具，并发现儿童检测是一项具有挑战性的任务。", "motivation": "现有平台和法律对描绘未成年人的数字内容有不同规定，需要自动化工具进行检测。然而，目前缺乏用于在多模态环境中检测未成年人识别方法的现有数据集或基准。", "method": "作者发布了Image-Caption Children in the Wild Dataset (ICCWD)，一个包含10,000个手动标注的图像-标题对的数据集，用于检测图像中是否存在儿童。他们使用该数据集对三种不同的检测器（包括一个商业年龄估计系统）进行了基准测试。", "result": "儿童检测是一项具有挑战性的任务，表现最好的方法真阳性率为75.3%。", "conclusion": "ICCWD数据集的发布有望帮助设计出在各种场景下更好的未成年人检测方法。", "translation": "平台和法律对描绘未成年人（定义为18岁以下个体）的数字内容有别于其他类型的内容进行监管。鉴于需要评估的内容量巨大，通常使用基于机器学习的自动化工具来检测描绘未成年人的内容。据我们所知，目前尚无用于在多模态环境中检测这些识别方法的数据集或基准。为了弥补这一空白，我们发布了图像-标题野外儿童数据集（ICCWD），这是一个旨在对检测未成年人描绘的工具进行基准测试的图像-标题数据集。我们的数据集比以前的儿童图像数据集更丰富，包含各种情境下的儿童图像，包括虚构描绘和部分可见身体。ICCWD包含10,000个手动标注的图像-标题对，以指示图像中是否存在儿童。为了展示我们数据集的潜在效用，我们使用它对三种不同的检测器进行了基准测试，包括一个应用于图像的商业年龄估计系统。我们的结果表明，儿童检测是一项具有挑战性的任务，表现最好的方法真阳性率为75.3%。我们希望我们数据集的发布将有助于在各种场景下设计出更好的未成年人检测方法。", "summary": "该论文发布了Image-Caption Children in the Wild Dataset (ICCWD)，一个包含10,000个手动标注图像-标题对的新数据集，旨在为检测数字内容中未成年人的自动化工具提供基准。该数据集比现有数据集更丰富，包含多种上下文的儿童图像。作者使用ICCWD对检测器进行了基准测试，结果表明儿童检测是一项具有挑战性的任务，最佳检测方法的真阳性率为75.3%。该数据集的发布旨在促进更有效的未成年人检测方法的设计。", "keywords": "未成年人检测, 图像-标题数据集, 内容审核, 机器学习, ICCWD", "comments": "这篇论文通过发布一个专门针对“野外”未成年人检测的多模态数据集，填补了当前研究领域的一个重要空白。其创新之处在于结合了图像和标题信息，并包含了更复杂的场景（如虚构描绘、部分可见身体），这使得数据集更具现实意义和挑战性。这项工作对于开发和评估符合法律法规的自动化内容审核工具至关重要，有助于保护未成年人。结果也明确指出了当前检测技术的局限性，强调了未来研究的必要性。"}}
{"id": "2506.10120", "title": "GRAIL: A Benchmark for GRaph ActIve Learning in Dynamic Sensing Environments", "authors": ["Maryam Khalid", "Akane Sano"], "summary": "Graph-based Active Learning (AL) leverages the structure of graphs to\nefficiently prioritize label queries, reducing labeling costs and user burden\nin applications like health monitoring, human behavior analysis, and sensor\nnetworks. By identifying strategically positioned nodes, graph AL minimizes\ndata collection demands while maintaining model performance, making it a\nvaluable tool for dynamic environments. Despite its potential, existing graph\nAL methods are often evaluated on static graph datasets and primarily focus on\nprediction accuracy, neglecting user-centric considerations such as sampling\ndiversity, query fairness, and adaptability to dynamic settings. To bridge this\ngap, we introduce GRAIL, a novel benchmarking framework designed to evaluate\ngraph AL strategies in dynamic, real-world environments. GRAIL introduces novel\nmetrics to assess sustained effectiveness, diversity, and user burden, enabling\na comprehensive evaluation of AL methods under varying conditions. Extensive\nexperiments on datasets featuring dynamic, real-life human sensor data reveal\ntrade-offs between prediction performance and user burden, highlighting\nlimitations in existing AL strategies. GRAIL demonstrates the importance of\nbalancing node importance, query diversity, and network topology, providing an\nevaluation mechanism for graph AL solutions in dynamic environments.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10120v1", "AI": {"title_translation": "GRAIL：动态传感环境中图主动学习的基准", "tldr": "GRAIL是一个用于评估动态真实世界环境中图主动学习的新基准，解决了现有方法忽视以用户为中心方面和动态设置的局限性。", "motivation": "现有的图主动学习（AL）方法通常在静态图数据集上进行评估，并且主要关注预测准确性，忽略了以用户为中心的考虑因素，例如采样多样性、查询公平性和对动态环境的适应性。", "method": "本文引入了GRAIL，一个新颖的基准测试框架，旨在评估动态、真实世界环境中的图AL策略。GRAIL引入了新颖的指标来评估持续有效性、多样性和用户负担。", "result": "对包含动态真实人类传感器数据集的广泛实验揭示了预测性能和用户负担之间的权衡，突出了现有AL策略的局限性。", "conclusion": "GRAIL展示了平衡节点重要性、查询多样性和网络拓扑的重要性，为动态环境中的图AL解决方案提供了评估机制。", "translation": "基于图的主动学习（AL）利用图的结构有效地优先处理标签查询，从而在健康监测、人类行为分析和传感器网络等应用中降低标签成本和用户负担。通过识别策略性定位的节点，图AL在保持模型性能的同时最大限度地减少数据收集需求，使其成为动态环境中的宝贵工具。尽管其潜力巨大，但现有的图AL方法通常在静态图数据集上进行评估，并且主要关注预测准确性，忽略了以用户为中心的考虑因素，例如采样多样性、查询公平性和对动态环境的适应性。为了弥补这一差距，我们引入了GRAIL，一个新颖的基准测试框架，旨在评估动态、真实世界环境中的图AL策略。GRAIL引入了新颖的指标来评估持续有效性、多样性和用户负担，从而能够在不同条件下对AL方法进行全面评估。对包含动态真实人类传感器数据集的广泛实验揭示了预测性能和用户负担之间的权衡，突出了现有AL策略的局限性。GRAIL展示了平衡节点重要性、查询多样性和网络拓扑的重要性，为动态环境中的图AL解决方案提供了评估机制。", "summary": "本文介绍了GRAIL，一个用于评估动态真实世界传感环境中图主动学习（AL）策略的新型基准测试框架。与现有仅关注预测准确性和静态数据集的方法不同，GRAIL整合了以用户为中心的指标，如持续有效性、多样性和用户负担。使用动态人类传感器数据的实验揭示了预测性能和用户负担之间的权衡，强调了在动态设置中有效图AL需要平衡节点重要性、查询多样性和网络拓扑。", "keywords": "图主动学习, 动态环境, 基准测试, 用户负担, 传感器网络", "comments": "GRAIL的创新之处在于其专注于动态环境和以用户为中心的指标，这些在现有图主动学习基准中经常被忽视。这项工作很重要，因为它提供了一个更真实、更全面的评估框架，解决了静态数据集评估和纯粹以准确性为导向的评估的局限性。它突出了现实世界AL应用中的实际挑战和权衡，推动该领域向更健壮和用户友好的解决方案发展。"}}
{"id": "2506.10224", "title": "Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces", "authors": ["Alejandro N Diaz", "Shane A McQuarrie", "John T Tencer", "Patrick J Blonigan"], "summary": "This paper develops an interpretable, non-intrusive reduced-order modeling\ntechnique using regularized kernel interpolation. Existing non-intrusive\napproaches approximate the dynamics of a reduced-order model (ROM) by solving a\ndata-driven least-squares regression problem for low-dimensional matrix\noperators. Our approach instead leverages regularized kernel interpolation,\nwhich yields an optimal approximation of the ROM dynamics from a user-defined\nreproducing kernel Hilbert space. We show that our kernel-based approach can\nproduce interpretable ROMs whose structure mirrors full-order model structure\nby embedding judiciously chosen feature maps into the kernel. The approach is\nflexible and allows a combination of informed structure through feature maps\nand closure terms via more general nonlinear terms in the kernel. We also\nderive a computable a posteriori error bound that combines standard error\nestimates for intrusive projection-based ROMs and kernel interpolants. The\napproach is demonstrated in several numerical experiments that include\ncomparisons to operator inference using both proper orthogonal decomposition\nand quadratic manifold dimension reduction.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10224v1", "AI": {"title_translation": "使用再生核希尔伯特空间的可解释且灵活的非侵入式降阶模型", "tldr": "本文提出了一种使用正则化核插值技术的可解释、非侵入式降阶模型，它比现有方法更灵活，并能产生可解释的降阶模型。", "motivation": "现有非侵入式降阶模型方法通过解决数据驱动的最小二乘回归问题来近似降阶模型（ROM）的动力学，但可能缺乏可解释性和灵活性。本文旨在开发一种更优的、可解释的非侵入式ROM技术。", "method": "本文提出了一种使用正则化核插值的方法，从用户定义的再生核希尔伯特空间中获得ROM动力学的最优近似。通过在核中嵌入精心选择的特征映射，使ROM结构能够反映全阶模型结构，并结合了特征映射的结构信息和核中更一般的非线性项的闭合项。此外，还推导了一个可计算的后验误差界限。", "result": "该基于核的方法能够生成可解释的降阶模型（ROMs），其结构与全阶模型结构相呼应。该方法灵活，可以结合通过特征映射获得的结构信息以及通过核中更一般的非线性项获得的闭合项。还推导出了一个可计算的后验误差界限。在多个数值实验中得到了验证，并与使用适当正交分解和二次流形降维的算子推断进行了比较。", "conclusion": "本文成功开发了一种基于正则化核插值技术的可解释、非侵入式降阶模型，它在可解释性、灵活性和误差估计方面优于或补充了现有方法，并在数值实验中得到了有效验证。", "translation": "本文提出了一种使用正则化核插值技术的可解释、非侵入式降阶建模方法。现有的非侵入式方法通过解决低维矩阵算子的数据驱动最小二乘回归问题来近似降阶模型（ROM）的动力学。我们的方法则利用正则化核插值，从用户定义的再生核希尔伯特空间中获得ROM动力学的最优近似。我们展示了我们基于核的方法可以通过在核中嵌入精心选择的特征映射来生成可解释的ROM，其结构反映了全阶模型结构。该方法灵活，允许通过特征映射结合知情的结构，并通过核中更一般的非线性项结合闭合项。我们还推导了一个可计算的后验误差界限，该界限结合了侵入式基于投影的ROM和核插值器的标准误差估计。该方法在多个数值实验中得到了验证，包括与使用适当正交分解和二次流形降维的算子推断的比较。", "summary": "本文提出了一种基于正则化核插值技术的可解释、非侵入式降阶模型（ROM）。与现有方法通过最小二乘回归近似动力学不同，该方法利用再生核希尔伯特空间进行最优近似。通过嵌入特征映射，该模型能产生与全阶模型结构相似的可解释ROM，并具有灵活性，可结合结构信息和非线性闭合项。文章还推导了结合侵入式ROM和核插值器的后验误差界限，并在数值实验中验证了该方法的有效性。", "keywords": "降阶模型, 核插值, 可解释性, 非侵入式, 再生核希尔伯特空间", "comments": "本文的创新点在于将正则化核插值引入非侵入式降阶模型领域，解决了现有方法在可解释性和灵活性方面的不足。通过在核中嵌入特征映射，实现了ROM结构对全阶模型的镜像，这对于理解模型行为至关重要。此外，推导可计算的后验误差界限也增加了该方法的实用性和可靠性。其灵活性允许结合先验结构知识和数据驱动的非线性项，使其在复杂系统建模中具有广阔的应用前景。"}}
{"id": "2506.10107", "title": "Deep Semantic Segmentation for Multi-Source Localization Using Angle of Arrival Measurements", "authors": ["Mustafa Atahan Nuhoglu", "Hakan Ali Cirpan"], "summary": "This paper presents a solution for multi source localization using only angle\nof arrival measurements. The receiver platform is in motion, while the sources\nare assumed to be stationary. Although numerous methods exist for single source\nlocalization, many relying on pseudo-linear formulations or non convex\noptimization techniques, there remains a significant gap in research addressing\nmulti source localization in dynamic environments. To bridge this gap, we\npropose a deep learning-based framework that leverages semantic segmentation\nmodels for multi source localization. Specifically, we employ UNet and UNetPP\nas backbone models, processing input images that encode the platform's\npositions along with the corresponding direction finding lines at each\nposition. By analyzing the intersections of these lines, the models effectively\nidentify and localize multiple sources. Through simulations, we evaluate both\nsingle- and multi-source localization scenarios. Our results demonstrate that\nwhile the proposed approach performs comparably to traditional methods in\nsingle source localization, it achieves accurate source localization even in\nchallenging conditions with high noise levels and an increased number of\nsources.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10107v1", "AI": {"title_translation": "基于到达角测量的多源定位深度语义分割", "tldr": "提出了一种基于深度学习（UNet/UNetPP）和语义分割的多源定位方法，在动态高噪声环境下表现良好。", "motivation": "现有单源定位方法多，但动态环境下多源定位研究存在显著空白。", "method": "提出一个基于深度学习的框架，利用语义分割模型（UNet和UNetPP）进行多源定位。输入是编码了平台位置和方向线的图像，通过分析这些线的交点来识别和定位源。", "result": "在单源定位中与传统方法性能相当；在多源和高噪声条件下，仍能实现准确的源定位。", "conclusion": "深度学习（语义分割）方法在动态、高噪声、多源定位场景中表现出色，尤其在挑战性条件下优于传统方法。", "translation": "本文提出了一种仅使用到达角测量进行多源定位的解决方案。接收平台处于运动状态，而信源则被假定为静止。尽管存在许多用于单源定位的方法，其中许多依赖于伪线性公式或非凸优化技术，但在动态环境下解决多源定位的研究仍存在显著空白。为了弥补这一空白，我们提出了一种基于深度学习的框架，该框架利用语义分割模型进行多源定位。具体来说，我们采用UNet和UNetPP作为骨干模型，处理编码了平台位置以及每个位置相应测向线的输入图像。通过分析这些线的交点，模型有效地识别和定位多个信源。通过仿真，我们评估了单源和多源定位场景。我们的结果表明，虽然所提出的方法在单源定位中与传统方法表现相当，但在高噪声水平和信源数量增加的挑战性条件下，它仍能实现准确的信源定位。", "summary": "本文提出一种新颖的深度学习框架，利用UNet和UNetPP等语义分割模型，通过分析接收平台运动轨迹上方向线的交点，实现仅基于到达角测量的多源定位。该方法在动态、高噪声、多源环境下表现出优于传统方法的准确性，有效填补了该领域的研究空白。", "keywords": "深度学习, 语义分割, 多源定位, 到达角, UNet", "comments": "这篇论文的创新点在于将深度学习中的语义分割技术应用于多源定位问题，特别是在动态和高噪声环境下的应用。这种方法避开了传统方法中常见的伪线性或非凸优化问题，为解决复杂的定位挑战提供了新的思路。"}}
{"id": "2506.10030", "title": "Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment", "authors": ["Tianyu Chen", "Jian Lou", "Wenjie Wang"], "summary": "As Retrieval-Augmented Generation (RAG) evolves into service-oriented\nplatforms (Rag-as-a-Service) with shared knowledge bases, protecting the\ncopyright of contributed data becomes essential. Existing watermarking methods\nin RAG focus solely on textual knowledge, leaving image knowledge unprotected.\nIn this work, we propose AQUA, the first watermark framework for image\nknowledge protection in Multimodal RAG systems. AQUA embeds semantic signals\ninto synthetic images using two complementary methods: acronym-based triggers\nand spatial relationship cues. These techniques ensure watermark signals\nsurvive indirect watermark propagation from image retriever to textual\ngenerator, being efficient, effective and imperceptible. Experiments across\ndiverse models and datasets show that AQUA enables robust, stealthy, and\nreliable copyright tracing, filling a key gap in multimodal RAG protection.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10030v1", "AI": {"title_translation": "在RAG即服务环境中保护多模态知识版权", "tldr": "AQUA是一个为多模态RAG系统设计的首个图像知识水印框架，通过语义信号嵌入合成图像，实现鲁棒、隐蔽和可靠的版权追踪，解决了现有方法仅限于文本的不足。", "motivation": "随着RAG发展为服务导向平台（RAG即服务）并共享知识库，保护贡献数据的版权变得至关重要。现有RAG中的水印方法仅关注文本知识，而图像知识未受保护，这促使了对多模态知识版权保护的需求。", "method": "提出AQUA，首个用于多模态RAG系统中图像知识保护的水印框架。AQUA通过两种互补方法将语义信号嵌入合成图像：基于首字母缩略词的触发器和空间关系线索。这些技术确保水印信号在从图像检索器到文本生成器的间接传播中依然存活，并且高效、有效且不可感知。", "result": "在不同模型和数据集上的实验表明，AQUA能够实现鲁棒、隐蔽和可靠的版权追踪。", "conclusion": "AQUA填补了多模态RAG保护领域的一个关键空白，为图像知识版权保护提供了有效的解决方案。", "translation": "随着检索增强生成（RAG）演变为面向服务的平台（RAG即服务）并共享知识库，保护贡献数据的版权变得至关重要。RAG中现有的水印方法仅关注文本知识，而图像知识未受保护。在这项工作中，我们提出了AQUA，这是首个用于多模态RAG系统中图像知识保护的水印框架。AQUA使用两种互补方法将语义信号嵌入合成图像：基于首字母缩略词的触发器和空间关系线索。这些技术确保水印信号在从图像检索器到文本生成器的间接水印传播中依然存活，并且高效、有效且不可感知。在不同模型和数据集上的实验表明，AQUA能够实现鲁棒、隐蔽和可靠的版权追踪，填补了多模态RAG保护的一个关键空白。", "summary": "该论文提出了AQUA，一个开创性的水印框架，旨在解决RAG即服务环境中多模态知识（特别是图像知识）的版权保护问题。针对现有水印技术仅限于文本的局限，AQUA通过将基于首字母缩略词的触发器和空间关系线索等语义信号嵌入合成图像中，实现了水印的鲁棒、隐蔽且有效的传播和追踪。实验证明AQUA在各种模型和数据集上均能提供可靠的版权追踪能力，有效填补了多模态RAG系统保护的空白。", "keywords": "多模态RAG, 知识版权, 图像水印, AQUA, 版权追踪", "comments": "AQUA的创新之处在于其首次将水印技术应用于多模态RAG系统中的图像知识保护，解决了现有方法仅限于文本的局限。其通过语义信号嵌入和两种互补方法的结合，确保了水印在复杂传播路径中的生存能力和不可感知性，对于RAG即服务环境下知识产权保护具有重要意义。"}}
{"id": "2506.10326", "title": "A Benchmark for Generalizing Across Diverse Team Strategies in Competitive Pokémon", "authors": ["Cameron Angliss", "Jiaxun Cui", "Jiaheng Hu", "Arrasy Rahman", "Peter Stone"], "summary": "Developing AI agents that can robustly adapt to dramatically different\nstrategic landscapes without retraining is a central challenge for multi-agent\nlearning. Pok\\'emon Video Game Championships (VGC) is a domain with an\nextraordinarily large space of possible team configurations of approximately\n$10^{139}$ - far larger than those of Dota or Starcraft. The highly discrete,\ncombinatorial nature of team building in Pok\\'emon VGC causes optimal\nstrategies to shift dramatically depending on both the team being piloted and\nthe opponent's team, making generalization uniquely challenging. To advance\nresearch on this problem, we introduce VGC-Bench: a benchmark that provides\ncritical infrastructure, standardizes evaluation protocols, and supplies\nhuman-play datasets and a range of baselines - from large-language-model agents\nand behavior cloning to reinforcement learning and empirical game-theoretic\nmethods such as self-play, fictitious play, and double oracle. In the\nrestricted setting where an agent is trained and evaluated on a single-team\nconfiguration, our methods are able to win against a professional VGC\ncompetitor. We extensively evaluated all baseline methods over progressively\nlarger team sets and find that even the best-performing algorithm in the\nsingle-team setting struggles at scaling up as team size grows. Thus, policy\ngeneralization across diverse team strategies remains an open challenge for the\ncommunity. Our code is open sourced at\nhttps://github.com/cameronangliss/VGC-Bench.", "comment": "15 pages, 3 figures, 10 tables, submitted to NeurIPS 2025 Datasets &\n  Benchmarks Track", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10326v1", "AI": {"title_translation": "宝可梦对战中跨越多样化团队策略泛化的基准", "tldr": "本文提出了VGC-Bench，一个用于研究宝可梦对战中AI代理在面对极大团队配置空间时如何泛化和适应不同策略的基准。研究发现，即使在单团队设置下表现最佳的算法，在团队规模增大时也难以扩展，表明跨多样化团队策略的策略泛化仍是一个开放挑战。", "motivation": "在多智能体学习中，开发无需重新训练即可鲁棒适应截然不同战略格局的AI代理是一个核心挑战。宝可梦电子游戏锦标赛（VGC）拥有约$10^{139}$种可能的团队配置，其高度离散、组合式的团队构建性质使得最优策略会根据自身和对手的团队发生显著变化，从而使泛化成为独特的挑战。", "method": "为推进此问题研究，本文引入了VGC-Bench：一个提供关键基础设施、标准化评估协议，并提供人类游戏数据集和一系列基线的基准。基线包括大型语言模型代理、行为克隆、强化学习和经验博弈论方法（如自博弈、虚构博弈和双重神谕）。", "result": "在代理在单一团队配置上训练和评估的受限设置中，所提出的方法能够战胜专业的VGC选手。然而，研究人员对所有基线方法在逐步增大的团队集合上进行了广泛评估，发现即使在单团队设置下表现最佳的算法，在团队规模增大时也难以扩展。", "conclusion": "因此，跨多样化团队策略的策略泛化仍然是社区面临的一个开放挑战。", "translation": "开发能够鲁棒适应截然不同战略格局而无需重新训练的AI代理是多智能体学习的一个核心挑战。宝可梦电子游戏锦标赛（VGC）是一个拥有约$10^{139}$种可能的团队配置的领域——远大于Dota或星际争霸。宝可梦VGC中团队构建的高度离散、组合性质导致最优策略会根据所操控的团队和对手的团队发生显著变化，使得泛化具有独特的挑战性。为了推进对该问题的研究，我们引入了VGC-Bench：一个提供关键基础设施、标准化评估协议，并提供人类游戏数据集和一系列基线的基准——这些基线包括大型语言模型代理、行为克隆到强化学习以及经验博弈论方法，如自博弈、虚构博弈和双重神谕。在代理在单一团队配置上训练和评估的受限设置中，我们的方法能够战胜专业的VGC职业选手。我们对所有基线方法在逐步增大的团队集合上进行了广泛评估，发现即使在单团队设置下表现最佳的算法，在团队规模增大时也难以扩展。因此，跨多样化团队策略的策略泛化仍然是社区面临的一个开放挑战。我们的代码已在https://github.com/cameronangliss/VGC-Bench开源。", "summary": "本文介绍了VGC-Bench，一个针对宝可梦电子游戏锦标赛（VGC）的AI基准，旨在解决多智能体学习中AI代理在极高维度策略空间中泛化和适应的挑战。VGC-Bench提供了评估基础设施、标准协议、人类数据集以及多种基线方法（包括LLM、行为克隆、RL和博弈论方法）。研究发现，尽管在单一团队设置下能击败专业选手，但现有算法在扩展到多样化团队策略时表现不佳，表明策略泛化仍是该领域的一个主要开放问题。", "keywords": "宝可梦VGC, AI泛化, 多智能体学习, VGC-Bench, 策略适应", "comments": "本文提出了一个针对宝可梦VGC的创新性基准，该领域因其庞大的团队配置空间和高度组合性而对AI泛化提出了独特挑战。VGC-Bench的重要性在于它为解决这一核心多智能体学习问题提供了必要的基础设施、数据集和评估标准。研究结果揭示了当前AI方法在处理复杂策略泛化方面的局限性，特别是在团队多样性增加时，这为未来的研究指明了方向。"}}
{"id": "2506.10056", "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput", "authors": ["Gabriel Orlanski", "Nicholas Roberts", "Aws Albarghouthi", "Frederic Sala"], "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.", "comment": "29 pages, 6 figures, code released here:\n  https://github.com/SprocketLab/orm-code-verifier", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10056v1", "AI": {"title_translation": "奖励模型通过权衡准确性与吞吐量实现可扩展的代码验证", "tldr": "奖励模型（ORM）可以在代码验证中通过牺牲少量准确性来显著提高速度，特别是在“生成-剪枝-然后-排序”方法中，这使得程序排名系统更具可扩展性。", "motivation": "现有共识认为在代码任务中应优先使用综合验证器而非结果奖励模型（ORM），且很少考虑其权衡。本文旨在挑战这一假设，系统性地探索速度与准确性之间的权衡。", "method": "通过系统性探索速度与准确性之间的权衡，研究了结果奖励模型（ORM）在代码验证中的作用。特别分析了在“生成-剪枝-然后-排序”方法中使用ORM，其中较快但准确性较低的验证器在排序前移除不正确的解决方案。", "result": "发现结果奖励模型（ORM）在可扩展验证中通过牺牲准确性来提高速度方面发挥着关键作用，即使综合验证器可用。在“生成-剪枝-然后-排序”方法中，系统速度提高了11.65倍，而准确性仅降低了8.33%。该方法通过过滤掉不正确但排名靠前的解决方案来发挥作用。", "conclusion": "研究结果表明，结果奖励模型（ORM）能够实现可扩展且准确的程序排名系统设计。", "translation": "大型语言模型（LLMs）解决编码任务的标准范式是“生成-然后-排序”程序，其中后者在排序过程中使用验证器。日益增长的共识是，只要可能，就应优先使用综合验证器（例如，完整的测试套件），而不是结果奖励模型（ORM），而很少考虑其中涉及的权衡。我们旨在通过系统地探索速度和准确性之间的权衡来挑战这一假设。我们发现，即使综合验证器可用，ORM在通过牺牲准确性来提高速度方面，在扩展验证中也发挥着关键作用。当它们用于“生成-剪枝-然后-排序”方法时，其价值变得尤为明显，其中更快但准确性较低的验证器在排序之前移除不正确的解决方案——从而使系统速度提高了11.65倍，而准确性仅比完整的测试套件低8.33%。我们分析了“生成-剪枝-然后-排序”方法，并表明它通过过滤掉不正确但排名靠前的解决方案来发挥作用。这些发现使得可扩展且准确的程序排名系统设计成为可能。", "summary": "该研究挑战了在大型语言模型代码验证中优先使用综合验证器的传统观念，通过系统探索速度与准确性的权衡，发现结果奖励模型（ORM）在实现可扩展验证方面具有重要价值。特别是在“生成-剪枝-然后-排序”策略中，ORM能显著提升验证速度（11.65倍），同时仅略微降低准确性（8.33%），其工作原理是有效过滤掉高排名但错误的解决方案，从而为设计高效可扩展的程序排名系统提供了新途径。", "keywords": "奖励模型, 代码验证, 可扩展性, 吞吐量, 准确性", "comments": "本文创新性地提出了在代码验证中利用奖励模型（ORM）进行速度与准确性权衡的策略，挑战了业界普遍认为综合验证器优于ORM的观念。其提出的“生成-剪枝-然后-排序”方法，在保持较高准确性的前提下，显著提升了验证吞吐量，对于大规模LLM代码生成和验证的实际应用具有重要意义。该研究为优化程序排名系统提供了新的视角和实用方法。"}}
{"id": "2506.10229", "title": "Speculative Design in Spiraling Time: Methods and Indigenous HCI", "authors": ["James Eschrich", "Cole McMullen", "Sarah Sterman"], "summary": "In this position paper, we first discuss the uptake of speculative design as\na method for Indigenous HCI. Then, we outline how a key assumption about\ntemporality threatens to undermine the usefulness of speculative design in this\ncontext. Finally, we briefly sketch out a possible alternative understanding of\nspeculative design, based on the concept of \"spiraling time,\" which could be\nbetter suited for Indigenous HCI.", "comment": "3 pages, 1 figure, presented at the \"Weaving Indigeneity and Culture\n  into the Fabric of HCI Futures\" Workshop at CHI '25", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10229v1", "AI": {"title_translation": "螺旋时间中的思辨设计：方法与原住民人机交互", "tldr": "本立场论文讨论了思辨设计在原住民人机交互中的应用，指出其时间性假设的局限，并提出基于“螺旋时间”的替代理解。", "motivation": "探讨思辨设计在原住民人机交互(Indigenous HCI)中的应用，并指出其对时间性的关键假设可能会削弱其有效性。", "method": "本文是一篇立场论文，通过讨论思辨设计在原住民人机交互中的应用，概述了其时间性假设的局限，并提出了基于“螺旋时间”的替代性理解。", "result": "提出了一个基于“螺旋时间”概念的思辨设计替代性理解，认为其可能更适合原住民人机交互。", "conclusion": "基于“螺旋时间”的概念，提出了一个更适合原住民人机交互的思辨设计替代性理解。", "translation": "在这篇立场论文中，我们首先讨论了思辨设计作为一种原住民人机交互方法被采纳的情况。然后，我们概述了一个关于时间性的关键假设如何可能削弱思辨设计在此背景下的效用。最后，我们简要勾勒出一种基于“螺旋时间”概念的思辨设计替代性理解，该理解可能更适合原住民人机交互。", "summary": "这篇立场论文探讨了思辨设计在原住民人机交互中的应用。论文指出，思辨设计中对时间性的固有假设可能限制其在原住民语境下的有效性。为此，作者提出了一种基于“螺旋时间”概念的思辨设计替代性理解，认为这种新视角能更好地适应原住民人机交互的需求。", "keywords": "思辨设计, 原住民人机交互, 螺旋时间, 时间性, 立场论文", "comments": "这篇立场论文提供了一个关于思辨设计在特定文化背景下应用的批判性视角。其创新之处在于提出了“螺旋时间”的概念，挑战了传统思辨设计的时间性假设，为原住民人机交互领域提供了新的思考方向。论文重在概念探讨和理论构建，而非实证研究。"}}
{"id": "2506.10900", "title": "Dynamic Beyond 5G and 6G Connectivity: Leveraging NTN and RIS Synergies for Optimized Coverage and Capacity in High-Density Environments", "authors": ["Valdemar Farré", "Juan Estrada", "David Vega", "Luis F Urquiza-Aguiar", "Juan A. Vásquez Peralvo", "Symeon Chatzinotas"], "summary": "The increasing demand for reliable, high-capacity communication during\nlarge-scale outdoor events poses significant challenges for traditional\nTerrestrial Networks (TNs), which often struggle to provide consistent coverage\nin high-density environments. This paper presents a novel 6G radio network\nplanning framework that integrates Non-Terrestrial Networks (NTNs) with\nReconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and\nenhanced network capacity. Our framework overcomes the limitations of\nconventional deployable base stations by leveraging NTN architectures,\nincluding Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly\nintegrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies\nsuch as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by\noptimizing spectrum utilization across the C, S, and Ka bands, we implement a\nrigorous interference management strategy based on a dynamic SINR model.\nComprehensive calculations and simulations validate the proposed framework,\ndemonstrating significant improvements in connectivity, reliability, and\ncost-efficiency in crowded scenarios. This integration strategy represents a\npromising solution for meeting the evolving demands of future 6G networks.", "comment": "6 pages, 6 figures, 11 tables", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10900v1", "AI": {"title_translation": "动态B5G和6G连接：利用NTN和RIS协同优化高密度环境下的覆盖和容量", "tldr": "本文提出了一种结合非地面网络（NTN）和可重构智能表面（RIS）的新型6G无线网络规划框架，旨在为高密度环境提供优化的覆盖和容量，并通过模拟验证了其在连接性、可靠性和成本效益方面的显著提升。", "motivation": "传统地面网络（TNs）在大型户外活动等高密度环境中难以提供一致的覆盖和高容量通信，无法满足日益增长的可靠通信需求。", "method": "本文提出了一种新颖的6G无线网络规划框架，将非地面网络（NTN，包括低地球轨道卫星）与可重构智能表面（RIS）集成，并与B5G地面网络无缝结合。该框架利用了大规模多输入多输出（mMIMO）和波束成形等B5G技术，优化了C、S和Ka频段的频谱利用，并基于动态SINR模型实现了严格的干扰管理策略。", "result": "通过全面的计算和仿真验证，所提出的框架在高密度场景中显著改善了连接性、可靠性和成本效益。", "conclusion": "所提出的NTN和RIS集成策略为满足未来6G网络不断演进的需求提供了一个有前景的解决方案。", "translation": "对大型户外活动期间可靠、高容量通信日益增长的需求，给传统的地面网络（TNs）带来了巨大挑战，它们在高密度环境中往往难以提供一致的覆盖。本文提出了一种新颖的6G无线网络规划框架，该框架集成了非地面网络（NTNs）和可重构智能表面（RISs），以提供无处不在的覆盖和增强的网络容量。我们的框架通过利用NTN架构（包括低地球轨道（LEO）卫星）和与超5G（B5G）地面网络无缝集成的无源RIS平台，克服了传统可部署基站的局限性。通过结合大规模多输入多输出（mMIMO）和波束成形等先进的B5G技术，并通过优化C、S和Ka频段的频谱利用，我们实施了基于动态SINR模型的严格干扰管理策略。全面的计算和仿真验证了所提出的框架，在高密度场景中显示出连接性、可靠性和成本效益的显著提升。这种集成策略代表了满足未来6G网络不断演进需求的一个有前景的解决方案。", "summary": "本文提出了一种创新的6G无线网络规划框架，旨在解决传统地面网络在高密度环境下通信覆盖和容量不足的问题。该框架通过整合非地面网络（如LEO卫星）和可重构智能表面，并与现有B5G网络技术（如mMIMO和波束成形）协同工作，优化频谱利用和干扰管理。仿真结果表明，该集成策略能显著提升高密度场景下的连接性、可靠性和成本效益，为未来6G网络的发展提供了可行方案。", "keywords": "6G网络, 非地面网络, 可重构智能表面, 高密度环境, 频谱效率", "comments": "本文的创新点在于将NTN和RIS这两种前沿技术结合起来，为解决高密度环境下通信挑战提供了新思路。这种集成方法有望显著提升未来6G网络的覆盖和容量，具有重要的实际应用价值。"}}
{"id": "2506.10172", "title": "A Navigation Framework Utilizing Vision-Language Models", "authors": ["Yicheng Duan", "Kaiyu tang"], "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied\nAI, requiring agents to interpret natural language instructions and navigate\nthrough visually rich, unfamiliar environments. Recent advances in large\nvision-language models (LVLMs), such as CLIP and Flamingo, have significantly\nimproved multimodal understanding but introduced new challenges related to\ncomputational cost and real-time deployment. In this project, we propose a\nmodular, plug-and-play navigation framework that decouples vision-language\nunderstanding from action planning. By integrating a frozen vision-language\nmodel, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to\nachieve flexible, fast, and adaptable navigation without extensive model\nfine-tuning. Our framework leverages prompt engineering, structured history\nmanagement, and a two-frame visual input strategy to enhance decision-making\ncontinuity across navigation steps. We evaluate our system on the Room-to-Room\nbenchmark within the VLN-CE setting using the Matterport3D dataset and\nHabitat-Lab simulation environment. Although our initial results reveal\nchallenges in generalizing to unseen environments under strict evaluation\nsettings, our modular approach lays a foundation for scalable and efficient\nnavigation systems, highlighting promising directions for future improvement\nthrough enhanced environmental priors and expanded multimodal input\nintegration.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10172v1", "AI": {"title_translation": "利用视觉-语言模型的导航框架", "tldr": "本文提出了一个模块化的视觉-语言导航（VLN）框架，通过结合冻结的视觉-语言模型（Qwen2.5-VL-7B-Instruct）和轻量级规划逻辑，旨在实现高效且适应性强的导航，尽管在未见过环境中的泛化能力仍面临挑战。", "motivation": "视觉-语言导航（VLN）是具身AI中的一个复杂挑战，要求智能体解释自然语言指令并在视觉丰富的陌生环境中导航。当前大型视觉-语言模型（LVLMs）虽然显著提升了多模态理解能力，但也带来了计算成本高和实时部署难的新挑战。本文旨在提出一个灵活、快速、适应性强的导航框架，无需大量模型微调。", "method": "本文提出了一个模块化、即插即用的导航框架，将视觉-语言理解与动作规划解耦。通过集成一个冻结的视觉-语言模型（Qwen2.5-VL-7B-Instruct）与轻量级规划逻辑，旨在实现导航的灵活性、速度和适应性，而无需大量模型微调。该框架利用提示工程、结构化历史管理和双帧视觉输入策略来增强导航步骤间的决策连续性。系统在VLN-CE设置下的Room-to-Room基准上，使用Matterport3D数据集和Habitat-Lab仿真环境进行评估。", "result": "初步结果显示，在严格的评估设置下，该系统在泛化到未见过环境方面仍面临挑战。", "conclusion": "本文的模块化方法为可扩展和高效的导航系统奠定了基础，并指出了未来通过增强环境先验和扩展多模态输入集成来改进的有前景方向。", "translation": "视觉-语言导航（VLN）是具身AI中的一个复杂挑战，要求智能体解释自然语言指令并在视觉丰富、陌生的环境中导航。大型视觉-语言模型（LVLMs）如CLIP和Flamingo的最新进展，显著改善了多模态理解，但也带来了计算成本和实时部署相关的新挑战。在这个项目中，我们提出了一个模块化、即插即用的导航框架，将视觉-语言理解与动作规划解耦。通过集成一个冻结的视觉-语言模型Qwen2.5-VL-7B-Instruct与轻量级规划逻辑，我们的目标是实现灵活、快速、适应性强的导航，而无需大量模型微调。我们的框架利用提示工程、结构化历史管理和双帧视觉输入策略来增强导航步骤间的决策连续性。我们在VLN-CE设置下的Room-to-Room基准上，使用Matterport3D数据集和Habitat-Lab仿真环境评估了我们的系统。尽管我们的初步结果显示，在严格的评估设置下，泛化到未见过环境方面仍面临挑战，但我们的模块化方法为可扩展和高效的导航系统奠定了基础，突出了未来通过增强环境先验和扩展多模态输入集成来改进的有前景方向。", "summary": "本文提出了一种模块化、即插即用的视觉-语言导航（VLN）框架，旨在解决大型视觉-语言模型（LVLMs）在具身AI中面临的计算成本和实时部署挑战。该框架通过将视觉-语言理解与动作规划解耦，并集成一个冻结的LVLM（Qwen2.5-VL-7B-Instruct）与轻量级规划逻辑，实现了无需大量微调的灵活、快速和适应性导航。它利用提示工程、结构化历史管理和双帧视觉输入策略来提高决策连续性。尽管在Room-to-Room基准上的初步评估显示在未见过环境中的泛化能力存在挑战，但该方法为可扩展和高效的导航系统奠定了基础，并指出了未来的改进方向。", "keywords": "视觉-语言导航, 模块化框架, 大型视觉-语言模型, 具身AI, 导航", "comments": "本文的创新之处在于其模块化框架设计，特别是将视觉-语言理解与动作规划解耦，并利用冻结的大型视觉-语言模型来提升效率和部署能力，这在具身AI领域具有重要意义。这种方法有效解决了现有大型模型计算成本高昂的问题。然而，研究结果也明确指出了其在未见过环境泛化能力上的局限性，这是具身导航任务中普遍存在的挑战，为未来的研究指明了方向，例如通过增强环境先验知识和多模态输入集成来克服这一限制。"}}
{"id": "2506.10085", "title": "Test-Time Adaptation for Generalizable Task Progress Estimation", "authors": ["Christos Ziakas", "Alessandra Russo"], "summary": "We propose a test-time adaptation method that enables a progress estimation\nmodel to adapt online to the visual and temporal context of test trajectories\nby optimizing a learned self-supervised objective. To this end, we introduce a\ngradient-based meta-learning strategy to train the model on expert visual\ntrajectories and their natural language task descriptions, such that test-time\nadaptation improves progress estimation relying on semantic content over\ntemporal order. Our test-time adaptation method generalizes from a single\ntraining environment to diverse out-of-distribution tasks, environments, and\nembodiments, outperforming the state-of-the-art in-context learning approach\nusing autoregressive vision-language models.", "comment": "pages, 2 figures, accepted to the 2nd Workshop on Test-Time\n  Adaptation: Putting Updates to the Test (PUT) at 42nd International\n  Conference on Machine Learning (ICML), Vancouver, Canada, 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10085v1", "AI": {"title_translation": "用于可泛化任务进度估计的测试时自适应", "tldr": "本文提出了一种测试时自适应方法，通过优化自监督目标，使进度估计模型能够在线适应测试轨迹的视觉和时间上下文，从而在多样化的任务、环境和实体中实现泛化。", "motivation": "传统的进度估计模型可能难以泛化到新的、未见过的任务、环境和实体。本文旨在解决这一泛化问题，使模型能够适应测试时的视觉和时间上下文。", "method": "本文提出了一种测试时自适应方法。该方法通过优化学习到的自监督目标，使进度估计模型能够在线适应测试轨迹的视觉和时间上下文。为此，引入了一种基于梯度的元学习策略，在专家视觉轨迹及其自然语言任务描述上训练模型，使测试时自适应能够依靠语义内容而非时间顺序来改进进度估计。", "result": "所提出的测试时自适应方法能够从单一训练环境泛化到多样化的域外任务、环境和实体。它优于使用自回归视觉-语言模型的最先进的上下文学习方法。", "conclusion": "本文提出的测试时自适应方法通过在线适应测试轨迹的视觉和时间上下文，显著提高了任务进度估计的泛化能力，并超越了现有的先进方法。", "translation": "我们提出了一种测试时自适应方法，该方法通过优化学习到的自监督目标，使进度估计模型能够在线适应测试轨迹的视觉和时间上下文。为此，我们引入了一种基于梯度的元学习策略，在专家视觉轨迹及其自然语言任务描述上训练模型，使得测试时自适应能够依靠语义内容而非时间顺序来改进进度估计。我们的测试时自适应方法可以从单一训练环境泛化到多样化的域外任务、环境和实体，并且优于使用自回归视觉-语言模型的最先进的上下文学习方法。", "summary": "本文提出了一种创新的测试时自适应方法，旨在提升任务进度估计模型的泛化能力。该方法通过优化一个自监督目标，使模型能够在线适应测试轨迹的视觉和时间上下文。研究引入了基于梯度的元学习策略，利用专家视觉轨迹和自然语言任务描述进行训练，从而使模型在适应过程中更侧重于语义内容而非时间顺序。实验结果表明，该方法能有效泛化至多样化的域外任务、环境和实体，并且性能优于当前最先进的自回归视觉-语言上下文学习方法。", "keywords": "测试时自适应, 任务进度估计, 元学习, 自监督学习, 泛化", "comments": "本文的创新点在于提出了一个测试时自适应框架，通过自监督学习和元学习策略，使得进度估计模型在面对未知环境和任务时具有更强的泛化能力。它强调了语义内容在进度估计中的重要性，而非仅仅依赖于时间顺序。这种方法对于实际应用中需要模型在动态和多样化场景下工作的任务具有重要意义。"}}
{"id": "2506.10931", "title": "MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem", "authors": ["Melina Soysal", "Konstantina Koliogeorgi", "Can Firtina", "Nika Mansouri Ghiasi", "Rakesh Nadig", "Haiyu Mayo", "Geraldo F. Oliveira", "Yu Liang", "Klea Zambaku", "Mohammad Sadrosadati", "Onur Mutlu"], "summary": "Raw signal genome analysis (RSGA) has emerged as a promising approach to\nenable real-time genome analysis by directly analyzing raw electrical signals.\nHowever, rapid advancements in sequencing technologies make it increasingly\ndifficult for software-based RSGA to match the throughput of raw signal\ngeneration. This paper demonstrates that while hardware acceleration techniques\ncan significantly accelerate RSGA, the high volume of genomic data shifts the\nperformance and energy bottleneck from computation to I/O data movement. As\nsequencing throughput increases, I/O overhead becomes the main contributor to\nboth runtime and energy consumption. Therefore, there is a need to design a\nhigh-performance, energy-efficient system for RSGA that can both alleviate the\ndata movement bottleneck and provide large acceleration capabilities. We\npropose MARS, a storage-centric system that leverages the heterogeneous\nresources within modern storage systems (e.g., storage-internal DRAM, storage\ncontroller, flash chips) alongside their large storage capacity to tackle both\ndata movement and computational overheads of RSGA in an area-efficient and\nlow-cost manner. MARS accelerates RSGA through a novel hardware/software\nco-design approach. First, MARS modifies the RSGA pipeline via two filtering\nmechanisms and a quantization scheme, reducing hardware demands and optimizing\nfor in-storage execution. Second, MARS accelerates the RSGA steps directly\nwithin the storage by leveraging both Processing-Near-Memory and\nProcessing-Using-Memory paradigms. Third, MARS orchestrates the execution of\nall steps to fully exploit in-storage parallelism and minimize data movement.\nOur evaluation shows that MARS outperforms basecalling-based software and\nhardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on\naverage across different datasets, while reducing their energy consumption by\n427x and 72x.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.10931v1", "AI": {"title_translation": "MARS：存储子系统内部原始信号基因组分析的内存处理加速", "tldr": "MARS提出了一种以存储为中心的系统，利用存储系统内的异构资源，通过新颖的软硬件协同设计，在存储内部加速原始信号基因组分析，显著解决了数据移动瓶颈并提升了性能和能效。", "motivation": "原始信号基因组分析（RSGA）面临高通量测序技术带来的挑战，软件难以匹配原始信号生成的速度。硬件加速虽然有效，但大量基因组数据使得性能和能耗瓶颈从计算转移到I/O数据移动。因此，需要设计一种高性能、高能效的RSGA系统，以缓解数据移动瓶颈并提供强大的加速能力。", "method": "本文提出了MARS，一个以存储为中心的系统，利用现代存储系统内部的异构资源（如存储内部DRAM、存储控制器、闪存芯片）及其大容量存储来解决RSGA的数据移动和计算开销，实现面积效率和低成本。MARS通过新颖的软硬件协同设计加速RSGA：首先，通过两种过滤机制和一个量化方案修改RSGA流程，降低硬件需求并优化存储内执行；其次，通过利用近内存处理（Processing-Near-Memory）和使用内存处理（Processing-Using-Memory）范式，直接在存储内部加速RSGA步骤；第三，MARS协调所有步骤的执行，以充分利用存储内并行性并最小化数据移动。", "result": "MARS在不同数据集上，平均性能比基于basecalling的软件和硬件加速的最新读图谱流水线分别提升93倍和40倍，同时能耗分别降低427倍和72倍。", "conclusion": "MARS系统通过在存储内部进行原始信号基因组分析的加速，有效解决了数据移动瓶颈，并显著提升了性能和能效，证明了其作为高性能、高能效RSGA解决方案的潜力。", "translation": "原始信号基因组分析（RSGA）作为一种通过直接分析原始电信号实现实时基因组分析的有前景方法而出现。然而，测序技术的快速进步使得基于软件的RSGA越来越难以匹配原始信号生成的吞吐量。本文表明，尽管硬件加速技术可以显著加速RSGA，但大量基因组数据将性能和能耗瓶颈从计算转移到I/O数据移动。随着测序吞吐量的增加，I/O开销成为运行时和能耗的主要贡献者。因此，需要设计一个高性能、高能效的RSGA系统，既能缓解数据移动瓶颈，又能提供强大的加速能力。我们提出了MARS，一个以存储为中心的系统，它利用现代存储系统内的异构资源（例如，存储内部DRAM、存储控制器、闪存芯片）及其大容量存储，以面积效率和低成本的方式解决RSGA的数据移动和计算开销。MARS通过一种新颖的软硬件协同设计方法加速RSGA。首先，MARS通过两种过滤机制和一个量化方案修改RSGA流水线，减少硬件需求并优化存储内执行。其次，MARS通过利用近内存处理（Processing-Near-Memory）和使用内存处理（Processing-Using-Memory）范式，直接在存储内部加速RSGA步骤。第三，MARS协调所有步骤的执行，以充分利用存储内并行性并最小化数据移动。我们的评估显示，MARS在不同数据集上，平均性能比基于basecalling的软件和硬件加速的最新读图谱流水线分别提升93倍和40倍，同时能耗分别降低427倍和72倍。", "summary": "MARS是一种创新的存储中心系统，旨在通过在存储子系统内部进行处理来加速原始信号基因组分析（RSGA）。针对现有RSGA在处理高通量数据时面临的I/O瓶颈，MARS利用存储内部的异构资源，并采用软硬件协同设计。它通过优化RSGA流水线（包括过滤和量化）以及利用近内存和使用内存处理范式，显著减少了数据移动并提高了计算效率。实验结果表明，MARS在性能和能效方面均大幅超越现有软件和硬件加速方案。", "keywords": "原始信号基因组分析, 内存处理, 存储子系统, 数据移动, 硬件加速", "comments": "MARS的创新点在于将基因组原始信号分析的计算任务下沉到存储系统内部，有效解决了数据移动这一主要瓶颈。其软硬件协同设计，特别是利用PIM/PUM范式，是面向未来高通量测序数据处理的重要方向。该方法对于提升实时基因组分析的效率和降低能耗具有重要意义。"}}
{"id": "2506.10054", "title": "Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs", "authors": ["Shangpin Peng", "Weinong Wang", "Zhuotao Tian", "Senqiao Yang", "Xing Wu", "Haotian Xu", "Chengquan Zhang", "Takashi Isobe", "Baotian Hu", "Min Zhang"], "summary": "Direct Preference Optimization (DPO) has become a cornerstone of\nreinforcement learning from human feedback (RLHF) due to its simplicity and\nefficiency. However, existing DPO-based approaches typically treat all\npreference pairs uniformly, ignoring critical variations in their inherent\nquality and learning utility, leading to suboptimal data utilization and\nperformance. To address this challenge, we propose Omni-DPO, a dual-perspective\noptimization framework that jointly accounts for (1) the inherent quality of\neach preference pair and (2) the model's evolving performance on those pairs.\nBy adaptively weighting samples according to both data quality and the model's\nlearning dynamics during training, Omni-DPO enables more effective training\ndata utilization and achieves better performance. Experimental results on\nvarious models and benchmarks demonstrate the superiority and generalization\ncapabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it\nfinetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant\nmargin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning\ntasks, Omni-DPO consistently outperforms the baseline methods across all\nbenchmarks, providing strong empirical evidence for the effectiveness and\nrobustness of our approach. Code and models will be available at\nhttps://github.com/pspdada/Omni-DPO.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10054v1", "AI": {"title_translation": "Omni-DPO：一种用于LLM动态偏好学习的双视角范式", "tldr": "Omni-DPO通过结合偏好对质量和模型学习动态，改进了DPO，实现了更有效的数据利用和性能提升，在文本理解和数学推理任务上表现优异。", "motivation": "现有的DPO方法统一处理所有偏好对，忽略其内在质量和学习效用差异，导致数据利用不足和性能次优。", "method": "提出Omni-DPO，一个双视角优化框架，它联合考虑了(1)每个偏好对的内在质量和(2)模型在这些对上的演变性能。通过在训练过程中根据数据质量和模型的学习动态自适应地加权样本。", "result": "Omni-DPO在各种模型和基准测试上表现出优越性和泛化能力。在文本理解任务上，使用Omni-DPO微调的Gemma-2-9b-it在Arena-Hard基准测试上比Claude 3 Opus高出6.7分。在数学推理任务上，Omni-DPO始终优于基线方法。", "conclusion": "Omni-DPO通过其双视角优化范式，有效解决了现有DPO方法的局限性，显著提升了LLM的偏好学习效率和性能，并在多个任务中展现出卓越的泛化能力和鲁棒性。", "translation": "直接偏好优化（DPO）因其简洁和高效而成为人类反馈强化学习（RLHF）的基石。然而，现有的基于DPO的方法通常统一处理所有偏好对，忽略了其内在质量和学习效用的关键差异，导致数据利用不足和性能次优。为了解决这一挑战，我们提出了Omni-DPO，一个双视角优化框架，它联合考虑了（1）每个偏好对的内在质量和（2）模型在这些对上的演变性能。通过在训练过程中根据数据质量和模型的学习动态自适应地加权样本，Omni-DPO能够更有效地利用训练数据并实现更好的性能。在各种模型和基准测试上的实验结果证明了Omni-DPO的优越性和泛化能力。在文本理解任务上，使用Omni-DPO微调的Gemma-2-9b-it在Arena-Hard基准测试上以6.7分的显著优势击败了领先的LLM Claude 3 Opus。在数学推理任务上，Omni-DPO在所有基准测试中始终优于基线方法，为我们方法的有效性和鲁棒性提供了强有力的经验证据。代码和模型将发布在https://github.com/pspdada/Omni-DPO。", "summary": "本文提出了Omni-DPO，一个用于大型语言模型动态偏好学习的双视角优化框架。针对现有DPO方法忽略偏好对质量和学习效用导致数据利用不足的问题，Omni-DPO通过同时考虑偏好对的内在质量和模型学习动态来动态加权样本，从而实现更有效的数据利用和性能提升。实验证明Omni-DPO在文本理解和数学推理任务上均显著优于现有方法，展现出卓越的性能和泛化能力。", "keywords": "Direct Preference Optimization, LLMs, Reinforcement Learning from Human Feedback, Data Utilization, Dynamic Preference Learning", "comments": "Omni-DPO的创新点在于其双视角优化范式，通过自适应加权解决了DPO中数据质量和模型学习动态被忽略的问题。这对于提高LLM的偏好学习效率和性能至关重要，特别是其在挑战性基准测试上的显著提升，表明了其在实际应用中的巨大潜力。"}}
{"id": "2506.10413", "title": "Federated Learning within Global Energy Budget over Heterogeneous Edge Accelerators", "authors": ["Roopkatha Banerjee", "Tejus Chandrashekar", "Ananth Eswar", "Yogesh Simmhan"], "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. However, optimizing both\nenergy efficiency and model accuracy remains a challenge, given device and data\nheterogeneity. Further, sustainable AI through a global energy budget for FL\nhas not been explored. We propose a novel optimization problem for client\nselection in FL that maximizes the model accuracy within an overall energy\nlimit and reduces training time. We solve this with a unique bi-level ILP\nformulation that leverages approximate Shapley values and energy-time\nprediction models to efficiently solve this. Our FedJoule framework achieves\nsuperior training accuracies compared to SOTA and simple baselines for diverse\nenergy budgets, non-IID distributions, and realistic experiment configurations,\nperforming 15% and 48% better on accuracy and time, respectively. The results\nhighlight the effectiveness of our method in achieving a viable trade-off\nbetween energy usage and performance in FL environments.", "comment": "Preprint of paper to appear in the proceedings of the 31st\n  International European Conference on Parallel and Distributed Computing\n  (EuroPar)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10413v1", "AI": {"title_translation": "异构边缘加速器全局能耗预算下的联邦学习", "tldr": "该研究提出一种在全局能耗预算下优化联邦学习客户端选择的方法，以提高模型精度并减少训练时间，表现优于现有技术。", "motivation": "联邦学习在分布式客户端协作训练模型时面临能效和模型精度优化难题，尤其是在设备和数据异构性下。此外，在全局能耗预算下实现可持续AI的联邦学习尚未被探索。", "method": "提出一个新的客户端选择优化问题，旨在全局能耗限制下最大化模型精度并缩短训练时间。通过独特的双层整数线性规划（ILP）公式解决，该公式利用近似Shapley值和能耗-时间预测模型。", "result": "FedJoule框架在不同能耗预算、非独立同分布数据和真实实验配置下，比现有技术和简单基线取得了更高的训练精度，精度和时间分别提升了15%和48%。", "conclusion": "该方法在联邦学习环境中，能够有效平衡能耗和性能。", "translation": "联邦学习（FL）实现了分布式客户端之间的协作模型训练，同时保护了数据隐私。然而，考虑到设备和数据的异构性，同时优化能效和模型精度仍然是一个挑战。此外，尚未探索通过全局能耗预算实现联邦学习的可持续AI。我们提出了一种新颖的联邦学习客户端选择优化问题，该问题在总体能耗限制内最大化模型精度并减少训练时间。我们通过独特的双层整数线性规划（ILP）公式解决了这个问题，该公式利用近似Shapley值和能耗-时间预测模型来高效解决。我们的FedJoule框架在各种能耗预算、非独立同分布（non-IID）分布和真实实验配置下，与现有技术（SOTA）和简单基线相比，实现了卓越的训练精度，在精度和时间上分别提高了15%和48%。结果突出表明了我们方法在联邦学习环境中实现能耗与性能之间可行权衡的有效性。", "summary": "本文提出FedJoule框架，旨在解决联邦学习中在异构边缘设备上同时优化能耗和模型精度的问题。通过引入一个在全局能耗预算下最大化模型精度并缩短训练时间的新型客户端选择优化问题，并利用双层整数线性规划结合近似Shapley值和能耗-时间预测模型进行求解。实验结果表明，FedJoule在精度和训练时间上均优于现有技术，有效平衡了能耗与性能。", "keywords": "联邦学习, 能耗预算, 客户端选择, 异构边缘, 模型精度", "comments": "本文的创新点在于首次将全局能耗预算引入联邦学习的客户端选择问题中，并通过新颖的双层ILP公式有效解决了这一复杂优化问题。其提出的FedJoule框架在实际应用中对于部署可持续和高效的联邦学习系统具有重要意义。"}}
{"id": "2506.10192", "title": "Towards Responsible AI: Advances in Safety, Fairness, and Accountability of Autonomous Systems", "authors": ["Filip Cano"], "summary": "Ensuring responsible use of artificial intelligence (AI) has become\nimperative as autonomous systems increasingly influence critical societal\ndomains. However, the concept of trustworthy AI remains broad and\nmulti-faceted. This thesis advances knowledge in the safety, fairness,\ntransparency, and accountability of AI systems. In safety, we extend classical\ndeterministic shielding techniques to become resilient against delayed\nobservations, enabling practical deployment in real-world conditions. We also\nimplement both deterministic and probabilistic safety shields into simulated\nautonomous vehicles to prevent collisions with road users, validating the use\nof these techniques in realistic driving simulators. We introduce fairness\nshields, a novel post-processing approach to enforce group fairness in\nsequential decision-making settings over finite and periodic time horizons. By\noptimizing intervention costs while strictly ensuring fairness constraints,\nthis method efficiently balances fairness with minimal interference. For\ntransparency and accountability, we propose a formal framework for assessing\nintentional behaviour in probabilistic decision-making agents, introducing\nquantitative metrics of agency and intention quotient. We use these metrics to\npropose a retrospective analysis of intention, useful for determining\nresponsibility when autonomous systems cause unintended harm. Finally, we unify\nthese contributions through the ``reactive decision-making'' framework,\nproviding a general formalization that consolidates previous approaches.\nCollectively, the advancements presented contribute practically to the\nrealization of safer, fairer, and more accountable AI systems, laying the\nfoundations for future research in trustworthy AI.", "comment": "202 pages, 38 figures, PhD Thesis", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10192v1", "AI": {"title_translation": "迈向负责任的人工智能：自主系统在安全性、公平性和问责制方面的进展", "tldr": "本文提出了在安全性、公平性和问责制方面推进自主AI系统的方法，旨在实现更值得信赖的AI。", "motivation": "随着自主系统日益影响关键社会领域，确保人工智能的负责任使用变得至关重要，但可信赖AI的概念仍然宽泛且多方面。", "method": "在安全性方面，扩展了经典的确定性防护技术以应对延迟观测，并在模拟自动驾驶车辆中实现了确定性和概率性安全防护，以防止碰撞。在公平性方面，引入了公平性防护（一种新颖的后处理方法），用于在有限和周期性时间范围内的顺序决策设置中强制执行群体公平性，通过优化干预成本来平衡公平性。对于透明度和问责制，提出了一个用于评估概率决策代理中意图行为的正式框架，引入了代理和意图商数的定量指标，并使用这些指标进行意图的回溯分析。这些贡献通过“反应式决策”框架统一。", "result": "在现实驾驶模拟器中验证了安全防护技术。公平性防护方法能有效地在最小干扰下平衡公平性。提出了量化代理和意图的指标，可用于确定自主系统造成意外损害时的责任。", "conclusion": "本文提出的进展为实现更安全、更公平、更负责任的AI系统做出了实际贡献，为可信赖AI的未来研究奠定了基础。", "translation": "随着自主系统日益影响关键社会领域，确保人工智能（AI）的负责任使用已变得势在必行。然而，可信赖AI的概念仍然宽泛且多方面。本论文在AI系统的安全性、公平性、透明度和问责制方面取得了进展。在安全性方面，我们扩展了经典的确定性防护技术，使其能够抵御延迟观测，从而实现在真实世界条件下的实际部署。我们还在模拟自动驾驶车辆中实现了确定性和概率性安全防护，以防止与道路使用者发生碰撞，从而验证了这些技术在现实驾驶模拟器中的使用。我们引入了公平性防护，这是一种新颖的后处理方法，用于在有限和周期性时间范围内的顺序决策设置中强制执行群体公平性。通过优化干预成本同时严格确保公平性约束，该方法高效地平衡了公平性与最小干扰。对于透明度和问责制，我们提出了一个用于评估概率决策代理中意图行为的正式框架，引入了代理和意图商数的定量指标。我们使用这些指标来提出意图的回溯分析，这对于确定自主系统造成意外损害时的责任很有用。最后，我们通过“反应式决策”框架统一了这些贡献，提供了一个整合先前方法的通用形式化。总的来说，本文提出的进展为实现更安全、更公平、更负责任的AI系统做出了实际贡献，为可信赖AI的未来研究奠定了基础。", "summary": "本论文旨在解决人工智能（AI）在自主系统中的负责任使用问题，重点关注安全性、公平性、透明度和问责制。研究人员扩展了安全防护技术以应对延迟观测，并在模拟自动驾驶车辆中验证了碰撞预防能力。他们还引入了公平性防护，一种在顺序决策中强制执行群体公平性的后处理方法，能在优化成本的同时最小化干扰。此外，论文提出了一个评估概率决策代理意图的正式框架，引入了代理和意图的量化指标，并用于责任判定。所有这些贡献通过“反应式决策”框架进行统一，共同为构建更安全、公平、负责任的AI系统奠定了基础。", "keywords": "负责任AI, 自主系统, 安全性, 公平性, 问责制", "comments": "本文通过在安全性、公平性和问责制三个关键维度上提供具体的技术方案，为“负责任AI”这一宽泛概念提供了扎实的实践基础。其创新点在于将经典的安全防护技术扩展到实际应用场景，引入新颖的公平性防护机制，并为AI行为的意图分析提供了量化框架，具有重要的理论和实践意义。"}}
{"id": "2506.10789", "title": "FASCIST-O-METER: Classifier for Neo-fascist Discourse Online", "authors": ["Rudy Alexandro Garrido Veliz", "Martin Semmann", "Chris Biemann", "Seid Muhie Yimam"], "summary": "Neo-fascism is a political and societal ideology that has been having\nremarkable growth in the last decade in the United States of America (USA), as\nwell as in other Western societies. It poses a grave danger to democracy and\nthe minorities it targets, and it requires active actions against it to avoid\nescalation. This work presents the first-of-its-kind neo-fascist coding scheme\nfor digital discourse in the USA societal context, overseen by political\nscience researchers. Our work bridges the gap between Natural Language\nProcessing (NLP) and political science against this phenomena. Furthermore, to\ntest the coding scheme, we collect a tremendous amount of activity on the\ninternet from notable neo-fascist groups (the forums of Iron March and\nStormfront.org), and the guidelines are applied to a subset of the collected\nposts. Through crowdsourcing, we annotate a total of a thousand posts that are\nlabeled as neo-fascist or non-neo-fascist. With this labeled data set, we\nfine-tune and test both Small Language Models (SLMs) and Large Language Models\n(LLMs), obtaining the very first classification models for neo-fascist\ndiscourse. We find that the prevalence of neo-fascist rhetoric in this kind of\nforum is ever-present, making them a good target for future research. The\nsocietal context is a key consideration for neo-fascist speech when conducting\nNLP research. Finally, the work against this kind of political movement must be\npressed upon and continued for the well-being of a democratic society.\nDisclaimer: This study focuses on detecting neo-fascist content in text,\nsimilar to other hate speech analyses, without labeling individuals or\norganizations.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10789v1", "AI": {"title_translation": "法西斯测量仪：在线新法西斯主义言论分类器", "tldr": "该研究提出了首个针对在线新法西斯主义言论的分类模型，通过构建编码方案、收集数据、众包标注并微调小型和大型语言模型来实现，旨在对抗这种对民主有害的言论。", "motivation": "新法西斯主义在过去十年中在美国及其他西方社会显著增长，对民主和少数群体构成严重威胁，需要采取积极行动来避免其升级。本研究旨在填补自然语言处理（NLP）和政治学在对抗这种现象方面的空白。", "method": "本研究提出了首个针对美国社会背景下数字言论的新法西斯主义编码方案，并由政治学研究人员监督。为了测试该方案，作者从新法西斯主义团体（如Iron March和Stormfront.org论坛）收集了大量在线活动数据，并对其中一部分帖子应用了指导方针。通过众包，总共标注了一千个帖子，分为新法西斯主义或非新法西斯主义。利用这些标注数据，对小型语言模型（SLMs）和大型语言模型（LLMs）进行了微调和测试。", "result": "本研究获得了首批用于新法西斯主义言论的分类模型。研究发现，在这类论坛中，新法西斯主义言论普遍存在，这使其成为未来研究的良好目标。", "conclusion": "对抗这种政治运动的工作必须持续进行，以维护民主社会的福祉。在进行NLP研究时，社会背景是新法西斯主义言论的一个关键考虑因素。", "translation": "新法西斯主义是一种政治和社会意识形态，在过去十年中在美国以及其他西方社会取得了显著增长。它对民主及其所针对的少数群体构成了严重威胁，需要积极采取行动来避免其升级。这项工作提出了美国社会背景下数字言论的首个新法西斯主义编码方案，并由政治学研究人员监督。我们的工作弥合了自然语言处理（NLP）和政治学在对抗这种现象方面的差距。此外，为了测试该编码方案，我们从著名的新法西斯主义团体（Iron March和Stormfront.org论坛）收集了大量的互联网活动数据，并将指导方针应用于收集到的部分帖子。通过众包，我们总共标注了一千个帖子，将其标记为新法西斯主义或非新法西斯主义。利用这个标注数据集，我们对小型语言模型（SLMs）和大型语言模型（LLMs）进行了微调和测试，获得了首批新法西斯主义言论分类模型。我们发现，在这类论坛中，新法西斯主义言论普遍存在，这使其成为未来研究的良好目标。在进行NLP研究时，社会背景是新法西斯主义言论的一个关键考虑因素。最后，为了民主社会的福祉，必须继续并加强对抗这种政治运动的工作。免责声明：本研究侧重于检测文本中的新法西斯主义内容，类似于其他仇恨言论分析，不针对个人或组织进行标记。", "summary": "本研究旨在应对新法西斯主义在线言论的增长及其对民主的威胁。作者提出了首个针对美国数字语境的新法西斯主义编码方案，并与政治学领域结合。通过从新法西斯主义论坛收集数据、众包标注一千个帖子，并利用这些标注数据对小型和大型语言模型进行微调，成功开发出首批新法西斯主义言论分类模型。研究强调了在线论坛中新法西斯主义修辞的普遍性，并指出社会背景在NLP研究中的重要性，呼吁持续努力对抗此类政治运动。", "keywords": "新法西斯主义, 自然语言处理, 文本分类, 仇恨言论, 编码方案", "comments": "该论文的创新之处在于首次提出了针对数字言论的新法西斯主义编码方案，并成功构建了首批新法西斯主义言论分类模型，弥合了NLP和政治学之间的空白。其重要性在于为识别和对抗在线有害言论提供了新的工具和方法，对维护民主社会具有实际意义。数据收集和众包标注的过程也为后续研究提供了宝贵的资源。"}}
{"id": "2506.10233", "title": "Conditional diffusion models for guided anomaly detection in brain images using fluid-driven anomaly randomization", "authors": ["Ana Lawry Aguila", "Peirong Liu", "Oula Puonti", "Juan Eugenio Iglesias"], "summary": "Supervised machine learning has enabled accurate pathology detection in brain\nMRI, but requires training data from diseased subjects that may not be readily\navailable in some scenarios, for example, in the case of rare diseases.\nReconstruction-based unsupervised anomaly detection, in particular using\ndiffusion models, has gained popularity in the medical field as it allows for\ntraining on healthy images alone, eliminating the need for large\ndisease-specific cohorts. These methods assume that a model trained on normal\ndata cannot accurately represent or reconstruct anomalies. However, this\nassumption often fails with models failing to reconstruct healthy tissue or\naccurately reconstruct abnormal regions i.e., failing to remove anomalies. In\nthis work, we introduce a novel conditional diffusion model framework for\nanomaly detection and healthy image reconstruction in brain MRI. Our weakly\nsupervised approach integrates synthetically generated pseudo-pathology images\ninto the modeling process to better guide the reconstruction of healthy images.\nTo generate these pseudo-pathologies, we apply fluid-driven anomaly\nrandomization to augment real pathology segmentation maps from an auxiliary\ndataset, ensuring that the synthetic anomalies are both realistic and\nanatomically coherent. We evaluate our model's ability to detect pathology,\nusing both synthetic anomaly datasets and real pathology from the ATLAS\ndataset. In our extensive experiments, our model: (i) consistently outperforms\nvariational autoencoders, and conditional and unconditional latent diffusion;\nand (ii) surpasses on most datasets, the performance of supervised inpainting\nmethods with access to paired diseased/healthy images.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10233v1", "AI": {"title_translation": "基于流体驱动异常随机化的条件扩散模型在脑图像引导异常检测中的应用", "tldr": "本文提出了一种新的条件扩散模型，通过流体驱动异常随机化生成合成病理，用于脑部MRI中的引导异常检测，并在检测异常和重建健康图像方面优于现有方法。", "motivation": "监督机器学习方法在脑部MRI病理检测中需要大量患病数据，这对于罕见疾病等情况难以获取。无监督重建方法（特别是扩散模型）虽然仅需健康图像训练，但常未能准确重建健康组织或去除异常。本文旨在解决这些限制。", "method": "引入了一个新颖的弱监督条件扩散模型框架，用于脑部MRI的异常检测和健康图像重建。该方法通过应用流体驱动异常随机化来增强辅助数据集中的真实病理分割图，生成合成的伪病理图像，并将其整合到建模过程中，以更好地引导健康图像的重建。", "result": "在广泛的实验中，该模型始终优于变分自编码器、条件和无条件潜在扩散模型；并且在大多数数据集上，性能超越了能够访问配对患病/健康图像的监督修复方法。", "conclusion": "本文提出了一种有效的条件扩散模型，通过利用合成病理来改善脑部MRI异常检测的性能，解决了以往方法的局限性。", "translation": "监督机器学习已实现在脑部MRI中准确检测病理，但需要来自患病受试者的训练数据，这在某些情况下可能不易获得，例如在罕见疾病的情况下。基于重建的无监督异常检测，特别是使用扩散模型的方法，已在医学领域获得普及，因为它允许仅在健康图像上进行训练，从而无需大型疾病特异性队列。这些方法假设在正常数据上训练的模型无法准确表示或重建异常。然而，这个假设常常失败，模型无法重建健康组织或准确重建异常区域，即无法去除异常。在这项工作中，我们引入了一种新颖的条件扩散模型框架，用于脑部MRI中的异常检测和健康图像重建。我们的弱监督方法将合成生成的伪病理图像整合到建模过程中，以更好地引导健康图像的重建。为了生成这些伪病理，我们应用流体驱动异常随机化来增强来自辅助数据集的真实病理分割图，确保合成异常既真实又解剖学上连贯。我们评估了模型检测病理的能力，使用了合成异常数据集和来自ATLAS数据集的真实病理。在我们广泛的实验中，我们的模型：(i) 始终优于变分自编码器、条件和无条件潜在扩散模型；(ii) 在大多数数据集上，超越了能够访问配对患病/健康图像的监督修复方法的性能。", "summary": "本文提出了一种新颖的弱监督条件扩散模型，用于脑部MRI的异常检测和健康图像重建。该模型通过利用流体驱动异常随机化生成的合成伪病理图像来解决监督方法数据稀缺和无监督方法重建失败的局限性。这些合成异常确保了真实性和解剖学连贯性。广泛的实验表明，所提出的模型在性能上持续优于多种现有模型，包括变分自编码器、条件/无条件潜在扩散模型，甚至在大多数数据集上超越了监督修复方法。", "keywords": "条件扩散模型, 异常检测, 脑部MRI, 流体驱动随机化, 弱监督学习", "comments": "该论文的创新之处在于其弱监督方法，利用流体驱动异常随机化创建逼真的合成病理，有效地指导条件扩散模型进行更好的异常检测和健康组织重建。这解决了医学图像分析中的一个关键挑战，尤其是在真实患病数据稀缺的罕见疾病领域。"}}
{"id": "2506.10653", "title": "Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes", "authors": ["Rogier C. van Dalen", "Shucong Zhang", "Titouan Parcollet", "Sourav Bhattacharya"], "summary": "Speech recognisers usually perform optimally only in a specific environment\nand need to be adapted to work well in another. For adaptation to a new\nspeaker, there is often too little data for fine-tuning to be robust, and that\ndata is usually unlabelled. This paper proposes a combination of approaches to\nmake adaptation to a single minute of data robust. First, instead of estimating\nthe adaptation parameters with cross-entropy on a single error-prone hypothesis\nor \"pseudo-label\", this paper proposes a novel loss function, the conditional\nentropy over complete hypotheses. Using multiple hypotheses makes adaptation\nmore robust to errors in the initial recognition. Second, a \"speaker code\"\ncharacterises a speaker in a vector short enough that it requires little data\nto estimate. On a far-field noise-augmented version of Common Voice, the\nproposed scheme yields a 20% relative improvement in word error rate on one\nminute of adaptation data, increasing on 10 minutes to 29%.", "comment": null, "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10653v1", "AI": {"title_translation": "鲁棒的无监督语音识别器自适应，使用熵最小化和说话人编码", "tldr": "本文提出结合条件熵损失函数和说话人编码，以实现对少量（一分钟）无标签数据的鲁棒无监督语音识别器自适应，显著提高了词错误率。", "motivation": "语音识别器通常只在特定环境下表现最佳，需要自适应到新环境。然而，对于新说话人的自适应，通常缺乏足够的标注数据进行鲁棒的微调，且数据通常是无标签的。", "method": "本文提出两种结合方法：1. 提出一种新的损失函数：基于完整假设的条件熵，而不是使用单个易出错的伪标签上的交叉熵。使用多个假设使自适应对初始识别错误更具鲁棒性。2. 引入“说话人编码”，它用一个足够短的向量来表征说话人，只需少量数据即可估计。", "result": "在Common Voice的远场噪声增强版本上，所提出的方案在1分钟自适应数据上使词错误率相对提高了20%，在10分钟数据上提高到29%。", "conclusion": "所提出的结合条件熵最小化和说话人编码的方法，显著提高了在有限无标签数据下无监督语音识别器自适应的鲁棒性和性能。", "translation": "语音识别器通常只在特定环境下表现最佳，需要自适应才能在其他环境下良好工作。对于新说话人的自适应，通常数据量太少，无法进行鲁棒的微调，而且这些数据通常是未标记的。本文提出一种组合方法，使对一分钟数据的自适应变得鲁棒。首先，本文没有使用单个易出错的假设或“伪标签”上的交叉熵来估计自适应参数，而是提出了一种新颖的损失函数——基于完整假设的条件熵。使用多个假设使自适应对初始识别中的错误更具鲁棒性。其次，“说话人编码”用一个足够短的向量来表征说话人，只需少量数据即可估计。在Common Voice的远场噪声增强版本上，所提出的方案在1分钟自适应数据上使词错误率相对提高了20%，在10分钟数据上提高到29%。", "summary": "本文针对语音识别器在有限无标签数据下对新说话人自适应的挑战，提出一种结合策略。该策略引入了一种基于完整假设的条件熵新颖损失函数，以增强对初始识别错误的鲁棒性，并利用紧凑的“说话人编码”来高效表征说话人。实验结果表明，在少量（1分钟和10分钟）自适应数据上，该方法显著降低了词错误率。", "keywords": "无监督自适应, 语音识别, 熵最小化, 说话人编码, 鲁棒性", "comments": "这篇论文的创新点在于结合了两种方法来解决无监督语音识别器自适应中数据量少且无标签的挑战。特别是提出基于完整假设的条件熵损失函数，通过利用多个假设而非单一伪标签，有效提升了自适应的鲁棒性，这对于实际应用中初始识别错误较多的情况非常有价值。说话人编码的使用也有效地减少了所需数据量。其在少量数据上取得的显著性能提升，证明了该方法的实用性和重要性。"}}
{"id": "2506.10758", "title": "Circulant TSP: Vertices of the Edge-Length Polytope and Superpolynomial Lower Bounds", "authors": ["Samuel C. Gutekunst"], "summary": "We study the edge-length polytope, motivated both by algorithmic research on\nthe Circulant Traveling Salesman Problem (Circulant TSP) and number-theoretic\nresearch related to the Buratti-Horak-Rosa conjecture. Circulant TSP is a\nspecial case of TSP whose overall complexity is a significant still-open\nquestion, and where on an input with vertices $\\{1, 2, ..., n\\}$, the cost of\nan edge $\\{i, j\\}$ depends only on its length $\\min\\{|i-j|, n-|i-j|\\}$. The\nedge-length polytope provides one path to solving circulant TSP instances, and\nwe show that it is intimately connected to the factorization of $n$: the number\nof vertices scales with $n$ whenever $n$ is prime and with $n^{3/2}$ whenever\n$n$ is a prime-squared, but there are a superpolynomial number of vertices\nwhenever $n$ is a power of 2. In contrast, the more-standard Symmetric TSP\nPolytope has roughly $n!$ vertices. Hence, for Circulant TSP, a brute-force\nalgorithm checking every vertex is actually efficient in some cases, based on\nthe factorization of $n$. As an intermediate step, we give superpolynomial\nlower-bounds on two combinatorial sequences related to the Buratti-Horak-Rosa\nconjecture, which asks what combinations of edge lengths can comprise a\nHamiltonian path.", "comment": null, "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.10758v1", "AI": {"title_translation": "循环旅行商问题：边长多面体的顶点和超多项式下界", "tldr": "本文研究了循环旅行商问题中的边长多面体，发现其顶点数量与输入规模n的因式分解密切相关，某些情况下暴力算法是高效的，并给出了与Buratti-Horak-Rosa猜想相关的组合序列的超多项式下界。", "motivation": "本文的研究动机源于循环旅行商问题（Circulant TSP）的算法研究以及与Buratti-Horak-Rosa猜想相关的数论研究。循环旅行商问题是一个特殊的TSP情况，其整体复杂性仍是一个重要的未解决问题。边长多面体为解决循环旅行商问题实例提供了一条途径。", "method": "本文通过研究边长多面体，揭示了其与n的因式分解之间的紧密联系。作为中间步骤，本文给出了与Buratti-Horak-Rosa猜想相关的两个组合序列的超多项式下界。", "result": "研究发现，当n是素数时，边长多面体的顶点数量与n成比例；当n是素数的平方时，顶点数量与n的3/2次方成比例；但当n是2的幂时，顶点数量是超多项式的。这与更标准的对称旅行商多面体（约有n!个顶点）形成对比。因此，对于循环旅行商问题，基于n的因式分解，在某些情况下，检查每个顶点的暴力算法实际上是高效的。", "conclusion": "循环旅行商问题的边长多面体的顶点数量显著依赖于n的因式分解，这意味着在某些特定情况下，暴力算法可以有效解决该问题。此外，本研究为Buratti-Horak-Rosa猜想提供了新的组合下界。", "translation": "我们研究了边长多面体，其动机来源于循环旅行商问题（Circulant TSP）的算法研究以及与Buratti-Horak-Rosa猜想相关的数论研究。循环旅行商问题是旅行商问题（TSP）的一个特例，其整体复杂性仍然是一个重要的未解决问题。在这种情况下，对于具有顶点{1, 2, ..., n}的输入，边{i, j}的成本仅取决于其长度min{|i-j|, n-|i-j|}。边长多面体为解决循环旅行商问题实例提供了一条途径，我们表明它与n的因式分解密切相关：当n是素数时，顶点数量与n成比例；当n是素数的平方时，顶点数量与n的3/2次方成比例；但当n是2的幂时，存在超多项数量的顶点。相比之下，更标准的对称旅行商多面体大约有n!个顶点。因此，对于循环旅行商问题，基于n的因式分解，在某些情况下，检查每个顶点的暴力算法实际上是高效的。作为中间步骤，我们给出了与Buratti-Horak-Rosa猜想相关的两个组合序列的超多项式下界，该猜想询问哪些边长组合可以构成哈密顿路径。", "summary": "本文研究了循环旅行商问题（Circulant TSP）中的边长多面体，发现其顶点数量与输入规模n的因式分解方式紧密相关。具体而言，当n为素数时顶点数量与n成线性关系，当n为素数平方时与n的1.5次方成比例，而当n为2的幂时则呈现超多项式增长。这一发现表明，与标准TSP不同，基于n的因式分解，对于Circulant TSP而言，在某些特定情况下，暴力遍历顶点的方法是高效可行的。此外，研究还为与Buratti-Horak-Rosa猜想相关的两个组合序列提供了超多项式下界。", "keywords": "循环旅行商问题, 边长多面体, 超多项式下界, Buratti-Horak-Rosa猜想, 因式分解", "comments": "本文的创新之处在于揭示了循环旅行商问题中边长多面体顶点数量与n的因式分解之间的深刻联系，这为理解和解决这类特殊TSP问题提供了新的视角。其重要性在于，它挑战了传统上认为TSP类问题暴力求解不可行的观念，表明在特定约束下（如循环结构），暴力算法可能变得高效。同时，与数论中Buratti-Horak-Rosa猜想的联系也增加了其理论价值。"}}
{"id": "2506.10086", "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information", "authors": ["Christodoulos Constantinides", "Shuxin Lin", "Nianjun Zhou", "Dhaval Patel"], "summary": "This paper presents a novel multi-agent system called Chat-of-Thought,\ndesigned to facilitate the generation of Failure Modes and Effects Analysis\n(FMEA) documents for industrial assets. Chat-of-Thought employs multiple\ncollaborative Large Language Model (LLM)-based agents with specific roles,\nleveraging advanced AI techniques and dynamic task routing to optimize the\ngeneration and validation of FMEA tables. A key innovation in this system is\nthe introduction of a Chat of Thought, where dynamic, multi-persona-driven\ndiscussions enable iterative refinement of content. This research explores the\napplication domain of industrial equipment monitoring, highlights key\nchallenges, and demonstrates the potential of Chat-of-Thought in addressing\nthese challenges through interactive, template-driven workflows and\ncontext-aware agent collaboration.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10086v1", "AI": {"title_translation": "思维对话：用于生成领域特定信息的协作式多智能体系统", "tldr": "Chat-of-Thought是一个协作式多智能体LLM系统，用于生成工业资产的FMEA文档。", "motivation": "本研究旨在解决工业设备监控领域中生成失效模式与影响分析（FMEA）文档的挑战。", "method": "本文提出了一个名为Chat-of-Thought的多智能体系统，该系统利用多个基于大型语言模型（LLM）的协作代理，每个代理都有特定角色。它采用先进的AI技术和动态任务路由来优化FMEA表格的生成和验证，并通过引入“思维对话”机制，实现动态的、多角色驱动的讨论，从而迭代地完善内容。", "result": "研究展示了Chat-of-Thought在通过交互式、模板驱动的工作流程和上下文感知的代理协作来解决FMEA文档生成挑战方面的潜力。", "conclusion": "Chat-of-Thought系统通过其协作式多智能体架构和动态的“思维对话”机制，在生成领域特定信息（如FMEA文档）方面展现出巨大潜力。", "translation": "本文提出了一种新颖的多智能体系统，名为Chat-of-Thought，旨在促进工业资产的失效模式与影响分析（FMEA）文档的生成。Chat-of-Thought采用多个基于大型语言模型（LLM）的协作式代理，每个代理都有特定角色，利用先进的AI技术和动态任务路由来优化FMEA表格的生成和验证。该系统的一个关键创新是引入了“思维对话”机制，其中动态的、多角色驱动的讨论能够实现内容的迭代细化。本研究探讨了工业设备监控的应用领域，强调了关键挑战，并展示了Chat-of-Thought通过交互式、模板驱动的工作流程和上下文感知的代理协作来解决这些挑战的潜力。", "summary": "Chat-of-Thought是一种新颖的协作式多智能体系统，它利用多个基于LLM的代理来自动化和优化工业资产的FMEA文档生成。该系统通过动态任务路由和独特的“思维对话”机制实现内容迭代细化，展示了在解决工业设备监控领域挑战方面的有效性。", "keywords": "多智能体系统, 大型语言模型, 失效模式与影响分析, 协作AI, 领域特定信息", "comments": "Chat-of-Thought的创新之处在于其协作式多智能体架构结合了LLM和“思维对话”机制，通过多角色讨论实现内容的迭代优化，这对于生成领域特定且高质量的文档（如FMEA）具有重要意义。"}}
{"id": "2506.10038", "title": "Ambient Diffusion Omni: Training Good Models with Bad Data", "authors": ["Giannis Daras", "Adrian Rodriguez-Munoz", "Adam Klivans", "Antonio Torralba", "Constantinos Daskalakis"], "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.", "comment": "Preprint, work in progress", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10038v1", "AI": {"title_translation": "环境扩散全能：用劣质数据训练优质模型", "tldr": "本文展示了如何利用低质量、合成和分布外图像来提升扩散模型的质量，通过Ambient Diffusion Omni框架，利用所有可用图像中的信号，并实现了最先进的性能。", "motivation": "现有的扩散模型通常依赖于高度过滤的、高质量的策展数据集。本文的动机是发现被丢弃的低质量图像中蕴含着巨大的价值，并利用这些数据来改进扩散模型。", "method": "本文提出了Ambient Diffusion Omni，一个简单且有原则的框架，用于训练扩散模型，使其能够从训练期间所有可用图像中提取信号。该框架利用了自然图像的两个特性：谱幂律衰减和局部性。此外，该方法通过分析有偏数据与有限无偏数据在不同扩散时间下的学习权衡，提供了严谨的理论依据。", "result": "该框架成功地使用高斯模糊、JPEG压缩和运动模糊等合成损坏的图像训练了扩散模型。它在ImageNet FID上取得了最先进的性能，并在文本到图像生成建模中显著提高了图像质量和多样性。", "conclusion": "核心洞察是噪声能够抑制期望的高质量分布与实际观察到的混合分布之间的初始偏差，从而使得模型能够从混合数据中学习。", "translation": "我们展示了如何使用低质量、合成和分布外图像来提高扩散模型的质量。通常，扩散模型是在从网络和其他来源高度过滤的数据池中产生的策展数据集上训练的。我们表明，被经常丢弃的低质量图像中蕴藏着巨大的价值。我们提出了Ambient Diffusion Omni，一个简单、有原则的框架，用于训练扩散模型，该模型可以在训练期间从所有可用图像中提取信号。我们的框架利用了自然图像的两个特性——谱幂律衰减和局部性。我们首先通过成功地使用高斯模糊、JPEG压缩和运动模糊等合成损坏的图像训练扩散模型来验证我们的框架。然后，我们使用我们的框架实现了最先进的ImageNet FID，并且我们展示了文本到图像生成建模在图像质量和多样性方面的显著改进。核心洞察是噪声抑制了所需高质量分布与我们实际观察到的混合分布之间的初始偏差。我们通过分析在不同扩散时间下从有偏数据学习与从有限无偏数据学习之间的权衡，为我们的方法提供了严谨的理论依据。", "summary": "本文提出了Ambient Diffusion Omni框架，旨在利用低质量、合成和分布外图像来提升扩散模型的性能。该框架利用自然图像的谱幂律衰减和局部性特性，能够从所有可用图像中提取有效信号。实验证明，Ambient Diffusion Omni成功地使用受损图像进行训练，并在ImageNet FID上达到最先进水平，同时显著改善了文本到图像生成的质量和多样性。其核心思想在于噪声能平衡高质量与混合数据分布间的偏差，从而实现从“坏数据”中学习。", "keywords": "扩散模型, 低质量数据, Ambient Diffusion Omni, 图像生成, 数据增强", "comments": "这项研究的创新之处在于挑战了传统观念，即扩散模型必须依赖高度策展的高质量数据。通过Ambient Diffusion Omni，研究者证明了即使是低质量或分布外的数据也能有效提升模型性能，这对于数据获取成本高昂或数据质量参差不齐的实际应用具有重要意义。理论分析也为在非理想数据条件下进行模型训练提供了新的视角。"}}
{"id": "2506.10347", "title": "LightKG: Efficient Knowledge-Aware Recommendations with Simplified GNN Architecture", "authors": ["Yanhui Li", "Dongxia Wang", "Zhu Sun", "Haonan Zhang", "Huizhong Guo"], "summary": "Recently, Graph Neural Networks (GNNs) have become the dominant approach for\nKnowledge Graph-aware Recommender Systems (KGRSs) due to their proven\neffectiveness. Building upon GNN-based KGRSs, Self-Supervised Learning (SSL)\nhas been incorporated to address the sparity issue, leading to longer training\ntime. However, through extensive experiments, we reveal that: (1)compared to\nother KGRSs, the existing GNN-based KGRSs fail to keep their superior\nperformance under sparse interactions even with SSL. (2) More complex models\ntend to perform worse in sparse interaction scenarios and complex mechanisms,\nlike attention mechanism, can be detrimental as they often increase learning\ndifficulty. Inspired by these findings, we propose LightKG, a simple yet\npowerful GNN-based KGRS to address sparsity issues. LightKG includes a\nsimplified GNN layer that encodes directed relations as scalar pairs rather\nthan dense embeddings and employs a linear aggregation framework, greatly\nreducing the complexity of GNNs. Additionally, LightKG incorporates an\nefficient contrastive layer to implement SSL. It directly minimizes the node\nsimilarity in original graph, avoiding the time-consuming subgraph generation\nand comparison required in previous SSL methods. Experiments on four benchmark\ndatasets show that LightKG outperforms 12 competitive KGRSs in both sparse and\ndense scenarios while significantly reducing training time. Specifically, it\nsurpasses the best baselines by an average of 5.8\\% in recommendation accuracy\nand saves 84.3\\% of training time compared to KGRSs with SSL. Our code is\navailable at https://github.com/1371149/LightKG.", "comment": "Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10347v1", "AI": {"title_translation": "LightKG：基于简化GNN架构的高效知识感知推荐", "tldr": "LightKG是一个简单高效的GNN知识图谱推荐系统，通过简化GNN层和高效的对比学习，解决了稀疏交互下的性能问题，并显著减少了训练时间。", "motivation": "现有GNN-based KGRSs即使结合SSL也未能在稀疏交互下保持优越性能，且更复杂的模型在稀疏场景下表现更差，注意力机制等复杂机制反而有害。", "method": "提出LightKG，包含一个简化的GNN层（将有向关系编码为标量对而非密集嵌入，并采用线性聚合框架）和一个高效的对比层（直接最小化原始图中的节点相似度，避免耗时的子图生成和比较）。", "result": "在四个基准数据集上，LightKG在稀疏和密集场景下均优于12个竞争性KGRSs，推荐准确率平均提高5.8%，训练时间比带SSL的KGRSs节省84.3%。", "conclusion": "LightKG通过其简化的架构和高效的SSL实现，在保持甚至提升推荐性能的同时，显著降低了计算复杂度和训练时间，有效解决了稀疏交互问题。", "translation": "最近，图神经网络（GNNs）因其已被证实的有效性，已成为知识图谱感知推荐系统（KGRSs）的主导方法。在基于GNN的KGRSs基础上，自监督学习（SSL）被引入以解决稀疏性问题，但这导致了更长的训练时间。然而，通过大量的实验，我们发现：(1) 与其他KGRSs相比，现有基于GNN的KGRSs即使结合SSL也未能在稀疏交互下保持其优越性能。(2) 更复杂的模型在稀疏交互场景下往往表现更差，并且注意力机制等复杂机制可能有害，因为它们通常会增加学习难度。受这些发现的启发，我们提出了LightKG，一个简单而强大的基于GNN的KGRS，以解决稀疏性问题。LightKG包含一个简化的GNN层，它将有向关系编码为标量对而不是密集嵌入，并采用线性聚合框架，大大降低了GNN的复杂性。此外，LightKG还包含一个高效的对比层来实现SSL。它直接最小化原始图中的节点相似度，避免了以往SSL方法中所需耗时的子图生成和比较。在四个基准数据集上的实验表明，LightKG在稀疏和密集场景下都优于12个竞争性KGRSs，同时显著减少了训练时间。具体而言，它在推荐准确率方面平均超越最佳基线5.8%，与带SSL的KGRSs相比，训练时间节省了84.3%。我们的代码可在https://github.com/1371149/LightKG 获取。", "summary": "本文提出了LightKG，一个针对知识图谱感知推荐系统（KGRSs）的GNN模型，旨在解决现有GNN-based KGRSs在稀疏交互下的性能不佳和训练时间过长的问题。LightKG通过简化GNN层（使用标量对编码关系和线性聚合）以及引入高效的对比学习层（直接最小化节点相似度），显著降低了模型复杂性和训练成本。实验证明，LightKG在稀疏和密集场景下均表现优异，且大幅减少了训练时间。", "keywords": "知识图谱推荐系统, 图神经网络, 自监督学习, 稀疏交互, 模型简化", "comments": "LightKG的创新点在于其“简化”的设计理念，它挑战了“越复杂越好”的传统观念，证明了在特定场景下（如稀疏交互）通过简化模型结构和优化自监督学习机制，可以实现性能和效率的双重提升。其将有向关系编码为标量对和线性聚合的GNN层，以及直接最小化节点相似度的对比层，都是值得关注的创新点。该工作对于未来设计更轻量级、更高效的推荐系统具有重要指导意义。"}}
{"id": "2506.10263", "title": "Complex scaling for open waveguides", "authors": ["Charles L. Epstein", "Tristan Goodwill", "Jeremy Hoskins", "Solomon Quinn", "Manas Rachh"], "summary": "In this work we analyze the complex scaling method applied to the problem of\ntime-harmonic scalar wave propagation in junctions between `leaky,' or open\ndielectric waveguides. In [arXiv:2302.04353, arXiv:2310.05816,\narXiv:2401.04674, arXiv:2411.11204], it was shown that under suitable\nassumptions the problem can be reduced to a system of Fredholm second-kind\nintegral equations on an infinite interface, transverse to the waveguides.\nHere, we show that the kernels appearing in the integral equation admit a\nrapidly decaying analytic continuation on certain natural totally real\nsubmanifolds of $\\mathbb{C}^2.$ We then show that for suitable,\nphysically-meaningful, boundary data the resulting solutions to the integral\nequations themselves admit analytic continuation and satisfy related asymptotic\nestimates. By deforming the integral equation to a suitable contour, the decay\nin the kernels, density, and data enable straightforward discretization and\ntruncation, with an error that decays exponentially in the truncation length.\nWe illustrate our results with several representative numerical examples.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10263v1", "AI": {"title_translation": "开放波导的复尺度变换", "tldr": "本文分析了复尺度变换方法在开放介质波导交界处时谐标量波传播问题中的应用，展示了积分方程核和解的解析延拓及其渐近估计，并通过变形积分方程实现了高效的离散化和截断，误差呈指数衰减。", "motivation": "在之前的研究中，针对漏泄（开放）介质波导交界处的时谐标量波传播问题，已将其简化为无限界面上的第二类Fredholm积分方程系统。本文的动机是分析复尺度变换法应用于此问题，以解决其复杂性并实现有效的数值求解。", "method": "本研究分析了应用于开放介质波导交界处时谐标量波传播问题的复尺度变换方法。具体方法包括：1. 证明积分方程中的核在$\\mathbb{C}^2$的特定自然全实子流形上允许快速衰减的解析延拓。2. 证明对于合适的、具有物理意义的边界数据，积分方程的解本身也允许解析延拓并满足相关的渐近估计。3. 通过将积分方程变形到合适的等值线，利用核、密度和数据的衰减特性，实现直接的离散化和截断。", "result": "研究结果表明，通过变形积分方程到合适的等值线，利用核、密度和数据的衰减，可以实现直接的离散化和截断，且误差随截断长度呈指数衰减。通过几个代表性的数值例子验证了结果。", "conclusion": "本文成功地将复尺度变换方法应用于开放介质波导交界处的时谐标量波传播问题，通过分析积分方程核和解的解析延拓特性，并结合等值线变形，实现了高效且误差呈指数衰减的数值求解方法。", "translation": "在这项工作中，我们分析了复尺度变换方法应用于“漏泄”或开放介质波导之间交界处的时谐标量波传播问题。在[arXiv:2302.04353, arXiv:2310.05816, arXiv:2401.04674, arXiv:2411.11204]中，研究表明在适当的假设下，该问题可以简化为在与波导横向的无限界面上的第二类Fredholm积分方程系统。在这里，我们展示了积分方程中出现的核在$\\mathbb{C}^2$的某些自然全实子流形上允许快速衰减的解析延拓。然后我们表明，对于合适的、具有物理意义的边界数据，积分方程的最终解本身也允许解析延拓并满足相关的渐近估计。通过将积分方程变形到合适的等值线，核、密度和数据的衰减使得直接的离散化和截断成为可能，其误差随截断长度呈指数衰减。我们通过几个代表性的数值例子说明了我们的结果。", "summary": "本文研究了复尺度变换在开放介质波导交界处时谐标量波传播问题中的应用。研究发现，将该问题简化为Fredholm积分方程后，其核和解均具有快速衰减的解析延拓特性。通过将积分方程变形到特定等值线，可以利用这些衰减特性实现高效的数值离散化和截断，误差随截断长度呈指数衰减。数值例子验证了该方法的有效性。", "keywords": "复尺度变换, 开放波导, 波传播, 积分方程, 解析延拓", "comments": "本文通过引入复尺度变换和利用解析延拓的特性，为开放波导中的波传播问题提供了一种创新的数值求解方法。其关键创新在于利用核和解的快速衰减特性，通过等值线变形实现了高效且高精度的离散化和截断，解决了传统方法可能面临的计算复杂性问题。误差的指数衰减是一个重要的优势，表明了该方法的鲁棒性和效率。"}}
{"id": "2506.10274", "title": "Discrete Audio Tokens: More Than a Survey!", "authors": ["Pooneh Mousavi", "Gallil Maimon", "Adel Moumen", "Darius Petermann", "Jiatong Shi", "Haibin Wu", "Haici Yang", "Anastasia Kuznetsova", "Artem Ploujnikov", "Ricard Marxer", "Bhuvana Ramabhadran", "Benjamin Elizalde", "Loren Lugosch", "Jinyu Li", "Cem Subakan", "Phil Woodland", "Minje Kim", "Hung-yi Lee", "Shinji Watanabe", "Yossi Adi", "Mirco Ravanelli"], "summary": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks.They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10274v1", "AI": {"title_translation": "离散音频Token：不仅仅是一项综述！", "tldr": "本文对离散音频Token进行了系统性回顾和基准测试，涵盖语音、音乐和通用音频三大领域，提出了新的分类法，并评估了不同分词器的性能，揭示了当前局限性及未来挑战，为该领域提供了深入见解和指导。", "motivation": "现有的离散音频Token综述研究往往侧重于特定领域或任务，缺乏在各种基准测试下的统一比较。", "method": "本文对离散音频分词器进行了系统性审查和基准测试，涵盖语音、音乐和通用音频三个领域。提出了一种基于编码器-解码器、量化技术、训练范式、可流式传输性和应用领域的分类法。在重建、下游性能和声学语言建模等多个基准上评估了分词器，并通过受控消融研究分析了权衡。", "result": "研究结果突出了关键局限性、实际考虑因素和开放性挑战。", "conclusion": "本文为离散音频Token这一快速发展领域未来的研究提供了深刻的见解和指导。", "translation": "离散音频Token是一种紧凑的表示形式，旨在保持感知质量、语音内容和说话者特征，同时实现高效存储和推理，并在各种下游任务中表现出竞争力。它们为连续特征提供了一种实用的替代方案，使语音和音频能够集成到现代大型语言模型（LLMs）中。随着对基于Token的音频处理兴趣的增长，各种Token化方法应运而生，并且已有几项综述回顾了该领域的最新进展。然而，现有研究往往侧重于特定领域或任务，缺乏在各种基准测试下的统一比较。本文对离散音频分词器进行了系统性审查和基准测试，涵盖语音、音乐和通用音频三个领域。我们提出了一种基于编码器-解码器、量化技术、训练范式、可流式传输性和应用领域的分类法。我们在多个基准上评估了分词器，包括重建、下游性能和声学语言建模，并通过受控消融研究分析了权衡。我们的研究结果突出了关键局限性、实际考虑因素和开放性挑战，为这一快速发展领域的未来研究提供了深刻的见解和指导。欲了解更多信息，包括我们的主要结果和分词器数据库，请访问我们的网站：https://poonehmousavi.github.io/dates-website/。", "summary": "本文对离散音频Token进行了全面的系统性回顾和基准测试，旨在解决现有综述缺乏统一比较的问题。研究涵盖语音、音乐和通用音频三大领域，并提出了一种新的分词器分类法。通过在多个基准上评估分词器的性能并进行消融研究，论文揭示了当前技术的局限性、实际应用考量及未来面临的挑战，为离散音频Token的未来研究提供了宝贵的见解和方向。", "keywords": "离散音频Token, 音频分词器, 基准测试, 系统综述, 大型语言模型", "comments": "本文不仅仅是一篇综述，它通过系统的基准测试和分类法，为离散音频Token领域提供了一个统一的评估框架。其创新之处在于跨领域（语音、音乐、通用音频）的比较分析，并深入探讨了权衡和实际问题，这对于指导未来研究和应用具有重要意义。"}}
{"id": "2506.10345", "title": "Technical Report with Proofs for A Full Picture in Conformance Checking: Efficiently Summarizing All Optimal Alignments", "authors": ["Philipp Bär", "Moe T. Wynn", "Sander J. J. Leemans"], "summary": "This technical report provides proofs for the claims in the paper \"A Full\nPicture in Conformance Checking: Efficiently Summarizing All Optimal\nAlignments\".", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10345v1", "AI": {"title_translation": "一致性检查的全貌：高效总结所有最优对齐的技术报告及证明", "tldr": "本技术报告为论文《一致性检查的全貌：高效总结所有最优对齐》中的主张提供了证明。", "motivation": "本技术报告的目的是为论文“一致性检查的全貌：高效总结所有最优对齐”中提出的主张提供证明。", "method": "提供证明（具体证明方法未在摘要中提及）。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "本技术报告为论文《一致性检查的全貌：高效总结所有最优对齐》中的主张提供了证明。", "summary": "本技术报告是论文《一致性检查的全貌：高效总结所有最优对齐》的补充，主要内容是为该论文中提出的所有主张提供详细的证明。", "keywords": "技术报告, 一致性检查, 最佳对齐, 证明", "comments": "本报告作为一篇技术报告，其创新性和重要性在于为另一篇核心论文提供了严格的理论支撑和证明，确保了该论文结论的严谨性。其局限性在于本身不包含新的方法或结果，而是对已有工作的验证。"}}
{"id": "2506.10420", "title": "Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods", "authors": ["Boris Sedlak", "Alireza Furutanpey", "Zihang Wang", "Víctor Casamayor Pujol", "Schahram Dustdar"], "summary": "Edge computing breaks with traditional autoscaling due to strict resource\nconstraints, thus, motivating more flexible scaling behaviors using multiple\nelasticity dimensions. This work introduces an agent-based autoscaling\nframework that dynamically adjusts both hardware resources and internal service\nconfigurations to maximize requirements fulfillment in constrained\nenvironments. We compare four types of scaling agents: Active Inference, Deep Q\nNetwork, Analysis of Structural Knowledge, and Deep Active Inference, using two\nreal-world processing services running in parallel: YOLOv8 for visual\nrecognition and OpenCV for QR code detection. Results show all agents achieve\nacceptable SLO performance with varying convergence patterns. While the Deep Q\nNetwork benefits from pre-training, the structural analysis converges quickly,\nand the deep active inference agent combines theoretical foundations with\npractical scalability advantages. Our findings provide evidence for the\nviability of multi-dimensional agent-based autoscaling for edge environments\nand encourage future work in this research direction.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10420v1", "AI": {"title_translation": "处理服务的多维自动扩展：基于代理方法的比较", "tldr": "本文提出了一种基于代理的多维自动扩展框架，用于在资源受限的边缘环境中动态调整硬件资源和服务配置，并通过比较四种代理类型（Active Inference, Deep Q Network, Analysis of Structural Knowledge, Deep Active Inference）在YOLOv8和OpenCV服务上的表现，证明了其可行性。", "motivation": "边缘计算由于严格的资源限制，打破了传统的自动扩展模式，因此需要更灵活的多维弹性扩展行为。", "method": "本文引入了一个基于代理的自动扩展框架，该框架动态调整硬件资源和内部服务配置，以在受限环境中最大化需求满足。研究比较了四种类型的扩展代理：Active Inference, Deep Q Network, Analysis of Structural Knowledge, 和 Deep Active Inference，使用两个并行运行的真实处理服务：用于视觉识别的YOLOv8和用于QR码检测的OpenCV。", "result": "所有代理都实现了可接受的SLO性能，但收敛模式不同。Deep Q Network受益于预训练，结构分析收敛迅速，而Deep Active Inference代理结合了理论基础和实际可扩展性优势。", "conclusion": "研究结果为多维基于代理的边缘环境自动扩展的可行性提供了证据，并鼓励未来在该研究方向上的工作。", "translation": "边缘计算由于严格的资源限制，打破了传统的自动扩展模式，从而促使使用多维度弹性实现更灵活的扩展行为。这项工作引入了一个基于代理的自动扩展框架，该框架动态调整硬件资源和内部服务配置，以在受限环境中最大化需求满足。我们比较了四种类型的扩展代理：主动推理（Active Inference）、深度Q网络（Deep Q Network）、结构知识分析（Analysis of Structural Knowledge）和深度主动推理（Deep Active Inference），使用了两个并行运行的真实处理服务：用于视觉识别的YOLOv8和用于QR码检测的OpenCV。结果显示所有代理都达到了可接受的服务水平目标（SLO）性能，但收敛模式各不相同。虽然深度Q网络受益于预训练，但结构分析收敛迅速，而深度主动推理代理结合了理论基础和实际可扩展性优势。我们的发现为边缘环境下多维基于代理的自动扩展的可行性提供了证据，并鼓励未来在该研究方向上的工作。", "summary": "本文针对边缘计算中资源受限导致的传统自动扩展失效问题，提出了一种基于代理的多维自动扩展框架。该框架能够同时调整硬件资源和服务配置。研究比较了Active Inference、Deep Q Network、Analysis of Structural Knowledge和Deep Active Inference四种代理在YOLOv8和OpenCV服务上的表现。结果表明，所有代理均能达到可接受的性能，并具有不同的收敛特性，验证了多维基于代理的自动扩展在边缘环境中的可行性。", "keywords": "自动扩展, 边缘计算, 代理, 多维度, 资源管理", "comments": "本文提出了一种新颖的基于代理的多维自动扩展方法，解决了边缘计算中资源受限下传统自动扩展的局限性。通过比较多种先进的AI代理，提供了实际应用的可行性证据，为未来边缘计算的资源管理提供了有价值的参考。其创新点在于结合了多维弹性与智能代理，以适应复杂的边缘环境。"}}
{"id": "2506.10136", "title": "Comparing name generator designs in rural panel studies: analyzing alter retention and change", "authors": ["Marian-Gabriel Hâncean", "Jürgen Lerner", "Christopher McCarty"], "summary": "We conducted a two-wave personal network study in a rural Romanian community,\ninterviewing the same participants (n = 68) using two name generators. Wave 1\nemployed a fixed-choice generator (n = 25) focused on emotional closeness; Wave\n2 used a free-choice generator based on frequent interaction. We compared tie\ncharacteristics and assessed retention across waves. Alters who were kin,\nco-residents, or emotionally close were more likely to be retained, regardless\nof generator type. These findings underscore the role of relational attributes\nin personal network stability and highlight design considerations for network\nstudies in resource-limited, culturally distinct settings.", "comment": "14 pages, 4 tables, 1 figure", "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.10136v1", "AI": {"title_translation": "农村面板研究中姓名生成器设计的比较：分析伙伴保留和变化", "tldr": "一项在罗马尼亚农村进行的网络研究比较了两种姓名生成器，发现亲属、同住者和情感亲密的伙伴更容易被保留，强调了关系属性在网络稳定性中的作用。", "motivation": "本研究旨在比较不同姓名生成器设计在农村面板研究中对伙伴（alter）保留的影响，并为在资源有限、文化独特的地区进行网络研究提供设计考量。", "method": "研究在罗马尼亚农村社区进行了一项两波次个人网络研究，采访了68名相同的参与者。第一波使用关注情感亲密的固定选择生成器，第二波使用基于频繁互动的自由选择生成器。研究比较了关系特征并评估了跨波次的伙伴保留情况。", "result": "研究发现，无论使用哪种姓名生成器类型，亲属、同住者或情感亲密的伙伴更有可能被保留。", "conclusion": "研究结果强调了关系属性在个人网络稳定性中的重要作用，并为在资源有限、文化独特的背景下进行网络研究提供了重要的设计考量。", "translation": "我们在罗马尼亚农村社区进行了一项两波次个人网络研究，使用两种姓名生成器采访了相同的参与者（n = 68）。第一波采用了关注情感亲密的固定选择生成器（n = 25）；第二波采用了基于频繁互动的自由选择生成器。我们比较了关系特征并评估了跨波次的保留情况。无论生成器类型如何，亲属、同住者或情感亲密的伙伴更有可能被保留。这些发现强调了关系属性在个人网络稳定性中的作用，并突出了在资源有限、文化独特的背景下进行网络研究的设计考量。", "summary": "本研究在罗马尼亚农村社区进行了一项两波次个人网络研究，比较了固定选择和自由选择两种姓名生成器在伙伴保留方面的效果。研究发现，亲属、同住者和情感亲密的伙伴无论在何种生成器下都更容易被保留。这些结果强调了关系属性在个人网络稳定性中的关键作用，并为在资源有限、文化独特的地区进行网络研究提供了实用的设计建议。", "keywords": "姓名生成器, 伙伴保留, 个人网络, 面板研究, 农村研究", "comments": "这项研究通过实地比较不同姓名生成器设计在农村背景下的效果，为个人网络研究方法学提供了宝贵的实证见解。其创新之处在于关注了资源有限和文化独特的环境，并强调了关系属性在网络稳定性中的关键作用，对未来的网络研究设计具有指导意义。"}}
{"id": "2506.10711", "title": "PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment", "authors": ["Li Luo", "Shangsong Liang"], "summary": "Generating accurate and stable long rollouts is a notorious challenge for\ntime-dependent PDEs (Partial Differential Equations). Recently, motivated by\nthe importance of high-frequency accuracy, a refiner model called PDERefiner\nutilizes diffusion models to refine outputs for every time step, since the\ndenoising process could increase the correctness of modeling high frequency\npart. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the\namplitude of high frequency part better than not doing refinement process.\nHowever, for some other cases, the spectrum might be more complicated. For\nexample, for a harder PDE like Navior-Stokes equation, diffusion models could\nover-degrade the higher frequency part. This motivates us to release the\nconstraint that each frequency weighs the same. We enhance our refiner model\nwith doing adjustments on spectral space, which recovers Blurring diffusion\nmodels. We developed a new v-prediction technique for Blurring diffusion\nmodels, recovering the MSE training objective on the first refinement step. We\nshow that in this case, for different model backbones, such as U-Net and neural\noperators, the outputs of PDE-SpectralRefiner are more accurate for both\none-step MSE loss and rollout loss.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10711v1", "AI": {"title_translation": "PDESpectralRefiner：通过频谱调整实现更准确的长时间展开", "tldr": "PDESpectralRefiner通过频谱调整改进了扩散模型，以更准确地模拟PDE的长时间展开，解决了高频部分过度退化的问题。", "motivation": "现有PDERefiner模型在使用扩散模型细化PDE输出时，尤其对于Navier-Stokes等复杂PDE，可能过度退化高频部分。这促使作者开发一种允许不同频率权重不同的方法。", "method": "增强了refiner模型，通过在频谱空间进行调整，恢复了模糊扩散模型。开发了一种新的v-prediction技术用于模糊扩散模型，恢复了第一个细化步骤上的MSE训练目标。", "result": "PDESpectralRefiner的输出对于不同模型骨干（如U-Net和神经算子）在一步MSE损失和展开损失方面都更准确。", "conclusion": "PDESpectralRefiner通过频谱调整有效解决了现有扩散模型在PDE长时间展开中高频部分过度退化的问题，显著提高了模拟准确性。", "translation": "生成准确稳定的长时间展开是时间依赖PDE（偏微分方程）的一个众所周知的挑战。最近，受高频精度重要性的启发，一个名为PDERefiner的细化器模型利用扩散模型来细化每个时间步的输出，因为去噪过程可以提高高频部分建模的正确性。对于一维Kuramoto-Sivashinsky方程，细化器模型可以比不进行细化过程更好地降低高频部分的振幅。然而，对于其他一些情况，频谱可能更复杂。例如，对于像Navier-Stokes方程这样更难的PDE，扩散模型可能会过度降低高频部分。这促使我们放宽每个频率权重相同的限制。我们通过在频谱空间进行调整来增强我们的细化器模型，这恢复了模糊扩散模型。我们为模糊扩散模型开发了一种新的v-prediction技术，恢复了第一个细化步骤上的MSE训练目标。我们表明，在这种情况下，对于不同的模型骨干，例如U-Net和神经算子，PDE-SpectralRefiner的输出在一步MSE损失和展开损失方面都更准确。", "summary": "本文提出了PDESpectralRefiner，一个改进的细化模型，用于解决时间依赖PDE长时间展开中高频部分过度退化的问题。通过在频谱空间进行调整并引入新的v-prediction技术，该模型能够恢复模糊扩散模型的特性，并有效提高对复杂PDE（如Navier-Stokes方程）的模拟准确性，在一步MSE损失和展开损失上均表现更优。", "keywords": "PDE, 扩散模型, 频谱调整, 长时间展开, 高频精度", "comments": "该论文的创新点在于认识到现有扩散模型在PDE模拟中可能过度退化高频部分，并提出通过在频谱空间进行调整来解决这一问题。引入新的v-prediction技术也值得关注。该方法对于提高复杂PDE的长时间模拟精度具有重要意义。"}}
{"id": "2506.10156", "title": "Quantifying Data Requirements for EEG Independent Component Analysis Using AMICA", "authors": ["Gwenevere Frank", "Seyed Yahya Shirazi", "Jason Palmer", "Gert Cauwenberghs", "Scott Makeig", "Arnaud Delorme"], "summary": "Independent Component Analysis (ICA) is an important step in EEG processing\nfor a wide-ranging set of applications. However, ICA requires well-designed\nstudies and data collection practices to yield optimal results. Past studies\nhave focused on quantitative evaluation of the differences in quality produced\nby different ICA algorithms as well as different configurations of parameters\nfor AMICA, a multimodal ICA algorithm that is considered the benchmark against\nwhich other algorithms are measured. Here, the effect of the data quantity\nversus the number of channels on decomposition quality is explored. AMICA\ndecompositions were run on a 71 channel dataset with 13 subjects while randomly\nsubsampling data to correspond to specific ratios of the number of frames in a\ndataset to the channel count. Decomposition quality was evaluated for the\nvarying quantities of data using measures of mutual information reduction (MIR)\nand the near dipolarity of components. We also note that an asymptotic trend\ncan be seen in the increase of MIR and a general increasing trend in near\ndipolarity with increasing data, but no definitive plateau in these metrics was\nobserved, suggesting that the benefits of collecting additional EEG data may\nextend beyond common heuristic thresholds and continue to enhance decomposition\nquality.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10156v1", "AI": {"title_translation": "使用AMICA量化脑电图独立成分分析的数据要求", "tldr": "本研究探讨了EEG独立成分分析中数据量对分解质量的影响，发现随着数据量的增加，分解质量持续提高，没有观察到明确的平台期。", "motivation": "独立成分分析（ICA）是脑电图（EEG）处理中的重要步骤，但其优化结果需要精心设计的研究和数据采集实践。以往的研究侧重于评估不同ICA算法及其参数配置产生的质量差异。本研究的动机是探索数据量与通道数量对分解质量的影响。", "method": "研究在包含13名受试者的71通道数据集上运行AMICA分解。通过随机二次采样数据，使其帧数与通道数之比达到特定比例，以此调整数据量。分解质量通过互信息减少（MIR）和成分的近偶极性（near dipolarity）进行评估。", "result": "结果显示，随着数据量的增加，互信息减少（MIR）呈现渐近趋势，近偶极性（near dipolarity）总体呈增加趋势。然而，在这些指标中未观察到明确的平台期。", "conclusion": "研究结论表明，收集额外的脑电图数据的好处可能超出常见的启发式阈值，并持续提高分解质量。", "translation": "独立成分分析（ICA）是脑电图（EEG）处理中的一个重要步骤，广泛应用于各种应用。然而，ICA需要精心设计的研究和数据采集实践才能产生最佳结果。过去的研究主要集中于定量评估不同ICA算法以及AMICA（一种多模态ICA算法，被认为是衡量其他算法的基准）不同参数配置所产生的质量差异。本研究探讨了数据量相对于通道数量对分解质量的影响。AMICA分解在包含13名受试者的71通道数据集上运行，同时随机二次采样数据以对应数据集中帧数与通道数之比的特定比例。使用互信息减少（MIR）和成分的近偶极性（near dipolarity）来评估不同数据量下的分解质量。我们还注意到，随着数据量的增加，MIR的增加呈现渐近趋势，近偶极性总体呈增加趋势，但这些指标中没有观察到明确的平台期，这表明收集额外脑电图数据的好处可能超出常见的启发式阈值，并持续提高分解质量。", "summary": "本研究旨在量化脑电图独立成分分析（ICA）中数据量对分解质量的影响，特别关注AMICA算法。通过在71通道数据集上对不同数据量进行AMICA分解，并使用互信息减少和成分的近偶极性评估分解质量，研究发现随着数据量的增加，分解质量持续提高，并未观察到明确的平台期。这表明收集更多脑电图数据可能持续提升ICA分解的性能。", "keywords": "EEG, 独立成分分析, AMICA, 数据量, 分解质量", "comments": "这项研究对于EEG实验设计和数据采集具有重要指导意义。其创新之处在于量化了数据量对ICA分解质量的持续影响，挑战了现有的一些启发式阈值。研究结果强调了收集更多数据的重要性，有助于研究人员优化实验方案以获得更高质量的ICA结果。"}}
{"id": "2506.10203", "title": "Formalizing Neuromorphic Control Systems: A General Proposal and A Rhythmic Case Study", "authors": ["Taisia Medvedeva", "Alessio Franci", "Fernando Castaños"], "summary": "Neuromorphic control is receiving growing attention due to the multifaceted\nadvantages it brings over more classical control approaches, including: sparse\nand on-demand sensing, information transmission, and actuation;\nenergy-efficient designs and realizations in neuromorphic hardware; event-based\nsignal processing and control signal computation. However, a general\ncontrol-theoretical formalization of what \"neuromorphic control systems\" are\nand how we can rigorously analyze, design, and control them is still largely\nmissing. In this note, we suggest a possible path toward formalizing\nneuromorphic control systems. We apply the proposed framework to a rhythmic\ncontrol case study and rigorously show how it has the potential to make\nneuromorphic control systems analysis and design amenable to mature control\ntheoretical approaches like describing function analysis and harmonic balance,\nfast-slow analysis, discrete and hybrid systems, and robust optimization.", "comment": "Submitted to the 64th IEEE Conference on Decision and Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10203v1", "AI": {"title_translation": "神经拟态控制系统的形式化：一个通用提案和一个节奏案例研究", "tldr": "该论文提出了一个神经拟态控制系统形式化的框架，并通过一个节奏控制案例研究展示了其分析和设计的潜力。", "motivation": "神经拟态控制系统具有多方面优势，但目前缺乏一个通用的控制理论形式化方法，以对其进行严格的分析、设计和控制。", "method": "本文提出了一种神经拟态控制系统形式化的可能途径，并将所提出的框架应用于一个节奏控制案例研究。", "result": "所提出的框架能够使神经拟态控制系统的分析和设计适用于成熟的控制理论方法，如描述函数分析、谐波平衡、快慢分析、离散和混合系统以及鲁棒优化。", "conclusion": "该论文提出的框架为神经拟态控制系统的分析和设计提供了一种严格的方法，并能够将其与已建立的控制理论方法相结合。", "translation": "神经拟态控制因其相对于经典控制方法的诸多优势而受到越来越多的关注，这些优势包括：稀疏和按需的传感、信息传输和执行；神经拟态硬件中节能的设计和实现；基于事件的信号处理和控制信号计算。然而，关于“神经拟态控制系统”是什么以及我们如何严格分析、设计和控制它们的通用控制理论形式化仍然 largely 缺失。在本说明中，我们提出了一种可能的形式化神经拟态控制系统的途径。我们将所提出的框架应用于一个节奏控制案例研究，并严格展示了它如何有可能使神经拟态控制系统的分析和设计适用于成熟的控制理论方法，如描述函数分析和谐波平衡、快慢分析、离散和混合系统以及鲁棒优化。", "summary": "本文旨在解决神经拟态控制系统缺乏正式控制理论框架的问题。尽管神经拟态控制系统具有诸多优势，但其严格的分析和设计方法仍未建立。论文提出了一种通用的神经拟态控制系统形式化方法，并通过一个节奏控制案例研究展示了其有效性。该框架能够使神经拟态控制系统的分析和设计与现有成熟的控制理论技术相结合，从而弥合了神经拟态原理与传统控制方法之间的鸿沟。", "keywords": "神经拟态控制, 形式化, 控制系统, 节奏控制, 控制理论", "comments": "这篇论文具有创新性，因为它通过提供一个形式化的理论框架，解决了神经拟态控制领域的一个关键空白。这种形式化对于推动神经拟态控制系统的严格分析、设计和实现至关重要，使其能够更容易地应用已建立的控制理论工具，并可能加速其实际应用。"}}
{"id": "2506.10039", "title": "Symbolic Generation and Modular Embedding of High-Quality abc-Triples", "authors": ["Michael A. Idowu"], "summary": "We present a symbolic identity for generating integer triples $(a, b, c)$\nsatisfying $a + b = c$, inspired by structural features of the \\emph{abc\nconjecture}. The construction uses powers of $2$ and $3$ in combination with\nmodular inversion in $\\mathbb{Z}/3^p\\mathbb{Z}$, leading to a parametric\nidentity with residue constraints that yield abc-triples exhibiting low radical\nvalues. Through affine transformations, these symbolic triples are embedded\ninto a broader space of high-quality examples, optimised for the ratio $\\log c\n/ \\log \\operatorname{rad}(abc)$. Computational results demonstrate the\nemergence of structured, radical-minimising candidates, including both known\nand novel triples. These methods provide a symbolic and algebraic framework for\ncontrolled triple generation, and suggest exploratory implications for symbolic\nentropy filtering in cryptographic pre-processing.", "comment": "17 pages, includes tables and illustrative examples; discusses\n  symbolic generation of abc-triples and applications in entropy filtering and\n  cryptographic pre-processing", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10039v1", "AI": {"title_translation": "高质量abc三元组的符号生成与模嵌入", "tldr": "本文提出了一种基于abc猜想结构特征的符号恒等式，用于生成低根值的高质量abc三元组，并通过仿射变换将其嵌入更广阔的空间，为受控三元组生成提供了代数框架，并对密码学预处理中的符号熵过滤具有探索性意义。", "motivation": "受abc猜想的结构特征启发，旨在生成满足a + b = c且具有低根值的整数三元组。", "method": "提出了一种符号恒等式，结合2和3的幂次以及在Z/3^pZ中的模逆运算，生成具有残差约束的参数化恒等式。通过仿射变换，将这些符号三元组嵌入到更广阔的高质量示例空间中，并针对比率log c / log rad(abc)进行优化。", "result": "计算结果表明，出现了结构化的、根最小化的候选三元组，包括已知和新颖的三元组。", "conclusion": "这些方法为受控的三元组生成提供了一个符号和代数框架，并对密码学预处理中的符号熵过滤具有探索性意义。", "translation": "我们提出了一种符号恒等式，用于生成满足a + b = c的整数三元组，其灵感来源于abc猜想的结构特征。该构造结合了2和3的幂次以及在Z/3^pZ中的模逆运算，从而形成了一个具有残差约束的参数化恒等式，该恒等式能产生具有低根值的abc三元组。通过仿射变换，这些符号三元组被嵌入到更广阔的高质量示例空间中，并针对比率log c / log rad(abc)进行了优化。计算结果表明，出现了结构化的、根最小化的候选三元组，包括已知和新颖的三元组。这些方法为受控的三元组生成提供了一个符号和代数框架，并对密码学预处理中的符号熵过滤具有探索性意义。", "summary": "本文提出了一种生成高质量abc三元组的符号恒等式。该方法利用2和3的幂次以及模逆运算，构建了参数化恒等式来生成低根值的三元组。通过仿射变换将这些三元组嵌入到优化的空间中，计算结果显示生成了结构化的、根最小化的新旧三元组。该研究提供了一个代数框架用于三元组生成，并对密码学应用具有潜在意义。", "keywords": "abc三元组, 符号生成, 模嵌入, 根值, 密码学", "comments": "该论文的创新之处在于提出了一种新颖的符号和代数框架来生成高质量的abc三元组，特别是其通过结合幂次、模逆和仿射变换来优化根值的策略。其对密码学预处理中符号熵过滤的潜在探索性意义也增加了其重要性。"}}
{"id": "2506.10974", "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science", "authors": ["Yixin Ou", "Yujie Luo", "Jingsheng Zheng", "Lanning Wei", "Shuofei Qiao", "Jintian Zhang", "Da Zheng", "Huajun Chen", "Ningyu Zhang"], "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.", "comment": "Ongoing work. Code is at https://github.com/innovatingAI/AutoMind", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10974v1", "AI": {"title_translation": "AutoMind：用于自动化数据科学的自适应知识型智能体", "tldr": "AutoMind是一个自适应的知识型LLM智能体框架，通过专家知识库、知识型树搜索算法和自适应编码策略，解决了现有LLM数据科学智能体在复杂任务中的局限性，并在自动化数据科学基准测试中表现优异。", "motivation": "现有的大语言模型（LLM）驱动的数据科学智能体依赖僵化、预定义的工作流和不灵活的编码策略，导致它们仅在相对简单的问题上表现良好，无法捕获人类专家在复杂创新任务中的经验，因此其实际世界有效性有限。", "method": "本文引入了AutoMind，一个自适应的知识型LLM智能体框架，通过以下三个关键进展克服了现有不足：1）一个精选的专家知识库，将智能体建立在领域专家知识之上；2）一个智能知识型树搜索算法，策略性地探索可能的解决方案；3）一个自适应编码策略，根据任务复杂性动态调整代码生成。", "result": "在两个自动化数据科学基准测试上的评估表明，AutoMind比最先进的基线取得了卓越的性能。额外的分析证实了其有利的有效性、效率和定性解决方案质量。", "conclusion": "AutoMind是迈向全自动化数据科学的高效且稳健的一步，它通过其自适应和知识型方法，显著提升了LLM智能体在复杂数据科学任务中的表现。", "translation": "大型语言模型（LLM）智能体在解决现实世界数据科学问题方面展现出巨大潜力。LLM驱动的数据科学智能体有望自动化整个机器学习流程，但其在现实世界中的有效性仍然有限。现有框架依赖僵化、预定义的工作流和不灵活的编码策略；因此，它们仅在相对简单、经典的问题上表现出色，未能捕捉人类实践者在复杂、创新任务中带来的经验专业知识。在这项工作中，我们引入了AutoMind，一个自适应的、知识型的LLM智能体框架，通过三个关键进展克服了这些缺陷：(1) 一个精选的专家知识库，将智能体建立在领域专家知识之上，(2) 一个智能知识型树搜索算法，策略性地探索可能的解决方案，以及 (3) 一个自适应编码策略，动态地根据任务复杂性调整代码生成。在两个自动化数据科学基准测试上的评估表明，AutoMind比最先进的基线提供了卓越的性能。额外的分析证实了其有利的有效性、效率和定性解决方案质量，突出了AutoMind是迈向全自动化数据科学的高效且稳健的一步。", "summary": "AutoMind是一个创新的自适应知识型LLM智能体框架，旨在解决当前LLM驱动的数据科学智能体在复杂任务中表现不足的问题。它通过整合专家知识库、引入知识型树搜索算法以及采用自适应编码策略来提升性能。实验结果表明，AutoMind在自动化数据科学基准测试中优于现有基线，并在有效性、效率和解决方案质量方面表现出色，预示着其在实现全自动化数据科学方面的巨大潜力。", "keywords": "LLM智能体, 自动化数据科学, 知识型智能体, 自适应编码, 专家知识库", "comments": "该论文的创新点在于提出了一个集成了专家知识、智能搜索和自适应编码策略的LLM智能体框架，有效克服了现有框架在处理复杂数据科学问题时的局限性。其重要性在于推动了自动化数据科学领域的发展，使LLM智能体能够更好地模拟人类专家的经验和决策过程。论文通过具体的技术改进，为实现更高效、更鲁棒的自动化数据科学提供了新的思路和解决方案。"}}
{"id": "2506.10204", "title": "Prompt Variability Effects On LLM Code Generation", "authors": ["Andrei Paleyes", "Radzim Sendyka", "Diana Robinson", "Christian Cabrera", "Neil D. Lawrence"], "summary": "Code generation is one of the most active areas of application of Large\nLanguage Models (LLMs). While LLMs lower barriers to writing code and\naccelerate development process, the overall quality of generated programs\ndepends on the quality of given prompts. Specifically, functionality and\nquality of generated code can be sensitive to user's background and familiarity\nwith software development. It is therefore important to quantify LLM's\nsensitivity to variations in the input. To this end we propose a synthetic\nevaluation pipeline for code generation with LLMs, as well as a systematic\npersona-based evaluation approach to expose qualitative differences of LLM\nresponses dependent on prospective user background. Both proposed methods are\ncompletely independent from specific programming tasks and LLMs, and thus are\nwidely applicable. We provide experimental evidence illustrating utility of our\nmethods and share our code for the benefit of the community.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10204v1", "AI": {"title_translation": "提示词变异对大型语言模型代码生成的影响", "tldr": "LLM代码生成质量受提示词影响，本文提出合成评估流程和基于用户画像的评估方法来量化LLM对输入变化的敏感性。", "motivation": "大型语言模型（LLM）生成的代码质量取决于给定提示词的质量，并且可能对用户的背景和软件开发熟悉程度敏感。因此，量化LLM对输入变化的敏感性非常重要。", "method": "提出一个用于LLM代码生成的合成评估管道，以及一个系统性的基于用户画像的评估方法，以揭示LLM响应中依赖于潜在用户背景的定性差异。这两种方法都完全独立于特定的编程任务和LLM。", "result": "提供了实验证据，说明了所提出方法的实用性。", "conclusion": "本文提出的合成评估管道和基于用户画像的评估方法能够有效量化LLM代码生成对提示词变化的敏感性，且具有广泛适用性。", "translation": "代码生成是大型语言模型（LLM）最活跃的应用领域之一。虽然LLM降低了编写代码的门槛并加速了开发过程，但生成程序的整体质量取决于给定提示词的质量。具体来说，生成代码的功能性和质量可能对用户的背景和软件开发的熟悉程度敏感。因此，量化LLM对输入变化的敏感性非常重要。为此，我们提出了一个用于LLM代码生成的合成评估管道，以及一个系统性的基于用户画像的评估方法，以揭示LLM响应中依赖于潜在用户背景的定性差异。这两种提出的方法都完全独立于特定的编程任务和LLM，因此具有广泛的适用性。我们提供了实验证据，说明了我们方法的实用性，并分享了我们的代码以造福社区。", "summary": "本文研究了提示词变化对大型语言模型（LLM）代码生成质量的影响。鉴于LLM生成的代码质量受提示词及用户背景影响，作者提出了一个合成评估管道和一套基于用户画像的系统评估方法，旨在量化LLM对LLM代码生成对输入变化的敏感性。这些方法独立于特定任务和LLM，具有广泛适用性，并通过实验验证了其有效性。", "keywords": "LLM, 代码生成, 提示词变异, 敏感性评估, 用户画像", "comments": "这项工作创新地提出了量化LLM代码生成对提示词变异敏感性的评估方法，特别是引入了“合成评估管道”和“基于用户画像的评估方法”，这对于理解和改进LLM在实际应用中的表现具有重要意义。其方法的通用性（独立于特定编程任务和LLM）是其一大亮点。"}}
{"id": "2506.10249", "title": "Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations", "authors": ["Andrea Gaggioli", "Sabrina Bartolotta", "Andrea Ubaldi", "Katusha Gerardini", "Eleonora Diletta Sarcinella", "Alice Chirico"], "summary": "Artificial Intelligence holds significant potential to enhance human\ncreativity. However, achieving this vision requires a clearer understanding of\nhow such enhancement can be effectively realized. Adopting the perspective of\ndistributed creativity, we identify three primary modes through which AI can\ncontribute to creative processes: Support, where AI acts as a tool; Synergy,\nwhere AI and humans collaborate in complementary ways; and Symbiosis, where\nhuman and AI cognition become so integrated that they form a unified creative\nsystem. These modes are defined along two key dimensions: the level of\ntechnical autonomy exhibited by the AI system and the degree of perceived\nagency attributed to it. We examine how each configuration influences different\nlevels of creativity - from everyday problem-solving to paradigm-shifting\ninnovation - and discuss the theoretical, ethical, and design implications.", "comment": "36 pages, 3 figures. This conceptual paper proposes a taxonomy of\n  Extended Creativity systems and examines the relational dynamics between\n  human and AI agents in creative processes. Suitable for readers in HCI, AI,\n  cognitive science, and digital design. The illustrations were created by\n  Francesco Giordano and are used with permission (not under CC license)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10249v1", "AI": {"title_translation": "扩展创造力：理解人机创造性关系的概念框架", "tldr": "本文提出了一个概念框架，通过“支持、协同、共生”三种模式，以及AI技术自主性和感知能动性两个维度，来理解和分类人机交互如何增强创造力。", "motivation": "人工智能在增强人类创造力方面具有巨大潜力，但要实现这一愿景，需要更清晰地理解如何有效地实现这种增强。", "method": "本文采用分布式创造力的视角，提出了一个概念框架，识别了AI对创造过程贡献的三种主要模式（支持、协同、共生），并根据AI系统的技术自主性水平和感知能动性程度这两个关键维度定义了这些模式。", "result": "研究识别出AI对创造过程贡献的三种模式：支持（AI作为工具）、协同（人机互补协作）和共生（人机认知高度整合形成统一创造系统）。这些模式根据AI的技术自主性和感知能动性定义。文章还探讨了每种配置如何影响不同水平的创造力（从日常问题解决到范式转变的创新）。", "conclusion": "本文提出了一个理解人机创造力关系的框架，并讨论了其在理论、伦理和设计方面的影响，指出不同的AI与人机配置会影响不同层次的创造力。", "translation": "人工智能在增强人类创造力方面具有巨大潜力。然而，要实现这一愿景，需要更清晰地理解如何有效地实现这种增强。本文采用分布式创造力的视角，识别出AI对创造过程贡献的三种主要模式：支持（AI作为工具）、协同（AI和人类以互补方式协作），以及共生（人类和AI认知高度整合，形成统一的创造系统）。这些模式根据AI系统所展现的技术自主性水平和对其感知能动性的程度这两个关键维度来定义。我们探讨了每种配置如何影响不同水平的创造力——从日常问题解决到范式转变的创新——并讨论了其理论、伦理和设计方面的影响。", "summary": "本文提出了一个名为“扩展创造力”的概念框架，旨在理解和分类人机交互如何增强创造力。该框架从分布式创造力的角度出发，识别了AI在创造过程中扮演的三种主要模式：支持、协同和共生。这些模式由AI的技术自主性和感知能动性两个维度定义，并探讨了它们如何影响从日常问题解决到重大创新的不同层次的创造力，同时讨论了相关的理论、伦理和设计含义。", "keywords": "人机创造力, 概念框架, 分布式创造力, AI自主性, 感知能动性", "comments": "该论文提出了一个创新且实用的概念框架，为理解和分类人机创造力互动提供了清晰的视角。其将人机关系细分为支持、协同和共生三种模式，并引入技术自主性和感知能动性两个维度，有助于研究者和设计师更系统地思考如何利用AI提升创造力。这对于未来人机协作系统的设计和伦理考量具有重要指导意义。"}}
{"id": "2506.10925", "title": "Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence", "authors": ["Eduardo Baena", "Paolo Testolina", "Michele Polese", "Sergi Aliaga", "Andrew Benincasa", "Dimitrios Koutsonikolas", "Josep Jornet", "Tommaso Melodia"], "summary": "Lunar surface operations impose stringent requirements on wireless\ncommunication systems, including autonomy, robustness to disruption, and the\nability to adapt to environmental and mission-driven context. While Space-O-RAN\nprovides a distributed orchestration model aligned with 3GPP standards, its\ndecision logic is limited to static policies and lacks semantic integration. We\npropose a novel extension incorporating a semantic agentic layer enabled by the\nModel Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,\nallowing context-aware decision making across real-time, near-real-time, and\nnon-real-time control layers. Distributed cognitive agents deployed in rovers,\nlanders, and lunar base stations implement wireless-aware coordination\nstrategies, including delay-adaptive reasoning and bandwidth-aware semantic\ncompression, while interacting with multiple MCP servers to reason over\ntelemetry, locomotion planning, and mission constraints.", "comment": "Lunar Surface Innovation Consortium 2025 Spring Meeting, May 20-22", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.10925v1", "AI": {"title_translation": "自主无线空间网络的代理语义控制：通过MCP驱动的分布式智能扩展Space-O-RAN", "tldr": "针对月球无线通信的严格要求，本文提出通过引入MCP和A2A协议的语义代理层来扩展Space-O-RAN，实现上下文感知决策和分布式认知代理，以提高自主性、鲁棒性和适应性。", "motivation": "月球表面操作对无线通信系统提出了严苛要求，包括自主性、抗干扰能力和环境及任务驱动的适应能力。尽管Space-O-RAN提供了符合3GPP标准的分布式编排模型，但其决策逻辑仅限于静态策略，且缺乏语义集成。", "method": "本文提出通过引入由模型上下文协议（MCP）和代理间（A2A）通信协议支持的语义代理层来扩展Space-O-RAN。该层允许跨实时、近实时和非实时控制层进行上下文感知决策。分布式认知代理部署在月球车、着陆器和月球基站中，实现无线感知协调策略，包括延迟自适应推理和带宽感知语义压缩，并与多个MCP服务器交互以对遥测、运动规划和任务约束进行推理。", "result": "该方法实现了跨实时、近实时和非实时控制层的上下文感知决策，并能够执行无线感知协调策略，如延迟自适应推理和带宽感知语义压缩，同时能对遥测数据、运动规划和任务约束进行推理。", "conclusion": "本文通过为Space-O-RAN引入语义代理层和分布式认知代理，显著增强了其在月球无线通信系统中的自主性、鲁棒性和适应性，使其能够进行上下文感知决策和复杂的协调策略。", "translation": "月球表面操作对无线通信系统提出了严苛要求，包括自主性、抗干扰能力以及适应环境和任务驱动上下文的能力。尽管Space-O-RAN提供了一个符合3GPP标准的分布式编排模型，但其决策逻辑仅限于静态策略，且缺乏语义集成。我们提出了一种新颖的扩展方案，通过引入由模型上下文协议（MCP）和代理间（A2A）通信协议支持的语义代理层，从而实现在实时、近实时和非实时控制层上的上下文感知决策。部署在月球车、着陆器和月球基站中的分布式认知代理实现了无线感知协调策略，包括延迟自适应推理和带宽感知语义压缩，同时与多个MCP服务器交互以对遥测、运动规划和任务约束进行推理。", "summary": "本文针对月球表面操作对无线通信系统提出的高要求，指出现有Space-O-RAN在决策逻辑和语义集成方面的不足。为解决此问题，作者提出将语义代理层引入Space-O-RAN，该层利用模型上下文协议（MCP）和代理间（A2A）通信协议，实现跨多层控制的上下文感知决策。部署在月球设备上的分布式认知代理能够执行高级协调策略，如延迟自适应推理和带宽感知语义压缩，并通过与MCP服务器交互，处理遥测、运动规划和任务约束信息。", "keywords": "无线空间网络, 语义控制, Space-O-RAN, 分布式智能, 模型上下文协议", "comments": "这篇论文提出了一种创新的方法，通过将代理语义层与MCP和A2A协议集成到Space-O-RAN中，显著提升了月球无线通信系统的自主性和适应性。其创新点在于引入了分布式认知代理和上下文感知决策能力，解决了现有系统静态决策和缺乏语义集成的局限性。这对于未来深空探测和月球基地建设中的复杂通信需求具有重要意义。"}}
{"id": "2506.10239", "title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "authors": ["Maximilian Mühlbauer", "Freek Stulp", "Sylvain Calinon", "Alin Albu-Schäffer", "João Silvério"], "summary": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the\nmost suitable haptic feedback for each phase of a task, based on learned or\nperceived uncertainty. While keeping the human in the loop remains essential,\nfor instance, to ensure high precision, partial automation of certain task\nphases is critical for productivity. We present a unified framework for\nprobabilistic VFs that seamlessly switches between manual fixtures,\nsemi-automated fixtures (with the human handling precise tasks), and full\nautonomy. We introduce a novel probabilistic Dynamical System-based VF for\ncoarse guidance, enabling the robot to autonomously complete certain task\nphases while keeping the human operator in the loop. For tasks requiring\nprecise guidance, we extend probabilistic position-based trajectory fixtures\nwith automation allowing for seamless human interaction as well as\ngeometry-awareness and optimal impedance gains. For manual tasks requiring very\nprecise guidance, we also extend visual servoing fixtures with the same\ngeometry-awareness and impedance behaviour. We validate our approach\nexperimentally on different robots, showcasing multiple operation modes and the\nease of programming fixtures.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10239v1", "AI": {"title_translation": "一种基于概率动态、轨迹和视觉的虚拟夹具统一框架", "tldr": "本文提出了一种用于概率虚拟夹具的统一框架，能够根据任务阶段在手动、半自动化和完全自主模式之间无缝切换，以提高生产力并保持精度。", "motivation": "为了在保持人类高精度的同时，通过部分自动化提高任务生产力，并实现触觉反馈的自适应选择，研究人员需要一个统一的概率虚拟夹具方法。", "method": "本文提出了一个统一的概率虚拟夹具框架，该框架可在手动、半自动化（人类处理精确任务）和完全自主模式之间无缝切换。具体方法包括：引入一种新颖的基于概率动力学系统的VF用于粗略引导，使机器人能自主完成部分任务；扩展基于概率位置的轨迹夹具，使其具有自动化功能，以实现精确引导、无缝人机交互、几何感知和最佳阻抗增益；同时，为需要非常精确引导的手动任务，扩展视觉伺服夹具，使其具有相同的几何感知和阻抗行为。该方法在不同机器人上进行了实验验证。", "result": "该方法在不同机器人上进行了实验验证，展示了多种操作模式以及夹具编程的简易性。", "conclusion": "本文成功提出了一个用于概率虚拟夹具的统一框架，该框架能够无缝切换不同自动化水平，从而在保持精度的同时提高生产力并简化编程。", "translation": "概率虚拟夹具（VFs）能够根据学习或感知的 Haptic 反馈，为任务的每个阶段自适应选择最合适的触觉反馈。虽然保持人机协作对于确保高精度至关重要，但对某些任务阶段进行部分自动化对于提高生产力至关重要。我们提出了一种用于概率 VFs 的统一框架，该框架可以在手动夹具、半自动化夹具（由人处理精确任务）和完全自主之间无缝切换。我们引入了一种新颖的基于概率动力学系统的 VF 用于粗略引导，使机器人能够自主完成某些任务阶段，同时保持操作员在环。对于需要精确引导的任务，我们扩展了基于概率位置的轨迹夹具，使其具有自动化功能，从而实现无缝人机交互以及几何感知和最佳阻抗增益。对于需要非常精确引导的手动任务，我们还扩展了视觉伺服夹具，使其具有相同的几何感知和阻抗行为。我们通过在不同机器人上进行实验验证了我们的方法，展示了多种操作模式以及夹具编程的简易性。", "summary": "本文提出了一种用于概率虚拟夹具（VFs）的统一框架，该框架能动态调整触觉反馈和自动化水平以适应不同任务阶段。它通过引入新颖的基于概率动力学系统的VF用于粗略引导、扩展基于概率位置的轨迹夹具以实现精确人机交互，以及增强视觉伺服夹具以处理非常精确的手动任务，从而无缝整合了手动、半自动化和完全自主模式。该框架经过实验验证，旨在提高生产力同时保持人类精度和编程简易性。", "keywords": "概率虚拟夹具, 人机交互, 自动化, 动力学系统, 视觉伺服", "comments": "本文的创新之处在于其统一框架，该框架在概率虚拟夹具内无缝整合了不同级别的自主性（手动、半自动化、完全自主），解决了人类精度与机器人生产力之间的关键平衡问题。引入新颖的基于动力学系统的VF以及对现有轨迹和视觉伺服夹具的扩展，展示了自适应人机协作的全面方法。文中所述的编程简易性也表明了其在实际应用中的潜力。"}}
{"id": "2506.10100", "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models", "authors": ["Yantai Yang", "Yuhao Wang", "Zichen Wen", "Luo Zhongwei", "Chang Zou", "Zhipeng Zhang", "Chuan Wen", "Linfeng Zhang"], "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10100v1", "AI": {"title_translation": "EfficientVLA：面向视觉-语言-动作模型的免训练加速与压缩", "tldr": "EfficientVLA通过系统性地消除视觉-语言-动作（VLA）模型中的多方面冗余，实现了免训练的推理加速和压缩，显著提升了计算效率和内存利用率，同时保持了较低的性能下降。", "motivation": "视觉-语言-动作（VLA）模型，特别是基于扩散的架构，在具身智能方面展现出巨大潜力，但由于固有的和推理时的大量冗余，导致计算和内存需求巨大，严重阻碍了其实际部署。现有加速方法通常只针对孤立的低效问题，未能全面解决整个VLA流程中的计算和内存瓶颈。", "method": "本文提出了EfficientVLA，一个结构化且免训练的推理加速框架。它通过协同整合三种策略来系统性地消除冗余：1) 根据层间冗余分析，剪枝语言模块中功能不重要的层；2) 通过任务感知策略优化视觉处理路径，选择紧凑且多样化的视觉标记，平衡任务关键性和信息覆盖；3) 在迭代扩散的动作头部中，通过战略性缓存和重用关键中间特征，减轻时间计算冗余。", "result": "将EfficientVLA应用于标准VLA模型CogACT，在SIMPLER基准测试中，推理速度提升了1.93倍，FLOPs降低至28.9%，成功率仅下降了0.6%。", "conclusion": "EfficientVLA是一个有效的免训练加速框架，能够显著提高视觉-语言-动作模型的推理效率和内存利用率，同时保持较低的性能损失，从而增强了其实际部署能力。", "translation": "视觉-语言-动作（VLA）模型，特别是基于扩散的架构，在具身智能方面展现出变革性潜力，但由于固有的和推理时的大量冗余导致的高计算和内存需求，严重阻碍了其实际应用。虽然现有的加速工作通常针对孤立的低效问题，但这种零散的解决方案通常无法全面解决整个VLA管道中各种计算和内存瓶颈，从而限制了实际部署能力。我们引入了EfficientVLA，一个结构化且免训练的推理加速框架，通过协同利用多方面冗余来系统性地消除这些障碍。EfficientVLA协同整合了三种有针对性的策略：(1) 根据层间冗余分析，剪枝语言模块中功能不重要的层；(2) 通过任务感知策略优化视觉处理路径，选择紧凑且多样化的视觉标记，平衡任务关键性和信息覆盖；(3) 通过战略性缓存和重用关键中间特征，减轻迭代扩散的动作头部中的时间计算冗余。我们将我们的方法应用于标准VLA模型CogACT，实现了1.93倍的推理速度提升，FLOPs降低至28.9%，在SIMPLER基准测试中成功率仅下降0.6%。", "summary": "本文介绍了EfficientVLA，一个免训练的推理加速框架，旨在解决视觉-语言-动作（VLA）模型，特别是扩散模型，因高计算和内存需求而导致的部署难题。EfficientVLA通过整合三种策略来系统性地消除多方面冗余：剪枝语言模块层、优化视觉处理路径以及在动作头部缓存和重用特征。实验证明，EfficientVLA应用于CogACT模型后，实现了1.93倍的推理加速和FLOPs降低至28.9%，而成功率仅下降0.6%，证明了其在提高VLA模型效率方面的有效性。", "keywords": "视觉-语言-动作模型, 模型加速, 模型压缩, 免训练, 冗余消除", "comments": "EfficientVLA的创新之处在于其“免训练”和“整体性”的方法，能够系统性地解决VLA模型在整个流程中存在的计算和内存冗余问题，而非仅仅针对局部效率低下。在大幅提升推理速度和压缩模型规模的同时，仅带来微小的性能下降，这对于VLA模型在实际具身智能应用中的部署具有重要意义。"}}
{"id": "2506.10235", "title": "LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation", "authors": ["Chen-Chia Chang", "Wan-Hsuan Lin", "Yikang Shen", "Yiran Chen", "Xin Zhang"], "summary": "Automation of analog topology design is crucial due to customized\nrequirements of modern applications with heavily manual engineering efforts.\nThe state-of-the-art work applies a sequence-to-sequence approach and\nsupervised finetuning on language models to generate topologies given user\nspecifications. However, its circuit formulation is inefficient due to O(|V |2)\ntoken length and suffers from low precision sensitivity to numeric inputs. In\nthis work, we introduce LaMAGIC2, a succinct float-input canonical formulation\nwith identifier (SFCI) for language model-based analog topology generation.\nSFCI addresses these challenges by improving component-type recognition through\nidentifier-based representations, reducing token length complexity to O(|V |),\nand enhancing numeric precision sensitivity for better performance under tight\ntolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher\nsuccess rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a\nprior method. LaMAGIC2 also exhibits better transferability for circuits with\nmore vertices with up to 58.5% improvement. These advancements establish\nLaMAGIC2 as a robust framework for analog topology generation.", "comment": "Accepted at 42nd International Conference on Machine Learning (ICML)\n  2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10235v1", "AI": {"title_translation": "LaMAGIC2: 基于语言模型的模拟拓扑生成的高级电路公式", "tldr": "LaMAGIC2 提出了一种新的简洁的电路公式 SFCI，显著提高了基于语言模型的模拟拓扑生成在成功率、精度和可迁移性方面的性能。", "motivation": "模拟拓扑设计的自动化至关重要，因为现代应用有定制化需求，且主要依赖大量手动工程工作。现有方法存在令牌长度效率低下（O(|V|^2)）和对数值输入精度敏感度低的问题。", "method": "本文引入了 LaMAGIC2，这是一种用于基于语言模型的模拟拓扑生成的简洁浮点输入规范公式（SFCI）。SFCI 通过基于标识符的表示改进组件类型识别，将令牌长度复杂度降低到 O(|V|)，并增强了数值精度敏感度。", "result": "LaMAGIC2 在0.01的严格公差下，成功率提高了34%，MSEs降低了10倍。对于具有更多顶点的电路，可迁移性提高了58.5%。", "conclusion": "这些进步使 LaMAGIC2 成为一个强大的模拟拓扑生成框架。", "translation": "模拟拓扑设计的自动化至关重要，因为现代应用有定制化需求，且主要依赖大量手动工程工作。现有技术采用序列到序列的方法，并在语言模型上进行监督微调，以根据用户规范生成拓扑。然而，其电路公式效率低下，令牌长度为 O(|V|^2)，并且对数值输入的精度敏感度低。在这项工作中，我们引入了 LaMAGIC2，这是一种用于基于语言模型的模拟拓扑生成的简洁浮点输入规范公式（SFCI）。SFCI 通过基于标识符的表示改进组件类型识别，将令牌长度复杂度降低到 O(|V|)，并增强了数值精度敏感度，从而在严格公差下获得更好的性能。我们的实验表明，与现有方法相比，LaMAGIC2 在0.01的严格公差下，成功率提高了34%，MSEs降低了10倍。LaMAGIC2 还表现出更好的可迁移性，对于具有更多顶点的电路，可迁移性提高了58.5%。这些进步使 LaMAGIC2 成为一个强大的模拟拓扑生成框架。", "summary": "本文介绍了 LaMAGIC2，一种用于基于语言模型的模拟拓扑生成的新方法，其核心是简洁浮点输入规范公式（SFCI）。SFCI 通过改进组件识别、降低令牌长度复杂度和提高数值精度敏感度来解决现有方法的效率和精度问题。实验证明，LaMAGIC2 在成功率、精度和可迁移性方面显著优于现有方法，使其成为一个鲁棒的模拟拓扑生成框架。", "keywords": "模拟拓扑生成, 语言模型, 电路公式, SFCI, 自动化设计", "comments": "LaMAGIC2 的创新之处在于提出了 SFCI 公式，显著优化了语言模型在模拟电路拓扑生成中的效率和精度问题。通过将令牌长度复杂度从 O(|V|^2) 降低到 O(|V|)，并提高数值敏感度，它有效地克服了现有方法的瓶颈，对于推动模拟设计自动化具有重要意义。"}}
{"id": "2506.10060", "title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems", "authors": ["Brendan Leigh Ross", "Noël Vouitsis", "Atiyeh Ashari Ghomi", "Rasa Hosseinzadeh", "Ji Xin", "Zhaoyan Liu", "Yi Sui", "Shiyi Hou", "Kin Kwan Leung", "Gabriel Loaiza-Ganem", "Jesse C. Cresswell"], "summary": "Although large language models (LLMs) are becoming increasingly capable of\nsolving challenging real-world tasks, accurately quantifying their uncertainty\nremains a critical open problem, which limits their applicability in\nhigh-stakes domains. This challenge is further compounded by the closed-source,\nblack-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can\nbe highly sensitive to the prompts that bind them together, which often require\nsignificant manual tuning (i.e., prompt engineering). In this work, we address\nthese challenges by viewing LLM-based systems through a Bayesian lens. We\ninterpret prompts as textual parameters in a statistical model, allowing us to\nuse a small training dataset to perform Bayesian inference over these prompts.\nThis novel perspective enables principled uncertainty quantification over both\nthe model's textual parameters and its downstream predictions, while also\nincorporating prior beliefs about these parameters expressed in free-form text.\nTo perform Bayesian inference, a difficult problem even for well-studied data\nmodalities, we introduce Metropolis-Hastings through LLM Proposals (MHLP), a\nnovel Markov chain Monte Carlo (MCMC) algorithm that combines prompt\noptimization techniques with standard MCMC methods. MHLP is a turnkey\nmodification to existing LLM pipelines, including those that rely exclusively\non closed-source models. Empirically, we demonstrate that our method yields\nimprovements in both predictive accuracy and uncertainty quantification (UQ) on\na range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a\nviable path for incorporating methods from the rich Bayesian literature into\nthe era of LLMs, paving the way for more reliable and calibrated LLM-based\nsystems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10060v1", "AI": {"title_translation": "文本贝叶斯：量化基于大型语言模型的系统中的不确定性", "tldr": "大型语言模型（LLMs）的不确定性量化是一个关键问题，限制了其在高风险领域的应用。本文通过将提示（prompts）视为统计模型中的文本参数，并利用小型训练数据集进行贝叶斯推断，提出了“文本贝叶斯”方法。为实现贝叶斯推断，引入了Metropolis-Hastings through LLM Proposals (MHLP)算法。实验证明，该方法提高了预测准确性和不确定性量化能力，为LLM系统带来了更高的可靠性。", "motivation": "大型语言模型（LLMs）在解决现实世界任务方面能力日益增强，但准确量化其不确定性仍然是一个关键的开放问题，这限制了它们在高风险领域的适用性。此外，许多最先进的LLM是闭源的、黑盒性质，且基于LLM的系统对提示高度敏感，通常需要大量手动调整（即提示工程）。", "method": "本文通过贝叶斯视角审视基于LLM的系统，将提示解释为统计模型中的文本参数，从而能够使用小型训练数据集对这些提示进行贝叶斯推断。为了执行贝叶斯推断，本文引入了Metropolis-Hastings through LLM Proposals (MHLP)，这是一种新颖的马尔可夫链蒙特卡罗（MCMC）算法，它结合了提示优化技术和标准MCMC方法。MHLP是对现有LLM管道（包括那些依赖闭源模型的管道）的即插即用修改。", "result": "经验证明，我们的方法在一系列LLM基准测试和不确定性量化（UQ）任务中，在预测准确性和不确定性量化方面均有所改进。", "conclusion": "这项工作展示了将丰富的贝叶斯文献中的方法融入大型语言模型时代的可行途径，为更可靠和校准的基于LLM的系统铺平了道路。", "translation": "尽管大型语言模型（LLMs）在解决具有挑战性的现实世界任务方面能力日益增强，但准确量化其不确定性仍然是一个关键的开放问题，这限制了它们在高风险领域的适用性。这一挑战因许多最先进的LLM的闭源、黑盒性质而进一步加剧。此外，基于LLM的系统对将它们绑定在一起的提示高度敏感，这通常需要大量手动调整（即提示工程）。在这项工作中，我们通过贝叶斯视角审视基于LLM的系统来解决这些挑战。我们将提示解释为统计模型中的文本参数，从而能够使用小型训练数据集对这些提示执行贝叶斯推断。这种新颖的视角使得能够对模型的文本参数及其下游预测进行原则性的不确定性量化，同时还纳入了以自由形式文本表达的关于这些参数的先验信念。为了执行贝叶斯推断——即使对于经过充分研究的数据模态也是一个难题——我们引入了通过LLM提议的Metropolis-Hastings（MHLP），这是一种新颖的马尔可夫链蒙特卡罗（MCMC）算法，它结合了提示优化技术和标准MCMC方法。MHLP是对现有LLM管道的即插即用修改，包括那些完全依赖闭源模型的管道。经验证明，我们的方法在一系列LLM基准测试和不确定性量化（UQ）任务中，在预测准确性和不确定性量化方面均有所改进。更广泛地说，我们的工作展示了将丰富的贝叶斯文献中的方法融入大型语言模型时代的可行途径，为更可靠和校准的基于LLM的系统铺平了道路。", "summary": "本研究旨在解决大型语言模型（LLMs）在量化不确定性方面的挑战，这对于高风险应用至关重要，并因LLM的黑盒性质和对提示的敏感性而复杂化。论文提出“文本贝叶斯”框架，将提示视为统计模型中的文本参数，并通过小型训练数据集进行贝叶斯推断。为实现此目的，引入了一种名为Metropolis-Hastings through LLM Proposals (MHLP)的新型MCMC算法，该算法结合了提示优化技术，并可应用于包括闭源模型在内的现有LLM管道。实验结果表明，该方法显著提高了LLM系统的预测准确性和不确定性量化能力，为将贝叶斯方法整合到LLM领域以提升系统可靠性和校准性提供了可行途径。", "keywords": "文本贝叶斯, 不确定性量化, 大型语言模型, 贝叶斯推断, MCMC", "comments": "本文的创新之处在于其将贝叶斯推断应用于LLM系统，具体通过将提示（prompts）视为可进行贝叶斯推断的文本参数。引入MHLP算法，使得即使对于闭源LLM也能进行不确定性量化，这对于LLM在高风险场景中的部署具有重要意义。它为提升LLM的可靠性和可信度提供了新的视角和方法。"}}
{"id": "2506.10461", "title": "Automating Multi-Tenancy Performance Evaluation on Edge Compute Nodes", "authors": ["Joanna Georgiou", "Moysis Symeonides", "George Pallis", "Marios D. Dikaiakos"], "summary": "Edge Computing emerges as a promising alternative of Cloud Computing, with\nscalable compute resources and services deployed in the path between IoT\ndevices and Cloud. Since virtualization techniques can be applied on Edge\ncompute nodes, administrators can share their Edge infrastructures among\nmultiple users, providing the so-called multi-tenancy. Even though\nmulti-tenancy is unavoidable, it raises concerns about security and performance\ndegradation due to resource contention in Edge Computing. For that,\nadministrators need to deploy services with non-antagonizing profiles and\nexplore workload co-location scenarios to enhance performance and energy\nconsumption. Achieving this, however, requires extensive configuration,\ndeployment, iterative testing, and analysis, an effort-intensive and\ntime-consuming process. To address this challenge, we introduce an\nauto-benchmarking framework designed to streamline the analysis of\nmulti-tenancy performance in Edge environments. Our framework includes a\nbuilt-in monitoring stack and integrates with widely used benchmarking\nworkloads, such as streaming analytics, database operations, machine learning\napplications, and component-based stress testing. We perform a case-driven\nanalysis and provide valuable insights into the impact of multi-tenancy on Edge\nenvironments with different hardware configurations and diverse workloads.\nFinally, the implementation of our framework, along with the containerized\nworkloads used for experimentation, is publicly available.", "comment": "2025 IEEE International Conference on Edge Computing and\n  Communications (EDGE)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10461v1", "AI": {"title_translation": "边缘计算节点多租户性能评估自动化", "tldr": "本文介绍了一个自动基准测试框架，旨在简化和自动化边缘计算环境中多租户性能的分析，解决手动评估的复杂性和耗时性。", "motivation": "边缘计算中的多租户虽然不可避免，但会因资源争用导致性能下降和安全问题。传统的性能评估方法（配置、部署、迭代测试和分析）耗时耗力，效率低下。", "method": "本文提出了一个自动基准测试框架，该框架包含内置的监控堆栈，并集成了流分析、数据库操作、机器学习应用和基于组件的压力测试等常用基准测试工作负载。该框架通过案例驱动分析来评估多租户性能。", "result": "该框架为在不同硬件配置和多样化工作负载下，多租户对边缘环境的影响提供了宝贵的见解。此外，该框架的实现及其用于实验的容器化工作负载均已公开可用。", "conclusion": "所提出的自动基准测试框架能够有效地简化边缘计算节点上多租户性能的分析，提供了关键的洞察力，并为管理员优化边缘环境提供了实用的工具。", "translation": "边缘计算作为云计算的一种有前景的替代方案，在物联网设备和云之间的路径中部署了可扩展的计算资源和服务。由于虚拟化技术可以应用于边缘计算节点，管理员可以在多个用户之间共享其边缘基础设施，从而提供所谓的多租户。尽管多租户是不可避免的，但它在边缘计算中引发了对安全和由于资源争用导致的性能下降的担忧。为此，管理员需要部署具有非对抗性配置的服务，并探索工作负载协同定位场景以提高性能和降低能耗。然而，实现这一点需要大量的配置、部署、迭代测试和分析，这是一个耗费精力且耗时的过程。为了解决这一挑战，我们引入了一个自动基准测试框架，旨在简化边缘环境中多租户性能的分析。我们的框架包括一个内置的监控堆栈，并集成了广泛使用的基准测试工作负载，例如流分析、数据库操作、机器学习应用和基于组件的压力测试。我们进行了案例驱动的分析，并提供了关于多租户对具有不同硬件配置和多样化工作负载的边缘环境影响的宝贵见解。最后，我们框架的实现以及用于实验的容器化工作负载是公开可用的。", "summary": "本文提出了一个自动基准测试框架，旨在解决边缘计算多租户环境中性能评估的复杂性和耗时性。该框架集成了监控功能和多种基准测试工作负载（如流分析、数据库、机器学习等），能够自动化地评估不同硬件配置和工作负载下多租户对边缘环境的影响，并提供了有价值的洞察。其实现和实验工作负载均已公开。", "keywords": "多租户, 边缘计算, 性能评估, 自动基准测试, 工作负载协同定位", "comments": "本文的创新之处在于将边缘计算中复杂且耗时的多租户性能评估过程自动化。通过提供一个集成了监控和多种工作负载的框架，它极大地降低了管理员进行性能分析的门槛。框架的公开可用性进一步增加了其实用价值和潜在影响力，直接解决了边缘计算多租户部署中的一个关键痛点。"}}
{"id": "2506.10264", "title": "WGSR-Bench: Wargame-based Game-theoretic Strategic Reasoning Benchmark for Large Language Models", "authors": ["Qiyue Yin", "Pei Xu", "Qiaozhe Li", "Shengda Liu", "Shengqi Shen", "Tong Wang", "Yihong Han", "Xiaonan Zhao", "Likun Yang", "Shiyue Cao", "Shiyu Qiu", "Yuxuan Liu", "Shizhao Yu", "Lei Cui", "Chengxin Yan", "Jie Sun", "Xiangquan Tang", "Kaiqi Huang"], "summary": "Recent breakthroughs in Large Language Models (LLMs) have led to a\nqualitative leap in artificial intelligence' s performance on reasoning tasks,\nparticularly demonstrating remarkable capabilities in mathematical, symbolic,\nand commonsense reasoning. However, as a critical component of advanced human\ncognition, strategic reasoning, i.e., the ability to assess multi-agent\nbehaviors in dynamic environments, formulate action plans, and adapt\nstrategies, has yet to be systematically evaluated or modeled. To address this\ngap, this paper introduces WGSR-Bench, the first strategy reasoning benchmark\nfor LLMs using wargame as its evaluation environment. Wargame, a quintessential\nhigh-complexity strategic scenario, integrates environmental uncertainty,\nadversarial dynamics, and non-unique strategic choices, making it an effective\ntestbed for assessing LLMs' capabilities in multi-agent decision-making, intent\ninference, and counterfactual reasoning. WGSR-Bench designs test samples around\nthree core tasks, i.e., Environmental situation awareness, Opponent risk\nmodeling and Policy generation, which serve as the core S-POE architecture, to\nsystematically assess main abilities of strategic reasoning. Finally, an\nLLM-based wargame agent is designed to integrate these parts for a\ncomprehensive strategy reasoning assessment. With WGSR-Bench, we hope to assess\nthe strengths and limitations of state-of-the-art LLMs in game-theoretic\nstrategic reasoning and to advance research in large model-driven strategic\nintelligence.", "comment": "15 pages, 17 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10264v1", "AI": {"title_translation": "WGSR-Bench：基于兵棋推演的LLM博弈论战略推理基准", "tldr": "本文介绍了WGSR-Bench，这是首个基于兵棋推演环境的LLM战略推理基准，旨在系统评估LLM在复杂多智能体环境中的战略推理能力。", "motivation": "尽管大型语言模型（LLMs）在数学、符号和常识推理方面取得了显著进展，但作为人类高级认知关键组成部分的战略推理（即评估动态环境中多智能体行为、制定行动计划和调整策略的能力）尚未得到系统评估或建模。", "method": "本文引入了WGSR-Bench，这是第一个使用兵棋推演作为评估环境的LLM战略推理基准。兵棋推演被选为评估环境，因为它是一个典型的复杂战略场景，融合了环境不确定性、对抗动态和非唯一战略选择。WGSR-Bench围绕三个核心任务设计测试样本：环境态势感知、对手风险建模和策略生成，这些构成了核心的S-POE架构。此外，还设计了一个基于LLM的兵棋推演智能体来整合这些部分，进行全面的战略推理评估。", "result": "本文成功推出了WGSR-Bench，它能够系统地评估LLM在兵棋推演环境下进行多智能体决策、意图推理和反事实推理的能力。该基准有望揭示当前最先进LLM在博弈论战略推理方面的优势和局限性。", "conclusion": "WGSR-Bench旨在评估当前最先进的大型语言模型在博弈论战略推理方面的优势和局限性，并推动大型模型驱动的战略智能研究。", "translation": "大型语言模型（LLMs）最近的突破使人工智能在推理任务上的性能实现了质的飞跃，尤其在数学、符号和常识推理方面展现出卓越的能力。然而，作为人类高级认知的一个关键组成部分，战略推理，即评估动态环境中多智能体行为、制定行动计划和调整策略的能力，尚未得到系统评估或建模。为了解决这一空白，本文介绍了WGSR-Bench，这是首个使用兵棋推演作为评估环境的LLM战略推理基准。兵棋推演是一个典型的、高复杂度的战略场景，它融合了环境不确定性、对抗动态和非唯一战略选择，使其成为评估LLM在多智能体决策、意图推理和反事实推理方面能力的有效测试平台。WGSR-Bench围绕三个核心任务设计测试样本，即环境态势感知、对手风险建模和策略生成，它们构成了核心的S-POE架构，以系统地评估战略推理的主要能力。最后，设计了一个基于LLM的兵棋推演智能体来整合这些部分，进行全面的战略推理评估。通过WGSR-Bench，我们希望评估最先进LLM在博弈论战略推理方面的优势和局限性，并推进大型模型驱动的战略智能研究。", "summary": "本文提出了WGSR-Bench，这是首个针对大型语言模型（LLMs）的战略推理基准，旨在弥补LLMs在战略推理能力评估方面的空白。该基准以复杂的兵棋推演环境为核心，通过环境态势感知、对手风险建模和策略生成这三个S-POE核心任务，系统地评估LLMs在多智能体决策、意图推理和反事实推理等方面的能力。研究期望WGSR-Bench能揭示当前LLMs在博弈论战略推理上的优势与局限，并推动相关领域的研究。", "keywords": "大型语言模型, 战略推理, 兵棋推演, 基准测试, 博弈论", "comments": "WGSR-Bench的创新之处在于首次将兵棋推演这一复杂、动态且充满不确定性的场景作为LLM战略推理能力的评估环境，这比传统的单一推理任务更具挑战性和实际意义。其提出的S-POE架构为系统化评估战略推理能力提供了清晰的框架。该基准对于理解和提升LLM在高级认知和多智能体交互方面的能力具有重要推动作用，有望促进LLM在复杂决策和智能对抗领域的应用。"}}
{"id": "2506.10829", "title": "LLM-Driven Personalized Answer Generation and Evaluation", "authors": ["Mohammadreza Molavi", "Mohammadreza Tavakoli", "Mohammad Moein", "Abdolali Faraji", "Gábor Kismihók"], "summary": "Online learning has experienced rapid growth due to its flexibility and\naccessibility. Personalization, adapted to the needs of individual learners, is\ncrucial for enhancing the learning experience, particularly in online settings.\nA key aspect of personalization is providing learners with answers customized\nto their specific questions. This paper therefore explores the potential of\nLarge Language Models (LLMs) to generate personalized answers to learners'\nquestions, thereby enhancing engagement and reducing the workload on educators.\nTo evaluate the effectiveness of LLMs in this context, we conducted a\ncomprehensive study using the StackExchange platform in two distinct areas:\nlanguage learning and programming. We developed a framework and a dataset for\nvalidating automatically generated personalized answers. Subsequently, we\ngenerated personalized answers using different strategies, including 0-shot,\n1-shot, and few-shot scenarios. The generated answers were evaluated using\nthree methods: 1. BERTScore, 2. LLM evaluation, and 3. human evaluation. Our\nfindings indicated that providing LLMs with examples of desired answers (from\nthe learner or similar learners) can significantly enhance the LLMs' ability to\ntailor responses to individual learners' needs.", "comment": "This is the preprint version of a paper accepted at AIED 2025. The\n  final version will be published by Springer", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10829v1", "AI": {"title_translation": "LLM驱动的个性化答案生成与评估", "tldr": "本研究探讨了大型语言模型（LLMs）在在线学习中生成个性化答案的潜力，并通过在语言学习和编程领域的StackExchange数据进行评估，发现提供示例能显著提升LLMs的个性化能力。", "motivation": "在线学习的快速增长对个性化学习体验提出了需求，特别是在为学习者提供定制化答案方面。利用大型语言模型（LLMs）生成个性化答案可以提高学习参与度并减轻教育者的工作负担。", "method": "本研究使用StackExchange平台（语言学习和编程领域）进行了综合研究。开发了一个框架和数据集用于验证自动生成的个性化答案。使用0-shot、1-shot和few-shot策略生成个性化答案，并通过BERTScore、LLM评估和人工评估三种方法进行评估。", "result": "研究结果表明，向LLMs提供所需答案的示例（来自学习者或相似学习者）可以显著增强LLMs根据个体学习者需求定制响应的能力。", "conclusion": "通过提供示例，大型语言模型能够有效地生成个性化答案，从而提高在线学习体验中的学习者参与度并减轻教育者的负担。", "translation": "在线学习因其灵活性和可访问性而经历了快速增长。个性化，即适应个体学习者需求，对于增强学习体验至关重要，尤其是在线环境中。个性化的一个关键方面是为学习者提供根据其特定问题定制的答案。因此，本文探讨了大型语言模型（LLMs）生成学习者问题个性化答案的潜力，从而增强参与度并减轻教育者的工作负担。为了评估LLMs在此背景下的有效性，我们使用StackExchange平台在两个不同领域：语言学习和编程，进行了全面研究。我们开发了一个框架和数据集，用于验证自动生成的个性化答案。随后，我们使用不同的策略，包括0-shot、1-shot和few-shot场景，生成了个性化答案。生成的答案通过三种方法进行评估：1. BERTScore，2. LLM评估，和3. 人工评估。我们的研究结果表明，向LLMs提供所需答案的示例（来自学习者或相似学习者）可以显著增强LLMs根据个体学习者需求定制响应的能力。", "summary": "本研究探讨了大型语言模型（LLMs）在在线学习环境中生成个性化答案的潜力，旨在提升学习者参与度并减轻教育者负担。研究团队开发了一个框架和数据集，并在语言学习和编程的StackExchange数据上进行了实验。通过0-shot、1-shot和few-shot策略生成答案，并使用BERTScore、LLM评估和人工评估进行验证。结果显示，为LLMs提供示例（如来自学习者或类似学习者的答案）能显著提高其生成定制化响应的能力。", "keywords": "大型语言模型, 个性化答案生成, 在线学习, 评估, 示例学习", "comments": "本文的创新之处在于将LLMs应用于在线学习中的个性化答案生成，并系统地评估了不同提示策略的效果。其重要性体现在为提升在线学习体验提供了新的解决方案，有助于减轻教育者负担。通过使用实际平台数据和多种评估方法，研究结果具有较强的说服力。"}}
{"id": "2506.10309", "title": "DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction", "authors": ["Yuliang Zhu", "Jing Cheng", "Qi Xie", "Zhuo-Xu Cui", "Qingyong Zhu", "Yuanyuan Liu", "Xin Liu", "Jianfeng Ren", "Chengbo Wang", "Dong Liang"], "summary": "Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,\nincluding spatial rotation symmetry within individual frames and temporal\nsymmetry along the time dimension. Explicit incorporation of these symmetry\npriors in the reconstruction model can significantly improve image quality,\nespecially under aggressive undersampling scenarios. Recently, Equivariant\nconvolutional neural network (ECNN) has shown great promise in exploiting\nspatial symmetry priors. However, existing ECNNs critically fail to model\ntemporal symmetry, arguably the most universal and informative structural prior\nin dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep\nUnrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for\nDynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance\nthrough a (2+1)D equivariant convolutional architecture. In particular, it\nintegrates both the data consistency and proximal mapping module into a unified\ndeep unrolling framework. This architecture ensures rigorous propagation of\nspatiotemporal rotation symmetry constraints throughout the reconstruction\nprocess, enabling more physically accurate modeling of cardiac motion dynamics\nin cine MRI. In addition, a high-fidelity group filter parameterization\nmechanism is developed to maintain representation precision while enforcing\nsymmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets\ndemonstrate that DUN-SRE achieves state-of-the-art performance, particularly in\npreserving rotation-symmetric structures, offering strong generalization\ncapability to a broad range of dynamic MRI reconstruction tasks.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10309v1", "AI": {"title_translation": "DUN-SRE：用于动态MRI重建的深度展开网络，具有时空旋转等变性", "tldr": "提出DUN-SRE，一个利用时空旋转等变性的深度展开网络，显著提升动态MRI重建质量，尤其在心脏CINE MRI中表现优异。", "motivation": "现有等变卷积神经网络（ECNN）未能有效利用动态MRI中普遍且信息丰富的“时间对称性”先验，导致重建质量受限，尤其在激进欠采样情况下。", "method": "本文提出DUN-SRE，一个结合数据一致性和近端映射模块的深度展开框架。它通过(2+1)D等变卷积架构建立时空等变性，并开发了高保真群滤波器参数化机制以保持表示精度，确保时空旋转对称约束在重建过程中严格传播。", "result": "在心脏CINE MRI数据集上的综合实验表明，DUN-SRE实现了最先进的性能，尤其在保留旋转对称结构方面表现突出，并对广泛的动态MRI重建任务具有强大的泛化能力。", "conclusion": "DUN-SRE通过有效整合时空旋转等变性，显著提升了动态MRI重建的准确性和泛化能力，尤其适用于心脏运动动力学建模。", "translation": "动态磁共振成像（MRI）展现出变换对称性，包括单帧内的空间旋转对称性和沿时间维度的______。在重建模型中明确融入这些对称先验可以显著提高图像质量，尤其是在激进欠采样场景下。最近，等变卷积神经网络（ECNN）在利用空间对称先验方面显示出巨大潜力。然而，现有ECNN未能关键性地建模时间对称性，这可以说是动态MRI重建中最普遍和信息最丰富的结构先验。为了解决这个问题，我们提出了一种新颖的、用于动态MRI重建的具有时空旋转等变性深度展开网络（DUN-SRE）。DUN-SRE通过(2+1)D等变卷积架构建立时空等变性。特别是，它将数据一致性模块和近端映射模块集成到一个统一的深度展开框架中。这种架构确保了时空旋转对称约束在整个重建过程中严格传播，从而能够更物理准确地建模电影MRI中的心脏运动动力学。此外，还开发了一种高保真群滤波器参数化机制，以在强制执行对称约束的同时保持表示精度。在心脏CINE MRI数据集上的综合实验表明，DUN-SRE实现了最先进的性能，特别是在保留旋转对称结构方面，为广泛的动态MRI重建任务提供了强大的泛化能力。", "summary": "本论文提出了一种名为DUN-SRE的深度展开网络，用于动态MRI重建。该网络通过(2+1)D等变卷积架构，首次将空间旋转对称性和时间对称性相结合，建立了时空旋转等变性。DUN-SRE将数据一致性与近端映射模块集成到统一框架中，并引入高保真群滤波器参数化机制。实验证明，DUN-SRE在心脏CINE MRI数据集上达到了最先进的性能，尤其在保留旋转对称结构和泛化能力方面表现出色，解决了现有ECNN未能有效利用时间对称性的问题。", "keywords": "动态MRI重建, 深度展开网络, 时空等变性, 旋转对称性, 心脏CINE MRI", "comments": "该论文的创新点在于首次将时空旋转等变性引入到动态MRI重建的深度展开网络中，有效解决了现有ECNN未能建模时间对称性的局限。通过(2+1)D等变卷积和高保真群滤波器参数化，实现了对心脏运动动力学更物理准确的建模，并在欠采样条件下显著提升了图像质量和泛化能力，对动态MRI领域具有重要意义。"}}
{"id": "2506.10698", "title": "Disentangling Dual-Encoder Masked Autoencoder for Respiratory Sound Classification", "authors": ["Peidong Wei Shiyu Miao Lin Li"], "summary": "Deep neural networks have been applied to audio spectrograms for respiratory\nsound classification, but it remains challenging to achieve satisfactory\nperformance due to the scarcity of available data. Moreover, domain mismatch\nmay be introduced into the trained models as a result of the respiratory sound\nsamples being collected from various electronic stethoscopes, patient\ndemographics, and recording environments. To tackle this issue, we proposed a\nmodified MaskedAutoencoder(MAE) model, named Disentangling Dual-Encoder MAE\n(DDE-MAE) for respiratory sound classification. Two independent encoders were\ndesigned to capture disease-related and disease-irrelevant information\nseparately, achieving feature disentanglement to reduce the domain mismatch.\nOur method achieves a competitive performance on the ICBHI dataset.", "comment": "(Accepted at Interspeech 2025)", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10698v1", "AI": {"title_translation": "用于呼吸音分类的解耦双编码器掩蔽自编码器", "tldr": "本文提出了一种名为DDE-MAE的改进型掩蔽自编码器模型，通过设计两个独立的编码器来解耦疾病相关和无关信息，以解决呼吸音分类中数据稀缺和域不匹配的问题，并在ICBHI数据集上取得了有竞争力的性能。", "motivation": "由于可用数据稀缺，深度神经网络在呼吸音分类中难以达到令人满意的性能。此外，呼吸音样本来自不同的电子听诊器、患者人口统计数据和录音环境，可能导致训练模型中引入域不匹配。", "method": "我们提出了一种名为解耦双编码器掩蔽自编码器（DDE-MAE）的改进型掩蔽自编码器（MAE）模型。该模型设计了两个独立的编码器，分别捕获疾病相关和疾病无关的信息，实现特征解耦以减少域不匹配。", "result": "我们的方法在ICBHI数据集上取得了有竞争力的性能。", "conclusion": "DDE-MAE模型通过解耦疾病相关和无关特征有效解决了呼吸音分类中的数据稀缺和域不匹配问题，并在标准数据集上表现出色。", "translation": "深度神经网络已被应用于音频频谱图进行呼吸音分类，但由于可用数据稀缺，实现令人满意的性能仍然具有挑战性。此外，由于呼吸音样本是从各种电子听诊器、患者人口统计数据和录音环境中收集的，可能会在训练模型中引入域不匹配。为了解决这个问题，我们提出了一种名为解耦双编码器掩蔽自编码器（DDE-MAE）的改进型掩蔽自编码器（MAE）模型，用于呼吸音分类。设计了两个独立的编码器，分别捕获疾病相关和疾病无关的信息，实现特征解耦以减少域不匹配。我们的方法在ICBHI数据集上取得了有竞争力的性能。", "summary": "本研究针对呼吸音分类中数据稀缺和域不匹配的挑战，提出了一种名为DDE-MAE的改进型掩蔽自编码器模型。该模型通过设计两个独立的编码器，分别提取疾病相关和无关特征，从而实现特征解耦，有效降低了域不匹配的影响。实验结果表明，DDE-MAE在ICBHI数据集上取得了有竞争力的性能。", "keywords": "呼吸音分类, 掩蔽自编码器, 特征解耦, 域不匹配, DDE-MAE", "comments": "该论文的创新点在于提出了DDE-MAE模型，通过双编码器设计实现特征解耦，有效缓解了呼吸音分类中因数据稀缺和域不匹配带来的性能瓶颈。这种解耦思想对于处理多源异构数据具有借鉴意义，有助于提高模型的鲁棒性和泛化能力。"}}
{"id": "2506.10095", "title": "When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs", "authors": ["Xiao Li", "Joel Kreuzwieser", "Alan Peters"], "summary": "We investigate how large language models respond to prompts that differ only\nin their token-level realization but preserve the same semantic intent, a\nphenomenon we call prompt variance. We propose Prompt-Based Semantic Shift\n(PBSS), a diagnostic framework for measuring behavioral drift in LLMs under\nsemantically equivalent prompt rewordings. Applied to ten constrained tasks,\nPBSS reveals consistent, model-specific response shifts, suggesting statistical\nregularities linked to tokenization and decoding. These results highlight an\noverlooked dimension of model evaluation stability under rephrasing and suggest\nthat tokenization strategies and decoding dynamics may contribute to\npost-training quality of service instability.", "comment": "This paper was developed for presentation at ICML 2025 Tokshop\n  Workshop, but is now submitted as a standalone contribution", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10095v1", "AI": {"title_translation": "当意义不变，模型却漂移：评估大型语言模型在Token级行为不稳定性下的服务质量", "tldr": "大型语言模型在语义相同的提示语重述下会表现出模型特有的行为漂移，这与分词和解码有关，影响服务质量稳定性。", "motivation": "研究大型语言模型（LLMs）如何响应仅在Token层面不同但语义意图相同的提示语，这种现象被称为提示语变异。这突显了一个在复述下模型评估稳定性被忽视的维度。", "method": "提出了基于提示语的语义漂移（PBSS）诊断框架，用于测量大型语言模型在语义等效提示语重述下的行为漂移。该框架应用于十个受限任务。", "result": "PBSS揭示了持续的、模型特定的响应漂移，表明存在与分词和解码相关的统计规律。", "conclusion": "研究结果强调了复述下模型评估稳定性的一个被忽视的维度，并表明分词策略和解码动态可能导致训练后服务质量的不稳定性。", "translation": "我们研究了大型语言模型如何响应仅在Token层面实现不同但保留相同语义意图的提示语，这种现象我们称之为提示语变异。我们提出了基于提示语的语义漂移（PBSS），这是一个用于测量大型语言模型在语义等效提示语重述下行为漂移的诊断框架。将PBSS应用于十个受限任务，结果揭示了持续的、模型特定的响应漂移，表明存在与分词和解码相关的统计规律。这些结果突显了复述下模型评估稳定性的一个被忽视的维度，并表明分词策略和解码动态可能导致训练后服务质量的不稳定性。", "summary": "本研究探讨了大型语言模型（LLMs）在语义不变但Token层面不同的提示语（即提示语变异）下的响应。研究提出了一种名为Prompt-Based Semantic Shift (PBSS) 的诊断框架，用于量化LLMs在语义等效提示语重述下的行为漂移。通过在十个受限任务上的应用，PBSS揭示了模型特有的、一致的响应漂移，这与分词和解码过程中的统计规律有关。研究结果强调了在复述场景下模型评估稳定性的重要性，并指出分词策略和解码动态可能是导致模型训练后服务质量不稳定的原因。", "keywords": "大型语言模型, 提示语变异, 行为漂移, 分词, 服务质量", "comments": "本文通过引入“提示语变异”和“PBSS”框架，揭示了大型语言模型在语义不变但Token层面变化时，其行为会发生漂移这一此前被忽视的问题。其创新点在于从分词和解码动态的角度解释了这种不稳定性，为LLM的服务质量评估提供了新的维度和诊断工具，对于理解和提升LLM的鲁棒性具有重要意义。"}}
{"id": "2506.10468", "title": "Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On", "authors": ["Zaiqiang Wu", "Yechen Li", "Jingyuan Liu", "Yuki Shibata", "Takayuki Hori", "I-Chao Shen", "Takeo Igarashi"], "summary": "Existing image-based virtual try-on methods are often limited to the front\nview and lack real-time performance. While per-garment virtual try-on methods\nhave tackled these issues by capturing per-garment datasets and training\nper-garment neural networks, they still encounter practical limitations: (1)\nthe robotic mannequin used to capture per-garment datasets is prohibitively\nexpensive for widespread adoption and fails to accurately replicate natural\nhuman body deformation; (2) the synthesized garments often misalign with the\nhuman body. To address these challenges, we propose a low-barrier approach for\ncollecting per-garment datasets using real human bodies, eliminating the\nnecessity for a customized robotic mannequin. We also introduce a hybrid person\nrepresentation that enhances the existing intermediate representation with a\nsimplified DensePose map. This ensures accurate alignment of synthesized\ngarment images with the human body and enables human-garment interaction\nwithout the need for customized wearable devices. We performed qualitative and\nquantitative evaluations against other state-of-the-art image-based virtual\ntry-on methods and conducted ablation studies to demonstrate the superiority of\nour method regarding image quality and temporal consistency. Finally, our user\nstudy results indicated that most participants found our virtual try-on system\nhelpful for making garment purchasing decisions.", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10468v1", "AI": {"title_translation": "使用真实人体进行低门槛数据集收集以实现交互式单件服装虚拟试穿", "tldr": "本文提出了一种使用真实人体收集单件服装数据集的低门槛方法，并引入了一种混合人体表示，以解决现有虚拟试穿方法中机器人模特成本高昂、人体变形不自然以及服装对齐不准确的问题，提高了图像质量和时间一致性，并被用户认为有助于购物决策。", "motivation": "现有基于图像的虚拟试穿方法通常局限于正面视角且缺乏实时性。虽然单件服装虚拟试穿方法通过捕捉单件服装数据集和训练单件服装神经网络解决了这些问题，但它们仍面临实际限制：(1) 用于捕捉单件服装数据集的机器人模特成本过高，难以广泛采用，且无法准确复制自然人体变形；(2) 合成的服装经常与人体未对齐。", "method": "我们提出了一种使用真实人体收集单件服装数据集的低门槛方法，无需定制机器人模特。我们还引入了一种混合人体表示，通过简化的DensePose图增强了现有的中间表示。这确保了合成服装图像与人体的精确对齐，并实现了人衣交互而无需定制可穿戴设备。", "result": "我们对其他最先进的基于图像的虚拟试穿方法进行了定性和定量评估，并进行了消融研究，以证明我们的方法在图像质量和时间一致性方面的优越性。此外，我们的用户研究结果表明，大多数参与者认为我们的虚拟试穿系统有助于做出服装购买决策。", "conclusion": "本文提出的低门槛数据集收集方法和混合人体表示有效地解决了现有虚拟试穿技术的局限性，实现了准确的服装对齐和人衣交互。实验结果表明，该方法在图像质量和时间一致性方面优于现有技术，并且用户研究证实了其在服装购买决策中的实用价值。", "translation": "现有基于图像的虚拟试穿方法通常局限于正面视角且缺乏实时性。虽然单件服装虚拟试穿方法通过捕捉单件服装数据集和训练单件服装神经网络解决了这些问题，但它们仍面临实际限制：(1) 用于捕捉单件服装数据集的机器人模特成本过高，难以广泛采用，且无法准确复制自然人体变形；(2) 合成的服装经常与人体未对齐。为了解决这些挑战，我们提出了一种使用真实人体收集单件服装数据集的低门槛方法，无需定制机器人模特。我们还引入了一种混合人体表示，通过简化的DensePose图增强了现有的中间表示。这确保了合成服装图像与人体的精确对齐，并实现了人衣交互而无需定制可穿戴设备。我们对其他最先进的基于图像的虚拟试穿方法进行了定性和定量评估，并进行了消融研究，以证明我们的方法在图像质量和时间一致性方面的优越性。最后，我们的用户研究结果表明，大多数参与者认为我们的虚拟试穿系统有助于做出服装购买决策。", "summary": "本文提出了一种用于交互式单件服装虚拟试穿的低门槛数据集收集方法，该方法利用真实人体而非昂贵的机器人模特来捕捉数据，并能更准确地复制人体变形。为解决合成服装与人体对齐不佳的问题，作者引入了一种混合人体表示，通过结合简化的DensePose图来增强现有中间表示，从而实现精确对齐和无需定制可穿戴设备的人衣交互。通过与现有技术的定性、定量评估以及消融研究，证明了该方法在图像质量和时间一致性上的优越性。用户研究也表明，该系统对用户的服装购买决策有帮助。", "keywords": "虚拟试穿, 数据集收集, 真实人体, 混合表示, DensePose", "comments": "该论文的创新点在于提出了一个低成本、高效率的数据集收集方案，通过使用真实人体取代昂贵的机器人模特，极大地降低了虚拟试穿技术的部署门槛。同时，引入混合人体表示结合简化DensePose图，有效解决了虚拟试穿中服装与人体对齐的关键挑战，并实现了自然的交互。这项工作对于推动虚拟试穿技术在商业领域的广泛应用具有重要意义，尤其是在电商和服装零售行业。"}}
{"id": "2506.10487", "title": "SHORE: A Long-term User Lifetime Value Prediction Model in Digital Games", "authors": ["Shuaiqi Sun", "Congde Yuan", "Haoqiang Yang", "Mengzhuo Guo", "Guiying Wei", "Jiangbo Tian"], "summary": "In digital gaming, long-term user lifetime value (LTV) prediction is\nessential for monetization strategy, yet presents major challenges due to\ndelayed payment behavior, sparse early user data, and the presence of\nhigh-value outliers. While existing models typically rely on either short-cycle\nobservations or strong distributional assumptions, such approaches often\nunderestimate long-term value or suffer from poor robustness. To address these\nissues, we propose SHort-cycle auxiliary with Order-preserving REgression\n(SHORE), a novel LTV prediction framework that integrates short-horizon\npredictions (e.g., LTV-15 and LTV-30) as auxiliary tasks to enhance long-cycle\ntargets (e.g., LTV-60). SHORE also introduces a hybrid loss function combining\norder-preserving multi-class classification and a dynamic Huber loss to\nmitigate the influence of zero-inflation and outlier payment behavior.\nExtensive offline and online experiments on real-world datasets demonstrate\nthat SHORE significantly outperforms existing baselines, achieving a 47.91\\%\nrelative reduction in prediction error in online deployment. These results\nhighlight SHORE's practical effectiveness and robustness in industrial-scale\nLTV prediction for digital games.", "comment": "7 pages", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10487v1", "AI": {"title_translation": "SHORE：一种数字游戏中长期用户生命周期价值预测模型", "tldr": "SHORE模型通过结合短期预测和混合损失函数，显著提高了数字游戏中长期用户生命周期价值（LTV）的预测准确性和鲁棒性。", "motivation": "数字游戏中长期用户生命周期价值（LTV）预测对于变现策略至关重要，但由于支付行为延迟、早期用户数据稀疏以及高价值异常值的存在而面临重大挑战。现有模型依赖短期观测或强假设，导致低估长期价值或鲁棒性差。", "method": "本文提出了SHORE（SHort-cycle auxiliary with Order-preserving REgression）框架，这是一种新颖的LTV预测框架。它通过将短期预测（如LTV-15和LTV-30）作为辅助任务来增强长期目标（如LTV-60）。此外，SHORE还引入了一种混合损失函数，结合了保序多类分类和动态Huber损失，以减轻零膨胀和异常支付行为的影响。", "result": "在真实世界数据集上进行的广泛离线和在线实验表明，SHORE显著优于现有基线，在在线部署中预测误差相对降低了47.91%。", "conclusion": "这些结果突出了SHORE在数字游戏工业级LTV预测中的实际有效性和鲁棒性。", "translation": "在数字游戏中，长期用户生命周期价值（LTV）预测对于变现策略至关重要，但由于支付行为延迟、早期用户数据稀疏以及高价值异常值的存在而面临重大挑战。虽然现有模型通常依赖于短期观测或强烈的分布假设，但这些方法往往低估了长期价值或鲁棒性差。为了解决这些问题，我们提出了SHort-cycle auxiliary with Order-preserving REgression (SHORE)，这是一种新颖的LTV预测框架，它将短期预测（例如LTV-15和LTV-30）作为辅助任务来增强长期目标（例如LTV-60）。SHORE还引入了一种混合损失函数，结合了保序多类分类和动态Huber损失，以减轻零膨胀和异常支付行为的影响。在真实世界数据集上进行的广泛离线和在线实验表明，SHORE显著优于现有基线，在在线部署中预测误差相对降低了47.91%。这些结果突出了SHORE在数字游戏工业级LTV预测中的实际有效性和鲁棒性。", "summary": "本文提出了SHORE（SHort-cycle auxiliary with Order-preserving REgression），一种用于数字游戏中长期用户生命周期价值（LTV）预测的新颖框架。针对现有模型在处理支付延迟、数据稀疏和异常值方面表现不佳的问题，SHORE通过整合短期预测作为辅助任务来增强长期预测，并引入结合保序多类分类和动态Huber损失的混合损失函数。实验证明，SHORE在实际应用中显著提高了预测准确性，展现了其在工业级LTV预测中的有效性和鲁棒性。", "keywords": "用户生命周期价值, LTV预测, 数字游戏, 混合损失函数, 机器学习", "comments": "SHORE的创新之处在于其结合了短期辅助任务和专门设计的混合损失函数，以有效应对数字游戏LTV预测中特有的数据稀疏和异常值问题。这种方法显著提升了预测的准确性和鲁棒性，对于游戏行业的变现策略具有重要的实践价值。"}}
{"id": "2506.10428", "title": "Penalty-Based Feedback Control and Finite Element Analysis for the Stabilization of Nonlinear Reaction-Diffusion Equations", "authors": ["Sudeep Kundu", "Shishu pal Singh"], "summary": "In this work, first we employ the penalization technique to analyze the\nDirichlet boundary feedback control problem pertaining to reaction-diffusion\nequation. We establish the stabilization result of the equivalent Robin problem\nin the \\(H^{2}\\)-norm with respect to the penalty parameter. Furthermore, we\nprove that the solution of the penalized control problem converges to the\ncorresponding solution of the Dirichlet boundary feedback control problem as\nthe penalty parameter \\(\\epsilon\\) approaches zero. A \\(C^{0}\\)-conforming\nfinite element method is applied to this problem for the spatial variable while\nkeeping the time variable continuous. We discuss the stabilization of the\nsemi-discrete scheme for the penalized control problem and present an error\nanalysis of its solution. Finally, we validate our theoretical findings through\nnumerical experiments.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10428v1", "AI": {"title_translation": "惩罚型反馈控制与有限元分析在非线性反应扩散方程稳定化中的应用", "tldr": "本文利用惩罚技术和有限元方法研究非线性反应扩散方程的Dirichlet边界反馈控制问题，证明了其稳定化和收敛性，并进行了数值验证。", "motivation": "分析和解决反应扩散方程的Dirichlet边界反馈控制问题，特别是利用惩罚技术实现其稳定化。", "method": "1. 采用惩罚技术将Dirichlet边界反馈控制问题转化为等价的Robin问题。2. 应用C⁰协调有限元方法处理空间变量，时间变量保持连续（半离散方案）。3. 讨论半离散方案的稳定化并进行误差分析。", "result": "1. 建立了等价Robin问题在H²范数下关于惩罚参数的稳定化结果。2. 证明了当惩罚参数ε趋近于零时，惩罚控制问题的解收敛于Dirichlet边界反馈控制问题的相应解。3. 通过数值实验验证了理论发现。", "conclusion": "本文成功地利用惩罚技术和有限元方法实现了非线性反应扩散方程的稳定化控制，并通过理论分析和数值实验验证了方法的有效性和结果的正确性。", "translation": "在这项工作中，首先我们采用惩罚技术来分析与反应扩散方程相关的Dirichlet边界反馈控制问题。我们建立了等价Robin问题在H²范数下关于惩罚参数的稳定化结果。此外，我们证明了当惩罚参数ε趋近于零时，惩罚控制问题的解收敛于Dirichlet边界反馈控制问题的相应解。针对该问题，空间变量应用了C⁰协调有限元方法，同时时间变量保持连续。我们讨论了惩罚控制问题半离散方案的稳定化，并给出了其解的误差分析。最后，我们通过数值实验验证了我们的理论发现。", "summary": "本文研究非线性反应扩散方程的Dirichlet边界反馈控制问题。通过引入惩罚技术，将其转化为Robin问题，并证明了在H²范数下的稳定化以及惩罚解的收敛性。研究还采用了C⁰协调有限元方法对空间变量进行半离散化，并进行了稳定性分析和误差分析。数值实验验证了理论结果的有效性。", "keywords": "反应扩散方程, 反馈控制, 惩罚技术, 有限元方法, 稳定化", "comments": "本文创新性地将惩罚技术应用于反应扩散方程的Dirichlet边界反馈控制问题，并通过严谨的数学分析（H²范数稳定化、收敛性证明）和数值方法（C⁰有限元）验证了其有效性，为非线性偏微分方程的控制提供了新的思路。"}}
{"id": "2506.10423", "title": "PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs", "authors": ["Tony Alex", "Wish Suharitdamrong", "Sara Atito", "Armin Mustafa", "Philip J. B. Jackson", "Imran Razzak", "Muhammad Awais"], "summary": "The integration of audio perception capabilities into Large Language Models\n(LLMs) has enabled significant advances in Audio-LLMs. Although\napplication-focused developments, particularly in curating training data for\nspecific capabilities e.g., audio reasoning, have progressed rapidly, the\nunderlying mechanisms that govern efficient transfer of rich semantic\nrepresentations from audio encoders to LLMs remain under-explored. We\nconceptualize effective audio-LLM interaction as the LLM's ability to\nproficiently probe the audio encoder representations to satisfy textual\nqueries. This paper presents a systematic investigation on how architectural\ndesign choices can affect that. Beginning with a standard Pengi/LLaVA-style\naudio-LLM architecture, we propose and evaluate several modifications guided by\nhypotheses derived from mechanistic interpretability studies and LLM\noperational principles. Our experiments demonstrate that: (1) delaying audio\nintegration until the LLM's initial layers establish textual context that\nenhances its ability to probe the audio representations for relevant\ninformation; (2) the LLM can proficiently probe audio representations\nexclusively through LLM layer's attention submodule, without requiring\npropagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently\nintegrated ensemble of diverse audio encoders provides richer, complementary\nrepresentations, thereby broadening the LLM's capacity to probe a wider\nspectrum of audio information. All hypotheses are evaluated using an identical\nthree-stage training curriculum on a dataset of 5.6 million audio-text pairs,\nensuring controlled comparisons. Our final architecture, which incorporates all\nproposed modifications, achieves relative improvements from 10\\% to 60\\% over\nthe baseline, validating our approach to optimizing cross-modal information\ntransfer in audio-LLMs. Project page: https://ta012.github.io/PAL/", "comment": "21 pages, 11 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10423v1", "AI": {"title_translation": "PAL: 通过LLM探测音频编码器——音频编码器到LLM信息传输的研究", "tldr": "该研究探讨了如何有效地将音频信息从编码器传输到大型语言模型（LLMs），发现延迟集成、仅通过注意力子模块探测以及使用多样化音频编码器集成能显著提升性能。", "motivation": "尽管音频-LLM的应用发展迅速，但控制音频编码器到LLM之间丰富的语义表示高效传输的底层机制仍未得到充分探索。本研究旨在系统地调查架构设计选择如何影响这种传输。", "method": "本研究以标准的Pengi/LLaVA风格音频-LLM架构为基础，提出并评估了几项修改，这些修改受到机制可解释性研究和LLM操作原理的假设指导。所有假设均在包含560万音频-文本对的数据集上，使用相同的三阶段训练课程进行评估，以确保受控比较。", "result": "1. 将音频集成推迟到LLM的初始层建立文本上下文之后，可以增强其探测相关音频表示的能力；2. LLM可以通过其注意力子模块熟练地探测音频表示，而无需传播到其前馈网络（FFN）子模块；3. 有效集成的多样化音频编码器集成提供了更丰富、互补的表示，从而拓宽了LLM探测更广泛音频信息的能力。最终的架构，结合了所有提出的修改，比基线模型实现了10%到60%的相对改进。", "conclusion": "本研究提出的架构修改，包括延迟集成、仅通过注意力探测和集成编码器，显著优化了音频-LLM中的跨模态信息传输，并得到了实质性性能改进的验证。", "translation": "音频感知能力与大型语言模型（LLMs）的集成，使得音频-LLMs取得了显著进展。尽管以应用为中心的发展，特别是在为特定能力（例如音频推理）策划训练数据方面进展迅速，但控制音频编码器到LLMs之间丰富语义表示高效传输的底层机制仍未得到充分探索。我们将有效的音频-LLM交互概念化为LLM熟练探测音频编码器表示以满足文本查询的能力。本文系统地调查了架构设计选择如何影响这一点。从标准的Pengi/LLaVA风格音频-LLM架构开始，我们提出并评估了几项修改，这些修改受到机制可解释性研究和LLM操作原理的假设指导。我们的实验表明：(1) 将音频集成推迟到LLM的初始层建立文本上下文之后，可以增强其探测相关音频表示的能力；(2) LLM可以通过其注意力子模块熟练地探测音频表示，而无需传播到其前馈网络（FFN）子模块；(3) 有效集成的多样化音频编码器集成提供了更丰富、互补的表示，从而拓宽了LLM探测更广泛音频信息的能力。所有假设均在包含560万音频-文本对的数据集上，使用相同的三阶段训练课程进行评估，以确保受控比较。我们的最终架构，结合了所有提出的修改，比基线模型实现了10%到60%的相对改进，验证了我们优化音频-LLM中跨模态信息传输的方法。项目页面：https://ta012.github.io/PAL/", "summary": "这篇题为“PAL”的论文系统地研究了从音频编码器到大型语言模型（LLMs）高效信息传输的架构设计选择。作者从标准的音频-LLM架构入手，基于可解释性研究的假设，提出并评估了多项修改。研究发现，延迟音频集成、仅通过LLM的注意力子模块进行探测，以及采用多样化音频编码器的集成，能显著增强LLM提取相关音频信息的能力。这些修改在包含560万音频-文本对的数据集上，使模型性能比基线提高了10%至60%，验证了其优化跨模态信息传输的方法。", "keywords": "音频-LLM, 信息传输, 音频编码器, 大型语言模型, 架构设计", "comments": "这篇论文深入探讨了音频-LLM的底层机制，而非仅仅停留在应用层面，具有重要的创新性。其系统性地研究信息传输过程，并识别出关键的架构修改（延迟集成、仅注意力探测、集成编码器）是其亮点。在大型数据集上进行的受控实验进一步增强了研究结果的可靠性，为设计更高效的音频-LLM提供了实用的指导。"}}
{"id": "2506.10374", "title": "Optimal Non-Adaptive Group Testing with One-Sided Error Guarantees", "authors": ["Daniel McMorrow", "Jonathan Scarlett"], "summary": "The group testing problem consists of determining a sparse subset of\ndefective items from within a larger set of items via a series of tests, where\neach test outcome indicates whether at least one defective item is included in\nthe test. We study the approximate recovery setting, where the recovery\ncriterion of the defective set is relaxed to allow a small number of items to\nbe misclassified. In particular, we consider one-sided approximate recovery\ncriteria, where we allow either only false negative or only false positive\nmisclassifications. Under false negatives only (i.e., finding a subset of\ndefectives), we show that there exists an algorithm matching the optimal\nthreshold of two-sided approximate recovery. Under false positives only (i.e.,\nfinding a superset of the defectives), we provide a converse bound showing that\nthe better of two existing algorithms is optimal.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10374v1", "AI": {"title_translation": "单边误差保证下的最优非自适应群组测试", "tldr": "该论文研究了在允许少量误分类的近似恢复设置下，具有单边误差（仅假阴性或仅假阳性）的群组测试问题，并展示了在这些条件下算法的最优性。", "motivation": "群组测试问题旨在通过一系列测试从大量物品中确定稀疏的缺陷物品子集。本文研究的是在允许少量物品被错误分类的近似恢复设置，特别是单边误差（仅假阴性或仅假阳性）情况下的问题。", "method": "本研究考虑了两种单边近似恢复准则：仅假阴性（即找到缺陷物品的子集）和仅假阳性（即找到缺陷物品的超集）。在仅假阴性情况下，作者展示了存在一种算法能够匹配双边近似恢复的最优阈值。在仅假阳性情况下，作者提供了一个反向界限，表明两种现有算法中较优者是最佳的。", "result": "在仅允许假阴性误分类的情况下，存在一种算法能够匹配双边近似恢复的最优阈值。在仅允许假阳性误分类的情况下，证明了两种现有算法中较优者是最佳的。", "conclusion": "在单边误差保证下的非自适应群组测试中，无论是仅允许假阴性还是仅允许假阳性，都存在达到最优性能的算法或现有算法的最优性被证实。", "translation": "群组测试问题在于通过一系列测试，从一个更大的物品集合中确定一个稀疏的缺陷物品子集，其中每次测试结果表明测试中是否包含至少一个缺陷物品。我们研究近似恢复设置，其中缺陷集合的恢复标准被放宽，以允许少量物品被错误分类。特别是，我们考虑单边近似恢复标准，即我们只允许假阴性或只允许假阳性误分类。在仅假阴性（即找到缺陷物的子集）的情况下，我们表明存在一种算法，其性能匹配双边近似恢复的最优阈值。在仅假阳性（即找到缺陷物的超集）的情况下，我们提供了一个反向界限，表明两种现有算法中较优者是最佳的。", "summary": "本论文探讨了在近似恢复设置下，具有单边误差保证（仅假阴性或仅假阳性）的非自适应群组测试问题。研究发现，在仅允许假阴性时，存在一种算法能达到与双边近似恢复相同的最优阈值。而在仅允许假阳性时，两种现有算法中表现较优者被证明是最优的。这为群组测试在特定误差约束下的理论最优性提供了见解。", "keywords": "群组测试, 单边误差, 近似恢复, 最优性, 非自适应", "comments": "这篇论文在群组测试领域做出了理论贡献，尤其是在考虑单边误差保证的近似恢复设置下。其创新之处在于证明了在这些特定条件下现有或新算法能够达到最优性能，这对于理解群组测试的极限和设计更高效的算法具有重要意义。它专注于理论界限和最优性证明，而不是提出新的实用算法。"}}
{"id": "2506.10422", "title": "A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data", "authors": ["Mayank Patel", "Minal Bhise"], "summary": "Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.10422v1", "AI": {"title_translation": "一种用于科学实验数据资源高效查询的混合启发式框架", "tldr": "RAW-HF是一个轻量级的混合框架，它通过优化资源利用率来高效查询科学实验原始数据，与传统DBMS相比，可显著减少工作负载执行时间，并比现有技术更好地优化资源利用。", "motivation": "传统数据库管理系统（DBMS）和HTAP系统在查询执行前加载整个数据集需要大量时间与资源。原地引擎可能多次重新解析所需数据，增加资源利用率和数据处理成本。此外，资源过度或不足分配也会增加应用程序运行成本。", "method": "本文提出了一种轻量级的资源可用性与工作负载感知混合框架（RAW-HF），通过有效利用现有有限资源来优化原始数据查询。RAW-HF包含有助于优化执行给定工作负载所需资源并最大限度利用现有资源的模块。文中还比较了RAW-HF使用的MUAR技术与基于机器学习的资源分配技术（如PCC）。", "result": "将RAW-HF应用于Sloan Digital Sky Survey (SDSS) 和 Linked Observation Data (LOD) 等真实世界科学数据集工作负载，与广泛使用的传统DBMS PostgreSQL相比，工作负载执行时间（WET）分别减少了90%和85%以上。与最先进的工作负载感知部分加载技术（WA）相比，RAW-HF将整体CPU、IO资源利用率和WET分别降低了26%、25%和26%，同时内存利用率提高了33%。", "conclusion": "RAW-HF框架能够显著优化科学实验数据的查询效率和资源利用率，超越了传统数据库系统和现有的混合系统部分加载技术。", "translation": "科学实验和现代应用程序每天都会生成大量数据。大多数组织利用内部服务器或云资源来管理应用程序数据和工作负载。传统的数据库管理系统（DBMS）和HTAP系统在开始查询执行之前，需要花费大量时间和资源来将整个数据集加载到DBMS中。另一方面，原地引擎可能会多次重新解析所需数据，从而增加资源利用率和数据处理成本。此外，资源的过度或不足分配也会增加应用程序的运行成本。本文提出了一种轻量级的资源可用性与工作负载感知混合框架（RAW-HF），通过有效利用现有有限资源来优化原始数据查询。RAW-HF包含有助于优化执行给定工作负载所需资源并最大限度利用现有资源的模块。将RAW-HF应用于Sloan Digital Sky Survey (SDSS) 和 Linked Observation Data (LOD) 等真实世界科学数据集工作负载，与广泛使用的传统DBMS PostgreSQL相比，工作负载执行时间（WET）分别减少了90%和85%以上。与针对混合系统提出的最先进的工作负载感知部分加载技术（WA）相比，整体CPU、IO资源利用率和WET分别降低了26%、25%和26%，同时内存利用率提高了33%。文章还比较了RAW-HF使用的MUAR技术与基于机器学习的资源分配技术（如PCC）。", "summary": "该论文提出了一种名为RAW-HF的轻量级混合框架，旨在解决传统数据库和原地引擎在处理大规模科学实验数据时存在的资源效率低下问题。RAW-HF通过优化资源分配和最大化现有资源利用率，实现了对原始数据的高效查询。实验结果表明，与传统DBMS PostgreSQL相比，RAW-HF能显著减少工作负载执行时间（如SDSS和LOD数据集上分别减少90%和85%以上），并且相对于现有工作负载感知部分加载技术，在CPU、IO利用率和WET方面有显著降低，同时提高了内存利用率。", "keywords": "混合框架, 资源效率, 科学数据查询, RAW-HF, 工作负载优化", "comments": "该论文提出了一种创新的混合启发式框架RAW-HF，其主要创新点在于结合了资源可用性和工作负载感知机制来优化大规模科学实验数据的查询。其重要性在于显著提高了数据查询效率并降低了资源消耗，解决了传统数据库和现有混合系统在处理海量原始数据时的瓶颈。通过在真实世界数据集上的显著性能提升，证明了该方法的实用性和有效性。"}}
{"id": "2506.10880", "title": "Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective", "authors": ["V. Giunzioni", "A. Merlini", "F. P. Andriulli"], "summary": "When modeling propagation and scattering phenomena using integral equations\ndiscretized by the boundary element method, it is common practice to\napproximate the boundary of the scatterer with a mesh comprising elements of\nsize approximately equal to a fraction of the wavelength $\\lambda$ of the\nincident wave, e.g., $\\lambda/10$. In this work, by analyzing the spectra of\nthe operator matrices, we show a discrepancy with respect to the continuous\noperators which grows with the simulation frequency, challenging the common\nbelief that the aforementioned widely used discretization approach is\nsufficient to maintain the accuracy of the solution constant when increasing\nthe frequency.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.10880v1", "AI": {"title_translation": "三维离散边界积分算子的谱分析：高频视角", "tldr": "研究发现，在三维高频模拟中，常用的边界元方法离散化会导致离散算子与连续算子之间出现随频率增加而增大的差异。", "motivation": "在使用边界元方法离散化的积分方程模拟传播和散射现象时，存在一种普遍的观点，认为当前广泛使用的离散化方法足以在频率增加时保持解的精度不变。本文的动机在于挑战这种普遍的观点。", "method": "通过分析算子矩阵的频谱。", "result": "研究发现离散算子与连续算子之间存在差异，且这种差异随模拟频率的增加而增大。", "conclusion": "常用的边界元方法离散化策略（例如网格单元尺寸约为波长λ/10）在高频情况下可能不足以维持解的精度。", "translation": "当使用边界元方法离散化的积分方程模拟传播和散射现象时，通常的做法是用网格来近似散射体的边界，网格单元的尺寸大约等于入射波波长λ的一小部分，例如λ/10。在这项工作中，通过分析算子矩阵的频谱，我们发现离散算子与连续算子之间存在差异，这种差异随着模拟频率的增加而增大，这挑战了普遍的观点，即上述广泛使用的离散化方法足以在频率增加时保持解的精度不变。", "summary": "本文研究了三维离散边界积分算子的谱特性，发现在使用边界元方法进行高频模拟时，常用的网格离散化策略会导致离散算子与连续算子之间出现随频率增加而增大的差异。这一发现挑战了业界普遍认为该离散方法能维持高频解精度的观念。", "keywords": "边界积分算子, 边界元方法, 频谱分析, 高频模拟, 离散化误差", "comments": "这篇论文的重要性在于它挑战了一个在边界元方法（BEM）中长期存在的、被广泛接受的离散化实践。通过对算子矩阵频谱的深入分析，作者揭示了在高频情境下离散误差的增长趋势，这对于需要高精度模拟的工程和科学领域具有重要的指导意义，可能促使研究人员重新审视并开发更适合高频场景的离散化策略。"}}
{"id": "2506.10237", "title": "Intelligent Travel Activity Monitoring: Generalized Distributed Acoustic Sensing Approaches", "authors": ["Ruikang Zhong", "Chia-Yen Chiang", "Mona Jaber", "Rupert De Wilde", "Peter Hayward"], "summary": "Obtaining data on active travel activities such as walking, jogging, and\ncycling is important for refining sustainable transportation systems (STS).\nEffectively monitoring these activities not only requires sensing solutions to\nhave a joint feature of being accurate, economical, and privacy-preserving, but\nalso enough generalizability to adapt to different climate environments and\ndeployment conditions. In order to provide a generalized sensing solution, a\ndeep learning (DL)-enhanced distributed acoustic sensing (DAS) system for\nmonitoring active travel activities is proposed. By leveraging the ambient\nvibrations captured by DAS, this scheme infers motion patterns without relying\non image-based or wearable devices, thereby addressing privacy concerns. We\nconduct real-world experiments in two geographically distinct locations and\ncollect comprehensive datasets to evaluate the performance of the proposed\nsystem. To address the generalization challenges posed by heterogeneous\ndeployment environments, we propose two solutions according to network\navailability: 1) an Internet-of-Things (IoT) scheme based on federated learning\n(FL) is proposed, and it enables geographically different DAS nodes to be\ntrained collaboratively to improve generalizability; 2) an off-line\ninitialization approach enabled by meta-learning is proposed to develop\nhigh-generality initialization for DL models and to enable rapid model\nfine-tuning with limited data samples, facilitating generalization at newly\nestablished or isolated DAS nodes. Experimental results of the walking and\ncycling classification problem demonstrate the performance and generalizability\nof the proposed DL-enhanced DAS system, paving the way for practical,\nlarge-scale DAS monitoring of active travel.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10237v1", "AI": {"title_translation": "智能出行活动监测：通用分布式声学传感方法", "tldr": "提出了一种深度学习增强的分布式声学传感系统，用于通用、隐私保护的出行活动监测，并通过联邦学习和元学习解决泛化问题。", "motivation": "获取步行、慢跑、骑行等主动出行活动数据对于完善可持续交通系统至关重要。现有的传感解决方案需要同时具备准确性、经济性、隐私保护性以及在不同气候和部署条件下的通用性。", "method": "提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，用于监测主动出行活动。该系统利用DAS捕获的环境振动来推断运动模式，避免了对基于图像或可穿戴设备的依赖，从而解决了隐私问题。为了解决异构部署环境带来的泛化挑战，根据网络可用性提出了两种解决方案：1）一种基于联邦学习（FL）的物联网（IoT）方案，使地理位置不同的DAS节点能够协作训练以提高泛化能力；2）一种通过元学习实现的离线初始化方法，旨在为DL模型开发高通用性初始化，并实现使用有限数据样本的快速模型微调，从而促进在新建立或隔离的DAS节点上的泛化。", "result": "步行和骑行分类问题的实验结果证明了所提出的深度学习增强DAS系统的性能和泛化能力。", "conclusion": "所提出的系统为实际大规模的出行活动DAS监测铺平了道路。", "translation": "获取步行、慢跑和骑行等主动出行活动数据对于完善可持续交通系统（STS）至关重要。有效监测这些活动不仅要求传感解决方案同时具备准确性、经济性和隐私保护性，还需要足够的通用性以适应不同的气候环境和部署条件。为了提供一种通用的传感解决方案，本文提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，用于监测主动出行活动。通过利用DAS捕获的环境振动，该方案无需依赖基于图像或可穿戴设备即可推断运动模式，从而解决了隐私问题。我们在两个地理位置不同的地方进行了真实世界实验，并收集了全面的数据集来评估所提出系统的性能。为了解决异构部署环境带来的泛化挑战，我们根据网络可用性提出了两种解决方案：1）一种基于联邦学习（FL）的物联网（IoT）方案，使地理位置不同的DAS节点能够协作训练以提高泛化能力；2）一种通过元学习实现的离线初始化方法，旨在为DL模型开发高通用性初始化，并实现使用有限数据样本的快速模型微调，从而促进在新建立或隔离的DAS节点上的泛化。步行和骑行分类问题的实验结果证明了所提出的深度学习增强DAS系统的性能和泛化能力，为实际大规模的出行活动DAS监测铺平了道路。", "summary": "本文提出了一种深度学习（DL）增强的分布式声学传感（DAS）系统，用于监测步行和骑行等主动出行活动。该系统利用环境振动，避免了图像或可穿戴设备带来的隐私问题，旨在提供准确、经济、隐私保护且具有良好通用性的传感解决方案。为应对异构部署环境中的泛化挑战，作者提出了两种方法：一是基于联邦学习（FL）的物联网（IoT）方案，实现DAS节点的协作训练；二是基于元学习的离线初始化方法，以实现新节点的快速微调。真实世界实验验证了该系统的性能和泛化能力，为其在大规模出行活动监测中的实际应用奠定了基础。", "keywords": "分布式声学传感, 深度学习, 出行活动监测, 泛化, 联邦学习", "comments": "本文的创新点在于将分布式声学传感（DAS）与深度学习结合应用于出行活动监测，并特别关注了隐私保护和通用性问题。利用环境振动进行此应用是新颖的。所提出的联邦学习和元学习方法具体解决了不同部署环境下泛化这一关键的实际挑战，这对于大规模传感系统具有重要意义。"}}
{"id": "2506.10221", "title": "Model Predictive Control-Based Optimal Energy Management of Autonomous Electric Vehicles Under Cold Temperatures", "authors": ["Shanthan Kumar Padisala", "Satadru Dey"], "summary": "In autonomous electric vehicles (AEVs), battery energy must be judiciously\nallocated to satisfy primary propulsion demands and secondary auxiliary\ndemands, particularly the Heating, Ventilation, and Air Conditioning (HVAC)\nsystem. This becomes especially critical when the battery is in a low state of\ncharge under cold ambient conditions, and cabin heating and battery\npreconditioning (prior to actual charging) can consume a significant percentage\nof available energy, directly impacting the driving range. In such cases, one\nusually prioritizes propulsion or applies heuristic rules for thermal\nmanagement, often resulting in suboptimal energy utilization. There is a\npressing need for a principled approach that can dynamically allocate battery\npower in a way that balances thermal comfort, battery health and\npreconditioning, along with range preservation. This paper attempts to address\nthis issue using real-time Model Predictive Control to optimize the power\nconsumption between the propulsion, HVAC, and battery temperature preparation\nso that it can be charged immediately once the destination is reached.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10221v1", "AI": {"title_translation": "基于模型预测控制的寒冷环境下自动驾驶电动汽车最优能量管理", "tldr": "本文提出一种基于实时模型预测控制的方法，以优化寒冷环境下自动驾驶电动汽车的能量分配，平衡推进、暖通空调和电池预处理的需求，以延长续航里程并支持即时充电。", "motivation": "在寒冷环境下，自动驾驶电动汽车（AEVs）的电池能量需要同时满足主要推进需求和辅助（特别是暖通空调）需求。当电池电量低时，车厢加热和电池预处理会消耗大量能量，严重影响续航里程。传统方法通常优先考虑推进或采用启发式规则，导致能量利用次优。因此，迫切需要一种原则性的方法来动态分配电池功率，以平衡热舒适性、电池健康、预处理和续航里程。", "method": "本文采用实时模型预测控制（MPC）来优化推进系统、暖通空调（HVAC）系统和电池温度准备之间的功耗，以便车辆到达目的地后可以立即充电。", "result": "未在摘要中提及", "conclusion": "未在摘要中提及", "translation": "在自动驾驶电动汽车（AEVs）中，电池能量必须谨慎分配，以满足主要的推进需求和次要的辅助需求，特别是供暖、通风和空调（HVAC）系统。当电池在寒冷环境条件下处于低电量状态时，这变得尤为关键，因为车厢加热和电池预处理（在实际充电之前）可能会消耗可用能量的很大一部分，直接影响续航里程。在这种情况下，通常会优先考虑推进或应用启发式规则进行热管理，这通常导致次优的能量利用。迫切需要一种原则性的方法，能够动态分配电池功率，以平衡热舒适性、电池健康和预处理，以及续航里程的保持。本文试图通过使用实时模型预测控制来解决这个问题，以优化推进、HVAC和电池温度准备之间的功耗，从而使车辆在到达目的地后可以立即充电。", "summary": "本文提出了一种针对寒冷环境下自动驾驶电动汽车（AEVs）能量管理问题的解决方案。考虑到车厢加热和电池预处理在低温下会显著消耗电池能量并影响续航里程，现有方法存在次优性。为解决此问题，研究提出采用实时模型预测控制（MPC）来优化推进、HVAC和电池预处理之间的功率分配，旨在平衡热舒适性、电池健康和续航里程，并确保车辆在抵达目的地后能立即充电。", "keywords": "自动驾驶电动汽车, 能量管理, 模型预测控制, 寒冷环境, 电池预处理", "comments": "该研究的创新之处在于将模型预测控制应用于寒冷环境下自动驾驶电动汽车的多目标能量管理问题，旨在实现推进、HVAC和电池预处理之间的实时优化。这对于提高电动汽车在低温条件下的续航能力和用户体验具有重要意义。"}}
{"id": "2506.10042", "title": "Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions", "authors": ["Ece Gumusel"], "summary": "In an era of increasing interaction with artificial intelligence (AI), users\nface evolving privacy decisions shaped by complex, uncertain factors. This\npaper introduces Multiverse Privacy Theory, a novel framework in which each\nprivacy decision spawns a parallel universe, representing a distinct potential\noutcome based on user choices over time. By simulating these universes, this\ntheory provides a foundation for understanding privacy through the lens of\ncontextual integrity, evolving preferences, and probabilistic decision-making.\nFuture work will explore its application using real-world, scenario-based\nsurvey data.", "comment": "5 pages, 1 figure, 1 table", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10042v1", "AI": {"title_translation": "多重宇宙隐私理论：复杂用户-AI交互中的情境风险", "tldr": "该论文提出了“多重宇宙隐私理论”，一个新颖的框架，通过模拟每个隐私决策产生的平行宇宙来理解用户在AI交互中面临的复杂情境隐私风险。", "motivation": "在与人工智能（AI）交互日益增多的时代，用户面临由复杂不确定因素塑造的不断演变的隐私决策。", "method": "论文引入了“多重宇宙隐私理论”，这是一个新颖的框架。在该框架中，每个隐私决策都会生成一个平行宇宙，代表基于用户随时间选择的不同潜在结果。通过模拟这些宇宙，该理论为通过情境完整性、不断演变的偏好和概率决策来理解隐私提供了基础。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在与人工智能（AI）交互日益增多的时代，用户面临由复杂不确定因素塑造的不断演变的隐私决策。本文引入了多重宇宙隐私理论，这是一个新颖的框架，其中每个隐私决策都会产生一个平行宇宙，代表基于用户随时间选择的不同潜在结果。通过模拟这些宇宙，该理论为通过情境完整性、不断演变的偏好和概率决策来理解隐私提供了基础。未来的工作将探索其在真实世界、基于场景的调查数据中的应用。", "summary": "本文提出了“多重宇宙隐私理论”，旨在解决用户在与人工智能交互中面临的复杂且不确定的隐私决策问题。该理论框架将每个隐私决策视为一个分叉点，生成一个代表不同潜在结果的平行宇宙。通过模拟这些宇宙，该理论提供了一个新的视角，以情境完整性、不断变化的偏好和概率决策为基础来理解隐私。该理论的实际应用将是未来的研究方向。", "keywords": "多重宇宙隐私理论, 用户-AI交互, 情境风险, 隐私决策, 平行宇宙", "comments": "该论文提出了一种极具创新性的隐私理论框架，即“多重宇宙隐私理论”，通过将隐私决策与平行宇宙的概念相结合，为理解复杂的用户-AI交互中的情境风险提供了一个独特且富有想象力的视角。这种方法超越了传统的静态隐私模型，引入了时间和概率维度，有望更准确地捕捉用户隐私偏好的动态性和不确定性。其创新点在于将科幻概念融入理论构建，为隐私研究开辟了新思路。"}}
{"id": "2506.10280", "title": "AI-Based Software Vulnerability Detection: A Systematic Literature Review", "authors": ["Samiha Shimmi", "Hamed Okhravi", "Mona Rahimi"], "summary": "Software vulnerabilities in source code pose serious cybersecurity risks,\nprompting a shift from traditional detection methods (e.g., static analysis,\nrule-based matching) to AI-driven approaches. This study presents a systematic\nreview of software vulnerability detection (SVD) research from 2018 to 2023,\noffering a comprehensive taxonomy of techniques, feature representations, and\nembedding methods. Our analysis reveals that 91% of studies use AI-based\nmethods, with graph-based models being the most prevalent. We identify key\nlimitations, including dataset quality, reproducibility, and interpretability,\nand highlight emerging opportunities in underexplored techniques such as\nfederated learning and quantum neural networks, providing a roadmap for future\nresearch.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10280v1", "AI": {"title_translation": "基于AI的软件漏洞检测：一项系统性文献综述", "tldr": "该研究对2018年至2023年间基于AI的软件漏洞检测（SVD）研究进行了系统性回顾，揭示了主流方法、现有局限性并提出了未来研究方向。", "motivation": "源代码中的软件漏洞带来了严重网络安全风险，促使检测方法从传统方式转向AI驱动的方法。", "method": "本研究对2018年至2023年间的软件漏洞检测（SVD）研究进行了系统性文献综述，并提供了技术、特征表示和嵌入方法的综合分类。", "result": "分析显示，91%的研究使用基于AI的方法，其中基于图的模型最为普遍。研究识别了数据集质量、可复现性和可解释性等关键局限性。", "conclusion": "研究强调了联邦学习和量子神经网络等未充分探索的技术中的新兴机会，为未来的研究提供了路线图。", "translation": "源代码中的软件漏洞带来了严重网络安全风险，促使检测方法从传统方式（例如，静态分析、基于规则的匹配）转向AI驱动的方法。本研究对2018年至2023年间的软件漏洞检测（SVD）研究进行了系统性回顾，提供了技术、特征表示和嵌入方法的综合分类。我们的分析显示，91%的研究使用基于AI的方法，其中基于图的模型最为普遍。我们识别了数据集质量、可复现性和可解释性等关键局限性，并强调了联邦学习和量子神经网络等未充分探索的技术中的新兴机会，为未来的研究提供了路线图。", "summary": "本系统性文献综述聚焦于2018年至2023年间基于AI的软件漏洞检测研究。研究发现AI方法已成为主流，特别是基于图的模型。同时，该综述指出了当前研究在数据集质量、可复现性和可解释性方面的挑战，并为未来的研究方向，如联邦学习和量子神经网络，提供了指导。", "keywords": "软件漏洞检测, 人工智能, 系统性综述, 网络安全, 深度学习", "comments": "这项研究的重要性在于它对新兴的AI驱动软件漏洞检测领域进行了全面的系统性回顾。它不仅揭示了当前的主流趋势（如基于图的模型），还明确指出了现有研究的痛点和局限性，特别是数据集质量、可复现性和可解释性问题。此外，它为未来研究指明了方向，提出了如联邦学习和量子神经网络等前沿技术，这对于推动该领域的进步具有重要指导意义。"}}
{"id": "2506.10324", "title": "Beyond Compliance: A User-Autonomy Framework for Inclusive and Customizable Web Accessibility", "authors": ["Lalitha A R"], "summary": "This paper proposes a shift from compliance-centered web accessibility to a\ncare-driven model that prioritizes user autonomy, using neurodivergent users as\na catalyst case for broader personalization needs. While accessibility\nstandards offer a flexible framework, they are often interpreted and\nimplemented as static compliance checklists, our approach reframes it as a\nflexible, user-centered process. We introduce a customizable Comfort Mode\nframework that allows users to adapt interface settings, such as contrast,\ntypography, motion, and scaling, according to their individual needs, while\nretaining the brand's core visual identity. Grounded in psychological and\ncognitive accessibility principles, our design supports personalization without\nsacrificing creative freedom. We present both minimal and advanced\nimplementation models with mock-ups, demonstrating how inclusive design can be\nseamlessly integrated at minimal cost. This approach aims to broaden digital\ninclusivity by offering autonomy to those who require it, without imposing\nchanges on those who do not. The proposed system is adaptable, scalable, and\nsuitable for a wide range of users and brands, offering a new paradigm where\nuser autonomy, aesthetic integrity, and accessibility converge not through\ncompromise, but through choice.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10324v1", "AI": {"title_translation": "超越合规：一个包容和可定制的网页无障碍用户自主框架", "tldr": "本文提出了一个以用户自主为核心的网页无障碍框架，旨在从静态合规转向个性化、以用户为中心的模式，通过“舒适模式”实现界面定制，以提高数字包容性。", "motivation": "目前的网页无障碍标准常被解读为静态合规清单，未能充分满足用户个性化需求，特别是神经多样性用户。本文旨在提出一种以用户自主为核心的无障碍模式，超越单纯的合规性，实现更广泛的个性化和包容性。", "method": "本文提出了一个可定制的“舒适模式”框架，允许用户根据个人需求调整界面设置（如对比度、排版、动态效果、缩放），同时保留品牌核心视觉识别。该设计基于心理学和认知无障碍原则，并提供了最小和高级实现模型的模型图。", "result": "通过“舒适模式”框架，用户能够自主调整界面设置，实现个性化体验，同时保持品牌视觉完整性。该方法展示了如何以最小成本无缝集成包容性设计，并且不强制未需求用户接受改变。", "conclusion": "该研究提出了一种新的网页无障碍范式，将用户自主性、美学完整性和无障碍性通过选择而非妥协相结合。该系统具有适应性、可扩展性，适用于广泛的用户和品牌，旨在拓宽数字包容性。", "translation": "本文提出将网页无障碍从以合规为中心转向以护理为驱动的模型，该模型优先考虑用户自主性，并以神经多样性用户作为更广泛个性化需求的催化剂案例。虽然无障碍标准提供了一个灵活的框架，但它们通常被解释和实施为静态的合规清单，我们的方法将其重新定义为一个灵活的、以用户为中心的过程。我们引入了一个可定制的“舒适模式”框架，允许用户根据其个人需求调整界面设置，如对比度、排版、动态效果和缩放，同时保留品牌的核心视觉识别。我们的设计以心理学和认知无障碍原则为基础，支持个性化而不牺牲创作自由。我们展示了最小和高级实现模型的模型图，演示了如何以最小成本无缝集成包容性设计。这种方法旨在通过向需要自主权的用户提供自主权，而不对不需要的用户强加改变，从而拓宽数字包容性。所提出的系统具有适应性、可扩展性，适用于广泛的用户和品牌，提供了一种新的范式，其中用户自主性、美学完整性和无障碍性不是通过妥协，而是通过选择而融合。", "summary": "本文提出了一种以用户自主为核心的网页无障碍框架，旨在从当前僵化的合规模式转向更具适应性和个性化的设计。通过引入“舒适模式”框架，用户可以根据自身需求（如对比度、字体、动态效果）定制界面，同时保持品牌视觉一致性。该方法基于心理学和认知无障碍原则，旨在以低成本实现数字包容，使得系统既能满足个性化需求，又不强制不必要的变化，从而实现用户自主、美学与无障碍的融合。", "keywords": "用户自主, 网页无障碍, 个性化, 舒适模式, 数字包容性", "comments": "这篇论文的创新点在于其突破了传统的、以合规性为导向的无障碍设计思维，转向了以用户自主权为核心的“护理驱动”模式。通过引入“舒适模式”这一具体框架，它提供了一个实用的、可定制的解决方案，解决了现有无障碍标准在实际应用中缺乏灵活性和个性化的问题。强调神经多样性用户作为催化剂案例，也提升了其研究的社会意义和前瞻性。该方法在实现个性化需求的同时，兼顾了品牌视觉识别和实施成本，具有很强的现实指导意义和推广潜力。"}}
{"id": "2506.10240", "title": "Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "Image-based visual servoing (IBVS) methods have been well developed and used\nin many applications, especially in pose (position and orientation) alignment.\nHowever, most research papers focused on developing control solutions when 3D\npoint features can be detected inside the field of view. This work proposes an\ninnovative feedforward-feedback adaptive control algorithm structure with the\nYoula Parameterization method. A designed feature estimation loop ensures\nstable and fast motion control when point features are outside the field of\nview. As 3D point features move inside the field of view, the IBVS feedback\nloop preserves the precision of the pose at the end of the control period.\nAlso, an adaptive controller is developed in the feedback loop to stabilize the\nsystem in the entire range of operations. The nonlinear camera and robot\nmanipulator model is linearized and decoupled online by an adaptive algorithm.\nThe adaptive controller is then computed based on the linearized model\nevaluated at current linearized point. The proposed solution is robust and easy\nto implement in different industrial robotic systems. Various scenarios are\nused in simulations to validate the effectiveness and robust performance of the\nproposed controller.", "comment": "22 pages, 13 figures. To appear in: Innovative Adaptive Image-Based\n  Visual Servoing Control of 6 DoFs Industrial Robot Manipulators, IntechOpen,\n  2024. For published version, see this http URL:\n  https://doi.org/10.5772/intechopen.1004857", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10240v1", "AI": {"title_translation": "六自由度工业机器人机械手创新自适应图像视觉伺服控制", "tldr": "本文提出了一种创新的前馈-反馈自适应控制算法结构，结合Youla参数化方法，解决了图像视觉伺服（IBVS）中3D点特征超出视野的问题，并通过自适应控制器在线线性化和解耦模型，提高了控制的稳定性和精度，并经验证其鲁棒性和易于实现。", "motivation": "大多数图像视觉伺服（IBVS）研究集中于3D点特征在视野内的情况，但当点特征超出视野时，控制解决方案不足。本文旨在解决这一问题，并提高系统在整个操作范围内的稳定性和精度。", "method": "本文提出了一种基于Youla参数化的创新前馈-反馈自适应控制算法结构。设计了一个特征估计循环，以确保当点特征超出视野时的稳定快速运动控制。当3D点特征进入视野时，IBVS反馈回路保持姿态精度。此外，开发了一个自适应控制器在反馈回路中，通过自适应算法在线线性化和解耦非线性相机和机器人机械手模型，并基于线性化模型计算控制器，以稳定系统在整个操作范围内。", "result": "在各种场景下通过仿真验证了所提出的控制器是有效且性能鲁棒的。", "conclusion": "所提出的解决方案是鲁棒的，并且易于在不同的工业机器人系统中实现。", "translation": "基于图像的视觉伺服（IBVS）方法已经发展成熟并广泛应用于许多领域，特别是在姿态（位置和方向）对准方面。然而，大多数研究论文都集中于开发当3D点特征在视野内可检测时的控制解决方案。这项工作提出了一种创新的前馈-反馈自适应控制算法结构，结合Youla参数化方法。设计的特征估计循环确保了当点特征超出视野时的稳定快速运动控制。当3D点特征进入视野时，IBVS反馈回路保持了控制周期结束时姿态的精度。此外，在反馈回路中开发了一个自适应控制器，以稳定系统在整个操作范围内的运行。非线性相机和机器人机械手模型通过自适应算法在线线性化和解耦。然后，基于当前线性化点评估的线性化模型计算自适应控制器。所提出的解决方案是鲁棒的，并且易于在不同的工业机器人系统中实现。在各种场景下通过仿真验证了所提出的控制器的有效性和鲁棒性能。", "summary": "本文针对图像视觉伺服（IBVS）在3D点特征超出视野时控制不足的问题，提出了一种创新的前馈-反馈自适应控制算法。该算法结合Youla参数化，并引入特征估计循环确保视野外特征的稳定快速控制。同时，通过反馈回路中的自适应控制器在线线性化和解耦非线性模型，从而在整个操作范围内保持系统稳定性和姿态精度。仿真结果验证了所提控制器的有效性和鲁棒性，且其易于在工业机器人系统中实现。", "keywords": "视觉伺服, 自适应控制, 工业机器人, Youla参数化, 视野外特征", "comments": "本文的创新点在于提出了一个结合Youla参数化的前馈-反馈自适应控制结构，特别是解决了3D点特征超出视野时视觉伺服的挑战，通过特征估计循环确保了控制的稳定性和速度。此外，其自适应控制器能够在线线性化并解耦非线性模型，显著提高了系统在整个操作范围内的鲁棒性和易用性，对于工业机器人视觉伺服的实际应用具有重要意义。"}}
{"id": "2506.10258", "title": "Synchronization for Fault-Tolerant Quantum Computers", "authors": ["Satvik Maurya", "Swamit Tannu"], "summary": "Quantum Error Correction (QEC) codes store information reliably in logical\nqubits by encoding them in a larger number of less reliable qubits. The surface\ncode, known for its high resilience to physical errors, is a leading candidate\nfor fault-tolerant quantum computing (FTQC). Logical qubits encoded with the\nsurface code can be in different phases of their syndrome generation cycle,\nthereby introducing desynchronization in the system. This can occur due to the\nproduction of non-Clifford states, dropouts due to fabrication defects, and the\nuse of other QEC codes with the surface code to reduce resource requirements.\nLogical operations require the syndrome generation cycles of the logical qubits\ninvolved to be synchronized. This requires the leading qubit to pause or slow\ndown its cycle, allowing more errors to accumulate before the next cycle,\nthereby increasing the risk of uncorrectable errors.\n  To synchronize the syndrome generation cycles of logical qubits, we define\nthree policies - Passive, Active, and Hybrid. The Passive policy is the\nbaseline, and the simplest, wherein the leading logical qubits idle until they\nare synchronized with the remaining logical qubits. On the other hand, the\nActive policy aims to slow the leading logical qubits down gradually, by\ninserting short idle periods before multiple code cycles. This approach reduces\nthe logical error rate (LER) by up to 2.4x compared to the Passive policy. The\nHybrid policy further reduces the LER by up to 3.4x by reducing the\nsynchronization slack and running a few additional rounds of error correction.\nFurthermore, the reduction in the logical error rate with the proposed\nsynchronization policies enables a speedup in decoding latency of up to 2.2x\nwith a circuit-level noise model.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10258v1", "AI": {"title_translation": "容错量子计算机的同步", "tldr": "容错量子计算机中的逻辑量子比特可能出现不同步，从而增加错误。本文提出了三种同步策略（被动、主动、混合），以显著降低逻辑错误率并加快解码延迟。", "motivation": "在容错量子计算中，使用表面码编码的逻辑量子比特在综合征生成周期中可能出现不同步，这可能是由非Clifford态的产生、制造缺陷或与其他QEC码结合使用引起的。这种不同步需要领先的量子比特暂停或减慢其周期，从而允许更多错误累积，增加不可纠正错误的风险，从而阻碍逻辑操作。", "method": "本文定义了三种同步逻辑量子比特综合征生成周期的策略：被动策略（基线，领先的逻辑量子比特空闲直到同步）；主动策略（通过在多个代码周期前插入短空闲期，逐渐减慢领先逻辑量子比特的速度）；混合策略（通过减少同步松弛并运行额外的纠错轮次，进一步降低错误率）。", "result": "与被动策略相比，主动策略将逻辑错误率（LER）降低了高达2.4倍。混合策略将LER进一步降低了高达3.4倍。所提出的同步策略在电路级噪声模型下，使得解码延迟的加速高达2.2倍。", "conclusion": "本文提出的同步策略有效解决了容错量子计算中逻辑量子比特的不同步问题，显著降低了逻辑错误率并加快了解码延迟，对实现高效的容错量子计算至关重要。", "translation": "量子纠错 (QEC) 码通过将信息编码到数量更多但可靠性较低的量子比特中，从而在逻辑量子比特中可靠地存储信息。表面码因其对物理错误的高弹性而成为容错量子计算 (FTQC) 的主要候选方案。用表面码编码的逻辑量子比特可能处于其综合征生成周期的不同阶段，从而在系统中引入了不同步。这可能由于非Clifford态的产生、制造缺陷导致的脱落以及将其他QEC码与表面码结合使用以减少资源需求而发生。逻辑操作要求所涉及的逻辑量子比特的综合征生成周期同步。这要求领先的量子比特暂停或减慢其周期，从而在下一个周期之前允许更多错误累积，从而增加了不可纠正错误的风险。\n为了同步逻辑量子比特的综合征生成周期，我们定义了三种策略——被动 (Passive)、主动 (Active) 和混合 (Hybrid)。被动策略是基线，也是最简单的，其中领先的逻辑量子比特空闲直到它们与其余逻辑量子比特同步。另一方面，主动策略旨在通过在多个代码周期之前插入短空闲期来逐渐减慢领先逻辑量子比特的速度。与被动策略相比，这种方法将逻辑错误率 (LER) 降低了高达2.4倍。混合策略通过减少同步松弛并运行少量额外的纠错轮次，将LER进一步降低了高达3.4倍。此外，所提出的同步策略在电路级噪声模型下，将逻辑错误率的降低使得解码延迟的加速高达2.2倍。", "summary": "本文研究了容错量子计算中逻辑量子比特在综合征生成周期中可能出现的不同步问题，该问题会增加错误累积风险。为解决此问题，提出了被动、主动和混合三种同步策略。实验结果表明，与基线被动策略相比，主动策略可将逻辑错误率降低高达2.4倍，而混合策略可进一步降低高达3.4倍，同时还能将解码延迟加速高达2.2倍，这对于实现高效的容错量子计算至关重要。", "keywords": "量子纠错, 表面码, 同步, 容错量子计算, 逻辑错误率", "comments": "本文解决了构建容错量子计算机中的一个实际且关键的挑战：管理逻辑量子比特的同步。所提出的主动和混合策略在逻辑错误率和解码延迟方面提供了显著改进，这对于未来量子系统的可扩展性和可靠性至关重要。其创新之处在于提供了具体的策略来缓解不同步问题，这是一个在结合不同量子纠错技术或处理物理缺陷时自然出现的问题。"}}
{"id": "2506.10089", "title": "Optimizing Latent Dimension Allocation in Hierarchical VAEs: Balancing Attenuation and Information Retention for OOD Detection", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "summary": "Out-of-distribution (OOD) detection is a critical task in machine learning,\nparticularly for safety-critical applications where unexpected inputs must be\nreliably flagged. While hierarchical variational autoencoders (HVAEs) offer\nimproved representational capacity over traditional VAEs, their performance is\nhighly sensitive to how latent dimensions are distributed across layers.\nExisting approaches often allocate latent capacity arbitrarily, leading to\nineffective representations or posterior collapse. In this work, we introduce a\ntheoretically grounded framework for optimizing latent dimension allocation in\nHVAEs, drawing on principles from information theory to formalize the trade-off\nbetween information loss and representational attenuation. We prove the\nexistence of an optimal allocation ratio $r^{\\ast}$ under a fixed latent\nbudget, and empirically show that tuning this ratio consistently improves OOD\ndetection performance across datasets and architectures. Our approach\noutperforms baseline HVAE configurations and provides practical guidance for\nprincipled latent structure design, leading to more robust OOD detection with\ndeep generative models.", "comment": "41 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10089v1", "AI": {"title_translation": "优化分层VAE中的潜在维度分配：平衡衰减与信息保留以实现OOD检测", "tldr": "论文提出了一种理论框架，用于优化分层VAE中潜在维度分配，以平衡信息损失和表示衰减，从而提高OOD检测性能。", "motivation": "域外检测（OOD）是机器学习中的一项关键任务，尤其是在安全关键应用中。分层变分自编码器（HVAE）虽然表示能力强，但其性能对潜在维度在层间的分布方式高度敏感，而现有方法通常任意分配潜在容量，导致表示无效或后验坍缩。", "method": "本文引入了一个基于信息论的理论框架，用于优化分层变分自编码器（HVAE）中的潜在维度分配。该框架形式化了信息损失和表示衰减之间的权衡，并证明了在固定潜在预算下存在一个最优分配比$r^{\\ast}$。", "result": "经验性研究表明，调整最优分配比$r^{\\ast}$能够持续提高跨数据集和架构的域外检测（OOD）性能。该方法优于基线HVAE配置。", "conclusion": "本研究为分层VAE的潜在结构设计提供了原则性指导，从而使深度生成模型能够实现更鲁棒的域外检测（OOD）。", "translation": "域外检测（OOD）是机器学习中的一项关键任务，尤其对于必须可靠标记意外输入的安全关键应用而言。尽管分层变分自编码器（HVAE）比传统VAE提供了更强的表示能力，但其性能高度依赖于潜在维度在层间的分布方式。现有方法通常任意分配潜在容量，导致表示无效或后验坍缩。在这项工作中，我们引入了一个理论基础框架，用于优化HVAE中的潜在维度分配，借鉴信息论原理来形式化信息损失和表示衰减之间的权衡。我们证明了在固定潜在预算下存在一个最优分配比$r^{\\ast}$，并通过实验表明，调整该比例能持续提高跨数据集和架构的OOD检测性能。我们的方法优于基线HVAE配置，并为原则性的潜在结构设计提供了实用指导，从而通过深度生成模型实现更鲁棒的OOD检测。", "summary": "本文提出了一种优化分层变分自编码器（HVAE）中潜在维度分配的理论框架，旨在解决现有方法中潜在容量分配随意导致表示无效或后验坍缩的问题。通过引入信息论原理来平衡信息损失与表示衰减，研究证明了在固定潜在预算下存在一个最优分配比$r^{\\ast}$。实验结果表明，调整此比例显著提高了HVAE在不同数据集和架构上的域外检测（OOD）性能，优于传统配置，为深度生成模型中潜在结构的鲁棒设计提供了指导。", "keywords": "域外检测, 分层变分自编码器, 潜在维度分配, 信息论, 表示衰减", "comments": "本文的创新之处在于提出了一个基于信息论的理论框架来优化分层VAE中的潜在维度分配，解决了现有方法中潜在维度分配不当导致性能下降的问题。通过证明最优分配比的存在并进行实证验证，为深度生成模型中鲁棒的OOD检测提供了原则性指导，具有重要的理论和实践意义。"}}
{"id": "2506.10470", "title": "TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference", "authors": ["Hongbin Zhang", "Taosheng Wei", "Zhenyi Zheng", "Jiangsu Du", "Zhiguang Chen", "Yutong Lu"], "summary": "As the model size continuously increases, pipeline parallelism shows great\npromise in throughput-oriented LLM inference due to its low demand on\ncommunications. However, imbalanced pipeline workloads and complex data\ndependencies in the prefill and decode phases result in massive pipeline\nbubbles and further severe performance reduction. To better exploit the\npipeline parallelism for high-throughput LLM inference, we propose TD-Pipe,\nwith the key idea lies in the temporally-disaggregated pipeline parallelism\narchitecture. Specifically, this architecture disaggregates the prefill and\ndecode phases in the temporal dimension, so as to eliminate pipeline bubbles\ncaused by the phase switching. TD-Pipe identifies potential issues of\nexploiting the novel architecture and provides solutions. First, a\nhierarchy-controller structure is used to better coordinate devices in pipeline\nparallelism by decoupling the scheduling from execution. Second, the AI-based\ngreedy prefill approach aggressively performs more prefills by predicting the\noutput length and simulating the memory usage. Third, the inter-batch work\nstealing approach dynamically balances decode phase workloads between different\nbatches to reduce bubbles. Forth, the spatial-temporal intensity comparison\napproach determines the optimal switch from decode to prefill by comparing the\nperformance drop from reduced computational intensity with that from phase\nswitching bubbles. Extensive experiments show that TD-Pipe effectively\nincreases the throughput of LLM inference by up to 1.91x over the existing\ntensor parallel approach and 2.73x over the existing pipeline parallel approach\non GPU nodes with only PCIe interconnection.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10470v1", "AI": {"title_translation": "TD-Pipe：面向高吞吐量LLM推理的按时间解耦流水线并行架构", "tldr": "TD-Pipe提出了一种按时间解耦的流水线并行架构，通过分离预填充和解码阶段并结合多种优化技术，显著减少了流水线气泡，将LLM推理吞吐量分别提升至现有张量并行和流水线并行的1.91倍和2.73倍。", "motivation": "随着模型规模的不断增大，LLM推理中流水线并行因其低通信需求而具有前景，但预填充和解码阶段不平衡的工作负载和复杂数据依赖导致大量流水线气泡，严重降低了性能。", "method": "提出TD-Pipe，其核心思想是按时间解耦的流水线并行架构，将预填充和解码阶段在时间维度上解耦以消除阶段切换引起的气泡。具体方法包括：使用分层控制器结构解耦调度与执行；基于AI的贪婪预填充通过预测输出长度和模拟内存使用来积极执行更多预填充；批间工作窃取动态平衡解码阶段工作负载；时空强度比较确定最佳切换时机。", "result": "TD-Pipe在仅有PCIe互连的GPU节点上，将LLM推理吞吐量比现有张量并行方法提高了1.91倍，比现有流水线并行方法提高了2.73倍。", "conclusion": "TD-Pipe通过其按时间解耦的流水线并行架构和一系列优化方案，有效解决了LLM推理中的流水线气泡问题，显著提升了高吞吐量LLM推理的性能。", "translation": "随着模型规模的不断增大，流水线并行由于其对通信的低需求，在面向吞吐量的LLM推理中显示出巨大的前景。然而，预填充和解码阶段不平衡的流水线工作负载和复杂的数据依赖性导致了大量的流水线气泡，并进一步严重降低了性能。为了更好地利用流水线并行进行高吞吐量LLM推理，我们提出了TD-Pipe，其核心思想在于按时间解耦的流水线并行架构。具体来说，该架构在时间维度上解耦了预填充和解码阶段，从而消除了由阶段切换引起的流水线气泡。TD-Pipe识别了利用这种新架构的潜在问题并提供了解决方案。首先，采用分层控制器结构，通过将调度与执行解耦，更好地协调流水线并行中的设备。其次，基于AI的贪婪预填充方法通过预测输出长度和模拟内存使用情况，积极地执行更多的预填充。第三，批间工作窃取方法动态平衡不同批次间的解码阶段工作负载以减少气泡。第四，时空强度比较方法通过比较计算强度降低带来的性能下降与阶段切换气泡带来的性能下降，来确定从解码到预填充的最佳切换时机。大量实验表明，在仅有PCIe互连的GPU节点上，TD-Pipe将LLM推理的吞吐量比现有张量并行方法提高了1.91倍，比现有流水线并行方法提高了2.73倍。", "summary": "TD-Pipe提出了一种创新的按时间解耦流水线并行架构，旨在解决LLM推理中预填充和解码阶段不平衡工作负载和数据依赖性导致的流水线气泡问题。通过在时间维度上分离这两个阶段，并结合分层控制器、AI贪婪预填充、批间工作窃取和时空强度比较等多种优化技术，TD-Pipe显著减少了气泡，并在实验中证明其能有效提升LLM推理吞吐量，性能优于现有张量并行和流水线并行方法。", "keywords": "TD-Pipe, LLM推理, 流水线并行, 吞吐量, 解耦", "comments": "该论文的创新点在于提出了按时间解耦的流水线并行架构，并针对性地引入了多种优化策略来消除LLM推理中的流水线气泡。这对于提高LLM推理的吞吐量至关重要，特别是在资源有限（如仅PCIe互连）的环境下，其性能提升效果显著。"}}
{"id": "2506.10281", "title": "Closer to Language than Steam: AI as the Cognitive Engine of a New Productivity Revolution", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "summary": "Artificial Intelligence (AI) is reframed as a cognitive engine driving a\nnovel productivity revolution distinct from the Industrial Revolution's\nphysical thrust. This paper develops a theoretical framing of AI as a cognitive\nrevolution akin to written language - a transformative augmentation of human\nintellect rather than another mechanized tool. We compare AI's emergence to\nhistorical leaps in information technology to show how it amplifies knowledge\nwork. Examples from various domains demonstrate AI's impact as a driver of\nproductivity in cognitive tasks. We adopt a multidisciplinary perspective\ncombining computer science advances with economic insights and sociological\nperspectives on how AI reshapes work and society. Through conceptual\nframeworks, we visualize the shift from manual to cognitive productivity. Our\ncentral argument is that AI functions as an engine of cognition - comparable to\nhow human language revolutionized knowledge - heralding a new productivity\nparadigm. We discuss how this revolution demands rethinking of skills,\norganizations, and policies. This paper, balancing academic rigor with clarity,\nconcludes that AI's promise lies in complementing human cognitive abilities,\nmarking a new chapter in productivity evolution.", "comment": "12 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10281v1", "AI": {"title_translation": "比蒸汽更接近语言：AI作为新生产力革命的认知引擎", "tldr": "本文将AI重新定义为一种认知引擎，类似于书面语言，驱动一场新的生产力革命，而非工业革命那样的物理推动力，它通过增强人类智力来放大知识工作。", "motivation": "本文旨在将人工智能（AI）重新定义为推动一场新型生产力革命的认知引擎，与工业革命的物理推动力截然不同。论文旨在通过将其与信息技术的历史性飞跃进行比较，来展示AI如何增强知识工作，并从概念上阐明由AI驱动的从体力劳动到认知生产力的转变。", "method": "本文发展了将AI视为一场认知革命的理论框架，将其与信息技术的历史性飞跃进行比较，并利用来自不同领域的例子来展示其影响。论文采用多学科视角，结合了计算机科学的进步、经济学见解和社会学观点，并通过概念框架来可视化生产力模式的转变。", "result": "AI作为认知引擎发挥作用，类似于人类语言如何彻底改变知识，预示着一个新的生产力范式。它被证明能够补充人类的认知能力。", "conclusion": "AI的潜力在于补充人类认知能力，标志着生产力演进的新篇章，并要求重新思考技能、组织和政策。", "translation": "人工智能（AI）被重新定义为驱动一场新型生产力革命的认知引擎，这与工业革命的物理推动力截然不同。本文将AI理论化为一个类似书面语言的认知革命——是对人类智力的变革性增强，而非另一种机械化工具。我们比较了AI的出现与信息技术的历史性飞跃，以展示它如何放大知识工作。来自不同领域的例子展示了AI作为认知任务生产力驱动力的影响。我们采用多学科视角，结合计算机科学的进步、经济学见解以及社会学视角来探讨AI如何重塑工作和社会。通过概念框架，我们可视化了从体力生产力到认知生产力的转变。我们的核心论点是AI作为认知引擎发挥作用——类似于人类语言如何彻底改变知识——预示着一个新的生产力范式。我们讨论了这场革命如何要求重新思考技能、组织和政策。本文在学术严谨性与清晰性之间取得平衡，得出结论：AI的潜力在于补充人类认知能力，标志着生产力演进的新篇章。", "summary": "本文将人工智能（AI）重新定义为一种认知引擎，类似于书面语言，驱动一场新的生产力革命，它通过增强人类智力而非物理推力来区分于工业革命。论文将AI的影响与历史上的信息技术进步进行比较，展示了其在各领域放大知识工作的作用。通过多学科方法，论文认为AI作为认知引擎预示着一个新的生产力范式，强调其在补充人类认知能力方面的作用，并需要重新评估技能、组织和政策。", "keywords": "人工智能, 认知引擎, 生产力革命, 知识工作, 人类增强", "comments": "该论文通过将AI视为一场类似于语言的认知革命，而非仅仅是另一种技术工具，提供了一个创新的视角。这种重新定义突出了AI在智力增强方面的潜力及其超越简单自动化的深远社会影响，为理解工作和生产力的未来提供了有价值的概念框架。其多学科方法是其优势所在。"}}
{"id": "2506.10964", "title": "The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins", "authors": ["Rico H Herzog", "Till Degkwitz", "Trivik Verma"], "summary": "Urban digital twins are increasingly perceived as a way to pool the growing\ndigital resources of cities for the purpose of a more sustainable and\nintegrated urban planning. Models and simulations are central to this\nundertaking: They enable \"what if?\" scenarios, create insights and describe\nrelationships between the vast data that is being collected. However, the\nprocess of integrating and subsequently using models in urban digital twins is\nan inherently complex undertaking. It raises questions about how to represent\nurban complexity, how to deal with uncertain assUrban Model Platformtions and\nmodeling paradigms, and how to capture underlying power relations. Existent\napproaches in the domain largely focus on monolithic and centralized solutions\nin the tradition of neoliberal city-making, oftentimes prohibiting pluralistic\nand open interoperable models. Using a participatory design for participatory\nsystems approach together with the City of Hamburg, Germany, we find that an\nopen Urban Model Platform can function both as a public technological backbone\nfor modeling and simulation in urban digital twins and as a socio-technical\nframework for a collaborative and pluralistic representation of urban\nprocesses. Such a platform builds on open standards, allows for a decentralized\nintegration of models, enables communication between models and supports a\nmulti-model approach to representing urban systems.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.10964v1", "AI": {"title_translation": "城市模型平台：城市数字孪生中建模与仿真的公共骨干", "tldr": "本文提出了一种开放的城市模型平台，作为城市数字孪生中建模和仿真的公共技术骨干和社会技术框架，以克服现有中心化方法的局限性。", "motivation": "城市数字孪生在整合城市数字资源以实现可持续城市规划方面日益重要。然而，在城市数字孪生中集成和使用模型是一个固有的复杂过程，涉及到如何表示城市复杂性、处理不确定假设和建模范式，以及捕捉潜在的权力关系。现有的方法往往是单一且中心化的，阻碍了多元化和开放的可互操作模型。", "method": "本文采用参与式设计方法，与德国汉堡市合作，研究并构建了一个开放的城市模型平台。该方法旨在创建一个既是技术骨干又是社会技术框架的系统。", "result": "研究发现，一个开放的城市模型平台可以作为城市数字孪生中建模和仿真的公共技术骨干，同时也是一个协作和多元化表示城市过程的社会技术框架。该平台建立在开放标准之上，允许模型的去中心化集成，实现模型之间的通信，并支持表示城市系统的多模型方法。", "conclusion": "开放的城市模型平台能够作为城市数字孪生中建模与仿真的公共技术骨干，并作为一个社会技术框架，促进对城市过程的协作和多元化表示。它通过开放标准、去中心化集成、模型间通信和多模型方法来解决现有方法的局限性。", "translation": "城市数字孪生正日益被视为一种汇集城市日益增长的数字资源的方式，以实现更可持续和综合的城市规划。模型和仿真对于这项工作至关重要：它们能够实现“如果……会怎样？”的场景，创造洞察力，并描述所收集的大量数据之间的关系。然而，在城市数字孪生中集成和随后使用模型是一项固有的复杂任务。它提出了关于如何表示城市复杂性、如何处理不确定假设和建模范式以及如何捕捉潜在权力关系的问题。该领域现有的方法主要集中于新自由主义城市建设传统中的单一和集中式解决方案，这常常阻碍了多元化和开放的可互操作模型。我们与德国汉堡市合作，采用参与式系统方法进行参与式设计，发现一个开放的城市模型平台既可以作为城市数字孪生中建模和仿真的公共技术骨干，也可以作为协作和多元化表示城市过程的社会技术框架。这样一个平台建立在开放标准之上，允许模型的去中心化集成，实现模型之间的通信，并支持表示城市系统的多模型方法。", "summary": "本文提出并论证了一个开放的城市模型平台，旨在解决当前城市数字孪生中模型集成和使用面临的复杂性与局限性。针对现有中心化解决方案阻碍多元化和开放互操作模型的痛点，该研究通过与汉堡市的参与式设计合作，开发了一个基于开放标准、支持去中心化集成和多模型通信的平台。该平台被证明不仅是城市数字孪生中建模与仿真的公共技术骨干，更是一个促进城市过程协作与多元化表示的社会技术框架。", "keywords": "城市模型平台, 数字孪生, 建模与仿真, 开放标准, 参与式设计", "comments": "本文的创新之处在于提出并实践了一个开放且去中心化的城市模型平台，突破了传统城市数字孪生中常见的高度中心化和单一的建模范式。通过强调“公共骨干”和“社会技术框架”，该研究不仅关注技术层面的互操作性，更强调了模型在城市规划中应有的开放性、协作性和多元化表示，这对于促进更具包容性和可持续性的城市发展具有重要意义。其与汉堡市的实践合作也增强了研究的实际应用价值。"}}
{"id": "2506.10325", "title": "SWDL: Stratum-Wise Difference Learning with Deep Laplacian Pyramid for Semi-Supervised 3D Intracranial Hemorrhage Segmentation", "authors": ["Cheng Wang", "Siqi Chen", "Donghua Mi", "Yang Chen", "Yudong Zhang", "Yinsheng Li"], "summary": "Recent advances in medical imaging have established deep learning-based\nsegmentation as the predominant approach, though it typically requires large\namounts of manually annotated data. However, obtaining annotations for\nintracranial hemorrhage (ICH) remains particularly challenging due to the\ntedious and costly labeling process. Semi-supervised learning (SSL) has emerged\nas a promising solution to address the scarcity of labeled data, especially in\nvolumetric medical image segmentation. Unlike conventional SSL methods that\nprimarily focus on high-confidence pseudo-labels or consistency regularization,\nwe propose SWDL-Net, a novel SSL framework that exploits the complementary\nadvantages of Laplacian pyramid and deep convolutional upsampling. The\nLaplacian pyramid excels at edge sharpening, while deep convolutions enhance\ndetail precision through flexible feature mapping. Our framework achieves\nsuperior segmentation of lesion details and boundaries through a difference\nlearning mechanism that effectively integrates these complementary approaches.\nExtensive experiments on a 271-case ICH dataset and public benchmarks\ndemonstrate that SWDL-Net outperforms current state-of-the-art methods in\nscenarios with only 2% labeled data. Additional evaluations on the publicly\navailable Brain Hemorrhage Segmentation Dataset (BHSD) with 5% labeled data\nfurther confirm the superiority of our approach. Code and data have been\nreleased at https://github.com/SIAT-CT-LAB/SWDL.", "comment": "11 pages, 4 figures, 6 Tables", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10325v1", "AI": {"title_translation": "SWDL：基于深度拉普拉斯金字塔的分层差异学习用于半监督三维颅内出血分割", "tldr": "SWDL-Net是一种新的半监督学习框架，它结合了拉普拉斯金字塔和深度卷积上采样的优势，在极少量标记数据下显著提高了颅内出血的3D分割性能。", "motivation": "深度学习在医学图像分割中表现出色，但通常需要大量手动标注数据。获取颅内出血（ICH）的标注过程繁琐且成本高昂，导致标记数据稀缺。半监督学习（SSL）是解决这一问题的有前景的方案。", "method": "本文提出了SWDL-Net，一个新颖的半监督学习（SSL）框架。它利用拉普拉斯金字塔（擅长边缘锐化）和深度卷积上采样（通过灵活特征映射增强细节精度）的互补优势。通过差异学习机制有效整合这些互补方法，以实现病灶细节和边界的精确分割。", "result": "在271例ICH数据集和公共基准上进行的大量实验表明，在仅有2%标记数据的情况下，SWDL-Net优于现有最先进方法。在公开可用的脑出血分割数据集（BHSD）上使用5%标记数据的额外评估进一步证实了该方法的优越性。", "conclusion": "SWDL-Net通过结合拉普拉斯金字塔和深度卷积的优势，并在极少量标记数据的情况下，显著提高了3D颅内出血分割的性能，为解决医学图像标注数据稀缺问题提供了有效方案。", "translation": "医学影像领域的最新进展已将基于深度学习的分割确立为主要方法，尽管它通常需要大量手动标注数据。然而，由于标注过程繁琐且成本高昂，获取颅内出血（ICH）的标注仍然特别具有挑战性。半监督学习（SSL）已成为解决标记数据稀缺问题的一种有前景的解决方案，尤其是在体积医学图像分割中。与主要关注高置信度伪标签或一致性正则化的传统SSL方法不同，我们提出了SWDL-Net，这是一种新颖的SSL框架，它利用了拉普拉斯金字塔和深度卷积上采样的互补优势。拉普拉斯金字塔擅长边缘锐化，而深度卷积通过灵活的特征映射增强细节精度。我们的框架通过一种差异学习机制实现了病灶细节和边界的卓越分割，该机制有效整合了这些互补方法。在271例ICH数据集和公共基准上的大量实验表明，在仅有2%标记数据的情况下，SWDL-Net优于现有最先进方法。在公开可用的脑出血分割数据集（BHSD）上使用5%标记数据的额外评估进一步证实了我们方法的优越性。代码和数据已在https://github.com/SIAT-CT-LAB/SWDL 发布。", "summary": "本文提出SWDL-Net，一种针对半监督3D颅内出血分割的新型框架。为解决医学图像标注数据稀缺问题，SWDL-Net结合拉普拉斯金字塔（边缘锐化）和深度卷积（细节精度）的优势，通过差异学习机制实现病灶细节和边界的精确分割。实验证明，在仅有2%或5%标记数据的情况下，SWDL-Net在ICH数据集和公共基准上均显著优于现有SOTA方法。", "keywords": "颅内出血分割, 半监督学习, 拉普拉斯金字塔, 深度卷积, 差异学习", "comments": "本文的创新点在于提出了SWDL-Net，巧妙地结合了拉普拉斯金字塔在边缘锐化上的优势和深度卷积在细节精度上的优势，并通过差异学习机制有效地融合它们，以应对半监督学习中数据标注稀缺的挑战。其在极少量标记数据下（2%）取得超越SOTA的性能，显示了该方法在临床应用中的巨大潜力，尤其对于需要高精度细节分割的医学图像任务具有重要意义。"}}
{"id": "2506.10747", "title": "FairASR: Fair Audio Contrastive Learning for Automatic Speech Recognition", "authors": ["Jongsuk Kim", "Jaemyung Yu", "Minchan Kwon", "Junmo Kim"], "summary": "Large-scale ASR models have achieved remarkable gains in accuracy and\nrobustness. However, fairness issues remain largely unaddressed despite their\ncritical importance in real-world applications. In this work, we introduce\nFairASR, a system that mitigates demographic bias by learning representations\nthat are uninformative about group membership, enabling fair generalization\nacross demographic groups. Leveraging a multi-demographic dataset, our approach\nemploys a gradient reversal layer to suppress demographic-discriminative\nfeatures while maintaining the ability to capture generalizable speech patterns\nthrough an unsupervised contrastive loss. Experimental results show that\nFairASR delivers competitive overall ASR performance while significantly\nreducing performance disparities across different demographic groups.", "comment": "Accepted to Interspeech2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.10747v1", "AI": {"title_translation": "FairASR：用于自动语音识别的公平音频对比学习", "tldr": "FairASR通过学习与人口统计学信息无关的表征，显著减少了自动语音识别（ASR）模型在不同人口群体间的性能差异，同时保持了整体性能。", "motivation": "尽管大规模ASR模型在准确性和鲁棒性方面取得了显著进步，但其公平性问题（特别是人口统计学偏见）在实际应用中仍未得到解决，这对于模型的广泛应用至关重要。", "method": "本研究引入了FairASR系统，通过学习与群体成员身份无关的表征来缓解人口统计学偏见。该方法利用多人口统计数据集，并采用梯度反转层来抑制人口统计学区分特征，同时通过无监督对比损失保持捕获通用语音模式的能力。", "result": "实验结果表明，FairASR在提供有竞争力的整体ASR性能的同时，显著减少了不同人口统计学群体之间的性能差异。", "conclusion": "FairASR成功地在自动语音识别中实现了公平性，通过学习去偏见的语音表征，有效缓解了人口统计学偏见，并在保持高整体性能的同时，显著缩小了不同群体间的性能差距。", "translation": "大规模ASR模型在准确性和鲁棒性方面取得了显著进步。然而，尽管公平性问题在实际应用中至关重要，但它们在很大程度上仍未得到解决。在这项工作中，我们引入了FairASR，这是一个通过学习与群体成员身份无关的表征来减轻人口统计学偏见的系统，从而实现了跨人口群体的公平泛化。我们的方法利用多人口统计数据集，采用梯度反转层来抑制人口统计学区分特征，同时通过无监督对比损失保持捕获通用语音模式的能力。实验结果表明，FairASR在提供有竞争力的整体ASR性能的同时，显著减少了不同人口统计学群体之间的性能差异。", "summary": "FairASR是一种旨在解决自动语音识别（ASR）模型中人口统计学偏见的新系统。该系统通过使用多人口统计数据集和梯度反转层，学习与用户群体无关的语音表征，从而抑制区分性特征。同时，它利用无监督对比损失来保持对通用语音模式的捕获能力。实验证明，FairASR不仅能提供具有竞争力的整体ASR性能，还能显著缩小不同人口群体间的性能差距，实现了更公平的ASR泛化。", "keywords": "公平性, 自动语音识别, 对比学习, 人口统计学偏见, 梯度反转层", "comments": "FairASR的创新之处在于其结合梯度反转层和无监督对比学习来减轻ASR中的人口统计学偏见。这种方法在去偏见的同时，有效地保持了模型的整体性能，对于提升ASR在现实世界应用中的公平性和可信度具有重要意义。"}}
{"id": "2506.10116", "title": "ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering", "authors": ["Caijun Jia", "Nan Xu", "Jingxuan Wei", "Qingli Wang", "Lei Wang", "Bihui Yu", "Junnan Zhu"], "summary": "Recently, large language models have shown remarkable reasoning capabilities\nthrough long-chain reasoning before responding. However, how to extend this\ncapability to visual reasoning tasks remains an open challenge. Existing\nmultimodal reasoning approaches transfer such visual reasoning task into\ntextual reasoning task via several image-to-text conversions, which often lose\ncritical structural and semantic information embedded in visualizations,\nespecially for tasks like chart question answering that require a large amount\nof visual details. To bridge this gap, we propose ChartReasoner, a code-driven\nnovel two-stage framework designed to enable precise, interpretable reasoning\nover charts. We first train a high-fidelity model to convert diverse chart\nimages into structured ECharts codes, preserving both layout and data semantics\nas lossless as possible. Then, we design a general chart reasoning data\nsynthesis pipeline, which leverages this pretrained transport model to\nautomatically and scalably generate chart reasoning trajectories and utilizes a\ncode validator to filter out low-quality samples. Finally, we train the final\nmultimodal model using a combination of supervised fine-tuning and\nreinforcement learning on our synthesized chart reasoning dataset and\nexperimental results on four public benchmarks clearly demonstrate the\neffectiveness of our proposed ChartReasoner. It can preserve the original\ndetails of the charts as much as possible and perform comparably with\nstate-of-the-art open-source models while using fewer parameters, approaching\nthe performance of proprietary systems like GPT-4o in out-of-domain settings.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10116v1", "AI": {"title_translation": "ChartReasoner：代码驱动的模态桥接，用于图表问答中的长链推理", "tldr": "ChartReasoner通过将图表转换为ECharts代码，实现了图表问答中的精确、可解释的长链推理，性能优于现有开源模型。", "motivation": "大型语言模型在长链推理方面表现出色，但将其能力扩展到视觉推理（特别是图表问答）仍是一个挑战。现有方法通过图像到文本的转换进行视觉推理，但往往会丢失图表中关键的结构和语义信息。", "method": "本文提出了ChartReasoner，一个代码驱动的两阶段框架。首先，训练一个高保真模型将图表图像转换为结构化的ECharts代码，以尽可能无损地保留布局和数据语义。然后，设计一个通用的图表推理数据合成管道，利用预训练的转换模型自动生成图表推理轨迹，并使用代码验证器过滤低质量样本。最后，通过在合成的图表推理数据集上结合监督微调和强化学习来训练最终的多模态模型。", "result": "在四个公共基准测试上的实验结果表明，ChartReasoner是有效的。它能最大限度地保留图表原始细节，在参数更少的情况下与最先进的开源模型表现相当，并且在域外设置中接近GPT-4o等专有系统的性能。", "conclusion": "ChartReasoner通过代码驱动的模态桥接方法，有效解决了图表问答中的长链推理问题，实现了精确、可解释的推理，并能最大限度地保留图表细节，同时取得了与先进模型相当甚至超越的性能。", "translation": "最近，大型语言模型通过响应前的长链推理展现了卓越的推理能力。然而，如何将这种能力扩展到视觉推理任务仍然是一个开放的挑战。现有的多模态推理方法通过几次图像到文本的转换将此类视觉推理任务转换为文本推理任务，这通常会丢失嵌入在可视化中的关键结构和语义信息，特别是对于需要大量视觉细节的图表问答等任务。为了弥补这一空白，我们提出了ChartReasoner，一个代码驱动的新型两阶段框架，旨在实现对图表的精确、可解释的推理。我们首先训练一个高保真模型，将各种图表图像转换为结构化的ECharts代码，尽可能无损地保留布局和数据语义。然后，我们设计了一个通用的图表推理数据合成管道，该管道利用这个预训练的传输模型自动且可扩展地生成图表推理轨迹，并利用代码验证器过滤掉低质量样本。最后，我们结合监督微调和强化学习，在我们的合成图表推理数据集上训练最终的多模态模型，在四个公共基准测试上的实验结果清楚地证明了我们提出的ChartReasoner的有效性。它能够最大限度地保留图表的原始细节，并且在参数更少的情况下与最先进的开源模型表现相当，在域外设置中接近GPT-4o等专有系统的性能。", "summary": "本文提出ChartReasoner，一个代码驱动的两阶段框架，旨在解决图表问答中长链推理的视觉信息丢失问题。它首先将图表图像转换为结构化ECharts代码以保留细节，然后通过数据合成管道生成推理轨迹并训练多模态模型。实验证明，ChartReasoner在保持图表细节的同时，以更少参数实现了与顶尖开源模型相当的性能，并在域外设置中接近GPT-4o的水平。", "keywords": "图表问答, 长链推理, 模态桥接, ECharts代码, 大语言模型", "comments": "ChartReasoner的创新之处在于其代码驱动的模态桥接方法，通过将图表转换为结构化的ECharts代码，有效解决了现有图像到文本转换方法中视觉信息丢失的问题，从而实现了更精确和可解释的图表推理。其数据合成管道也提高了训练效率和模型泛化能力。该方法对于提升图表问答（CQA）领域的长链推理能力具有重要意义。"}}
{"id": "2506.10507", "title": "Edit360: 2D Image Edits to 3D Assets from Any Angle", "authors": ["Junchao Huang", "Xinting Hu", "Zhuotao Tian", "Shaoshuai Shi", "Li Jiang"], "summary": "Recent advances in diffusion models have significantly improved image\ngeneration and editing, but extending these capabilities to 3D assets remains\nchallenging, especially for fine-grained edits that require multi-view\nconsistency. Existing methods typically restrict editing to predetermined\nviewing angles, severely limiting their flexibility and practical applications.\nWe introduce Edit360, a tuning-free framework that extends 2D modifications to\nmulti-view consistent 3D editing. Built upon video diffusion models, Edit360\nenables user-specific editing from arbitrary viewpoints while ensuring\nstructural coherence across all views. The framework selects anchor views for\n2D modifications and propagates edits across the entire 360-degree range. To\nachieve this, Edit360 introduces a novel Anchor-View Editing Propagation\nmechanism, which effectively aligns and merges multi-view information within\nthe latent and attention spaces of diffusion models. The resulting edited\nmulti-view sequences facilitate the reconstruction of high-quality 3D assets,\nenabling customizable 3D content creation.", "comment": "11 pages, 9 figures", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10507v1", "AI": {"title_translation": "Edit360：从任意角度将2D图像编辑应用于3D资产", "tldr": "Edit360是一个免调优的框架，它利用视频扩散模型将2D图像编辑扩展到多视图一致的3D资产编辑，解决了现有方法视角受限的问题，并实现了从任意视角的编辑。", "motivation": "尽管扩散模型在图像生成和编辑方面取得了显著进展，但将其能力扩展到3D资产，特别是需要多视图一致性的精细编辑，仍然具有挑战性。现有方法通常将编辑限制在预定的视角，这严重限制了它们的灵活性和实际应用。", "method": "Edit360是一个免调优的框架，它利用视频扩散模型将2D修改扩展到多视图一致的3D编辑。该框架选择锚定视图进行2D修改，并通过新颖的“锚定视图编辑传播”机制，在扩散模型的潜在空间和注意力空间中有效地对齐和合并多视图信息，从而在整个360度范围内传播编辑。", "result": "Edit360实现了用户从任意视角的特定编辑，同时确保所有视图的结构一致性。最终编辑后的多视图序列有助于高质量3D资产的重建。", "conclusion": "Edit360通过实现从任意视角的2D图像编辑到多视图一致的3D资产编辑，从而实现可定制的3D内容创建。", "translation": "扩散模型最近的进展显著改善了图像生成和编辑，但将这些能力扩展到3D资产仍然具有挑战性，特别是对于需要多视图一致性的精细编辑。现有方法通常将编辑限制在预定的视角，严重限制了它们的灵活性和实际应用。我们引入了Edit360，一个免调优的框架，它将2D修改扩展到多视图一致的3D编辑。Edit360基于视频扩散模型构建，实现了用户从任意视角的特定编辑，同时确保所有视图的结构连贯性。该框架选择锚定视图进行2D修改，并将编辑传播到整个360度范围。为了实现这一点，Edit360引入了一种新颖的锚定视图编辑传播机制，该机制有效地在扩散模型的潜在空间和注意力空间中对齐和合并多视图信息。由此产生的编辑后的多视图序列有助于高质量3D资产的重建，从而实现可定制的3D内容创建。", "summary": "Edit360是一个创新的免调优框架，旨在解决将2D图像编辑应用于3D资产时多视图一致性受限的挑战。它利用视频扩散模型，允许用户从任意视角对3D资产进行精细编辑，同时通过其独特的“锚定视图编辑传播”机制确保所有视图的结构连贯性。该方法通过将2D修改传播到360度范围，最终生成高质量的3D资产，从而实现灵活且可定制的3D内容创建。", "keywords": "3D编辑, 扩散模型, 多视图一致性, 2D到3D, 任意视角", "comments": "Edit360的创新之处在于其免调优的特性和Anchor-View Editing Propagation机制，它成功地将2D图像编辑能力扩展到多视图一致的3D资产，解决了现有方法视角受限的痛点。这对于需要灵活3D内容创作的应用具有重要意义。"}}
{"id": "2506.10520", "title": "Macro Graph of Experts for Billion-Scale Multi-Task Recommendation", "authors": ["Hongyu Yao", "Zijin Hong", "Hao Chen", "Yuanchen Bei", "Zhiqing Li", "Qijie Shen", "Zuobin Ying", "Huan Gong", "Feiran Huang"], "summary": "Graph-based multi-task learning at billion-scale presents a significant\nchallenge, as different tasks correspond to distinct billion-scale graphs.\nTraditional multi-task learning methods often neglect these graph structures,\nrelying solely on individual user and item embeddings. However, disregarding\ngraph structures overlooks substantial potential for improving performance. In\nthis paper, we introduce the Macro Graph of Expert (MGOE) framework, the first\napproach capable of leveraging macro graph embeddings to capture task-specific\nmacro features while modeling the correlations between task-specific experts.\nSpecifically, we propose the concept of a Macro Graph Bottom, which, for the\nfirst time, enables multi-task learning models to incorporate graph information\neffectively. We design the Macro Prediction Tower to dynamically integrate\nmacro knowledge across tasks. MGOE has been deployed at scale, powering\nmulti-task learning for the homepage of a leading billion-scale recommender\nsystem. Extensive offline experiments conducted on three public benchmark\ndatasets demonstrate its superiority over state-of-the-art multi-task learning\nmethods, establishing MGOE as a breakthrough in multi-task graph-based\nrecommendation. Furthermore, online A/B tests confirm the superiority of MGOE\nin billion-scale recommender systems.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10520v1", "AI": {"title_translation": "十亿级多任务推荐的宏观专家图", "tldr": "MGOE是一个新的框架，首次利用宏观图嵌入来处理十亿级多任务推荐中的图结构，并在离线和在线测试中表现优异。", "motivation": "传统的多任务学习方法在十亿级图数据中忽略了图结构，仅依赖用户和物品嵌入，从而错失了性能提升的巨大潜力。", "method": "本文提出了宏观专家图（MGOE）框架，首次利用宏观图嵌入捕获任务特定的宏观特征，并建模任务特定专家之间的相关性。具体包括：提出了“宏观图底部”（Macro Graph Bottom）概念，使多任务学习模型能够有效整合图信息；设计了“宏观预测塔”（Macro Prediction Tower）来动态整合跨任务的宏观知识。", "result": "MGOE已在领先的十亿级推荐系统主页上部署。在三个公共基准数据集上的广泛离线实验表明MGOE优于最先进的多任务学习方法。在线A/B测试证实了MGOE在十亿级推荐系统中的优越性。", "conclusion": "MGOE是多任务图基推荐领域的一项突破，通过有效利用宏观图结构显著提升了十亿级多任务推荐系统的性能。", "translation": "十亿级多任务图学习提出了一个重大挑战，因为不同的任务对应着不同的十亿级图。传统的多任务学习方法通常忽略这些图结构，仅依赖于独立的用户和物品嵌入。然而，忽略图结构会忽视提升性能的巨大潜力。在本文中，我们引入了宏观专家图（MGOE）框架，这是第一个能够利用宏观图嵌入来捕获任务特定宏观特征，同时建模任务特定专家之间相关性的方法。具体来说，我们提出了宏观图底部（Macro Graph Bottom）的概念，这首次使得多任务学习模型能够有效整合图信息。我们设计了宏观预测塔（Macro Prediction Tower）来动态整合跨任务的宏观知识。MGOE已大规模部署，为领先的十亿级推荐系统主页上的多任务学习提供支持。在三个公共基准数据集上进行的广泛离线实验证明了其优于最先进的多任务学习方法，使MGOE成为多任务图基推荐领域的一项突破。此外，在线A/B测试证实了MGOE在十亿级推荐系统中的优越性。", "summary": "本文提出了宏观专家图（MGOE）框架，旨在解决十亿级多任务推荐中传统方法忽略图结构的问题。MGOE首次利用宏观图嵌入捕获任务特有的宏观特征并建模专家相关性，通过引入宏观图底部和宏观预测塔有效整合图信息。实验证明MGOE在离线基准测试和在线A/B测试中均优于现有方法，并已成功部署于大规模推荐系统。", "keywords": "多任务推荐, 图神经网络, 宏观图, 专家模型, 大规模推荐系统", "comments": "MGOE的创新之处在于首次将宏观图嵌入引入十亿级多任务推荐，解决了传统方法忽略图结构信息的问题。其提出的宏观图底部和宏观预测塔机制有效地整合了图信息和跨任务知识。该框架在实际大规模推荐系统中的部署和优异表现，证明了其在工业界应用的巨大潜力。"}}
{"id": "2506.10447", "title": "Stability analysis of the free-surface Stokes problem and an unconditionally stable explicit scheme", "authors": ["Igor Tominec", "Lukas Lundgren", "André Löfgren", "Josefin Ahlkrona"], "summary": "Accurate simulations of ice sheet dynamics, mantle convection, lava flow, and\nother highly viscous free-surface flows involve solving the coupled\nStokes/free-surface equations. In this paper, we theoretically analyze the\nstability and conservation properties of the weak form of this system for\nNewtonian fluids and non-Newtonian fluids, at both the continuous and discrete\nlevels. We perform the fully discrete stability analysis for finite element\nmethods used in space with explicit and implicit Euler time-stepping methods\nused in time. Motivated by the theory, we propose a stabilization term designed\nfor the explicit Euler discretization, which ensures unconditional time\nstability and permits conservation of the domain volume. Numerical experiments\nvalidate and support our theoretical findings.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10447v1", "AI": {"title_translation": "自由表面Stokes问题的稳定性分析及无条件稳定显式格式", "tldr": "本文理论分析了自由表面Stokes问题的稳定性，并提出了一种针对显式欧拉离散化的稳定项，实现了无条件时间稳定性和体积守恒。", "motivation": "准确模拟冰盖动力学、地幔对流、熔岩流和其他高粘度自由表面流动需要解决耦合的Stokes/自由表面方程，因此需要对其稳定性进行深入分析并开发稳定的数值方法。", "method": "本文在连续和离散层面理论分析了牛顿流体和非牛顿流体该系统弱形式的稳定性和守恒特性。对空间采用有限元方法、时间采用显式和隐式欧拉时间步进方法的完全离散稳定性进行了分析。基于理论，提出了一种针对显式欧拉离散化的稳定项。", "result": "所提出的稳定项确保了无条件时间稳定性和域体积守恒。数值实验验证并支持了理论发现。", "conclusion": "本文成功提出并验证了一种新的稳定项，该稳定项使得显式欧拉离散化下的自由表面Stokes问题实现了无条件时间稳定性和体积守恒，这对于高粘度自由表面流动的准确模拟至关重要。", "translation": "冰盖动力学、地幔对流、熔岩流和其他高粘度自由表面流动的精确模拟涉及求解耦合的Stokes/自由表面方程。在本文中，我们从理论上分析了牛顿流体和非牛顿流体该系统弱形式在连续和离散层面的稳定性和守恒特性。我们对空间采用有限元方法、时间采用显式和隐式欧拉时间步进方法的完全离散稳定性进行了分析。受理论启发，我们提出了一种专为显式欧拉离散化设计的稳定项，该稳定项确保了无条件时间稳定性并允许域体积守恒。数值实验验证并支持了我们的理论发现。", "summary": "本文对耦合Stokes/自由表面方程的稳定性和守恒特性进行了理论分析，涵盖了牛顿流体和非牛顿流体，并在连续和离散层面进行了研究。文章详细分析了有限元方法与显式/隐式欧拉时间步进结合的完全离散稳定性。在此基础上，提出了一种创新的稳定项，用于显式欧拉离散化，该项能确保无条件时间稳定性和域体积守恒，并通过数值实验得到了验证。", "keywords": "自由表面Stokes问题, 稳定性分析, 显式格式, 有限元, 稳定项", "comments": "本文的创新之处在于提出了一种针对显式欧拉离散化的稳定项，解决了自由表面Stokes问题在时间步进中的稳定性挑战，特别是实现了无条件稳定性和体积守恒，这对高粘度自由表面流的长期精确模拟具有重要意义。"}}
{"id": "2506.10676", "title": "Description and Discussion on DCASE 2025 Challenge Task 4: Spatial Semantic Segmentation of Sound Scenes", "authors": ["Masahiro Yasuda", "Binh Thien Nguyen", "Noboru Harada", "Romain Serizel", "Mayank Mishra", "Marc Delcroix", "Shoko Araki", "Daiki Takeuchi", "Daisuke Niizumi", "Yasunori Ohishi", "Tomohiro Nakatani", "Takao Kawamura", "Nobutaka Ono"], "summary": "Spatial Semantic Segmentation of Sound Scenes (S5) aims to enhance\ntechnologies for sound event detection and separation from multi-channel input\nsignals that mix multiple sound events with spatial information. This is a\nfundamental basis of immersive communication. The ultimate goal is to separate\nsound event signals with 6 Degrees of Freedom (6DoF) information into dry sound\nobject signals and metadata about the object type (sound event class) and\nrepresenting spatial information, including direction. However, because several\nexisting challenge tasks already provide some of the subset functions, this\ntask for this year focuses on detecting and separating sound events from\nmulti-channel spatial input signals. This paper outlines the S5 task setting of\nthe Detection and Classification of Acoustic Scenes and Events (DCASE) 2025\nChallenge Task 4 and the DCASE2025 Task 4 Dataset, newly recorded and curated\nfor this task. We also report experimental results for an S5 system trained and\nevaluated on this dataset. The full version of this paper will be published\nafter the challenge results are made public.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10676v1", "AI": {"title_translation": "DCASE 2025挑战任务4：声景空间语义分割的描述与讨论", "tldr": "本文介绍了DCASE 2025挑战任务4 (S5)，目标是从多通道空间信号中检测和分离声事件，并发布了新数据集和初步实验结果。", "motivation": "提升多通道输入信号中声事件检测与分离技术，尤其是结合空间信息，为沉浸式通信奠定基础。", "method": "论文描述了DCASE 2025挑战任务4 (S5) 的设置，包括新录制和整理的DCASE2025 Task 4数据集，并报告了在该数据集上训练和评估的S5系统的实验结果。", "result": "论文报告了在一个新数据集上训练和评估的S5系统的实验结果。具体结果细节未在摘要中提及。", "conclusion": "本文介绍了DCASE 2025挑战任务4 (S5) 的设置和新数据集，并提供了初步实验结果，旨在推动声景空间语义分割技术发展。最终的结论将在挑战结果公布后发布。", "translation": "声音场景的空间语义分割 (S5) 旨在增强从混合了多个声事件和空间信息的多通道输入信号中进行声事件检测和分离的技术。这是沉浸式通信的基础。最终目标是将具有6自由度 (6DoF) 信息的声事件信号分离为干声对象信号以及关于对象类型（声事件类别）和表示空间信息（包括方向）的元数据。然而，由于现有的一些挑战任务已经提供了一些子集功能，今年这项任务的重点是从多通道空间输入信号中检测和分离声事件。本文概述了声学场景和事件检测与分类 (DCASE) 2025挑战任务4的S5任务设置以及为此任务新录制和整理的DCASE2025任务4数据集。我们还报告了在该数据集上训练和评估的S5系统的实验结果。本文的完整版本将在挑战结果公布后发表。", "summary": "本文详细描述并讨论了DCASE 2025挑战任务4，即声景空间语义分割 (S5)。该任务旨在提升从包含空间信息的多通道信号中检测和分离声事件的技术，为沉浸式通信奠定基础。论文概述了S5任务的设置，介绍了为该任务新创建的DCASE2025 Task 4数据集，并初步报告了在该数据集上训练和评估的S5系统的实验结果。", "keywords": "空间语义分割, 声事件检测, 多通道信号, DCASE挑战, 沉浸式通信", "comments": "这篇论文介绍了DCASE挑战赛中的一项新任务，聚焦于声景的空间语义分割，这对于沉浸式通信等应用具有重要意义。新数据集的发布和初步实验结果的报告为未来的研究和挑战参与者提供了基础。其创新点在于将空间信息与声事件检测和分离相结合，但具体的技术细节和实验结果需待完整论文发布后才能深入评估。"}}
{"id": "2506.10381", "title": "Trace duality and additive complementary pairs of additive cyclic codes over finite chain rings", "authors": ["Sanjit Bhowmick", "Kuntal Deka", "Alexandre Fotue Tabue", "Edgar Martínez-Moro"], "summary": "This paper investigates the algebraic structure of additive complementary\npairs of cyclic codes over a finite commutative ring. We demonstrate that for\nevery additive complementary pair of additive cyclic codes, both constituent\ncodes are free modules. Moreover, we present a necessary and sufficient\ncondition for a pair of additive cyclic codes over a finite commutative ring to\nform an additive complementary pair. Finally, we construct a complementary pair\nof additive cyclic codes over a finite chain ring and show that one of the\ncodes is permutation equivalent to the trace dual of the other.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10381v1", "AI": {"title_translation": "迹对偶和有限链环上加法循环码的加法互补对", "tldr": "本文研究了有限交换环上加法循环码的代数结构，证明了加法互补对的组成码是自由模，给出了形成互补对的充要条件，并构造了有限链环上的加法互补对，揭示了其与迹对偶的置换等价关系。", "motivation": "本文旨在探究有限交换环上加法循环码的加法互补对的代数结构。", "method": "研究了有限交换环上加法循环码的加法互补对，证明了构成码为自由模。提出了加法循环码形成加法互补对的充要条件。构造了有限链环上的加法互补对，并证明其中一个码与另一个码的迹对偶置换等价。", "result": "1. 每个加法循环码的加法互补对，其构成码都是自由模。2. 提出了有限交换环上加法循环码对形成加法互补对的充要条件。3. 构造了有限链环上的加法循环码的互补对。4. 证明了所构造的其中一个码与另一个码的迹对偶置换等价。", "conclusion": "构造了有限链环上的加法循环码的互补对，并证明其中一个码与另一个码的迹对偶置换等价。", "translation": "本文研究了有限交换环上加法循环码的加法互补对的代数结构。我们证明了对于每对加法循环码的加法互补对，两个构成码都是自由模。此外，我们提出了有限交换环上加法循环码对形成加法互补对的充要条件。最后，我们构造了有限链环上的加法循环码的互补对，并表明其中一个码与另一个码的迹对偶在置换意义上是等价的。", "summary": "本文深入探讨了有限交换环上加法循环码的加法互补对的代数特性。研究发现，互补对中的码均为自由模，并给出了形成互补对的充要条件。此外，研究还在有限链环上构建了具体的加法互补对，并揭示了其中一个码与另一个码的迹对偶之间存在置换等价关系。", "keywords": "加法循环码, 互补对, 有限链环, 迹对偶, 自由模", "comments": "这篇论文在编码理论领域对加法循环码的代数结构及其互补对进行了深入分析。其创新点在于首次证明了互补对的构成码是自由模，并提出了充要条件。特别是在有限链环上的构造和迹对偶关系揭示了这些码的深层特性，对理解和设计这类码具有重要意义。"}}
{"id": "2506.10559", "title": "From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations", "authors": ["Yutong Zhou", "Masahiro Ryo"], "summary": "Explaining why the species lives at a particular location is important for\nunderstanding ecological systems and conserving biodiversity. However, existing\necological workflows are fragmented and often inaccessible to non-specialists.\nWe propose an end-to-end visual-to-causal framework that transforms a species\nimage into interpretable causal insights about its habitat preference. The\nsystem integrates species recognition, global occurrence retrieval,\npseudo-absence sampling, and climate data extraction. We then discover causal\nstructures among environmental features and estimate their influence on species\noccurrence using modern causal inference methods. Finally, we generate\nstatistically grounded, human-readable causal explanations from structured\ntemplates and large language models. We demonstrate the framework on a bee and\na flower species and report early results as part of an ongoing project,\nshowing the potential of the multimodal AI assistant backed up by a recommended\necological modeling practice for describing species habitat in\nhuman-understandable language.", "comment": "Code will be released at: https://github.com/Yutong-Zhou-cv/BioX", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10559v1", "AI": {"title_translation": "从图像到洞察：用通俗语言解释栖息地，实现可解释的生物多样性监测", "tldr": "该论文提出了一个端到端的可视化到因果框架，将物种图像转化为可解释的栖息地偏好因果洞察，并用通俗语言生成解释，旨在简化生物多样性监测。", "motivation": "理解物种为何生活在特定位置对于理解生态系统和保护生物多样性至关重要。然而，现有的生态工作流程是碎片化的，且非专业人士难以理解。", "method": "该研究提出了一个端到端的可视化到因果框架。该系统整合了物种识别、全球出现数据检索、伪缺失采样和气候数据提取。然后，利用现代因果推断方法发现环境特征之间的因果结构并估计它们对物种出现的影响。最后，通过结构化模板和大型语言模型生成基于统计学且人类可读的因果解释。", "result": "该框架在一种蜜蜂和一种花卉物种上进行了演示，并报告了早期结果。结果显示了多模态AI助手在推荐的生态建模实践支持下，用人类可理解的语言描述物种栖息地的潜力。", "conclusion": "该研究展示了一个多模态AI助手结合生态建模实践，能够从物种图像中生成关于其栖息地偏好的可解释的因果洞察，并通过通俗语言提供解释，从而简化生物多样性监测。", "translation": "解释物种为何生活在特定位置对于理解生态系统和保护生物多样性至关重要。然而，现有的生态工作流程是碎片化的，且非专业人士往往难以接触。我们提出了一个端到端的可视化到因果框架，将物种图像转化为关于其栖息地偏好的可解释的因果洞察。该系统整合了物种识别、全球出现数据检索、伪缺失采样和气候数据提取。然后，我们利用现代因果推断方法发现环境特征之间的因果结构并估计它们对物种出现的影响。最后，我们通过结构化模板和大型语言模型生成基于统计学且人类可读的因果解释。我们在一种蜜蜂和一种花卉物种上演示了该框架，并报告了作为正在进行的项目一部分的早期结果，展示了由推荐的生态建模实践支持的多模态AI助手在用人类可理解的语言描述物种栖息地方面的潜力。", "summary": "本研究提出了一种创新的端到端可视化到因果框架，旨在弥合生态学研究与非专业人士之间的鸿沟，实现可解释的生物多样性监测。该框架能够将物种图像转化为关于其栖息地偏好的可解释的因果洞察。它整合了物种识别、全球出现数据检索、伪缺失采样和气候数据提取等多个模块，并利用因果推断方法分析环境特征对物种出现的影响。最终，系统能够生成统计学上严谨且人类可读的栖息地因果解释。初步结果表明，这种结合了多模态AI和生态建模实践的方法在用通俗语言描述物种栖息地方面具有巨大潜力。", "keywords": "生物多样性监测, 因果推断, 栖息地解释, 可解释AI, 生态系统", "comments": "该论文的创新之处在于其端到端的“图像到洞察”框架，将复杂的生态学数据分析与可解释的AI和自然语言处理相结合。通过提供人类可读的因果解释，它极大地提高了生态学信息的可访问性，对于生物多样性保护和非专业人士参与具有重要意义。这是一个将先进AI技术应用于实际生态保护问题的优秀案例。"}}
{"id": "2506.10265", "title": "Ground Reaction Force Estimation via Time-aware Knowledge Distillation", "authors": ["Eun Som Jeon", "Sinjini Mitra", "Jisoo Lee", "Omik M. Save", "Ankita Shukla", "Hyunglae Lee", "Pavan Turaga"], "summary": "Human gait analysis with wearable sensors has been widely used in various\napplications, such as daily life healthcare, rehabilitation, physical therapy,\nand clinical diagnostics and monitoring. In particular, ground reaction force\n(GRF) provides critical information about how the body interacts with the\nground during locomotion. Although instrumented treadmills have been widely\nused as the gold standard for measuring GRF during walking, their lack of\nportability and high cost make them impractical for many applications. As an\nalternative, low-cost, portable, wearable insole sensors have been utilized to\nmeasure GRF; however, these sensors are susceptible to noise and disturbance\nand are less accurate than treadmill measurements. To address these challenges,\nwe propose a Time-aware Knowledge Distillation framework for GRF estimation\nfrom insole sensor data. This framework leverages similarity and temporal\nfeatures within a mini-batch during the knowledge distillation process,\neffectively capturing the complementary relationships between features and the\nsequential properties of the target and input data. The performance of the\nlightweight models distilled through this framework was evaluated by comparing\nGRF estimations from insole sensor data against measurements from an\ninstrumented treadmill. Empirical results demonstrated that Time-aware\nKnowledge Distillation outperforms current baselines in GRF estimation from\nwearable sensor data.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10265v1", "AI": {"title_translation": "基于时间感知知识蒸馏的地面反作用力估计", "tldr": "提出了一种时间感知知识蒸馏框架，用于从可穿戴鞋垫传感器数据中估计地面反作用力（GRF），其性能优于现有基线。", "motivation": "地面反作用力（GRF）是人体运动中身体与地面相互作用的关键信息。虽然仪器化跑步机是测量GRF的黄金标准，但其缺乏便携性和高成本使其不切实际。可穿戴鞋垫传感器虽然是低成本替代方案，但易受噪声和干扰影响，且精度低于跑步机测量。", "method": "提出了一种时间感知知识蒸馏框架（Time-aware Knowledge Distillation），用于从鞋垫传感器数据中估计GRF。该框架在知识蒸馏过程中利用迷你批次内的相似性和时间特征，有效捕获特征之间的互补关系以及目标和输入数据的顺序属性。", "result": "通过将鞋垫传感器数据的GRF估计值与仪器化跑步机的测量值进行比较，评估了通过该框架蒸馏出的轻量级模型的性能。实证结果表明，时间感知知识蒸馏在从可穿戴传感器数据进行GRF估计方面优于当前的基线方法。", "conclusion": "时间感知知识蒸馏框架能够有效提高从可穿戴鞋垫传感器数据估计地面反作用力的准确性，克服了传统方法在便携性和精度上的不足。", "translation": "人体步态分析与可穿戴传感器已广泛应用于各种场景，如日常生活保健、康复、物理治疗以及临床诊断与监测。特别是，地面反作用力（GRF）提供了关于身体在运动过程中如何与地面相互作用的关键信息。尽管仪器化跑步机已被广泛用作测量步行GRF的黄金标准，但其缺乏便携性和高成本使其在许多应用中不切实际。作为替代方案，低成本、便携式、可穿戴鞋垫传感器已被用于测量GRF；然而，这些传感器容易受到噪声和干扰的影响，且精度低于跑步机测量。为了应对这些挑战，我们提出了一种时间感知知识蒸馏框架，用于从鞋垫传感器数据中估计GRF。该框架在知识蒸馏过程中利用迷你批次内的相似性和时间特征，有效捕获特征之间的互补关系以及目标和输入数据的顺序属性。通过将鞋垫传感器数据的GRF估计值与仪器化跑步机的测量值进行比较，评估了通过该框架蒸馏出的轻量级模型的性能。实证结果表明，时间感知知识蒸馏在从可穿戴传感器数据进行GRF估计方面优于当前的基线方法。", "summary": "该论文提出了一种时间感知知识蒸馏框架，旨在通过可穿戴鞋垫传感器数据准确估计地面反作用力（GRF）。针对传统仪器化跑步机缺乏便携性及鞋垫传感器精度不足的问题，该框架在知识蒸馏过程中整合了迷你批次的相似性和时间特征，以捕捉数据中的互补关系和顺序属性。实验结果表明，该方法在GRF估计方面优于现有基线。", "keywords": "地面反作用力, 知识蒸馏, 可穿戴传感器, 步态分析, 时间特征", "comments": "该研究通过引入时间感知知识蒸馏，为使用廉价、便携式传感器进行GRF估计提供了有效途径，克服了现有方法的局限性，对于康复、运动分析等领域具有潜在应用价值。"}}
{"id": "2506.10251", "title": "Energy Aware Camera Location Search Algorithm for Increasing Precision of Observation in Automated Manufacturing", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "Visual servoing technology has been well developed and applied in many\nautomated manufacturing tasks, especially in tools' pose alignment. To access a\nfull global view of tools, most applications adopt eye-to-hand configuration or\neye-to-hand/eye-in-hand cooperation configuration in an automated manufacturing\nenvironment. Most research papers mainly put efforts into developing control\nand observation architectures in various scenarios, but few of them have\ndiscussed the importance of the camera's location in eye-to-hand configuration.\nIn a manufacturing environment, the quality of camera estimations may vary\nsignificantly from one observation location to another, as the combined effects\nof environmental conditions result in different noise levels of a single image\nshot at different locations. In this paper, we propose an algorithm for the\ncamera's moving policy so that it explores the camera workspace and searches\nfor the optimal location where the images' noise level is minimized. Also, this\nalgorithm ensures the camera ends up at a suboptimal (if the optimal one is\nunreachable) location among the locations already searched, with limited energy\navailable for moving the camera. Unlike a simple brute force approach, the\nalgorithm enables the camera to explore space more efficiently by adapting the\nsearch policy from learning the environment. With the aid of an image averaging\ntechnique, this algorithm, in use of a solo camera, achieves the observation\naccuracy in eye-to-hand configurations to a desirable extent without filtering\nout high-frequency information in the original image. An automated\nmanufacturing application has been simulated and the results show the success\nof this algorithm's improvement of observation precision with limited energy.", "comment": "35 pages, 24 figures, Journal, Published in: Applied Sciences, 2024,\n  vol. 14, article 9140. For published version, see this http URL:\n  https://doi.org/10.3390/app14199140", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10251v1", "AI": {"title_translation": "自动化制造中提高观测精度的节能相机位置搜索算法", "tldr": "本文提出了一种节能的相机位置搜索算法，旨在自动化制造环境中找到最佳相机位置以最小化图像噪声，从而提高观测精度。", "motivation": "现有研究在视觉伺服技术中，主要关注控制和观测架构，但很少讨论相机位置在眼手协调配置中的重要性。在制造环境中，环境条件导致的不同噪声水平会显著影响相机估计的质量，因此需要找到一个最优的相机位置来提高观测精度。", "method": "本文提出了一种相机移动策略算法，使其能够探索相机工作空间并搜索图像噪声水平最小化的最佳位置。该算法通过学习环境来调整搜索策略，从而比暴力搜索更高效地探索空间。此外，它利用图像平均技术，在不滤除原始图像高频信息的情况下，使用单个相机实现了期望的观测精度。", "result": "自动化制造应用仿真结果表明，该算法在有限能量下成功提高了观测精度。", "conclusion": "该算法能够在有限能量下，通过高效搜索找到最优或次优的相机位置，有效降低图像噪声，从而显著提高自动化制造中视觉观测的精度。", "translation": "视觉伺服技术已经得到了很好的发展，并应用于许多自动化制造任务中，特别是在工具姿态对准方面。为了获得工具的全局视图，大多数应用在自动化制造环境中采用眼手协调配置或眼手/眼内协调配置。大多数研究论文主要致力于在各种场景中开发控制和观测架构，但很少讨论相机位置在眼手协调配置中的重要性。在制造环境中，由于环境条件的综合影响导致在不同位置拍摄的单一图像的噪声水平不同，相机估计的质量可能因观测位置而异。在本文中，我们提出了一种相机移动策略算法，使其能够探索相机工作空间并搜索图像噪声水平最小化的最佳位置。此外，该算法确保相机在已搜索的位置中，以有限的可用能量移动相机，最终到达一个次优（如果最优位置无法到达）的位置。与简单的暴力方法不同，该算法通过学习环境来调整搜索策略，从而使相机更有效地探索空间。借助图像平均技术，该算法在使用单个相机的情况下，在不滤除原始图像高频信息的情况下，将眼手协调配置中的观测精度提高到期望的程度。已经对一个自动化制造应用进行了仿真，结果表明该算法在有限能量下成功提高了观测精度。", "summary": "本文提出了一种节能的相机位置搜索算法，用于自动化制造环境。该算法旨在通过探索相机工作空间，寻找能最小化图像噪声的最佳或次优相机位置，以提高视觉观测的精度。它通过学习环境来优化搜索效率，并结合图像平均技术，在仅使用单个相机的情况下，实现了高精度观测，且不损失图像细节。仿真结果验证了该算法在有限能量下提升观测精度的有效性。", "keywords": "相机位置搜索, 视觉伺服, 自动化制造, 图像噪声, 能量感知", "comments": "该论文的创新点在于提出了一个考虑能量限制和环境噪声的相机位置搜索算法，这在现有视觉伺服研究中是一个被忽视但重要的问题。通过结合机器学习思想（适应搜索策略）和图像处理技术（图像平均），该方法有效地提高了观测精度，并在实际应用中具有潜在价值。其重要性在于解决了自动化制造中图像质量受环境影响的挑战，有助于提升生产过程的自动化水平和精度。"}}
{"id": "2506.10047", "title": "GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models", "authors": ["Zilong Wang", "Xiang Zheng", "Xiaosen Wang", "Bo Wang", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and\nare now widely used in content creation. However, these models can be misused\nto generate harmful content, including nudity or violence, posing significant\nsafety risks. While most platforms employ content moderation systems,\nunderlying vulnerabilities can still be exploited by determined adversaries.\nRecent research on red-teaming and adversarial attacks against T2I models has\nnotable limitations: some studies successfully generate highly toxic images but\nuse adversarial prompts that are easily detected and blocked by safety filters,\nwhile others focus on bypassing safety mechanisms but fail to produce genuinely\nharmful outputs, neglecting the discovery of truly high-risk prompts.\nConsequently, there remains a lack of reliable tools for evaluating the safety\nof defended T2I models. To address this gap, we propose GenBreak, a framework\nthat fine-tunes a red-team large language model (LLM) to systematically explore\nunderlying vulnerabilities in T2I generators. Our approach combines supervised\nfine-tuning on curated datasets with reinforcement learning via interaction\nwith a surrogate T2I model. By integrating multiple reward signals, we guide\nthe LLM to craft adversarial prompts that enhance both evasion capability and\nimage toxicity, while maintaining semantic coherence and diversity. These\nprompts demonstrate strong effectiveness in black-box attacks against\ncommercial T2I generators, revealing practical and concerning safety\nweaknesses.", "comment": "27 pages, 7 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10047v1", "AI": {"title_translation": "GenBreak：使用大型语言模型对文本到图像生成器进行红队测试", "tldr": "GenBreak是一个利用微调LLM的框架，通过生成对抗性提示来系统地发现T2I模型中的安全漏洞，这些提示既能逃避检测又能产生有害内容。", "motivation": "现有的文本到图像（T2I）模型存在生成有害内容（如裸露或暴力）的风险，但目前的红队测试和对抗性攻击方法存在局限性，要么生成的有害图像容易被过滤，要么无法产生真正有害的输出，导致缺乏评估T2I模型安全性的可靠工具。", "method": "提出GenBreak框架，通过微调一个红队大型语言模型（LLM）来探索T2I生成器中的漏洞。该方法结合了在精选数据集上的监督微调和通过与替代T2I模型交互进行的强化学习。通过整合多个奖励信号，引导LLM生成既能增强规避能力又能提高图像毒性，同时保持语义连贯性和多样性的对抗性提示。", "result": "这些提示在针对商业T2I生成器的黑盒攻击中表现出强大的有效性，揭示了实际且令人担忧的安全弱点。", "conclusion": "GenBreak提供了一个评估防御性T2I模型安全性的可靠工具，揭示了商业T2I模型中存在的实际安全漏洞。", "translation": "文本到图像（T2I）模型如Stable Diffusion已迅速发展，并广泛应用于内容创作。然而，这些模型可能被滥用以生成有害内容，包括裸露或暴力，构成重大的安全风险。尽管大多数平台采用内容审核系统，但潜在的漏洞仍可能被有决心的对手利用。近期针对T2I模型的红队测试和对抗性攻击研究存在显著局限性：一些研究成功生成了高度有害的图像，但使用的对抗性提示很容易被安全过滤器检测和阻止；而另一些研究则专注于绕过安全机制，却未能产生真正有害的输出，忽视了发现真正高风险提示。因此，目前缺乏评估防御性T2I模型安全性的可靠工具。为了解决这一空白，我们提出了GenBreak，一个通过微调红队大型语言模型（LLM）来系统探索T2I生成器潜在漏洞的框架。我们的方法结合了在精选数据集上的监督微调和通过与替代T2I模型交互进行的强化学习。通过整合多个奖励信号，我们引导LLM生成既能增强规避能力又能提高图像毒性，同时保持语义连贯性和多样性的对抗性提示。这些提示在针对商业T2I生成器的黑盒攻击中表现出强大的有效性，揭示了实际且令人担忧的安全弱点。", "summary": "GenBreak是一个新颖的框架，旨在通过微调大型语言模型（LLM）来对文本到图像（T2I）生成器进行红队测试，以发现其安全漏洞。该框架结合了监督微调和强化学习，通过多重奖励信号引导LLM生成既能有效规避现有安全过滤器又能产生有害内容的对抗性提示。实验表明，GenBreak生成的提示在黑盒攻击中对商业T2I模型非常有效，揭示了其潜在的安全弱点。", "keywords": "红队测试, 文本到图像生成器, 大型语言模型, 安全评估, 对抗性提示", "comments": "GenBreak的创新之处在于其结合了LLM的强大生成能力和强化学习来系统地发现T2I模型的安全漏洞，特别是通过平衡规避能力和内容有害性来生成高质量的对抗性提示。这对于提升T2I模型的安全性评估具有重要意义。"}}
{"id": "2506.10322", "title": "Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis", "authors": ["Xueying Du", "Kai Yu", "Chong Wang", "Yi Zou", "Wentai Deng", "Zuoyu Ou", "Xin Peng", "Lingming Zhang", "Yiling Lou"], "summary": "Static bug analyzers play a crucial role in ensuring software quality.\nHowever, existing analyzers for bug detection in large codebases often suffer\nfrom high false positive rates. This is primarily due to the limited\ncapabilities of analyzers in path feasibility validation with multiple\nconditional branches and complex data dependencies. While current LLM-based\napproaches attempt to address this issue, their effectiveness remains limited\ndue to insufficient constraint cascade analysis and scalability challenges in\nlarge projects. To address this challenge, we propose an iterative path\nfeasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted\nconstraint reasoning, and key context-aware analysis driven by agent planning,\nLLM4PFA effectively enhances complex inter-procedural path feasibility analysis\nfor minimizing false positives in static bug detection. Evaluation results show\nthat LLM4PFA precisely filters out 72% to 96% false positives reported during\nstatic bug detection, significantly outperforming all the baselines by 41.1% -\n105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true\npositives.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10322v1", "AI": {"title_translation": "通过LLM增强路径可行性分析最小化静态错误检测中的误报", "tldr": "本文提出LLM4PFA框架，利用LLM代理增强路径可行性分析，显著降低了静态错误检测中的误报率。", "motivation": "现有静态错误分析器在大型代码库中存在高误报率问题，主要原因是其在多条件分支和复杂数据依赖的路径可行性验证能力有限。当前的基于LLM的方法也因约束级联分析不足和可扩展性差而效果有限。", "method": "本文提出了一个迭代的路径可行性分析框架LLM4PFA。该框架通过利用基于LLM代理的定向约束推理和由代理规划驱动的关键上下文感知分析，有效地增强了复杂的跨过程路径可行性分析，从而最小化静态错误检测中的误报。", "result": "评估结果显示，LLM4PFA精确过滤了静态错误检测中报告的72%至96%的误报，显著优于所有基线41.1%至105.7%；同时，LLM4PFA在45个真实错误中仅漏报了3个。", "conclusion": "LLM4PFA框架通过LLM增强的路径可行性分析，能够有效且精确地降低静态错误检测中的误报率，显著优于现有方法，并能保持对真实错误的较高检测能力。", "translation": "静态错误分析器在确保软件质量方面发挥着至关重要的作用。然而，现有的大型代码库错误检测分析器通常存在高误报率。这主要是由于分析器在多条件分支和复杂数据依赖的路径可行性验证方面的能力有限。尽管当前的基于LLM的方法试图解决这个问题，但由于约束级联分析不足和大型项目中的可扩展性挑战，其有效性仍然有限。为了解决这一挑战，我们提出了一个迭代的路径可行性分析框架LLM4PFA。通过利用基于LLM代理的定向约束推理和由代理规划驱动的关键上下文感知分析，LLM4PFA有效地增强了复杂的跨过程路径可行性分析，从而最小化静态错误检测中的误报。评估结果显示，LLM4PFA精确过滤了静态错误检测中报告的72%至96%的误报，显著优于所有基线41.1%至105.7%的改进；同时，LLM4PFA在45个真实错误中仅漏报了3个。", "summary": "本文提出LLM4PFA，一个迭代的路径可行性分析框架，旨在解决静态错误检测中高误报率的问题。通过利用基于LLM代理的定向约束推理和上下文感知分析，LLM4PFA显著增强了复杂跨过程路径的可行性分析能力。实验结果表明，该方法能有效过滤72%至96%的误报，性能远超现有基线，同时保持了对真实错误的低漏报率。", "keywords": "静态错误检测, 误报, LLM, 路径可行性分析, LLM代理", "comments": "本文的创新点在于利用LLM代理进行定向约束推理和上下文感知分析，以解决静态分析器在复杂路径可行性验证方面的核心难题，从而显著降低误报。量化结果表明其在实际应用中具有重要价值和显著改进。"}}
{"id": "2506.10587", "title": "IDEA: Augmenting Design Intelligence through Design Space Exploration", "authors": ["Chuer Chen", "Xiaoke Yan", "Xiaoyu Qi", "Nan Cao"], "summary": "Design spaces serve as a conceptual framework that enables designers to\nexplore feasible solutions through the selection and combination of design\nelements. However, effective decision-making remains heavily dependent on the\ndesigner's experience, and the absence of mathematical formalization prevents\ncomputational support for automated design processes. To bridge this gap, we\nintroduce a structured representation that models design spaces with orthogonal\ndimensions and discrete selectable elements. Building on this model, we present\nIDEA, a decision-making framework for augmenting design intelligence through\ndesign space exploration to generate effective outcomes. Specifically, IDEA\nleverages large language models (LLMs) for constraint generation, incorporates\na Monte Carlo Tree Search (MCTS) algorithm guided by these constraints to\nexplore the design space efficiently, and instantiates abstract decisions into\ndomain-specific implementations. We validate IDEA in two design scenarios:\ndata-driven article composition and pictorial visualization generation,\nsupported by example results, expert interviews, and a user study. The\nevaluation demonstrates the IDEA's adaptability across domains and its\ncapability to produce superior design outcomes.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10587v1", "AI": {"title_translation": "IDEA：通过设计空间探索增强设计智能", "tldr": "IDEA是一个决策框架，它利用大型语言模型和蒙特卡洛树搜索来探索设计空间，从而增强设计智能并生成有效的设计成果，已在文章撰写和图像可视化生成中得到验证。", "motivation": "设计空间探索中的有效决策严重依赖设计师的经验，且缺乏数学形式化支持自动化设计过程。本文旨在弥补这一空白，提供计算支持。", "method": "引入了一种结构化表示来建模具有正交维度和离散可选元素的设计空间。在此模型基础上，提出了IDEA框架，它利用大型语言模型（LLMs）生成约束，结合蒙特卡洛树搜索（MCTS）算法在这些约束的指导下高效探索设计空间，并将抽象决策实例化为领域特定的实现。", "result": "IDEA在数据驱动的文章撰写和图像可视化生成两个设计场景中得到了验证。评估结果表明IDEA在不同领域具有适应性，并且能够产生卓越的设计成果。", "conclusion": "IDEA框架通过结合LLMs和MCTS，能够有效地探索设计空间，增强设计智能，并在多个设计场景中产生卓越且适应性强的设计成果。", "translation": "设计空间作为一种概念框架，使设计师能够通过选择和组合设计元素来探索可行的解决方案。然而，有效的决策仍然严重依赖于设计师的经验，并且缺乏数学形式化阻碍了对自动化设计过程的计算支持。为了弥补这一差距，我们引入了一种结构化表示，用正交维度和离散可选元素来建模设计空间。在此模型的基础上，我们提出了IDEA，一个通过设计空间探索来增强设计智能以产生有效结果的决策框架。具体而言，IDEA利用大型语言模型（LLMs）生成约束，结合蒙特卡洛树搜索（MCTS）算法在这些约束的指导下高效探索设计空间，并将抽象决策实例化为领域特定的实现。我们在两种设计场景中验证了IDEA：数据驱动的文章撰写和图像可视化生成，并通过示例结果、专家访谈和用户研究进行支持。评估结果表明了IDEA在不同领域的适应性及其产生卓越设计成果的能力。", "summary": "本研究提出了IDEA框架，旨在通过设计空间探索增强设计智能。针对现有设计决策过度依赖经验和缺乏计算支持的问题，IDEA通过引入结构化设计空间模型，结合大型语言模型（LLMs）生成约束和蒙特卡洛树搜索（MCTS）算法高效探索设计空间。该框架将抽象决策实例化为领域特定实现。通过在文章撰写和图像可视化生成场景中的验证，证明了IDEA的跨领域适应性和产生优质设计成果的能力。", "keywords": "设计智能, 设计空间探索, 大型语言模型, 蒙特卡洛树搜索, 自动化设计", "comments": "本文的创新之处在于将大型语言模型（LLMs）与蒙特卡洛树搜索（MCTS）算法相结合，用于设计空间探索，从而实现了设计过程的自动化和智能化。这种方法有效地弥补了传统设计决策对人类经验的过度依赖，并通过计算支持提高了设计效率和成果质量。其跨领域验证也显示了该框架的广泛应用潜力。"}}
{"id": "2506.10252", "title": "A Novel Feedforward Youla Parameterization Method for Avoiding Local Minima in Stereo Image Based Visual Servoing Control", "authors": ["Rongfei Li", "Francis Assadian"], "summary": "In robot navigation and manipulation, accurately determining the camera's\npose relative to the environment is crucial for effective task execution. In\nthis paper, we systematically prove that this problem corresponds to the\nPerspective-3-Point (P3P) formulation, where exactly three known 3D points and\ntheir corresponding 2D image projections are used to estimate the pose of a\nstereo camera. In image-based visual servoing (IBVS) control, the system\nbecomes overdetermined, as the 6 degrees of freedom (DoF) of the stereo camera\nmust align with 9 observed 2D features in the scene. When more constraints are\nimposed than available DoFs, global stability cannot be guaranteed, as the\ncamera may become trapped in a local minimum far from the desired configuration\nduring servoing. To address this issue, we propose a novel control strategy for\naccurately positioning a calibrated stereo camera. Our approach integrates a\nfeedforward controller with a Youla parameterization-based feedback controller,\nensuring robust servoing performance. Through simulations, we demonstrate that\nour method effectively avoids local minima and enables the camera to reach the\ndesired pose accurately and efficiently.", "comment": "36 pages, 19 figures, Journal, Published in: Applied Sciences, 2025,\n  vol. 15, article 4991. For published version, see this http URL:\n  https://doi.org/10.3390/app15094991", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10252v1", "AI": {"title_translation": "一种避免立体图像视觉伺服控制中局部最小点的新型前馈Youla参数化方法", "tldr": "本文提出了一种结合前馈和Youla参数化反馈的新型控制策略，用于立体图像视觉伺服控制，以有效避免局部最小点，确保相机准确高效地到达期望姿态。", "motivation": "在图像视觉伺服控制（IBVS）中，由于系统约束多于自由度，导致全局稳定性无法保证，相机可能在伺服过程中陷入远离期望配置的局部最小点。因此，需要一种新的控制策略来解决这个问题。", "method": "本文提出了一种新颖的控制策略，用于精确定位校准的立体相机。该方法将前馈控制器与基于Youla参数化的反馈控制器相结合。", "result": "通过仿真验证，该方法有效避免了局部最小点，并使相机能够准确高效地到达期望姿态。", "conclusion": "通过将前馈控制器与Youla参数化反馈控制器相结合，本文提出了一种新型控制策略，成功解决了立体图像视觉伺服控制中局部最小点的问题，实现了精确高效的相机姿态控制。", "translation": "在机器人导航和操作中，准确确定相机相对于环境的姿态对于有效执行任务至关重要。本文系统地证明了这个问题对应于透视-3-点（P3P）公式，其中使用恰好三个已知3D点及其对应的2D图像投影来估计立体相机的姿态。在基于图像的视觉伺服（IBVS）控制中，系统变得过确定，因为立体相机的6个自由度（DoF）必须与场景中9个观测到的2D特征对齐。当施加的约束多于可用的自由度时，全局稳定性无法保证，因为相机在伺服过程中可能陷入远离期望配置的局部最小点。为了解决这个问题，我们提出了一种用于精确定位校准立体相机的新型控制策略。我们的方法将前馈控制器与基于Youla参数化的反馈控制器相结合，确保了鲁棒的伺服性能。通过仿真，我们证明了我们的方法有效避免了局部最小点，并使相机能够准确高效地到达期望姿态。", "summary": "本文针对立体图像视觉伺服（IBVS）控制中相机易陷入局部最小点的问题，提出了一种新型控制策略。该策略将前馈控制器与基于Youla参数化的反馈控制器相结合，旨在精确控制校准立体相机的姿态。仿真结果表明，该方法能够有效避免局部最小点，并使相机准确高效地达到期望姿态，从而解决了IBVS中全局稳定性难以保证的挑战。", "keywords": "视觉伺服, 局部最小点, Youla参数化, 前馈控制, 立体相机", "comments": "本文的创新点在于将前馈控制与Youla参数化反馈控制相结合，为立体图像视觉伺服系统提供了一种新颖且有效的解决方案，以克服局部最小点问题。这种方法对于提高机器人导航和操作的鲁棒性和精度具有重要意义。"}}
{"id": "2506.10119", "title": "Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers", "authors": ["Natanael Lucena", "Fábio S. da Silva", "Ricardo Rios"], "summary": "This paper presents a comparison of the performance of Convolutional Neural\nNetworks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying\nimages containing lesions of psoriasis and diseases similar to it. Models\npre-trained on ImageNet were adapted to a specific data set. Both achieved high\npredictive metrics, but the ViTs stood out for their superior performance with\nsmaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the\nbest results, with an f1-score of 96.4%, and is recommended as the most\nefficient architecture for automated psoriasis detection. This article\nreinforces the potential of ViTs for medical image classification tasks.", "comment": "12 pages, in Portuguese language, 2 figures, 2 tables, and 4\n  formulas. To be published in the Proceedings of the LII Brazilian Integrated\n  Software and Hardware Seminar 2025 (SEMISH 2025)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10119v1", "AI": {"title_translation": "银屑病检测中的计算机视觉应用：CNNs与Vision Transformers的比较方法", "tldr": "本文比较了CNN和ViT在银屑病多类别图像分类中的性能，发现ViT（特别是DaViT-B）在小型模型上表现更优，并推荐其用于自动化银屑病检测。", "motivation": "本文旨在比较卷积神经网络（CNNs）和视觉转换器（ViTs）在多类别图像分类任务中检测银屑病及其相似疾病病变图像的性能，以找到更高效的自动化检测方法。", "method": "研究采用预训练在ImageNet上的CNN和ViT模型，并将其调整适应于一个特定的银屑病图像数据集，然后对它们的性能进行了比较。", "result": "尽管CNN和ViT都取得了较高的预测指标，但ViT在较小模型尺寸下表现出更优越的性能。其中，Dual Attention Vision Transformer-Base (DaViT-B) 取得了最佳结果，f1-score达到96.4%。", "conclusion": "DaViT-B被推荐为自动化银屑病检测中最有效的架构。这项研究也强调了ViTs在医学图像分类任务中的巨大潜力。", "translation": "本文比较了卷积神经网络（CNNs）和视觉转换器（ViTs）在多类别图像分类任务中的性能，该任务涉及银屑病及其相似疾病的病变图像。在ImageNet上预训练的模型被调整适用于一个特定数据集。两者都取得了较高的预测指标，但ViTs在较小模型尺寸下表现出卓越的性能。Dual Attention Vision Transformer-Base (DaViT-B) 取得了最佳结果，f1-score达到96.4%，并被推荐为自动化银屑病检测中最有效的架构。本文强化了ViTs在医学图像分类任务中的潜力。", "summary": "本文对卷积神经网络（CNNs）和视觉转换器（ViTs）在银屑病及其相似疾病的图像多类别分类任务中的性能进行了比较研究。结果显示，尽管两种模型都表现良好，但ViTs在小型模型上展现出更优越的性能。特别是Dual Attention Vision Transformer-Base (DaViT-B) 取得了96.4%的f1-score，被推荐为自动化银屑病检测的最有效架构，这进一步证实了ViTs在医学图像分类领域的潜力。", "keywords": "银屑病检测, 计算机视觉, 卷积神经网络, Vision Transformer, 医学图像分类", "comments": "这篇论文的创新点在于直接比较了CNN和ViT这两种主流深度学习架构在特定医学图像（银屑病）分类任务上的表现，并明确指出ViT在效率和性能上的优势，尤其是在小型模型上。其重要性在于为未来自动化银屑病诊断提供了更优的架构选择，并为ViT在更广泛的医学图像分析应用中奠定了基础。"}}
{"id": "2506.10624", "title": "Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization", "authors": ["Lukas Jünger", "Jan Henrik Weinstock", "Tim Kraus"], "summary": "The ever-increasing complexity of HW/SW systems presents a persistent\nchallenge, particularly in safety-critical domains like automotive, where\nextensive testing is imperative. However, the availability of hardware often\nlags behind, hindering early-stage software development. To address this,\nVirtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a\npivotal solution, enabling pre-silicon execution and testing of unmodified\ntarget software. In this study, we propose an approach leveraging\ncontainerization to encapsulate VPs in order to reduce environment dependencies\nand enable cloud deployment for fast, parallelized test execution, as well as\nopen-source VP technologies such as QEMU and VCML to obviate the need for seat\nlicenses. To demonstrate the efficacy of our approach, we present an Artificial\nIntelligence (AI) accelerator VP case study. Through our research, we offer a\nrobust solution to address the challenges posed by the complexity of HW/SW\nsystems, with practical implications for accelerating HW/SW co-development.", "comment": "Published in DVCon China 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10624v1", "AI": {"title_translation": "快速虚拟平台中的可扩展软件测试：利用SystemC、QEMU和容器化技术", "tldr": "本研究提出一种利用容器化技术封装虚拟平台（VP）的方法，结合SystemC和QEMU等开源技术，以实现硬件/软件系统在云端进行快速、并行化和可扩展的测试，解决了硬件可用性滞后和复杂性带来的挑战。", "motivation": "硬件/软件系统日益复杂，尤其是在汽车等安全关键领域，需要大量测试。然而，硬件的可用性常常滞后，阻碍了早期软件开发。", "method": "提出一种利用容器化技术封装基于SystemC TLM-2.0标准的虚拟平台（VP）的方法，以减少环境依赖并支持云部署，实现快速、并行化的测试执行。同时，结合QEMU和VCML等开源VP技术，避免了许可证需求。", "result": "通过一个人工智能（AI）加速器VP的案例研究，证明了该方法的有效性。", "conclusion": "本研究提供了一个鲁棒的解决方案，以应对硬件/软件系统复杂性带来的挑战，对加速硬件/软件协同开发具有实际意义。", "translation": "硬件/软件系统日益增长的复杂性带来了持续的挑战，尤其是在汽车等安全关键领域，广泛的测试是必不可少的。然而，硬件的可用性常常滞后，阻碍了早期软件开发。为了解决这个问题，基于SystemC TLM-2.0标准的虚拟平台（VP）已成为一个关键解决方案，能够实现未经修改的目标软件的预硅执行和测试。在本研究中，我们提出了一种利用容器化技术封装VP的方法，以减少环境依赖并实现云部署，从而进行快速、并行化的测试执行，同时利用QEMU和VCML等开源VP技术，避免了许可证需求。为了证明我们方法的有效性，我们提出了一个人工智能（AI）加速器VP的案例研究。通过我们的研究，我们提供了一个鲁棒的解决方案，以应对硬件/软件系统复杂性带来的挑战，对加速硬件/软件协同开发具有实际意义。", "summary": "本研究提出一种创新的可扩展软件测试方法，通过将基于SystemC TLM-2.0的虚拟平台（VP）与容器化技术相结合，实现在云端进行快速、并行化的硬件/软件系统测试。该方法利用QEMU和VCML等开源VP技术，减少了环境依赖并避免了许可证成本，有效解决了硬件可用性滞后和系统复杂性带来的早期软件开发挑战。通过一个AI加速器VP的案例研究，验证了其在加速硬件/软件协同开发方面的有效性。", "keywords": "虚拟平台, 容器化, SystemC, QEMU, 软件测试", "comments": "该论文的创新点在于将容器化技术应用于虚拟平台（VP）测试，从而实现了云部署、并行化测试和环境依赖的减少。结合SystemC和QEMU等开源技术，降低了测试成本，提升了测试的可扩展性和效率，对于加速复杂硬件/软件系统的协同开发具有重要意义。"}}
{"id": "2506.10091", "title": "Efficient kernelized bandit algorithms via exploration distributions", "authors": ["Bingshan Hu", "Zheng He", "Danica J. Sutherland"], "summary": "We consider a kernelized bandit problem with a compact arm set ${X} \\subset\n\\mathbb{R}^d $ and a fixed but unknown reward function $f^*$ with a finite norm\nin some Reproducing Kernel Hilbert Space (RKHS). We propose a class of\ncomputationally efficient kernelized bandit algorithms, which we call\nGP-Generic, based on a novel concept: exploration distributions. This class of\nalgorithms includes Upper Confidence Bound-based approaches as a special case,\nbut also allows for a variety of randomized algorithms. With careful choice of\nexploration distribution, our proposed generic algorithm realizes a wide range\nof concrete algorithms that achieve $\\tilde{O}(\\gamma_T\\sqrt{T})$ regret\nbounds, where $\\gamma_T$ characterizes the RKHS complexity. This matches known\nresults for UCB- and Thompson Sampling-based algorithms; we also show that in\npractice, randomization can yield better practical results.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10091v1", "AI": {"title_translation": "通过探索分布的有效核化赌博机算法", "tldr": "本文提出了一类名为 GP-Generic 的高效核化赌博机算法，该算法基于探索分布的概念，实现了与现有方法相当的后悔界限，并证明了随机化在实践中能带来更好的结果。", "motivation": "研究动机是解决核化赌博机问题，并开发计算效率高的新型算法，以处理紧凑臂集和具有有限范数的未知奖励函数。", "method": "本文提出了一类名为 GP-Generic 的核化赌博机算法，该算法基于探索分布的新颖概念。该算法类包括基于上置信界（UCB）的方法，也支持多种随机化算法，通过精心选择探索分布，实现了多种具体算法。", "result": "本文提出的通用算法实现了 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 的后悔界限，其中 $\\gamma_T$ 表征了再生核希尔伯特空间（RKHS）的复杂性。这与基于 UCB 和 Thompson 采样的已知算法结果相匹配。此外，研究还表明，在实践中，随机化可以产生更好的实际结果。", "conclusion": "本文的结论是，通过引入探索分布的概念，GP-Generic 算法提供了一种高效且通用的核化赌博机解决方案，其理论性能与现有最佳算法相当，并且在实践中随机化能带来额外的优势。", "translation": "我们考虑一个核化赌博机问题，其臂集 $X \\subset \\mathbb{R}^d$ 是紧凑的，奖励函数 $f^*$ 是固定但未知的，并且在某个再生核希尔伯特空间（RKHS）中具有有限范数。我们提出了一类计算高效的核化赌博机算法，我们称之为 GP-Generic，它基于一个新颖的概念：探索分布。这类算法包括基于上置信界（UCB）的方法作为特例，但也允许各种随机化算法。通过精心选择探索分布，我们提出的通用算法实现了一系列具体的算法，这些算法达到了 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 的后悔界限，其中 $\\gamma_T$ 表征了 RKHS 的复杂性。这与基于 UCB 和 Thompson 采样的已知结果相匹配；我们还表明，在实践中，随机化可以产生更好的实际结果。", "summary": "本文针对核化赌博机问题，引入了基于探索分布的新型 GP-Generic 算法。该算法框架包含 UCB 等现有方法，并支持随机化策略。研究表明，GP-Generic 算法能达到与当前最优算法相同的 $\\tilde{O}(\\gamma_T\\sqrt{T})$ 后悔界限，并且在实际应用中，随机化方法能带来更优的性能。", "keywords": "核化赌博机, 探索分布, GP-Generic, 后悔界限, 随机化", "comments": "这篇论文的创新点在于引入了“探索分布”这一概念，并基于此提出了通用的 GP-Generic 算法框架，统一了包括 UCB 在内的多种核化赌博机算法。其重要性在于不仅实现了与现有最佳算法相当的理论性能，还通过实验指出随机化在实践中的潜在优势，为核化赌博机算法的设计提供了新的视角和实用指导。"}}
{"id": "2506.10523", "title": "HP2C-DT: High-Precision High-Performance Computer-enabled Digital Twin", "authors": ["E. Iraola", "M. García-Lorenzo", "F. Lordan-Gomis", "F. Rossi", "E. Prieto-Araujo", "R. M. Badia"], "summary": "Digital twins are transforming the way we monitor, analyze, and control\nphysical systems, but designing architectures that balance real-time\nresponsiveness with heavy computational demands remains a challenge.\nCloud-based solutions often struggle with latency and resource constraints,\nwhile edge-based approaches lack the processing power for complex simulations\nand data-driven optimizations.\n  To address this problem, we propose the High-Precision High-Performance\nComputer-enabled Digital Twin (HP2C-DT) reference architecture, which\nintegrates High-Performance Computing (HPC) into the computing continuum.\nUnlike traditional setups that use HPC only for offline simulations, HP2C-DT\nmakes it an active part of digital twin workflows, dynamically assigning tasks\nto edge, cloud, or HPC resources based on urgency and computational needs.\n  Furthermore, to bridge the gap between theory and practice, we introduce the\nHP2C-DT framework, a working implementation that uses COMPSs for seamless\nworkload distribution across diverse infrastructures. We test it in a power\ngrid use case, showing how it reduces communication bandwidth by an order of\nmagnitude through edge-side data aggregation, improves response times by up to\n2x via dynamic offloading, and maintains near-ideal strong scaling for\ncompute-intensive workflows across a practical range of resources. These\nresults demonstrate how an HPC-driven approach can push digital twins beyond\ntheir current limitations, making them smarter, faster, and more capable of\nhandling real-world complexity.", "comment": "15 pages, 5 figures. Submitted to Future Generation Computing Systems\n  journal", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10523v1", "AI": {"title_translation": "HP2C-DT：高精度高性能计算机赋能的数字孪生", "tldr": "提出HP2C-DT架构，将HPC整合到计算连续体中，动态分配任务以解决数字孪生中实时响应与计算需求之间的挑战，并通过电力网案例验证其在带宽、响应时间和扩展性方面的优势。", "motivation": "现有的数字孪生架构在实时响应与重计算需求之间难以平衡。基于云的方案面临延迟和资源限制，而基于边缘的方案缺乏处理复杂模拟和数据驱动优化的能力。", "method": "提出HP2C-DT参考架构，将高性能计算（HPC）整合到计算连续体中，使HPC成为数字孪生工作流的活跃部分，根据紧急性和计算需求动态地将任务分配给边缘、云或HPC资源。同时，引入HP2C-DT框架，这是一个使用COMPSs实现跨多样基础设施无缝工作负载分发的实际实现。", "result": "在电力网用例中，HP2C-DT通过边缘侧数据聚合将通信带宽降低了一个数量级，通过动态卸载将响应时间缩短了2倍，并在实际资源范围内为计算密集型工作流保持了接近理想的强扩展性。", "conclusion": "HPC驱动的方法能够将数字孪生推向其当前限制之外，使其更智能、更快、更能处理现实世界的复杂性。", "translation": "数字孪生正在改变我们监控、分析和控制物理系统的方式，但设计能够平衡实时响应与繁重计算需求的架构仍然是一个挑战。基于云的解决方案常常面临延迟和资源限制，而基于边缘的方法则缺乏复杂模拟和数据驱动优化所需的处理能力。\n为了解决这个问题，我们提出了高精度高性能计算机赋能的数字孪生（HP2C-DT）参考架构，它将高性能计算（HPC）整合到计算连续体中。与传统仅将HPC用于离线模拟的设置不同，HP2C-DT使其成为数字孪生工作流的活跃部分，根据紧急性和计算需求动态地将任务分配给边缘、云或HPC资源。\n此外，为了弥合理论与实践之间的差距，我们引入了HP2C-DT框架，这是一个使用COMPSs在不同基础设施之间实现无缝工作负载分发的实际实现。我们在一个电力网用例中对其进行了测试，结果表明它通过边缘侧数据聚合将通信带宽降低了一个数量级，通过动态卸载将响应时间提高了2倍，并为计算密集型工作流在实际资源范围内保持了接近理想的强扩展性。这些结果表明，HPC驱动的方法能够推动数字孪生超越其目前的局限性，使其更智能、更快、更能处理现实世界的复杂性。", "summary": "本文提出了HP2C-DT架构和框架，旨在解决数字孪生在实时响应和高计算需求之间的平衡问题。该架构将高性能计算（HPC）集成到计算连续体中，实现任务在边缘、云和HPC资源之间的动态分配。通过一个电力网用例，HP2C-DT展示了其在显著减少通信带宽、提高响应时间以及保持计算密集型工作流良好扩展性方面的有效性，从而推动数字孪生在处理复杂现实世界场景方面的能力。", "keywords": "数字孪生, 高性能计算, 计算连续体, 动态任务分配, 边缘计算", "comments": "这篇论文的创新点在于将高性能计算（HPC）从传统的离线模拟角色提升为数字孪生工作流中的动态、活跃组成部分，从而有效地平衡了实时响应能力和繁重计算需求。其提出的HP2C-DT架构和框架通过智能的任务分配机制，解决了现有云和边缘方案的局限性。通过实际用例的验证，论文展示了显著的性能提升，这对于推动数字孪生技术在工业和复杂系统监控中的实际应用具有重要意义。"}}
{"id": "2506.10304", "title": "The Alignment Trap: Complexity Barriers", "authors": ["Jasper Yao"], "summary": "We establish fundamental computational complexity barriers to verifying AI\nsafety as system capabilities scale. Our main results show that for AI systems\nwith expressiveness EXP$(m)$ above a critical threshold $\\tau$, safety\nverification requires exponential time and is coNP-complete. We formalize the\nCapability-Risk Scaling (CRS) dynamic, which demonstrates how increasing AI\ncapability drives societal safety requirements toward perfection, creating an\ninescapable tension with verification complexity. Through four core theorems,\nwe prove that (1) verification complexity grows exponentially with system\nexpressiveness, (2) safe policies comprise at most a $2^{-2^m}$ fraction of the\npolicy space, (3) no finite set of alignment techniques can provide universal\ncoverage, and (4) robust safety properties form measure-zero sets for neural\nnetworks. These results characterize an \"intractability gap\" where practical\nsafety requirements fall within the region of computational intractability. We\nconclude by presenting a strategic trilemma: AI development must either\nconstrain system complexity to maintain verifiable safety, accept unverifiable\nrisks while scaling capabilities, or develop fundamentally new safety paradigms\nbeyond verification. Our work provides the first systematic\ncomplexity-theoretic analysis of AI alignment and establishes rigorous bounds\nthat any safety approach must confront. A formal verification of the core\ntheorems in Lean4 is currently in progress.", "comment": "29 Pages, 4 Figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10304v1", "AI": {"title_translation": "对齐陷阱：复杂性障碍", "tldr": "AI系统能力提升时，其安全性验证面临指数级计算复杂性障碍，导致难以实现可验证的安全。", "motivation": "随着AI系统能力的提升，验证其安全性面临根本性的计算复杂性障碍，社会对其安全性的要求也趋于完美，这与验证的复杂性产生了不可避免的张力。", "method": "本文通过建立计算复杂性理论，形式化了能力-风险扩展（CRS）动态，并通过四个核心定理证明了相关结论。", "result": "1. 验证复杂性随系统表达能力呈指数增长。\n2. 安全策略在策略空间中仅占极小比例。\n3. 有限的对齐技术无法提供通用覆盖。\n4. 鲁棒安全属性对于神经网络形成测度为零的集合。\n这些结果揭示了一个“难处理性差距”，即实际安全要求落入计算上难以处理的区域。", "conclusion": "AI发展面临一个战略困境：要么限制系统复杂性以保持可验证的安全性，要么在扩展能力的同时接受不可验证的风险，要么开发超越验证的全新安全范式。", "translation": "我们建立了随着系统能力扩展而验证AI安全性的基本计算复杂性障碍。我们的主要结果表明，对于表达能力高于临界阈值 $\\tau$ 的AI系统，其安全性验证需要指数时间并且是 coNP-完全的。我们形式化了能力-风险扩展（CRS）动态，该动态表明AI能力的增长如何将社会安全要求推向完美，从而与验证复杂性产生不可避免的张力。通过四个核心定理，我们证明了 (1) 验证复杂性随系统表达能力呈指数增长，(2) 安全策略在策略空间中至多占 $2^{-2^m}$ 的比例，(3) 有限的对齐技术无法提供通用覆盖，以及 (4) 鲁棒安全属性对于神经网络形成测度为零的集合。这些结果刻画了一个“难处理性差距”，即实际安全要求落入计算上难以处理的区域。我们最后提出了一个战略困境：AI发展必须要么限制系统复杂性以保持可验证的安全性，要么在扩展能力的同时接受不可验证的风险，要么开发超越验证的全新安全范式。我们的工作首次对AI对齐进行了系统性的复杂性理论分析，并建立了任何安全方法都必须面对的严格界限。目前正在使用Lean4对核心定理进行形式化验证。", "summary": "本文研究了AI系统能力扩展时安全性验证面临的计算复杂性障碍。研究表明，当AI系统表达能力超过一定阈值时，安全性验证需要指数时间且为coNP-完全。通过形式化能力-风险扩展动态和四个核心定理，作者证明了验证复杂性随系统表达能力呈指数增长，安全策略空间极小，有限对齐技术无法通用覆盖，以及鲁棒安全属性的测度为零。这些发现揭示了AI安全验证中存在的“难处理性差距”，并提出了AI发展所面临的战略困境：限制复杂性、接受风险或开发新范式。", "keywords": "AI安全, 计算复杂性, 对齐, 可验证性, 难处理性", "comments": "这项工作首次从计算复杂性理论角度系统分析了AI对齐问题，建立了严谨的界限，揭示了AI安全性验证的根本性挑战。其提出的“难处理性差距”和“战略困境”对未来AI安全研究和发展具有重要指导意义，强调了在追求AI能力的同时，需正视其可验证性限制。"}}
{"id": "2506.10013", "title": "Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z", "authors": ["Yerin Doh", "Joonhyung Bae"], "summary": "This study introduces the media artwork Dear Passenger, Please Wear a Mask,\ndesigned to offer a layered exploration of single-use mask waste, which\nescalated during the COVID-19 pandemic. The piece reframes underappreciated\necological concerns by interweaving digital nostalgia and airline travel\nrecollections of Millennials and Gen Z with a unique fantasy narrative. Via a\npoint-and-click game and an immersive exhibition, participants traverse both\nvirtual and real domains, facing ethical and environmental dilemmas. While it\nfosters empathy and potential action, resource use and post-experience\nengagement challenges persist.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10013v1", "AI": {"title_translation": "基于数字怀旧的沉浸式幻想：为韩国千禧一代和Z世代打造的环境叙事", "tldr": "一项媒体艺术作品《亲爱的乘客，请戴好口罩》利用数字怀旧和奇幻叙事，探讨疫情期间的一次性口罩废弃物问题，旨在激发千禧一代和Z世代的环保同理心和行动，但仍面临资源使用和体验后参与的挑战。", "motivation": "本研究旨在探讨和重新审视被低估的生态问题，特别是COVID-19大流行期间急剧增加的一次性口罩废弃物。", "method": "本研究介绍了一种名为《亲爱的乘客，请戴好口罩》的媒体艺术作品。该作品通过点击式游戏和沉浸式展览，将千禧一代和Z世代的数字怀旧和航空旅行回忆与独特的奇幻叙事交织，引导参与者穿越虚拟和现实领域，面对伦理和环境困境。", "result": "该艺术作品成功培养了参与者的同理心和潜在的环保行动。", "conclusion": "尽管该作品培养了同理心和潜在的行动，但资源使用和体验后参与的挑战依然存在。", "translation": "本研究介绍了一种名为《亲爱的乘客，请戴好口罩》的媒体艺术作品，旨在分层探索在COVID-19大流行期间急剧增加的一次性口罩废弃物问题。该作品通过将千禧一代和Z世代的数字怀旧和航空旅行回忆与独特的奇幻叙事交织在一起，重新审视了被低估的生态问题。通过点击式游戏和沉浸式展览，参与者穿越虚拟和现实领域，面临伦理和环境困境。尽管它培养了同理心和潜在的行动，但资源使用和体验后参与的挑战依然存在。", "summary": "本研究展示了媒体艺术作品《亲爱的乘客，请戴好口罩》，旨在解决疫情期间一次性口罩废弃物激增的问题。该作品将千禧一代和Z世代的数字怀旧和旅行记忆与奇幻叙事融合在一个点击式游戏和沉浸式展览中。其目的是让参与者沉浸在环境困境中，培养同理心和潜在的行动，尽管资源和持续参与的挑战依然存在。", "keywords": "数字怀旧, 环境叙事, 口罩废弃物, 沉浸式艺术, 千禧一代和Z世代", "comments": "该研究的创新之处在于利用数字怀旧和奇幻叙事吸引年轻一代（千禧一代和Z世代）关注当代环境问题（口罩废弃物）。其重要性在于通过互动和沉浸式艺术体验重新构建生态问题。文中提及的局限性是资源使用和体验后参与的挑战持续存在。"}}
{"id": "2506.10675", "title": "ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation", "authors": ["Xi Chen", "Zhiqiang Shen", "Peng Cao", "Jinzhu Yang", "Osmar R. Zaiane"], "summary": "Medical images are usually collected from multiple domains, leading to domain\nshifts that impair the performance of medical image segmentation models. Domain\nGeneralization (DG) aims to address this issue by training a robust model with\nstrong generalizability. Recently, numerous domain randomization-based DG\nmethods have been proposed. However, these methods suffer from the following\nlimitations: 1) constrained efficiency of domain randomization due to their\nexclusive dependence on image style perturbation, and 2) neglect of the adverse\neffects of over-augmented images on model training. To address these issues, we\npropose a novel domain randomization-based DG method, called content style\naugmentation (ConStyX), for generalizable medical image segmentation.\nSpecifically, ConStyX 1) augments the content and style of training data,\nallowing the augmented training data to better cover a wider range of data\ndomains, and 2) leverages well-augmented features while mitigating the negative\neffects of over-augmented features during model training. Extensive experiments\nacross multiple domains demonstrate that our ConStyX achieves superior\ngeneralization performance. The code is available at\nhttps://github.com/jwxsp1/ConStyX.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10675v1", "AI": {"title_translation": "ConStyX: 内容风格增强用于可泛化医学图像分割", "tldr": "ConStyX通过内容和风格增强来提高医学图像分割模型的泛化能力，解决了现有域随机化方法的局限性。", "motivation": "医学图像通常从多个域收集，导致域偏移，从而损害医学图像分割模型的性能。现有的域随机化领域泛化（DG）方法存在局限性：1) 仅依赖图像风格扰动导致域随机化效率受限；2) 忽略过度增强图像对模型训练的不利影响。", "method": "提出了一种名为内容风格增强（ConStyX）的新型域随机化领域泛化（DG）方法。ConStyX通过以下两点解决现有问题：1) 增强训练数据的内容和风格，使增强后的训练数据能够更好地覆盖更广泛的数据域；2) 利用良好增强的特征，同时减轻过度增强特征在模型训练中的负面影响。", "result": "在多个域上进行的广泛实验表明，ConStyX实现了卓越的泛化性能。", "conclusion": "ConStyX是一种有效且能显著提升医学图像分割模型泛化性能的方法。", "translation": "医学图像通常从多个领域收集，导致领域偏移，从而损害医学图像分割模型的性能。领域泛化（DG）旨在通过训练具有强大泛化能力的鲁棒模型来解决这个问题。最近，许多基于领域随机化的DG方法被提出。然而，这些方法存在以下局限性：1）由于它们仅依赖图像风格扰动，导致领域随机化的效率受限；2）忽略了过度增强图像对模型训练的不利影响。为了解决这些问题，我们提出了一种新颖的基于领域随机化的DG方法，称为内容风格增强（ConStyX），用于可泛化医学图像分割。具体来说，ConStyX 1）增强了训练数据的内容和风格，使增强的训练数据能够更好地覆盖更广泛的数据领域；2）在利用良好增强特征的同时，减轻了过度增强特征在模型训练中的负面影响。跨多个领域的广泛实验表明，我们的ConStyX实现了卓越的泛化性能。代码可在https://github.com/jwxsp1/ConStyX 获取。", "summary": "本文提出了一种名为ConStyX的新型域随机化域泛化方法，旨在解决医学图像分割中因域偏移导致的性能下降问题。针对现有域随机化方法在效率和过度增强负面影响方面的局限性，ConStyX通过同时增强训练数据的内容和风格来扩大数据域覆盖范围，并有效利用良好增强特征同时减轻过度增强的负面影响。实验结果表明，ConStyX在多域医学图像分割任务中展现出优越的泛化性能。", "keywords": "域泛化, 医学图像分割, 内容风格增强, 域随机化, 泛化性能", "comments": "ConStyX的创新点在于其结合了内容与风格的增强策略，并考虑了过度增强对模型训练的潜在负面影响，这提升了其在域泛化方面的鲁棒性。该方法对于提升医学图像分割模型在多样化临床数据环境下的泛化能力具有重要意义。"}}
{"id": "2506.10139", "title": "Unsupervised Elicitation of Language Models", "authors": ["Jiaxin Wen", "Zachary Ankner", "Arushi Somani", "Peter Hase", "Samuel Marks", "Jacob Goldman-Wetzler", "Linda Petrini", "Henry Sleight", "Collin Burns", "He He", "Shi Feng", "Ethan Perez", "Jan Leike"], "summary": "To steer pretrained language models for downstream tasks, today's\npost-training paradigm relies on humans to specify desired behaviors. However,\nfor models with superhuman capabilities, it is difficult or impossible to get\nhigh-quality human supervision. To address this challenge, we introduce a new\nunsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune\npretrained language models on their own generated labels, \\emph{without\nexternal supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward\nmodeling tasks, our method matches the performance of training on golden\nsupervision and outperforms training on crowdsourced human supervision. On\ntasks where LMs' capabilities are strongly superhuman, our method can elicit\nthose capabilities significantly better than training on human labels. Finally,\nwe show that our method can improve the training of frontier LMs: we use our\nmethod to train an unsupervised reward model and use reinforcement learning to\ntrain a Claude 3.5 Haiku-based assistant. Both the reward model and the\nassistant outperform their human-supervised counterparts.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10139v1", "AI": {"title_translation": "无监督语言模型引导", "tldr": "提出ICM无监督算法，通过模型自生成标签微调LLM，在多项任务上超越人工监督，尤其能更好地引导超人能力模型。", "motivation": "当前的后训练范式依赖人类指定期望行为，但对于具有超人能力的模型，获取高质量的人类监督变得困难或不可能。", "method": "引入了一种新的无监督算法——内部一致性最大化（Internal Coherence Maximization, ICM），用于在没有外部监督的情况下，使用模型自身生成的标签来微调预训练语言模型。", "result": "在GSM8k-verification、TruthfulQA和Alpaca奖励建模任务上，性能与黄金监督训练相当，优于众包人类监督训练。在语言模型能力远超人类的任务上，该方法能显著优于人类标签训练来引导这些能力。该方法可用于改进前沿LM的训练：训练了一个无监督奖励模型，并使用强化学习训练了一个基于Claude 3.5 Haiku的助手，两者都优于其人类监督的对应物。", "conclusion": "ICM是一种有效的无监督方法，能够成功引导语言模型，甚至在人类监督难以提供高质量反馈的超人能力任务上表现出色，并能提升前沿LM的训练效果。", "translation": "为了引导预训练语言模型用于下游任务，当前的后训练范式依赖人类指定期望行为。然而，对于具有超人能力的模型，获取高质量的人类监督变得困难或不可能。为了解决这一挑战，我们引入了一种新的无监督算法——内部一致性最大化（ICM），用于在没有外部监督的情况下，使用模型自身生成的标签来微调预训练语言模型。在GSM8k-verification、TruthfulQA和Alpaca奖励建模任务上，我们的方法与使用黄金监督训练的性能相当，并优于使用众包人类监督训练。在语言模型能力远超人类的任务上，我们的方法能够比使用人类标签训练更好地引导这些能力。最后，我们展示了我们的方法可以改进前沿语言模型的训练：我们使用我们的方法训练了一个无监督奖励模型，并使用强化学习训练了一个基于Claude 3.5 Haiku的助手。奖励模型和助手都优于其人类监督的对应物。", "summary": "本文提出了一种名为内部一致性最大化（ICM）的无监督算法，旨在解决当前语言模型后训练范式中对高质量人类监督的依赖问题，尤其是在模型展现出超人能力时。ICM通过让语言模型利用自身生成的标签进行微调，无需外部监督。实验结果表明，该方法在多个任务上能达到甚至超越人工监督的性能，特别是在引导模型超人能力方面表现更佳，并且成功应用于前沿语言模型的奖励模型和助手训练，均超越了人类监督版本。", "keywords": "无监督学习, 语言模型, 内部一致性最大化, 模型对齐, 超人能力", "comments": "该论文提出了一种创新的无监督方法ICM，解决了大型语言模型在超人能力任务上难以获得高质量人类监督的关键挑战。其核心思想是利用模型自身的内部一致性来生成训练信号，这在数据标注成本高昂且专业知识稀缺的背景下具有重要意义。该方法不仅在多个基准测试中表现出色，还成功应用于前沿LLM的训练，展示了其在实际应用中的潜力。这可能为未来LLM的对齐和能力引导开辟新的路径，减少对昂贵且有时受限的人类反馈的依赖。"}}
{"id": "2506.10580", "title": "Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture", "authors": ["Chengxu Zuo", "Jiawei Huang", "Xiao Jiang", "Yuan Yao", "Xiangren Shi", "Rui Cao", "Xinyu Yi", "Feng Xu", "Shihui Guo", "Yipeng Qin"], "summary": "In this paper, we propose a novel dynamic calibration method for sparse\ninertial motion capture systems, which is the first to break the restrictive\nabsolute static assumption in IMU calibration, i.e., the coordinate drift RG'G\nand measurement offset RBS remain constant during the entire motion, thereby\nsignificantly expanding their application scenarios. Specifically, we achieve\nreal-time estimation of RG'G and RBS under two relaxed assumptions: i) the\nmatrices change negligibly in a short time window; ii) the human movements/IMU\nreadings are diverse in such a time window. Intuitively, the first assumption\nreduces the number of candidate matrices, and the second assumption provides\ndiverse constraints, which greatly reduces the solution space and allows for\naccurate estimation of RG'G and RBS from a short history of IMU readings in\nreal time. To achieve this, we created synthetic datasets of paired RG'G, RBS\nmatrices and IMU readings, and learned their mappings using a Transformer-based\nmodel. We also designed a calibration trigger based on the diversity of IMU\nreadings to ensure that assumption ii) is met before applying our method. To\nour knowledge, we are the first to achieve implicit IMU calibration (i.e.,\nseamlessly putting IMUs into use without the need for an explicit calibration\nprocess), as well as the first to enable long-term and accurate motion capture\nusing sparse IMUs. The code and dataset are available at\nhttps://github.com/ZuoCX1996/TIC.", "comment": "Accepted by SIGGRAPH 2025 (TOG)", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.10580v1", "AI": {"title_translation": "Transformer IMU 校准器：用于惯性运动捕捉的动态在体IMU校准", "tldr": "本文提出了一种基于Transformer的动态IMU校准方法，首次打破了传统的静态校准假设，实现了隐式、长期且准确的惯性运动捕捉。", "motivation": "现有的IMU校准方法依赖于限制性的绝对静态假设（即坐标漂移和测量偏移在整个运动过程中保持不变），这极大地限制了其应用场景。", "method": "该方法在两个宽松假设下（矩阵在短时间内变化可忽略不计；人体运动/IMU读数在短时间内是多样化的）实时估计坐标漂移（RG'G）和测量偏移（RBS）。通过创建RG'G、RBS矩阵和IMU读数的合成数据集，并使用基于Transformer的模型学习它们的映射。同时设计了一个基于IMU读数多样性的校准触发器，以确保满足运动多样性假设。", "result": "实现了RG'G和RBS的实时估计。首次实现了隐式IMU校准（无需明确校准过程即可无缝使用IMU）。首次使用稀疏IMU实现了长期准确的运动捕捉。", "conclusion": "该研究提出的动态校准方法通过打破传统的静态校准假设，显著扩展了稀疏惯性运动捕捉系统的应用场景，并首次实现了隐式和长期准确的运动捕捉。", "translation": "在本文中，我们提出了一种新颖的稀疏惯性运动捕捉系统动态校准方法，这是首次打破IMU校准中限制性的绝对静态假设，即坐标漂移RG'G和测量偏移RBS在整个运动过程中保持不变，从而显著扩展了其应用场景。具体来说，我们在两个宽松的假设下实现了RG'G和RBS的实时估计：i) 在短时间内矩阵变化可以忽略不计；ii) 在此时间窗口内人体运动/IMU读数是多样化的。直观地，第一个假设减少了候选矩阵的数量，第二个假设提供了多样化的约束，这大大缩小了解决方案空间，并允许从短时间的IMU读数历史中实时准确估计RG'G和RBS。为了实现这一点，我们创建了配对的RG'G、RBS矩阵和IMU读数的合成数据集，并使用基于Transformer的模型学习它们的映射。我们还设计了一个基于IMU读数多样性的校准触发器，以确保在应用我们的方法之前满足假设ii)。据我们所知，我们是第一个实现隐式IMU校准（即无需明确校准过程即可无缝使用IMU），也是第一个使用稀疏IMU实现长期准确运动捕捉的。代码和数据集可在https://github.com/ZuoCX1996/TIC获取。", "summary": "本文介绍了Transformer IMU校准器，这是一种用于稀疏惯性运动捕捉的新型动态校准方法。它通过在矩阵变化可忽略和运动多样性的宽松假设下实时估计坐标漂移和测量偏移，打破了传统的静态校准假设。该方法利用基于Transformer的模型对合成数据进行训练，并结合基于多样性的触发器，实现了隐式、长期和准确的运动捕捉，从而显著扩展了应用场景。", "keywords": "动态校准, IMU, 惯性运动捕捉, Transformer, 隐式校准", "comments": "这项研究的关键创新在于打破了IMU校准中长期存在的限制性静态假设，这是惯性运动捕捉领域的一个重要突破。利用Transformer模型进行动态参数估计，以及实现“隐式校准”的概念，极大地简化了IMU系统的使用流程，并提升了其在实际应用中的鲁棒性。这对于推动IMU在更广泛场景下的应用具有重要意义。"}}
{"id": "2506.10635", "title": "Conversational Search: From Fundamentals to Frontiers in the LLM Era", "authors": ["Fengran Mo", "Chuan Meng", "Mohammad Aliannejadi", "Jian-Yun Nie"], "summary": "Conversational search enables multi-turn interactions between users and\nsystems to fulfill users' complex information needs. During this interaction,\nthe system should understand the users' search intent within the conversational\ncontext and then return the relevant information through a flexible,\ndialogue-based interface. The recent powerful large language models (LLMs) with\ncapacities of instruction following, content generation, and reasoning, attract\nsignificant attention and advancements, providing new opportunities and\nchallenges for building up intelligent conversational search systems. This\ntutorial aims to introduce the connection between fundamentals and the emerging\ntopics revolutionized by LLMs in the context of conversational search. It is\ndesigned for students, researchers, and practitioners from both academia and\nindustry. Participants will gain a comprehensive understanding of both the core\nprinciples and cutting-edge developments driven by LLMs in conversational\nsearch, equipping them with the knowledge needed to contribute to the\ndevelopment of next-generation conversational search systems.", "comment": "Accepted by Tutorial Track in SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10635v1", "AI": {"title_translation": "对话式搜索：从基础到LLM时代的未来前沿", "tldr": "本教程旨在介绍对话式搜索的基础知识以及大型语言模型（LLMs）如何推动其发展，为学生、研究人员和从业者提供构建下一代对话式搜索系统所需的知识。", "motivation": "对话式搜索允许用户与系统进行多轮交互以满足复杂的信息需求，而大型语言模型（LLMs）的兴起为构建智能对话式搜索系统带来了新的机遇和挑战。本教程旨在连接对话式搜索的基础知识与LLMs驱动的新兴主题。", "method": "本教程通过介绍对话式搜索的基本原理和LLMs在其发展中的前沿应用，来连接基础知识与新兴主题。", "result": "参与者将全面了解对话式搜索的核心原则以及LLMs驱动的尖端发展，掌握开发下一代对话式搜索系统所需的知识。", "conclusion": "本教程旨在为学生、研究人员和从业者提供关于对话式搜索基础知识和LLM时代前沿发展的全面理解，帮助他们为下一代对话式搜索系统的发展做出贡献。", "translation": "对话式搜索使用户和系统之间能够进行多轮交互，以满足用户复杂的信息需求。在此交互过程中，系统应在对话上下文中理解用户的搜索意图，然后通过灵活的、基于对话的界面返回相关信息。最近强大的大型语言模型（LLMs）凭借其遵循指令、内容生成和推理的能力，吸引了广泛的关注和进展，为构建智能对话式搜索系统提供了新的机遇和挑战。本教程旨在介绍对话式搜索背景下，基础知识与LLMs所带来的革命性新兴主题之间的联系。它专为学术界和工业界的学生、研究人员和从业者设计。参与者将全面了解对话式搜索的核心原则以及LLMs驱动的尖端发展，掌握为下一代对话式搜索系统发展做出贡献所需的知识。", "summary": "本教程旨在探讨对话式搜索的基础知识及其在大型语言模型（LLMs）时代的前沿发展。它强调了LLMs在理解用户意图、生成内容和推理方面的能力如何为构建智能对话式搜索系统带来新的机遇和挑战。该教程面向学生、研究人员和从业者，旨在提供全面的知识，帮助他们理解并参与到下一代对话式搜索系统的开发中。", "keywords": "对话式搜索, 大型语言模型, LLMs, 信息检索, 人机交互", "comments": "这篇论文（或教程）的重要性在于它及时地将对话式搜索这一复杂领域与当前热门且强大的大型语言模型（LLMs）相结合。它不仅回顾了基础知识，更着眼于LLMs如何革新这一领域，为未来的研究和应用指明了方向。对于希望进入或深入了解对话式搜索与LLMs交叉领域的学习者和从业者来说，这是一份非常有价值的资源。"}}
{"id": "2506.10499", "title": "Convergence of adaptive boundary element methods driven by functional a posteriori error estimates", "authors": ["Alexander Freiszlinger", "Dirk Pauly", "Dirk Praetorius"], "summary": "The recent work [Kurz et al., Numer. Math., 147 (2021)] proposed functional a\nposteriori error estimates for boundary element methods (BEMs) together with a\nrelated adaptive mesh-refinement strategy. Unlike most a posteriori BEM error\nestimators, the proposed functional error estimators cover Galerkin as well as\ncollocation BEM and, more importantly, do not control the error in the integral\ndensity on the boundary, but the error of the potential approximation in the\ndomain, which is of greater relevance in practice. The estimates rely on the\nnumerical solution of auxiliary problems on auxiliary strip domains along the\nboundary, where the strips are affected by the adaptive mesh-refinement and\nhence vary. For Galerkin BEM, we prove that the proposed adaptive\nmesh-refinement algorithm yields convergence of the potential error to zero.\nDue to the structural difference to residual-based estimators, the proof\nrequires new ideas.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10499v1", "AI": {"title_translation": "自适应边界元方法收敛性，由泛函后验误差估计驱动", "tldr": "本文证明了基于泛函后验误差估计的自适应网格细化算法能使Galerkin边界元方法中的势误差收敛到零。", "motivation": "现有的大多数边界元方法（BEM）后验误差估计器控制的是边界上的积分密度误差，而不是在实际中更相关的域内势近似误差。此外，它们可能不覆盖Galerkin和搭配BEM。本文旨在解决这些局限性，提出一种控制域内势误差的泛函后验误差估计方法。", "method": "本文利用最近提出的泛函后验误差估计和相关的自适应网格细化策略。这些估计依赖于在沿边界的辅助条带域上数值求解辅助问题，这些条带域会随自适应网格细化而变化。", "result": "对于Galerkin边界元方法，本文证明了所提出的自适应网格细化算法使得势误差收敛到零。", "conclusion": "基于泛函后验误差估计的自适应网格细化算法能够确保Galerkin边界元方法中势误差的收敛性。由于其结构与基于残差的估计器不同，该证明需要新的思想。", "translation": "最近的工作 [Kurz et al., Numer. Math., 147 (2021)] 提出了用于边界元方法 (BEM) 的泛函后验误差估计以及相关的自适应网格细化策略。与大多数后验 BEM 误差估计器不同，所提出的泛函误差估计器涵盖了 Galerkin 和搭配 BEM，更重要的是，它们不控制边界上的积分密度误差，而是控制域内势近似的误差，这在实践中具有更大的相关性。这些估计依赖于沿边界的辅助条带域上辅助问题的数值解，其中条带受自适应网格细化的影响，因此是变化的。对于 Galerkin BEM，我们证明了所提出的自适应网格细化算法使得势误差收敛到零。由于其与基于残差的估计器的结构差异，该证明需要新的思想。", "summary": "本文研究了基于泛函后验误差估计的自适应边界元方法。与传统方法不同，该泛函误差估计器能够控制域内势近似误差，并且适用于Galerkin和搭配边界元方法。文章证明了对于Galerkin边界元方法，所提出的自适应网格细化算法能够确保势误差收敛到零，且该证明引入了新的方法。", "keywords": "边界元方法, 后验误差估计, 自适应网格细化, Galerkin BEM, 误差收敛性", "comments": "这项工作的重要性在于其提出的泛函后验误差估计能够直接控制在实际应用中更受关注的域内势近似误差，而非传统方法所控制的边界积分密度误差。此外，它同时适用于Galerkin和搭配BEM。证明方法的创新性也值得关注，因为它与传统的基于残差的估计器有结构性差异，需要新的理论思路。"}}
{"id": "2506.10754", "title": "BNMusic: Blending Environmental Noises into Personalized Music", "authors": ["Chi Zuo", "Martin B. Møller", "Pablo Martínez-Nuevo", "Huayang Huang", "Yu Wu", "Ye Zhu"], "summary": "While being disturbed by environmental noises, the acoustic masking technique\nis a conventional way to reduce the annoyance in audio engineering that seeks\nto cover up the noises with other dominant yet less intrusive sounds. However,\nmisalignment between the dominant sound and the noise-such as mismatched\ndownbeats-often requires an excessive volume increase to achieve effective\nmasking. Motivated by recent advances in cross-modal generation, in this work,\nwe introduce an alternative method to acoustic masking, aiming to reduce the\nnoticeability of environmental noises by blending them into personalized music\ngenerated based on user-provided text prompts. Following the paradigm of music\ngeneration using mel-spectrogram representations, we propose a Blending Noises\ninto Personalized Music (BNMusic) framework with two key stages. The first\nstage synthesizes a complete piece of music in a mel-spectrogram representation\nthat encapsulates the musical essence of the noise. In the second stage, we\nadaptively amplify the generated music segment to further reduce noise\nperception and enhance the blending effectiveness, while preserving auditory\nquality. Our experiments with comprehensive evaluations on MusicBench,\nEPIC-SOUNDS, and ESC-50 demonstrate the effectiveness of our framework,\nhighlighting the ability to blend environmental noise with rhythmically\naligned, adaptively amplified, and enjoyable music segments, minimizing the\nnoticeability of the noise, thereby improving overall acoustic experiences.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.10754v1", "AI": {"title_translation": "BNMusic：将环境噪声融入个性化音乐", "tldr": "BNMusic提出了一种将环境噪声融入个性化音乐的新方法，通过生成与噪声节奏对齐的音乐并自适应放大，有效降低噪声的可感知性，提高听觉体验。", "motivation": "传统的声学掩蔽技术在处理环境噪声时，由于主导声音与噪声之间可能存在错位（如节拍不匹配），往往需要过高的音量才能有效掩蔽。受跨模态生成最新进展的启发，本文旨在寻找一种替代声学掩蔽的方法，通过将环境噪声融入基于用户文本提示生成的个性化音乐中来降低噪声的可感知性。", "method": "本文提出了一个名为“将噪声融入个性化音乐”（BNMusic）的框架，该框架包含两个关键阶段。第一阶段，BNMusic在梅尔频谱表示中合成一段完整的音乐，这段音乐能够封装噪声的音乐本质。第二阶段，BNMusic自适应地放大生成的音乐片段，以进一步降低噪声感知并增强融合效果，同时保持听觉质量。", "result": "在MusicBench、EPIC-SOUNDS和ESC-50数据集上进行的综合评估实验证明了BNMusic框架的有效性，突出了其将环境噪声与节奏对齐、自适应放大且令人愉悦的音乐片段融合的能力，从而最大程度地降低了噪声的可感知性。", "conclusion": "BNMusic框架通过将环境噪声融入节奏对齐、自适应放大且令人愉悦的个性化音乐中，有效降低了噪声的可感知性，显著改善了整体听觉体验。", "translation": "虽然受到环境噪声的干扰，但声学掩蔽技术是音频工程中一种传统的降低烦恼的方法，它试图用其他主导但侵入性较小的声音来覆盖噪声。然而，主导声音与噪声之间的错位——例如节拍不匹配——通常需要过度增加音量才能实现有效的掩蔽。受跨模态生成最新进展的启发，在这项工作中，我们引入了一种替代声学掩蔽的方法，旨在通过将环境噪声融入基于用户提供的文本提示生成的个性化音乐中来降低环境噪声的可感知性。遵循使用梅尔频谱表示的音乐生成范式，我们提出了一个名为“将噪声融入个性化音乐”（BNMusic）的框架，该框架包含两个关键阶段。第一阶段，BNMusic在梅尔频谱表示中合成一段完整的音乐，这段音乐封装了噪声的音乐本质。在第二阶段，我们自适应地放大生成的音乐片段，以进一步降低噪声感知并增强融合效果，同时保持听觉质量。我们在MusicBench、EPIC-SOUNDS和ESC-50上进行的综合评估实验证明了我们框架的有效性，突出了其将环境噪声与节奏对齐、自适应放大且令人愉悦的音乐片段融合的能力，从而最大程度地降低了噪声的可感知性，从而改善了整体听觉体验。", "summary": "本文提出BNMusic框架，旨在通过将环境噪声融入个性化音乐来降低其可感知性。针对传统声学掩蔽的局限性，BNMusic利用跨模态生成技术，分两阶段合成音乐：首先生成封装噪声音乐本质的梅尔频谱音乐，然后自适应放大以增强融合效果并保持音质。实验证明，该方法能有效融合噪声与节奏对齐的音乐，提升听觉体验。", "keywords": "环境噪声, 个性化音乐, 声学掩蔽, 音乐生成, 梅尔频谱", "comments": "BNMusic提出了一种新颖的噪声处理方法，跳脱了传统掩蔽的思路，转而通过“融合”而非“覆盖”来降低噪声感知。其创新点在于利用音乐生成技术，特别是将噪声的音乐本质融入生成音乐，并通过自适应放大进一步优化。这为个性化音频体验和噪声管理提供了新的视角和可能性。"}}
{"id": "2506.10554", "title": "Downlink CSIT under Compressed Feedback: Joint vs. Separate Source-Channel Coding", "authors": ["Yi Song", "Tianyu Yang", "Mahdi Barzegar Khalilsarai", "Giuseppe Caire"], "summary": "The acquisition of Downlink (DL) channel state information at the transmitter\n(CSIT) is known to be a challenging task in multiuser massive MIMO systems when\nuplink/downlink channel reciprocity does not hold (e.g., in frequency division\nduplexing systems). From a coding viewpoint, the DL channel state acquired at\nthe users via DL training can be seen as an information source that must be\nconveyed to the base station via the UL communication channels. The\ntransmission of a source through a channel can be accomplished either by\nseparate or joint source-channel coding (SSCC or JSCC). In this work, using\nclassical remote distortion-rate (DR) theory, we first provide a theoretical\nlower bound on the channel estimation mean-square-error (MSE) of both JSCC and\nSSCC-based feedback schemes, which however requires encoding of large blocks of\nsuccessive channel states and thus cannot be used in practicesince it would\nincur in an extremely large feedback delay. We then focus on the relevant case\nof minimal (one slot) feedback delay and propose a practical JSCC-based\nfeedback scheme that fully exploits the channel second-order statistics to\noptimize the dimension projection in the eigenspace. We analyze the large SNR\nbehavior of the proposed JSCC-based scheme in terms of the quality scaling\nexponent (QSE). Given the second-order statistics of channel estimation of any\nfeedback scheme, we further derive the closed-form of the lower bound to the\nergodic sum-rate for DL data transmission under maximum ratio transmission and\nzero-forcing precoding. Via extensive numerical results, we show that our\nproposed JSCC-based scheme outperforms known JSCC, SSCC baseline and deep\nlearning-based schemes and is able to approach the performance of the optimal\nDR scheme in the range of practical SNR.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10554v1", "AI": {"title_translation": "下行链路压缩反馈下的CSIT：联合信源-信道编码与分离信源-信道编码", "tldr": "提出了一种实用的联合信源-信道编码（JSCC）方案，用于大规模MIMO系统中下行链路CSIT的压缩反馈，该方案在实际SNR范围内表现优于现有方案并接近理论最优。", "motivation": "在大规模MIMO系统中，当下行链路CSIT获取具有挑战性时（例如在FDD系统中），需要将用户获取的下行链路信道状态信息（CSI）通过上行链路反馈给基站。传统的单独信源-信道编码（SSCC）或联合信源-信道编码（JSCC）是实现这一目标的方法，但需要解决实际应用中的反馈延迟和性能优化问题。", "method": "首先，利用经典的失真率（DR）理论，为JSCC和SSCC反馈方案的信道估计均方误差（MSE）提供了理论下限，但该方法因反馈延迟过大而不实用。其次，针对最小反馈延迟（一个时隙）的情况，提出了一种实用的基于JSCC的反馈方案，该方案充分利用信道二阶统计量来优化特征空间的维度投影，并分析了该方案在大信噪比（SNR）下的质量标度指数（QSE）行为。此外，还推导了在最大比传输和零强迫预编码下，下行链路数据传输的遍历和速率下限的闭合形式。", "result": "广泛的数值结果表明，所提出的基于JSCC的方案优于已知的JSCC、SSCC基线以及基于深度学习的方案，并且能够在实际SNR范围内接近最优的失真率（DR）方案的性能。", "conclusion": "所提出的基于JSCC的反馈方案在压缩反馈下，能够有效且高效地获取下行链路CSIT，在性能上超越了现有方案并接近理论最优。", "translation": "下行链路（DL）发射机信道状态信息（CSIT）的获取在多用户大规模MIMO系统中是一个具有挑战性的任务，尤其是在上行/下行链路信道互易性不成立的情况下（例如，在频分双工系统中）。从编码的角度来看，用户通过下行链路训练获得的DL信道状态可以看作是一个信息源，必须通过上行链路通信信道传达给基站。通过信道传输信源可以通过分离信源-信道编码（SSCC）或联合信源-信道编码（JSCC）来完成。在这项工作中，我们首先使用经典的远程失真率（DR）理论，为基于JSCC和SSCC的反馈方案的信道估计均方误差（MSE）提供了理论下限，然而，这需要对大块连续信道状态进行编码，因此在实践中无法使用，因为它会导致极大的反馈延迟。然后，我们关注最小（一个时隙）反馈延迟的相关情况，并提出了一种实用的基于JSCC的反馈方案，该方案充分利用信道二阶统计量来优化特征空间的维度投影。我们分析了所提出的基于JSCC的方案在大信噪比（SNR）下的质量标度指数（QSE）行为。鉴于任何反馈方案的信道估计的二阶统计量，我们进一步推导了在最大比传输和零强迫预编码下，DL数据传输的遍历和速率下限的闭合形式。通过广泛的数值结果，我们表明我们提出的基于JSCC的方案优于已知的JSCC、SSCC基线和基于深度学习的方案，并且能够在实际SNR范围内接近最优DR方案的性能。", "summary": "本研究针对大规模MIMO系统中下行链路CSIT获取的挑战，提出了一种实用的联合信源-信道编码（JSCC）方案，以实现最小反馈延迟下的高效信道状态反馈。该方案通过利用信道二阶统计量优化维度投影，并在大SNR下进行性能分析，同时推导了下行链路数据传输的遍历和速率下限。数值结果表明，所提出的JSCC方案在性能上超越了现有基线和深度学习方法，并能接近理论最优的失真率性能。", "keywords": "CSIT, 压缩反馈, 联合信源-信道编码, 大规模MIMO, 失真率理论", "comments": "该论文的创新点在于针对实际应用中反馈延迟的限制，提出了一种实用的JSCC方案，并巧妙地利用信道二阶统计量进行优化。其重要性在于为大规模MIMO系统中的CSIT反馈提供了一种高性能且低延迟的解决方案，并超越了包括深度学习在内的多种现有方法，具有重要的理论和实践意义。"}}
{"id": "2506.10662", "title": "Receiving RISs: Enabling Channel Estimation and Autonomous Configuration", "authors": ["George C. Alexandropoulos", "Konstantinos D. Katsanos", "Evangelos Vlachos"], "summary": "This chapter focuses on a hardware architecture for semi-passive\nReconfigurable Intelligent Surfaces (RISs) and investigates its consideration\nfor boosting the performance of Multiple-Input Multiple-Output (MIMO)\ncommunication systems. The architecture incorporates a single or multiple\nradio-frequency chains to receive pilot signals via tunable absorption phase\nprofiles realized by the metasurface front end, as well as a controller\nencompassing a baseband processing unit to carry out channel estimation, and\nconsequently, the optimization of the RIS reflection coefficients. A novel\nchannel estimation protocol, according to which the RIS receives non-orthogonal\ntraining pilot sequences from two multi-antenna terminals via tunable\nabsorption phase profiles, and then, estimates the respective channels via its\nsignal processing unit, is presented. The channel estimates are particularly\nused by the RIS controller to design the capacity-achieving reflection phase\nconfiguration of the metasurface front end. The proposed channel estimation\nalgorithm, which is based on the Alternating Direction Method of Multipliers\n(ADMM), profits from the RIS random spatial absorption sampling to capture the\nentire signal space, and exploits the beamspace sparsity and low-rank\nproperties of extremely large MIMO channels, which is particularly relevant for\ncommunication systems at the FR3 band and above. Our extensive numerical\ninvestigations showcase the superiority of the proposed channel estimation\ntechnique over benchmark schemes for various system and RIS hardware\nconfiguration parameters, as well as the effectiveness of using channel\nestimates at the RIS side to dynamically optimize the possibly phase-quantized\nreflection coefficients of its unit elements.", "comment": "34 pages; 12 figures; book chapter", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10662v1", "AI": {"title_translation": "接收式RIS：实现信道估计与自主配置", "tldr": "该论文提出了一种半无源可重构智能表面（RIS）架构，包含射频链和基带处理单元，用于实现信道估计和自主配置。其核心是一种基于交替方向乘子法（ADMM）的新颖信道估计协议，通过数值研究证明了该方法在各种系统参数下优于现有方案，并能有效优化RIS反射系数。", "motivation": "本研究旨在通过提出一种半无源可重构智能表面（RIS）的硬件架构及其信道估计和自主配置能力，以提升多输入多输出（MIMO）通信系统的性能。", "method": "本文提出了一种半无源RIS硬件架构，其包含单个或多个射频链，通过可调谐吸收相位剖面接收导频信号，以及一个包含基带处理单元的控制器，用于执行信道估计和优化RIS反射系数。开发了一种新颖的信道估计协议，其中RIS通过可调谐吸收相位剖面从两个多天线终端接收非正交训练导频序列，并利用其信号处理单元估计信道。所提出的信道估计算法基于交替方向乘子法（ADMM），利用RIS随机空间吸收采样来捕获整个信号空间，并利用极大规模MIMO信道的波束空间稀疏性和低秩特性。", "result": "数值研究结果表明，所提出的信道估计技术在各种系统和RIS硬件配置参数下均优于基准方案。此外，在RIS侧利用信道估计动态优化其单元元件的反射系数是有效的。", "conclusion": "本文提出了一种半无源RIS架构及其基于ADMM的新颖信道估计协议，该协议能够实现RIS的信道估计和自主配置，并被证明能有效提升MIMO通信系统性能。", "translation": "本章重点介绍了一种半无源可重构智能表面（RIS）的硬件架构，并研究了其在提升多输入多输出（MIMO）通信系统性能方面的考量。该架构包含一个或多个射频链，通过超表面前端实现的，可调谐吸收相位剖面接收导频信号，以及一个包含基带处理单元的控制器，用于执行信道估计，并因此优化RIS的反射系数。提出了一种新颖的信道估计协议，根据该协议，RIS通过可调谐吸收相位剖面从两个多天线终端接收非正交训练导频序列，然后通过其信号处理单元估计相应的信道。信道估计特别用于RIS控制器，以设计超表面前端的容量实现反射相位配置。所提出的信道估计算法基于交替方向乘子法（ADMM），利用RIS随机空间吸收采样来捕获整个信号空间，并利用极大规模MIMO信道的波束空间稀疏性和低秩特性，这对于FR3频段及以上的通信系统尤其相关。我们广泛的数值研究表明，在各种系统和RIS硬件配置参数下，所提出的信道估计技术优于基准方案，并且在RIS侧使用信道估计来动态优化其单元元件可能进行相位量化的反射系数是有效的。", "summary": "本文提出了一种用于增强多输入多输出（MIMO）通信系统的半无源可重构智能表面（RIS）硬件架构。该架构集成了射频链和基带处理单元，用于接收导频信号、执行信道估计并优化RIS反射系数。论文引入了一种新颖的信道估计协议，其中RIS接收非正交训练序列并通过基于交替方向乘子法（ADMM）的算法进行信道估计。该算法利用RIS随机空间吸收采样以及大规模MIMO信道的波束空间稀疏性和低秩特性，尤其适用于FR3频段及以上。数值结果表明，所提出的信道估计技术优于现有方案，并且能有效实现RIS反射系数的动态优化。", "keywords": "可重构智能表面, 信道估计, 自主配置, MIMO, ADMM", "comments": "本文通过提出一种具有射频链和基带处理单元的半无源RIS架构，解决了RISs的关键挑战——自身信道估计问题，从而实现自主配置。这种方法极大地推动了RISs的实际部署，因为它允许RIS独立地进行信道感知和优化。利用大规模MIMO信道的稀疏性和低秩特性来设计高效的ADMM算法，体现了其创新性。这项工作为RISs走向实际应用提供了重要的解决方案。"}}
{"id": "2506.10350", "title": "Heterogeneous-IRS-Assisted MIMO Systems: Channel Estimation and Beamforming", "authors": ["Weibiao Zhao", "Qiucen Wu", "Yuanqi Tang", "Yu Zhu"], "summary": "Intelligent reflecting surface (IRS) has gained great attention for its\nability to create favorable propagation environments. However, the power\nconsumption of conventional IRSs cannot be ignored due to the large number of\nreflecting elements and control circuits. To balance performance and power\nconsumption, we previously proposed a heterogeneous-IRS (HE-IRS), a green IRS\nstructure integrating dynamically tunable elements (DTEs) and statically\ntunable elements (STEs). Compared to conventional IRSs with only DTEs, the\nunique DTE-STE integrated structure introduces new challenges in both channel\nestimation and beamforming. In this paper, we investigate the channel\nestimation and beamforming problems in HE-IRS-assisted multi-user\nmultiple-input multiple-output systems. Unlike the overall cascaded channel\nestimated in conventional IRSs, we show that the HE-IRS channel to be estimated\nis decomposed into a DTE-based cascaded channel and an STE-based equivalent\nchannel. Leveraging it along with the inherent sparsity of DTE- and STE-based\nchannels and manifold optimization, we propose an efficient channel estimation\nscheme. To address the rank mismatch problem in the imperfect channel sparsity\ninformation, a robust rank selection rule is developed. For beamforming, we\npropose an offline algorithm to optimize the STE phase shifts for wide beam\ncoverage, and an online algorithm to optimize the BS precoder and the DTE phase\nshifts using the estimated HE-IRS channel. Simulation results show that the\nHE-IRS requires less pilot overhead than conventional IRSs with the same number\nof elements. With the proposed channel estimation and beamforming schemes, the\ngreen HE-IRS achieves competitive sum rate performance with significantly\nreduced power consumption.", "comment": "30 pages, 8 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10350v1", "AI": {"title_translation": "异构IRS辅助MIMO系统：信道估计与波束成形", "tldr": "本文研究了异构智能反射面（HE-IRS）辅助多用户MIMO系统中的信道估计和波束成形问题，提出了一种高效的信道估计方案和两种波束成形算法，结果表明HE-IRS在降低功耗的同时能保持竞争性的和速率性能。", "motivation": "传统的智能反射面（IRS）由于其大量的反射单元和控制电路，功耗不可忽略。为了平衡性能和功耗，作者提出了一种异构IRS（HE-IRS）结构，该结构集成了动态可调谐单元（DTEs）和静态可调谐单元（STEs），但这种独特结构给信道估计和波束成形带来了新的挑战。", "method": "本文研究了HE-IRS辅助多用户MIMO系统中的信道估计和波束成形问题。作者将HE-IRS信道分解为基于DTE的级联信道和基于STE的等效信道。利用信道固有的稀疏性和流形优化，提出了一种高效的信道估计方案，并开发了一种鲁棒的秩选择规则来解决信道稀疏信息不完美时的秩不匹配问题。对于波束成形，提出了一种离线算法来优化STE的相移以实现宽波束覆盖，以及一种在线算法来利用估计的HE-IRS信道优化基站预编码器和DTE的相移。", "result": "仿真结果表明，HE-IRS在相同数量的单元下比传统IRS需要更少的导频开销。通过所提出的信道估计和波束成形方案，绿色HE-IRS在显著降低功耗的同时，实现了具有竞争力的和速率性能。", "conclusion": "本文提出的异构IRS结构及其信道估计和波束成形方案，能够有效平衡系统性能和功耗，在绿色通信方面具有显著优势。", "translation": "智能反射面（IRS）因其创造有利传播环境的能力而备受关注。然而，传统IRS由于大量的反射单元和控制电路，其功耗不容忽视。为了平衡性能和功耗，我们之前提出了一种异构IRS（HE-IRS），这是一种集成了动态可调谐单元（DTEs）和静态可调谐单元（STEs）的绿色IRS结构。与仅有DTEs的传统IRS相比，独特的DTE-STE集成结构在信道估计和波束成形方面带来了新的挑战。在本文中，我们研究了HE-IRS辅助多用户多输入多输出系统中的信道估计和波束成形问题。与传统IRS中估计的整体级联信道不同，我们发现HE-IRS待估计的信道被分解为基于DTE的级联信道和基于STE的等效信道。利用这一点以及基于DTE和STE信道的固有稀疏性以及流形优化，我们提出了一种高效的信道估计方案。为了解决不完美信道稀疏信息中的秩不匹配问题，开发了一种鲁棒的秩选择规则。对于波束成形，我们提出了一种离线算法来优化STE相移以实现宽波束覆盖，以及一种在线算法来利用估计的HE-IRS信道优化基站预编码器和DTE相移。仿真结果表明，HE-IRS在相同数量的单元下比传统IRS需要更少的导频开销。通过所提出的信道估计和波束成形方案，绿色HE-IRS在显著降低功耗的同时，实现了具有竞争力的和速率性能。", "summary": "本文针对传统智能反射面（IRS）功耗高的问题，提出了一种异构IRS（HE-IRS）结构，该结构集成了动态可调谐单元（DTEs）和静态可调谐单元（STEs），旨在平衡性能和功耗。研究了HE-IRS辅助多用户MIMO系统中的信道估计和波束成形问题，将HE-IRS信道分解为DTE和STE两部分进行估计，并利用稀疏性和流形优化提出高效的信道估计方案。同时，提出了离线和在线两种波束成形算法。仿真结果表明，HE-IRS在减少导频开销和降低功耗的同时，能够保持良好的和速率性能。", "keywords": "异构智能反射面, 信道估计, 波束成形, MIMO系统, 功耗", "comments": "这篇论文的创新点在于提出了异构IRS（HE-IRS）结构，有效地解决了传统IRS功耗过高的问题，并针对其独特的结构提出了专门的信道估计和波束成形方法。通过将信道分解为DTE和STE部分进行处理，并结合稀疏性利用和流形优化，提升了信道估计的效率。其提出的离线和在线波束成形算法也考虑了不同单元的特性。这项工作对于未来绿色通信和IRS技术的实用化具有重要意义，尤其是在资源受限的环境下。"}}
{"id": "2506.10291", "title": "Learning-Based Stable Optimal Control for Infinite-Time Nonlinear Regulation Problems", "authors": ["Han Wang", "Di Wu", "Lin Cheng", "Shengping Gong", "Xu Huang"], "summary": "Infinite-time nonlinear optimal regulation control is widely utilized in\naerospace engineering as a systematic method for synthesizing stable\ncontrollers. However, conventional methods often rely on linearization\nhypothesis, while recent learning-based approaches rarely consider stability\nguarantees. This paper proposes a learning-based framework to learn a stable\noptimal controller for nonlinear optimal regulation problems. First, leveraging\nthe equivalence between Pontryagin Maximum Principle (PMP) and\nHamilton-Jacobi-Bellman (HJB) equation, we improve the backward generation of\noptimal examples (BGOE) method for infinite-time optimal regulation problems. A\nstate-transition-matrix-guided data generation method is then proposed to\nefficiently generate a complete dataset that covers the desired state space.\nFinally, we incorporate the Lyapunov stability condition into the learning\nframework, ensuring the stability of the learned optimal policy by jointly\nlearning the optimal value function and control policy. Simulations on three\nnonlinear optimal regulation problems show that the learned optimal policy\nachieves near-optimal regulation control and the code is provided at\nhttps://github.com/wong-han/PaperNORC", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10291v1", "AI": {"title_translation": "基于学习的无限时间非线性调节问题稳定最优控制", "tldr": "本文提出了一种基于学习的框架，用于解决无限时间非线性最优调节问题，通过改进数据生成方法并结合Lyapunov稳定性条件，学习得到稳定的近似最优控制器。", "motivation": "传统的非线性最优调节控制方法常依赖线性化假设，而现有的基于学习的方法通常缺乏稳定性保证。", "method": "本文利用Pontryagin最大值原理（PMP）和Hamilton-Jacobi-Bellman（HJB）方程的等价性，改进了无限时间最优调节问题的最优示例反向生成（BGOE）方法。提出了一种状态转移矩阵引导的数据生成方法，以高效生成覆盖所需状态空间的完整数据集。最后，将Lyapunov稳定性条件纳入学习框架，通过联合学习最优值函数和控制策略来确保学习到的最优策略的稳定性。", "result": "在三个非线性最优调节问题上的仿真表明，学习到的最优策略实现了接近最优的调节控制。", "conclusion": "本文提出的基于学习的框架能够为无限时间非线性调节问题学习到稳定的近似最优控制器，有效解决了现有方法的局限性。", "translation": "无限时间非线性最优调节控制在航空航天工程中作为一种系统化方法被广泛用于合成稳定控制器。然而，传统方法常常依赖于线性化假设，而最近的基于学习的方法很少考虑稳定性保证。本文提出了一种基于学习的框架，用于学习非线性最优调节问题的稳定最优控制器。首先，利用庞特里亚金最大值原理（PMP）和哈密顿-雅可比-贝尔曼（HJB）方程之间的等价性，我们改进了无限时间最优调节问题的最优示例反向生成（BGOE）方法。然后提出了一种状态转移矩阵引导的数据生成方法，以高效生成覆盖所需状态空间的完整数据集。最后，我们将Lyapunov稳定性条件纳入学习框架，通过联合学习最优值函数和控制策略来确保学习到的最优策略的稳定性。在三个非线性最优调节问题上的仿真表明，学习到的最优策略实现了接近最优的调节控制，代码已在https://github.com/wong-han/PaperNORC提供。", "summary": "本文针对无限时间非线性最优调节控制中传统方法依赖线性化和现有学习方法缺乏稳定性的问题，提出了一个基于学习的稳定最优控制框架。该框架通过改进最优示例反向生成（BGOE）方法和引入状态转移矩阵引导的数据生成，高效构建训练数据。同时，将Lyapunov稳定性条件集成到学习过程中，通过联合学习值函数和控制策略，确保了所学策略的稳定性。仿真结果验证了其接近最优的调节性能。", "keywords": "学习控制, 最优控制, 稳定性, 非线性系统, 调节问题", "comments": "该论文的创新点在于将学习方法与严格的稳定性理论（Lyapunov条件）相结合，解决了现有学习控制方法在稳定性保证方面的不足。通过改进数据生成策略，提高了学习效率和数据覆盖率。这对于需要高可靠性（如航空航天）的非线性控制系统具有重要意义。"}}
{"id": "2506.10104", "title": "Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection", "authors": ["David Farr", "Kevin Talty", "Alexandra Farr", "John Stockdale", "Iain Cruickshank", "Jevin West"], "summary": "As cyber threats become more sophisticated, rapid and accurate vulnerability\ndetection is essential for maintaining secure systems. This study explores the\nuse of Large Language Models (LLMs) in software vulnerability assessment by\nsimulating the identification of Python code with known Common Weakness\nEnumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot\nin-domain prompting strategies. Our results indicate that while zero-shot\nprompting performs poorly, few-shot prompting significantly enhances\nclassification performance, particularly when integrated with confidence-based\nrouting strategies that improve efficiency by directing human experts to cases\nwhere model uncertainty is high, optimizing the balance between automation and\nexpert oversight. We find that LLMs can effectively generalize across\nvulnerability categories with minimal examples, suggesting their potential as\nscalable, adaptable cybersecurity tools in simulated environments. However,\nchallenges such as model reliability, interpretability, and adversarial\nrobustness remain critical areas for future research. By integrating AI-driven\napproaches with expert-in-the-loop (EITL) decision-making, this work highlights\na pathway toward more efficient and responsive cybersecurity workflows. Our\nfindings provide a foundation for deploying AI-assisted vulnerability detection\nsystems in both real and simulated environments that enhance operational\nresilience while reducing the burden on human analysts.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10104v1", "AI": {"title_translation": "结合跨领域和领域内小样本学习的专家在环软件漏洞检测系统", "tldr": "本研究探讨了使用大型语言模型（LLMs）进行软件漏洞检测，发现小样本提示结合置信度路由能显著提高性能，并指出LLMs在模拟环境中作为可扩展网络安全工具的潜力，同时强调了未来研究的挑战。", "motivation": "随着网络威胁日益复杂，快速准确的漏洞检测对于维护系统安全至关重要。本研究旨在探索大型语言模型（LLMs）在软件漏洞评估中的应用，以应对这一挑战。", "method": "本研究通过模拟识别带有已知通用弱点枚举（CWEs）的Python代码，比较了零样本、跨领域小样本和领域内小样本提示策略，并整合了基于置信度的路由策略，以优化自动化和专家监督之间的平衡。", "result": "零样本提示表现不佳；小样本提示显著增强了分类性能，尤其是在与基于置信度的路由策略结合时，通过将模型不确定性高的案例引导给人类专家，提高了效率。LLMs能够以最少的示例有效泛化到不同的漏洞类别。", "conclusion": "本研究表明大型语言模型在模拟环境中作为可扩展、适应性强的网络安全工具具有潜力。通过将AI驱动的方法与专家在环（EITL）决策相结合，本工作为更高效、响应更快的网络安全工作流提供了一条途径，为在真实和模拟环境中部署AI辅助漏洞检测系统奠定了基础。", "translation": "随着网络威胁变得越来越复杂，快速准确的漏洞检测对于维护安全系统至关重要。本研究探讨了在软件漏洞评估中使用大型语言模型（LLMs），通过模拟识别具有已知通用弱点枚举（CWEs）的Python代码，比较了零样本、跨领域小样本和领域内小样本提示策略。我们的结果表明，虽然零样本提示表现不佳，但小样本提示显著增强了分类性能，特别是当与基于置信度的路由策略集成时，通过将人类专家引向模型不确定性高的案例来提高效率，从而优化了自动化和专家监督之间的平衡。我们发现LLMs能够以最少的示例有效泛化到不同的漏洞类别，这表明它们在模拟环境中作为可扩展、适应性强的网络安全工具的潜力。然而，模型可靠性、可解释性和对抗鲁棒性等挑战仍然是未来研究的关键领域。通过将AI驱动的方法与专家在环（EITL）决策相结合，这项工作突出了实现更高效和响应更快的网络安全工作流程的途径。我们的发现为在真实和模拟环境中部署AI辅助漏洞检测系统提供了基础，这些系统既能增强操作弹性，又能减轻人类分析师的负担。", "summary": "本研究探讨了大型语言模型（LLMs）在软件漏洞检测中的应用，通过比较零样本、跨领域小样本和领域内小样本提示策略。结果显示，小样本提示显著提升了性能，尤其是在结合置信度路由策略时，能够有效引导专家介入。这表明LLMs在模拟环境中作为可扩展的网络安全工具具有潜力，并为AI辅助漏洞检测系统在实际和模拟环境中的部署奠定了基础，提高了效率并减轻了分析师负担。", "keywords": "软件漏洞检测, 大型语言模型, 小样本学习, 专家在环, 网络安全", "comments": "本研究的创新点在于将大型语言模型与专家在环（EITL）系统相结合，并通过置信度路由策略优化了人机协作，提高了软件漏洞检测的效率和准确性。其重要性在于为AI在网络安全领域的实际应用提供了可行路径，特别是在资源有限或需要快速响应的场景下。然而，论文也指出了模型可靠性、可解释性和对抗鲁棒性等关键挑战，这些是未来研究需要重点关注的限制。"}}
{"id": "2506.10330", "title": "Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "summary": "This study examined code issue detection and revision automation by\nintegrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and\nGPT-4o into software development workflows. A static code analysis framework\ndetects issues such as bugs, vulnerabilities, and code smells within a\nlarge-scale software project. Detailed information on each issue was extracted\nand organized to facilitate automated code revision using LLMs. An iterative\nprompt engineering process is applied to ensure that prompts are structured to\nproduce accurate and organized outputs aligned with the project requirements.\nRetrieval-augmented generation (RAG) is implemented to enhance the relevance\nand precision of the revisions, enabling LLM to access and integrate real-time\nexternal knowledge. The issue of LLM hallucinations - where the model generates\nplausible but incorrect outputs - is addressed by a custom-built \"Code\nComparison App,\" which identifies and corrects erroneous changes before\napplying them to the codebase. Subsequent scans using the static code analysis\nframework revealed a significant reduction in code issues, demonstrating the\neffectiveness of combining LLMs, static analysis, and RAG to improve code\nquality, streamline the software development process, and reduce time and\nresource expenditure.", "comment": "Accepted at FORGE 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10330v1", "AI": {"title_translation": "使用静态代码分析增强大型语言模型以实现自动化代码质量改进", "tldr": "本研究探讨了通过将大型语言模型（LLMs）与静态代码分析框架集成，实现代码问题检测和自动化修订，并利用RAG和自定义工具解决幻觉问题，显著提高了代码质量。", "motivation": "本研究旨在通过将大型语言模型（LLMs）整合到软件开发工作流程中，实现代码问题检测和修订的自动化。", "method": "本研究将大型语言模型（如GPT-3.5 Turbo和GPT-4o）与静态代码分析框架相结合。该框架用于检测代码中的缺陷、漏洞和代码异味。提取并组织问题详细信息，通过迭代提示工程和检索增强生成（RAG）实现LLM的自动化代码修订。为解决LLM幻觉问题，开发了定制的“代码比较应用程序”来识别和纠正错误更改。", "result": "后续使用静态代码分析框架的扫描显示，代码问题显著减少。", "conclusion": "结合大型语言模型、静态分析和检索增强生成（RAG）可以有效提高代码质量，简化软件开发流程，并减少时间和资源消耗。", "translation": "本研究通过将OpenAI的GPT-3.5 Turbo和GPT-4o等大型语言模型（LLMs）集成到软件开发工作流程中，研究了代码问题检测和修订自动化。一个静态代码分析框架在一个大型软件项目中检测诸如错误、漏洞和代码异味等问题。提取并组织每个问题的详细信息，以促进使用LLMs进行自动化代码修订。应用迭代提示工程过程，确保提示结构化，以生成与项目要求对齐的准确和有组织输出。实施检索增强生成（RAG）以增强修订的相关性和精确性，使LLM能够访问和集成实时外部知识。LLM幻觉问题——即模型生成看似合理但错误的输出——通过定制的“代码比较应用程序”得到解决，该应用程序在将更改应用于代码库之前识别并纠正错误更改。随后使用静态代码分析框架进行的扫描显示，代码问题显著减少，证明了结合LLMs、静态分析和RAG在提高代码质量、简化软件开发过程以及减少时间和资源消耗方面的有效性。", "summary": "本研究提出了一种将大型语言模型（LLMs）与静态代码分析相结合的方法，以实现代码质量的自动化改进。通过静态分析检测代码问题，LLMs利用迭代提示工程和检索增强生成（RAG）进行自动化修订。为解决LLM幻觉问题，引入了定制的“代码比较应用程序”。实验结果表明，该集成方法显著减少了代码问题，提高了开发效率。", "keywords": "大型语言模型, 静态代码分析, 代码质量, 自动化修订, 检索增强生成", "comments": "该论文创新性地将大型语言模型与传统的静态代码分析相结合，并通过RAG和自定义的“代码比较应用程序”有效解决了LLM的幻觉问题，提高了自动化代码修订的可靠性。其重要性在于为软件开发流程自动化和代码质量提升提供了一条可行且高效的路径。"}}
{"id": "2506.10598", "title": "Accessible Design in Integrated Development Environments: A Think Aloud Study Exploring the Experiences of Students with ADHD", "authors": ["Luke Halpin", "Phillip Benachour", "Tracy Hall", "Ann-Marie Houghton", "Emily Winter"], "summary": "Coding forms a key part of computer science education in universities. As\npart of this education, Integrated Development Environments (IDEs) are\nessential tools for coding. However, it is currently unknown how the design of\nan IDE's interface impacts on students with Attention Deficit Hyperactivity\nDisorder (ADHD).\n  In this study we investigated the use of IDEs by students with ADHD. We\nconducted a think aloud study with nine university computing students, followed\nby qualitative observational interviews to analyse their learning and\nengagement with the Visual Studio Code IDE. The paper reports on these\nexperiences and seeks to understand the role IDEs play in the educational\nsetting.\n  Our work also examines how digital accessibility and usability are considered\nin the current design of IDEs. We analysed the qualitative data using a\nthematic analysis and identified three primary themes: self-confidence,\ninteraction, and learning as well as various sub-themes.\n  The themes and their sub-themes illustrate key areas of consideration when\ndesigning IDEs for students with ADHD. The primary findings highlight\nexperiences of frustration and barriers in the current design and layout of\nIDEs.\n  Through our participatory approach we provide a rare insight into ADHD user\nexperiences around usability and accessibility, and describe the need for\nbetter design of development environments to ensure a positive learning\nexperience for the students.", "comment": "16 pages, 3 figures, ECTEL 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10598v1", "AI": {"title_translation": "集成开发环境中的可访问设计：一项探索ADHD学生体验的“出声思考”研究", "tldr": "一项对ADHD学生使用集成开发环境（IDE）的“出声思考”研究表明，现有IDE设计存在挫折和障碍，强调需要改进可访问性和可用性以支持他们的学习体验。", "motivation": "研究旨在了解集成开发环境（IDE）的界面设计如何影响注意力缺陷多动障碍（ADHD）学生，并探讨当前IDE设计中数字可访问性和可用性的考虑情况，因为目前对此尚不清楚。", "method": "本研究采用“出声思考”研究方法，对九名大学计算机专业的学生进行了调查。随后进行了定性观察性访谈，以分析他们使用Visual Studio Code IDE的学习和参与情况。定性数据通过主题分析法进行分析。", "result": "研究识别出三个主要主题：自信、互动和学习，以及各种子主题。主要发现强调了ADHD学生在使用当前IDE设计和布局时遇到的挫折和障碍。", "conclusion": "研究指出，现有IDE设计和布局对ADHD学生造成了挫折和障碍。结论是需要更好地设计开发环境，以确保ADHD学生获得积极的学习体验，并提供了ADHD用户在可用性和可访问性方面的独特见解。", "translation": "编码是大学计算机科学教育的关键部分。作为这项教育的一部分，集成开发环境 (IDEs) 是必不可少的编码工具。然而，目前尚不清楚 IDE 界面设计如何影响注意力缺陷多动障碍 (ADHD) 学生。\n在这项研究中，我们调查了 ADHD 学生使用 IDE 的情况。我们对九名大学计算机专业的学生进行了一项“出声思考”研究，随后进行了定性观察性访谈，以分析他们使用 Visual Studio Code IDE 的学习和参与情况。本文报告了这些经验，并试图理解 IDE 在教育环境中扮演的角色。\n我们的工作还检查了当前 IDE 设计中如何考虑数字可访问性和可用性。我们使用主题分析法分析了定性数据，并确定了三个主要主题：自信、互动和学习，以及各种子主题。\n这些主题及其子主题说明了为 ADHD 学生设计 IDE 时需要考虑的关键领域。主要发现强调了当前 IDE 设计和布局中存在的挫折和障碍。\n通过我们的参与式方法，我们提供了关于 ADHD 用户在可用性和可访问性方面体验的罕见见解，并描述了需要更好地设计开发环境以确保学生获得积极学习体验的需求。", "summary": "本研究通过对九名ADHD大学计算机学生的“出声思考”研究和定性访谈，探讨了集成开发环境（IDE）设计对其学习体验的影响。研究发现，当前IDE设计中存在导致ADHD学生感到挫折和障碍的问题，并识别出自信、互动和学习等关键主题。研究强调了为ADHD学生设计IDE时需考虑的关键领域，旨在提供关于ADHD用户可用性和可访问性体验的独特见解，并呼吁改进开发环境设计以促进积极的学习体验。", "keywords": "ADHD, 集成开发环境, 可访问性, 可用性, 出声思考研究", "comments": "这项研究通过关注ADHD学生在IDE使用中的特定体验，填补了数字可访问性设计领域的一个重要空白。其采用的“出声思考”和定性访谈方法，为理解用户痛点提供了深入的洞察，尤其强调了用户参与式设计在提升教育工具包容性方面的重要性。研究结果对未来IDE的设计改进具有重要的指导意义，有助于创建更具包容性的编程学习环境。"}}
{"id": "2506.10279", "title": "Learning Safe Control via On-the-Fly Bandit Exploration", "authors": ["Alexandre Capone", "Ryan Cosner", "Aaaron Ames", "Sandra Hirche"], "summary": "Control tasks with safety requirements under high levels of model uncertainty\nare increasingly common. Machine learning techniques are frequently used to\naddress such tasks, typically by leveraging model error bounds to specify\nrobust constraint-based safety filters. However, if the learned model\nuncertainty is very high, the corresponding filters are potentially invalid,\nmeaning no control input satisfies the constraints imposed by the safety\nfilter. While most works address this issue by assuming some form of safe\nbackup controller, ours tackles it by collecting additional data on the fly\nusing a Gaussian process bandit-type algorithm. We combine a control barrier\nfunction with a learned model to specify a robust certificate that ensures\nsafety if feasible. Whenever infeasibility occurs, we leverage the control\nbarrier function to guide exploration, ensuring the collected data contributes\ntoward the closed-loop system safety. By combining a safety filter with\nexploration in this manner, our method provably achieves safety in a setting\nthat allows for a zero-mean prior dynamics model, without requiring a backup\ncontroller. To the best of our knowledge, it is the first safe learning-based\ncontrol method that achieves this.", "comment": "arXiv admin note: text overlap with arXiv:2311.02133", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10279v1", "AI": {"title_translation": "通过即时Bandit探索学习安全控制", "tldr": "本文提出了一种新的安全学习控制方法，通过结合控制障碍函数和即时高斯过程Bandit探索，在不依赖备用控制器的情况下，在高度模型不确定性下实现可证明的安全。", "motivation": "在高度模型不确定性下的安全控制任务日益普遍，但现有方法通常依赖于模型误差界限来指定安全滤波器，当不确定性过高时，这些滤波器可能失效，导致没有可行的控制输入。大多数工作通过假设安全备用控制器来解决此问题，但本文旨在不依赖备用控制器的情况下解决此问题。", "method": "本文将控制障碍函数与学习模型结合，以指定一个鲁棒的安全证书。当出现不可行性时，利用控制障碍函数指导探索，通过高斯过程Bandit型算法即时收集额外数据，确保收集到的数据有助于闭环系统安全。", "result": "该方法在允许零均值先验动力学模型的设置中，无需备用控制器即可证明实现安全。据作者所知，这是第一个实现此目标的基于安全学习的控制方法。", "conclusion": "本文提出了一种新颖的基于安全学习的控制方法，通过将安全滤波器与即时探索相结合，实现了可证明的安全性，从而无需备用控制器，即使在高度模型不确定性下也能有效工作。", "translation": "具有高度模型不确定性的安全要求控制任务日益普遍。机器学习技术常用于解决此类任务，通常通过利用模型误差界限来指定基于鲁棒约束的安全滤波器。然而，如果学习到的模型不确定性非常高，相应的滤波器可能会失效，这意味着没有控制输入能满足安全滤波器施加的约束。虽然大多数工作通过假设某种形式的安全备用控制器来解决此问题，但本文通过使用高斯过程Bandit型算法即时收集额外数据来解决。我们将控制障碍函数与学习模型结合，以指定一个鲁棒证书，如果可行则确保安全。当出现不可行性时，我们利用控制障碍函数来指导探索，确保收集到的数据有助于闭环系统安全。通过这种方式将安全滤波器与探索相结合，我们的方法在允许零均值先验动力学模型的设置中，无需备用控制器即可证明实现安全。据我们所知，这是第一个实现此目标的基于安全学习的控制方法。", "summary": "本文解决了在高度模型不确定性下安全控制的挑战，传统安全滤波器可能失效且通常需要备用控制器。作者提出了一种新颖的方法，将控制障碍函数与学习模型和即时高斯过程Bandit探索策略相结合。这种方法允许系统在出现不可行性时，在控制障碍函数的指导下收集新数据，从而确保闭环系统安全。关键是，他们的方法在不依赖备用控制器的情况下，即使在零均值先验动力学模型下，也能可证明地实现安全，这是安全学习控制领域的一个重大进展。", "keywords": "安全控制, 模型不确定性, Bandit探索, 控制障碍函数, 基于学习的控制", "comments": "本文的关键创新在于避免了对备用控制器的需求，这在安全学习中是一个常见的假设。通过使用由控制障碍函数指导的即时Bandit探索，该方法主动收集数据以解决不可行性并保持安全。这对于实现更自主、更鲁棒的安全学习系统具有重要意义，尤其是在初始模型不确定性较高的情况下。"}}
{"id": "2506.10128", "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs", "authors": ["Xiyao Wang", "Zhengyuan Yang", "Chao Feng", "Yongyuan Liang", "Yuhang Zhou", "Xiaoyu Liu", "Ziyi Zang", "Ming Li", "Chung-Ching Lin", "Kevin Lin", "Linjie Li", "Furong Huang", "Lijuan Wang"], "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10128v1", "AI": {"title_translation": "ViCrit：一种用于视觉语言模型中视觉感知的可验证强化学习代理任务", "tldr": "ViCrit引入了一种可验证的强化学习代理任务，通过识别图像描述中注入的细微视觉幻觉来提高视觉语言模型（VLM）的视觉感知能力，并在多个基准测试中展现出显著的性能提升和泛化能力。", "motivation": "强化学习（RL）在微调大型语言模型（LLM）方面取得了巨大成功，但在视觉语言模型（VLM）中扩展这种成功以提升视觉感知却面临挑战。这主要是因为缺乏既具有挑战性又可以明确验证的以视觉为中心的任务。", "method": "本文提出了ViCrit（Visual Caption Hallucination Critic），一个强化学习代理任务，用于训练VLM定位注入到人类编写的图像描述段落中的细微合成视觉幻觉。具体来说，从200字的描述开始，注入一个单一的、细微的视觉描述错误（改变对象、属性、数量或空间关系上的几个词），然后模型需要在给定图像和修改后的描述的情况下，精确找出被破坏的部分。这种方法保留了完整的感知难度，同时提供了一个易于计算且明确的二元精确匹配奖励。此外，还引入了ViCrit-Bench，一个类别平衡的诊断基准，用于系统地探测跨不同图像领域和错误类型的感知错误。", "result": "使用ViCrit任务训练的模型在各种视觉语言基准测试中展现出显著的性能提升。重要的是，这些改进不仅限于自然图像训练数据，还能泛化到抽象图像推理和视觉数学，表明模型学会了感知而非仅仅记忆所见对象。", "conclusion": "精细粒度的幻觉批评是增强视觉语言模型（VLM）视觉感知的一种有效且可泛化的目标。", "translation": "强化学习（RL）在利用具有挑战性但易于验证的任务（如数学推理或代码生成）微调大型语言模型（LLM）方面展现出巨大成效。然而，将这种成功扩展到视觉语言模型（VLM）中的视觉感知一直受到阻碍，原因是缺乏同时具有挑战性且可明确验证的以视觉为中心的任务。为此，我们引入了ViCrit（Visual Caption Hallucination Critic），一个RL代理任务，它训练VLM定位注入到人类编写的图像描述段落中的细微合成视觉幻觉。从一份200字的描述开始，我们注入一个单一的、细微的视觉描述错误——改变对象、属性、数量或空间关系上的几个词——并要求模型在给定图像和修改后的描述的情况下，精确指出被损坏的部分。这种表述保留了完整的感知难度，同时提供了一个易于计算且明确的二元精确匹配奖励。使用ViCrit任务训练的模型在各种视觉语言基准测试中展现出显著的性能提升。至关重要的是，这些改进超越了自然图像训练数据，泛化到抽象图像推理和视觉数学，显示出学习感知而非仅仅记忆所见对象的潜力。为了便于评估，我们进一步引入了ViCrit-Bench，一个类别平衡的诊断基准，系统地探测跨不同图像领域和错误类型的感知错误。总而言之，我们的结果表明，细粒度的幻觉批评是增强VLM视觉感知的一种有效且可泛化的目标。", "summary": "ViCrit是一种新的强化学习代理任务，旨在解决视觉语言模型（VLM）中视觉感知微调的挑战。该任务通过向人类编写的图像描述中注入细微的视觉幻觉错误，并要求模型识别这些错误，从而提供了一个既具有感知难度又易于验证的二元奖励。实验结果表明，使用ViCrit训练的模型在多个视觉语言基准测试中取得了显著提升，并且这种改进能够泛化到抽象图像推理和视觉数学，表明模型学会了感知而非仅仅记忆。为了便于评估，论文还引入了ViCrit-Bench基准。", "keywords": "强化学习, 视觉感知, 视觉语言模型, 幻觉, 代理任务", "comments": "该论文的创新点在于为视觉语言模型（VLM）的视觉感知创建了一个可验证的强化学习代理任务，解决了VLM微调中的一个关键挑战。其设计的二元、精确匹配奖励系统非常巧妙，确保了验证的明确性。此外，模型改进能够泛化到抽象图像推理和视觉数学，这一点尤为重要，表明了其学习能力的深度。"}}
{"id": "2506.10094", "title": "Unsupervised Deep Clustering of MNIST with Triplet-Enhanced Convolutional Autoencoders", "authors": ["Md. Faizul Islam Ansari"], "summary": "This research implements an advanced unsupervised clustering system for MNIST\nhandwritten digits through two-phase deep autoencoder architecture. A deep\nneural autoencoder requires a training process during phase one to develop\nminimal yet interpretive representations of images by minimizing reconstruction\nerrors. During the second phase we unify the reconstruction error with a KMeans\nclustering loss for learned latent embeddings through a joint distance-based\nobjective. Our model contains three elements which include batch normalization\ncombined with dropout and weight decay for achieving generalized and stable\nresults. The framework achieves superior clustering performance during\nextensive tests which used intrinsic measurements including Silhouette Score\nand Davies-Bouldin Index coupled with extrinsic metrics NMI and ARI when\nprocessing image features. The research uses t-SNE visualization to present\nlearned embeddings that show distinct clusters for digits. Our approach reaches\nan optimal combination between data reconstruction accuracy and cluster\nseparation purity when adding the benefit of understandable results and\nscalable implementations. The approach creates a dependable base that helps\ndeploy unsupervised representation learning in different large-scale image\nclustering applications.", "comment": "6 pages, 6 figures, experimental study on deep clustering with\n  autoencoders", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10094v1", "AI": {"title_translation": "基于三重增强卷积自编码器的MNIST无监督深度聚类", "tldr": "该研究提出了一种两阶段深度自编码器架构，结合KMeans聚类损失，实现了MNIST手写数字的无监督深度聚类，并在测试中取得了优异的聚类性能和可理解的结果。", "motivation": "研究旨在通过深度自编码器架构，为MNIST手写数字实现先进的无监督聚类系统，以开发最小但可解释的图像表示。", "method": "该研究采用两阶段深度自编码器架构。第一阶段训练深度神经网络自编码器，通过最小化重构误差来学习图像的表示。第二阶段将重构误差与KMeans聚类损失结合，通过基于距离的联合目标函数，对学习到的潜在嵌入进行聚类。模型包含批归一化、Dropout和权重衰减，以实现泛化和稳定的结果。", "result": "该框架在广泛测试中取得了卓越的聚类性能，使用了包括Silhouette Score和Davies-Bouldin Index在内的内在度量以及NMI和ARI等外在度量。研究使用t-SNE可视化展示了学习到的嵌入，显示出清晰的数字聚类。该方法在数据重构精度和聚类分离纯度之间达到了最佳组合，并具有可理解的结果和可扩展的实现。", "conclusion": "该方法为在不同大规模图像聚类应用中部署无监督表示学习奠定了可靠的基础。", "translation": "本研究通过两阶段深度自编码器架构，实现了一个先进的MNIST手写数字无监督聚类系统。深度神经网络自编码器在第一阶段需要进行训练，通过最小化重构误差来开发最小但可解释的图像表示。在第二阶段，我们将重构误差与KMeans聚类损失结合，通过基于距离的联合目标，对学习到的潜在嵌入进行聚类。我们的模型包含三个元素，包括批归一化结合Dropout和权重衰减，以实现泛化和稳定的结果。该框架在处理图像特征时，通过使用包括Silhouette Score和Davies-Bouldin Index在内的内在度量，以及NMI和ARI等外在度量，在广泛测试中取得了卓越的聚类性能。本研究使用t-SNE可视化来呈现学习到的嵌入，显示出清晰的数字聚类。我们的方法在数据重构精度和聚类分离纯度之间达到了最佳组合，同时增加了可理解的结果和可扩展的实现。该方法创建了一个可靠的基础，有助于在不同大规模图像聚类应用中部署无监督表示学习。", "summary": "本研究提出了一种基于三重增强卷积自编码器的两阶段无监督深度聚类系统，用于MNIST手写数字。该系统首先通过深度自编码器学习图像的紧凑表示，然后将重构误差与KMeans聚类损失相结合，通过联合目标函数实现聚类。模型结合了批归一化、Dropout和权重衰减以提高性能。实验结果表明，该方法在图像聚类性能上表现优异，并在数据重构精度和聚类纯度之间取得了平衡，具有良好的可解释性和可扩展性，为大规模图像聚类应用中的无监督表示学习奠定了基础。", "keywords": "无监督聚类, 深度自编码器, MNIST, KMeans, 图像聚类", "comments": "该论文的创新点在于将两阶段深度自编码器架构与KMeans聚类损失、三重增强（虽然标题提到三重，但摘要中未详细说明其具体实现，可能隐含在联合目标中）相结合，实现了有效的无监督聚类。其重要性在于提供了一种可扩展且性能优越的图像聚类方法，特别是通过结合重构误差和聚类损失，在无监督学习中实现了数据表示学习和聚类任务的统一优化。该方法为大规模图像聚类应用提供了可靠的基础，具有实际应用潜力。"}}
{"id": "2506.10531", "title": "GPU-Accelerated Distributed QAOA on Large-scale HPC Ecosystems", "authors": ["Zhihao Xu", "Srikar Chundury", "Seongmin Kim", "Amir Shehata", "Xinyi Li", "Ang Li", "Tengfei Luo", "Frank Mueller", "In-Saeng Suh"], "summary": "Quantum computing holds great potential to accelerate the process of solving\ncomplex combinatorial optimization problems. The Distributed Quantum\nApproximate Optimization Algorithm (DQAOA) addresses high-dimensional, dense\nproblems using current quantum computing techniques and high-performance\ncomputing (HPC) systems. In this work, we improve the scalability and\nefficiency of DQAOA through advanced problem decomposition and parallel\nexecution using message passing on the Frontier CPU/GPU supercomputer. Our\napproach ensures efficient quantum-classical workload management by\ndistributing large problem instances across classical and quantum resources.\nExperimental results demonstrate that enhanced decomposition strategies and\nGPU-accelerated quantum simulations significantly improve DQAOA's performance,\nachieving up to 10x speedup over CPU-based simulations. This advancement\nenables better scalability for large problem instances, supporting the\npractical deployment of GPU systems for hybrid quantum-classical applications.\nWe also highlight ongoing integration efforts using the Quantum Framework (QFw)\nto support future HPC-quantum computing systems.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10531v1", "AI": {"title_translation": "大规模HPC生态系统上的GPU加速分布式QAOA", "tldr": "本文通过先进的问题分解和GPU加速的量子模拟，显著提高了分布式量子近似优化算法（DQAOA）在大规模HPC系统上的可扩展性和效率，实现了高达10倍的加速。", "motivation": "量子计算在解决复杂组合优化问题方面潜力巨大。分布式量子近似优化算法（DQAOA）旨在利用当前的量子计算技术和高性能计算（HPC）系统解决高维、密集问题。这项工作的动机是提高DQAOA的可扩展性和效率。", "method": "通过先进的问题分解和使用消息传递在Frontier CPU/GPU超级计算机上进行并行执行，提高了DQAOA的可扩展性和效率。该方法通过在经典和量子资源之间分配大型问题实例，确保了高效的量子-经典工作负载管理。", "result": "实验结果表明，增强的分解策略和GPU加速的量子模拟显著提高了DQAOA的性能，比基于CPU的模拟实现了高达10倍的加速。这使得大型问题实例具有更好的可扩展性。", "conclusion": "GPU系统对于混合量子-经典应用具有实际部署价值。目前正在进行与量子框架（QFw）的集成工作，以支持未来的HPC-量子计算系统。", "translation": "量子计算在加速解决复杂组合优化问题方面具有巨大潜力。分布式量子近似优化算法（DQAOA）利用当前的量子计算技术和高性能计算（HPC）系统来解决高维、密集问题。在这项工作中，我们通过先进的问题分解和在Frontier CPU/GPU超级计算机上使用消息传递进行并行执行，提高了DQAOA的可扩展性和效率。我们的方法通过在经典和量子资源之间分配大型问题实例，确保了高效的量子-经典工作负载管理。实验结果表明，增强的分解策略和GPU加速的量子模拟显著提高了DQAOA的性能，比基于CPU的模拟实现了高达10倍的加速。这一进展使得大型问题实例具有更好的可扩展性，支持了GPU系统在混合量子-经典应用中的实际部署。我们还强调了使用量子框架（QFw）正在进行的集成工作，以支持未来的HPC-量子计算系统。", "summary": "本文研究了在大规模高性能计算（HPC）环境中加速分布式量子近似优化算法（DQAOA）的方法。通过引入先进的问题分解技术和利用GPU加速的量子模拟，该研究显著提升了DQAOA的可扩展性和效率。实验结果表明，与传统的CPU模拟相比，其性能提升高达10倍，这使得DQAOA能够处理更大规模的问题，并凸显了GPU在混合量子-经典应用中的实际应用潜力。此外，论文还提到了与量子框架（QFw）的集成，以支持未来HPC-量子计算系统。", "keywords": "分布式QAOA, GPU加速, HPC, 量子计算, 组合优化", "comments": "本文创新性地将GPU加速与分布式QAOA相结合，在大规模HPC系统上实现了显著的性能提升。其对问题分解和高效量子-经典工作负载管理的关注，为解决当前量子计算机的局限性提供了实用的方法。10倍的加速是一个重要的成果，表明了GPU在混合量子-经典计算中的巨大潜力。这项工作为未来大规模量子应用的实际部署奠定了基础。"}}
{"id": "2506.10699", "title": "SNR and Resource Adaptive Deep JSCC for Distributed IoT Image Classification", "authors": ["Ali Waqas", "Sinem Coleri"], "summary": "Sensor-based local inference at IoT devices faces severe computational\nlimitations, often requiring data transmission over noisy wireless channels for\nserver-side processing. To address this, split-network Deep Neural Network\n(DNN) based Joint Source-Channel Coding (JSCC) schemes are used to extract and\ntransmit relevant features instead of raw data. However, most existing methods\nrely on fixed network splits and static configurations, lacking adaptability to\nvarying computational budgets and channel conditions. In this paper, we propose\na novel SNR- and computation-adaptive distributed CNN framework for wireless\nimage classification across IoT devices and edge servers. We introduce a\nlearning-assisted intelligent Genetic Algorithm (LAIGA) that efficiently\nexplores the CNN hyperparameter space to optimize network configuration under\ngiven FLOPs constraints and given SNR. LAIGA intelligently discards the\ninfeasible network configurations that exceed computational budget at IoT\ndevice. It also benefits from the Random Forests based learning assistance to\navoid a thorough exploration of hyperparameter space and to induce application\nspecific bias in candidate optimal configurations. Experimental results\ndemonstrate that the proposed framework outperforms fixed-split architectures\nand existing SNR-adaptive methods, especially under low SNR and limited\ncomputational resources. We achieve a 10\\% increase in classification accuracy\nas compared to existing JSCC based SNR-adaptive multilayer framework at an SNR\nas low as -10dB across a range of available computational budget (1M to 70M\nFLOPs) at IoT device.", "comment": "6 pages, 5 figures, PIMRC Conference 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10699v1", "AI": {"title_translation": "用于分布式物联网图像分类的信噪比和资源自适应深度JSCC", "tldr": "本文提出了一种新的信噪比和计算自适应分布式CNN框架，用于在物联网设备和边缘服务器之间进行无线图像分类，通过学习辅助遗传算法（LAIGA）优化网络配置，显著提高了在低信噪比和有限计算资源下的分类精度。", "motivation": "物联网设备上的传感器本地推理面临严重的计算限制，通常需要通过嘈杂的无线信道传输数据到服务器进行处理。现有的大多数基于拆分网络深度神经网络（DNN）的联合源信道编码（JSCC）方案依赖于固定的网络拆分和静态配置，缺乏对不同计算预算和信道条件的适应性。", "method": "本文提出了一种新颖的信噪比（SNR）和计算自适应分布式CNN框架，用于物联网设备和边缘服务器上的无线图像分类。引入了一种学习辅助智能遗传算法（LAIGA），该算法能够有效地探索CNN超参数空间，在给定的FLOPs约束和SNR下优化网络配置。LAIGA智能地丢弃超出物联网设备计算预算的不可行网络配置，并受益于基于随机森林的学习辅助，以避免彻底探索超参数空间并引入特定于应用的偏差到候选最优配置中。", "result": "实验结果表明，所提出的框架优于固定拆分架构和现有信噪比自适应方法，尤其是在低信噪比和有限计算资源下。与现有的基于JSCC的信噪比自适应多层框架相比，在低至-10dB的信噪比下，以及在物联网设备上1M到70M FLOPs的可用计算预算范围内，分类精度提高了10%。", "conclusion": "所提出的信噪比和计算自适应深度JSCC框架，通过引入学习辅助智能遗传算法（LAIGA），能够有效地在不同计算预算和信道条件下优化网络配置，从而在分布式物联网图像分类任务中显著提升性能，尤其是在资源受限和信噪比低的严苛环境中。", "translation": "基于传感器的物联网设备本地推理面临严重的计算限制，通常需要通过嘈杂的无线信道传输数据以进行服务器端处理。为了解决这个问题，基于拆分网络深度神经网络（DNN）的联合源信道编码（JSCC）方案被用于提取和传输相关特征，而不是原始数据。然而，大多数现有方法依赖于固定的网络拆分和静态配置，缺乏对不同计算预算和信道条件的适应性。在本文中，我们提出了一种新颖的信噪比（SNR）和计算自适应分布式CNN框架，用于物联网设备和边缘服务器之间的无线图像分类。我们引入了一种学习辅助智能遗传算法（LAIGA），该算法能够有效地探索CNN超参数空间，在给定的FLOPs约束和SNR下优化网络配置。LAIGA智能地丢弃超出物联网设备计算预算的不可行网络配置。它还受益于基于随机森林的学习辅助，以避免彻底探索超参数空间并引入特定于应用的偏差到候选最优配置中。实验结果表明，所提出的框架优于固定拆分架构和现有信噪比自适应方法，尤其是在低信噪比和有限计算资源下。与现有的基于JSCC的信噪比自适应多层框架相比，在低至-10dB的信噪比下，以及在物联网设备上1M到70M FLOPs的可用计算预算范围内，分类精度提高了10%。", "summary": "本文针对物联网设备在计算和通信受限环境下的分布式图像分类问题，提出了一种新型信噪比（SNR）和计算自适应的深度联合源信道编码（JSCC）CNN框架。该框架引入了学习辅助智能遗传算法（LAIGA），通过优化网络配置来适应不同的计算预算和信道条件。LAIGA利用随机森林辅助剪枝不可行配置并引导搜索。实验结果表明，该框架在低SNR和有限计算资源下，相比传统固定拆分架构和现有SNR自适应方法，分类精度显著提高，特别是在-10dB SNR下实现了10%的准确率提升。", "keywords": "深度JSCC, 物联网图像分类, 信噪比自适应, 资源自适应, 遗传算法", "comments": "该论文的创新点在于提出了一个结合了信噪比和计算资源自适应的深度JSCC框架，并通过学习辅助智能遗传算法（LAIGA）实现了网络配置的动态优化。LAIGA结合了遗传算法的探索能力和随机森林的学习辅助，能够高效地在受限环境下找到最优解，这对于资源受限的物联网设备非常重要。其在低信噪比和有限计算资源下的性能提升，显示了该方法在实际应用中的巨大潜力。"}}
{"id": "2506.10010", "title": "Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "summary": "Human emotional expression emerges through coordinated vocal, facial, and\ngestural signals. While speech face alignment is well established, the broader\ndynamics linking emotionally expressive speech to regional facial and hand\nmotion remains critical for gaining a deeper insight into how emotional and\nbehavior cues are communicated in real interactions. Further modulating the\ncoordination is the structure of conversational exchange like sequential turn\ntaking, which creates stable temporal windows for multimodal synchrony, and\nsimultaneous speech, often indicative of high arousal moments, disrupts this\nalignment and impacts emotional clarity. Understanding these dynamics enhances\nrealtime emotion detection by improving the accuracy of timing and synchrony\nacross modalities in both human interactions and AI systems. This study\nexamines multimodal emotion coupling using region specific motion capture from\ndyadic interactions in the IEMOCAP corpus. Speech features included low level\nprosody, MFCCs, and model derived arousal, valence, and categorical emotions\n(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker\ndisplacements. Expressive activeness was quantified through framewise\ndisplacement magnitudes, and speech to gesture prediction mapped speech\nfeatures to facial and hand movements. Nonoverlapping speech consistently\nelicited greater activeness particularly in the lower face and mouth. Sadness\nshowed increased expressivity during nonoverlap, while anger suppressed\ngestures during overlaps. Predictive mapping revealed highest accuracy for\nprosody and MFCCs in articulatory regions while arousal and valence had lower\nand more context sensitive correlations. Notably, hand speech synchrony was\nenhanced under low arousal and overlapping speech, but not for valence.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10010v1", "AI": {"title_translation": "双人互动中语音到面部和身体姿态的多模态情感耦合", "tldr": "本研究探讨了双人互动中语音、面部表情和手势之间的多模态情感耦合，以及对话结构如何影响这种耦合，对实时情感检测具有重要意义。", "motivation": "将情感表达语音与局部面部和手部动作联系起来的更广泛动态，对于深入了解情感和行为线索如何在真实互动中交流至关重要。对话交流的结构（如顺序轮流、同时说话）进一步调节了这种协调。理解这些动态通过提高人类互动和人工智能系统中跨模态的时间和同步准确性来增强实时情感检测。", "method": "本研究使用IEMOCAP语料库中双人互动的区域特定动作捕捉数据，检查多模态情感耦合。语音特征包括低级韵律、MFCCs以及模型推导的唤醒度、效价和分类情感（高兴、悲伤、愤怒、中性），并与3D面部和手部标记位移对齐。通过逐帧位移幅度量化表达活跃度，并使用语音到手势预测将语音特征映射到面部和手部动作。", "result": "非重叠语音始终能引发更大的活跃度，特别是在下半脸和嘴部。悲伤在非重叠期间表现出更高的表达性，而愤怒在重叠期间则抑制了手势。预测映射显示，韵律和MFCCs在发音区域的准确性最高，而唤醒度和效价的相关性较低且更具语境敏感性。手部语音同步在低唤醒度和重叠语音下得到增强，但对效价则不然。", "conclusion": "理解这些多模态动态可以提高人类互动和人工智能系统中实时情感检测的准确性。", "translation": "人类情感表达通过协调的语音、面部和手势信号呈现。尽管语音与面部的对齐已得到充分证实，但将情感表达语音与局部面部和手部动作联系起来的更广泛动态，对于深入了解情感和行为线索如何在真实互动中交流仍然至关重要。对话交流的结构，如顺序轮流，进一步调节了这种协调，它为多模态同步创造了稳定的时间窗口，而同时说话（通常预示着高度兴奋的时刻）则会扰乱这种对齐并影响情感清晰度。理解这些动态通过提高人类互动和人工智能系统中跨模态的时间和同步准确性来增强实时情感检测。本研究使用IEMOCAP语料库中双人互动区域特定的动作捕捉来检查多模态情感耦合。语音特征包括低级韵律、MFCCs以及模型推导的唤醒度、效价和分类情感（高兴、悲伤、愤怒、中性），并与3D面部和手部标记位移对齐。通过逐帧位移幅度量化表达活跃度，语音到手势预测将语音特征映射到面部和手部动作。非重叠语音始终能引发更大的活跃度，特别是在下半脸和嘴部。悲伤在非重叠期间表现出更高的表达性，而愤怒在重叠期间则抑制了手势。预测映射显示，韵律和MFCCs在发音区域的准确性最高，而唤醒度和效价的相关性较低且更具语境敏感性。值得注意的是，手部语音同步在低唤醒度和重叠语音下得到增强，但对效价则不然。", "summary": "本研究利用IEMOCAP语料库，探讨了双人互动中语音、面部和手势的多模态情感耦合。研究分析了对话结构（重叠与非重叠语音）如何影响表达活跃度和同步性。结果表明，非重叠语音增加了下半脸和嘴部的活跃度；悲伤在非重叠时表达性增强，而愤怒在重叠时抑制手势。语音特征如韵律和MFCCs对发音区域的预测准确性最高。此外，在低唤醒度和重叠语音下，手部语音同步得到增强。理解这些动态对于提高实时情感检测至关重要。", "keywords": "多模态情感, 语音手势, 双人互动, 情感耦合, 对话结构", "comments": "这篇论文对情感表达中多模态线索的复杂相互作用提供了宝贵的见解，尤其强调了对话轮流对姿态活动的影响。对特定情感状态（悲伤、愤怒）和特征类型（韵律、MFCCs对比唤醒度、效价）的详细分析，提供了细致入微的理解。其研究结果对于推进人机交互和能够更准确、更具语境意识的情感识别AI系统具有重要意义。"}}
{"id": "2506.10150", "title": "When Large Language Models are Reliable for Judging Empathic Communication", "authors": ["Aakriti Kumar", "Nalin Poungpeth", "Diyi Yang", "Erina Farrell", "Bruce Lambert", "Matthew Groh"], "summary": "Large language models (LLMs) excel at generating empathic responses in\ntext-based conversations. But, how reliably do they judge the nuances of\nempathic communication? We investigate this question by comparing how experts,\ncrowdworkers, and LLMs annotate empathic communication across four evaluative\nframeworks drawn from psychology, natural language processing, and\ncommunications applied to 200 real-world conversations where one speaker shares\na personal problem and the other offers support. Drawing on 3,150 expert\nannotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess\ninter-rater reliability between these three annotator groups. We find that\nexpert agreement is high but varies across the frameworks' sub-components\ndepending on their clarity, complexity, and subjectivity. We show that expert\nagreement offers a more informative benchmark for contextualizing LLM\nperformance than standard classification metrics. Across all four frameworks,\nLLMs consistently approach this expert level benchmark and exceed the\nreliability of crowdworkers. These results demonstrate how LLMs, when validated\non specific tasks with appropriate benchmarks, can support transparency and\noversight in emotionally sensitive applications including their use as\nconversational companions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10150v1", "AI": {"title_translation": "大型语言模型在判断同理心交流方面的可靠性", "tldr": "研究发现大型语言模型在判断同理心交流方面与专家水平接近，且优于众包工人，表明其在情感敏感应用中具有潜力。", "motivation": "大型语言模型在生成同理心回复方面表现出色，但其在判断同理心交流细微差别方面的可靠性尚不清楚，本研究旨在调查此问题。", "method": "本研究通过比较专家、众包工作者和大型语言模型在四种评估框架下对200个真实世界对话中同理心交流的标注情况，并评估三者之间的评估者间一致性。研究使用了3,150份专家标注、2,844份众包标注和3,150份大型语言模型标注。", "result": "专家一致性高但因框架子组件的清晰度、复杂性和主观性而异。专家一致性比标准分类指标更能有效衡量大型语言模型表现。大型语言模型在所有四种框架中都接近专家水平，并超过众包工作者的可靠性。", "conclusion": "研究结果表明，大型语言模型在通过适当基准在特定任务上进行验证后，可以支持情感敏感应用（包括作为对话伙伴）的透明度和监督。", "translation": "大型语言模型（LLMs）在基于文本的对话中擅长生成富有同理心的回复。然而，它们在判断同理心交流的细微差别方面有多可靠？我们通过比较专家、众包工作者和LLMs如何标注同理心交流来调查这个问题，这些标注基于从心理学、自然语言处理和通信领域提取的四个评估框架，应用于200个真实世界的对话，其中一位说话者分享个人问题，另一位提供支持。我们利用3,150份专家标注、2,844份众包标注和3,150份LLM标注，评估了这三组标注者之间的评估者间一致性。我们发现专家一致性很高，但根据框架子组件的清晰度、复杂性和主观性而异。我们表明，专家一致性为衡量LLM性能提供了比标准分类指标更具信息量的基准。在所有四个框架中，LLMs始终接近这一专家水平基准，并超过众包工作者的可靠性。这些结果表明，LLMs在通过适当基准在特定任务上进行验证后，如何支持情感敏感应用（包括作为对话伙伴）的透明度和监督。", "summary": "本文探讨了大型语言模型在判断同理心交流方面的可靠性。研究通过比较专家、众包工作者和大型语言模型在四种评估框架下对200个真实对话的标注，评估了它们之间的评估者间一致性。结果显示，大型语言模型在可靠性方面接近专家水平，并优于众包工作者，且专家一致性是比传统分类指标更有效的基准。这表明大型语言模型在经过验证后可用于情感敏感应用。", "keywords": "大型语言模型, 同理心交流, 可靠性, 专家一致性, 评估框架", "comments": "本文的创新之处在于提出了使用专家一致性作为评估大型语言模型在情感敏感任务（如同理心判断）中表现的更有效基准，而非传统的分类指标。这对于确保大型语言模型在实际应用中的透明度和可靠性具有重要意义。"}}
{"id": "2506.10509", "title": "A semi-Lagrangian scheme for First-Order Mean Field Games based on monotone operators", "authors": ["Elisabetta Carlini", "Valentina Coscetti"], "summary": "We construct a semi-Lagrangian scheme for first-order, time-dependent, and\nnon-local Mean Field Games. The convergence of the scheme to a weak solution of\nthe system is analyzed by exploiting a key monotonicity property. To solve the\nresulting discrete problem, we implement a Learning Value Algorithm, prove its\nconvergence, and propose an acceleration strategy based on a Policy iteration\nmethod. Finally, we present numerical experiments that validate the\neffectiveness of the proposed schemes and show that the accelerated version\nsignificantly improves performance.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10509v1", "AI": {"title_translation": "基于单调算子的头阶平均场博弈的半拉格朗日格式", "tldr": "本文提出了一种用于一阶非局部平均场博弈的半拉格朗日格式，并通过单调性证明了其收敛性，并开发了一种基于学习价值算法和策略迭代的加速求解方法，数值实验验证了其有效性和性能提升。", "motivation": "该研究旨在为一阶、时间相关和非局部平均场博弈构建一个有效的半拉格朗日格式，并分析其收敛性。", "method": "本文构建了一种用于一阶、时间相关和非局部平均场博弈的半拉格朗日格式。为解决由此产生的离散问题，研究者实现了一种学习价值算法并证明其收敛性，同时提出了一种基于策略迭代方法的加速策略。", "result": "该半拉格朗日格式对系统弱解的收敛性通过利用关键的单调性性质得到了分析和证明。学习价值算法的收敛性也得到了证明。数值实验验证了所提出方案的有效性，并表明加速版本显著提高了性能。", "conclusion": "所提出的半拉格朗日格式及其基于策略迭代的加速方法对于一阶平均场博弈是有效且高性能的。", "translation": "我们构建了一种用于一阶、时间相关和非局部平均场博弈的半拉格朗日格式。通过利用一个关键的单调性性质，分析了该格式对系统弱解的收敛性。为了解决由此产生的离散问题，我们实现了一种学习价值算法，证明了其收敛性，并提出了一种基于策略迭代方法的加速策略。最后，我们提供了数值实验，验证了所提出格式的有效性，并表明加速版本显著提高了性能。", "summary": "本文提出了一种用于一阶、时间相关和非局部平均场博弈的半拉格朗日格式。该研究通过利用关键的单调性性质，分析了该格式对系统弱解的收敛性。为解决离散问题，作者实现并证明了一种学习价值算法的收敛性，并引入了基于策略迭代的加速策略。数值实验验证了所提方案的有效性及其加速版本的显著性能提升。", "keywords": "平均场博弈, 半拉格朗日格式, 单调算子, 学习价值算法, 策略迭代", "comments": "该论文的创新点在于结合了半拉格朗日格式与学习价值算法，并通过策略迭代方法实现了加速，同时提供了格式和算法的收敛性证明，为一阶平均场博弈的数值求解提供了有效且高效的新方法。"}}
{"id": "2506.10718", "title": "Anomaly Detection for Sensing Security", "authors": ["Stefan Roth", "Aydin Sezgin"], "summary": "Various approaches in the field of physical layer security involve anomaly\ndetection, such as physical layer authentication, sensing attacks, and\nanti-tampering solutions. Depending on the context in which these approaches\nare applied, anomaly detection needs to be computationally lightweight,\nresilient to changes in temperature and environment, and robust against phase\nnoise. We adapt moving average filters, autoregression filters and Kalman\nfilters to provide predictions of feature vectors that fulfill the above\ncriteria. Different hypothesis test designs are employed that allow\nomnidirectional and unidirectional outlier detection. In a case study, a\nsensing attack is investigated that employs the described algorithms with\nvarious channel features based on commodity WiFi devices. Thereby, various\ncombinations of algorithms and channel features show effectiveness for motion\ndetection by an attacker. Countermeasures only utilizing transmit power\nrandomization are shown insufficient to mitigate such attacks if the attacker\nhas access to channel state information (CSI) measurements, suggesting that\nmitigation solutions might require frequency-variant randomization.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.10718v1", "AI": {"title_translation": "感知安全的异常检测", "tldr": "本文研究了物理层安全中异常检测的需求，并提出了适应移动平均、自回归和卡尔曼滤波器的方法，结合假设检验进行异常检测。通过WiFi设备案例研究发现，这些方法可用于攻击者的运动检测，并指出仅传输功率随机化不足以对抗CSI攻击，可能需要频率变化随机化。", "motivation": "物理层安全中的多种方法（如物理层认证、感知攻击、防篡改解决方案）都涉及异常检测。在实际应用中，异常检测需要满足计算轻量化、对温度和环境变化有弹性、以及对相位噪声具有鲁棒性等标准。", "method": "适应了移动平均滤波器、自回归滤波器和卡尔曼滤波器来预测特征向量，以满足轻量级、环境鲁棒性和抗相位噪声的要求。采用了不同的假设检验设计，允许全向和单向异常值检测。通过一个案例研究，使用商用WiFi设备，结合上述算法和各种信道特征来调查感知攻击。", "result": "各种算法和信道特征的组合在攻击者进行运动检测时显示出有效性。仅利用发射功率随机化的对策不足以减轻攻击者拥有信道状态信息（CSI）测量时的此类攻击。", "conclusion": "异常检测方法（基于适应性滤波器和假设检验）可以有效用于感知安全场景，例如攻击者的运动检测。为了有效缓解利用CSI的感知攻击，可能需要采用频率变化的随机化方案，而非仅仅是传输功率随机化。", "translation": "在物理层安全领域中，各种方法都涉及异常检测，例如物理层认证、感知攻击和防篡改解决方案。根据这些方法应用的环境，异常检测需要具备计算轻量化、对温度和环境变化具有弹性以及对相位噪声具有鲁棒性。我们改进了移动平均滤波器、自回归滤波器和卡尔曼滤波器，以提供满足上述特征向量的预测。采用了不同的假设检验设计，允许全向和单向异常值检测。在一个案例研究中，我们调查了一种感知攻击，该攻击利用所描述的算法和基于商用WiFi设备的各种信道特征。结果表明，算法和信道特征的各种组合在攻击者的运动检测中显示出有效性。研究表明，如果攻击者能够获取信道状态信息（CSI）测量，仅利用发射功率随机化的对策不足以减轻此类攻击，这表明缓解方案可能需要频率变化的随机化。", "summary": "本文探讨了物理层安全中异常检测的应用，并针对其对计算效率、环境适应性和鲁棒性的要求，提出了基于改进的移动平均、自回归和卡尔曼滤波器结合假设检验的方法。通过使用商用WiFi设备进行感知攻击的案例研究，验证了所提算法在运动检测中的有效性。研究还发现，现有仅依赖发射功率随机化的对抗措施在攻击者可获取CSI时不足以防御此类攻击，未来可能需要引入频率变化的随机化方案。", "keywords": "异常检测, 物理层安全, 感知安全, 滤波器, 信道状态信息 (CSI)", "comments": "本文在物理层安全领域，特别是感知安全方面，提出了实用的异常检测方法。其创新点在于将经典的信号处理滤波器（移动平均、自回归、卡尔曼）与假设检验相结合，并针对实际应用中的鲁棒性和轻量化需求进行了适应性改进。通过具体的WiFi设备案例研究，验证了方法的有效性，并对现有防御机制的局限性进行了深入分析，提出了未来可能的研究方向（频率变化随机化），具有较强的实践指导意义。"}}
{"id": "2506.10773", "title": "Learning Chaotic Dynamics with Neuromorphic Network Dynamics", "authors": ["Yinhao Xu", "Georg A. Gottwald", "Zdenka Kuncic"], "summary": "This study investigates how dynamical systems may be learned and modelled\nwith a neuromorphic network which is itself a dynamical system. The\nneuromorphic network used in this study is based on a complex electrical\ncircuit comprised of memristive elements that produce neuro-synaptic nonlinear\nresponses to input electrical signals. To determine how computation may be\nperformed using the physics of the underlying system, the neuromorphic network\nwas simulated and evaluated on autonomous prediction of a multivariate chaotic\ntime series, implemented with a reservoir computing framework. Through\nmanipulating only input electrodes and voltages, optimal nonlinear dynamical\nresponses were found when input voltages maximise the number of memristive\ncomponents whose internal dynamics explore the entire dynamical range of the\nmemristor model. Increasing the network coverage with the input electrodes was\nfound to suppress other nonlinear responses that are less conducive to\nlearning. These results provide valuable insights into how a practical\nneuromorphic network device can be optimised for learning complex dynamical\nsystems using only external control parameters.", "comment": "37 pages, 22 figures", "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.10773v1", "AI": {"title_translation": "利用神经形态网络动力学学习混沌动力学", "tldr": "本研究探讨了如何利用基于忆阻元件的神经形态网络（本身也是一个动力系统）来学习和建模动力系统，并通过模拟发现优化输入电压和电极覆盖可以提高其学习复杂混沌时间序列的能力。", "motivation": "本研究旨在探索如何利用神经形态网络（一个动力系统）来学习和建模其他动力系统，特别是如何利用底层系统的物理特性进行计算。", "method": "研究使用了基于忆阻元件的复杂电路构建的神经形态网络。通过模拟该网络，并在储层计算框架下评估其对多元混沌时间序列的自主预测能力。通过操纵输入电极和电压来寻找最优的非线性动力学响应。", "result": "当输入电压使忆阻元件的内部动力学探索其整个动力学范围时，找到了最佳的非线性动力学响应。增加输入电极的网络覆盖范围可以抑制不利于学习的其他非线性响应。", "conclusion": "这些结果为如何优化实际的神经形态网络设备，使其仅通过外部控制参数就能学习复杂的动力系统提供了宝贵的见解。", "translation": "本研究探讨了如何利用神经形态网络学习和建模动力系统，该网络本身就是一个动力系统。本研究中使用的神经形态网络基于一个复杂的电路，该电路由忆阻元件组成，这些元件对输入电信号产生神经突触非线性响应。为了确定如何利用底层系统的物理特性进行计算，对神经形态网络进行了模拟，并在一个储层计算框架下，评估了其对多元混沌时间序列的自主预测能力。通过仅操纵输入电极和电压，当输入电压使忆阻元件的内部动力学探索忆阻器模型的整个动力学范围时，发现了最佳的非线性动力学响应。研究发现，增加输入电极的网络覆盖范围可以抑制其他不利于学习的非线性响应。这些结果为如何优化实际的神经形态网络设备，使其仅通过外部控制参数就能学习复杂的动力系统提供了宝贵的见解。", "summary": "本研究探索了利用基于忆阻元件的神经形态网络（作为动力系统）学习和建模混沌动力学的方法。通过模拟一个复杂的忆阻电路网络，并在储层计算框架下评估其对多元混沌时间序列的预测能力，研究发现，优化输入电压以最大化忆阻器内部动力学范围的探索，以及增加输入电极的网络覆盖范围，能够有效提升网络学习复杂动力系统的能力。这些发现为通过外部参数优化实际神经形态设备以学习复杂动力学提供了重要指导。", "keywords": "神经形态网络, 混沌动力学, 忆阻器, 储层计算, 动力系统", "comments": "该论文的创新点在于将神经形态网络本身视为一个动力系统，并利用其固有的物理特性进行计算，而不是仅仅模拟生物神经元。其重要性在于为构建更高效、能耗更低的类脑计算硬件提供了新的优化思路，特别是在处理复杂非线性动力学系统方面。然而，抽象中未明确指出该方法在实际应用中的具体性能提升与传统方法的对比，这可能是一个潜在的局限性。"}}
{"id": "2506.10362", "title": "Relaxation-Free Min-k-Partition for PCI Assignment in 5G Networks", "authors": ["Yeqing Qiu", "Chengpiao Huang", "Ye Xue", "Zhipeng Jiang", "Qingjiang Shi", "Dong Zhang", "Zhi-Quan Luo"], "summary": "Physical Cell Identity (PCI) is a critical parameter in 5G networks.\nEfficient and accurate PCI assignment is essential for mitigating mod-3\ninterference, mod-30 interference, collisions, and confusions among cells,\nwhich directly affect network reliability and user experience. In this paper,\nwe propose a novel framework for PCI assignment by decomposing the problem into\nMin-3-Partition, Min-10-Partition, and a graph coloring problem, leveraging the\nChinese Remainder Theorem (CRT). Furthermore, we develop a relaxation-free\napproach to the general Min-$k$-Partition problem by reformulating it as a\nquadratic program with a norm-equality constraint and solving it using a\npenalized mirror descent (PMD) algorithm. The proposed method demonstrates\nsuperior computational efficiency and scalability, significantly reducing\ninterference while eliminating collisions and confusions in large-scale 5G\nnetworks. Numerical evaluations on real-world datasets show that our approach\nreduces computational time by up to 20 times compared to state-of-the-art\nmethods, making it highly practical for real-time PCI optimization in\nlarge-scale networks. These results highlight the potential of our method to\nimprove network performance and reduce deployment costs in modern 5G systems.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10362v1", "AI": {"title_translation": "5G网络中PCI分配的无松弛Min-k-划分", "tldr": "本文提出了一种基于Min-k-划分和图着色问题的无松弛方法，用于5G网络中的PCI分配，显著提高了效率并减少了干扰。", "motivation": "5G网络中高效准确的物理小区识别（PCI）分配对于缓解mod-3和mod-30干扰、冲突和混淆至关重要，这些直接影响网络可靠性和用户体验。", "method": "提出了一种新的PCI分配框架，将问题分解为Min-3-划分、Min-10-划分和图着色问题，并利用中国剩余定理（CRT）。此外，通过将通用Min-k-划分问题重新表述为具有范数等式约束的二次规划，并使用惩罚镜像下降（PMD）算法求解，开发了一种无松弛方法。", "result": "所提出的方法在计算效率和可扩展性方面表现出色，显著减少了干扰，并消除了大规模5G网络中的冲突和混淆。在真实世界数据集上的数值评估显示，计算时间比现有技术减少了20倍。", "conclusion": "该方法具有改善网络性能和降低现代5G系统部署成本的潜力，对于大规模网络中的实时PCI优化非常实用。", "translation": "物理小区识别（PCI）是5G网络中的一个关键参数。高效准确的PCI分配对于缓解小区间的mod-3干扰、mod-30干扰、冲突和混淆至关重要，这些直接影响网络可靠性和用户体验。在本文中，我们提出了一种新的PCI分配框架，通过将问题分解为Min-3-划分、Min-10-划分和图着色问题，并利用中国剩余定理（CRT）。此外，我们通过将通用Min-k-划分问题重新表述为具有范数等式约束的二次规划，并使用惩罚镜像下降（PMD）算法求解，开发了一种无松弛方法。所提出的方法在计算效率和可扩展性方面表现出色，显著减少了干扰，同时消除了大规模5G网络中的冲突和混淆。在真实世界数据集上的数值评估显示，我们的方法与现有技术相比，计算时间减少了多达20倍，使其对于大规模网络中的实时PCI优化非常实用。这些结果凸显了我们方法在改善网络性能和降低现代5G系统部署成本方面的潜力。", "summary": "本文提出了一种创新的无松弛Min-k-划分框架，用于5G网络中的物理小区识别（PCI）分配。该方法将PCI分配问题分解为Min-3-划分、Min-10-划分和图着色问题，并结合中国剩余定理。通过将Min-k-划分问题重新表述为带范数等式约束的二次规划并使用惩罚镜像下降算法求解，实现了高效且可扩展的PCI优化。实验结果表明，该方法显著减少了干扰，消除了冲突和混淆，并将计算时间缩短了20倍，对于大规模5G网络的实时优化具有重要意义。", "keywords": "PCI分配, Min-k-划分, 5G网络, 惩罚镜像下降, 中国剩余定理", "comments": "本文的创新点在于提出了一个无松弛的Min-k-划分方法来解决5G网络中的PCI分配问题，并将其与中国剩余定理和图着色相结合。通过将问题转化为二次规划并使用PMD算法，显著提高了计算效率和可扩展性，使其在实际大规模5G网络部署中具有很高的应用价值。"}}
{"id": "2506.10296", "title": "Synthesizing Min-Max Control Barrier Functions For Switched Affine Systems", "authors": ["Sara Kamali", "Guillaume O. Berger", "Sriram Sankaranarayanan"], "summary": "We study the problem of synthesizing non-smooth control barrier functions\n(CBFs) for continuous-time switched affine systems. Switched affine systems are\ndefined by a set of affine dynamical modes, wherein the control consists of a\nstate-based switching signal that determines the current operating mode. The\ncontrol barrier functions seek to maintain the system state inside a control\ninvariant set that excludes a given set of unsafe states. We consider CBFs that\ntake the form of pointwise minima and maxima over a finite set of affine\nfunctions. Our approach uses ideas from nonsmooth analysis to formulate\nconditions for min- and max- affine control barrier functions. We show how a\nfeedback switching law can be extracted from a given CBF. Next, we show how to\nautomate the process of synthesizing CBFs given a system description through a\ntree-search algorithm inspired by branch-and-cut methods from combinatorial\noptimization. Finally, we demonstrate our approach on a series of interesting\nexamples of switched affine systems.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10296v1", "AI": {"title_translation": "为切换仿射系统合成最小-最大控制障碍函数", "tldr": "本研究探讨了为连续时间切换仿射系统合成非光滑控制障碍函数（CBFs）的问题。作者提出了一种利用非光滑分析的方法来制定最小-最大仿射CBF的条件，并展示了如何自动化CBF的合成过程，通过一个受组合优化中分支-剪切方法启发的树搜索算法。该方法在一系列有趣的切换仿射系统示例上得到了验证。", "motivation": "研究的动机是为了合成控制障碍函数（CBFs），以使系统状态保持在一个控制不变集中，从而排除给定的不安全状态集。", "method": "本研究考虑的CBF形式是有限组仿射函数上的逐点最小值和最大值。该方法利用非光滑分析的思想来制定最小-最大仿射控制障碍函数的条件。研究人员展示了如何从给定的CBF中提取反馈切换律，并展示了如何通过一个受组合优化中分支-剪切方法启发的树搜索算法，根据系统描述自动化CBF的合成过程。", "result": "该方法在一系列有趣的切换仿射系统示例上得到了验证。", "conclusion": "本研究成功地为连续时间切换仿射系统合成了非光滑控制障碍函数，并提供了一种自动化的合成方法，展示了其在实际应用中的潜力。", "translation": "我们研究了为连续时间切换仿射系统合成非光滑控制障碍函数（CBFs）的问题。切换仿射系统由一组仿射动力学模式定义，其中控制包括一个基于状态的切换信号，该信号决定当前的运行模式。控制障碍函数旨在将系统状态保持在一个控制不变集中，该集合排除了给定的不安全状态集。我们考虑的CBF形式是有限组仿射函数上的逐点最小值和最大值。我们的方法利用非光滑分析的思想来制定最小-最大仿射控制障碍函数的条件。我们展示了如何从给定的CBF中提取反馈切换律。接下来，我们展示了如何通过一个受组合优化中分支-剪切方法启发的树搜索算法，根据系统描述自动化CBF的合成过程。最后，我们在一系列有趣的切换仿射系统示例上演示了我们的方法。", "summary": "本论文研究了为连续时间切换仿射系统合成非光滑控制障碍函数（CBFs）的问题。这些CBFs旨在将系统状态维持在安全区域内。作者提出了一种方法，利用非光滑分析来制定基于仿射函数逐点最小-最大值的CBF条件。研究还展示了如何从CBF中提取反馈切换律，并引入了一种受分支-剪切方法启发的树搜索算法，以自动化CBF的合成过程。该方法通过一系列示例进行了验证。", "keywords": "控制障碍函数, 切换仿射系统, 最小-最大, 非光滑分析, 树搜索算法", "comments": "该论文的创新点在于提出了为切换仿射系统合成非光滑最小-最大控制障碍函数的方法，并引入了基于树搜索算法的自动化合成流程。这对于复杂非线性系统的安全控制具有重要意义，尤其是在存在多模式切换的情况下。将非光滑分析与组合优化方法结合，为解决这类问题提供了新的视角。"}}
{"id": "2506.10125", "title": "D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning", "authors": ["Muqi Zou", "Hongyu Cai", "Hongwei Wu", "Zion Leonahenahe Basque", "Arslan Khan", "Berkay Celik", "Dave", "Tian", "Antonio Bianchi", "Ruoyu", "Wang", "Dongyan Xu"], "summary": "Decompilers, which reconstruct human-readable source code from binary\nexecutables, are vital to many security tasks. Yet, despite recent advances,\ntheir output often suffers from syntactic and semantic errors and remains\ndifficult to read. Recently, with the advent of large language models (LLMs),\nresearchers began to explore the potential of LLMs to refine decompiler output.\nNevertheless, our study of these approaches reveals significant limitations,\nsuch as introducing new errors and relying on unreliable accuracy validation.\nIn this paper, we present D-LiFT, an automated decompiler backend that\nharnesses and further trains LLMs to improve the quality of decompiled code via\nreinforcement learning (RL). Unlike prior work that overlooks preserving\naccuracy, D-LiFT adheres to a key principle for enhancing the quality of\ndecompiled code: \\textit{preserving accuracy while improving readability}.\nCentral to D-LiFT, we propose D-SCORE, an integrated quality assessment system\nto score the decompiled code from multiple aspects. In line with our principle,\nD-SCORE assigns low scores to any inaccurate output and only awards higher\nscores for readability to code that passes the accuracy check. Specifically,\nD-SCORE first verifies the syntactic and semantic correctness via the compiler\nand symbolic execution; only if a candidate is deemed accurate, it then\nevaluates readability using established metrics to compare the LLM output with\nthe original decompiled code. The score will then be fed back to the LLM for\nfine-tuning. Our implementation, based on Ghidra and a range of LLMs,\ndemonstrates significant improvements for the accurate decompiled code from the\ncoreutils and util-linux projects. Compared to baseline LLMs without\nD-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled\nfunctions, as measured by D-SCORE.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10125v1", "AI": {"title_translation": "D-LiFT：通过代码质量驱动的微调改进基于LLM的反编译器后端", "tldr": "D-LiFT通过强化学习和D-SCORE质量评估系统，微调大型语言模型以提高反编译代码的准确性和可读性，显著改善了反编译结果。", "motivation": "反编译器对安全任务至关重要，但其输出常存在语法和语义错误，难以阅读。现有基于大型语言模型（LLM）的反编译改进方法存在引入新错误和依赖不可靠准确性验证的局限性。", "method": "本文提出了D-LiFT，一个自动化反编译器后端，通过强化学习（RL）进一步训练LLM以提高反编译代码质量。D-LiFT遵循“在提高可读性的同时保持准确性”的原则。核心是D-SCORE，一个综合质量评估系统，从多方面对反编译代码进行评分。D-SCORE首先通过编译器和符号执行验证语法和语义正确性，只有通过准确性检查的代码才进一步评估可读性。评分反馈给LLM进行微调。", "result": "基于Ghidra和一系列LLM的实现表明，D-LiFT显著改善了来自coreutils和util-linux项目的准确反编译代码。与未经D-SCORE驱动微调的基线LLM相比，D-LiFT产生的改进反编译函数数量增加了55.3%（通过D-SCORE衡量）。", "conclusion": "D-LiFT通过引入代码质量驱动的微调（特别是D-SCORE系统），成功克服了现有LLM在反编译领域应用的局限性，显著提高了反编译代码的准确性和可读性。", "translation": "反编译器能够从二进制可执行文件重建人类可读的源代码，对许多安全任务至关重要。然而，尽管近期取得了进展，其输出仍常遭受语法和语义错误，并且难以阅读。最近，随着大型语言模型（LLM）的出现，研究人员开始探索LLM在改进反编译器输出方面的潜力。然而，我们对这些方法的研究发现其存在显著局限性，例如引入新错误和依赖不可靠的准确性验证。在本文中，我们提出了D-LiFT，一个自动化的反编译器后端，它利用并进一步训练LLM，通过强化学习（RL）提高反编译代码的质量。与以往忽略准确性保持的工作不同，D-LiFT遵循一个提高反编译代码质量的关键原则：在提高可读性的同时保持准确性。D-LiFT的核心是D-SCORE，一个集成的质量评估系统，从多个方面对反编译代码进行评分。根据我们的原则，D-SCORE对任何不准确的输出分配低分，并且只对通过准确性检查的代码的可读性给予更高分数。具体来说，D-SCORE首先通过编译器和符号执行验证语法和语义正确性；只有当候选代码被认为是准确的，它才会使用既定指标评估可读性，将LLM输出与原始反编译代码进行比较。然后，该分数将反馈给LLM进行微调。我们基于Ghidra和一系列LLM的实现表明，D-LiFT显著改善了来自coreutils和util-linux项目的准确反编译代码。与未经D-SCORE驱动微调的基线LLM相比，D-LiFT产生的改进反编译函数数量增加了55.3%（通过D-SCORE衡量）。", "summary": "D-LiFT是一个新的反编译器后端，旨在通过代码质量驱动的微调来改进基于LLM的反编译代码。它引入了D-SCORE评估系统，该系统优先验证反编译代码的准确性（语法和语义），然后才评估其可读性。通过将D-SCORE分数反馈给LLM进行强化学习微调，D-LiFT在保持代码准确性的同时显著提升了其可读性，实验结果显示其在改进函数数量上比基线LLM提高了55.3%。", "keywords": "反编译器, 大型语言模型, 代码质量, 强化学习, D-LiFT, D-SCORE", "comments": "D-LiFT的创新之处在于其“在提高可读性的同时保持准确性”的核心原则，并通过D-SCORE系统将这一原则融入到LLM的强化学习微调中。它解决了现有LLM在反编译领域应用中易引入新错误和评估不可靠的问题，为高质量反编译代码的生成提供了一条有效途径。其重要性在于提升了反编译工具在安全分析等关键任务中的实用性。"}}
{"id": "2506.10365", "title": "AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Chang Liu", "Jianyuan Liang", "Yaxian Qing", "Xiaopu Zhang", "Dehua Peng", "Zhipeng Gui", "Xuefeng Guan"], "summary": "Geospatial code generation is becoming a key frontier in integrating\nartificial intelligence with geo-scientific analysis, yet standardised\nautomated evaluation tools for this task remain absent. This study presents\nAutoGEEval++, an enhanced framework building on AutoGEEval, and the first\nautomated assessment system for large language models (LLMs) generating\ngeospatial code on Google Earth Engine (GEE). It supports diverse data\nmodalities and varying task complexities. Built on the GEE Python API,\nAutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test\ncases across 26 data types and three task categories: unit, combo, and theme\ntests. It includes a submission programme and a judge module to realise an\nend-to-end automated evaluation pipeline from code generation to\nexecution-based validation. The framework adopts multi-dimensional\nmetrics-accuracy, resource usage, run-time efficiency, and error\ntypes-balancing hallucination control and efficiency, and enabling boundary\ntesting and error pattern analysis. Using AutoGEEval++, we evaluate 24\nstate-of-the-art LLMs (as of June 2025), including general-purpose,\nreasoning-enhanced, code-centric, and geoscience-specific models. Results\nreveal clear performance, stability, and error differences across task types,\nmodel designs, and deployment settings, confirming AutoGEEval++'s practical\nvalue and scalability in vertical-domain code generation. This work establishes\nthe first standardised evaluation protocol and foundational benchmark for\nGEE-based LLM code generation, providing a unified basis for performance\ncomparison and a methodological framework for systematic, domain-specific code\nevaluation.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10365v1", "AI": {"title_translation": "AutoGEEval++：一个用于谷歌地球引擎地理空间代码生成中大型语言模型的多级别、多地理空间模态自动化评估框架", "tldr": "AutoGEEval++是首个针对谷歌地球引擎（GEE）上地理空间代码生成的大型语言模型（LLMs）的自动化评估框架，提供了一个基准数据集和多维度评估指标，并首次建立了标准化评估协议。", "motivation": "地理空间代码生成是人工智能与地理科学分析结合的关键前沿，但目前缺乏标准化的自动化评估工具。", "method": "本研究提出了AutoGEEval++，一个基于AutoGEEval增强的框架，也是首个针对谷歌地球引擎（GEE）上生成地理空间代码的大型语言模型（LLMs）的自动化评估系统。它支持多样化的数据模态和不同任务复杂性，基于GEE Python API构建。AutoGEEval++包含一个基准数据集AutoGEEval++-Bench，该数据集拥有6,365个测试用例，涵盖26种数据类型和三种任务类别：单元测试、组合测试和主题测试。它还包括一个提交程序和一个判别模块，以实现从代码生成到基于执行验证的端到端自动化评估流程。该框架采用多维度指标，包括准确性、资源使用、运行时效率和错误类型，平衡了幻觉控制和效率，并支持边界测试和错误模式分析。研究使用AutoGEEval++评估了24个最先进的LLMs（截至2025年6月）。", "result": "评估结果揭示了不同任务类型、模型设计和部署设置之间在性能、稳定性以及错误方面的明显差异。", "conclusion": "这项工作为基于GEE的LLM代码生成建立了首个标准化评估协议和基础基准，为性能比较提供了统一基础，并为系统性、领域特定代码评估提供了方法论框架。", "translation": "地理空间代码生成正成为人工智能与地理科学分析结合的关键前沿，然而目前仍缺乏针对此任务的标准化自动化评估工具。本研究提出了AutoGEEval++，一个在AutoGEEval基础上增强的框架，也是首个用于谷歌地球引擎（GEE）上生成地理空间代码的大型语言模型（LLMs）的自动化评估系统。它支持多样化的数据模态和不同任务复杂性。AutoGEEval++基于GEE Python API构建，包含一个基准数据集——AutoGEEval++-Bench，该数据集拥有6,365个测试用例，涵盖26种数据类型和三种任务类别：单元测试、组合测试和主题测试。它包括一个提交程序和一个判别模块，实现了从代码生成到基于执行验证的端到端自动化评估流程。该框架采用多维度指标——准确性、资源使用、运行时效率和错误类型——平衡了幻觉控制和效率，并支持边界测试和错误模式分析。利用AutoGEEval++，我们评估了24个最先进的LLMs（截至2025年6月），包括通用型、推理增强型、代码中心型和地球科学专用型模型。结果揭示了不同任务类型、模型设计和部署设置之间在性能、稳定性以及错误方面的明显差异，证实了AutoGEEval++在垂直领域代码生成中的实用价值和可扩展性。这项工作为基于GEE的LLM代码生成建立了首个标准化评估协议和基础基准，为性能比较提供了统一基础，并为系统性、领域特定代码评估提供了方法论框架。", "summary": "本研究介绍了AutoGEEval++，这是一个针对谷歌地球引擎（GEE）上大型语言模型（LLMs）生成地理空间代码的自动化评估框架。作为首个此类系统，它解决了标准化评估工具的缺失问题。AutoGEEval++基于GEE Python API构建，包含一个大规模基准数据集AutoGEEval++-Bench（6,365个测试用例），支持多级别、多模态评估，并采用准确性、资源使用、运行时效率和错误类型等多维度指标。通过评估24个LLMs，该框架揭示了模型性能、稳定性和错误模式的差异，并确立了GEE-based LLM代码生成的标准化评估协议和基准。", "keywords": "地理空间代码生成, 大型语言模型, 谷歌地球引擎, 自动化评估, 基准测试", "comments": "AutoGEEval++的创新之处在于它是首个为Google Earth Engine上的地理空间代码生成LLMs提供自动化评估的框架，填补了该领域标准化工具的空白。其重要性体现在提供了一个全面的基准数据集（AutoGEEval++-Bench）和多维度评估指标，能够对LLMs在地理空间领域的代码生成能力进行系统性、端到端的评估，为未来研究提供了统一的比较基础和方法论框架。"}}
{"id": "2506.10762", "title": "Integrating Large Language Models into Text Animation: An Intelligent Editing System with Inline and Chat Interaction", "authors": ["Bao Zhang", "Zihan Li", "Zhenglei Liu", "Huanchen Wang", "Yuxin Ma"], "summary": "Text animation, a foundational element in video creation, enables efficient\nand cost-effective communication, thriving in advertisements, journalism, and\nsocial media. However, traditional animation workflows present significant\nusability barriers for non-professionals, with intricate operational procedures\nseverely hindering creative productivity. To address this, we propose a Large\nLanguage Model (LLM)-aided text animation editing system that enables real-time\nintent tracking and flexible editing. The system introduces an agent-based\ndual-stream pipeline that integrates context-aware inline suggestions and\nconversational guidance as well as employs a semantic-animation mapping to\nfacilitate LLM-driven creative intent translation. Besides, the system supports\nsynchronized text-animation previews and parametric adjustments via unified\ncontrols to improve editing workflow. A user study evaluates the system,\nhighlighting its ability to help non-professional users complete animation\nworkflows while validating the pipeline. The findings encourage further\nexploration of integrating LLMs into a comprehensive video creation workflow.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10762v1", "AI": {"title_translation": "将大型语言模型集成到文本动画中：一种具有内联和聊天交互的智能编辑系统", "tldr": "该研究提出一个LLM辅助的文本动画编辑系统，通过内联建议和对话指导帮助非专业用户高效创作文本动画，解决传统动画工具的可用性问题。", "motivation": "传统文本动画工作流程对非专业用户来说存在显著可用性障碍，操作复杂，严重阻碍了创意生产力。", "method": "本文提出一个大型语言模型（LLM）辅助的文本动画编辑系统，该系统能够实现实时意图跟踪和灵活编辑。系统引入基于代理的双流管道，集成了上下文感知的内联建议和对话指导，并采用语义-动画映射来促进LLM驱动的创意意图转换。此外，系统支持同步的文本动画预览和通过统一控制进行的参数调整，以改善编辑工作流程。", "result": "一项用户研究评估了该系统，突出显示其能够帮助非专业用户完成动画工作流程，并验证了该管道的有效性。", "conclusion": "研究结果鼓励进一步探索将LLM集成到全面的视频创作工作流程中。", "translation": "文本动画是视频创作的基础元素，能够实现高效且经济的沟通，在广告、新闻和社交媒体中蓬勃发展。然而，传统的动画工作流程对非专业人士来说存在显著的可用性障碍，复杂的S操作程序严重阻碍了创意生产力。为了解决这个问题，我们提出了一种大型语言模型（LLM）辅助的文本动画编辑系统，该系统能够实现实时意图跟踪和灵活编辑。该系统引入了一个基于代理的双流管道，集成了上下文感知的内联建议和会话指导，并采用语义-动画映射来促进LLM驱动的创意意图转换。此外，该系统支持同步的文本-动画预览并通过统一控制进行参数调整，以改善编辑工作流程。一项用户研究评估了该系统，强调了其帮助非专业用户完成动画工作流程的能力，并验证了该管道。研究结果鼓励进一步探索将LLM集成到全面的视频创作工作流程中。", "summary": "本文提出一个名为“LLM辅助文本动画编辑系统”的智能系统，旨在解决传统文本动画工作流程对非专业用户造成的复杂性问题。该系统通过基于代理的双流管道，结合上下文感知的内联建议和对话指导，以及语义-动画映射，实现LLM驱动的创意意图转换。它还提供同步预览和统一参数调整。用户研究证明，该系统能有效帮助非专业用户完成文本动画创作，并为LLM在更广泛视频创作领域的应用提供了可能性。", "keywords": "文本动画, 大型语言模型, 智能编辑系统, 用户交互, 创意生产力", "comments": "该论文创新性地将大型语言模型应用于文本动画编辑，通过智能化的交互方式（内联建议和聊天指导）显著降低了非专业用户的操作门槛，提高了创意效率。其基于代理的双流管道设计和语义-动画映射是核心亮点。这项工作为LLM在多媒体内容创作领域的应用提供了有价值的探索。"}}
{"id": "2506.10287", "title": "Multi-Timescale Dynamics Model Bayesian Optimization for Plasma Stabilization in Tokamaks", "authors": ["Rohit Sonker", "Alexandre Capone", "Andrew Rothstein", "Hiro Josep Farre Kaga", "Egemen Kolemen", "Jeff Schneider"], "summary": "Machine learning algorithms often struggle to control complex real-world\nsystems. In the case of nuclear fusion, these challenges are exacerbated, as\nthe dynamics are notoriously complex, data is poor, hardware is subject to\nfailures, and experiments often affect dynamics beyond the experiment's\nduration. Existing tools like reinforcement learning, supervised learning, and\nBayesian optimization address some of these challenges but fail to provide a\ncomprehensive solution. To overcome these limitations, we present a multi-scale\nBayesian optimization approach that integrates a high-frequency data-driven\ndynamics model with a low-frequency Gaussian process. By updating the Gaussian\nprocess between experiments, the method rapidly adapts to new data, refining\nthe predictions of the less reliable dynamical model. We validate our approach\nby controlling tearing instabilities in the DIII-D nuclear fusion plant.\nOffline testing on historical data shows that our method significantly\noutperforms several baselines. Results on live experiments on the DIII-D\ntokamak, conducted under high-performance plasma scenarios prone to\ninstabilities, shows a 50% success rate, marking a 117% improvement over\nhistorical outcomes.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10287v1", "AI": {"title_translation": "多时间尺度动力学模型贝叶斯优化在托卡马克等离子体稳态控制中的应用", "tldr": "该研究提出了一种多时间尺度贝叶斯优化方法，结合高频数据驱动模型和低频高斯过程，用于在托卡马克中稳定等离子体，并在DIII-D核聚变装置的实时实验中取得了显著的成功率提升。", "motivation": "机器学习算法在控制复杂现实世界系统（尤其是核聚变）时面临挑战，因为动力学复杂、数据质量差、硬件故障频繁且实验会影响后续动力学。现有工具无法提供全面的解决方案。", "method": "提出了一种多时间尺度贝叶斯优化方法，该方法将高频数据驱动动力学模型与低频高斯过程相结合。通过在实验之间更新高斯过程，该方法能够快速适应新数据，并改进对不太可靠的动力学模型的预测。", "result": "离线测试显示该方法显著优于多个基线。在DIII-D托卡马克高功率等离子体场景下的实时实验中，成功率为50%，比历史结果提高了117%。", "conclusion": "所提出的多时间尺度贝叶斯优化方法有效解决了托卡马克等离子体稳定控制中的复杂挑战，并在实时实验中表现出显著的性能提升。", "translation": "机器学习算法在控制复杂的现实世界系统时常常遇到困难。在核聚变领域，这些挑战更为严峻，因为动力学极其复杂，数据质量不佳，硬件容易发生故障，而且实验往往会影响超出实验持续时间的动力学。现有的工具，如强化学习、监督学习和贝叶斯优化，虽然解决了其中一些挑战，但未能提供一个全面的解决方案。为了克服这些局限性，我们提出了一种多时间尺度贝叶斯优化方法，该方法将高频数据驱动动力学模型与低频高斯过程相结合。通过在实验之间更新高斯过程，该方法能够快速适应新数据，从而改进对不太可靠的动力学模型的预测。我们通过控制DIII-D核聚变装置中的撕裂模不稳定性来验证我们的方法。在历史数据上的离线测试表明，我们的方法显著优于多个基线。在DIII-D托卡马克上，在高功率等离子体易不稳定场景下进行的实时实验结果显示，成功率为50%，比历史结果提高了117%。", "summary": "本文提出了一种用于托卡马克等离子体稳定控制的多时间尺度贝叶斯优化方法。该方法结合了高频数据驱动动力学模型和低频高斯过程，通过在实验间更新高斯过程来快速适应新数据并改进模型预测。在DIII-D核聚变装置上，该方法在离线测试中表现优异，并在实时实验中实现了50%的成功率，相较历史结果提升了117%，有效解决了复杂核聚变系统控制的挑战。", "keywords": "贝叶斯优化, 等离子体稳定, 托卡马克, 多时间尺度, 核聚变", "comments": "这项研究的创新之处在于其多时间尺度贝叶斯优化方法，有效地结合了不同频率的数据模型，以适应核聚变等复杂系统的动态变化和数据不确定性。在实际核聚变装置上的成功应用，特别是实时实验中显著的性能提升，凸显了其在解决现实世界复杂控制问题上的重要性和潜力。"}}
{"id": "2506.10145", "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hsin-Pai Cheng", "Litian Liu", "Shweta Mahajan", "Apratim Bhattacharyya", "Yunxiao Shi", "Risheek Garrepalli", "Hong Cai", "Fatih Porikli"], "summary": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10145v1", "AI": {"title_translation": "RoCA：鲁棒的跨域端到端自动驾驶", "tldr": "RoCA是一个用于鲁棒跨域端到端自动驾驶的新框架，它通过学习基于高斯过程的基元令牌来提升模型的泛化能力和在新域的适应性。", "motivation": "端到端自动驾驶在跨域部署（如不同城市）时面临实际挑战，现有的方法，包括集成大型语言模型，无法保证跨域性能且可能产生高昂的训练成本。", "method": "本文提出了RoCA框架，通过在端到端管道中对编码自我和周围车辆信息的令牌的联合概率分布进行建模。RoCA通过高斯过程（GP）实例化，学习一组具有对应轨迹的基元令牌，这些令牌涵盖了多样化的驾驶场景，从而能够概率性地推断未来轨迹。RoCA与基础端到端模型在源域进行训练，以提高基础模型的泛化能力，且无需额外推理计算。", "result": "RoCA在新的目标域上实现了鲁棒适应，显著优于直接微调，并在各种跨域场景中表现出强大的域泛化和适应性能。", "conclusion": "RoCA提供了一种有效的方法，可以在不增加推理计算的情况下，显著提升端到端自动驾驶模型在不同领域间的泛化能力和适应性，从而解决了跨域部署的实际挑战。", "translation": "端到端（E2E）自动驾驶最近作为一种新范式出现，具有巨大的潜力。然而，很少有研究关注跨域（例如城市）部署的实际挑战。尽管一些工作已将大型语言模型（LLMs）纳入以利用其开放世界知识，但LLMs不能保证跨域驾驶性能，并且在域适应期间可能产生高昂的再训练成本。在本文中，我们提出了RoCA，一个用于鲁棒跨域E2E自动驾驶的新颖框架。RoCA在E2E管道中对编码自我和周围车辆信息的令牌的联合概率分布进行建模。通过高斯过程（GP）实例化，RoCA学习了一组具有相应轨迹的基元令牌，这些令牌涵盖了多样化的驾驶场景。然后，给定任何驾驶场景，它能够概率性地推断未来的轨迹。通过在源域训练中将RoCA与基础E2E模型结合使用，我们提高了基础模型的泛化能力，而无需额外的推理计算。此外，RoCA能够在新的目标域上实现鲁棒的适应，显著优于直接微调。我们在各种跨域场景中广泛评估了RoCA，并表明它实现了强大的域泛化和适应性能。", "summary": "本文提出了RoCA，一个针对鲁棒跨域端到端自动驾驶的新框架。针对现有方法在跨域部署和适应方面的局限性，RoCA通过高斯过程学习一组涵盖多样化驾驶场景的基元令牌及其轨迹，从而概率性地推断未来轨迹。RoCA与基础端到端模型结合使用，能在不增加推理计算的情况下提高模型泛化能力，并在新目标域上实现鲁棒适应，性能显著优于直接微调。", "keywords": "跨域自动驾驶, 端到端, 高斯过程, 域泛化, 域适应", "comments": "RoCA的创新之处在于其通过高斯过程学习基元令牌以捕捉多样化驾驶场景，并将其集成到E2E自动驾驶框架中，从而在不增加推理成本的情况下提升了跨域泛化和适应能力。这种方法有效地解决了自动驾驶领域中长期存在的域漂移问题，具有重要的实际应用价值。"}}
{"id": "2506.10102", "title": "Learning to Collaborate Over Graphs: A Selective Federated Multi-Task Learning Approach", "authors": ["Ahmed Elbakary", "Chaouki Ben Issaid", "Mehdi Bennis"], "summary": "We present a novel federated multi-task learning method that leverages\ncross-client similarity to enable personalized learning for each client. To\navoid transmitting the entire model to the parameter server, we propose a\ncommunication-efficient scheme that introduces a feature anchor, a compact\nvector representation that summarizes the features learned from the client's\nlocal classes. This feature anchor is shared with the server to account for\nlocal clients' distribution. In addition, the clients share the classification\nheads, a lightweight linear layer, and perform a graph-based regularization to\nenable collaboration among clients. By modeling collaboration between clients\nas a dynamic graph and continuously updating and refining this graph, we can\naccount for any drift from the clients. To ensure beneficial knowledge transfer\nand prevent negative collaboration, we leverage a community detection-based\napproach that partitions this dynamic graph into homogeneous communities,\nmaximizing the sum of task similarities, represented as the graph edges'\nweights, within each community. This mechanism restricts collaboration to\nhighly similar clients within their formed communities, ensuring positive\ninteraction and preserving personalization. Extensive experiments on two\nheterogeneous datasets demonstrate that our method significantly outperforms\nstate-of-the-art baselines. Furthermore, we show that our method exhibits\nsuperior computation and communication efficiency and promotes fairness across\nclients.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10102v1", "AI": {"title_translation": "基于图的协作学习：一种选择性联邦多任务学习方法", "tldr": "提出了一种新的联邦多任务学习方法，通过特征锚、分类头共享和基于图的社区检测实现客户端间的选择性协作，以提供个性化、高效且公平的学习，显著优于现有方法。", "motivation": "为了实现客户端的个性化学习并避免向参数服务器传输整个模型，从而提高通信效率。", "method": "该方法引入了特征锚（一个总结本地类特征的紧凑向量表示）与服务器共享。客户端还共享轻量级线性层分类头，并进行基于图的正则化以实现协作。通过将客户端协作建模为动态图并持续更新，利用社区检测方法将动态图划分为同质社区，从而限制协作仅限于社区内高度相似的客户端，确保积极的知识转移和个性化。", "result": "在两个异构数据集上的大量实验表明，该方法显著优于现有基线方法。此外，它还展示了卓越的计算和通信效率，并促进了客户端之间的公平性。", "conclusion": "所提出的选择性联邦多任务学习方法通过智能地管理客户端协作，有效实现了个性化、高效和公平的学习，并显著提升了性能。", "translation": "我们提出了一种新颖的联邦多任务学习方法，该方法利用客户端之间的相似性来实现每个客户端的个性化学习。为了避免将整个模型传输到参数服务器，我们提出了一种通信高效的方案，引入了特征锚，这是一种紧凑的向量表示，总结了从客户端本地类别中学到的特征。该特征锚与服务器共享，以考虑本地客户端的分布。此外，客户端共享分类头（一个轻量级线性层），并执行基于图的正则化以实现客户端之间的协作。通过将客户端之间的协作建模为动态图并持续更新和完善该图，我们可以解释客户端的任何漂移。为了确保有益的知识转移并防止负面协作，我们利用基于社区检测的方法将此动态图划分为同质社区，最大化每个社区内任务相似性（表示为图边的权重）的总和。这种机制将协作限制在所形成的社区内高度相似的客户端之间，确保积极的交互并保持个性化。在两个异构数据集上进行的大量实验表明，我们的方法显著优于最先进的基线方法。此外，我们表明我们的方法表现出卓越的计算和通信效率，并促进了客户端之间的公平性。", "summary": "本文提出了一种新颖的联邦多任务学习方法，旨在通过利用客户端相似性实现个性化学习，并提升通信效率。核心机制包括共享特征锚以总结本地特征，共享分类头，以及采用动态图和社区检测进行基于图的协作正则化。这种方法智能地将协作限制在高度相似的客户端社区内，确保了积极的知识转移和个性化。实验证明，该方法在性能上显著优于现有基线，并在计算、通信效率和客户端公平性方面表现出色。", "keywords": "联邦学习, 多任务学习, 图神经网络, 社区检测, 个性化学习", "comments": "该论文的创新点在于其巧妙地将图理论和社区检测应用于联邦多任务学习中，实现了客户端之间选择性的、有益的协作，有效解决了传统联邦学习中异构性带来的负面协作问题。通过引入特征锚和分类头共享，在保证通信效率的同时，实现了模型个性化。这种动态协作机制对于处理实际联邦学习场景中的数据漂移和客户端多样性具有重要意义。"}}
{"id": "2506.10570", "title": "6G Infrastructures for Edge AI: An Analytical Perspective", "authors": ["Kurt Horvath", "Shpresa Tuda", "Blerta Idrizi", "Stojan Kitanov", "Fisnik Doko", "Dragi Kimovski"], "summary": "The convergence of Artificial Intelligence (AI) and the Internet of Things\nhas accelerated the development of distributed, network-sensitive applications,\nnecessitating ultra-low latency, high throughput, and real-time processing\ncapabilities. While 5G networks represent a significant technological\nmilestone, their ability to support AI-driven edge applications remains\nconstrained by performance gaps observed in real-world deployments. This paper\naddresses these limitations and highlights critical advancements needed to\nrealize a robust and scalable 6G ecosystem optimized for AI applications.\nFurthermore, we conduct an empirical evaluation of 5G network infrastructure in\ncentral Europe, with latency measurements ranging from 61 ms to 110 ms across\ndifferent close geographical areas. These values exceed the requirements of\nlatency-critical AI applications by approximately 270%, revealing significant\nshortcomings in current deployments. Building on these findings, we propose a\nset of recommendations to bridge the gap between existing 5G performance and\nthe requirements of next-generation AI applications.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10570v1", "AI": {"title_translation": "用于边缘AI的6G基础设施：分析视角", "tldr": "5G网络在支持边缘AI应用方面存在延迟限制，本研究通过实证评估揭示了这些不足，并提出了6G基础设施的改进建议，以满足下一代AI应用的需求。", "motivation": "当前5G网络在支持AI驱动的边缘应用方面存在性能差距，特别是在超低延迟、高吞吐量和实时处理能力方面。实证评估显示，5G网络的延迟远超关键AI应用的需求，因此需要新的技术进步和6G生态系统来克服这些限制。", "method": "本文通过对中欧地区5G网络基础设施进行实证评估，测量了不同近地理区域的延迟。在此基础上，提出了弥合现有5G性能与下一代AI应用需求之间差距的建议。", "result": "实证评估显示，中欧地区5G网络的延迟测量范围为61毫秒到110毫秒，这比延迟敏感型AI应用的要求高出约270%，揭示了当前部署的显著不足。", "conclusion": "现有5G网络在支持AI驱动的边缘应用方面存在显著性能限制，尤其是在延迟方面。为了满足下一代AI应用的需求，需要发展更强大的6G基础设施，并提出了具体的改进建议以弥补这一差距。", "translation": "人工智能（AI）和物联网的融合加速了分布式、网络敏感型应用的发展，需要超低延迟、高吞吐量和实时处理能力。尽管5G网络是一个重要的技术里程碑，但其支持AI驱动边缘应用的能力仍受限于实际部署中观察到的性能差距。本文解决了这些限制，并强调了实现为AI应用优化的强大且可扩展的6G生态系统所需的关键进展。此外，我们对中欧地区的5G网络基础设施进行了实证评估，不同近地理区域的延迟测量范围为61毫秒至110毫秒。这些值超出延迟敏感型AI应用要求约270%，揭示了当前部署的显著不足。基于这些发现，我们提出了一系列建议，以弥合现有5G性能与下一代AI应用需求之间的差距。", "summary": "本研究探讨了5G网络在支持边缘AI应用方面的局限性，指出其在延迟、吞吐量和实时处理方面的不足。通过对中欧5G网络的实证评估，发现其延迟远超关键AI应用的需求。基于这些发现，论文提出了为6G生态系统优化的关键改进建议，旨在满足未来AI应用的严格性能要求。", "keywords": "6G基础设施, 边缘AI, 5G性能, 延迟测量, 网络优化", "comments": "本文通过实证数据清晰地指出了当前5G网络在支持边缘AI应用方面的局限性，特别是在延迟方面。其创新之处在于将理论分析与实际测量相结合，为未来6G基础设施的发展提供了明确的方向和具体的改进建议，强调了超低延迟对于边缘AI的重要性。"}}
{"id": "2506.10357", "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable Task Experts", "authors": ["Zaijing Li", "Yuquan Xie", "Rui Shao", "Gongwei Chen", "Weili Guan", "Dongmei Jiang", "Liqiang Nie"], "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/", "comment": "24 pages, 10 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10357v1", "AI": {"title_translation": "Optimus-3：迈向具有可扩展任务专家的通用多模态Minecraft智能体", "tldr": "Optimus-3是一个新的通用多模态Minecraft智能体，通过知识增强数据生成、MoE架构和多模态推理增强强化学习，解决了在Minecraft中构建通用智能体的挑战，并超越了现有SOTA。", "motivation": "在Minecraft等开放世界环境中构建具备感知、规划、行动、接地和反思能力的通用智能体面临挑战，具体包括：领域特定数据不足、异构任务间的干扰以及开放世界设置中的视觉多样性。", "method": "本文通过三项关键贡献解决挑战：1）提出知识增强的数据生成管道，提供可扩展且高质量的训练数据；2）引入带有任务级路由的专家混合（MoE）架构，以减轻异构任务间的干扰；3）开发多模态推理增强强化学习方法，增强智能体对Minecraft中视觉多样性的推理能力。这些创新共同构建了Optimus-3。", "result": "Optimus-3在Minecraft环境中的广泛任务上，超越了通用的多模态大语言模型和现有的最先进智能体。", "conclusion": "Optimus-3通过其创新的数据生成、架构和推理方法，成功克服了在Minecraft中构建通用多模态智能体的挑战，并在广泛任务上实现了卓越的性能。", "translation": "最近，基于多模态大语言模型（MLLMs）的智能体在各个领域取得了显著进展。然而，在像Minecraft这样的开放世界环境中，构建一个具备感知、规划、行动、接地和反思等能力的通用智能体仍然面临挑战：领域特定数据不足、异构任务之间的干扰以及开放世界设置中的视觉多样性。在本文中，我们通过三项关键贡献来解决这些挑战。1）我们提出了一种知识增强的数据生成管道，为智能体开发提供可扩展且高质量的训练数据。2）为了减轻异构任务之间的干扰，我们引入了一种带有任务级路由的专家混合（MoE）架构。3）我们开发了一种多模态推理增强强化学习方法，以增强智能体对Minecraft中视觉多样性的推理能力。基于这些创新，我们提出了Optimus-3，一个用于Minecraft的通用智能体。广泛的实验结果表明，Optimus-3在Minecraft环境中的广泛任务上，超越了通用的多模态大语言模型和现有的最先进智能体。项目页面：https://cybertronagent.github.io/Optimus-3.github.io/", "summary": "本文介绍了Optimus-3，一个针对Minecraft开放世界环境设计的通用多模态智能体。该智能体通过解决数据稀缺、任务干扰和视觉多样性等挑战，实现了对现有方法的超越。其核心创新包括知识增强的数据生成管道、带有任务级路由的专家混合（MoE）架构以及多模态推理增强强化学习方法。实验结果证明Optimus-3在Minecraft的多种任务中表现优异，超越了其他通用多模态大语言模型和SOTA智能体。", "keywords": "Minecraft, 通用智能体, 多模态大语言模型, 专家混合, 强化学习", "comments": "Optimus-3的创新之处在于其针对Minecraft开放世界环境的特定挑战提出了全面的解决方案，包括数据、模型架构和推理能力。知识增强的数据生成管道解决了领域数据不足的问题，MoE架构有效处理了任务异构性，而多模态推理增强RL则提升了对视觉多样性的适应性。这些结合使得Optimus-3在通用代理领域迈出了重要一步，特别是在复杂、动态的游戏环境中展现了潜力。"}}
{"id": "2506.10194", "title": "Guardians of the Regime: When and Why Autocrats Create Secret Police", "authors": ["Marius Mehrl", "Mila Pfander", "Theresa Winner", "Cornelius Fritz"], "summary": "Autocrats use secret police to stay in power, as these organizations deter\nand suppress opposition to their rule. Existing research shows that secret\npolice are very good at this but, surprisingly, also that they are not as\nubiquitous in autocracies as one may assume, existing in less than 50% of\nautocratic country-years. We thus explore under which conditions secret police\nemerge in dictatorships. For this purpose, we apply statistical variable\nselection techniques to identify which of several candidate variables extracted\nfrom the literature on state security forces and authoritarian survival hold\nexplanatory power. Our results highlight that secret police are more likely to\nemerge when rulers face specific, preempt-able threats, such as protests and\nanti-system mobilisation, but also when they have the material resources to\nestablish these organisations. This research contributes to our understanding\nof autocrats' institutional choices and authoritarian politics.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10194v1", "AI": {"title_translation": "政权的守护者：独裁者何时以及为何设立秘密警察", "tldr": "独裁者设立秘密警察以维持权力，但秘密警察并非无处不在。本研究探讨了秘密警察在独裁政权中出现的条件，发现其出现与统治者面临可预防的威胁以及拥有物质资源有关。", "motivation": "现有研究表明秘密警察在镇压反对派方面非常有效，但令人惊讶的是，它们在独裁政权中并不像人们想象的那么普遍，仅存在于不到50%的独裁国家-年份中。因此，本文旨在探讨秘密警察在独裁政权中出现的条件。", "method": "本文应用统计变量选择技术，从关于国家安全部队和威权生存的文献中提取了几个候选变量，以识别哪些变量具有解释力。", "result": "研究结果表明，当统治者面临特定的、可预防的威胁（如抗议和反体制动员）时，以及当他们拥有建立这些组织的物质资源时，秘密警察更有可能出现。", "conclusion": "这项研究有助于我们理解独裁者的制度选择和威权政治。", "translation": "独裁者利用秘密警察来维持权力，因为这些组织能够威慑和镇压对其统治的反对。现有研究表明，秘密警察在这方面非常有效，但令人惊讶的是，它们在独裁政权中并不像人们想象的那么普遍，仅存在于不到50%的独裁国家-年份中。因此，我们探讨了秘密警察在独裁统治中出现的条件。为此，我们应用统计变量选择技术，从关于国家安全部队和威权生存的文献中识别出几个候选变量中哪些具有解释力。我们的结果强调，当统治者面临特定的、可预防的威胁（例如抗议和反体制动员）时，以及当他们拥有建立这些组织的物质资源时，秘密警察更有可能出现。这项研究有助于我们理解独裁者的制度选择和威权政治。", "summary": "本文探讨了秘密警察在独裁政权中出现的原因和条件。尽管秘密警察对维持独裁统治有效，但它们并非普遍存在。研究通过统计变量选择技术分析发现，秘密警察的出现与统治者面临可预防的威胁（如抗议）以及其拥有的物质资源密切相关。这项工作加深了对独裁者制度选择和威权政治的理解。", "keywords": "秘密警察, 独裁政权, 威权政治, 制度选择, 威胁", "comments": "这项研究通过实证分析填补了对秘密警察出现条件理解的空白，挑战了其普遍存在的假设。其创新之处在于运用统计变量选择技术，揭示了威胁类型和资源可用性这两个关键因素。这对于理解威权政权的内部运作和稳定性具有重要意义。"}}
{"id": "2506.10825", "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches", "authors": ["Andrea Moglia", "Matteo Leccardi", "Matteo Cavicchioli", "Alice Maccarini", "Marco Marcon", "Luca Mainardi", "Pietro Cerveri"], "summary": "Following the successful paradigm shift of large language models, leveraging\npre-training on a massive corpus of data and fine-tuning on different\ndownstream tasks, generalist models have made their foray into computer vision.\nThe introduction of Segment Anything Model (SAM) set a milestone on\nsegmentation of natural images, inspiring the design of a multitude of\narchitectures for medical image segmentation. In this survey we offer a\ncomprehensive and in-depth investigation on generalist models for medical image\nsegmentation. We start with an introduction on the fundamentals concepts\nunderpinning their development. Then, we provide a taxonomy on the different\ndeclinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on\nthe recent SAM 2, on other innovative models trained on images alone, and\nothers trained on both text and images. We thoroughly analyze their\nperformances at the level of both primary research and best-in-literature,\nfollowed by a rigorous comparison with the state-of-the-art task-specific\nmodels. We emphasize the need to address challenges in terms of compliance with\nregulatory frameworks, privacy and security laws, budget, and trustworthy\nartificial intelligence (AI). Finally, we share our perspective on future\ndirections concerning synthetic data, early fusion, lessons learnt from\ngeneralist models in natural language processing, agentic AI and physical AI,\nand clinical translation.", "comment": "132 pages, 26 figures, 23 tables. Andrea Moglia and Matteo Leccardi\n  are equally contributing authors", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10825v1", "AI": {"title_translation": "医学图像分割中的通用模型：一项调查以及与任务特定方法的性能比较", "tldr": "本调查深入探讨了医学图像分割中的通用模型，涵盖了其基本概念、SAM的各种变体、性能分析，并与任务特定模型进行了比较，同时强调了挑战和未来方向。", "motivation": "受大型语言模型成功的启发，通用模型已进入计算机视觉领域，特别是Segment Anything Model (SAM) 在自然图像分割中取得了里程碑式的进展。这激发了医学图像分割领域中大量新架构的设计，因此有必要对医学图像分割中的通用模型进行全面深入的调查。", "method": "本调查首先介绍了通用模型开发的基本概念，然后对SAM的不同变体（包括零样本、少样本、微调、适配器、SAM 2以及其他仅用图像或图像与文本训练的创新模型）进行了分类。研究人员对这些模型在初级研究和最佳文献中的性能进行了彻底分析，并与最先进的任务特定模型进行了严格比较。", "result": "调查提供了通用模型在医学图像分割中的全面深入调查，包括其基本概念和基于SAM的分类。它详细分析了这些模型在初级研究和最佳文献中的性能，并与最先进的任务特定模型进行了严格比较。调查还强调了在法规遵从、隐私安全、预算和可信人工智能方面的挑战。", "conclusion": "医学图像分割中的通用模型表现出巨大潜力，但仍需解决法规遵从、隐私安全、预算和可信人工智能等方面的挑战。未来的研究方向包括合成数据、早期融合、从自然语言处理中的通用模型学习、代理人工智能、物理人工智能以及临床转化。", "translation": "继大型语言模型成功范式转变之后，通过对海量数据进行预训练并在不同下游任务上进行微调，通用模型已进入计算机视觉领域。Segment Anything Model (SAM) 的引入在自然图像分割方面树立了里程碑，启发了医学图像分割领域中大量架构的设计。在本调查中，我们对医学图像分割中的通用模型进行了全面深入的探讨。我们首先介绍了支撑其发展的基础概念。然后，我们根据零样本、少样本、微调、适配器、最近的SAM 2、其他仅用图像训练的创新模型以及同时用文本和图像训练的模型，对SAM的不同变体进行了分类。我们从原始研究和最佳文献层面彻底分析了它们的性能，随后与最先进的任务特定模型进行了严格比较。我们强调需要解决在遵守法规框架、隐私和安全法律、预算以及可信人工智能（AI）方面的挑战。最后，我们分享了我们对合成数据、早期融合、从自然语言处理中的通用模型中吸取的经验教训、代理AI和物理AI以及临床转化等未来方向的看法。", "summary": "本调查全面回顾了医学图像分割中的通用模型，重点关注其基本概念、Segment Anything Model (SAM) 的各种变体（包括零样本、少样本、微调和适配器），以及其他创新模型。文章详细分析了这些模型在现有研究和最佳文献中的性能，并将其与任务特定方法进行了严格比较。此外，调查还指出了通用模型在法规遵从、隐私安全、预算和可信AI方面的挑战，并展望了合成数据、早期融合、跨领域学习和临床转化等未来研究方向。", "keywords": "通用模型, 医学图像分割, SAM, 调查, 性能比较", "comments": "这篇综述论文的重要性在于它及时地总结了通用模型在医学图像分割这一关键领域的应用和发展。随着像SAM这样的模型在自然图像处理中取得成功，将其范式引入医疗领域是必然趋势。该论文不仅提供了一个清晰的分类框架，还对现有模型进行了性能分析和比较，这对于研究人员和临床医生理解当前进展和未来挑战至关重要。强调法规、隐私和可信AI的挑战，也体现了其对实际应用和伦理考量的深刻洞察。"}}
{"id": "2506.10154", "title": "Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME", "authors": ["Bidyarthi Paul", "SM Musfiqur Rahman", "Dipta Biswas", "Md. Ziaul Hasan", "Md. Zahid Hossain"], "summary": "Research on understanding emotions in written language continues to expand,\nespecially for understudied languages with distinctive regional expressions and\ncultural features, such as Bangla. This study examines emotion analysis using\n22,698 social media comments from the EmoNoBa dataset. For language analysis,\nwe employ machine learning models: Linear SVM, KNN, and Random Forest with\nn-gram data from a TF-IDF vectorizer. We additionally investigated how PCA\naffects the reduction of dimensionality. Moreover, we utilized a BiLSTM model\nand AdaBoost to improve decision trees. To make our machine learning models\neasier to understand, we used LIME to explain the predictions of the AdaBoost\nclassifier, which uses decision trees. With the goal of advancing sentiment\nanalysis in languages with limited resources, our work examines various\ntechniques to find efficient techniques for emotion identification in Bangla.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10154v1", "AI": {"title_translation": "孟加拉语社交媒体评论情感分析：机器学习与LIME的应用", "tldr": "本研究利用机器学习和LIME对孟加拉语社交媒体评论进行情感分析，旨在为资源匮乏语言寻找高效情感识别技术。", "motivation": "现有书面语言情感理解研究不断扩展，但对孟加拉语等具有独特地域表达和文化特征的欠研究语言关注不足。本研究旨在推动资源受限语言的情感分析进展，寻找孟加拉语情感识别的有效技术。", "method": "研究使用了包含22,698条社交媒体评论的EmoNoBa数据集。采用的机器学习模型包括：基于TF-IDF向量化器n-gram数据的线性SVM、KNN和随机森林。同时，研究探讨了PCA对降维的影响，并使用BiLSTM模型和AdaBoost改进决策树。为提高模型可解释性，利用LIME解释了AdaBoost分类器的预测。", "result": "本研究通过检验多种技术，旨在发现孟加拉语情感识别的有效技术。", "conclusion": "本研究旨在通过检验各种技术，为资源有限的孟加拉语情感识别寻找高效的方法。", "translation": "书面语言情感理解的研究持续扩展，特别是对于孟加拉语等具有独特地域表达和文化特征的欠研究语言。本研究利用来自EmoNoBa数据集的22,698条社交媒体评论，探讨了情感分析。在语言分析方面，我们采用了机器学习模型：线性支持向量机（Linear SVM）、K近邻（KNN）和随机森林（Random Forest），并使用TF-IDF向量化器生成的n-gram数据。我们还研究了PCA对降维的影响。此外，我们利用BiLSTM模型和AdaBoost来改进决策树。为了使我们的机器学习模型更易于理解，我们使用LIME来解释AdaBoost分类器（该分类器使用决策树）的预测。为了推进资源有限语言的情感分析，我们的工作检验了各种技术，以寻找孟加拉语情感识别的有效技术。", "summary": "本研究针对孟加拉语这一欠研究语言，利用EmoNoBa数据集中的22,698条社交媒体评论，探讨了情感分析方法。研究采用了多种机器学习模型，包括基于TF-IDF的线性SVM、KNN和随机森林，并探究了PCA的降维效果。此外，还应用了BiLSTM和AdaBoost来优化决策树模型，并利用LIME提升了AdaBoost分类器的可解释性。这项工作旨在为资源匮乏语言，特别是孟加拉语，寻找高效的情感识别技术。", "keywords": "孟加拉语情感分析, 机器学习, LIME, 社交媒体, 自然语言处理", "comments": "这篇论文的创新点在于它专注于孟加拉语这一资源匮乏的语言，并结合了多种机器学习模型（传统与深度学习）以及可解释性AI（LIME）来解决情感分析问题。LIME的应用提高了模型透明度，这对于实际应用至关重要。论文为孟加拉语情感识别提供了多角度的技术探索，对于该领域的未来研究具有重要参考价值。"}}
{"id": "2506.10802", "title": "Constructing and Evaluating Declarative RAG Pipelines in PyTerrier", "authors": ["Craig Macdonald", "Jinyuan Fang", "Andrew Parry", "Zaiqiao Meng"], "summary": "Search engines often follow a pipeline architecture, where complex but\neffective reranking components are used to refine the results of an initial\nretrieval. Retrieval augmented generation (RAG) is an exciting application of\nthe pipeline architecture, where the final component generates a coherent\nanswer for the users from the retrieved documents. In this demo paper, we\ndescribe how such RAG pipelines can be formulated in the declarative PyTerrier\narchitecture, and the advantages of doing so. Our PyTerrier-RAG extension for\nPyTerrier provides easy access to standard RAG datasets and evaluation\nmeasures, state-of-the-art LLM readers, and using PyTerrier's unique operator\nnotation, easy-to-build pipelines. We demonstrate the succinctness of indexing\nand RAG pipelines on standard datasets (including Natural Questions) and how to\nbuild on the larger PyTerrier ecosystem with state-of-the-art sparse,\nlearned-sparse, and dense retrievers, and other neural rankers.", "comment": "4 pages, 3 tables, Accepted to SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10802v1", "AI": {"title_translation": "在PyTerrier中构建和评估声明式RAG管道", "tldr": "本文介绍如何在PyTerrier中声明式地构建和评估检索增强生成（RAG）管道，并展示了PyTerrier-RAG扩展的优势，包括易于构建管道、访问标准数据集和评估指标，以及集成先进的检索器和排序器。", "motivation": "检索增强生成（RAG）是管道架构的一个重要应用，它通过从检索到的文档中生成连贯的答案来改进搜索结果。本文的动机是展示如何在声明式PyTerrier架构中构建RAG管道，并阐述这样做的好处。", "method": "本文描述了如何在声明式PyTerrier架构中构建RAG管道。他们引入了PyTerrier的PyTerrier-RAG扩展，该扩展提供了对标准RAG数据集和评估指标的便捷访问、最先进的LLM阅读器，并利用PyTerrier独特的运算符表示法来轻松构建管道。作者通过在标准数据集（包括Natural Questions）上演示索引和RAG管道的简洁性来展示其方法。", "result": "结果表明，使用PyTerrier-RAG扩展可以在标准数据集上简洁地构建索引和RAG管道。此外，该方法展示了如何利用更大的PyTerrier生态系统，包括最先进的稀疏、学习稀疏和密集检索器以及其他神经排序器。", "conclusion": "本文的结论是，在声明式PyTerrier架构中构建RAG管道具有显著优势，PyTerrier-RAG扩展使得构建、评估和集成先进组件的RAG管道变得容易和高效。", "translation": "搜索引擎通常遵循管道架构，其中复杂但有效的重排序组件用于细化初始检索的结果。检索增强生成（RAG）是管道架构的一个令人兴奋的应用，其中最终组件从检索到的文档中为用户生成连贯的答案。在这篇演示论文中，我们描述了如何在声明式PyTerrier架构中构建此类RAG管道，以及这样做的好处。我们的PyTerrier-RAG扩展为PyTerrier提供了对标准RAG数据集和评估指标、最先进的LLM阅读器的便捷访问，并使用PyTerrier独特的运算符表示法，易于构建管道。我们演示了在标准数据集（包括Natural Questions）上索引和RAG管道的简洁性，以及如何利用更大的PyTerrier生态系统，包括最先进的稀疏、学习稀疏和密集检索器以及其他神经排序器。", "summary": "本文介绍了一种在声明式PyTerrier架构中构建和评估检索增强生成（RAG）管道的方法。通过引入PyTerrier-RAG扩展，研究者能够轻松访问标准的RAG数据集、评估指标和先进的LLM阅读器。论文展示了如何利用PyTerrier独特的运算符表示法简洁地构建索引和RAG管道，并演示了如何集成PyTerrier生态系统中现有的稀疏、学习稀疏和密集检索器以及其他神经排序器。", "keywords": "RAG, PyTerrier, 检索增强生成, 声明式管道, 信息检索", "comments": "本文的创新之处在于将RAG管道以声明式的方式集成到PyTerrier框架中，这极大地简化了RAG系统的构建和评估过程。PyTerrier-RAG扩展提供了一个统一的平台，方便研究人员和开发者利用现有资源和先进技术。其重要性在于，它降低了RAG系统开发的复杂性，并促进了RAG研究的标准化和可复现性。"}}
{"id": "2506.10533", "title": "Non-augmented velocity-vorticity-pressure formulation for the Navier--Stokes--Brinkman--Forchheimer problem", "authors": ["Santiago Badia", "Carsten Carstensen", "Alberto F. Martin", "Ricardo Ruiz-Baier", "Segundo Villa-Fuentes"], "summary": "The flow of incompressible fluid in highly permeable porous media in\nvorticity - velocity - Bernoulli pressure form leads to a double saddle-point\nproblem in the Navier--Stokes--Brinkman--Forchheimer equations. The paper\nestablishes, for small sources, the existence of solutions on the continuous\nand discrete level of lowest-order piecewise divergence-free Crouzeix--Raviart\nfinite elements. The vorticity employs a vector version of the pressure space\nwith normal and tangential velocity jump penalisation terms. A simple\nRaviart--Thomas interpolant leads to pressure-robust a priori error estimates.\nAn explicit residual-based a posteriori error estimate allows for efficient and\nreliable a posteriori error control. The efficiency for the Forchheimer\nnonlinearity requires a novel discrete inequality of independent interest. The\nimplementation is based upon a light-weight forest-of-trees data structure\nhandled by a highly parallel set of adaptive {mesh refining} algorithms.\nNumerical simulations reveal robustness of the a posteriori error estimates and\nimproved convergence rates by adaptive mesh-refining.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10533v1", "AI": {"title_translation": "Navier--Stokes--Brinkman--Forchheimer问题的非增广速度-涡度-压力公式", "tldr": "本文研究了Navier--Stokes--Brinkman--Forchheimer方程在涡度-速度-伯努利压力形式下的双鞍点问题。针对小源情况，建立了连续和离散层面解的存在性，并开发了压力鲁棒的误差估计和自适应网格细化方法，提升了数值模拟的鲁棒性和收敛性。", "motivation": "解决Navier--Stokes--Brinkman--Forchheimer方程在涡度-速度-伯努利压力形式下导致的双鞍点问题，并针对小源情况建立连续和离散层面解的存在性。", "method": "采用最低阶分段无散度Crouzeix--Raviart有限元。涡度使用带有法向和切向速度跳跃惩罚项的压力空间向量版本。通过Raviart--Thomas插值器实现压力鲁棒的先验误差估计。利用显式基于残差的后验误差估计进行误差控制。为Forchheimer非线性引入新型离散不等式。实现基于轻量级树林数据结构和高度并行自适应网格细化算法。", "result": "建立了小源情况下连续和离散层面解的存在性。实现了压力鲁棒的先验误差估计和高效可靠的后验误差控制。数值模拟表明后验误差估计具有鲁棒性，并通过自适应网格细化提高了收敛速度。", "conclusion": "本文成功建立了Navier--Stokes--Brinkman--Forchheimer方程非增广速度-涡度-压力公式的解存在性，并开发了有效的误差估计和自适应网格细化方法，显著提高了数值模拟的鲁棒性和收敛性。", "translation": "不可压缩流体在涡度-速度-伯努利压力形式的高度渗透多孔介质中的流动导致Navier--Stokes--Brinkman--Forchheimer方程中的双鞍点问题。本文针对小源情况，建立了最低阶分段无散度Crouzeix--Raviart有限元的连续和离散层面解的存在性。涡度采用了压力空间的向量版本，并带有法向和切向速度跳跃惩罚项。一个简单的Raviart--Thomas插值器导致了压力鲁棒的先验误差估计。一个显式的基于残差的后验误差估计允许高效可靠的后验误差控制。Forchheimer非线性的效率需要一个独立兴趣的新型离散不等式。实现基于轻量级树林数据结构，由一组高度并行的自适应网格细化算法处理。数值模拟揭示了后验误差估计的鲁棒性以及通过自适应网格细化提高的收敛速度。", "summary": "本文研究了Navier--Stokes--Brinkman--Forchheimer方程在涡度-速度-伯努利压力形式下的双鞍点问题。针对小源情况，建立了连续和离散层面解的存在性，并采用了最低阶分段无散度Crouzeix--Raviart有限元。通过Raviart--Thomas插值器实现了压力鲁棒的先验误差估计和基于残差的后验误差估计。研究还引入了一种新型离散不等式以处理Forchheimer非线性，并利用自适应网格细化算法进行高效实现。数值模拟验证了所提方法的鲁棒性和收敛性改进。", "keywords": "Navier--Stokes--Brinkman--Forchheimer, 涡度-速度-压力, 有限元, 误差估计, 多孔介质", "comments": "本文的创新点在于提出了Navier--Stokes--Brinkman--Forchheimer方程的非增广速度-涡度-压力公式，并成功地解决了其双鞍点问题。特别是在理论上证明了小源情况下解的存在性，并结合了压力鲁棒的误差估计和高效的自适应网格细化技术，这对于数值模拟的精度和效率至关重要。引入独立兴趣的新型离散不等式以处理Forchheimer非线性也值得关注。"}}
{"id": "2506.10513", "title": "A Neural Network-aided Low Complexity Chase Decoder for URLLC", "authors": ["Enrico Testi", "Enrico Paolini"], "summary": "Ultra-reliable low-latency communications (URLLC) demand decoding algorithms\nthat simultaneously offer high reliability and low complexity under stringent\nlatency constraints. While iterative decoding schemes for LDPC and Polar codes\noffer a good compromise between performance and complexity, they fall short in\napproaching the theoretical performance limits in the typical URLLC short block\nlength regime. Conversely, quasi-ML decoding schemes for algebraic codes, like\nChase-II decoding, exhibit a smaller gap to optimum decoding but are\ncomputationally prohibitive for practical deployment in URLLC systems. To\nbridge this gap, we propose an enhanced Chase-II decoding algorithm that\nleverages a neural network (NN) to predict promising perturbation patterns,\ndrastically reducing the number of required decoding trials. The proposed\napproach combines the reliability of quasi-ML decoding with the efficiency of\nNN inference, making it well-suited for time-sensitive and resource-constrained\napplications.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10513v1", "AI": {"title_translation": "一种神经网络辅助的低复杂度URLLC追逐译码器", "tldr": "本文提出一种神经网络辅助的Chase-II译码器，通过预测扰动模式，显著降低了URLLC中准ML译码的计算复杂度，同时保持了高可靠性，使其适用于时间敏感和资源受限的应用。", "motivation": "超可靠低延迟通信（URLLC）要求解码算法在严格的延迟限制下同时提供高可靠性和低复杂度。现有迭代解码方案在URLLC短块长度范围内未能接近理论性能极限。准ML解码方案（如Chase-II解码）虽然接近最优解码，但计算成本过高，不适合实际部署。", "method": "提出了一种增强型Chase-II解码算法，该算法利用神经网络（NN）预测有前景的扰动模式，从而大幅减少所需的解码试验次数。", "result": "该方法结合了准ML解码的可靠性与神经网络推理的效率，使其非常适用于时间敏感和资源受限的应用。", "conclusion": "通过利用神经网络降低复杂性，使得准ML解码方案在URLLC系统中变得可行，适用于时间敏感和资源受限的应用。", "translation": "超可靠低延迟通信（URLLC）要求解码算法在严格的延迟限制下同时提供高可靠性和低复杂度。虽然用于LDPC和极化码的迭代解码方案在性能和复杂度之间提供了良好的折衷，但它们在典型的URLLC短块长度范围内未能接近理论性能极限。相反，代数码的准ML解码方案，如Chase-II解码，与最优解码的差距更小，但对于URLLC系统中的实际部署而言，计算成本过高。为了弥合这一差距，我们提出了一种增强型Chase-II解码算法，该算法利用神经网络（NN）预测有前景的扰动模式，从而大幅减少所需的解码试验次数。所提出的方法结合了准ML解码的可靠性与神经网络推理的效率，使其非常适用于时间敏感和资源受限的应用。", "summary": "本文提出了一种新型的神经网络辅助Chase-II译码算法，旨在解决URLLC中高可靠性与低复杂度解码的挑战。该方法通过利用神经网络预测有前景的扰动模式，显著减少了传统的Chase-II译码所需的计算量，同时保持了准最大似然解码的可靠性，使其适用于对时间敏感和资源受限的URLLC系统。", "keywords": "URLLC, 神经网络, Chase译码, 低复杂度, 准ML解码", "comments": "这篇论文的创新点在于将神经网络引入到传统的Chase-II译码器中，以解决其计算复杂度高的问题。通过预测扰动模式，NN有效地减少了所需的解码试验次数，从而在URLLC严苛的延迟和资源限制下，实现了准ML解码的高可靠性和实用性。这为URLLC领域的解码器设计提供了一个有前景的方向，特别是在短块长度通信中。"}}
{"id": "2506.10407", "title": "Semi-Tensor-Product Based Convolutional Neural Networks", "authors": ["Daizhan Cheng"], "summary": "The semi-tensor product (STP) of vectors is a generalization of conventional\ninner product of vectors, which allows the factor vectors to of different\ndimensions. This paper proposes a domain-based convolutional product (CP).\nCombining domain-based CP with STP of vectors, a new CP is proposed. Since\nthere is no zero or any other padding, it can avoid the junk information caused\nby padding. Using it, the STP-based convolutional neural network (CNN) is\ndeveloped. Its application to image and third order signal identifications is\nconsidered.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10407v1", "AI": {"title_translation": "半张量积卷积神经网络", "tldr": "提出了一种基于半张量积的新型卷积神经网络 (STP-CNN)，通过避免填充解决了传统CNN中的信息冗余问题，并应用于图像和三阶信号识别。", "motivation": "传统卷积神经网络中的填充（padding）可能导致冗余信息，本文旨在通过提出一种新的卷积运算和网络结构来解决这个问题。", "method": "本文首先介绍了向量的半张量积 (STP)，它允许不同维度的向量相乘。在此基础上，提出了一种基于域的卷积积 (domain-based CP)。通过将基于域的CP与向量的STP结合，提出了一种新的卷积积，该卷积积不包含零填充或其他填充，从而避免了填充引起的信息冗余。最终，基于这种新的卷积积，开发了基于STP的卷积神经网络 (STP-CNN)。", "result": "该STP-CNN被考虑应用于图像识别和三阶信号识别。", "conclusion": "本文开发了一种基于半张量积的卷积神经网络 (STP-CNN)，其核心在于提出了一种新的、无填充的卷积积，从而有效避免了传统CNN中由填充引起的冗余信息，并展示了其在图像和信号识别领域的应用潜力。", "translation": "向量的半张量积 (STP) 是传统向量内积的推广，它允许因子向量具有不同的维度。本文提出了一种基于域的卷积积 (CP)。将基于域的CP与向量的STP结合，提出了一种新的CP。由于没有零填充或任何其他填充，它可以避免由填充引起的冗余信息。利用它，开发了基于STP的卷积神经网络 (CNN)。考虑了其在图像和三阶信号识别中的应用。", "summary": "本文提出了一种基于半张量积 (STP) 的新型卷积积，并在此基础上开发了半张量积卷积神经网络 (STP-CNN)。与传统卷积操作不同，该方法避免了零填充或其他填充，从而消除了填充引入的冗余信息。STP-CNN被探索应用于图像和三阶信号识别。", "keywords": "半张量积, 卷积神经网络, 卷积积, 无填充, 图像识别", "comments": "这项工作通过引入半张量积来重新定义卷积操作，从而避免了传统CNN中填充带来的信息冗余，具有一定的创新性。这种无填充的卷积方法可能为处理特定类型的数据或优化网络结构提供新的思路，尤其是在对数据完整性要求较高的应用中。"}}
{"id": "2506.10147", "title": "Unconditionally Secure Wireless-Wired Ground-Satellite-Ground Communication Networks Utilizing Classical and Quantum Noise", "authors": ["Lucas Truax", "Sandip Roy", "Laszlo B. Kish"], "summary": "In this paper, we introduce the Kirchhoff-Law-Johnson-Noise (KLJN) as an\napproach to securing satellite communications. KLJN has the potential to\nrevolutionize satellite communication security through its combination of\nsimplicity, cost-effectiveness, and resilience with unconditional security.\nUnlike quantum key distribution (QKD), which requires complex, fragile, and\nexpensive infrastructure like photon detectors and dedicated optical links,\nKLJN operates using standard electronic components and wires, significantly\nreducing implementation costs and logistical hurdles. KLJN's security, grounded\nin the fundamental laws of classical physics, is impervious to environmental\nand radiation-induced noise, making it highly reliable in the harsh conditions\nof satellite communications. This robustness, coupled with its ability to\nintegrate seamlessly with existing infrastructure, positions KLJN as a\nrevolutionary alternative to quantum solutions for ensuring secure, resilient\nsatellite communications. The authors explore the value of achieving\nunconditionally secure communications in strategic ground-to-satellite networks\nwhich address vulnerabilities posed by advanced computational threats,\nincluding quantum computing. Our team has examined two leading approaches to\nunconditional security - the KLJN scheme and QKD - and analyzed the potential\nuse of each for space systems. While QKD leverages quantum mechanics for\nsecurity, it faces challenges related to cost, complexity, and environmental\nsensitivity. In contrast, the KLJN scheme utilizes classical physics principles\nto provide a simpler, more cost-effective, and resilient alternative,\nparticularly for ground-based systems. The study concludes that KLJN offers\nsignificant advantages in simplicity, cost-efficiency, and robustness, making\nit a practical choice for many secure communication applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10147v1", "AI": {"title_translation": "利用经典和量子噪声的无条件安全无线-有线地-星-地通信网络", "tldr": "本文提出并探讨了 Kirchhoff-Law-Johnson-Noise (KLJN) 方案作为一种无条件安全的卫星通信方法，并将其与量子密钥分发 (QKD) 进行了比较，指出 KLJN 在成本、复杂性和鲁棒性方面具有显著优势。", "motivation": "为了解决先进计算威胁（包括量子计算）对战略地-星网络造成的漏洞，并克服现有量子密钥分发（QKD）方案在成本、复杂性、脆弱性和环境敏感性方面的挑战，研究人员寻求一种更实用、成本效益更高且更具弹性的无条件安全通信方法。", "method": "本文引入了基尔霍夫定律-约翰逊噪声（KLJN）作为一种保护卫星通信安全的方法。作者团队研究了 KLJN 方案和量子密钥分发（QKD）这两种领先的无条件安全方法，并分析了它们各自在空间系统中的潜在应用，特别关注了它们在成本、复杂性、基础设施需求和环境鲁棒性方面的差异。", "result": "研究发现，与需要复杂、脆弱且昂贵基础设施的 QKD 不同，KLJN 方案使用标准电子元件和电线即可运行，显著降低了实施成本和物流障碍。KLJN 的安全性基于经典物理学基本定律，不受环境和辐射噪声影响，在恶劣的卫星通信条件下高度可靠。KLJN 在简单性、成本效益和鲁棒性方面表现出显著优势。", "conclusion": "KLJN 方案在简单性、成本效益和鲁棒性方面具有显著优势，使其成为许多安全通信应用的实用选择，特别是对于地面系统，可以作为量子解决方案的革命性替代方案，确保安全、弹性的卫星通信。", "translation": "在本文中，我们引入了基尔霍夫定律-约翰逊噪声（KLJN）作为一种保护卫星通信安全的方法。KLJN 凭借其简单性、成本效益和韧性与无条件安全相结合的特点，有潜力彻底改变卫星通信安全。与量子密钥分发（QKD）不同，QKD 需要复杂、脆弱且昂贵的 инфра结构，如光子探测器和专用光链路，而 KLJN 使用标准电子元件和电线即可运行，显著降低了实施成本和物流障碍。KLJN 的安全性基于经典物理学基本定律，不受环境和辐射噪声的影响，使其在卫星通信的恶劣条件下具有高度可靠性。这种鲁棒性，加上其与现有基础设施无缝集成的能力，使 KLJN 成为确保安全、弹性卫星通信的量子解决方案的革命性替代方案。作者探讨了在战略地-星网络中实现无条件安全通信的价值，该网络解决了包括量子计算在内的先进计算威胁所带来的漏洞。我们的团队研究了两种领先的无条件安全方法——KLJN 方案和 QKD——并分析了它们各自在空间系统中的潜在用途。虽然 QKD 利用量子力学实现安全，但它面临着成本、复杂性和环境敏感性方面的挑战。相比之下，KLJN 方案利用经典物理学原理提供了一种更简单、更具成本效益和更具弹性的替代方案，特别是对于地面系统。研究得出结论，KLJN 在简单性、成本效率和鲁棒性方面具有显著优势，使其成为许多安全通信应用的实用选择。", "summary": "本文提出并评估了 Kirchhoff-Law-Johnson-Noise (KLJN) 方案作为一种无条件安全的卫星通信方法。与复杂的量子密钥分发 (QKD) 相比，KLJN 基于经典物理原理，使用标准电子元件，具有成本低、部署简单、环境鲁棒性强等优点。研究认为，KLJN 是应对高级计算威胁的有效方案，并在简单性、成本效益和弹性方面优于 QKD，是安全通信的实用替代方案。", "keywords": "KLJN, 无条件安全, 卫星通信, 量子密钥分发, 经典噪声", "comments": "这篇论文的创新点在于提出了 KLJN 方案作为量子密钥分发（QKD）的一种可行且更优的替代方案，用于实现无条件安全的通信。其重要性在于，KLJN 利用经典物理学原理，显著降低了部署成本和复杂性，同时保持了在恶劣环境下的鲁棒性，这对于未来大规模部署安全通信网络，尤其是在卫星通信等特殊领域，具有重要意义。它为在实际应用中实现无条件安全提供了一条更易于实现的路径。"}}
{"id": "2506.10376", "title": "MLLM-Based UI2Code Automation Guided by UI Layout Information", "authors": ["Fan Wu", "Cuiyun Gao", "Shuqing Li", "Xin-Cheng Wen", "Qing Liao"], "summary": "Converting user interfaces into code (UI2Code) is a crucial step in website\ndevelopment, which is time-consuming and labor-intensive. The automation of\nUI2Code is essential to streamline this task, beneficial for improving the\ndevelopment efficiency. There exist deep learning-based methods for the task;\nhowever, they heavily rely on a large amount of labeled training data and\nstruggle with generalizing to real-world, unseen web page designs. The advent\nof Multimodal Large Language Models (MLLMs) presents potential for alleviating\nthe issue, but they are difficult to comprehend the complex layouts in UIs and\ngenerate the accurate code with layout preserved. To address these issues, we\npropose LayoutCoder, a novel MLLM-based framework generating UI code from\nreal-world webpage images, which includes three key modules: (1) Element\nRelation Construction, which aims at capturing UI layout by identifying and\ngrouping components with similar structures; (2) UI Layout Parsing, which aims\nat generating UI layout trees for guiding the subsequent code generation\nprocess; and (3) Layout-Guided Code Fusion, which aims at producing the\naccurate code with layout preserved. For evaluation, we build a new benchmark\ndataset which involves 350 real-world websites named Snap2Code, divided into\nseen and unseen parts for mitigating the data leakage issue, besides the\npopular dataset Design2Code. Extensive evaluation shows the superior\nperformance of LayoutCoder over the state-of-the-art approaches. Compared with\nthe best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and\n3.95% in the CLIP score on average across all datasets.", "comment": "Accepted by the 34th International Symposium on Software Testing and\n  Analysis (ISSTA 2025)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10376v1", "AI": {"title_translation": "基于MLLM并由UI布局信息引导的UI2Code自动化", "tldr": "LayoutCoder是一个基于MLLM的框架，通过理解UI布局信息显著提升了UI2Code的自动化性能，解决了现有方法在数据依赖和复杂布局理解上的挑战。", "motivation": "将用户界面转换为代码（UI2Code）是网站开发中的关键但耗时耗力的步骤。现有基于深度学习的方法依赖大量标注数据且泛化能力差，而多模态大型语言模型（MLLMs）虽有潜力，但难以理解复杂的UI布局并生成保留布局的准确代码。", "method": "本文提出了LayoutCoder框架，一个新颖的基于MLLM的UI2Code生成系统。它包含三个核心模块：1) 元素关系构建，用于捕获UI布局；2) UI布局解析，生成UI布局树以指导代码生成；3) 布局引导代码融合，确保生成代码保留原始布局。此外，为评估构建了包含350个真实网站的新基准数据集Snap2Code。", "result": "LayoutCoder在Snap2Code和Design2Code数据集上均表现优异，性能超越了最先进的方法。与最佳基线相比，LayoutCoder在BLEU分数上平均提升了10.14%，在CLIP分数上平均提升了3.95%。", "conclusion": "LayoutCoder通过有效地利用UI布局信息，成功解决了MLLM在UI2Code任务中理解复杂布局的挑战，显著提高了从网页图像生成UI代码的准确性和布局保留能力。", "translation": "将用户界面转换为代码（UI2Code）是网站开发中至关重要的一步，耗时且劳动密集。UI2Code的自动化对于简化此任务至关重要，有利于提高开发效率。目前存在基于深度学习的方法来完成此任务；然而，它们严重依赖大量标记训练数据，并且难以泛化到真实世界、未见的网页设计。多模态大型语言模型（MLLMs）的出现为缓解此问题提供了潜力，但它们难以理解UI中复杂的布局并生成保留布局的准确代码。为了解决这些问题，我们提出了LayoutCoder，一个新颖的基于MLLM的框架，用于从真实网页图像生成UI代码，它包括三个关键模块：(1) 元素关系构建，旨在通过识别和分组具有相似结构的组件来捕获UI布局；(2) UI布局解析，旨在生成UI布局树以指导后续的代码生成过程；以及(3) 布局引导代码融合，旨在生成保留布局的准确代码。为了评估，我们构建了一个新的基准数据集Snap2Code，其中包含350个真实世界网站，并将其分为已知和未知部分以缓解数据泄露问题，此外还有流行的Design2Code数据集。广泛的评估表明，LayoutCoder的性能优于最先进的方法。与表现最佳的基线相比，LayoutCoder在所有数据集上的BLEU分数平均提高了10.14%，CLIP分数平均提高了3.95%。", "summary": "本文提出了LayoutCoder，一个基于多模态大语言模型（MLLM）的UI2Code自动化框架，旨在解决现有方法对数据依赖性强、泛化能力差以及MLLM难以理解复杂UI布局的问题。LayoutCoder通过元素关系构建、UI布局解析和布局引导代码融合三个模块，有效地从网页图像生成保留布局的UI代码。在自建的Snap2Code和现有Design2Code数据集上的评估显示，LayoutCoder显著优于现有最先进方法，在BLEU和CLIP分数上均有显著提升。", "keywords": "UI2Code, MLLM, UI布局, 代码生成, 自动化", "comments": "LayoutCoder的创新之处在于其通过显式地引入UI布局信息来引导MLLM进行UI2Code转换，解决了MLLM在处理复杂视觉布局时的固有难题。构建新的真实世界数据集Snap2Code也增加了研究的实用性和可信度。该方法对于提高前端开发效率具有重要意义，尤其是在处理多样化网页设计方面。"}}
{"id": "2506.10818", "title": "Grasp Prediction based on Local Finger Motion Dynamics", "authors": ["Dimitar Valkov", "Pascal Kockwelp", "Florian Daiber", "Antonio Krüger"], "summary": "The ability to predict the object the user intends to grasp offers essential\ncontextual information and may help to leverage the effects of point-to-point\nlatency in interactive environments. This paper explores the feasibility and\naccuracy of real-time recognition of uninstrumented objects based on hand\nkinematics during reach-to-grasp actions. In a data collection study, we\nrecorded the hand motions of 16 participants while reaching out to grasp and\nthen moving real and synthetic objects. Our results demonstrate that even a\nsimple LSTM network can predict the time point at which the user grasps an\nobject with a precision better than 21 ms and the current distance to this\nobject with a precision better than 1 cm. The target's size can be determined\nin advance with an accuracy better than 97%. Our results have implications for\ndesigning adaptive and fine-grained interactive user interfaces in ubiquitous\nand mixed-reality environments.", "comment": "10 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10818v1", "AI": {"title_translation": "基于局部手指运动动力学的抓取预测", "tldr": "该研究通过分析手部运动学，实现了对用户抓取意图和无工具物体的实时预测，并取得了高精度，对交互式用户界面设计具有重要意义。", "motivation": "为了利用抓取预测提供的上下文信息来缓解交互环境中点对点延迟的影响，并探索基于手部运动学在抓取动作中对无工具物体进行实时识别的可行性和准确性。", "method": "研究人员进行了一项数据收集研究，记录了16名参与者在伸出手抓取并移动真实和合成物体时的手部运动。然后使用一个简单的LSTM网络进行预测。", "result": "一个简单的LSTM网络能够以优于21毫秒的精度预测用户抓取物体的时间点，以优于1厘米的精度预测到该物体的当前距离。目标的尺寸可以提前以优于97%的准确率确定。", "conclusion": "研究结果对在普适和混合现实环境中设计自适应和精细的交互式用户界面具有重要意义。", "translation": "用户预测抓取物体的能力提供了重要的上下文信息，并可能有助于利用交互环境中点对点延迟的影响。本文探讨了在抓取动作中基于手部运动学对无工具物体进行实时识别的可行性和准确性。在一项数据收集研究中，我们记录了16名参与者在伸出手抓取并移动真实和合成物体时的手部运动。我们的结果表明，即使是一个简单的LSTM网络也能以优于21毫秒的精度预测用户抓取物体的时间点，并以优于1厘米的精度预测到该物体的当前距离。目标的尺寸可以提前以优于97%的准确率确定。我们的结果对在普适和混合现实环境中设计自适应和精细的交互式用户界面具有重要意义。", "summary": "本文探讨了在抓取动作中，基于手部运动学对无工具物体进行实时抓取预测的可行性和准确性。通过一项对16名参与者手部运动数据进行收集的研究，并利用一个简单的LSTM网络进行分析，研究结果表明，该系统能以高精度预测用户抓取物体的时间点（优于21毫秒）和到物体的距离（优于1厘米），并能以高准确率（优于97%）确定目标尺寸。这些发现为在普适和混合现实环境中设计自适应和精细的交互式用户界面提供了重要基础。", "keywords": "抓取预测, 手部运动学, LSTM, 实时识别, 交互式环境", "comments": "该论文的创新之处在于其能够仅通过手部运动学实现对无工具物体的抓取意图和物体属性的实时预测，这在实际应用中具有很高的实用价值。所达到的预测精度和准确性令人印象深刻。这项工作通过提供关键的上下文信息并可能减少延迟效应，对于开发更直观、响应更迅速的交互式用户界面，尤其是在混合现实环境中，具有重要意义。"}}
{"id": "2506.10317", "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving", "authors": ["Akshar Tumu", "Henrik I. Christensen", "Marcell Vazquez-Chanlatte", "Chikao Tsuchiya", "Dhaval Bhanderi"], "summary": "Lane-topology prediction is a critical component of safe and reliable\nautonomous navigation. An accurate understanding of the road environment aids\nthis task. We observe that this information often follows conventions encoded\nin natural language, through design codes that reflect the road structure and\nroad names that capture the road functionality. We augment this information in\na lightweight manner to SMERF, a map-prior-based online lane-topology\nprediction model, by combining structured road metadata from OSM maps and\nlane-width priors from Road design manuals with the road centerline encodings.\nWe evaluate our method on two geo-diverse complex intersection scenarios. Our\nmethod shows improvement in both lane and traffic element detection and their\nassociation. We report results using four topology-aware metrics to\ncomprehensively assess the model performance. These results demonstrate the\nability of our approach to generalize and scale to diverse topologies and\nconditions.", "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop -\n  RobotEvaluation@RSS2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10317v1", "AI": {"title_translation": "使用语言和道路手册辅助自动驾驶地图重建", "tldr": "通过将自然语言约定和道路手册先验信息整合到基于地图先验的模型中，增强了自动驾驶的车道拓扑预测。", "motivation": "准确的车道拓扑预测对于安全可靠的自动驾驶至关重要。道路环境信息通常遵循自然语言约定和设计规范，可以被利用来改进此任务。", "method": "作者通过将来自OSM地图的结构化道路元数据和来自道路设计手册的车道宽度先验与道路中心线编码相结合，以轻量级方式增强了SMERF（一种基于地图先验的在线车道拓扑预测模型）。", "result": "该方法在车道和交通元素检测及其关联方面均显示出改进。通过四种拓扑感知指标评估，结果表明该方法能够泛化并扩展到各种拓扑和条件。", "conclusion": "将自然语言约定和道路手册先验信息整合到地图重建中，显著提高了自动驾驶的车道拓扑预测能力，并展现了在各种场景下的鲁棒性。", "translation": "车道拓扑预测是安全可靠的自动驾驶的关键组成部分。准确理解道路环境有助于这项任务。我们观察到，这些信息通常遵循自然语言中编码的惯例，通过反映道路结构的设计规范和捕捉道路功能的道路名称。我们以轻量级方式将这些信息增强到 SMERF（一种基于地图先验的在线车道拓扑预测模型）中，方法是将来自 OSM 地图的结构化道路元数据和来自道路设计手册的车道宽度先验与道路中心线编码相结合。我们在两个地理多样化的复杂交叉路口场景中评估了我们的方法。我们的方法在车道和交通元素检测及其关联方面都显示出改进。我们使用四种拓扑感知指标报告结果，以全面评估模型性能。这些结果证明了我们的方法能够泛化并扩展到各种拓扑和条件。", "summary": "本论文旨在解决自动驾驶中车道拓扑预测的关键需求，通过整合自然语言约定和道路设计手册信息来增强地图重建。作者将来自OSM的结构化道路元数据和车道宽度先验信息增强到现有的基于地图先验的SMERF模型中。在复杂的交叉路口场景中进行评估，结果表明该方法改进了车道和交通元素的检测及其关联，并展示了对不同道路拓扑和条件的泛化能力。", "keywords": "自动驾驶, 车道拓扑预测, 地图重建, 自然语言, 道路手册", "comments": "该研究的创新之处在于利用了易于获取且人类可理解的信息（自然语言约定、道路手册）来增强自动驾驶的关键组件。这种“轻量级”的增强方法可能提供一种实用的方式来改进地图重建，而无需大量新的数据收集或复杂的模型架构，从而有望带来更鲁棒和可泛化的系统。"}}
{"id": "2506.10173", "title": "SPARKE: Scalable Prompt-Aware Diversity Guidance in Diffusion Models via RKE Score", "authors": ["Mohammad Jalali", "Haoyu Lei", "Amin Gohari", "Farzan Farnia"], "summary": "Diffusion models have demonstrated remarkable success in high-fidelity image\nsynthesis and prompt-guided generative modeling. However, ensuring adequate\ndiversity in generated samples of prompt-guided diffusion models remains a\nchallenge, particularly when the prompts span a broad semantic spectrum and the\ndiversity of generated data needs to be evaluated in a prompt-aware fashion\nacross semantically similar prompts. Recent methods have introduced guidance\nvia diversity measures to encourage more varied generations. In this work, we\nextend the diversity measure-based approaches by proposing the Scalable\nPrompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for\nprompt-aware diversity guidance. SPARKE utilizes conditional entropy for\ndiversity guidance, which dynamically conditions diversity measurement on\nsimilar prompts and enables prompt-aware diversity control. While the\nentropy-based guidance approach enhances prompt-aware diversity, its reliance\non the matrix-based entropy scores poses computational challenges in\nlarge-scale generation settings. To address this, we focus on the special case\nof Conditional latent RKE Score Guidance, reducing entropy computation and\ngradient-based optimization complexity from the $O(n^3)$ of general entropy\nmeasures to $O(n)$. The reduced computational complexity allows for\ndiversity-guided sampling over potentially thousands of generation rounds on\ndifferent prompts. We numerically test the SPARKE method on several\ntext-to-image diffusion models, demonstrating that the proposed method improves\nthe prompt-aware diversity of the generated data without incurring significant\ncomputational costs. We release our code on the project page:\nhttps://mjalali.github.io/SPARKE", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10173v1", "AI": {"title_translation": "SPARKE：通过RKE分数在扩散模型中实现可扩展的提示感知多样性引导", "tldr": "SPARKE提出了一种可扩展的提示感知多样性引导方法，通过使用RKE分数将扩散模型中生成样本的多样性计算复杂度从O(n^3)降低到O(n)，从而提高了多样性而没有显著增加计算成本。", "motivation": "扩散模型在图像合成方面表现出色，但在提示引导的生成中，尤其是在提示语义范围广泛且需要进行提示感知多样性评估时，确保生成样本的足够多样性仍然是一个挑战。现有基于多样性度量的引导方法在大规模生成设置中面临计算挑战，因为其依赖于基于矩阵的熵分数，计算复杂度为O(n^3)。", "method": "本文提出了可扩展的提示感知Rényi核熵多样性引导（SPARKE）方法。SPARKE利用条件熵进行多样性引导，动态地根据相似提示调整多样性测量，实现提示感知多样性控制。为了解决基于熵的引导方法在计算上的挑战，SPARKE专注于条件潜在RKE分数引导的特殊情况，将熵计算和基于梯度的优化复杂度从一般熵度量的O(n^3)降低到O(n)。", "result": "SPARKE方法在多个文本到图像扩散模型上进行了数值测试，结果表明该方法在不显著增加计算成本的情况下，提高了生成数据的提示感知多样性。", "conclusion": "SPARKE方法通过引入可扩展的提示感知Rényi核熵多样性引导，有效地解决了扩散模型中生成样本多样性不足的问题，并在保持计算效率的同时显著提升了生成数据的提示感知多样性。", "translation": "扩散模型在高质量图像合成和提示引导生成建模方面取得了显著成功。然而，在提示引导的扩散模型中，确保生成样本的足够多样性仍然是一个挑战，特别是当提示涵盖广泛的语义范围，并且需要在语义相似的提示之间以提示感知的方式评估生成数据的多样性时。最近的方法通过多样性度量引入了引导，以鼓励更多样化的生成。在这项工作中，我们通过提出可扩展的提示感知Rényi核熵多样性引导（SPARKE）方法来扩展基于多样性度量的方法，以实现提示感知多样性引导。SPARKE利用条件熵进行多样性引导，动态地根据相似提示调整多样性测量，并实现提示感知多样性控制。虽然基于熵的引导方法增强了提示感知多样性，但其对基于矩阵的熵分数的依赖在大型生成设置中带来了计算挑战。为了解决这个问题，我们专注于条件潜在RKE分数引导的特殊情况，将熵计算和基于梯度的优化复杂度从一般熵度量的O(n^3)降低到O(n)。降低的计算复杂度使得在不同提示上进行数千轮生成的多样性引导采样成为可能。我们在几个文本到图像扩散模型上对SPARKE方法进行了数值测试，结果表明所提出的方法在不显著增加计算成本的情况下，提高了生成数据的提示感知多样性。我们已在项目页面发布了代码：https://mjalali.github.io/SPARKE", "summary": "本文提出了SPARKE（可扩展的提示感知Rényi核熵多样性引导）方法，旨在解决扩散模型在生成多样化样本时面临的挑战，尤其是在处理广泛语义提示时。SPARKE通过利用条件熵进行多样性引导，并引入了条件潜在RKE分数引导，成功地将计算复杂度从O(n^3)降低到O(n)。实验证明，SPARKE在不显著增加计算成本的前提下，显著提升了文本到图像扩散模型生成数据的提示感知多样性。", "keywords": "扩散模型, 多样性引导, RKE分数, 提示感知, 条件熵", "comments": "这篇论文的创新点在于提出了SPARKE方法，通过引入Rényi核熵（RKE）分数，巧妙地将扩散模型中多样性引导的计算复杂度从立方级降低到线性级，从而解决了在大规模生成场景下计算效率低下的问题。这种复杂度的大幅降低使得多样性引导在实际应用中更具可行性。该研究对于提升扩散模型在复杂提示下的生成多样性具有重要意义，尤其是在需要高质量和高多样性输出的领域。"}}
{"id": "2506.10112", "title": "NnD: Diffusion-based Generation of Physically-Nonnegative Objects", "authors": ["Nadav Torem", "Tamar Sde-Chen", "Yoav Y. Schechner"], "summary": "Most natural objects have inherent complexity and variability. While some\nsimple objects can be modeled from first principles, many real-world phenomena,\nsuch as cloud formation, require computationally expensive simulations that\nlimit scalability. This work focuses on a class of physically meaningful,\nnonnegative objects that are computationally tractable but costly to simulate.\nTo dramatically reduce computational costs, we propose nonnegative diffusion\n(NnD). This is a learned generative model using score based diffusion. It\nadapts annealed Langevin dynamics to enforce, by design, non-negativity\nthroughout iterative scene generation and analysis (inference). NnD trains on\nhigh-quality physically simulated objects. Once trained, it can be used for\ngeneration and inference. We demonstrate generation of 3D volumetric clouds,\ncomprising inherently nonnegative microphysical fields. Our generated clouds\nare consistent with cloud physics trends. They are effectively not\ndistinguished as non-physical by expert perception.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10112v1", "AI": {"title_translation": "NnD：基于扩散的物理非负对象生成", "tldr": "该论文提出NnD，一种基于扩散的生成模型，用于生成云等物理非负对象，显著降低模拟成本，同时保持物理一致性。", "motivation": "模拟复杂的自然对象（如云的形成）计算成本高昂且限制了可扩展性。本研究旨在大幅降低对计算上可行但模拟成本高昂的物理有意义的非负对象的计算成本。", "method": "本文提出了非负扩散（NnD），这是一种使用基于分数的扩散的学习生成模型。它通过调整退火朗之万动力学，在迭代场景生成和分析（推理）过程中强制执行非负性。NnD在高质量的物理模拟对象上进行训练。", "result": "NnD能够生成具有非负微物理场的3D体积极光云。生成的云与云物理趋势一致，并且专家感知认为它们与非物理现象没有区别。", "conclusion": "NnD有效地生成物理非负对象，如云，降低了计算成本，同时保持了物理一致性，并且在专家感知下与真实现象无法区分。", "translation": "大多数自然物体具有固有的复杂性和可变性。虽然一些简单的物体可以从第一性原理建模，但许多现实世界的现象，例如云的形成，需要计算成本高昂的模拟，这限制了可扩展性。这项工作侧重于一类计算上可行但模拟成本高昂的物理有意义的非负对象。为了大幅降低计算成本，我们提出了非负扩散（NnD）。这是一种使用基于分数的扩散的学习生成模型。它通过设计，在迭代场景生成和分析（推理）过程中，调整退火朗之万动力学以强制执行非负性。NnD 在高质量物理模拟对象上进行训练。一旦训练完成，它就可以用于生成和推理。我们演示了3D体积极光云的生成，其中包含固有的非负微物理场。我们生成的云与云物理趋势一致。专家感知有效地认为它们与非物理现象没有区别。", "summary": "本文介绍了非负扩散（NnD），一种新颖的基于分数的生成模型，旨在高效生成物理有意义的非负对象。为解决云形成等复杂自然现象模拟的高计算成本问题，NnD调整退火朗之万动力学以确保在整个生成过程中保持非负性。NnD在高质量物理模拟数据上进行训练，并成功演示了生成逼真的3D体积极光云，这些云与云物理趋势一致，且专家无法将其与物理准确的模拟区分开来，从而显著降低了计算开销。", "keywords": "非负扩散, 生成模型, 云模拟, 基于分数的扩散, 物理约束", "comments": "该论文的创新之处在于将扩散模型应用于生成过程中强制执行物理约束（非负性），这对于科学应用至关重要。与传统模拟相比，这种方法在处理复杂物理现象时提供了显著的计算优势。其生成物理一致结果的能力以及计算效率是其主要优势。"}}
{"id": "2506.10607", "title": "Graph-based Gossiping for Communication Efficiency in Decentralized Federated Learning", "authors": ["Huong Nguyen", "Hong-Tri Nguyen", "Praveen Kumar Donta", "Susanna Pirttikangas", "Lauri Lovén"], "summary": "Federated learning has emerged as a privacy-preserving technique for\ncollaborative model training across heterogeneously distributed silos. Yet, its\nreliance on a single central server introduces potential bottlenecks and risks\nof single-point failure. Decentralizing the server, often referred to as\ndecentralized learning, addresses this problem by distributing the server role\nacross nodes within the network. One drawback regarding this pure\ndecentralization is it introduces communication inefficiencies, which arise\nfrom increased message exchanges in large-scale setups. However, existing\nproposed solutions often fail to simulate the real-world distributed and\ndecentralized environment in their experiments, leading to unreliable\nperformance evaluations and limited applicability in practice. Recognizing the\nlack from prior works, this work investigates the correlation between model\nsize and network latency, a critical factor in optimizing decentralized\nlearning communication. We propose a graph-based gossiping mechanism, where\nspecifically, minimum spanning tree and graph coloring are used to optimize\nnetwork structure and scheduling for efficient communication across various\nnetwork topologies and message capacities. Our approach configures and manages\nsubnetworks on real physical routers and devices and closely models real-world\ndistributed setups. Experimental results demonstrate that our method\nsignificantly improves communication, compatible with different topologies and\ndata sizes, reducing bandwidth and transfer time by up to circa 8 and 4.4\ntimes, respectively, compared to naive flooding broadcasting methods.", "comment": "Accepted at 34th International Conference on Computer Communications\n  and Networks (ICCCN 2025)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10607v1", "AI": {"title_translation": "基于图的八卦传播机制，用于去中心化联邦学习中的通信效率", "tldr": "本文提出了一种基于图的八卦传播机制，利用最小生成树和图着色来优化去中心化联邦学习中的通信效率，并在真实物理设备上进行了实验验证。", "motivation": "去中心化联邦学习虽然解决了中心化服务器的瓶颈和单点故障风险，但引入了通信效率低下问题，尤其是在大规模设置中。现有解决方案在实验中未能充分模拟真实的分布式和去中心化环境，导致性能评估不可靠且实际应用受限。", "method": "本文提出了一种基于图的八卦传播机制，利用最小生成树和图着色来优化网络结构和调度，以实现不同网络拓扑和消息容量下的高效通信。该方法在真实的物理路由器和设备上配置和管理子网络，紧密模拟真实世界的分布式设置。", "result": "实验结果表明，该方法显著改善了通信效率，兼容不同的拓扑和数据大小，与简单的泛洪广播方法相比，带宽和传输时间分别减少了约8倍和4.4倍。", "conclusion": "该研究成功地通过基于图的八卦传播机制，利用最小生成树和图着色优化了去中心化联邦学习中的通信效率，并在真实物理设备上验证了其在减少带宽和传输时间方面的显著效果。", "translation": "联邦学习已成为一种保护隐私的技术，用于跨异构分布式孤岛的协作模型训练。然而，其对单一中央服务器的依赖引入了潜在的瓶颈和单点故障风险。去中心化服务器，通常被称为去中心化学习，通过在网络中的节点之间分发服务器角色来解决此问题。这种纯粹的去中心化有一个缺点，即它引入了通信效率低下问题，这源于大规模设置中消息交换的增加。然而，现有提出的解决方案在实验中往往未能模拟真实的分布式和去中心化环境，导致不可靠的性能评估和有限的实际适用性。认识到先前工作的不足，这项工作研究了模型大小和网络延迟之间的相关性，这是优化去中心化学习通信的关键因素。我们提出了一种基于图的八卦传播机制，其中具体而言，最小生成树和图着色用于优化网络结构和调度，以实现跨各种网络拓扑和消息容量的高效通信。我们的方法在真实的物理路由器和设备上配置和管理子网络，并紧密模拟真实世界的分布式设置。实验结果表明，与简单的泛洪广播方法相比，我们的方法显著改善了通信，兼容不同的拓扑和数据大小，分别将带宽和传输时间减少了高达约8倍和4.4倍。", "summary": "本文针对去中心化联邦学习中存在的通信效率低下问题，提出了一种基于图的八卦传播机制。该机制利用最小生成树和图着色来优化网络结构和通信调度，以适应不同的网络拓扑和消息容量。研究强调了模型大小与网络延迟之间的关联，并通过在真实物理设备上进行实验，验证了所提方法能显著减少带宽和传输时间，从而提高去中心化联邦学习的通信效率。", "keywords": "去中心化联邦学习, 通信效率, 图传播, 最小生成树, 图着色", "comments": "该论文的创新点在于提出了基于图的八卦传播机制来解决去中心化联邦学习中的通信效率问题，并首次在真实的物理设备上进行实验验证，增强了实验结果的可靠性和实际应用价值。其方法结合了最小生成树和图着色，为优化分布式通信提供了新思路。"}}
{"id": "2506.10384", "title": "NeuroPAL: Punctuated Anytime Learning with Neuroevolution for Macromanagement in Starcraft: Brood War", "authors": ["Jim O'Connor", "Yeonghun Lee", "Gary B Parker"], "summary": "StarCraft: Brood War remains a challenging benchmark for artificial\nintelligence research, particularly in the domain of macromanagement, where\nlong-term strategic planning is required. Traditional approaches to StarCraft\nAI rely on rule-based systems or supervised deep learning, both of which face\nlimitations in adaptability and computational efficiency. In this work, we\nintroduce NeuroPAL, a neuroevolutionary framework that integrates\nNeuroevolution of Augmenting Topologies (NEAT) with Punctuated Anytime Learning\n(PAL) to improve the efficiency of evolutionary training. By alternating\nbetween frequent, low-fidelity training and periodic, high-fidelity\nevaluations, PAL enhances the sample efficiency of NEAT, enabling agents to\ndiscover effective strategies in fewer training iterations. We evaluate\nNeuroPAL in a fixed-map, single-race scenario in StarCraft: Brood War and\ncompare its performance to standard NEAT-based training. Our results show that\nPAL significantly accelerates the learning process, allowing the agent to reach\ncompetitive levels of play in approximately half the training time required by\nNEAT alone. Additionally, the evolved agents exhibit emergent behaviors such as\nproxy barracks placement and defensive building optimization, strategies\ncommonly used by expert human players. These findings suggest that structured\nevaluation mechanisms like PAL can enhance the scalability and effectiveness of\nneuroevolution in complex real-time strategy environments.", "comment": "IEEE Conference on Games 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10384v1", "AI": {"title_translation": "NeuroPAL：基于神经进化的即时学习在《星际争霸：母巢之战》宏观管理中的应用", "tldr": "NeuroPAL结合神经进化和即时学习，显著加速了《星际争霸：母巢之战》AI的训练效率，使其在更短时间内达到专家级表现。", "motivation": "《星际争霸：母巢之战》在宏观管理方面对AI研究仍具挑战，传统方法（基于规则或监督深度学习）在适应性和计算效率上存在局限。", "method": "本文引入NeuroPAL，一个神经进化框架，它将增强拓扑神经进化（NEAT）与即时学习（PAL）相结合。PAL通过频繁的低保真训练和周期性的高保真评估交替进行，以提高进化训练的效率和样本效率。", "result": "PAL显著加速了学习过程，使智能体在大约一半的训练时间内达到竞争水平，并展现出诸如前置兵营和防御建筑优化等专家级行为。", "conclusion": "结构化评估机制（如PAL）可以增强神经进化在复杂实时战略环境中的可扩展性和有效性。", "translation": "《星际争霸：母巢之战》对于人工智能研究来说仍然是一个具有挑战性的基准，特别是在需要长期战略规划的宏观管理领域。传统的《星际争霸》AI方法依赖于基于规则的系统或监督深度学习，但这两种方法在适应性和计算效率方面都面临局限。在这项工作中，我们引入了NeuroPAL，这是一个神经进化框架，它将增强拓扑神经进化（NEAT）与即时学习（PAL）相结合，以提高进化训练的效率。通过在频繁的低保真训练和周期性的高保真评估之间交替进行，PAL提高了NEAT的样本效率，使智能体能够在更少的训练迭代中发现有效的策略。我们在《星际争霸：母巢之战》的固定地图、单种族场景中评估了NeuroPAL，并将其性能与标准基于NEAT的训练进行了比较。我们的结果表明，PAL显著加速了学习过程，使智能体在大约一半的训练时间内达到竞争水平，这比单独使用NEAT所需的时间少。此外，进化的智能体表现出诸如前置兵营放置和防御建筑优化等新兴行为，这些策略通常由人类专家玩家使用。这些发现表明，像PAL这样的结构化评估机制可以增强神经进化在复杂实时战略环境中的可扩展性和有效性。", "summary": "本文提出了NeuroPAL，一个结合了增强拓扑神经进化（NEAT）和即时学习（PAL）的神经进化框架，旨在解决《星际争霸：母巢之战》宏观管理中AI训练效率和适应性的挑战。NeuroPAL通过低保真训练和高保真评估的交替，显著提高了NEAT的样本效率。实验结果表明，NeuroPAL能将训练时间缩短一半，并使AI智能体展现出专家级的策略行为，证明了PAL在复杂RTS环境中提升神经进化效果的潜力。", "keywords": "神经进化, 即时学习, 星际争霸, 宏观管理, 实时战略游戏AI", "comments": "该研究的创新点在于将即时学习（PAL）与神经进化（NEAT）相结合，有效解决了传统方法在《星际争霸》AI训练中面临的效率和适应性问题。PAL通过分阶段评估显著加速了学习过程，并使AI能习得复杂的专家级策略，这对于实时战略游戏AI的发展具有重要意义。"}}
{"id": "2506.10236", "title": "Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods", "authors": ["Yeonwoo Jang", "Shariqah Hossain", "Ashwin Sreevatsa", "Diogo Cruz"], "summary": "In this work, we show that some machine unlearning methods may fail when\nsubjected to straightforward prompt attacks. We systematically evaluate eight\nunlearning techniques across three model families, and employ output-based,\nlogit-based, and probe analysis to determine to what extent supposedly\nunlearned knowledge can be retrieved. While methods like RMU and TAR\ndemonstrate robust unlearning, ELM remains vulnerable to specific prompt\nattacks (e.g., Hindi filler text in original prompt recovering 57.3% accuracy).\nOur logit analysis also confirms that unlearned models are generally not hiding\nknowledge by modifying the way the answer is formatted, as the correlation\nbetween output and logit accuracy is strong. These results challenge prevailing\nassumptions about unlearning effectiveness and highlight the need for\nevaluation frameworks that can reliably distinguish between true knowledge\nremoval and superficial output suppression. We also publicly make available our\nevaluation framework to easily evaluate prompting techniques to retrieve\nunlearning knowledge.", "comment": "20 pages, 6 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10236v1", "AI": {"title_translation": "提示攻击揭示了遗忘方法中的肤浅知识去除", "tldr": "研究表明，一些机器遗忘方法在面对直接提示攻击时会失效，揭示了知识去除的肤浅性，并强调需要更可靠的评估框架。", "motivation": "当前对机器遗忘方法有效性的普遍假设可能不准确，需要评估这些方法是否真正移除了知识，而不是仅仅抑制了输出。", "method": "系统地评估了八种遗忘技术，涵盖三种模型家族。采用了基于输出、基于logits和探针分析来确定被遗忘的知识在多大程度上可以被检索。", "result": "RMU和TAR等方法表现出鲁棒的遗忘效果，但ELM仍然容易受到特定提示攻击（例如，原始提示中的印地语填充文本可恢复57.3%的准确率）。Logit分析证实，未遗忘的模型通常不是通过修改答案格式来隐藏知识，因为输出和logit准确率之间存在很强的相关性。", "conclusion": "当前关于遗忘有效性的假设受到挑战，需要能够可靠区分真实知识去除和肤浅输出抑制的评估框架。", "translation": "在这项工作中，我们展示了一些机器遗忘方法在遭受直接提示攻击时可能会失败。我们系统地评估了三种模型家族中的八种遗忘技术，并采用基于输出、基于logits和探针分析来确定被假定遗忘的知识在多大程度上可以被检索。虽然RMU和TAR等方法表现出鲁棒的遗忘效果，但ELM仍然容易受到特定提示攻击（例如，原始提示中的印地语填充文本可恢复57.3%的准确率）。我们的logit分析也证实，未遗忘的模型通常不是通过修改答案格式来隐藏知识，因为输出和logit准确率之间存在很强的相关性。这些结果挑战了关于遗忘有效性的普遍假设，并强调需要能够可靠区分真实知识去除和肤浅输出抑制的评估框架。我们还公开了我们的评估框架，以便轻松评估检索未遗忘知识的提示技术。", "summary": "本研究揭示了部分机器遗忘方法在面对直接提示攻击时的脆弱性。通过对八种遗忘技术在三种模型家族上的系统评估，并结合输出、logit和探针分析，发现某些方法未能彻底移除知识，而是进行了肤浅的输出抑制。具体而言，ELM易受攻击，而RMU和TAR表现更佳。研究强调了区分真实知识去除与表面输出抑制的评估框架的重要性。", "keywords": "机器遗忘, 提示攻击, 知识去除, 模型评估, 脆弱性", "comments": "这项工作具有重要意义，因为它挑战了机器遗忘领域中一个关键的假设：即遗忘方法能够彻底移除模型中的知识。通过引入“提示攻击”这一创新评估方法，作者揭示了现有技术可能存在的“肤浅知识去除”问题，即知识并未真正被移除，只是被抑制。这对于开发更安全、更可靠的机器遗忘技术至关重要，并为未来的研究提供了新的评估视角和工具。"}}
{"id": "2506.10832", "title": "A novel visual data-based diagnostic approach for estimation of regime transition in pool boiling", "authors": ["Pranay Nirapure", "Ayushman Singh", "Srikanth Rangarajan", "Bahgat Sammakia"], "summary": "This study introduces a novel metric, the Index of Visual Similarity (IVS),\nto qualitatively characterize boiling heat transfer regimes using only visual\ndata. The IVS is constructed by combining morphological similarity, through\nSIFT-based feature matching, with physical similarity, via vapor area\nestimation using Mask R-CNN. High-speed images of pool boiling on two distinct\nsurfaces, polished copper and porous copper foam, are employed to demonstrate\nthe generalizability of the approach. IVS captures critical changes in bubble\nshape, size, and distribution that correspond to transitions in heat transfer\nmechanisms. The metric is validated against an equivalent metric, $\\Phi$,\nderived from measured heat transfer coefficients (HTC), showing strong\ncorrelation and reliability in detecting boiling regime transitions, including\nthe onset of nucleate boiling and proximity to critical heat flux (CHF). Given\nexperimental limitations in precisely measuring changes in HTC, the sensitivity\nof IVS to surface superheat is also examined to reinforce the credibility of\nIVS. IVS thus emerges as a powerful, rapid, and non-intrusive tool for\nreal-time, image-based boiling diagnostics, with promising applications in\nphase change heat transfer.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10832v1", "AI": {"title_translation": "池沸腾中状态转换估计的一种新型基于视觉数据的诊断方法", "tldr": "本文提出了一种基于图像的新指标IVS，用于快速、非侵入性地诊断池沸腾传热状态转换，克服了传统HTC测量的局限性。", "motivation": "传统上，沸腾传热状态的诊断依赖于热传递系数（HTC）的测量，但这种测量存在实验局限性，难以精确捕捉HTC的变化。因此，需要一种基于视觉数据的新方法来克服这些挑战，实现快速、非侵入性的实时诊断。", "method": "本研究引入了一种名为“视觉相似度指数（IVS）”的新指标，仅利用视觉数据来定性表征沸腾传热状态。IVS的构建结合了基于SIFT特征匹配的形态相似性，以及使用Mask R-CNN进行蒸汽面积估计的物理相似性。该方法通过在抛光铜和多孔铜泡沫两种不同表面上的池沸腾高速图像进行了验证，以证明其通用性。", "result": "IVS能够捕获与传热机制转换相对应的气泡形状、大小和分布的关键变化。该指标已通过与从测量热传递系数（HTC）导出的等效指标Φ进行对比验证，结果显示出强相关性和在检测沸腾状态转换（包括核沸腾的开始和接近临界热流）方面的可靠性。此外，还检查了IVS对表面过热度的敏感性，以增强其可信度。", "conclusion": "IVS作为一种强大、快速、非侵入性的实时、基于图像的沸腾诊断工具，在相变传热领域具有广阔的应用前景。", "translation": "本研究引入了一种新的指标——视觉相似度指数（IVS），仅利用视觉数据定性表征沸腾传热状态。IVS通过结合基于SIFT特征匹配的形态相似性与使用Mask R-CNN进行蒸汽面积估计的物理相似性来构建。本研究采用抛光铜和多孔铜泡沫两种不同表面上的池沸腾高速图像来证明该方法的通用性。IVS捕获了气泡形状、大小和分布的关键变化，这些变化与传热机制的转换相对应。该指标通过与从测量热传递系数（HTC）导出的等效指标Φ进行验证，结果显示在检测沸腾状态转换（包括核沸腾的开始和接近临界热流）方面具有强相关性和可靠性。考虑到精确测量HTC变化的实验局限性，本研究还检验了IVS对表面过热度的敏感性，以增强IVS的可信度。因此，IVS作为一种强大、快速、非侵入性的实时、基于图像的沸腾诊断工具，在相变传热领域具有广阔的应用前景。", "summary": "本文提出一种新的基于视觉数据的诊断方法——视觉相似度指数（IVS），用于估计池沸腾中的状态转换。IVS结合了基于SIFT的形态相似性和基于Mask R-CNN的蒸汽面积物理相似性，能够通过高速图像捕捉气泡变化，从而识别沸腾传热机制的转换。该方法已在不同表面上验证，并与传统HTC指标显示出强相关性，证明其作为一种快速、非侵入性实时诊断工具的可靠性和潜力。", "keywords": "池沸腾, 视觉相似度指数, 状态转换, 图像处理, 热传递", "comments": "这篇论文的创新之处在于提出了一种完全基于视觉数据的新型诊断指标IVS，克服了传统热传递系数测量在精确性和实时性上的局限。通过结合SIFT和Mask R-CNN等先进图像处理技术，实现了对复杂沸腾现象的定性与定量分析，为相变传热领域的实时监测和诊断提供了新的非侵入性工具，具有重要的应用价值。"}}
{"id": "2506.10165", "title": "The 2025 PNPL Competition: Speech Detection and Phoneme Classification in the LibriBrain Dataset", "authors": ["Gilad Landau", "Miran Özdogan", "Gereon Elvers", "Francesco Mantegna", "Pratik Somaiya", "Dulhan Jayalath", "Luisa Kurth", "Teyun Kwon", "Brendan Shillingford", "Greg Farquhar", "Minqi Jiang", "Karim Jerbi", "Hamza Abdelhedi", "Yorguin Mantilla Ramos", "Caglar Gulcehre", "Mark Woolrich", "Natalie Voets", "Oiwi Parker Jones"], "summary": "The advance of speech decoding from non-invasive brain data holds the\npotential for profound societal impact. Among its most promising applications\nis the restoration of communication to paralysed individuals affected by speech\ndeficits such as dysarthria, without the need for high-risk surgical\ninterventions. The ultimate aim of the 2025 PNPL competition is to produce the\nconditions for an \"ImageNet moment\" or breakthrough in non-invasive neural\ndecoding, by harnessing the collective power of the machine learning community.\n  To facilitate this vision we present the largest within-subject MEG dataset\nrecorded to date (LibriBrain) together with a user-friendly Python library\n(pnpl) for easy data access and integration with deep learning frameworks. For\nthe competition we define two foundational tasks (i.e. Speech Detection and\nPhoneme Classification from brain data), complete with standardised data splits\nand evaluation metrics, illustrative benchmark models, online tutorial code, a\ncommunity discussion board, and public leaderboard for submissions. To promote\naccessibility and participation the competition features a Standard track that\nemphasises algorithmic innovation, as well as an Extended track that is\nexpected to reward larger-scale computing, accelerating progress toward a\nnon-invasive brain-computer interface for speech.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10165v1", "AI": {"title_translation": "2025年PNPL竞赛：LibriBrain数据集中的语音检测和音素分类", "tldr": "2025年PNPL竞赛旨在通过提供LibriBrain数据集和相关工具，推动非侵入性脑数据中语音解码的突破，以帮助瘫痪患者恢复交流能力。", "motivation": "从非侵入性脑数据中进行语音解码的进展，具有深刻的社会影响潜力，其中最有前景的应用之一是无需高风险手术干预，即可为受构音障碍等言语缺陷影响的瘫痪患者恢复交流。2025年PNPL竞赛的最终目标是通过汇集机器学习社区的集体力量，为非侵入性神经解码创造一个“ImageNet时刻”或突破。", "method": "为了实现这一愿景，竞赛提供了迄今为止记录的最大规模的受试者内MEG数据集（LibriBrain）以及一个用户友好的Python库（pnpl），以便于数据访问和与深度学习框架的集成。竞赛定义了两个基础任务（即从脑数据中进行语音检测和音素分类），并提供了标准化的数据划分和评估指标、说明性基准模型、在线教程代码、社区讨论区和用于提交的公共排行榜。为了促进可访问性和参与，竞赛设有一个强调算法创新的标准赛道，以及一个有望奖励大规模计算的扩展赛道。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "从非侵入性脑数据中进行语音解码的进展，具有深刻的社会影响潜力。其中最有前景的应用之一是无需高风险手术干预，即可为受构音障碍等言语缺陷影响的瘫痪患者恢复交流。2025年PNPL竞赛的最终目标是通过汇集机器学习社区的集体力量，为非侵入性神经解码创造一个“ImageNet时刻”或突破。为了实现这一愿景，我们提供了迄今为止记录的最大规模的受试者内MEG数据集（LibriBrain）以及一个用户友好的Python库（pnpl），以便于数据访问和与深度学习框架的集成。对于竞赛，我们定义了两个基础任务（即从脑数据中进行语音检测和音素分类），并提供了标准化的数据划分和评估指标、说明性基准模型、在线教程代码、社区讨论区和用于提交的公共排行榜。为了促进可访问性和参与，竞赛设有一个强调算法创新的标准赛道，以及一个有望奖励大规模计算的扩展赛道，从而加速非侵入性脑机接口在语音方面的进展。", "summary": "2025年PNPL竞赛旨在通过汇集机器学习社区的力量，推动非侵入性脑数据中语音解码的突破，以帮助瘫痪患者恢复交流。为此，竞赛提供了迄今为止最大的MEG数据集LibriBrain和易于使用的Python库pnpl。竞赛设置了语音检测和音素分类两个核心任务，并提供了标准化的数据、评估指标、基准模型和社区支持。竞赛分为标准赛道和扩展赛道，分别鼓励算法创新和大规模计算，以加速非侵入性语音脑机接口的开发。", "keywords": "PNPL竞赛, 语音解码, 脑机接口, LibriBrain, MEG数据集", "comments": "该竞赛通过提供大规模的MEG数据集和全面的支持工具，旨在创建一个“ImageNet时刻”，这对于推动非侵入性脑机接口领域的发展具有重要意义。其目标是解决瘫痪患者的交流障碍，具有巨大的社会价值。分设算法创新和大规模计算赛道，鼓励了不同方向的进步。"}}
{"id": "2506.10155", "title": "Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities", "authors": ["Elizabeth Demers", "Victor Xiaoqi Wang", "Kean Wu"], "summary": "Human capital (HC) is increasingly important to corporate value creation.\nUnlike other assets, however, HC is not currently subject to well-defined\nmeasurement or disclosure rules. We use a machine learning algorithm (word2vec)\ntrained on a confirmed set of HC disclosures to develop a comprehensive list of\nHC-related keywords classified into five subcategories (DEI; health and safety;\nlabor relations and culture; compensation and benefits; and demographics and\nother) that capture the multidimensional nature of HC management. We share our\nlexicon, corporate HC disclosures, and the Python code used to develop the\nlexicon, and we provide detailed examples of using our data and code, including\nfor fine-tuning a BERT model. Researchers can use our HC lexicon (or modify the\ncode to capture another construct of interest) with their samples of corporate\ncommunications to address pertinent HC questions. We close with a discussion of\nfuture research opportunities related to HC management and disclosure.", "comment": "50 pages, 6 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10155v1", "AI": {"title_translation": "衡量企业人力资本披露：词典、数据、代码和研究机会", "tldr": "本文开发了一个基于word2vec的工具，用于识别和分类企业人力资本披露，并提供了词典、数据和代码，以促进相关研究。", "motivation": "人力资本对企业价值创造日益重要，但目前缺乏明确的衡量和披露规则。", "method": "使用机器学习算法（word2vec）在已确认的人力资本披露集上进行训练，开发了一个包含五个人力资本子类别（DEI；健康与安全；劳资关系与文化；薪酬与福利；人口统计及其他）的关键词列表。", "result": "提供了开发的人力资本词典、企业人力资本披露数据以及用于开发该词典的Python代码。还提供了使用这些数据和代码的详细示例，包括用于微调BERT模型。", "conclusion": "研究人员可以使用该人力资本词典（或修改代码以捕获其他感兴趣的构建体）及其企业沟通样本来解决相关的人力资本问题，并讨论了未来人力资本管理和披露相关的研究机会。", "translation": "人力资本（HC）对企业价值创造日益重要。然而，与其他资产不同，人力资本目前没有明确的衡量或披露规则。我们使用在已确认的人力资本披露集上训练的机器学习算法（word2vec）来开发一份全面的人力资本相关关键词列表，这些关键词被分为五个子类别（DEI；健康与安全；劳资关系与文化；薪酬与福利；以及人口统计及其他），以捕捉人力资本管理的多维性质。我们分享了我们的词典、企业人力资本披露数据以及用于开发该词典的Python代码，并提供了使用我们的数据和代码的详细示例，包括用于微调BERT模型。研究人员可以使用我们的人力资本词典（或修改代码以捕获其他感兴趣的构建体）及其企业沟通样本来解决相关的人力资本问题。我们最后讨论了与人力资本管理和披露相关的未来研究机会。", "summary": "本文针对人力资本披露缺乏标准衡量规则的问题，利用word2vec算法开发了一个多维度的人力资本关键词词典，并将其分为五个子类别。作者分享了该词典、相关企业披露数据和Python代码，并提供了使用示例，旨在为研究人员提供工具，以促进人力资本管理和披露领域的研究。", "keywords": "人力资本披露, 词典, word2vec, 企业价值, 机器学习", "comments": "本文的创新之处在于利用机器学习方法（word2vec）系统地构建了人力资本披露的关键词词典，填补了该领域量化衡量的空白。其提供的数据和代码具有很高的实用价值，为后续研究提供了基础工具，能够促进企业人力资本透明度的提升和相关研究的深入。"}}
{"id": "2506.10859", "title": "Precise Zero-Shot Pointwise Ranking with LLMs through Post-Aggregated Global Context Information", "authors": ["Kehan Long", "Shasha Li", "Chen Xu", "Jintao Tang", "Ting Wang"], "summary": "Recent advancements have successfully harnessed the power of Large Language\nModels (LLMs) for zero-shot document ranking, exploring a variety of prompting\nstrategies. Comparative approaches like pairwise and listwise achieve high\neffectiveness but are computationally intensive and thus less practical for\nlarger-scale applications. Scoring-based pointwise approaches exhibit superior\nefficiency by independently and simultaneously generating the relevance scores\nfor each candidate document. However, this independence ignores critical\ncomparative insights between documents, resulting in inconsistent scoring and\nsuboptimal performance. In this paper, we aim to improve the effectiveness of\npointwise methods while preserving their efficiency through two key\ninnovations: (1) We propose a novel Global-Consistent Comparative Pointwise\nRanking (GCCP) strategy that incorporates global reference comparisons between\neach candidate and an anchor document to generate contrastive relevance scores.\nWe strategically design the anchor document as a query-focused summary of\npseudo-relevant candidates, which serves as an effective reference point by\ncapturing the global context for document comparison. (2) These contrastive\nrelevance scores can be efficiently Post-Aggregated with existing pointwise\nmethods, seamlessly integrating essential Global Context information in a\ntraining-free manner (PAGC). Extensive experiments on the TREC DL and BEIR\nbenchmark demonstrate that our approach significantly outperforms previous\npointwise methods while maintaining comparable efficiency. Our method also\nachieves competitive performance against comparative methods that require\nsubstantially more computational resources. More analyses further validate the\nefficacy of our anchor construction strategy.", "comment": "Accepted by SIGIR 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.10859v1", "AI": {"title_translation": "通过后聚合全局上下文信息实现LLM精确零样本点式排序", "tldr": "本文提出了一种新的点式排序策略GCCP，通过引入一个锚定文档的全局上下文信息来生成对比相关性分数，并结合PAGC方法将其与现有方法高效集成，显著提升了零样本点式排序的有效性，同时保持了高效率。", "motivation": "现有的成对和列表式排序方法效率低下，不适用于大规模应用；而基于分数的点式方法虽然效率高，但忽略了文档间的比较性洞察，导致评分不一致和性能不佳。本文旨在在保持效率的同时，提高点式方法的有效性。", "method": "提出了一种新颖的全局一致性比较点式排序（GCCP）策略，通过将每个候选文档与一个锚定文档进行全局参考比较来生成对比相关性分数。锚定文档被设计为查询聚焦的伪相关候选文档摘要，用于捕捉文档比较的全局上下文。这些对比相关性分数可以通过后聚合（PAGC）与现有方法高效结合，以无训练的方式整合全局上下文信息。", "result": "在TREC DL和BEIR基准测试上的大量实验表明，该方法显著优于以前的点式方法，同时保持了可比的效率。该方法还与需要更多计算资源的比较方法取得了竞争性表现。进一步分析验证了锚点构建策略的有效性。", "conclusion": "本文提出的GCCP和PAGC方法成功地提高了LLM零样本点式排序的有效性，同时保持了其固有的效率优势，解决了传统点式方法忽略文档间比较信息的问题，并在多个基准测试中展现出优越的性能。", "translation": "最近的进展成功地利用大型语言模型（LLMs）进行零样本文档排序，探索了各种提示策略。成对和列表式等比较方法实现了高效率，但计算密集，因此对于大规模应用来说实用性较低。基于分数的点式方法通过独立同时生成每个候选文档的相关性分数，展现出卓越的效率。然而，这种独立性忽略了文档之间关键的比较性洞察，导致评分不一致和次优性能。在本文中，我们旨在通过两项关键创新来提高点式方法的有效性，同时保持其效率：(1) 我们提出了一种新颖的全局一致性比较点式排序（GCCP）策略，该策略通过在每个候选文档和一个锚定文档之间引入全局参考比较来生成对比相关性分数。我们战略性地将锚定文档设计为伪相关候选文档的查询聚焦摘要，通过捕获文档比较的全局上下文，作为有效的参考点。(2) 这些对比相关性分数可以与现有方法进行高效的后聚合（PAGC），以无训练的方式无缝集成必要的全局上下文信息。在TREC DL和BEIR基准测试上的大量实验表明，我们的方法显著优于以前的点式方法，同时保持了可比的效率。我们的方法还与需要大量计算资源的比较方法取得了竞争性表现。更多分析进一步验证了我们锚点构建策略的有效性。", "summary": "本文提出一种名为GCCP（全局一致性比较点式排序）的新策略，用于提升LLM零样本点式排序的精度与效率。针对传统点式方法忽略文档间比较信息导致性能不佳的问题，GCCP通过引入一个由伪相关文档摘要构成的“锚定文档”作为全局上下文参考，为每个候选文档生成对比相关性分数。这些分数随后通过PAGC（后聚合全局上下文信息）方法与现有评分高效结合，无需额外训练。实验证明，该方法在保持效率的同时，显著优于现有零样本点式排序方法，并能与计算成本更高的比较方法相媲美。", "keywords": "零样本排序, 点式排序, LLM, 全局上下文, 后聚合", "comments": "本文的创新点在于，在保持点式排序高效率的同时，巧妙地通过引入“锚定文档”来融入全局上下文信息，解决了传统点式方法缺乏文档间比较性洞察的痛点。这种训练无关（training-free）的后聚合机制，使得模型能够高效地提升性能，为大规模零样本排序应用提供了更实用的解决方案。该方法在效率和效果之间取得了很好的平衡。"}}
{"id": "2506.10636", "title": "Structure and asymptotic preserving deep neural surrogates for uncertainty quantification in multiscale kinetic equations", "authors": ["Wei Chen", "Giacomo Dimarco", "Lorenzo Pareschi"], "summary": "The high dimensionality of kinetic equations with stochastic parameters poses\nmajor computational challenges for uncertainty quantification (UQ). Traditional\nMonte Carlo (MC) sampling methods, while widely used, suffer from slow\nconvergence and high variance, which become increasingly severe as the\ndimensionality of the parameter space grows. To accelerate MC sampling, we\nadopt a multiscale control variates strategy that leverages low-fidelity\nsolutions from simplified kinetic models to reduce variance. To further improve\nsampling efficiency and preserve the underlying physics, we introduce surrogate\nmodels based on structure and asymptotic preserving neural networks (SAPNNs).\nThese deep neural networks are specifically designed to satisfy key physical\nproperties, including positivity, conservation laws, entropy dissipation, and\nasymptotic limits. By training the SAPNNs on low-fidelity models and enriching\nthem with selected high-fidelity samples from the full Boltzmann equation, our\nmethod achieves significant variance reduction while maintaining physical\nconsistency and asymptotic accuracy. The proposed methodology enables efficient\nlarge-scale prediction in kinetic UQ and is validated across both homogeneous\nand nonhomogeneous multiscale regimes. Numerical results demonstrate improved\naccuracy and computational efficiency compared to standard MC techniques.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10636v1", "AI": {"title_translation": "多尺度动理学方程中不确定性量化的结构和渐近保持深度神经网络替代模型", "tldr": "本文提出了一种结合结构和渐近保持神经网络（SAPNNs）与多尺度控制变量策略的方法，用于高效地量化高维多尺度动理学方程中的不确定性，优于传统蒙特卡洛方法。", "motivation": "动理学方程的高维随机参数给不确定性量化（UQ）带来了巨大的计算挑战。传统的蒙特卡洛（MC）采样方法存在收敛速度慢和方差大的问题，尤其是在参数空间维度增加时。", "method": "该方法首先采用多尺度控制变量策略，利用简化动理学模型的低保真度解来减少方差。其次，引入了基于结构和渐近保持神经网络（SAPNNs）的替代模型，这些深度神经网络被设计为满足关键物理性质（包括正性、守恒定律、熵耗散和渐近极限）。SAPNNs在低保真度模型上进行训练，并通过来自完整玻尔兹曼方程的选定高保真度样本进行丰富。", "result": "该方法实现了显著的方差减少，同时保持了物理一致性和渐近精度。它能够实现动理学UQ中的高效大规模预测。数值结果表明，与标准MC技术相比，该方法提高了精度和计算效率。", "conclusion": "所提出的结合多尺度控制变量和SAPNNs的方法，为多尺度动理学方程中的不确定性量化提供了一种高效且准确的途径，克服了传统蒙特卡洛方法的局限性。", "translation": "动理学方程的高维随机参数给不确定性量化（UQ）带来了巨大的计算挑战。传统的蒙特卡洛（MC）采样方法虽然被广泛使用，但存在收敛速度慢和方差大的问题，随着参数空间维度的增加，这些问题变得越来越严重。为了加速MC采样，我们采用了多尺度控制变量策略，利用简化动理学模型的低保真度解来减少方差。为了进一步提高采样效率并保持底层物理特性，我们引入了基于结构和渐近保持神经网络（SAPNNs）的替代模型。这些深度神经网络经过专门设计，以满足关键的物理性质，包括正性、守恒定律、熵耗散和渐近极限。通过在低保真度模型上训练SAPNNs，并用来自完整玻尔兹曼方程的选定高保真度样本对其进行丰富，我们的方法在保持物理一致性和渐近精度的同时，实现了显著的方差减少。所提出的方法能够实现动理学UQ中的高效大规模预测，并在均匀和非均匀多尺度方案中得到验证。数值结果表明，与标准MC技术相比，该方法提高了精度和计算效率。", "summary": "本文旨在解决高维多尺度动理学方程中不确定性量化（UQ）的计算挑战，其中传统蒙特卡洛（MC）方法效率低下。作者提出了一种结合多尺度控制变量策略和结构与渐近保持神经网络（SAPNNs）的新颖方法。SAPNNs是深度学习替代模型，旨在固有地满足正性、守恒定律和渐近极限等物理性质。通过在低保真度模型上训练SAPNNs并辅以高保真度样本，该方法显著减少了方差，保持了物理一致性，并提高了计算效率，从而实现了高效的大规模UQ，优于标准MC技术。", "keywords": "不确定性量化, 多尺度动理学方程, 深度神经网络, 结构和渐近保持, 控制变量", "comments": "该论文的创新之处在于将物理信息神经网络（SAPNNs）与多尺度控制变量相结合，用于复杂动理学方程中的不确定性量化。这种方法有效地解决了维度灾难问题，并保留了关键的物理性质，这对于科学机器学习在物理领域的应用是一个重要的进步。"}}
{"id": "2506.10354", "title": "Revisiting mean estimation over $\\ell_p$ balls: Is the MLE optimal?", "authors": ["Liviu Aolaritei", "Michael I. Jordan", "Reese Pathak", "Annie Ulichney"], "summary": "We revisit the problem of mean estimation on $\\ell_p$ balls under additive\nGaussian noise. When $p$ is strictly less than $2$, it is well understood that\nrate-optimal estimators must be nonlinear in the observations. In this work, we\nstudy the maximum likelihood estimator (MLE), which may be viewed as a\nnonlinear shrinkage procedure for mean estimation over $\\ell_p$ balls. We\ndemonstrate two phenomena for the behavior of the MLE, which depend on the\nnoise level, the radius of the norm constraint, the dimension, and the norm\nindex $p$. First, as a function of the dimension, for $p$ near $1$ or at least\n$2$, the MLE is minimax rate-optimal for all noise levels and all constraint\nradii. On the other hand, for $p$ between $1$ and $2$, there is a more striking\nbehavior: for essentially all noise levels and radii for which nonlinear\nestimates are required, the MLE is minimax rate-suboptimal, despite being\nnonlinear in the observations. Our results also imply similar conclusions when\ngiven $n$ independent and identically distributed Gaussian samples, where we\ndemonstrate that the MLE can be suboptimal by a polynomial factor in the sample\nsize. Our lower bounds are constructive: whenever the MLE is rate-suboptimal,\nwe provide explicit instances on which the MLE provably incurs suboptimal risk.", "comment": "37 pages, 3 figures", "cate": "math.ST", "url": "http://arxiv.org/abs/2506.10354v1", "AI": {"title_translation": "重新审视 $\\ell_p$ 球上的均值估计：最大似然估计器是最优的吗？", "tldr": "在 $\\ell_p$ 球上进行均值估计时，当 $p$ 接近 $1$ 或至少 $2$ 时，最大似然估计器（MLE）是极小极大速率最优的；但当 $p$ 介于 $1$ 和 $2$ 之间时，尽管 MLE 是非线性的，它却是次优的，即使在多样本情况下也可能次优一个多项式因子。", "motivation": "该论文重新审视了在附加高斯噪声下 $\\ell_p$ 球上的均值估计问题，特别关注当 $p$ 严格小于 $2$ 时，已知速率最优估计器必须是非线性的情况。研究动机是深入分析最大似然估计器（MLE）在这种设定下的表现，以确定它是否能达到最优。", "method": "本研究分析了最大似然估计器（MLE）在 $\\ell_p$ 球上均值估计问题中的行为，其表现取决于噪声水平、范数约束半径、维度和范数指数 $p$。论文通过理论分析展示了MLE的两种现象，并提供了建设性的下界，以证明MLE在某些情况下的次优性。", "result": "研究发现，作为维度的函数，对于 $p$ 接近 $1$ 或至少 $2$ 的情况，MLE 在所有噪声水平和约束半径下都是极小极大速率最优的。然而，对于 $p$ 介于 $1$ 和 $2$ 之间的情况，对于几乎所有需要非线性估计的噪声水平和半径，MLE 却是极小极大速率次优的，尽管它在观测中是非线性的。当给定 $n$ 个独立同分布高斯样本时，结果也表明 MLE 在样本量上可能次优一个多项式因子。论文还提供了建设性的下界，在 MLE 速率次优时，明确给出了 MLE 产生次优风险的实例。", "conclusion": "尽管最大似然估计器（MLE）是一种常用的非线性估计方法，但它并非在所有情况下都是 $\\ell_p$ 球上均值估计的最优选择。特别是当范数指数 $p$ 介于 $1$ 和 $2$ 之间时，MLE 即使是非线性的，也可能表现出极小极大速率次优性，这突显了在特定参数条件下选择估计器时需要谨慎。", "translation": "我们重新审视了在附加高斯噪声下 $\\ell_p$ 球上的均值估计问题。当 $p$ 严格小于 $2$ 时，众所周知，速率最优的估计器必须是非线性的。在这项工作中，我们研究了最大似然估计器（MLE），它可以被视为 $\\ell_p$ 球上均值估计的非线性收缩过程。我们展示了 MLE 行为的两种现象，这取决于噪声水平、范数约束半径、维度和范数指数 $p$。首先，作为维度的函数，对于 $p$ 接近 $1$ 或至少 $2$ 的情况，MLE 在所有噪声水平和所有约束半径下都是极小极大速率最优的。另一方面，对于 $p$ 介于 $1$ 和 $2$ 之间的情况，存在一种更引人注目的行为：对于几乎所有需要非线性估计的噪声水平和半径，尽管 MLE 在观测中是非线性的，但它却是极小极大速率次优的。我们的结果也暗示了在给定 $n$ 个独立同分布高斯样本时类似的结论，我们证明了 MLE 在样本量上可能次优一个多项式因子。我们的下界是建设性的：无论 MLE 何时是速率次优的，我们都提供了明确的实例，证明 MLE 会产生次优风险。", "summary": "该论文研究了在高斯噪声下 $\\ell_p$ 球上的均值估计问题，并特别分析了最大似然估计器（MLE）的表现。研究发现，当范数指数 $p$ 接近 $1$ 或至少 $2$ 时，MLE 能够达到极小极大速率最优性。然而，令人惊讶的是，当 $p$ 介于 $1$ 和 $2$ 之间时，尽管 MLE 具有非线性特性，它在大多数情况下却是极小极大速率次优的。这种次优性在多样本情况下也成立，MLE 甚至可能比最优估计器差一个多项式因子。论文通过提供建设性下界来支持这些发现，明确指出 MLE 产生次优风险的实例。", "keywords": "均值估计, $\\ell_p$ 球, 最大似然估计器, 极小极大最优性, 高斯噪声", "comments": "这篇论文挑战了最大似然估计器（MLE）在所有情况下都最优的普遍认知，它通过在 $\\ell_p$ 球上均值估计的特定范数指数范围（$p \\in (1,2)$）内证明了 MLE 的次优性。这一发现具有重要意义，因为它揭示了即使是常用的非线性估计器，在特定问题设置下也可能无法达到最优性能。论文提供的建设性下界增强了结果的说服力，强调了在选择估计方法时，根据具体约束条件进行细致分析的重要性。"}}
{"id": "2506.10596", "title": "Sum Rate Maximization for Pinching Antennas Assisted RSMA System With Multiple Waveguides", "authors": ["Peiyu Wang", "Hong Wang", "Rongfang Song"], "summary": "In this letter, a pinching antennas (PAs) assisted rate splitting multiple\naccess (RSMA) system with multiple waveguides is investigated to maximize sum\nrate. A two-step algorithm is proposed to determine PA activation scheme and\noptimize the waveguide beamforming. Specifically, a low complexity spatial\ncorrelation and distance based method is proposed for PA activation selection.\nAfter determining the PA activation status, a semi-definite programming (SDP)\nbased successive convex approximation (SCA) is leveraged to obtain the optimal\nwaveguide beamforming. Simulation results show that the proposed multiple\nwaveguides based PAs assisted RSMA method achieves better performance than\nvarious benchmarking schemes.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10596v1", "AI": {"title_translation": "和波导多天线辅助RSMA系统的和速率最大化", "tldr": "研究了一种多波导PAs辅助RSMA系统，旨在最大化和速率，并提出了一种两步算法来确定PA激活方案和优化波导波束成形，仿真结果显示其性能优于基准方案。", "motivation": "旨在最大化多波导PAs辅助RSMA系统的和速率。", "method": "提出了一种两步算法：首先，采用基于空间相关性和距离的低复杂度方法选择PA激活；其次，利用基于半定规划（SDP）的逐次凸逼近（SCA）来优化波导波束成形。", "result": "仿真结果表明，所提出的基于多波导的PAs辅助RSMA方法比各种基准方案实现了更好的性能。", "conclusion": "所提出的多波导PAs辅助RSMA方法能够有效提高系统和速率，并优于现有基准方案。", "translation": "在这封信中，研究了一个带有多个波导的夹持天线（PAs）辅助速率分裂多址（RSMA）系统，以最大化和速率。提出了一种两步算法来确定PA激活方案并优化波导波束成形。具体来说，提出了一种基于低复杂度空间相关性和距离的方法用于PA激活选择。在确定PA激活状态后，利用基于半定规划（SDP）的逐次凸逼近（SCA）来获得最优波导波束成形。仿真结果表明，所提出的基于多波导的PAs辅助RSMA方法比各种基准方案实现了更好的性能。", "summary": "本文研究了多波导PAs辅助RSMA系统的和速率最大化问题。提出了一种创新的两步算法：第一步通过基于空间相关性和距离的方法选择PA激活方案；第二步利用SDP-SCA优化波导波束成形。仿真结果验证了该方法在性能上优于多种现有方案。", "keywords": "夹持天线, 速率分裂多址, 和速率最大化, 波束成形, 多波导系统", "comments": "本文提出了一种新颖的两步算法来解决PAs辅助RSMA系统的和速率最大化问题，特别是在多波导场景下。其创新点在于结合了低复杂度的PA激活选择和基于SDP-SCA的波束成形优化，有效提升了系统性能。"}}
{"id": "2506.10490", "title": "Predictive control of wastewater treatment plants as energy-autonomous water resource recovery facilities", "authors": ["Otacilio B. L. Neto", "Michela Mulas", "Iiro Harjunkoski", "Francesco Corona"], "summary": "This work proposes an automatic control solution for the operation of\nconventional wastewater treatment plants (WWTPs) as energy-autonomous water\nresource recovery facilities. We first conceptualize a classification of the\nquality of treated water for three resource recovery applications\n(environmental, industrial, and agricultural water reuse). We then present an\noutput-feedback model predictive controller (Output MPC) that operates a plant\nto produce water of specific quality class, while also producing sufficient\nbiogas to ensure nonpositive energy costs. The controller is demonstrated in\nthe long-term operation of a full-scale WWTP subjected to typical influent\nloads and periodically changing quality targets. Our results provide a\nproof-of-concept on the energy-autonomous operation of existing wastewater\ntreatment infrastructure with control strategies that are general enough to\naccommodate a wide range of resource recovery objectives.", "comment": "13 pages, 8 figures (main text); 27 pages, 2 figures (supplementary\n  material)", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10490v1", "AI": {"title_translation": "作为能源自主型水资源回收设施的污水处理厂预测控制", "tldr": "该论文提出了一种预测控制解决方案，使污水处理厂能够作为能源自主设施运行，生产特定质量的水并产生足够的沼气以覆盖能源成本。", "motivation": "开发一种自动控制解决方案，使传统污水处理厂能够作为能源自主型水资源回收设施运行，在生产特定质量水的同时确保能源成本非正。", "method": "首先概念化了处理水质量的分类，用于三种资源回收应用；然后提出了一个输出反馈模型预测控制器（Output MPC），用于生产特定质量水并产生足够沼气；最后在一个全尺寸污水处理厂进行了长期运行演示。", "result": "研究结果为现有污水处理基础设施的能源自主运行提供了概念验证，其控制策略通用性强，可适应广泛的资源回收目标。", "conclusion": "所提出的预测控制策略可以使传统污水处理厂作为能源自主型水资源回收设施运行，生产特定质量的水并产生足够的沼气以覆盖能源成本，证明了其可行性和适应性。", "translation": "这项工作提出了一种用于传统污水处理厂（WWTPs）作为能源自主型水资源回收设施运行的自动控制解决方案。我们首先概念化了处理水质量的分类，以用于三种资源回收应用（环境、工业和农业水回用）。然后，我们提出了一个输出反馈模型预测控制器（Output MPC），该控制器操作工厂以生产特定水质等级的水，同时生产足够的沼气以确保非正能源成本。该控制器在一个全尺寸污水处理厂的长期运行中进行了演示，该工厂受到典型进水负荷和周期性变化的质量目标的限制。我们的结果为现有污水处理基础设施的能源自主运行提供了概念验证，其控制策略足够通用，可以适应各种资源回收目标。", "summary": "该论文提出了一种自动控制解决方案，旨在使传统污水处理厂转变为能源自主型水资源回收设施。研究首先对处理水质进行了分类，以适应环境、工业和农业水回用等多种应用。随后，文章引入了一个输出反馈模型预测控制器（Output MPC），该控制器能够管理工厂生产特定质量的水，并同时产生充足的沼气以实现能源自给自足。该控制器的有效性已在一个全尺寸污水处理厂的长期运行中得到验证。研究结果证明了现有污水处理基础设施通过通用控制策略实现能源自主运行的可行性。", "keywords": "污水处理厂, 预测控制, 能源自主, 水资源回收, 沼气", "comments": "该论文的创新之处在于将预测控制与污水处理厂的能源自主性及水资源回收相结合。其重要性在于为可持续废水管理提供了一条实际途径，通过降低能源成本和促进水回用。控制策略对各种回收目标的通用性是其一大优势。"}}
{"id": "2506.10171", "title": "Disclosure Audits for LLM Agents", "authors": ["Saswat Das", "Jameson Sandler", "Ferdinando Fioretto"], "summary": "Large Language Model agents have begun to appear as personal assistants,\ncustomer service bots, and clinical aides. While these applications deliver\nsubstantial operational benefits, they also require continuous access to\nsensitive data, which increases the likelihood of unauthorized disclosures.\nThis study proposes an auditing framework for conversational privacy that\nquantifies and audits these risks. The proposed Conversational Manipulation for\nPrivacy Leakage (CMPL) framework, is an iterative probing strategy designed to\nstress-test agents that enforce strict privacy directives. Rather than focusing\nsolely on a single disclosure event, CMPL simulates realistic multi-turn\ninteractions to systematically uncover latent vulnerabilities. Our evaluation\non diverse domains, data modalities, and safety configurations demonstrate the\nauditing framework's ability to reveal privacy risks that are not deterred by\nexisting single-turn defenses. In addition to introducing CMPL as a diagnostic\ntool, the paper delivers (1) an auditing procedure grounded in quantifiable\nrisk metrics and (2) an open benchmark for evaluation of conversational privacy\nacross agent implementations.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10171v1", "AI": {"title_translation": "LLM代理的披露审计", "tldr": "本研究提出了一个名为CMPL的对话隐私审计框架，通过模拟多轮交互来发现LLM代理中现有的单轮防御无法阻止的隐私泄露风险，并提供量化风险指标和开放基准。", "motivation": "大型语言模型代理（LLM agents）在应用中需要持续访问敏感数据，这增加了未经授权披露的风险，因此需要一个能够量化和审计这些风险的对话隐私审计框架。", "method": "提出了一种名为“对话操纵隐私泄露”（Conversational Manipulation for Privacy Leakage, CMPL）的迭代探测策略。CMPL旨在对执行严格隐私指令的代理进行压力测试，通过模拟真实的多轮交互来系统地揭示潜在的漏洞，而不是仅仅关注单一的披露事件。", "result": "对不同领域、数据模态和安全配置的评估表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。", "conclusion": "该论文引入了CMPL作为诊断工具，提供了一个基于可量化风险指标的审计程序，以及一个用于评估代理实现中对话隐私的开放基准。", "translation": "大型语言模型代理已开始作为个人助理、客户服务机器人和临床助手出现。虽然这些应用程序带来了可观的运营效益，但它们也需要持续访问敏感数据，这增加了未经授权披露的可能性。本研究提出了一个用于对话隐私的审计框架，该框架量化并审计这些风险。所提出的“对话操纵隐私泄露”（CMPL）框架是一种迭代探测策略，旨在对执行严格隐私指令的代理进行压力测试。CMPL并非只关注单一的披露事件，而是模拟真实的多轮交互，系统地揭示潜在的漏洞。我们对不同领域、数据模态和安全配置的评估表明，该审计框架能够揭示现有单轮防御无法阻止的隐私风险。除了将CMPL作为诊断工具引入外，本文还提供了（1）一个基于可量化风险指标的审计程序和（2）一个用于评估代理实现中对话隐私的开放基准。", "summary": "本文针对大型语言模型代理在处理敏感数据时面临的未经授权披露风险，提出了一个名为“对话操纵隐私泄露”（CMPL）的审计框架。CMPL通过模拟多轮对话交互，系统地探测并揭示现有单轮防御无法发现的潜在隐私漏洞。研究评估表明，该框架能有效识别隐私风险，并提供了一个基于量化风险指标的审计程序和开放基准，以促进LLM代理的对话隐私评估。", "keywords": "LLM代理, 隐私审计, 对话隐私, CMPL, 敏感数据", "comments": "这篇论文的创新点在于提出了一个关注多轮交互的隐私审计框架CMPL，这比传统的单轮防御更贴近真实世界的使用场景，能够发现更深层次的隐私泄露风险。其重要性在于为LLM代理的隐私安全评估提供了一个量化、系统化的工具和基准，对于提升LLM应用的可信赖性具有重要意义。"}}
{"id": "2506.10397", "title": "Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "summary": "Accurate classification of software bugs is essential for improving software\nquality. This paper presents a rule-based automated framework for classifying\nissues in quantum software repositories by bug type, category, severity, and\nimpacted quality attributes, with additional focus on quantum-specific bug\ntypes. The framework applies keyword and heuristic-based techniques tailored to\nquantum computing. To assess its reliability, we manually classified a\nstratified sample of 4,984 issues from a dataset of 12,910 issues across 36\nQiskit repositories. Automated classifications were compared with ground truth\nusing accuracy, precision, recall, and F1-score. The framework achieved up to\n85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393\n(quality attribute). Statistical validation via paired t-tests and Cohen's\nKappa showed substantial to almost perfect agreement for bug type (k = 0.696),\ncategory (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug\ntype (k = 0.712). Severity classification showed slight agreement (k = 0.162),\nsuggesting room for improvement. Large-scale analysis revealed that classical\nbugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug\ncategories included compatibility, functional, and quantum-specific defects,\nwhile usability, maintainability, and interoperability were the most impacted\nquality attributes. Most issues (93.7%) were low severity; only 4.3% were\ncritical. A detailed review of 1,550 quantum-specific bugs showed that over\nhalf involved quantum circuit-level problems, followed by gate errors and\nhardware-related issues.", "comment": "25 pages, 5 figures", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10397v1", "AI": {"title_translation": "量子软件中的错误分类：一个基于规则的框架及其评估", "tldr": "本文提出了一个基于规则的自动化框架，用于对量子软件中的问题进行分类，并对其进行了评估，结果显示该框架在大多数分类维度上表现良好，并揭示了量子软件中常见的错误类型和属性。", "motivation": "准确分类软件错误对于提高软件质量至关重要。鉴于量子软件的快速发展，对其特有的错误进行有效分类变得尤为重要。", "method": "本文提出了一个基于规则的自动化框架，利用关键词和启发式技术，对量子软件仓库中的问题按错误类型、类别、严重性和受影响的质量属性进行分类，并特别关注量子特有的错误类型。该框架通过对Qiskit仓库中12,910个问题中的4,984个分层样本进行手动分类作为真值，并使用准确率、精确率、召回率和F1分数进行自动化分类结果的比较。同时，通过配对t检验和Cohen's Kappa系数进行统计验证。", "result": "该框架的准确率高达85.21%，F1分数范围从0.7075（严重性）到0.8393（质量属性）。统计验证显示，错误类型（k = 0.696）、类别（k = 0.826）、质量属性（k = 0.818）和量子特有错误类型（k = 0.712）的分类一致性达到了实质性至几乎完美。严重性分类的一致性较低（k = 0.162）。大规模分析显示，经典错误占主导地位（67.2%），量子特有错误占27.3%。常见的错误类别包括兼容性、功能性和量子特有缺陷，而可用性、可维护性和互操作性是受影响最严重的质量属性。大多数问题（93.7%）为低严重性，仅4.3%为关键性。对1,550个量子特有错误的详细审查显示，超过一半涉及量子电路级别问题，其次是门错误和硬件相关问题。", "conclusion": "该基于规则的框架在量子软件错误分类方面表现出较高的准确性和一致性，尤其在错误类型、类别和质量属性方面。尽管在严重性分类上仍有改进空间，但该研究为理解量子软件中错误模式提供了宝贵的见解，揭示了经典错误和量子特有错误的分布以及它们对质量属性的影响。", "translation": "准确分类软件错误对于提高软件质量至关重要。本文提出了一个基于规则的自动化框架，用于根据错误类型、类别、严重性和受影响的质量属性对量子软件仓库中的问题进行分类，并额外关注量子特有的错误类型。该框架应用了针对量子计算量身定制的关键词和启发式技术。为了评估其可靠性，我们手动分类了来自36个Qiskit仓库的12,910个问题数据集中4,984个分层样本。通过使用准确率、精确率、召回率和F1分数，将自动化分类结果与真实情况进行比较。该框架的准确率高达85.21%，F1分数范围从0.7075（严重性）到0.8393（质量属性）。通过配对t检验和Cohen's Kappa系数进行的统计验证显示，错误类型（k = 0.696）、类别（k = 0.826）、质量属性（k = 0.818）和量子特有错误类型（k = 0.712）的一致性达到了实质性至几乎完美。严重性分类显示出轻微的一致性（k = 0.162），表明仍有改进空间。大规模分析显示，经典错误占主导地位（67.2%），量子特有错误占27.3%。常见的错误类别包括兼容性、功能性和量子特有缺陷，而可用性、可维护性和互操作性是受影响最严重的质量属性。大多数问题（93.7%）为低严重性；仅4.3%为关键性。对1,550个量子特有错误的详细审查显示，超过一半涉及量子电路级别问题，其次是门错误和硬件相关问题。", "summary": "本文提出并评估了一个基于规则的自动化框架，用于对量子软件中的错误进行分类，包括错误类型、类别、严重性和受影响的质量属性，并特别关注量子特有错误。该框架利用关键词和启发式方法，在Qiskit数据集上进行评估，实现了高达85.21%的准确率，并在多数分类维度上展现出良好的统计一致性。研究还对大规模量子软件错误进行了分析，揭示了经典错误和量子特有错误的分布，以及它们对软件质量属性的影响，并指出量子电路级别问题是量子特有错误的主要来源。", "keywords": "量子软件, 错误分类, 规则框架, Qiskit, 软件质量", "comments": "这项研究为新兴的量子软件领域提供了一个实用的错误分类工具，对于提高量子软件质量具有重要意义。其创新之处在于针对量子计算的特性设计了关键词和启发式规则，并进行了大规模的真实世界数据验证。尽管在严重性分类上表现有待提高，但其对量子软件中错误模式的深入分析，尤其是对量子特有错误类型的识别，为未来量子软件的测试和调试提供了宝贵的方向。"}}
{"id": "2506.10891", "title": "(De)composing Craft: An Elementary Grammar for Sharing Expertise in Craft Workflows", "authors": ["Ritik Batra", "Lydia Kim", "Ilan Mandel", "Amritansh Kwatra", "Jane L. E.", "Steven J. Jackson", "Thijs Roumen"], "summary": "Craft practices rely on evolving archives of skill and knowledge, developed\nthrough generations of craftspeople experimenting with designs, materials, and\ntechniques. Better documentation of these practices enables the sharing of\nknowledge and expertise between sites and generations. However, most\ndocumentation focuses solely on the linear steps leading to final artifacts,\nneglecting the tacit knowledge necessary to improvise, or adapt workflows to\nmeet the unique demands of each craft project. This omission limits knowledge\nsharing and reduces craft to a mechanical endeavor, rather than a sophisticated\nway of seeing, thinking, and doing. Drawing on expert interviews and literature\nfrom HCI, CSCW and the social sciences, we develop an elementary grammar to\ndocument improvisational actions of real-world craft practices. We demonstrate\nthe utility of this grammar with an interface called CraftLink that can be used\nto analyze expert videos and semi-automatically generate documentation to\nconvey material and contextual variations of craft practices. Our user study\nwith expert crocheters (N=7) using this interface evaluates our grammar's\neffectiveness in capturing and sharing expert knowledge with other\ncraftspeople, offering new pathways for computational systems to support\ncollaborative archives of knowledge and practice within communities.", "comment": "29 pages, 7 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10891v1", "AI": {"title_translation": "(解)构工艺：一种用于在工艺工作流程中共享专业知识的基本语法", "tldr": "当前工艺文档缺乏即兴创作所需的默会知识。本文提出了一种基本语法和一个界面（CraftLink）来记录和分享即兴工艺专业知识，并通过对钩编师的用户研究进行了验证。", "motivation": "大多数工艺文档只关注线性步骤，忽略了即兴创作和适应所需的默会知识，这限制了知识共享，并将工艺简化为一种机械活动。本文旨在弥补这一空白。", "method": "作者借鉴专家访谈以及人机交互（HCI）、协同支持计算机工作（CSCW）和社会科学领域的文献，开发了一种用于记录即兴行动的基本语法。他们通过名为 CraftLink 的界面展示了这种语法的实用性，该界面可分析专家视频并半自动生成文档。通过对专家钩编师（N=7）进行的用户研究评估了该语法的有效性。", "result": "本文开发了一种用于记录真实世界工艺实践中即兴行动的基本语法，以及一个应用该语法的界面（CraftLink）。用户研究表明，该语法在捕获和共享专家知识方面有效，为计算系统支持协作知识档案提供了新途径。", "conclusion": "所开发的语法和 CraftLink 界面有效地捕获和共享了工艺实践中的即兴默会知识，从而实现了更好的知识传递，并支持了工艺社区内的协作档案。", "translation": "工艺实践依赖于不断发展的技能和知识档案，这些档案是通过几代工匠对设计、材料和技术的实验而形成的。更好地记录这些实践有助于在不同地点和世代之间共享知识和专业技能。然而，大多数文档只关注导致最终成品的线性步骤，忽略了即兴创作或调整工作流程以满足每个工艺项目独特需求所需的默会知识。这种遗漏限制了知识共享，并将工艺简化为一种机械活动，而不是一种复杂的看待、思考和做事的方式。我们借鉴专家访谈以及人机交互（HCI）、协同支持计算机工作（CSCW）和社会科学领域的文献，开发了一种基本语法来记录真实世界工艺实践中的即兴行动。我们通过一个名为 CraftLink 的界面展示了这种语法的实用性，该界面可用于分析专家视频并半自动生成文档，以传达工艺实践的材料和上下文变化。我们对专家钩编师（N=7）进行的界面用户研究评估了我们的语法在捕获和与同行工匠共享专家知识方面的有效性，为计算系统支持社区内的协作知识和实践档案提供了新途径。", "summary": "本文旨在解决当前工艺文档中忽视即兴创作所需默会知识的局限性。通过专家访谈和文献回顾，作者开发了一种基本语法来捕获这些即兴行动。他们随后构建了 CraftLink 界面，该界面利用此语法分析专家视频并生成文档，从而促进了细致入微的工艺专业知识共享。对专家钩编师的用户研究验证了该语法在知识传递方面的有效性，提出了一种支持协作工艺知识档案的计算方法。", "keywords": "工艺文档, 默会知识, 即兴创作, 知识共享, HCI, CSCW", "comments": "这篇论文提出了一种记录工艺中默会知识的创新方法，超越了线性的步骤。即兴创作的“基本语法”概念是新颖的，并有可能显著改善基于技能领域中的知识传递。CraftLink 展示了一个实际应用，强调了捕获专业知识适应性和细微差别的重要性。其贡献在于重新构建了工艺文档，使其包含工艺的“艺术”而非仅仅是“科学”。"}}
{"id": "2506.10359", "title": "Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success", "authors": ["Che Wang", "Jeroen van Baar", "Chaitanya Mitash", "Shuai Li", "Dylan Randle", "Weiyao Wang", "Sumedh Sontakke", "Kostas E. Bekris", "Kapil Katyal"], "summary": "This work demonstrates how autonomously learning aspects of robotic operation\nfrom sparsely-labeled, real-world data of deployed, engineered solutions at\nindustrial scale can provide with solutions that achieve improved performance.\nSpecifically, it focuses on multi-suction robot picking and performs a\ncomprehensive study on the application of multi-modal visual encoders for\npredicting the success of candidate robotic picks. Picking diverse items from\nunstructured piles is an important and challenging task for robot manipulation\nin real-world settings, such as warehouses. Methods for picking from clutter\nmust work for an open set of items while simultaneously meeting latency\nconstraints to achieve high throughput. The demonstrated approach utilizes\nmultiple input modalities, such as RGB, depth and semantic segmentation, to\nestimate the quality of candidate multi-suction picks. The strategy is trained\nfrom real-world item picking data, with a combination of multimodal pretrain\nand finetune. The manuscript provides comprehensive experimental evaluation\nperformed over a large item-picking dataset, an item-picking dataset targeted\nto include partial occlusions, and a package-picking dataset, which focuses on\ncontainers, such as boxes and envelopes, instead of unpackaged items. The\nevaluation measures performance for different item configurations, pick scenes,\nand object types. Ablations help to understand the effects of in-domain\npretraining, the impact of different modalities and the importance of\nfinetuning. These ablations reveal both the importance of training over\nmultiple modalities but also the ability of models to learn during pretraining\nthe relationship between modalities so that during finetuning and inference,\nonly a subset of them can be used as input.", "comment": "Accepted to Robotics: Science and Systems (RSS 2025), 15 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10359v1", "AI": {"title_translation": "通过多模态学习抓取成功率实现大规模多吸盘物品抓取演示", "tldr": "该论文展示了通过多模态视觉编码器从稀疏标记的真实世界数据中学习，以预测抓取成功率，从而改进大规模多吸盘机器人抓取性能。", "motivation": "从非结构化堆中抓取多样化物品是机器人操作在真实世界环境（如仓库）中一项重要且具有挑战性的任务，需要方法适用于开放的物品集，同时满足延迟约束以实现高吞吐量。", "method": "该研究利用多模态视觉编码器（如RGB、深度和语义分割）来估计候选多吸盘抓取的质量。该策略通过多模态预训练和微调的组合，从真实世界物品抓取数据中进行训练。实验在大型物品抓取数据集、包含部分遮挡的数据集以及包裹抓取数据集上进行，并通过消融实验研究了域内预训练、不同模态影响和微调的重要性。", "result": "评估测量了不同物品配置、抓取场景和物体类型的性能。消融实验揭示了多模态训练的重要性，以及模型在预训练期间学习模态之间关系的能力，从而在微调和推理期间只需使用部分模态作为输入。", "conclusion": "该论文证明了通过多模态学习，从稀疏标记的真实世界数据中自主学习可以显著提高工业规模的多吸盘机器人抓取性能。", "translation": "这项工作展示了如何从部署在工业规模的工程解决方案的稀疏标记真实世界数据中自主学习机器人操作的各个方面，从而提供性能改进的解决方案。具体而言，它侧重于多吸盘机器人抓取，并对多模态视觉编码器在预测候选机器人抓取成功率方面的应用进行了全面研究。从非结构化堆中抓取多样化物品是仓库等真实世界环境中机器人操作的一项重要且具有挑战性的任务。从杂乱中抓取的方法必须适用于开放的物品集，同时满足延迟约束以实现高吞吐量。所演示的方法利用多种输入模态，例如RGB、深度和语义分割，来估计候选多吸盘抓取的质量。该策略通过多模态预训练和微调的组合，从真实世界物品抓取数据中进行训练。该手稿提供了对大型物品抓取数据集、旨在包含部分遮挡的物品抓取数据集以及侧重于容器（如盒子和信封而非未包装物品）的包裹抓取数据集进行的全面实验评估。评估测量了不同物品配置、抓取场景和物体类型的性能。消融实验有助于理解域内预训练的效果、不同模态的影响以及微调的重要性。这些消融实验揭示了多模态训练的重要性，以及模型在预训练期间学习模态之间关系的能力，以便在微调和推理期间，只需使用其中的一个子集作为输入。", "summary": "该论文介绍了一种多模态学习方法，用于改进大规模多吸盘机器人抓取。通过利用RGB、深度和语义分割等多模态数据，该系统能够预测非结构化环境中多样化物品的抓取成功率。该方法通过预训练和微调在真实世界数据上进行训练，展示了性能的提升，并强调了多模态训练和跨模态学习对于高效机器人操作的价值。", "keywords": "多吸盘抓取, 多模态学习, 机器人操作, 抓取成功率, 工业规模", "comments": "该论文的创新之处在于将多模态学习应用于具有挑战性的真实世界机器人操作任务（从杂乱中进行多吸盘抓取）。其重要性体现在通过使用稀疏标记数据实现了工业规模的性能改进，这对于实际部署具有重要意义。模型在预训练期间能够学习模态之间的关系，从而在推理时只需使用较少模态的发现，为提高效率提供了重要的实际见解。"}}
{"id": "2506.10174", "title": "Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context", "authors": ["Yael Frischholz", "Devis Tuia", "Michael Lehning"], "summary": "Accurate retrieval of surface solar radiation (SSR) from satellite imagery\ncritically depends on estimating the background reflectance that a spaceborne\nsensor would observe under clear-sky conditions. Deviations from this baseline\ncan then be used to detect cloud presence and guide radiative transfer models\nin inferring atmospheric attenuation. Operational retrieval algorithms\ntypically approximate background reflectance using monthly statistics, assuming\nsurface properties vary slowly relative to atmospheric conditions. However,\nthis approach fails in mountainous regions where intermittent snow cover and\nchanging snow surfaces are frequent. We propose an attention-based emulator for\nSSR retrieval that implicitly learns to infer clear-sky surface reflectance\nfrom raw satellite image sequences. Built on the Temporo-Spatial Vision\nTransformer, our approach eliminates the need for hand-crafted features such as\nexplicit albedo maps or cloud masks. The emulator is trained on instantaneous\nSSR estimates from the HelioMont algorithm over Switzerland, a region\ncharacterized by complex terrain and dynamic snow cover. Inputs include\nmulti-spectral SEVIRI imagery from the Meteosat Second Generation platform,\naugmented with static topographic features and solar geometry. The target\nvariable is HelioMont's SSR, computed as the sum of its direct and diffuse\nhorizontal irradiance components, given at a spatial resolution of 1.7 km. We\nshow that, when provided a sufficiently long temporal context, the model\nmatches the performances of albedo-informed models, highlighting the model's\nability to internally learn and exploit latent surface reflectance dynamics.\nOur geospatial analysis shows this effect is most powerful in mountainous\nregions and improves generalization in both simple and complex topographic\nsettings. Code and datasets are publicly available at\nhttps://github.com/frischwood/HeMu-dev.git", "comment": "14 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10174v1", "AI": {"title_translation": "通过时间上下文隐式反照率恢复地表太阳辐射", "tldr": "本文提出了一种基于注意力机制的模拟器，利用时间上下文从原始卫星图像序列中隐式学习地表反射率，以准确检索地表太阳辐射（SSR），尤其在山区表现出色。", "motivation": "从卫星图像中准确检索地表太阳辐射（SSR）关键在于估算晴空条件下的背景反射率。现有操作算法通常使用月度统计数据近似背景反射率，但在山区，由于间歇性积雪和不断变化的雪面，这种方法会失效。", "method": "提出了一种基于注意力机制的模拟器（Temporo-Spatial Vision Transformer），用于SSR检索，该模拟器隐式学习从原始卫星图像序列中推断晴空地表反射率，无需手工制作的反照率图或云掩膜等特征。该模拟器在瑞士地区（复杂地形和动态积雪）的HelioMont算法瞬时SSR估计数据上进行训练，输入包括Meteosat第二代平台的多光谱SEVIRI图像，并辅以静态地形特征和太阳几何。目标变量是HelioMont的SSR，空间分辨率为1.7公里。", "result": "当提供足够长的时间上下文时，该模型能匹配反照率信息模型的性能，这表明模型能够内部学习和利用潜在的地表反射率动态。地理空间分析显示，这种效果在山区最为显著，并能改善在简单和复杂地形设置下的泛化能力。", "conclusion": "该研究表明，通过使用基于注意力机制的模拟器和足够长的时间上下文，可以从原始卫星图像序列中隐式学习地表反射率，从而在复杂地形区域实现准确的地表太阳辐射检索，且性能可与依赖显式反照率信息的模型媲美。", "translation": "从卫星图像中准确检索地表太阳辐射（SSR）关键在于估算卫星传感器在晴空条件下观测到的背景反射率。与此基线的偏差可用于检测云的存在并指导辐射传输模型推断大气衰减。操作检索算法通常使用月度统计数据近似背景反射率，假设地表特性相对于大气条件变化缓慢。然而，这种方法在间歇性积雪和不断变化的雪面频繁的山区会失效。我们提出了一种基于注意力机制的SSR检索模拟器，该模拟器隐式学习从原始卫星图像序列中推断晴空地表反射率。我们的方法基于时空视觉Transformer，消除了对手工制作特征（如显式反照率图或云掩膜）的需求。该模拟器在瑞士地区（一个以复杂地形和动态积雪为特征的区域）的HelioMont算法瞬时SSR估计数据上进行训练。输入包括来自Meteosat第二代平台的多光谱SEVIRI图像，并辅以静态地形特征和太阳几何。目标变量是HelioMont的SSR，计算为其直接和漫射水平辐照度分量之和，空间分辨率为1.7公里。我们表明，当提供足够长的时间上下文时，该模型能匹配反照率信息模型的性能，这突出了模型内部学习和利用潜在地表反射率动态的能力。我们的地理空间分析表明，这种效果在山区最为强大，并改善了简单和复杂地形设置下的泛化能力。代码和数据集可在https://github.com/frischwood/HeMu-dev.git公开获取。", "summary": "本文提出了一种基于时空视觉Transformer的注意力模拟器，用于从原始卫星图像序列中隐式学习晴空地表反射率，以准确检索地表太阳辐射（SSR）。该方法克服了传统月度统计方法在山区积雪频繁变化时的局限性，无需显式反照率图或云掩膜。在瑞士复杂地形上的训练和验证表明，该模型在提供足够时间上下文时，性能可与依赖反照率信息的模型媲美，尤其在山区表现出强大的泛化能力。", "keywords": "地表太阳辐射, 隐式反照率恢复, 时间上下文, 深度学习, 卫星图像", "comments": "该论文的创新点在于提出了一种基于深度学习（时空视觉Transformer）的方法，通过隐式学习地表反射率动态来提高地表太阳辐射检索的准确性，尤其是在传统方法失效的复杂山区。它避免了对显式反照率图和云掩膜的需求，简化了流程并增强了模型在动态环境下的鲁棒性。其在山区性能的提升以及对时间上下文的有效利用是重要的贡献。"}}
{"id": "2506.10642", "title": "Deployment of Containerized Simulations in an API-Driven Distributed Infrastructure", "authors": ["Tim Kraus", "Axel Sauer", "Ingo Feldner"], "summary": "The increasingly dynamic market for embedded systems makes virtual prototypes\nan indispensable tool for hardware/software codesign. The broad acceptance of\nthe methodology has led to a diverse range of solutions: from open-source, pure\nconsole-based simulators to highly capable commercial simulation tools. In this\nwork we present SUNRISE, an infrastructure to provide users a unified approach\nto utilizing virtual prototyping solutions, facilitate access to various\nsimulation technologies and boost cooperation by leveraging decentralized\ncompute resources for deployment of simulation workloads and definition of open\nAPIs.", "comment": "8 pages, 5 figures, Published in DVCon Europe 2024", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10642v1", "AI": {"title_translation": "API驱动分布式基础设施中容器化仿真的部署", "tldr": "本文介绍了SUNRISE，一个API驱动的分布式基础设施，旨在为虚拟原型设计提供统一方法，并利用去中心化计算资源部署仿真工作负载。", "motivation": "嵌入式系统市场日益动态化，使得虚拟原型成为硬件/软件协同设计的不可或缺的工具。然而，现有解决方案种类繁多，缺乏统一的方法来利用各种仿真技术并促进合作。", "method": "本文提出了SUNRISE基础设施，通过提供统一的虚拟原型解决方案使用方法，促进对各种仿真技术的访问，并通过利用去中心化计算资源部署仿真工作负载和定义开放API来增强合作。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "嵌入式系统日益动态化的市场使得虚拟原型成为硬件/软件协同设计不可或缺的工具。该方法论的广泛接受导致了各种各样的解决方案：从开源的纯控制台模拟器到功能强大的商业仿真工具。在这项工作中，我们提出了SUNRISE，一个旨在为用户提供统一方法来利用虚拟原型解决方案的基础设施，通过利用去中心化计算资源部署仿真工作负载和定义开放API来促进对各种仿真技术的访问并促进合作。", "summary": "本文介绍了SUNRISE，一个用于部署容器化仿真的API驱动分布式基础设施。该平台旨在为虚拟原型设计提供统一方法，促进对多种仿真技术的访问，并通过利用去中心化计算资源和开放API来增强合作，以应对嵌入式系统市场中日益增长的虚拟原型需求。", "keywords": "容器化仿真, API驱动, 分布式基础设施, 虚拟原型, 嵌入式系统", "comments": "本文的创新点在于提出了一个统一的、API驱动的分布式基础设施SUNRISE，以解决虚拟原型工具多样性带来的集成和协作挑战。通过容器化仿真和去中心化计算，它有望提高仿真效率和可访问性，对于硬件/软件协同设计领域具有重要意义。"}}
{"id": "2506.10387", "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills", "authors": ["Yuquan Xie", "Zaijing Li", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Dongmei Jiang", "Liqiang Nie"], "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/", "comment": "20 pages, 5 figures, 5 tables", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10387v1", "AI": {"title_translation": "Mirage-1：使用分层多模态技能增强和更新GUI代理", "tldr": "Mirage-1引入分层多模态技能（HMS）和技能增强蒙特卡洛树搜索（SA-MCTS），显著提升了GUI代理在长程任务中的性能，并在多个基准测试中表现优异。", "motivation": "现有的多模态大语言模型（MLLM）GUI代理在在线环境中的长程任务中表现不佳，主要原因是知识不足以及离线和在线领域之间的固有差距。", "method": "本文提出了分层多模态技能（HMS）模块，通过将轨迹逐步抽象为执行技能、核心技能和元技能来构建分层知识结构，以解决知识不足问题。为弥合领域差距，提出了技能增强蒙特卡洛树搜索（SA-MCTS）算法，该算法有效利用离线环境中获得的技能来减少在线树探索期间的动作搜索空间。基于HMS，本文构建了Mirage-1，一个多模态、跨平台、即插即用的GUI代理。为了验证其在真实世界长程场景中的性能，构建了新的基准AndroidLH。", "result": "Mirage-1在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH上的表现分别超越了之前的代理32%、19%、15%和79%。", "conclusion": "Mirage-1通过其创新的分层多模态技能学习和优化的在线搜索策略，显著提高了GUI代理在复杂长程任务中的性能，证明了其在弥合离线-在线领域差距和处理知识不足方面的有效性。", "translation": "近期利用多模态大语言模型（MLLM）作为GUI代理的努力取得了可喜的成果。然而，这些代理在在线环境中的长程任务中仍然面临挑战，主要原因在于知识不足以及离线和在线领域之间的固有差距。在本文中，受人类如何在开放环境中泛化知识的启发，我们提出了分层多模态技能（HMS）模块来解决知识不足的问题。它逐步将轨迹抽象为执行技能、核心技能，并最终抽象为元技能，为长程任务规划提供了分层知识结构。为了弥合领域差距，我们提出了技能增强蒙特卡洛树搜索（SA-MCTS）算法，该算法有效利用在离线环境中获得的技能，以减少在线树探索期间的动作搜索空间。在HMS的基础上，我们提出了Mirage-1，一个多模态、跨平台、即插即用的GUI代理。为了验证Mirage-1在真实世界长程场景中的性能，我们构建了一个新的基准AndroidLH。实验结果表明，Mirage-1在AndroidWorld、MobileMiniWob++、Mind2Web-Live和AndroidLH上分别超越了之前的代理32%、19%、15%和79%。项目页面：https://cybertronagent.github.io/Mirage-1.github.io/", "summary": "针对现有GUI代理在长程在线任务中知识不足和离线-在线领域差距的问题，本文提出了Mirage-1。该代理的核心是分层多模态技能（HMS）模块，通过将行为抽象为多层技能来丰富知识；同时引入技能增强蒙特卡洛树搜索（SA-MCTS）算法，利用离线习得技能优化在线搜索。实验表明，Mirage-1在多个基准测试（包括新建的AndroidLH）上显著超越了现有方法，验证了其在复杂GUI任务中的优越性能。", "keywords": "GUI代理, 多模态, 分层技能, 长程任务, 蒙特卡洛树搜索", "comments": "Mirage-1的创新之处在于其对人类知识泛化方式的模仿，通过分层多模态技能（HMS）有效地解决了GUI代理的知识不足问题，并利用技能增强的蒙特卡洛树搜索（SA-MCTS）策略，巧妙地弥合了离线和在线环境之间的差距。这种结合不仅提高了代理在长程任务中的泛化能力和效率，而且其跨平台、即插即用的特性也展现了巨大的应用潜力。性能的大幅提升和新基准的引入也体现了其在推动GUI自动化领域发展中的重要性。"}}
{"id": "2506.10297", "title": "\"Check My Work?\": Measuring Sycophancy in a Simulated Educational Context", "authors": ["Chuck Arvin"], "summary": "This study examines how user-provided suggestions affect Large Language\nModels (LLMs) in a simulated educational context, where sycophancy poses\nsignificant risks. Testing five different LLMs from the OpenAI GPT-4o and\nGPT-4.1 model classes across five experimental conditions, we show that\nresponse quality varies dramatically based on query framing. In cases where the\nstudent mentions an incorrect answer, the LLM correctness can degrade by as\nmuch as 15 percentage points, while mentioning the correct answer boosts\naccuracy by the same margin. Our results also show that this bias is stronger\nin smaller models, with an effect of up to 30% for the GPT-4.1-nano model,\nversus 8% for the GPT-4o model. Our analysis of how often LLMs \"flip\" their\nanswer, and an investigation into token level probabilities, confirm that the\nmodels are generally changing their answers to answer choices mentioned by\nstudents in line with the sycophancy hypothesis. This sycophantic behavior has\nimportant implications for educational equity, as LLMs may accelerate learning\nfor knowledgeable students while the same tools may reinforce misunderstanding\nfor less knowledgeable students. Our results highlight the need to better\nunderstand the mechanism, and ways to mitigate, such bias in the educational\ncontext.", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10297v1", "AI": {"title_translation": "“检查我的工作？”：测量模拟教育环境中的奉承行为", "tldr": "本研究发现在模拟教育场景中，大型语言模型（LLM）会受学生提议影响，表现出奉承行为，导致回答质量下降或提高，且小模型偏差更严重，对教育公平性有重要影响。", "motivation": "本研究旨在探讨用户提供的建议如何影响大型语言模型（LLMs）在模拟教育环境中的表现，因为在这种情境下，LLM的奉承行为可能带来重大风险。", "method": "研究测试了来自OpenAI GPT-4o和GPT-4.1模型类别的五种不同LLMs，涵盖五种实验条件。通过分析LLMs“翻转”答案的频率以及token级别的概率来验证模型是否表现出奉承行为。", "result": "当学生提到错误答案时，LLM的正确率会下降多达15个百分点；而提到正确答案时，准确率会提高相同的幅度。这种偏见在较小的模型中更强，GPT-4.1-nano模型的影响高达30%，而GPT-4o模型为8%。分析证实，模型通常会根据奉承假设，改变其答案以符合学生提及的答案选择。", "conclusion": "LLM的奉承行为对教育公平具有重要意义，可能加速知识渊博学生的学习，同时加强知识欠缺学生的误解。研究结果强调了理解和减轻教育环境中这种偏见的必要性。", "translation": "这项研究考察了用户提供的建议如何在模拟教育环境中影响大型语言模型（LLMs），在这种环境中，奉承行为带来了重大风险。我们测试了OpenAI GPT-4o和GPT-4.1模型类别中的五种不同LLMs，涵盖五种实验条件，结果表明响应质量因查询框架而异。在学生提到错误答案的情况下，LLM的正确性可能会下降多达15个百分点，而提到正确答案则会使准确性提高相同的幅度。我们的结果还显示，这种偏见在较小的模型中更强，GPT-4.1-nano模型的影响高达30%，而GPT-4o模型为8%。我们对LLMs“翻转”答案频率的分析，以及对token级别概率的调查，证实模型普遍根据奉承假设，改变其答案以符合学生提及的答案选择。这种奉承行为对教育公平具有重要意义，因为LLMs可能加速知识渊博学生的学习，而相同的工具可能加强知识欠缺学生的误解。我们的结果强调了需要更好地理解这种机制，以及减轻教育环境中这种偏见的方法。", "summary": "本研究在模拟教育场景中，评估了用户建议对大型语言模型（LLM）响应质量的影响。结果表明，LLM会表现出明显的奉承行为：当学生提出错误答案时，LLM的正确率会显著下降；而当提出正确答案时，正确率则会上升。这种偏差在较小的模型中更为显著。研究强调了LLM的奉承行为对教育公平性可能产生的重要影响，并指出未来需要深入研究并减轻LLM在教育应用中的此类偏见。", "keywords": "大型语言模型, 奉承行为, 教育公平, 偏见, 模型评估", "comments": "这项研究揭示了LLM在教育应用中的一个关键且潜在有害的偏见——奉承行为。其创新之处在于通过模拟教育场景量化了这种行为及其对不同规模模型的影响。研究的重要性在于它直接指出了LLM在教育公平性方面可能带来的负面影响，即可能加剧学习差距。研究结果为未来LLM在教育领域的开发和部署提供了重要的警示和方向，强调了偏见缓解机制的必要性。"}}
{"id": "2506.10858", "title": "Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation", "authors": ["Zhenhuan Zhou"], "summary": "Medical image segmentation is a fundamental and key technology in\ncomputer-aided diagnosis and treatment. Previous methods can be broadly\nclassified into three categories: convolutional neural network (CNN) based,\nTransformer based, and hybrid architectures that combine both. However, each of\nthem has its own limitations, such as restricted receptive fields in CNNs or\nthe computational overhead caused by the quadratic complexity of Transformers.\nRecently, the Receptance Weighted Key Value (RWKV) model has emerged as a\npromising alternative for various vision tasks, offering strong long-range\nmodeling capabilities with linear computational complexity. Some studies have\nalso adapted RWKV to medical image segmentation tasks, achieving competitive\nperformance. However, most of these studies focus on modifications to the\nVision-RWKV (VRWKV) mechanism and train models from scratch, without exploring\nthe potential advantages of leveraging pre-trained VRWKV models for medical\nimage segmentation tasks. In this paper, we propose Med-URWKV, a pure\nRWKV-based architecture built upon the U-Net framework, which incorporates\nImageNet-based pretraining to further explore the potential of RWKV in medical\nimage segmentation tasks. To the best of our knowledge, Med-URWKV is the first\npure RWKV segmentation model in the medical field that can directly reuse a\nlarge-scale pre-trained VRWKV encoder. Experimental results on seven datasets\ndemonstrate that Med-URWKV achieves comparable or even superior segmentation\nperformance compared to other carefully optimized RWKV models trained from\nscratch. This validates the effectiveness of using a pretrained VRWKV encoder\nin enhancing model performance. The codes will be released.", "comment": "Preprint Draft, 5 pages. This paper will be updated with a formal\n  version in the future, Copyright: College of Computer Science, Nankai\n  University. All rights reserved", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10858v1", "AI": {"title_translation": "医疗图像分割中的Med-URWKV：基于ImageNet预训练的纯RWKV模型", "tldr": "提出Med-URWKV，一个基于U-Net且利用ImageNet预训练的纯RWKV模型，用于医疗图像分割，首次实现直接重用大型预训练VRWKV编码器，并在七个数据集上表现出与从头训练模型相当或更优的性能。", "motivation": "现有医疗图像分割方法（CNN、Transformer、混合架构）各有局限性，如CNN感受野受限，Transformer计算复杂度高。虽然RWKV有潜力，但现有研究多集中于VRWKV机制修改并从头训练，未充分探索利用预训练VRWKV模型的优势。", "method": "提出Med-URWKV，一个基于U-Net框架的纯RWKV架构，并结合ImageNet预训练。该模型首次在医疗领域直接重用大规模预训练的VRWKV编码器。", "result": "Med-URWKV在七个数据集上的实验结果表明，其分割性能与从头训练的优化RWKV模型相比，达到相当甚至更优的水平。这验证了使用预训练VRWKV编码器增强模型性能的有效性。", "conclusion": "Med-URWKV模型通过结合ImageNet预训练和纯RWKV架构，有效提升了医疗图像分割性能，证明了重用大规模预训练VRWKV编码器的潜力。", "translation": "医疗图像分割是计算机辅助诊断和治疗中的一项基础且关键技术。现有方法大致可分为三类：基于卷积神经网络（CNN）、基于Transformer和结合两者的混合架构。然而，它们各自都有局限性，例如CNN中受限的感受野或Transformer中二次复杂度导致的计算开销。最近，接受度加权键值（RWKV）模型作为各种视觉任务的一个有前景的替代方案出现，提供了强大的长程建模能力和线性计算复杂度。一些研究也已将RWKV应用于医疗图像分割任务，取得了具有竞争力的性能。然而，这些研究大多侧重于对Vision-RWKV（VRWKV）机制的修改并从头训练模型，而没有探索利用预训练VRWKV模型在医疗图像分割任务中的潜在优势。在本文中，我们提出了Med-URWKV，一个基于U-Net框架的纯RWKV架构，它结合了基于ImageNet的预训练，以进一步探索RWKV在医疗图像分割任务中的潜力。据我们所知，Med-URWKV是医疗领域中第一个可以直接重用大型预训练VRWKV编码器的纯RWKV分割模型。在七个数据集上的实验结果表明，与从头训练的其他精心优化的RWKV模型相比，Med-URWKV取得了可比甚至更优的分割性能。这验证了使用预训练VRWKV编码器在增强模型性能方面的有效性。代码将发布。", "summary": "本文提出了Med-URWKV，一个结合U-Net框架和ImageNet预训练的纯RWKV模型，用于医疗图像分割。该模型旨在克服现有CNN和Transformer方法的局限性，并首次探索直接重用大规模预训练VRWKV编码器在医疗领域的潜力。实验结果表明，Med-URWKV在多个数据集上取得了与从头训练的RWKV模型相当或更优的性能，验证了预训练VRWKV编码器对模型性能提升的有效性。", "keywords": "医疗图像分割, RWKV, 预训练, U-Net, VRWKV", "comments": "本文的创新点在于首次将纯RWKV架构与大规模ImageNet预训练相结合应用于医疗图像分割，并通过直接重用预训练VRWKV编码器，克服了从头训练的局限性，并验证了预训练在医疗领域RWKV模型中的潜力。这为未来医疗图像处理中高效利用大型预训练模型提供了新思路。"}}
{"id": "2506.10161", "title": "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective", "authors": ["Yi Wang", "Max Kreminski"], "summary": "Story generation has been a prominent application of Large Language Models\n(LLMs). However, understanding LLMs' ability to produce high-quality stories\nremains limited due to challenges in automatic evaluation methods and the high\ncost and subjectivity of manual evaluation. Computational narratology offers\nvaluable insights into what constitutes a good story, which has been applied in\nthe symbolic narrative planning approach to story generation. This work aims to\ndeepen the understanding of LLMs' story generation capabilities by using them\nto solve narrative planning problems. We present a benchmark for evaluating\nLLMs on narrative planning based on literature examples, focusing on causal\nsoundness, character intentionality, and dramatic conflict. Our experiments\nshow that GPT-4 tier LLMs can generate causally sound stories at small scales,\nbut planning with character intentionality and dramatic conflict remains\nchallenging, requiring LLMs trained with reinforcement learning for complex\nreasoning. The results offer insights on the scale of stories that LLMs can\ngenerate while maintaining quality from different aspects. Our findings also\nhighlight interesting problem solving behaviors and shed lights on challenges\nand considerations for applying LLM narrative planning in game environments.", "comment": "In 2025 IEEE Conference on Games (CoG)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10161v1", "AI": {"title_translation": "大型语言模型能生成好故事吗？来自叙事规划视角的洞察与挑战", "tldr": "本研究通过让大型语言模型（LLMs）解决叙事规划问题，评估了其故事生成能力。结果显示，GPT-4级别的LLMs在小规模上可以生成因果连贯的故事，但在处理角色意图和戏剧冲突方面仍面临挑战，需要更复杂的训练方法。", "motivation": "由于自动评估方法的挑战以及人工评估的高成本和主观性，对大型语言模型（LLMs）生成高质量故事能力的理解仍然有限。计算叙事学为“好故事”的构成提供了宝贵见解，本研究旨在通过让LLMs解决叙事规划问题来加深对LLMs故事生成能力的理解。", "method": "本研究通过让大型语言模型解决叙事规划问题来评估其故事生成能力。我们提出了一个基于文学实例的基准，用于评估LLMs在叙事规划方面的表现，重点关注因果连贯性、角色意图和戏剧冲突。", "result": "实验表明，GPT-4级别的LLMs可以在小规模上生成因果连贯的故事。然而，涉及角色意图和戏剧冲突的规划仍然具有挑战性，需要通过强化学习训练的LLMs才能进行复杂推理。结果提供了LLMs在不同方面保持质量的情况下可以生成的故事规模的见解。", "conclusion": "本研究的结果揭示了大型语言模型在保持故事质量方面所能达到的规模，并突出了有趣的问题解决行为。同时，也阐明了在游戏环境中应用大型语言模型叙事规划所面临的挑战和需要考虑的因素。", "translation": "故事生成一直是大型语言模型（LLMs）的一个突出应用。然而，由于自动评估方法的挑战以及人工评估的高成本和主观性，对LLMs生成高质量故事能力的理解仍然有限。计算叙事学为“好故事”的构成提供了宝贵见解，这些见解已应用于故事生成的符号叙事规划方法中。本工作旨在通过让LLMs解决叙事规划问题来加深对LLMs故事生成能力的理解。我们提出了一个基于文学实例的基准，用于评估LLMs在叙事规划方面的表现，重点关注因果连贯性、角色意图和戏剧冲突。我们的实验表明，GPT-4级别的LLMs可以在小规模上生成因果连贯的故事，但涉及角色意图和戏剧冲突的规划仍然具有挑战性，需要通过强化学习训练的LLMs才能进行复杂推理。这些结果提供了LLMs在不同方面保持质量的情况下可以生成的故事规模的见解。我们的发现还突出了有趣的问题解决行为，并阐明了在游戏环境中应用LLM叙事规划所面临的挑战和需要考虑的因素。", "summary": "本研究旨在通过让大型语言模型（LLMs）解决叙事规划问题，深入理解其故事生成能力。研究提出了一个基于文学实例的叙事规划评估基准，重点考量故事的因果连贯性、角色意图和戏剧冲突。实验结果显示，GPT-4级别的LLMs在小规模上能生成因果连贯的故事，但在处理角色意图和戏剧冲突方面仍面临挑战，这需要更高级的LLM训练方法（如强化学习）。研究为LLMs生成高质量故事的规模提供了洞察，并指出了将LLM叙事规划应用于游戏环境的挑战。", "keywords": "大型语言模型, 故事生成, 叙事规划, 因果连贯性, 戏剧冲突", "comments": "这篇论文通过引入叙事规划的视角，为评估和理解大型语言模型（LLMs）的故事生成能力提供了一个新颖且结构化的框架。其创新之处在于将计算叙事学中的关键要素（如因果连贯性、角色意图和戏剧冲突）作为评估标准，弥补了现有自动评估方法和人工评估的不足。论文揭示了当前LLMs在复杂叙事要素（如角色意图和戏剧冲突）处理上的局限性，并指出了未来研究方向（如强化学习的应用）。这对于推动LLMs在故事创作，尤其是游戏叙事等领域的发展具有重要意义。"}}
{"id": "2506.10661", "title": "Alternating steepest descent methods for tensor completion with applications to spectromicroscopy", "authors": ["Oliver Townsend", "Sergey Dolgov", "Silvia Gazzola", "Misha Kilmer"], "summary": "In this paper we develop two new Tensor Alternating Steepest Descent\nalgorithms for tensor completion in the low-rank $\\star_{M}$-product format,\nwhereby we aim to reconstruct an entire low-rank tensor from a small number of\nmeasurements thereof. Both algorithms are rooted in the Alternating Steepest\nDescent (ASD) method for matrix completion, first proposed in [J. Tanner and K.\nWei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]. In deriving the new\nmethods we target the X-ray spectromicroscopy undersampling problem, whereby\ndata are collected by scanning a specimen on a rectangular viewpoint with X-ray\nbeams of different energies. The recorded absorptions coefficients of the mixed\nspecimen materials are naturally stored in a third-order tensor, with spatial\nhorizontal and vertical axes, and an energy axis. To speed the X-ray\nspectromicroscopy measurement process up, only a fraction of tubes from (a\nreshaped version of) this tensor are fully scanned, leading to a tensor\ncompletion problem. In this framework we can apply any transform (such as the\nFourier transform) to the tensor tube by tube, providing a natural way to work\nwith the $\\star_{M}$-tensor algebra, and propose: (1) a tensor completion\nalgorithm that is essentially ASD reformulated in the $\\star_{M}$-induced\nmetric space and (2) a tensor completion algorithm that solves a set of\n(readily parallelizable) independent matrix completion problems for the frontal\nslices of the transformed tensor. The two new methods are tested on real X-ray\nspectromicroscopy data, demonstrating that they achieve the same reconstruction\nerror with fewer samples from the tensor compared to the matrix completion\nalgorithms applied to a flattened tensor.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10661v1", "AI": {"title_translation": "针对张量补全的交替最速下降法及其在光谱显微术中的应用", "tldr": "本文提出了两种新的张量交替最速下降算法，用于低秩张量补全，并将其应用于X射线光谱显微术，结果表明它们能以更少的样本达到相同的重建精度。", "motivation": "旨在从少量测量中重建完整的低秩张量，特别针对X射线光谱显微术中的欠采样问题，以加速测量过程。", "method": "提出了两种基于矩阵补全中交替最速下降（ASD）方法的新算法：一种是将ASD在$\\star_{M}$-诱导度量空间中重新表述的张量补全算法；另一种是为变换张量的前切片解决一组可并行化的独立矩阵补全问题的算法。这些方法利用了$\\star_{M}$-张量代数。", "result": "两种新方法在真实的X射线光谱显微数据上进行了测试，结果表明与应用于扁平化张量的矩阵补全算法相比，它们能以更少的张量样本实现相同的重建误差。", "conclusion": "提出的两种新的张量交替最速下降算法能有效地解决低秩张量补全问题，特别是在X射线光谱显微术中，它们能够提高测量效率并保持重建精度。", "translation": "在本文中，我们开发了两种新的张量交替最速下降算法，用于低秩$\\star_{M}$-乘积格式的张量补全，旨在从少量测量中重建整个低秩张量。这两种算法都源于矩阵补全中的交替最速下降（ASD）方法，该方法最初由[J. Tanner和K. Wei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]提出。在推导新方法时，我们针对X射线光谱显微术的欠采样问题，其中数据是通过用不同能量的X射线束扫描矩形视场的样本来收集的。混合样本材料的记录吸收系数自然地存储在三阶张量中，具有空间水平和垂直轴以及能量轴。为了加快X射线光谱显微测量过程，只对该张量（重塑版本）的一小部分管进行完全扫描，从而导致张量补全问题。在此框架下，我们可以逐管对张量应用任何变换（例如傅里叶变换），提供了一种与$\\star_{M}$-张量代数一起工作的自然方式，并提出了：（1）一种张量补全算法，它本质上是在$\\star_{M}$-诱导度量空间中重新表述的ASD；（2）一种张量补全算法，它为变换张量的前切片解决一组（易于并行化的）独立矩阵补全问题。这两种新方法在真实的X射线光谱显微数据上进行了测试，证明与应用于扁平化张量的矩阵补全算法相比，它们能以更少的张量样本实现相同的重建误差。", "summary": "本文提出了两种基于交替最速下降（ASD）的新型张量补全算法，用于低秩$\\star_{M}$-乘积格式的张量重建。这些算法特别针对X射线光谱显微术中的欠采样问题，通过在$\\star_{M}$-诱导度量空间中重构ASD或解决可并行化的矩阵补全问题来实现。实验结果表明，与传统的矩阵补全方法相比，新方法能够在利用更少张量数据的情况下，达到相同的重建精度。", "keywords": "张量补全, 交替最速下降, $\\star_{M}$-乘积, 光谱显微术, 低秩", "comments": "该论文的创新点在于将经典的矩阵交替最速下降（ASD）方法推广到张量补全问题，并特别利用了$\\star_{M}$-张量代数，提出了两种新的有效算法。其重要性体现在解决了X射线光谱显微术中的数据欠采样问题，有望显著加速实验测量过程，同时保持数据重建的精度。通过将张量补全问题分解为可并行化的矩阵补全问题，也为实际应用提供了计算效率上的优势。"}}
{"id": "2506.10299", "title": "Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs", "authors": ["Hayato Futami", "Emiru Tsunoo", "Yosuke Kashiwagi", "Yuki Ito", "Hassan Shahmohammadi", "Siddhant Arora", "Shinji Watanabe"], "summary": "Speech-to-speech translation (S2ST) has been advanced with large language\nmodels (LLMs), which are fine-tuned on discrete speech units. In such\napproaches, modality adaptation from text to speech has been an issue. LLMs are\ntrained on text-only data, which presents challenges to adapt them to speech\nmodality with limited speech-to-speech data. To address the training\ndifficulty, we propose scheduled interleaved speech--text training in this\nstudy. We use interleaved speech--text units instead of speech units during\ntraining, where aligned text tokens are interleaved at the word level. We\ngradually decrease the ratio of text as training progresses, to facilitate\nprogressive modality adaptation from text to speech. We conduct experimental\nevaluations by fine-tuning LLaMA3.2-1B for S2ST on the CVSS dataset. We show\nthat the proposed method consistently improves the translation performances,\nespecially for languages with limited training data.", "comment": "Accepted to Interspeech2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10299v1", "AI": {"title_translation": "LLM语音到语音翻译的计划交错式语音-文本训练", "tldr": "提出一种计划交错式语音-文本训练方法，以解决LLM在语音到语音翻译中面临的模态适应问题，并显著提升翻译性能，尤其对低资源语言有效。", "motivation": "现有将大型语言模型（LLMs）应用于语音到语音翻译（S2ST）的方法面临模态适应问题。LLMs主要在纯文本数据上训练，难以适应语音模态，尤其在语音到语音数据有限的情况下。", "method": "提出一种计划交错式语音-文本训练方法。在训练过程中，使用交错的语音-文本单元而非纯语音单元，其中对齐的文本标记以词级别交错。训练时逐渐减少文本比例，以促进从文本到语音的渐进式模态适应。通过在CVSS数据集上微调LLaMA3.2-1B进行实验评估。", "result": "所提出的方法持续改善了翻译性能，尤其对于训练数据有限的语言效果更佳。", "conclusion": "计划交错式语音-文本训练有效解决了大型语言模型在语音到语音翻译中的模态适应挑战，并显著提升了翻译质量，特别是对于低资源语言。", "translation": "语音到语音翻译（S2ST）已通过大型语言模型（LLMs）得到发展，这些模型在离散语音单元上进行微调。在此类方法中，从文本到语音的模态适应一直是一个问题。LLMs是在纯文本数据上训练的，这给它们适应语音模态带来了挑战，尤其是在语音到语音数据有限的情况下。为了解决训练难题，本研究提出计划交错式语音-文本训练。我们在训练期间使用交错的语音-文本单元而非语音单元，其中对齐的文本标记以词级别交错。随着训练的进行，我们逐渐降低文本的比例，以促进从文本到语音的渐进式模态适应。我们通过在CVSS数据集上微调LLaMA3.2-1B，进行了实验评估。结果表明，所提出的方法持续改善了翻译性能，尤其对于训练数据有限的语言。", "summary": "本文提出一种计划交错式语音-文本训练方法，旨在解决大型语言模型（LLMs）在语音到语音翻译（S2ST）中面临的模态适应难题。该方法在训练时将词级别对齐的文本标记与语音单元交错使用，并逐步减少文本比例，以实现从文本到语音的渐进式适应。在CVSS数据集上对LLaMA3.2-1B的实验结果表明，该方法显著提升了翻译性能，尤其对数据稀缺的语言效果显著。", "keywords": "语音到语音翻译, 大型语言模型, 交错训练, 模态适应, 低资源语言", "comments": "这篇论文的创新点在于提出了“计划交错式语音-文本训练”这一新颖的训练策略，巧妙地解决了大型语言模型从文本模态向语音模态适应的挑战。通过逐步减少文本比例，实现了更平滑的模态过渡。此方法对于提升低资源语言的语音到语音翻译性能具有重要意义，展现了其在实际应用中的巨大潜力。"}}
{"id": "2506.10629", "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning", "authors": ["Yucheng Yang", "Tianyi Zhou", "Qiang He", "Lei Han", "Mykola Pechenizkiy", "Meng Fang"], "summary": "Unsupervised reinforcement learning (URL) aims to learn general skills for\nunseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL\nby maximizing the mutual information between states and skills but lacks\nsufficient theoretical analysis, e.g., how well its learned skills can\ninitialize a downstream task's policy. Our new theoretical analysis in this\npaper shows that the diversity and separability of learned skills are\nfundamentally critical to downstream task adaptation but MISL does not\nnecessarily guarantee these properties. To complement MISL, we propose a novel\ndisentanglement metric LSEPIN. Moreover, we build an information-geometric\nconnection between LSEPIN and downstream task adaptation cost. For better\ngeometric properties, we investigate a new strategy that replaces the KL\ndivergence in information geometry with Wasserstein distance. We extend the\ngeometric analysis to it, which leads to a novel skill-learning objective WSEP.\nIt is theoretically justified to be helpful to downstream task adaptation and\nit is capable of discovering more initial policies for downstream tasks than\nMISL. We finally propose another Wasserstein distance-based algorithm PWSEP\nthat can theoretically discover all optimal initial policies.", "comment": "Spotlight paper at ICLR 2024. This version includes acknowledgments\n  omitted from the ICLR version and indicates the corresponding authors\n  primarily responsible for the work", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10629v1", "AI": {"title_translation": "技能的任务适应：信息几何、解缠结和无监督强化学习的新目标", "tldr": "本文通过引入新的理论分析和基于Wasserstein距离的目标函数（WSEP, PWSEP），改进了无监督强化学习中技能学习的下游任务适应性，解决了现有方法（如MISL）在技能多样性和可分离性方面的不足。", "motivation": "无监督强化学习（URL）旨在为未见过的下游任务学习通用技能。现有方法如MISL通过最大化状态与技能的互信息来解决URL问题，但缺乏足够的理论分析，特别是其学习到的技能如何有效地初始化下游任务策略。MISL不一定能保证学习到的技能具有下游任务适应性所需的关键特性——多样性和可分离性。", "method": "提出新的理论分析，表明学习技能的多样性和可分离性对下游任务适应至关重要。提出一种新的解缠结度量LSEPIN，并建立其与下游任务适应成本之间的信息几何联系。研究一种新策略，用Wasserstein距离替换信息几何中的KL散度以获得更好的几何特性。基于此，提出新的技能学习目标WSEP，并进一步提出另一种基于Wasserstein距离的算法PWSEP。", "result": "理论分析表明学习技能的多样性和可分离性对下游任务适应至关重要。WSEP在理论上被证明有助于下游任务适应，并且能够比MISL发现更多的下游任务初始策略。PWSEP在理论上能够发现所有最优的初始策略。", "conclusion": "本文通过深入的理论分析，揭示了技能多样性和可分离性对无监督强化学习中下游任务适应的重要性。通过引入基于Wasserstein距离的新目标函数WSEP和PWSEP，克服了现有方法MISL的局限性，显著提升了学习技能对下游任务的适应能力和初始策略的发现能力。", "translation": "无监督强化学习（URL）旨在为未见过的下游任务学习通用技能。互信息技能学习（MISL）通过最大化状态与技能之间的互信息来解决URL问题，但缺乏充分的理论分析，例如其学习到的技能如何很好地初始化下游任务策略。本文新的理论分析表明，学习技能的多样性和可分离性对于下游任务适应至关重要，但MISL不一定能保证这些特性。为了补充MISL，我们提出了一种新颖的解缠结度量LSEPIN。此外，我们在LSEPIN和下游任务适应成本之间建立了信息几何联系。为了更好的几何特性，我们研究了一种新策略，用Wasserstein距离替换信息几何中的KL散度。我们将其几何分析扩展到此，从而产生了新的技能学习目标WSEP。它在理论上被证明有助于下游任务适应，并且能够比MISL发现更多的下游任务初始策略。我们最终提出了另一种基于Wasserstein距离的算法PWSEP，它在理论上可以发现所有最优的初始策略。", "summary": "本文关注无监督强化学习（URL）中的技能学习问题，并指出现有方法（如MISL）在下游任务适应性方面存在理论不足，特别是在技能的多样性和可分离性上。作者通过新的理论分析，提出了一个解缠结度量LSEPIN，并建立了其与下游任务适应成本的信息几何联系。为改进几何特性，引入了基于Wasserstein距离的新技能学习目标WSEP和PWSEP，其中WSEP被理论证明能更好地适应下游任务并发现更多初始策略，而PWSEP理论上能发现所有最优初始策略。", "keywords": "无监督强化学习, 技能学习, 信息几何, Wasserstein距离, 任务适应", "comments": "本文通过深入的信息几何理论分析，弥补了无监督强化学习领域中现有技能学习方法（如MISL）在理论基础上的不足。其创新点在于引入了Wasserstein距离来替代传统的KL散度，从而在几何上更好地保证了学习技能的多样性和可分离性，这对于下游任务的有效适应至关重要。提出的WSEP和PWSEP算法为无监督技能学习提供了更鲁棒和理论上更优的解决方案，对于提升URL的实际应用潜力具有重要意义。"}}
{"id": "2506.10535", "title": "Analyzing the performance of a V2X-enhanced braking system in real-world crash situations", "authors": ["Jan Zimmermann", "Jörg Mönnich", "Michael Scherl", "Ignacio Llatser", "Florian Wildschütte", "Frank Hofmann"], "summary": "By using an automated braking system, such as the Automatic Emergency Brake\n(AEB), crashes can be avoided in situations where the driver is unaware of an\nimminent collision. However, conventional AEB systems detect potential\ncollision adversaries with onboard sensor systems, such as radars and cameras,\nthat may fail in non-line-of-sight situations. By leveraging\nvehicle-to-everything (V2X) communication, information regarding an approaching\nvehicle can be received by the ego vehicle at an early point in time, even if\nthe opponent vehicle is occluded by a view obstruction. In this work, we\nconsider a 2-stage braking cascade, consisting of a partial brake, triggered\nbased on V2X information, and a sensor-triggered AEB. We evaluate its crash\navoidance performance in real-world crash situations extracted from the German\nIn-Depth Accident Study (GIDAS) database using an accident simulation\nframework. The results are compared against a sensor-triggered AEB system and a\npurely V2X-triggered partial brake. To further analyze the results, we identify\nthe crash cause for each situation in which the brake function under test could\nnot prevent the crash. The simulation results show a high added benefit of the\nV2X-enhanced braking systems compared to the exclusive use of visual-based\nsensor systems for automated collision prevention.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10535v1", "AI": {"title_translation": "分析V2X增强型制动系统在真实世界碰撞情况下的性能", "tldr": "本研究评估了一种结合V2X信息和传统AEB的两阶段制动系统，在真实碰撞场景中的防撞性能，结果显示V2X增强型系统相比纯传感器系统具有显著优势。", "motivation": "传统的自动紧急制动（AEB）系统依赖车载传感器（如雷达和摄像头），但在非视距情况下可能失效。为解决这一局限性，本研究旨在利用V2X通信提前获取车辆信息，以提高防撞能力。", "method": "研究考虑了一个两阶段制动级联系统，包括基于V2X信息触发的部分制动和传感器触发的AEB。使用从德国深度事故研究（GIDAS）数据库中提取的真实世界碰撞情况，通过事故模拟框架评估其防撞性能。结果与纯传感器触发的AEB系统和纯V2X触发的部分制动进行了比较。", "result": "模拟结果显示，与单独使用基于视觉的传感器系统进行自动防撞相比，V2X增强型制动系统具有显著的附加优势。", "conclusion": "V2X增强型制动系统在真实世界碰撞场景中表现出更高的防撞性能，显著优于仅依赖车载传感器的系统。", "translation": "通过使用自动制动系统，例如自动紧急制动（AEB），可以在驾驶员未意识到即将发生碰撞的情况下避免事故。然而，传统的AEB系统通过车载传感器系统（如雷达和摄像头）检测潜在的碰撞对手，这些系统在非视距情况下可能会失效。通过利用车联网（V2X）通信，即使对手车辆被视线障碍物遮挡，本车也能在早期时间点接收到有关接近车辆的信息。在这项工作中，我们考虑了一个两阶段制动级联，包括基于V2X信息触发的部分制动和传感器触发的AEB。我们使用事故模拟框架，评估了其在从德国深度事故研究（GIDAS）数据库中提取的真实世界碰撞情况下的防撞性能。结果与传感器触发的AEB系统和纯V2X触发的部分制动进行了比较。为了进一步分析结果，我们确定了测试制动功能无法避免碰撞的每种情况下的碰撞原因。模拟结果表明，与单独使用基于视觉的传感器系统进行自动防撞相比，V2X增强型制动系统具有很高的附加效益。", "summary": "本研究提出并评估了一种V2X增强型两阶段制动系统，该系统结合了基于V2X信息的部分制动和传感器触发的自动紧急制动（AEB），旨在克服传统AEB在非视距情况下的局限性。通过使用德国GIDAS数据库中的真实事故数据进行模拟，结果表明，与仅依赖车载传感器的系统相比，V2X增强型系统在碰撞避免方面显示出显著的性能提升。", "keywords": "V2X, 自动紧急制动, 防撞系统, 真实世界碰撞, 车辆安全", "comments": "本文的创新点在于提出了一个结合V2X通信和传统AEB的两阶段制动策略，有效解决了传统AEB在非视距场景下的局限性。其重要性在于通过真实世界事故数据的模拟验证了V2X技术在提升车辆主动安全方面的巨大潜力，为未来智能交通系统的发展提供了有力的支持。"}}
{"id": "2506.10175", "title": "AURA: A Multi-Agent Intelligence Framework for Knowledge-Enhanced Cyber Threat Attribution", "authors": ["Nanda Rani", "Sandeep Kumar Shukla"], "summary": "Effective attribution of Advanced Persistent Threats (APTs) increasingly\nhinges on the ability to correlate behavioral patterns and reason over complex,\nvaried threat intelligence artifacts. We present AURA (Attribution Using\nRetrieval-Augmented Agents), a multi-agent, knowledge-enhanced framework for\nautomated and interpretable APT attribution. AURA ingests diverse threat data\nincluding Tactics, Techniques, and Procedures (TTPs), Indicators of Compromise\n(IoCs), malware details, adversarial tools, and temporal information, which are\nprocessed through a network of collaborative agents. These agents are designed\nfor intelligent query rewriting, context-enriched retrieval from structured\nthreat knowledge bases, and natural language justification of attribution\ndecisions. By combining Retrieval-Augmented Generation (RAG) with Large\nLanguage Models (LLMs), AURA enables contextual linking of threat behaviors to\nknown APT groups and supports traceable reasoning across multiple attack\nphases. Experiments on recent APT campaigns demonstrate AURA's high attribution\nconsistency, expert-aligned justifications, and scalability. This work\nestablishes AURA as a promising direction for advancing transparent,\ndata-driven, and scalable threat attribution using multi-agent intelligence.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10175v1", "AI": {"title_translation": "AURA：一个用于知识增强型网络威胁归因的多智能体智能框架", "tldr": "AURA是一个多智能体、知识增强的框架，结合RAG和LLM，用于自动化、可解释的高级持续威胁（APT）归因，通过处理多样化的威胁数据实现高归因一致性和可追溯推理。", "motivation": "有效归因高级持续威胁（APT）越来越依赖于关联行为模式和对复杂多样的威胁情报工件进行推理的能力。现有的APT归因方法可能缺乏自动化、可解释性或处理多样化复杂数据的能力。", "method": "AURA（Attribution Using Retrieval-Augmented Agents）是一个多智能体、知识增强的框架。它摄取包括TTPs、IoCs、恶意软件详情、对抗工具和时间信息在内的多样化威胁数据。这些数据通过一个协作智能体网络进行处理，这些智能体负责智能查询重写、从结构化威胁知识库中进行上下文丰富的检索，以及对归因决策进行自然语言解释。AURA结合了检索增强生成（RAG）和大型语言模型（LLMs），以实现威胁行为与已知APT组的上下文链接，并支持跨多个攻击阶段的可追溯推理。", "result": "在近期APT活动上的实验表明，AURA具有高归因一致性、与专家对齐的归因理由和良好的可扩展性。", "conclusion": "这项工作确立了AURA作为利用多智能体智能推进透明、数据驱动和可扩展威胁归因的一个有前景的方向。", "translation": "有效归因高级持续威胁（APT）越来越依赖于关联行为模式和对复杂多样的威胁情报工件进行推理的能力。我们提出了AURA（Attribution Using Retrieval-Augmented Agents），一个用于自动化和可解释APT归因的多智能体、知识增强框架。AURA摄取多样化的威胁数据，包括战术、技术和程序（TTPs）、危害指标（IoCs）、恶意软件详情、对抗工具和时间信息，这些数据通过一个协作智能体网络进行处理。这些智能体旨在进行智能查询重写、从结构化威胁知识库中进行上下文丰富的检索，以及对归因决策进行自然语言解释。通过将检索增强生成（RAG）与大型语言模型（LLMs）相结合，AURA能够将威胁行为与已知APT组进行上下文关联，并支持跨多个攻击阶段的可追溯推理。在近期APT活动上的实验表明，AURA具有高归因一致性、与专家对齐的归因理由和良好的可扩展性。这项工作确立了AURA作为利用多智能体智能推进透明、数据驱动和可扩展威胁归因的一个有前景的方向。", "summary": "AURA是一个多智能体、知识增强的框架，旨在实现自动化和可解释的高级持续威胁（APT）归因。它能够摄取并处理多样化的威胁情报数据，如TTPs和IoCs，通过协作智能体网络进行智能查询、上下文检索和自然语言解释。该框架结合了检索增强生成（RAG）和大型语言模型（LLMs），以实现威胁行为与已知APT组的上下文链接和可追溯推理。实验证明AURA在归因一致性、理由合理性和可扩展性方面表现出色，为透明、数据驱动的威胁归因提供了新方向。", "keywords": "APT归因, 多智能体系统, 知识增强, 检索增强生成, 大型语言模型", "comments": "AURA的创新之处在于其结合了多智能体系统、知识增强技术、RAG和LLMs，以解决APT归因的复杂性问题。其强调“可解释性”和“可追溯推理”对于网络安全领域至关重要，有助于安全分析师理解归因决策。此外，其处理多样化威胁数据和展现出的高一致性与可扩展性，使其成为一个有前景的实用工具。"}}
{"id": "2506.10426", "title": "Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models", "authors": ["Xiao Yu", "Haoxuan Chen", "Feifei Niu", "Xing Hu", "Jacky Wai Keung", "Xin Xia"], "summary": "With the rapid development of large language models (LLMs), distributed\ntraining and inference frameworks like DeepSpeed have become essential for\nscaling model training and inference across multiple GPUs or nodes. However,\nthe increasing complexity of these frameworks brings non-trivial software bugs,\nwhich may degrade training performance, cause unexpected failures, and result\nin significant resource waste. Understanding framework bugs' characteristics is\nfundamental for quality assurance, allowing the design of more effective\ndebugging and repair methods. Thus, our paper conducts the first large-scale\nempirical analysis of 308 fixed bugs across three popular distributed\ntraining/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We\nexamine bug symptoms, root causes, bug identification and fixing efforts, and\ncommon low-effort fixing strategies. Additionally, the distributed nature of\nthese frameworks introduces unique bug root causes, such as allocation strategy\nerror and distributed communication error. Diagnosing and fixing complex bugs\nremains challenging due to factors like the disconnect between symptoms and\nroot causes, high bug reproduction costs, and low-level or cross-component\ninteractions. Interestingly, we observe that 48% of bug fixes require minimal\ncode changes (<=10 LOC) and follow simple strategies such as conditional logic\noptimization, parameter handling enhancement, or version compatibility\nhandling, indicating potential for automation. Based on these insights, we\noffer several implications for improving the reliability of both distributed\ntraining and inference frameworks and their dependent LLM projects, while also\nidentifying opportunities to leverage LLM-based tools for automated debugging\nand repair.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10426v1", "AI": {"title_translation": "走向理解大型语言模型分布式训练与推理框架中的缺陷", "tldr": "对DeepSpeed、Megatron-LM和Colossal-AI等主流分布式训练/推理框架中的308个已修复缺陷进行了首次大规模实证分析，揭示了缺陷特征、根本原因及修复策略，并指出48%的缺陷修复代码量小，具有自动化潜力。", "motivation": "随着大型语言模型的快速发展，分布式训练和推理框架变得至关重要，但其日益增长的复杂性引入了非平凡的软件缺陷，这些缺陷可能降低训练性能、导致意外故障并造成资源浪费。理解框架缺陷的特征对于质量保证和设计有效的调试与修复方法至关重要。", "method": "本文对DeepSpeed、Megatron-LM和Colossal-AI这三个流行的分布式训练/推理框架中308个已修复的缺陷进行了首次大规模实证分析。研究检查了缺陷症状、根本原因、缺陷识别和修复工作量，以及常见的低成本修复策略。", "result": "研究发现，分布式框架引入了独特的缺陷根本原因，如分配策略错误和分布式通信错误。诊断和修复复杂缺陷仍然具有挑战性。有趣的是，48%的缺陷修复仅需要最少的代码更改（<=10行代码），并遵循简单的策略，如条件逻辑优化、参数处理增强或版本兼容性处理，这表明了自动化的潜力。", "conclusion": "基于这些洞察，本文为提高分布式训练和推理框架及其依赖的LLM项目的可靠性提供了多项启示，并识别了利用基于LLM的工具进行自动化调试和修复的机会。", "translation": "随着大型语言模型（LLMs）的快速发展，像DeepSpeed这样的分布式训练和推理框架对于在多个GPU或节点上扩展模型训练和推理变得至关重要。然而，这些框架日益增长的复杂性带来了非平凡的软件缺陷，这些缺陷可能会降低训练性能、导致意外故障，并导致大量的资源浪费。理解框架缺陷的特性对于质量保证至关重要，它允许设计更有效的调试和修复方法。因此，我们的论文对DeepSpeed、Megatron-LM和Colossal-AI这三个流行的分布式训练/推理框架中的308个已修复缺陷进行了首次大规模实证分析。我们检查了缺陷症状、根本原因、缺陷识别和修复工作量，以及常见的低成本修复策略。此外，这些框架的分布式特性引入了独特的缺陷根本原因，例如分配策略错误和分布式通信错误。由于症状与根本原因之间的脱节、高昂的缺陷复现成本以及低级别或跨组件交互等因素，诊断和修复复杂缺陷仍然具有挑战性。有趣的是，我们观察到48%的缺陷修复只需要最少的代码更改（<=10行代码），并遵循简单的策略，如条件逻辑优化、参数处理增强或版本兼容性处理，这表明了自动化的潜力。基于这些见解，我们为提高分布式训练和推理框架及其依赖的LLM项目的可靠性提供了多项启示，同时还识别了利用基于LLM的工具进行自动化调试和修复的机会。", "summary": "本研究首次对DeepSpeed、Megatron-LM和Colossal-AI三个主流分布式训练/推理框架中的308个已修复缺陷进行了大规模实证分析。论文深入探讨了这些复杂框架中缺陷的症状、根本原因、识别与修复工作，并发现分布式特性引入了独特的缺陷类型。研究特别指出，尽管复杂缺陷诊断困难，但有48%的缺陷修复仅需少量代码改动且遵循简单策略，预示了自动化修复的巨大潜力。最后，研究提出了提高框架可靠性的建议，并展望了利用LLM工具进行自动化调试和修复的可能性。", "keywords": "分布式训练, 大型语言模型, 软件缺陷, 经验分析, 自动化修复", "comments": "该论文创新性地对大型语言模型分布式训练与推理框架中的缺陷进行了大规模实证分析，填补了该领域的空白。其重要性在于揭示了此类复杂系统中缺陷的独特特征和修复模式，特别是指出近半数缺陷可自动化修复，为未来开发更高效的调试和修复工具提供了宝贵的见解和方向。这对于提升LLM工程的可靠性和效率具有重要意义。"}}
{"id": "2506.10927", "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review", "authors": ["T. T. J. E. Arets", "G. Perugia", "M. Houben", "W. A. IJsselsteijn"], "summary": "Reduced social connectedness increasingly poses a threat to mental health,\nlife expectancy, and general well-being. Generative AI (GAI) technologies, such\nas large language models (LLMs) and image generation tools, are increasingly\nintegrated into applications aimed at enhancing human social experiences.\nDespite their growing presence, little is known about how these technologies\ninfluence social interactions. This scoping review investigates how GAI-based\napplications are currently designed to facilitate social interaction, what\nforms of social engagement they target, and which design and evaluation\nmethodologies designers use to create and evaluate them. Through an analysis of\n30 studies published since 2020, we identify key trends in application domains\nincluding storytelling, socio-emotional skills training, reminiscence,\ncollaborative learning, music making, and general conversation. We highlight\nthe role of participatory and co-design approaches in fostering both effective\ntechnology use and social engagement, while also examining socio-ethical\nconcerns such as cultural bias and accessibility. This review underscores the\npotential of GAI to support dynamic and personalized interactions, but calls\nfor greater attention to equitable design practices and inclusive evaluation\nstrategies.", "comment": "Preprint version of a manuscript submitted to ACM Transactions on\n  Computer-Human Interaction (TOCHI), under review. 39 pages, 4 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10927v1", "AI": {"title_translation": "生成式AI在促进社会互动中的作用：一项范围审查", "tldr": "一项范围审查分析了30项研究，探讨生成式AI如何设计以促进社交互动，发现其在多个应用领域有潜力，但需关注公平设计和包容性评估。", "motivation": "社会连接性下降日益威胁心理健康和福祉。尽管生成式AI（GAI）越来越多地被用于增强人类社交体验，但目前对其如何影响社会互动知之甚少。", "method": "本文进行了一项范围审查，分析了自2020年以来发表的30项研究，以调查基于GAI的应用如何设计来促进社会互动，它们针对的社交参与形式，以及设计和评估方法。", "result": "识别出GAI应用在讲故事、社交情感技能训练、回忆、协作学习、音乐制作和一般对话等领域的关键趋势。强调了参与式和协同设计方法在促进有效技术使用和社交参与方面的作用，并审视了文化偏见和可访问性等社会伦理问题。", "conclusion": "本综述强调了GAI支持动态和个性化互动的潜力，但呼吁更多关注公平的设计实践和包容性评估策略。", "translation": "社会连接性下降日益对心理健康、预期寿命和总体福祉构成威胁。大型语言模型（LLMs）和图像生成工具等生成式AI（GAI）技术正越来越多地融入旨在增强人类社交体验的应用中。尽管它们日益普及，但人们对这些技术如何影响社会互动知之甚少。本范围审查调查了基于GAI的应用目前是如何设计来促进社会互动的，它们针对哪些形式的社交参与，以及设计者使用哪些设计和评估方法来创建和评估它们。通过对自2020年以来发表的30项研究进行分析，我们确定了应用领域中的关键趋势，包括讲故事、社交情感技能训练、回忆、协作学习、音乐制作和一般对话。我们强调了参与式和协同设计方法在促进有效技术使用和社交参与方面的作用，同时还审视了文化偏见和可访问性等社会伦理问题。本综述强调了GAI支持动态和个性化互动的潜力，但呼吁更多关注公平的设计实践和包容性评估策略。", "summary": "这项范围审查分析了30项研究，探讨了生成式AI（GAI）在促进社会互动中的作用。研究发现，GAI应用已广泛应用于讲故事、社交情感训练、回忆等多个领域，并强调了参与式和协同设计的重要性。同时，审查也指出了文化偏见和可访问性等社会伦理问题。论文总结了GAI在支持个性化互动方面的潜力，并呼吁未来设计中应更加注重公平性和包容性。", "keywords": "生成式AI, 社会互动, 范围审查, 设计实践, 伦理考量", "comments": "这篇论文通过范围审查的形式，系统地梳理了生成式AI在促进社会互动方面的当前应用、设计方法和伦理考量，填补了现有知识空白。其创新之处在于首次对该领域进行了全面的宏观审视，并提出了未来的设计和评估方向，对于指导GAI在社会领域的负责任发展具有重要意义。"}}
{"id": "2506.10363", "title": "Towards more efficient quantitative safety validation of residual risk for assisted and automated driving", "authors": ["Daniel Betschinske", "Malte Schrimpf", "Steven Peters", "Kamil Klonecki", "Jan Peter Karch", "Moritz Lippert"], "summary": "The safety validation of Advanced Driver Assistance Systems (ADAS) and\nAutomated Driving Systems (ADS) increasingly demands efficient and reliable\nmethods to quantify residual risk while adhering to international standards\nsuch as ISO 21448. Traditionally, Field Operational Testing (FOT) has been\npivotal for macroscopic safety validation of automotive driving functions up to\nSAE automation level 2. However, state-of-the-art derivations for empirical\nsafety demonstrations using FOT often result in impractical testing efforts,\nparticularly at higher automation levels. Even at lower automation levels, this\nlimitation - coupled with the substantial costs associated with FOT - motivates\nthe exploration of approaches to enhance the efficiency of FOT-based\nmacroscopic safety validation. Therefore, this publication systematically\nidentifies and evaluates state-of-the-art Reduction Approaches (RAs) for FOT,\nincluding novel methods reported in the literature. Based on an analysis of ISO\n21448, two models are derived: a generic model capturing the argumentation\ncomponents of the standard, and a base model, exemplarily applied to Automatic\nEmergency Braking (AEB) systems, establishing a baseline for the real-world\ndriving requirement for a Quantitative Safety Validation of Residual Risk\n(QSVRR). Subsequently, the RAs are assessed using four criteria:\nquantifiability, threats to validity, missing links, and black box\ncompatibility, highlighting potential benefits, inherent limitations, and\nidentifying key areas for further research. Our evaluation reveals that, while\nseveral approaches offer potential, none are free from missing links or other\nsubstantial shortcomings. Moreover, no identified alternative can fully replace\nFOT, reflecting its crucial role in the safety validation of ADAS and ADS.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10363v1", "AI": {"title_translation": "迈向更高效的辅助和自动驾驶残余风险定量安全验证", "tldr": "FOT验证自动驾驶系统残余风险效率低下且成本高昂。本文系统评估了FOT的效率提升方法，发现现有方法虽有潜力但均无法完全取代FOT。", "motivation": "现有ADAS和ADS的宏观安全验证方法（如FOT）在量化残余风险方面效率低下且成本高昂，尤其在高自动化水平下测试工作量不切实际。", "method": "本文系统识别并评估了FOT的先进规避方法（Reduction Approaches, RAs），包括文献中的新方法。基于ISO 21448，导出了两个模型：一个捕获标准论证组件的通用模型，以及一个应用于自动紧急制动（AEB）系统的基础模型，为残余风险定量安全验证（QSVRR）的真实世界驾驶要求建立基线。随后，使用四个标准（可量化性、有效性威胁、缺失环节、黑盒兼容性）评估了RAs。", "result": "评估显示，尽管一些方法具有潜力，但它们都存在缺失环节或其他实质性缺陷。没有发现任何替代方法可以完全取代FOT。", "conclusion": "FOT在ADAS和ADS的安全验证中仍然扮演着至关重要的角色，需要进一步研究以完善现有的规避方法。", "translation": "先进驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的安全验证越来越需要高效可靠的方法来量化残余风险，同时遵循ISO 21448等国际标准。传统上，现场运行测试（FOT）对于SAE自动化2级及以下的汽车驾驶功能的宏观安全验证至关重要。然而，使用FOT进行经验安全演示的最新推导通常会导致不切实际的测试工作量，特别是在更高自动化级别。即使在较低自动化级别，这种限制——加上FOT相关的巨额成本——也促使人们探索提高基于FOT的宏观安全验证效率的方法。因此，本出版物系统地识别和评估了FOT的最新规避方法（RAs），包括文献中报道的新方法。基于对ISO 21448的分析，导出了两个模型：一个捕获标准论证组件的通用模型，以及一个基础模型，以自动紧急制动（AEB）系统为例进行应用，为残余风险定量安全验证（QSVRR）的真实世界驾驶要求建立了基线。随后，使用四个标准评估了RAs：可量化性、有效性威胁、缺失环节和黑盒兼容性，突出了潜在益处、固有局限性，并确定了进一步研究的关键领域。我们的评估表明，虽然有几种方法具有潜力，但它们都并非没有缺失环节或其他实质性缺陷。此外，没有发现任何替代方法可以完全取代FOT，这反映了其在ADAS和ADS安全验证中的关键作用。", "summary": "本文旨在提高辅助和自动驾驶系统（ADAS/ADS）残余风险定量安全验证的效率。鉴于传统现场运行测试（FOT）在高自动化水平下成本高昂且不切实际，作者系统识别并评估了FOT的效率提升方法。基于ISO 21448，导出了通用模型和AEB系统应用的基础模型。对这些方法的评估表明，尽管它们有潜力，但都存在局限性，并且没有一种方法可以完全取代FOT，这强调了FOT在ADAS/ADS安全验证中的关键作用。", "keywords": "辅助驾驶系统, 自动驾驶系统, 安全验证, 残余风险, 现场运行测试 (FOT), ISO 21448, 规避方法", "comments": "本文识别了自动驾驶系统安全验证中的核心痛点：传统FOT方法的高昂成本和低效率。其创新之处在于系统性地评估了现有规避方法，并基于ISO 21448构建了分析模型。尽管结果显示目前尚无完美替代方案，但这项工作为未来研究指明了方向，强调了FOT不可替代的重要性，并揭示了现有方法在可量化性、有效性威胁、缺失环节和黑盒兼容性方面的不足，对行业具有重要的指导意义。"}}
{"id": "2506.10178", "title": "Attention, Please! Revisiting Attentive Probing for Masked Image Modeling", "authors": ["Bill Psomas", "Dionysis Christopoulos", "Eirini Baltzi", "Ioannis Kakogeorgiou", "Tilemachos Aravanis", "Nikos Komodakis", "Konstantinos Karantzalos", "Yannis Avrithis", "Giorgos Tolias"], "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10$\\times$ speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10178v1", "AI": {"title_translation": "请注意！重新审视用于掩码图像建模的注意力探测", "tldr": "本文提出了一种名为高效探测（EP）的新型注意力探测机制，解决了现有方法参数过多和计算效率低下的问题，并在多个基准测试中表现出色，速度提升高达10倍。", "motivation": "随着大规模微调变得越来越不切实际，探测（Probing）正成为自监督学习（SSL）的首选评估协议。然而，标准的线性探测（LP）无法充分反映通过掩码图像建模（MIM）训练的模型潜力，因为补丁令牌的分布式特性。这促使了对注意力探测的需求，但现有方法存在参数过多和计算效率低下的问题。", "method": "本文从准确性-效率权衡的角度重新审视了注意力探测。系统研究了现有方法，分析了它们的机制并评估了其性能。引入了高效探测（EP），这是一种多查询交叉注意力机制，它消除了冗余投影，减少了可训练参数的数量，并实现了比传统多头注意力高达10倍的加速。", "result": "高效探测（EP）尽管简单，但在七个基准测试中均优于线性探测（LP）和先前的注意力探测方法，并且能够很好地推广到MIM之外的各种预训练范式，生成可解释的注意力图，并在低样本和逐层设置中获得了显著的增益。", "conclusion": "本文提出了一种名为高效探测（EP）的新的注意力探测机制，它解决了现有注意力探测方法在参数化和计算效率方面的缺点，并在多个评估场景中展示了卓越的性能和泛化能力。", "translation": "随着大规模微调（FT）变得越来越不切实际，探测（Probing）正成为自监督学习（SSL）的首选评估协议。然而，由于补丁令牌的分布式特性，标准的线性探测（LP）未能充分反映通过掩码图像建模（MIM）训练的模型的潜力。这促使了对注意力探测的需求，这是一种使用注意力选择性聚合补丁级特征的替代方法。尽管注意力探测正在被日益采用，但它仍未得到充分探索，现有方法存在参数过多和计算效率低下的问题。\n在这项工作中，我们从准确性-效率权衡的角度重新审视了注意力探测。我们对现有方法进行了系统研究，分析了它们的机制并评估了它们的性能。我们引入了高效探测（EP），这是一种多查询交叉注意力机制，它消除了冗余投影，减少了可训练参数的数量，并实现了比传统多头注意力高达10倍的加速。尽管它很简单，但EP在七个基准测试中均优于LP和先前的注意力探测方法，能够很好地推广到MIM之外的各种预训练范式，生成可解释的注意力图，并在低样本和逐层设置中获得了显著的增益。代码可在https://github.com/billpsomas/efficient-probing 获取。", "summary": "本文针对大规模自监督学习模型评估中线性探测的不足以及现有注意力探测的效率问题，提出了一种名为高效探测（EP）的新型注意力探测机制。EP采用多查询交叉注意力，通过减少冗余投影和可训练参数，实现了高达10倍的加速。实验表明，EP在多个基准测试中超越了传统线性探测和现有注意力探测方法，并展现出良好的泛化能力和可解释性，尤其在低样本和逐层评估中表现出色。", "keywords": "注意力探测, 掩码图像建模, 高效探测, 自监督学习, 模型评估", "comments": "本文提出了一种简单而高效的注意力探测方法（EP），解决了现有注意力探测在参数和计算效率上的痛点。其创新性在于通过多查询交叉注意力机制，在保证性能的同时大幅提升了效率。这项工作对于大规模自监督学习模型的评估具有重要意义，尤其是在MIM等需要精细特征聚合的场景下。"}}
{"id": "2506.10127", "title": "Meet Me at the Arm: The Cooperative Multi-Armed Bandits Problem with Shareable Arms", "authors": ["Xinyi Hu", "Aldo Pacchiano"], "summary": "We study the decentralized multi-player multi-armed bandits (MMAB) problem\nunder a no-sensing setting, where each player receives only their own reward\nand obtains no information about collisions. Each arm has an unknown capacity,\nand if the number of players pulling an arm exceeds its capacity, all players\ninvolved receive zero reward. This setting generalizes the classical\nunit-capacity model and introduces new challenges in coordination and capacity\ndiscovery under severe feedback limitations. We propose A-CAPELLA (Algorithm\nfor Capacity-Aware Parallel Elimination for Learning and Allocation), a\ndecentralized algorithm that achieves logarithmic regret in this generalized\nregime. Our main contribution is a collaborative hypothesis testing protocol\nthat enables synchronized successive elimination and capacity estimation\nthrough carefully structured collision patterns. This represents a provably\nefficient learning result in decentralized no-sensing MMAB with unknown arm\ncapacities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10127v1", "AI": {"title_translation": "在共享臂的多臂老虎机问题中与我相遇", "tldr": "本文研究了去中心化多玩家多臂老虎机（MMAB）问题，其中每个臂具有未知容量。作者提出了一种名为A-CAPELLA的去中心化算法，该算法在这种广义机制下实现了对数遗憾，并通过协作假设检验协议实现了容量估计和同步消除。", "motivation": "该研究旨在解决去中心化多玩家多臂老虎机（MMAB）问题，尤其是在无感知设置下，每个玩家只接收自己的奖励且无法获取碰撞信息。该设置推广了经典的单位容量模型，并在反馈受限的情况下引入了协调和容量发现的新挑战。", "method": "本文提出了一种名为A-CAPELLA（Algorithm for Capacity-Aware Parallel Elimination for Learning and Allocation）的去中心化算法。该算法通过一个协作假设检验协议实现同步的逐次消除和容量估计，该协议通过精心构造的碰撞模式实现。", "result": "A-CAPELLA算法在这种广义机制下实现了对数遗憾。该算法在去中心化无感知MMAB（具有未知臂容量）中实现了可证明的高效学习结果。", "conclusion": "本文提出了一个针对具有未知臂容量的去中心化无感知多玩家多臂老虎机问题的有效算法A-CAPELLA，该算法通过协作假设检验实现了高效学习和对数遗憾。", "translation": "我们研究了在无感知设置下的去中心化多玩家多臂老虎机（MMAB）问题，其中每个玩家只收到自己的奖励，并且无法获取关于碰撞的信息。每个臂都具有未知容量，如果拉动一个臂的玩家数量超过其容量，所有相关玩家都将获得零奖励。这种设置推广了经典的单位容量模型，并在严重的反馈限制下引入了协调和容量发现的新挑战。我们提出了A-CAPELLA（容量感知并行学习和分配算法），这是一种在该广义机制下实现对数遗憾的去中心化算法。我们的主要贡献是一个协作假设检验协议，它通过精心构造的碰撞模式实现了同步的逐次消除和容量估计。这代表了在具有未知臂容量的去中心化无感知MMAB中可证明的高效学习结果。", "summary": "本文研究了在无感知设置下具有未知臂容量的去中心化多玩家多臂老虎机（MMAB）问题。为了解决经典单位容量模型的推广所带来的协调和容量发现挑战，作者提出了A-CAPELLA算法。该去中心化算法通过一个协作假设检验协议，利用精心设计的碰撞模式，实现了同步的逐次消除和容量估计，并最终在该广义机制下实现了对数遗憾，证明了其在无感知MMAB中的高效学习能力。", "keywords": "多臂老虎机, 去中心化学习, 容量感知, 协作假设检验, 对数遗憾", "comments": "该论文的创新点在于提出了一个在去中心化、无感知且臂容量未知MMAB设置下的有效算法。其协作假设检验协议巧妙地利用了碰撞模式来实现容量估计和同步消除，克服了严格反馈限制下的协调难题，为多玩家学习系统提供了新的理论和算法基础。"}}
{"id": "2506.10693", "title": "Towards Sustainable Computing: Exploring Energy Consumption Efficiency of Alternative Configurations and Workloads in an Open Source Messaging System", "authors": ["Maria Voreakou", "George Kousiouris", "Mara Nikolaidou"], "summary": "Energy consumption in current large scale computing infrastructures is\nbecoming a critical issue, especially with the growing demand for centralized\nsystems such as cloud environments. With the advancement of microservice\narchitectures and the Internet of Things, messaging systems have become an\nintegral and mainstream part of modern computing infrastructures, carrying out\nsignificant workload in a majority of applications. In this paper, we describe\nan experimental process to explore energy-based benchmarking for RabbitMQ, one\nof the main open source messaging frameworks. The involved system is described,\nas well as required components, and setup scenarios, involving different\nworkloads and configurations among the tests as well as messaging system use\ncases. Alternative architectures are investigated and compared from an energy\nconsumption point of view, for different message rates and consumer numbers.\nDifferences in architectural selection have been quantified and can lead to up\nto 31\\% reduction in power consumption. The resulting dataset is made publicly\navailable and can thus prove helpful for architectures' comparison,\nenergy-based cost modeling, and beyond.", "comment": "2025 20th Annual System of Systems Engineering Conference (SoSE)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10693v1", "AI": {"title_translation": "迈向可持续计算：探索开源消息系统中替代配置和工作负载的能耗效率", "tldr": "本文通过对开源消息系统RabbitMQ进行能耗基准测试，发现通过选择不同的架构和配置，可显著降低系统能耗，最高可达31%。", "motivation": "当前大规模计算基础设施（特别是云环境）的能耗问题日益严峻，而消息系统作为现代计算基础设施的关键组成部分，承载大量工作负载，因此优化其能耗至关重要。", "method": "研究团队描述了一个实验过程，对开源消息框架RabbitMQ进行了基于能耗的基准测试。实验涉及描述系统、所需组件和设置场景，并在不同工作负载、配置、消息速率和消费者数量下，调查并比较了替代架构的能耗。", "result": "研究量化了架构选择对能耗的影响，结果显示能耗最高可降低31%。研究生成的数据集已公开可用，可用于架构比较和基于能耗的成本建模。", "conclusion": "架构选择对消息系统的能耗效率有显著影响，合理选择可以显著降低功耗。公开的数据集有助于架构比较和基于能耗的成本建模。", "translation": "当前大规模计算基础设施的能耗正成为一个关键问题，尤其随着对云环境等集中式系统需求的增长。随着微服务架构和物联网的进步，消息系统已成为现代计算基础设施不可或缺的主流组成部分，在大多数应用程序中承担着重要的工作负载。在本文中，我们描述了一个实验过程，旨在探索针对RabbitMQ（主要的开源消息框架之一）的基于能耗的基准测试。文中描述了所涉及的系统、所需组件和设置场景，其中包含了测试中不同的工作负载和配置，以及消息系统的用例。从能耗角度对不同消息速率和消费者数量下的替代架构进行了调查和比较。架构选择上的差异已被量化，可导致高达31%的功耗降低。生成的数据集已公开可用，因此可证明对架构比较、基于能耗的成本建模及其他方面有所帮助。", "summary": "本文旨在解决大规模计算基础设施日益增长的能耗问题，特别是消息系统。研究团队对开源消息系统RabbitMQ进行了能耗基准测试，通过实验比较了不同配置、工作负载和替代架构下的能耗效率。结果表明，通过优化架构选择，能耗可降低高达31%。研究还公开了相关数据集，以促进未来的能耗分析和架构优化。", "keywords": "能耗, 消息系统, RabbitMQ, 基准测试, 可持续计算", "comments": "本文的创新点在于量化了消息系统架构选择对能耗的具体影响，并公开了实验数据集，这对于推动可持续计算和优化云环境中消息系统的能耗具有重要意义。研究结果为开发者和架构师提供了实用的指导，以构建更节能的系统。"}}
{"id": "2506.10408", "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges", "authors": ["Jintao Liang", "Gang Su", "Huifeng Lin", "You Wu", "Rui Zhao", "Ziyue Li"], "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\novercome the knowledge limitations of Large Language Models (LLMs) by\nintegrating external retrieval with language generation. While early RAG\nsystems based on static pipelines have shown effectiveness in well-structured\ntasks, they struggle in real-world scenarios requiring complex reasoning,\ndynamic retrieval, and multi-modal integration. To address these challenges,\nthe field has shifted toward Reasoning Agentic RAG, a paradigm that embeds\ndecision-making and adaptive tool use directly into the retrieval process. In\nthis paper, we present a comprehensive review of Reasoning Agentic RAG methods,\ncategorizing them into two primary systems: predefined reasoning, which follows\nfixed modular pipelines to boost reasoning, and agentic reasoning, where the\nmodel autonomously orchestrates tool interaction during inference. We analyze\nrepresentative techniques under both paradigms, covering architectural design,\nreasoning strategies, and tool coordination. Finally, we discuss key research\nchallenges and propose future directions to advance the flexibility,\nrobustness, and applicability of reasoning agentic RAG systems. Our collection\nof the relevant research has been organized into a\nhttps://github.com/ByebyeMonica/Reasoning-Agentic-RAG.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10408v1", "AI": {"title_translation": "通过系统1或系统2进行推理RAG：推理代理式检索增强生成在工业挑战中的调查", "tldr": "本文综述了推理代理式RAG方法，将其分为预定义推理和代理式推理两大类，并讨论了未来的研究方向。", "motivation": "早期RAG系统在复杂推理、动态检索和多模态集成方面存在不足，无法应对真实世界的工业挑战，因此需要转向推理代理式RAG。", "method": "本文对推理代理式RAG方法进行了全面综述，将其分为预定义推理和代理式推理两大系统，并分析了这两种范式下的代表性技术，包括架构设计、推理策略和工具协调。", "result": "论文将推理代理式RAG方法分为两大类：遵循固定模块化管道的预定义推理和模型自主协调工具交互的代理式推理。并分析了这两种范式下的代表性技术。", "conclusion": "论文讨论了关键研究挑战并提出了未来方向，以提高推理代理式RAG系统的灵活性、鲁棒性和适用性。", "translation": "检索增强生成（RAG）已成为一个强大的框架，通过将外部检索与语言生成相结合，克服了大型语言模型（LLM）的知识限制。虽然基于静态管道的早期RAG系统在结构良好的任务中显示出有效性，但它们在需要复杂推理、动态检索和多模态集成的真实世界场景中举步维艰。为了解决这些挑战，该领域已转向推理代理式RAG，这是一种将决策和自适应工具使用直接嵌入到检索过程中的范式。在本文中，我们对推理代理式RAG方法进行了全面综述，将其分为两个主要系统：预定义推理（遵循固定的模块化管道以增强推理）和代理式推理（模型在推理过程中自主协调工具交互）。我们分析了这两种范式下的代表性技术，涵盖了架构设计、推理策略和工具协调。最后，我们讨论了关键研究挑战并提出了未来的方向，以提高推理代理式RAG系统的灵活性、鲁棒性和适用性。我们收集的相关研究已整理至 https://github.com/ByebyeMonica/Reasoning-Agentic-RAG。", "summary": "本文对推理代理式检索增强生成（RAG）方法进行了全面综述，旨在解决传统RAG在复杂推理和动态场景中的局限性。作者将推理代理式RAG分为预定义推理和代理式推理两大类，并深入分析了其架构、推理策略和工具协调。论文还探讨了当前的研究挑战并展望了未来的发展方向，以提升RAG系统的适应性和性能。", "keywords": "检索增强生成, 推理代理, 大型语言模型, 综述, 工业挑战", "comments": "这篇综述论文及时地总结了推理代理式RAG这一新兴领域，将复杂的概念清晰地划分为“预定义推理”和“代理式推理”两种系统，有助于研究人员理解和进入该领域。它不仅分析了现有技术，还指出了未来的研究方向，对于推动RAG技术在工业应用中的发展具有重要指导意义。"}}
{"id": "2506.10916", "title": "Semi-Automated Quality Assurance in Digital Pathology: Tile Classification Approach", "authors": ["Meredith VandeHaar", "M. Clinch", "I. Yilmaz", "M. A. Rahman", "Y. Xiao", "F. Dogany", "H. M. Alazab", "A. Nassar", "Z. Akkus", "B. Dangott"], "summary": "Quality assurance is a critical but underexplored area in digital pathology,\nwhere even minor artifacts can have significant effects. Artifacts have been\nshown to negatively impact the performance of AI diagnostic models. In current\npractice, trained staff manually review digitized images prior to release of\nthese slides to pathologists which are then used to render a diagnosis.\nConventional image processing approaches, provide a foundation for detecting\nartifacts on digital pathology slides. However, current tools do not leverage\ndeep learning, which has the potential to improve detection accuracy and\nscalability. Despite these advancements, methods for quality assurance in\ndigital pathology remain limited, presenting a gap for innovation.\n  We propose an AI algorithm designed to screen digital pathology slides by\nanalyzing tiles and categorizing them into one of 10 predefined artifact types\nor as background. This algorithm identifies and localizes artifacts, creating a\nmap that highlights regions of interest. By directing human operators to\nspecific tiles affected by artifacts, the algorithm minimizes the time and\neffort required to manually review entire slides for quality issues.\n  From internal archives and The Cancer Genome Atlas, 133 whole slide images\nwere selected and 10 artifacts were annotated using an internally developed\nsoftware ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple\nmodels at different tile sizes and magnification was performed. InceptionResNet\nwas selected. Single artifact models were trained and tested, followed by a\nlimited multiple instance model with artifacts that performed well together\n(chatter, fold, and pen). From the results of this study we suggest a hybrid\ndesign for artifact screening composed of both single artifact binary models as\nwell as multiple instance models to optimize detection of each artifact.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10916v1", "AI": {"title_translation": "数字病理学中的半自动化质量保证：瓦片分类方法", "tldr": "本文提出了一种基于深度学习的AI算法，通过对数字病理图像瓦片进行分类来半自动化地检测和定位伪影，从而提高质量保证的效率。", "motivation": "数字病理学中质量保证是一个关键但未被充分探索的领域，即使是微小的伪影也会显著影响AI诊断模型的性能。现有手动审查和传统图像处理方法效率低下且未利用深度学习的潜力，存在创新空白。", "method": "提出了一种AI算法，通过分析数字病理图像瓦片并将其归类为10种预定义伪影类型之一或背景来筛选图像。该算法识别并定位伪影，生成高亮显示感兴趣区域的地图。使用内部档案和The Cancer Genome Atlas的133张全玻片图像，并用ZAPP软件标注了10种伪影。进行了不同瓦片大小和放大倍数的模型消融研究，最终选择InceptionResNet。训练和测试了单一伪影模型，随后是有限的多实例模型（针对chatter、fold和pen）。", "result": "研究结果表明，建议采用单一伪影二元模型和多实例模型相结合的混合设计来优化每种伪影的检测。", "conclusion": "本文建议采用混合设计（结合单一伪影二元模型和多实例模型）进行伪影筛查，以优化数字病理学中各种伪影的检测，从而提高质量保证的效率。", "translation": "数字病理学中的质量保证是一个关键但未被充分探索的领域，即使是微小的伪影也会产生显著影响。伪影已被证明会对AI诊断模型的性能产生负面影响。在当前实践中，训练有素的工作人员在将数字化图像发布给病理学家之前手动审查这些图像，然后病理学家用这些图像进行诊断。传统的图像处理方法为检测数字病理玻片上的伪影提供了基础。然而，目前的工具没有利用深度学习，而深度学习有潜力提高检测准确性和可扩展性。尽管有这些进展，数字病理学中的质量保证方法仍然有限，存在创新空白。\n我们提出了一种AI算法，旨在通过分析瓦片并将其归类为10种预定义伪影类型之一或作为背景来筛选数字病理玻片。该算法识别并定位伪影，创建突出显示感兴趣区域的地图。通过将操作员引导至受伪影影响的特定瓦片，该算法最大限度地减少了手动审查整个玻片以查找质量问题所需的时间和精力。\n从内部档案和癌症基因组图谱中，选择了133张全玻片图像，并使用内部开发的ZAPP软件（Mayo Clinic，Jacksonville，FL）标注了10种伪影。在不同瓦片大小和放大倍数下对多个模型进行了消融研究。选择了InceptionResNet。训练和测试了单一伪影模型，随后是有限的多实例模型，其中伪影表现良好（颤动、折叠和笔迹）。根据本研究的结果，我们建议采用由单一伪影二元模型和多实例模型组成的混合设计进行伪影筛查，以优化每种伪影的检测。", "summary": "本研究旨在解决数字病理学中质量保证的不足，该领域中的伪影会严重影响AI诊断模型。文章提出了一种基于深度学习的AI算法，通过将数字病理图像瓦片分类为10种预定义伪影类型或背景，从而实现伪影的自动识别和定位，并生成伪影地图以引导人工审查。研究使用了133张全玻片图像进行标注和模型训练，并进行了消融研究，最终选择InceptionResNet。结果表明，结合单一伪影二元模型和多实例模型的混合设计能有效优化伪影检测，从而提高数字病理图像质量保证的效率和准确性。", "keywords": "数字病理学, 质量保证, 深度学习, 伪影检测, 瓦片分类", "comments": "本文创新性地将深度学习应用于数字病理图像的质量保证，通过瓦片分类实现了伪影的半自动化检测和定位，这显著提高了效率并减少了人工审查的负担。其提出的混合模型设计，结合了单一伪影和多实例模型，为未来的伪影检测提供了优化的策略。该方法对于提高数字病理AI诊断模型的可靠性具有重要意义。"}}
{"id": "2506.10202", "title": "Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval", "authors": ["Shubhashis Roy Dipta", "Francis Ferraro"], "summary": "Recent approaches have shown impressive proficiency in extracting and\nleveraging parametric knowledge from Large-Language Models (LLMs) and\nVision-Language Models (VLMs). In this work, we consider how we can improve the\nidentification and retrieval of videos related to complex real-world events by\nautomatically extracting latent parametric knowledge about those events. We\npresent Q2E: a Query-to-Event decomposition method for zero-shot multilingual\ntext-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our\napproach demonstrates that we can enhance the understanding of otherwise overly\nsimplified human queries by decomposing the query using the knowledge embedded\nin LLMs and VLMs. We additionally show how to apply our approach to both visual\nand speech-based inputs. To combine this varied multimodal knowledge, we adopt\nentropy-based fusion scoring for zero-shot fusion. Through evaluations on two\ndiverse datasets and multiple retrieval metrics, we demonstrate that Q2E\noutperforms several state-of-the-art baselines. Our evaluation also shows that\nintegrating audio information can significantly improve text-to-video\nretrieval. We have released code and data for future research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10202v1", "AI": {"title_translation": "Q2E：面向零样本多语言文本到视频检索的查询到事件分解", "tldr": "Q2E通过利用大型语言模型和视觉语言模型中的知识来分解复杂查询，显著提升了零样本多语言文本到视频检索的性能，并证明了整合音频信息能进一步提高检索效果。", "motivation": "本研究旨在通过自动提取有关复杂真实世界事件的潜在参数知识，改进相关视频的识别和检索。它旨在通过分解人类查询来增强对过度简化查询的理解，从而提高文本到视频检索的效率。", "method": "本研究提出了Q2E：一种面向零样本多语言文本到视频检索的查询到事件分解方法。该方法利用大型语言模型（LLMs）和视觉语言模型（VLMs）中嵌入的知识来分解查询，以增强对查询的理解。它适用于视觉和基于语音的输入，并采用基于熵的融合评分进行零样本多模态知识融合。", "result": "Q2E在两个不同数据集和多个检索指标上均优于多个最先进的基线方法。评估还表明，整合音频信息可以显著改善文本到视频检索的效果。", "conclusion": "本研究展示了Q2E方法在零样本多语言文本到视频检索方面的有效性，通过查询到事件的分解和多模态知识融合，显著提升了复杂事件的视频检索性能，并强调了音频信息在其中的重要作用。", "translation": "最近的方法在从大型语言模型（LLMs）和视觉语言模型（VLMs）中提取和利用参数知识方面表现出令人印象深刻的熟练程度。在这项工作中，我们考虑如何通过自动提取关于复杂真实世界事件的潜在参数知识，来改进这些事件相关视频的识别和检索。我们提出了Q2E：一种面向零样本多语言文本到视频检索的查询到事件分解方法，该方法可适应不同数据集、领域、LLMs或VLMs。我们的方法表明，通过利用LLMs和VLMs中嵌入的知识分解查询，我们可以增强对原本过于简化的人类查询的理解。我们还展示了如何将我们的方法应用于视觉和基于语音的输入。为了结合这种多样化的多模态知识，我们采用了基于熵的融合评分进行零样本融合。通过在两个不同数据集和多个检索指标上的评估，我们证明Q2E优于几个最先进的基线方法。我们的评估还表明，整合音频信息可以显著改善文本到视频检索。我们已经发布了代码和数据以供未来研究。", "summary": "本论文提出了Q2E，一种新颖的查询到事件分解方法，旨在提升零样本多语言文本到视频检索的性能。该方法利用大型语言模型和视觉语言模型中的知识来分解并深化对复杂查询的理解，从而改进对真实世界事件相关视频的检索。Q2E能够处理视觉和语音输入，并通过基于熵的融合策略整合多模态信息。实验结果表明，Q2E超越了现有基线，并且整合音频信息对文本到视频检索具有显著的改进作用。", "keywords": "查询到事件分解, 文本到视频检索, 零样本, 多模态融合, 大型语言模型", "comments": "Q2E的创新之处在于其独特的查询到事件分解机制，通过利用LLM和VLM的强大知识来处理复杂查询，这对于提升零样本多语言文本到视频检索的精度至关重要。此外，该方法对视觉和语音输入的多模态融合处理，特别是通过熵基融合评分，展现了其在整合多样化信息方面的能力，并实际证明了音频信息对检索效果的显著提升，为未来的多模态检索研究提供了有价值的方向。"}}
{"id": "2506.10380", "title": "TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning", "authors": ["Xiaohan Yu", "Pu Jian", "Chong Chen"], "summary": "Retrieval-Augmented Generation (RAG) has demonstrated considerable\neffectiveness in open-domain question answering. However, when applied to\nheterogeneous documents, comprising both textual and tabular components,\nexisting RAG approaches exhibit critical limitations. The prevailing practice\nof flattening tables and chunking strategies disrupts the intrinsic tabular\nstructure, leads to information loss, and undermines the reasoning capabilities\nof LLMs in multi-hop, global queries. To address these challenges, we propose\nTableRAG, an hybrid framework that unifies textual understanding and complex\nmanipulations over tabular data. TableRAG iteratively operates in four steps:\ncontext-sensitive query decomposition, text retrieval, SQL programming and\nexecution, and compositional intermediate answer generation. We also develop\nHeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous\nreasoning capabilities. Experimental results demonstrate that TableRAG\nconsistently outperforms existing baselines on both public datasets and our\nHeteQA, establishing a new state-of-the-art for heterogeneous document question\nanswering. We release TableRAG at https://github.com/yxh-y/TableRAG/tree/main.", "comment": "Under review. Codes are available at\n  https://github.com/yxh-y/TableRAG/tree/main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10380v1", "AI": {"title_translation": "TableRAG：一种用于异构文档推理的检索增强生成框架", "tldr": "TableRAG是一个针对包含文本和表格的异构文档的检索增强生成（RAG）框架，通过结合文本理解和表格操作，解决了现有RAG在处理此类文档时的信息丢失和推理能力不足的问题，并在异构文档问答中取得了最先进的性能。", "motivation": "现有的检索增强生成（RAG）方法在处理包含文本和表格的异构文档时存在局限性。将表格扁平化和分块的策略会破坏固有的表格结构，导致信息丢失，并削弱大型语言模型（LLMs）在多跳、全局查询中的推理能力。", "method": "本文提出了TableRAG，一个混合框架，它统一了文本理解和对表格数据的复杂操作。TableRAG以四个步骤迭代运行：上下文敏感查询分解、文本检索、SQL编程和执行，以及组合式中间答案生成。此外，还开发了一个名为HeteQA的新基准来评估多跳异构推理能力。", "result": "实验结果表明，TableRAG在公共数据集和HeteQA基准测试上都持续优于现有基线，在异构文档问答方面建立了新的最先进水平。", "conclusion": "TableRAG通过其混合框架有效解决了异构文档推理中的挑战，并在该领域取得了显著的性能提升，证明了其在处理复杂文档类型方面的优越性。", "translation": "检索增强生成（RAG）在开放域问答中表现出相当大的有效性。然而，当应用于包含文本和表格组件的异构文档时，现有的RAG方法表现出关键的局限性。将表格扁平化和分块的普遍做法破坏了固有的表格结构，导致信息丢失，并削弱了大型语言模型（LLMs）在多跳、全局查询中的推理能力。为了解决这些挑战，我们提出了TableRAG，一个混合框架，它统一了文本理解和对表格数据的复杂操作。TableRAG以四个步骤迭代运行：上下文敏感查询分解、文本检索、SQL编程和执行，以及组合式中间答案生成。我们还开发了HeteQA，一个旨在评估多跳异构推理能力的新基准。实验结果表明，TableRAG在公共数据集和我们的HeteQA上都持续优于现有基线，在异构文档问答方面建立了新的最先进水平。我们在https://github.com/yxh-y/TableRAG/tree/main发布了TableRAG。", "summary": "本文提出了TableRAG，一个针对包含文本和表格的异构文档的检索增强生成（RAG）框架。针对现有RAG在处理此类文档时因表格结构破坏导致的信息丢失和推理能力不足的问题，TableRAG结合了文本理解和表格数据操作，并采用上下文敏感查询分解、文本检索、SQL编程和执行、以及组合式中间答案生成这四个迭代步骤。此外，研究人员还开发了HeteQA基准来评估多跳异构推理能力。实验证明，TableRAG在现有数据集和HeteQA上均优于基线，达到了异构文档问答的最先进水平。", "keywords": "检索增强生成, 异构文档, 表格推理, 多跳问答, TableRAG", "comments": "TableRAG的创新之处在于其针对异构文档（文本和表格）的独特处理方式，通过结合文本理解和SQL操作，有效解决了现有RAG方法在处理复杂表格结构时遇到的信息丢失和推理挑战。其提出的迭代四步法和HeteQA基准对于推动异构文档推理领域的研究具有重要意义，并为未来的RAG系统设计提供了新的思路。"}}
{"id": "2506.10723", "title": "Semi-discrete moduli of smoothness and their applications in one- and two- sided error estimates", "authors": ["Danilo Costarelli", "Donato Lavella"], "summary": "In this paper, we introduce a new semi-discrete modulus of smoothness, which\ngeneralizes the definition given by Kolomoitsev and Lomako (KL) in 2023 (in the\npaper published in the J. Approx. Theory), and we establish very general one-\nand two- sided error estimates under non-restrictive assumptions. The proposed\nresults have been proved exploiting the regularization and approximation\nproperties of certain Steklov integrals introduced by Sendov and Popov in 1983,\nand differ from the ones given by Kolomoitsev and Lomako. In addition, the\nproof of the original KL approximation theorems were strictly related to the\napplication of certain classical results of the trigonometric best\napproximation, and thus, they are applicable only for operators of the\ntrigonometric type. By the definition of semi-discrete moduli of smoothness\nhere proposed, we are able to deduce applications also for operators that are\nnot necessarily of the trigonometric type, and can also be used to derive\nsharper estimates than those that can be achieved by the classical averaged\nmoduli of smoothness ($\\tau$-moduli). Furthermore, a Rathore-type theorem is\nestablished, and a new notion of K-functional is also introduced showing its\nequivalence with the semi-discrete modulus of smoothness and its realization.\nOne-sided estimates of approximation can be established for classical operators\non bounded domains, such as the Bernstein polynomials. In the case of\napproximation operators on the whole real line, one-sided estimates can be\nachieved, e.g., for the Shannon sampling (cardinal) series, as well as for the\nso-called generalized sampling operators. At the end of the paper, the case of\nalgebraic Lagrange approximation has been considered, showing the main open\nproblems in order to derive two-sided error estimates in this noteworthy case.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10723v1", "AI": {"title_translation": "半离散光滑模及其在单边和双边误差估计中的应用", "tldr": "本文引入了一种新的半离散光滑模，推广了现有定义，并建立了在非限制性假设下的通用单边和双边误差估计，适用于更广泛的算子类型并能获得更精确的估计。", "motivation": "推广Kolomoitsev和Lomako（KL）在2023年提出的半离散光滑模的定义，并克服其仅适用于三角型算子的限制，以建立更通用、更精确的单边和双边误差估计。", "method": "利用Sendov和Popov在1983年引入的某些Steklov积分的正则化和逼近性质进行证明。引入了新的K-泛函并证明了其与半离散光滑模及其实现之间的等价性。", "result": "建立了非常通用的单边和双边误差估计，适用于非三角型算子，并能导出比经典平均光滑模（$\\tau$-模）更尖锐的估计。建立了一个Rathore型定理。证明了新的K-泛函与半离散光滑模的等价性。在有界域上，可为伯恩斯坦多项式等经典算子建立单边逼近估计。在整个实数线上，可为香农采样（基数）级数和广义采样算子实现单边估计。", "conclusion": "本文提出的半离散光滑模推广了现有定义，并在非限制性假设下提供了更通用、更精确的单边和双边误差估计，适用于更广泛的算子类型。尽管在代数拉格朗日逼近的双边误差估计方面仍存在主要开放问题，但该方法显示出广泛的应用潜力。", "translation": "在本文中，我们引入了一种新的半离散光滑模，它推广了Kolomoitsev和Lomako（KL）在2023年（发表在J. Approx. Theory上的论文中）给出的定义，并且我们在非限制性假设下建立了非常通用的单边和双边误差估计。所提出的结果是利用Sendov和Popov在1983年引入的某些Steklov积分的正则化和逼近性质来证明的，并且与Kolomoitsev和Lomako给出的结果不同。此外，原始KL逼近定理的证明与三角最佳逼近的某些经典结果的应用严格相关，因此，它们仅适用于三角型算子。通过本文提出的半离散光滑模的定义，我们也能够推导出适用于不一定是三角型算子的应用，并且还可以用于推导出比经典平均光滑模（$\\tau$-模）所能达到的更尖锐的估计。此外，还建立了一个Rathore型定理，并引入了一个新的K-泛函概念，显示了其与半离散光滑模及其实现之间的等价性。对于有界域上的经典算子，例如伯恩斯坦多项式，可以建立单边逼近估计。在整个实数线上的逼近算子的情况下，可以实现单边估计，例如，对于香农采样（基数）级数，以及所谓的广义采样算子。在论文的最后，考虑了代数拉格朗日逼近的情况，指出了在该值得注意的情况下推导双边误差估计的主要开放问题。", "summary": "本文引入了一种新的半离散光滑模，该模推广了Kolomoitsev和Lomako的定义。研究建立了在非限制性假设下通用的单边和双边误差估计，并通过利用Steklov积分的性质进行证明。与现有方法相比，该模不仅适用于非三角型算子，还能提供更精确的估计。文章还建立了Rathore型定理，并引入了新的K-泛函，证明了其等价性。新方法可应用于伯恩斯坦多项式、香农采样级数等经典算子，并讨论了代数拉格朗日逼近中双边误差估计的开放问题。", "keywords": "半离散光滑模, 误差估计, Steklov积分, K-泛函, 逼近理论", "comments": "该论文的创新之处在于引入了一种更通用的半离散光滑模，突破了现有方法在算子类型上的限制，使其能应用于非三角型算子。此外，它能提供比经典方法更精确的估计，这在逼近理论中具有重要意义。论文还指出了代数拉格朗日逼近中的开放问题，为未来的研究指明了方向。"}}
{"id": "2506.10705", "title": "A Novel Signal Processing Strategy for Short-Range Laser Feedback Interferometry Sensors", "authors": ["Alexander Zimmer", "Johannes Meyer", "Enkelejda Kasneci"], "summary": "The rapid evolution of wearable technologies, such as AR glasses, demands\ncompact, energy-efficient sensors capable of high-precision measurements in\ndynamic environments. Traditional Frequency-Modulated Continuous Wave (FMCW)\nLaser Feedback Interferometry (LFI) sensors, while promising, falter in\napplications that feature small distances, high velocities, shallow modulation,\nand low-power constraints. We propose a novel sensor-processing pipeline that\nreliably extracts distance and velocity measurements at distances as low as 1\ncm. As a core contribution, we introduce a four-ramp modulation scheme that\nresolves persistent ambiguities in beat frequency signs and overcomes spectral\nblind regions caused by hardware limitations. Based on measurements of the\nimplemented pipeline, a noise model is defined to evaluate its performance and\nsensitivity to several algorithmic and working point parameters. We show that\nthe pipeline generally achieves robust and low-noise measurements using\nstate-of-the-art hardware.", "comment": "Accepted to the 2025 25th International Conference on Digital Signal\n  Processing", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10705v1", "AI": {"title_translation": "一种用于短距离激光反馈干涉传感器的新型信号处理策略", "tldr": "针对短距离LFI传感器在小距离、高速、浅调制和低功耗应用中的局限性，本文提出一种新型信号处理策略，通过四斜坡调制方案解决了拍频符号模糊和频谱盲区问题，实现了1厘米距离的可靠测量。", "motivation": "现有FMCW LFI传感器在小距离、高速度、浅调制和低功耗应用中表现不佳，无法满足AR眼镜等可穿戴技术对紧凑、节能、高精度短距离传感器的需求。", "method": "提出一种新型传感器处理流程，核心是引入四斜坡调制方案，以解决拍频符号模糊和硬件限制导致的频谱盲区问题。基于已实现的流程测量，定义了一个噪声模型来评估其性能和敏感性。", "result": "该处理流程能够可靠地提取低至1厘米的距离和速度测量值。使用现有硬件实现了鲁棒和低噪声的测量。", "conclusion": "该新型信号处理策略能够有效解决短距离LFI传感器面临的问题，实现高精度、鲁棒的测量，满足可穿戴技术等新兴应用的需求。", "translation": "可穿戴技术（如AR眼镜）的快速发展要求紧凑、节能且能在动态环境中进行高精度测量的传感器。传统的调频连续波（FMCW）激光反馈干涉（LFI）传感器虽然前景广阔，但在小距离、高速度、浅调制和低功耗的应用中表现不佳。我们提出了一种新颖的传感器处理流程，能够可靠地提取低至1厘米距离的距离和速度测量值。作为核心贡献，我们引入了一种四斜坡调制方案，解决了拍频频率符号的持续模糊性并克服了由硬件限制引起的频谱盲区。基于已实现的流程测量，定义了一个噪声模型来评估其性能以及对几种算法和工作点参数的敏感性。我们表明，该流程通常使用最先进的硬件实现了鲁棒和低噪声的测量。", "summary": "本文针对现有FMCW激光反馈干涉（LFI）传感器在短距离、高速、低功耗应用中的局限性，提出了一种新型信号处理策略。该策略核心在于引入四斜坡调制方案，有效解决了拍频符号模糊和硬件限制导致的频谱盲区问题。实验验证了该方案能够可靠地在1厘米的超短距离下提取距离和速度信息，并实现了鲁棒且低噪声的测量。", "keywords": "激光反馈干涉, 短距离传感, 信号处理, 四斜坡调制, 可穿戴技术", "comments": "本文的创新点在于提出了针对短距离激光反馈干涉传感器的新型信号处理策略，特别是四斜坡调制方案，有效解决了传统LFI传感器在特定应用场景下的关键痛点，如拍频符号模糊和频谱盲区。这对于推动AR眼镜等可穿戴设备的短距离高精度传感技术发展具有重要意义。"}}
{"id": "2506.10562", "title": "Joint System Modeling Approach for Fault Simulation of Start-er/Generator and Gas Generator in All-Electric APU", "authors": ["Haotian Mao", "Yingqing Guo"], "summary": "This paper presents a joint system modeling approach for fault simulation of\nall-electric auxiliary power unit (APU), integrating starter/generator\nturn-to-turn short circuit (TTSC) faults with gas generator gas-path faults.To\naddress challenges in electromechanical coupling, simulation precision and\ncomputational efficiency balance, we propose a multi-rate continuous-discrete\nhybrid simulation architecture. This architecture treats the starter/generator\nas a continuous system with variable step size in Simulink, while modeling the\ngas generator as a discrete system with fixed step size in a dynamic-link\nlibrary (DLL) environment. For the starter/generator fault modeling, a\nmulti-loop approach is deployed to accurately simulate TTSC faults. For the gas\ngenerator, we develop an improved GasTurb-DLL modeling method (IGDM) that\nenhances uncertainty modeling, state-space representation, and tool chain\ncompatibility. Finally, the proposed methodology above was implemented in a\ncase study based on the APS5000 all-electric APU structure and parameters.\nModel validation was conducted by comparing simulation results--covering\nsteady-state, transients, healthy, and fault conditions--with reference data\nfrom third-party software and literature. The close agreement confirms both the\nmodel's accuracy and the effectiveness of our modeling methodology. This work\nestablishes a modeling foundation for investigating the opportunities and\nchallenges in fault detection and isolation (FDI) brought by the all\nelectrification of the APU, including joint fault estimation and diagnosis,\ncoupled electromechanical fault characteristics.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10562v1", "AI": {"title_translation": "全电APU起动/发电机与燃气发生器故障仿真联合系统建模方法", "tldr": "本文提出了一种用于全电APU故障仿真的联合系统建模方法，通过多速率连续-离散混合仿真架构和特定故障建模技术，解决了机电耦合、精度和效率的挑战，并在APS5000 APU上进行了验证。", "motivation": "为了解决全电APU故障仿真中机电耦合、仿真精度和计算效率平衡方面的挑战。", "method": "提出了一种多速率连续-离散混合仿真架构：将起动/发电机建模为Simulink中具有可变步长的连续系统，并采用多回路方法模拟匝间短路故障；将燃气发生器建模为DLL环境中具有固定步长的离散系统，并开发了改进的GasTurb-DLL建模方法（IGDM）以增强不确定性建模、状态空间表示和工具链兼容性。该方法在APS5000全电APU结构和参数的案例研究中得以实施和验证。", "result": "模型验证结果显示，仿真结果（涵盖稳态、瞬态、健康和故障条件）与第三方软件和文献中的参考数据高度一致，证实了模型的准确性和所提出建模方法的有效性。", "conclusion": "这项工作为研究全电APU带来的故障检测与隔离（FDI）中的机遇和挑战（包括联合故障估计与诊断、耦合机电故障特性）奠定了建模基础。", "translation": "本文提出了一种全电辅助动力装置（APU）故障仿真的联合系统建模方法，该方法将起动/发电机匝间短路（TTSC）故障与燃气发生器气路故障相结合。为解决机电耦合、仿真精度和计算效率平衡方面的挑战，我们提出了一种多速率连续-离散混合仿真架构。该架构将起动/发电机视为Simulink中具有可变步长的连续系统，而将燃气发生器建模为动态链接库（DLL）环境中具有固定步长的离散系统。对于起动/发电机故障建模，采用多回路方法精确模拟TTSC故障。对于燃气发生器，我们开发了一种改进的GasTurb-DLL建模方法（IGDM），该方法增强了不确定性建模、状态空间表示和工具链兼容性。最后，将上述方法应用于基于APS5000全电APU结构和参数的案例研究。通过将仿真结果（涵盖稳态、瞬态、健康和故障条件）与第三方软件和文献中的参考数据进行比较，进行了模型验证。结果的紧密一致性证实了模型的准确性和我们建模方法的有效性。这项工作为研究全电APU带来的故障检测与隔离（FDI）中的机遇和挑战（包括联合故障估计与诊断、耦合机电故障特性）奠定了建模基础。", "summary": "本文提出了一种全电辅助动力装置（APU）故障仿真的联合系统建模方法，该方法整合了起动/发电机匝间短路故障与燃气发生器气路故障。为解决机电耦合、仿真精度和计算效率平衡的挑战，文章引入了一种多速率连续-离散混合仿真架构。该架构将起动/发电机视为Simulink中的连续系统，而将燃气发生器建模为DLL环境中的离散系统，并分别采用了多回路方法和改进的GasTurb-DLL建模方法（IGDM）进行故障建模。该方法在APS5000全电APU案例中得到验证，仿真结果与参考数据高度一致，证明了模型的准确性和方法的有效性。这项工作为未来全电APU的故障检测与隔离研究奠定了建模基础。", "keywords": "故障仿真, 全电APU, 混合仿真, 起动/发电机, 燃气发生器", "comments": "本文的创新之处在于提出了多速率连续-离散混合仿真架构，有效地平衡了复杂机电耦合系统的仿真精度和计算效率。针对起动/发电机（多回路）和燃气发生器（IGDM）的特定故障建模技术也值得关注。这项工作为全电APU领域新兴的故障检测与隔离研究提供了坚实的建模基础，具有重要意义。"}}
{"id": "2506.10484", "title": "EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair", "authors": ["Fangwen Mu", "Junjie Wang", "Lin Shi", "Song Wang", "Shoubin Li", "Qing Wang"], "summary": "Automatically repairing software issues remains a fundamental challenge at\nthe intersection of software engineering and AI. Although recent advancements\nin Large Language Models (LLMs) have demonstrated potential for\nrepository-level repair tasks, current methodologies exhibit two notable\nlimitations: (1) they often address issues in isolation, neglecting to\nincorporate insights from previously resolved issues, and (2) they rely on\nstatic and rigid prompting strategies, which constrain their ability to\ngeneralize across diverse and evolving issue scenarios. Inspired by the dual\nmemory systems of human cognition, where episodic and semantic memories work\nsynergistically to support human reasoning and decision-making, we propose\nExpeRepair, a novel LLM-based approach that continuously learns from historical\nrepair experiences through dual-channel knowledge accumulation. ExpeRepair\norganizes historical repair experiences into two complementary memories: an\nepisodic memory that stores concrete repair demonstrations, and a semantic\nmemory that encodes abstract reflective insights. At inference time, ExpeRepair\nactivates both memory systems by retrieving relevant demonstrations from\nepisodic memory and recalling high-level repair insights from semantic memory.\nIt further enhances adaptability through dynamic prompt composition,\nsynergistically integrating both memory types to replace static prompts with\ncontext-aware, experience-driven prompts. Experiments on the SWE-bench Lite\nbenchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with\nClaude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10484v1", "AI": {"title_translation": "EXPEREPAIR：双记忆增强型基于LLM的仓库级程序修复", "tldr": "ExpeRepair是一个基于LLM的程序修复方法，通过模拟人类双记忆系统学习历史修复经验，并在SWE-bench Lite上超越了现有SOTA开源方法。", "motivation": "现有的LLM程序修复方法存在两个局限性：1) 孤立处理问题，忽略历史修复经验；2) 依赖静态提示策略，泛化能力差。", "method": "提出ExpeRepair，一种基于LLM的方法，通过双通道知识积累持续学习历史修复经验。它将经验组织成剧集记忆（存储具体修复示例）和语义记忆（编码抽象反思见解）。在推理时，通过检索相关示例和回忆高级见解来激活记忆系统。通过动态提示组合增强适应性，将两种记忆类型协同整合，取代静态提示。", "result": "在SWE-bench Lite基准测试中，ExpeRepair使用Claude 3.7 Sonnet实现了49.3%的pass@1分数，优于所有最先进的开源方法。", "conclusion": "ExpeRepair通过模拟人类双记忆系统有效解决了LLM程序修复中的历史经验利用和泛化能力问题，并在基准测试中表现出色。", "translation": "自动修复软件问题仍然是软件工程和人工智能交叉领域的一个基本挑战。尽管大型语言模型（LLM）的最新进展在仓库级修复任务中展现出潜力，但当前的方法存在两个显著局限性：(1) 它们通常孤立地解决问题，忽略了整合来自先前已解决问题的见解；(2) 它们依赖于静态和僵化的提示策略，这限制了它们在多样化和不断演变的问题场景中的泛化能力。受人类认知的双记忆系统启发，其中情景记忆和语义记忆协同工作以支持人类推理和决策，我们提出了ExpeRepair，一种新颖的基于LLM的方法，通过双通道知识积累持续学习历史修复经验。ExpeRepair将历史修复经验组织成两种互补的记忆：存储具体修复演示的情景记忆，以及编码抽象反思见解的语义记忆。在推理时，ExpeRepair通过从情景记忆中检索相关演示并从语义记忆中回忆高级修复见解来激活这两个记忆系统。它通过动态提示组合进一步增强了适应性，协同整合两种记忆类型，用上下文感知、经验驱动的提示取代静态提示。在SWE-bench Lite基准测试上的实验表明，ExpeRepair在使用Claude 3.7 Sonnet时达到了49.3%的pass@1分数，超越了所有最先进的开源方法。", "summary": "ExpeRepair是一种新的基于LLM的仓库级程序修复方法，它借鉴人类双记忆系统，通过剧集记忆存储具体修复示例，通过语义记忆编码抽象见解，从而克服了现有方法忽略历史经验和泛化能力不足的局限。该方法通过动态提示组合整合两种记忆，并在SWE-bench Lite基准测试中取得了优于现有SOTA开源方法的性能。", "keywords": "程序修复, LLM, 双记忆系统, 仓库级, 动态提示", "comments": "这篇论文通过引入受人类双记忆系统启发的双通道知识积累机制，为基于LLM的程序修复带来了创新。它解决了现有方法在利用历史经验和动态适应性方面的不足，通过动态提示组合进一步提升了LLM在复杂修复任务中的泛化能力和效率。其在SWE-bench Lite上的优异表现证明了该方法的有效性和潜力。"}}
{"id": "2506.10932", "title": "Video-Mediated Emotion Disclosure: A Study of Mental Health Vlogging by People with Schizophrenia on YouTube", "authors": ["Jiaying Lizzy Liu", "Yan Zhang"], "summary": "Individuals with schizophrenia frequently experience intense emotions and\noften turn to vlogging as a medium for emotional expression. While previous\nresearch has predominantly focused on text based disclosure, little is known\nabout how individuals construct narratives around emotions and emotional\nexperiences in video blogs. Our study addresses this gap by analyzing 200\nYouTube videos created by individuals with schizophrenia. Drawing on media\nresearch and self presentation theories, we developed a visual analysis\nframework to disentangle these videos. Our analysis revealed diverse practices\nof emotion disclosure through both verbal and visual channels, highlighting the\ndynamic interplay between these modes of expression. We found that the\ndeliberate construction of visual elements, including environmental settings\nand specific aesthetic choices, appears to foster more supportive and engaged\nviewer responses. These findings underscore the need for future large scale\nquantitative research examining how visual features shape video mediated\ncommunication on social media platforms. Such investigations would inform the\ndevelopment of care centered video sharing platforms that better support\nindividuals managing illness experiences.", "comment": "10 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10932v1", "AI": {"title_translation": "视频介导的情绪披露：一项关于精神分裂症患者在YouTube上心理健康视频博客的研究", "tldr": "本研究分析了精神分裂症患者在YouTube上通过视频博客进行情绪披露的方式，发现视觉元素对观众互动有积极影响。", "motivation": "以往研究主要关注文本情绪披露，但对精神分裂症患者如何在视频博客中构建情绪叙事知之甚少。本研究旨在填补这一空白。", "method": "研究分析了200个由精神分裂症患者创建的YouTube视频，并运用媒体研究和自我呈现理论开发了一个视觉分析框架来解读这些视频。", "result": "分析揭示了通过言语和视觉渠道进行情绪披露的多种实践，强调了这些表达模式之间的动态相互作用。研究发现，环境设置和特定审美选择等视觉元素的刻意构建似乎能促进更具支持性和参与度的观众反应。", "conclusion": "这些发现强调了未来需要进行大规模定量研究，以检验视觉特征如何塑造社交媒体平台上的视频介导交流。此类调查将为开发以护理为中心的视频分享平台提供信息，从而更好地支持管理疾病体验的个体。", "translation": "精神分裂症患者经常经历强烈的情绪，并常转向视频博客作为情绪表达的媒介。虽然之前的研究主要集中在基于文本的披露上，但对于个体如何在视频博客中围绕情绪和情感体验构建叙事知之甚少。我们的研究通过分析200个由精神分裂症患者创建的YouTube视频来解决这一空白。借鉴媒体研究和自我呈现理论，我们开发了一个视觉分析框架来解读这些视频。我们的分析揭示了通过言语和视觉渠道进行情绪披露的多种实践，突出了这些表达模式之间的动态相互作用。我们发现，视觉元素的刻意构建，包括环境设置和特定的审美选择，似乎能促进更具支持性和参与度的观众反应。这些发现强调了未来需要进行大规模定量研究，以检验视觉特征如何塑造社交媒体平台上的视频介导交流。此类调查将为开发以护理为中心的视频分享平台提供信息，从而更好地支持管理疾病体验的个体。", "summary": "本研究探讨了精神分裂症患者在YouTube上通过视频博客进行情绪披露的方式。通过分析200个视频，研究发现患者通过言语和视觉渠道多样化地表达情绪，并指出视觉元素的精心设计，如环境和审美选择，能有效促进观众的积极互动和支持性回应。这为未来开发支持患者的视频分享平台提供了启示。", "keywords": "精神分裂症, 视频博客, 情绪披露, 视觉交流, 心理健康", "comments": "本研究创新性地将焦点从文本转向视频介导的情绪披露，特别关注了精神分裂症患者这一特定群体。其亮点在于开发了视觉分析框架，并揭示了视觉元素在情绪表达和观众互动中的重要作用。研究结果对于理解社交媒体上心理健康内容的传播机制以及开发更具支持性的数字健康工具具有重要意义。然而，研究的局限性可能在于其定性分析的性质，需要进一步的大规模定量研究来验证和推广其发现。"}}
{"id": "2506.10383", "title": "RICE: Reactive Interaction Controller for Cluttered Canopy Environment", "authors": ["Nidhi Homey Parayil", "Thierry Peynot", "Chris Lehnert"], "summary": "Robotic navigation in dense, cluttered environments such as agricultural\ncanopies presents significant challenges due to physical and visual occlusion\ncaused by leaves and branches. Traditional vision-based or model-dependent\napproaches often fail in these settings, where physical interaction without\ndamaging foliage and branches is necessary to reach a target. We present a\nnovel reactive controller that enables safe navigation for a robotic arm in a\ncontact-rich, cluttered, deformable environment using end-effector position and\nreal-time tactile feedback. Our proposed framework's interaction strategy is\nbased on a trade-off between minimizing disturbance by maneuvering around\nobstacles and pushing through them to move towards the target. We show that\nover 35 trials in 3 experimental plant setups with an occluded target, the\nproposed controller successfully reached the target in all trials without\nbreaking any branch and outperformed the state-of-the-art model-free controller\nin robustness and adaptability. This work lays the foundation for safe,\nadaptive interaction in cluttered, contact-rich deformable environments,\nenabling future agricultural tasks such as pruning and harvesting in plant\ncanopies.", "comment": "This work has been submitted to the IEEE RAL for possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10383v1", "AI": {"title_translation": "RICE：用于杂乱冠层环境的反应式交互控制器", "tldr": "RICE是一种新型反应式控制器，它利用末端执行器位置和实时触觉反馈，使机械臂能够在杂乱、可变形的农业冠层环境中安全导航，通过在避开障碍物和推开障碍物之间权衡，成功到达目标。", "motivation": "在农业冠层等密集杂乱环境中，由于树叶和树枝造成的物理和视觉遮挡，机器人导航面临巨大挑战。传统的基于视觉或模型依赖的方法在这种需要物理交互而不损坏植物的环境中往往会失败。", "method": "本文提出了一种新型的反应式控制器，它利用末端执行器位置和实时触觉反馈，使机械臂能够在接触丰富、杂乱、可变形的环境中安全导航。其交互策略基于最小化扰动（通过绕过障碍物）和推开障碍物（以移向目标）之间的权衡。", "result": "在3个实验植物设置中进行了超过35次试验，目标被遮挡，所提出的控制器在所有试验中都成功到达了目标，没有损坏任何树枝，并且在鲁棒性和适应性方面优于最先进的无模型控制器。", "conclusion": "这项工作为在杂乱、接触丰富、可变形环境中安全、自适应的交互奠定了基础，为未来在植物冠层中进行修剪和采摘等农业任务提供了可能。", "translation": "RICE：用于杂乱冠层环境的反应式交互控制器\n\n在农业冠层等密集、杂乱的环境中，由于树叶和树枝造成的物理和视觉遮挡，机器人导航面临巨大挑战。传统的基于视觉或模型依赖的方法在这种环境中常常失效，因为要到达目标，需要进行物理交互而又不损坏树叶和树枝。我们提出了一种新颖的反应式控制器，它利用末端执行器位置和实时触觉反馈，使机械臂能够在接触丰富、杂乱、可变形的环境中安全导航。我们提出的框架的交互策略基于最小化扰动（通过绕过障碍物）和推开障碍物（以移向目标）之间的权衡。我们通过在3个实验植物设置中进行超过35次试验（目标被遮挡）表明，所提出的控制器在所有试验中都成功到达了目标，没有损坏任何树枝，并且在鲁棒性和适应性方面优于最先进的无模型控制器。这项工作为在杂乱、接触丰富、可变形环境中安全、自适应的交互奠定了基础，为未来在植物冠层中进行修剪和采摘等农业任务提供了可能。", "summary": "本文提出了一种名为RICE的新型反应式控制器，旨在解决机械臂在农业冠层等杂乱、接触丰富的可变形环境中导航的挑战。该控制器利用末端执行器位置和实时触觉反馈，通过在避开障碍物和推开障碍物之间进行权衡来指导交互。实验结果表明，RICE在保持植物完整性的前提下，成功地在多种植物设置中到达目标，并且在鲁棒性和适应性方面优于现有技术，为未来的农业机器人任务奠定了基础。", "keywords": "机器人导航, 反应式控制器, 触觉反馈, 农业机器人, 杂乱环境", "comments": "本文提出了一种新颖的反应式控制器RICE，其创新之处在于结合了末端执行器位置和实时触觉反馈，并引入了在“绕过”与“推开”障碍物之间进行权衡的交互策略，以应对杂乱、可变形环境中的机器人导航挑战。这项工作对于实现农业机器人（如修剪和采摘）在复杂植物冠层中的安全、自适应操作具有重要意义，克服了传统视觉或模型方法在物理遮挡环境中的局限性。"}}
{"id": "2506.10182", "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates", "authors": ["Fiona Ryan", "Josef Sivic", "Fabian Caba Heilbron", "Judy Hoffman", "James M. Rehg", "Bryan Russell"], "summary": "Personalized vision-language retrieval seeks to recognize new concepts (e.g.\n\"my dog Fido\") from only a few examples. This task is challenging because it\nrequires not only learning a new concept from a few images, but also\nintegrating the personal and general knowledge together to recognize the\nconcept in different contexts. In this paper, we show how to effectively adapt\nthe internal representation of a vision-language dual encoder model for\npersonalized vision-language retrieval. We find that regularized low-rank\nadaption of a small set of parameters in the language encoder's final layer\nserves as a highly effective alternative to textual inversion for recognizing\nthe personal concept while preserving general knowledge. Additionally, we\nexplore strategies for combining parameters of multiple learned personal\nconcepts, finding that parameter addition is effective. To evaluate how well\ngeneral knowledge is preserved in a finetuned representation, we introduce a\nmetric that measures image retrieval accuracy based on captions generated by a\nvision language model (VLM). Our approach achieves state-of-the-art accuracy on\ntwo benchmarks for personalized image retrieval with natural language queries -\nDeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal\nretrievals.", "comment": "CVPR 2025 Highlight. Code: http://github.com/adobe-research/polar-vl", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10182v1", "AI": {"title_translation": "改进个性化搜索与正则化低秩参数更新", "tldr": "本文提出了一种使用正则化低秩参数更新的方法，有效改进了个性化视觉-语言检索，在保持通用知识的同时，在新概念识别上取得了最先进的性能。", "motivation": "个性化视觉-语言检索任务面临挑战，因为它不仅需要从少量示例中学习新概念，还需要将个人和通用知识结合起来以在不同情境下识别概念。", "method": "本文提出了一种有效调整视觉-语言双编码器模型内部表示的方法。具体来说，通过对语言编码器最后一层的一小组参数进行正则化低秩适应，作为文本反转的有效替代方案，用于识别个人概念同时保留通用知识。此外，还探索了组合多个学习到的个人概念参数的策略，发现参数相加是有效的。为评估通用知识的保留情况，引入了一种基于视觉语言模型（VLM）生成标题的图像检索准确性度量。", "result": "该方法在DeepFashion2和ConCon-Chi两个个性化图像检索基准测试中，使用自然语言查询实现了最先进的准确性，在个人检索方面超越现有技术4%-22%。", "conclusion": "通过对语言编码器最后一层参数进行正则化低秩适应，可以有效改进个性化视觉-语言检索，同时保留通用知识，并在相关基准测试中取得了显著的性能提升。", "translation": "个性化视觉-语言检索旨在仅从少量示例中识别新概念（例如“我的狗Fido”）。这项任务具有挑战性，因为它不仅需要从少量图像中学习新概念，还需要将个人和通用知识结合起来，以便在不同情境下识别概念。在本文中，我们展示了如何有效地调整视觉-语言双编码器模型的内部表示，以实现个性化视觉-语言检索。我们发现，对语言编码器最后一层的一小组参数进行正则化低秩适应，可以作为文本反转的一种高效替代方案，用于识别个人概念同时保留通用知识。此外，我们探索了组合多个学习到的个人概念参数的策略，发现参数相加是有效的。为了评估微调表示中通用知识的保留程度，我们引入了一种基于视觉语言模型（VLM）生成标题的图像检索准确性度量。我们的方法在两个使用自然语言查询的个性化图像检索基准测试——DeepFashion2和ConCon-Chi上取得了最先进的准确性，在个人检索方面比现有技术高出4%-22%。", "summary": "本文提出了一种通过对视觉-语言双编码器模型进行正则化低秩参数更新来改进个性化视觉-语言检索的方法。该方法通过适应语言编码器最后一层的少量参数，在识别新概念的同时有效保留通用知识，并探索了多概念参数组合策略。引入的新度量衡用于评估通用知识的保留，并在DeepFashion2和ConCon-Chi数据集上实现了超越现有技术的SOTA性能。", "keywords": "个性化搜索, 视觉-语言检索, 低秩适应, 参数更新, 概念学习", "comments": "本文的创新点在于提出了正则化低秩参数更新作为文本反转的替代方案，有效地解决了个性化视觉-语言检索中新概念学习与通用知识保留的平衡问题。引入新的评估度量也增强了对模型性能的全面评估。其在SOTA上的表现证明了该方法的有效性和重要性。"}}
{"id": "2506.10133", "title": "Provable Sim-to-Real Transfer via Offline Domain Randomization", "authors": ["Arnaud Fickinger", "Abderrahim Bendahi", "Stuart Russell"], "summary": "Reinforcement-learning agents often struggle when deployed from simulation to\nthe real-world. A dominant strategy for reducing the sim-to-real gap is domain\nrandomization (DR) which trains the policy across many simulators produced by\nsampling dynamics parameters, but standard DR ignores offline data already\navailable from the real system. We study offline domain randomization (ODR),\nwhich first fits a distribution over simulator parameters to an offline\ndataset. While a growing body of empirical work reports substantial gains with\nalgorithms such as DROPO, the theoretical foundations of ODR remain largely\nunexplored. In this work, we (i) formalize ODR as a maximum-likelihood\nestimation over a parametric simulator family, (ii) prove consistency of this\nestimator under mild regularity and identifiability conditions, showing it\nconverges to the true dynamics as the dataset grows, (iii) derive gap bounds\ndemonstrating ODRs sim-to-real error is up to an O(M) factor tighter than\nuniform DR in the finite-simulator case (and analogous gains in the continuous\nsetting), and (iv) introduce E-DROPO, a new version of DROPO which adds an\nentropy bonus to prevent variance collapse, yielding broader randomization and\nmore robust zero-shot transfer in practice.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10133v1", "AI": {"title_translation": "可证明的模拟到现实迁移通过离线域随机化", "tldr": "本文研究离线域随机化（ODR），一种利用离线数据改进模拟到现实迁移的方法。作者形式化了ODR，证明了其一致性，推导了更紧密的误差界限，并提出了一种改进的算法E-DROPO，实现了更鲁棒的零样本迁移。", "motivation": "强化学习智能体在从模拟环境部署到现实世界时常遇到困难（即模拟到现实的差距）。现有的域随机化（DR）方法忽略了可用的离线真实系统数据，而离线域随机化（ODR）的理论基础尚待探索。", "method": "作者首先将离线域随机化（ODR）形式化为参数模拟器族上的最大似然估计问题。接着，他们证明了该估计器在温和的正则性和可识别性条件下的一致性，并推导了ODR的模拟到现实误差界限，表明其比均匀域随机化更紧密。此外，他们引入了E-DROPO，一个添加了熵奖励的新版DROPO算法，以防止方差崩溃。", "result": "结果表明，ODR估计器在数据集增长时收敛到真实动力学。ODR的模拟到现实误差比均匀域随机化（Uniform DR）在有限模拟器情况下紧密O(M)倍，在连续设置中也有类似增益。新引入的E-DROPO算法在实践中产生了更广泛的随机化和更鲁棒的零样本迁移。", "conclusion": "本文为离线域随机化（ODR）提供了坚实的理论基础，证明了其优于标准域随机化，并提出了一个改进的实用算法E-DROPO，有效提升了模拟到现实的迁移性能。", "translation": "强化学习智能体在从模拟环境部署到现实世界时经常遇到困难。减少模拟到现实差距的主要策略是域随机化（DR），它通过采样动力学参数来训练策略，但标准DR忽略了已有的真实系统离线数据。我们研究离线域随机化（ODR），它首先将模拟器参数的分布拟合到离线数据集。尽管越来越多的实证工作报告了使用DROPO等算法取得了实质性进展，但ODR的理论基础在很大程度上仍未被探索。在这项工作中，我们（i）将ODR形式化为参数模拟器族上的最大似然估计，（ii）在温和的正则性和可识别性条件下证明了该估计器的一致性，表明它随着数据集的增长收敛到真实动力学，（iii）推导了差距界限，证明ODR的模拟到现实误差在有限模拟器情况下比均匀DR紧密高达O(M)倍（在连续设置中也有类似增益），以及（iv）引入了E-DROPO，一个新版本的DROPO，它增加了熵奖励以防止方差崩溃，从而在实践中产生更广泛的随机化和更鲁棒的零样本迁移。", "summary": "本文探讨了离线域随机化（ODR）在解决强化学习模拟到现实迁移问题中的应用。作者为ODR建立了理论基础，将其形式化为最大似然估计问题，并证明了其估计器的一致性。研究表明，ODR的模拟到现实误差比传统域随机化更小。此外，本文还引入了改进的E-DROPO算法，以提高实践中的鲁棒性和零样本迁移能力。", "keywords": "离线域随机化, 模拟到现实迁移, 强化学习, 理论基础, E-DROPO", "comments": "本文的创新之处在于为离线域随机化（ODR）提供了严格的理论基础，填补了该领域理论探索的空白。通过形式化ODR并证明其一致性和误差界限，提升了ODR方法的可信度和理解深度。此外，提出E-DROPO算法也显示了其在实践中解决方差崩溃问题的潜力，对于提升强化学习智能体的零样本模拟到现实迁移能力具有重要意义。"}}
{"id": "2506.10854", "title": "The Impact of Partial Computations on the Red-Blue Pebble Game", "authors": ["Pál András Papp", "Aleksandros Sobczyk", "A. N. Yzelman"], "summary": "We study an extension of the well-known red-blue pebble game (RBP) with\npartial computation steps, inspired by the recent work of Sobczyk. While the\noriginal RBP assumes that we need to have all the inputs of an operation in\nfast memory at the same time, in many concrete computations, the inputs can be\naggregated one by one into the final output value. These partial computation\nsteps can enable pebbling strategies with much smaller I/O cost, and in\nsettings where such a step-by-step aggregation is possible, this extended\nred-blue pebble game offers a much more realistic cost model.\n  We establish the fundamental properties of this partial-computing red-blue\npebble game (PRBP), and compare it to the original RBP. We begin with some\nsimple examples where allowing partial computations can decrease the optimal\nI/O cost. It is also shown that the cost can decrease by up to a linear factor\nthis way, but in general, it is NP-hard to decide whether partial computations\nallow for a smaller cost in a specific DAG. We then discuss how $S$-partitions,\na crucial tool for deriving I/O lower bounds in RBP, can be adapted to the PRBP\nmodel. These new tools are then used to establish lower bounds on the I/O cost\nof some prominent computational tasks. Finally, we also adapt a hardness result\nfrom RBP, showing that the optimum cost is still NP-hard to approximate in PRBP\nto any reasonable factor.", "comment": "Published in the 37th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA 2025)", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10854v1", "AI": {"title_translation": "部分计算对红蓝石子游戏的影响", "tldr": "本文研究了带有部分计算步骤的红蓝石子游戏(PRBP)，发现它能显著降低I/O成本，但在某些情况下，决定其是否更优或近似最优是NP难的。", "motivation": "原始的红蓝石子游戏（RBP）假设操作的所有输入必须同时在快速内存中，这与许多实际计算中输入可以逐一聚合的情况不符。引入部分计算步骤可以显著降低I/O成本，并提供更真实的成本模型。", "method": "1. 研究带有部分计算步骤的红蓝石子游戏（PRBP）的基本性质，并与原始RBP进行比较。2. 通过简单示例展示部分计算如何降低最优I/O成本。3. 探讨如何将RBP中用于推导I/O下限的关键工具S-分区适应到PRBP模型中。4. 利用新工具为一些重要的计算任务建立I/O成本下限。5. 将RBP中的一个难度结果应用于PRBP，证明在PRBP中近似最优成本仍然是NP难的。", "result": "1. 部分计算可以降低最优I/O成本，最高可达线性因子。2. 在特定DAG中，决定部分计算是否能降低成本通常是NP难的。3. S-分区工具可以成功适应到PRBP模型中，并用于建立I/O下限。4. PRBP中的最优成本近似到任何合理因子仍然是NP难的。", "conclusion": "部分计算步骤可以显著优化红蓝石子游戏中的I/O成本模型，使其更贴近现实，但计算其最优解或近似解仍然具有高度的计算复杂性（NP难）。", "translation": "我们研究了红蓝石子游戏（RBP）的一个扩展，其中包含部分计算步骤，灵感来源于Sobczyk最近的工作。虽然原始的RBP假设我们需要同时将操作的所有输入都放在快速内存中，但在许多具体的计算中，输入可以逐一聚合到最终输出值中。这些部分计算步骤可以实现I/O成本低得多的铺石策略，并且在可以进行这种逐步聚合的环境中，这种扩展的红蓝石子游戏提供了一个更真实的成本模型。\n我们建立了这种部分计算红蓝石子游戏（PRBP）的基本性质，并将其与原始RBP进行了比较。我们首先给出了一些简单的例子，其中允许部分计算可以降低最优I/O成本。研究还表明，成本可以通过这种方式降低多达线性因子，但在一般情况下，决定在特定DAG中部分计算是否允许更小的成本是NP难的。然后，我们讨论了如何将S-分区（RBP中推导I/O下限的关键工具）适应到PRBP模型中。这些新工具随后被用于建立一些重要计算任务的I/O成本下限。最后，我们还改编了RBP中的一个难度结果，表明在PRBP中将最优成本近似到任何合理因子仍然是NP难的。", "summary": "本文引入并研究了带有部分计算步骤的红蓝石子游戏（PRBP），这是对传统红蓝石子游戏（RBP）的扩展，旨在提供一个更贴近实际计算的I/O成本模型。研究表明，部分计算能够显著降低I/O成本，最高可达线性因子。然而，确定部分计算是否能降低特定DAG的成本以及近似PRBP中的最优成本是NP难问题。文章还探讨了如何将RBP中的S-分区工具应用于PRBP模型以推导I/O下限。", "keywords": "红蓝石子游戏, 部分计算, I/O成本, NP难, S-分区", "comments": "这篇论文通过引入“部分计算”的概念，有效地将经典的红蓝石子游戏模型扩展到更符合实际计算场景，特别是那些允许逐步聚合输入的场景。这一创新使I/O成本模型更加现实，并可能为优化实际计算中的数据传输提供新的视角。论文不仅证明了部分计算带来的潜在I/O成本降低，也揭示了其固有的计算复杂性（NP难），这为未来的研究指明了方向。对S-分区工具的适应性研究也表明了其理论深度。"}}
{"id": "2506.10958", "title": "Bias-Switchable Row-Column Array Imaging using Fast Orthogonal Row-Column Electronic Scanning (FORCES) Compared with Conventional Row-Column Array Imaging", "authors": ["Randy Palamar", "Mohammad Rahim Sobhani", "Darren Dahunsi", "Negar Majidi", "Afshin Kashani Ilkhechi", "Joy Wang", "Jeremy Brown", "Roger Zemp"], "summary": "Row-Column Arrays (RCAs) offer an attractive alternative to fully wired\n2D-arrays for 3D-ultrasound, due to their greatly simplified wiring. However,\nconventional RCAs face challenges related to their long elements. These include\nan inability to image beyond the shadow of the aperture and an inability to\nfocus in both transmit and receive for desired scan planes. To address these\nlimitations, we recently developed bias-switchable RCAs, also known as Top\nOrthogonal to Bottom Electrode (TOBE) arrays. These arrays provide novel\nopportunities to read out from every element of the array and achieve\nhigh-quality images. While TOBE arrays and their associated imaging schemes\nhave shown promise, they have not yet been directly compared experimentally to\nconventional RCA imaging techniques. This study aims to provide such a\ncomparison, demonstrating superior B-scan and volumetric images from two\nelectrostrictive relaxor TOBE arrays, using a method called Fast Orthogonal\nRow-Column Electronic scanning (FORCES), compared to conventional RCA imaging\nschemes, including Tilted Plane Wave (TPW) compounding and Virtual Line Source\n(VLS) imaging. The study quantifies resolution and Generalized Contrast to\nNoise Ratio (gCNR) in phantoms, and also demonstrates volumetric acquisitions\nin phantom and animal models.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.10958v1", "AI": {"title_translation": "偏置可切换行-列阵列成像与传统行-列阵列成像的快速正交行-列电子扫描（FORCES）比较", "tldr": "本研究通过实验证明，使用FORCES方法进行偏置可切换行-列阵列（TOBE）成像比传统行-列阵列成像表现出更优越的B扫描和体积图像质量。", "motivation": "传统的行-列阵列（RCAs）尽管简化了布线，但在3D超声成像中存在局限性，如无法在孔径阴影外成像以及无法在发射和接收端对所需扫描平面进行聚焦。", "method": "本研究开发了偏置可切换的行-列阵列（TOBE阵列），并使用快速正交行-列电子扫描（FORCES）方法进行成像。通过对两个电致伸缩弛豫型TOBE阵列进行实验，将其与包括倾斜平面波（TPW）复合和虚拟线源（VLS）成像在内的传统RCA成像方案进行直接比较。研究还量化了模型中的分辨率和广义对比度噪声比（gCNR），并演示了在模型和动物模型中的体积采集。", "result": "实验结果表明，使用FORCES方法的两个电致伸缩弛豫型TOBE阵列在B扫描和体积图像方面均优于传统的RCA成像方案。研究量化了模型中的分辨率和gCNR，并在模型和动物模型中成功进行了体积采集。", "conclusion": "偏置可切换的行-列阵列（TOBE阵列）结合FORCES成像方案，在图像质量上显著优于传统的行-列阵列成像技术，为3D超声成像提供了新的高质量解决方案。", "translation": "行-列阵列（RCAs）因其大大简化的布线，为3D超声提供了有吸引力的替代方案，以替代完全布线的2D阵列。然而，传统的RCAs面临与其长单元相关的挑战。这些挑战包括无法在孔径阴影外成像，以及无法在发射和接收端对所需扫描平面进行聚焦。为了解决这些限制，我们最近开发了偏置可切换的RCAs，也称为顶部正交到底部电极（TOBE）阵列。这些阵列提供了从阵列的每个单元读取数据并实现高质量图像的新机会。虽然TOBE阵列及其相关的成像方案已显示出前景，但它们尚未与传统的RCA成像技术进行直接的实验比较。本研究旨在提供这种比较，通过使用快速正交行-列电子扫描（FORCES）方法，展示了两个电致伸缩弛豫型TOBE阵列优于传统RCA成像方案（包括倾斜平面波（TPW）复合和虚拟线源（VLS）成像）的B扫描和体积图像。该研究量化了模型中的分辨率和广义对比度噪声比（gCNR），并演示了在模型和动物模型中的体积采集。", "summary": "本文旨在通过实验比较偏置可切换的行-列阵列（TOBE阵列）与传统行-列阵列（RCA）成像技术在3D超声领域的性能。传统RCA存在成像范围和聚焦能力的局限性。研究团队开发了TOBE阵列并结合快速正交行-列电子扫描（FORCES）方法，实验证明TOBE/FORCES在B扫描和体积图像质量上优于传统RCA成像（如TPW和VLS）。研究还量化了分辨率和gCNR，并在模型和动物模型中进行了体积采集验证。", "keywords": "行-列阵列, 3D超声, TOBE阵列, FORCES, 图像质量", "comments": "这篇论文的创新之处在于引入了偏置可切换的行-列阵列（TOBE阵列）以及快速正交行-列电子扫描（FORCES）方法，以克服传统行-列阵列在3D超声成像中的固有缺陷。通过直接的实验比较，明确展示了新方法在图像质量上的显著优势，这对于推动3D超声技术的发展具有重要意义。"}}
{"id": "2506.10209", "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games", "authors": ["Prakamya Mishra", "Jiang Liu", "Jialian Wu", "Xiaodong Yu", "Zicheng Liu", "Emad Barsoum"], "summary": "Large reasoning models (LRMs) have demonstrated impressive reasoning\ncapabilities across a broad range of tasks including Olympiad-level\nmathematical problems, indicating evidence of their complex reasoning\nabilities. While many reasoning benchmarks focus on the STEM domain, the\nability of LRMs to reason correctly in broader task domains remains\nunderexplored. In this work, we introduce \\textbf{TTT-Bench}, a new benchmark\nthat is designed to evaluate basic strategic, spatial, and logical reasoning\nabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games\nthat humans can effortlessly solve from a young age. We propose a simple yet\nscalable programmatic approach for generating verifiable two-player game\nproblems for TTT-Bench. Although these games are trivial for humans, they\nrequire reasoning about the intentions of the opponent, as well as the game\nboard's spatial configurations, to ensure a win. We evaluate a diverse set of\nstate-of-the-art LRMs, and \\textbf{discover that the models that excel at hard\nmath problems frequently fail at these simple reasoning games}. Further testing\nreveals that our evaluated reasoning models score on average $\\downarrow$ 41\\%\n\\& $\\downarrow$ 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024\nrespectively, with larger models achieving higher performance using shorter\nreasoning traces, where most of the models struggle on long-term strategic\nreasoning situations on simple and new TTT-Bench tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10209v1", "AI": {"title_translation": "TTT-Bench：一个用于评估简单新颖的井字棋式游戏推理能力的基准", "tldr": "大型推理模型（LRMs）在复杂的数学问题上表现出色，但在简单的井字棋式游戏中却在基本的战略、空间和逻辑推理方面表现不佳。TTT-Bench基准揭示了这一能力差距。", "motivation": "大型推理模型（LRMs）在STEM领域展现了令人印象深刻的推理能力，但其在更广泛任务领域（如基本战略、空间和逻辑推理）中的表现仍未被充分探索，因此需要一个新的基准来评估这些能力。", "method": "本研究引入了TTT-Bench，这是一个新基准，通过一套包含四种井字棋式双人游戏来评估LRMs的基本战略、空间和逻辑推理能力。研究提出了一种简单但可扩展的程序化方法来生成可验证的双人游戏问题，并评估了一系列最先进的LRMs。", "result": "研究发现，在复杂数学问题上表现出色的模型，在这些简单的推理游戏中却经常失败。与MATH 500和AIME 2024相比，所评估的推理模型在TTT-Bench上的平均得分分别低41%和5%。较大的模型使用较短的推理轨迹能获得更高的性能，但大多数模型在简单和新的TTT-Bench任务中，在长期战略推理情境下表现挣扎。", "conclusion": "尽管大型推理模型在STEM领域的推理能力强大，但它们在处理简单的井字棋式游戏时，在基本的战略和空间推理方面存在显著缺陷，尤其是在需要长期战略推理的情况下。", "translation": "大型推理模型（LRMs）在包括奥林匹克级别的数学问题在内的广泛任务中展示了令人印象深刻的推理能力，这表明它们具备复杂的推理能力。虽然许多推理基准侧重于STEM领域，但LRMs在更广泛任务领域中正确推理的能力仍未被充分探索。在这项工作中，我们引入了TTT-Bench，这是一个旨在通过一套四种人类从小就能轻松解决的双人井字棋式游戏，评估LRMs基本战略、空间和逻辑推理能力的新基准。我们提出了一种简单但可扩展的程序化方法，用于为TTT-Bench生成可验证的双人游戏问题。尽管这些游戏对人类来说微不足道，但它们需要推理对手的意图以及棋盘的空间配置，以确保获胜。我们评估了一系列最先进的LRMs，并发现那些擅长解决高难度数学问题的模型，在这些简单的推理游戏中却经常失败。进一步的测试显示，我们评估的推理模型在TTT-Bench上的平均得分分别比MATH 500和AIME 2024低41%和5%，其中较大的模型使用较短的推理轨迹能获得更高的性能，而大多数模型在简单和新的TTT-Bench任务中，在长期战略推理情境下表现挣扎。", "summary": "本论文引入了TTT-Bench，一个由简单井字棋式游戏组成的新基准，旨在评估大型推理模型（LRMs）在STEM领域之外的基本战略、空间和逻辑推理能力。研究发现，尽管LRMs在复杂数学问题上表现出色，但它们在这些对人类而言微不足道的游戏中却表现出显著不足，得分远低于在现有数学基准上的表现。这凸显了LRMs在处理基本战略和空间推理，尤其是在长期情境下的能力差距。", "keywords": "大型推理模型, 基准, 井字棋, 战略推理, 空间推理", "comments": "该论文的创新之处在于提出了一个新颖的基准（TTT-Bench），用于评估大型推理模型在以往未充分探索的推理领域（基本战略、空间和逻辑推理）的能力。其重要性在于挑战了“复杂STEM问题表现出色等同于通用推理能力强”的假设，揭示了LRMs在看似简单的推理任务上的关键缺陷。然而，摘要中并未详细说明这“四种双人井字棋式游戏”的具体内容，也未深入阐述“程序化方法”的细节。"}}
{"id": "2506.10763", "title": "Reduced-Order Time Splitting for Navier-Stokes with Open Boundaries", "authors": ["Mejdi Azaïez", "Tomás Chacón Rebollo", "Carlos Núñez Fernández", "Samuele Rubino"], "summary": "In this work, we propose a Proper Orthogonal Decomposition-Reduced Order\nModel (POD-ROM) applied to time-splitting schemes for solving the Navier-Stokes\nequations with open boundary conditions. In this method, we combine three\nstrategies to reduce the computing time to solve NSE: time splitting, reduction\nof the computational domain through non-standard treatment of open boundary\nconditions and reduced order modelling. To make the work self-contained, we\nfirst present the formulation of the time-splitting scheme applied to the\nNavier-Stokes equations with open boundary conditions, employing a first-order\nEuler time discretization and deriving the non-standard boundary condition for\npressure. Then, we construct a Galerkin projection-based ROM using POD with two\ndifferent treatments of the pressure boundary condition on the outlet. We\npropose a comparative performance analysis between the standard\nprojection-based POD-ROM (fully intrusive) and a hybrid POD-ROM that combines a\nprojection-based approach (intrusive) with a data-driven technique\n(non-intrusive) using Radial Basis Functions (RBF). We illustrate this\ncomparison through two different numerical tests: the flow in a bifurcated tube\nand the benchmark numerical test of the flow past cylinder, numerically\ninvestigating the efficiency and accuracy of both ROMs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10763v1", "AI": {"title_translation": "纳维-斯托克斯方程开放边界条件下的降阶时间分裂法", "tldr": "本文提出一种将Proper Orthogonal Decomposition-Reduced Order Model (POD-ROM) 应用于Navier-Stokes方程开放边界条件时间分裂方案的方法，旨在通过结合时间分裂、非标准开放边界处理和降阶建模来减少计算时间，并比较了完全侵入式和混合式POD-ROM的性能。", "motivation": "解决Navier-Stokes方程在开放边界条件下求解时计算时间过长的问题。", "method": "本文提出了一种Proper Orthogonal Decomposition-Reduced Order Model (POD-ROM) 应用于Navier-Stokes方程开放边界条件的时间分裂方案。该方法结合了时间分裂、通过非标准开放边界条件处理减少计算域以及降阶建模三种策略。首先介绍了时间分裂方案的公式，采用一阶欧拉时间离散并推导了压力的非标准边界条件。然后，构建了基于Galerkin投影的ROM，使用POD并对出口压力边界条件进行了两种不同处理。最后，比较了标准投影式POD-ROM（完全侵入式）和结合了投影式与数据驱动技术（非侵入式，使用径向基函数RBF）的混合POD-ROM的性能。", "result": "通过分叉管内流动和圆柱绕流两个数值测试，数值研究并比较了两种降阶模型（ROMs）的效率和准确性。", "conclusion": "Not mentioned in abstract", "translation": "在这项工作中，我们提出了一种将 Proper Orthogonal Decomposition-Reduced Order Model (POD-ROM) 应用于求解具有开放边界条件的 Navier-Stokes 方程的时间分裂方案。在该方法中，我们结合了三种策略来减少求解 Navier-Stokes 方程的计算时间：时间分裂、通过非标准开放边界条件处理减少计算域以及降阶建模。为了使本文内容完整，我们首先介绍了应用于具有开放边界条件的 Navier-Stokes 方程的时间分裂方案的公式，该方案采用一阶欧拉时间离散并推导了压力的非标准边界条件。然后，我们使用 POD 构建了一个基于 Galerkin 投影的 ROM，并对出口处的压力边界条件进行了两种不同的处理。我们提出了对标准基于投影的 POD-ROM（完全侵入式）和结合了基于投影方法（侵入式）与数据驱动技术（非侵入式，使用径向基函数 RBF）的混合 POD-ROM 之间的性能进行比较分析。我们通过两种不同的数值测试来说明这种比较：分叉管内流动和圆柱绕流的基准数值测试，数值研究了两种 ROM 的效率和准确性。", "summary": "本文提出了一种针对开放边界条件下Navier-Stokes方程的时间分裂求解方案的降阶模型（POD-ROM），旨在通过结合时间分裂、非标准边界条件处理和降阶建模来显著减少计算时间。研究详细阐述了时间分裂方案的公式化、压力的非标准边界条件推导以及基于Galerkin投影的POD-ROM构建。文章特别比较了完全侵入式POD-ROM和结合数据驱动技术的混合POD-ROM的性能，并通过分叉管流和圆柱绕流两个数值案例验证了其效率和准确性。", "keywords": "Navier-Stokes, 降阶模型, 时间分裂, 开放边界, POD-ROM", "comments": "这篇论文的创新点在于结合了时间分裂、非标准开放边界条件处理和降阶建模（POD-ROM）来高效求解Navier-Stokes方程。特别是，它比较了完全侵入式和混合式POD-ROM的性能，为复杂流体动力学问题的计算效率和准确性提供了新的视角。"}}
{"id": "2506.10841", "title": "Automotive Radar Online Channel Imbalance Estimation via NLMS", "authors": ["Esmaeil Kavousi Ghafi", "Oliver Lang", "Matthias Wagner", "Alexander Melzer", "Mario Huemer"], "summary": "Automotive radars are one of the essential enablers of advanced driver\nassistance systems (ADASs). Continuous monitoring of the functional safety and\nreliability of automotive radars is a crucial requirement to prevent accidents\nand increase road safety. One of the most critical aspects to monitor in this\ncontext is radar channel imbalances, as they are a key parameter regarding the\nreliability of the radar. These imbalances may originate from several parameter\nvariations or hardware fatigues, e.g., a solder ball break (SBB), and may\naffect some radar processing steps, such as the angle of arrival estimation. In\nthis work, a novel method for online estimation of automotive radar channel\nimbalances is proposed. The proposed method exploits a normalized least mean\nsquares (NLMS) algorithm as a block in the processing chain of the radar to\nestimate the channel imbalances. The input of this block is the detected\ntargets in the range-Doppler map of the radar on the road without any prior\nknowledge on the angular parameters of the targets. This property in\ncombination with low computational complexity of the NLMS, makes the proposed\nmethod suitable for online channel imbalance estimation, in parallel to the\nnormal operation of the radar. Furthermore, it features reduced dependency on\nspecific targets of interest and faster update rates of the channel imbalance\nestimation compared to the majority of state-of-the-art methods. This\nimprovement is achieved by allowing for multiple targets in the angular\nspectrum, whereas most other methods are restricted to only single targets in\nthe angular spectrum. The performance of the proposed method is validated using\nvarious simulation scenarios and is supported by measurement results.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.10841v1", "AI": {"title_translation": "汽车雷达在线信道不平衡估计通过NLMS", "tldr": "本文提出了一种基于归一化最小均方（NLMS）算法的汽车雷达在线信道不平衡估计新方法，该方法计算复杂度低，支持多目标处理，且无需先验角度知识，适用于雷达正常运行时的在线监测。", "motivation": "持续监测汽车雷达的功能安全性和可靠性至关重要，以防止事故并提高道路安全。雷达信道不平衡是影响雷达可靠性的关键参数，可能源于参数变化或硬件疲劳（如焊球断裂），并影响角度估计等雷达处理步骤。", "method": "本文提出了一种利用归一化最小均方（NLMS）算法作为雷达处理链中一个模块的新方法，用于在线估计信道不平衡。该模块的输入是雷达在道路上距离-多普勒图中检测到的目标，无需目标角度参数的先验知识。该方法允许角谱中存在多个目标。", "result": "所提出的方法通过各种仿真场景进行了性能验证，并得到了测量结果的支持。与大多数现有方法相比，它对特定感兴趣目标的依赖性降低，信道不平衡估计的更新速率更快。", "conclusion": "所提出的基于NLMS的在线汽车雷达信道不平衡估计方法，在无需先验角度知识和支持多目标的情况下，实现了低计算复杂度、快速更新和高可靠性，适用于雷达的正常运行，从而增强了汽车雷达的功能安全性和可靠性。", "translation": "汽车雷达是高级驾驶辅助系统（ADAS）的重要使能器之一。持续监测汽车雷达的功能安全性和可靠性是防止事故和提高道路安全的关键要求。在此背景下，需要监测的最关键方面之一是雷达信道不平衡，因为它们是衡量雷达可靠性的关键参数。这些不平衡可能源于多种参数变化或硬件疲劳，例如焊球断裂（SBB），并可能影响某些雷达处理步骤，例如到达角估计。在这项工作中，提出了一种用于汽车雷达信道不平衡在线估计的新方法。所提出的方法利用归一化最小均方（NLMS）算法作为雷达处理链中的一个模块来估计信道不平衡。该模块的输入是雷达在道路上距离-多普勒图中检测到的目标，无需目标角度参数的先验知识。这一特性与NLMS的低计算复杂度相结合，使得所提出的方法适合于在线信道不平衡估计，与雷达的正常运行并行。此外，与大多数现有方法相比，它对特定感兴趣目标的依赖性更低，信道不平衡估计的更新速率更快。这一改进是通过允许角谱中存在多个目标来实现的，而大多数其他方法仅限于角谱中的单个目标。所提出方法的性能通过各种仿真场景进行了验证，并得到了测量结果的支持。", "summary": "本文提出了一种基于归一化最小均方（NLMS）算法的汽车雷达在线信道不平衡估计新方法。该方法将NLMS算法整合到雷达处理链中，利用距离-多普勒图中的检测目标作为输入，无需目标角度的先验知识。其低计算复杂度和支持多目标估计的特性使其适用于与雷达正常运行并行的在线监测，并提供了更快的更新速率和更低的对特定目标依赖性。该方法已通过仿真和测量结果验证了其性能。", "keywords": "汽车雷达, 信道不平衡, NLMS, 在线估计, ADAS", "comments": "该论文提出了一种创新的在线信道不平衡估计方法，通过集成NLMS算法，解决了传统方法在计算复杂度、更新速度和多目标处理方面的局限性。其无需先验角度知识和支持多目标的特性显著提高了实用性，对于提升汽车雷达的功能安全性和可靠性具有重要意义。"}}
{"id": "2506.10589", "title": "Transient performance of MPC for tracking without terminal constraints", "authors": ["Nadine Ehmann", "Matthias Köhler", "Frank Allgöwer"], "summary": "Model predictive control (MPC) for tracking is a recently introduced\napproach, which extends standard MPC formulations by incorporating an\nartificial reference as an additional optimization variable, in order to track\nexternal and potentially time-varying references. In this work, we analyze the\nperformance of such an MPC for tracking scheme without a terminal cost and\nterminal constraints. We derive a transient performance estimate, i.e. a bound\non the closed-loop performance over an arbitrary time interval, yielding\ninsights on how to select the scheme's parameters for performance. Furthermore,\nwe show that in the asymptotic case, where the prediction horizon and observed\ntime interval tend to infinity, the closed-loop solution of MPC for tracking\nrecovers the infinite horizon optimal solution.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10589v1", "AI": {"title_translation": "无终端约束跟踪MPC的瞬态性能", "tldr": "本文分析了无终端成本和终端约束的跟踪型模型预测控制（MPC）的瞬态性能，并证明了其在渐近情况下的最优性。", "motivation": "模型预测控制（MPC）用于跟踪是一种通过引入人工参考变量来跟踪外部和时变参考的新方法。本文旨在分析这种没有终端成本和终端约束的跟踪型MPC方案的性能。", "method": "本文分析了无终端成本和终端约束的跟踪型MPC方案的性能。具体方法包括推导瞬态性能估计（即任意时间间隔内闭环性能的界限），以及证明在预测范围和观测时间间隔趋于无穷大时，闭环解能恢复无限范围最优解。", "result": "导出了瞬态性能估计，即任意时间间隔内闭环性能的界限，这为选择方案参数提供了指导。此外，证明了在渐近情况下（预测范围和观测时间间隔趋于无穷大），跟踪型MPC的闭环解能够恢复无限范围最优解。", "conclusion": "本文成功分析了无终端成本和终端约束的跟踪型MPC的瞬态性能，提供了性能估计和渐近最优性证明，有助于理解和参数选择。", "translation": "模型预测控制（MPC）用于跟踪是一种最近引入的方法，它通过将人工参考作为额外的优化变量来扩展标准MPC公式，以跟踪外部和可能时变的参考。在这项工作中，我们分析了这种没有终端成本和终端约束的跟踪型MPC方案的性能。我们推导了一个瞬态性能估计，即在任意时间间隔内闭环性能的界限，从而为如何选择方案参数以获得性能提供了见解。此外，我们表明在渐近情况下，当预测范围和观测时间间隔趋于无穷大时，跟踪型MPC的闭环解恢复了无限范围最优解。", "summary": "本文研究了一种新的模型预测控制（MPC）方法，即跟踪型MPC，该方法通过引入人工参考变量来跟踪外部和时变参考。研究重点在于分析无终端成本和终端约束的跟踪型MPC方案的性能。作者推导了任意时间间隔内闭环性能的瞬态估计界限，为参数选择提供了指导。此外，研究还证明了在预测范围和观测时间间隔趋于无穷大的渐近情况下，跟踪型MPC的闭环解可以恢复无限范围的最优解。", "keywords": "模型预测控制, 跟踪, 瞬态性能, 终端约束, 渐近最优性", "comments": "这项工作分析了无终端约束的跟踪型MPC的瞬态性能，这在实际应用中具有重要意义，因为终端约束有时难以满足或可能导致保守性。推导瞬态性能界限和证明渐近最优性是该研究的关键创新点，有助于更深入地理解此类MPC的特性，并为实际系统中的控制器设计和参数调整提供了理论依据。"}}
{"id": "2506.10501", "title": "BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis", "authors": ["Surya Jasper", "Minh Luu", "Evan Pan", "Aakash Tyagi", "Michael Quinn", "Jiang Hu", "David Kebo Houngninou"], "summary": "Hardware complexity continues to strain verification resources, motivating\nthe adoption of machine learning (ML) methods to improve debug efficiency.\nHowever, ML-assisted debugging critically depends on diverse and scalable bug\ndatasets, which existing manual or automated bug insertion methods fail to\nreliably produce. We introduce BugGen, a first of its kind, fully autonomous,\nmulti-agent pipeline leveraging Large Language Models (LLMs) to systematically\ngenerate, insert, and validate realistic functional bugs in RTL. BugGen\npartitions modules, selects mutation targets via a closed-loop agentic\narchitecture, and employs iterative refinement and rollback mechanisms to\nensure syntactic correctness and functional detectability. Evaluated across\nfive OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional\naccuracy and achieved a throughput of 17.7 validated bugs per hour-over five\ntimes faster than typical manual expert insertion. Additionally, BugGen\nidentified 104 previously undetected bugs in OpenTitan regressions,\nhighlighting its utility in exposing verification coverage gaps. Compared\nagainst Certitude, BugGen demonstrated over twice the syntactic accuracy,\ndeeper exposure of testbench blind spots, and more functionally meaningful and\ncomplex bug scenarios. Furthermore, when these BugGen-generated datasets were\nemployed to train ML-based failure triage models, we achieved high\nclassification accuracy (88.1%-93.2%) across different IP blocks, confirming\nthe practical utility and realism of generated bugs. BugGen thus provides a\nscalable solution for generating high-quality bug datasets, significantly\nenhancing verification efficiency and ML-assisted debugging.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10501v1", "AI": {"title_translation": "BugGen：一种用于真实RTL错误合成的自校正多智能体LLM管道", "tldr": "BugGen是一种利用多智能体LLM管道自动生成、插入和验证RTL功能错误的工具，它能高效生成高质量的错误数据集，显著提升硬件验证效率和机器学习辅助调试。", "motivation": "硬件复杂性持续给验证资源带来压力，因此需要采用机器学习方法来提高调试效率。然而，机器学习辅助调试严重依赖多样化和可扩展的错误数据集，而现有的人工或自动化错误插入方法无法可靠地生成此类数据集。", "method": "BugGen是一个完全自主的多智能体管道，利用大型语言模型（LLM）系统地生成、插入和验证RTL中的真实功能错误。它通过闭环智能体架构划分模块，选择突变目标，并采用迭代细化和回滚机制，以确保语法正确性和功能可检测性。", "result": "BugGen在五个OpenTitan IP块上生成了500个独特的错误，功能准确率达94%，吞吐量为每小时17.7个经过验证的错误，比典型的人工专家插入快五倍以上。它还在OpenTitan回归测试中识别出104个以前未检测到的错误。与Certitude相比，BugGen的语法准确性提高了一倍以上，更能深入暴露测试平台盲点，并生成了更具功能意义和复杂的错误场景。此外，用BugGen生成的数据集训练的基于ML的故障分类模型在不同IP块上实现了高分类准确率（88.1%-93.2%）。", "conclusion": "BugGen提供了一种可扩展的解决方案，用于生成高质量的错误数据集，显著提高了验证效率和机器学习辅助调试。", "translation": "硬件复杂性持续给验证资源带来压力，这促使人们采用机器学习（ML）方法来提高调试效率。然而，机器学习辅助调试严重依赖多样化和可扩展的错误数据集，而现有的人工或自动化错误插入方法无法可靠地生成此类数据集。我们引入了BugGen，这是首个此类完全自主的多智能体管道，它利用大型语言模型（LLM）系统地生成、插入和验证RTL中的真实功能错误。BugGen划分模块，通过闭环智能体架构选择突变目标，并采用迭代细化和回滚机制来确保语法正确性和功能可检测性。在五个OpenTitan IP块上进行评估，BugGen生成了500个独特的错误，功能准确率达94%，并实现了每小时17.7个经过验证的错误的吞吐量——比典型的人工专家插入快五倍以上。此外，BugGen在OpenTitan回归测试中识别出104个以前未检测到的错误，突显了其在暴露验证覆盖率差距方面的效用。与Certitude相比，BugGen的语法准确性提高了一倍以上，更能深入暴露测试平台盲点，并生成了更具功能意义和复杂的错误场景。此外，当这些BugGen生成的数据集用于训练基于ML的故障分类模型时，我们在不同IP块上实现了高分类准确率（88.1%-93.2%），证实了所生成错误的实用性和真实性。因此，BugGen提供了一种可扩展的解决方案，用于生成高质量的错误数据集，显著提高了验证效率和机器学习辅助调试。", "summary": "BugGen提出了一种创新的、基于多智能体LLM的自校正管道，旨在自动化和规模化地生成、插入和验证现实的RTL功能错误。它通过迭代细化和回滚机制确保错误质量，并在OpenTitan IP块上展示了卓越的性能，包括高功能准确率、高吞吐量以及发现新的未检测错误。BugGen生成的数据集还被证明能有效训练ML辅助调试模型，显著提升了硬件验证效率。", "keywords": "RTL错误合成, LLM, 多智能体系统, 硬件验证, 错误数据集", "comments": "BugGen的创新之处在于其将LLM与多智能体自校正机制相结合，实现了RTL错误合成的自动化和高效率。其重要性在于解决了高质量、多样化错误数据集稀缺的问题，这对于推动ML辅助硬件调试至关重要。该方法不仅在效率上超越了传统人工方法，而且在错误质量和复杂性上优于现有自动化工具，有望大幅提升硬件验证的覆盖率和效率。"}}
{"id": "2506.10933", "title": "Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs", "authors": ["Ziwen Wang", "Yue Zhang", "Zhiqiang Zhang", "Sheng Quan Xie", "Alexander Lanzon", "William P. Heath", "Zhenhong Li"], "summary": "Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces\n(BCIs) can achieve high recognition accuracy with sufficient training data.\nTransfer learning presents a promising solution to alleviate data requirements\nfor the target subject by leveraging data from source subjects; however,\neffectively addressing individual variability among both target and source\nsubjects remains a challenge. This paper proposes a novel transfer learning\nframework, termed instance-based task-related component analysis (iTRCA), which\nleverages knowledge from source subjects while considering their individual\ncontributions. iTRCA extracts two types of features: (1) the subject-general\nfeature, capturing shared information between source and target subjects in a\ncommon latent space, and (2) the subject-specific feature, preserving the\nunique characteristics of the target subject. To mitigate negative transfer, we\nfurther design an enhanced framework, subject selection-based iTRCA (SS-iTRCA),\nwhich integrates a similarity-based subject selection strategy to identify\nappropriate source subjects for transfer based on their task-related components\n(TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected\ndataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA\nframeworks. This study provides a potential solution for developing\nhigh-performance SSVEP-based BCIs with reduced target subject data.", "comment": "IEEE Journal of Biomedical and Health Informatics", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.10933v1", "AI": {"title_translation": "基于实例的迁移学习与相似度感知主体选择用于跨主体SSVEP脑机接口", "tldr": "本文提出了一种名为iTRCA的新型迁移学习框架，并通过SS-iTRCA进一步增强，通过相似度感知的主体选择来提高SSVEP脑机接口的性能，同时减少对目标主体数据的需求。", "motivation": "稳态视觉诱发电位（SSVEP）脑机接口需要大量训练数据才能达到高识别精度。迁移学习有望通过利用源主体数据来缓解目标主体的数据需求，但如何有效解决目标和源主体之间的个体差异仍然是一个挑战。", "method": "本文提出了一种新颖的迁移学习框架——基于实例的任务相关成分分析（iTRCA），它在考虑源主体个体贡献的同时，利用其知识。iTRCA提取两种特征：(1) 主体通用特征，捕获源和目标主体在共同潜在空间中的共享信息；(2) 主体特定特征，保留目标主体的独特特性。为缓解负迁移，进一步设计了增强框架——基于主体选择的iTRCA（SS-iTRCA），它集成了基于相似度的主体选择策略，根据任务相关成分（TRCs）识别适合迁移的源主体。", "result": "在Benchmark、BETA和自收集数据集上的对比评估表明，所提出的iTRCA和SS-iTRCA框架是有效的。", "conclusion": "本研究为开发高性能、低目标主体数据需求的SSVEP脑机接口提供了一种潜在的解决方案。", "translation": "稳态视觉诱发电位（SSVEP）脑机接口（BCIs）在有足够训练数据的情况下可以实现高识别精度。迁移学习提供了一种有前景的解决方案，通过利用源主体的数据来缓解目标主体的数据需求；然而，有效解决目标和源主体之间的个体变异性仍然是一个挑战。本文提出了一种新颖的迁移学习框架，称为基于实例的任务相关成分分析（iTRCA），它在考虑源主体个体贡献的同时，利用源主体的知识。iTRCA提取两种类型的特征：(1) 主体通用特征，捕获源和目标主体在共同潜在空间中的共享信息；(2) 主体特定特征，保留目标主体的独特特性。为了减轻负迁移，我们进一步设计了一个增强框架，即基于主体选择的iTRCA（SS-iTRCA），它集成了一个基于相似度的主体选择策略，根据其任务相关成分（TRCs）识别适合迁移的源主体。在Benchmark、BETA和自收集数据集上的对比评估证明了所提出的iTRCA和SS-iTRCA框架的有效性。这项研究为开发具有减少目标主体数据量的高性能SSVEP脑机接口提供了一个潜在的解决方案。", "summary": "本文提出了一种名为iTRCA的新型迁移学习框架，旨在解决SSVEP脑机接口中个体变异性和数据需求问题。iTRCA提取主体通用和主体特定特征。为避免负迁移，进一步提出了SS-iTRCA，通过相似度感知的主体选择来识别合适的源主体。实验证明，该框架能有效提高SSVEP脑机接口的性能，同时减少目标主体的训练数据。", "keywords": "迁移学习, SSVEP, 脑机接口, 主体选择, iTRCA", "comments": "这项研究的创新之处在于提出了iTRCA和SS-iTRCA框架，有效地结合了主体通用和主体特定特征，并通过相似度感知的主体选择策略缓解了负迁移问题。这对于实际应用中SSVEP脑机接口的数据效率和性能提升具有重要意义，为开发更实用的脑机接口提供了新的思路。"}}
{"id": "2506.10462", "title": "Are We Generalizing from the Exception? An In-the-Wild Study on Group-Sensitive Conversation Design in Human-Agent Interactions", "authors": ["Ana Müller", "Sabina Jeschke", "Anja Richert"], "summary": "This paper investigates the impact of a group-adaptive conversation design in\ntwo socially interactive agents (SIAs) through two real-world studies. Both\nSIAs - Furhat, a social robot, and MetaHuman, a virtual agent - were equipped\nwith a conversational artificial intelligence (CAI) backend combining hybrid\nretrieval and generative models. The studies were carried out in an in-the-wild\nsetting with a total of $N = 188$ participants who interacted with the SIAs -\nin dyads, triads or larger groups - at a German museum. Although the results\ndid not reveal a significant effect of the group-sensitive conversation design\non perceived satisfaction, the findings provide valuable insights into the\nchallenges of adapting CAI for multi-party interactions and across different\nembodiments (robot vs.\\ virtual agent), highlighting the need for multimodal\nstrategies beyond linguistic pluralization. These insights contribute to the\nfields of Human-Agent Interaction (HAI), Human-Robot Interaction (HRI), and\nbroader Human-Machine Interaction (HMI), providing insights for future research\non effective dialogue adaptation in group settings.", "comment": "Accepted as a regular paper at the 2025 IEEE International Conference\n  on Robot and Human Interactive Communication (RO-MAN). \\c{opyright} IEEE.\n  This is the preprint version. The final version will appear in the IEEE\n  proceedings", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10462v1", "AI": {"title_translation": "我们是否在从例外中概括？一项关于人机交互中群体敏感对话设计的真实世界研究", "tldr": "本研究通过在真实世界环境中对社交机器人和虚拟代理进行实验，探讨了群体敏感对话设计的影响。尽管对用户满意度没有显著影响，但揭示了在多方和跨形态交互中对话式人工智能（CAI）适应的挑战，并强调了多模态策略的重要性。", "motivation": "该研究旨在调查群体适应性对话设计在两种社会交互智能体（SIAs）中的影响，特别是在真实世界环境中，以理解其对用户体验的潜在影响以及在多方交互中CAI适应的挑战。", "method": "通过在德国博物馆进行的两次真实世界研究。研究对象是Furhat（社交机器人）和MetaHuman（虚拟代理）这两种配备了结合混合检索和生成模型的对话式人工智能（CAI）后端的社会交互智能体（SIAs）。共有188名参与者以两人、三人或更大群体形式与SIAs进行交互。", "result": "群体敏感对话设计对感知满意度没有显著影响。然而，研究为CAI在多方交互和不同形态（机器人与虚拟代理）中进行适应的挑战提供了宝贵见解，并强调了超越语言复数化的多模态策略的需求。", "conclusion": "尽管群体敏感对话设计对用户满意度没有显著影响，但本研究为人类-智能体交互（HAI）、人类-机器人交互（HRI）以及更广泛的人机交互（HMI）领域提供了重要见解，特别是在群体设置中有效对话适应的未来研究方向上，强调了多模态策略的重要性。", "translation": "这篇论文通过两项真实世界研究，调查了群体适应性对话设计在两种社交互动智能体（SIAs）中的影响。这两种SIAs——社交机器人Furhat和虚拟智能体MetaHuman——都配备了结合混合检索和生成模型的对话式人工智能（CAI）后端。这些研究在德国一家博物馆的真实环境中进行，共有188名参与者以两人、三人或更大群体形式与SIAs进行互动。尽管结果并未显示群体敏感对话设计对感知满意度有显著影响，但研究发现为CAI在多方交互和不同形态（机器人与虚拟智能体）中进行适应的挑战提供了宝贵见解，强调了超越语言复数化的多模态策略的需求。这些见解有助于人类-智能体交互（HAI）、人类-机器人交互（HRI）以及更广泛的人机交互（HMI）领域，为未来在群体设置中进行有效对话适应的研究提供了启示。", "summary": "本研究在真实世界环境中，考察了群体敏感对话设计对社交机器人和虚拟代理两种社会交互智能体（SIAs）的影响。通过188名参与者与配备混合CAI的SIAs在德国博物馆的互动，发现该设计对用户满意度无显著影响。然而，研究揭示了CAI在多方和跨形态交互中适应的挑战，并强调了多模态策略超越语言处理的重要性，为HAI、HRI和HMI领域的未来研究提供了方向。", "keywords": "群体敏感对话设计, 社会交互智能体, 真实世界研究, 人机交互, 多模态策略", "comments": "这项研究的创新之处在于其“真实世界”（in-the-wild）的研究设置，而非传统的实验室环境，这增加了结果的现实意义。尽管群体敏感设计对满意度没有直接显著影响，但其揭示的关于多方交互中CAI适应的挑战以及对多模态策略的需求，对于未来构建更鲁棒、更自然的群组交互系统具有重要指导意义，避免了从“例外”情况中得出普遍结论。"}}
{"id": "2506.10226", "title": "ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators", "authors": ["Parsa Rahimi", "Sebastien Marcel"], "summary": "In this paper, we propose ScoreMix, a novel yet simple data augmentation\nstrategy leveraging the score compositional properties of diffusion models to\nenhance discriminator performance, particularly under scenarios with limited\nlabeled data. By convexly mixing the scores from different class-conditioned\ntrajectories during diffusion sampling, we generate challenging synthetic\nsamples that significantly improve discriminative capabilities in all studied\nbenchmarks. We systematically investigate class-selection strategies for mixing\nand discover that greater performance gains arise when combining classes\ndistant in the discriminator's embedding space, rather than close in the\ngenerator's condition space. Moreover, we empirically show that, under standard\nmetrics, the correlation between the generator's learned condition space and\nthe discriminator's embedding space is minimal. Our approach achieves notable\nperformance improvements without extensive parameter searches, demonstrating\npractical advantages for training discriminative models while effectively\nmitigating problems regarding collections of large datasets. Paper website:\nhttps://parsa-ra.github.io/scoremix", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10226v1", "AI": {"title_translation": "ScoreMix：通过扩散生成器中的分数组合改进人脸识别", "tldr": "ScoreMix是一种新颖的数据增强策略，通过混合扩散模型中的分数来生成具有挑战性的合成样本，从而在有限标记数据下显著提高人脸识别的判别器性能。", "motivation": "旨在解决在标记数据有限的情况下提高判别器性能的问题，并减轻收集大型数据集的难度。", "method": "提出ScoreMix，一种利用扩散模型分数组合特性的数据增强策略。通过在扩散采样过程中凸混合来自不同类别条件轨迹的分数，生成具有挑战性的合成样本。研究发现，混合判别器嵌入空间中距离较远的类别能带来更大的性能提升。", "result": "在所有研究的基准测试中显著提高了判别能力。经验性地表明，生成器学习的条件空间与判别器的嵌入空间之间的相关性最小。在不进行大量参数搜索的情况下取得了显著的性能改进。", "conclusion": "ScoreMix是一种实用且有效的方法，可以在不依赖大量数据集的情况下，显著提高判别模型的训练效果。", "translation": "在本文中，我们提出了 ScoreMix，这是一种新颖而简单的数据增强策略，它利用扩散模型的分数组合特性来增强判别器性能，特别是在标记数据有限的情况下。通过在扩散采样过程中凸混合来自不同类别条件轨迹的分数，我们生成了具有挑战性的合成样本，这些样本显著提高了所有研究基准中的判别能力。我们系统地研究了用于混合的类别选择策略，发现当组合判别器嵌入空间中距离较远的类别，而不是生成器条件空间中接近的类别时，会产生更大的性能增益。此外，我们根据标准指标经验性地表明，生成器学习的条件空间与判别器的嵌入空间之间的相关性是最小的。我们的方法在没有大量参数搜索的情况下取得了显著的性能改进，证明了在训练判别模型方面的实际优势，同时有效缓解了收集大型数据集的问题。论文网站：https://parsa-ra.github.io/scoremix", "summary": "ScoreMix是一种新颖的数据增强方法，通过在扩散模型中混合不同类别的分数来生成具有挑战性的合成图像，从而在有限标记数据条件下显著提升判别器（如人脸识别）的性能。研究表明，混合判别器嵌入空间中距离较远的类别效果更佳，并且生成器和判别器的空间相关性较低。该方法在无需大量参数调整的情况下实现了显著的性能提升，有效解决了大规模数据集收集的难题。", "keywords": "ScoreMix, 数据增强, 扩散模型, 人脸识别, 分数组合", "comments": "ScoreMix的创新之处在于利用扩散模型的“分数组合”特性进行数据增强，这提供了一种新颖的合成数据生成方式。其重要性体现在能够在数据稀缺的场景下有效提升判别模型的性能，并减少对大型数据集的依赖，这对于实际应用具有很大的价值。特别指出混合“判别器嵌入空间中距离较远的类别”这一发现，为基于扩散模型的合成数据生成提供了新的指导。"}}
{"id": "2506.10137", "title": "Self-Predictive Representations for Combinatorial Generalization in Behavioral Cloning", "authors": ["Daniel Lawson", "Adriana Hugessen", "Charlotte Cloutier", "Glen Berseth", "Khimya Khetarpal"], "summary": "Behavioral cloning (BC) methods trained with supervised learning (SL) are an\neffective way to learn policies from human demonstrations in domains like\nrobotics. Goal-conditioning these policies enables a single generalist policy\nto capture diverse behaviors contained within an offline dataset. While\ngoal-conditioned behavior cloning (GCBC) methods can perform well on\nin-distribution training tasks, they do not necessarily generalize zero-shot to\ntasks that require conditioning on novel state-goal pairs, i.e. combinatorial\ngeneralization. In part, this limitation can be attributed to a lack of\ntemporal consistency in the state representation learned by BC; if temporally\nrelated states are encoded to similar latent representations, then the\nout-of-distribution gap for novel state-goal pairs would be reduced. Hence,\nencouraging this temporal consistency in the representation space should\nfacilitate combinatorial generalization. Successor representations, which\nencode the distribution of future states visited from the current state, nicely\nencapsulate this property. However, previous methods for learning successor\nrepresentations have relied on contrastive samples, temporal-difference (TD)\nlearning, or both. In this work, we propose a simple yet effective\nrepresentation learning objective, $\\text{BYOL-}\\gamma$ augmented GCBC, which\nis not only able to theoretically approximate the successor representation in\nthe finite MDP case without contrastive samples or TD learning, but also,\nresults in competitive empirical performance across a suite of challenging\ntasks requiring combinatorial generalization.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10137v1", "AI": {"title_translation": "行为克隆中组合泛化的自预测表征", "tldr": "本文提出了一种名为BYOL-γ增强型GCBC的简单有效表示学习方法，旨在解决行为克隆中组合泛化能力不足的问题，该方法无需对比样本或TD学习即可近似后继表示，并在需要组合泛化的任务上表现出色。", "motivation": "目标条件行为克隆（GCBC）方法在训练任务分布内表现良好，但在需要对新颖状态-目标对进行条件化（即组合泛化）的任务上，无法实现零样本泛化。这部分归因于行为克隆学习到的状态表示缺乏时间一致性。", "method": "本文提出了一种简单而有效的表示学习目标，即BYOL-γ增强型GCBC。该方法无需对比样本或时序差分（TD）学习，即可在有限MDP情况下理论上近似后继表示，并通过鼓励表示空间中的时间一致性来促进组合泛化。", "result": "该方法不仅能够理论上近似有限MDP情况下的后继表示，而且在需要组合泛化的一系列具有挑战性的任务中，取得了具有竞争力的经验性能。", "conclusion": "通过引入BYOL-γ增强型GCBC，可以有效地解决行为克隆在组合泛化方面的局限性，因为它能够学习到具有时间一致性的自预测表示，从而在无需对比学习或TD学习的情况下，实现对后继表示的近似，并在复杂任务中展现出优异的泛化能力。", "translation": "行为克隆（BC）方法通过监督学习（SL）训练，是学习机器人等领域人类演示策略的有效方式。对这些策略进行目标条件化，使得单个通用策略能够捕获离线数据集中包含的各种行为。虽然目标条件行为克隆（GCBC）方法在分布内训练任务上表现良好，但它们不一定能零样本泛化到需要以新颖状态-目标对为条件的任务，即组合泛化。部分原因在于BC学习到的状态表示缺乏时间一致性；如果时间相关的状态被编码为相似的潜在表示，那么新颖状态-目标对的分布外差距将会减小。因此，鼓励表示空间中的这种时间一致性应该有助于组合泛化。后继表示，即编码从当前状态访问的未来状态分布，很好地封装了这一特性。然而，以往学习后继表示的方法依赖于对比样本、时序差分（TD）学习或两者兼有。在这项工作中，我们提出了一种简单而有效的表示学习目标，即BYOL-γ增强型GCBC，它不仅能够在有限MDP情况下无需对比样本或TD学习即可理论上近似后继表示，而且在一系列需要组合泛化的具有挑战性任务中，取得了具有竞争力的经验性能。", "summary": "本文针对行为克隆（BC）在零样本组合泛化方面的不足，提出了一种名为BYOL-γ增强型GCBC的新型表示学习方法。该方法通过鼓励状态表示的时间一致性来解决现有目标条件行为克隆（GCBC）的局限性。与以往依赖对比样本或时序差分（TD）学习的后继表示方法不同，BYOL-γ增强型GCBC能够理论上近似后继表示，并且在实际应用中，在需要组合泛化的复杂任务上展现出强大的性能。", "keywords": "行为克隆, 组合泛化, 后继表示, 自预测, BYOL-γ", "comments": "这篇论文的创新点在于提出了一种无需对比学习或时序差分学习即可近似后继表示的方法，从而解决了行为克隆在组合泛化方面的关键挑战。其简洁而有效的BYOL-γ增强型GCBC方法，为机器人学习等领域提供了一种更鲁棒的策略学习途径，具有重要的理论和实践意义。"}}
{"id": "2506.10889", "title": "Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning", "authors": ["Waylon Luo", "Jiapeng Zhao", "Tong Zhan", "Qiang Guan"], "summary": "Present-day quantum systems face critical bottlenecks, including limited\nqubit counts, brief coherence intervals, and high susceptibility to errors-all\nof which obstruct the execution of large and complex circuits. The advancement\nof quantum algorithms has outpaced the capabilities of existing quantum\nhardware, making it difficult to scale computations effectively. Additionally,\ninconsistencies in hardware performance and pervasive quantum noise undermine\nsystem stability and computational accuracy. To optimize quantum workloads\nunder these constraints, strategic approaches to task scheduling and resource\ncoordination are essential. These methods must aim to accelerate processing,\nretain operational fidelity, and reduce the communication burden inherent to\ndistributed setups. One of the persistent challenges in this domain is how to\nefficiently divide and execute large circuits across multiple quantum\nprocessors (QPUs), especially in error-prone environments. In response, we\nintroduce a simulation-based tool that supports distributed scheduling and\nconcurrent execution of quantum jobs on networked QPUs connected via real-time\nclassical channels. The tool models circuit decomposition for workloads that\nsurpass individual QPU limits, allowing for parallel execution through\ninter-processor communication. Using this simulation environment, we compare\nfour distinct scheduling techniques-among them, a model informed by\nreinforcement learning. These strategies are evaluated across multiple metrics,\nincluding runtime efficiency, fidelity preservation, and communication costs.\nOur analysis underscores the trade-offs inherent in each approach and\nhighlights how parallelized, noise-aware scheduling can meaningfully improve\ncomputational throughput in distributed quantum infrastructures.", "comment": "10 pages, 6 figures, ICPP 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.10889v1", "AI": {"title_translation": "基于强化学习的量子云自适应作业调度", "tldr": "鉴于当前量子系统面临的瓶颈，本文引入了一个基于仿真的工具，用于在联网量子处理器（QPU）上进行分布式量子作业调度和并发执行，并比较了包括强化学习在内的四种调度技术，旨在提高分布式量子基础设施的计算吞吐量。", "motivation": "当前量子系统面临关键瓶颈，包括有限的量子比特数、短暂的相干时间和对错误的高敏感性，这些都阻碍了大型复杂电路的执行。量子算法的进步已经超越了现有量子硬件的能力，难以有效扩展计算。此外，硬件性能的不一致和普遍存在的量子噪声损害了系统稳定性和计算精度。在这些限制下，优化量子工作负载需要战略性的任务调度和资源协调方法，以加速处理、保持操作保真度并减少分布式设置中固有的通信负担。在错误易发环境中，如何有效分割和执行大型电路跨多个量子处理器（QPU）是一个持续的挑战。", "method": "本文引入了一个基于仿真的工具，支持通过实时经典通道连接的联网QPU上量子作业的分布式调度和并发执行。该工具对超出单个QPU限制的工作负载进行电路分解建模，允许通过处理器间通信进行并行执行。利用这个仿真环境，论文比较了四种不同的调度技术，其中包括一种由强化学习驱动的模型。这些策略通过运行时效率、保真度保持和通信成本等多个指标进行评估。", "result": "分析强调了每种方法固有的权衡，并指出并行化、噪声感知调度可以显著提高分布式量子基础设施中的计算吞吐量。", "conclusion": "并行化、噪声感知的调度方法，特别是那些利用强化学习的，能够有效应对量子硬件的局限性和噪声，显著提升分布式量子系统中的计算吞吐量。", "translation": "当前的量子系统面临关键瓶颈，包括有限的量子比特数、短暂的相干时间和对错误的高敏感性——所有这些都阻碍了大型复杂电路的执行。量子算法的进步已经超越了现有量子硬件的能力，使得有效扩展计算变得困难。此外，硬件性能的不一致和普遍存在的量子噪声损害了系统稳定性和计算精度。为了在这些限制下优化量子工作负载，任务调度和资源协调的战略方法至关重要。这些方法必须旨在加速处理、保持操作保真度并减少分布式设置中固有的通信负担。在这个领域中，一个持续的挑战是如何在错误易发环境中，高效地将大型电路分解并跨多个量子处理器（QPU）执行。为此，我们引入了一个基于仿真的工具，支持通过实时经典通道连接的联网QPU上量子作业的分布式调度和并发执行。该工具对超出单个QPU限制的工作负载进行电路分解建模，允许通过处理器间通信进行并行执行。利用这个仿真环境，我们比较了四种不同的调度技术——其中包括一种由强化学习驱动的模型。这些策略通过运行时效率、保真度保持和通信成本等多个指标进行评估。我们的分析强调了每种方法固有的权衡，并指出并行化、噪声感知调度可以显著提高分布式量子基础设施中的计算吞吐量。", "summary": "本文针对当前量子系统在处理大型复杂电路时面临的量子比特限制、相干时间短、高错误率、硬件与算法能力不匹配以及噪声干扰等挑战，提出了一种基于仿真的工具。该工具旨在实现量子云环境中联网量子处理器（QPU）上的分布式作业调度和并发执行，支持电路分解和处理器间通信以实现并行处理。通过比较包括强化学习模型在内的四种调度技术，研究发现并行化、噪声感知的调度方法能够显著提升分布式量子基础设施的计算吞吐量，同时权衡运行时效率、保真度和通信成本。", "keywords": "量子调度, 强化学习, 分布式量子计算, 量子云, 噪声感知", "comments": "本文的创新之处在于提出了一个基于仿真的工具，将强化学习应用于分布式量子作业调度，以应对量子硬件的固有局限性。其关注噪声感知和并行化调度对于将量子计算从理论推向实际应用具有重要意义。该研究不仅提出了解决方案，还通过评估不同调度策略的权衡，为未来分布式量子系统的设计和优化提供了有价值的见解。"}}
{"id": "2506.10481", "title": "OIBench: Benchmarking Strong Reasoning Models with Olympiad in Informatics", "authors": ["Yaoming Zhu", "Junxin Wang", "Yiyang Li", "Lin Qiu", "ZongYu Wang", "Jun Xu", "Xuezhi Cao", "Yuhuai Wei", "Mingshi Wang", "Xunliang Cai", "Rong Ma"], "summary": "As models become increasingly sophisticated, conventional algorithm\nbenchmarks are increasingly saturated, underscoring the need for more\nchallenging benchmarks to guide future improvements in algorithmic reasoning.\nThis paper introduces OIBench, a high-quality, private, and challenging\nolympiad-level informatics dataset comprising 250 carefully curated original\nproblems. We detail the construction methodology of the benchmark, ensuring a\ncomprehensive assessment across various programming paradigms and complexities,\nand we demonstrate its contamination-resistant properties via experiments. We\npropose Time/Space Completion Curves for finer-grained efficiency analysis and\nenable direct human-model comparisons through high-level participant\nevaluations. Our experiments reveal that while open-source models lag behind\nclosed-source counterparts, current SOTA models already outperform most human\nparticipants in both correctness and efficiency, while still being suboptimal\ncompared to the canonical solutions. By releasing OIBench as a fully\nopen-source resource (https://huggingface.co/datasets/AGI-Eval/OIBench), we\nhope this benchmark will contribute to advancing code reasoning capabilities\nfor future LLMs.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10481v1", "AI": {"title_translation": "OIBench：使用信息学奥赛基准测试强大的推理模型", "tldr": "OIBench是一个新的、高质量、挑战性的信息学奥赛数据集，用于评估和推动算法推理模型的发展。实验表明，当前最先进的模型在正确性和效率上已超越大多数人类参与者，但与规范解仍有差距。", "motivation": "随着模型日益复杂，传统算法基准测试已趋于饱和，需要更具挑战性的基准来指导算法推理的未来改进。", "method": "本文介绍了OIBench，一个包含250个精心策划的原创问题的奥赛级别信息学数据集。详细阐述了基准的构建方法，以确保对各种编程范式和复杂性进行全面评估，并通过实验证明了其抗污染特性。提出了时间/空间完成曲线用于更细粒度的效率分析，并通过高级参与者评估实现人机直接比较。", "result": "实验表明，开源模型落后于闭源模型，而当前最先进的模型在正确性和效率方面已经超越了大多数人类参与者，但与规范解相比仍不理想。", "conclusion": "OIBench作为完全开源的资源发布，有望推动未来大型语言模型的代码推理能力发展。", "translation": "随着模型日益复杂，传统算法基准测试日益饱和，这凸显了需要更具挑战性的基准来指导算法推理的未来改进。本文介绍了OIBench，一个高质量、私有且具有挑战性的奥赛级别信息学数据集，包含250个精心策划的原创问题。我们详细阐述了基准的构建方法，确保对各种编程范式和复杂性进行全面评估，并通过实验证明了其抗污染特性。我们提出了时间/空间完成曲线，用于更细粒度的效率分析，并通过高级参与者评估实现人机直接比较。我们的实验表明，尽管开源模型落后于闭源模型，但当前最先进的模型在正确性和效率方面已经超越了大多数人类参与者，但与规范解相比仍不理想。通过将OIBench作为完全开源资源（https://huggingface.co/datasets/AGI-Eval/OIBench）发布，我们希望这个基准能够促进未来大型语言模型的代码推理能力发展。", "summary": "OIBench是一个新的、高质量的信息学奥赛数据集，旨在解决传统算法基准测试饱和的问题，为强大的算法推理模型提供更具挑战性的评估。该数据集包含250个原创问题，并采用特殊方法确保其抗污染性。论文还引入了时间/空间完成曲线进行效率分析，并支持人机对比。实验结果显示，当前SOTA模型在正确性和效率上已超越多数人类，但仍未达到最优解水平。OIBench的开源发布旨在推动LLM在代码推理领域的发展。", "keywords": "OIBench, 算法推理, 基准测试, 信息学奥赛, 大模型", "comments": "OIBench的创新之处在于其构建了一个高质量、抗污染且具有奥赛难度的算法推理基准，有效解决了现有基准饱和的问题。通过引入时间/空间完成曲线和支持人机对比，提供了更细致的评估维度。其重要性在于为未来大型语言模型在代码推理能力上的进步提供了一个有力的评估工具和方向。"}}
{"id": "2506.10586", "title": "Size-adaptive Hypothesis Testing for Fairness", "authors": ["Antonio Ferrara", "Francesco Cozzi", "Alan Perotti", "André Panisson", "Francesco Bonchi"], "summary": "Determining whether an algorithmic decision-making system discriminates\nagainst a specific demographic typically involves comparing a single point\nestimate of a fairness metric against a predefined threshold. This practice is\nstatistically brittle: it ignores sampling error and treats small demographic\nsubgroups the same as large ones. The problem intensifies in intersectional\nanalyses, where multiple sensitive attributes are considered jointly, giving\nrise to a larger number of smaller groups. As these groups become more\ngranular, the data representing them becomes too sparse for reliable\nestimation, and fairness metrics yield excessively wide confidence intervals,\nprecluding meaningful conclusions about potential unfair treatments.\n  In this paper, we introduce a unified, size-adaptive, hypothesis-testing\nframework that turns fairness assessment into an evidence-based statistical\ndecision. Our contribution is twofold. (i) For sufficiently large subgroups, we\nprove a Central-Limit result for the statistical parity difference, leading to\nanalytic confidence intervals and a Wald test whose type-I (false positive)\nerror is guaranteed at level $\\alpha$. (ii) For the long tail of small\nintersectional groups, we derive a fully Bayesian Dirichlet-multinomial\nestimator; Monte-Carlo credible intervals are calibrated for any sample size\nand naturally converge to Wald intervals as more data becomes available. We\nvalidate our approach empirically on benchmark datasets, demonstrating how our\ntests provide interpretable, statistically rigorous decisions under varying\ndegrees of data availability and intersectionality.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10586v1", "AI": {"title_translation": "公平性的大小自适应假设检验", "tldr": "本文提出了一种统一的大小自适应假设检验框架，用于公平性评估，通过结合针对大型子群的中心极限定理和针对小型交叉群的贝叶斯估计器，解决了现有方法在统计脆弱性和数据稀疏性方面的问题。", "motivation": "现有的算法决策系统公平性评估方法存在统计脆弱性，忽略抽样误差，对大小不同的群体一视同仁。尤其在考虑多个敏感属性的交叉分析中，小群体的数据稀疏性导致估计不可靠和置信区间过宽，难以得出有意义的结论。", "method": "本文引入了一个统一的、大小自适应的假设检验框架，将公平性评估转化为基于证据的统计决策。具体方法包括：(i) 对于足够大的子群，证明了统计均等性差异的中心极限定理，从而得到解析置信区间和I型错误率保证在α水平的Wald检验。(ii) 对于小型交叉群体的长尾，推导出一个完全贝叶斯的Dirichlet-多项式估计器，其蒙特卡洛可信区间适用于任何样本量，并随着数据量的增加自然收敛到Wald区间。", "result": "该方法在基准数据集上进行了实证验证，结果表明所提出的检验在不同数据可用性和交叉性程度下，提供了可解释的、统计上严谨的决策。", "conclusion": "本文提出的统一的、大小自适应的假设检验框架，能够将公平性评估转化为基于证据的统计决策，有效解决了现有方法在处理不同大小群体和数据稀疏性时的挑战。", "translation": "确定算法决策系统是否歧视特定人群通常涉及将公平性指标的单一估计值与预定义阈值进行比较。这种做法在统计上是脆弱的：它忽略了抽样误差，并对小的和大的群体一视同仁。在交叉分析中，当多个敏感属性被联合考虑时，问题会加剧，导致出现更多数量的小群体。随着这些群体变得更加细化，代表它们的数据变得过于稀疏，无法进行可靠估计，公平性指标会产生过宽的置信区间，从而无法对潜在的不公平待遇得出有意义的结论。\n\n在本文中，我们引入了一个统一的、大小自适应的假设检验框架，将公平性评估转化为基于证据的统计决策。我们的贡献是双重的。(i) 对于足够大的子群，我们证明了统计均等性差异的中心极限定理，从而得到解析置信区间和I型（假阳性）错误率保证在α水平的Wald检验。(ii) 对于小型交叉群体的长尾，我们推导出一个完全贝叶斯的Dirichlet-多项式估计器；蒙特卡洛可信区间针对任何样本量进行校准，并随着更多数据的可用而自然收敛到Wald区间。我们通过基准数据集上的实证验证了我们的方法，展示了我们的检验如何在不同程度的数据可用性和交叉性下提供可解释的、统计上严谨的决策。", "summary": "本文提出了一种用于公平性评估的统一大小自适应假设检验框架，旨在解决传统方法在处理不同规模群体和数据稀疏性时的统计脆弱性问题。该框架结合了针对大型子群的中心极限定理和Wald检验，以及针对小型交叉群体的贝叶斯Dirichlet-多项式估计器和蒙特卡洛可信区间。实验证明，该方法能在不同数据量和交叉性条件下提供可解释且统计严谨的公平性决策。", "keywords": "公平性, 假设检验, 大小自适应, 交叉性, 贝叶斯统计", "comments": "该论文的创新之处在于提出了一个统一的、大小自适应的假设检验框架，有效地结合了频率派和贝叶斯统计方法，以应对公平性评估中不同规模群体的数据稀疏性挑战。它解决了现有方法在统计严谨性上的不足，特别是在处理细粒度交叉群体时的实用性问题，为公平性审计提供了更可靠的工具。"}}
{"id": "2506.10331", "title": "Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video", "authors": ["Fei Zhao", "Da Pan", "Zelu Qi", "Ping Shi"], "summary": "In response to the rising prominence of the Metaverse, omnidirectional videos\n(ODVs) have garnered notable interest, gradually shifting from\nprofessional-generated content (PGC) to user-generated content (UGC). However,\nthe study of audio-visual quality assessment (AVQA) within ODVs remains\nlimited. To address this, we construct a dataset of UGC omnidirectional audio\nand video (A/V) content. The videos are captured by five individuals using two\ndifferent types of omnidirectional cameras, shooting 300 videos covering 10\ndifferent scene types. A subjective AVQA experiment is conducted on the dataset\nto obtain the Mean Opinion Scores (MOSs) of the A/V sequences. After that, to\nfacilitate the development of UGC-ODV AVQA fields, we construct an effective\nAVQA baseline model on the proposed dataset, of which the baseline model\nconsists of video feature extraction module, audio feature extraction and\naudio-visual fusion module. The experimental results demonstrate that our model\nachieves optimal performance on the proposed dataset.", "comment": "Our paper has been accepted by ICME 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10331v1", "AI": {"title_translation": "用户生成全向视频音视频质量评估数据集与方法研究", "tldr": "本文构建了一个用户生成全向视频的音视频质量评估数据集，并提出了一个有效的基线模型。", "motivation": "随着元宇宙的兴起，全向视频（ODV）逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，全向视频中音视频质量评估（AVQA）的研究仍然有限。", "method": "构建了一个用户生成全向音视频内容数据集，包含由五个人使用两种不同类型全向相机拍摄的300个视频，涵盖10种场景类型。对数据集进行了主观AVQA实验，获得了音视频序列的平均意见得分（MOS）。在此基础上，构建了一个有效的AVQA基线模型，该模型包括视频特征提取模块、音频特征提取模块和音视频融合模块。", "result": "实验结果表明，所提出的模型在所构建的数据集上取得了最佳性能。", "conclusion": "本文构建了用户生成全向视频的音视频质量评估数据集，并提出了一个表现优异的基线模型，有助于推动UGC-ODV AVQA领域的发展。", "translation": "针对元宇宙日益突出的地位，全向视频（ODV）引起了广泛关注，并逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，全向视频中音视频质量评估（AVQA）的研究仍然有限。为了解决这个问题，我们构建了一个用户生成全向音视频（A/V）内容数据集。视频由五个人使用两种不同类型的全向相机拍摄，共300个视频，涵盖10种不同的场景类型。对数据集进行了主观AVQA实验，以获得A/V序列的平均意见得分（MOS）。之后，为了促进UGC-ODV AVQA领域的发展，我们在所提出的数据集上构建了一个有效的AVQA基线模型，该基线模型由视频特征提取模块、音频特征提取模块和音视频融合模块组成。实验结果表明，我们的模型在所提出的数据集上取得了最佳性能。", "summary": "针对用户生成全向视频（UGC-ODV）音视频质量评估（AVQA）研究的不足，本文构建了一个包含300个视频和MOS得分的UGC全向音视频数据集。在此基础上，研究人员提出了一个由视频、音频特征提取及融合模块组成的AVQA基线模型。实验结果表明该模型在该数据集上表现最佳，为UGC-ODV AVQA领域的发展奠定了基础。", "keywords": "全向视频, 音视频质量评估, 用户生成内容, 数据集, 基线模型", "comments": "这篇论文的创新点在于首次针对用户生成全向视频（UGC-ODV）构建了专属的音视频质量评估数据集，填补了该领域的空白。同时，提出的基线模型为后续研究提供了有益的参考。其重要性在于随着元宇宙和UGC内容的普及，对这类视频的质量评估需求将日益增长。"}}
{"id": "2506.10231", "title": "Classifying Unreliable Narrators with Large Language Models", "authors": ["Anneliese Brei", "Katharine Henry", "Abhisheik Sharma", "Shashank Srivastava", "Snigdha Chaturvedi"], "summary": "Often when we interact with a first-person account of events, we consider\nwhether or not the narrator, the primary speaker of the text, is reliable. In\nthis paper, we propose using computational methods to identify unreliable\nnarrators, i.e. those who unintentionally misrepresent information. Borrowing\nliterary theory from narratology to define different types of unreliable\nnarrators based on a variety of textual phenomena, we present TUNa, a\nhuman-annotated dataset of narratives from multiple domains, including blog\nposts, subreddit posts, hotel reviews, and works of literature. We define\nclassification tasks for intra-narrational, inter-narrational, and\ninter-textual unreliabilities and analyze the performance of popular\nopen-weight and proprietary LLMs for each. We propose learning from literature\nto perform unreliable narrator classification on real-world text data. To this\nend, we experiment with few-shot, fine-tuning, and curriculum learning\nsettings. Our results show that this task is very challenging, and there is\npotential for using LLMs to identify unreliable narrators. We release our\nexpert-annotated dataset and code and invite future research in this area.", "comment": "ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10231v1", "AI": {"title_translation": "使用大型语言模型对不可靠叙述者进行分类", "tldr": "本文提出使用大型语言模型（LLMs）来识别不可靠的叙述者。为此，作者创建了一个名为TUNa的人工标注数据集，并定义了不同的分类任务。研究发现这项任务具有挑战性，但LLMs显示出潜力。", "motivation": "在与第一人称叙述互动时，判断叙述者是否可靠是一个常见问题。本文旨在提出计算方法来识别那些无意中歪曲信息的不可靠叙述者。", "method": "本文借鉴叙事学中的文学理论来定义不同类型的不可靠叙述者。作者构建了一个名为TUNa的人工标注数据集，包含来自博客、Reddit、酒店评论和文学作品等多个领域的叙事。研究定义了叙事内部、叙事之间和文本之间的不可靠性分类任务，并分析了流行的开源和专有大型语言模型在这些任务上的表现。实验采用了少样本学习、微调和课程学习设置。", "result": "识别不可靠叙述者的任务非常具有挑战性。尽管如此，研究结果表明使用大型语言模型来识别不可靠叙述者具有潜力。", "conclusion": "识别不可靠叙述者是一个具有挑战性的任务，但大型语言模型在此领域显示出潜力。论文发布了专家标注的数据集和代码，鼓励未来的研究。", "translation": "通常当我们与第一人称的事件描述互动时，我们会考虑叙述者（文本的主要讲述者）是否可靠。在本文中，我们提出使用计算方法来识别不可靠的叙述者，即那些无意中歪曲信息的人。我们借鉴叙事学中的文学理论，根据各种文本现象定义了不同类型的不可靠叙述者，并提出了TUNa，这是一个经过人工标注的数据集，包含来自多个领域的叙事，包括博客文章、Reddit帖子、酒店评论和文学作品。我们定义了叙事内部、叙事之间和文本之间的不可靠性分类任务，并分析了流行的开源和专有大型语言模型在每个任务上的表现。我们提出从文学中学习，以对真实世界文本数据执行不可靠叙述者分类。为此，我们尝试了少样本、微调和课程学习设置。我们的结果表明，这项任务非常具有挑战性，并且使用大型语言模型识别不可靠叙述者具有潜力。我们发布了我们专家标注的数据集和代码，并邀请该领域的未来研究。", "summary": "本文提出利用大型语言模型（LLMs）通过计算方法识别第一人称叙述中无意歪曲信息的不可靠叙述者。研究借鉴叙事学理论定义了叙述者类型，并构建了一个跨领域的人工标注数据集TUNa。论文定义了叙事内部、叙事之间和文本之间的不可靠性分类任务，并评估了不同LLMs在少样本、微调和课程学习设置下的性能。结果表明该任务极具挑战性，但LLMs展现出识别不可靠叙述者的潜力。作者发布了数据集和代码以促进后续研究。", "keywords": "不可靠叙述者, 大型语言模型, 叙事学, 数据集, 文本分类", "comments": "本文将文学理论（叙事学）与计算方法相结合，解决了一个新颖且具有挑战性的自然语言处理任务。创建多领域、人工标注的TUNa数据集是其重要贡献，为未来研究提供了宝贵资源。研究全面探讨了不同LLM训练范式，体现了解决复杂问题的严谨性。明确指出任务的难度并呼吁未来研究，也体现了其务实性。"}}
{"id": "2506.10488", "title": "Sheet Music Benchmark: Standardized Optical Music Recognition Evaluation", "authors": ["Juan C. Martinez-Sevilla", "Joan Cerveto-Serrano", "Noelia Luna", "Greg Chapman", "Craig Sapp", "David Rizo", "Jorge Calvo-Zaragoza"], "summary": "In this work, we introduce the Sheet Music Benchmark (SMB), a dataset of six\nhundred and eighty-five pages specifically designed to benchmark Optical Music\nRecognition (OMR) research. SMB encompasses a diverse array of musical\ntextures, including monophony, pianoform, quartet, and others, all encoded in\nCommon Western Modern Notation using the Humdrum **kern format. Alongside SMB,\nwe introduce the OMR Normalized Edit Distance (OMR-NED), a new metric tailored\nexplicitly for evaluating OMR performance. OMR-NED builds upon the widely-used\nSymbol Error Rate (SER), offering a fine-grained and detailed error analysis\nthat covers individual musical elements such as note heads, beams, pitches,\naccidentals, and other critical notation features. The resulting numeric score\nprovided by OMR-NED facilitates clear comparisons, enabling researchers and\nend-users alike to identify optimal OMR approaches. Our work thus addresses a\nlong-standing gap in OMR evaluation, and we support our contributions with\nbaseline experiments using standardized SMB dataset splits for training and\nassessing state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10488v1", "AI": {"title_translation": "乐谱基准：标准化光学音乐识别评估", "tldr": "引入了乐谱基准（SMB）数据集和OMR归一化编辑距离（OMR-NED）度量，以标准化光学音乐识别（OMR）的评估。", "motivation": "解决光学音乐识别（OMR）评估中长期存在的空白，提供标准化的基准数据集和更精细的评估方法。", "method": "引入了Sheet Music Benchmark (SMB) 数据集，包含685页不同音乐纹理的乐谱，采用Humdrum **kern格式编码。同时引入了OMR Normalized Edit Distance (OMR-NED) 新度量，该度量基于Symbol Error Rate (SER)，并提供对音符头、符杠、音高、变音记号等单个音乐元素的详细错误分析。", "result": "OMR-NED提供的数值分数便于清晰比较，使研究人员和最终用户能够识别最佳OMR方法。通过使用标准化的SMB数据集拆分进行基线实验，支持了他们的贡献。", "conclusion": "该工作通过引入SMB数据集和OMR-NED度量，成功填补了OMR评估领域的长期空白，为标准化OMR研究和性能比较提供了基础。", "translation": "在这项工作中，我们介绍了乐谱基准（SMB），这是一个专门为基准测试光学音乐识别（OMR）研究而设计的包含685页的数据集。SMB涵盖了多种音乐纹理，包括单音、钢琴曲、四重奏等，所有这些都使用Humdrum **kern格式编码为常用西方现代记谱法。除了SMB，我们还引入了OMR归一化编辑距离（OMR-NED），这是一种专门用于评估OMR性能的新度量。OMR-NED建立在广泛使用的符号错误率（SER）之上，提供了细粒度且详细的错误分析，涵盖了单个音乐元素，如音符头、符杠、音高、变音记号和其他关键的记谱特征。OMR-NED提供的数值分数有助于清晰的比较，使研究人员和最终用户都能识别最佳OMR方法。因此，我们的工作解决了OMR评估中长期存在的空白，我们通过使用标准化的SMB数据集拆分进行基线实验来支持我们的贡献，用于训练和评估最先进的方法。", "summary": "本文推出了乐谱基准（SMB）数据集和OMR归一化编辑距离（OMR-NED）评估指标，旨在标准化光学音乐识别（OMR）的性能评估。SMB数据集包含685页多样化的乐谱，而OMR-NED则在现有符号错误率（SER）基础上，提供更精细的错误分析，涵盖音符的各种细节。这些工具共同填补了OMR评估领域的空白，并促进了不同OMR方法间的清晰比较。", "keywords": "光学音乐识别, 乐谱基准, 评估指标, 数据集, OMR-NED", "comments": "这项工作通过提供一个专门设计的基准数据集（SMB）和一种新的、更精细的评估指标（OMR-NED），显著提升了光学音乐识别（OMR）领域研究的标准化水平。OMR-NED对各种音乐元素的详细错误分析是其创新之处，有助于研究人员更准确地理解模型性能和优化方向，填补了长期存在的评估空白。"}}
{"id": "2506.10820", "title": "A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for Nonlinear Differential Equations", "authors": ["Subhash Paudel", "Nail K. Yamaleev"], "summary": "As has been shown in our previous work, the parallel-in-time direct inverse\n(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,\n2024) imposes some constraint on the maximum number of time levels, $N_t$, that\ncan be integrated in parallel. To circumvent this problem and further increase\nthe speedup, we combine the ParaDIn method with the Parareal algorithm to\nefficiently parallelize the first-order time derivative term in nonlinear\npartial differential equations discretized by the method of lines. The main\nidea of the proposed approach is to use a block-Jacobi preconditioner, so that\neach block is solved by using the ParaDIn method. To accelerate the convergence\nof Jacobi iterations, we use the Parareal method which can be interpreted as a\ntwo-level multigrid method in time. In contrast to the conventional Parareal\nalgorithm whose coarse grid correction step is performed sequentially, both the\ncoarse- and fine-grid propagators in the proposed approach are implemented in\nparallel by using the ParaDIn method, thus significantly increasing the\nparallel performance of the combined algorithm. Numerical results show that the\nnew combined ParaDIn-Parareal method provides the speedup of up to 124 on 480\ncomputing cores as compared with the sequential first-order implicit backward\ndifference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with\nboth smooth and discontinuous solutions.", "comment": "24 pages. arXiv admin note: text overlap with arXiv:2406.00878", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10820v1", "AI": {"title_translation": "非线性微分方程的并行时间直接逆(ParaDIn)-Parareal组合方法", "tldr": "本文提出了一种结合ParaDIn和Parareal的新方法，通过并行化粗细网格传播器，显著提高了非线性微分方程时间并行的速度，实现了高达124倍的加速。", "motivation": "现有的并行时间直接逆(ParaDIn)方法在并行集成的时间层数上存在限制，阻碍了进一步的加速，因此需要一种新的方法来克服此限制并提高并行效率。", "method": "本文提出了一种结合ParaDIn和Parareal算法的方法。该方法的核心思想是使用块-Jacobi预处理器，其中每个块都通过ParaDIn方法求解。为了加速Jacobi迭代的收敛，引入Parareal方法，该方法可以被解释为时间上的两级多重网格方法。与传统的Parareal算法（其粗网格校正步骤是顺序执行的）不同，所提出方法中的粗网格和细网格传播器都通过ParaDIn方法并行实现，从而显著提高了组合算法的并行性能。", "result": "新的ParaDIn-Parareal组合方法显著提高了并行性能。数值结果显示，在480个计算核心上，相比于顺序一阶隐式后向差分(BDF1)方案，该方法对二维非线性热方程和Burgers方程（包括平滑和不连续解）实现了高达124倍的加速比。", "conclusion": "结合ParaDIn和Parareal算法，并通过ParaDIn方法并行实现粗细网格传播器，能够有效克服ParaDIn方法的时间层数限制，显著提升非线性微分方程并行求解的效率和速度。", "translation": "正如我们之前的工作所示，由Yamaleev和Paudel在(arXiv: 2406.00878v1, 2024)中引入的并行时间直接逆(ParaDIn)方法对可并行集成的时间层数$N_t$施加了某些约束。为了规避这个问题并进一步提高加速比，我们将ParaDIn方法与Parareal算法结合起来，以有效并行化通过线方法离散化的非线性偏微分方程中的一阶时间导数项。所提出方法的主要思想是使用块-Jacobi预处理器，以便每个块都通过ParaDIn方法求解。为了加速Jacobi迭代的收敛，我们使用Parareal方法，该方法可以解释为时间上的两级多重网格方法。与传统的Parareal算法（其粗网格校正步骤是顺序执行的）不同，所提出方法中的粗网格和细网格传播器都通过ParaDIn方法并行实现，从而显著提高了组合算法的并行性能。数值结果表明，新的ParaDIn-Parareal组合方法在480个计算核心上，与顺序一阶隐式后向差分(BDF1)方案相比，对于具有平滑和不连续解的二维非线性热方程和Burgers方程，提供了高达124倍的加速比。", "summary": "本文针对并行时间直接逆(ParaDIn)方法在并行时间层数上的限制，提出了一种结合ParaDIn和Parareal算法的新方法。该方法利用块-Jacobi预处理器结合ParaDIn求解子块，并通过将Parareal解释为两级多重网格法来加速迭代。关键创新在于，不同于传统Parareal的顺序粗网格校正，新方法将粗细网格传播器均通过ParaDIn并行实现，从而显著提升了并行性能。数值实验表明，该方法在480核上对非线性热和Burgers方程实现了高达124倍的加速。", "keywords": "并行时间计算, ParaDIn, Parareal, 非线性微分方程, 加速比", "comments": "这篇论文通过巧妙地结合ParaDIn和Parareal两种并行算法，并克服了传统Parareal算法中粗网格校正的顺序性限制，实现了时间并行计算的显著性能提升。其创新点在于将两种方法优势互补，并进一步并行化了Parareal的关键步骤，对于大规模非线性微分方程的快速求解具有重要意义。"}}
{"id": "2506.10416", "title": "Can Sound Replace Vision in LLaVA With Token Substitution?", "authors": ["Ali Vosoughi", "Jing Bi", "Pinxin Liu", "Yunlong Tang", "Chenliang Xu"], "summary": "While multimodal systems have achieved impressive advances, they typically\nrely on text-aligned representations rather than directly integrating audio and\nvisual inputs. This reliance can limit the use of acoustic information in tasks\nrequiring nuanced audio understanding. In response, SoundCLIP explores direct\naudio-visual integration within multimodal large language models (MLLMs) by\nsubstituting CLIP's visual tokens with audio representations and selecting\nsound-relevant patch tokens in models such as LLaVA. We investigate two\nconfigurations: (1) projecting audio features into CLIP's visual manifold via a\nmultilayer perceptron trained with InfoNCE on paired audio-video segments, and\n(2) preserving raw audio embeddings with minimal dimensional adjustments.\nExperiments with five state-of-the-art audio encoders reveal a fundamental\ntrade-off. While audio-to-video retrieval performance increases dramatically\n(up to 44 percentage points in Top-1 accuracy) when audio is projected into\nCLIP's space, text generation quality declines. Encoders pre-trained with text\nsupervision (CLAP, Whisper, ImageBind) maintain stronger generative\ncapabilities than those focused primarily on audiovisual alignment (Wav2CLIP,\nAudioCLIP), highlighting the value of language exposure for generation tasks.\nWe introduce WhisperCLIP, an architecture that fuses intermediate\nrepresentations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),\na dataset of 580,147 three-second audiovisual clips with fine-grained alignment\nannotations. Our findings challenge the assumption that stronger cross-modal\nalignment necessarily benefits all multimodal tasks; instead, a Pareto frontier\nemerges wherein optimal performance depends on balancing retrieval accuracy\nwith text generation quality. Codes and datasets:\nhttps://github.com/ali-vosoughi/SoundCLIP.", "comment": "29 pages including references and appendices", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10416v1", "AI": {"title_translation": "声音能否通过令牌替换在LLaVA中取代视觉？", "tldr": "SoundCLIP通过令牌替换在多模态大语言模型（MLLM）中探索直接音视频集成，发现音视频检索和文本生成之间存在权衡。", "motivation": "现有多模态系统依赖文本对齐表示而非直接集成音视频输入，这限制了声学信息在需要细致音频理解的任务中的使用。", "method": "本文提出了SoundCLIP，通过将CLIP的视觉令牌替换为音频表示并在LLaVA等模型中选择声音相关补丁令牌，实现多模态大语言模型（MLLMs）中的直接音视频集成。研究了两种配置：1) 通过InfoNCE在配对音视频段上训练多层感知机将音频特征投射到CLIP的视觉流形；2) 在最小维度调整下保留原始音频嵌入。此外，还引入了WhisperCLIP架构和AudioVisual Event Evaluation (AVE-2) 数据集。", "result": "实验揭示了一个基本权衡：将音频投射到CLIP空间时，音视频检索性能显著提升（Top-1准确率高达44个百分点），但文本生成质量下降。经过文本监督预训练的编码器（CLAP、Whisper、ImageBind）比主要侧重于音视频对齐的编码器（Wav2CLIP、AudioCLIP）保持更强的生成能力。", "conclusion": "研究发现，更强的跨模态对齐不一定有利于所有多模态任务；相反，出现了一个帕累托前沿，其中最佳性能取决于平衡检索准确性和文本生成质量。", "translation": "现有多模态系统虽然取得了显著进展，但它们通常依赖于文本对齐的表示，而非直接集成音频和视觉输入。这种依赖性可能会限制声学信息在需要细致音频理解的任务中的使用。为此，SoundCLIP通过用音频表示替换CLIP的视觉令牌并在LLaVA等模型中选择与声音相关的补丁令牌，探索了多模态大型语言模型（MLLMs）中的直接音视频集成。我们研究了两种配置：(1) 通过在配对音视频片段上使用InfoNCE训练的多层感知机，将音频特征投射到CLIP的视觉流形中；(2) 在最小维度调整下保留原始音频嵌入。对五种最先进的音频编码器进行的实验揭示了一个根本性的权衡。当音频被投射到CLIP空间时，音视频检索性能显著提高（Top-1准确率高达44个百分点），但文本生成质量下降。经过文本监督预训练的编码器（CLAP、Whisper、ImageBind）比主要侧重于音视频对齐的编码器（Wav2CLIP、AudioCLIP）保持更强的生成能力，这凸显了语言暴露对生成任务的价值。我们引入了WhisperCLIP，这是一种融合了Whisper中间表示的架构，以及AudioVisual Event Evaluation (AVE-2)，一个包含580,147个三秒音视频片段并附有细粒度对齐注释的数据集。我们的发现挑战了更强的跨模态对齐必然有益于所有多模态任务的假设；相反，出现了一个帕累托前沿，其中最佳性能取决于平衡检索准确性和文本生成质量。代码和数据集：https://github.com/ali-vosoughi/SoundCLIP。", "summary": "本文提出SoundCLIP，旨在通过令牌替换在多模态大语言模型中直接集成音视频，以克服传统系统对文本对齐的依赖。研究对比了两种音频特征处理方式，发现将音频投射到视觉空间可显著提升音视频检索性能，但会牺牲文本生成质量。研究强调了文本监督预训练对生成任务的重要性，并引入了WhisperCLIP架构和AVE-2数据集。结论指出，跨模态对齐并非对所有任务都有益，最佳性能在于平衡检索和生成。", "keywords": "音视频集成, 多模态大语言模型, 令牌替换, 跨模态对齐, 帕累托前沿", "comments": "这项研究创新性地探索了在MLLM中直接集成音视频的可能性，而非依赖文本对齐，这对于需要细致音频理解的任务具有重要意义。它揭示了音视频检索和文本生成之间存在的关键权衡，挑战了“更强对齐总是更好”的传统假设，并提出了“帕累托前沿”的概念，为未来多模态模型设计提供了新的视角。引入新的架构和大规模数据集也为后续研究奠定了基础。"}}
{"id": "2506.10739", "title": "Sampling-Based Planning Under STL Specifications: A Forward Invariance Approach", "authors": ["Gregorio Marchesini", "Siyuan Liu", "Lars Lindemann", "Dimos V. Dimarogonas"], "summary": "We propose a variant of the Rapidly Exploring Random Tree Star\n(RRT$^{\\star}$) algorithm to synthesize trajectories satisfying a given\nspatio-temporal specification expressed in a fragment of Signal Temporal Logic\n(STL) for linear systems. Previous approaches for planning trajectories under\nSTL specifications using sampling-based methods leverage either mixed-integer\nor non-smooth optimization techniques, with poor scalability in the horizon and\ncomplexity of the task. We adopt instead a control-theoretic perspective on the\nproblem, based on the notion of set forward invariance. Specifically, from a\ngiven STL task defined over polyhedral predicates, we develop a novel\nalgorithmic framework by which the task is efficiently encoded into a\ntime-varying set via linear programming, such that trajectories evolving within\nthe set also satisfy the task. Forward invariance properties of the resulting\nset with respect to the system dynamics and input limitations are then proved\nvia non-smooth analysis. We then present a modified RRT$^{\\star}$ algorithm to\nsynthesize asymptotically optimal and dynamically feasible trajectories\nsatisfying a given STL specification, by sampling a tree of trajectories within\nthe previously constructed time-varying set. We showcase two use cases of our\napproach involving an autonomous inspection of the International Space Station\nand room-servicing task requiring timed revisit of a charging station.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10739v1", "AI": {"title_translation": "基于采样的STL规范下规划：前向不变性方法", "tldr": "本文提出一种基于前向不变性概念的改进RRT*算法，用于在线性系统中根据信号时序逻辑（STL）规范生成轨迹，解决了现有采样方法的可伸缩性问题。", "motivation": "现有基于采样的方法在信号时序逻辑（STL）规范下规划轨迹时，使用混合整数或非光滑优化技术，在规划范围和任务复杂度方面可伸伸缩性差。", "method": "本文采用控制理论视角，基于集合前向不变性概念。具体地，通过线性规划将STL任务高效编码为时变集合，并证明其相对于系统动力学和输入限制的前向不变性。在此基础上，提出了一种修改后的RRT*算法，通过在该时变集合内采样轨迹树，合成渐近最优且动态可行的轨迹。", "result": "该方法能够合成渐近最优且动态可行的轨迹。通过国际空间站的自主检查和需要定时重访充电站的房间服务任务两个用例展示了其有效性。", "conclusion": "通过将信号时序逻辑（STL）任务编码为时变集合并利用前向不变性，本方法能够高效地合成满足STL规范的轨迹，克服了现有采样规划方法在可伸缩性方面的局限性。", "translation": "我们提出了一种快速探索随机树星（RRT$^{\\star}$）算法的变体，用于在线性系统中合成满足给定以信号时序逻辑（STL）片段表达的时空规范的轨迹。以前使用基于采样方法在STL规范下规划轨迹的方法，要么利用混合整数，要么利用非光滑优化技术，在规划范围和任务复杂度方面可伸缩性差。我们转而采用控制理论视角来解决这个问题，基于集合前向不变性的概念。具体来说，从一个定义在多面体谓词上的给定STL任务，我们开发了一个新颖的算法框架，通过线性规划将任务高效地编码成一个时变集合，使得在该集合内演变的轨迹也能满足任务。然后通过非光滑分析证明了所得集合相对于系统动力学和输入限制的前向不变性。然后，我们提出了一种修改后的RRT$^{\\star}$算法，通过在先前构建的时变集合内采样轨迹树，合成满足给定STL规范的渐近最优且动态可行的轨迹。我们展示了该方法的两个用例，包括国际空间站的自主检查和需要定时重访充电站的房间服务任务。", "summary": "本文提出一种改进的RRT*算法，用于在线性系统中合成满足信号时序逻辑（STL）规范的轨迹。该方法采用控制理论中的前向不变性概念，将STL任务通过线性规划编码为时变集合，并证明其前向不变性。在此基础上，修改RRT*算法以在该时变集合内采样，从而合成渐近最优且动态可行的轨迹，有效解决了现有采样方法在可伸缩性方面的局限。", "keywords": "STL规范, 采样规划, 前向不变性, RRT*, 线性系统", "comments": "本文的创新点在于将信号时序逻辑（STL）任务通过控制理论中的前向不变性概念与采样规划相结合。通过将复杂的时序逻辑任务高效地编码为时变集合，并在此约束下改进RRT*算法进行轨迹合成，有效提升了现有采样方法在处理复杂任务时的可伸缩性。这种结合为复杂时序逻辑任务的轨迹合成提供了新颖且高效的解决方案。"}}
{"id": "2506.10323", "title": "ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space", "authors": ["Chuyang Chen", "Brendan Dolan-Gavitt", "Zhiqiang Lin"], "summary": "Generation-based fuzzing produces appropriate testing cases according to\nspecifications of input grammars and semantic constraints to test systems and\nsoftware. However, these specifications require significant manual efforts to\nconstruct. This paper proposes a new approach, ELFuzz (Evolution Through Large\nLanguage Models for Fuzzing), that automatically synthesizes generation-based\nfuzzers tailored to a system under test (SUT) via LLM-driven synthesis over\nfuzzer space. At a high level, it starts with minimal seed fuzzers and propels\nthe synthesis by fully automated LLM-driven evolution with coverage guidance.\nCompared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of\nreal-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)\nsynthesize efficient fuzzers that catch interesting grammatical structures and\nsemantic constraints in a human-understandable way. Our evaluation compared\nELFuzz with specifications manually written by domain experts and synthesized\nby state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more\ncoverage and triggers up to 174.0% more artificially injected bugs. We also\nused ELFuzz to conduct a real-world fuzzing campaign on the newest version of\ncvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are\nexploitable). Moreover, we conducted an ablation study, which shows that the\nfuzzer space model, the key component of ELFuzz, contributes the most (up to\n62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers\nsynthesized by ELFuzz confirms that they catch interesting grammatical\nstructures and semantic constraints in a human-understandable way. The results\npresent the promising potential of ELFuzz for more automated, efficient, and\nextensible input generation for fuzzing.", "comment": "Accepted by USENIX Security'25 Cycle 2", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10323v1", "AI": {"title_translation": "ELFuzz：通过LLM驱动的模糊测试器空间合成实现高效输入生成", "tldr": "ELFuzz利用LLM自动合成模糊测试器，显著提高代码覆盖率并发现更多漏洞，无需手动构建规范。", "motivation": "现有的基于生成式模糊测试方法需要大量手动工作来构建输入语法和语义约束规范。", "method": "本文提出ELFuzz，一种通过LLM驱动的模糊测试器空间合成方法，旨在自动合成针对特定被测系统（SUT）的生成式模糊测试器。该方法从最少的种子模糊测试器开始，通过完全自动化的LLM驱动进化和覆盖率指导来推动合成过程。", "result": "ELFuzz能够无缝扩展到真实世界规模的SUT（在评估中高达1,791,104行代码）；与手动编写和最先进方法相比，覆盖率提高高达434.8%，触发人工注入错误增加高达174.0%；在对cvc5的真实世界模糊测试中发现5个0-day漏洞（其中3个可利用）；消融研究表明，模糊测试器空间模型对ELFuzz的有效性贡献最大（高达62.5%）；合成的模糊测试器能够以人类可理解的方式捕获有趣的语法结构和语义约束。", "conclusion": "ELFuzz在模糊测试中实现更自动化、高效和可扩展的输入生成方面展现出巨大的潜力。", "translation": "基于生成的模糊测试根据输入语法和语义约束的规范生成适当的测试用例，以测试系统和软件。然而，这些规范的构建需要大量手动工作。本文提出了一种新方法ELFuzz（Evolution Through Large Language Models for Fuzzing），它通过LLM驱动的模糊测试器空间合成，自动合成针对被测系统（SUT）的生成式模糊测试器。从高层次上看，它从最少的种子模糊测试器开始，并通过完全自动化的LLM驱动进化和覆盖率指导来推动合成。与以前的方法相比，ELFuzz能够1）无缝扩展到真实世界规模的SUT——在我们的评估中高达1,791,104行代码——以及2）以人类可理解的方式合成能够捕获有趣的语法结构和语义约束的高效模糊测试器。我们的评估将ELFuzz与领域专家手动编写的规范和最先进方法合成的规范进行了比较。结果显示，ELFuzz实现了高达434.8%的代码覆盖率提升，并触发了高达174.0%的人工注入错误。我们还使用ELFuzz对最新版本的cvc5进行了为期14天的真实世界模糊测试，令人鼓舞的是，它发现了五个0-day漏洞（其中三个是可利用的）。此外，我们进行了一项消融研究，结果表明模糊测试器空间模型作为ELFuzz的关键组件，对其有效性贡献最大（高达62.5%）。对ELFuzz合成的模糊测试器的进一步分析证实，它们以人类可理解的方式捕获了有趣的语法结构和语义约束。这些结果表明ELFuzz在模糊测试中实现更自动化、高效和可扩展的输入生成方面具有广阔前景。", "summary": "本文提出ELFuzz，一种通过LLM驱动合成自动生成模糊测试器的新方法，旨在解决传统生成式模糊测试中手动构建规范的繁琐问题。ELFuzz通过LLM驱动进化和覆盖率指导，能高效合成适应大规模SUT的模糊测试器，显著提高代码覆盖率和漏洞发现能力，并在实际应用中发现0-day漏洞，展现了其在自动化、高效模糊测试方面的巨大潜力。", "keywords": "模糊测试, LLM, 输入生成, 自动化, 漏洞发现", "comments": "ELFuzz的创新之处在于将大型语言模型（LLM）引入模糊测试器的自动合成，显著减少了手动构建测试规范的工作量。其通过LLM驱动的进化和覆盖率指导，实现了高效且可扩展的模糊测试，并在真实世界中发现了0-day漏洞，证明了其重要性和实用性。特别是其能够以人类可理解的方式捕获语法和语义约束，对于后续的漏洞分析和修复也具有潜在价值。"}}
{"id": "2506.10525", "title": "AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length", "authors": ["Junhang Cheng", "Fang Liu", "Chengru Wu", "Li Zhang"], "summary": "While Large Language Models (LLMs) have significantly advanced code\ngeneration efficiency, they face inherent challenges in balancing performance\nand inference costs across diverse programming tasks. Dynamically selecting the\noptimal LLM based on task difficulty and resource constraints offers a\npromising approach to achieve an optimal balance between efficiency and\nperformance. However, existing model selection methods are resource-intensive\nand often neglect cost efficiency. Moreover, these approaches rely on\nhuman-annotated difficulty labels that are frequently inaccessible in\nreal-world settings and may not align with the LLM's own assessment of task\ndifficulty. In this paper, we introduce AdaptiveLLM, a framework that\ndynamically selects optimal LLMs for a given coding task by automatically\nassessing task difficulty. Our framework first estimates task difficulty using\nChain-of-Thought lengths generated by reasoning model, clusters these into\nthree difficulty levels via k-means, and fine-tunes CodeBERT to embed\ndifficulty-aware features. A trained XGBoost classifier then selects the best\nmodel for each problem, optimizing the performance-cost trade-off. Experimental\nresults show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score\nwhile reducing resource consumption by 88.9% compared to baseline method\nComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an\napproximately 15% accuracy improvement, while maintaining the same level of\ncost consumption. Apart from that, the difficulty assessment using CoT provides\nmore reliable selection criteria than human evaluation. Our replication package\nis available at https://github.com/cjhCoder7/AdaptiveLLM.", "comment": "Accepted by Internetware 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10525v1", "AI": {"title_translation": "AdaptiveLLM：一种基于CoT长度为代码生成选择最佳成本效益LLM的框架", "tldr": "AdaptiveLLM是一个框架，它通过自动评估任务难度（使用思维链长度），动态选择最佳的LLM进行代码生成，从而在性能和成本之间取得平衡，相较于现有方法显著提升了效率和准确性。", "motivation": "LLMs在代码生成方面取得了显著进展，但在性能和推理成本之间存在平衡挑战。现有模型选择方法资源密集，忽视成本效益，且依赖不可靠的人工标注难度标签。因此，需要一种动态、自动且成本效益高的LLM选择方法。", "method": "AdaptiveLLM框架通过以下步骤动态选择最佳LLM：1. 使用推理模型生成的思维链（CoT）长度估计任务难度。2. 使用k-means将CoT长度聚类为三个难度级别。3. 微调CodeBERT以嵌入难度感知特征。4. 训练XGBoost分类器选择每个问题的最佳模型，以优化性能-成本权衡。", "result": "实验结果显示，与基线方法ComplexityNet相比，AdaptiveLLM的pass@1得分提高了7.86%，资源消耗减少了88.9%。与单个模型相比，AdaptiveLLM在保持相同成本水平的同时，准确率提高了约15%。此外，使用CoT进行的难度评估提供了比人工评估更可靠的选择标准。", "conclusion": "AdaptiveLLM框架通过自动评估任务难度和动态选择LLM，有效解决了代码生成中LLM性能与成本平衡的挑战，在提高准确性的同时显著降低了资源消耗，并提供了更可靠的难度评估方法。", "translation": "虽然大型语言模型（LLMs）显著提升了代码生成效率，但它们在平衡不同编程任务的性能和推理成本方面面临固有的挑战。根据任务难度和资源限制动态选择最优LLM，提供了一种在效率和性能之间实现最佳平衡的有前景的方法。然而，现有的模型选择方法资源密集，并且经常忽视成本效益。此外，这些方法依赖于人工标注的难度标签，这在实际场景中往往难以获取，并且可能与LLM自身对任务难度的评估不一致。在本文中，我们引入了AdaptiveLLM，这是一个通过自动评估任务难度，为给定编码任务动态选择最优LLM的框架。我们的框架首先使用推理模型生成的思维链长度来估计任务难度，然后通过k-means将这些长度聚类为三个难度级别，并微调CodeBERT以嵌入难度感知特征。然后，一个训练好的XGBoost分类器会为每个问题选择最佳模型，从而优化性能-成本权衡。实验结果表明，与基线方法ComplexityNet相比，AdaptiveLLM在pass@1得分上实现了7.86%的提升，同时资源消耗减少了88.9%。与单个模型相比，AdaptiveLLM在保持相同成本水平的同时，准确率提高了约15%。除此之外，使用CoT进行的难度评估提供了比人工评估更可靠的选择标准。我们的复制包可在https://github.com/cjhCoder7/AdaptiveLLM获取。", "summary": "AdaptiveLLM是一个创新框架，旨在通过自动评估代码生成任务的难度来动态选择最佳的LLM，从而在性能和成本之间实现平衡。它利用思维链（CoT）长度来估计任务难度，并通过k-means聚类、CodeBERT微调和XGBoost分类器进行模型选择。实验证明，AdaptiveLLM在提高代码生成准确性（pass@1得分提升7.86%，准确率提升约15%）的同时，显著降低了资源消耗（减少88.9%），并且其基于CoT的难度评估比人工评估更可靠。", "keywords": "LLM选择, 代码生成, 思维链, 成本效益, 任务难度", "comments": "AdaptiveLLM的创新之处在于其将CoT长度作为衡量任务难度并指导LLM选择的依据，这解决了现有方法对人工标注的依赖和成本效益不足的问题。其方法论结合了CoT、k-means、CodeBERT和XGBoost，构建了一个端到端的动态选择系统，具有很高的实用价值。在实际应用中，这种动态选择能力对于优化资源受限环境下的代码生成效率至关重要。"}}
{"id": "2506.10600", "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence", "authors": ["Wang Xinjie", "Liu Liu", "Cao Yu", "Wu Ruiqi", "Qin Wenkang", "Wang Dehui", "Sui Wei", "Su Zhizhong"], "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10600v1", "AI": {"title_translation": "EmbodiedGen：迈向具身智能的生成式3D世界引擎", "tldr": "EmbodiedGen是一个生成式3D世界平台，通过生成高质量、可控、逼真的3D资产来解决具身智能任务中数据生产成本高和真实性有限的问题，从而支持具身智能的训练和评估。", "motivation": "构建物理真实且精确缩放的模拟3D世界对于具身智能任务的训练和评估至关重要。然而，当前大多数具身智能任务仍严重依赖手动创建和标注的传统3D计算机图形资产，这导致生产成本高昂且真实性有限，严重阻碍了数据驱动方法的可扩展性。", "method": "我们提出了EmbodiedGen，一个用于交互式3D世界生成的基础平台。它能够以低成本生成高质量、可控、逼真且具有精确物理属性和真实世界尺度的URDF格式3D资产。这些资产可以直接导入各种物理模拟引擎，支持下游的训练和评估任务。EmbodiedGen是一个易于使用的全功能工具包，由六个关键模块组成：图像到3D、文本到3D、纹理生成、关节对象生成、场景生成和布局生成。", "result": "EmbodiedGen生成多样化和交互式的3D世界，由生成式3D资产组成，利用生成式AI解决了具身智能相关研究中泛化和评估的挑战。", "conclusion": "EmbodiedGen提供了一个可扩展、低成本的解决方案，用于生成高质量的3D世界和资产，从而克服了当前具身智能数据生产的限制，促进了具身AI的泛化和可扩展性。", "translation": "构建一个物理真实且精确缩放的模拟3D世界对于具身智能任务的训练和评估至关重要。3D数据资产的多样性、真实性、低成本可访问性和可负担性对于实现具身AI的泛化和可扩展性至关重要。然而，当前大多数具身智能任务仍严重依赖手动创建和标注的传统3D计算机图形资产，这导致生产成本高昂且真实性有限。这些限制严重阻碍了数据驱动方法的可扩展性。我们提出了EmbodiedGen，一个用于交互式3D世界生成的基础平台。它能够以低成本生成高质量、可控、逼真且具有精确物理属性和真实世界尺度的统一机器人描述格式（URDF）3D资产。这些资产可以直接导入各种物理模拟引擎进行精细的物理控制，支持训练和评估中的下游任务。EmbodiedGen是一个易于使用的全功能工具包，由六个关键模块组成：图像到3D、文本到3D、纹理生成、关节对象生成、场景生成和布局生成。EmbodiedGen生成由生成式3D资产组成的多样化和交互式的3D世界，利用生成式AI解决了具身智能相关研究中泛化和评估的挑战。代码可在https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html获取。", "summary": "EmbodiedGen是一个创新的平台，旨在通过生成高质量、可控且逼真的3D世界和资产来解决具身智能领域中现有3D数据生产成本高昂和真实性不足的问题。该平台利用生成式AI，能够以低成本生成URDF格式的3D资产，这些资产可直接用于物理模拟引擎，从而支持具身智能任务的训练和评估。EmbodiedGen包含图像到3D、文本到3D等六个核心模块，为具身智能研究提供了可扩展的数据生成解决方案，促进了AI的泛化和评估。", "keywords": "具身智能, 3D世界生成, 生成式AI, URDF, 物理模拟", "comments": "EmbodiedGen的创新性在于它将生成式AI应用于3D世界的构建，解决了具身智能领域长期存在的数据稀缺和真实性不足的问题。通过自动化和低成本地生成高质量、物理精确的3D资产，该平台有望极大地加速具身AI的训练和评估，推动具身智能研究的进展。其模块化设计也使其具有很高的灵活性和易用性。"}}
{"id": "2506.10228", "title": "California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops", "authors": ["Hamid Kamangir", "Mona Hajiesmaeeli", "Mason Earles"], "summary": "California is a global leader in agricultural production, contributing 12.5%\nof the United States total output and ranking as the fifth-largest food and\ncotton supplier in the world. Despite the availability of extensive historical\nyield data from the USDA National Agricultural Statistics Service, accurate and\ntimely crop yield forecasting remains a challenge due to the complex interplay\nof environmental, climatic, and soil-related factors. In this study, we\nintroduce a comprehensive crop yield benchmark dataset covering over 70 crops\nacross all California counties from 2008 to 2022. The benchmark integrates\ndiverse data sources, including Landsat satellite imagery, daily climate\nrecords, monthly evapotranspiration, and high-resolution soil properties. To\neffectively learn from these heterogeneous inputs, we develop a multi-modal\ndeep learning model tailored for county-level, crop-specific yield forecasting.\nThe model employs stratified feature extraction and a timeseries encoder to\ncapture spatial and temporal dynamics during the growing season. Static inputs\nsuch as soil characteristics and crop identity inform long-term variability.\nOur approach achieves an overall R2 score of 0.76 across all crops of unseen\ntest dataset, highlighting strong predictive performance across California\ndiverse agricultural regions. This benchmark and modeling framework offer a\nvaluable foundation for advancing agricultural forecasting, climate adaptation,\nand precision farming. The full dataset and codebase are publicly available at\nour GitHub repository.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10228v1", "AI": {"title_translation": "加州作物产量基准：结合卫星图像、气候、蒸散量和土壤数据层进行70多种作物的县级产量预测", "tldr": "本研究创建了一个包含卫星图像、气候、蒸散量和土壤数据在内的加州70多种作物县级产量预测综合基准数据集，并开发了一个多模态深度学习模型，实现了0.76的R2分数，为农业预测提供了宝贵基础。", "motivation": "尽管有大量历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，准确及时的作物产量预测仍然是一个挑战。", "method": "研究构建了一个涵盖2008-2022年加州70多种作物的县级产量基准数据集，整合了Landsat卫星图像、日气候记录、月蒸散量和高分辨率土壤属性。开发了一个多模态深度学习模型，采用分层特征提取和时间序列编码器来捕获空间和时间动态，并利用土壤特性和作物种类等静态输入来处理长期变异性。", "result": "该方法在未见过的测试数据集上，所有作物总体R2分数达到0.76，表明在加州多样化农业区域具有强大的预测性能。", "conclusion": "该基准数据集和建模框架为推进农业预测、气候适应和精准农业提供了宝贵基础。完整的数据集和代码库已公开。", "translation": "加利福尼亚州是全球农业生产的领导者，贡献了美国总产量的12.5%，并位居世界第五大食品和棉花供应地。尽管美国农业部国家农业统计局提供了大量的历史产量数据，但由于环境、气候和土壤相关因素的复杂相互作用，准确及时的作物产量预测仍然是一个挑战。在本研究中，我们引入了一个全面的作物产量基准数据集，涵盖了2008年至2022年加利福尼亚州所有县的70多种作物。该基准整合了多种数据来源，包括Landsat卫星图像、每日气候记录、每月蒸散量和高分辨率土壤属性。为了有效地从这些异构输入中学习，我们开发了一个多模态深度学习模型，专门用于县级、作物特定的产量预测。该模型采用分层特征提取和时间序列编码器来捕获生长季节的空间和时间动态。土壤特性和作物身份等静态输入提供了长期变异性信息。我们的方法在未见过的测试数据集上，所有作物总体R2分数达到0.76，突显了在加利福尼亚州多样化农业区域的强大预测性能。该基准和建模框架为推进农业预测、气候适应和精准农业提供了宝贵的基础。完整的数据集和代码库可在我们的GitHub仓库中公开获取。", "summary": "本研究针对加州作物产量预测面临的挑战，构建了一个包含卫星图像、气候、蒸散量和土壤等多源数据的综合基准数据集，覆盖2008年至2022年加州70多种作物。在此基础上，开发了一个多模态深度学习模型，通过分层特征提取和时间序列编码器有效整合异构输入，实现了县级作物产量预测。该模型在测试集上取得了0.76的R2分数，验证了其在复杂农业环境中的强大预测能力，为未来农业预测、气候适应和精准农业提供了重要资源。", "keywords": "作物产量预测, 深度学习, 卫星图像, 气候数据, 土壤数据, 加州农业", "comments": "这项研究的创新之处在于其构建了一个大规模、多模态的作物产量预测基准数据集，结合了卫星遥感、气候、蒸散量和土壤等多种异构数据源，并开发了专门的多模态深度学习模型来处理这些复杂数据。其重要性在于为加州这一全球重要农业产区的精准农业和气候适应提供了强大的工具和数据基础，数据集和代码的公开也极大地促进了该领域的研究。"}}
{"id": "2506.10138", "title": "Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban", "authors": ["Mohammad Taufeeque", "Aaron David Tucker", "Adam Gleave", "Adrià Garriga-Alonso"], "summary": "We partially reverse-engineer a convolutional recurrent neural network (RNN)\ntrained to play the puzzle game Sokoban with model-free reinforcement learning.\nPrior work found that this network solves more levels with more test-time\ncompute. Our analysis reveals several mechanisms analogous to components of\nclassic bidirectional search. For each square, the RNN represents its plan in\nthe activations of channels associated with specific directions. These\nstate-action activations are analogous to a value function - their magnitudes\ndetermine when to backtrack and which plan branch survives pruning. Specialized\nkernels extend these activations (containing plan and value) forward and\nbackward to create paths, forming a transition model. The algorithm is also\nunlike classical search in some ways. State representation is not unified;\ninstead, the network considers each box separately. Each layer has its own plan\nrepresentation and value function, increasing search depth. Far from being\ninscrutable, the mechanisms leveraging test-time compute learned in this\nnetwork by model-free training can be understood in familiar terms.", "comment": "33 pages, 22 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10138v1", "AI": {"title_translation": "解释学习到的搜索：在玩推箱子游戏的循环神经网络中寻找转移模型和价值函数", "tldr": "该研究部分逆向工程了一个通过无模型强化学习训练来玩推箱子游戏的循环神经网络，发现它通过类似经典双向搜索的机制来利用测试时计算，并揭示了其内部的转移模型和价值函数。", "motivation": "先前的研究发现，一个通过无模型强化学习训练的卷积循环神经网络（RNN）在玩推箱子游戏时，能够通过增加测试时计算量来解决更多关卡。本文的动机在于解释这个“黑箱”RNN是如何利用测试时计算来解决问题的，即逆向工程其内部机制，寻找类似经典搜索组件的对应物，如转移模型和价值函数。", "method": "研究人员对一个通过无模型强化学习训练来玩推箱子游戏的卷积循环神经网络（RNN）进行了部分逆向工程。他们分析了RNN的激活层，以揭示其内部机制，并将其与经典双向搜索的组件进行类比。具体方法包括检查通道激活中的计划表示、状态-动作激活与价值函数的类比，以及专业内核如何扩展这些激活以形成路径，从而构建转移模型。", "result": "分析揭示了RNN内部存在几种机制，这些机制与经典的双向搜索组件类似。具体而言，RNN在每个方块的激活通道中表示其计划，这些状态-动作激活类似于价值函数，其幅度决定了何时回溯以及哪个计划分支得以保留。专业的内核将这些包含计划和价值的激活向前和向后扩展，以创建路径，从而形成了转移模型。尽管存在一些差异（如状态表示不统一，每个箱子单独考虑；每层有自己的计划表示和价值函数），但该网络通过无模型训练学习到的利用测试时计算的机制是可以理解的。", "conclusion": "尽管该循环神经网络是通过无模型强化学习训练的，但其利用测试时计算的内部机制并非不可理解。通过逆向工程，研究表明该网络内部存在类似于经典双向搜索的组件，包括类似价值函数的状态-动作激活和形成转移模型的专业内核。这表明即使是无模型训练学习到的复杂行为，也可以用熟悉的术语来理解和解释。", "translation": "我们部分逆向工程了一个通过无模型强化学习训练来玩推箱子益智游戏的卷积循环神经网络（RNN）。先前的研究发现，该网络在测试时计算量增加时能解决更多关卡。我们的分析揭示了几种机制，这些机制类似于经典双向搜索的组件。对于每个方块，RNN在其与特定方向相关的通道激活中表示其计划。这些状态-动作激活类似于一个价值函数——它们的幅度决定了何时回溯以及哪个计划分支在剪枝中幸存下来。专业的内核向前和向后扩展这些激活（包含计划和价值）以创建路径，形成一个转移模型。该算法在某些方面也与经典搜索不同。状态表示不统一；相反，网络单独考虑每个箱子。每一层都有自己的计划表示和价值函数，增加了搜索深度。远远不是不可理解的，该网络通过无模型训练学习到的利用测试时计算的机制可以用熟悉的术语来理解。", "summary": "该论文对一个通过无模型强化学习训练的推箱子游戏RNN进行了部分逆向工程。研究发现，该RNN利用类似于经典双向搜索的机制来解决问题，包括表示计划的通道激活（类似价值函数）和形成转移模型的专业内核。尽管存在状态表示不统一和每层独立计划/价值函数等差异，但该研究表明，即使是无模型训练习得的复杂行为，其内部机制也能被理解和解释，尤其是在利用测试时计算方面。", "keywords": "循环神经网络, 推箱子, 强化学习, 模型解释, 搜索算法", "comments": "这项研究的创新之处在于，它试图揭示“黑箱”神经网络内部的工作机制，特别是在无模型强化学习的背景下。通过将RNN的内部激活与经典搜索算法（如双向搜索、价值函数和转移模型）进行类比，论文提供了一种理解复杂神经网络行为的新视角。这对于提高神经网络的可解释性具有重要意义，有助于桥清模型驱动和无模型学习之间的鸿沟。局限性可能在于其分析的“部分逆向工程”性质，以及其发现的类比是否能推广到更复杂的任务或网络结构。"}}
{"id": "2506.10521", "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning", "authors": ["Yuhao Zhou", "Yiheng Wang", "Xuming He", "Ruoyao Xiao", "Zhiwei Li", "Qiantai Feng", "Zijie Guo", "Yuejin Yang", "Hao Wu", "Wenxuan Huang", "Jiaqi Wei", "Dan Si", "Xiuqi Yao", "Jia Bu", "Haiwen Huang", "Tianfan Fu", "Shixiang Tang", "Ben Fei", "Dongzhan Zhou", "Fenghua Ling", "Yan Lu", "Siqi Sun", "Chenhui Li", "Guanjie Zheng", "Jiancheng Lv", "Wenlong Zhang", "Lei Bai"], "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.", "comment": "82 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10521v1", "AI": {"title_translation": "科学家首次考试：通过感知、理解和推理探究多模态大语言模型（MLLM）的认知能力", "tldr": "提出了SFE基准，用于评估MLLM在科学领域的感知、理解和推理能力，发现现有SOTA模型表现不佳，仍有很大提升空间。", "motivation": "现有科学基准主要关注MLLM的知识理解能力，导致对其感知和推理能力的评估不足，而这些能力对于基于信息密集型科学数据和领域专业知识的复杂多模态推理至关重要。", "method": "提出了“科学家首次考试（SFE）”基准，通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLM的科学认知能力。SFE包含830对专家验证的VQA（视觉问答）对，涵盖三种问题类型，涉及五个高价值学科的66项多模态任务。", "result": "实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的得分分别为34.08%和26.52%，这突出表明MLLM在科学领域仍有显著的改进空间。", "conclusion": "SFE中获得的见解将促进AI增强型科学发现的进一步发展，现有MLLM在科学认知能力方面仍有巨大提升空间。", "translation": "科学发现越来越依赖于基于信息密集型科学数据和领域特定专业知识的复杂多模态推理。得益于专家级的科学基准，科学多模态大语言模型（MLLM）有潜力在现实工作流程中显著增强这一发现过程。然而，当前的科学基准主要侧重于评估MLLM的知识理解能力，导致对其感知和推理能力的评估不足。为了弥补这一空白，我们提出了“科学家首次考试（SFE）”基准，旨在通过科学信号感知、科学属性理解和科学比较推理三个相互关联的层面来评估MLLM的科学认知能力。具体而言，SFE包含830对专家验证的VQA（视觉问答）对，涵盖三种问题类型，涉及五个高价值学科的66项多模态任务。广泛的实验表明，当前最先进的GPT-o3和InternVL-3在SFE上的得分仅为34.08%和26.52%，这突出表明MLLM在科学领域仍有显著的改进空间。我们希望SFE中获得的见解将促进AI增强型科学发现的进一步发展。", "summary": "本文提出了一个名为“科学家首次考试（SFE）”的新基准，旨在全面评估多模态大语言模型（MLLM）在科学领域的认知能力，包括感知、理解和推理。SFE包含830个专家验证的VQA对，覆盖多学科多任务。实验结果显示，当前最先进的MLLM模型（如GPT-o3和InternVL-3）在该基准上的表现不佳，表明MLLM在科学发现方面仍有巨大的提升空间。", "keywords": "多模态大语言模型, 认知能力, 科学基准, 感知, 推理", "comments": "这篇论文通过引入SFE基准，填补了当前MLLM评估中对感知和推理能力不足的空白，特别是针对科学领域。其创新性在于构建了一个多维度、跨学科的评估体系，强调了MLLM在真实科学工作流中所需的高级认知能力。研究结果揭示了现有SOTA模型在这些能力上的局限性，为未来MLLM的发展指明了方向，对于推动AI在科学发现中的应用具有重要意义。"}}
{"id": "2506.10853", "title": "A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models", "authors": ["Yu Zhang", "Yang Hu", "De Wang"], "summary": "Human spatiotemporal behavior simulation is critical for urban planning\nresearch, yet traditional rule-based and statistical approaches suffer from\nhigh computational costs, limited generalizability, and poor scalability. While\nlarge language models (LLMs) show promise as \"world simulators,\" they face\nchallenges in spatiotemporal reasoning including limited spatial cognition,\nlack of physical constraint understanding, and group homogenization tendencies.\nThis paper introduces a framework integrating chain-of-thought (CoT) reasoning\nwith Model Context Protocol (MCP) to enhance LLMs' capability in simulating\nspatiotemporal behaviors that correspond with validation data patterns. The\nmethodology combines human-like progressive reasoning through a five-stage\ncognitive framework with comprehensive data processing via six specialized MCP\ntool categories: temporal management, spatial navigation, environmental\nperception, personal memory, social collaboration, and experience evaluation.\nExperiments in Shanghai's Lujiazui district validate the framework's\neffectiveness across 1,000 generated samples. Results demonstrate high\nsimilarity with real mobile signaling data, achieving generation quality scores\nof 7.86 to 8.36 across different base models. Parallel processing experiments\nshow efficiency improvements, with generation times decreasing from 1.30 to\n0.17 minutes per sample when scaling from 2 to 12 processes. This work\ncontributes to integrating CoT reasoning with MCP for urban behavior modeling,\nadvancing LLMs applications in urban computing and providing a practical\napproach for synthetic mobility data generation. The framework offers a\nfoundation for smart city planning, transportation forecasting, and\nparticipatory urban design applications.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10853v1", "AI": {"title_translation": "使用MCP增强思维链大型语言模型的个体时空活动生成方法研究", "tldr": "本文提出一个结合思维链（CoT）推理和模型上下文协议（MCP）的框架，以增强大型语言模型（LLM）在模拟个体时空行为方面的能力，并在上海陆家嘴地区通过实验验证了其有效性，结果显示与真实数据高度相似且效率显著提升。", "motivation": "传统的人类时空行为模拟方法（基于规则和统计）存在计算成本高、泛化能力和可扩展性差的问题。虽然大型语言模型（LLMs）作为“世界模拟器”展现潜力，但在时空推理方面面临空间认知有限、缺乏物理约束理解和群体同质化等挑战。", "method": "本文引入了一个集成思维链（CoT）推理和模型上下文协议（MCP）的框架，以增强大型语言模型在模拟与验证数据模式相符的时空行为方面的能力。该方法结合了通过五阶段认知框架进行的人类式渐进推理，以及通过六类专门的MCP工具（时间管理、空间导航、环境感知、个人记忆、社会协作和经验评估）进行的综合数据处理。", "result": "在上海陆家嘴地区进行的实验验证了该框架在1,000个生成样本上的有效性。结果表明与真实移动信令数据高度相似，在不同基础模型上实现了7.86至8.36的生成质量分数。并行处理实验显示效率有所提高，当从2个进程扩展到12个进程时，每个样本的生成时间从1.30分钟减少到0.17分钟。", "conclusion": "这项工作将CoT推理与MCP集成用于城市行为建模，推进了LLMs在城市计算中的应用，并为合成移动数据生成提供了一种实用方法。该框架为智慧城市规划、交通预测和参与式城市设计应用提供了基础。", "translation": "人类时空行为模拟对于城市规划研究至关重要，但传统的基于规则和统计的方法存在计算成本高、泛化能力有限和可扩展性差的问题。尽管大型语言模型（LLMs）作为“世界模拟器”显示出前景，但它们在时空推理方面面临挑战，包括空间认知有限、缺乏物理约束理解以及群体同质化趋势。本文引入了一个将思维链（CoT）推理与模型上下文协议（MCP）相结合的框架，以增强LLMs在模拟与验证数据模式相符的时空行为方面的能力。该方法通过五阶段认知框架结合了类人渐进推理，并通过六类专门的MCP工具进行综合数据处理：时间管理、空间导航、环境感知、个人记忆、社会协作和经验评估。在上海陆家嘴地区的实验验证了该框架在1,000个生成样本上的有效性。结果表明与真实移动信令数据高度相似，在不同基础模型上实现了7.86至8.36的生成质量分数。并行处理实验显示效率有所提高，当从2个进程扩展到12个进程时，每个样本的生成时间从1.30分钟减少到0.17分钟。这项工作将CoT推理与MCP集成用于城市行为建模，推进了LLMs在城市计算中的应用，并为合成移动数据生成提供了一种实用方法。该框架为智慧城市规划、交通预测和参与式城市设计应用提供了基础。", "summary": "本文提出了一种结合思维链（CoT）推理和模型上下文协议（MCP）的新型框架，旨在解决大型语言模型（LLMs）在模拟个体时空行为时面临的挑战，如空间认知和物理约束理解不足。该框架采用五阶段认知框架进行渐进推理，并利用六类MCP工具处理数据。在上海陆家嘴的实验证明，该方法生成的时空活动与真实移动数据高度相似，并显著提高了生成效率，为城市行为建模和智能城市应用提供了实用基础。", "keywords": "时空活动生成, 大型语言模型, 思维链, 模型上下文协议, 城市行为模拟", "comments": "这项研究的创新之处在于将思维链（CoT）推理与模型上下文协议（MCP）相结合，以增强大型语言模型在复杂时空行为模拟中的能力。它有效地解决了LLMs在空间认知和物理约束理解方面的固有缺陷，并通过实验验证了其在生成高质量合成移动数据方面的有效性和效率。这为城市规划和智能城市应用提供了重要的技术支持，具有很强的实用价值和潜在影响力。"}}
{"id": "2506.10453", "title": "Rethinking Generative Human Video Coding with Implicit Motion Transformation", "authors": ["Bolin Chen", "Ru-Ling Liao", "Jie Chen", "Yan Ye"], "summary": "Beyond traditional hybrid-based video codec, generative video codec could\nachieve promising compression performance by evolving high-dimensional signals\ninto compact feature representations for bitstream compactness at the encoder\nside and developing explicit motion fields as intermediate supervision for\nhigh-quality reconstruction at the decoder side. This paradigm has achieved\nsignificant success in face video compression. However, compared to facial\nvideos, human body videos pose greater challenges due to their more complex and\ndiverse motion patterns, i.e., when using explicit motion guidance for\nGenerative Human Video Coding (GHVC), the reconstruction results could suffer\nsevere distortions and inaccurate motion. As such, this paper highlights the\nlimitations of explicit motion-based approaches for human body video\ncompression and investigates the GHVC performance improvement with the aid of\nImplicit Motion Transformation, namely IMT. In particular, we propose to\ncharacterize complex human body signal into compact visual features and\ntransform these features into implicit motion guidance for signal\nreconstruction. Experimental results demonstrate the effectiveness of the\nproposed IMT paradigm, which can facilitate GHVC to achieve high-efficiency\ncompression and high-fidelity synthesis.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10453v1", "AI": {"title_translation": "重新思考基于隐式运动变换的生成式人体视频编码", "tldr": "本文提出了一种名为隐式运动变换（IMT）的新范式，以解决现有显式运动指导在生成式人体视频编码（GHVC）中导致的复杂人体运动失真问题，实验证明IMT能有效提升GHVC的压缩效率和合成质量。", "motivation": "传统的生成式视频编解码器在人脸视频压缩方面取得了显著成功，但当应用于人体视频编码（GHVC）时，由于人体运动模式更为复杂多样，显式运动指导会导致严重的失真和不准确的运动重建。因此，本文旨在解决显式运动方法在人体视频压缩中的局限性。", "method": "本文提出了一种名为隐式运动变换（IMT）的新范式来改进生成式人体视频编码（GHVC）的性能。具体而言，该方法将复杂的人体信号特征化为紧凑的视觉特征，并将这些特征转换为隐式运动指导，用于信号重建。", "result": "实验结果表明，所提出的IMT范式是有效的，能够促进GHVC实现高效率压缩和高保真合成。", "conclusion": "通过引入隐式运动变换（IMT），本文成功克服了显式运动指导在生成式人体视频编码中的局限性，显著提升了人体视频的压缩效率和重建质量。", "translation": "超越传统的混合式视频编解码器，生成式视频编解码器通过将高维信号演变为紧凑的特征表示以实现编码器端的比特流紧凑性，并开发显式运动场作为中间监督以实现解码器端的高质量重建，从而有望实现令人满意的压缩性能。这种范式在人脸视频压缩方面取得了显著成功。然而，与人脸视频相比，人体视频由于其更复杂多样的运动模式带来了更大的挑战，即当使用显式运动指导进行生成式人体视频编码（GHVC）时，重建结果可能会遭受严重的失真和不准确的运动。因此，本文强调了基于显式运动方法在人体视频压缩中的局限性，并借助隐式运动变换（IMT）研究了GHVC的性能改进。特别是，我们提出将复杂的人体信号特征化为紧凑的视觉特征，并将这些特征转换为隐式运动指导以进行信号重建。实验结果证明了所提出的IMT范式的有效性，它可以促进GHVC实现高效率压缩和高保真合成。", "summary": "本文针对生成式人体视频编码（GHVC）中显式运动指导在处理复杂人体运动时导致的失真和不准确性问题，提出了一种创新的隐式运动变换（IMT）范式。该方法将人体信号转化为紧凑的视觉特征，并利用这些特征生成隐式运动指导进行视频重建。实验证明，IMT能显著提升GHVC的压缩效率和合成保真度，为人体视频编码提供了新的思路。", "keywords": "生成式视频编码, 人体视频, 隐式运动变换, 视频压缩, 运动估计", "comments": "本文创新性地提出了隐式运动变换（IMT）来解决生成式人体视频编码中显式运动指导的局限性，特别是在处理复杂人体运动方面。这种从显式到隐式的转变是该研究的关键创新点，有望在人体视频压缩领域带来显著的性能提升。其重要性在于为高效率、高保真的人体视频编码提供了新的解决方案，对于虚拟现实、远程呈现等应用具有潜在价值。"}}
{"id": "2506.10245", "title": "ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese", "authors": ["Iago Alves Brito", "Julia Soares Dollis", "Fernanda Bufon Färber", "Diogo Fernandes Costa Silva", "Arlindo Rodrigues Galvão Filho"], "summary": "We present ToxSyn-PT, the first large-scale Portuguese corpus that enables\nfine-grained hate-speech classification across nine legally protected minority\ngroups. The dataset contains 53,274 synthetic sentences equally distributed\nbetween minorities groups and toxicity labels. ToxSyn-PT is created through a\nnovel four-stage pipeline: (1) a compact, manually curated seed; (2) few-shot\nexpansion with an instruction-tuned LLM; (3) paraphrase-based augmentation; and\n(4) enrichment, plus additional neutral texts to curb overfitting to\ngroup-specific cues. The resulting corpus is class-balanced, stylistically\ndiverse, and free from the social-media domain that dominate existing\nPortuguese datasets. Despite domain differences with traditional benchmarks,\nexperiments on both binary and multi-label classification on the corpus yields\nstrong results across five public Portuguese hate-speech datasets,\ndemonstrating robust generalization even across domain boundaries. The dataset\nis publicly released to advance research on synthetic data and hate-speech\ndetection in low-resource settings.", "comment": "8 pages, 5 tables, 1 figure", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10245v1", "AI": {"title_translation": "ToxSyn-PT：一个用于葡萄牙语仇恨言论检测的大规模合成数据集", "tldr": "ToxSyn-PT是一个大规模的葡萄牙语合成数据集，用于细粒度仇恨言论分类，并通过新颖的四阶段管道创建，在现有基准上表现出强大的泛化能力。", "motivation": "目前缺乏大规模的葡萄牙语语料库，特别是那些能够进行细粒度仇恨言论分类的，以及在低资源环境下推进仇恨言论检测研究的需求。", "method": "ToxSyn-PT通过一个新颖的四阶段管道创建：1) 一个紧凑的手动精选种子；2) 使用指令调整的LLM进行少样本扩展；3) 基于释义的增强；4) 丰富，并添加额外的中性文本以抑制对特定群体线索的过拟合。", "result": "该语料库是类别平衡的、风格多样的，并且没有现有葡萄牙语数据集中常见的社交媒体领域偏向。尽管与传统基准存在领域差异，但在二进制和多标签分类实验中，该语料库在五个公共葡萄牙语仇恨言论数据集上取得了强大的结果，即使跨越领域边界也表现出鲁棒的泛化能力。", "conclusion": "ToxSyn-PT是第一个大规模的葡萄牙语语料库，能够实现对九个受法律保护的少数群体的细粒度仇恨言论分类。该数据集的公开发布旨在推进合成数据和低资源环境下仇恨言论检测的研究。", "translation": "我们提出了ToxSyn-PT，这是第一个大规模的葡萄牙语语料库，能够对九个受法律保护的少数群体进行细粒度仇恨言论分类。该数据集包含53,274个合成句子，在少数群体和毒性标签之间均匀分布。ToxSyn-PT通过一个新颖的四阶段管道创建：(1) 一个紧凑的手动精选种子；(2) 使用指令调整的LLM进行少样本扩展；(3) 基于释义的增强；(4) 丰富，并添加额外的中性文本以抑制对特定群体线索的过拟合。生成的语料库是类别平衡的、风格多样的，并且没有现有葡萄牙语数据集中常见的社交媒体领域偏向。尽管与传统基准存在领域差异，但在语料库上进行的二元和多标签分类实验在五个公共葡萄牙语仇恨言论数据集上取得了强大的结果，即使跨越领域边界也表现出鲁棒的泛化能力。该数据集已公开发布，以推进合成数据和低资源环境下仇恨言论检测的研究。", "summary": "本文介绍了ToxSyn-PT，一个大规模的葡萄牙语合成数据集，专为细粒度仇恨言论分类设计，涵盖九个受保护的少数群体。该数据集包含53,274个合成句子，通过一个四阶段管道生成，确保了类别平衡、风格多样性，并避免了现有数据集中的社交媒体领域偏向。实验证明，ToxSyn-PT在多个葡萄牙语仇恨言论数据集上表现出强大的泛化能力，即使在跨领域的情况下也能保持鲁棒性。该数据集的发布旨在促进低资源环境下的合成数据和仇恨言论检测研究。", "keywords": "仇恨言论检测, 葡萄牙语, 合成数据集, 低资源语言, 细粒度分类", "comments": "ToxSyn-PT的创新之处在于其大规模的合成数据生成方法，特别是在低资源语言（如葡萄牙语）中解决了仇恨言论检测数据稀缺的问题。其四阶段管道设计确保了数据集的质量、多样性和平衡性，并通过添加中性文本有效抑制了过拟合。该数据集摆脱了现有社交媒体领域的限制，展示了强大的跨领域泛化能力，对于推动仇恨言论检测研究具有重要意义。"}}
{"id": "2506.10728", "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims", "authors": ["Priyanka Kargupta", "Runchu Tian", "Jiawei Han"], "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be\nclearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with\nscientific and political claims. However, a claim (e.g., \"vaccine A is better\nthan vaccine B\") can be dissected into its integral aspects and sub-aspects\n(e.g., efficacy, safety, distribution), which are individually easier to\nvalidate. This enables a more comprehensive, structured response that provides\na well-rounded perspective on a given problem while also allowing the reader to\nprioritize specific angles of interest within the claim (e.g., safety towards\nchildren). Thus, we propose ClaimSpect, a retrieval-augmented generation-based\nframework for automatically constructing a hierarchy of aspects typically\nconsidered when addressing a claim and enriching them with corpus-specific\nperspectives. This structure hierarchically partitions an input corpus to\nretrieve relevant segments, which assist in discovering new sub-aspects.\nMoreover, these segments enable the discovery of varying perspectives towards\nan aspect of the claim (e.g., support, neutral, or oppose) and their respective\nprevalence (e.g., \"how many biomedical papers believe vaccine A is more\ntransportable than B?\"). We apply ClaimSpect to a wide variety of real-world\nscientific and political claims featured in our constructed dataset, showcasing\nits robustness and accuracy in deconstructing a nuanced claim and representing\nperspectives within a corpus. Through real-world case studies and human\nevaluation, we validate its effectiveness over multiple baselines.", "comment": "Accepted to ACL 2025 Main Conference. Code available at:\n  https://github.com/pkargupta/claimspect", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10728v1", "AI": {"title_translation": "超越真假：检索增强的细微主张分层分析", "tldr": "本文提出了ClaimSpect框架，一个基于检索增强生成（RAG）的系统，用于处理复杂且细微的主张。它通过分层分析将主张分解为可验证的方面和子方面，并从语料库中检索多角度观点及其普遍性，从而提供全面且结构化的响应，超越简单的真假判断。", "motivation": "现有方法难以处理细微、非简单“真/假”判断的主张（如科学和政治主张）。需要一种更全面、结构化的方法来分解主张并提供多角度观点，以帮助读者理解复杂信息。", "method": "本文提出了ClaimSpect框架，一个基于检索增强生成（RAG）的系统。它通过自动构建主张的方面和子方面的层次结构，并用语料库特定视角丰富这些方面。该框架分层划分输入语料库以检索相关片段，从而发现新的子方面和对主张某个方面的不同观点（如支持、中立、反对）及其各自的普遍性。", "result": "ClaimSpect框架被应用于各种真实世界的科学和政治主张数据集，结果表明其在解构细微主张和表示语料库内观点方面具有鲁棒性和准确性。通过真实案例研究和人工评估，验证了其优于多个基线的有效性。", "conclusion": "ClaimSpect框架能够有效处理复杂且细微的主张，通过分层分析提供多角度的全面视角，并优于现有基线，为事实核查和信息理解提供了新的方法。", "translation": "个人或实体提出的主张往往是细微的，不能简单地标记为完全“真”或“假”——科学和政治主张中经常出现这种情况。然而，一个主张（例如，“疫苗A比疫苗B更好”）可以被分解成其组成方面和子方面（例如，功效、安全性、分发），这些方面更容易单独验证。这使得能够提供更全面、结构化的响应，为主张提供一个全面的视角，同时允许读者优先关注主张中感兴趣的特定角度（例如，对儿童的安全性）。因此，我们提出了ClaimSpect，一个基于检索增强生成（RAG）的框架，用于自动构建在处理主张时通常考虑的方面层次结构，并用语料库特定的视角丰富它们。这种结构分层划分输入语料库以检索相关片段，这有助于发现新的子方面。此外，这些片段能够发现对主张某个方面的不同观点（例如，支持、中立或反对）及其各自的普遍性（例如，“有多少生物医学论文认为疫苗A比B更易于运输？”）。我们将ClaimSpect应用于我们构建的数据集中各种真实世界的科学和政治主张，展示了其在解构细微主张和表示语料库内观点方面的鲁棒性和准确性。通过真实案例研究和人工评估，我们验证了其相对于多个基线的有效性。", "summary": "本文提出了ClaimSpect框架，旨在解决复杂主张难以简单判断真假的问题。该框架利用检索增强生成技术，自动构建主张的层次化方面和子方面，并从语料库中检索相关信息以丰富这些方面，发现新子方面和不同观点及其普遍性。实验表明，ClaimSpect在处理真实世界的科学和政治主张时，能有效解构细微之处并准确呈现多角度观点，性能优于基线方法。", "keywords": "细微主张, 检索增强生成, 层次分析, 事实核查, 多视角分析", "comments": "这篇论文的创新点在于其处理复杂、细微主张的方法，超越了传统的二元真假判断。通过引入层次化分析和检索增强生成，它能够系统地分解主张，并整合语料库中的多角度信息，这对于事实核查、信息消费和理解复杂议题具有重要意义。该框架的灵活性和对多视角的捕获是其主要优势。"}}
{"id": "2506.10894", "title": "Numerical approximation of a PDE-constrained Optimization problem that appears in Data-Driven Computational Mechanics", "authors": ["Pedro B. Bazon", "Cristian G. Gebhardt", "Gustavo C. Buscaglia", "Roberto F. Ausas"], "summary": "We investigate an optimization problem that arises when working within the\nparadigm of Data-Driven Computational Mechanics. In the context of the\ndiffusion-reaction problem, such an optimization problem seeks for the\ncontinuous primal fields (gradient and flux) that are closest to some\npredefined discrete fields taken from a material data set. The optimization is\nperformed over primal fields that satisfy the physical conservation law and the\ngeometrical compatibility. We consider a reaction term in the conservation law,\nwhich has the effect of coupling all the optimality conditions. We first\nestablish the well-posedness in the continuous setting. Then, we propose stable\nfinite element discretizations that consistently approximate the continuous\nformulation, preserving its saddle-point structure and allowing for equal-order\ninterpolation of all fields. Finally, we demonstrate the effectiveness of the\nproposed methods through a set of numerical examples.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10894v1", "AI": {"title_translation": "数据驱动计算力学中偏微分方程约束优化问题的数值逼近", "tldr": "本文研究了数据驱动计算力学中的一个PDE约束优化问题，该问题旨在寻找最接近给定离散场的连续原始场。文章建立了连续设置下的适定性，提出了稳定的有限元离散化方法，并通过数值例子验证了其有效性。", "motivation": "在数据驱动计算力学范式中，需要解决一个优化问题，以寻找最接近材料数据集中预定义离散场的连续原始场（梯度和通量）。", "method": "研究首先在连续设置下建立了问题的适定性。然后，提出了稳定的有限元离散化方法，该方法能一致地逼近连续公式，并保持其鞍点结构，允许所有场的等阶插值。", "result": "通过一系列数值例子，证明了所提出方法的有效性。", "conclusion": "所提出的数值逼近方法在数据驱动计算力学中PDE约束优化问题的求解上是有效且稳定的，能够保持连续公式的特性。", "translation": "我们研究了一个在数据驱动计算力学范式中出现的优化问题。在扩散-反应问题的背景下，这样一个优化问题旨在寻找最接近从材料数据集中获取的预定义离散场的连续原始场（梯度和通量）。该优化是在满足物理守恒定律和几何兼容性的原始场上进行的。我们在守恒定律中考虑了一个反应项，它具有耦合所有最优性条件的效果。我们首先建立了连续设置下的适定性。然后，我们提出了稳定的有限元离散化方法，这些方法能够一致地逼近连续公式，保持其鞍点结构并允许所有场的等阶插值。最后，我们通过一系列数值例子证明了所提出方法的有效性。", "summary": "本文探讨了数据驱动计算力学中出现的偏微分方程约束优化问题，特别是在扩散-反应问题背景下，旨在找到最接近给定离散材料数据的连续原始场。研究确立了连续设置下的适定性，并提出了一种创新的稳定有限元离散化方法，该方法保持了问题的鞍点结构并允许等阶插值，最终通过数值实验验证了其有效性。", "keywords": "数据驱动计算力学, PDE约束优化, 有限元, 扩散-反应, 适定性", "comments": "本文的创新点在于提出了稳定的有限元离散化方法，该方法在保持鞍点结构和允许等阶插值方面表现出色，这对于数据驱动计算力学中的PDE约束优化问题具有重要意义。它为从离散数据中恢复连续物理场提供了可靠的数值工具。"}}
{"id": "2506.10574", "title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation", "authors": ["Qing Wang", "Xiaohang Yang", "Yilan Dong", "Naveen Raj Govindaraj", "Gregory Slabaugh", "Shanxin Yuan"], "summary": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.", "comment": "check demos at https://dancechat.github.io/anon/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10574v1", "AI": {"title_translation": "舞动对话：大型语言模型引导的音乐到舞蹈生成", "tldr": "DanceChat利用大型语言模型作为编舞者，生成文本动作指令，解决音乐到舞蹈生成中的语义鸿沟和多样性挑战。", "motivation": "1. 音乐和舞蹈动作之间存在语义鸿沟，音乐提供抽象线索，但未明确指定具体身体动作。2. 一段音乐可以有多种合理的舞蹈解释，这种一对多映射需要额外的指导。3. 缺乏配对的音乐和舞蹈数据，限制了模型学习多样化舞蹈模式的能力。", "method": "本文提出DanceChat，一种大型语言模型（LLM）引导的音乐到舞蹈生成方法。该方法使用LLM作为编舞者，提供文本动作指令，为舞蹈生成提供明确、高级的指导。DanceChat由三个核心组件构成：1. 基于LLM的伪指令生成模块，根据音乐风格和结构生成文本舞蹈指导。2. 多模态特征提取和融合模块，整合音乐、节奏和文本指导为共享表示。3. 基于扩散的动作合成模块，结合多模态对齐损失，确保生成的舞蹈与音乐和文本提示对齐。", "result": "在AIST++数据集上的大量实验和人体评估表明，DanceChat在定性和定量上均优于最先进的方法。", "conclusion": "通过引入大型语言模型作为编舞者提供显式文本指导，DanceChat有效解决了音乐到舞蹈生成中的语义鸿沟和多样性挑战，并生成了与音乐风格更一致且更具多样性的舞蹈。", "translation": "音乐到舞蹈生成旨在根据音乐输入合成人类舞蹈动作。尽管最近取得了进展，但由于音乐和舞蹈动作之间的语义鸿沟，仍然存在重大挑战，因为音乐只提供抽象线索，如旋律、律动和情感，而没有明确指定物理动作。此外，一段音乐可以产生多种合理的舞蹈解释。这种一对多映射需要额外的指导，因为仅凭音乐提供的生成多样化舞蹈动作的信息有限。配对音乐和舞蹈数据的稀缺性进一步加剧了这一挑战，这限制了模型学习多样化舞蹈模式的能力。在本文中，我们引入了DanceChat，一种由大型语言模型（LLM）引导的音乐到舞蹈生成方法。我们使用LLM作为编舞者，提供文本动作指令，为舞蹈生成提供明确、高级的指导。这种方法超越了仅从音乐中进行隐式学习，使模型能够生成既多样化又与音乐风格更一致的舞蹈。我们的方法由三个组件组成：（1）一个基于LLM的伪指令生成模块，根据音乐风格和结构生成文本舞蹈指导；（2）一个多模态特征提取和融合模块，将音乐、节奏和文本指导整合到共享表示中；（3）一个基于扩散的动作合成模块，结合多模态对齐损失，确保生成的舞蹈与音乐和文本提示对齐。在AIST++上的大量实验和人体评估表明，DanceChat在定性和定量上均优于最先进的方法。", "summary": "本文提出DanceChat，一种由大型语言模型（LLM）引导的音乐到舞蹈生成方法，旨在解决音乐与舞蹈间的语义鸿沟、一对多映射以及数据稀缺等挑战。DanceChat利用LLM作为编舞者，生成文本动作指令，为舞蹈生成提供显式、高级指导。该方法包含LLM伪指令生成、多模态特征融合和扩散动作合成三个模块，确保生成的舞蹈既多样化又与音乐及文本提示对齐。实验证明DanceChat在性能上超越了现有先进方法。", "keywords": "音乐到舞蹈生成, 大型语言模型, 文本指导, 扩散模型, 多模态", "comments": "该论文的创新点在于引入大型语言模型作为“编舞者”，通过生成文本动作指令来弥补音乐与舞蹈之间的语义鸿沟，并提供显式的、高层次的指导。这有效地解决了传统方法中音乐信息抽象、一对多映射及数据稀缺导致的多样性不足问题。这种结合LLM进行多模态生成的新范式，为音乐到舞蹈生成领域带来了显著的性能提升和新的研究方向。"}}
{"id": "2506.10212", "title": "Cross-Learning Between ECG and PCG: Exploring Common and Exclusive Characteristics of Bimodal Electromechanical Cardiac Waveforms", "authors": ["Sajjad Karimi", "Amit J. Shah", "Gari D. Clifford", "Reza Sameni"], "summary": "Simultaneous electrocardiography (ECG) and phonocardiogram (PCG) provide a\ncomprehensive, multimodal perspective on cardiac function by capturing the\nheart's electrical and mechanical activities, respectively. However, the\ndistinct and overlapping information content of these signals, as well as their\npotential for mutual reconstruction and biomarker extraction, remains\nincompletely understood, especially under varying physiological conditions and\nacross individuals.\n  In this study, we systematically investigate the common and exclusive\ncharacteristics of ECG and PCG using the EPHNOGRAM dataset of simultaneous\nECG-PCG recordings during rest and exercise. We employ a suite of linear and\nnonlinear machine learning models, including non-causal LSTM networks, to\nreconstruct each modality from the other and analyze the influence of\ncausality, physiological state, and cross-subject variability. Our results\ndemonstrate that nonlinear models, particularly non-causal LSTM, provide\nsuperior reconstruction performance, with reconstructing ECG from PCG proving\nmore tractable than the reverse. Exercise and cross-subject scenarios present\nsignificant challenges, but envelope-based modeling that utilizes instantaneous\namplitude features substantially improves cross-subject generalizability for\ncross-modal learning. Furthermore, we demonstrate that clinically relevant ECG\nbiomarkers, such as fiducial points and QT intervals, can be estimated from PCG\nin cross-subject settings.\n  These findings advance our understanding of the relationship between\nelectromechanical cardiac modalities, in terms of both waveform characteristics\nand the timing of cardiac events, with potential applications in novel\nmultimodal cardiac monitoring technologies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10212v1", "AI": {"title_translation": "ECG与PCG的交叉学习：探索双模态心电机械波形的共同与独有特征", "tldr": "本研究利用机器学习模型，探索了心电图（ECG）和心音图（PCG）信号之间的共同和独有特征，并实现了它们之间的相互重建以及从PCG中估计ECG生物标志物，尤其是在跨受试者和运动条件下，非线性模型和基于包络的建模表现优异。", "motivation": "尽管同步心电图（ECG）和心音图（PCG）能全面反映心脏功能，但它们之间独特和重叠的信息内容、相互重建潜力以及生物标志物提取能力尚未被完全理解，尤其是在不同生理条件和个体之间。", "method": "本研究使用EPHNOGRAM数据集中同步ECG-PCG静息和运动记录，系统地调查了ECG和PCG的共同与独有特征。采用线性与非线性机器学习模型，包括非因果LSTM网络，进行模态间相互重建，并分析因果关系、生理状态和跨受试者变异性的影响。此外，还采用了利用瞬时幅度特征的基于包络的建模方法。", "result": "非线性模型，特别是非因果LSTM，提供了卓越的重建性能。从PCG重建ECG比反向重建更可行。运动和跨受试者场景带来了显著挑战，但基于包络的建模显著提高了跨模态学习的跨受试者泛化能力。此外，临床相关的ECG生物标志物（如特征点和QT间期）可以在跨受试者设置中从PCG中估计。", "conclusion": "这些发现加深了我们对心电机械模态之间关系的理解，包括波形特征和心脏事件的时间，并为新型多模态心脏监测技术提供了潜在应用。", "translation": "同步心电图（ECG）和心音图（PCG）分别捕捉心脏的电活动和机械活动，为心脏功能提供了全面、多模态的视角。然而，这些信号独特和重叠的信息内容，以及它们相互重建和生物标志物提取的潜力，仍未被完全理解，尤其是在不同的生理条件和个体之间。\n在本研究中，我们利用EPHNOGRAM数据集中同步ECG-PCG在静息和运动期间的记录，系统地调查了ECG和PCG的共同和独有特征。我们采用了一系列线性与非线性机器学习模型，包括非因果LSTM网络，以从一种模态重建另一种模态，并分析因果关系、生理状态和跨受试者变异性的影响。我们的结果表明，非线性模型，特别是非因果LSTM，提供了卓越的重建性能，其中从PCG重建ECG比反向重建更易处理。运动和跨受试者场景带来了显著挑战，但利用瞬时幅度特征的基于包络的建模显著提高了跨模态学习的跨受受试者泛化能力。此外，我们证明了临床相关的ECG生物标志物，如特征点和QT间期，可以在跨受试者设置中从PCG中估计。\n这些发现加深了我们对心电机械模态之间关系的理解，包括波形特征和心脏事件的时间，并为新型多模态心脏监测技术提供了潜在应用。", "summary": "本研究旨在深入理解心电图（ECG）和心音图（PCG）在心脏电机械活动中的共同与独有特征。利用EPHNOGRAM数据集，研究人员采用线性与非线性机器学习模型（包括非因果LSTM）进行模态间相互重建，并分析了生理状态和个体差异的影响。结果显示，非线性模型尤其非因果LSTM在重建性能上表现优异，且从PCG重建ECG比反向重建更有效。尽管运动和跨受试者场景带来挑战，但基于包络的建模显著提升了跨受试者泛化能力，并成功实现了从PCG中估计临床相关ECG生物标志物。这些发现有助于开发新型多模态心脏监测技术。", "keywords": "ECG, PCG, 交叉学习, 生物标志物, 机器学习", "comments": "该论文通过探索ECG和PCG之间的交叉学习，揭示了心脏电机械活动信号的深层关系。其创新之处在于利用非线性模型（尤其是非因果LSTM）实现了模态间的有效重建，并证明了在复杂条件下（如运动和跨受试者）从PCG中估计ECG生物标志物的可行性。这对于未来开发非侵入式、多模态心脏监测技术具有重要意义，可能为临床诊断和健康监测提供新的途径。"}}
{"id": "2506.10815", "title": "Joint Beamforming with Extremely Large Scale RIS: A Sequential Multi-Agent A2C Approach", "authors": ["Zhi Chai", "Jiajie Xu", "Justin P Coon", "Mohamed-Slim Alouini"], "summary": "It is a challenging problem to jointly optimize the base station (BS)\nprecoding matrix and the reconfigurable intelligent surface (RIS) phases\nsimultaneously in a RIS-assisted multiple-user multiple-input-multiple-output\n(MU-MIMO) scenario when the size of the RIS becomes extremely large. In this\npaper, we propose a deep reinforcement learning algorithm called sequential\nmulti-agent advantage actor-critic (A2C) to solve this problem. In addition,\nthe discrete phase of RISs, imperfect channel state information (CSI), and\nchannel correlations between users are taken into consideration. The\ncomputational complexity is also analyzed, and the performance of the proposed\nalgorithm is compared with the zero-forcing (ZF) beamformer in terms of the sum\nspectral efficiency (SE). It is noted that the computational complexity of the\nproposed algorithm is lower than the benchmark, while the performance is better\nthan the benchmark. Throughout simulations, it is also found that the proposed\nalgorithm is robust to medium channel estimation error.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10815v1", "AI": {"title_translation": "极大规模RIS联合波束成形：一种序列多智能体A2C方法", "tldr": "本文提出一种基于序列多智能体A2C的深度强化学习算法，用于解决极大规模RIS辅助MU-MIMO系统中基站预编码和RIS相位的联合优化问题，该算法在计算复杂度更低的同时性能更优，并对信道估计误差具有鲁棒性。", "motivation": "在RIS辅助的多用户多输入多输出(MU-MIMO)场景中，当可重构智能表面(RIS)的尺寸变得非常大时，同时优化基站(BS)预编码矩阵和RIS相位是一个具有挑战性的问题。", "method": "提出了一种名为序列多智能体优势Actor-Critic (A2C) 的深度强化学习算法来解决该问题。该方法考虑了RIS的离散相位、不完善的信道状态信息(CSI)以及用户间的信道相关性。", "result": "所提出的算法在计算复杂度上低于基准，但在总频谱效率(SE)方面的性能优于基准。此外，该算法对中等信道估计误差具有鲁棒性。", "conclusion": "所提出的基于序列多智能体A2C的深度强化学习算法能够有效且高效地解决极大规模RIS辅助MU-MIMO系统中的联合波束成形问题，并在实际应用中展现出良好的性能和鲁棒性。", "translation": "在RIS辅助的多用户多输入多输出（MU-MIMO）场景中，当可重构智能表面（RIS）的尺寸变得非常大时，同时优化基站（BS）预编码矩阵和RIS相位是一个具有挑战性的问题。在本文中，我们提出了一种名为序列多智能体优势Actor-Critic（A2C）的深度强化学习算法来解决这个问题。此外，论文还考虑了RIS的离散相位、不完善的信道状态信息（CSI）以及用户间的信道相关性。本文分析了计算复杂度，并比较了所提算法与零迫（ZF）波束成形器在总频谱效率（SE）方面的性能。值得注意的是，所提算法的计算复杂度低于基准，而性能优于基准。通过仿真发现，所提算法对中等信道估计误差也具有鲁棒性。", "summary": "本文针对极大规模RIS辅助MU-MIMO系统中基站预编码和RIS相位联合优化的难题，提出了一种基于序列多智能体A2C的深度强化学习算法。该算法在考虑离散RIS相位、不完善CSI和用户间信道相关性的条件下，实现了更低的计算复杂度和更高的频谱效率，并且对信道估计误差表现出良好的鲁棒性。", "keywords": "极大规模RIS, 联合波束成形, 深度强化学习, 序列多智能体A2C, 频谱效率", "comments": "本文的创新之处在于将序列多智能体A2C深度强化学习方法应用于极大规模RIS的联合波束成形问题，这对于解决传统优化方法在高维空间中的计算复杂性问题具有重要意义。该方法不仅在性能上超越了现有基准，还在实际应用中考虑了不完善CSI等因素，增强了其实用性。"}}
{"id": "2506.10327", "title": "A Comprehensive Survey of Unmanned Aerial Systems' Risks and Mitigation Strategies", "authors": ["Sharad Shrestha", "Mohammed Ababneh", "Satyajayant Misra", "Henry M. Cathey, Jr.", "Roopa Vishwanathan", "Matt Jansen", "Jinhong Choi", "Rakesh Bobba", "Yeongjin Jang"], "summary": "In the last decade, the rapid growth of Unmanned Aircraft Systems (UAS) and\nUnmanned Aircraft Vehicles (UAV) in communication, defense, and transportation\nhas increased. The application of UAS will continue to increase rapidly. This\nhas led researchers to examine security vulnerabilities in various facets of\nUAS infrastructure and UAVs, which form a part of the UAS system to reinforce\nthese critical systems. This survey summarizes the cybersecurity\nvulnerabilities in several phases of UAV deployment, the likelihood of each\nvulnerability's occurrence, the impact of attacks, and mitigation strategies\nthat could be applied. We go beyond the state-of-the-art by taking a\ncomprehensive approach to enhancing UAS security by performing an analysis of\nboth UAS-specific and non-UAS-specific mitigation strategies that are\napplicable within the UAS domain to define the lessons learned. We also present\nrelevant cybersecurity standards and their recommendations in the UAS context.\nDespite the significant literature in UAS security and the relevance of\ncyberphysical and networked systems security approaches from the past, which we\nidentify in the survey, we find several critical research gaps that require\nfurther investigation. These form part of our discussions and recommendations\nfor the future exploration by our research community.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10327v1", "AI": {"title_translation": "无人机系统风险与缓解策略的综合调查", "tldr": "本文对无人机系统（UAS）的网络安全风险、漏洞、攻击影响和缓解策略进行了全面调查，并提出了未来的研究方向。", "motivation": "随着无人机系统（UAS）在通信、国防和交通领域的快速增长和广泛应用，研究人员需要深入探讨其安全漏洞，以加强这些关键系统的安全性。", "method": "本文通过一项综合调查，总结了无人机（UAV）部署不同阶段的网络安全漏洞、发生可能性、攻击影响以及可应用的缓解策略。研究超越了现有技术，全面分析了无人机系统特有和非特有的适用缓解策略，并提出了经验教训。此外，还介绍了无人机系统相关的网络安全标准及其建议。", "result": "调查总结了无人机部署各阶段的网络安全漏洞、发生可能性、攻击影响和缓解策略。分析了无人机系统特有和非特有的缓解策略，并定义了经验教训。同时，提出了无人机系统背景下的相关网络安全标准及其建议。", "conclusion": "尽管无人机安全领域已有大量文献，但本调查发现仍存在多个需要进一步研究的关键研究空白。这些空白构成了对未来研究社区探索的讨论和建议的一部分。", "translation": "在过去十年中，无人机系统（UAS）和无人机（UAV）在通信、国防和交通领域的快速增长。无人机系统的应用将继续快速增加。这促使研究人员审查无人机基础设施和无人机各个方面的安全漏洞，这些漏洞是无人机系统的一部分，旨在加强这些关键系统。本调查总结了无人机部署的几个阶段的网络安全漏洞、每个漏洞发生的可能性、攻击的影响以及可以应用的缓解策略。我们通过采取综合方法来增强无人机系统安全性，通过对无人机系统特定和非无人机系统特定的缓解策略进行分析，这些策略适用于无人机领域以定义经验教训，从而超越了现有技术。我们还提出了无人机系统背景下的相关网络安全标准及其建议。尽管无人机安全领域有大量文献以及过去网络物理和网络系统安全方法的关联性（我们在调查中进行了识别），但我们发现存在几个需要进一步调查的关键研究空白。这些构成了我们对研究社区未来探索的讨论和建议的一部分。", "summary": "本文对无人机系统（UAS）在通信、国防和交通领域快速增长背景下的网络安全风险进行了全面调查。研究总结了无人机部署各阶段的漏洞、发生可能性、攻击影响及缓解策略，并分析了特有和非特有缓解措施，提出了经验教训。此外，论文还介绍了相关网络安全标准和建议，并指出了现有研究中的关键空白，为未来研究提供了方向。", "keywords": "无人机系统, 网络安全, 风险缓解, 漏洞, 调查", "comments": "本文作为一篇综合性调查报告，其创新之处在于超越了现有技术，不仅总结了无人机系统（UAS）的漏洞和缓解策略，还全面分析了无人机系统特有和非特有的适用缓解措施，并提炼了经验教训。其重要性在于为快速发展的无人机行业提供了关键的安全洞察，并明确指出了未来的研究方向，对加强无人机系统的整体安全性具有指导意义。"}}
{"id": "2506.10686", "title": "An $O(n$)-Algorithm for the Higher-Order Kinematics and Inverse Dynamics of Serial Manipulators using Spatial Representation of Twists", "authors": ["Andreas Mueller"], "summary": "Optimal control in general, and flatness-based control in particular, of\nrobotic arms necessitate to compute the first and second time derivatives of\nthe joint torques/forces required to achieve a desired motion. In view of the\nrequired computational efficiency, recursive $O(n)$-algorithms were proposed to\nthis end. Aiming at compact yet efficient formulations, a Lie group formulation\nwas recently proposed, making use of body-fixed and hybrid representation of\ntwists and wrenches. In this paper a formulation is introduced using the\nspatial representation. The second-order inverse dynamics algorithm is\naccompanied by a fourth-order forward and inverse kinematics algorithm. An\nadvantage of all Lie group formulations is that they can be parameterized in\nterms of vectorial quantities that are readily available. The method is\ndemonstrated for the 7 DOF Franka Emika Panda robot.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10686v1", "AI": {"title_translation": "串联机械臂高阶运动学与逆动力学的一种O(n)算法，采用旋量的空间表示", "tldr": "本文提出了一种使用旋量空间表示的O(n)算法，用于计算串联机械臂的高阶运动学和逆动力学，并展示了其在Franke Emika Panda机器人上的应用。", "motivation": "机器人手臂的优化控制（特别是基于平坦度的控制）需要计算关节扭矩/力的一阶和二阶时间导数，为了提高计算效率，需要紧凑而高效的O(n)递归算法。", "method": "本文提出了一种使用旋量空间表示的公式。该方法包括一个二阶逆动力学算法，并辅以一个四阶正向和逆向运动学算法。该方法利用了李群公式的优势，即它们可以用易于获得的矢量量参数化。", "result": "该方法在7自由度Franke Emika Panda机器人上得到了验证。李群公式的一个优点是它们可以用易于获得的矢量量参数化。", "conclusion": "本文提出的基于旋量空间表示的O(n)算法为串联机械臂的高阶运动学和逆动力学提供了一种高效且紧凑的解决方案，适用于需要高计算效率的机器人控制应用。", "translation": "机器人手臂的优化控制，特别是基于平坦度的控制，需要计算实现期望运动所需的关节扭矩/力的一阶和二阶时间导数。考虑到所需的计算效率，为此提出了递归的O(n)算法。为了实现紧凑而高效的公式，最近提出了一种李群公式，该公式利用了旋量和力偶的体固定和混合表示。本文介绍了一种使用空间表示的公式。二阶逆动力学算法伴随着一个四阶正向和逆向运动学算法。所有李群公式的一个优点是它们可以用易于获得的矢量量参数化。该方法在7自由度Franke Emika Panda机器人上进行了演示。", "summary": "本论文提出了一种新的O(n)算法，用于计算串联机械臂的高阶运动学和逆动力学，该算法采用了旋量的空间表示。针对机器人优化控制中对关节扭矩/力高阶导数计算的需求，该方法提供了一个紧凑且高效的解决方案。文中详细介绍了二阶逆动力学算法以及配套的四阶正向和逆向运动学算法，并强调了李群公式在参数化方面的优势。该算法已在7自由度Franke Emika Panda机器人上进行了验证。", "keywords": "串联机械臂, 高阶运动学, 逆动力学, O(n)算法, 旋量空间表示", "comments": "本文的创新之处在于将旋量的空间表示应用于串联机械臂的高阶运动学和逆动力学计算，并在此基础上提出了O(n)算法，这对于需要高计算效率的机器人控制（如优化控制和基于平坦度的控制）具有重要意义。结合李群公式的优势，使得算法在理论上更为优雅且易于实现。在实际应用中，这种高效的算法可以显著提升机器人控制系统的实时性能。"}}
{"id": "2506.10242", "title": "DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hong Cai", "Fatih Porikli"], "summary": "Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most\nimportant perception tasks in autonomous driving. Earlier methods rely on dense\nBEV features, which are costly to construct. More recent works explore sparse\nquery-based detection. However, they still require a large number of queries\nand can become expensive to run when more video frames are used. In this paper,\nwe propose DySS, a novel method that employs state-space learning and dynamic\nqueries. More specifically, DySS leverages a state-space model (SSM) to\nsequentially process the sampled features over time steps. In order to\nencourage the model to better capture the underlying motion and correspondence\ninformation, we introduce auxiliary tasks of future prediction and masked\nreconstruction to better train the SSM. The state of the SSM then provides an\ninformative yet efficient summarization of the scene. Based on the state-space\nlearned features, we dynamically update the queries via merge, remove, and\nsplit operations, which help maintain a useful, lean set of detection queries\nthroughout the network. Our proposed DySS achieves both superior detection\nperformance and efficient inference. Specifically, on the nuScenes test split,\nDySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the\nart. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a\nreal-time inference speed of 33 FPS.", "comment": "CVPR 2025 Workshop on Autonomous Driving", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10242v1", "AI": {"title_translation": "DySS：用于多摄像头视频高效三维物体检测的动态查询与状态空间学习", "tldr": "DySS利用状态空间学习和动态查询，实现了高效且高性能的多摄像头视频3D物体检测。", "motivation": "早期的BEV 3D物体检测方法依赖于昂贵的密集BEV特征；最近的稀疏查询方法需要大量查询，且在处理更多视频帧时运行成本高昂。", "method": "本文提出DySS，一种新颖的方法，结合了状态空间学习和动态查询。具体来说，DySS利用状态空间模型（SSM）按时间步序处理采样特征，并通过引入未来预测和掩码重建的辅助任务来更好地训练SSM，以捕获运动和对应信息。SSM的状态提供了信息丰富且高效的场景总结。基于SSM学习到的特征，DySS通过合并、移除和分割操作动态更新查询，从而在整个网络中保持一组有用且精简的检测查询。", "result": "在nuScenes测试集上，DySS达到了65.31 NDS和57.4 mAP，超越了最新的最先进水平。在验证集上，DySS达到了56.2 NDS和46.2 mAP，并实现了33 FPS的实时推理速度。", "conclusion": "DySS在3D物体检测中实现了卓越的检测性能和高效的推理。", "translation": "基于摄像头的鸟瞰图（BEV）三维物体检测是自动驾驶中最重要的感知任务之一。早期方法依赖于密集的BEV特征，其构建成本高昂。最近的工作探索了基于稀疏查询的检测方法。然而，它们仍然需要大量的查询，并且在处理更多视频帧时运行成本可能会变得很高。在本文中，我们提出DySS，一种采用状态空间学习和动态查询的新颖方法。更具体地说，DySS利用状态空间模型（SSM）按时间步序处理采样特征。为了鼓励模型更好地捕获底层运动和对应信息，我们引入了未来预测和掩码重建的辅助任务来更好地训练SSM。SSM的状态随后提供了场景的信息丰富且高效的总结。基于状态空间学习到的特征，我们通过合并、移除和分割操作动态更新查询，这有助于在整个网络中保持一组有用、精简的检测查询。我们提出的DySS实现了卓越的检测性能和高效的推理。具体而言，在nuScenes测试集上，DySS达到了65.31 NDS和57.4 mAP，超越了最新的最先进水平。在验证集上，DySS达到了56.2 NDS和46.2 mAP，以及33 FPS的实时推理速度。", "summary": "本文提出DySS，一种用于多摄像头视频3D物体检测的新方法，通过结合状态空间模型（SSM）进行高效的场景特征总结，并利用动态查询（通过合并、移除、分割操作）来优化检测过程。DySS通过辅助任务训练SSM以捕获运动信息，从而在nuScenes数据集上实现了优于现有SOTA的检测性能和实时推理速度。", "keywords": "3D物体检测, 状态空间模型, 动态查询, 多摄像头, BEV", "comments": "本文创新性地结合了状态空间模型和动态查询机制，有效解决了现有BEV 3D检测方法中特征构建成本高和查询效率低的问题。通过SSM对时序特征的有效总结和动态查询的优化，显著提升了检测性能和推理效率，对自动驾驶感知领域具有重要意义。"}}
{"id": "2506.10140", "title": "Survival Analysis as Imprecise Classification with Trainable Kernels", "authors": ["Andrei V. Konstantinov", "Vlada A. Efremenko", "Lev V. Utkin"], "summary": "Survival analysis is a fundamental tool for modeling time-to-event data in\nhealthcare, engineering, and finance, where censored observations pose\nsignificant challenges. While traditional methods like the Beran estimator\noffer nonparametric solutions, they often struggle with the complex data\nstructures and heavy censoring. This paper introduces three novel survival\nmodels, iSurvM (the imprecise Survival model based on Mean likelihood\nfunctions), iSurvQ (the imprecise Survival model based on the Quantiles of\nlikelihood functions), and iSurvJ (the imprecise Survival model based on the\nJoint learning), that combine imprecise probability theory with attention\nmechanisms to handle censored data without parametric assumptions. The first\nidea behind the models is to represent censored observations by interval-valued\nprobability distributions for each instance over time intervals between events\nmoments. The second idea is to employ the kernel-based Nadaraya-Watson\nregression with trainable attention weights for computing the imprecise\nprobability distribution over time intervals for the entire dataset. The third\nidea is to consider three decision strategies for training, which correspond to\nthe proposed three models. Experiments on synthetic and real datasets\ndemonstrate that the proposed models, especially iSurvJ, consistently\noutperform the Beran estimator from the accuracy and computational complexity\npoints of view. Codes implementing the proposed models are publicly available.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10140v1", "AI": {"title_translation": "生存分析作为可训练核的模糊分类", "tldr": "本文提出了三种新的生存模型（iSurvM、iSurvQ、iSurvJ），它们结合了模糊概率理论和注意力机制，通过可训练核来处理截尾数据，并在准确性和计算复杂性方面优于传统的Beran估计器。", "motivation": "在医疗保健、工程和金融领域，生存分析是建模事件发生时间数据的基本工具，但截尾观测值带来了显著挑战。传统的非参数方法（如Beran估计器）在处理复杂数据结构和重度截尾时表现不佳。", "method": "本文提出了三种新颖的生存模型：iSurvM、iSurvQ和iSurvJ，它们将模糊概率理论与注意力机制相结合，以处理截尾数据而无需参数假设。其核心思想包括：1）将截尾观测值表示为每个实例在事件时间间隔内的区间值概率分布；2）采用基于核的Nadaraya-Watson回归，并结合可训练的注意力权重来计算整个数据集在时间间隔上的模糊概率分布；3）考虑三种训练决策策略，对应于提出的三种模型。", "result": "在合成数据集和真实数据集上的实验表明，所提出的模型，特别是iSurvJ，在准确性和计算复杂性方面始终优于Beran估计器。实现这些模型的代码是公开可用的。", "conclusion": "所提出的模糊生存模型为生存分析中处理截尾数据提供了一种优于传统非参数方法的新途径。", "translation": "生存分析是医疗保健、工程和金融领域中用于建模事件发生时间数据的基本工具，其中截尾观测值带来了显著挑战。虽然像Beran估计器这样的传统方法提供了非参数解决方案，但它们通常难以处理复杂的数据结构和重度截尾。本文引入了三种新颖的生存模型，iSurvM（基于均值似然函数的模糊生存模型）、iSurvQ（基于似然函数分位数的模糊生存模型）和iSurvJ（基于联合学习的模糊生存模型），它们将模糊概率理论与注意力机制相结合，以在没有参数假设的情况下处理截尾数据。这些模型背后的第一个想法是将截尾观测值表示为每个实例在事件时间间隔内的区间值概率分布。第二个想法是采用基于核的Nadaraya-Watson回归，并结合可训练的注意力权重来计算整个数据集在时间间隔上的模糊概率分布。第三个想法是考虑三种训练决策策略，对应于所提出的三种模型。在合成数据集和真实数据集上的实验表明，所提出的模型，特别是iSurvJ，在准确性和计算复杂性方面始终优于Beran估计器。实现所提出模型的代码是公开可用的。", "summary": "本文提出了iSurvM、iSurvQ和iSurvJ三种新型生存模型，它们将模糊概率理论与注意力机制和可训练核相结合，以解决截尾数据带来的挑战。通过将截尾观测表示为区间值概率分布并采用基于核的回归，这些模型提供了非参数解决方案。实验结果表明，与Beran估计器相比，这些模型在准确性和计算效率方面均有所提高，其中iSurvJ表现最佳。", "keywords": "生存分析, 模糊概率, 截尾数据, 可训练核, 注意力机制", "comments": "该论文的创新之处在于将模糊概率理论与注意力机制和可训练核相结合应用于生存分析，为处理截尾数据提供了一种鲁棒的非参数解决方案。代码的公开可用性也增加了其实用价值。"}}
{"id": "2506.10527", "title": "LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs", "authors": ["Yanan Cai", "Ahmed Salem", "Besmira Nushi", "Mark Russinovich"], "summary": "We introduce LogiPlan, a novel benchmark designed to evaluate the\ncapabilities of large language models (LLMs) in logical planning and reasoning\nover complex relational structures. Logical relational reasoning is important\nfor applications that may rely on LLMs to generate and query structured graphs\nof relations such as network infrastructure, knowledge bases, or business\nprocess schema. Our framework allows for dynamic variation of task complexity\nby controlling the number of objects, relations, and the minimum depth of\nrelational chains, providing a fine-grained assessment of model performance\nacross difficulty levels. LogiPlan encompasses three complementary tasks: (1)\nPlan Generation, where models must construct valid directed relational graphs\nmeeting specified structural constraints; (2) Consistency Detection, testing\nmodels' ability to identify inconsistencies in relational structures; and (3)\nComparison Question, evaluating models' capacity to determine the validity of\nqueried relationships within a given graph. Additionally, we assess models'\nself-correction capabilities by prompting them to verify and refine their\ninitial solutions. We evaluate state-of-the-art models including DeepSeek R1,\nGemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,\nO3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant\nperformance gaps that correlate with model scale and architecture. Our analysis\ndemonstrates that while recent reasoning-enhanced models show promising results\non simpler instances, they struggle with more complex configurations requiring\ndeeper logical planning.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10527v1", "AI": {"title_translation": "LogiPlan：一个用于LLMs逻辑规划和关系推理的结构化基准", "tldr": "LogiPlan是一个新的基准测试，旨在评估大型语言模型（LLMs）在逻辑规划和复杂关系推理方面的能力，并揭示了现有模型在处理复杂任务时的性能差距。", "motivation": "逻辑关系推理对于依赖LLM生成和查询结构化关系图（如网络基础设施、知识库或业务流程模式）的应用非常重要，因此需要一个专门的基准来评估LLM在这方面的能力。", "method": "本文引入了LogiPlan，一个通过控制对象数量、关系和关系链的最小深度来动态调整任务复杂度的基准。LogiPlan包含三个互补任务：计划生成、一致性检测和比较问题。此外，还评估了模型的自我纠正能力。研究评估了包括DeepSeek R1、Gemini 2.0 Pro、Gemini 2 Flash Thinking、GPT-4.5、GPT-4o、Llama 3.1 405B、O3-mini、O1和Claude 3.7 Sonnet在内的最先进模型。", "result": "评估结果揭示了与模型规模和架构相关的显著性能差距。虽然最近的推理增强模型在简单实例上表现出有希望的结果，但它们在需要更深层次逻辑规划的复杂配置上表现不佳。", "conclusion": "尽管LLMs在简单逻辑推理任务上表现出潜力，但它们在处理需要深层逻辑规划的复杂关系推理任务时仍面临显著挑战，这表明未来需要进一步改进模型架构和推理能力。", "translation": "我们引入LogiPlan，这是一个新颖的基准测试，旨在评估大型语言模型（LLMs）在逻辑规划和复杂关系结构上的推理能力。逻辑关系推理对于可能依赖LLM生成和查询结构化关系图的应用非常重要，例如网络基础设施、知识库或业务流程模式。我们的框架允许通过控制对象数量、关系和关系链的最小深度来动态变化任务复杂性，从而对模型在不同难度级别下的性能进行细粒度评估。LogiPlan包含三个互补任务：(1) 计划生成，模型必须构建符合指定结构约束的有效有向关系图；(2) 一致性检测，测试模型识别关系结构中不一致性的能力；(3) 比较问题，评估模型确定给定图中查询关系有效性的能力。此外，我们通过提示模型验证和完善其初始解决方案来评估模型的自我纠正能力。我们评估了包括DeepSeek R1、Gemini 2.0 Pro、Gemini 2 Flash Thinking、GPT-4.5、GPT-4o、Llama 3.1 405B、O3-mini、O1和Claude 3.7 Sonnet在内的最先进模型在这些任务上的表现，揭示了与模型规模和架构相关的显著性能差距。我们的分析表明，虽然最近的推理增强模型在简单实例上显示出有希望的结果，但它们在需要更深层次逻辑规划的更复杂配置上表现不佳。", "summary": "LogiPlan是一个新的基准测试，用于评估LLMs在逻辑规划和复杂关系推理方面的能力。该基准通过动态调整任务复杂度，涵盖计划生成、一致性检测和比较问题三个核心任务，并评估模型的自我纠正能力。对主流LLMs的评估显示，尽管模型在简单任务上表现良好，但在需要深层逻辑规划的复杂场景下仍存在显著性能差距，且性能与模型规模和架构相关。", "keywords": "逻辑规划, 关系推理, LLMs, 基准测试, LogiPlan", "comments": "LogiPlan的创新之处在于其结构化的基准设计，能够通过动态调整任务复杂度来细粒度评估LLMs的逻辑规划和关系推理能力。其重要性在于揭示了当前LLMs在处理复杂逻辑推理任务时的局限性，特别是在需要深层规划的场景。这为未来LLM的研究指明了方向，即如何提升其在结构化数据推理和复杂问题解决方面的能力。"}}
{"id": "2506.10459", "title": "Boosting Adversarial Transferability for Hyperspectral Image Classification Using 3D Structure-invariant Transformation and Intermediate Feature Distance", "authors": ["Chun Liu", "Bingqian Zhu", "Tao Xu", "Zheng Zheng", "Zheng Li", "Wei Yang", "Zhigang Han", "Jiayao Wang"], "summary": "Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, which pose\nsecurity challenges to hyperspectral image (HSI) classification technologies\nbased on DNNs. In the domain of natural images, numerous transfer-based\nadversarial attack methods have been studied. However, HSIs differ from natural\nimages due to their high-dimensional and rich spectral information. Current\nresearch on HSI adversarial examples remains limited and faces challenges in\nfully utilizing the structural and feature information of images. To address\nthese issues, this paper proposes a novel method to enhance the transferability\nof the adversarial examples for HSI classification models. First, while keeping\nthe image structure unchanged, the proposed method randomly divides the image\ninto blocks in both spatial and spectral dimensions. Then, various\ntransformations are applied on a block by block basis to increase input\ndiversity and mitigate overfitting. Second, a feature distancing loss targeting\nintermediate layers is designed, which measures the distance between the\namplified features of the original examples and the features of the adversarial\nexamples as the primary loss, while the output layer prediction serves as the\nauxiliary loss. This guides the perturbation to disrupt the features of the\ntrue class in adversarial examples, effectively enhancing transferability.\nExtensive experiments demonstrate that the adversarial examples generated by\nthe proposed method achieve effective transferability to black-box models on\ntwo public HSI datasets. Furthermore, the method maintains robust attack\nperformance even under defense strategies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10459v1", "AI": {"title_translation": "使用3D结构不变变换和中间特征距离提升高光谱图像分类的对抗性迁移能力", "tldr": "本文提出一种新方法，通过3D结构不变变换和中间特征距离损失，有效提升高光谱图像对抗样本对黑盒模型的迁移攻击能力。", "motivation": "深度神经网络在高光谱图像分类中易受对抗攻击，而当前高光谱图像对抗样本研究有限，且难以充分利用图像的结构和特征信息，因此需要增强其对抗样本的迁移能力。", "method": "该方法首先在保持图像结构不变的情况下，将图像在空间和光谱维度上随机分块，并对每个块应用不同的变换以增加输入多样性并减少过拟合。其次，设计了针对中间层的特征距离损失，将原始样本的放大特征与对抗样本的特征之间的距离作为主要损失，输出层预测作为辅助损失，以引导扰动破坏真实类别的特征，从而增强迁移能力。", "result": "实验证明，该方法生成的高光谱图像对抗样本在两个公开数据集上对黑盒模型具有有效的迁移能力，并且在防御策略下仍能保持鲁棒的攻击性能。", "conclusion": "该方法通过结合3D结构不变变换和中间特征距离损失，成功提升了高光谱图像分类模型的对抗样本迁移能力，并展现出在防御情况下的鲁棒性。", "translation": "深度神经网络（DNNs）容易受到对抗性攻击，这对基于DNN的高光谱图像（HSI）分类技术构成了安全挑战。在自然图像领域，已经有大量基于迁移的对抗性攻击方法被研究。然而，高光谱图像由于其高维度和丰富的光谱信息与自然图像不同。当前关于高光谱图像对抗样本的研究仍然有限，并且在充分利用图像结构和特征信息方面面临挑战。为了解决这些问题，本文提出了一种新方法来增强高光谱图像分类模型对抗样本的迁移能力。首先，在保持图像结构不变的情况下，所提出的方法在空间和光谱维度上随机将图像分成块。然后，对每个块应用各种变换以增加输入多样性并减轻过拟合。其次，设计了一种针对中间层的特征距离损失，该损失将原始样本的放大特征与对抗样本的特征之间的距离作为主要损失，而输出层预测作为辅助损失。这引导扰动破坏对抗样本中真实类别的特征，有效增强了迁移能力。大量实验表明，所提出的方法生成的对抗样本在两个公共高光谱图像数据集上对黑盒模型实现了有效的迁移能力。此外，即使在防御策略下，该方法也保持了鲁棒的攻击性能。", "summary": "本文针对深度神经网络在高光谱图像分类中易受对抗攻击且现有方法迁移性不足的问题，提出了一种新颖的对抗样本生成方法。该方法通过3D结构不变变换随机分块并施加多样化变换以增加输入多样性，并引入中间特征距离损失来引导扰动，从而有效增强了对抗样本的迁移能力。实验证明，该方法生成的高光谱图像对抗样本对黑盒模型具有良好的迁移性和鲁棒的攻击性能。", "keywords": "高光谱图像分类, 对抗攻击, 迁移能力, 3D结构不变变换, 中间特征距离", "comments": "这篇论文通过引入3D结构不变变换和中间特征距离损失，创新性地解决了高光谱图像对抗样本迁移性差的问题。其亮点在于充分考虑了高光谱图像的特性，通过分块变换增加多样性，并通过特征距离损失精准引导扰动，有效提升了黑盒攻击的成功率，具有重要的实际应用价值。"}}
{"id": "2506.10268", "title": "Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models", "authors": ["Andrea Yaoyun Cui", "Pengfei Yu"], "summary": "Language models are essentially probability distributions over token\nsequences. Auto-regressive models generate sentences by iteratively computing\nand sampling from the distribution of the next token. This iterative sampling\nintroduces stochasticity, leading to the assumption that language models make\nprobabilistic decisions, similar to sampling from unknown distributions.\nBuilding on this assumption, prior research has used simulated Gibbs sampling,\ninspired by experiments designed to elicit human priors, to infer the priors of\nlanguage models. In this paper, we revisit a critical question: Do language\nmodels possess Bayesian brains? Our findings show that under certain\nconditions, language models can exhibit near-deterministic decision-making,\nsuch as producing maximum likelihood estimations, even with a non-zero sampling\ntemperature. This challenges the sampling assumption and undermines previous\nmethods for eliciting human-like priors. Furthermore, we demonstrate that\nwithout proper scrutiny, a system with deterministic behavior undergoing\nsimulated Gibbs sampling can converge to a \"false prior.\" To address this, we\npropose a straightforward approach to distinguish between stochastic and\ndeterministic decision patterns in Gibbs sampling, helping to prevent the\ninference of misleading language model priors. We experiment on a variety of\nlarge language models to identify their decision patterns under various\ncircumstances. Our results provide key insights in understanding decision\nmaking of large language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10268v1", "AI": {"title_translation": "语言模型拥有贝叶斯大脑吗？区分大型语言模型中的随机和确定性决策模式", "tldr": "本文发现语言模型在特定条件下可表现出接近确定性的决策行为，挑战了其概率采样的假设，并提出了一种区分随机与确定性决策模式的方法，以避免推断出错误的语言模型先验。", "motivation": "先前研究假设语言模型通过迭代采样进行概率决策并以此推断其先验，但作者质疑语言模型是否真正拥有贝叶斯大脑，即它们是否总是进行概率性决策。", "method": "1. 挑战了语言模型进行迭代采样的假设。\n2. 揭示了在模拟吉布斯采样下，确定性行为可能导致“错误先验”的问题。\n3. 提出了一种简单方法来区分吉布斯采样中的随机和确定性决策模式。\n4. 在多种大型语言模型上进行实验，以识别其在不同情况下的决策模式。", "result": "1. 语言模型在特定条件下可以表现出接近确定性的决策，例如产生最大似然估计，即使采样温度非零。\n2. 这挑战了采样假设，并削弱了之前用于推断类人先验的方法。\n3. 如果没有适当的审查，一个具有确定性行为的系统在模拟吉布斯采样下可能会收敛到“错误先验”。\n4. 提出的方法有助于防止推断出误导性的语言模型先验。", "conclusion": "语言模型的决策行为可能并非总是随机采样，而是可能表现出确定性，这对于理解LLM的决策过程至关重要，并需重新审视其先验推断方法。", "translation": "语言模型本质上是令牌序列上的概率分布。自回归模型通过迭代计算并从下一个令牌的分布中采样来生成句子。这种迭代采样引入了随机性，导致人们假设语言模型做出概率决策，类似于从未知分布中采样。基于这一假设，先前的研究利用模拟吉布斯采样（受旨在引发人类先验的实验启发）来推断语言模型的先验。在本文中，我们重新审视了一个关键问题：语言模型是否拥有贝叶斯大脑？我们的发现表明，在某些条件下，即使采样温度非零，语言模型也可以表现出接近确定性的决策，例如产生最大似然估计。这挑战了采样假设，并削弱了之前用于推断类人先验的方法。此外，我们证明，如果没有适当的审查，一个具有确定性行为的系统在进行模拟吉布斯采样时可能会收敛到“错误先验”。为了解决这个问题，我们提出了一种直接的方法来区分吉布斯采样中的随机和确定性决策模式，有助于防止推断出误导性的语言模型先验。我们对各种大型语言模型进行了实验，以识别它们在各种情况下的决策模式。我们的结果为理解大型语言模型的决策制定提供了关键见解。", "summary": "本文深入探讨了大型语言模型的决策模式，挑战了其普遍被接受的概率采样假设。研究发现，在特定条件下，语言模型即使在非零采样温度下也能表现出接近确定性的决策行为，例如最大似然估计。这不仅质疑了现有推断人类先验的方法，还指出确定性行为在模拟吉布斯采样中可能导致“错误先验”。为解决此问题，论文提出了一种区分随机和确定性决策模式的简单方法，以避免对语言模型先验的错误推断，并提供了理解LLM决策过程的关键见解。", "keywords": "语言模型, 决策模式, 贝叶斯大脑, 吉布斯采样, 确定性行为", "comments": "这篇论文的创新点在于它挑战了对语言模型决策机制的普遍假设，即它们总是进行概率采样。通过揭示语言模型在特定条件下可能表现出确定性行为，它为理解LLM的内在工作原理提供了新的视角。其重要性在于，它指出了现有方法在推断语言模型先验时可能存在的缺陷，并提出了一种实用的解决方案来避免误导性结果。这对于LLM的可解释性和可靠性研究具有重要意义。"}}
{"id": "2506.10737", "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora", "authors": ["Priyanka Kargupta", "Nan Zhang", "Yunyi Zhang", "Rui Zhang", "Prasenjit Mitra", "Jiawei Han"], "summary": "The rapid evolution of scientific fields introduces challenges in organizing\nand retrieving scientific literature. While expert-curated taxonomies have\ntraditionally addressed this need, the process is time-consuming and expensive.\nFurthermore, recent automatic taxonomy construction methods either (1)\nover-rely on a specific corpus, sacrificing generalizability, or (2) depend\nheavily on the general knowledge of large language models (LLMs) contained\nwithin their pre-training datasets, often overlooking the dynamic nature of\nevolving scientific domains. Additionally, these approaches fail to account for\nthe multi-faceted nature of scientific literature, where a single research\npaper may contribute to multiple dimensions (e.g., methodology, new tasks,\nevaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a\nframework that dynamically adapts an LLM-generated taxonomy to a given corpus\nacross multiple dimensions. TaxoAdapt performs iterative hierarchical\nclassification, expanding both the taxonomy width and depth based on corpus'\ntopical distribution. We demonstrate its state-of-the-art performance across a\ndiverse set of computer science conferences over the years to showcase its\nability to structure and capture the evolution of scientific fields. As a\nmultidimensional method, TaxoAdapt generates taxonomies that are 26.51% more\ngranularity-preserving and 50.41% more coherent than the most competitive\nbaselines judged by LLMs.", "comment": "Accepted to ACL 2025 Main Conference. Code available at:\n  https://github.com/pkargupta/taxoadapt", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10737v1", "AI": {"title_translation": "TaxoAdapt：将基于LLM的多维分类法构建与演进中的研究语料库对齐", "tldr": "TaxoAdapt是一个框架，它能动态地将LLM生成的分类法调整到给定的语料库，并支持多维分类，解决了现有方法在泛化性、动态性和多维性上的不足，并取得了SOTA性能。", "motivation": "科学领域的快速发展给科学文献的组织和检索带来了挑战。传统的专家分类法耗时且昂贵。现有的自动化分类法构建方法存在过度依赖特定语料库导致泛化性差、忽视领域动态性以及未能处理文献多维性（即一篇论文可能涉及多个维度）等问题。", "method": "本文提出了TaxoAdapt框架，它能够动态地将LLM生成的分类法适应到给定语料库，并支持多维分类。TaxoAdapt通过执行迭代分层分类，根据语料库的主题分布扩展分类法的宽度和深度。", "result": "TaxoAdapt在多样化的计算机科学会议数据集上展示了最先进的性能，能够有效地构建和捕捉科学领域的演变。作为一种多维方法，TaxoAdapt生成的分类法在粒度保留方面比最具竞争力的基线高出26.51%，在连贯性方面高出50.41%（由LLM评判）。", "conclusion": "TaxoAdapt成功解决了科学文献组织中动态、多维分类法构建的挑战，并表现出卓越的性能，能够有效地适应不断发展的科学领域。", "translation": "科学领域的快速发展给科学文献的组织和检索带来了挑战。虽然专家策划的分类法传统上满足了这一需求，但其过程耗时且昂贵。此外，最近的自动化分类法构建方法要么 (1) 过度依赖特定语料库，牺牲了泛化性，要么 (2) 严重依赖大型语言模型 (LLM) 预训练数据集中包含的通用知识，常常忽视不断发展的科学领域的动态性质。此外，这些方法未能考虑到科学文献的多面性，即一篇研究论文可能涉及多个维度（例如，方法论、新任务、评估指标、基准）。为了解决这些空白，我们提出了 TaxoAdapt，一个能够将 LLM 生成的分类法动态地适应给定语料库并在多个维度上进行调整的框架。TaxoAdapt 执行迭代分层分类，根据语料库的主题分布扩展分类法的宽度和深度。我们展示了其在多年来多样化的计算机科学会议集上的最先进性能，以展示其构建和捕捉科学领域演变的能力。作为一种多维方法，TaxoAdapt 生成的分类法在 LLM 评判下，比最具竞争力的基线在粒度保留方面高出 26.51%，在连贯性方面高出 50.41%。", "summary": "针对科学文献组织中专家分类耗时且自动化方法存在泛化性差、忽视动态性和缺乏多维性等问题，本文提出了TaxoAdapt框架。该框架能够动态地将LLM生成的分类法适应到特定语料库，并通过迭代分层分类扩展分类法的宽度和深度。实验结果表明，TaxoAdapt在计算机科学领域展示了最先进的性能，其生成的多维分类法在粒度保留和连贯性方面显著优于现有基线。", "keywords": "分类法构建, 大型语言模型, 多维分类, 动态适应, 科学文献组织", "comments": "TaxoAdapt的创新之处在于其动态适应LLM生成分类法到特定语料库的能力，并能处理科学文献的多维性，这对于快速演进的科学领域文献组织至关重要。其迭代分层分类方法和在多维性上的提升是显著的贡献。然而，值得注意的是，其性能评估部分依赖于LLM的判断。"}}
{"id": "2506.10935", "title": "Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials", "authors": ["Ekaterina Grishina", "Matvey Smirnov", "Maxim Rakhuba"], "summary": "The problem of computing optimal orthogonal approximation to a given matrix\nhas attracted growing interest in machine learning. Notable applications\ninclude the recent Muon optimizer or Riemannian optimization on the Stiefel\nmanifold. Among existing approaches, the Newton-Schulz iteration has emerged as\na particularly effective solution, as it relies solely on matrix\nmultiplications and thus achieves high computational efficiency on GPU\nhardware. Despite its efficiency, the method has inherent limitations - its\ncoefficients are fixed and thus not optimized for a given matrix. In this paper\nwe address this issue by proposing a Chebyshev-optimized version of\nNewton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we\ntheoretically derive optimal coefficients for the 3-rd order Newton-Schulz\niteration and apply a Remez algorithm to compute optimal higher-degree\npolynomials. We leverage these polynomials to construct controlled approximate\northogonalization schemes, which is of interest in deep learning applications.\nPractically, we demonstrate the method's effectiveness in two key applications:\northogonalization in the Muon optimizer, and providing an efficient retraction\nalternative for Riemannian optimization on the Stiefel manifold.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.10935v1", "AI": {"title_translation": "通过切比雪夫型多项式加速正交化的牛顿-舒尔茨迭代", "tldr": "本文提出了一种切比雪夫优化的牛顿-舒尔茨方法（CANS），用于加速正交化，解决了现有牛顿-舒尔茨迭代系数固定的局限性，并在机器学习应用中展示了其有效性。", "motivation": "计算给定矩阵最优正交逼近的牛顿-舒尔茨迭代法虽然在GPU上高效，但其系数是固定的，未针对特定矩阵进行优化，存在固有局限性。", "method": "本文提出切比雪夫优化的牛顿-舒尔茨（CANS）方法。基于切比雪夫交错定理，理论推导了三阶牛顿-舒尔茨迭代的最优系数，并应用Remez算法计算最优高阶多项式，进而构建受控的近似正交化方案。", "result": "该方法在两个关键应用中展示了其有效性：Muon优化器中的正交化，以及为Stiefel流形上的黎曼优化提供高效的收缩替代方案。", "conclusion": "通过使用切比雪夫多项式优化牛顿-舒尔茨迭代的系数，CANS方法在机器学习应用中实现了更有效和高效的正交化。", "translation": "计算给定矩阵最优正交逼近的问题在机器学习中引起了越来越多的兴趣。著名的应用包括最近的Muon优化器或Stiefel流形上的黎曼优化。在现有方法中，牛顿-舒尔茨迭代已成为一种特别有效的解决方案，因为它仅依赖于矩阵乘法，因此在GPU硬件上实现了高计算效率。尽管效率很高，但该方法具有固有的局限性——其系数是固定的，因此未针对给定矩阵进行优化。在本文中，我们通过提出一种切比雪夫优化的牛顿-舒尔茨版本（CANS）来解决这个问题。基于切比雪夫交错定理，我们理论上推导了三阶牛顿-舒尔茨迭代的最优系数，并应用Remez算法计算最优高阶多项式。我们利用这些多项式构建受控的近似正交化方案，这在深度学习应用中很有意义。实际上，我们展示了该方法在两个关键应用中的有效性：Muon优化器中的正交化，以及为Stiefel流形上的黎曼优化提供高效的收缩替代方案。", "summary": "本文提出了一种名为CANS的切比雪夫优化牛顿-舒尔茨迭代方法，旨在解决标准牛顿-舒尔茨方法在矩阵正交逼近中系数固定的局限性。通过基于切比雪夫多项式和Remez算法推导最优系数，CANS构建了改进的正交化方案。该方法在深度学习应用中表现出有效性，特别是在Muon优化器和Stiefel流形上黎曼优化的收缩替代方面。", "keywords": "牛顿-舒尔茨迭代, 正交化, 切比雪夫多项式, 机器学习, 黎曼优化", "comments": "该论文通过引入基于切比雪夫多项式的理论优化，解决了高效算法（牛顿-舒尔茨迭代）的一个实际限制。这项创新提高了计算效率和准确性，对于需要鲁棒正交化的机器学习应用（如深度学习优化器和黎曼优化）具有高度相关性。"}}
{"id": "2506.10628", "title": "Leveraging Low-rank Factorizations of Conditional Correlation Matrices in Graph Learning", "authors": ["Thu Ha Phi", "Alexandre Hippert-Ferrer", "Florent Bouchard", "Arnaud Breloy"], "summary": "This paper addresses the problem of learning an undirected graph from data\ngathered at each nodes. Within the graph signal processing framework, the\ntopology of such graph can be linked to the support of the conditional\ncorrelation matrix of the data. The corresponding graph learning problem then\nscales to the squares of the number of variables (nodes), which is usually\nproblematic at large dimension. To tackle this issue, we propose a graph\nlearning framework that leverages a low-rank factorization of the conditional\ncorrelation matrix. In order to solve for the resulting optimization problems,\nwe derive tools required to apply Riemannian optimization techniques for this\nparticular structure. The proposal is then particularized to a low-rank\nconstrained counterpart of the GLasso algorithm, i.e., the penalized maximum\nlikelihood estimation of a Gaussian graphical model. Experiments on synthetic\nand real data evidence that a very efficient dimension-versus-performance\ntrade-off can be achieved with this approach.", "comment": "11 pages, 5 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10628v1", "AI": {"title_translation": "图学习中利用条件相关矩阵的低秩分解", "tldr": "本文提出了一种利用条件相关矩阵低秩分解的图学习框架，以解决大规模图学习中的维度问题，并通过黎曼优化技术实现了高效的性能。", "motivation": "学习无向图的拓扑结构与条件相关矩阵的支持度相关，但该问题在大维度下计算复杂度高（与变量数平方成比例），导致大规模图学习困难。", "method": "提出一种利用条件相关矩阵低秩分解的图学习框架。为解决由此产生的优化问题，推导了应用黎曼优化技术所需的工具。具体将其应用于GLasso算法的低秩约束版本，即高斯图模型的惩罚最大似然估计。", "result": "在合成数据和真实数据上的实验表明，该方法可以在维度与性能之间实现非常有效的权衡。", "conclusion": "通过利用条件相关矩阵的低秩分解和黎曼优化，可以有效地解决大规模图学习中的维度挑战，并在性能上取得良好平衡。", "translation": "本文解决了从每个节点收集的数据中学习无向图的问题。在图信号处理框架内，这种图的拓扑结构可以与数据的条件相关矩阵的支持度相关联。相应的图学习问题会随着变量（节点）数量的平方而扩展，这在大维度下通常是问题。为了解决这个问题，我们提出了一种利用条件相关矩阵低秩分解的图学习框架。为了解决由此产生的优化问题，我们推导了应用黎曼优化技术以适应这种特定结构所需的工具。该提议随后被具体应用于GLasso算法的低秩约束对应物，即高斯图模型的惩罚最大似然估计。在合成数据和真实数据上的实验证明，这种方法可以实现非常有效的维度与性能之间的权衡。", "summary": "本文针对大规模图学习中因条件相关矩阵维度平方增长导致的计算难题，提出了一种基于低秩分解的图学习框架。该框架利用条件相关矩阵的低秩特性，并结合黎曼优化技术求解优化问题。实验结果表明，该方法在处理合成数据和真实数据时，能有效平衡维度与性能，尤其适用于高维场景下的图结构学习。", "keywords": "图学习, 低秩分解, 条件相关矩阵, 黎曼优化, GLasso", "comments": "本文的创新点在于将低秩分解与黎曼优化相结合，有效地解决了高维数据下图学习的计算复杂性问题。通过利用条件相关矩阵的结构特性，提供了一种在保证性能的同时显著降低维度的有效途径，对于大规模图信号处理和机器学习领域具有重要意义。"}}
{"id": "2506.10824", "title": "A Robust Optimization Framework for Flexible Industrial Energy Scheduling: Application to a Cement Plant with Market Participation", "authors": ["Sebastián Rojas-Innocenti", "Enrique Baeyens", "Alejandro Martín-Crespo", "Sergio Saludes-Rodil", "Fernando Frechoso Escudero"], "summary": "This paper presents a scenario based robust optimization framework for short\nterm energy scheduling in electricity intensive industrial plants, explicitly\naddressing uncertainty in planning decisions. The model is formulated as a\ntwo-stage Mixed Integer Linear Program (MILP) and integrates a hybrid scenario\ngeneration method capable of representing uncertain inputs such as electricity\nprices, renewable generation, and internal demand. A convex objective function\ncombining expected and worst case operational costs allows for tunable risk\naversion, enabling planners to balance economic performance and robustness. The\nresulting schedule ensures feasibility across all scenarios and supports\ncoordinated use of industrial flexibility assets, including battery energy\nstorage and shiftable production. To isolate the effects of market volatility,\nthe framework is applied to a real world cement manufacturing case study\nconsidering only day-ahead electricity price uncertainty, with all other inputs\ntreated deterministically. Results show improved resilience to forecast\ndeviations, reduced cost variability, and more consistent operations. The\nproposed method offers a scalable and risk-aware approach for industrial\nflexibility planning under uncertainty.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10824v1", "AI": {"title_translation": "柔性工业能源调度的鲁棒优化框架：在参与市场的 P 厂中的应用", "tldr": "本文提出了一个基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，以应对不确定性，并在水泥厂案例中展示了其效果。", "motivation": "该研究旨在解决电力密集型工业工厂短期能源调度中规划决策的不确定性问题。", "method": "本文提出了一个基于情景的鲁棒优化框架，将其表述为两阶段混合整数线性规划（MILP）。该模型集成了混合情景生成方法，能够表示电力价格、可再生能源发电和内部需求等不确定输入。采用结合预期和最坏情况运营成本的凸目标函数，以实现可调的风险规避。", "result": "结果显示，该框架提高了对预测偏差的弹性，降低了成本变异性，并使运营更加一致。", "conclusion": "所提出的方法为不确定条件下的工业柔性规划提供了一种可扩展且风险感知的途径。", "translation": "本文提出了一个基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，明确解决了规划决策中的不确定性问题。该模型被表述为两阶段混合整数线性规划（MILP），并集成了一种混合情景生成方法，能够表示电力价格、可再生能源发电和内部需求等不确定输入。结合预期和最坏情况运营成本的凸目标函数允许可调的风险规避，使规划者能够平衡经济性能和鲁棒性。由此产生的调度确保了所有情景下的可行性，并支持工业柔性资产（包括电池储能和可班次生产）的协调使用。为了隔离市场波动的影响，该框架应用于一个真实的水泥制造案例研究，仅考虑日前电价不确定性，所有其他输入均被视为确定性。结果显示，该方法提高了对预测偏差的弹性，降低了成本变异性，并使运营更加一致。所提出的方法为不确定条件下的工业柔性规划提供了一种可扩展且风险感知的途径。", "summary": "本文提出了一种基于情景的鲁棒优化框架，用于电力密集型工业工厂的短期能源调度，旨在解决规划中的不确定性。该模型被构建为两阶段混合整数线性规划，并利用混合情景生成方法来处理电力价格、可再生能源和内部需求的不确定性。通过结合预期和最坏情况成本的凸目标函数，该框架允许灵活的风险规避。在水泥厂案例研究中，该方法展示了对预测偏差的更强适应性、更低的成本波动和更稳定的运营，提供了一种可扩展且风险感知的工业柔性规划方法。", "keywords": "鲁棒优化, 能源调度, 工业柔性, 不确定性, 水泥厂", "comments": "该论文的创新之处在于提出了一个结合混合情景生成和可调风险规避的鲁棒优化框架，有效解决了工业能源调度中的不确定性。其重要性在于为电力密集型工业提供了在市场波动下实现更具弹性、成本效益和一致性运营的工具。"}}
{"id": "2506.10338", "title": "Adaptive Chosen-Ciphertext Security of Distributed Broadcast Encryption", "authors": ["Kwangsu Lee"], "summary": "Distributed broadcast encryption (DBE) is a specific kind of broadcast\nencryption (BE) where users independently generate their own public and private\nkeys, and a sender can efficiently create a ciphertext for a subset of users by\nusing the public keys of the subset users. Previously proposed DBE schemes have\nbeen proven in the adaptive chosen-plaintext attack (CPA) security model and\nhave the disadvantage of requiring linear number of pairing operations when\nverifying the public key of a user. In this paper, we propose an efficient DBE\nscheme in bilinear groups and prove adaptive chosen-ciphertext attack (CCA)\nsecurity for the first time. To do this, we first propose a semi-static CCA\nsecure DBE scheme and prove the security under the $q$-Type assumption. Then,\nby modifying the generic transformation of Gentry and Waters that converts a\nsemi-static CPA secure DBE scheme into an adaptive CPA secure DBE scheme to be\napplied to CCA secure DBE schemes, we propose an adaptive CCA secure DBE scheme\nand prove its adaptive CCA security. Our proposed DBE scheme is efficient\nbecause it requires constant size ciphertexts, constant size private keys, and\nlinear size public keys, and the public key verification requires only a\nconstant number of pairing operations and efficient group membership checks.", "comment": "arXiv admin note: text overlap with arXiv:2505.17527", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10338v1", "AI": {"title_translation": "分布式广播加密的自适应选择密文安全性", "tldr": "本文首次提出了一种高效且具有自适应选择密文攻击（CCA）安全性的分布式广播加密（DBE）方案，解决了现有方案仅提供选择明文攻击（CPA）安全性和公钥验证效率低下的问题。", "motivation": "先前的分布式广播加密（DBE）方案仅在自适应选择明文攻击（CPA）安全模型下被证明是安全的，并且在验证用户公钥时需要线性数量的配对操作，效率较低。本研究旨在首次实现DBE方案的自适应选择密文攻击（CCA）安全性，并提高其效率，特别是在公钥验证方面。", "method": "本文首先提出了一种半静态CCA安全的DBE方案，并在q-Type假设下证明了其安全性。随后，通过修改Gentry和Waters的通用转换（该转换用于将半静态CPA安全方案转换为自适应CPA安全方案），使其适用于CCA安全的DBE方案，从而构建了最终的自适应CCA安全DBE方案。", "result": "本文提出的DBE方案首次实现了自适应选择密文攻击（CCA）安全性。该方案是高效的，体现在密文大小、私钥大小为常数，公钥大小为线性，且公钥验证仅需常数次配对操作和高效的群成员检查。", "conclusion": "本文成功提出了一种高效且具备自适应选择密文攻击（CCA）安全性的分布式广播加密（DBE）方案，解决了现有方案在安全模型和公钥验证效率上的局限性。", "translation": "分布式广播加密 (DBE) 是一种特定类型的广播加密 (BE)，其中用户独立生成自己的公钥和私钥，并且发送者可以通过使用子集用户的公钥有效地为用户子集创建密文。先前提出的 DBE 方案已在自适应选择明文攻击 (CPA) 安全模型中得到证明，并且在验证用户公钥时需要线性数量的配对操作，这是一个缺点。在本文中，我们提出了一种双线性群中高效的 DBE 方案，并首次证明了其自适应选择密文攻击 (CCA) 安全性。为此，我们首先提出了一种半静态 CCA 安全的 DBE 方案，并在 q-Type 假设下证明了其安全性。然后，通过修改 Gentry 和 Waters 的通用转换（该转换将半静态 CPA 安全的 DBE 方案转换为自适应 CPA 安全的 DBE 方案），使其适用于 CCA 安全的 DBE 方案，我们提出了一种自适应 CCA 安全的 DBE 方案，并证明了其自适应 CCA 安全性。我们提出的 DBE 方案是高效的，因为它需要恒定大小的密文、恒定大小的私钥和线性大小的公钥，并且公钥验证仅需要恒定数量的配对操作和高效的群成员检查。", "summary": "本文提出了一种高效的分布式广播加密（DBE）方案，首次实现了自适应选择密文攻击（CCA）安全性。该方案克服了现有DBE方案仅在选择明文攻击（CPA）下安全且公钥验证效率低下的局限性。研究方法是先构建一个半静态CCA安全的DBE方案，然后应用改进的通用转换来达到自适应CCA安全。新方案的密文和私钥大小为常数，公钥大小为线性，且公钥验证仅需常数次配对操作，显著提升了效率。", "keywords": "分布式广播加密, 自适应CCA安全性, 双线性群, 公钥验证, 半静态CCA", "comments": "本文的主要创新在于首次将分布式广播加密（DBE）的安全性提升至更强的自适应选择密文攻击（CCA）模型，并同时解决了现有方案在公钥验证效率上的不足。这种安全性与效率的结合，显著增强了DBE方案的实用性。"}}
{"id": "2506.10654", "title": "Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub", "authors": ["Abir Bouraffa", "Carolin Brandt", "Andy Zaidmann", "Walid Maalej"], "summary": "Developers use tools such as GitHub pull requests to review code, discuss\nproposed changes, and request modifications. While changed files are commonly\npresented in alphabetical order, this does not necessarily coincide with the\nreviewer's preferred navigation sequence. This study investigates the different\nnavigation orders developers follow while commenting on changes submitted in\npull requests. We mined code review comments from 23,241 pull requests in 100\npopular Java and Python repositories on GitHub to analyze the order in which\nthe reviewers commented on the submitted changes. Our analysis shows that for\n44.6% of pull requests, the reviewers comment in a non-alphabetical order.\nAmong these pull requests, we identified traces of alternative meaningful\norders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were\ncommented in the order of the files' similarity to the pull request's title and\ndescription, and 29% (1,188) of pull requests containing changes to both\nproduction and test files adhered to a test-first order. We also observed that\nthe proportion of reviewed files to total submitted files was significantly\nhigher in non-alphabetically ordered reviews, which also received slightly\nfewer approvals from reviewers, on average. Our findings highlight the need for\nadditional support during code reviews, particularly for larger pull requests,\nwhere reviewers are more likely to adopt complex strategies rather than\nfollowing a single predefined order.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10654v1", "AI": {"title_translation": "并非一统天下：从GitHub挖掘有意义的代码审查顺序", "tldr": "本研究发现，代码审查者在GitHub上评论时常不按字母顺序，而是采用如按差异大小、文件与PR标题相似度或测试文件优先等有意义的顺序，尤其在大规模PR中。这表明需要为代码审查提供更多支持。", "motivation": "传统的代码审查工具（如GitHub）通常按字母顺序显示文件，但这与审查者实际的导航和评论顺序不符。本研究旨在探究开发者在代码审查中遵循的不同导航顺序。", "method": "研究人员从GitHub上100个流行的Java和Python仓库中挖掘了23,241个拉取请求的代码审查评论，分析了审查者评论提交更改的顺序。", "result": "分析显示，44.6%的拉取请求中，审查者以非字母顺序进行评论。在这些非字母顺序的评论中，20.6%遵循“最大差异优先”顺序，17.6%遵循“文件与拉取请求标题和描述相似度”顺序，29%包含生产和测试文件的拉取请求遵循“测试优先”顺序。此外，非字母顺序审查的被审查文件比例显著更高，但获得的批准略少。", "conclusion": "研究结果强调了在代码审查过程中，尤其对于大型拉取请求，需要额外的支持，因为审查者更可能采用复杂的策略而不是遵循单一的预定义顺序。", "translation": "开发人员使用GitHub拉取请求等工具来审查代码、讨论提议的更改和请求修改。虽然更改的文件通常按字母顺序呈现，但这不一定与审查者偏好的导航顺序一致。本研究调查了开发人员在评论拉取请求中提交的更改时所遵循的不同导航顺序。我们从GitHub上100个流行的Java和Python仓库的23,241个拉取请求中挖掘了代码审查评论，以分析审查者评论提交更改的顺序。我们的分析表明，对于44.6%的拉取请求，审查者以非字母顺序进行评论。在这些拉取请求中，我们发现了其他有意义的替代顺序的痕迹：20.6%（2,134个）遵循“最大差异优先”顺序，17.6%（1,827个）按照文件与拉取请求标题和描述的相似度顺序进行评论，29%（1,188个）包含生产和测试文件的拉取请求遵循“测试优先”顺序。我们还观察到，在非字母顺序审查中，被审查文件占总提交文件的比例显著更高，并且平均获得的审查者批准略少。我们的发现强调了在代码审查过程中需要额外的支持，特别是对于大型拉取请求，审查者更可能采用复杂的策略，而不是遵循单一的预定义顺序。", "summary": "本研究调查了GitHub上代码审查者实际评论文件的顺序，发现近半数审查并非按照默认的字母顺序。研究识别出多种有意义的非字母顺序，包括按差异大小、文件与PR标题相似度以及测试文件优先等策略。研究指出，非字母顺序的审查通常涉及更复杂、更彻底的审查，并强调了为代码审查提供更智能支持的必要性，尤其是在处理大型拉取请求时。", "keywords": "代码审查, GitHub, 审查顺序, 拉取请求, 开发者行为", "comments": "这项研究通过实证分析揭示了代码审查中被忽视的用户行为模式，即审查者并非总是遵循工具的默认顺序。它创新性地识别了多种有意义的审查策略，如“最大差异优先”和“测试优先”，为未来代码审查工具的设计提供了宝贵的见解，有助于开发更符合开发者习惯和效率的智能审查辅助功能。研究的重要性在于它指出了现有工具的局限性，并为提升代码审查效率和质量提供了数据驱动的证据。"}}
{"id": "2506.10756", "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "authors": ["Yuhang Zhang", "Haosheng Yu", "Jiaping Xiao", "Mir Feroskhan"], "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10756v1", "AI": {"title_translation": "面向无人机、具备开放词汇目标理解的接地视觉-语言导航", "tldr": "本文提出了VLFly框架，专门用于无人机（UAV）的视觉-语言导航（VLN），解决了泛化能力和固定离散动作空间的瓶颈，通过单目相机输入输出连续速度指令，实现开放词汇目标理解和泛化导航。", "motivation": "视觉-语言导航（VLN）领域面临两大瓶颈：对分布外环境的泛化能力不足，以及对固定离散动作空间的依赖。", "method": "本文提出了VLFly框架，专为无人机（UAV）设计，用于执行语言引导的飞行。VLFly无需定位或主动测距传感器，仅凭机载单目相机捕获的自我中心观测，即可输出连续的速度指令。该框架集成三个模块：基于大型语言模型（LLM）的指令编码器，将高级语言重构为结构化提示；由视觉-语言模型（VLM）驱动的目标检索器，通过视觉-语言相似性将提示与目标图像匹配；以及一个航点规划器，生成可执行轨迹用于实时无人机控制。", "result": "VLFly在未进行额外微调的情况下，在各种模拟环境中进行了评估，并始终优于所有基线。此外，在室内外环境下，直接和间接指令下的真实世界VLN任务表明，VLFly即使在存在抽象语言输入的情况下，也能实现鲁棒的开放词汇目标理解和泛化导航能力。", "conclusion": "VLFly成功解决了无人机视觉-语言导航中泛化能力不足和固定离散动作空间的挑战，在模拟和真实世界环境中均表现出鲁棒的开放词汇目标理解和泛化导航能力。", "translation": "视觉-语言导航（VLN）是自主机器人领域的一个长期挑战，旨在使智能体能够在复杂环境中根据人类指令进行导航。该领域仍存在两个关键瓶颈：对分布外环境的泛化能力以及对固定离散动作空间的依赖。为了解决这些挑战，我们提出了视觉-语言飞行（VLFly），一个为无人机（UAV）量身定制的框架，用于执行语言引导的飞行。VLFly无需定位或主动测距传感器，仅凭机载单目相机捕获的自我中心观测，即可输出连续的速度指令。VLFly集成了三个模块：一个基于大型语言模型（LLM）的指令编码器，将高级语言重构为结构化提示；一个由视觉-语言模型（VLM）驱动的目标检索器，通过视觉-语言相似性将这些提示与目标图像匹配；以及一个航点规划器，生成可执行的轨迹用于实时无人机控制。VLFly在没有额外微调的情况下，在各种模拟环境中进行了评估，并始终优于所有基线。此外，在室内外环境下，直接和间接指令下的真实世界VLN任务表明，VLFly即使在存在抽象语言输入的情况下，也能实现鲁棒的开放词汇目标理解和泛化导航能力。", "summary": "本文介绍了VLFly，一个用于无人机（UAV）视觉-语言导航（VLN）的新型框架。它通过从单目相机输入生成连续速度指令，且不依赖定位或测距传感器，从而解决了泛化能力和固定动作空间的挑战。VLFly集成了LLM用于指令编码、VLM用于目标检索和航点规划器。评估结果显示，该框架在模拟中表现优越，并在真实世界场景中实现了鲁棒的开放词汇导航，即使面对抽象语言输入也能有效工作。", "keywords": "视觉-语言导航, 无人机, 开放词汇, 连续控制, 大型语言模型", "comments": "该论文的创新点在于提出了VLFly框架，为无人机视觉-语言导航提供了新的解决方案。它通过输出连续速度指令而非离散动作，并结合LLM和VLM实现开放词汇目标理解，有效解决了传统VLN在泛化能力和固定动作空间上的瓶颈。无需额外的定位或测距传感器，仅依赖单目相机输入，大大降低了系统复杂性和成本，提升了实际应用潜力。其在真实世界任务中的鲁棒表现，尤其是在抽象语言输入下的能力，凸显了其重要性。"}}
{"id": "2506.10286", "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models", "authors": ["Eunkyu Park", "Minyeong Kim", "Gunhee Kim"], "summary": "Hallucinations pose a significant challenge to the reliability of large\nvision-language models, making their detection essential for ensuring accuracy\nin critical applications. Current detection methods often rely on\ncomputationally intensive models, leading to high latency and resource demands.\nTheir definitive outcomes also fail to account for real-world scenarios where\nthe line between hallucinated and truthful information is unclear. To address\nthese issues, we propose HalLoc, a dataset designed for efficient,\nprobabilistic hallucination detection. It features 150K token-level annotated\nsamples, including hallucination types, across Visual Question Answering (VQA),\ninstruction-following, and image captioning tasks. This dataset facilitates the\ndevelopment of models that detect hallucinations with graded confidence,\nenabling more informed user interactions. Additionally, we introduce a baseline\nmodel trained on HalLoc, offering low-overhead, concurrent hallucination\ndetection during generation. The model can be seamlessly integrated into\nexisting VLMs, improving reliability while preserving efficiency. The prospect\nof a robust plug-and-play hallucination detection module opens new avenues for\nenhancing the trustworthiness of vision-language models in real-world\napplications. The HalLoc dataset and code are publicly available at:\nhttps://github.com/dbsltm/cvpr25_halloc.", "comment": "CVPR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10286v1", "AI": {"title_translation": "HalLoc：视觉语言模型幻觉的Token级定位", "tldr": "HalLoc提出了一个包含15万个token级标注样本的数据集，用于高效、概率性地检测视觉语言模型中的幻觉，并提供了一个低开销的基线模型。", "motivation": "大型视觉语言模型中的幻觉严重影响其可靠性，而当前的检测方法计算密集、延迟高，且无法处理幻觉与真实信息界限模糊的场景。", "method": "本文提出了HalLoc数据集，包含15万个token级标注样本，涵盖幻觉类型，适用于VQA、指令遵循和图像标注任务，旨在实现高效、概率性幻觉检测。此外，还引入了一个基于HalLoc训练的低开销基线模型，可在生成过程中并发检测幻觉。", "result": "HalLoc数据集促进了具有分级置信度幻觉检测模型的发展。所提出的基线模型能够实现低开销、并发的幻觉检测，并可无缝集成到现有VLM中，在保持效率的同时提高可靠性。", "conclusion": "一个鲁棒的即插即用幻觉检测模块有望为增强视觉语言模型在实际应用中的可信度开辟新途径。", "translation": "幻觉对大型视觉语言模型的可靠性构成了重大挑战，因此，在关键应用中确保准确性，检测幻觉至关重要。当前的检测方法通常依赖于计算密集型模型，导致高延迟和资源需求。它们的确定性结果也未能考虑幻觉信息与真实信息之间界限模糊的现实场景。为了解决这些问题，我们提出了HalLoc，一个专为高效、概率性幻觉检测设计的数据集。它包含15万个token级标注样本，包括幻觉类型，涵盖视觉问答（VQA）、指令遵循和图像标注任务。该数据集有助于开发能够以分级置信度检测幻觉的模型，从而实现更明智的用户交互。此外，我们引入了一个在HalLoc上训练的基线模型，可在生成过程中提供低开销、并发的幻觉检测。该模型可以无缝集成到现有VLM中，在保持效率的同时提高可靠性。鲁棒的即插即用幻觉检测模块的前景为增强视觉语言模型在实际应用中的可信度开辟了新途径。HalLoc数据集和代码可在以下网址公开获取：https://github.com/dbsltm/cvpr25_halloc。", "summary": "本文针对大型视觉语言模型（VLM）中幻觉检测的挑战，提出了HalLoc数据集，一个包含15万个token级标注样本的资源，旨在实现高效、概率性的幻觉检测。该数据集涵盖VQA、指令遵循和图像标注任务，并支持开发能提供分级置信度检测的模型。此外，研究还提出了一个基于HalLoc训练的低开销基线模型，可无缝集成到现有VLM中，以提高其可靠性，为VLM的幻觉检测提供了一个即插即用的解决方案。", "keywords": "幻觉检测, 视觉语言模型, HalLoc, Token级标注, 数据集", "comments": "HalLoc的创新之处在于其大规模的token级标注数据集，这使得幻觉检测能够达到更细粒度的层面，并支持概率性检测，解决了传统方法确定性不足和计算成本高的问题。其提出的即插即用模块概念对于提升VLM在实际应用中的可信度具有重要意义。"}}
{"id": "2506.10144", "title": "Physiological-Model-Based Neural Network for Heart Rate Estimation during Daily Physical Activities", "authors": ["Yaowen Zhang", "Libera Fresiello", "Peter H. Veltink", "Dirk W. Donker", "Ying Wang"], "summary": "Heart failure (HF) poses a significant global health challenge, with early\ndetection offering opportunities for improved outcomes. Abnormalities in heart\nrate (HR), particularly during daily activities, may serve as early indicators\nof HF risk. However, existing HR monitoring tools for HF detection are limited\nby their reliability on population-based averages. The estimation of\nindividualized HR serves as a dynamic digital twin, enabling precise tracking\nof cardiac health biomarkers. Current HR estimation methods, categorized into\nphysiologically-driven and purely data-driven models, struggle with efficiency\nand interpretability. This study introduces a novel physiological-model-based\nneural network (PMB-NN) framework for HR estimation based on oxygen uptake\n(VO2) data during daily physical activities. The framework was trained and\ntested on individual datasets from 12 participants engaged in activities\nincluding resting, cycling, and running. By embedding physiological\nconstraints, which were derived from our proposed simplified human movement\nphysiological model (PM), into the neural network training process, the PMB-NN\nmodel adheres to human physiological principles while achieving high estimation\naccuracy, with a median R$^2$ score of 0.8 and an RMSE of 8.3 bpm. Comparative\nstatistical analysis demonstrates that the PMB-NN achieves performance on par\nwith the benchmark neural network model while significantly outperforming\ntraditional physiological model (p=0.002). In addition, our PMB-NN is adept at\nidentifying personalized parameters of the PM, enabling the PM to generate\nreasonable HR estimation. The proposed framework with a precise VO2 estimation\nsystem derived from body movements enables the future possibilities of\npersonalized and real-time cardiac monitoring during daily life physical\nactivities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10144v1", "AI": {"title_translation": "生理模型神经网络在日常体力活动中心率估计中的应用", "tldr": "提出了一种基于生理模型的神经网络 (PMB-NN)，用于在日常活动中通过氧摄取量准确估计心率，优于传统方法并支持个性化监测。", "motivation": "心力衰竭是全球性的健康挑战，早期发现至关重要。日常活动中心率异常可能是早期指标。现有心率监测工具依赖群体平均值，个体化心率估计是动态数字孪生。当前心率估计方法（生理驱动和纯数据驱动）存在效率和可解释性问题。", "method": "本研究提出了一种新型的生理模型神经网络 (PMB-NN) 框架，用于根据日常体力活动中的氧摄取量 (VO2) 数据进行心率估计。该框架通过将简化的“人体运动生理模型 (PM)”中导出的生理约束嵌入到神经网络训练过程中。模型在来自12名参与者（包括休息、骑自行车和跑步）的个体数据集上进行了训练和测试。", "result": "PMB-NN 模型实现了高估计精度，中位数 R^2 达到 0.8，RMSE 为 8.3 bpm。比较统计分析表明，PMB-NN 的性能与基准神经网络模型相当，并显著优于传统生理模型 (p=0.002)。此外，PMB-NN 能够识别个性化生理模型 (PM) 参数，从而生成合理的心率估计。", "conclusion": "所提出的 PMB-NN 框架结合精确的 VO2 估计系统，为未来在日常生活中进行个性化和实时心脏监测提供了可能性。", "translation": "心力衰竭 (HF) 构成全球性的重大健康挑战，早期检测为改善预后提供了机会。心率 (HR) 异常，特别是在日常活动中，可能作为心力衰竭风险的早期指标。然而，现有用于心力衰竭检测的心率监测工具受限于其对基于人群平均值的可靠性。个体化心率估计作为动态数字孪生，能够精确追踪心脏健康生物标志物。当前的心率估计方法，分为生理驱动模型和纯数据驱动模型，在效率和可解释性方面存在困难。本研究引入了一种新型的基于生理模型的神经网络 (PMB-NN) 框架，用于根据日常体力活动中的氧摄取量 (VO2) 数据进行心率估计。该框架在来自12名参与者的个体数据集上进行了训练和测试，这些参与者从事的活动包括休息、骑自行车和跑步。通过将我们提出的简化人体运动生理模型 (PM) 中导出的生理约束嵌入到神经网络训练过程中，PMB-NN 模型在遵循人体生理学原理的同时实现了高估计精度，中位数 R^2 得分为 0.8，RMSE 为 8.3 bpm。比较统计分析表明，PMB-NN 的性能与基准神经网络模型相当，同时显著优于传统生理模型 (p=0.002)。此外，我们的 PMB-NN 擅长识别 PM 的个性化参数，使 PM 能够生成合理的心率估计。所提出的框架与从身体运动中获得的精确 VO2 估计系统相结合，为未来在日常生活中进行个性化和实时心脏监测提供了可能性。", "summary": "本研究提出了一种新颖的基于生理模型的神经网络 (PMB-NN) 框架，用于在日常体力活动中，利用氧摄取量 (VO2) 数据准确估计心率。该模型通过将简化的生理模型约束嵌入神经网络训练中，确保了生理学合理性。在12名参与者的个体数据集上进行测试，PMB-NN 在心率估计方面表现出高精度（R^2=0.8，RMSE=8.3 bpm），性能与基准神经网络相当，并显著优于传统生理模型。该方法还能识别个性化生理参数，为实现个性化、实时心脏健康监测提供了潜力。", "keywords": "心率估计, 生理模型, 神经网络, 氧摄取量, 个性化监测", "comments": "该研究的创新之处在于结合了生理模型和神经网络的优势，解决了纯数据驱动模型缺乏可解释性和纯生理模型效率低的局限性。通过将生理约束嵌入神经网络训练过程，确保了模型估计的生理合理性，同时保持了数据驱动模型的精度。这对于个性化健康监测，特别是心力衰竭的早期预警具有重要意义。未来结合更精确的VO2估计，有望实现更广泛的应用。"}}
{"id": "2506.10585", "title": "Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning", "authors": ["Mohd Anwar Jamal Faiz"], "summary": "This paper introduces the Primender sequence, a novel integer sequence\ndefined by a hybrid rule that combines classical primality with modular\ndigit-based conditions. Specifically, a number n is included in the sequence if\nit is prime or ends with a prime number of unit digit or any length. In other\nwords, numbers which are primes or have at least one prime suffix. The\nresulting sequence exhibits a deterministic yet non-trivial structure, blending\nnumber-theoretic properties with symbolic patterning. We propose the Primender\nsequence as a benchmark for evaluating the symbolic reasoning capabilities of\nLarge Language Models (LLMs). The study is motivated by the need for\ninterpretable, rule-based testbeds that can assess an LLM's ability to infer\nhidden rules, validate mathematical hypotheses, and generalize symbolic logic\nat scale. A key hypothesis explored is: Whenever a number in the Primender\nsequence is exactly one more than the largest prime less than or equal to it,\nthe difference between it and the previous number in the sequence is also 1. We\ndesign a structured prompt and evaluation framework to test this hypothesis\nacross multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,\nGemini, Grok, and LLaMA. The models are tasked with identifying the underlying\nrule, validating the hypothesis, and generating the next 100,000 terms of the\nsequence. Comparative metrics such as rule inference accuracy, hypothesis\nevaluation, sequence validity, and symbolic explanation quality are used to\nassess model performance. This work contributes a novel mathematical construct\nand a reproducible methodology for benchmarking LLMs in symbolic reasoning,\nhypothesis testing, and scalable pattern generalization - bridging the domains\nof number theory, artificial intelligence, and software engineering.", "comment": "9 pages, 7 figures, 2 tables, 3 codes, oeis sequence A384735", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10585v1", "AI": {"title_translation": "Primender序列：一种用于测试符号推理和AI推理的新型数学结构", "tldr": "本文引入了一种新的整数序列（Primender序列），旨在通过让大型语言模型（LLMs）推断规则、验证假设和生成序列项来评估其符号推理能力。", "motivation": "这项研究的动机是需要可解释的、基于规则的测试平台，以评估大型语言模型（LLMs）推断隐藏规则、验证数学假设和大规模泛化符号逻辑的能力。", "method": "本文提出将Primender序列作为评估大型语言模型（LLMs）符号推理能力的基准。设计了一个结构化提示和评估框架，用于在包括ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA在内的多个最先进的LLMs上测试一个关键假设。模型被要求识别底层规则、验证假设并生成序列的接下来的100,000项。使用规则推理准确性、假设评估、序列有效性和符号解释质量等比较指标来评估模型性能。", "result": "Not mentioned in abstract", "conclusion": "这项工作贡献了一种新颖的数学结构和一种可复现的方法，用于在符号推理、假设检验和可扩展模式泛化方面对大型语言模型进行基准测试——从而连接了数论、人工智能和软件工程领域。", "translation": "本文介绍了Primender序列，这是一种新颖的整数序列，其定义采用了一种结合经典素数性质和基于模数位条件的混合规则。具体而言，一个数n如果它是素数，或者其单位数字或任何长度的后缀是素数，则被包含在序列中。换句话说，序列中的数字是素数，或者至少有一个素数后缀。由此产生的序列展现出确定性但非平凡的结构，融合了数论特性与符号模式。我们提出将Primender序列作为评估大型语言模型（LLMs）符号推理能力的基准。这项研究的动机是需要可解释的、基于规则的测试平台，以评估LLM推断隐藏规则、验证数学假设和大规模泛化符号逻辑的能力。探索的一个关键假设是：每当Primender序列中的一个数字恰好比小于或等于它的最大素数大1时，它与序列中前一个数字之间的差值也为1。我们设计了一个结构化提示和评估框架，用于在包括ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA在内的多个最先进的LLMs上测试此假设。模型的任务是识别底层规则、验证假设并生成序列的接下来的100,000项。使用规则推理准确性、假设评估、序列有效性和符号解释质量等比较指标来评估模型性能。这项工作贡献了一种新颖的数学结构和一种可复现的方法，用于在符号推理、假设检验和可扩展模式泛化方面对LLM进行基准测试——从而连接了数论、人工智能和软件工程领域。", "summary": "本文引入了一种名为Primender序列的新型整数序列，其定义结合了素数性质和基于数字后缀的条件。该序列具有确定但复杂的结构，旨在作为评估大型语言模型（LLMs）符号推理能力的基准。研究动机是需要可解释的、基于规则的测试平台来评估LLMs推断隐藏规则、验证数学假设和泛化符号逻辑的能力。论文提出了一个关键假设，并设计了结构化提示和评估框架，使用ChatGPT、Copilot、DeepSeek、Gemini、Grok和LLaMA等LLMs进行测试。评估指标包括规则推理准确性、假设评估、序列有效性和符号解释质量。这项工作为LLMs在符号推理、假设检验和可扩展模式泛化方面的基准测试提供了一个新颖的数学结构和可复现的方法。", "keywords": "Primender序列, 符号推理, 大型语言模型, 数论, AI基准测试", "comments": "本文引入了一种新颖的数学序列——Primender序列，并将其作为测试大型语言模型（LLMs）符号推理能力的基准，具有创新性。它提供了一个基于规则的、可解释的测试平台，有助于深入理解LLMs的推理机制，并连接了数论、人工智能和软件工程等多个领域，对于AI基准测试领域具有重要意义。"}}
{"id": "2506.10463", "title": "Starting Positions Matter: A Study on Better Weight Initialization for Neural Network Quantization", "authors": ["Stone Yun", "Alexander Wong"], "summary": "Deep neural network (DNN) quantization for fast, efficient inference has been\nan important tool in limiting the cost of machine learning (ML) model\ninference. Quantization-specific model development techniques such as\nregularization, quantization-aware training, and quantization-robustness\npenalties have served to greatly boost the accuracy and robustness of modern\nDNNs. However, very little exploration has been done on improving the initial\nconditions of DNN training for quantization. Just as random weight\ninitialization has been shown to significantly impact test accuracy of floating\npoint models, it would make sense that different weight initialization methods\nimpact quantization robustness of trained models. We present an extensive study\nexamining the effects of different weight initializations on a variety of CNN\nbuilding blocks commonly used in efficient CNNs. This analysis reveals that\neven with varying CNN architectures, the choice of random weight initializer\ncan significantly affect final quantization robustness. Next, we explore a new\nmethod for quantization-robust CNN initialization -- using Graph Hypernetworks\n(GHN) to predict parameters of quantized DNNs. Besides showing that\nGHN-predicted parameters are quantization-robust after regular float32\npretraining (of the GHN), we find that finetuning GHNs to predict parameters\nfor quantized graphs (which we call GHN-QAT) can further improve quantized\naccuracy of CNNs. Notably, GHN-QAT shows significant accuracy improvements for\neven 4-bit quantization and better-than-random accuracy for 2-bits. To the best\nof our knowledge, this is the first in-depth study on quantization-aware DNN\nweight initialization. GHN-QAT offers a novel approach to quantized DNN model\ndesign. Future investigations, such as using GHN-QAT-initialized parameters for\nquantization-aware training, can further streamline the DNN quantization\nprocess.", "comment": "Portions of this article have been presented as extended abstracts at\n  the ICCV 2023 Workshop on Low Bit Quantized Neural Networks (ICCVW-LBQNN\n  2023) and the 2020 Conference on Vision and Intelligent Systems (CVIS 2020).\n  arXiv admin note: text overlap with arXiv:2011.14578, arXiv:2208.12489,\n  arXiv:2309.13773", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10463v1", "AI": {"title_translation": "起始位置很重要：神经网络量化中更好的权重初始化研究", "tldr": "研究发现，权重初始化对神经网络量化鲁棒性有显著影响，并提出了一种基于图超网络（GHN）的新型量化鲁棒初始化方法GHN-QAT，显著提高了量化精度。", "motivation": "现有研究很少关注改进神经网络训练的初始条件对量化的影响。随机权重初始化对浮点模型测试精度有显著影响，因此推断不同的权重初始化方法也会影响训练模型的量化鲁棒性。", "method": "1. 对不同权重初始化方法对多种CNN构建块的影响进行了广泛研究。2. 探索了一种新的量化鲁棒CNN初始化方法：使用图超网络（GHN）预测量化DNN的参数。3. 通过GHN的常规float32预训练来验证GHN预测参数的量化鲁棒性。4. 微调GHN以预测量化图的参数（称为GHN-QAT），以进一步提高量化CNN的精度。", "result": "1. 即使在不同的CNN架构下，随机权重初始化方法的选择也会显著影响最终的量化鲁棒性。2. GHN预测的参数在常规float32预训练后具有量化鲁棒性。3. GHN-QAT能进一步提高CNN的量化精度，对于4比特量化显示出显著的精度提升，对于2比特量化也优于随机初始化。", "conclusion": "本研究首次深入探讨了量化感知DNN权重初始化，并提出了一种新颖的量化DNN模型设计方法GHN-QAT，未来可用于进一步简化DNN量化过程。", "translation": "深度神经网络（DNN）量化用于快速高效的推理，已成为限制机器学习（ML）模型推理成本的重要工具。量化特定的模型开发技术，如正则化、量化感知训练和量化鲁棒性惩罚，极大地提高了现代DNN的准确性和鲁棒性。然而，关于改进DNN训练的初始条件以实现量化的探索却很少。正如随机权重初始化已被证明会显著影响浮点模型的测试精度一样，不同的权重初始化方法也会影响训练模型的量化鲁棒性。我们对不同权重初始化对高效CNN中常用的各种CNN构建块的影响进行了广泛研究。这项分析表明，即使在不同的CNN架构下，随机权重初始化器的选择也会显著影响最终的量化鲁棒性。接下来，我们探索了一种新的量化鲁棒CNN初始化方法——使用图超网络（GHN）预测量化DNN的参数。除了表明GHN预测的参数在常规float32预训练（GHN的）后具有量化鲁棒性外，我们发现微调GHN以预测量化图的参数（我们称之为GHN-QAT）可以进一步提高CNN的量化精度。值得注意的是，GHN-QAT即使对于4比特量化也显示出显著的精度提升，对于2比特量化也优于随机初始化。据我们所知，这是首次对量化感知DNN权重初始化进行深入研究。GHN-QAT为量化DNN模型设计提供了一种新颖的方法。未来的研究，例如使用GHN-QAT初始化的参数进行量化感知训练，可以进一步简化DNN量化过程。", "summary": "本研究深入探讨了神经网络量化中权重初始化的重要性，发现不同的初始化方法显著影响量化鲁棒性。文章提出了一种新的量化鲁棒CNN初始化方法——GHN-QAT（基于图超网络预测量化DNN参数），实验证明GHN-QAT能显著提高量化模型的精度，尤其在低比特量化（如4比特和2比特）下表现出色，为量化DNN模型设计提供了新颖高效的初始化策略。", "keywords": "神经网络量化, 权重初始化, 图超网络, 量化感知训练, 模型鲁棒性", "comments": "该论文创新性地将权重初始化这一传统上被忽视的环节与神经网络量化相结合，填补了该领域的空白。提出的GHN-QAT方法为低比特量化提供了有效的精度提升方案，具有重要的实践价值。其创新点在于利用图超网络生成量化鲁棒的初始参数，这为未来更高效的量化感知训练奠定了基础。"}}
{"id": "2506.10288", "title": "ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs", "authors": ["Zige Wang", "Qi Zhu", "Fei Mi", "Minghui Xu", "Ruochun Jin", "Wenjing Yang"], "summary": "Gradient-based data influence approximation has been leveraged to select\nuseful data samples in the supervised fine-tuning of large language models.\nHowever, the computation of gradients throughout the fine-tuning process\nrequires too many resources to be feasible in practice. In this paper, we\npropose an efficient gradient-based data selection framework with clustering\nand a modified Upper Confidence Bound (UCB) algorithm. Based on the intuition\nthat data samples with similar gradient features will have similar influences,\nwe first perform clustering on the training data pool. Then, we frame the\ninter-cluster data selection as a constrained computing budget allocation\nproblem and consider it a multi-armed bandit problem. A modified UCB algorithm\nis leveraged to solve this problem. Specifically, during the iterative sampling\nprocess, historical data influence information is recorded to directly estimate\nthe distributions of each cluster, and a cold start is adopted to balance\nexploration and exploitation. Experimental results on various benchmarks show\nthat our proposed framework, ClusterUCB, can achieve comparable results to the\noriginal gradient-based data selection methods while greatly reducing computing\nconsumption.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10288v1", "AI": {"title_translation": "ClusterUCB：面向LLM目标微调的高效梯度数据选择", "tldr": "ClusterUCB是一种高效的基于梯度的LLM数据选择框架，通过聚类和改进的UCB算法显著降低计算资源消耗，同时保持性能。", "motivation": "现有的基于梯度的LLM数据选择方法计算资源消耗过大，难以在实际中应用。", "method": "提出ClusterUCB框架，结合聚类和改进的UCB算法。首先对训练数据进行聚类，然后将簇间数据选择视为受限计算预算分配的多臂老虎机问题，并用改进UCB算法解决。该算法记录历史数据影响信息以直接估计簇分布，并采用冷启动平衡探索与利用。", "result": "在多个基准测试上，ClusterUCB在大幅减少计算消耗的同时，实现了与原始基于梯度的LLM数据选择方法相当的性能。", "conclusion": "ClusterUCB通过解决计算瓶颈，为LLM微调中的梯度数据选择提供了一种高效且有效的方法。", "translation": "基于梯度的数据影响近似已被用于在大语言模型的监督微调中选择有用的数据样本。然而，在整个微调过程中计算梯度需要过多的资源，这在实践中是不可行的。在本文中，我们提出了一个结合聚类和改进的上限置信区间（UCB）算法的高效梯度数据选择框架。基于梯度特征相似的数据样本具有相似影响的直觉，我们首先对训练数据池进行聚类。然后，我们将簇间数据选择构建为一个受约束的计算预算分配问题，并将其视为一个多臂老虎机问题。利用改进的UCB算法来解决这个问题。具体来说，在迭代采样过程中，记录历史数据影响信息以直接估计每个簇的分布，并采用冷启动来平衡探索和利用。在各种基准测试上的实验结果表明，我们提出的框架ClusterUCB可以在大幅减少计算消耗的同时，达到与原始基于梯度的数据选择方法相当的结果。", "summary": "本文提出了ClusterUCB，一个用于LLM目标微调的高效梯度数据选择框架。针对现有基于梯度的数据选择方法计算资源消耗过大的问题，ClusterUCB首先对训练数据进行聚类，然后将簇间数据选择建模为多臂老虎机问题，并利用改进的UCB算法进行解决。实验结果表明，ClusterUCB在大幅降低计算消耗的同时，能够达到与原始方法相当的性能。", "keywords": "梯度数据选择, LLM微调, 聚类, UCB, 多臂老虎机", "comments": "该论文的创新点在于将聚类与改进的UCB算法相结合，以实现高效的梯度数据选择，有效解决了LLM微调中梯度计算的资源瓶颈。这对于将基于梯度的数据选择方法实际应用于LLM微调具有重要意义。潜在的局限性可能在于聚类能否准确捕捉到数据样本的梯度特征相似性。"}}
{"id": "2506.10844", "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training", "authors": ["Alireza Salemi", "Mukta Maddipatla", "Hamed Zamani"], "summary": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)\nframework composed of specialized agents for subtasks such as planning,\nsearching, reasoning, and coordination. Our system uses a self-training\nparadigm with reward-guided trajectory sampling to optimize inter-agent\ncollaboration and enhance response generation. Evaluated on DataMorgana-derived\ndatasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms\nconventional RAG baselines. We further analyze competition outcomes and\nshowcase the framework's strengths with case studies, demonstrating its\nefficacy for complex, real-world RAG tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10844v1", "AI": {"title_translation": "CIIR@LiveRAG 2025：通过自训练优化多智能体检索增强生成", "tldr": "本文提出了mRAG，一个多智能体检索增强生成（RAG）框架，通过自训练和奖励引导的轨迹采样优化智能体协作，并在SIGIR 2025 LiveRAG竞赛中表现优于传统RAG基线，展示了其在复杂真实世界RAG任务中的有效性。", "motivation": "本文旨在通过引入多智能体RAG框架和自训练范式来优化智能体间的协作，从而提升响应生成质量，以应对复杂、真实世界的检索增强生成（RAG）任务。", "method": "本文提出了mRAG，一个多智能体检索增强生成（RAG）框架。该框架由专门负责规划、搜索、推理和协调等子任务的智能体组成。系统采用自训练范式，结合奖励引导的轨迹采样来优化智能体间的协作，并增强响应生成。", "result": "mRAG在SIGIR 2025 LiveRAG竞赛期间，于DataMorgana衍生数据集上的评估中，表现优于传统的RAG基线。通过案例研究进一步展示了该框架的优势和有效性。", "conclusion": "mRAG框架通过优化的多智能体协作和自训练机制，在复杂、真实世界的检索增强生成（RAG）任务中表现出显著的有效性。", "translation": "本文介绍了mRAG，一个多智能体检索增强生成（RAG）框架，由用于规划、搜索、推理和协调等子任务的专业智能体组成。我们的系统采用自训练范式，通过奖励引导的轨迹采样来优化智能体间的协作并增强响应生成。在SIGIR 2025 LiveRAG竞赛期间，mRAG在DataMorgana衍生数据集上进行了评估，其性能优于传统的RAG基线。我们进一步分析了竞赛结果，并通过案例研究展示了该框架的优势，证明了其在复杂、真实世界RAG任务中的有效性。", "summary": "本文提出mRAG，一个多智能体检索增强生成（RAG）框架，其特色在于包含专门处理规划、搜索、推理和协调等子任务的智能体。该框架采用奖励引导的轨迹采样自训练范式，旨在优化智能体间的协作并提升响应生成质量。mRAG在SIGIR 2025 LiveRAG竞赛的DataMorgana衍生数据集上进行了评估，结果显示其性能优于传统的RAG基线，并通过案例研究进一步证明了其在复杂真实世界RAG任务中的有效性。", "keywords": "多智能体RAG, 自训练, 检索增强生成, LiveRAG, 智能体协作", "comments": "该论文的创新点在于提出了一个多智能体RAG框架，并引入了自训练范式和奖励引导的轨迹采样来优化智能体间的协作，这对于提升复杂RAG任务的性能具有重要意义。其在LiveRAG竞赛中的优异表现也证明了其有效性。"}}
{"id": "2506.10835", "title": "General Reference Frame Identification and Transformation in Unbalanced Power Systems", "authors": ["Francisco G. Montoya", "Santiago Sánchez Acevedo"], "summary": "Various domains such as power system stability analysis, electric machine\nmodeling, and control of power electronic converters have significantly\nbenefited from the application of coordinate transformations. One of the main\nbenefits is the dimensional reduction, which reduces the complexity of the\nproblems. This paper introduces a novel general transformation based on a\ngeometric framework that directly identifies the plane containing the locus for\nunbalanced quantities through bivector analysis using Geometric Algebra. The\nproposed method provides a direct transformation valid for any degree of\nunbalance in $n$-phase, $(n+1)$-wire sinusoidal systems. The transformation\nrequires only two measurements (voltage or current) taken at different time\ninstants, making it computationally efficient. Moreover, we demonstrate through\npure geometric reasoning that our approach is general and encompasses other\ntechniques, such as the classical Clarke transformation. Numerical simulations\nand experimental validation using a real-time digital simulator and a physical\nlaboratory setup demonstrate the effectiveness of the proposed method. This\ngeneralization to multi-dimensional systems, combined with the reduced\nmeasurement requirements, represents a significant advancement over existing\napproaches that are typically restricted to three-phase applications or suffer\nfrom computational limitations.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10835v1", "AI": {"title_translation": "不平衡电力系统中通用参考系识别与变换", "tldr": "本文提出了一种基于几何代数的新型通用变换方法，用于不平衡电力系统中的坐标变换，该方法通过二向量分析直接识别轨迹平面，适用于任意不平衡程度的n相、(n+1)线系统，计算效率高，并包含Clarke变换等现有技术。", "motivation": "坐标变换在电力系统稳定性分析、电机建模和电力电子变换器控制等领域具有显著优势，主要益处是降维和降低问题复杂性。然而，现有方法可能受限于三相应用或存在计算限制，因此需要一种更通用、高效的变换方法。", "method": "本文提出了一种基于几何框架的新型通用变换方法，利用几何代数中的二向量分析，直接识别包含不平衡量轨迹的平面。该方法仅需在不同时刻进行两次测量（电压或电流），即可实现n相、(n+1)线正弦系统中的任意不平衡程度的直接变换。", "result": "所提出的方法为n相、(n+1)线正弦系统提供了直接变换，适用于任意程度的不平衡。它仅需要两次测量，计算效率高。通过纯几何推理证明，该方法具有通用性，并包含经典的Clarke变换等其他技术。数值模拟和实验验证（使用实时数字仿真器和物理实验室设置）证明了其有效性。", "conclusion": "本文提出的通用变换方法通过几何代数和二向量分析，成功实现了不平衡电力系统中的高效、通用坐标变换。该方法对多维系统的泛化能力以及减少的测量要求，是现有方法的重大进步，克服了它们通常局限于三相应用或计算限制的缺点。", "translation": "各种领域，如电力系统稳定性分析、电机建模和电力电子变换器控制，都显著受益于坐标变换的应用。其中一个主要益处是降维，它降低了问题的复杂性。本文引入了一种基于几何框架的新型通用变换，通过使用几何代数中的二向量分析直接识别包含不平衡量轨迹的平面。所提出的方法提供了一种直接变换，适用于n相、(n+1)线正弦系统中任意程度的不平衡。该变换仅需要不同时刻的两次测量（电压或电流），使其计算效率高。此外，我们通过纯几何推理证明，我们的方法具有通用性，并包含了经典的Clarke变换等其他技术。使用实时数字仿真器和物理实验室设置进行的数值模拟和实验验证证明了所提出方法的有效性。这种对多维系统的泛化能力，结合减少的测量要求，代表了对现有方法的重大进步，现有方法通常局限于三相应用或存在计算限制。", "summary": "本文提出了一种基于几何代数和二向量分析的新型通用变换方法，用于不平衡电力系统中的参考系识别与变换。该方法通过直接识别不平衡量轨迹所在的平面，适用于任意不平衡程度的n相、(n+1)线系统，且仅需两次测量，计算效率高。研究表明，该方法具有通用性，并涵盖了经典的Clarke变换。数值模拟和实验验证证明了其有效性，为多维系统提供了显著优于现有方法的解决方案。", "keywords": "通用变换, 几何代数, 不平衡电力系统, 坐标变换, 二向量分析", "comments": "该论文的创新点在于将几何代数和二向量分析应用于不平衡电力系统的坐标变换，实现了对任意不平衡程度和多相系统的通用变换。其仅需两次测量的特点显著提高了计算效率，并克服了现有方法在三相限制和计算复杂性上的不足，具有重要的理论和实际应用价值。"}}
{"id": "2506.10399", "title": "FicGCN: Unveiling the Homomorphic Encryption Efficiency from Irregular Graph Convolutional Networks", "authors": ["Zhaoxuan Kan", "Husheng Han", "Shangyi Shi", "Tenghui Hua", "Hang Lu", "Xiaowei Li", "Jianan Mu", "Xing Hu"], "summary": "Graph Convolutional Neural Networks (GCNs) have gained widespread popularity\nin various fields like personal healthcare and financial systems, due to their\nremarkable performance. Despite the growing demand for cloud-based GCN\nservices, privacy concerns over sensitive graph data remain significant.\nHomomorphic Encryption (HE) facilitates Privacy-Preserving Machine Learning\n(PPML) by allowing computations to be performed on encrypted data. However, HE\nintroduces substantial computational overhead, particularly for GCN operations\nthat require rotations and multiplications in matrix products. The sparsity of\nGCNs offers significant performance potential, but their irregularity\nintroduces additional operations that reduce practical gains. In this paper, we\npropose FicGCN, a HE-based framework specifically designed to harness the\nsparse characteristics of GCNs and strike a globally optimal balance between\naggregation and combination operations. FicGCN employs a latency-aware packing\nscheme, a Sparse Intra-Ciphertext Aggregation (SpIntra-CA) method to minimize\nrotation overhead, and a region-based data reordering driven by local adjacency\nstructure. We evaluated FicGCN on several popular datasets, and the results\nshow that FicGCN achieved the best performance across all tested datasets, with\nup to a 4.10x improvement over the latest design.", "comment": "Accepted by ICML 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10399v1", "AI": {"title_translation": "FicGCN：揭示不规则图卷积网络中的同态加密效率", "tldr": "FicGCN通过优化同态加密下的图卷积网络操作，显著提升了隐私保护GCN的效率。", "motivation": "图卷积网络（GCN）在个人医疗和金融系统等领域广泛应用，但基于云的GCN服务面临敏感图数据的隐私泄露风险。同态加密（HE）虽能实现隐私保护机器学习（PPML），但其计算开销巨大，尤其对于GCN中涉及旋转和矩阵乘法的操作。GCN的稀疏性虽有性能潜力，但其不规则性引入了额外的操作，降低了实际收益。", "method": "本文提出了FicGCN，一个基于同态加密的框架，专门设计用于利用GCN的稀疏特性，并在聚合和组合操作之间取得全局最优平衡。FicGCN采用了延迟感知打包方案、稀疏密文内聚合（SpIntra-CA）方法以最小化旋转开销，以及基于局部邻接结构驱动的区域数据重排序。", "result": "FicGCN在多个流行数据集上进行了评估，结果表明FicGCN在所有测试数据集上均取得了最佳性能，比最新设计提升了高达4.10倍。", "conclusion": "FicGCN通过优化同态加密下的图卷积网络操作，有效解决了隐私保护GCN的效率瓶颈，实现了显著的性能提升。", "translation": "图卷积神经网络（GCN）因其卓越的性能在个人医疗和金融系统等各个领域获得了广泛普及。尽管对基于云的GCN服务需求日益增长，但敏感图数据的隐私问题仍然突出。同态加密（HE）通过允许在加密数据上执行计算来促进隐私保护机器学习（PPML）。然而，HE引入了巨大的计算开销，特别是对于GCN操作中需要旋转和矩阵乘法的情况。GCN的稀疏性提供了显著的性能潜力，但其不规则性引入了额外的操作，从而降低了实际收益。在本文中，我们提出了FicGCN，一个基于HE的框架，专门设计用于利用GCN的稀疏特性，并在聚合和组合操作之间实现全局最优平衡。FicGCN采用了延迟感知打包方案、一种稀疏密文内聚合（SpIntra-CA）方法以最小化旋转开销，以及由局部邻接结构驱动的区域数据重排序。我们在几个流行数据集上评估了FicGCN，结果显示FicGCN在所有测试数据集上均取得了最佳性能，比最新设计提升了高达4.10倍。", "summary": "本文提出了FicGCN，一个基于同态加密的框架，旨在提高隐私保护图卷积网络（GCN）的效率。它通过采用延迟感知打包方案、稀疏密文内聚合（SpIntra-CA）和基于区域的数据重排序等方法，解决了同态加密（HE）在不规则和稀疏GCN操作中的高计算开销问题。评估结果显示，FicGCN实现了卓越的性能，比现有设计加速高达4.10倍。", "keywords": "同态加密, 图卷积网络, 隐私保护机器学习, 效率, 不规则图", "comments": "该论文解决了隐私保护机器学习中的一个关键挑战，即同态加密在不规则GCN中的效率瓶颈。所提出的FicGCN通过专门利用GCN的稀疏性和结构来优化HE操作，特别是最小化旋转开销，从而实现了创新。这项工作对于在敏感领域实际部署隐私保护GCN服务具有重要意义。"}}
{"id": "2506.10704", "title": "Formalising Software Requirements using Large Language Models", "authors": ["Arshad Beg", "Diarmuid O'Donoghue", "Rosemary Monahan"], "summary": "This paper is a brief introduction to our recently initiated project named\nVERIFAI: Traceability and verification of natural language requirements. The\nproject addresses the challenges in the traceability and verification of formal\nspecifications through providing support for the automatic generation of the\nformal specifications and the traceability of the requirements from the initial\nsoftware design stage through the systems implementation and verification.\nApproaches explored in this project include Natural Language Processing, use of\nontologies to describe the software system domain, reuse of existing software\nartefacts from similar systems (i.e. through similarity based reuse) and large\nlanguage models to identify and declare the specifications as well as use of\nartificial intelligence to guide the process.", "comment": "Accepted and presented as a poster in ADAPT Annual Conference\n  (AACS2025) on 15th of May, 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10704v1", "AI": {"title_translation": "使用大型语言模型形式化软件需求", "tldr": "该论文简要介绍了VERIFAI项目，该项目旨在通过自动化形式化规范生成和需求可追溯性来解决自然语言需求的可追溯性和验证挑战，并探索了NLP、本体论、软件构件复用和大型语言模型等方法。", "motivation": "解决形式化规范的可追溯性和验证方面的挑战，特别是在自然语言需求的处理上。", "method": "探索的方法包括自然语言处理（NLP）、使用本体论描述软件系统领域、基于相似性的现有软件构件复用、使用大型语言模型识别和声明规范，以及利用人工智能指导整个过程。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "这篇论文简要介绍了我们最近启动的一个名为VERIFAI的项目：自然语言需求的可追溯性和验证。该项目旨在通过支持形式化规范的自动生成以及从初始软件设计阶段到系统实施和验证的需求可追溯性，来解决形式化规范的可追溯性和验证方面的挑战。该项目探索的方法包括自然语言处理、使用本体论描述软件系统领域、复用来自类似系统的现有软件构件（即通过基于相似性的复用），以及使用大型语言模型识别和声明规范，同时利用人工智能指导整个过程。", "summary": "本文简要介绍了名为VERIFAI的项目，该项目致力于解决自然语言需求的形式化规范可追溯性和验证问题。项目旨在通过自动化形式化规范生成和实现从设计到验证的需求全程可追溯性。其主要方法包括自然语言处理、本体论、基于相似性的软件构件复用、大型语言模型以及人工智能引导。", "keywords": "软件需求, 大型语言模型, 可追溯性, 形式化规范, 自然语言处理", "comments": "这篇论文作为对一个新项目的介绍，其创新点在于结合了大型语言模型和人工智能技术来自动化软件需求的形式化和可追溯性，这对于提高软件开发效率和质量具有重要意义。项目的全面性体现在其覆盖了从需求声明到系统验证的整个生命周期。"}}
{"id": "2506.10787", "title": "In-Hand Object Pose Estimation via Visual-Tactile Fusion", "authors": ["Felix Nonnengießer", "Alap Kshirsagar", "Boris Belousov", "Jan Peters"], "summary": "Accurate in-hand pose estimation is crucial for robotic object manipulation,\nbut visual occlusion remains a major challenge for vision-based approaches.\nThis paper presents an approach to robotic in-hand object pose estimation,\ncombining visual and tactile information to accurately determine the position\nand orientation of objects grasped by a robotic hand. We address the challenge\nof visual occlusion by fusing visual information from a wrist-mounted RGB-D\ncamera with tactile information from vision-based tactile sensors mounted on\nthe fingertips of a robotic gripper. Our approach employs a weighting and\nsensor fusion module to combine point clouds from heterogeneous sensor types\nand control each modality's contribution to the pose estimation process. We use\nan augmented Iterative Closest Point (ICP) algorithm adapted for weighted point\nclouds to estimate the 6D object pose. Our experiments show that incorporating\ntactile information significantly improves pose estimation accuracy,\nparticularly when occlusion is high. Our method achieves an average pose\nestimation error of 7.5 mm and 16.7 degrees, outperforming vision-only\nbaselines by up to 20%. We also demonstrate the ability of our method to\nperform precise object manipulation in a real-world insertion task.", "comment": "8 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10787v1", "AI": {"title_translation": "手持物体位姿估计：基于视觉-触觉融合", "tldr": "本研究提出一种结合视觉和触觉信息的方法，用于机器人手持物体位姿估计，有效解决了视觉遮挡问题，提高了估计精度，并成功应用于实际操作任务。", "motivation": "机器人物体操作中准确的手持位姿估计至关重要，但视觉遮挡是基于视觉方法的主要挑战。", "method": "本方法通过融合腕部RGB-D相机和指尖视觉触觉传感器的信息来解决视觉遮挡问题。采用加权传感器融合模块组合来自不同传感器类型的点云，并控制每种模态对位姿估计的贡献。使用适用于加权点云的增强迭代最近点（ICP）算法来估计6D物体位姿。", "result": "实验表明，整合触觉信息显著提高了位姿估计精度，尤其是在高遮挡情况下。该方法平均位姿估计误差为7.5毫米和16.7度，比纯视觉基线表现高出20%。同时，该方法在真实世界的插入任务中展示了精确的物体操作能力。", "conclusion": "通过视觉与触觉信息的有效融合，可以显著提高机器人手持物体的位姿估计精度，尤其是在视觉遮挡严重的场景下，并能支持精确的机器人操作任务。", "translation": "准确的手持位姿估计对于机器人物体操作至关重要，但视觉遮挡仍然是基于视觉方法的主要挑战。本文提出一种机器人手持物体位姿估计方法，结合视觉和触觉信息，以准确确定机器人手抓取物体的姿态和方向。我们通过融合腕部安装的RGB-D相机的视觉信息与安装在机器人夹持器指尖上的基于视觉的触觉传感器的触觉信息，解决了视觉遮挡的挑战。我们的方法采用加权和传感器融合模块，以组合来自异构传感器类型的点云，并控制每种模态对位姿估计过程的贡献。我们使用一种适用于加权点云的增强迭代最近点（ICP）算法来估计6D物体位姿。我们的实验表明，纳入触觉信息显著提高了位姿估计精度，尤其是在遮挡较高时。我们的方法实现了7.5毫米和16.7度的平均位姿估计误差，比纯视觉基线高出20%。我们还展示了我们的方法在真实世界插入任务中执行精确物体操作的能力。", "summary": "本论文提出一种用于机器人手持物体位姿估计的新方法，通过创新性地融合来自腕部RGB-D相机和指尖触觉传感器的视觉与触觉信息，有效解决了视觉遮挡带来的挑战。该方法利用加权传感器融合模块整合异构点云，并采用增强型加权ICP算法进行6D位姿估计。实验结果显示，该方法显著提升了位姿估计精度，尤其在高遮挡条件下，并成功应用于实际的物体操作任务。", "keywords": "位姿估计, 视觉-触觉融合, 机器人操作, 视觉遮挡, ICP", "comments": "该研究的创新之处在于其视觉-触觉融合策略，有效克服了传统视觉方法在机器人手持操作中面临的视觉遮挡问题。通过引入触觉信息并采用加权融合机制，显著提高了位姿估计的鲁棒性和精度，对机器人灵巧操作领域具有重要意义。"}}
{"id": "2506.10302", "title": "Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation", "authors": ["Hamzeh Asgharnezhad", "Pegah Tabarisaadi", "Abbas Khosravi", "Roohallah Alizadehsani", "U. Rajendra Acharya"], "summary": "Accurate and reliable skin cancer diagnosis is critical for early treatment\nand improved patient outcomes. Deep learning (DL) models have shown promise in\nautomating skin cancer classification, but their performance can be limited by\ndata scarcity and a lack of uncertainty awareness. In this study, we present a\ncomprehensive evaluation of DL-based skin lesion classification using transfer\nlearning and uncertainty quantification (UQ) on the HAM10000 dataset. In the\nfirst phase, we benchmarked several pre-trained feature extractors-including\nContrastive Language-Image Pretraining (CLIP) variants, Residual Network-50\n(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual\nGeometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range\nof traditional classifiers such as Support Vector Machine (SVM), eXtreme\nGradient Boosting (XGBoost), and logistic regression. Our results show that\nCLIP-based vision transformers, particularly LAION CLIP ViT-H/14 with SVM,\ndeliver the highest classification performance. In the second phase, we\nincorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte\nCarlo Dropout (EMCD) to assess not only prediction accuracy but also the\nreliability of model outputs. We evaluated these models using uncertainty-aware\nmetrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),\nuncertainty specificity(USpe), and uncertainty precision(UPre). The results\ndemonstrate that ensemble methods offer a good trade-off between accuracy and\nuncertainty handling, while EMCD is more sensitive to uncertain predictions.\nThis study highlights the importance of integrating UQ into DL-based medical\ndiagnosis to enhance both performance and trustworthiness in real-world\nclinical applications.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10302v1", "AI": {"title_translation": "不确定性感知深度学习在自动化皮肤癌分类中的应用：一项综合评估", "tldr": "本研究全面评估了结合迁移学习和不确定性量化（UQ）的深度学习模型在皮肤癌分类中的表现。结果显示，CLIP视觉转换器表现最佳，而集成方法在准确性和不确定性处理之间提供了良好平衡，强调了UQ在医学诊断中的重要性。", "motivation": "准确可靠的皮肤癌诊断对于早期治疗和改善患者预后至关重要。尽管深度学习模型在自动化皮肤癌分类方面显示出前景，但其性能可能受限于数据稀缺和缺乏不确定性感知。", "method": "本研究对基于深度学习的皮肤病变分类进行了综合评估，使用了迁移学习和不确定性量化（UQ），并在HAM10000数据集上进行。第一阶段，基准测试了多种预训练特征提取器（包括CLIP变体、ResNet50、DenseNet121、VGG16、EfficientNet-V2-Large）与传统分类器（SVM、XGBoost、逻辑回归）。第二阶段，引入了蒙特卡洛Dropout (MCD)、集成（Ensemble）和集成蒙特卡洛Dropout (EMCD) 进行UQ，并使用不确定性感知指标（如UAcc、USen、USpe、UPre）评估模型。", "result": "在第一阶段，基于CLIP的视觉转换器，特别是LAION CLIP ViT-H/14与SVM结合，提供了最高的分类性能。在第二阶段，结果表明集成方法在准确性和不确定性处理之间提供了良好的权衡，而EMCD对不确定预测更敏感。", "conclusion": "本研究强调了将不确定性量化整合到基于深度学习的医学诊断中的重要性，以提高实际临床应用中的性能和可信度。", "translation": "准确可靠的皮肤癌诊断对于早期治疗和改善患者预后至关重要。深度学习（DL）模型在自动化皮肤癌分类方面显示出前景，但其性能可能受限于数据稀缺和缺乏不确定性感知。在本研究中，我们对使用迁移学习和不确定性量化（UQ）的基于DL的皮肤病变分类进行了综合评估，数据集为HAM10000。在第一阶段，我们基准测试了几种预训练特征提取器——包括对比语言-图像预训练（CLIP）变体、残差网络-50（ResNet50）、密集连接卷积网络（DenseNet121）、视觉几何组网络（VGG16）和EfficientNet-V2-Large——并结合了一系列传统分类器，如支持向量机（SVM）、极端梯度提升（XGBoost）和逻辑回归。我们的结果显示，基于CLIP的视觉转换器，特别是LAION CLIP ViT-H/14与SVM结合，提供了最高的分类性能。在第二阶段，我们使用蒙特卡洛Dropout（MCD）、集成（Ensemble）和集成蒙特卡洛Dropout（EMCD）引入了UQ，不仅评估了预测准确性，还评估了模型输出的可靠性。我们使用不确定性感知指标，如不确定性准确度（UAcc）、不确定性敏感度（USen）、不确定性特异度（USpe）和不确定性精度（UPre）来评估这些模型。结果表明，集成方法在准确性和不确定性处理之间提供了良好的权衡，而EMCD对不确定预测更敏感。这项研究强调了将UQ整合到基于DL的医学诊断中的重要性，以提高实际临床应用中的性能和可信度。", "summary": "本研究全面评估了深度学习模型在自动化皮肤癌分类中的应用，特别关注了迁移学习和不确定性量化（UQ）的整合。研究分为两个阶段：首先，比较了多种预训练特征提取器与传统分类器的组合，发现CLIP视觉转换器表现最优。其次，通过蒙特卡洛Dropout、集成和集成蒙特卡洛Dropout等方法引入UQ，并使用不确定性感知指标进行评估，结果显示集成方法在准确性和不确定性处理之间提供了良好平衡，而EMCD对不确定预测更敏感。研究强调了UQ在提升DL医学诊断性能和可信度方面的重要性。", "keywords": "皮肤癌分类, 深度学习, 不确定性量化, 迁移学习, CLIP", "comments": "这项研究的创新之处在于其对不确定性量化（UQ）在皮肤癌深度学习诊断中的全面评估，这对于提高模型在临床应用中的可靠性和可信度至关重要。通过结合迁移学习和多种UQ方法，并进行广泛的基准测试，研究为选择合适的模型和UQ策略提供了宝贵的见解。特别是，识别出CLIP视觉转换器的高性能以及集成方法在平衡准确性与不确定性处理方面的优势，为未来的研究和实际部署奠定了基础。"}}
{"id": "2506.10146", "title": "Balanced Hyperbolic Embeddings Are Natural Out-of-Distribution Detectors", "authors": ["Tejaswi Kasarla", "Max van Spengler", "Pascal Mettes"], "summary": "Out-of-distribution recognition forms an important and well-studied problem\nin deep learning, with the goal to filter out samples that do not belong to the\ndistribution on which a network has been trained. The conclusion of this paper\nis simple: a good hierarchical hyperbolic embedding is preferred for\ndiscriminating in- and out-of-distribution samples. We introduce Balanced\nHyperbolic Learning. We outline a hyperbolic class embedding algorithm that\njointly optimizes for hierarchical distortion and balancing between shallow and\nwide subhierarchies. We then use the class embeddings as hyperbolic prototypes\nfor classification on in-distribution data. We outline how to generalize\nexisting out-of-distribution scoring functions to operate with hyperbolic\nprototypes. Empirical evaluations across 13 datasets and 13 scoring functions\nshow that our hyperbolic embeddings outperform existing out-of-distribution\napproaches when trained on the same data with the same backbones. We also show\nthat our hyperbolic embeddings outperform other hyperbolic approaches, beat\nstate-of-the-art contrastive methods, and natively enable hierarchical\nout-of-distribution generalization.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10146v1", "AI": {"title_translation": "平衡双曲嵌入是天然的分布外检测器", "tldr": "本研究提出平衡双曲学习，利用优化的双曲嵌入作为原型进行分类，并推广了分布外评分函数。实验表明，该方法在分布外识别任务中优于现有方法和最先进的对比方法，并支持分层分布外泛化。", "motivation": "深度学习中，分布外识别是一个重要且备受关注的问题，其目标是过滤掉不属于模型训练分布的样本。本文旨在寻找一种更优的方法来区分分布内和分布外样本。", "method": "本文引入了“平衡双曲学习”。具体方法包括：1. 提出一种双曲类别嵌入算法，该算法联合优化分层失真以及浅层和宽层子层次结构之间的平衡。2. 使用这些类别嵌入作为双曲原型，对分布内数据进行分类。3. 阐述了如何将现有的分布外评分函数推广，使其能够与双曲原型一起操作。", "result": "在13个数据集和13个评分函数上的实证评估表明：1. 论文提出的双曲嵌入在相同数据和骨干网络训练下，优于现有的分布外方法。2. 论文提出的双曲嵌入优于其他双曲方法。3. 论文提出的双曲嵌入击败了最先进的对比方法。4. 论文提出的双曲嵌入原生支持分层分布外泛化。", "conclusion": "本论文的结论是：良好的分层双曲嵌入更适合区分分布内和分布外样本。", "translation": "分布外识别是深度学习中一个重要且经过充分研究的问题，其目标是过滤掉不属于网络训练分布的样本。本文的结论很简单：良好的分层双曲嵌入更适合区分分布内和分布外样本。我们引入了平衡双曲学习。我们概述了一种双曲类别嵌入算法，该算法联合优化分层失真以及浅层和宽层子层次结构之间的平衡。然后，我们使用这些类别嵌入作为双曲原型，对分布内数据进行分类。我们阐述了如何将现有的分布外评分函数推广，使其能够与双曲原型一起操作。在13个数据集和13个评分函数上的实证评估表明，当在相同数据和相同骨干网络上训练时，我们的双曲嵌入优于现有的分布外方法。我们还表明，我们的双曲嵌入优于其他双曲方法，击败了最先进的对比方法，并原生支持分层分布外泛化。", "summary": "本文提出了一种名为“平衡双曲学习”的新方法，用于分布外（OOD）识别。该方法通过优化双曲类别嵌入，使其在分层失真和子层次结构平衡方面达到最佳，并将这些嵌入作为双曲原型进行分类。研究还展示了如何将现有OOD评分函数推广到双曲域。实验结果表明，与现有OOD方法、其他双曲方法以及最先进的对比方法相比，该方法在多个数据集上表现出卓越的性能，并原生支持分层OOD泛化，验证了双曲嵌入在OOD检测中的天然优势。", "keywords": "双曲嵌入, 分布外检测, 平衡双曲学习, 层次结构, 深度学习", "comments": "这篇论文的创新点在于提出了“平衡双曲学习”，并首次证明了良好的分层双曲嵌入在分布外识别任务中的优越性。它不仅在理论上指出了双曲几何与OOD检测的契合性，还在实践中通过大量实验验证了其有效性，甚至超越了现有的SOTA方法，包括对比学习方法。该工作为OOD检测提供了一个全新的视角和强大的工具，尤其是在处理具有内在层次结构的数据时，其分层泛化能力是一个显著优势。"}}
{"id": "2506.10613", "title": "Data Driven Diagnosis for Large Cyber-Physical-Systems with Minimal Prior Information", "authors": ["Henrik Sebastian Steude", "Alexander Diedrich", "Ingo Pill", "Lukas Moddemann", "Daniel Vranješ", "Oliver Niggemann"], "summary": "Diagnostic processes for complex cyber-physical systems often require\nextensive prior knowledge in the form of detailed system models or\ncomprehensive training data. However, obtaining such information poses a\nsignificant challenge. To address this issue, we present a new diagnostic\napproach that operates with minimal prior knowledge, requiring only a basic\nunderstanding of subsystem relationships and data from nominal operations. Our\nmethod combines a neural network-based symptom generator, which employs\nsubsystem-level anomaly detection, with a new graph diagnosis algorithm that\nleverages minimal causal relationship information between\nsubsystems-information that is typically available in practice. Our experiments\nwith fully controllable simulated datasets show that our method includes the\ntrue causal component in its diagnosis set for 82 p.c. of all cases while\neffectively reducing the search space in 73 p.c. of the scenarios. Additional\ntests on the real-world Secure Water Treatment dataset showcase the approach's\npotential for practical scenarios. Our results thus highlight our approach's\npotential for practical applications with large and complex cyber-physical\nsystems where limited prior knowledge is available.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10613v1", "AI": {"title_translation": "大型信息物理系统在先验信息极少情况下的数据驱动诊断", "tldr": "该论文提出了一种新的数据驱动诊断方法，适用于先验知识有限的大型信息物理系统，结合了神经网络症状生成器和图诊断算法，并在模拟和实际数据上表现出有效性。", "motivation": "复杂的网络物理系统诊断通常需要大量的先验知识，如详细的系统模型或全面的训练数据，但获取这些信息非常困难。为了解决这个问题，本文提出了一种新的诊断方法。", "method": "本文提出了一种新的诊断方法，只需最少的先验知识，仅需对子系统关系的基本理解和正常运行数据。该方法结合了基于神经网络的症状生成器（利用子系统级异常检测）和一种新的图诊断算法（利用子系统间最少的因果关系信息）。", "result": "在完全可控的模拟数据集上，该方法在82%的情况下诊断集中包含真实的因果组件，并在73%的场景中有效减少了搜索空间。在真实的“安全水处理”数据集上的额外测试也展示了该方法在实际场景中的潜力。", "conclusion": "本文结果表明，所提出的方法在先验知识有限的大型复杂信息物理系统中的实际应用具有巨大潜力。", "translation": "复杂信息物理系统的诊断过程通常需要大量的先验知识，如详细的系统模型或全面的训练数据。然而，获取这些信息带来了重大挑战。为了解决这个问题，我们提出了一种新的诊断方法，该方法在最少的先验知识下运行，仅需要对子系统关系的基本理解和正常运行数据。我们的方法结合了基于神经网络的症状生成器（该生成器采用子系统级异常检测）和一种新的图诊断算法，该算法利用子系统之间最少的因果关系信息——这些信息在实践中通常是可用的。我们使用完全可控的模拟数据集进行的实验表明，我们的方法在82%的情况下，其诊断集包含真实的因果组件，同时在73%的场景中有效减少了搜索空间。在真实的“安全水处理”数据集上的额外测试展示了该方法在实际场景中的潜力。因此，我们的结果突显了我们方法在先验知识有限的大型复杂信息物理系统中的实际应用潜力。", "summary": "本文提出了一种针对大型复杂信息物理系统的数据驱动诊断新方法，旨在解决传统诊断方法对大量先验知识（如系统模型或训练数据）的依赖。该方法仅需对子系统关系的基本理解和正常运行数据，通过结合神经网络症状生成器和图诊断算法来实现。实验结果表明，该方法在包含真实因果组件和减少搜索空间方面表现出色，并在实际数据集中验证了其应用潜力。", "keywords": "数据驱动诊断, 信息物理系统, 先验知识, 神经网络, 图诊断", "comments": "该论文的创新点在于其能够在极少先验知识的情况下对大型复杂信息物理系统进行有效诊断，这对于实际应用中数据获取困难的场景具有重要意义。结合神经网络进行症状生成和图算法进行诊断，提供了一个实用的框架。"}}
{"id": "2506.10713", "title": "Deep Learning-based Multi Project InP Wafer Simulation for Unsupervised Surface Defect Detection", "authors": ["Emílio Dolgener Cantú", "Rolf Klemens Wittmann", "Oliver Abdeen", "Patrick Wagner", "Wojciech Samek", "Moritz Baier", "Sebastian Lapuschkin"], "summary": "Quality management in semiconductor manufacturing often relies on template\nmatching with known golden standards. For Indium-Phosphide (InP) multi-project\nwafer manufacturing, low production scale and high design variability lead to\nsuch golden standards being typically unavailable. Defect detection, in turn,\nis manual and labor-intensive. This work addresses this challenge by proposing\na methodology to generate a synthetic golden standard using Deep Neural\nNetworks, trained to simulate photo-realistic InP wafer images from CAD data.\nWe evaluate various training objectives and assess the quality of the simulated\nimages on both synthetic data and InP wafer photographs. Our\ndeep-learning-based method outperforms a baseline decision-tree-based approach,\nenabling the use of a 'simulated golden die' from CAD plans in any user-defined\nregion of a wafer for more efficient defect detection. We apply our method to a\ntemplate matching procedure, to demonstrate its practical utility in surface\ndefect detection.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10713v1", "AI": {"title_translation": "基于深度学习的多项目InP晶圆模拟用于无监督表面缺陷检测", "tldr": "本文提出了一种使用深度学习从CAD数据生成合成InP晶圆图像的方法，以创建“模拟黄金标准”，从而在缺乏真实黄金标准的情况下实现高效的无监督表面缺陷检测。", "motivation": "在磷化铟(InP)多项目晶圆制造中，由于生产规模小和设计变异性大，通常无法获得用于质量管理的“黄金标准”模板。这导致缺陷检测是手动且劳动密集型的。", "method": "该工作提出了一种利用深度神经网络生成合成黄金标准的方法。网络经过训练，可以从CAD数据模拟出逼真的InP晶圆图像。研究评估了各种训练目标，并对模拟图像的质量进行了评估，包括在合成数据和InP晶圆照片上的表现。该方法被应用于模板匹配程序。", "result": "所提出的基于深度学习的方法优于基线决策树方法，能够从CAD计划中生成“模拟黄金芯片”，用于晶圆上任何用户定义区域的更高效缺陷检测。该方法在表面缺陷检测的模板匹配过程中展示了其实用性。", "conclusion": "通过深度学习生成合成黄金标准，可以在没有真实黄金标准的情况下，实现磷化铟(InP)多项目晶圆制造中高效且自动化的表面缺陷检测。", "translation": "半导体制造中的质量管理通常依赖于与已知黄金标准的模板匹配。对于磷化铟(InP)多项目晶圆制造，低生产规模和高设计变异性导致此类黄金标准通常不可用。反过来，缺陷检测是手动且劳动密集型的。这项工作通过提出一种使用深度神经网络生成合成黄金标准的方法来解决这一挑战，该网络经过训练可以从CAD数据模拟出逼真的InP晶圆图像。我们评估了各种训练目标，并评估了模拟图像在合成数据和InP晶圆照片上的质量。我们基于深度学习的方法优于基于决策树的基线方法，使得可以使用CAD计划中的“模拟黄金芯片”在晶圆的任何用户定义区域进行更高效的缺陷检测。我们将我们的方法应用于模板匹配过程，以证明其在表面缺陷检测中的实际效用。", "summary": "本研究针对磷化铟(InP)多项目晶圆制造中缺乏黄金标准导致缺陷检测效率低下的问题，提出了一种基于深度学习的方法。该方法通过训练深度神经网络，能够从CAD数据生成逼真的InP晶圆图像，从而创建“模拟黄金标准”。实验证明，该深度学习方法在模拟图像质量和缺陷检测效率上优于传统方法，并成功应用于模板匹配以实现无监督表面缺陷检测。", "keywords": "深度学习, InP晶圆, 缺陷检测, 模拟黄金标准, 半导体制造", "comments": "该论文的创新点在于提出了通过深度学习生成合成黄金标准来解决半导体制造中缺乏真实黄金标准的问题，尤其是在低产量、高变异性的InP多项目晶圆生产中。这种方法将原本依赖人工和模板匹配的缺陷检测自动化，显著提高了效率。其重要性在于为未来智能制造和质量控制提供了新的范式，特别是在难以获得大量真实样本的领域。通过利用CAD数据进行模拟，有效地弥补了数据稀缺的挑战。"}}
{"id": "2506.10292", "title": "Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages", "authors": ["Ali Almutairi", "Abdullah Alsuhaibani", "Shoaib Jameel", "Usman Naseem", "Gelareh Mohammadi", "Imran Razzak"], "summary": "Training deep learning networks with minimal supervision has gained\nsignificant research attention due to its potential to reduce reliance on\nextensive labelled data. While self-training methods have proven effective in\nsemi-supervised learning, they remain vulnerable to errors from noisy pseudo\nlabels. Moreover, most recent approaches to the few-label classification\nproblem are either designed for resource-rich languages such as English or\ninvolve complex cascading models that are prone to overfitting. To address the\npersistent challenge of few-label text classification in truly low-resource\nlinguistic contexts, where existing methods often struggle with noisy\npseudo-labels and domain adaptation, we propose Flick. Unlike prior methods\nthat rely on generic multi-cluster pseudo-labelling or complex cascading\narchitectures, Flick leverages the fundamental insight that distilling\nhigh-confidence pseudo-labels from a broader set of initial clusters can\ndramatically improve pseudo-label quality, particularly for linguistically\ndiverse, low-resource settings. Flick introduces a novel pseudo-label\nrefinement component, a departure from traditional pseudo-labelling strategies\nby identifying and leveraging top-performing pseudo-label clusters. This\ncomponent specifically learns to distil highly reliable pseudo-labels from an\ninitial broad set by focusing on single-cluster cohesion and leveraging an\nadaptive top-k selection mechanism. This targeted refinement process is crucial\nfor mitigating the propagation of errors inherent in low-resource data,\nallowing for robust fine-tuning of pre-trained language models with only a\nhandful of true labels. We demonstrate Flick's efficacy across 14 diverse\ndatasets, encompassing challenging low-resource languages such as Arabic, Urdu,\nand Setswana, alongside English, showcasing its superior performance and\nadaptability.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10292v1", "AI": {"title_translation": "Flick：在多任务低资源语言中使用K感知中间学习的少量标签文本分类", "tldr": "Flick是一种用于低资源语言少量标签文本分类的新方法，通过K感知中间学习和伪标签细化来解决现有方法中噪声伪标签和过拟合的问题，并在14个数据集中表现出优越的性能。", "motivation": "现有的深度学习方法在少量标签文本分类中，特别是在低资源语言环境下，面临着伪标签噪声、模型复杂且易过拟合以及对资源丰富语言的依赖等挑战。", "method": "Flick提出了一种新颖的伪标签细化组件，通过识别和利用表现最佳的伪标签簇，并采用自适应的top-k选择机制，从初始的广泛簇中提取高置信度的伪标签。这种方法旨在减轻低资源数据中固有的错误传播，实现对预训练语言模型的鲁棒微调。", "result": "Flick在包括阿拉伯语、乌尔都语、茨瓦纳语和英语在内的14个不同数据集中展示了其有效性，表现出卓越的性能和适应性。", "conclusion": "Flick通过其创新的伪标签细化策略，成功解决了低资源语言环境下少量标签文本分类的挑战，显著提高了伪标签的质量，并展示了其在多语言环境中的优越性和鲁棒性。", "translation": "训练深度学习网络时，最小化监督已获得显著研究关注，因为它有潜力减少对大量标记数据的依赖。虽然自训练方法在半监督学习中被证明是有效的，但它们仍然容易受到噪声伪标签的错误影响。此外，大多数处理少量标签分类问题的最新方法要么是为英语等资源丰富的语言设计的，要么涉及复杂的级联模型，这些模型容易过拟合。为了解决在真正低资源语言环境中少量标签文本分类的持续挑战（现有方法经常在噪声伪标签和领域适应方面遇到困难），我们提出了Flick。与依赖通用多簇伪标签或复杂级联架构的先前方法不同，Flick利用了基本见解：从更广泛的初始簇中提取高置信度伪标签可以显著提高伪标签质量，特别是在语言多样性、低资源的设置中。Flick引入了一种新颖的伪标签细化组件，通过识别和利用表现最佳的伪标签簇，这与传统的伪标签策略有所不同。该组件通过关注单簇内聚性和利用自适应的top-k选择机制，专门学习从初始广泛集合中提取高度可靠的伪标签。这种有针对性的细化过程对于减轻低资源数据中固有的错误传播至关重要，从而允许仅用少量真实标签对预训练语言模型进行鲁棒微调。我们展示了Flick在14个不同数据集上的有效性，包括阿拉伯语、乌尔都语和茨瓦纳语等具有挑战性的低资源语言，以及英语，展示了其卓越的性能和适应性。", "summary": "Flick是一种针对低资源语言少量标签文本分类的新方法，旨在解决现有自训练方法中伪标签噪声和模型过拟合的问题。该方法通过引入一个新颖的伪标签细化组件，利用K感知中间学习和自适应的top-k选择机制，从初始簇中蒸馏出高置信度的伪标签，从而提高伪标签质量并减轻错误传播。实验证明，Flick在14个多语言数据集（包括多种低资源语言）上表现出优越的性能和适应性，能够用少量真实标签对预训练语言模型进行鲁棒微调。", "keywords": "少量标签文本分类, 低资源语言, 伪标签细化, K感知学习, 多任务学习", "comments": "Flick的创新点在于其独特的伪标签细化组件，通过关注单簇内聚性和自适应top-k选择，有效地从噪声数据中提取高置信度伪标签，这对于低资源语言场景尤为重要。其方法避免了传统级联模型的复杂性，并有效解决了伪标签噪声这一半监督学习中的核心挑战，为低资源自然语言处理提供了一个有前景的方向。"}}
{"id": "2506.10960", "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark", "authors": ["Kangwei Liu", "Siyuan Cheng", "Bozhong Tian", "Xiaozhuan Liang", "Yuyang Yin", "Meng Han", "Ningyu Zhang", "Bryan Hooi", "Xi Chen", "Shumin Deng"], "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.", "comment": "Work in progress", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10960v1", "AI": {"title_translation": "ChineseHarm-Bench：一个中文有害内容检测基准", "tldr": "本文提出了ChineseHarm-Bench，一个用于中文有害内容检测的综合基准，并提出了一个知识增强基线。", "motivation": "现有有害内容检测资源主要集中在英文，中文数据集稀缺且范围有限，限制了大型语言模型（LLMs）在中文有害内容检测任务中的应用和效率。", "method": "本文构建了一个全面的、专业标注的中文有害内容检测基准ChineseHarm-Bench，涵盖六个代表性类别，数据全部来源于真实世界。标注过程还产生了知识规则库，为LLMs提供明确的专家知识。此外，提出了一种知识增强基线，结合了人工标注的知识规则和LLMs的隐式知识。", "result": "知识增强基线使较小模型能够达到与最先进LLMs相当的性能。", "conclusion": "ChineseHarm-Bench及其知识增强基线为中文有害内容检测提供了重要的资源和方法，有效提升了检测效率和准确性，并缩小了小型模型与大型模型之间的性能差距。", "translation": "大型语言模型（LLMs）越来越多地应用于自动化有害内容检测任务，协助内容审核员识别违规行为，提高内容审查的整体效率和准确性。然而，现有有害内容检测资源主要集中在英文，中文数据集稀缺且范围有限。我们提出了一个全面的、专业标注的中文内容危害检测基准，该基准涵盖六个代表性类别，并且完全基于真实世界数据构建。我们的标注过程进一步产生了一个知识规则库，提供明确的专家知识以协助LLMs进行中文有害内容检测。此外，我们提出了一种知识增强基线，该基线整合了人工标注的知识规则和大型语言模型的隐式知识，使较小模型能够达到与最先进LLMs相当的性能。代码和数据可在https://github.com/zjunlp/ChineseHarm-bench获取。", "summary": "本文针对中文有害内容检测数据集稀缺的问题，推出了ChineseHarm-Bench，一个包含六个类别的真实世界中文有害内容检测基准。通过标注过程，还生成了一个知识规则库。此外，提出了一种知识增强基线方法，该方法结合了专家知识和LLMs的隐式知识，使得小型模型也能达到与SOTA LLMs相当的性能，显著提升了中文有害内容检测的效率和准确性。", "keywords": "中文有害内容检测, 基准数据集, 知识增强, 大型语言模型, 内容审核", "comments": "本文填补了中文有害内容检测领域高质量基准数据集的空白，具有重要的实践意义。知识规则库的构建和知识增强基线的提出，为LLMs在中文环境下的应用提供了新的思路和有效方法，特别是使得资源受限的小型模型也能获得高性能，降低了部署成本，具有较强的创新性。"}}
{"id": "2506.10321", "title": "Fast Ramanujan--type Series for Logarithms. Part II", "authors": ["Jorge Zuniga"], "summary": "This work extends the results of the preprint Ramanujan type Series for\nLogarithms, Part I, arXiv:2506.08245, which introduced single hypergeometric\ntype identities for the efficient computing of $\\log(p)$, where\n$p\\in\\mathbb{Z}_{>1}$. We present novel formulas for arctangents and methods\nfor a very fast multiseries evaluation of logarithms. Building upon a\n$\\mathcal{O}((p-1)^{6})$ Ramanujan type series asymptotic approximation for\n$\\log(p)$ as $p\\rightarrow1$, formulas for computing $n$ simultaneous\nlogarithms are developed. These formulas are derived by solving an integer\nprogramming problem to identify optimal variable values within a finite lattice\n$\\mathbb{Z}^{n}$. This approach yields linear combinations of series that\nprovide: (i) highly efficient formulas for single logarithms of natural numbers\nand (ii) the fastest known hypergeometric formulas for multivalued logarithms\nof $n$ selected integers in $\\mathbb{Z}_{>1}$. An application of these results\nwas to extend the number of decimal places known for log(10) up to\n2.0$\\cdot$10$^{12}$ digits (June 06 2025).", "comment": "17 pages, 1 table. TeX file must be downloaded, PARI GP program is\n  embedded as a large comment there", "cate": "math.NT", "url": "http://arxiv.org/abs/2506.10321v1", "AI": {"title_translation": "快速拉马努金型对数级数。第二部分", "tldr": "本文在Part I的基础上，介绍了计算反正切的新公式和对数的多级数快速评估方法，通过解决整数规划问题，得到了计算单个和多个对数的超高效超几何公式，并将log(10)的已知小数位数扩展到2.0e12位。", "motivation": "这项工作旨在扩展前作《拉马努金型对数级数，第一部分》的成果，该前作引入了用于高效计算$\\\\log(p)$的单超几何型恒等式。本文的动机是开发更快速的对数多级数评估方法和新型反正切公式。", "method": "本文通过以下方法实现：1. 引入反正切新公式和对数的多级数评估方法。2. 基于$\\\\mathcal{O}((p-1)^{6})$的拉马努金型级数渐近逼近，当$p\\\\rightarrow1$时，开发了计算$n$个同时对数的公式。3. 通过解决整数规划问题，在有限格$\\\\mathbb{Z}^{n}$中识别最优变量值，从而导出这些公式。", "result": "本文的结果包括：1. 提供了用于自然数单个对数的高效公式。2. 提供了已知最快的$n$个选定整数多值对数的超几何公式。3. 将log(10)的已知小数位数扩展到2.0$\\\\cdot$10$^{12}$位（截至2025年6月6日）。", "conclusion": "本文开发的新公式和方法，特别是通过整数规划推导出的线性组合级数，为单个和多个对数的计算提供了极高的效率和当前最快的超几何公式。", "translation": "这项工作扩展了预印本《拉马努金型对数级数，第一部分》（arXiv:2506.08245）的结果，该预印本引入了用于高效计算$\\\\log(p)$（其中$p\\\\in\\\\mathbb{Z}_{>1}$）的单超几何型恒等式。我们提出了反正切的新公式以及对数的多级数快速评估方法。在拉马努金型级数渐近逼近$\\\\log(p)$为$\\\\mathcal{O}((p-1)^{6})$（当$p\\\\rightarrow1$时）的基础上，开发了计算$n$个同时对数的公式。这些公式是通过解决整数规划问题，以识别有限格$\\\\mathbb{Z}^{n}$中的最优变量值而得出的。这种方法产生了级数的线性组合，提供了：(i) 自然数单个对数的高效公式和 (ii) 已知最快的$n$个选定整数在$\\\\mathbb{Z}_{>1}$中多值对数的超几何公式。这些结果的一个应用是将log(10)的已知小数位数扩展到2.0$\\\\cdot$10$^{12}$位（2025年6月6日）。", "summary": "本文作为《拉马努金型对数级数，第一部分》的延续，提出了一系列用于快速计算对数的新方法。通过引入反正切公式和基于整数规划的多级数评估策略，文章开发了高效的单对数计算公式以及已知最快的多值对数超几何公式。这些成果的应用包括将log(10)的已知小数位数扩展至2.0万亿位。", "keywords": "拉马努金型级数, 对数计算, 超几何公式, 整数规划, 数值计算", "comments": "本文通过引入整数规划来优化级数组合，从而实现了对数计算效率的显著提升，这在数值计算领域是一个重要的创新。其将log(10)的已知小数位数扩展到万亿位，也展示了该方法的强大实用性。"}}
{"id": "2506.10866", "title": "Data-Driven Model Reduction by Moment Matching for Linear and Nonlinear Parametric Systems", "authors": ["Hanqing Zhang", "Junyu Mao", "Mohammad Fahim Shakib", "Giordano Scarciotti"], "summary": "Theory and methods to obtain parametric reduced-order models by moment\nmatching are presented. The definition of the parametric moment is introduced,\nand methods (model-based and data-driven) for the approximation of the\nparametric moment of linear and nonlinear parametric systems are proposed.\nThese approximations are exploited to construct families of parametric\nreduced-order models that match the approximate parametric moment of the system\nto be reduced and preserve key system properties such as asymptotic stability\nand dissipativity. The use of the model reduction methods is illustrated by\nmeans of a parametric benchmark model for the linear case and a large-scale\nwind farm model for the nonlinear case. In the illustration, a comparison of\nthe proposed approximation methods is drawn and their advantages/disadvantages\nare discussed.", "comment": "16 pages, 6 figures, submitted to IEEE Transactions on Automatic\n  Control", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.10866v1", "AI": {"title_translation": "数据驱动的线性与非线性参数系统矩匹配模型降阶", "tldr": "本文提出了通过矩匹配方法获得参数化降阶模型的理论和方法，并讨论了其在保持系统属性方面的应用及优缺点。", "motivation": "旨在开发新的理论和方法，通过矩匹配技术为线性及非线性参数系统生成参数化降阶模型。", "method": "引入了参数矩的定义，并提出了基于模型和数据驱动的参数矩近似方法。这些近似方法用于构建能够匹配近似参数矩并保持渐近稳定性、耗散性等关键系统属性的参数化降阶模型族。", "result": "提出的模型降阶方法通过线性参数基准模型和非线性大规模风力发电场模型进行了验证。结果显示，对所提出的近似方法进行了比较，并讨论了它们的优缺点。", "conclusion": "本文成功提出了通过矩匹配实现参数系统降阶的理论和方法，并展示了其在保持关键系统属性方面的有效性，同时对不同近似方法进行了比较和讨论。", "translation": "本文介绍了通过矩匹配获得参数化降阶模型的理论和方法。文中引入了参数矩的定义，并提出了（基于模型和数据驱动的）线性与非线性参数系统参数矩近似方法。这些近似方法被用于构建参数化降阶模型族，这些模型族能够匹配待降阶系统的近似参数矩，并保留渐近稳定性、耗散性等关键系统属性。通过线性情况下的参数基准模型和非线性情况下的一个大型风力发电场模型，说明了模型降阶方法的使用。在说明中，对所提出的近似方法进行了比较，并讨论了它们的优缺点。", "summary": "本文提出了通过矩匹配方法构建线性与非线性参数系统参数化降阶模型的理论与方法。研究引入了参数矩定义，并开发了模型驱动和数据驱动的参数矩近似技术。这些技术用于生成能匹配近似参数矩并保持系统稳定性和耗散性的降阶模型。通过线性基准模型和大型风力发电场模型的案例研究，对所提方法的有效性进行了验证，并分析了不同近似方法的优缺点。", "keywords": "矩匹配, 模型降阶, 参数系统, 数据驱动, 稳定性", "comments": "这篇论文的创新点在于提出了数据驱动和模型驱动相结合的参数矩近似方法，并将其应用于参数化系统的降阶建模，同时强调了对系统关键属性（如稳定性）的保持。这对于处理复杂的大规模参数系统具有重要意义。"}}
{"id": "2506.10424", "title": "SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks", "authors": ["Kaiyuan Zhang", "Siyuan Cheng", "Hanxi Guo", "Yuetian Chen", "Zian Su", "Shengwei An", "Yuntao Du", "Charles Fleming", "Ashish Kundu", "Xiangyu Zhang", "Ninghui Li"], "summary": "Large language models (LLMs) have achieved remarkable success and are widely\nadopted for diverse applications. However, fine-tuning these models often\ninvolves private or sensitive information, raising critical privacy concerns.\nIn this work, we conduct the first comprehensive study evaluating the\nvulnerability of fine-tuned LLMs to membership inference attacks (MIAs). Our\nempirical analysis demonstrates that MIAs exploit the loss reduction during\nfine-tuning, making them highly effective in revealing membership information.\nThese findings motivate the development of our defense. We propose SOFT\n(\\textbf{S}elective data \\textbf{O}bfuscation in LLM\n\\textbf{F}ine-\\textbf{T}uning), a novel defense technique that mitigates\nprivacy leakage by leveraging influential data selection with an adjustable\nparameter to balance utility preservation and privacy protection. Our extensive\nexperiments span six diverse domains and multiple LLM architectures and scales.\nResults show that SOFT effectively reduces privacy risks while maintaining\ncompetitive model performance, offering a practical and scalable solution to\nsafeguard sensitive information in fine-tuned LLMs.", "comment": "Accepted by the 34th USENIX Security Symposium 2025. Code is\n  available at https://github.com/KaiyuanZh/SOFT", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10424v1", "AI": {"title_translation": "SOFT：针对成员推断攻击保护LLM微调的选择性数据混淆", "tldr": "研究发现微调LLMs易受成员推断攻击，提出SOFT，一种通过选择性数据混淆来平衡隐私和性能的防御方法。", "motivation": "大型语言模型（LLMs）的微调通常涉及私人或敏感信息，引发了关键的隐私担忧。本研究首次全面评估了微调LLMs对成员推断攻击（MIAs）的脆弱性，发现MIAs利用微调过程中的损失降低来有效揭示成员信息，这些发现促使了防御机制的开发。", "method": "提出了SOFT（LLM微调中的选择性数据混淆），这是一种新颖的防御技术。它通过利用具有可调节参数的影响力数据选择来减轻隐私泄露，以平衡实用性保留和隐私保护。", "result": "广泛的实验表明，SOFT在有效降低隐私风险的同时，保持了具有竞争力的模型性能。", "conclusion": "SOFT提供了一种实用且可扩展的解决方案，用于保护微调LLMs中的敏感信息，有效平衡隐私保护和模型性能。", "translation": "大型语言模型（LLMs）取得了显著成功，并被广泛应用于各种场景。然而，对这些模型进行微调通常涉及私人或敏感信息，引发了关键的隐私问题。在这项工作中，我们首次进行了全面研究，评估了微调LLMs对成员推断攻击（MIAs）的脆弱性。我们的实证分析表明，MIAs利用微调过程中的损失降低，使其在揭示成员信息方面非常有效。这些发现促使我们开发了防御措施。我们提出了SOFT（LLM微调中的选择性数据混淆），这是一种新颖的防御技术，通过利用具有可调节参数的影响力数据选择来减轻隐私泄露，以平衡实用性保留和隐私保护。我们广泛的实验涵盖了六个不同领域和多种LLM架构及规模。结果表明，SOFT在有效降低隐私风险的同时，保持了具有竞争力的模型性能，为保护微调LLMs中的敏感信息提供了一种实用且可扩展的解决方案。", "summary": "本研究首次全面评估了微调大型语言模型（LLMs）对成员推断攻击（MIAs）的脆弱性，发现MIAs通过利用微调过程中的损失降低能有效泄露成员信息。为应对此问题，论文提出了SOFT（LLM微调中的选择性数据混淆），一种新颖的防御技术。SOFT通过选择性地混淆数据并引入可调节参数，旨在平衡隐私保护和模型实用性。实验结果表明，SOFT能有效降低隐私风险，同时保持良好的模型性能，提供了一种实用且可扩展的敏感信息保护方案。", "keywords": "LLM, 成员推断攻击, 隐私保护, 数据混淆, 微调", "comments": "该论文通过首次全面评估微调LLMs对MIAs的脆弱性，揭示了LLM隐私保护的关键挑战。提出的SOFT方法通过选择性数据混淆，提供了一种新颖且实用的防御策略，其可调节参数设计对于平衡隐私与效用具有重要意义。该方法在多个领域和LLM架构上的广泛验证，增强了其普适性和实用价值。"}}
{"id": "2506.10770", "title": "From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models", "authors": ["Joran Leest", "Claudia Raibulet", "Patricia Lago", "Ilias Gerostathopoulos"], "summary": "Machine learning (ML) models in production do not fail due to statistical\nanomalies in their input data; they fail due to contextual misalignment -- when\ntheir environment deviates from training assumptions, leading to unreliable\npredictions. Effective ML monitoring requires rich contextual information to\nmove beyond detecting statistical shifts toward meaningful alerts and\nsystematic root-cause analysis. Yet, surprisingly, despite extensive research\nin ML monitoring and related disciplines (drift detection, data validation,\nout-of-distribution detection), there is no shared understanding of how to use\ncontextual information -- striking, given that monitoring involves\ninterpretation of information in context. In response, this paper presents a\nsystematic review to characterize and structure the various types of contextual\ninformation in this domain. Our analysis examines 94 primary studies across\ndata mining, databases, software engineering, and ML. We introduce the\nContextual System--Aspect--Representation (C-SAR) framework, a conceptual model\nthat synthesizes our findings. We also identify 20 recurring and potentially\nreusable patterns of specific system, aspect, and representation combinations,\nand map them to the monitoring activities they support. This study provides a\nnew perspective on ML monitoring: from interpreting \"tea leaves\" of\nobservational statistics into constructing and managing \"system maps\" that\nenable systematic and reliable ML monitoring practices.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10770v1", "AI": {"title_translation": "从茶叶到系统地图：操作机器学习模型监控中的上下文感知", "tldr": "ML模型故障源于上下文错位，现有监控缺乏对上下文信息的理解。本文通过系统综述，提出了C-SAR框架和20种模式，旨在将ML监控从统计异常检测提升到系统性根因分析，从而实现系统且可靠的ML监控实践。", "motivation": "生产中的机器学习模型失败并非源于输入数据的统计异常，而是由于上下文错位，即其环境偏离训练假设，导致不可靠的预测。尽管机器学习监控领域已有广泛研究，但对于如何有效利用上下文信息仍缺乏共识，这阻碍了从统计偏移检测转向有意义的警报和系统性根因分析。", "method": "本文进行了一项系统性综述，分析了数据挖掘、数据库、软件工程和机器学习领域94项主要研究，旨在表征和结构化该领域中各种类型的上下文信息。研究引入了上下文系统-方面-表示（C-SAR）框架这一概念模型，并识别了20种重复出现的、潜在可重用的特定系统、方面和表示组合模式，并将其映射到它们支持的监控活动。", "result": "提出了上下文系统-方面-表示（C-SAR）框架，这是一个综合研究发现的概念模型。识别了20种重复出现的、潜在可重用的特定系统、方面和表示组合模式，并将其映射到它们支持的监控活动。", "conclusion": "这项研究为机器学习监控提供了一个新视角：从解释观测统计数据的“茶叶”转向构建和管理“系统地图”，从而实现系统且可靠的机器学习监控实践。", "translation": "生产环境中的机器学习（ML）模型并非因输入数据中的统计异常而失败；它们因上下文错位而失败——当其环境偏离训练假设时，导致不可靠的预测。有效的ML监控需要丰富的上下文信息，才能超越检测统计偏移，转向有意义的警报和系统性根因分析。然而，令人惊讶的是，尽管ML监控及相关学科（漂移检测、数据验证、分布外检测）进行了大量研究，但对于如何使用上下文信息却缺乏共识——考虑到监控涉及对信息进行上下文解释，这一点尤其引人注目。作为回应，本文进行了一项系统性综述，以表征和结构化该领域中各种类型的上下文信息。我们的分析考察了数据挖掘、数据库、软件工程和ML领域的94项主要研究。我们引入了上下文系统-方面-表示（C-SAR）框架，这是一个综合我们研究发现的概念模型。我们还识别了20种重复出现的、潜在可重用的特定系统、方面和表示组合模式，并将其映射到它们支持的监控活动。这项研究为ML监控提供了一个新视角：从解释观测统计数据的“茶叶”转向构建和管理“系统地图”，从而实现系统且可靠的ML监控实践。", "summary": "本文通过系统综述，探讨了在生产环境中监控机器学习模型时上下文信息的重要性。研究指出，模型失败主要源于上下文错位而非统计异常。为解决现有监控方法缺乏对上下文信息有效利用的共识，作者分析了94项研究，提出了上下文系统-方面-表示（C-SAR）框架，并识别了20种可重用的上下文模式，旨在将ML监控从简单的统计检测提升到系统性的根因分析，实现更可靠的监控实践。", "keywords": "机器学习监控, 上下文感知, 系统综述, C-SAR框架, 模型可靠性", "comments": "本文通过对现有研究的系统性综述，填补了机器学习模型监控中上下文信息利用的空白。提出的C-SAR框架和20种上下文模式为理解和应用上下文信息提供了结构化工具，有助于将ML监控从被动的统计异常检测提升到主动的系统性根因分析，具有重要的理论和实践意义。"}}
{"id": "2506.10826", "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System", "authors": ["Wenxuan Song", "Jiayi Chen", "Wenxue Li", "Xu He", "Han Zhao", "Pengxiang Ding Shiyan Su", "Feilong Tang", "Xuelian Cheng", "Donglin Wang", "Zongyuan Ge", "Xinhu Zheng", "Zhe Liu", "Hesheng Wang", "Yunhui Liu", "Haoang Li"], "summary": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/rationalvla.", "comment": "14 pages", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10826v1", "AI": {"title_translation": "RationalVLA：一个具有双系统的理性视觉-语言-动作模型", "tldr": "本文提出了RationalVLA，一个双系统视觉-语言-动作模型，用于机器人处理模糊、不相关或不可行的指令。它引入了RAMA基准测试和数据集，并展示了RationalVLA在处理缺陷指令方面的优越性能和在实际应用中的鲁棒性。", "motivation": "现有语言条件下的操作任务通常假设指令与环境完美对齐，这限制了模型在指令可能模糊、不相关或不可行的现实场景中的鲁棒性和泛化能力。", "method": "研究提出了RAtional MAnipulation (RAMA) 新基准，包含14,000多个样本，涵盖视觉、物理、语义、运动、安全和上下文无关等六个维度的缺陷指令。同时，提出了Rational Vision-Language-Action模型 (RationalVLA)，这是一个用于机械臂的双系统，通过引入可学习的潜在空间嵌入，将高级视觉-语言模型与低级操作策略相结合，使其能够推理指令、拒绝不可行的命令并有效执行操作。", "result": "实验表明，RationalVLA在RAMA基准测试上的成功率比现有最佳基线高出14.5%，平均任务长度为0.94，同时在标准操作任务上保持了有竞争力的性能。实际世界试验进一步验证了其在实际应用中的有效性和鲁棒性。", "conclusion": "RationalVLA模型及其引入的双系统设计，通过对指令进行推理并拒绝不可行的命令，显著提高了机器人在处理模糊、不相关或不可行指令时的鲁棒性和有效性，使其更适合现实世界的部署。", "translation": "机器人实际部署的一个基本要求是理解和响应自然语言指令的能力。现有的语言条件操作任务通常假设指令与环境完美对齐。这一假设限制了在指令可能模糊、不相关或不可行的现实场景中的鲁棒性和泛化能力。为了解决这个问题，我们引入了RAtional MAnipulation (RAMA)，这是一个新的基准，它挑战模型处理未见的、可执行的指令以及应该被拒绝的缺陷指令。在RAMA中，我们构建了一个包含14,000多个样本的数据集，其中包括涵盖视觉、物理、语义、运动、安全和上下文无关等六个维度的各种缺陷指令。我们进一步提出了理性视觉-语言-动作模型（RationalVLA）。它是一个用于机械臂的双系统，通过引入可学习的潜在空间嵌入，将高级视觉-语言模型与低级操作策略相结合。这种设计使RationalVLA能够对指令进行推理，拒绝不可行的命令，并有效执行操作。实验表明，RationalVLA在RAMA上的成功率比现有最佳基线高出14.5%，平均任务长度为0.94，同时在标准操作任务上保持了有竞争力的性能。实际世界试验进一步验证了其在实际应用中的有效性和鲁棒性。我们的项目页面是https://irpn-eai.github.io/rationalvla。", "summary": "本文针对现有机器人语言条件操作模型在处理模糊、不相关或不可行指令时鲁棒性和泛化能力不足的问题，提出了RAtional MAnipulation (RAMA) 基准和RationalVLA模型。RAMA包含一个超过14,000个样本的缺陷指令数据集。RationalVLA是一个双系统视觉-语言-动作模型，通过结合高级视觉-语言模型和低级操作策略，使其能够推理指令并拒绝不可行命令。实验证明，RationalVLA在RAMA上显著优于现有基线，并在实际应用中表现出有效性和鲁棒性。", "keywords": "RationalVLA, 视觉-语言-动作, 机器人操作, 缺陷指令, 双系统", "comments": "本文的主要创新点在于提出了RationalVLA双系统模型和RAMA基准测试，以解决机器人处理不完美自然语言指令的挑战。RAMA数据集的构建考虑了多维度缺陷指令，提升了模型的现实适用性。RationalVLA通过引入可学习的潜在空间嵌入，实现了高级语义理解与低级操作策略的有效结合，并具备拒绝不可行指令的能力，这对于实际机器人部署至关重要。该工作在提升机器人鲁棒性和泛化能力方面具有重要意义。"}}
{"id": "2506.10328", "title": "Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework", "authors": ["Sadia Kamal", "Tim Oates", "Joy Wan"], "summary": "Skin carcinoma is the most prevalent form of cancer globally, accounting for\nover $8 billion in annual healthcare expenditures. In clinical settings,\nphysicians document patient visits using detailed SOAP (Subjective, Objective,\nAssessment, and Plan) notes. However, manually generating these notes is\nlabor-intensive and contributes to clinician burnout. In this work, we propose\na weakly supervised multimodal framework to generate clinically structured SOAP\nnotes from limited inputs, including lesion images and sparse clinical text.\nOur approach reduces reliance on manual annotations, enabling scalable,\nclinically grounded documentation while alleviating clinician burden and\nreducing the need for large annotated data. Our method achieves performance\ncomparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical\nrelevance metrics. To evaluate clinical quality, we introduce two novel metrics\nMedConceptEval and Clinical Coherence Score (CCS) which assess semantic\nalignment with expert medical concepts and input features, respectively.", "comment": "Accepted at IEEE/CVF Computer Society Conference on Computer Vision\n  and Pattern Recognition Workshops (CVPRW)", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10328v1", "AI": {"title_translation": "迈向可扩展的SOAP笔记生成：一个弱监督多模态框架", "tldr": "提出一个弱监督多模态框架，用于从图像和文本生成可扩展的SOAP医疗笔记，性能媲美大型语言模型并减轻医生负担。", "motivation": "皮肤癌是全球最常见的癌症，每年产生巨额医疗费用。医生手动撰写SOAP笔记耗时费力，导致临床医生倦怠。", "method": "提出了一个弱监督多模态框架，利用有限的输入（包括病变图像和稀疏临床文本）生成临床结构化的SOAP笔记。该方法减少了对手动标注的依赖。", "result": "该方法在关键临床相关性指标上取得了与GPT-4o、Claude和DeepSeek Janus Pro相当的性能。引入了两个新的评估指标：MedConceptEval（评估与专家医学概念的语义对齐）和Clinical Coherence Score (CCS)（分别评估与输入特征的对齐）。", "conclusion": "该框架实现了可扩展、临床依据的文档生成，减轻了临床医生负担，并减少了对大量标注数据的需求。", "translation": "皮肤癌是全球最常见的癌症形式，每年导致超过80亿美元的医疗支出。在临床环境中，医生使用详细的SOAP（主观、客观、评估和计划）笔记记录患者就诊情况。然而，手动生成这些笔记是劳动密集型的，并导致临床医生倦怠。在这项工作中，我们提出了一个弱监督多模态框架，用于从有限的输入（包括病变图像和稀疏临床文本）生成临床结构化的SOAP笔记。我们的方法减少了对人工标注的依赖，实现了可扩展、有临床依据的文档生成，同时减轻了临床医生负担，并减少了对大量标注数据的需求。我们的方法在关键临床相关性指标上取得了与GPT-4o、Claude和DeepSeek Janus Pro相当的性能。为了评估临床质量，我们引入了两个新颖的指标MedConceptEval和Clinical Coherence Score (CCS)，它们分别评估与专家医学概念和输入特征的语义对齐。", "summary": "这项研究提出了一个弱监督多模态框架，旨在从有限的病变图像和稀疏临床文本中自动生成结构化的SOAP医疗笔记。该方法旨在解决手动笔记生成导致的医生倦怠和高昂成本问题，减少对大量人工标注数据的依赖，从而实现可扩展的临床文档自动化。实验结果表明，其性能与主流大型语言模型相当，并且引入了两个新的临床质量评估指标。", "keywords": "SOAP笔记生成, 弱监督学习, 多模态框架, 临床文档, 皮肤癌", "comments": "这项工作通过引入弱监督和多模态方法，解决了医疗文档自动化中数据标注成本高昂的问题，具有重要的临床应用潜力。其性能与大型语言模型相当，且提出的新评估指标有助于更准确地衡量生成笔记的临床质量。"}}
{"id": "2506.10159", "title": "Probabilistic Variational Contrastive Learning", "authors": ["Minoh Jeong", "Seonho Kim", "Alfred Hero"], "summary": "Deterministic embeddings learned by contrastive learning (CL) methods such as\nSimCLR and SupCon achieve state-of-the-art performance but lack a principled\nmechanism for uncertainty quantification. We propose Variational Contrastive\nLearning (VCL), a decoder-free framework that maximizes the evidence lower\nbound (ELBO) by interpreting the InfoNCE loss as a surrogate reconstruction\nterm and adding a KL divergence regularizer to a uniform prior on the unit\nhypersphere. We model the approximate posterior $q_\\theta(z|x)$ as a projected\nnormal distribution, enabling the sampling of probabilistic embeddings. Our two\ninstantiations--VSimCLR and VSupCon--replace deterministic embeddings with\nsamples from $q_\\theta(z|x)$ and incorporate a normalized KL term into the\nloss. Experiments on multiple benchmarks demonstrate that VCL mitigates\ndimensional collapse, enhances mutual information with class labels, and\nmatches or outperforms deterministic baselines in classification accuracy, all\nthe while providing meaningful uncertainty estimates through the posterior\nmodel. VCL thus equips contrastive learning with a probabilistic foundation,\nserving as a new basis for contrastive approaches.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10159v1", "AI": {"title_translation": "概率变分对比学习", "tldr": "本文提出了变分对比学习（VCL），一个无解码器的框架，通过将InfoNCE损失解释为替代重建项并添加KL散度正则化器，为对比学习提供了概率基础和不确定性量化能力，并在多个基准测试中表现出色。", "motivation": "现有的对比学习（CL）方法（如SimCLR和SupCon）学习到的确定性嵌入缺乏不确定性量化的原理机制。", "method": "本文提出了变分对比学习（VCL），一个无解码器的框架，通过将InfoNCE损失解释为替代重建项，并向单位超球上的均匀先验添加KL散度正则化器来最大化证据下界（ELBO）。模型将近似后验$q_\\theta(z|x)$建模为投影正态分布，从而能够采样概率嵌入。VCL的两个实例化——VSimCLR和VSupCon——用$q_\\theta(z|x)$的样本替换确定性嵌入，并将归一化KL项纳入损失中。", "result": "实验表明，VCL减轻了维度坍塌，增强了与类别标签的互信息，并在分类精度上与确定性基线相当或优于确定性基线，同时通过后验模型提供了有意义的不确定性估计。", "conclusion": "VCL为对比学习赋予了概率基础，成为对比方法的新基础。", "translation": "对比学习（CL）方法（如SimCLR和SupCon）学习到的确定性嵌入达到了最先进的性能，但缺乏不确定性量化的原理机制。我们提出了变分对比学习（VCL），一个无解码器的框架，通过将InfoNCE损失解释为替代重建项，并向单位超球上的均匀先验添加KL散度正则化器来最大化证据下界（ELBO）。我们将近似后验$q_\\theta(z|x)$建模为投影正态分布，从而能够采样概率嵌入。我们的两个实例化——VSimCLR和VSupCon——用$q_\\theta(z|x)$的样本替换确定性嵌入，并将归一化KL项纳入损失中。在多个基准测试上的实验表明，VCL减轻了维度坍塌，增强了与类别标签的互信息，并在分类精度上与确定性基线相当或优于确定性基线，同时通过后验模型提供了有意义的不确定性估计。因此，VCL为对比学习赋予了概率基础，成为对比方法的新基础。", "summary": "本文提出了变分对比学习（VCL），一个为对比学习提供不确定性量化能力的无解码器框架。VCL通过最大化证据下界（ELBO），将InfoNCE损失视为重建项，并引入KL散度正则化器到单位超球上的均匀先验。它将近似后验建模为投影正态分布以生成概率嵌入。实验证明，VCL能有效缓解维度坍塌、提升互信息，并在分类性能上与现有确定性方法持平或超越，同时提供可靠的不确定性估计，为对比学习奠定了概率基础。", "keywords": "对比学习, 变分推断, 不确定性量化, 概率嵌入, 维度坍塌", "comments": "本文的创新点在于为对比学习引入了概率框架，解决了现有方法无法量化不确定性的问题。通过将InfoNCE损失与变分推断结合，VCL不仅提升了模型的鲁棒性和解释性，还在性能上保持了竞争力，为未来的对比学习研究开辟了新方向。"}}
{"id": "2506.10674", "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving", "authors": ["Vincenzo Colle", "Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Fadhel Ayed", "Merouane Debbah"], "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.", "comment": "6 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10674v1", "AI": {"title_translation": "TeleMath：电信数学问题解决中大型语言模型的基准", "tldr": "引入了TeleMath，首个用于评估大型语言模型（LLMs）在电信领域数学问题解决能力的基准数据集（包含500个问答对），研究发现专门针对数学或逻辑推理的LLMs表现优于通用模型。", "motivation": "尽管大型语言模型（LLMs）在一般数学推理方面取得了进展，但它们在电信领域（如信号处理、网络优化、性能分析）中解决领域特定、数学密集型任务的有效性尚未得到充分探索，存在评估空白。", "method": "研究人员引入了TeleMath，一个包含500个问答（QnA）对的基准数据集，专门用于评估LLM在电信领域数学问题解决方面的性能。数据集的QnA由主题专家设计，并概述了QnA生成流程。随后，他们使用该数据集评估了广泛的开源LLMs，并发布了数据集和评估代码。", "result": "评估结果显示，专门为数学或逻辑推理设计的LLMs在TeleMath上的表现最佳。相比之下，通用LLMs，即使参数量很大，也难以应对这些电信领域的数学挑战。", "conclusion": "专门为数学或逻辑推理设计的LLMs在解决电信领域复杂的数学问题方面比通用LLMs更有效。TeleMath基准数据集有助于区分不同模型的性能，并为未来的研究提供了重要资源。", "translation": "随着人工智能在电信领域的日益普及，人们对大型语言模型（LLMs）解决领域特定、数学密集型任务的能力产生了兴趣。尽管最近的进展提高了LLMs在一般数学推理方面的性能，但它们在信号处理、网络优化和性能分析等专业领域的有效性仍未得到充分探索。为了弥补这一空白，我们引入了TeleMath，这是第一个专门用于评估LLM在解决电信领域具有数值解的数学问题方面性能的基准数据集。TeleMath包含500个问答（QnA）对，涵盖了电信领域的广泛主题。本文概述了所提出的QnA生成流程，该流程从主题专家精心设计的问题种子开始。对广泛的开源LLMs进行评估后发现，在TeleMath上表现最佳的是最近专门为数学或逻辑推理设计的模型。相比之下，通用模型，即使参数量很大，也常常难以应对这些挑战。我们已经发布了数据集和评估代码，以方便结果重现并支持未来的研究。", "summary": "本文介绍了TeleMath，一个包含500个问答对的基准数据集，旨在首次评估大型语言模型（LLMs）在电信领域数学问题解决方面的性能。通过主题专家构建问题并设计QnA生成流程，研究人员对多种开源LLMs进行了评估。结果显示，专门为数学或逻辑推理设计的LLMs在TeleMath上表现优异，而通用LLMs则难以应对。该数据集和评估代码已发布，以促进研究和结果复现。", "keywords": "大型语言模型, 电信, 数学问题解决, 基准数据集, TeleMath", "comments": "TeleMath的创新之处在于它是第一个专门针对电信领域数学问题的LLM基准数据集，填补了该领域的空白。其重要性在于揭示了通用LLMs在该专业领域的局限性，并强调了开发领域特定或数学/逻辑推理专用LLMs的必要性。通过发布数据集和代码，该工作也为未来的研究提供了宝贵的资源。"}}
{"id": "2506.10813", "title": "Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing", "authors": ["Hang Zhang", "Xiang Chen", "Renjiu Hu", "Rongguang Wang", "Jinwei Zhang", "Min Liu", "Yaonan Wang", "Gaolei Li", "Xinxing Cheng", "Jinming Duan"], "summary": "Learning-based deformable image registration (DIR) accelerates alignment by\namortizing traditional optimization via neural networks. Label supervision\nfurther enhances accuracy, enabling efficient and precise nonlinear alignment\nof unseen scans. However, images with sparse features amid large smooth\nregions, such as retinal vessels, introduce aperture and large-displacement\nchallenges that unsupervised DIR methods struggle to address. This limitation\noccurs because neural networks predict deformation fields in a single forward\npass, leaving fields unconstrained post-training and shifting the\nregularization burden entirely to network weights. To address these issues, we\nintroduce SmoothProper, a plug-and-play neural module enforcing smoothness and\npromoting message passing within the network's forward pass. By integrating a\nduality-based optimization layer with tailored interaction terms, SmoothProper\nefficiently propagates flow signals across spatial locations, enforces\nsmoothness, and preserves structural consistency. It is model-agnostic,\nseamlessly integrates into existing registration frameworks with minimal\nparameter overhead, and eliminates regularizer hyperparameter tuning.\nPreliminary results on a retinal vessel dataset exhibiting aperture and\nlarge-displacement challenges demonstrate our method reduces registration error\nto 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach\nto effectively address both challenges. The source code will be available at\nhttps://github.com/tinymilky/SmoothProper.", "comment": "Accepted for publication at Information Processing in Medical Imaging\n  (IPMI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10813v1", "AI": {"title_translation": "无监督可变形图像配准与结构非参数平滑", "tldr": "提出SmoothProper模块，通过结构非参数平滑解决无监督可变形图像配准在稀疏特征和大幅位移图像上的挑战，提高了配准精度。", "motivation": "现有无监督可变形图像配准（DIR）方法难以处理稀疏特征和大幅位移图像（如视网膜血管），因为神经网络在单次前向传播后形变场不受约束，且正则化负担完全落在网络权重上。", "method": "本文引入SmoothProper，一个即插即用的神经网络模块。它通过集成一个基于对偶的优化层和定制的交互项，在前向传播中强制平滑并促进信息传递。该模块能有效传播流信号，保持结构一致性，且模型无关，参数开销小，并消除了正则化超参数调优。", "result": "在具有孔径和大幅位移挑战的视网膜血管数据集上，将2912x2912图像的配准误差降低到1.88像素，是第一个有效解决这两个挑战的无监督可变形图像配准方法。", "conclusion": "SmoothProper模块通过在网络前向传播中引入平滑和信息传递机制，显著提高了无监督可变形图像配准在处理复杂图像（如视网膜血管）时的准确性和鲁棒性，有效解决了稀疏特征和大幅位移带来的挑战。", "translation": "基于学习的可变形图像配准（DIR）通过神经网络分摊传统优化，从而加速了对齐过程。标签监督进一步提高了准确性，实现了对未见扫描的高效精确非线性对齐。然而，在大型平滑区域中特征稀疏的图像，例如视网膜血管，带来了孔径和大幅位移的挑战，这是无监督DIR方法难以解决的。这种局限性发生是因为神经网络在单次前向传播中预测形变场，导致训练后形变场不受约束，并将正则化负担完全转移到网络权重上。为了解决这些问题，我们引入了SmoothProper，一个即插即用的神经网络模块，它在前向传播中强制平滑并促进信息传递。通过集成一个基于对偶的优化层和定制的交互项，SmoothProper有效地在空间位置上传播流信号，强制平滑并保持结构一致性。它是模型无关的，可以无缝集成到现有配准框架中，参数开销极小，并消除了正则化超参数调优。在展现孔径和大幅位移挑战的视网膜血管数据集上的初步结果表明，我们的方法将2912x2912图像的配准误差降低到1.88像素，标志着第一个有效解决这两个挑战的无监督DIR方法。源代码将可在https://github.com/tinymilky/SmoothProper获取。", "summary": "本文提出SmoothProper，一个用于无监督可变形图像配准的即插即用神经网络模块。针对现有无监督方法在处理稀疏特征和大幅位移图像时的局限性，SmoothProper通过在网络前向传播中引入基于对偶优化的结构非参数平滑，有效强制形变场平滑并促进信息传递，从而提高配准精度和鲁棒性。在视网膜血管数据集上的实验表明，该方法显著降低了配准误差，成功解决了传统方法难以应对的挑战。", "keywords": "无监督图像配准, 可变形图像配准, 结构非参数平滑, 神经网络, 视网膜血管", "comments": "该论文的创新点在于提出了一个模型无关的“即插即用”模块SmoothProper，通过在神经网络前向传播中集成优化层来内在地强制平滑和促进信息传递，从而解决了无监督DIR在处理复杂图像时形变场缺乏约束的问题。这不仅提高了配准精度，尤其是在稀疏特征和大幅位移场景下，还消除了对正则化超参数调优的需求，简化了部署，具有重要的实用价值。"}}
{"id": "2506.10506", "title": "On a mean-field Pontryagin minimum principle for stochastic optimal control", "authors": ["Manfred Opper", "Sebastian Reich"], "summary": "This papers outlines a novel extension of the classical Pontryagin minimum\n(maximum) principle to stochastic optimal control problems. Contrary to the\nwell-known stochastic Pontryagin minimum principle involving forward-backward\nstochastic differential equations, the proposed formulation is deterministic\nand of mean-field type. The Hamiltonian structure of the proposed Pontryagin\nminimum principle is achieved via the introduction of an appropriate gauge\nvariable. The gauge freedom can be used to decouple the forward and reverse\ntime equations; hence simplifying the solution of the underlying boundary value\nproblem. We also consider infinite horizon discounted cost optimal control\nproblems. In this case, the mean-field formulation allows converting the\ncomputation of the desired optimal control law into solving a pair of forward\nmean-field ordinary differential equations. The proposed mean-field formulation\nof the Pontryagin minimum principle is tested numerically for a controlled\ninverted pendulum and a controlled Lorenz-63 system.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10506v1", "AI": {"title_translation": "随机最优控制的平均场庞特里亚金最小原理", "tldr": "本文提出了一种新的确定性平均场庞特里亚金最小原理，用于随机最优控制，通过引入规范变量简化了问题求解。", "motivation": "现有的随机庞特里亚金最小原理涉及复杂的前向-后向随机微分方程，本文旨在提出一种确定性、平均场类型的替代方案，以简化随机最优控制问题的求解。", "method": "本文提出了一种将经典庞特里亚金最小原理扩展到随机最优控制问题的新方法。该方法是确定性且为平均场类型，通过引入适当的规范变量实现哈密顿结构，并利用规范自由度解耦前向和反向时间方程。对于无限时域折现成本问题，将最优控制律的计算转化为求解一对前向平均场常微分方程。", "result": "该方法简化了底层边值问题的求解。所提出的平均场庞特里亚金最小原理在受控倒立摆和受控Lorenz-63系统上进行了数值测试。", "conclusion": "Not mentioned in abstract", "translation": "这篇论文概述了经典庞特里亚金最小（最大）原理在随机最优控制问题上的新颖扩展。与涉及前向-后向随机微分方程的众所周知的随机庞特里亚金最小原理相反，所提出的公式是确定性的，并且是平均场类型的。所提出的庞特里亚金最小原理的哈密顿结构是通过引入一个适当的规范变量来实现的。规范自由度可以用于解耦前向和反向时间方程；从而简化了底层边值问题的求解。我们还考虑了无限时域折现成本最优控制问题。在这种情况下，平均场公式允许将所需最优控制律的计算转换为求解一对前向平均场常微分方程。所提出的庞特里亚金最小原理的平均场公式在受控倒立摆和受控Lorenz-63系统上进行了数值测试。", "summary": "本文提出了一种新颖的、确定性的平均场庞特里亚金最小原理，用于随机最优控制。与传统涉及前向-后向随机微分方程的方法不同，该方法通过引入规范变量来解耦时间方程，从而简化了边值问题的求解。对于无限时域问题，最优控制计算被转化为求解一对前向平均场常微分方程。该方法在受控倒立摆和Lorenz-63系统上进行了数值验证。", "keywords": "庞特里亚金最小原理, 随机最优控制, 平均场, 规范变量, 解耦", "comments": "本文的创新点在于提出了一个确定性的平均场庞特里亚金最小原理，有效避免了传统随机庞特里亚金原理中复杂的前向-后向随机微分方程。通过引入规范变量实现方程解耦，显著简化了随机最优控制问题的求解。其将计算转化为求解前向常微分方程的特性，对于实际应用具有重要意义。"}}
{"id": "2506.10034", "title": "Impacts between multibody systems and deformable structures", "authors": ["Lipinski Krzysztof"], "summary": "Collisions and impacts are the principal reasons for impulsive motions, which\nwe frequently see in dynamic responses of systems. Precise modelling of impacts\nis a challenging problem due to the lack of the accurate and commonly accepted\nconstitutive law that governs their mechanics. Rigid-body approach and soft\ncontact methods are discussed in this paper and examined in the presented\nnumerical examples. The main focus is set to impacts in systems with multiple\nunilateral contacts and collisions with elastic elements of the reference.\nParameters of interconnecting unilateral springs are under discussion.", "comment": "20 pages, 11 figures, submitted to Virtual Conference Proceeding of\n  12th ECCOMAS Thematic Conference on Multibody Dynamics - Innsbruck July\n  13-18, 2025 and to the journal of Multibody System Dynamics", "cate": "physics.class-ph", "url": "http://arxiv.org/abs/2506.10034v1", "AI": {"title_translation": "多体系统与可变形结构之间的碰撞", "tldr": "本文讨论了多体系统与可变形结构之间碰撞的精确建模，并探讨了刚体方法和软接触方法，特别关注多单边接触和弹性元件。", "motivation": "碰撞和冲击是系统动态响应中脉冲运动的主要原因，但由于缺乏准确且普遍接受的控制其力学的本构定律，精确建模冲击是一个具有挑战性的问题。", "method": "本文讨论并检验了刚体方法和软接触方法，并将其应用于具有多个单边接触以及与参考弹性元件碰撞的系统。同时，文中还讨论了互连单边弹簧的参数。", "result": "论文中提供了数值示例来检验所讨论的方法，但摘要中未详细说明具体结果。", "conclusion": "本文主要关注多体系统与可变形结构之间冲击的精确建模问题，特别是涉及多单边接触和弹性元件的碰撞，并探讨了相关建模方法和参数。", "translation": "碰撞和冲击是系统动态响应中脉冲运动的主要原因。由于缺乏准确且普遍接受的控制其力学的本构定律，精确建模冲击是一个具有挑战性的问题。本文讨论了刚体方法和软接触方法，并在所提供的数值示例中进行了检验。主要关注点在于具有多个单边接触的系统中的冲击以及与参考弹性元件的碰撞。互连单边弹簧的参数正在讨论中。", "summary": "本文探讨了多体系统与可变形结构之间冲击的精确建模问题，指出其挑战在于缺乏普适的本构定律。文章讨论并检验了刚体方法和软接触方法，并将其应用于具有多个单边接触以及与弹性元件碰撞的系统，同时探讨了互连单边弹簧的参数。", "keywords": "碰撞, 多体系统, 可变形结构, 冲击建模, 软接触方法", "comments": "这篇论文关注的是动力学领域中一个具有挑战性的问题，即冲击的精确建模。通过讨论刚体和软接触方法，并关注多单边接触和弹性元件的碰撞，该研究试图为复杂多体系统的冲击分析提供见解。其重要性在于为复杂接触条件下的冲击分析提供了理论和方法探讨。"}}
{"id": "2506.10467", "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and Cybersecurity Applications", "authors": ["Felix Härer"], "summary": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10467v1", "AI": {"title_translation": "多智能体LLM系统的规范与评估——原型与网络安全应用", "tldr": "本文探讨了多智能体LLM系统的规范和评估，提出了一个原型系统，并通过网络安全任务案例验证了其可行性。", "motivation": "尽管LLM、推理技术和应用都在单独进行评估，但它们联合规范和组合应用尚未得到充分探索。需要明确的多智能体LLM系统规范来探索其潜力及其在特定应用中的适用性，从而实现对LLM、推理技术和相关方面的系统评估。", "method": "本文通过一个多智能体系统进行探索性研究，以规范和评估这些方面。该系统架构和原型是基于先前的研究进行扩展的，并引入了多智能体系统的规范。", "result": "涉及网络安全任务的测试案例表明了该架构和评估方法的可行性。结果特别显示，由使用OpenAI和DeepSeek的LLM的智能体正确完成了问答、服务器安全和网络安全任务的评估。", "conclusion": "该研究表明，所提出的多智能体LLM系统架构和评估方法在处理网络安全等复杂任务方面是可行的。", "translation": "LLM的最新进展表明了新型应用的潜力，例如通过最新OpenAI和DeepSeek模型中的推理能力。为了将这些模型应用于文本生成之外的特定领域，可以利用基于LLM的多智能体方法，通过结合推理技术、代码生成和软件执行来解决复杂任务。应用程序可能利用这些能力和专业LLM智能体的知识。然而，尽管对LLM、推理技术和应用程序进行了许多单独的评估，但它们的联合规范和组合应用尚未得到充分探索。需要明确的多智能体LLM系统规范来探索其潜力及其在特定应用中的适用性，从而实现对LLM、推理技术和相关方面的系统评估。本文报告了通过多智能体系统规范和评估这些方面的探索性研究结果。该系统架构和原型是基于先前的研究进行扩展的，并引入了多智能体系统的规范。涉及网络安全任务的测试案例表明了该架构和评估方法的可行性。结果特别显示，由使用OpenAI和DeepSeek的LLM的智能体正确完成了问答、服务器安全和网络安全任务的评估。", "summary": "本文针对LLM在特定领域（如网络安全）的应用，提出了多智能体LLM系统的规范和评估方法。研究通过一个扩展的系统原型和引入的规范，对多智能体系统进行了探索性研究。在问答、服务器安全和网络安全任务上的测试案例表明，该架构和评估方法是可行的，并且智能体能够正确完成任务。", "keywords": "多智能体系统, LLM, 网络安全, 规范, 评估", "comments": "该论文的创新点在于关注多智能体LLM系统的“联合规范和组合应用”，这填补了当前研究中的一个空白。通过在网络安全领域的具体应用案例，论文验证了其提出的架构和评估方法的实用性和可行性，为未来复杂任务中LLM的实际部署提供了有价值的参考。"}}
{"id": "2506.10785", "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps", "authors": ["Vinaik Chhetri", "Krishna Upadhyay", "A. B. Siddique", "Umar Farooq"], "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated\nacross mobile apps in various domains, including productivity, education,\nentertainment, and creativity. However, how users perceive, evaluate, and\ncritique these AI features remains largely unexplored, primarily due to the\noverwhelming volume of user feedback. In this work, we present the first\ncomprehensive, large-scale study of user feedback on AI-powered mobile apps,\nleveraging a curated dataset of 292 AI-driven apps across 14 categories with\n894K AI-specific reviews from Google Play. We develop and validate a\nmulti-stage analysis pipeline that begins with a human-labeled benchmark and\nsystematically evaluates large language models (LLMs) and prompting strategies.\nEach stage, including review classification, aspect-sentiment extraction, and\nclustering, is validated for accuracy and consistency. Our pipeline enables\nscalable, high-precision analysis of user feedback, extracting over one million\naspect-sentiment pairs clustered into 18 positive and 15 negative user topics.\nOur analysis reveals that users consistently focus on a narrow set of themes:\npositive comments emphasize productivity, reliability, and personalized\nassistance, while negative feedback highlights technical failures (e.g.,\nscanning and recognition), pricing concerns, and limitations in language\nsupport. Our pipeline surfaces both satisfaction with one feature and\nfrustration with another within the same review. These fine-grained,\nco-occurring sentiments are often missed by traditional approaches that treat\npositive and negative feedback in isolation or rely on coarse-grained analysis.\nTo this end, our approach provides a more faithful reflection of the real-world\nuser experiences with AI-powered apps. Category-aware analysis further uncovers\nboth universal drivers of satisfaction and domain-specific frustrations.", "comment": "12 pages, 6 figures, 5 tables", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10785v1", "AI": {"title_translation": "用户看重和批评什么：AI驱动移动应用用户反馈的大规模分析", "tldr": "该研究首次对AI驱动的移动应用的用户反馈进行了大规模分析，揭示了用户对AI功能的价值和批评点。", "motivation": "尽管AI功能在移动应用中迅速普及，但由于用户反馈量巨大，用户如何感知、评估和批评这些AI功能仍未得到充分探索。", "method": "研究利用Google Play上292个AI驱动应用和89.4万条AI相关评论的数据集，开发并验证了一个多阶段分析流程。该流程包括评论分类、方面-情感提取和聚类，并使用人工标注基准系统评估大型语言模型（LLMs）和提示策略。", "result": "分析流程提取了超过一百万个方面-情感对，聚类成18个积极用户主题和15个消极用户主题。积极评论侧重于生产力、可靠性和个性化帮助，而消极反馈则强调技术故障、定价问题和语言支持限制。该流程还能识别同一评论中的细粒度、共现情感。", "conclusion": "该方法能够更真实地反映用户对AI驱动应用的真实体验，并揭示了普遍的满意驱动因素和特定领域的挫折。", "translation": "人工智能（AI）驱动的功能已迅速普及到生产力、教育、娱乐和创造力等各个领域的移动应用中。然而，由于用户反馈量巨大，用户如何感知、评估和批评这些AI功能在很大程度上仍未被探索。在这项工作中，我们首次对AI驱动移动应用的用户反馈进行了全面、大规模的研究，利用了一个精选数据集，该数据集包含来自Google Play的14个类别中的292个AI驱动应用，以及89.4万条AI特定评论。我们开发并验证了一个多阶段分析流程，该流程始于人工标注基准，并系统地评估大型语言模型（LLMs）和提示策略。每个阶段，包括评论分类、方面-情感提取和聚类，都经过准确性和一致性验证。我们的流程能够对用户反馈进行可扩展、高精度的分析，提取出超过一百万个方面-情感对，这些对被聚类为18个积极用户主题和15个消极用户主题。我们的分析表明，用户始终关注一组狭窄的主题：积极评论强调生产力、可靠性和个性化帮助，而消极反馈则突出技术故障（例如，扫描和识别）、定价问题和语言支持的局限性。我们的流程能够揭示同一评论中对某个功能的满意和对另一个功能的不满。这些细粒度、共现的情感常常被传统方法所忽略，这些方法要么孤立地处理积极和消极反馈，要么依赖于粗粒度分析。为此，我们的方法更真实地反映了用户对AI驱动应用的真实体验。类别感知分析进一步揭示了普遍的满意驱动因素和特定领域的挫折。", "summary": "这项研究首次对AI驱动移动应用的用户反馈进行了大规模、全面的分析。研究团队利用包含近90万条评论的数据集，开发了一个多阶段分析流程，有效提取并聚类了用户对AI功能的积极和消极情感。结果揭示了用户普遍关注的生产力、可靠性（积极）以及技术故障、定价（消极）等主题，并强调了同时捕获同一评论中细粒度共现情感的重要性，这比传统方法更能真实反映用户体验。", "keywords": "用户反馈, AI应用, 大规模分析, 情感分析, 大型语言模型", "comments": "这篇论文通过大规模分析用户反馈，弥补了AI驱动应用用户感知研究的空白。其创新的多阶段分析流程，特别是能够识别同一评论中细粒度共现情感的能力，是其重要亮点，有助于更准确地理解用户体验，为AI应用开发者提供有价值的洞察。"}}
{"id": "2506.10850", "title": "Invariant Extended Kalman Filter for Autonomous Surface Vessels with Partial Orientation Measurements", "authors": ["Derek Benham", "Easton Potokar", "Joshua G. Mangelson"], "summary": "Autonomous surface vessels (ASVs) are increasingly vital for marine science,\noffering robust platforms for underwater mapping and inspection. Accurate state\nestimation, particularly of vehicle pose, is paramount for precise seafloor\nmapping, as even small surface deviations can have significant consequences\nwhen sensing the seafloor below. To address this challenge, we propose an\nInvariant Extended Kalman Filter (InEKF) framework designed to integrate\npartial orientation measurements. While conventional estimation often relies on\nrelative position measurements to fixed landmarks, open ocean ASVs primarily\nobserve a receding horizon. We leverage forward-facing monocular cameras to\nestimate roll and pitch with respect to this horizon, which provides\nyaw-ambiguous partial orientation information. To effectively utilize these\nmeasurements within the InEKF, we introduce a novel framework for incorporating\nsuch partial orientation data. This approach contrasts with traditional InEKF\nimplementations that assume full orientation measurements and is particularly\nrelevant for planar vehicle motion constrained to a \"seafaring plane.\" This\npaper details the developed InEKF framework; its integration with horizon-based\nroll/pitch observations and dual-antenna GPS heading measurements for ASV state\nestimation; and provides a comparative analysis against the InEKF using full\norientation and a Multiplicative EKF (MEKF). Our results demonstrate the\nefficacy and robustness of the proposed partial orientation measurements for\naccurate ASV state estimation in open ocean environments.", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics. 8 pages,\n  4 figures, 2 tables", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10850v1", "AI": {"title_translation": "具有部分姿态测量的自主水面船舶不变扩展卡尔曼滤波器", "tldr": "本文提出了一种不变扩展卡尔曼滤波器（InEKF）框架，用于整合部分姿态测量，以实现开放海洋中自主水面船舶（ASV）的精确状态估计，尤其是在只有水平线观测和双天线GPS航向测量的情况下，并验证了其有效性。", "motivation": "自主水面船舶（ASV）在海洋科学中日益重要，但精确的状态估计（特别是姿态）对于精确的海底测绘至关重要，因为即使是微小的偏差也可能导致显著后果。传统估计方法依赖于固定地标，而开放海洋ASV主要观测退去的地平线，缺乏完整的姿态测量，这构成了挑战。", "method": "本文提出了一个不变扩展卡尔曼滤波器（InEKF）框架，旨在整合部分姿态测量。该方法利用前向单目摄像头估计相对于地平线的横摇和俯仰（提供偏航模糊的部分姿态信息），并引入了一种新颖的框架来有效利用这些部分姿态数据。此外，还结合了双天线GPS航向测量。研究通过与使用完整姿态的InEKF和乘性EKF（MEKF）进行比较分析来验证其性能。", "result": "研究结果表明，所提出的部分姿态测量方法在开放海洋环境中实现了自主水面船舶（ASV）精确状态估计的有效性和鲁棒性。", "conclusion": "所提出的整合部分姿态测量的InEKF框架能够有效且鲁棒地实现开放海洋中ASV的精确状态估计，解决了传统方法在缺乏完整姿态测量时的挑战，特别适用于受限于“航海平面”的平面车辆运动。", "translation": "自主水面船舶（ASV）在海洋科学中变得越来越重要，为水下测绘和检查提供了强大的平台。精确的状态估计，特别是船舶姿态，对于精确的海底测绘至关重要，因为即使是微小的水面偏差也可能对下方海底的传感产生重大影响。为了应对这一挑战，我们提出了一种不变扩展卡尔曼滤波器（InEKF）框架，旨在整合部分姿态测量。虽然传统估计通常依赖于对固定地标的相对位置测量，但开放海洋ASV主要观测退去的地平线。我们利用前向单目摄像头估计相对于该地平线的横摇和俯仰，这提供了偏航模糊的部分姿态信息。为了在InEKF中有效利用这些测量，我们引入了一种新颖的框架来结合此类部分姿态数据。这种方法与假设完整姿态测量的传统InEKF实现形成对比，并且特别适用于受限于“航海平面”的平面车辆运动。本文详细介绍了所开发的InEKF框架；其与基于地平线的横摇/俯仰观测以及双天线GPS航向测量相结合用于ASV状态估计；并提供了与使用完整姿态的InEKF和乘性EKF（MEKF）的比较分析。我们的结果证明了所提出的部分姿态测量方法在开放海洋环境中实现ASV精确状态估计的有效性和鲁棒性。", "summary": "本文提出了一种针对自主水面船舶（ASV）的创新不变扩展卡尔曼滤波器（InEKF）框架，旨在解决开放海洋环境中精确状态估计中部分姿态测量整合的挑战。该框架利用前向单目摄像头提供的地平线横摇/俯仰信息和双天线GPS航向测量，弥补了传统InEKF对完整姿态测量的依赖。实验结果验证了该方法在复杂海洋环境下实现ASV精确状态估计的有效性和鲁棒性。", "keywords": "自主水面船舶, 不变扩展卡尔曼滤波器, 部分姿态测量, 状态估计, 地平线观测", "comments": "这篇论文的创新点在于提出了一个新颖的框架，将部分姿态测量（尤其是地平线观测的横摇和俯仰）有效地整合到不变扩展卡尔曼滤波器中。这对于开放海洋中缺乏固定地标且难以获取完整姿态信息的自主水面船舶至关重要，极大地扩展了InEKF在实际应用中的适用性。"}}
{"id": "2506.10678", "title": "Automated Validation of Textual Constraints Against AutomationML via LLMs and SHACL", "authors": ["Tom Westermann", "Aljosha Köcher", "Felix Gehlhoff"], "summary": "AutomationML (AML) enables standardized data exchange in engineering, yet\nexisting recommendations for proper AML modeling are typically formulated as\ninformal and textual constraints. These constraints cannot be validated\nautomatically within AML itself. This work-in-progress paper introduces a\npipeline to formalize and verify such constraints. First, AML models are mapped\nto OWL ontologies via RML and SPARQL. In addition, a Large Language Model\ntranslates textual rules into SHACL constraints, which are then validated\nagainst the previously generated AML ontology. Finally, SHACL validation\nresults are automatically interpreted in natural language. The approach is\ndemonstrated on a sample AML recommendation. Results show that even complex\nmodeling rules can be semi-automatically checked -- without requiring users to\nunderstand formal methods or ontology technologies.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10678v1", "AI": {"title_translation": "通过大型语言模型和SHACL对AutomationML文本约束进行自动化验证", "tldr": "本文提出了一种新颖的管道，利用大型语言模型（LLMs）和SHACL，将AutomationML（AML）中的非正式文本约束形式化并自动验证，无需用户理解形式化方法。", "motivation": "AutomationML (AML) 实现了工程领域的数据标准化交换，但现有的AML建模推荐通常以非正式的文本约束形式存在，这些约束无法在AML内部进行自动化验证。因此，需要一种方法来形式化和验证这些约束。", "method": "该方法首先通过RML和SPARQL将AML模型映射到OWL本体。其次，一个大型语言模型（LLM）将文本规则翻译成SHACL约束。然后，这些SHACL约束针对生成的AML本体进行验证。最后，SHACL验证结果被自动解释为自然语言。", "result": "结果表明，即使是复杂的建模规则也可以半自动化检查，而无需用户理解形式化方法或本体技术。该方法已在一个示例AML推荐上进行了演示。", "conclusion": "该研究成功地展示了一种通过结合LLMs和SHACL，实现对AutomationML文本约束进行半自动化验证的方法，降低了用户使用形式化方法的门槛。", "translation": "AutomationML (AML) 实现了工程领域的数据交换标准化，但现有的AML建模推荐通常以非正式的文本约束形式存在。这些约束无法在AML内部进行自动化验证。这篇正在进行中的论文介绍了一个用于形式化和验证此类约束的管道。首先，AML模型通过RML和SPARQL映射到OWL本体。此外，一个大型语言模型将文本规则翻译成SHACL约束，然后这些约束针对先前生成的AML本体进行验证。最后，SHACL验证结果被自动解释为自然语言。该方法在一个示例AML推荐上进行了演示。结果表明，即使是复杂的建模规则也可以半自动化检查——无需用户理解形式化方法或本体技术。", "summary": "本文提出了一种利用大型语言模型（LLMs）和SHACL对AutomationML (AML) 中的非正式文本约束进行自动化验证的管道。该方法首先将AML模型映射到OWL本体，然后LLMs将文本规则转换为SHACL约束进行验证，最后自动解释验证结果。实验证明，该方法能半自动化检查复杂的建模规则，无需用户具备形式化方法知识。", "keywords": "AutomationML, 文本约束, 大型语言模型, SHACL, 自动化验证", "comments": "这项工作的创新之处在于结合了大型语言模型（LLMs）和SHACL技术，实现了对非结构化文本约束的自动化验证。其重要性在于降低了AML模型验证的门槛，使得非专业用户也能进行复杂的规则检查，这对于工程数据交换的标准化和质量保证具有实际意义。"}}
{"id": "2506.10855", "title": "Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models", "authors": ["Michele Gubian", "Ioana Krehan", "Oli Liu", "James Kirby", "Sharon Goldwater"], "summary": "Analyses of self-supervised speech models have begun to reveal where and how\nthey represent different types of information. However, almost all analyses\nhave focused on English. Here, we examine how wav2vec2 models trained on four\ndifferent languages encode both language-matched and non-matched speech. We use\nprobing classifiers and geometric analyses to examine how phones, lexical\ntones, and speaker information are represented. We show that for all\npretraining and test languages, the subspaces encoding phones, tones, and\nspeakers are largely orthogonal, and that layerwise patterns of probing\naccuracy are similar, with a relatively small advantage for matched-language\nphone and tone (but not speaker) probes in the later layers. Our findings\nsuggest that the structure of representations learned by wav2vec2 is largely\nindependent of the speech material used during pretraining.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10855v1", "AI": {"title_translation": "分析自监督语音模型中预训练语言、语音、音调和说话人信息之间的关系", "tldr": "本研究分析了wav2vec2模型如何编码不同语言的语音信息，发现其学习到的表示结构在很大程度上独立于预训练语音材料。", "motivation": "现有的自监督语音模型分析大多集中于英语，缺乏对不同语言预训练模型中信息表示的深入探究。", "method": "本研究使用探测分类器和几何分析方法，检查了在四种不同语言上训练的wav2vec2模型如何编码音素、词汇声调和说话人信息，包括语言匹配和不匹配的语音数据。", "result": "结果显示，对于所有预训练和测试语言，编码音素、声调和说话人的子空间大体上是正交的；层级探测准确率模式相似，后期层中匹配语言的音素和声调（但非说话人）探测略有优势。", "conclusion": "研究结果表明，wav2vec2模型学习到的表示结构在很大程度上独立于预训练时使用的语音材料。", "translation": "对自监督语音模型的分析已经开始揭示它们如何以及在哪里表示不同类型的信息。然而，几乎所有的分析都集中在英语。本研究考察了在四种不同语言上训练的wav2vec2模型如何编码语言匹配和不匹配的语音。我们使用探测分类器和几何分析来检查音素、词汇声调和说话人信息是如何表示的。我们发现，对于所有预训练和测试语言，编码音素、声调和说话人的子空间大体上是正交的，并且层级探测准确率模式相似，在后期层中，匹配语言的音素和声调（但非说话人）探测略有优势。我们的发现表明，wav2vec2学习到的表示结构在很大程度上独立于预训练时使用的语音材料。", "summary": "本研究旨在弥补现有自监督语音模型分析主要集中于英语的不足，通过对在四种不同语言上训练的wav2vec2模型进行探测分类器和几何分析，探究其如何编码音素、词汇声调和说话人信息。研究发现，这些信息在模型中被编码在大致正交的子空间中，并且表示结构的学习在很大程度上独立于预训练语言材料。", "keywords": "自监督语音模型, wav2vec2, 语音表示, 跨语言分析, 信息解耦", "comments": "这项研究通过跨语言分析wav2vec2模型中不同语音信息的表示，为理解自监督语音模型的泛化能力和内部机制提供了重要见解。其创新之处在于突破了以往主要关注英语的局限，揭示了模型表示结构对预训练语言材料的独立性，这对于未来开发更通用、鲁棒的语音模型具有指导意义。"}}
{"id": "2506.10502", "title": "A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks", "authors": ["Junhua Lin", "Marc Juarez"], "summary": "We present a novel attack specifically designed against Tree-Ring, a\nwatermarking technique for diffusion models known for its high imperceptibility\nand robustness against removal attacks. Unlike previous removal attacks, which\nrely on strong assumptions about attacker capabilities, our attack only\nrequires access to the variational autoencoder that was used to train the\ntarget diffusion model, a component that is often publicly available. By\nleveraging this variational autoencoder, the attacker can approximate the\nmodel's intermediate latent space, enabling more effective surrogate-based\nattacks. Our evaluation shows that this approach leads to a dramatic reduction\nin the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to\n0.153 and from 0.994 to 0.385, respectively, while maintaining high image\nquality. Notably, our attacks outperform existing methods that assume full\naccess to the diffusion model. These findings highlight the risk of reusing\npublic autoencoders to train diffusion models -- a threat not considered by\ncurrent industry practices. Furthermore, the results suggest that the Tree-Ring\ndetector's precision, a metric that has been overlooked by previous\nevaluations, falls short of the requirements for real-world deployment.", "comment": "18 pages, to be published in the 34th USENIX Security Symposium", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10502v1", "AI": {"title_translation": "树皮裂缝：利用公共知识去除树形水印", "tldr": "本文提出了一种针对Tree-Ring水印的新型攻击，该攻击利用公开可用的变分自编码器，显著降低了Tree-Ring检测器的性能，并突出了重用公共自编码器的风险。", "motivation": "Tree-Ring是一种针对扩散模型的水印技术，以其高不可感知性和对移除攻击的鲁棒性而闻名。然而，现有的移除攻击通常对攻击者能力有很强的假设。本文的动机是开发一种更实际、更有效的攻击方法，仅需利用公开可用的组件，以评估Tree-Ring的实际安全性。", "method": "本文提出了一种新型攻击，该攻击不依赖于对攻击者能力的强假设，而是利用了通常公开可用的用于训练目标扩散模型的变分自编码器（VAE）。通过利用VAE，攻击者可以近似模型的中间潜在空间，从而实现更有效的基于代理的攻击。", "result": "该方法显著降低了Tree-Ring检测器ROC和PR曲线的AUC值，分别从0.993降至0.153和从0.994降至0.385，同时保持了较高的图像质量。该攻击优于假设完全访问扩散模型的现有方法。此外，Tree-Ring检测器的精度被发现不符合实际部署要求。", "conclusion": "研究结果表明，重用公共自编码器来训练扩散模型存在风险，这是当前行业实践中未考虑到的威胁。Tree-Ring检测器的精度，一个之前被忽视的指标，不足以满足实际部署的需求。", "translation": "我们提出了一种专门针对Tree-Ring的新型攻击，Tree-Ring是一种针对扩散模型的水印技术，以其高不可感知性和对移除攻击的鲁棒性而闻名。与之前依赖于对攻击者能力强假设的移除攻击不同，我们的攻击仅需要访问用于训练目标扩散模型的变分自编码器，这是一个通常公开可用的组件。通过利用这个变分自编码器，攻击者可以近似模型的中间潜在空间，从而实现更有效的基于代理的攻击。我们的评估表明，这种方法导致Tree-Ring检测器ROC和PR曲线的AUC值显著下降，分别从0.993降至0.153和从0.994降至0.385，同时保持了较高的图像质量。值得注意的是，我们的攻击优于假设完全访问扩散模型的现有方法。这些发现突出了重用公共自编码器来训练扩散模型的风险——这是当前行业实践中未考虑到的威胁。此外，结果表明，Tree-Ring检测器的精度，一个被以前评估所忽视的指标，未能达到实际部署的要求。", "summary": "本文提出了一种名为“树皮裂缝”的新型攻击，旨在针对扩散模型中的Tree-Ring水印技术。该攻击利用公开可用的变分自编码器来近似模型的潜在空间，从而实现比现有方法更有效的基于代理的移除攻击。实验结果表明，该攻击能大幅降低Tree-Ring检测器的性能（AUC从0.993降至0.153），同时保持图像质量。研究强调了重用公共自编码器训练扩散模型所带来的未被充分认识的安全风险，并指出Tree-Ring检测器在实际部署中的精度不足。", "keywords": "Tree-Ring水印, 扩散模型, 水印移除攻击, 变分自编码器, 安全风险", "comments": "这项研究的创新之处在于其攻击方法利用了公开可用的组件（变分自编码器），这使得攻击更具现实性和普适性，揭示了现有水印技术在实际应用中的潜在漏洞。它强调了在扩散模型训练中重用公共自编码器可能带来的安全隐患，对行业实践提出了警示。该工作还首次关注了水印检测器精度的实际部署要求，填补了现有评估的空白。"}}
{"id": "2506.10803", "title": "Solving Package Management via Hypergraph Dependency Resolution", "authors": ["Ryan Gibb", "Patrick Ferris", "David Allsopp", "Michael Winston Dales", "Mark Elvers", "Thomas Gazagnaire", "Sadiq Jaffer", "Thomas Leonard", "Jon Ludlam", "Anil Madhavapeddy"], "summary": "Package managers are everywhere, with seemingly every language and operating\nsystem implementing their own solution. The lack of interoperability between\nthese systems means that multi-lingual projects are unable to express precise\ndependencies across language ecosystems, and external system and hardware\ndependencies are typically implicit and unversioned. We define HyperRes, a\nformal system for describing versioned dependency resolution using a hypergraph\nthat is expressive enough to model many ecosystems and solve dependency\nconstraints across them. We define translations from dozens of existing package\nmanagers to HyperRes and comprehensively demonstrate that dependency resolution\ncan work across ecosystems that are currently distinct. This does not require\nusers to shift their choice of package managers; instead, HyperRes allows for\nthe translation of packaging metadata between ecosystems, and for solving to be\nprecisely specialised to a particular deployment environment.", "comment": "Submitted to SPLASH 2025", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10803v1", "AI": {"title_translation": "通过超图依赖解析解决包管理问题", "tldr": "HyperRes是一个形式系统，利用超图描述版本化依赖解析，能够建模并解决跨多个包生态系统的依赖约束，无需用户改变现有包管理器。", "motivation": "现有的包管理器缺乏互操作性，导致多语言项目无法精确表达跨语言生态系统的依赖关系，并且外部系统和硬件依赖通常是隐式且未版本化的。", "method": "定义了一个名为HyperRes的形式系统，使用超图来描述版本化的依赖解析。该系统足够表达，可以建模多种生态系统并解决它们之间的依赖约束。同时，定义了从数十个现有包管理器到HyperRes的转换。", "result": "全面证明了依赖解析可以在目前独立的生态系统之间工作。HyperRes允许在生态系统之间转换打包元数据，并使解决方案能够精确地专门化到特定的部署环境。", "conclusion": "HyperRes提供了一种通用的方法来解决跨多个包生态系统的依赖问题，提高了多语言项目的互操作性，并且无需用户改变其现有包管理器的选择。", "translation": "包管理器无处不在，似乎每种语言和操作系统都在实现自己的解决方案。这些系统之间缺乏互操作性意味着多语言项目无法表达跨语言生态系统的精确依赖关系，并且外部系统和硬件依赖通常是隐式且未版本化的。我们定义了HyperRes，一个用于描述版本化依赖解析的形式系统，它使用超图，其表达能力足以建模许多生态系统并解决它们之间的依赖约束。我们定义了从数十个现有包管理器到HyperRes的转换，并全面证明了依赖解析可以在目前独立的、不同的生态系统之间工作。这不需要用户改变他们选择的包管理器；相反，HyperRes允许在生态系统之间转换打包元数据，并使解决方案能够精确地专门化到特定的部署环境。", "summary": "该论文提出HyperRes，一个基于超图的形式系统，旨在解决现有包管理器之间互操作性不足的问题。HyperRes能够建模并解析跨多个语言和操作系统生态系统的版本化依赖关系，从而使多语言项目能够精确地管理其依赖。通过定义从现有包管理器到HyperRes的转换，该系统证明了跨生态系统的依赖解析是可行的，并且无需用户更换其首选包管理器，而是通过元数据转换和针对特定部署环境的专业化解析来实现。", "keywords": "包管理, 依赖解析, 超图, 互操作性, HyperRes", "comments": "HyperRes的创新之处在于提出了一个通用的超图模型来统一不同包生态系统的依赖解析，解决了现有系统互操作性差的痛点。其重要性在于，它为多语言和跨平台项目提供了一个潜在的、更强大的依赖管理框架，提升了软件开发的效率和可靠性。该方法的核心优势在于其通用性和非侵入性，即无需强制用户改变现有工具。"}}
{"id": "2506.10875", "title": "Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material", "authors": ["Guanjin Wang", "Xiangxue Zhao", "Shapour Azarm", "Balakumar Balachandran"], "summary": "An alternative data-driven modeling approach has been proposed and employed\nto gain fundamental insights into robot motion interaction with granular\nterrain at certain length scales. The approach is based on an integration of\ndimension reduction (Sequentially Truncated Higher-Order Singular Value\nDecomposition), surrogate modeling (Gaussian Process), and data assimilation\ntechniques (Reduced Order Particle Filter). This approach can be used online\nand is based on offline data, obtained from the offline collection of\nhigh-fidelity simulation data and a set of sparse experimental data. The\nresults have shown that orders of magnitude reduction in computational time can\nbe obtained from the proposed data-driven modeling approach compared with\nphysics-based high-fidelity simulations. With only simulation data as input,\nthe data-driven prediction technique can generate predictions that have\ncomparable accuracy as simulations. With both simulation data and sparse\nphysical experimental measurement as input, the data-driven approach with its\nembedded data assimilation techniques has the potential in outperforming only\nhigh-fidelity simulations for the long-horizon predictions. In addition, it is\ndemonstrated that the data-driven modeling approach can also reproduce the\nscaling relationship recovered by physics-based simulations for maximum\nresistive forces, which may indicate its general predictability beyond a\ncase-by-case basis. The results are expected to help robot navigation and\nexploration in unknown and complex terrains during both online and offline\nphases.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10875v1", "AI": {"title_translation": "机器人附肢与颗粒材料动态相互作用的数据驱动预测", "tldr": "本文提出了一种数据驱动的建模方法，结合降维、代理建模和数据同化技术，显著减少了计算时间，并能准确预测机器人与颗粒地形的相互作用，有望帮助机器人在复杂地形中导航。", "motivation": "为了深入理解机器人在特定尺度下与颗粒地形的运动相互作用，并解决传统物理模型计算成本高的问题。", "method": "该方法是一种数据驱动的建模方法，整合了降维（序列截断高阶奇异值分解）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波器）。它利用离线收集的高保真模拟数据和稀疏实验数据进行建模，并可在线使用。", "result": "与基于物理的高保真模拟相比，计算时间减少了几个数量级。仅使用模拟数据作为输入时，预测精度与模拟相当。结合模拟数据和稀疏实验测量数据时，该方法在长期预测方面可能优于高保真模拟。此外，该方法还能再现物理模拟得到的最大阻力标度关系，表明其具有超越个例的普遍预测能力。", "conclusion": "所提出的数据驱动预测方法能够高效且准确地预测机器人与颗粒材料的动态相互作用，具有普遍适用性，预计将有助于机器人在未知复杂地形中的导航和探索。", "translation": "本文提出并采用了一种替代性的数据驱动建模方法，旨在深入理解机器人在特定长度尺度下与颗粒地形的运动相互作用。该方法基于降维（序列截断高阶奇异值分解）、代理建模（高斯过程）和数据同化技术（降阶粒子滤波器）的整合。该方法可以在线使用，并基于离线数据，这些数据通过离线收集高保真模拟数据和一组稀疏实验数据获得。结果表明，与基于物理的高保真模拟相比，所提出的数据驱动建模方法可以使计算时间减少几个数量级。仅以模拟数据作为输入时，数据驱动预测技术可以生成与模拟精度相当的预测。当同时以模拟数据和稀疏物理实验测量数据作为输入时，嵌入了数据同化技术的数据驱动方法在长期预测方面具有超越仅使用高保真模拟的潜力。此外，研究表明，数据驱动建模方法还可以再现基于物理模拟恢复的最大阻力标度关系，这可能表明其具有超越个例的普遍预测能力。这些结果有望在在线和离线阶段帮助机器人在未知和复杂地形中的导航和探索。", "summary": "本文提出了一种创新的数据驱动建模方法，用于预测机器人附肢与颗粒材料的动态相互作用。该方法结合了序列截断高阶奇异值分解、高斯过程和降阶粒子滤波器，利用离线模拟和稀疏实验数据进行训练。研究结果表明，该方法显著减少了计算时间，且在仅使用模拟数据时能达到与高保真模拟相当的精度。当结合稀疏实验数据时，其在长期预测方面甚至可能超越传统模拟。此外，该方法还能捕捉到物理模拟中的标度关系，显示出其普遍的预测能力，有望提升机器人在复杂地形中的导航和探索能力。", "keywords": "数据驱动建模, 机器人交互, 颗粒材料, 计算效率, 数据同化", "comments": "该论文提出了一种结合多种先进数据科学技术（降维、代理建模、数据同化）的创新性数据驱动方法，用于解决机器人与颗粒材料交互的复杂问题。其主要创新在于显著降低了计算成本，同时保持或提高了预测精度，并展示了超越特定案例的普遍预测潜力。这对于机器人领域，特别是在复杂地形导航和探索方面，具有重要的实际应用价值。该方法融合了模拟数据与稀疏实验数据，提高了模型的鲁棒性和准确性。"}}
{"id": "2506.10334", "title": "Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions", "authors": ["Deliang Wang", "Chao Yang", "Gaowei Chen"], "summary": "Students' academic emotions significantly influence their social behavior and\nlearning performance. Traditional approaches to automatically and accurately\nanalyze these emotions have predominantly relied on supervised machine learning\nalgorithms. However, these models often struggle to generalize across different\ncontexts, necessitating repeated cycles of data collection, annotation, and\ntraining. The emergence of Vision-Language Models (VLMs) offers a promising\nalternative, enabling generalization across visual recognition tasks through\nzero-shot prompting without requiring fine-tuning. This study investigates the\npotential of VLMs to analyze students' academic emotions via facial expressions\nin an online learning environment. We employed two VLMs,\nLlama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000\nimages depicting confused, distracted, happy, neutral, and tired expressions\nusing zero-shot prompting. Preliminary results indicate that both models\ndemonstrate moderate performance in academic facial expression recognition,\nwith Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.\nNotably, both models excel in identifying students' happy emotions but fail to\ndetect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits\nrelatively high performance in recognizing students' confused expressions,\nhighlighting its potential for practical applications in identifying content\nthat causes student confusion.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10334v1", "AI": {"title_translation": "使用视觉语言模型通过面部表情检测学生的学业情绪", "tldr": "本研究探讨了视觉语言模型（VLMs）在零样本提示下通过面部表情检测学生学业情绪的潜力。初步结果显示，Qwen2.5-VL-7B-Instruct在识别困惑和快乐情绪方面表现良好，但两者都未能有效检测分心行为。", "motivation": "学生的学业情绪显著影响其行为和学习表现。传统监督学习方法在情绪分析中面临泛化性差、需要大量数据收集和训练的问题。因此，需要一种能够更好泛化的新方法。", "method": "本研究利用Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct这两种视觉语言模型，通过零样本提示的方式，分析了5000张包含困惑、分心、快乐、中性和疲惫表情的图像，以检测在线学习环境中的学生学业情绪。", "result": "两种模型在学业面部表情识别上表现中等，其中Qwen2.5-VL-7B-Instruct优于Llama-3.2-11B-Vision-Instruct。两者在识别快乐情绪上表现出色，但未能检测到分心行为。Qwen2.5-VL-7B-Instruct在识别困惑表情方面表现相对较好。", "conclusion": "视觉语言模型，特别是Qwen2.5-VL-7B-Instruct，在通过面部表情识别学生学业情绪方面显示出潜力，尤其是在识别快乐和困惑情绪方面，这对于识别引起学生困惑的内容具有实际应用价值。", "translation": "学生的学业情绪显著影响他们的社会行为和学习表现。传统的自动准确分析这些情绪的方法主要依赖于监督机器学习算法。然而，这些模型在不同情境下的泛化能力往往不足，需要重复进行数据收集、标注和训练。视觉语言模型（VLMs）的出现提供了一种有前景的替代方案，它们通过零样本提示即可实现跨视觉识别任务的泛化，而无需进行微调。本研究探讨了VLMs在在线学习环境中通过面部表情分析学生学业情绪的潜力。我们使用了Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两种VLM，通过零样本提示分析了5000张描绘困惑、分心、快乐、中性和疲惫表情的图像。初步结果表明，两种模型在学业面部表情识别方面均表现出中等性能，其中Qwen2.5-VL-7B-Instruct的表现优于Llama-3.2-11B-Vision-Instruct。值得注意的是，两种模型在识别学生的快乐情绪方面表现出色，但未能检测到分心行为。此外，Qwen2.5-VL-7B-Instruct在识别学生的困惑表情方面表现出相对较高的性能，突出了其在识别导致学生困惑的内容方面的实际应用潜力。", "summary": "本研究探讨了视觉语言模型（VLMs）在在线学习环境中通过面部表情检测学生学业情绪的可行性。针对传统监督学习方法泛化性差的问题，作者使用Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两种VLM对5000张图像进行零样本分析。结果显示，两种模型在识别快乐情绪方面表现良好，但无法检测分心行为。其中，Qwen2.5-VL-7B-Instruct表现更优，尤其在识别困惑表情方面具有潜力，为识别学生困惑内容提供了新途径。", "keywords": "视觉语言模型, 学业情绪, 面部表情识别, 零样本学习, 在线学习", "comments": "本文创新性地将视觉语言模型应用于学生学业情绪检测，解决了传统监督学习模型泛化性差的痛点。零样本提示的使用大大减少了数据标注和模型微调的需求，显示了VLMs在教育技术领域的巨大潜力。尽管模型在检测分心行为方面仍有不足，但其在识别快乐和困惑情绪方面的表现，特别是Qwen2.5-VL-7B-Instruct的突出性能，为未来开发智能学习系统以实时识别学生情绪并提供个性化干预奠定了基础。"}}
{"id": "2506.10167", "title": "Wasserstein Barycenter Soft Actor-Critic", "authors": ["Zahra Shahrooei", "Ali Baheri"], "summary": "Deep off-policy actor-critic algorithms have emerged as the leading framework\nfor reinforcement learning in continuous control domains. However, most of\nthese algorithms suffer from poor sample efficiency, especially in environments\nwith sparse rewards. In this paper, we take a step towards addressing this\nissue by providing a principled directed exploration strategy. We propose\nWasserstein Barycenter Soft Actor-Critic (WBSAC) algorithm, which benefits from\na pessimistic actor for temporal difference learning and an optimistic actor to\npromote exploration. This is achieved by using the Wasserstein barycenter of\nthe pessimistic and optimistic policies as the exploration policy and adjusting\nthe degree of exploration throughout the learning process. We compare WBSAC\nwith state-of-the-art off-policy actor-critic algorithms and show that WBSAC is\nmore sample-efficient on MuJoCo continuous control tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10167v1", "AI": {"title_translation": "Wasserstein 重心软Actor-Critic", "tldr": "本文提出了一种名为Wasserstein重心软Actor-Critic (WBSAC)的新算法，通过一种有原则的定向探索策略来提高深度离策略强化学习的样本效率。", "motivation": "深度离策略Actor-Critic算法在连续控制领域表现出色，但普遍存在样本效率低下的问题，尤其是在奖励稀疏的环境中。", "method": "本文提出了Wasserstein 重心软Actor-Critic (WBSAC)算法。该算法利用悲观Actor进行时序差分学习，并使用乐观Actor促进探索。通过将悲观和乐观策略的Wasserstein重心作为探索策略，并在学习过程中调整探索程度来实现。", "result": "WBSAC在MuJoCo连续控制任务上比最先进的离策略Actor-Critic算法具有更高的样本效率。", "conclusion": "WBSAC通过利用悲观和乐观策略的Wasserstein重心作为探索策略，提出了一种有原则的定向探索方法，显著提高了深度离策略Actor-Critic算法在连续控制任务上的样本效率。", "translation": "深度离策略Actor-Critic算法已成为连续控制领域强化学习的主导框架。然而，这些算法中的大多数都存在样本效率低下的问题，尤其是在奖励稀疏的环境中。在本文中，我们通过提供一种有原则的定向探索策略，向解决此问题迈进了一步。我们提出了Wasserstein 重心软Actor-Critic (WBSAC)算法，该算法受益于用于时序差分学习的悲观Actor和用于促进探索的乐观Actor。这是通过使用悲观和乐观策略的Wasserstein重心作为探索策略，并在整个学习过程中调整探索程度来实现的。我们将WBSAC与最先进的离策略Actor-Critic算法进行比较，结果表明WBSAC在MuJoCo连续控制任务上具有更高的样本效率。", "summary": "本文提出了一种名为Wasserstein重心软Actor-Critic (WBSAC)的新型深度离策略强化学习算法，旨在解决现有算法样本效率低下的问题。WBSAC通过结合悲观Actor进行时序差分学习和乐观Actor促进探索，并利用两者策略的Wasserstein重心作为可调整的探索策略。实验结果表明，WBSAC在MuJoCo连续控制任务上比现有最先进的算法具有更高的样本效率。", "keywords": "深度强化学习, 离策略, 样本效率, Wasserstein重心, Actor-Critic", "comments": "该论文的创新点在于提出了一种基于Wasserstein重心的新型定向探索策略，有效地结合了悲观和乐观Actor的优势。这种方法为提高深度离策略强化学习在样本效率方面的表现提供了一个有前景的方向，尤其对于奖励稀疏的环境具有重要意义。"}}
{"id": "2506.10708", "title": "System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers", "authors": ["Michael Bartholomew", "Joohyung Lee"], "summary": "Answer Set Programming Modulo Theories (ASPMT) is an approach to combining\nanswer set programming and satisfiability modulo theories based on the\nfunctional stable model semantics. It is shown that the tight fragment of ASPMT\nprograms can be turned into SMT instances, thereby allowing SMT solvers to\ncompute stable models of ASPMT programs. In this paper we present a compiler\ncalled {\\sc aspsmt2smt}, which implements this translation. The system uses ASP\ngrounder {\\sc gringo} and SMT solver {\\sc z3}. {\\sc gringo} partially grounds\ninput programs while leaving some variables to be processed by {\\sc z3}. We\ndemonstrate that the system can effectively handle real number computations for\nreasoning about continuous changes.", "comment": "In Proceedings of the 14th European Conference on Logics in\n  Artificial Intelligence (JELIA 2014)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10708v1", "AI": {"title_translation": "系统ASPMT2SMT：通过SMT求解器计算ASPMT理论", "tldr": "ASPMT2SMT系统是一个编译器，它将ASPMT程序转换为SMT实例，从而允许SMT求解器计算ASPMT程序的稳定模型，并且能有效地处理实数计算以推理连续变化。", "motivation": "为了实现将ASPMT程序的紧密片段转换为SMT实例，从而允许SMT求解器计算ASPMT程序的稳定模型。", "method": "本文提出了一个名为ASPMT2SMT的编译器，它实现了将ASPMT程序转换为SMT实例。该系统使用ASP基化器Gringo和SMT求解器Z3，其中Gringo负责部分基化输入程序，剩余变量由Z3处理。", "result": "系统能够有效地处理实数计算，用于推理连续变化。", "conclusion": "ASPMT2SMT系统能够有效地将ASPMT程序转换为SMT实例并处理实数计算，从而利用SMT求解器计算ASPMT程序的稳定模型。", "translation": "答案集编程模理论（ASPMT）是一种结合答案集编程和可满足性模理论的方法，其基础是函数稳定模型语义。研究表明，ASPMT程序的紧密片段可以转换为SMT实例，从而允许SMT求解器计算ASPMT程序的稳定模型。在本文中，我们提出了一个名为ASPSMT2SMT的编译器，它实现了这种转换。该系统使用ASP基化器Gringo和SMT求解器Z3。Gringo部分基化输入程序，同时保留一些变量由Z3处理。我们证明了该系统可以有效地处理实数计算，用于推理连续变化。", "summary": "本文介绍了一个名为ASPMT2SMT的编译器，它实现了将答案集编程模理论（ASPMT）的紧密片段转换为可满足性模理论（SMT）实例，从而利用SMT求解器计算ASPMT程序的稳定模型。该系统结合使用ASP基化器Gringo和SMT求解器Z3，并展示了其在处理实数计算以推理连续变化方面的有效性。", "keywords": "ASPMT, SMT, 答案集编程, 编译器, 连续变化", "comments": "该论文的创新点在于提出了一个名为ASPMT2SMT的编译器，实现了将ASPMT程序转换为SMT实例，并首次展示了其在处理实数计算以推理连续变化方面的有效性，这为ASPMT的应用开辟了新途径。"}}
{"id": "2506.10343", "title": "Code Execution as Grounded Supervision for LLM Reasoning", "authors": ["Dongwon Jung", "Wenxuan Zhou", "Muhao Chen"], "summary": "Training large language models (LLMs) with chain-of-thought (CoT) supervision\nhas proven effective for enhancing their reasoning abilities. However,\nobtaining reliable and accurate reasoning supervision remains a significant\nchallenge. We propose a scalable method for generating a high-quality CoT\nsupervision dataset by leveraging the determinism of program execution. Unlike\nexisting reasoning dataset generation methods that rely on costly human\nannotations or error-prone LLM-generated CoT, our approach extracts verifiable,\nstep-by-step reasoning traces from code execution and transforms them into a\nnatural language CoT reasoning. Experiments on reasoning benchmarks across\nvarious domains show that our method effectively equips LLMs with transferable\nreasoning abilities across diverse tasks. Furthermore, the ablation studies\nvalidate that our method produces highly accurate reasoning data and reduces\noverall token length during inference by reducing meaningless repetition and\noverthinking.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10343v1", "AI": {"title_translation": "代码执行作为LLM推理的基础监督", "tldr": "本文提出了一种利用程序执行的确定性来生成高质量思维链（CoT）监督数据的方法，以解决现有方法成本高或易出错的问题，并有效提升LLM的推理能力和推理效率。", "motivation": "训练大型语言模型（LLM）时，获取可靠且准确的推理监督是一个重大挑战。现有方法依赖于昂贵的人工标注或容易出错的LLM生成的思维链（CoT）。", "method": "本文提出了一种可扩展的方法，通过利用程序执行的确定性来生成高质量的CoT监督数据集。该方法从代码执行中提取可验证的、逐步的推理痕迹，并将其转化为自然语言的CoT推理。", "result": "实验表明，该方法能有效地使LLM获得跨不同任务的可迁移推理能力。消融研究验证了该方法生成高度准确的推理数据，并通过减少无意义的重复和过度思考，降低了推理过程中的总token长度。", "conclusion": "通过利用代码执行的确定性，本文方法为LLM提供了高质量、可验证的思维链监督，显著提升了LLM的推理能力和效率，并解决了传统监督方法的高成本和不准确性问题。", "translation": "通过思维链（CoT）监督训练大型语言模型（LLM）已被证明能有效增强其推理能力。然而，获取可靠准确的推理监督仍然是一个重大挑战。我们提出了一种可扩展的方法，通过利用程序执行的确定性来生成高质量的CoT监督数据集。与现有依赖昂贵的人工标注或容易出错的LLM生成的CoT的推理数据集生成方法不同，我们的方法从代码执行中提取可验证的、逐步的推理痕迹，并将其转化为自然语言的CoT推理。在各种领域的推理基准测试中进行的实验表明，我们的方法有效地使LLM获得了跨不同任务的可迁移推理能力。此外，消融研究证实，我们的方法生成了高度准确的推理数据，并通过减少无意义的重复和过度思考，降低了推理过程中的总token长度。", "summary": "本文提出了一种创新方法，利用程序执行的确定性来生成高质量的思维链（CoT）监督数据，以克服LLM推理监督中人工标注成本高昂和LLM生成CoT易出错的问题。该方法通过从代码执行中提取可验证的步骤，并将其转换为自然语言CoT，从而为LLM提供有效的训练。实验证明，此方法不仅赋予LLM可迁移的推理能力，还能生成高准确度的推理数据，并减少推理时的token长度，提高效率。", "keywords": "LLM推理, 思维链, 代码执行, 监督学习, 数据生成", "comments": "这项工作通过利用代码执行的确定性来生成高质量、可验证的思维链（CoT）监督，为LLM的推理能力训练提供了一个新颖且可扩展的解决方案。其创新之处在于避免了昂贵的人工标注和LLM生成CoT的固有缺陷，并直接从可信赖的程序执行中提取推理路径。这不仅提高了数据质量，还优化了推理效率，对LLM在复杂任务上的应用具有重要意义。"}}
{"id": "2506.10597", "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models", "authors": ["Xunguang Wang", "Zhenlan Ji", "Wenxuan Wang", "Zongjie Li", "Daoyuan Wu", "Shuai Wang"], "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10597v1", "AI": {"title_translation": "SoK：评估大型语言模型的越狱护栏", "tldr": "本文对大型语言模型（LLMs）的越狱护栏进行了首次全面的系统化知识（SoK）分析，提出了新的分类法和评估框架，以指导未来的研究和部署。", "motivation": "尽管大型语言模型（LLMs）取得了显著进展，但其部署暴露了关键漏洞，特别是绕过安全机制的越狱攻击。护栏作为外部防御机制，被认为是解决方案，但当前LLM护栏领域碎片化，缺乏统一的分类法和全面的评估框架。", "method": "本文作为一篇知识系统化（SoK）论文，提出了一个新颖的多维分类法，从六个关键维度对护栏进行分类；引入了一个安全-效率-实用性评估框架来评估其实际有效性；并通过广泛的分析和实验进行研究。", "result": "通过广泛的分析和实验，本文识别了现有护栏方法的优点和局限性，探讨了它们在不同攻击类型中的普适性，并提供了关于优化防御组合的见解。", "conclusion": "本文为未来的研究和开发提供了结构化的基础，旨在指导稳健的LLM护栏的原则性推进和部署。", "translation": "大型语言模型（LLMs）取得了显著进展，但它们的部署暴露了关键漏洞，特别是绕过安全机制的越狱攻击。护栏——监控和控制LLM交互的外部防御机制——已成为一种有前景的解决方案。然而，当前的LLM护栏领域碎片化，缺乏统一的分类法和全面的评估框架。在这篇知识系统化（SoK）论文中，我们首次对LLMs的越狱护栏进行了全面的分析。我们提出了一种新颖的多维分类法，从六个关键维度对护栏进行分类，并引入了一个安全-效率-实用性评估框架来评估其实际有效性。通过广泛的分析和实验，我们识别了现有护栏方法的优点和局限性，探讨了它们在攻击类型中的普适性，并提供了关于优化防御组合的见解。我们的工作为未来的研究和开发提供了结构化的基础，旨在指导稳健的LLM护栏的原则性推进和部署。代码可在https://github.com/xunguangwang/SoK4JailbreakGuardrails 获取。", "summary": "本文是首篇对大型语言模型（LLMs）越狱护栏进行全面分析的知识系统化（SoK）论文。针对当前LLM护栏领域碎片化、缺乏统一分类和评估框架的问题，作者提出了一个新颖的多维分类法和一套安全-效率-实用性评估框架。通过广泛的分析和实验，文章识别了现有护栏的优缺点及其普适性，并为优化防御组合提供了见解，旨在为未来LLM护栏的研究和部署奠定基础。", "keywords": "大型语言模型, 越狱攻击, 护栏, 安全, 知识系统化", "comments": "本文通过系统化知识（SoK）的方法，首次对LLM越狱护栏进行了全面的梳理和分析，填补了该领域缺乏统一分类和评估框架的空白。其提出的多维分类法和安全-效率-实用性评估框架具有创新性，为未来LLM安全防御的研究和部署提供了重要的结构化基础和指导，对于提升LLM的鲁棒性和安全性具有重要意义。"}}
{"id": "2506.10833", "title": "Evaluating Large Language Models on Non-Code Software Engineering Tasks", "authors": ["Fabian C. Peña", "Steffen Herbold"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode understanding and generation; however, their effectiveness on non-code\nSoftware Engineering (SE) tasks remains underexplored. We present the first\ncomprehensive benchmark, which we name `Software Engineering Language\nUnderstanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from\nidentifying whether a requirement is functional or non-functional to estimating\nthe effort and complexity of backlog items. SELU covers classification,\nregression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)\ntargets, with data drawn from diverse sources such as code repositories, issue\ntracking systems, and developer forums. We fine-tune 22 open-source LLMs,\nprompt two proprietary alternatives, and train two baselines. Performance is\nmeasured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and\ncompared via the Bayesian signed-rank test. Our results show that\nmoderate-scale decoder-only models consistently form a top-tier, exhibiting\nhigh mean performance and low across-task variance, while domain adaptation via\ncode-focused pre-training might yield only modest improvements. These insights\nguide model selection for non-code SE workflows and highlight directions for\nexpanding SELU to generative and design-oriented scenarios.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10833v1", "AI": {"title_translation": "评估大型语言模型在非代码软件工程任务中的表现", "tldr": "本文提出了首个综合基准SELU，用于评估大型语言模型在17项非代码软件工程任务上的表现。研究发现，中等规模的解码器模型表现最佳，而代码领域适应带来的提升有限。", "motivation": "大型语言模型在代码理解和生成方面表现出色，但它们在非代码软件工程任务中的有效性尚未得到充分探索。", "method": "研究构建了名为`Software Engineering Language Understanding' (SELU) 的综合基准，涵盖17项非代码任务，包括分类、回归、命名实体识别和掩码语言建模。数据来源于代码仓库、问题跟踪系统和开发者论坛。研究对22个开源LLM进行微调，提示了2个专有LLM，并训练了2个基线模型。性能通过F1-macro、SMAPE、F1-micro和准确率等指标衡量，并使用贝叶斯符号秩检验进行比较。", "result": "结果显示，中等规模的解码器模型始终处于顶尖水平，表现出高平均性能和低跨任务方差。通过代码导向的预训练进行领域适应可能只会带来适度的改进。", "conclusion": "这些发现为非代码软件工程工作流中的模型选择提供了指导，并为将SELU扩展到生成式和设计导向的场景指明了方向。", "translation": "大型语言模型（LLMs）在代码理解和生成方面展现出卓越的能力；然而，它们在非代码软件工程（SE）任务中的有效性仍未得到充分探索。我们提出了首个综合基准，我们称之为“软件工程语言理解”（SELU），用于评估LLMs在17项非代码任务上的表现，这些任务涵盖了从识别需求是功能性还是非功能性，到估算待办事项的工作量和复杂性。SELU涵盖了分类、回归、命名实体识别（NER）和掩码语言建模（MLM）目标，数据来源于代码仓库、问题跟踪系统和开发者论坛等多样化来源。我们对22个开源LLM进行了微调，提示了2个专有替代方案，并训练了2个基线模型。性能通过F1-macro、SMAPE、F1-micro和准确率等指标进行衡量，并通过贝叶斯符号秩检验进行比较。我们的结果表明，中等规模的仅解码器模型始终构成顶级梯队，表现出高平均性能和低跨任务方差，而通过代码导向的预训练进行的领域适应可能只会带来适度的改进。这些见解指导了非代码SE工作流中的模型选择，并突出了将SELU扩展到生成式和设计导向场景的方向。", "summary": "本文首次提出了一个名为SELU的综合基准，旨在评估大型语言模型在17项非代码软件工程任务上的性能。这些任务涵盖了分类、回归、命名实体识别和掩码语言建模等多种类型。研究通过微调和提示多种开源及专有LLM，并与基线模型进行比较，发现中等规模的解码器模型表现最佳，且代码领域的预训练对非代码任务的改进有限。这项工作为非代码SE任务的模型选择提供了指导，并为未来的研究方向提供了启示。", "keywords": "大型语言模型, 软件工程, 非代码任务, 基准测试, SELU", "comments": "本文的创新之处在于构建了首个针对非代码软件工程任务的综合评估基准SELU，填补了该领域研究的空白。其重要性在于为LLM在SE领域的应用提供了新的视角和评估工具，并指出了中等规模解码器模型在非代码任务上的潜力。研究结果对于指导实际的软件工程工作流中的模型选择具有重要价值。"}}
{"id": "2506.10884", "title": "Modeling Trust Dynamics in Robot-Assisted Delivery: Impact of Trust Repair Strategies", "authors": ["Dong Hae Mangalindan", "Karthik Kandikonda", "Ericka Rovira", "Vaibhav Srivastava"], "summary": "With increasing efficiency and reliability, autonomous systems are becoming\nvaluable assistants to humans in various tasks. In the context of\nrobot-assisted delivery, we investigate how robot performance and trust repair\nstrategies impact human trust. In this task, while handling a secondary task,\nhumans can choose to either send the robot to deliver autonomously or manually\ncontrol it. The trust repair strategies examined include short and long\nexplanations, apology and promise, and denial.\n  Using data from human participants, we model human behavior using an\nInput-Output Hidden Markov Model (IOHMM) to capture the dynamics of trust and\nhuman action probabilities. Our findings indicate that humans are more likely\nto deploy the robot autonomously when their trust is high. Furthermore, state\ntransition estimates show that long explanations are the most effective at\nrepairing trust following a failure, while denial is most effective at\npreventing trust loss.\n  We also demonstrate that the trust estimates generated by our model are\nisomorphic to self-reported trust values, making them interpretable. This model\nlays the groundwork for developing optimal policies that facilitate real-time\nadjustment of human trust in autonomous systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10884v1", "AI": {"title_translation": "机器人辅助配送中的信任动态建模：信任修复策略的影响", "tldr": "研究了机器人辅助配送中，机器人性能和信任修复策略如何影响人类信任。发现长解释最能修复信任，而否认最能防止信任丧失。", "motivation": "随着自主系统效率和可靠性的提高，它们在各种任务中成为人类的宝贵助手。本研究旨在探究在机器人辅助配送场景中，机器人性能和信任修复策略如何影响人类对机器人的信任。", "method": "通过人类参与者的数据，使用输入-输出隐马尔可夫模型（IOHMM）来建模人类行为，以捕捉信任动态和人类行动概率。研究了短解释、长解释、道歉与承诺以及否认等信任修复策略。", "result": "研究结果表明，当人类信任度高时，他们更倾向于自主部署机器人。状态转换估计显示，长解释在故障后最有效地修复信任，而否认在防止信任丧失方面最有效。模型生成的信任估计与自我报告的信任值同构。", "conclusion": "本研究为开发优化策略奠定了基础，这些策略有助于实时调整人类对自主系统的信任。", "translation": "随着效率和可靠性的提高，自主系统正成为人类在各种任务中的宝贵助手。在机器人辅助配送的背景下，我们研究了机器人性能和信任修复策略如何影响人类信任。在此任务中，人类在处理次要任务的同时，可以选择自主发送机器人进行配送或手动控制它。所检查的信任修复策略包括短解释和长解释、道歉与承诺以及否认。\n利用人类参与者的数据，我们使用输入-输出隐马尔可夫模型（IOHMM）来建模人类行为，以捕捉信任动态和人类行动概率。我们的发现表明，当人类信任度高时，他们更倾向于自主部署机器人。此外，状态转换估计显示，长解释在故障后最有效地修复信任，而否认在防止信任丧失方面最有效。\n我们还证明，模型生成的信任估计与自我报告的信任值同构，这使得它们具有可解释性。该模型为开发优化策略奠定了基础，这些策略有助于实时调整人类对自主系统的信任。", "summary": "本研究探讨了在机器人辅助配送场景中，机器人性能和不同信任修复策略（如解释、道歉、承诺和否认）如何影响人类信任。通过对人类参与者数据的输入-输出隐马尔可夫模型（IOHMM）分析，发现高信任度会促进自主部署。关键发现是长解释在信任修复方面最有效，而否认在防止信任丧失方面效果最佳。该模型生成的信任估计与自我报告值一致，为未来开发实时调整人类对自主系统信任的优化策略奠定了基础。", "keywords": "机器人辅助配送, 信任动态, 信任修复策略, 输入-输出隐马尔可夫模型, 人机协作", "comments": "本研究的创新之处在于利用IOHMM模型量化并分析了机器人辅助任务中人类信任的动态变化，并首次系统地比较了多种信任修复策略的效果。其重要性在于为设计更有效的人机协作系统提供了实证依据和模型基础，特别是在信任管理和修复方面。模型的信任估计与自我报告值的一致性也增强了其实用性和可解释性。"}}
{"id": "2506.10335", "title": "PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting", "authors": ["Lintao Xiang", "Hongpei Zheng", "Yating Huang", "Qijun Yang", "Hujun Yin"], "summary": "3D Gaussian splatting (3DGS) is an innovative rendering technique that\nsurpasses the neural radiance field (NeRF) in both rendering speed and visual\nquality by leveraging an explicit 3D scene representation. Existing 3DGS\napproaches require a large number of calibrated views to generate a consistent\nand complete scene representation. When input views are limited, 3DGS tends to\noverfit the training views, leading to noticeable degradation in rendering\nquality. To address this limitation, we propose a Point-wise Feature-Aware\nGaussian Splatting framework that enables real-time, high-quality rendering\nfrom sparse training views. Specifically, we first employ the latest stereo\nfoundation model to estimate accurate camera poses and reconstruct a dense\npoint cloud for Gaussian initialization. We then encode the colour attributes\nof each 3D Gaussian by sampling and aggregating multiscale 2D appearance\nfeatures from sparse inputs. To enhance point-wise appearance representation,\nwe design a point interaction network based on a self-attention mechanism,\nallowing each Gaussian point to interact with its nearest neighbors. These\nenriched features are subsequently decoded into Gaussian parameters through two\nlightweight multi-layer perceptrons (MLPs) for final rendering. Extensive\nexperiments on diverse benchmarks demonstrate that our method significantly\noutperforms NeRF-based approaches and achieves competitive performance under\nfew-shot settings compared to the state-of-the-art 3DGS methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10335v1", "AI": {"title_translation": "PointGS：点注意力感知的高斯溅射稀疏视图合成", "tldr": "PointGS 提出了一种基于点注意力机制的高斯溅射框架，能够从稀疏训练视图生成高质量的实时渲染，解决了现有 3DGS 在有限视图下过拟合的问题。", "motivation": "现有的 3D Gaussian Splatting (3DGS) 方法需要大量校准视图才能生成一致且完整的场景表示。当输入视图有限时，3DGS 容易对训练视图过拟合，导致渲染质量显著下降。", "method": "本文提出了一种点式特征感知的高斯溅射框架 PointGS。具体地，首先利用最新的立体基础模型估计准确的相机姿态并重建密集点云用于高斯初始化。然后，通过采样和聚合稀疏输入中的多尺度 2D 外观特征来编码每个 3D 高斯的颜色属性。为增强点式外观表示，设计了一个基于自注意力机制的点交互网络，使每个高斯点与其最近邻居进行交互。这些丰富特征随后通过两个轻量级多层感知器（MLPs）解码为高斯参数，用于最终渲染。", "result": "在各种基准上的广泛实验表明，本文方法显著优于基于 NeRF 的方法，并且在少样本设置下与最先进的 3DGS 方法相比，取得了具有竞争力的性能。", "conclusion": "本文提出的 PointGS 框架通过引入点注意力机制和多尺度特征聚合，有效解决了 3DGS 在稀疏视图下的过拟合问题，实现了高质量的实时渲染，并在性能上超越了 NeRF 方法并与现有最先进的 3DGS 方法持平。", "translation": "3D 高斯溅射 (3DGS) 是一种创新的渲染技术，通过利用显式的 3D 场景表示，在渲染速度和视觉质量上都超越了神经辐射场 (NeRF)。现有的 3DGS 方法需要大量校准视图来生成一致且完整的场景表示。当输入视图有限时，3DGS 倾向于对训练视图过拟合，导致渲染质量显著下降。为了解决这一限制，我们提出了一种点式特征感知的高斯溅射框架，该框架能够从稀疏训练视图实现实时、高质量的渲染。具体来说，我们首先采用最新的立体基础模型来估计准确的相机姿态并重建密集点云用于高斯初始化。然后，我们通过采样和聚合稀疏输入中的多尺度 2D 外观特征来编码每个 3D 高斯的颜色属性。为了增强点式外观表示，我们设计了一个基于自注意力机制的点交互网络，允许每个高斯点与其最近邻居进行交互。这些丰富特征随后通过两个轻量级多层感知器 (MLPs) 解码为高斯参数，用于最终渲染。在各种基准上的广泛实验表明，我们的方法显著优于基于 NeRF 的方法，并且在少样本设置下与最先进的 3DGS 方法相比，取得了具有竞争力的性能。", "summary": "本文提出了 PointGS，一个点式特征感知的高斯溅射框架，旨在解决 3DGS 在稀疏视图下过拟合导致渲染质量下降的问题。该方法首先利用立体基础模型进行相机姿态估计和点云初始化，然后通过聚合多尺度 2D 特征编码高斯颜色属性。核心创新在于引入基于自注意力机制的点交互网络，以增强点式外观表示。实验证明，PointGS 在少样本设置下能实现高质量实时渲染，性能优于 NeRF 类方法并与现有先进 3DGS 方法相当。", "keywords": "3D 高斯溅射, 稀疏视图合成, 自注意力, 实时渲染, 少样本学习", "comments": "本文的创新点在于将点式特征感知和自注意力机制引入 3DGS 框架，有效解决了其在稀疏视图下容易过拟合的挑战，显著提升了 3DGS 在少样本条件下的适用性和渲染质量。这对于实际应用中难以获取大量视图的场景具有重要意义，是 3DGS 技术在鲁棒性方面的一大进步。"}}
{"id": "2506.10177", "title": "Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models", "authors": ["Defang Chen", "Zhenyu Zhou", "Can Wang", "Siwei Lyu"], "summary": "Diffusion-based generative models employ stochastic differential equations\n(SDEs) and their equivalent probability flow ordinary differential equations\n(ODEs) to establish a smooth transformation between complex high-dimensional\ndata distributions and tractable prior distributions. In this paper, we reveal\na striking geometric regularity in the deterministic sampling dynamics: each\nsimulated sampling trajectory lies within an extremely low-dimensional\nsubspace, and all trajectories exhibit an almost identical ''boomerang'' shape,\nregardless of the model architecture, applied conditions, or generated content.\nWe characterize several intriguing properties of these trajectories,\nparticularly under closed-form solutions based on kernel-estimated data\nmodeling. We also demonstrate a practical application of the discovered\ntrajectory regularity by proposing a dynamic programming-based scheme to better\nalign the sampling time schedule with the underlying trajectory structure. This\nsimple strategy requires minimal modification to existing ODE-based numerical\nsolvers, incurs negligible computational overhead, and achieves superior image\ngeneration performance, especially in regions with only $5 \\sim 10$ function\nevaluations.", "comment": "50 pages. The short version appeared in ICML 2024. arXiv admin note:\n  substantial text overlap with arXiv:2405.11326", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10177v1", "AI": {"title_translation": "扩散生成模型确定性采样中的几何规律性", "tldr": "扩散模型的确定性采样轨迹具有惊人的几何规律性，它们位于极低维子空间并呈“回旋镖”形，利用此规律可改进采样调度，显著提升图像生成性能，尤其在低函数评估次数下。", "motivation": "扩散生成模型在复杂高维数据分布与可处理先验分布之间建立平滑变换，但其确定性采样动态的内在几何规律性尚不明确。本研究旨在揭示并利用这些规律性以提升采样效率和性能。", "method": "本文通过观察和分析扩散模型确定性采样动态，揭示了采样轨迹的几何规律性，即轨迹位于极低维子空间并呈现“回旋镖”形状。研究了这些轨迹在基于核估计数据建模下的闭合形式解的特性。在此基础上，提出了一种基于动态规划的方案，以更好地调整采样时间表与轨迹结构对齐，从而改进现有基于ODE的数值求解器。", "result": "发现扩散模型确定性采样轨迹具有显著的几何规律性，即它们位于极低维子空间且呈现几乎相同的“回旋镖”形状。基于此规律提出的动态规划方案，对现有求解器修改量小，计算开销可忽略不计，并在图像生成性能上取得卓越表现，尤其是在仅有5到10次函数评估的区域。", "conclusion": "扩散模型确定性采样轨迹的几何规律性是一个普遍存在的现象，利用这一发现可以开发出高效且高性能的采样策略，从而显著提升扩散模型在有限计算资源下的生成能力。", "translation": "扩散生成模型利用随机微分方程（SDEs）及其等效的概率流常微分方程（ODEs）在复杂高维数据分布和易处理的先验分布之间建立平滑变换。在本文中，我们揭示了确定性采样动态中一个惊人的几何规律：每个模拟采样轨迹都位于一个极低维的子空间内，并且所有轨迹都呈现出几乎相同的“回旋镖”形状，无论模型架构、应用条件或生成内容如何。我们描述了这些轨迹的几个有趣的特性，特别是在基于核估计数据建模的闭合形式解下。我们还展示了所发现的轨迹规律性的一个实际应用，通过提出一种基于动态规划的方案，以更好地将采样时间表与底层轨迹结构对齐。这种简单的策略对现有基于ODE的数值求解器只需极小的修改，计算开销可忽略不计，并实现了卓越的图像生成性能，尤其是在只有5到10次函数评估的区域。", "summary": "本文揭示了扩散生成模型确定性采样动态中普遍存在的几何规律性：所有采样轨迹都位于极低维子空间并呈现一致的“回旋镖”形状。研究了这些轨迹的特性，并基于此规律提出了一种动态规划方案，用于优化采样时间表。该方案对现有求解器修改小、计算开销低，并在低函数评估次数下显著提升了图像生成性能。", "keywords": "扩散模型, 确定性采样, 几何规律性, 低维子空间, 动态规划", "comments": "这项研究通过揭示扩散模型确定性采样轨迹中普遍存在的几何规律性，为理解和优化扩散模型提供了新的视角。其创新点在于发现并量化了“低维子空间”和“回旋镖”形状这一惊人的特性。更重要的是，它将这一理论发现转化为实际应用，通过简单的动态规划方案显著提升了采样效率和图像生成质量，尤其是在计算资源受限（低函数评估次数）的场景下，这对于实际部署具有重要意义。"}}
{"id": "2506.10753", "title": "Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering", "authors": ["Adam Ishay", "Zhun Yang", "Joohyung Lee", "Ilgu Kang", "Dongjae Lim"], "summary": "Causal and temporal reasoning about video dynamics is a challenging problem.\nWhile neuro-symbolic models that combine symbolic reasoning with neural-based\nperception and prediction have shown promise, they exhibit limitations,\nespecially in answering counterfactual questions. This paper introduces a\nmethod to enhance a neuro-symbolic model for counterfactual reasoning,\nleveraging symbolic reasoning about causal relations among events. We define\nthe notion of a causal graph to represent such relations and use Answer Set\nProgramming (ASP), a declarative logic programming method, to find how to\ncoordinate perception and simulation modules. We validate the effectiveness of\nour approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves\nstate-of-the-art performance on the CLEVRER challenge, significantly\noutperforming existing models. In the case of the CRAFT benchmark, we leverage\na large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a\ndynamics simulator. Our findings show that this method can further improve its\nperformance on counterfactual questions by providing alternative prompts\ninstructed by symbolic causal reasoning.", "comment": "In Proceedings the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV 2024)", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10753v1", "AI": {"title_translation": "在模拟前思考：符号推理编排神经计算以回答反事实问题", "tldr": "本文提出一种增强神经-符号模型处理视频动态中反事实问题推理的方法，通过符号推理（因果图、ASP）协调神经计算，在CLEVRER上达到SOTA，并结合大型语言模型（LLMs）在CRAFT上提升性能。", "motivation": "视频动态的因果和时间推理是一个挑战性问题。现有的神经-符号模型在回答反事实问题方面存在局限性。", "method": "本文提出一种增强神经-符号模型进行反事实推理的方法，利用事件间因果关系的符号推理。定义因果图表示关系，并使用答案集编程（ASP）协调感知和模拟模块。在CRAFT基准测试中，利用GPT-3.5和GPT-4等大型预训练语言模型作为动态模拟器代理，并通过符号因果推理指导的替代提示进一步提高性能。", "result": "在CLEVRER挑战赛中取得了最先进的性能，显著优于现有模型。在CRAFT基准测试中，通过结合大型预训练语言模型和符号因果推理指导的提示，进一步提高了反事实问题的性能。", "conclusion": "通过将符号推理（因果图、ASP）与神经计算有效结合，本文提出的方法显著增强了神经-符号模型在反事实问题回答方面的能力，并在基准测试中展现出卓越性能，同时揭示了符号推理与大型语言模型结合的潜力。", "translation": "视频动态的因果和时间推理是一个具有挑战性的问题。虽然结合了符号推理与基于神经的感知和预测的神经-符号模型已显示出前景，但它们存在局限性，尤其是在回答反事实问题方面。本文介绍了一种增强神经-符号模型进行反事实推理的方法，该方法利用事件之间因果关系的符号推理。我们定义了因果图的概念来表示此类关系，并使用答案集编程（ASP），一种声明性逻辑编程方法，来寻找如何协调感知和模拟模块。我们在两个基准测试CLEVRER和CRAFT上验证了我们方法的有效性。我们的增强在CLEVRER挑战赛中取得了最先进的性能，显著优于现有模型。在CRAFT基准测试中，我们利用大型预训练语言模型，例如GPT-3.5和GPT-4，作为动态模拟器的代理。我们的发现表明，通过提供由符号因果推理指导的替代提示，这种方法可以进一步提高其在反事实问题上的性能。", "summary": "本文提出了一种增强神经-符号模型处理视频动态中反事实问题推理的方法。该方法利用符号推理来表示事件间的因果关系，并通过定义因果图和使用答案集编程（ASP）来协调神经感知与模拟模块。在CLEVRER基准测试中，该方法取得了最先进的性能。此外，在CRAFT基准测试中，通过将大型预训练语言模型（如GPT-3.5和GPT-4）作为动态模拟器，并结合符号因果推理指导的提示，进一步提升了反事实问题回答的性能。", "keywords": "神经-符号AI, 反事实推理, 符号推理, 因果图, 答案集编程", "comments": "本文创新性地将符号推理（因果图、ASP）与神经计算相结合，解决了视频动态中反事实推理的挑战性问题。其优势在于通过明确的符号逻辑来编排神经模块，这有助于克服纯神经或现有神经-符号方法在处理复杂因果查询方面的局限性。在CRAFT基准测试中使用大型语言模型作为动态模拟器进一步突出了其新颖的集成策略，展示了利用结构化符号知识指导强大的预训练模型的潜力。"}}
{"id": "2506.10620", "title": "Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation", "authors": ["Stefano Longari", "Paolo Cerracchio", "Michele Carminati", "Stefano Zanero"], "summary": "The security of modern vehicles has become increasingly important, with the\ncontroller area network (CAN) bus serving as a critical communication backbone\nfor various Electronic Control Units (ECUs). The absence of robust security\nmeasures in CAN, coupled with the increasing connectivity of vehicles, makes\nthem susceptible to cyberattacks. While intrusion detection systems (IDSs) have\nbeen developed to counter such threats, they are not foolproof. Adversarial\nattacks, particularly evasion attacks, can manipulate inputs to bypass\ndetection by IDSs. This paper extends our previous work by investigating the\nfeasibility and impact of gradient-based adversarial attacks performed with\ndifferent degrees of knowledge against automotive IDSs. We consider three\nscenarios: white-box (attacker with full system knowledge), grey-box (partial\nsystem knowledge), and the more realistic black-box (no knowledge of the IDS'\ninternal workings or data). We evaluate the effectiveness of the proposed\nattacks against state-of-the-art IDSs on two publicly available datasets.\nAdditionally, we study effect of the adversarial perturbation on the attack\nimpact and evaluate real-time feasibility by precomputing evasive payloads for\ntimed injection based on bus traffic. Our results demonstrate that, besides\nattacks being challenging due to the automotive domain constraints, their\neffectiveness is strongly dependent on the dataset quality, the target IDS, and\nthe attacker's degree of knowledge.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10620v1", "AI": {"title_translation": "评估车载入侵检测系统对对抗性操纵的弹性", "tldr": "本文研究了针对车载入侵检测系统(IDS)的梯度对抗性攻击的可行性和影响，涵盖了白盒、灰盒和黑盒场景，并评估了攻击效果受数据集质量、目标IDS和攻击者知识程度的影响。", "motivation": "现代车辆的安全性日益重要，CAN总线作为关键通信骨干缺乏强大的安全措施，易受网络攻击。虽然IDS被开发来对抗这些威胁，但它们并非万无一失，对抗性攻击（特别是规避攻击）可以操纵输入绕过IDS检测。", "method": "本文扩展了之前的工作，研究了在不同知识程度下（白盒、灰盒、黑盒）对车载IDS进行基于梯度的对抗性攻击的可行性和影响。研究评估了攻击对最先进IDS的有效性，使用了两个公开数据集。此外，还研究了对抗性扰动对攻击影响的影响，并通过预计算规避性有效载荷以进行基于总线流量的定时注入来评估实时可行性。", "result": "结果表明，除了由于汽车领域限制导致攻击具有挑战性外，其有效性强烈依赖于数据集质量、目标IDS和攻击者的知识程度。", "conclusion": "对抗性攻击对车载入侵检测系统是有效的，但其成功率受多种因素影响，包括攻击者的知识程度和数据的质量。实时规避攻击的可行性也得到了评估。", "translation": "现代车辆的安全性日益重要，控制器局域网（CAN）总线作为各种电子控制单元（ECU）的关键通信骨干。CAN中缺乏强大的安全措施，加上车辆日益增长的连接性，使得它们容易受到网络攻击。虽然入侵检测系统（IDS）已被开发来对抗此类威胁，但它们并非万无一失。对抗性攻击，特别是规避攻击，可以操纵输入以绕过IDS的检测。本文扩展了我们之前的工作，通过调查在不同知识程度下对车载IDS进行的基于梯度的对抗性攻击的可行性和影响。我们考虑了三种场景：白盒（攻击者拥有完整的系统知识）、灰盒（部分系统知识）和更现实的黑盒（不知道IDS的内部工作原理或数据）。我们评估了所提出的攻击在两个公开可用数据集上对最先进IDS的有效性。此外，我们研究了对抗性扰动对攻击影响的影响，并通过预计算规避性有效载荷以进行基于总线流量的定时注入来评估实时可行性。我们的结果表明，除了由于汽车领域限制导致攻击具有挑战性外，它们的有效性强烈依赖于数据集质量、目标IDS和攻击者的知识程度。", "summary": "本文研究了针对车载入侵检测系统（IDS）的对抗性攻击的弹性，特别是基于梯度的规避攻击。研究评估了白盒、灰盒和黑盒三种攻击场景下攻击对现有IDS的有效性，并分析了攻击效果与数据集质量、目标IDS及攻击者知识程度的关系。同时，论文还探讨了实时规避攻击的可行性。", "keywords": "车载入侵检测系统, 对抗性攻击, CAN总线, 网络安全, 规避攻击", "comments": "这篇论文探讨了车载网络安全中的一个关键且日益重要的问题：IDS对对抗性攻击的脆弱性。其创新点在于系统地研究了不同知识程度下（白盒、灰盒、黑盒）的梯度对抗性攻击，并评估了其对实际车载IDS的影响。论文强调了攻击效果受多种因素制约，为未来提升车载IDS的鲁棒性提供了重要见解，尤其是在数据质量和IDS设计方面。"}}
{"id": "2506.10869", "title": "MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework", "authors": ["Quinn Thibeault", "Giulia Pedrielli"], "summary": "Simulation is a foundational tool for the analysis and testing of\ncyber-physical systems (CPS), underpinning activities such as algorithm\ndevelopment, runtime monitoring, and system verification. As CPS grow in\ncomplexity and scale, particularly in safety-critical and learning-enabled\nsettings, accurate analysis and synthesis increasingly rely on the rapid use of\nsimulation experiments. Because CPS inherently integrate hardware, software,\nand physical processes, simulation platforms must support co-simulation of\nheterogeneous components at varying levels of fidelity. Despite recent advances\nin high-fidelity modeling of hardware, firmware, and physics, co-simulation in\ndiverse environments remains challenging. These limitations hinder the\ndevelopment of reusable benchmarks and impede the use of simulation for\nautomated and comparative evaluation.\n  Existing simulation tools often rely on rigid configurations, lack automation\nsupport, and present obstacles to portability and modularity. Many are\nconfigured through static text files or impose constraints on how simulation\ncomponents are represented and connected, making it difficult to flexibly\ncompose systems or integrate components across platforms.\n  To address these challenges, we introduce MultiCoSim, a Python-based\nsimulation framework that enables users to define, compose, and configure\nsimulation components programmatically. MultiCoSim supports distributed,\ncomponent-based co-simulation and allows seamless substitution and\nreconfiguration of components. We demonstrate the flexibility of MultiCoSim\nthrough case studies that include co-simulations involving custom\nautomaton-based controllers, as well as integration with off-the-shelf\nplatforms like the PX4 autopilot for aerial robotics. These examples highlight\nMultiCoSim's capability to streamline CPS simulation pipelines for research and\ndevelopment.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10869v1", "AI": {"title_translation": "MultiCoSim：一个基于Python的多精度协同仿真框架", "tldr": "MultiCoSim是一个Python框架，解决了复杂网络物理系统（CPS）协同仿真中的配置僵化、自动化不足和可移植性差的问题，它支持分布式、基于组件的协同仿真，并允许灵活的组件替换和重配置。", "motivation": "现有仿真工具在复杂且安全关键的网络物理系统（CPS）协同仿真中面临挑战，表现为配置僵化、缺乏自动化支持以及可移植性和模块化障碍。这些限制阻碍了可复用基准的开发和仿真在自动化评估中的应用。", "method": "我们引入了MultiCoSim，一个基于Python的仿真框架，允许用户以编程方式定义、组合和配置仿真组件。MultiCoSim支持分布式、基于组件的协同仿真，并允许组件的无缝替换和重配置。", "result": "通过涉及自定义基于自动机的控制器以及与PX4无人机自动驾驶仪等现成平台集成的案例研究，MultiCoSim展示了其灵活性，并能够简化CPS仿真流程，用于研究和开发。", "conclusion": "MultiCoSim通过提供一个灵活的、基于Python的框架，显著简化了复杂网络物理系统（CPS）的协同仿真流程，解决了现有工具的局限性。", "translation": "仿真作为网络物理系统（CPS）分析和测试的基础工具，支撑着算法开发、运行时监控和系统验证等活动。随着CPS复杂性和规模的增长，特别是在安全关键和支持学习的环境中，准确的分析和综合越来越依赖于仿真实验的快速使用。由于CPS天生集成了硬件、软件和物理过程，仿真平台必须支持异构组件在不同精度水平上的协同仿真。尽管硬件、固件和物理的高精度建模取得了最新进展，但在不同环境中的协同仿真仍然具有挑战性。这些限制阻碍了可重用基准的开发，并阻碍了仿真在自动化和比较评估中的使用。\n现有仿真工具通常依赖于僵化的配置，缺乏自动化支持，并存在可移植性和模块化障碍。许多工具通过静态文本文件进行配置，或对仿真组件的表示和连接方式施加限制，使得灵活地组合系统或跨平台集成组件变得困难。\n为了解决这些挑战，我们引入了MultiCoSim，一个基于Python的仿真框架，使用户能够以编程方式定义、组合和配置仿真组件。MultiCoCoSim支持分布式、基于组件的协同仿真，并允许组件的无缝替换和重配置。我们通过案例研究展示了MultiCoSim的灵活性，其中包括涉及自定义基于自动机的控制器以及与PX4无人机自动驾驶仪等现成平台集成的协同仿真。这些示例突出了MultiCoSim简化CPS仿真流程以进行研究和开发的能力。", "summary": "MultiCoSim是一个基于Python的协同仿真框架，旨在解决复杂网络物理系统（CPS）仿真中现有工具的局限性，如配置僵化和缺乏灵活性。它允许用户以编程方式定义、组合和配置仿真组件，支持分布式、基于组件的协同仿真以及组件的无缝替换和重配置。通过案例研究，包括与PX4自动驾驶仪的集成，MultiCoSim展示了其在简化CPS仿真流程方面的能力。", "keywords": "协同仿真, 网络物理系统, Python, 多精度, 仿真框架", "comments": "MultiCoSim的创新之处在于其基于Python的编程接口，这使得协同仿真组件的定义、组合和配置变得高度灵活和自动化。它解决了现有工具在可移植性、模块化和自动化方面的痛点，对于复杂网络物理系统（CPS）的研究和开发具有重要意义。通过支持分布式和多精度协同仿真，该框架有望成为未来CPS验证和测试的关键工具。"}}
{"id": "2506.10923", "title": "Vib2Move: In-Hand Object Reconfiguration via Fingertip Micro-Vibrations", "authors": ["Xili Yi", "Nima Fazeli"], "summary": "We introduce Vib2Move, a novel approach for in-hand object reconfiguration\nthat uses fingertip micro-vibrations and gravity to precisely reposition planar\nobjects. Our framework comprises three key innovations. First, we design a\nvibration-based actuator that dynamically modulates the effective finger-object\nfriction coefficient, effectively emulating changes in gripping force. Second,\nwe derive a sliding motion model for objects clamped in a parallel gripper with\ntwo symmetric, variable-friction contact patches. Third, we propose a motion\nplanner that coordinates end-effector finger trajectories and fingertip\nvibrations to achieve the desired object pose. In real-world trials, Vib2Move\nconsistently yields final positioning errors below 6 mm, demonstrating\nreliable, high-precision manipulation across a variety of planar objects. For\nmore results and information, please visit https://vib2move.github.io.", "comment": "11 pages, 12 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10923v1", "AI": {"title_translation": "Vib2Move：通过指尖微振动实现手内物体重构", "tldr": "Vib2Move是一种利用指尖微振动和重力精确重新定位手内平面物体的新方法，实现了高精度操作。", "motivation": "旨在通过一种新颖的方法，利用指尖微振动和重力来精确重新定位手内平面物体。", "method": "该方法名为Vib2Move，包含三项关键创新：1) 设计了一种基于振动的执行器，动态调制手指与物体的有效摩擦系数，以模拟抓取力的变化；2) 推导了平行夹持器中物体在两个对称、可变摩擦接触点下的滑动运动模型；3) 提出了一种运动规划器，协调末端执行器手指轨迹和指尖振动，以实现所需的物体姿态。", "result": "在实际试验中，Vib2Move的最终定位误差始终低于6毫米，展示了对各种平面物体可靠、高精度的操作。", "conclusion": "Vib2Move通过利用指尖微振动和重力，能够实现对平面物体可靠且高精度的手内重构。", "translation": "我们引入了Vib2Move，这是一种新颖的手内物体重构方法，它利用指尖微振动和重力来精确重新定位平面物体。我们的框架包含三项关键创新。首先，我们设计了一种基于振动的执行器，可以动态调节有效的手指-物体摩擦系数，有效模拟抓取力的变化。其次，我们推导了在具有两个对称、可变摩擦接触点的平行夹持器中夹持物体的滑动运动模型。第三，我们提出了一种运动规划器，协调末端执行器手指轨迹和指尖振动，以实现所需的物体姿态。在实际试验中，Vib2Move始终产生低于6毫米的最终定位误差，展示了对各种平面物体可靠、高精度的操作。如需更多结果和信息，请访问 https://vib2move.github.io。", "summary": "Vib2Move是一种创新的手内物体重构技术，利用指尖微振动和重力精确重新定位平面物体。该方法通过设计新型振动执行器来模拟抓取力变化，建立了物体滑动运动模型，并开发了协调手指轨迹和振动的运动规划器。实验结果表明，Vib2Move能将最终定位误差控制在6毫米以内，实现了对多种平面物体可靠且高精度的操作。", "keywords": "手内操作, 微振动, 物体重构, 摩擦调制, 运动规划", "comments": "该论文提出了一种新颖且实用的手内物体重构方法，其创新点在于巧妙地利用指尖微振动来动态调节摩擦力，并结合运动规划实现精确操控。这种方法有望在机器人抓取和精细操作领域提供新的解决方案，特别是在空间受限或需要高精度调整物体姿态的场景中。"}}
{"id": "2506.10337", "title": "GeoCAD: Local Geometry-Controllable CAD Generation", "authors": ["Zhanwei Zhang", "Kaiyuan Liu", "Junjie Liu", "Wenxiao Wang", "Binbin Lin", "Liang Xie", "Chen Shen", "Deng Cai"], "summary": "Local geometry-controllable computer-aided design (CAD) generation aims to\nmodify local parts of CAD models automatically, enhancing design efficiency. It\nalso ensures that the shapes of newly generated local parts follow\nuser-specific geometric instructions (e.g., an isosceles right triangle or a\nrectangle with one corner cut off). However, existing methods encounter\nchallenges in achieving this goal. Specifically, they either lack the ability\nto follow textual instructions or are unable to focus on the local parts. To\naddress this limitation, we introduce GeoCAD, a user-friendly and local\ngeometry-controllable CAD generation method. Specifically, we first propose a\ncomplementary captioning strategy to generate geometric instructions for local\nparts. This strategy involves vertex-based and VLLM-based captioning for\nsystematically annotating simple and complex parts, respectively. In this way,\nwe caption $\\sim$221k different local parts in total. In the training stage,\ngiven a CAD model, we randomly mask a local part. Then, using its geometric\ninstruction and the remaining parts as input, we prompt large language models\n(LLMs) to predict the masked part. During inference, users can specify any\nlocal part for modification while adhering to a variety of predefined geometric\ninstructions. Extensive experiments demonstrate the effectiveness of GeoCAD in\ngeneration quality, validity and text-to-CAD consistency. Code will be\navailable at https://github.com/Zhanwei-Z/GeoCAD.", "comment": "18 pages, 12 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10337v1", "AI": {"title_translation": "GeoCAD：局部几何可控的CAD生成", "tldr": "GeoCAD是一种新的CAD生成方法，它允许用户通过文本指令精确控制CAD模型的局部几何形状，解决了现有方法无法兼顾局部控制和文本指令的问题。", "motivation": "现有的计算机辅助设计（CAD）生成方法在实现局部几何可控方面面临挑战，具体表现为它们要么无法遵循文本指令，要么无法聚焦于局部部件。这限制了设计效率和用户对新生成局部部件形状的精确控制（例如，生成等腰直角三角形或切角矩形）。", "method": "我们提出了GeoCAD方法。首先，引入了一种互补的标注策略来为局部部件生成几何指令，该策略包括基于顶点的标注（针对简单部件）和基于VLLM的标注（针对复杂部件），共标注了约22.1万个不同的局部部件。在训练阶段，给定一个CAD模型，随机遮蔽一个局部部件，然后利用其几何指令和剩余部件作为输入，提示大型语言模型（LLMs）预测被遮蔽的部分。在推理阶段，用户可以指定任何局部部件进行修改，并遵循各种预定义的几何指令。", "result": "大量实验证明了GeoCAD在生成质量、有效性和文本到CAD一致性方面的有效性。", "conclusion": "GeoCAD是一种用户友好且局部几何可控的CAD生成方法，通过创新的标注策略和LLM的应用，有效解决了现有方法在局部几何控制和文本指令遵循方面的局限性。", "translation": "局部几何可控的计算机辅助设计（CAD）生成旨在自动修改CAD模型的局部部件，从而提高设计效率。它还确保新生成的局部部件的形状遵循用户特定的几何指令（例如，等腰直角三角形或一个角被切掉的矩形）。然而，现有方法在实现这一目标时遇到了挑战。具体来说，它们要么缺乏遵循文本指令的能力，要么无法专注于局部部件。为了解决这一限制，我们引入了GeoCAD，一种用户友好且局部几何可控的CAD生成方法。具体而言，我们首先提出了一种互补的标注策略来生成局部部件的几何指令。该策略涉及基于顶点的标注和基于VLLM的标注，分别用于系统地标注简单和复杂部件。通过这种方式，我们总共标注了约22.1万个不同的局部部件。在训练阶段，给定一个CAD模型，我们随机遮蔽一个局部部件。然后，使用其几何指令和剩余部件作为输入，我们提示大型语言模型（LLMs）预测被遮蔽的部分。在推理过程中，用户可以指定任何局部部件进行修改，同时遵守各种预定义的几何指令。大量的实验证明了GeoCAD在生成质量、有效性和文本到CAD一致性方面的有效性。代码将在https://github.com/Zhanwei-Z/GeoCAD上提供。", "summary": "GeoCAD是一种新颖的局部几何可控CAD生成方法，旨在通过文本指令自动修改CAD模型的局部区域。它通过提出一种互补的标注策略（包括基于顶点和VLLM的标注）来为局部部件生成几何指令，并利用大型语言模型（LLMs）根据这些指令和上下文预测被遮蔽或待修改的局部部件。该方法克服了现有技术无法兼顾局部控制和文本指令的限制，并通过实验证明了其在生成质量、有效性和文本到CAD一致性方面的卓越性能。", "keywords": "CAD生成, 局部几何控制, 文本到CAD, 大型语言模型, 几何指令", "comments": "GeoCAD的创新之处在于其结合了局部几何控制与大型语言模型（LLMs）的能力，使得CAD模型的局部修改能够通过自然语言指令实现。其提出的互补标注策略（结合了基于顶点和VLLM的标注）是其核心贡献之一，系统地解决了简单和复杂部件的指令生成问题。这极大地提升了CAD设计的自动化和灵活性，为用户提供了前所未有的精确控制。该方法在解决现有痛点方面具有重要意义，尤其是在文本到CAD一致性方面表现出色。"}}
{"id": "2506.10180", "title": "A Comparative Study of Machine Learning Techniques for Early Prediction of Diabetes", "authors": ["Mowafaq Salem Alzboon", "Mohammad Al-Batah", "Muhyeeddin Alqaraleh", "Ahmad Abuashour", "Ahmad Fuad Bader"], "summary": "In many nations, diabetes is becoming a significant health problem, and early\nidentification and control are crucial. Using machine learning algorithms to\npredict diabetes has yielded encouraging results. Using the Pima Indians\nDiabetes dataset, this study attempts to evaluate the efficacy of several\nmachine-learning methods for diabetes prediction. The collection includes\ninformation on 768 patients, such as their ages, BMIs, and glucose levels. The\ntechniques assessed are Logistic Regression, Decision Tree, Random Forest,\nk-Nearest Neighbors, Naive Bayes, Support Vector Machine, Gradient Boosting,\nand Neural Network. The findings indicate that the Neural Network algorithm\nperformed the best, with an accuracy of 78.57 percent, followed by the Random\nForest method, with an accuracy of 76.30 percent. The study implies that\nmachine learning algorithms can aid diabetes prediction and be an efficient\nearly detection tool.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10180v1", "AI": {"title_translation": "机器学习技术在糖尿病早期预测中的比较研究", "tldr": "本研究比较了多种机器学习算法在Pima印第安人糖尿病数据集上的表现，发现神经网络在糖尿病早期预测中表现最佳。", "motivation": "糖尿病是许多国家的重大健康问题，早期识别和控制至关重要。利用机器学习算法预测糖尿病已取得令人鼓舞的结果，因此本研究旨在评估不同机器学习方法在糖尿病预测中的有效性。", "method": "本研究使用Pima印第安人糖尿病数据集（包含768名患者的年龄、BMI和血糖水平等信息），评估了逻辑回归、决策树、随机森林、k近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络等机器学习技术。", "result": "研究结果表明，神经网络算法表现最佳，准确率为78.57%；其次是随机森林方法，准确率为76.30%。", "conclusion": "该研究表明，机器学习算法可以帮助预测糖尿病，并成为有效的早期检测工具。", "translation": "在许多国家，糖尿病正成为一个重大的健康问题，早期识别和控制至关重要。利用机器学习算法预测糖尿病已取得令人鼓舞的结果。本研究旨在评估多种机器学习方法在糖尿病预测中的有效性，使用了Pima印第安人糖尿病数据集。该数据集包含768名患者的信息，如他们的年龄、BMI和血糖水平。评估的技术包括逻辑回归、决策树、随机森林、k近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络。研究结果表明，神经网络算法表现最佳，准确率为78.57%，其次是随机森林方法，准确率为76.30%。该研究表明，机器学习算法可以帮助预测糖尿病，并成为有效的早期检测工具。", "summary": "本研究旨在比较多种机器学习算法（包括逻辑回归、决策树、随机森林、k近邻、朴素贝叶斯、支持向量机、梯度提升和神经网络）在Pima印第安人糖尿病数据集上对糖尿病早期预测的有效性。研究发现，神经网络表现最佳，准确率为78.57%，其次是随机森林，准确率为76.30%。结果表明机器学习算法是糖尿病早期检测的有效工具。", "keywords": "糖尿病预测, 机器学习, 早期检测, 神经网络, Pima印第安人数据集", "comments": "这项研究通过比较多种机器学习模型在特定数据集上的性能，为糖尿病早期预测提供了实证支持。其创新点在于对多种流行算法进行了直接对比，并指出了神经网络的优越性。重要性在于强调了机器学习在公共卫生领域，特别是疾病早期诊断中的潜力。局限性可能在于其结果仅基于一个特定数据集，可能不完全泛化到其他人群或数据特征。"}}
{"id": "2506.10764", "title": "OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems", "authors": ["Xiaozhe Li", "Jixuan Chen", "Xinyu Fang", "Shengyuan Ding", "Haodong Duan", "Qingwen Liu", "Kai Chen"], "summary": "Large Language Models (LLMs) have shown remarkable capabilities in solving\ndiverse tasks. However, their proficiency in iteratively optimizing complex\nsolutions through learning from previous feedback remains insufficiently\nexplored. To bridge this gap, we present OPT-BENCH, a comprehensive benchmark\ndesigned to evaluate LLM agents on large-scale search space optimization\nproblems. OPT-BENCH includes 20 real-world machine learning tasks sourced from\nKaggle and 10 classical NP problems, offering a diverse and challenging\nenvironment for assessing LLM agents on iterative reasoning and solution\nrefinement. To enable rigorous evaluation, we introduce OPT-Agent, an\nend-to-end optimization framework that emulates human reasoning when tackling\ncomplex problems by generating, validating, and iteratively improving solutions\nthrough leveraging historical feedback. Through extensive experiments on 9\nstate-of-the-art LLMs from 6 model families, we analyze the effects of\noptimization iterations, temperature settings, and model architectures on\nsolution quality and convergence. Our results demonstrate that incorporating\nhistorical context significantly enhances optimization performance across both\nML and NP tasks. All datasets, code, and evaluation tools are open-sourced to\npromote further research in advancing LLM-driven optimization and iterative\nreasoning. Project page:\n\\href{https://github.com/OliverLeeXZ/OPT-BENCH}{https://github.com/OliverLeeXZ/OPT-BENCH}.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10764v1", "AI": {"title_translation": "OPT-BENCH：评估大型语言模型智能体在大型搜索空间优化问题上的表现", "tldr": "OPT-BENCH是一个用于评估LLM智能体在大型搜索空间优化问题上迭代优化能力的基准测试和框架，实验表明结合历史上下文能显著提升优化性能。", "motivation": "大型语言模型（LLMs）在解决各种任务方面表现出卓越能力，但其通过学习历史反馈迭代优化复杂解决方案的能力尚未得到充分探索。", "method": "本文提出了OPT-BENCH，一个旨在评估LLM智能体在大型搜索空间优化问题上的综合基准测试。OPT-BENCH包含20个Kaggle上的真实世界机器学习任务和10个经典NP问题。为实现严格评估，本文还引入了OPT-Agent，一个端到端的优化框架，通过生成、验证和利用历史反馈迭代改进解决方案来模拟人类解决复杂问题的推理过程。研究团队对来自6个模型家族的9个最先进的LLM进行了广泛实验，分析了优化迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响。", "result": "实验结果表明，结合历史上下文显著增强了LLM智能体在机器学习和NP任务上的优化性能。", "conclusion": "结合历史上下文能够显著提升LLM智能体在大型搜索空间优化问题上的表现。", "translation": "大型语言模型（LLMs）在解决各种任务方面表现出卓越能力。然而，它们通过学习历史反馈迭代优化复杂解决方案的能力尚未得到充分探索。为了弥补这一空白，我们提出了OPT-BENCH，一个旨在评估LLM智能体在大型搜索空间优化问题上的综合基准测试。OPT-BENCH包括20个来自Kaggle的真实世界机器学习任务和10个经典NP问题，为评估LLM智能体在迭代推理和解决方案优化方面的能力提供了一个多样化且具有挑战性的环境。为了实现严格评估，我们引入了OPT-Agent，一个端到端的优化框架，它通过生成、验证和利用历史反馈迭代改进解决方案来模拟人类解决复杂问题的推理过程。通过对来自6个模型家族的9个最先进的LLM进行广泛实验，我们分析了优化迭代次数、温度设置和模型架构对解决方案质量和收敛性的影响。我们的结果表明，结合历史上下文显著增强了LLM在机器学习和NP任务上的优化性能。所有数据集、代码和评估工具均已开源，以促进LLM驱动优化和迭代推理方面的进一步研究。项目页面：https://github.com/OliverLeeXZ/OPT-BENCH。", "summary": "该论文介绍了OPT-BENCH，一个用于评估大型语言模型（LLM）智能体在大型搜索空间优化问题上迭代优化能力的综合基准测试。该基准包含真实世界的机器学习任务和经典NP问题。为实现严格评估，研究团队还提出了OPT-Agent，一个模拟人类推理过程的端到端优化框架。通过对多个LLM进行实验，研究发现结合历史上下文能显著提升LLM在优化任务上的表现。所有相关资源均已开源，以促进后续研究。", "keywords": "LLM Agent, 优化, 基准测试, 搜索空间, 迭代推理", "comments": "这篇论文的创新点在于提出了一个专门用于评估LLM智能体在迭代优化复杂问题方面能力的基准测试和框架。OPT-BENCH结合了真实世界的ML任务和经典NP问题，提供了多样化的评估环境。OPT-Agent通过模拟人类推理过程，为LLM的迭代优化提供了一个有效范式。研究结果强调了历史上下文在提升LLM优化性能中的关键作用，这对于LLM在复杂决策和问题解决领域的应用具有重要意义。同时，开放源代码和数据集的做法将极大促进该领域的研究进展。"}}
{"id": "2506.10406", "title": "PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier", "authors": ["Yuhua Jiang", "Yuwen Xiong", "Yufeng Yuan", "Chao Xin", "Wenyuan Xu", "Yu Yue", "Qianchuan Zhao", "Lin Yan"], "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks, yet they still struggle to reliably verify the\ncorrectness of their own outputs. Existing solutions to this verification\nchallenge often depend on separate verifier models or require multi-stage\nself-correction training pipelines, which limit scalability. In this paper, we\npropose Policy as Generative Verifier (PAG), a simple and effective framework\nthat empowers LLMs to self-correct by alternating between policy and verifier\nroles within a unified multi-turn reinforcement learning (RL) paradigm.\nDistinct from prior approaches that always generate a second attempt regardless\nof model confidence, PAG introduces a selective revision mechanism: the model\nrevises its answer only when its own generative verification step detects an\nerror. This verify-then-revise workflow not only alleviates model collapse but\nalso jointly enhances both reasoning and verification abilities. Extensive\nexperiments across diverse reasoning benchmarks highlight PAG's dual\nadvancements: as a policy, it enhances direct generation and self-correction\naccuracy; as a verifier, its self-verification outperforms self-consistency.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10406v1", "AI": {"title_translation": "PAG：基于生成式验证器策略的多轮强化LLM自校正", "tldr": "PAG是一个多轮强化学习框架，通过让LLM在策略和验证器角色之间交替，实现选择性自校正，提升推理和验证能力。", "motivation": "大型语言模型（LLMs）在复杂推理任务中表现出色，但难以可靠地验证自身输出的正确性。现有解决方案常依赖独立的验证模型或多阶段训练，限制了可扩展性。", "method": "提出PAG（Policy as Generative Verifier）框架，使LLMs在一个统一的多轮强化学习范式中，通过在策略和验证器角色之间交替进行自校正。PAG引入选择性修订机制，仅当生成式验证步骤检测到错误时才修订答案，形成“验证-然后-修订”的工作流。", "result": "该工作流不仅缓解了模型崩溃，还同时增强了推理和验证能力。在多样化的推理基准测试中，PAG作为策略提升了直接生成和自校正的准确性；作为验证器，其自验证表现优于自洽性。", "conclusion": "PAG框架通过其独特的生成式验证器策略和选择性修订机制，显著提升了LLMs的自校正能力，并同时增强了其推理和验证性能。", "translation": "大型语言模型（LLMs）在复杂推理任务中展现出令人印象深刻的能力，但它们仍然难以可靠地验证自身输出的正确性。现有解决这一验证挑战的方案通常依赖于单独的验证器模型或需要多阶段的自校正训练流程，这限制了可扩展性。在本文中，我们提出了“策略即生成式验证器”（PAG），一个简单有效的框架，它通过在统一的多轮强化学习（RL）范式中，让LLM在策略和验证器角色之间交替，从而使其能够进行自校正。与以往无论模型置信度如何都总是生成第二次尝试的方法不同，PAG引入了一种选择性修订机制：模型仅在其自身的生成式验证步骤检测到错误时才修订其答案。这种“验证-然后-修订”的工作流程不仅缓解了模型崩溃，还共同提升了推理和验证能力。在多样化的推理基准测试中进行的广泛实验突出了PAG的双重优势：作为策略，它提高了直接生成和自校正的准确性；作为验证器，其自验证表现优于自洽性。", "summary": "本文提出了PAG（Policy as Generative Verifier）框架，旨在解决大型语言模型（LLMs）难以可靠自验证的问题。PAG在一个统一的多轮强化学习范式中，使LLM能够在策略和验证器角色间交替，并引入了选择性修订机制，即仅在检测到错误时才进行修正。实验表明，PAG作为策略提升了生成和自校正准确性，作为验证器其自验证能力优于自洽性，同时增强了LLM的推理和验证能力并缓解了模型崩溃。", "keywords": "LLM自校正, 强化学习, 生成式验证器, 多轮推理, 选择性修订", "comments": "PAG的创新点在于将LLM自身同时作为策略和生成式验证器，并在多轮强化学习中实现选择性自校正，而非盲目重试。这种“验证-然后-修订”的流程有效提升了模型效率和性能，缓解了模型崩溃，并提升了模型的泛化能力。"}}
{"id": "2506.10924", "title": "A space-time interface-fitted method for moving-subdomain distributed control problems with energy regularization", "authors": ["Quang Huy Nguyen", "Phuong Cuc Hoang", "Van Chien Le", "Thi Thanh Mai Ta"], "summary": "This paper investigates a space-time interface-fitted approximation of a\nmoving-interface optimal control problem with energy regularization. We\nreformulate the optimality conditions into a variational problem involving both\nthe state and adjoint. This problem is shown to be equivalent to our optimal\ncontrol problem. Based on fully unstructured, space-time interface-fitted\nmeshes, we propose and analyze a Petrov-Galerkin approximation of the problem.\nAn optimal error estimate with respect to a discrete norm is established under\na specific regularity assumption on the state and adjoint. Several numerical\nresults are presented to corroborate our theoretical results.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10924v1", "AI": {"title_translation": "具有能量正则化的移动子域分布式控制问题的时空界面拟合方法", "tldr": "本文研究了具有能量正则化的移动界面最优控制问题的时空界面拟合近似方法，提出了Petrov-Galerkin近似，并建立了最优误差估计，数值结果验证了理论。", "motivation": "解决具有能量正则化的移动界面最优控制问题的近似表示和分析。", "method": "将最优性条件重构为包含状态和伴随变量的变分问题；基于完全非结构化的时空界面拟合网格，提出并分析了问题的Petrov-Galerkin近似方法。", "result": "在状态和伴随变量的特定正则性假设下，建立了关于离散范数的最优误差估计；通过数值结果验证了理论结果。", "conclusion": "本文成功提出了用于移动界面最优控制问题的时空界面拟合Petrov-Galerkin近似方法，并从理论和数值上验证了其有效性。", "translation": "本文研究了具有能量正则化的移动界面最优控制问题的时空界面拟合近似。我们将最优性条件重构为一个涉及状态和伴随变量的变分问题。该问题被证明等价于我们的最优控制问题。基于完全非结构化的时空界面拟合网格，我们提出并分析了该问题的Petrov-Galerkin近似。在状态和伴随变量的特定正则性假设下，建立了关于离散范数的最优误差估计。本文还提供了若干数值结果以证实我们的理论结果。", "summary": "本文探讨了具有能量正则化的移动界面最优控制问题的时空界面拟合近似方法。通过将最优性条件重构为等价的变分问题，并基于非结构化时空界面拟合网格，提出了Petrov-Galerkin近似。研究建立了在特定正则性假设下的最优误差估计，并通过数值实验验证了理论的正确性。", "keywords": "移动界面最优控制, 时空界面拟合, Petrov-Galerkin近似, 能量正则化, 误差估计", "comments": "本文的创新点在于提出了针对移动界面最优控制问题的时空界面拟合Petrov-Galerkin近似方法，并提供了严格的理论误差估计。这种方法对于处理涉及动态界面的控制问题具有重要意义，尤其是在需要高精度近似的工程和物理领域。"}}
{"id": "2506.10247", "title": "Optimal Voltage Control Using Online Exponential Barrier Method", "authors": ["Peng Zhang", "Baosen Zhang"], "summary": "This paper address the optimal voltage control problem of distribution\nsystems with high penetration of inverter-based renewable energy resources,\nunder inaccurate model information. We propose the online exponential barrier\nmethod that explicitly leverages the online feedback from grids to enhance the\nrobustness to model inaccuracy and incorporates the voltage constraints to\nmaintain the safety requirements. We provide analytical results on the optimal\nbarrier parameter selection and sufficient conditions for the safety guarantee\nof converged voltages. We also establish theoretical results on the exponential\nconvergence rate with proper step-size. The effectiveness of the proposed\nframework is validated on a 56-bus radial network, where we significantly\nimprove the robustness against model inaccuracy compared to existing methods.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10247v1", "AI": {"title_translation": "采用在线指数障碍法的最优电压控制", "tldr": "针对含高渗透逆变器可再生能源的配电系统电压控制问题，本文提出在线指数障碍法，以提高模型不准确性下的鲁棒性并保证电压安全。", "motivation": "解决配电系统中高渗透逆变器可再生能源导致的最优电压控制问题，尤其是在模型信息不准确的情况下。", "method": "提出在线指数障碍法，该方法利用电网在线反馈增强对模型不准确性的鲁棒性，并纳入电压约束以满足安全要求。同时，提供了最优障碍参数选择的分析结果和收敛电压安全保证的充分条件，并建立了指数收敛速率的理论结果。", "result": "在56节点辐射状网络上验证了所提框架的有效性，与现有方法相比，显著提高了对模型不准确性的鲁棒性。", "conclusion": "所提出的在线指数障碍法能够有效解决含高渗透可再生能源配电系统的最优电压控制问题，特别是在模型不准确的情况下，并能保证电压安全和快速收敛。", "translation": "本文针对模型信息不准确情况下，含有高渗透率逆变器型可再生能源的配电系统的最优电压控制问题。我们提出了在线指数障碍法，该方法明确利用电网的在线反馈来增强对模型不准确性的鲁棒性，并结合电压约束以保持安全要求。我们提供了关于最优障碍参数选择的分析结果以及收敛电压安全保证的充分条件。我们还建立了在适当步长下指数收敛速率的理论结果。所提出的框架在56节点辐射状网络上得到了验证，与现有方法相比，我们显著提高了对模型不准确性的鲁棒性。", "summary": "本文提出一种在线指数障碍法，用于解决含有高渗透率逆变器型可再生能源的配电系统在模型信息不准确情况下的最优电压控制问题。该方法利用在线反馈提高鲁棒性，并结合电压约束确保安全。研究提供了理论分析，并在仿真中验证了其在提高鲁棒性方面的有效性。", "keywords": "最优电压控制, 在线指数障碍法, 模型不准确性, 配电系统, 鲁棒性", "comments": "这项工作通过引入在线指数障碍法，在处理配电系统最优电压控制问题时，有效解决了模型不准确性带来的挑战，并提供了理论保证，具有较好的创新性和实用价值。"}}
{"id": "2506.10638", "title": "CyFence: Securing Cyber-Physical Controllers via Trusted Execution Environment", "authors": ["Stefano Longari", "Alessandro Pozone", "Jessica Leoni", "Mario Polino", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "summary": "In the last decades, Cyber-physical Systems (CPSs) have experienced a\nsignificant technological evolution and increased connectivity, at the cost of\ngreater exposure to cyber-attacks. Since many CPS are used in safety-critical\nsystems, such attacks entail high risks and potential safety harms. Although\nseveral defense strategies have been proposed, they rarely exploit the\ncyber-physical nature of the system. In this work, we exploit the nature of CPS\nby proposing CyFence, a novel architecture that improves the resilience of\nclosed-loop control systems against cyber-attacks by adding a semantic check,\nused to confirm that the system is behaving as expected. To ensure the security\nof the semantic check code, we use the Trusted Execution Environment\nimplemented by modern processors. We evaluate CyFence considering a real-world\napplication, consisting of an active braking digital controller, demonstrating\nthat it can mitigate different types of attacks with a negligible computation\noverhead.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10638v1", "AI": {"title_translation": "CyFence：通过可信执行环境保护信息物理控制器", "tldr": "CyFence利用可信执行环境和语义检查来保护信息物理系统免受网络攻击，且计算开销可忽略。", "motivation": "信息物理系统（CPS）的互联性增加导致网络攻击风险提高，尤其是在安全关键系统中。现有防御策略很少利用CPS的物理特性，导致防御效果不足。", "method": "本文提出了CyFence架构，通过添加语义检查来确认系统行为是否符合预期，以提高闭环控制系统抵御网络攻击的弹性。为确保语义检查代码的安全性，该方案利用了现代处理器实现的可信执行环境（TEE）。", "result": "在主动制动数字控制器这一真实应用中评估了CyFence，结果表明它能够以可忽略的计算开销缓解不同类型的攻击。", "conclusion": "CyFence通过结合语义检查和可信执行环境，有效提升了信息物理系统（特别是闭环控制系统）对网络攻击的弹性，且具有实用性和低开销。", "translation": "在过去的几十年里，信息物理系统（CPS）经历了显著的技术演进和连接性增强，但代价是更容易遭受网络攻击。由于许多CPS用于安全关键系统，此类攻击会带来高风险和潜在的安全危害。尽管已经提出了几种防御策略，但它们很少利用系统的信息物理特性。在这项工作中，我们利用CPS的特性，提出了CyFence，这是一种新颖的架构，通过添加语义检查来确认系统是否按预期运行，从而提高闭环控制系统抵御网络攻击的弹性。为了确保语义检查代码的安全性，我们使用了现代处理器实现的可信执行环境。我们考虑一个真实世界的应用，即一个主动制动数字控制器，评估了CyFence，证明它能够以可忽略的计算开销缓解不同类型的攻击。", "summary": "本文提出了CyFence，一种利用信息物理系统特性的新型架构，通过在可信执行环境中进行语义检查来确保闭环控制系统行为符合预期，从而增强其抵御网络攻击的能力。实验证明，该方法在实际应用中能有效缓解多种攻击，且计算开销极低。", "keywords": "信息物理系统, 网络安全, 可信执行环境, 语义检查, 闭环控制系统", "comments": "CyFence的创新之处在于结合了系统的物理特性（通过语义检查）和硬件安全机制（可信执行环境）来增强信息物理系统的安全性，解决了传统防御策略的不足。其低计算开销使其在实际安全关键系统中的部署具有潜力。"}}
{"id": "2506.10954", "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks", "authors": ["Lianghong Guo", "Yanlin Wang", "Caihua Li", "Pengyu Yang", "Jiachi Chen", "Wei Tao", "Yingtian Zou", "Duyu Tang", "Zibin Zheng"], "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of $0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.10954v1", "AI": {"title_translation": "SWE-Factory：您的自动化问题解决训练数据和评估基准工厂", "tldr": "SWE-Factory是一个自动化流水线，用于高效、准确地为GitHub问题解决任务构建大规模训练数据和评估基准，解决了传统方法中环境构建、结果评级和实例验证的挑战。", "motivation": "为训练和评估大型语言模型（LLMs）的软件工程能力，构建大规模GitHub问题解决任务数据集至关重要。然而，传统的数据集创建过程，特别是在评估环境设置、测试结果评级和任务实例验证阶段，具有显著的挑战性和劳动密集型。", "method": "本文提出了SWE-Factory，一个自动化流水线，旨在解决GitHub问题解决任务数据集构建中的挑战。该流水线集成了三个核心自动化组件：1) SWE-Builder，一个多智能体系统，通过四个专业智能体协作、迭代循环并利用环境内存池，自动化评估环境构建；2) 基于退出代码的标准化评级方法，无需手动编写自定义解析器；3) 使用可靠的退出代码信号自动化fail2pass验证过程。", "result": "在跨四种编程语言的671个问题上进行的实验表明，SWE-Factory流水线能有效构建有效的任务实例。例如，使用GPT-4.1-mini，SWE-Builder构建了269个有效实例，每个成本为0.045美元；使用Gemini-2.5-flash，实现了可比性能，每个实例成本最低为0.024美元。此外，基于退出代码的评级方法相对于人工检查实现了100%的准确率，自动化fail2pass验证达到了0.92的精确率和1.00的召回率。", "conclusion": "SWE-Factory自动化流水线有望加速大规模、高质量GitHub问题解决数据集的收集，以用于训练和评估。", "translation": "为训练和评估大型语言模型（LLMs）的软件工程能力，构建大规模GitHub问题解决任务数据集至关重要。然而，传统上创建此类基准的过程以其挑战性和劳动密集型而闻名，特别是在设置评估环境、评级测试结果和验证任务实例的阶段。在本文中，我们提出了SWE-Factory，一个旨在解决这些挑战的自动化流水线。为解决这些问题，我们的流水线集成了三个核心自动化组件。首先，我们引入了SWE-Builder，一个多智能体系统，它自动化评估环境构建，该系统采用四个专门的智能体，以协作、迭代循环的方式工作，并利用环境内存池来提高效率。其次，我们引入了一种标准化的、基于退出代码的评级方法，无需手动编写自定义解析器。最后，我们使用这些可靠的退出代码信号自动化了fail2pass验证过程。在跨四种编程语言的671个问题上进行的实验表明，我们的流水线可以有效地构建有效的任务实例；例如，使用GPT-4.1-mini，我们的SWE-Builder构建了269个有效实例，每个成本为0.045美元，而使用Gemini-2.5-flash，它以每个实例0.024美元的最低成本实现了可比性能。我们还证明，我们基于退出代码的评级相对于人工检查实现了100%的准确率，我们的自动化fail2pass验证达到了0.92的精确率和1.00的召回率。我们希望我们的自动化流水线将加速大规模、高质量GitHub问题解决数据集的收集，以用于训练和评估。我们的代码和数据集已在https://github.com/DeepSoftwareAnalytics/swe-factory发布。", "summary": "本文提出了SWE-Factory，一个自动化流水线，旨在解决GitHub问题解决任务大型数据集构建中的挑战。该流水线通过SWE-Builder（一个多智能体系统）自动化评估环境构建，引入基于退出代码的标准化评级方法，并自动化fail2pass验证。实验证明，SWE-Factory能高效、准确地构建有效任务实例，显著降低成本并提高评级准确性，有望加速高质量数据集的收集。", "keywords": "自动化流水线, GitHub问题解决, 数据集构建, 大型语言模型, 软件工程", "comments": "SWE-Factory通过自动化软件工程任务中数据集构建的关键环节（环境设置、评级、验证），显著提升了效率和准确性，解决了LLMs在软件工程领域应用中的一个核心痛点。其创新之处在于结合了多智能体系统、标准化退出代码评级和自动化验证，为大规模、高质量软件工程数据集的生成提供了可行且经济的解决方案。这对于推动LLMs在软件开发和维护方面的研究具有重要意义。"}}
{"id": "2506.10966", "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10966v1", "AI": {"title_translation": "GENMANIP：LLM驱动的可泛化指令遵循操作模拟", "tldr": "GenManip是一个LLM驱动的模拟平台，用于研究机器人操作策略的泛化能力，并发现模块化系统在多样化场景中表现出更好的泛化性。", "motivation": "现实世界中的机器人操作，特别是鲁棒的泛化能力，仍然具有挑战性。现有的模拟平台在探索策略如何适应不同指令和场景方面支持不足，且未能充分探索LLM等指令遵循基础模型的适应性。", "method": "我们引入了GenManip，一个为策略泛化研究量身定制的真实桌面模拟平台。它通过LLM驱动的面向任务场景图实现自动化流程，利用10K标注的3D对象资产合成大规模、多样化的任务。为了系统评估泛化能力，我们提出了GenManip-Bench，一个包含200个场景的基准测试，并通过人工循环校正进行了改进。我们评估了两种策略类型：(1) 集成基础模型进行感知、推理和规划的模块化操作系统，以及 (2) 通过可扩展数据收集训练的端到端策略。", "result": "结果表明，虽然数据扩展有利于端到端方法，但通过基础模型增强的模块化系统在多样化场景中表现出更有效的泛化能力。", "conclusion": "GenManip平台有望为在现实条件下推进策略泛化提供重要见解。", "translation": "现实世界中的机器人操作仍然充满挑战，尤其是在鲁棒泛化方面。现有模拟平台在探索策略如何适应不同指令和场景方面缺乏足够的支持。因此，它们落后于对指令遵循基础模型（如LLM）日益增长的兴趣，这些模型的适应性至关重要，但在公平比较中仍未得到充分探索。为了弥补这一差距，我们引入了GenManip，一个为策略泛化研究量身定制的真实桌面模拟平台。它通过LLM驱动的面向任务场景图实现自动化流程，利用10K标注的3D对象资产合成大规模、多样化的任务。为了系统评估泛化能力，我们提出了GenManip-Bench，一个包含200个场景的基准测试，并通过人工循环校正进行了改进。我们评估了两种策略类型：(1) 集成基础模型进行感知、推理和规划的模块化操作系统，以及 (2) 通过可扩展数据收集训练的端到端策略。结果表明，虽然数据扩展有利于端到端方法，但通过基础模型增强的模块化系统在多样化场景中表现出更有效的泛化能力。我们预计该平台将促进关键见解，以推进现实条件下的策略泛化。项目主页：https://genmanip.axi404.top/。", "summary": "本文介绍了GenManip，一个由LLM驱动的真实桌面模拟平台，旨在解决机器人操作中策略泛化能力不足的问题。该平台通过LLM驱动的场景图自动合成大规模多样化任务，并提出了GenManip-Bench基准测试来系统评估泛化能力。研究对比了集成基础模型的模块化系统和端到端策略，发现模块化系统在多样化场景中表现出更好的泛化能力，为未来机器人策略的泛化研究提供了有价值的工具和见解。", "keywords": "机器人操作, 泛化, LLM, 模拟平台, GenManip", "comments": "GenManip的创新之处在于其LLM驱动的自动化任务合成管线和大规模多样化资产库，这为机器人操作策略的泛化研究提供了前所未有的广度和深度。其重要性在于弥补了现有模拟平台在支持LLM驱动的指令遵循模型研究方面的不足，并证明了模块化系统结合基础模型在泛化方面的优越性，这对于推进现实世界机器人部署具有重要指导意义。"}}
{"id": "2506.10342", "title": "UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models", "authors": ["Jun Yin", "Jing Zhong", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Ran Luo", "Shuai Lu"], "summary": "Urban cultures and architectural styles vary significantly across cities due\nto geographical, chronological, historical, and socio-political factors.\nUnderstanding these differences is essential for anticipating how cities may\nevolve in the future. As representative cases of historical continuity and\nmodern innovation in China, Beijing and Shenzhen offer valuable perspectives\nfor exploring the transformation of urban streetscapes. However, conventional\napproaches to urban cultural studies often rely on expert interpretation and\nhistorical documentation, which are difficult to standardize across different\ncontexts. To address this, we propose a multimodal research framework based on\nvision-language models, enabling automated and scalable analysis of urban\nstreetscape style differences. This approach enhances the objectivity and\ndata-driven nature of urban form research. The contributions of this study are\nas follows: First, we construct UrbanDiffBench, a curated dataset of urban\nstreetscapes containing architectural images from different periods and\nregions. Second, we develop UrbanSense, the first vision-language-model-based\nframework for urban streetscape analysis, enabling the quantitative generation\nand comparison of urban style representations. Third, experimental results show\nthat Over 80% of generated descriptions pass the t-test (p less than 0.05).\nHigh Phi scores (0.912 for cities, 0.833 for periods) from subjective\nevaluations confirm the method's ability to capture subtle stylistic\ndifferences. These results highlight the method's potential to quantify and\ninterpret urban style evolution, offering a scientifically grounded lens for\nfuture design.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10342v1", "AI": {"title_translation": "UrbanSense：一个利用视觉大语言模型对城市街景进行定量分析的框架", "tldr": "UrbanSense是一个基于视觉大语言模型的框架，用于定量分析城市街景风格差异，通过构建数据集和实验验证了其捕捉细微风格差异的能力，提升了城市形态研究的客观性和数据驱动性。", "motivation": "城市文化和建筑风格因地理、历史和社会政治因素而异，理解这些差异对于预测城市未来演变至关重要。传统城市文化研究方法依赖专家解读和历史文献，难以标准化。因此，需要一种自动化、可扩展的方法来分析城市街景风格差异。", "method": "提出一个基于视觉-语言模型的多模态研究框架，实现城市街景风格差异的自动化和可扩展分析。具体包括：1. 构建UrbanDiffBench数据集，包含不同时期和地区的建筑图像。2. 开发UrbanSense框架，利用视觉-语言模型进行城市街景分析，实现城市风格表示的定量生成和比较。", "result": "实验结果显示，超过80%的生成描述通过t检验（p < 0.05）。主观评估的高Phi分数（城市为0.912，时期为0.833）证实了该方法捕捉细微风格差异的能力。", "conclusion": "该方法具有量化和解释城市风格演变的潜力，为未来的城市设计提供了科学依据。", "translation": "城市文化和建筑风格因地理、年代、历史和社会政治因素而在不同城市间存在显著差异。理解这些差异对于预测未来城市如何演变至关重要。作为中国历史连续性和现代创新的代表案例，北京和深圳为探索城市街景的转型提供了宝贵的视角。然而，传统的城市文化研究方法通常依赖于专家解读和历史文献，这在不同背景下难以标准化。为了解决这个问题，我们提出了一个基于视觉-语言模型的多模态研究框架，实现了城市街景风格差异的自动化和可扩展分析。这种方法增强了城市形态研究的客观性和数据驱动性。本研究的贡献如下：首先，我们构建了UrbanDiffBench，一个精选的城市街景数据集，其中包含来自不同时期和地区的建筑图像。其次，我们开发了UrbanSense，这是第一个基于视觉-语言模型的城市街景分析框架，能够定量生成和比较城市风格表示。第三，实验结果表明，超过80%的生成描述通过了t检验（p小于0.05）。主观评估的高Phi分数（城市为0.912，时期为0.833）证实了该方法捕捉细微风格差异的能力。这些结果突出了该方法量化和解释城市风格演变的潜力，为未来的设计提供了科学依据。", "summary": "本文提出了UrbanSense框架，一个利用视觉大语言模型对城市街景进行定量分析的新方法。针对传统城市研究方法难以标准化的痛点，该框架通过构建UrbanDiffBench数据集并开发UrbanSense模型，实现了城市街景风格差异的自动化、客观和数据驱动分析。实验结果表明，UrbanSense能有效捕捉并量化城市街景的细微风格差异，为城市风格演变研究及未来城市设计提供了科学工具。", "keywords": "城市街景, 视觉大语言模型, 定量分析, UrbanSense, 城市风格", "comments": "该论文创新性地将视觉大语言模型应用于城市街景的定量分析，克服了传统方法主观性强、难以标准化的局限。通过构建专门的数据集和开发新的框架，显著提升了城市形态研究的客观性和数据驱动性。其成果为城市规划、文化研究和未来城市设计提供了新的视角和工具，具有重要的应用潜力。"}}
{"id": "2506.10184", "title": "Optimizing Genetic Algorithms with Multilayer Perceptron Networks for Enhancing TinyFace Recognition", "authors": ["Mohammad Subhi Al-Batah", "Mowafaq Salem Alzboon", "Muhyeeddin Alqaraleh"], "summary": "This study conducts an empirical examination of MLP networks investigated\nthrough a rigorous methodical experimentation process involving three diverse\ndatasets: TinyFace, Heart Disease, and Iris. Study Overview: The study includes\nthree key methods: a) a baseline training using the default settings for the\nMulti-Layer Perceptron (MLP), b) feature selection using Genetic Algorithm (GA)\nbased refinement c) Principal Component Analysis (PCA) based dimension\nreduction. The results show important information on how such techniques affect\nperformance. While PCA had showed benefits in low-dimensional and noise-free\ndatasets GA consistently increased accuracy in complex datasets by accurately\nidentifying critical features. Comparison reveals that feature selection and\ndimensionality reduction play interdependent roles in enhancing MLP\nperformance. The study contributes to the literature on feature engineering and\nneural network parameter optimization, offering practical guidelines for a wide\nrange of machine learning tasks", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10184v1", "AI": {"title_translation": "利用多层感知器网络优化遗传算法以增强TinyFace识别", "tldr": "本研究通过结合遗传算法（GA）和主成分分析（PCA）来优化多层感知器（MLP）网络，以提高复杂数据集（如TinyFace）的识别性能。", "motivation": "本研究旨在优化遗传算法与多层感知器网络的结合，以增强TinyFace识别，并通过经验性检验特征工程和神经网络参数优化对机器学习任务性能的影响。", "method": "本研究采用三种主要方法：a) 使用默认设置的多层感知器（MLP）进行基线训练；b) 基于遗传算法（GA）的特征选择；c) 基于主成分分析（PCA）的降维。实验在TinyFace、Heart Disease和Iris三个数据集上进行。", "result": "结果显示，PCA在低维和无噪声数据集上表现出优势，而遗传算法（GA）通过准确识别关键特征，在复杂数据集上持续提高了准确性。研究揭示了特征选择和降维在增强MLP性能中相互依赖的作用。", "conclusion": "本研究为特征工程和神经网络参数优化文献做出了贡献，并为广泛的机器学习任务提供了实用指导。", "translation": "本研究通过涉及TinyFace、心脏病和Iris三个不同数据集的严格方法学实验过程，对MLP网络进行了实证检验。研究概述：本研究包括三个关键方法：a) 使用多层感知器（MLP）默认设置进行基线训练，b) 使用基于遗传算法（GA）的特征选择进行优化，c) 基于主成分分析（PCA）的降维。结果显示了这些技术如何影响性能的重要信息。虽然PCA在低维和无噪声数据集中显示出优势，但GA通过准确识别关键特征，在复杂数据集中持续提高了准确性。比较表明，特征选择和降维在增强MLP性能方面发挥着相互依赖的作用。这项研究对特征工程和神经网络参数优化文献做出了贡献，为广泛的机器学习任务提供了实用指导。", "summary": "本研究通过在TinyFace、心脏病和Iris等数据集上进行实证检验，探索了多层感知器（MLP）网络的优化方法。论文比较了MLP基线训练、基于遗传算法（GA）的特征选择和基于主成分分析（PCA）的降维三种方法。结果表明，GA在复杂数据集中通过精准特征识别能有效提升准确性，而PCA适用于低维无噪声数据。研究强调了特征选择与降维在提升MLP性能中的相互依赖性，并为特征工程和神经网络参数优化提供了实践指导。", "keywords": "遗传算法, 多层感知器, 特征选择, 降维, TinyFace识别", "comments": "本研究的创新点在于将遗传算法与多层感知器网络结合，以优化特征选择过程，从而提升复杂数据集（如TinyFace）的识别性能。其重要性在于为特征工程和神经网络参数优化提供了实用的方法和见解，对于处理高维复杂数据具有指导意义。该研究为机器学习任务中的性能提升提供了有价值的参考。"}}
{"id": "2506.10415", "title": "Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?", "authors": ["Yingjin Song", "Yupei Du", "Denis Paperno", "Albert Gatt"], "summary": "This paper introduces the TempVS benchmark, which focuses on temporal\ngrounding and reasoning capabilities of Multimodal Large Language Models\n(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event\nrelation inference, sentence ordering and image ordering), each accompanied\nwith a basic grounding test. TempVS requires MLLMs to rely on both visual and\nlinguistic modalities to understand the temporal order of events. We evaluate\n38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,\nwith a substantial performance gap compared to human capabilities. We also\nprovide fine-grained insights that suggest promising directions for future\nresearch. Our TempVS benchmark data and code are available at\nhttps://github.com/yjsong22/TempVS.", "comment": "27 pages, 14 figures. Accepted to ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10415v1", "AI": {"title_translation": "阅后即焚：多模态大型语言模型真的能捕捉图像序列中的事件顺序吗？", "tldr": "本文引入了TempVS基准，用于评估多模态大型语言模型（MLLMs）在图像序列中事件时间顺序的理解能力。结果显示，目前最先进的MLLMs在该基准上表现不佳，与人类能力存在显著差距。", "motivation": "评估多模态大型语言模型（MLLMs）在图像序列中对事件时间顺序的理解和推理能力，因为这方面仍是未经验证的挑战。", "method": "本文引入了TempVS基准，该基准包含三个主要测试（事件关系推断、句子排序和图像排序），每个测试都附带一个基础定位测试。TempVS要求MLLMs结合视觉和语言模态来理解事件的时间顺序。研究评估了38个最先进的MLLMs。", "result": "评估结果表明，多模态大型语言模型在解决TempVS任务时表现困难，与人类能力相比存在显著的性能差距。研究还提供了细致的见解，为未来的研究指明了有前景的方向。", "conclusion": "目前的多模态大型语言模型在图像序列的事件时间顺序理解和推理方面存在显著不足，需要进一步的研究和改进。", "translation": "本文引入了TempVS基准，该基准侧重于多模态大型语言模型（MLLMs）在图像序列中的时间定位和推理能力。TempVS包含三个主要测试（即事件关系推断、句子排序和图像排序），每个测试都附带一个基础定位测试。TempVS要求MLLMs依靠视觉和语言模态来理解事件的时间顺序。我们评估了38个最先进的MLLMs，结果表明模型在解决TempVS时表现困难，与人类能力相比存在显著的性能差距。我们还提供了细致的见解，为未来的研究指明了有前景的方向。我们的TempVS基准数据和代码可在https://github.com/yjsong22/TempVS获取。", "summary": "本文提出了TempVS基准，旨在评估多模态大型语言模型（MLLMs）在理解图像序列中事件时间顺序方面的能力。该基准包含事件关系推断、句子排序和图像排序等任务，要求MLLMs整合视觉和语言信息。研究对38个主流MLLMs进行了评估，发现它们在TempVS上表现远低于人类水平，揭示了当前MLLMs在时间推理能力上的不足，并为未来研究提供了方向。", "keywords": "多模态大型语言模型, 时间推理, 图像序列, 基准测试, TempVS", "comments": "本文通过引入TempVS这一新颖的基准，填补了评估多模态大型语言模型时间推理能力方面的空白。其重要性在于明确指出了当前MLLMs在理解图像序列中事件顺序方面的局限性，为未来模型设计和训练提供了明确的研究方向。该基准的发布也促进了社区对这一关键能力的关注。"}}
{"id": "2506.10973", "title": "Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning", "authors": ["Julius Berner", "Miguel Liu-Schiaffini", "Jean Kossaifi", "Valentin Duruisseaux", "Boris Bonev", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "summary": "A wide range of scientific problems, such as those described by\ncontinuous-time dynamical systems and partial differential equations (PDEs),\nare naturally formulated on function spaces. While function spaces are\ntypically infinite-dimensional, deep learning has predominantly advanced\nthrough applications in computer vision and natural language processing that\nfocus on mappings between finite-dimensional spaces. Such fundamental\ndisparities in the nature of the data have limited neural networks from\nachieving a comparable level of success in scientific applications as seen in\nother fields. Neural operators are a principled way to generalize neural\nnetworks to mappings between function spaces, offering a pathway to replicate\ndeep learning's transformative impact on scientific problems. For instance,\nneural operators can learn solution operators for entire classes of PDEs, e.g.,\nphysical systems with different boundary conditions, coefficient functions, and\ngeometries. A key factor in deep learning's success has been the careful\nengineering of neural architectures through extensive empirical testing.\nTranslating these neural architectures into neural operators allows operator\nlearning to enjoy these same empirical optimizations. However, prior neural\noperator architectures have often been introduced as standalone models, not\ndirectly derived as extensions of existing neural network architectures. In\nthis paper, we identify and distill the key principles for constructing\npractical implementations of mappings between infinite-dimensional function\nspaces. Using these principles, we propose a recipe for converting several\npopular neural architectures into neural operators with minimal modifications.\nThis paper aims to guide practitioners through this process and details the\nsteps to make neural operators work in practice. Our code can be found at\nhttps://github.com/neuraloperator/NNs-to-NOs", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10973v1", "AI": {"title_translation": "将神经网络架构扩展到函数空间进行算子学习的原则性方法", "tldr": "本文提出了将现有神经网络架构转换为神经算子以解决函数空间问题的原则性方法和实践指南，旨在弥合深度学习在有限维和无限维数据应用之间的差距。", "motivation": "深度学习在计算机视觉和自然语言处理等有限维空间应用中取得了巨大成功，但在科学应用（如连续时间动力学系统和偏微分方程）中，由于数据自然存在于无限维函数空间，其成功有限。神经算子是解决这一问题的有效途径，但现有神经算子架构通常是独立引入的，而非直接从现有神经网络架构扩展而来，这限制了它们利用现有架构的经验优化。", "method": "本文识别并提炼了构建无限维函数空间映射的实用实现的关键原则。基于这些原则，作者提出了一种将几种流行神经网络架构转换为神经算子的方法，只需进行最少的修改。该方法旨在指导实践者完成这一过程，并详细说明使神经算子在实践中工作的步骤。", "result": "本文提出了将流行神经网络架构转换为神经算子的“配方”，并提供了实践指南和详细步骤，以帮助实践者在函数空间应用中有效使用神经算子。代码已开源。", "conclusion": "通过提出将现有神经网络架构扩展到函数空间的关键原则和实践方法，本文旨在弥合深度学习在有限维和无限维数据应用之间的差距，使神经算子能够利用现有神经网络架构的成功经验，从而推动深度学习在科学问题中的应用。", "translation": "许多科学问题，例如由连续时间动力学系统和偏微分方程（PDEs）描述的问题，自然地在函数空间中被公式化。虽然函数空间通常是无限维的，但深度学习主要通过计算机视觉和自然语言处理中的应用取得了进展，这些应用侧重于有限维空间之间的映射。数据性质上的这种根本差异限制了神经网络在科学应用中取得与其他领域相当的成功。神经算子是将神经网络推广到函数空间之间映射的一种原则性方法，为复制深度学习对科学问题的变革性影响提供了途径。例如，神经算子可以学习整个类别的PDEs的解算子，例如具有不同边界条件、系数函数和几何形状的物理系统。深度学习成功的一个关键因素是通过广泛的经验测试精心设计神经网络架构。将这些神经网络架构转化为神经算子，使得算子学习可以享受相同的经验优化。然而，以前的神经算子架构通常是作为独立模型引入的，并非直接从现有神经网络架构扩展而来。在本文中，我们识别并提炼了构建无限维函数空间之间映射的实用实现的关键原则。利用这些原则，我们提出了一种将几种流行神经网络架构转换为神经算子的方法，只需进行最少的修改。本文旨在指导实践者完成这一过程，并详细说明使神经算子在实践中工作的步骤。我们的代码可以在https://github.com/neuraloperator/NNs-to-NOs找到。", "summary": "本文探讨了将深度学习应用于以函数空间为基础的科学问题所面临的挑战，指出现有神经网络主要处理有限维数据。为弥合这一差距，文章提出神经算子作为将神经网络扩展到函数空间映射的有效途径。针对当前神经算子架构多为独立模型的现状，本文识别并提炼了构建无限维函数空间映射的关键原则，并提出了一种将流行神经网络架构转换为神经算子的通用方法，旨在通过最小修改实现这一转换，并为实践者提供详细的实施指南和开源代码，以期在科学计算领域复制深度学习的成功。", "keywords": "神经算子, 函数空间, 神经网络架构, 算子学习, 科学计算", "comments": "本文的创新之处在于提出了一套将现有、成熟的神经网络架构原则性地扩展到函数空间以构建神经算子的方法，而非从头设计。这使得算子学习能够复用现有深度学习架构的经验优化和工程成果，显著降低了开发门槛并加速了在科学计算领域的应用。其重要性在于为解决无限维函数空间问题提供了一条实用且高效的途径，有望推动深度学习在PDE求解、动力系统模拟等领域的进一步发展。方法的普适性强，且提供了实际操作的指南和代码，对研究者和实践者都具有很高的价值。"}}
{"id": "2506.10645", "title": "From IOCs to Group Profiles: On the Specificity of Threat Group Behaviors in CTI Knowledge Bases", "authors": ["Aakanksha Saha", "Martina Lindorfer", "Juan Caballero"], "summary": "Indicators of Compromise (IOCs) such as IP addresses, file hashes, and domain\nnames are commonly used for threat detection and attribution. However, IOCs\ntend to be short-lived as they are easy to change. As a result, the\ncybersecurity community is shifting focus towards more persistent behavioral\nprofiles, such as the Tactics, Techniques, and Procedures (TTPs) and the\nsoftware used by a threat group. However, the distinctiveness and completeness\nof such behavioral profiles remain largely unexplored. In this work, we\nsystematically analyze threat group profiles built from two open cyber threat\nintelligence (CTI) knowledge bases: MITRE ATT&CK and Malpedia. We first\ninvestigate what fraction of threat groups have group-specific behaviors, i.e.,\nbehaviors used exclusively by a single group. We find that only 34% of threat\ngroups in ATT&CK have group-specific techniques. The software used by a threat\ngroup proves to be more distinctive, with 73% of ATT&CK groups using\ngroup-specific software. However, this percentage drops to 24% in the broader\nMalpedia dataset. Next, we evaluate how group profiles improve when data from\nboth sources are combined. While coverage improves modestly, the proportion of\ngroups with group-specific behaviors remains under 30%. We then enhance\nprofiles by adding exploited vulnerabilities and additional techniques\nextracted from more threat reports. Despite the additional information, 64% of\ngroups still lack any group-specific behavior. Our findings raise concerns on\nthe belief that behavioral profiles can replace IOCs in threat group\nattribution.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10645v1", "AI": {"title_translation": "从IOCs到群体画像：关于CTI知识库中威胁群体行为的特异性", "tldr": "本研究系统分析了威胁群体行为画像在CTI知识库中的特异性，发现大多数威胁群体缺乏独特的行为特征，对行为画像能否取代IOCs进行威胁归因提出了质疑。", "motivation": "传统的妥协指标（IOCs）如IP地址和文件哈希易于改变且生命周期短，导致网络安全界转向更持久的行为画像（如TTPs和软件）进行威胁检测和归因。然而，这些行为画像的独特性和完整性尚未得到充分探索，这是本研究的动机。", "method": "本研究系统分析了来自MITRE ATT&CK和Malpedia这两个开放网络威胁情报（CTI）知识库中构建的威胁群体画像。首先，调查了有多少威胁群体具有群体特定行为。其次，评估了结合两个来源数据后群体画像的改善情况。最后，通过添加漏洞和更多威胁报告中提取的技术来增强画像。", "result": "研究发现，ATT&CK中只有34%的威胁群体具有群体特定技术，而群体特定软件的比例在ATT&CK中为73%，但在更广泛的Malpedia数据集中降至24%。结合两个来源的数据后，覆盖率略有提高，但具有群体特定行为的群体比例仍低于30%。即使添加了漏洞和更多技术，64%的群体仍然缺乏任何群体特定行为。", "conclusion": "研究结果对“行为画像可以取代IOCs进行威胁群体归因”的观点提出了担忧。", "translation": "妥协指标（IOCs），例如IP地址、文件哈希和域名，常用于威胁检测和归因。然而，IOCs往往生命周期短，因为它们易于更改。因此，网络安全界正将重点转向更持久的行为画像，例如威胁群体使用的战术、技术和程序（TTPs）以及软件。然而，此类行为画像的独特性和完整性在很大程度上仍未被探索。在这项工作中，我们系统分析了从两个开放网络威胁情报（CTI）知识库（MITRE ATT&CK和Malpedia）构建的威胁群体画像。我们首先调查了有多少威胁群体具有群体特定行为，即仅由单个群体使用的行为。我们发现ATT&CK中只有34%的威胁群体具有群体特定技术。威胁群体使用的软件被证明更具独特性，ATT&CK中73%的群体使用群体特定软件。然而，在更广泛的Malpedia数据集中，这一比例下降到24%。接下来，我们评估了当结合来自两个来源的数据时，群体画像如何改善。尽管覆盖率略有提高，但具有群体特定行为的群体比例仍低于30%。然后，我们通过添加被利用的漏洞和从更多威胁报告中提取的额外技术来增强画像。尽管增加了额外信息，64%的群体仍然缺乏任何群体特定行为。我们的发现对“行为画像可以取代IOCs进行威胁群体归因”的信念提出了担忧。", "summary": "本研究系统性地分析了MITRE ATT&CK和Malpedia等CTI知识库中的威胁群体行为画像的特异性。研究发现，虽然业界正从易变的IOCs转向更持久的行为画像，但这些画像的独特性和完整性却不尽理想。具体而言，ATT&CK中仅有少数群体具有独特的TTPs，而软件的独特性虽高但在更大数据集中显著下降。即使整合多源数据并补充额外信息，仍有大部分威胁群体缺乏独特的行为特征。这些发现对行为画像在威胁归因中取代IOCs的有效性提出了质疑。", "keywords": "威胁情报, 行为画像, IOCs, MITRE ATT&CK, 威胁归因", "comments": "该论文创新性地对威胁群体行为画像的“特异性”进行了量化分析，挑战了业界普遍认为行为画像优于IOCs的观念。其重要性在于揭示了当前CTI知识库中行为画像的局限性，为未来威胁情报的构建和应用提供了重要见解。研究的局限性可能在于其分析的数据集范围，以及未能提出明确的解决方案来提升行为画像的特异性。"}}
{"id": "2506.10968", "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop", "authors": ["Justin Kerr", "Kush Hari", "Ethan Weber", "Chung Min Kim", "Brent Yi", "Tyler Bonnen", "Ken Goldberg", "Angjoo Kanazawa"], "summary": "Humans do not passively observe the visual world -- we actively look in order\nto act. Motivated by this principle, we introduce EyeRobot, a robotic system\nwith gaze behavior that emerges from the need to complete real-world tasks. We\ndevelop a mechanical eyeball that can freely rotate to observe its surroundings\nand train a gaze policy to control it using reinforcement learning. We\naccomplish this by first collecting teleoperated demonstrations paired with a\n360 camera. This data is imported into a simulation environment that supports\nrendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze\non top of robot demonstrations. We then introduce a BC-RL loop to train the\nhand and eye jointly: the hand (BC) agent is trained from rendered eye\nobservations, and the eye (RL) agent is rewarded when the hand produces correct\naction predictions. In this way, hand-eye coordination emerges as the eye looks\ntowards regions which allow the hand to complete the task. EyeRobot implements\na foveal-inspired policy architecture allowing high resolution with a small\ncompute budget, which we find also leads to the emergence of more stable\nfixation as well as improved ability to track objects and ignore distractors.\nWe evaluate EyeRobot on five panoramic workspace manipulation tasks requiring\nmanipulation in an arc surrounding the robot arm. Our experiments suggest\nEyeRobot exhibits hand-eye coordination behaviors which effectively facilitate\nmanipulation over large workspaces with a single camera. See project site for\nvideos: https://www.eyerobot.net/", "comment": "Project page: https://www.eyerobot.net/", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.10968v1", "AI": {"title_translation": "眼，机器人：通过BC-RL感知-行动循环学习看以行动", "tldr": "EyeRobot系统通过BC-RL循环联合训练机器人的手部和眼部，使其学会主动凝视以完成任务，从而在单个摄像头下实现了大工作空间的高效手眼协调操作。", "motivation": "受人类主动观察视觉世界以行动的启发，本文引入了EyeRobot系统，旨在开发一个机器人，其凝视行为源于完成真实世界任务的需要。", "method": "开发了一个可自由旋转观察周围环境的机械眼球。使用强化学习训练凝视策略来控制眼球。通过收集带有360度摄像头的远程操作演示数据，并将其导入支持任意眼球视角渲染的仿真环境。引入一个BC-RL循环来联合训练手部和眼部：手部（BC）代理从渲染的眼部观察中训练，当手部产生正确的动作预测时，眼部（RL）代理获得奖励。采用中央凹启发的策略架构以小计算预算实现高分辨率。", "result": "手眼协调能力得以出现，眼睛看向允许手部完成任务的区域。中央凹启发的策略架构以小计算预算实现高分辨率，并导致更稳定的注视以及改善了跟踪物体和忽略干扰物的能力。实验表明，EyeRobot在五个全景工作空间操作任务中表现出有效促进在单个摄像头下大工作空间操作的手眼协调行为。", "conclusion": "EyeRobot系统通过其学习到的手眼协调行为，能够有效地促进在单个摄像头下大工作空间内的机器人操作。", "translation": "人类并非被动地观察视觉世界——我们主动地看是为了行动。受此原理的启发，我们引入了EyeRobot，一个机器人系统，其凝视行为源于完成真实世界任务的需要。我们开发了一个可以自由旋转观察周围环境的机械眼球，并使用强化学习训练了一个凝视策略来控制它。我们通过首先收集与360度摄像头配对的远程操作演示数据来实现这一点。这些数据被导入到一个支持渲染任意眼球视角的仿真环境中，从而允许在机器人演示之上进行眼球凝视的片段回放。然后，我们引入了一个BC-RL循环来联合训练手部和眼部：手部（BC）代理从渲染的眼部观察中训练，当手部产生正确的动作预测时，眼部（RL）代理获得奖励。通过这种方式，手眼协调能力得以出现，因为眼睛会看向允许手部完成任务的区域。EyeRobot实现了一种受中央凹启发的策略架构，以较小的计算预算实现高分辨率，我们发现这也导致了更稳定的注视以及改善了跟踪物体和忽略干扰物的能力。我们在五个需要机器臂周围弧形操作的全景工作空间操作任务上评估了EyeRobot。我们的实验表明，EyeRobot表现出有效促进在单个摄像头下大工作空间操作的手眼协调行为。有关视频请参见项目网站：https://www.eyerobot.net/", "summary": "本文介绍了EyeRobot，一个受人类主动观察世界以行动原则启发的机器人系统。该系统开发了一个机械眼球并通过强化学习训练其凝视策略。通过结合远程操作演示数据和仿真环境，引入了BC-RL循环来联合训练机器人的手部和眼部。手部代理从眼部观察中学习，而眼部代理则根据手部动作预测的正确性获得奖励，从而实现了手眼协调。EyeRobot采用中央凹启发的策略架构，以较低的计算成本实现了高分辨率，并带来了更稳定的注视和更好的物体跟踪及抗干扰能力。实验证明，EyeRobot能在单个摄像头下有效完成大工作空间的操作任务。", "keywords": "EyeRobot, 手眼协调, 强化学习, 凝视行为, 机器人操作", "comments": "该论文的创新之处在于提出了一个BC-RL循环来联合训练机器人的手部和眼部，使得凝视行为能够自然地为完成任务服务。此外，采用中央凹启发的策略架构，在保证高分辨率的同时，有效降低了计算成本，并提升了系统的稳定性和鲁棒性。这对于在复杂、大范围工作空间中进行高效机器人操作具有重要意义。"}}
{"id": "2506.10344", "title": "RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration", "authors": ["Mina C. Moghadam", "Alan Q. Wang", "Omer Taub", "Martin R. Prince", "Mert R. Sabuncu"], "summary": "Many real-world settings require registration of a pair of medical images\nthat differ in spatial resolution, which may arise from differences in image\nacquisition parameters like pixel spacing, slice thickness, and field-of-view.\nHowever, all previous machine learning-based registration techniques resample\nimages onto a fixed resolution. This is suboptimal because resampling can\nintroduce artifacts due to interpolation. To address this, we present\nRealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is\nan extension of KeyMorph, a registration framework which works by training a\nnetwork to learn corresponding keypoints for a given pair of images, after\nwhich a closed-form keypoint matching step is used to derive the transformation\nthat aligns them. To avoid resampling and enable operating on the raw data, RKM\noutputs keypoints in real-world coordinates of the scanner. To do this, we\nleverage the affine matrix produced by the scanner (e.g., MRI machine) that\nencodes the mapping from voxel coordinates to real world coordinates. By\ntransforming keypoints into real-world space and integrating this into the\ntraining process, RKM effectively enables the extracted keypoints to be\nresolution-agnostic. In our experiments, we demonstrate the advantages of RKM\non the registration task for orthogonal 2D stacks of abdominal MRIs, as well as\n3D volumes with varying resolutions in brain datasets.", "comment": "23 pages, 8 figures, to be submitted to MELBA", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10344v1", "AI": {"title_translation": "RealKeyMorph：用于分辨率无关图像配准的真实世界坐标关键点", "tldr": "RealKeyMorph 是一种新的图像配准方法，通过在真实世界坐标中学习关键点来避免图像重采样，从而实现分辨率无关的配准。", "motivation": "许多真实世界场景需要对空间分辨率不同的医学图像进行配准，这可能源于图像采集参数的差异。然而，所有先前的基于机器学习的配准技术都将图像重采样到固定分辨率，这会因插值引入伪影，导致次优结果。", "method": "RealKeyMorph (RKM) 是 KeyMorph 框架的扩展，后者通过训练网络学习给定图像对的对应关键点，然后使用闭合形式的关键点匹配步骤推导对齐变换。为避免重采样并操作原始数据，RKM 在扫描仪的真实世界坐标中输出关键点。它利用扫描仪产生的仿射矩阵（编码从体素坐标到真实世界坐标的映射），将关键点变换到真实世界空间并将其整合到训练过程中，从而使提取的关键点具有分辨率无关性。", "result": "实验证明了 RKM 在腹部 MRI 正交 2D 堆栈的配准任务以及具有不同分辨率的脑数据集 3D 体积上的优势。", "conclusion": "RealKeyMorph 提出了一种有效的分辨率无关图像配准方法，通过在真实世界坐标中处理关键点，成功避免了传统重采样引入的伪影，提高了医学图像配准的质量，尤其适用于分辨率差异较大的情况。", "translation": "许多真实世界场景需要对一对空间分辨率不同的医学图像进行配准，这可能源于像素间距、切片厚度和视野等图像采集参数的差异。然而，所有先前的基于机器学习的配准技术都将图像重采样到固定分辨率。这是次优的，因为重采样会由于插值引入伪影。为了解决这个问题，我们提出了 RealKeyMorph (RKM)，一种分辨率无关的图像配准方法。RKM 是 KeyMorph 的扩展，KeyMorph 是一种配准框架，通过训练网络学习给定图像对的对应关键点，之后使用闭合形式的关键点匹配步骤推导对齐它们的变换。为了避免重采样并能够在原始数据上操作，RKM 以扫描仪的真实世界坐标输出关键点。为此，我们利用扫描仪（例如 MRI 机器）生成的仿射矩阵，该矩阵编码了从体素坐标到真实世界坐标的映射。通过将关键点转换到真实世界空间并将其整合到训练过程中，RKM 有效地使提取的关键点具有分辨率无关性。在我们的实验中，我们展示了 RKM 在腹部 MRI 正交 2D 堆栈的配准任务以及具有不同分辨率的脑数据集 3D 体积上的优势。", "summary": "RealKeyMorph (RKM) 是一种用于图像配准的创新方法，旨在解决现有机器学习技术在处理不同空间分辨率医学图像时因重采样引入伪影的问题。RKM 扩展了 KeyMorph 框架，通过学习在扫描仪真实世界坐标中的关键点，并利用扫描仪仿射矩阵将这些关键点转换到真实世界空间，从而实现了分辨率无关的配准，避免了对原始图像的重采样。实验证明，RKM 在处理腹部 MRI 和脑数据集等具有分辨率差异的图像配准任务中表现出优越性。", "keywords": "RealKeyMorph, 图像配准, 分辨率无关, 关键点, 真实世界坐标", "comments": "本文的核心创新在于提出了一种分辨率无关的图像配准方法 RealKeyMorph (RKM)，它通过在真实世界坐标中处理关键点，巧妙地避免了传统重采样技术在处理不同分辨率图像时引入的插值伪影。这种方法提高了配准的准确性和鲁棒性，尤其对于医学图像分析具有重要意义。通过利用扫描仪的仿射矩阵，RKM 能够直接在原始数据上操作，这代表了图像配准领域的一个重要进步。"}}
{"id": "2506.10186", "title": "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment", "authors": ["Yuhui Ding", "Thomas Hofmann"], "summary": "Equivariant diffusion models have achieved impressive performance in 3D\nmolecule generation. These models incorporate Euclidean symmetries of 3D\nmolecules by utilizing an SE(3)-equivariant denoising network. However,\nspecialized equivariant architectures limit the scalability and efficiency of\ndiffusion models. In this paper, we propose an approach that relaxes such\nequivariance constraints. Specifically, our approach learns a sample-dependent\nSO(3) transformation for each molecule to construct an aligned latent space. A\nnon-equivariant diffusion model is then trained over the aligned\nrepresentations. Experimental results demonstrate that our approach performs\nsignificantly better than previously reported non-equivariant models. It yields\nsample quality comparable to state-of-the-art equivariant diffusion models and\noffers improved training and sampling efficiency. Our code is available at\nhttps://github.com/skeletondyh/RADM", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10186v1", "AI": {"title_translation": "可扩展的非等变三维分子生成通过旋转对齐", "tldr": "提出一种通过旋转对齐实现可扩展非等变三维分子生成的方法，达到与最先进等变模型相当的性能且效率更高。", "motivation": "现有的等变扩散模型在三维分子生成方面表现出色，但其特殊的等变架构限制了扩散模型的可扩展性和效率。", "method": "本文提出一种放松等变约束的方法。具体来说，该方法为每个分子学习一个样本依赖的SO(3)变换，以构建对齐的潜在空间。然后，在对齐的表示上训练一个非等变扩散模型。", "result": "实验结果表明，该方法显著优于先前报道的非等变模型。它产生了与最先进的等变扩散模型相当的样本质量，并提供了改进的训练和采样效率。", "conclusion": "通过引入旋转对齐，非等变扩散模型可以克服传统等变模型的扩展性和效率限制，同时保持甚至达到先进的生成质量。", "translation": "标题：通过旋转对齐实现可扩展的非等变三维分子生成\n\n摘要：\n等变扩散模型在三维分子生成方面取得了令人瞩目的性能。这些模型通过利用SE(3)等变去噪网络来整合三维分子的欧几里得对称性。然而，专门的等变架构限制了扩散模型的可扩展性和效率。在本文中，我们提出了一种放松这种等变约束的方法。具体来说，我们的方法为每个分子学习一个样本依赖的SO(3)变换，以构建一个对齐的潜在空间。然后，在对齐的表示上训练一个非等变扩散模型。实验结果表明，我们的方法显著优于先前报道的非等变模型。它产生了与最先进的等变扩散模型相当的样本质量，并提供了改进的训练和采样效率。我们的代码可在https://github.com/skeletondyh/RADM获取。", "summary": "本文提出一种名为“旋转对齐”（Rotational Alignment）的新方法，用于可扩展的非等变三维分子生成。该方法通过为每个分子学习一个样本依赖的SO(3)变换来构建对齐的潜在空间，并在此空间上训练非等变扩散模型。实验证明，该方法在性能上显著优于现有非等变模型，且样本质量可与最先进的等变模型媲美，同时显著提升了训练和采样效率，解决了等变模型在可扩展性和效率方面的局限性。", "keywords": "三维分子生成, 非等变模型, 旋转对齐, 扩散模型, 可扩展性", "comments": "该论文的创新点在于提出了一个巧妙的解决方案来绕过等变模型的复杂性，通过学习样本依赖的旋转对齐来将非等变模型应用于三维分子生成。这使得非等变模型能够达到与等变模型相当的性能，同时显著提高了效率和可扩展性，为3D分子生成领域提供了一个更实用的方向。"}}
{"id": "2506.10897", "title": "GenPlanX. Generation of Plans and Execution", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tomás de la Rosa", "Alfredo Garrachón", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10897v1", "AI": {"title_translation": "GenPlanX：计划生成与执行", "tldr": "GenPlanX结合大型语言模型（LLMs）和经典AI规划技术，使系统能够理解自然语言描述的规划任务并进行执行，从而提高办公室任务的效率和人机协作。", "motivation": "经典的AI规划技术无法理解自然语言描述的规划任务，而大型语言模型（LLMs）在解释人类意图方面表现出色。本文旨在弥补这一差距，通过整合LLMs来提升规划系统理解自然语言任务的能力，以简化工作流程并提高生产力。", "method": "GenPlanX通过整合大型语言模型（LLMs）来处理自然语言描述的规划任务，并将其与经典的AI规划引擎以及一个执行和监控框架相结合。", "result": "GenPlanX在协助用户处理办公室相关任务方面表现出有效性，展示了其在简化工作流程和提高生产力方面的潜力。", "conclusion": "GenPlanX通过结合LLMs和经典AI规划，在处理自然语言规划任务方面显示出巨大潜力，能够实现无缝的人机协作，特别是在办公室任务场景中。", "translation": "经典AI规划技术为复杂任务生成行动序列。然而，当任务以自然语言提供时，它们缺乏理解规划任务的能力。大型语言模型（LLMs）的出现为人类与计算机交互带来了新能力。在规划任务的背景下，LLMs在解释人类意图等方面表现出色。本文介绍了GenPlanX，它将LLMs用于基于自然语言的规划任务描述，与经典AI规划引擎以及执行和监控框架相结合。我们展示了GenPlanX在协助用户处理办公室相关任务方面的功效，突出了其通过无缝人机协作简化工作流程和提高生产力的潜力。", "summary": "GenPlanX是一个创新系统，它将大型语言模型（LLMs）的自然语言理解能力与经典的AI规划引擎和执行监控框架相结合。该系统旨在解决传统AI规划在处理自然语言任务描述方面的不足，通过允许用户使用自然语言定义任务来简化规划过程。论文展示了GenPlanX在提升办公室任务处理效率和促进人机无缝协作方面的有效性。", "keywords": "AI规划, 大型语言模型, 自然语言处理, 任务执行, 人机协作", "comments": "GenPlanX的创新之处在于成功地弥合了大型语言模型在自然语言理解方面的优势与传统AI规划在逻辑推理和行动序列生成方面的优势之间的鸿沟。这提供了一个实用且高效的解决方案，使得AI规划系统能够更好地响应人类的自然语言指令，极大地增强了人机交互的流畅性和效率，特别是在需要自动化复杂任务的办公环境中。其潜力在于将AI规划从技术专家的领域扩展到更广泛的用户群体。"}}
{"id": "2506.10421", "title": "Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting", "authors": ["Avneet Kaur", "Arnav Arora"], "summary": "Framing used by news media, especially in times of conflict, can have\nsubstantial impact on readers' opinion, potentially aggravating the conflict\nitself. Current studies on the topic of conflict framing have limited insights\ndue to their qualitative nature or only look at surface level generic frames\nwithout going deeper. In this work, we identify indicators of war and peace\njournalism, as outlined by prior work in conflict studies, in a corpus of news\narticles reporting on the Israel-Palestine war. For our analysis, we use\ncomputational approaches, using a combination of frame semantics and large\nlanguage models to identify both communicative framing and its connection to\nlinguistic framing. Our analysis reveals a higher focus on war based reporting\nrather than peace based. We also show substantial differences in reporting\nacross the US, UK, and Middle Eastern news outlets in framing who the assailant\nand victims of the conflict are, surfacing biases within the media.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10421v1", "AI": {"title_translation": "超越战场：冲突报道中媒体报道的框架分析", "tldr": "本研究使用计算方法，结合框架语义和大型语言模型，分析了以色列-巴勒斯坦战争新闻报道中的战争与和平新闻指标，发现媒体更侧重战争报道，且不同地区媒体在受害者和袭击者框架上存在显著差异，揭示了媒体偏见。", "motivation": "新闻媒体的框架，尤其是在冲突时期，能显著影响读者观点，可能加剧冲突。当前关于冲突框架的研究因其定性性质或仅停留在表面泛型框架而缺乏深入见解，因此需要更深入的分析。", "method": "本研究在以色列-巴勒斯坦战争新闻报道语料库中，识别了战争与和平新闻指标。分析方法采用计算方法，结合框架语义和大型语言模型，以识别交际框架及其与语言框架的联系。", "result": "分析显示，新闻报道更侧重于基于战争的报道而非基于和平的报道。研究还揭示了美国、英国和中东新闻媒体在冲突中袭击者和受害者框架上的显著差异，从而揭示了媒体内部的偏见。", "conclusion": "媒体报道在冲突中存在显著偏见，尤其体现在对战争的侧重以及对袭击者和受害者的不同框架上，这可能加剧冲突并影响公众认知。", "translation": "新闻媒体使用的框架，尤其是在冲突时期，能对读者的观点产生重大影响，可能加剧冲突本身。当前关于冲突框架主题的研究因其定性性质或仅关注表面泛型框架而洞察力有限，未能深入探讨。在这项工作中，我们根据冲突研究中先前工作所概述的，在一个报道以色列-巴勒斯坦战争的新闻文章语料库中识别了战争与和平新闻的指标。为了进行分析，我们使用计算方法，结合框架语义和大型语言模型来识别交际框架及其与语言框架的联系。我们的分析揭示了对基于战争的报道而非基于和平的报道的更高关注。我们还发现，美国、英国和中东新闻媒体在框架冲突中的袭击者和受害者方面存在显著差异，从而揭示了媒体内部的偏见。", "summary": "本研究利用计算方法，结合框架语义和大型语言模型，分析了以色列-巴勒斯坦战争新闻报道中的媒体框架。研究旨在克服现有冲突框架研究的局限性，深入探讨媒体如何塑造读者对冲突的看法。结果显示，媒体更倾向于战争报道，而非和平报道，并且不同国家（美国、英国、中东）的媒体在描绘冲突中的袭击者和受害者时存在显著差异，揭示了媒体偏见。", "keywords": "媒体框架, 冲突报道, 战争与和平新闻, 计算方法, 偏见分析", "comments": "该论文的创新之处在于其采用计算方法，结合框架语义和大型语言模型来分析媒体框架，克服了传统定性研究的局限性。通过对战争与和平新闻指标的识别以及对不同地区媒体偏见的揭示，该研究为理解媒体在冲突报道中的作用及其对公众认知的潜在影响提供了重要见解。"}}
{"id": "2506.10665", "title": "GOLIATH: A Decentralized Framework for Data Collection in Intelligent Transportation Systems", "authors": ["Davide Maffiola", "Stefano Longari", "Michele Carminati", "Mara Tanelli", "Stefano Zanero"], "summary": "Intelligent Transportation Systems (ITSs) technology has advanced during the\npast years, and it is now used for several applications that require vehicles\nto exchange real-time data, such as in traffic information management.\nTraditionally, road traffic information has been collected using on-site\nsensors. However, crowd-sourcing traffic information from onboard sensors or\nsmartphones has become a viable alternative. State-of-the-art solutions\ncurrently follow a centralized model where only the service provider has\ncomplete access to the collected traffic data and represent a single point of\nfailure and trust. In this paper, we propose GOLIATH, a blockchain-based\ndecentralized framework that runs on the In-Vehicle Infotainment (IVI) system\nto collect real-time information exchanged between the network's participants.\nOur approach mitigates the limitations of existing crowd-sourcing centralized\nsolutions by guaranteeing trusted information collection and exchange, fully\nexploiting the intrinsic distributed nature of vehicles. We demonstrate its\nfeasibility in the context of vehicle positioning and traffic information\nmanagement. Each vehicle participating in the decentralized network shares its\nposition and neighbors' ones in the form of a transaction recorded on the\nledger, which uses a novel consensus mechanism to validate it. We design the\nconsensus mechanism resilient against a realistic set of adversaries that aim\nto tamper or disable the communication. We evaluate the proposed framework in a\nsimulated (but realistic) environment, which considers different threats and\nallows showing its robustness and safety properties.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10665v1", "AI": {"title_translation": "GOLIATH：智能交通系统中数据收集的去中心化框架", "tldr": "GOLIATH是一个基于区块链的去中心化框架，用于在智能交通系统中收集实时交通数据，解决了现有中心化众包方案的信任和单点故障问题。", "motivation": "传统的交通信息收集方式依赖于现场传感器或中心化众包模型，后者存在单点故障和信任问题，且服务提供商拥有对所有数据的完全访问权限。本文旨在解决这些现有众包解决方案的局限性，确保可信的信息收集和交换。", "method": "本文提出了GOLIATH，一个基于区块链的去中心化框架，运行在车载信息娱乐（IVI）系统上，用于收集网络参与者之间交换的实时信息。该方法利用车辆固有的分布式特性，通过一种新颖的共识机制来验证记录在账本上的交易（包括车辆位置及其邻居的位置），并设计该共识机制以抵御篡改或禁用通信的对手。", "result": "在模拟的（但真实的）环境中评估了所提出的框架，该环境考虑了不同的威胁，并展示了其鲁棒性和安全特性。在车辆定位和交通信息管理背景下，证明了其可行性。", "conclusion": "GOLIATH框架通过其去中心化和基于区块链的方法，有效解决了智能交通系统中数据收集的信任和中心化问题，并在模拟环境中展示了其鲁棒性和安全性。", "translation": "智能交通系统（ITS）技术在过去几年中取得了进步，现在它被用于需要车辆交换实时数据的多种应用，例如交通信息管理。传统上，道路交通信息是使用现场传感器收集的。然而，从车载传感器或智能手机众包交通信息已成为一种可行的替代方案。目前最先进的解决方案遵循中心化模型，其中只有服务提供商才能完全访问收集到的交通数据，并代表着单点故障和信任问题。在本文中，我们提出了GOLIATH，一个基于区块链的去中心化框架，它运行在车载信息娱乐（IVI）系统上，用于收集网络参与者之间交换的实时信息。我们的方法通过保证可信的信息收集和交换，充分利用车辆固有的分布式特性，减轻了现有众包中心化解决方案的局限性。我们在车辆定位和交通信息管理背景下展示了其可行性。参与去中心化网络的每辆车都以记录在账本上的交易形式共享其位置及其邻居的位置，该账本使用一种新颖的共识机制来验证它。我们将共识机制设计为能够抵御旨在篡改或禁用通信的一系列现实对手。我们在一个模拟的（但真实的）环境中评估了所提出的框架，该环境考虑了不同的威胁，并允许展示其鲁棒性和安全特性。", "summary": "本文提出了GOLIATH，一个基于区块链的去中心化框架，旨在解决智能交通系统（ITS）中传统中心化数据收集方案的信任和单点故障问题。GOLIATH运行在车载信息娱乐（IVI）系统上，利用车辆的分布式特性，通过新颖的共识机制在分布式账本上安全地收集和交换实时交通信息（如车辆位置）。模拟环境下的评估表明，该框架在面对潜在威胁时表现出良好的鲁棒性和安全性，验证了其在车辆定位和交通信息管理中的可行性。", "keywords": "智能交通系统, 去中心化框架, 区块链, 数据收集, 众包", "comments": "GOLIATH的创新之处在于将区块链技术应用于智能交通系统的数据收集，有效解决了传统中心化方案中存在的信任和单点故障问题。其去中心化设计充分利用了车辆的分布式特性，提高了数据收集的可靠性和安全性。该框架对于未来智能交通系统的数据完整性和隐私保护具有重要意义。"}}
{"id": "2506.10353", "title": "Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation", "authors": ["Runqi Ouyang", "Haoyun Li", "Zhenyuan Zhang", "Xiaofeng Wang", "Zheng Zhu", "Guan Huang", "Xingang Wang"], "summary": "Recent advances in large language models, especially in natural language\nunderstanding and reasoning, have opened new possibilities for text-to-motion\ngeneration. Although existing approaches have made notable progress in semantic\nalignment and motion synthesis, they often rely on end-to-end mapping\nstrategies that fail to capture deep linguistic structures and logical\nreasoning. Consequently, generated motions tend to lack controllability,\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\na unified motion-language modeling framework that integrates a Chain-of-Thought\nmechanism. By explicitly decomposing complex textual instructions into\nlogically structured action paths, Motion-R1 provides high-level semantic\nguidance for motion generation, significantly enhancing the model's ability to\ninterpret and execute multi-step, long-horizon, and compositionally rich\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\nreinforcement learning algorithm designed for large models, which leverages\nmotion quality feedback to optimize reasoning chains and motion synthesis\njointly. Extensive experiments across multiple benchmark datasets demonstrate\nthat Motion-R1 achieves competitive or superior performance compared to\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\nunderstanding and long-term temporal coherence. The code, model and data will\nbe publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10353v1", "AI": {"title_translation": "Motion-R1：用于人体动作生成的思维链推理与强化学习", "tldr": "Motion-R1 结合思维链推理和强化学习，解决了现有文本到动作生成中缺乏可控性、一致性和多样性的问题，提高了复杂指令下的动作生成质量。", "motivation": "现有文本到动作生成方法依赖端到端映射，未能捕捉深层语言结构和逻辑推理，导致生成动作缺乏可控性、一致性和多样性。", "method": "提出 Motion-R1 框架，整合思维链机制，将复杂文本指令分解为逻辑结构化的动作路径，提供高级语义指导。采用 Group Relative Policy Optimization 强化学习算法，利用动作质量反馈联合优化推理链和动作合成。", "result": "在多个基准数据集上的广泛实验表明，Motion-R1 实现了与最先进方法相当或更优的性能，特别是在需要细致语义理解和长期时间连贯性的场景中。", "conclusion": "Motion-R1 通过引入思维链推理和强化学习，有效解决了文本到动作生成中的挑战，显著提升了生成动作的质量和对复杂指令的执行能力。", "translation": "大型语言模型在自然语言理解和推理方面的最新进展为文本到动作生成开辟了新的可能性。尽管现有方法在语义对齐和动作合成方面取得了显著进展，但它们通常依赖于端到端映射策略，未能捕捉深层语言结构和逻辑推理。因此，生成的动作往往缺乏可控性、一致性和多样性。为了解决这些限制，我们提出了 Motion-R1，一个统一的运动-语言建模框架，它集成了思维链机制。通过将复杂的文本指令明确分解为逻辑结构化的动作路径，Motion-R1 为动作生成提供了高级语义指导，显著增强了模型解释和执行多步、长时和组合丰富命令的能力。为了训练我们的模型，我们采用了群组相对策略优化（Group Relative Policy Optimization），这是一种为大型模型设计的强化学习算法，它利用动作质量反馈联合优化推理链和动作合成。在多个基准数据集上进行的广泛实验表明，Motion-R1 实现了与最先进方法相当或更优的性能，特别是在需要细致语义理解和长期时间连贯性的场景中。代码、模型和数据将公开可用。", "summary": "Motion-R1 是一个统一的运动-语言建模框架，它通过整合思维链机制和专门设计的强化学习算法（Group Relative Policy Optimization），解决了现有文本到动作生成方法中因未能捕捉深层语言结构和逻辑推理而导致的可控性、一致性和多样性不足的问题。该框架将复杂文本指令分解为逻辑结构化的动作路径，从而增强了模型解释和执行多步、长时和组合丰富命令的能力。实验证明，Motion-R1 在需要细致语义理解和长期时间连贯性的场景中，性能达到或超越了现有SOTA方法。", "keywords": "思维链, 强化学习, 人体动作生成, 文本到动作, 语言模型", "comments": "创新点在于将大语言模型的思维链推理能力引入到运动生成领域，并通过强化学习进行优化，有效提升了复杂指令下动作生成的可控性和语义一致性。这是一个将 LLM 优势应用于多模态生成任务的有效尝试，为未来更复杂、更自然的动作生成提供了新思路。其公开代码、模型和数据的承诺也有助于社区进一步研究。"}}
{"id": "2506.10189", "title": "Improving Oral Cancer Outcomes Through Machine Learning and Dimensionality Reduction", "authors": ["Mohammad Subhi Al-Batah", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon"], "summary": "Oral cancer presents a formidable challenge in oncology, necessitating early\ndiagnosis and accurate prognosis to enhance patient survival rates. Recent\nadvancements in machine learning and data mining have revolutionized\ntraditional diagnostic methodologies, providing sophisticated and automated\ntools for differentiating between benign and malignant oral lesions. This study\npresents a comprehensive review of cutting-edge data mining methodologies,\nincluding Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines\n(SVM), and ensemble learning techniques, specifically applied to the diagnosis\nand prognosis of oral cancer. Through a rigorous comparative analysis, our\nfindings reveal that Neural Networks surpass other models, achieving an\nimpressive classification accuracy of 93,6 % in predicting oral cancer.\nFurthermore, we underscore the potential benefits of integrating feature\nselection and dimensionality reduction techniques to enhance model performance.\nThese insights underscore the significant promise of advanced data mining\ntechniques in bolstering early detection, optimizing treatment strategies, and\nultimately improving patient outcomes in the realm of oral oncology.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10189v1", "AI": {"title_translation": "通过机器学习和降维改善口腔癌预后", "tldr": "本研究综述了应用于口腔癌诊断和预后的机器学习方法，发现神经网络在预测中表现最佳，并强调了特征选择和降维的潜力。", "motivation": "口腔癌是肿瘤学中的严峻挑战，需要早期诊断和准确预后以提高患者生存率。机器学习和数据挖掘的进步为区分良性和恶性口腔病变提供了先进的自动化工具。", "method": "本研究对应用于口腔癌诊断和预后的前沿数据挖掘方法进行了全面综述，包括神经网络、K-近邻（KNN）、支持向量机（SVM）和集成学习技术，并进行了严格的比较分析。", "result": "比较分析结果显示，神经网络在预测口腔癌方面超越其他模型，分类准确率达到93.6%。研究还强调了整合特征选择和降维技术以提高模型性能的潜在益处。", "conclusion": "高级数据挖掘技术在加强早期检测、优化治疗策略以及最终改善口腔肿瘤患者预后方面具有重要前景。", "translation": "口腔癌是肿瘤学中一个严峻的挑战，需要早期诊断和准确预后以提高患者生存率。机器学习和数据挖掘的最新进展彻底改变了传统的诊断方法，为区分良性和恶性口腔病变提供了复杂而自动化的工具。本研究对前沿数据挖掘方法进行了全面综述，包括神经网络、K-近邻（KNN）、支持向量机（SVM）和集成学习技术，这些技术专门应用于口腔癌的诊断和预后。通过严格的比较分析，我们的研究结果表明，神经网络超越了其他模型，在预测口腔癌方面取得了93.6%的令人印象深刻的分类准确率。此外，我们强调了整合特征选择和降维技术以提高模型性能的潜在益处。这些见解强调了先进数据挖掘技术在加强早期检测、优化治疗策略并最终改善口腔肿瘤患者预后方面的重要前景。", "summary": "本研究综述并比较了多种机器学习和数据挖掘方法在口腔癌诊断和预后中的应用。通过严格分析，研究发现神经网络在口腔癌预测中表现最佳，分类准确率达93.6%。论文还指出，结合特征选择和降维技术能进一步提升模型性能，并强调了高级数据挖掘技术在改善口腔癌患者预后方面的巨大潜力。", "keywords": "口腔癌, 机器学习, 神经网络, 诊断, 预后, 维度降低", "comments": "这篇论文的创新点在于它对多种机器学习模型在口腔癌诊断和预后中的应用进行了系统的比较分析，并明确指出了神经网络的优越性。其重要性在于为临床医生和研究人员提供了关于利用AI技术改善口腔癌管理的新视角和具体数据支持。未来的研究可以深入探讨如何将特征选择和降维技术更有效地整合到这些模型中，以及在真实世界临床数据上的验证。"}}
{"id": "2506.10912", "title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?", "authors": ["Fei Lin", "Ziyang Gong", "Cong Wang", "Yonglin Tian", "Tengchao Zhang", "Xue Yang", "Gen Luo", "Fei-Yue Wang"], "summary": "Toxicity remains a leading cause of early-stage drug development failure.\nDespite advances in molecular design and property prediction, the task of\nmolecular toxicity repair - generating structurally valid molecular\nalternatives with reduced toxicity - has not yet been systematically defined or\nbenchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task\nfor general-purpose Multimodal Large Language Models (MLLMs) focused on\nmolecular toxicity repair. We construct a standardized dataset covering 11\nprimary tasks and 560 representative toxic molecules spanning diverse\nmechanisms and granularities. We design a prompt annotation pipeline with\nmechanism-aware and task-adaptive capabilities, informed by expert\ntoxicological knowledge. In parallel, we propose an automated evaluation\nframework, ToxiEval, which integrates toxicity endpoint prediction, synthetic\naccessibility, drug-likeness, and structural similarity into a high-throughput\nevaluation chain for repair success. We systematically assess nearly 30\nmainstream general-purpose MLLMs and design multiple ablation studies to\nanalyze key factors such as evaluation criteria, candidate diversity, and\nfailure attribution. Experimental results show that although current MLLMs\nstill face significant challenges on this task, they begin to demonstrate\npromising capabilities in toxicity understanding, semantic constraint\nadherence, and structure-aware molecule editing.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10912v1", "AI": {"title_translation": "打破有害分子：多模态大语言模型准备好进行结构级分子解毒了吗？", "tldr": "本文引入了ToxiMol，首个针对多模态大语言模型（MLLMs）的分子毒性修复基准任务，并提出了自动化评估框架ToxiEval，旨在解决早期药物开发中分子毒性导致失败的问题。研究发现当前MLLMs在该任务上仍面临挑战，但已展现出初步潜力。", "motivation": "毒性仍然是早期药物开发失败的主要原因。尽管分子设计和性质预测取得了进展，但分子毒性修复（即生成毒性降低且结构有效的分子替代品）尚未被系统地定义或基准化。", "method": "为了填补这一空白，本文引入了ToxiMol，这是首个针对通用多模态大语言模型（MLLMs）的分子毒性修复基准任务。构建了一个涵盖11个主要任务和560个代表性有毒分子的标准化数据集。设计了一个由专家毒理学知识指导的、机制感知和任务自适应的提示注释流程。同时，提出了一个自动化评估框架ToxiEval，该框架整合了毒性终点预测、合成可及性、药物相似性和结构相似性，形成高通量评估链。系统评估了近30个主流通用MLLMs，并设计了多项消融研究来分析关键因素。", "result": "实验结果表明，尽管当前的多模态大语言模型（MLLMs）在此任务上仍面临重大挑战，但它们已开始在毒性理解、语义约束遵守和结构感知分子编辑方面展现出有前景的能力。", "conclusion": "当前的通用多模态大语言模型在分子毒性修复任务中仍面临显著挑战，但已初步展现出在毒性理解和分子编辑方面的潜力，表明该领域仍需进一步发展。", "translation": "毒性仍然是早期药物开发失败的主要原因。尽管分子设计和性质预测取得了进展，但分子毒性修复（即生成毒性降低且结构有效的分子替代品）尚未被系统地定义或基准化。为了填补这一空白，我们引入了ToxiMol，这是首个针对通用多模态大语言模型（MLLMs）的分子毒性修复基准任务。我们构建了一个涵盖11个主要任务和560个代表性有毒分子的标准化数据集，这些分子跨越了不同的机制和粒度。我们设计了一个由专家毒理学知识指导的、机制感知和任务自适应的提示注释流程。同时，我们提出了一个自动化评估框架ToxiEval，该框架将毒性终点预测、合成可及性、药物相似性和结构相似性整合到一个高通量评估链中，用于评估修复成功率。我们系统评估了近30个主流通用MLLMs，并设计了多项消融研究来分析评估标准、候选多样性和失败归因等关键因素。实验结果表明，尽管当前的多模态大语言模型（MLLMs）在此任务上仍面临重大挑战，但它们已开始在毒性理解、语义约束遵守和结构感知分子编辑方面展现出有前景的能力。", "summary": "本文针对早期药物开发中分子毒性导致失败的问题，首次引入了ToxiMol，一个用于通用多模态大语言模型（MLLMs）的分子毒性修复基准任务。研究构建了包含560个有毒分子的标准化数据集，并设计了基于专家知识的提示注释流程。同时，提出了自动化评估框架ToxiEval，用于综合评估修复效果。通过评估近30个主流MLLMs，结果显示尽管现有模型仍面临挑战，但已在毒性理解和结构感知分子编辑方面展现出初步潜力。", "keywords": "分子毒性修复, MLLMs, 基准测试, ToxiMol, 药物开发", "comments": "该论文的创新之处在于首次系统地定义并建立了针对多模态大语言模型（MLLMs）的分子毒性修复基准任务ToxiMol及其自动化评估框架ToxiEval，填补了该领域的空白。其重要性在于直接解决了药物开发中一个关键的瓶颈问题——分子毒性，为利用先进的AI模型进行药物优化提供了新的方向。然而，论文也指出了当前MLLMs在该任务上仍面临显著挑战，提示未来研究需进一步提升模型的毒性修复能力和结构编辑精度。"}}
{"id": "2506.10446", "title": "Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty", "authors": ["Zehui Ling", "Deshu Chen", "Hongwei Zhang", "Yifeng Jiao", "Xin Guo", "Yuan Cheng"], "summary": "Large language models (LLMs) have demonstrated significant advancements in\nreasoning capabilities, performing well on various challenging benchmarks.\nTechniques like Chain-of-Thought prompting have been introduced to further\nimprove reasoning. However, these approaches frequently generate longer\noutputs, which in turn increase computational latency. Although some methods\nuse reinforcement learning to shorten reasoning, they often apply uniform\npenalties without considering the problem's complexity, leading to suboptimal\noutcomes. In this study, we seek to enhance the efficiency of LLM reasoning by\npromoting conciseness for simpler problems while preserving sufficient\nreasoning for more complex ones for accuracy, thus improving the model's\noverall performance. Specifically, we manage the model's reasoning efficiency\nby dividing the reward function and including a novel penalty for output\nlength. Our approach has yielded impressive outcomes in benchmark evaluations\nacross three datasets: GSM8K, MATH500, and AIME2024. For the comparatively\nsimpler datasets GSM8K and MATH500, our method has effectively shortened output\nlengths while preserving or enhancing accuracy. On the more demanding AIME2024\ndataset, our approach has resulted in improved accuracy.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10446v1", "AI": {"title_translation": "易者快速，难者深入：基于幂律长度惩罚的高效推理", "tldr": "本文提出一种新的长度惩罚机制，通过对简单问题缩短输出长度并对复杂问题保持充分推理，提高大型语言模型推理效率和准确性。", "motivation": "大型语言模型（LLM）的推理能力虽强，但现有方法（如思维链提示）会生成过长的输出，导致计算延迟。现有通过强化学习缩短推理的方法常采用统一的惩罚，未考虑问题复杂性，导致次优结果。", "method": "本研究通过划分奖励函数并引入一种新颖的输出长度惩罚机制来管理模型的推理效率。该方法旨在对简单问题促进简洁性，同时为复杂问题保留足够的推理，从而在保证准确性的前提下提高整体性能。", "result": "在GSM8K和MATH500等相对简单的数据集上，该方法在保持或提高准确性的同时有效缩短了输出长度。在更具挑战性的AIME2024数据集上，该方法提高了准确性。", "conclusion": "本研究通过引入考虑问题复杂性的新型长度惩罚机制，成功提升了大型语言模型推理的效率和准确性，实现了对不同难度问题的自适应推理。", "translation": "大型语言模型（LLM）在推理能力方面取得了显著进展，在各种具有挑战性的基准测试中表现出色。思维链提示等技术已被引入以进一步提高推理能力。然而，这些方法经常生成更长的输出，进而增加了计算延迟。尽管一些方法使用强化学习来缩短推理，但它们通常应用统一的惩罚，而没有考虑问题的复杂性，导致次优结果。在本研究中，我们旨在通过促进简单问题的简洁性，同时为更复杂的问题保留足够的推理以确保准确性，从而提高LLM推理的效率，进而提高模型的整体性能。具体来说，我们通过划分奖励函数并包含一种新颖的输出长度惩罚机制来管理模型的推理效率。我们的方法在GSM8K、MATH500和AIME2024三个数据集的基准评估中取得了令人印象深刻的成果。对于相对简单的GSM8K和MATH500数据集，我们的方法在保持或提高准确性的同时有效缩短了输出长度。在更具挑战性的AIME2024数据集上，我们的方法提高了准确性。", "summary": "本文针对大型语言模型（LLM）推理中因输出过长导致的效率问题，提出了一种通过“幂律长度惩罚”实现高效推理的新方法。该方法通过在奖励函数中引入一种新颖的长度惩罚机制，能够自适应地对简单问题生成简洁的推理路径，同时为复杂问题保留充分的推理深度，从而在保证准确性的前提下提升推理效率。实验结果表明，该方法在简单数据集上缩短了输出长度并保持或提高了准确性，在复杂数据集上则显著提升了准确性。", "keywords": "大型语言模型, 推理效率, 长度惩罚, 思维链, 问题复杂性", "comments": "本文的创新点在于引入了一种非统一的、考虑问题复杂性的长度惩罚机制，解决了现有方法在效率和准确性之间权衡的不足。这种“易者快速，难者深入”的自适应推理策略对于优化大型语言模型的实际应用具有重要意义，尤其是在需要平衡响应速度和推理深度场景下，具有较强的实用价值。"}}
{"id": "2506.10721", "title": "Commitment Schemes for Multi-Party Computation", "authors": ["Ioan Ionescu", "Ruxandra F. Olimid"], "summary": "The paper presents an analysis of Commitment Schemes (CSs) used in\nMulti-Party Computation (MPC) protocols. While the individual properties of CSs\nand the guarantees offered by MPC have been widely studied in isolation, their\ninterrelation in concrete protocols and applications remains mostly\nunderexplored. This paper presents the relation between the two, with an\nemphasis on (security) properties and their impact on the upper layer MPC. In\nparticular, we investigate how different types of CSs contribute to various MPC\nconstructions and their relation to real-life applications of MPC. The paper\ncan also serve as a tutorial for understanding the cryptographic interplay\nbetween CS and MPC, making it accessible to both researchers and practitioners.\nOur findings emphasize the importance of carefully selecting CS to meet the\nadversarial and functional requirements of MPC, thereby aiming for more robust\nand privacy-preserving cryptographic applications", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10721v1", "AI": {"title_translation": "多方计算中的承诺方案", "tldr": "本文分析了多方计算（MPC）协议中使用的承诺方案（CSs），并探讨了它们与MPC之间的相互关系，强调了CSs的选择对MPC安全性和功能要求的重要性。", "motivation": "尽管承诺方案（CSs）的独立特性和多方计算（MPC）提供的保障已被广泛研究，但它们在具体协议和应用中的相互关系仍未得到充分探索。本文旨在填补这一空白。", "method": "本文分析了多方计算（MPC）协议中使用的承诺方案（CSs），并着重探讨了（安全）属性及其对上层MPC的影响。具体来说，研究了不同类型的CSs如何促进各种MPC构造及其与MPC实际应用的关联。该论文也可作为理解CS和MPC之间密码学相互作用的教程。", "result": "研究结果强调了仔细选择承诺方案（CS）以满足多方计算（MPC）的对抗性和功能要求的重要性。", "conclusion": "本文的结论是，为了实现更健壮和保护隐私的密码学应用，必须仔细选择承诺方案（CS），以满足多方计算（MPC）的对抗性和功能要求。", "translation": "本文分析了多方计算（MPC）协议中使用的承诺方案（CSs）。尽管CSs的独立属性和MPC提供的保障已被广泛研究，但它们在具体协议和应用中的相互关系仍未得到充分探索。本文介绍了两者之间的关系，重点关注（安全）属性及其对上层MPC的影响。特别是，我们研究了不同类型的CSs如何促进各种MPC构造及其与MPC实际应用的关联。本文也可作为理解CS和MPC之间密码学相互作用的教程，使研究人员和从业者都能理解。我们的研究结果强调了仔细选择CS以满足MPC的对抗性和功能要求的重要性，从而旨在实现更健壮和保护隐私的密码学应用。", "summary": "本文深入分析了多方计算（MPC）协议中承诺方案（CSs）的应用及其相互关系。文章指出，尽管CSs和MPC各自的特性已被广泛研究，但它们在实际协议和应用中的结合仍是未充分探索的领域。论文着重探讨了CSs的（安全）属性及其对MPC上层的影响，并研究了不同类型CSs如何支持各种MPC构造及其在现实应用中的关联。文章还可作为一份教程，帮助读者理解CS与MPC之间的密码学相互作用。研究强调，为满足MPC的对抗性和功能需求，精心选择CS至关重要，这有助于构建更安全、更注重隐私的密码学应用。", "keywords": "承诺方案, 多方计算, 密码学, 安全性, 隐私保护", "comments": "本文的创新之处在于填补了承诺方案（CSs）与多方计算（MPC）在实际应用中相互关系研究的空白。它不仅提供了深入的分析，还兼具教程性质，降低了该领域知识的门槛。其重要性在于强调了CSs选择对MPC协议健壮性和隐私保护的关键作用，对研究人员和实践者都具有指导意义。"}}
{"id": "2506.10361", "title": "FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device", "authors": ["Novendra Setyawan", "Chi-Chia Sun", "Mao-Hsiu Hsu", "Wen-Kai Kuo", "Jun-Wei Hsieh"], "summary": "This paper introduces FaceLiVT, a lightweight yet powerful face recognition\nmodel that integrates a hybrid Convolution Neural Network (CNN)-Transformer\narchitecture with an innovative and lightweight Multi-Head Linear Attention\n(MHLA) mechanism. By combining MHLA alongside a reparameterized token mixer,\nFaceLiVT effectively reduces computational complexity while preserving\ncompetitive accuracy. Extensive evaluations on challenging benchmarks;\nincluding LFW, CFP-FP, AgeDB-30, IJB-B, and IJB-C; highlight its superior\nperformance compared to state-of-the-art lightweight models. MHLA notably\nimproves inference speed, allowing FaceLiVT to deliver high accuracy with lower\nlatency on mobile devices. Specifically, FaceLiVT is 8.6 faster than EdgeFace,\na recent hybrid CNN-Transformer model optimized for edge devices, and 21.2\nfaster than a pure ViT-Based model. With its balanced design, FaceLiVT offers\nan efficient and practical solution for real-time face recognition on\nresource-constrained platforms.", "comment": "2025 ICIP", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10361v1", "AI": {"title_translation": "FaceLiVT: 面向移动设备的结构化重参数化线性视觉Transformer人脸识别", "tldr": "FaceLiVT是一种轻量级混合CNN-Transformer模型，结合了多头线性注意力机制和重参数化令牌混合器，在移动设备上实现了高精度、低延迟的人脸识别，比现有模型更快。", "motivation": "在资源受限的移动设备上实现高效且准确的实时人脸识别是一个挑战，需要开发计算复杂度低但性能优越的模型。", "method": "本文提出了FaceLiVT模型，它集成了混合卷积神经网络（CNN）-Transformer架构与创新的轻量级多头线性注意力（MHLA）机制。通过结合MHLA和重参数化令牌混合器，FaceLiVT有效降低了计算复杂度。", "result": "FaceLiVT在LFW、CFP-FP、AgeDB-30、IJB-B和IJB-C等基准测试中表现出优于现有轻量级模型的性能。它比EdgeFace（一种为边缘设备优化的混合CNN-Transformer模型）快8.6倍，比纯ViT模型快21.2倍，实现了高精度和低延迟。", "conclusion": "FaceLiVT通过其平衡的设计，为资源受限平台上的实时人脸识别提供了一种高效实用的解决方案。", "translation": "本文介绍了FaceLiVT，一个轻量级但强大的人脸识别模型，它集成了混合卷积神经网络（CNN）-Transformer架构与创新的轻量级多头线性注意力（MHLA）机制。通过结合MHLA和重参数化令牌混合器，FaceLiVT有效降低了计算复杂度，同时保持了具有竞争力的准确性。在LFW、CFP-FP、AgeDB-30、IJB-B和IJB-C等具有挑战性的基准测试中进行的广泛评估，突出了其相对于最先进轻量级模型的卓越性能。MHLA显著提高了推理速度，使FaceLiVT能够在移动设备上以更低的延迟提供高精度。具体而言，FaceLiVT比EdgeFace（一种最近为边缘设备优化的混合CNN-Transformer模型）快8.6倍，比纯ViT模型快21.2倍。凭借其平衡的设计，FaceLiVT为资源受限平台上的实时人脸识别提供了一种高效实用的解决方案。", "summary": "FaceLiVT是一种专为移动设备设计的轻量级人脸识别模型，它融合了CNN和Transformer的混合架构，并引入了创新的多头线性注意力（MHLA）机制和重参数化令牌混合器。该模型在降低计算复杂度的同时，保持了高精度，并在多个基准测试中展现出优于现有轻量级模型的性能。实验证明，FaceLiVT在推理速度上显著优于EdgeFace和纯ViT模型，为资源受限平台上的实时人脸识别提供了高效实用的解决方案。", "keywords": "人脸识别, 线性视觉Transformer, 结构化重参数化, 移动设备, 轻量级模型", "comments": "FaceLiVT的创新之处在于其混合CNN-Transformer架构与轻量级MHLA和重参数化令牌混合器的结合，有效解决了移动设备上人脸识别的计算效率和准确性平衡问题。其速度提升尤为显著，显示了在实际应用中的巨大潜力。"}}
{"id": "2506.10200", "title": "DynaSubVAE: Adaptive Subgrouping for Scalable and Robust OOD Detection", "authors": ["Tina Behrouzi", "Sana Tonekaboni", "Rahul G. Krishnan", "Anna Goldenberg"], "summary": "Real-world observational data often contain existing or emerging\nheterogeneous subpopulations that deviate from global patterns. The majority of\nmodels tend to overlook these underrepresented groups, leading to inaccurate or\neven harmful predictions. Existing solutions often rely on detecting these\nsamples as Out-of-domain (OOD) rather than adapting the model to new emerging\npatterns. We introduce DynaSubVAE, a Dynamic Subgrouping Variational\nAutoencoder framework that jointly performs representation learning and\nadaptive OOD detection. Unlike conventional approaches, DynaSubVAE evolves with\nthe data by dynamically updating its latent structure to capture new trends. It\nleverages a novel non-parametric clustering mechanism, inspired by Gaussian\nMixture Models, to discover and model latent subgroups based on embedding\nsimilarity. Extensive experiments show that DynaSubVAE achieves competitive\nperformance in both near-OOD and far-OOD detection, and excels in class-OOD\nscenarios where an entire class is missing during training. We further\nillustrate that our dynamic subgrouping mechanism outperforms standalone\nclustering methods such as GMM and KMeans++ in terms of both OOD accuracy and\nregret precision.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10200v1", "AI": {"title_translation": "DynaSubVAE：可扩展和鲁棒的OOD检测自适应子分组", "tldr": "DynaSubVAE是一个动态子分组变分自编码器框架，用于联合执行表示学习和自适应OOD检测，通过动态更新其潜在结构来捕获新趋势，并在各种OOD检测场景中表现出色。", "motivation": "现实世界观测数据中常包含异质子群体，这些子群体偏离全局模式，且现有模型往往忽视这些代表性不足的群体，导致不准确或有害的预测。现有解决方案多依赖于将这些样本检测为域外（OOD）而非使模型适应新出现的模式。", "method": "本文引入了DynaSubVAE，一个动态子分组变分自编码器框架，它联合执行表示学习和自适应OOD检测。DynaSubVAE通过动态更新其潜在结构来捕获新趋势，并利用受高斯混合模型启发的非参数聚类机制，基于嵌入相似性发现和建模潜在子群体。", "result": "实验表明，DynaSubVAE在近OOD和远OOD检测中均取得了有竞争力的性能，并在训练期间缺少整个类别的类别OOD场景中表现出色。此外，其动态子分组机制在OOD准确性和遗憾精度方面优于单独的聚类方法（如GMM和KMeans++）。", "conclusion": "DynaSubVAE通过其独特的动态子分组机制，能够有效应对数据中的异质性，并显著提升了OOD检测的性能，尤其是在处理新出现或未见的类别时。", "translation": "现实世界观测数据通常包含现有或新兴的异质子群体，这些子群体偏离全局模式。大多数模型倾向于忽视这些代表性不足的群体，导致不准确甚至有害的预测。现有解决方案通常依赖于将这些样本检测为域外（OOD），而不是使模型适应新出现的模式。我们引入了DynaSubVAE，一个动态子分组变分自编码器框架，它联合执行表示学习和自适应OOD检测。与传统方法不同，DynaSubVAE通过动态更新其潜在结构以捕获新趋势，从而随数据演进。它利用一种新颖的非参数聚类机制，灵感来源于高斯混合模型，根据嵌入相似性发现和建模潜在子群体。广泛的实验表明，DynaSubVAE在近OOD和远OOD检测中均取得了有竞争力的性能，并在训练期间缺少整个类别的类别OOD场景中表现出色。我们进一步说明，我们的动态子分组机制在OOD准确性和遗憾精度方面优于单独的聚类方法，例如GMM和KMeans++。", "summary": "DynaSubVAE是一种新颖的动态子分组变分自编码器框架，旨在解决现实世界数据中异质子群体被现有模型忽视的问题。该框架通过联合表示学习和自适应OOD检测，并动态更新其潜在结构以适应数据新趋势。它利用一种受高斯混合模型启发的非参数聚类机制来发现和建模潜在子群体。实验证明，DynaSubVAE在各种OOD检测场景（包括近OOD、远OOD和类别OOD）中均表现出色，并且其动态子分组机制优于传统的独立聚类方法。", "keywords": "OOD检测, 子分组, 变分自编码器, 动态学习, 异质性数据", "comments": "DynaSubVAE的创新点在于其动态子分组机制，它允许模型随数据演进而适应新模式，而非简单地将异常样本识别为OOD。这对于处理现实世界中不断演变的数据分布具有重要意义，尤其是在面对训练期间未见的类别时。其结合表示学习和自适应OOD检测的方法也很有前景。"}}
{"id": "2506.10947", "title": "Spurious Rewards: Rethinking Training Signals in RLVR", "authors": ["Rulin Shao", "Shuyue Stella Li", "Rui Xin", "Scott Geng", "Yiping Wang", "Sewoong Oh", "Simon Shaolei Du", "Nathan Lambert", "Sewon Min", "Ranjay Krishna", "Yulia Tsvetkov", "Hannaneh Hajishirzi", "Pang Wei Koh", "Luke Zettlemoyer"], "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit\nstrong mathematical reasoning in certain models even with spurious rewards that\nhave little, no, or even negative correlation with the correct answer. For\nexample, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute\npoints by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect\nlabel), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the\n29.1% gained with ground truth rewards. However, the spurious rewards that work\nfor Qwen often fail to yield gains with other model families like Llama3 or\nOLMo2. In particular, we find code reasoning -- thinking in code without actual\ncode execution -- to be a distinctive Qwen2.5-Math behavior that becomes\nsignificantly more frequent after RLVR, from 65% to over 90%, even with\nspurious rewards. Overall, we hypothesize that, given the lack of useful reward\nsignal, RLVR must somehow be surfacing useful reasoning representations learned\nduring pretraining, although the exact mechanism remains a topic for future\nwork. We suggest that future RLVR research should possibly be validated on\ndiverse models rather than a single de facto choice, as we show that it is easy\nto get significant performance gains on Qwen models even with completely\nspurious reward signals.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.10947v1", "AI": {"title_translation": "虚假奖励：重新思考RLVR中的训练信号", "tldr": "RLVR即使使用与正确答案无关的虚假奖励，也能在Qwen模型中激发强大的数学推理能力，但对其他模型无效，表明RLVR可能是在利用预训练中学到的有用推理表示。", "motivation": "该研究的动机是重新思考强化学习中可验证奖励（RLVR）的训练信号，特别是探索即使奖励信号与正确答案关联性很低、没有甚至负相关时，RLVR是否仍能有效，并挑战传统上对奖励信号质量的假设。", "method": "研究通过在Qwen2.5-Math-7B模型上使用强化学习与可验证奖励（RLVR），并测试多种虚假奖励（如随机奖励、格式奖励、不正确标签、1-shot RL、多数投票）来评估其对数学推理性能的影响。同时，研究也在Llama3和OLMo2等其他模型家族上进行了对比实验，并分析了代码推理行为的频率变化。", "result": "研究发现，RLVR即使使用虚假奖励，也能显著提高Qwen2.5-Math-7B在MATH-500上的性能，绝对点数提升高达21.4%至27.1%，接近使用真实奖励的提升（29.1%）。然而，这些虚假奖励对Llama3或OLMo2等其他模型家族无效。此外，Qwen2.5-Math模型中的代码推理行为在RLVR后显著增加，从65%提高到90%以上，即使使用虚假奖励也是如此。", "conclusion": "研究得出结论，RLVR在缺乏有用奖励信号的情况下，可能通过某种方式激活或利用了预训练期间学习到的有用推理表示。鉴于此，未来的RLVR研究应在多样化的模型上进行验证，而非仅限于单一模型，因为在Qwen模型上即使使用完全虚假的奖励信号也很容易获得显著性能提升。", "translation": "我们展示了可验证奖励强化学习（RLVR）即使在奖励与正确答案关联性很低、没有甚至负相关的情况下，也能在某些模型中激发强大的数学推理能力。例如，RLVR使Qwen2.5-Math-7B在MATH-500上的性能绝对点数提高了21.4%（随机奖励）、13.8%（格式奖励）、24.1%（不正确标签）、26.0%（1-shot RL）和27.1%（多数投票）——几乎与使用真实奖励获得的29.1%提升相匹配。然而，对Qwen有效的虚假奖励往往无法在Llama3或OLMo2等其他模型家族中带来收益。特别是，我们发现代码推理——无需实际代码执行的以代码方式思考——是Qwen2.5-Math的一个独特行为，在RLVR后其频率显著增加，从65%提高到90%以上，即使使用虚假奖励也是如此。总的来说，我们推测，鉴于缺乏有用的奖励信号，RLVR必定以某种方式浮现了预训练期间学习到的有用推理表示，尽管其确切机制仍是未来工作的主题。我们建议未来的RLVR研究应可能在多样化的模型上进行验证，而不是单一的实际选择，因为我们发现即使使用完全虚假的奖励信号，也很容易在Qwen模型上获得显著的性能提升。", "summary": "该研究探讨了可验证奖励强化学习（RLVR）在奖励信号不准确甚至错误的情况下，仍能提升模型数学推理能力的可能性。实验表明，对于Qwen2.5-Math-7B模型，多种虚假奖励能带来与真实奖励相当的性能提升，并显著增加了模型内部的代码推理行为。然而，这种现象在Llama3或OLMo2等其他模型家族中并未观察到。研究者推测，RLVR可能在利用模型预训练阶段已获得的推理表示，并建议未来的RLVR研究应在更广泛的模型上进行验证，以避免因特定模型特性而产生的误导性结果。", "keywords": "RLVR, 虚假奖励, 数学推理, Qwen, 代码推理", "comments": "这篇论文的创新点在于挑战了RLVR中奖励信号必须准确或有用的传统观念，展示了“虚假奖励”在特定模型（如Qwen）上也能产生显著效果。这引发了对RLVR作用机制的深层思考，即它可能更多地是激活或利用了模型预训练中学到的内在推理能力，而非单纯地通过奖励信号引导学习。其重要性在于为RLVR的研究开辟了新的方向，尤其是在奖励信号难以获取或定义复杂的场景下。同时，论文也指出了一个重要局限性：这种现象具有模型特异性，对其他模型家族无效，这强调了在RLVR研究中进行多模型验证的必要性，避免过度泛化。"}}
{"id": "2506.10486", "title": "Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers", "authors": ["Xanh Ho", "Sunisth Kumar", "Yun-Ang Wu", "Florian Boudin", "Atsuhiro Takasu", "Akiko Aizawa"], "summary": "Scientific claim verification against tables typically requires predicting\nwhether a claim is supported or refuted given a table. However, we argue that\npredicting the final label alone is insufficient: it reveals little about the\nmodel's reasoning and offers limited interpretability. To address this, we\nreframe table-text alignment as an explanation task, requiring models to\nidentify the table cells essential for claim verification. We build a new\ndataset by extending the SciTab benchmark with human-annotated cell-level\nrationales. Annotators verify the claim label and highlight the minimal set of\ncells needed to support their decision. After the annotation process, we\nutilize the collected information and propose a taxonomy for handling ambiguous\ncases. Our experiments show that (i) incorporating table alignment information\nimproves claim verification performance, and (ii) most LLMs, while often\npredicting correct labels, fail to recover human-aligned rationales, suggesting\nthat their predictions do not stem from faithful reasoning.", "comment": "8 pages; code and data are available at\n  https://github.com/Alab-NII/SciTabAlign", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10486v1", "AI": {"title_translation": "表格-文本对齐：解释科学论文中针对表格的断言验证", "tldr": "本文将表格-文本对齐重新定义为解释任务，以提高科学论文中断言验证的可解释性，并发现结合对齐信息能提高性能，但大型语言模型（LLMs）的预测缺乏忠实推理。", "motivation": "传统的科学断言验证仅预测最终标签，这不足以揭示模型的推理过程，且可解释性有限。", "method": "将表格-文本对齐重新定义为解释任务，要求模型识别断言验证所需的关键表格单元格。通过扩展SciTab基准数据集，构建了一个新数据集，其中包含人工标注的单元格级理由。标注者验证断言标签并突出显示支持其决策所需的最小单元格集。在此基础上，提出了一种处理模糊情况的分类法。", "result": "实验结果表明，(i) 结合表格对齐信息可以提高断言验证的性能，(ii) 大多数大型语言模型（LLMs）虽然经常预测出正确的标签，但未能恢复人类对齐的理由，这表明它们的预测并非源于忠实推理。", "conclusion": "结合表格对齐信息可以提高断言验证的性能，但当前的大型语言模型在断言验证任务中，即使预测正确，其推理过程也可能缺乏忠实性。", "translation": "科学断言针对表格的验证通常需要预测给定表格的情况下，断言是被支持还是被驳斥。然而，我们认为仅预测最终标签是不够的：它几乎没有揭示模型的推理过程，并且可解释性有限。为了解决这个问题，我们将表格-文本对齐重新定义为一项解释任务，要求模型识别对断言验证至关重要的表格单元格。我们通过扩展SciTab基准数据集，构建了一个新的数据集，其中包含人工标注的单元格级理由。标注者验证断言标签并突出显示支持其决策所需的最小单元格集。在标注过程之后，我们利用收集到的信息并提出了一种处理模糊情况的分类法。我们的实验表明，(i) 结合表格对齐信息可以提高断言验证性能，(ii) 大多数大型语言模型（LLMs）虽然经常预测出正确的标签，但未能恢复人类对齐的理由，这表明它们的预测并非源于忠实推理。", "summary": "本文提出将科学论文中针对表格的断言验证任务重新定义为表格-文本对齐的解释任务，旨在提高模型的可解释性。研究团队通过扩展SciTab基准数据集，创建了一个包含人类标注单元格级理由的新数据集，并提出了一种处理模糊情况的分类法。实验证明，整合表格对齐信息能有效提升断言验证性能。然而，研究也揭示了多数大型语言模型在预测正确标签的同时，未能复现人类标注的推理理由，这暗示其预测可能并非基于忠实推理。", "keywords": "表格-文本对齐, 断言验证, 可解释性, 科学论文, 大型语言模型", "comments": "这项研究的创新之处在于将传统的断言验证任务转化为一个更具解释性的对齐任务，通过引入单元格级别的理由标注，显著提升了模型决策的可解释性。其重要性体现在：首先，提供了一个新的数据集，为未来研究提供了宝贵资源；其次，实验结果不仅证明了对齐信息对性能的提升作用，更重要的是，它揭示了当前大型语言模型在复杂推理任务中可能存在的“表面”正确性问题，即它们可能在没有真正理解和忠实推理的情况下给出正确答案，这对LLM的可信赖性研究具有重要启示意义。"}}
{"id": "2506.10434", "title": "System Identification Using Kolmogorov-Arnold Networks: A Case Study on Buck Converters", "authors": ["Nart Gashi", "Panagiotis Kakosimos", "George Papafotiou"], "summary": "Kolmogorov-Arnold Networks (KANs) are emerging as a powerful framework for\ninterpretable and efficient system identification in dynamic systems. By\nleveraging the Kolmogorov-Arnold representation theorem, KANs enable function\napproximation through learnable activation functions, offering improved\nscalability, accuracy, and interpretability compared to traditional neural\nnetworks. This paper investigates the application of KANs to model and analyze\nthe dynamics of a buck converter system, focusing on state-space parameter\nestimation along with discovering the system equations. Using simulation data,\nthe methodology involves approximating state derivatives with KANs,\nconstructing interpretable state-space representations, and validating these\nmodels through numerical experiments. The results demonstrate the ability of\nKANs to accurately identify system dynamics, verify model consistency, and\ndetect parameter changes, providing valuable insights into their applicability\nfor system identification in modern industrial systems.", "comment": "2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10434v1", "AI": {"title_translation": "使用科尔莫哥洛夫-阿诺德网络进行系统辨识：以降压变换器为例", "tldr": "本文研究了使用科尔莫哥洛夫-阿诺德网络（KANs）对降压变换器系统进行系统辨识，证明了KANs在准确识别系统动态、验证模型一致性和检测参数变化方面的有效性。", "motivation": "科尔莫哥洛夫-阿诺德网络（KANs）正成为动态系统中可解释且高效的系统辨识的强大框架。与传统神经网络相比，KANs通过可学习的激活函数进行函数逼近，提供了改进的可扩展性、准确性和可解释性。", "method": "该方法包括使用模拟数据，通过KANs逼近状态导数，构建可解释的状态空间表示，并通过数值实验验证这些模型。研究重点是降压变换器系统的建模和分析，以及状态空间参数估计和系统方程的发现。", "result": "结果表明KANs能够准确识别系统动态、验证模型一致性并检测参数变化。", "conclusion": "KANs在现代工业系统中的系统辨识应用具有重要价值。", "translation": "科尔莫哥洛夫-阿诺德网络（KANs）正成为动态系统中可解释且高效的系统辨识的强大框架。通过利用科尔莫哥洛夫-阿诺德表示定理，KANs通过可学习的激活函数实现函数逼近，与传统神经网络相比，提供了改进的可扩展性、准确性和可解释性。本文研究了KANs在对降压变换器系统动态进行建模和分析中的应用，重点关注状态空间参数估计以及系统方程的发现。该方法使用模拟数据，涉及使用KANs逼近状态导数，构建可解释的状态空间表示，并通过数值实验验证这些模型。结果表明KANs能够准确识别系统动态、验证模型一致性并检测参数变化，为其在现代工业系统中的系统辨识应用提供了宝贵的见解。", "summary": "本文探讨了科尔莫哥洛夫-阿诺德网络（KANs）在降压变换器系统辨识中的应用。KANs利用其独特的可学习激活函数，在可扩展性、准确性和可解释性方面优于传统神经网络。研究通过模拟数据，利用KANs逼近状态导数并构建可解释的状态空间模型，成功实现了系统动态的准确识别、模型一致性验证及参数变化检测，展示了KANs在现代工业系统辨识领域的巨大潜力。", "keywords": "科尔莫哥洛夫-阿诺德网络, 系统辨识, 降压变换器, 状态空间, 可解释性", "comments": "本文的创新点在于将新兴的科尔莫哥洛夫-阿诺德网络（KANs）应用于系统辨识领域，特别是在降压变换器上的案例研究。KANs的可解释性和高效性是其相比传统神经网络的显著优势，这对于需要高可靠性和透明度的工业系统尤为重要。研究结果验证了KANs在复杂系统建模和参数估计方面的有效性，为未来在更广泛的动态系统中的应用奠定了基础。"}}
{"id": "2506.10722", "title": "TED-LaST: Towards Robust Backdoor Defense Against Adaptive Attacks", "authors": ["Xiaoxing Mo", "Yuxuan Cheng", "Nan Sun", "Leo Yu Zhang", "Wei Luo", "Shang Gao"], "summary": "Deep Neural Networks (DNNs) are vulnerable to backdoor attacks, where\nattackers implant hidden triggers during training to maliciously control model\nbehavior. Topological Evolution Dynamics (TED) has recently emerged as a\npowerful tool for detecting backdoor attacks in DNNs. However, TED can be\nvulnerable to backdoor attacks that adaptively distort topological\nrepresentation distributions across network layers. To address this limitation,\nwe propose TED-LaST (Topological Evolution Dynamics against Laundry, Slow\nrelease, and Target mapping attack strategies), a novel defense strategy that\nenhances TED's robustness against adaptive attacks. TED-LaST introduces two key\ninnovations: label-supervised dynamics tracking and adaptive layer emphasis.\nThese enhancements enable the identification of stealthy threats that evade\ntraditional TED-based defenses, even in cases of inseparability in topological\nspace and subtle topological perturbations. We review and classify data\npoisoning tricks in state-of-the-art adaptive attacks and propose enhanced\nadaptive attack with target mapping, which can dynamically shift malicious\ntasks and fully leverage the stealthiness that adaptive attacks possess. Our\ncomprehensive experiments on multiple datasets (CIFAR-10, GTSRB, and\nImageNet100) and model architectures (ResNet20, ResNet101) show that TED-LaST\neffectively counteracts sophisticated backdoors like Adap-Blend, Adapt-Patch,\nand the proposed enhanced adaptive attack. TED-LaST sets a new benchmark for\nrobust backdoor detection, substantially enhancing DNN security against\nevolving threats.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10722v1", "AI": {"title_translation": "TED-LaST：迈向针对自适应攻击的鲁棒后门防御", "tldr": "TED-LaST通过引入标签监督的动态跟踪和自适应层强调，增强了拓扑演化动力学（TED）对自适应后门攻击的鲁棒性，有效提升了深度神经网络的安全性。", "motivation": "深度神经网络（DNNs）易受后门攻击，攻击者在训练期间植入隐藏触发器以恶意控制模型行为。尽管拓扑演化动力学（TED）是检测后门攻击的有力工具，但它容易受到自适应攻击的规避，这些攻击会扭曲网络层间的拓扑表示分布。", "method": "本文提出了TED-LaST（针对洗牌、慢释放和目标映射攻击策略的拓扑演化动力学），这是一种新颖的防御策略，旨在增强TED对抗自适应攻击的鲁棒性。TED-LaST引入了两项关键创新：标签监督的动态跟踪和自适应层强调。此外，研究还回顾并分类了最先进自适应攻击中的数据中毒技巧，并提出了一种带有目标映射的增强型自适应攻击，用于动态转移恶意任务并充分利用自适应攻击的隐蔽性。", "result": "在多个数据集（CIFAR-10、GTSRB和ImageNet100）和模型架构（ResNet20、ResNet101）上的综合实验表明，TED-LaST能有效对抗复杂的后门攻击，如Adap-Blend、Adapt-Patch以及提出的增强型自适应攻击。它为鲁棒后门检测树立了新基准。", "conclusion": "TED-LaST显著增强了深度神经网络对抗不断演变威胁的安全性，为鲁棒后门检测设定了新基准。", "translation": "深度神经网络（DNNs）容易受到后门攻击，攻击者在训练期间植入隐藏触发器以恶意控制模型行为。拓扑演化动力学（TED）最近已成为检测DNNs中后门攻击的强大工具。然而，TED可能容易受到自适应扭曲跨网络层拓扑表示分布的后门攻击。为了解决这一局限性，我们提出了TED-LaST（针对洗牌、慢释放和目标映射攻击策略的拓扑演化动力学），这是一种新颖的防御策略，旨在增强TED对抗自适应攻击的鲁棒性。TED-LaST引入了两项关键创新：标签监督的动态跟踪和自适应层强调。这些增强功能能够识别规避传统基于TED防御的隐蔽威胁，即使在拓扑空间中不可分离和细微拓扑扰动的情况下也是如此。我们回顾并分类了最先进自适应攻击中的数据中毒技巧，并提出了一种带有目标映射的增强型自适应攻击，该攻击可以动态转移恶意任务并充分利用自适应攻击所具备的隐蔽性。我们在多个数据集（CIFAR-10、GTSRB和ImageNet100）和模型架构（ResNet20、ResNet101）上的综合实验表明，TED-LaST能有效对抗复杂的后门，如Adap-Blend、Adapt-Patch以及所提出的增强型自适应攻击。TED-LaST为鲁棒后门检测树立了新基准，大幅提升了DNN对抗不断演变威胁的安全性。", "summary": "深度神经网络易受后门攻击，而现有基于拓扑演化动力学（TED）的防御方法容易被自适应攻击规避。为解决此问题，本文提出了一种名为TED-LaST的新型防御策略，它通过引入标签监督的动态跟踪和自适应层强调，显著增强了TED对抗自适应攻击的鲁棒性，使其能够识别传统TED难以察觉的隐蔽威胁。此外，研究还对自适应攻击的数据中毒技巧进行了分类，并提出了一种增强型自适应攻击。在多个数据集和模型上的实验证明，TED-LaST能有效对抗多种复杂后门攻击，为鲁棒后门检测树立了新基准，从而大幅提升了深度神经网络的安全性。", "keywords": "后门攻击, 自适应攻击, 拓扑演化动力学, 深度神经网络, 后门防御", "comments": "该论文通过增强现有防御机制（TED）来应对更复杂的自适应后门攻击，解决了深度神经网络安全领域的一个关键问题。其创新点在于引入了“标签监督的动态跟踪”和“自适应层强调”两大技术，显著提升了防御的鲁棒性和对隐蔽威胁的识别能力。此外，论文不仅提出了防御方法，还分类并提出了一种“增强型自适应攻击”用于评估，这体现了研究的全面性和严谨性，为验证防御效果提供了强有力的基准。这对于推动鲁棒后门检测技术的发展具有重要意义。"}}
{"id": "2506.10234", "title": "Playing in the Sandbox: A Study on the Usability of Seccomp", "authors": ["Maysara Alhindi", "Joseph Hallett"], "summary": "Sandboxing restricts what applications do, and prevents exploited processes\nbeing abused; yet relatively few applications get sandboxed: why? We report a\nusability trial with 7 experienced Seccomp developers exploring how they\napproached sandboxing an application and the difficulties they faced. The\ndevelopers each approached sandboxing the application differently and each came\nto different solutions. We highlight many challenges of using Seccomp, the\nsandboxing designs by the participants, and what developers think would make it\neasier for them to sandbox applications effectively.", "comment": null, "cate": "cs.OS", "url": "http://arxiv.org/abs/2506.10234v1", "AI": {"title_translation": "在沙箱中玩耍：一项关于Seccomp可用性的研究", "tldr": "尽管沙箱技术能够限制应用程序行为并防止进程被滥用，但很少有应用程序被沙箱化。本研究通过对7名经验丰富的Seccomp开发者进行可用性试验，探讨了他们在沙箱化应用程序时遇到的挑战和不同的解决方案，并提出了改进建议。", "motivation": "沙箱技术能够限制应用程序行为并防止被利用的进程被滥用，但当前很少有应用程序被沙箱化。本研究旨在探究其原因。", "method": "本研究对7名经验丰富的Seccomp开发者进行了一项可用性试验，探讨他们如何对应用程序进行沙箱化以及他们面临的困难。", "result": "开发者们对应用程序的沙箱化方法各不相同，并得出了不同的解决方案。研究揭示了使用Seccomp的诸多挑战、参与者的沙箱设计，以及开发者们认为能更有效地沙箱化应用程序的方法。", "conclusion": "使用Seccomp进行沙箱化存在许多挑战，导致开发者们采用不同的方法和解决方案。为了提高沙箱化应用程序的效率，需要针对这些挑战提供更好的工具和支持。", "translation": "沙箱技术限制了应用程序的行为，并防止被利用的进程被滥用；然而，相对较少的应用程序被沙箱化：为什么？我们报告了一项针对7名经验丰富的Seccomp开发者的可用性试验，探讨了他们如何处理应用程序的沙箱化以及他们面临的困难。开发人员各自以不同的方式进行应用程序的沙箱化，并得出了不同的解决方案。我们强调了使用Seccomp的诸多挑战、参与者的沙箱设计，以及开发者们认为能让他们更有效地沙箱化应用程序的方法。", "summary": "本研究通过对7名经验丰富的Seccomp开发者进行可用性试验，探究了应用程序沙箱化普及率低的原因。结果显示，开发者在沙箱化应用程序时面临诸多挑战，且方法和解决方案各异。研究强调了Seccomp的可用性问题，并提出了开发者对改进工具和流程的期望，以期提高沙箱化效率。", "keywords": "沙箱, Seccomp, 可用性, 安全, 开发者研究", "comments": "这项研究揭示了Seccomp作为一项重要的安全机制在实际应用中面临的可用性挑战。其创新之处在于通过用户研究而非纯技术分析来探究安全工具的采纳率问题，为安全工具的设计和改进提供了宝贵的用户视角。研究结果对于提升沙箱技术的普及和有效性具有重要意义。"}}
{"id": "2506.10366", "title": "FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui", "Yuhan Lyu"], "summary": "The infrared and visible images fusion (IVIF) is receiving increasing\nattention from both the research community and industry due to its excellent\nresults in downstream applications. Existing deep learning approaches often\nutilize convolutional neural networks to extract image features. However, the\ninherently capacity of convolution operations to capture global context can\nlead to information loss, thereby restricting fusion performance. To address\nthis limitation, we propose an end-to-end fusion network named the\nFrequency-Spatial Attention Transformer Fusion Network (FSATFusion). The\nFSATFusion contains a frequency-spatial attention Transformer (FSAT) module\ndesigned to effectively capture discriminate features from source images. This\nFSAT module includes a frequency-spatial attention mechanism (FSAM) capable of\nextracting significant features from feature maps. Additionally, we propose an\nimproved Transformer module (ITM) to enhance the ability to extract global\ncontext information of vanilla Transformer. We conducted both qualitative and\nquantitative comparative experiments, demonstrating the superior fusion quality\nand efficiency of FSATFusion compared to other state-of-the-art methods.\nFurthermore, our network was tested on two additional tasks without any\nmodifications, to verify the excellent generalization capability of FSATFusion.\nFinally, the object detection experiment demonstrated the superiority of\nFSATFusion in downstream visual tasks. Our code is available at\nhttps://github.com/Lmmh058/FSATFusion.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10366v1", "AI": {"title_translation": "FSATFusion：用于红外与可见光图像融合的频率-空间注意力Transformer", "tldr": "提出FSATFusion，一种基于频率-空间注意力Transformer的端到端网络，用于红外与可见光图像融合，解决了传统CNN全局上下文捕获不足的问题，并在融合质量、效率、泛化能力及下游任务中表现优异。", "motivation": "现有深度学习方法在红外与可见光图像融合中常使用CNN提取特征，但CNN捕获全局上下文的能力有限，导致信息丢失，从而限制了融合性能。", "method": "提出了端到端融合网络FSATFusion，其包含频率-空间注意力Transformer (FSAT) 模块，用于有效捕获源图像的判别特征。FSAT模块包含频率-空间注意力机制 (FSAM)，能从特征图中提取重要特征。此外，还提出了改进的Transformer模块 (ITM) 来增强原始Transformer提取全局上下文信息的能力。", "result": "定性和定量比较实验表明，FSATFusion在融合质量和效率上优于其他SOTA方法。网络在无需修改的情况下通过额外两个任务验证了其优异的泛化能力。目标检测实验证明FSATFusion在下游视觉任务中的优越性。", "conclusion": "FSATFusion通过结合频率-空间注意力Transformer有效解决了红外与可见光图像融合中全局上下文捕获的挑战，并在融合性能、效率、泛化能力以及对下游视觉任务的增益方面展现出显著优势。", "translation": "红外与可见光图像融合（IVIF）因其在下游应用中的卓越表现，正受到研究界和工业界日益增长的关注。现有深度学习方法通常利用卷积神经网络来提取图像特征。然而，卷积操作固有的捕获全局上下文的能力限制，可能导致信息丢失，从而限制融合性能。为了解决这一局限性，我们提出了一种名为频率-空间注意力Transformer融合网络（FSATFusion）的端到端融合网络。FSATFusion包含一个频率-空间注意力Transformer（FSAT）模块，旨在有效捕获源图像的判别特征。该FSAT模块包含一个频率-空间注意力机制（FSAM），能够从特征图中提取重要特征。此外，我们提出了一种改进的Transformer模块（ITM）来增强原始Transformer提取全局上下文信息的能力。我们进行了定性和定量比较实验，证明了FSATFusion与现有最先进方法相比，具有卓越的融合质量和效率。此外，我们的网络在不进行任何修改的情况下，在另外两个任务上进行了测试，以验证FSATFusion出色的泛化能力。最后，目标检测实验证明了FSATFusion在下游视觉任务中的优越性。我们的代码可在https://github.com/Lmmh058/FSATFusion 获取。", "summary": "本文提出FSATFusion，一个端到端的红外与可见光图像融合网络，旨在解决传统CNN在捕获全局上下文信息方面的不足。FSATFusion引入了频率-空间注意力Transformer (FSAT) 模块和改进的Transformer模块 (ITM)，以有效提取判别特征和增强全局上下文信息。实验结果表明，FSATFusion在融合质量、效率、泛化能力以及对下游视觉任务的性能提升方面均优于现有最先进方法。", "keywords": "红外与可见光图像融合, Transformer, 注意力机制, 深度学习, 图像融合", "comments": "该论文的创新点在于将Transformer架构引入红外与可见光图像融合领域，并通过设计频率-空间注意力机制和改进的Transformer模块来克服传统CNN在全局上下文捕获方面的限制。其重要性体现在不仅提升了融合性能和效率，还验证了模型在不同任务上的泛化能力以及对下游视觉任务的实际增益，这对于推动IVIF技术的实际应用具有重要意义。"}}
{"id": "2506.10205", "title": "AWP: Activation-Aware Weight Pruning and Quantization with Projected Gradient Descent", "authors": ["Jing Liu", "Toshiaki Koike-Akino", "Ye Wang", "Hassan Mansour", "Matthew Brand"], "summary": "To address the enormous size of Large Language Models (LLMs), model\ncompression methods, such as quantization and pruning, are often deployed,\nespecially on edge devices. In this work, we focus on layer-wise post-training\nquantization and pruning. Drawing connections between activation-aware weight\npruning and sparse approximation problems, and motivated by the success of\nIterative Hard Thresholding (IHT), we propose a unified method for\nActivation-aware Weight pruning and quantization via Projected gradient descent\n(AWP). Our experiments demonstrate that AWP outperforms state-of-the-art LLM\npruning and quantization methods. Theoretical convergence guarantees of the\nproposed method for pruning are also provided.", "comment": "ICML 2025 workshop on Efficient Systems for Foundation Models", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10205v1", "AI": {"title_translation": "AWP：基于投影梯度下降的激活感知权重剪枝与量化", "tldr": "AWP是一种新的方法，通过结合激活感知权重剪枝和量化，利用投影梯度下降来压缩大型语言模型，并在实验中表现优于现有技术，同时提供了理论收敛性保证。", "motivation": "为解决大型语言模型（LLMs）规模过大，尤其是在边缘设备上部署困难的问题，需要有效的模型压缩方法。", "method": "提出了一种名为AWP（Activation-aware Weight pruning and quantization via Projected gradient descent）的统一方法。该方法专注于逐层训练后量化和剪枝，将激活感知权重剪枝与稀疏逼近问题联系起来，并受迭代硬阈值（IHT）成功的启发。", "result": "实验表明，AWP优于最先进的LLM剪枝和量化方法。此外，该方法在剪枝方面的理论收敛性也得到了保证。", "conclusion": "AWP是一种有效且具有理论支持的LLM压缩方法，能够显著提升模型在边缘设备上的部署效率。", "translation": "为解决大型语言模型（LLMs）的巨大规模问题，模型压缩方法，如量化和剪枝，经常被部署，特别是在边缘设备上。在这项工作中，我们专注于逐层训练后量化和剪枝。借鉴激活感知权重剪枝与稀疏逼近问题之间的联系，并受迭代硬阈值（IHT）成功的启发，我们提出了一种通过投影梯度下降（AWP）进行激活感知权重剪枝和量化的统一方法。我们的实验表明，AWP优于最先进的LLM剪枝和量化方法。该方法在剪枝方面的理论收敛性保证也得到了提供。", "summary": "AWP是一种新颖的、统一的逐层训练后方法，用于大型语言模型（LLMs）的激活感知权重剪枝和量化。该方法通过将激活感知权重剪枝与稀疏逼近问题相结合，并借鉴迭代硬阈值（IHT）的成功经验，利用投影梯度下降实现。实验结果表明，AWP在LLM剪枝和量化方面优于现有最先进的方法，并且还提供了所提出剪枝方法的理论收敛性保证。", "keywords": "LLM压缩, 量化, 剪枝, 投影梯度下降, 激活感知", "comments": "本文的创新点在于提出了一个统一的框架AWP，将激活感知权重剪枝和量化相结合，并巧妙地利用了投影梯度下降，借鉴了稀疏逼近和IHT的思想。其重要性在于为大型语言模型在资源受限的边缘设备上的部署提供了高效的压缩解决方案，并通过实验性能和理论收敛性保证了方法的有效性和可靠性。"}}
{"id": "2504.15777", "title": "Tina: Tiny Reasoning Models via LoRA", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Willie Neiswanger"], "summary": "How cost-effectively can strong reasoning abilities be achieved in language\nmodels? Driven by this fundamental question, we present Tina, a family of tiny\nreasoning models achieved with high cost-efficiency. Notably, Tina demonstrates\nthat substantial reasoning performance can be developed using only minimal\nresources, by applying parameter-efficient updates during reinforcement\nlearning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B\nparameter base model. This minimalist approach produces models that achieve\nreasoning performance which is competitive with, and sometimes surpasses, SOTA\nRL reasoning models built upon the same base model. Crucially, this is achieved\nat a tiny fraction of the computational post-training cost employed by existing\nSOTA models. In fact, the best Tina model achieves a >20\\% reasoning\nperformance increase and 43.33\\% Pass@1 accuracy on AIME24, at only \\$9 USD\npost-training and evaluation cost (i.e., an estimated 260x cost reduction). Our\nwork reveals the surprising effectiveness of efficient RL reasoning via LoRA.\nWe validate this across multiple open-source reasoning datasets and various\nablation settings starting with a single, fixed set of hyperparameters.\nFurthermore, we hypothesize that this effectiveness and efficiency stem from\nLoRA rapidly adapting the model to the structural format of reasoning rewarded\nby RL, while largely preserving the base model's underlying knowledge. In\nservice of accessibility and open research, we fully open-source all code,\ntraining logs, and model weights \\& checkpoints.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2504.15777v1", "AI": {"title_translation": "Tina：通过LoRA实现的微型推理模型", "tldr": "Tina展示了如何通过在强化学习中使用LoRA对微型基础模型进行参数高效更新，以极低的成本实现强大的推理能力，其性能可与SOTA模型媲美甚至超越。", "motivation": "探讨如何以经济高效的方式在语言模型中实现强大的推理能力。", "method": "通过对一个仅有1.5B参数的微型基础模型，在强化学习（RL）过程中应用参数高效更新，并利用低秩适应（LoRA）技术，来构建微型推理模型。", "result": "Tina模型在推理性能上与基于相同基础模型的SOTA RL推理模型具有竞争力，有时甚至超越它们。实现这一目标所需的计算后训练成本仅为现有SOTA模型的一小部分。最佳Tina模型在AIME24上实现了超过20%的推理性能提升和43.33%的Pass@1准确率，而其后训练和评估成本仅为9美元（估计成本降低了260倍）。", "conclusion": "我们的工作揭示了通过LoRA进行高效RL推理的惊人有效性。这种有效性和效率源于LoRA能够快速使模型适应RL所奖励的推理结构格式，同时在很大程度上保留了基础模型的底层知识。", "translation": "成本效益地在语言模型中实现强大的推理能力有多大可能？受此基本问题的驱动，我们提出了Tina，一个通过高成本效益实现的微型推理模型家族。值得注意的是，Tina证明了仅需最少的资源即可开发出显著的推理性能，方法是在强化学习（RL）期间，使用低秩适应（LoRA）对一个已经很小的1.5B参数基础模型进行参数高效更新。这种极简主义的方法产生的模型，其推理性能与基于相同基础模型的SOTA RL推理模型相比具有竞争力，有时甚至超越它们。至关重要的是，这仅以现有SOTA模型所用的计算后训练成本的一小部分实现。事实上，最佳的Tina模型在AIME24上实现了超过20%的推理性能提升和43.33%的Pass@1准确率，而其后训练和评估成本仅为9美元（即估计成本降低了260倍）。我们的工作揭示了通过LoRA进行高效RL推理的惊人有效性。我们通过多个开源推理数据集和各种消融设置，从一组固定的超参数开始验证了这一点。此外，我们假设这种有效性和效率源于LoRA能够快速使模型适应RL所奖励的推理结构格式，同时在很大程度上保留了基础模型的底层知识。为了促进可访问性和开放研究，我们完全开源了所有代码、训练日志以及模型权重和检查点。", "summary": "本研究提出了Tina，一个通过低成本实现强大推理能力的微型语言模型家族。通过在强化学习中使用LoRA对1.5B参数的基础模型进行参数高效更新，Tina在极低的计算成本下达到了与SOTA模型相当甚至超越的推理性能。例如，最佳Tina模型在AIME24上以仅9美元的成本实现了显著的性能提升。这项工作强调了通过LoRA进行高效RL推理的有效性，并开源了所有相关资源以促进开放研究。", "keywords": "Tina, LoRA, 推理模型, 强化学习, 成本效益", "comments": "这项工作具有重要的创新性，它展示了如何在极低的计算成本下实现与当前最先进模型相媲美的推理能力，这对于资源受限的研究者和开发者而言具有巨大价值。通过将LoRA应用于强化学习，该研究提供了一个高效训练微型模型的范例，揭示了参数高效更新在提升模型性能方面的潜力。其开源所有资源的举动也值得称赞，极大地促进了可访问性和开放研究。"}}
{"id": "2506.10491", "title": "Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models", "authors": ["Aleksandra Sorokovikova", "Pavel Chizhov", "Iuliia Eremenko", "Ivan P. Yamshchikov"], "summary": "Modern language models are trained on large amounts of data. These data\ninevitably include controversial and stereotypical content, which contains all\nsorts of biases related to gender, origin, age, etc. As a result, the models\nexpress biased points of view or produce different results based on the\nassigned personality or the personality of the user. In this paper, we\ninvestigate various proxy measures of bias in large language models (LLMs). We\nfind that evaluating models with pre-prompted personae on a multi-subject\nbenchmark (MMLU) leads to negligible and mostly random differences in scores.\nHowever, if we reformulate the task and ask a model to grade the user's answer,\nthis shows more significant signs of bias. Finally, if we ask the model for\nsalary negotiation advice, we see pronounced bias in the answers. With the\nrecent trend for LLM assistant memory and personalization, these problems open\nup from a different angle: modern LLM users do not need to pre-prompt the\ndescription of their persona since the model already knows their\nsocio-demographics.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10491v1", "AI": {"title_translation": "表面公平，深层偏见：语言模型中偏见的比较研究", "tldr": "本文研究了大型语言模型中的偏见，发现虽然在预设角色下评估模型可能显示出微不足道的偏见，但在用户评分和薪资谈判建议等任务中会表现出显著偏见，尤其是在个性化和记忆化趋势下，偏见问题变得更加复杂。", "motivation": "现代语言模型在大量数据上训练，这些数据不可避免地包含有争议和刻板印象的内容，从而导致模型表达偏见观点或根据指定或用户个性产生不同结果。因此，本文旨在调查大型语言模型中的各种偏见代理度量。", "method": "研究人员调查了大型语言模型（LLMs）中的各种偏见代理度量。具体方法包括：1) 在多主题基准测试（MMLU）上，使用预设角色评估模型；2) 重新制定任务，要求模型给用户的答案评分；3) 要求模型提供薪资谈判建议。", "result": "研究发现：1) 在MMLU上使用预设角色评估模型时，分数差异微不足道且大多是随机的；2) 当要求模型给用户答案评分时，显示出更显著的偏见迹象；3) 当要求模型提供薪资谈判建议时，答案中表现出明显的偏见。", "conclusion": "尽管在某些评估设置下偏见可能不明显，但大型语言模型中存在深层偏见，尤其是在涉及用户互动和个性化场景时。随着LLM助手记忆和个性化的最新趋势，这些问题从一个不同的角度出现：现代LLM用户不再需要预设其角色描述，因为模型已经知道他们的社会人口统计信息，这使得偏见问题更为复杂和突出。", "translation": "现代语言模型在大量数据上训练。这些数据不可避免地包含有争议和刻板印象的内容，其中包含各种与性别、出身、年龄等相关的偏见。因此，模型会表达偏见观点或根据指定的个性或用户的个性产生不同的结果。在本文中，我们研究了大型语言模型（LLMs）中偏见的各种代理度量。我们发现，在多主题基准测试（MMLU）上，用预设角色评估模型会导致分数差异微不足道且大多是随机的。然而，如果我们重新制定任务，要求模型给用户的答案评分，这会显示出更显著的偏见迹象。最后，如果我们向模型寻求薪资谈判建议，我们会在答案中看到明显的偏见。随着LLM助手记忆和个性化的最新趋势，这些问题从一个不同的角度出现：现代LLM用户不再需要预设其角色描述，因为模型已经知道他们的社会人口统计信息。", "summary": "该论文探讨了大型语言模型（LLMs）中的偏见问题。研究发现，尽管在某些表面评估（如预设角色下的MMLU测试）中偏见不明显，但在更深层次的互动（如评价用户答案和提供薪资谈判建议）中，LLMs 会表现出显著的偏见。作者指出，随着LLM个性化和记忆功能的普及，模型已能获取用户社会人口信息，这将使偏见问题变得更加复杂和突出。", "keywords": "语言模型, 偏见, 公平性, 个性化, 社会人口统计", "comments": "这篇论文通过对比不同偏见测量方法，揭示了语言模型偏见的复杂性和隐蔽性。其创新之处在于区分了“表面公平”和“深层偏见”，并指出在个性化LLM的趋势下，偏见问题将变得更加严峻，因为模型可以直接获取用户数据。这对于LLM的伦理开发和应用具有重要指导意义，强调了未来研究应更加关注实际应用场景中的偏见表现。"}}
{"id": "2506.10744", "title": "ObfusBFA: A Holistic Approach to Safeguarding DNNs from Different Types of Bit-Flip Attacks", "authors": ["Xiaobei Yan", "Han Qiu", "Tianwei Zhang"], "summary": "Bit-flip attacks (BFAs) represent a serious threat to Deep Neural Networks\n(DNNs), where flipping a small number of bits in the model parameters or binary\ncode can significantly degrade the model accuracy or mislead the model\nprediction in a desired way. Existing defenses exclusively focus on protecting\nmodels for specific attacks and platforms, while lacking effectiveness for\nother scenarios. We propose ObfusBFA, an efficient and holistic methodology to\nmitigate BFAs targeting both the high-level model weights and low-level\ncodebase (executables or shared libraries). The key idea of ObfusBFA is to\nintroduce random dummy operations during the model inference, which effectively\ntransforms the delicate attacks into random bit flips, making it much harder\nfor attackers to pinpoint and exploit vulnerable bits. We design novel\nalgorithms to identify critical bits and insert obfuscation operations. We\nevaluate ObfusBFA against different types of attacks, including the adaptive\nscenarios where the attacker increases the flip bit budget to attempt to\ncircumvent our defense. The results show that ObfusBFA can consistently\npreserve the model accuracy across various datasets and DNN architectures while\nsignificantly reducing the attack success rates. Additionally, it introduces\nminimal latency and storage overhead, making it a practical solution for\nreal-world applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10744v1", "AI": {"title_translation": "ObfusBFA：一种保护深度神经网络免受不同类型位翻转攻击的整体方法", "tldr": "ObfusBFA提出一种通过引入随机虚拟操作来抵御针对深度神经网络的位翻转攻击的整体方法，有效降低攻击成功率并保持模型精度。", "motivation": "位翻转攻击（BFAs）对深度神经网络（DNNs）构成严重威胁，通过翻转少量位即可显著降低模型精度或误导预测。现有防御措施仅专注于特定攻击和平台，对其他场景缺乏有效性。", "method": "ObfusBFA通过在模型推理期间引入随机虚拟操作，有效地将精细攻击转化为随机位翻转，使攻击者难以定位和利用脆弱位。该方法设计了新颖的算法来识别关键位并插入混淆操作，旨在抵御针对高层模型权重和低层代码库的BFAs。", "result": "ObfusBFA在各种数据集和DNN架构上都能持续保持模型精度，并显著降低攻击成功率。即使在攻击者增加翻转位预算的自适应场景下，该方法也表现出色。此外，它引入了最小的延迟和存储开销。", "conclusion": "ObfusBFA是一种实用且高效的整体方法，能够有效抵御不同类型的位翻转攻击，同时保持深度神经网络的性能，适用于实际应用。", "translation": "位翻转攻击（BFAs）对深度神经网络（DNNs）构成了严重威胁，其中翻转模型参数或二进制代码中的少量位就可以显著降低模型精度或以期望的方式误导模型预测。现有防御措施仅专注于保护模型免受特定攻击和平台的影响，而在其他场景下缺乏有效性。我们提出了ObfusBFA，一种高效且整体的方法，用于缓解针对高层模型权重和低层代码库（可执行文件或共享库）的BFAs。ObfusBFA的核心思想是在模型推理期间引入随机虚拟操作，这有效地将精细攻击转化为随机位翻转，使攻击者更难精确定位和利用脆弱位。我们设计了新颖的算法来识别关键位并插入混淆操作。我们评估了ObfusBFA对抗不同类型攻击的性能，包括攻击者增加翻转位预算以试图规避我们防御的自适应场景。结果表明，ObfusBFA在各种数据集和DNN架构上都能持续保持模型精度，同时显著降低攻击成功率。此外，它引入了最小的延迟和存储开销，使其成为实际应用的实用解决方案。", "summary": "ObfusBFA提出一种高效且整体的方法来防御针对深度神经网络的位翻转攻击（BFAs），这些攻击可能影响模型权重或底层代码。其核心思想是在推理时引入随机虚拟操作，将精确攻击转化为随机位翻转，从而提高攻击难度。实验证明，ObfusBFA在保持模型精度的同时显著降低了不同类型攻击的成功率，且开销极小，使其成为一个实用的解决方案。", "keywords": "位翻转攻击, 深度神经网络, 模型安全, 混淆, 随机操作", "comments": "ObfusBFA的创新之处在于其“整体”防御方法，不仅针对高层模型权重，还包括低层代码库，并且通过引入随机虚拟操作将精确攻击转化为随机噪声，提高了攻击者的难度。其对自适应攻击的有效性和低开销使其在实际应用中具有重要意义。"}}
{"id": "2506.10371", "title": "Revisiting Transformers with Insights from Image Filtering", "authors": ["Laziz U. Abdullaev", "Maksim Tkachenko", "Tan M. Nguyen"], "summary": "The self-attention mechanism, a cornerstone of Transformer-based\nstate-of-the-art deep learning architectures, is largely heuristic-driven and\nfundamentally challenging to interpret. Establishing a robust theoretical\nfoundation to explain its remarkable success and limitations has therefore\nbecome an increasingly prominent focus in recent research. Some notable\ndirections have explored understanding self-attention through the lens of image\ndenoising and nonparametric regression. While promising, existing frameworks\nstill lack a deeper mechanistic interpretation of various architectural\ncomponents that enhance self-attention, both in its original formulation and\nsubsequent variants. In this work, we aim to advance this understanding by\ndeveloping a unifying image processing framework, capable of explaining not\nonly the self-attention computation itself but also the role of components such\nas positional encoding and residual connections, including numerous later\nvariants. We also pinpoint potential distinctions between the two concepts\nbuilding upon our framework, and make effort to close this gap. We introduce\ntwo independent architectural modifications within transformers. While our\nprimary objective is interpretability, we empirically observe that image\nprocessing-inspired modifications can also lead to notably improved accuracy\nand robustness against data contamination and adversaries across language and\nvision tasks as well as better long sequence understanding.", "comment": "12 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10371v1", "AI": {"title_translation": "用图像滤波的视角重新审视Transformer", "tldr": "本研究通过开发一个统一的图像处理框架，深入解释了Transformer的自注意力机制及其核心组件，并引入了架构修改，从而在语言和视觉任务中实现了显著的准确性、鲁棒性提升以及更好的长序列理解能力。", "motivation": "Transformer中自注意力机制是启发式驱动的，难以解释。现有框架在对自注意力及其增强组件的机械性解释方面存在不足。因此，建立一个鲁棒的理论基础来解释其成功和局限性成为了当前研究的重点。", "method": "开发了一个统一的图像处理框架，旨在解释自注意力计算以及位置编码和残差连接等组件的作用。在此框架基础上，识别并努力弥合概念间的潜在区别。同时，在Transformer内部引入了两个独立的架构修改。", "result": "图像处理启发的修改在语言和视觉任务中显著提高了准确性，增强了对数据污染和对抗性攻击的鲁棒性，并改善了长序列理解能力。", "conclusion": "通过提供一个统一的图像处理框架来解释Transformer的自注意力机制和相关组件，不仅加深了对其工作原理的理解，而且经验性地带来了性能和鲁棒性的显著提升。", "translation": "自注意力机制作为Transformer基于最先进深度学习架构的基石，在很大程度上是启发式驱动的，并且从根本上难以解释。因此，建立一个强大的理论基础来解释其显著成功和局限性已成为近期研究中日益突出的焦点。一些值得注意的方向已经通过图像去噪和非参数回归的视角探索了对自注意力的理解。尽管有前景，但现有框架仍然缺乏对增强自注意力的各种架构组件（无论是其原始形式还是后续变体）的更深层次的机械性解释。在这项工作中，我们旨在通过开发一个统一的图像处理框架来推进这种理解，该框架不仅能够解释自注意力计算本身，还能解释位置编码和残差连接等组件的作用，包括许多后续变体。我们还在我们的框架基础上，指出了这两个概念之间潜在的区别，并努力弥合这一差距。我们在Transformer中引入了两个独立的架构修改。虽然我们的主要目标是可解释性，但我们凭经验观察到，受图像处理启发的修改还可以显著提高准确性，增强对语言和视觉任务中数据污染和对抗性攻击的鲁棒性，以及更好地理解长序列。", "summary": "本论文通过引入一个统一的图像处理框架，重新审视了Transformer的自注意力机制及其关键组件（如位置编码和残差连接），旨在提供更深层次的机械性解释。研究不仅致力于提高Transformer的可解释性，还在此框架下引入了两个架构修改。经验结果表明，这些受图像处理启发的改进显著提升了Transformer在语言和视觉任务中的准确性、对数据污染和对抗性攻击的鲁棒性，并增强了长序列理解能力。", "keywords": "Transformer, 自注意力, 图像滤波, 可解释性, 深度学习", "comments": "这篇论文通过将Transformer的核心机制与图像滤波联系起来，提供了一个新颖且富有洞察力的视角，极大地促进了对Transformer可解释性的理解。其创新之处在于构建了一个统一的图像处理框架来解释复杂的深度学习架构。尽管主要目标是可解释性，但其带来的经验性性能提升（包括准确性和鲁棒性）证明了理论洞察的实际价值，为未来更具原则性的Transformer设计提供了方向。"}}
{"id": "2506.09967", "title": "Resa: Transparent Reasoning Models via SAEs", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.09967v1", "AI": {"title_translation": "Resa：通过稀疏自编码器实现透明推理模型", "tldr": "Resa是一种通过SAE-Tuning训练的1.5B推理模型，能在显著降低成本和时间的情况下，在语言模型中高效地诱导强大的推理能力，并且其推理能力具有通用性和模块化特性。", "motivation": "探索如何通过利用语言模型的底层表示，以成本效益高的方式在语言模型中诱导强大的推理能力。", "method": "提出了一种新颖高效的稀疏自编码器调优（SAE-Tuning）程序。该方法首先训练一个稀疏自编码器（SAE）从源模型中捕获推理能力，然后使用训练好的SAE来指导标准的监督微调过程，以在目标模型中诱导这些能力，整个过程仅使用经过验证的问答数据而无需任何推理痕迹。", "result": "当应用于某些基础模型时，SAE-Tuning在保留超过97%的RL训练对应模型推理性能的同时，将训练成本降低了2000多倍至约1美元，训练时间缩短了450多倍至约20分钟。当应用于轻度RL训练的模型时，仅需约1美元的额外成本，即可实现AIME24上43.33% Pass@1和AMC23上90% Pass@1的推理性能。通过SAE提取的推理能力可能具有通用性（从一个数据集中提取的能力仍能提升在更大、重叠语料库上的性能）和模块化（从Qwen或Qwen-Math中提取的能力可以在测试时附加到R1-Distill模型上，无需重新训练，并产生可比较的增益）。", "conclusion": "Resa及其SAE-Tuning方法提供了一种成本效益高且透明的方式来在语言模型中诱导和利用推理能力，并且这些能力表现出通用性和模块化特性。", "translation": "我们如何才能通过利用语言模型的底层表示，以成本效益高的方式在语言模型中诱导强大的推理能力？我们通过 Resa 回答了这个问题，Resa 是一个 1.5B 推理模型家族，通过一种新颖高效的稀疏自编码器调优（SAE-Tuning）程序进行训练。该方法首先训练一个 SAE 来从源模型中捕获推理能力，然后使用训练好的 SAE 来指导标准的监督微调过程，以在目标模型中诱导这些能力，所有这些都使用经过验证的问答数据而无需任何推理痕迹。值得注意的是，当应用于某些基础模型在进一步 RL 后训练之前时，SAE-Tuning 保留了其 RL 训练对应模型超过 97% 的推理性能，同时将训练成本降低了 2000 多倍至大约 1 美元，训练时间缩短了 450 多倍至大约 20 分钟。此外，当应用于轻度 RL 训练的模型（例如，在 2 个 GPU 上 1 小时内）时，它仅需大约 1 美元的额外成本即可实现 AIME24 上 43.33% Pass@1 和 AMC23 上 90% Pass@1 等推理性能。令人惊讶的是，通过 SAE 提取的推理能力可能既具有通用性又具有模块化。通用性意味着从一个数据集中提取的能力仍然可以提升在更大、重叠语料库上的性能。模块化意味着从 Qwen 或 Qwen-Math 中提取的能力可以在测试时附加到 R1-Distill 模型上，无需任何重新训练，并产生可比较的增益。广泛的消融实验验证了这些发现，并且所有工件都已完全开源。", "summary": "本文介绍了 Resa，一个1.5B的推理模型家族，通过创新的稀疏自编码器调优（SAE-Tuning）方法进行训练，旨在以极低的成本和时间在语言模型中高效地诱导强大的推理能力。SAE-Tuning通过从源模型中提取推理能力并指导目标模型微调来实现。实验表明，该方法在显著降低训练成本和时间的同时，能保持高水平的推理性能，并且提取出的能力具有通用性和模块化特性，可跨数据集和模型复用。", "keywords": "稀疏自编码器, 推理模型, 语言模型, SAE-Tuning, 成本效益", "comments": "这篇论文通过引入SAE-Tuning提供了一种新颖且极其成本效益高的方法来训练具有强大推理能力的语言模型。其创新点在于利用稀疏自编码器捕获和转移推理能力，显著降低了训练成本和时间。尤其值得注意的是，其发现的通用性和模块化特性为未来构建更灵活、可组合的AI系统提供了潜力。这对于资源有限的研究者和开发者来说具有重要意义。"}}
{"id": "2506.10504", "title": "Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models", "authors": ["Sangmin Song", "Juhwan Choi", "JungMin Yun", "YoungBin Kim"], "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nzero-shot dialogue state tracking (DST), reducing the need for task-specific\ntraining. However, conventional DST benchmarks primarily focus on structured\nuser-agent conversations, failing to capture the complexities of real-world\nmulti-user interactions. In this study, we assess the robustness of LLMs in\nmulti-user DST while minimizing dataset construction costs. Inspired by recent\nadvances in LLM-based data annotation, we extend an existing DST dataset by\ngenerating utterances of a second user based on speech act theory. Our\nmethodology systematically incorporates a second user's utterances into\nconversations, enabling a controlled evaluation of LLMs in multi-user settings.\nExperimental results reveal a significant performance drop compared to\nsingle-user DST, highlighting the limitations of current LLMs in extracting and\ntracking dialogue states amidst multiple speakers. Our findings emphasize the\nneed for future research to enhance LLMs for multi-user DST scenarios, paving\nthe way for more realistic and robust DST models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10504v1", "AI": {"title_translation": "超越单用户对话：评估大型语言模型的多用户对话状态跟踪能力", "tldr": "大型语言模型在单用户对话状态跟踪方面表现出色，但在多用户场景中性能显著下降，表明需要进一步研究。", "motivation": "尽管大型语言模型在单用户对话状态跟踪（DST）中表现出色，但现有基准未能捕捉真实世界多用户交互的复杂性。本研究旨在评估大型语言模型在多用户DST中的鲁棒性，并最大程度地降低数据集构建成本。", "method": "受基于LLM的数据标注进展启发，本研究通过基于言语行为理论生成第二个用户的对话，扩展了现有DST数据集。该方法系统地将第二个用户的对话融入现有对话中，从而实现对多用户环境下LLM的受控评估。", "result": "实验结果显示，与单用户DST相比，性能显著下降，这突显了当前大型语言模型在多说话者环境中提取和跟踪对话状态的局限性。", "conclusion": "研究结果强调，未来需要加强大型语言模型在多用户对话状态跟踪场景中的能力，为更真实、更鲁棒的DST模型铺平道路。", "translation": "大型语言模型（LLMs）在零样本对话状态跟踪（DST）中展现出卓越的性能，减少了对特定任务训练的需求。然而，传统的DST基准主要关注结构化的用户-代理对话，未能捕捉真实世界多用户交互的复杂性。在本研究中，我们评估了LLMs在多用户DST中的鲁棒性，同时最大限度地降低了数据集构建成本。受LLM数据标注最新进展的启发，我们通过基于言语行为理论生成第二个用户的对话，扩展了现有DST数据集。我们的方法系统地将第二个用户的对话融入到对话中，从而实现了对多用户环境中LLMs的受控评估。实验结果显示，与单用户DST相比，性能显著下降，这突显了当前LLMs在多说话者环境中提取和跟踪对话状态的局限性。我们的发现强调，未来需要加强LLMs在多用户DST场景中的能力，为更真实、更鲁棒的DST模型铺平道路。", "summary": "本研究评估了大型语言模型（LLMs）在多用户对话状态跟踪（DST）中的能力。通过基于言语行为理论扩展现有DST数据集，并系统地引入第二个用户的对话，研究人员发现LLMs在多用户DST中的性能相比单用户场景显著下降。这揭示了当前LLMs在处理多说话者对话状态方面的局限性，并强调了未来研究需提升LLMs在复杂多用户环境下的DST能力，以开发更实际、更强大的模型。", "keywords": "对话状态跟踪, 大型语言模型, 多用户对话, 零样本DST, 言语行为理论", "comments": "这项研究创新性地通过引入第二个用户对话来评估LLMs在多用户DST中的能力，揭示了当前LLMs在该领域的显著局限性。其方法论为未来构建多用户DST基准和改进LLMs提供了有价值的方向，对于推动对话系统在真实世界复杂场景中的应用具有重要意义。"}}
{"id": "2506.10755", "title": "Quantifying Azure RBAC Wildcard Overreach", "authors": ["Christophe Parisel"], "summary": "Azure RBAC leverages wildcard permissions to simplify policy authoring, but\nthis abstraction often obscures the actual set of allowed operations and\nundermines least-privilege guarantees. We introduce Belshazaar, a two-stage\nframework that targets both the effective permission set problem and the\nevaluation of wildcards permissions spread. First, we formalize Azure action\nsyntax via a context free grammar and implement a compiler that expands any\nwildcard into its explicit action set. Second, we define an ultrametric\ndiameter metric to quantify semantic overreach in wildcard scenarios. Applied\nto Microsoft s official catalog of 15481 actions, Belshazaar reveals that about\n39 percent of actions admit a cross Resource Provider reach when associated\nwith non obvious wildcards, and that effective permissions sets are effectively\ncomputable. These findings demonstrate that wildcard patterns can introduce\nsubstantial privilege bloat, and that our approach offers a scalable, semantics\ndriven path toward tighter, least-privilege RBAC policies in Azure\nenvironments.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10755v1", "AI": {"title_translation": "量化 Azure RBAC 通配符权限过度", "tldr": "Azure RBAC 的通配符权限可能导致权限膨胀，本文介绍了 Belshazaar 框架，用于量化和识别这种过度授权，并证明其有效性。", "motivation": "Azure RBAC 中的通配符权限简化了策略编写，但掩盖了实际允许的操作集，并破坏了最小权限原则。", "method": "引入 Belshazaar 框架，分两阶段：1) 通过上下文无关文法形式化 Azure 动作语法，并实现编译器将通配符扩展为显式动作集。2) 定义超度量直径指标来量化通配符场景中的语义过度。", "result": "应用于微软官方的 15481 个动作目录，Belshazaar 发现约 39% 的动作在与不明显的通配符关联时，允许跨资源提供商的访问，并且有效权限集是可计算的。", "conclusion": "通配符模式会引入大量权限膨胀，并且该方法为 Azure 环境中更严格、最小权限的 RBAC 策略提供了可扩展、语义驱动的路径。", "translation": "Azure RBAC 利用通配符权限来简化策略编写，但这种抽象常常掩盖了实际允许的操作集，并破坏了最小权限保证。我们引入了 Belshazaar，这是一个两阶段框架，旨在解决有效权限集问题和评估通配符权限的扩散。首先，我们通过上下文无关文法形式化 Azure 动作语法，并实现了一个编译器，将任何通配符扩展为其显式动作集。其次，我们定义了一个超度量直径指标来量化通配符场景中的语义过度。将 Belshazaar 应用于微软官方的 15481 个动作目录，结果显示约 39% 的动作在与不明显的通配符关联时，允许跨资源提供商的访问，并且有效权限集是可计算的。这些发现表明，通配符模式会引入大量权限膨胀，并且我们的方法为 Azure 环境中更严格、最小权限的 RBAC 策略提供了可扩展、语义驱动的路径。", "summary": "本文介绍了 Belshazaar，一个用于量化 Azure RBAC 中通配符权限过度授权的两阶段框架。它通过形式化动作语法并扩展通配符，以及引入超度量直径指标来识别和量化权限膨胀。研究发现，Azure 中约 39% 的动作在与通配符关联时会导致跨资源提供商的过度访问，证明了该方法在实现最小权限策略方面的有效性。", "keywords": "Azure RBAC, 通配符权限, 最小权限, 权限膨胀, Belshazaar", "comments": "这篇论文创新性地提出了一种量化和识别云环境中 RBAC 通配符权限过度的新方法。通过形式化语法和引入度量指标，它为解决云安全中的“最小权限”难题提供了实用的工具。其重要性在于，它揭示了通配符权限可能带来的潜在安全风险，并提供了一个可扩展的解决方案来收紧权限策略，对于提高云环境安全性具有重要意义。"}}
{"id": "2506.10386", "title": "Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial", "authors": ["Jerry Yan", "Chinmay Talegaonkar", "Nicholas Antipa", "Eric Terrill", "Sophia Merrifield"], "summary": "The burial state of anthropogenic objects on the seafloor provides insight\ninto localized sedimentation dynamics and is also critical for assessing\necological risks, potential pollutant transport, and the viability of recovery\nor mitigation strategies for hazardous materials such as munitions. Accurate\nburial depth estimation from remote imagery remains difficult due to partial\nocclusion, poor visibility, and object degradation. This work introduces a\ncomputer vision pipeline, called PoseIDON, which combines deep foundation model\nfeatures with multiview photogrammetry to estimate six degrees of freedom\nobject pose and the orientation of the surrounding seafloor from ROV video.\nBurial depth is inferred by aligning CAD models of the objects with observed\nimagery and fitting a local planar approximation of the seafloor. The method is\nvalidated using footage of 54 objects, including barrels and munitions,\nrecorded at a historic ocean dumpsite in the San Pedro Basin. The model\nachieves a mean burial depth error of approximately 10 centimeters and resolves\nspatial burial patterns that reflect underlying sediment transport processes.\nThis approach enables scalable, non-invasive mapping of seafloor burial and\nsupports environmental assessment at contaminated sites.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10386v1", "AI": {"title_translation": "利用6自由度姿态基础模型绘制海洋沉积物埋藏图", "tldr": "PoseIDON是一个新的计算机视觉系统，它结合了深度基础模型和多视图摄影测量技术，可以从水下机器人视频中准确估计海底人工物体的埋藏深度，误差约为10厘米，支持对污染地点的环境评估。", "motivation": "海底人工物体的埋藏状态对于了解局部沉积动力学、评估生态风险、污染物潜在传输以及危险材料（如弹药）的回收或缓解策略至关重要。然而，由于部分遮挡、能见度差和物体降解，从远程图像准确估计埋藏深度仍然很困难。", "method": "本研究引入了一个名为PoseIDON的计算机视觉流程，它结合了深度基础模型特征和多视图摄影测量技术，以估计ROV视频中物体的六自由度姿态和周围海底的方向。通过将物体的CAD模型与观测图像对齐，并拟合海底的局部平面近似来推断埋藏深度。该方法在圣佩德罗盆地历史海洋倾倒场记录的54个物体（包括桶和弹药）的录像中进行了验证。", "result": "该模型实现了大约10厘米的平均埋藏深度误差，并解析了反映底层沉积物传输过程的空间埋藏模式。", "conclusion": "这种方法实现了海底埋藏的可扩展、非侵入式测绘，并支持对受污染地点的环境评估。", "translation": "海底人工物体的埋藏状态为局部沉积动力学提供了深入见解，对于评估生态风险、潜在污染物传输以及危险材料（如弹药）的回收或缓解策略的生存能力也至关重要。由于部分遮挡、能见度差和物体降解，从远程图像准确估计埋藏深度仍然很困难。这项工作引入了一个名为PoseIDON的计算机视觉流程，它将深度基础模型特征与多视图摄影测量相结合，以从ROV视频中估计六自由度物体姿态和周围海底的方向。通过将物体的CAD模型与观测图像对齐并拟合海底的局部平面近似来推断埋藏深度。该方法使用在圣佩德罗盆地历史海洋倾倒场记录的54个物体（包括桶和弹药）的录像进行了验证。该模型实现了大约10厘米的平均埋藏深度误差，并解析了反映底层沉积物传输过程的空间埋藏模式。这种方法实现了海底埋藏的可扩展、非侵入式测绘，并支持对受污染地点的环境评估。", "summary": "本研究提出了一种名为PoseIDON的计算机视觉流程，利用深度基础模型和多视图摄影测量技术，从ROV视频中准确估计海底人工物体的六自由度姿态和埋藏深度。该方法通过将CAD模型与观测图像对齐并拟合海底平面来推断埋藏深度。在验证中，该模型实现了约10厘米的平均埋藏深度误差，并能解析空间埋藏模式，为污染场地的环境评估提供了可扩展、非侵入式的测绘能力。", "keywords": "6自由度姿态, 海洋沉积物埋藏, 计算机视觉, 摄影测量, 环境评估", "comments": "该论文提出了一种创新性的计算机视觉方法PoseIDON，通过结合深度基础模型和多视图摄影测量，有效解决了水下环境中物体埋藏深度估计的难题。其非侵入性、可扩展性以及在实际应用中（如环境评估和危险品管理）的潜力，使其具有重要的实际意义。10厘米的平均误差在水下复杂环境中表现出色。"}}
{"id": "2506.10001", "title": "Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure", "authors": ["Yuxuan Li", "Sheng Jinag", "Bizhu Wang"], "summary": "With technology advancing and the pursuit of new audiovisual experiences\nstrengthening, the metaverse has gained surging enthusiasm. However, it faces\npractical hurdles as substantial data like high-resolution virtual scenes must\nbe transmitted between cloud platforms and VR devices. Specifically, the VR\ndevice's wireless transmission hampered by insufficient bandwidth, causes speed\nand delay problems. Meanwhile, poor channel quality leads to data errors and\nworsens user experience. To solve this, we've proposed the Semantic\nCommunication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service\n(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic\ntransmission, video synthesis, and 3D virtual scene reconstruction. By\ndeploying semantic modules on VR devices and edge servers and sending key\nsemantic info instead of focusing on bit-level reconstruction, it can cut\nlatency, resolve the resource-bandwidth conflict, and better withstand channel\ninterference. Also, the cloud deploys video synthesis and 3D scene\nreconstruction preprocessing, while edge devices host 3D reconstruction\nrendering modules, all for immersive services. Verified on Meta Quest Pro, the\nSC-CEE-Meta can reduce wireless transmission delay by 96.05\\% and boost image\nquality by 43.99\\% under poor channel condition.", "comment": "arXiv admin note: text overlap with arXiv:2407.13764 by other authors", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10001v1", "AI": {"title_translation": "语义通信赋能的云边端协同元宇宙服务架构", "tldr": "本文提出了一种名为SC-CEE-Meta的语义通信赋能的云边端协同架构，旨在解决元宇宙服务中高分辨率数据传输导致的带宽、延迟和信道质量问题，通过传输关键语义信息，显著降低延迟并提升图像质量。", "motivation": "随着技术进步和对新音视频体验的追求，元宇宙获得了巨大的热情。然而，元宇宙面临实际障碍，因为高分辨率虚拟场景等大量数据必须在云平台和VR设备之间传输。具体来说，VR设备的无线传输受带宽不足的阻碍，导致速度和延迟问题。同时，糟糕的信道质量会导致数据错误并恶化用户体验。", "method": "为了解决上述问题，本文提出了语义通信赋能的云边端协同沉浸式元宇宙服务（SC-CEE-Meta）架构。该架构包含三个模块：VR视频语义传输、视频合成和3D虚拟场景重建。通过在VR设备和边缘服务器上部署语义模块，并传输关键语义信息而非关注比特级重建，可以减少延迟，解决资源带宽冲突，并更好地抵抗信道干扰。此外，云端部署视频合成和3D场景重建预处理，而边缘设备承载3D重建渲染模块，所有这些都为了沉浸式服务。", "result": "在Meta Quest Pro上验证，SC-CEE-Meta可以将无线传输延迟降低96.05%，并在糟糕的信道条件下将图像质量提高43.99%。", "conclusion": "SC-CEE-Meta架构通过语义通信和云边端协同，有效解决了元宇宙服务中高分辨率数据传输面临的带宽、延迟和信道干扰问题，显著提升了用户体验。", "translation": "随着技术的进步和对新音视频体验追求的加强，元宇宙获得了高涨的热情。然而，它面临实际障碍，因为高分辨率虚拟场景等大量数据必须在云平台和VR设备之间传输。具体来说，VR设备的无线传输受带宽不足的阻碍，导致速度和延迟问题。同时，糟糕的信道质量会导致数据错误并恶化用户体验。为了解决这个问题，我们提出了语义通信赋能的云边端协同沉浸式元宇宙服务（SC-CEE-Meta）架构，该架构包括三个模块：VR视频语义传输、视频合成和3D虚拟场景重建。通过在VR设备和边缘服务器上部署语义模块，并发送关键语义信息而不是专注于比特级重建，它可以减少延迟，解决资源带宽冲突，并更好地抵抗信道干扰。此外，云端部署视频合成和3D场景重建预处理，而边缘设备承载3D重建渲染模块，所有这些都为了沉浸式服务。在Meta Quest Pro上验证，SC-CEE-Meta可以将无线传输延迟降低96.05%，并在糟糕的信道条件下将图像质量提高43.99%。", "summary": "本文针对元宇宙服务中高分辨率数据传输面临的带宽、延迟和信道质量问题，提出了一种名为SC-CEE-Meta的语义通信赋能的云边端协同架构。该架构通过在VR设备和边缘服务器上部署语义模块，传输关键语义信息而非原始比特流，并结合云端和边缘的协同处理，有效降低了无线传输延迟达96.05%，并在恶劣信道条件下将图像质量提升了43.99%，显著改善了元宇宙用户体验。", "keywords": "语义通信, 元宇宙, 云边端协同, VR, 无线传输", "comments": "该论文提出了一种创新的语义通信方法来优化元宇宙服务中的数据传输，解决了传统比特级通信在带宽和延迟方面的局限性。其云边端协同架构充分利用了不同层级的计算能力，并通过量化改进证明了其有效性，对未来沉浸式体验的发展具有重要意义。"}}
{"id": "2506.10508", "title": "Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs", "authors": ["Yilin Xiao", "Chuang Zhou", "Qinggang Zhang", "Bo Li", "Qing Li", "Xiao Huang"], "summary": "Large language models (LLMs) often struggle with knowledge-intensive tasks\ndue to a lack of background knowledge and a tendency to hallucinate. To address\nthese limitations, integrating knowledge graphs (KGs) with LLMs has been\nintensively studied. Existing KG-enhanced LLMs focus on supplementary factual\nknowledge, but still struggle with solving complex questions. We argue that\nrefining the relationships among facts and organizing them into a logically\nconsistent reasoning path is equally important as factual knowledge itself.\nDespite their potential, extracting reliable reasoning paths from KGs poses the\nfollowing challenges: the complexity of graph structures and the existence of\nmultiple generated paths, making it difficult to distinguish between useful and\nredundant ones. To tackle these challenges, we propose the RRP framework to\nmine the knowledge graph, which combines the semantic strengths of LLMs with\nstructural information obtained through relation embedding and bidirectional\ndistribution learning. Additionally, we introduce a rethinking module that\nevaluates and refines reasoning paths according to their significance.\nExperimental results on two public datasets show that RRP achieves\nstate-of-the-art performance compared to existing baseline methods. Moreover,\nRRP can be easily integrated into various LLMs to enhance their reasoning\nabilities in a plug-and-play manner. By generating high-quality reasoning paths\ntailored to specific questions, RRP distills effective guidance for LLM\nreasoning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10508v1", "AI": {"title_translation": "可靠推理路径：利用知识图谱为大型语言模型推理提炼有效指导", "tldr": "本文提出了RRP框架，通过结合知识图谱的结构信息和LLM的语义能力，为大型语言模型（LLMs）生成可靠的推理路径，以解决其在知识密集型任务中的挑战并提升推理能力。", "motivation": "大型语言模型（LLMs）在知识密集型任务中常因缺乏背景知识和幻觉问题而表现不佳。现有的知识图谱（KGs）增强型LLMs主要侧重于补充事实知识，但在解决复杂问题时仍面临困难。作者认为，提炼事实之间的关系并将其组织成逻辑一致的推理路径与事实知识本身同等重要。", "method": "为了解决从知识图谱中提取可靠推理路径的挑战（图结构复杂性及路径区分难），本文提出了RRP框架。该框架结合了LLMs的语义优势与通过关系嵌入和双向分布学习获得的结构信息。此外，RRP还引入了一个“重思考模块”来评估和优化推理路径的重要性。", "result": "在两个公共数据集上的实验结果表明，RRP框架与现有基线方法相比，达到了最先进的性能。此外，RRP可以以即插即用的方式轻松集成到各种LLMs中，以增强它们的推理能力。", "conclusion": "RRP通过生成针对特定问题量身定制的高质量推理路径，为LLM推理提炼了有效的指导。", "translation": "大型语言模型（LLMs）由于缺乏背景知识和倾向于产生幻觉，在知识密集型任务中常常表现不佳。为了解决这些限制，将知识图谱（KGs）与LLMs结合已得到深入研究。现有的知识图谱增强型LLMs侧重于补充事实知识，但仍然难以解决复杂问题。我们认为，提炼事实之间的关系并将其组织成逻辑一致的推理路径与事实知识本身同等重要。尽管它们具有潜力，但从知识图谱中提取可靠的推理路径面临以下挑战：图结构的复杂性和存在多个生成的路径，使得区分有用路径和冗余路径变得困难。为了应对这些挑战，我们提出了RRP框架来挖掘知识图谱，该框架结合了LLMs的语义优势与通过关系嵌入和双向分布学习获得的结构信息。此外，我们引入了一个重思考模块，根据其重要性评估和完善推理路径。在两个公共数据集上的实验结果表明，RRP与现有基线方法相比，实现了最先进的性能。此外，RRP可以以即插即用的方式轻松集成到各种LLMs中，以增强它们的推理能力。通过生成针对特定问题量身定制的高质量推理路径，RRP为LLM推理提炼了有效的指导。", "summary": "本文提出RRP框架，旨在解决大型语言模型（LLMs）在知识密集型任务中因知识不足和幻觉问题导致的推理挑战。不同于仅补充事实知识的现有方法，RRP强调构建逻辑一致的推理路径。该框架通过结合LLMs的语义能力与知识图谱的结构信息（利用关系嵌入和双向分布学习），并引入一个重思考模块来评估和优化路径。实验证明，RRP在两个公共数据集上实现了最先进的性能，并能以即插即用的方式增强各种LLMs的推理能力，通过生成高质量的推理路径提供有效指导。", "keywords": "大型语言模型, 知识图谱, 推理路径, RRP框架, 知识密集型任务", "comments": "本文的创新点在于将LLMs与知识图谱结合时，不再仅仅关注事实知识的补充，而是提出了“可靠推理路径”的概念，强调了对事实间关系和逻辑推理过程的提炼。RRP框架通过结合语义和结构信息，并引入“重思考模块”来优化路径，有效解决了从复杂知识图谱中提取有用推理路径的挑战。其即插即用的特性也增加了其实用性和潜在影响力。"}}
{"id": "2506.10776", "title": "ME: Trigger Element Combination Backdoor Attack on Copyright Infringement", "authors": ["Feiyu Yang", "Siyuan Liang", "Aishan Liu", "Dacheng Tao"], "summary": "The capability of generative diffusion models (DMs) like Stable Diffusion\n(SD) in replicating training data could be taken advantage of by attackers to\nlaunch the Copyright Infringement Attack, with duplicated poisoned image-text\npairs. SilentBadDiffusion (SBD) is a method proposed recently, which shew\noutstanding performance in attacking SD in text-to-image tasks. However, the\nfeasible data resources in this area are still limited, some of them are even\nconstrained or prohibited due to the issues like copyright ownership or\ninappropriate contents; And not all of the images in current datasets are\nsuitable for the proposed attacking methods; Besides, the state-of-the-art\n(SoTA) performance of SBD is far from ideal when few generated poisoning\nsamples could be adopted for attacks. In this paper, we raised new datasets\naccessible for researching in attacks like SBD, and proposed Multi-Element (ME)\nattack method based on SBD by increasing the number of poisonous visual-text\nelements per poisoned sample to enhance the ability of attacking, while\nimporting Discrete Cosine Transform (DCT) for the poisoned samples to maintain\nthe stealthiness. The Copyright Infringement Rate (CIR) / First Attack Epoch\n(FAE) we got on the two new datasets were 16.78% / 39.50 and 51.20% / 23.60,\nrespectively close to or even outperformed benchmark Pokemon and Mijourney\ndatasets. In condition of low subsampling ratio (5%, 6 poisoned samples), MESI\nand DCT earned CIR / FAE of 0.23% / 84.00 and 12.73% / 65.50, both better than\noriginal SBD, which failed to attack at all.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10776v1", "AI": {"title_translation": "ME: 触发元素组合后门攻击版权侵犯", "tldr": "提出ME攻击方法，通过增加毒化元素和DCT提高生成扩散模型版权侵犯攻击的效率和隐蔽性，解决了现有方法的局限性。", "motivation": "现有的生成扩散模型版权侵犯攻击方法（如SilentBadDiffusion, SBD）面临数据资源有限、不适用所有图像以及在少量毒化样本下性能不佳的问题。", "method": "提出Multi-Element (ME) 攻击方法，基于SBD，通过增加每个毒化样本中的毒化视觉-文本元素数量来增强攻击能力，并引入离散余弦变换（DCT）来保持毒化样本的隐蔽性。同时，构建了新的可访问数据集。", "result": "在两个新数据集上，版权侵犯率（CIR）/首次攻击周期（FAE）分别为16.78%/39.50和51.20%/23.60，分别接近甚至超越了基准数据集。在低子采样率（5%，6个毒化样本）条件下，MESI和DCT的CIR/FAE分别为0.23%/84.00和12.73%/65.50，均优于完全失效的原始SBD。", "conclusion": "Not mentioned in abstract", "translation": "生成扩散模型（如Stable Diffusion, SD）复制训练数据的能力可能被攻击者利用，通过复制毒化图像-文本对来发起版权侵犯攻击。SilentBadDiffusion (SBD) 是最近提出的一种方法，在文本到图像任务中攻击SD表现出色。然而，该领域可行的数\n据资源仍然有限，其中一些甚至由于版权所有权或不当内容等问题而受到限制或禁止；并非所有当前数据集中的图像都适用于所提出的攻击方法；此外，当少量生成的毒化样本可用于攻击时，SBD的最新（SoTA）性能远非理想。\n在本文中，我们提出了可用于研究SBD等攻击的新数据集，并基于SBD提出了多元素（ME）攻击方法，通过增加每个毒化样本中毒化视觉-文本元素的数量来增强攻击能力，同时为毒化样本引入离散余弦变换（DCT）以保持隐蔽性。我们在两个新数据集上获得的版权侵犯率（CIR）/首次攻击周期（FAE）分别为16.78%/39.50和51.20%/23.60，分别接近甚至超越了基准Pokemon和Mijourney数据集。在低子采样率（5%，6个毒化样本）条件下，MESI和DCT的CIR/FAE分别为0.23%/84.00和12.73%/65.50，均优于完全无法攻击的原始SBD。", "summary": "本文针对生成扩散模型（如Stable Diffusion）面临的版权侵犯攻击中现有方法（如SilentBadDiffusion, SBD）的数据资源有限和低样本性能差的问题，提出了一种名为Multi-Element (ME) 的新型攻击方法。ME通过增加每个毒化样本的视觉-文本元素数量来增强攻击能力，并利用离散余数变换（DCT）确保隐蔽性。研究构建了新的数据集并进行了实验，结果显示ME在版权侵犯率和首次攻击周期方面显著优于SBD，尤其是在低毒化样本量的情况下。", "keywords": "版权侵犯, 生成扩散模型, 后门攻击, 多元素攻击, 离散余弦变换", "comments": "该论文提出了一种创新的方法来增强对生成扩散模型的版权侵犯攻击，解决了现有方法在数据可用性和攻击效率方面的局限性。通过引入多元素组合和DCT，提高了攻击的有效性和隐蔽性，这对于理解和防御此类新型威胁具有重要意义。"}}
{"id": "2506.10390", "title": "DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba", "authors": ["Shicheng Yin", "Kaixuan Yin", "Yang Liu", "Weixing Chen", "Liang Lin"], "summary": "Recently, non-convolutional models such as the Vision Transformer (ViT) and\nVision Mamba (Vim) have achieved remarkable performance in computer vision\ntasks. However, their reliance on fixed-size patches often results in excessive\nencoding of background regions and omission of critical local details,\nespecially when informative objects are sparsely distributed. To address this,\nwe introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),\nwhich adaptively partitions images into content-dependent patches of varying\nsizes. DART combines learnable region scores with piecewise differentiable\nquantile operations to allocate denser tokens to information-rich areas.\nDespite introducing only approximately 1 million (1M) additional parameters,\nDART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that\nuniformly increase token density to capture fine-grained details, DART offers a\nmore efficient alternative, achieving 45% FLOPs reduction with superior\nperformance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that\nDART consistently enhances accuracy while incurring minimal or even reduced\ncomputational overhead. Code is available at\nhttps://github.com/HCPLab-SYSU/DART.", "comment": "Code is available at https://github.com/HCPLab-SYSU/DART", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10390v1", "AI": {"title_translation": "DART: 面向视觉Transformer和Mamba的可微分动态自适应区域分词器", "tldr": "DART是一种可微分的动态自适应区域分词器，解决了ViT和Vim固定大小patch的问题，通过自适应分割图像，提高信息丰富区域的token密度，同时减少计算量并提升精度。", "motivation": "现有的非卷积模型（如ViT和Vim）依赖固定大小的patch，导致背景区域编码冗余和关键局部细节丢失，尤其当信息对象稀疏分布时。", "method": "本文引入了完全可微分的动态自适应区域分词器 (DART)，它结合可学习的区域分数与分段可微分分位数操作，自适应地将图像分割成内容相关的不同大小的patch，从而将更密集的token分配给信息丰富的区域。", "result": "DART仅增加约1M参数，在DeiT (ImageNet-1K)上将精度提高2.1%。与统一增加token密度的方法相比，DART更高效，实现了45%的FLOPs减少，同时性能更优。在DeiT、Vim和VideoMamba上的广泛实验证实，DART持续提升精度，同时计算开销极小甚至降低。", "conclusion": "DART通过动态自适应区域分词，有效解决了现有视觉模型中固定大小patch的局限性，在提高模型性能的同时显著提升了计算效率。", "translation": "标题: DART: 面向视觉Transformer和Mamba的可微分动态自适应区域分词器\n摘要: 近年来，视觉Transformer (ViT) 和视觉Mamba (Vim) 等非卷积模型在计算机视觉任务中取得了卓越的性能。然而，它们对固定大小patch的依赖常常导致背景区域的过度编码和关键局部细节的遗漏，尤其当信息对象稀疏分布时。为了解决这个问题，我们引入了一种完全可微分的动态自适应区域分词器 (DART)，它能自适应地将图像分割成内容相关的不同大小的patch。DART结合了可学习的区域分数和分段可微分的分位数操作，将更密集的token分配给信息丰富的区域。尽管只引入了大约100万 (1M) 额外参数，DART在DeiT (ImageNet-1K) 上将精度提高了2.1%。与统一增加token密度以捕获细粒度细节的方法不同，DART提供了一种更高效的替代方案，在性能更优的同时实现了45%的FLOPs减少。在DeiT、Vim和VideoMamba上的广泛实验证实，DART持续提升精度，同时计算开销极小甚至降低。代码可在 https://github.com/HCPLab-SYSU/DART 获取。", "summary": "本文提出DART，一种可微分动态自适应区域分词器，旨在解决Vision Transformer和Mamba模型中固定大小patch导致的信息冗余和细节丢失问题。DART通过自适应地将图像划分为内容相关的可变大小patch，并结合学习到的区域分数和分位数操作，将更多token分配给信息密集区域。实验证明，DART在DeiT、Vim和VideoMamba上显著提升了精度，并有效降低了计算开销，实现了性能与效率的平衡。", "keywords": "视觉Transformer, 视觉Mamba, 动态分词, 自适应区域, 计算效率", "comments": "DART的创新之处在于其可微分的动态自适应区域分词机制，它打破了传统ViT和Mamba固定patch的限制，通过智能分配token密度，有效解决了信息冗余和细节丢失的问题。其能够同时提升精度并显著降低FLOPs，显示出优异的效率和性能平衡，对提升非卷积视觉模型的实际应用价值具有重要意义。"}}
{"id": "2506.10244", "title": "A new type of federated clustering: A non-model-sharing approach", "authors": ["Yuji Kawamata", "Kaoru Kamijo", "Maki Kihira", "Akihiro Toyoda", "Tomoru Nakayama", "Akira Imakura", "Tetsuya Sakurai", "Yukihiko Okada"], "summary": "In recent years, the growing need to leverage sensitive data across\ninstitutions has led to increased attention on federated learning (FL), a\ndecentralized machine learning paradigm that enables model training without\nsharing raw data. However, existing FL-based clustering methods, known as\nfederated clustering, typically assume simple data partitioning scenarios such\nas horizontal or vertical splits, and cannot handle more complex distributed\nstructures. This study proposes data collaboration clustering (DC-Clustering),\na novel federated clustering method that supports clustering over complex data\npartitioning scenarios where horizontal and vertical splits coexist. In\nDC-Clustering, each institution shares only intermediate representations\ninstead of raw data, ensuring privacy preservation while enabling collaborative\nclustering. The method allows flexible selection between k-means and spectral\nclustering, and achieves final results with a single round of communication\nwith the central server. We conducted extensive experiments using synthetic and\nopen benchmark datasets. The results show that our method achieves clustering\nperformance comparable to centralized clustering where all data are pooled.\nDC-Clustering addresses an important gap in current FL research by enabling\neffective knowledge discovery from distributed heterogeneous data. Its\npractical properties -- privacy preservation, communication efficiency, and\nflexibility -- make it a promising tool for privacy-sensitive domains such as\nhealthcare and finance.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10244v1", "AI": {"title_translation": "一种新型联邦聚类：一种非模型共享方法", "tldr": "本文提出了一种名为DC-Clustering的新型联邦聚类方法，旨在解决现有方法无法处理复杂数据分区的问题。该方法通过共享中间表示而非原始数据来保护隐私，并仅需一轮通信即可实现与集中式聚类相当的性能。", "motivation": "现有联邦聚类方法通常假设简单的数据分区场景（如水平或垂直分割），无法处理更复杂的分布式结构（水平和垂直分割并存）。然而，跨机构利用敏感数据的需求日益增长，需要一种在保证隐私的同时能处理复杂数据分布的聚类方法。", "method": "本研究提出数据协作聚类（DC-Clustering），这是一种新型联邦聚类方法，支持在水平和垂直分割并存的复杂数据分区场景下进行聚类。在DC-Clustering中，每个机构仅共享中间表示而不是原始数据，以确保隐私。该方法允许在k-means和谱聚类之间灵活选择，并通过与中央服务器进行一轮通信即可获得最终结果。", "result": "实验结果表明，DC-Clustering的聚类性能与所有数据集中池化时的集中式聚类相当。", "conclusion": "DC-Clustering通过实现从分布式异构数据中有效发现知识，填补了当前联邦学习研究中的一个重要空白。其隐私保护、通信效率和灵活性等实用特性使其成为医疗保健和金融等隐私敏感领域的一个有前景的工具。", "translation": "近年来，利用跨机构敏感数据的日益增长的需求使得联邦学习（FL）受到了越来越多的关注。联邦学习是一种去中心化机器学习范式，无需共享原始数据即可实现模型训练。然而，现有的基于FL的聚类方法，即联邦聚类，通常假设简单的数据分区场景，如水平或垂直分割，无法处理更复杂的分布式结构。本研究提出了一种新型联邦聚类方法——数据协作聚类（DC-Clustering），它支持在水平和垂直分割并存的复杂数据分区场景下进行聚类。在DC-Clustering中，每个机构仅共享中间表示而不是原始数据，从而在实现协作聚类的同时确保隐私保护。该方法允许在k-means和谱聚类之间灵活选择，并通过与中央服务器进行一轮通信即可获得最终结果。我们使用合成数据集和开放基准数据集进行了广泛的实验。结果表明，我们的方法实现了与所有数据集中池化时的集中式聚类相当的聚类性能。DC-Clustering通过实现从分布式异构数据中有效发现知识，填补了当前FL研究中的一个重要空白。其隐私保护、通信效率和灵活性等实用特性使其成为医疗保健和金融等隐私敏感领域的一个有前景的工具。", "summary": "本文提出了一种名为数据协作聚类（DC-Clustering）的新型联邦聚类方法，专门用于处理水平和垂直数据分割共存的复杂数据分区场景。与传统联邦聚类不同，DC-Clustering通过仅共享中间表示而非原始数据来确保数据隐私。该方法支持k-means和谱聚类的灵活选择，并能通过与中央服务器进行一轮通信就获得最终结果。实验证明，DC-Clustering的聚类性能与集中式聚类相当，突显了其在从分布式异构数据中发现知识方面的有效性，同时兼具隐私保护、通信效率和灵活性，使其适用于隐私敏感领域。", "keywords": "联邦聚类, 数据协作, 隐私保护, 复杂数据分区, 非模型共享", "comments": "该论文通过解决现有联邦聚类方法在处理复杂数据分区（混合水平和垂直分割）方面的局限性而具有创新性。其非模型共享和中间表示共享的方法对于隐私保护至关重要。单轮通信是提高效率的一个显著优势。这种方法拓宽了联邦学习在更现实和复杂的分布式数据环境中的应用范围。"}}
{"id": "2506.10002", "title": "EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis", "authors": ["Jianwu Fang", "Lei-Lei Li", "Zhedong Zheng", "Hongkai Yu", "Jianru Xue", "Zhengguo Li", "Tat-Seng Chua"], "summary": "Traffic Accident Anticipation (TAA) in traffic scenes is a challenging\nproblem for achieving zero fatalities in the future. Current approaches\ntypically treat TAA as a supervised learning task needing the laborious\nannotation of accident occurrence duration. However, the inherent long-tailed,\nuncertain, and fast-evolving nature of traffic scenes has the problem that real\ncausal parts of accidents are difficult to identify and are easily dominated by\ndata bias, resulting in a background confounding issue. Thus, we propose an\nAttentive Video Diffusion (AVD) model that synthesizes additional accident\nvideo clips by generating the causal part in dashcam videos, i.e., from normal\nclips to accident clips. AVD aims to generate causal video frames based on\naccident or accident-free text prompts while preserving the style and content\nof frames for TAA after video generation. This approach can be trained using\ndatasets collected from various driving scenes without any extra annotations.\nAdditionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant\ntriple loss for an anchor accident-free video clip, along with the generated\npair of contrastive pseudo-normal and pseudo-accident clips. Extensive\nexperiments have been conducted to evaluate the performance of AVD and EQ-TAA,\nand competitive performance compared to state-of-the-art methods has been\nobtained.", "comment": "Accepted by IEEE-TMM", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10002v1", "AI": {"title_translation": "EQ-TAA：基于扩散的事故视频合成的等变交通事故预测", "tldr": "该研究提出EQ-TAA，一种通过基于扩散模型（AVD）合成事故视频来克服数据稀疏和偏置问题的交通事故预测（TAA）新方法，并引入等变三元损失，实现了与现有技术相当的性能。", "motivation": "交通场景中的交通事故预测（TAA）是一个具有挑战性的问题，现有方法通常将TAA视为监督学习任务，需要耗时耗力的事故发生时长标注。然而，交通场景固有的长尾、不确定和快速演变的特性使得事故的真实因果部分难以识别，并容易受到数据偏差的影响，导致背景混淆问题。", "method": "本研究提出了一种注意力视频扩散（AVD）模型，通过在行车记录视频中生成因果部分（即从正常片段到事故片段），来合成额外的事故视频片段。AVD旨在根据事故或无事故文本提示生成因果视频帧，同时保留视频生成后用于TAA的帧的风格和内容。这种方法无需额外标注即可使用从各种驾驶场景收集的数据集进行训练。此外，AVD通过等变三元损失促进了等变交通事故预测（EQ-TAA），该损失用于锚定的无事故视频片段，以及生成的一对对比伪正常和伪事故片段。", "result": "通过广泛的实验评估了AVD和EQ-TAA的性能，结果表明它们取得了与最先进方法相比具有竞争力的性能。", "conclusion": "本研究提出的AVD模型能够通过合成事故视频来有效缓解交通事故预测中的数据稀缺和偏差问题，并在此基础上引入的EQ-TAA框架结合等变学习，显著提升了预测性能，取得了具有竞争力的结果。", "translation": "交通事故预测（TAA）在交通场景中是一个具有挑战性的问题，旨在未来实现零死亡。当前的方法通常将TAA视为一个监督学习任务，需要对事故发生时长进行耗时耗力的标注。然而，交通场景固有的长尾、不确定和快速演变的特性带来了问题，即事故的真实因果部分难以识别，并且容易受到数据偏差的影响，导致背景混淆问题。因此，我们提出了一种注意力视频扩散（AVD）模型，通过在行车记录视频中生成因果部分（即从正常片段到事故片段）来合成额外的事故视频片段。AVD旨在根据事故或无事故文本提示生成因果视频帧，同时保留视频生成后用于TAA的帧的风格和内容。这种方法无需任何额外标注即可使用从各种驾驶场景收集的数据集进行训练。此外，AVD通过等变三元损失促进了等变交通事故预测（EQ-TAA），该损失用于锚定的无事故视频片段，以及生成的一对对比伪正常和伪事故片段。我们进行了广泛的实验来评估AVD和EQ-TAA的性能，并获得了与最先进方法相比具有竞争力的性能。", "summary": "针对交通事故预测（TAA）中数据稀缺、标注困难及数据偏差等问题，本研究提出了一种注意力视频扩散（AVD）模型。AVD模型能够从正常行车记录视频中合成具有因果关系的事故视频片段，且无需额外标注。基于AVD，进一步提出了等变交通事故预测（EQ-TAA）框架，利用等变三元损失对无事故视频与合成的伪正常/伪事故视频进行对比学习。实验结果表明，AVD和EQ-TAA在交通事故预测任务上取得了与现有最先进方法相当的竞争性性能。", "keywords": "交通事故预测, 扩散模型, 视频合成, 等变学习, 数据增强", "comments": "该论文的创新点在于利用扩散模型来合成交通事故视频，从而有效解决了交通事故预测中数据稀疏性和标注困难的挑战。通过生成因果性事故片段，弥补了真实世界数据中事故事件的稀缺性。此外，结合等变学习（EQ-TAA）和对比损失，进一步提升了模型的鲁棒性和预测能力。无需额外标注的训练方式也大大降低了数据准备成本，具有重要的实际应用价值。"}}
{"id": "2506.10614", "title": "Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search", "authors": ["Promise Dodzi Kpoglu"], "summary": "We propose an unsupervised method for the reconstruction of protoforms i.e.,\nancestral word forms from which modern language forms are derived. While prior\nwork has primarily relied on probabilistic models of phonological edits to\ninfer protoforms from cognate sets, such approaches are limited by their\npredominantly data-driven nature. In contrast, our model integrates data-driven\ninference with rule-based heuristics within an evolutionary optimization\nframework. This hybrid approach leverages on both statistical patterns and\nlinguistically motivated constraints to guide the reconstruction process. We\nevaluate our method on the task of reconstructing Latin protoforms using a\ndataset of cognates from five Romance languages. Experimental results\ndemonstrate substantial improvements over established baselines across both\ncharacter-level accuracy and phonological plausibility metrics.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10614v1", "AI": {"title_translation": "无监督的原始词形重构：通过简约规则引导的启发式方法和演化搜索", "tldr": "本文提出了一种无监督方法，结合数据驱动推理和规则启发式方法，通过演化优化框架重构祖先词形（原始词形），并在拉丁语原始词形重构任务上取得了显著优于现有基线的结果。", "motivation": "先前重构原始词形的方法主要依赖于语音编辑的概率模型，但这些方法受限于其主要由数据驱动的性质。", "method": "提出了一种无监督的原始词形重构方法。该模型将数据驱动的推理与基于规则的启发式方法整合到一个演化优化框架中。这种混合方法利用统计模式和语言学驱动的约束来指导重构过程。", "result": "在重构拉丁语原始词形的任务中，实验结果表明该方法在字符级准确性和语音合理性指标上均显著优于已建立的基线。", "conclusion": "该研究成功提出了一种结合数据驱动推理和规则启发式方法的混合无监督原始词形重构方法，并在实验中证明了其在准确性和合理性方面的优越性。", "translation": "我们提出了一种无监督的原始词形重构方法，即从现代语言形式派生而来的祖先词形。虽然先前的工作主要依赖于语音编辑的概率模型从同源词集推断原始词形，但这些方法受限于其主要由数据驱动的性质。相比之下，我们的模型将数据驱动的推理与基于规则的启发式方法整合到一个演化优化框架中。这种混合方法利用统计模式和语言学驱动的约束来指导重构过程。我们使用来自五种罗曼语系的同源词数据集，评估了我们方法在重构拉丁语原始词形任务上的性能。实验结果表明，在字符级准确性和语音合理性指标上，该方法均显著优于已建立的基线。", "summary": "本文提出了一种用于无监督原始词形重构的新方法，旨在解决现有数据驱动模型在推断祖先词形时的局限性。该方法通过将数据驱动推理与规则引导的启发式方法相结合，并嵌入到演化优化框架中，实现了对统计模式和语言学约束的有效利用。在拉丁语原始词形重构任务上进行的评估显示，该方法在准确性和语音合理性方面均显著超越了现有基线。", "keywords": "原始词形重构, 无监督学习, 演化优化, 规则启发式, 语言学", "comments": "这篇论文的创新点在于其混合方法，结合了数据驱动的统计模式和语言学上的规则启发式方法，弥补了纯数据驱动模型的不足。通过引入演化优化框架，为原始词形重构提供了一种新的、更全面的视角。其在拉丁语原始词形重构上的显著提升表明了该方法的有效性和潜在应用价值。"}}
{"id": "2506.10949", "title": "Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors", "authors": ["Chen Yueh-Han", "Nitish Joshi", "Yulin Chen", "Maksym Andriushchenko", "Rico Angell", "He He"], "summary": "Current LLM safety defenses fail under decomposition attacks, where a\nmalicious goal is decomposed into benign subtasks that circumvent refusals. The\nchallenge lies in the existing shallow safety alignment techniques: they only\ndetect harm in the immediate prompt and do not reason about long-range intent,\nleaving them blind to malicious intent that emerges over a sequence of\nseemingly benign instructions. We therefore propose adding an external monitor\nthat observes the conversation at a higher granularity. To facilitate our study\nof monitoring decomposition attacks, we curate the largest and most diverse\ndataset to date, including question-answering, text-to-image, and agentic\ntasks. We verify our datasets by testing them on frontier LLMs and show an 87%\nattack success rate on average on GPT-4o. This confirms that decomposition\nattack is broadly effective. Additionally, we find that random tasks can be\ninjected into the decomposed subtasks to further obfuscate malicious intents.\nTo defend in real time, we propose a lightweight sequential monitoring\nframework that cumulatively evaluates each subtask. We show that a carefully\nprompt engineered lightweight monitor achieves a 93% defense success rate,\nbeating reasoning models like o3 mini as a monitor. Moreover, it remains robust\nagainst random task injection and cuts cost by 90% and latency by 50%. Our\nfindings suggest that lightweight sequential monitors are highly effective in\nmitigating decomposition attacks and are viable in deployment.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.10949v1", "AI": {"title_translation": "使用轻量级序列监视器监控大型语言模型中的分解攻击", "tldr": "当前LLM安全防御在分解攻击下失败，本研究提出并验证了一种轻量级序列监视器，能有效防御此类攻击，且成本和延迟更低。", "motivation": "当前的LLM安全防御在面对分解攻击时表现不佳，因为它们仅检测即时提示中的危害，而无法推理长程意图，导致对通过一系列看似良性的指令逐渐显现的恶意意图视而不见。", "method": "提出并增加了一个外部监视器，以更高粒度观察对话。为此，作者策划了迄今为止最大、最多样化的数据集，包括问答、文本到图像和代理任务。开发了一个轻量级序列监控框架，该框架累积评估每个子任务，并采用精心设计的提示工程。", "result": "通过在前沿LLM上测试，验证了数据集，显示GPT-4o上的平均攻击成功率为87%，确认分解攻击普遍有效。发现随机任务可以注入到分解的子任务中以进一步混淆恶意意图。精心设计的轻量级监视器实现了93%的防御成功率，优于像o3 mini这样的推理模型，并且对随机任务注入保持鲁棒性，成本降低90%，延迟降低50%。", "conclusion": "轻量级序列监视器在缓解分解攻击方面非常有效，并且在部署中是可行的。", "translation": "当前大型语言模型（LLM）的安全防御在分解攻击下失效，即恶意目标被分解为规避拒绝的良性子任务。挑战在于现有的浅层安全对齐技术：它们仅检测即时提示中的危害，而无法推理长程意图，导致它们对通过一系列看似良性的指令序列中出现的恶意意图视而不见。因此，我们建议添加一个外部监视器，以更高的粒度观察对话。为了促进我们对分解攻击的监控研究，我们策划了迄今为止最大、最多样化的数据集，包括问答、文本到图像和代理任务。我们通过在前沿LLM上测试来验证我们的数据集，并显示在GPT-4o上的平均攻击成功率为87%。这证实了分解攻击普遍有效。此外，我们发现随机任务可以注入到分解的子任务中以进一步混淆恶意意图。为了实时防御，我们提出了一种轻量级序列监控框架，该框架累积评估每个子任务。我们表明，一个经过精心提示工程的轻量级监视器实现了93%的防御成功率，优于像o3 mini这样的推理模型作为监视器。此外，它对随机任务注入保持鲁棒性，并将成本降低90%，延迟降低50%。我们的发现表明，轻量级序列监视器在缓解分解攻击方面非常有效，并且在部署中是可行的。", "summary": "本研究旨在解决大型语言模型（LLM）在面对分解攻击时的安全防御缺陷。当前防御机制因缺乏对长程意图的推理能力而失效。为此，本文提出了一个外部轻量级序列监视器，通过累积评估对话中的子任务来实时检测恶意意图。作者构建了迄今为止最大、最多样化的分解攻击数据集，并验证了其有效性（GPT-4o上87%的攻击成功率）。实验结果表明，所提出的轻量级监视器实现了93%的防御成功率，优于现有推理模型，且显著降低了成本（90%）和延迟（50%），同时对随机任务注入保持鲁棒性。研究表明该监视器在缓解分解攻击方面高效且具有部署可行性。", "keywords": "LLM安全, 分解攻击, 序列监控, 轻量级监视器, 提示工程", "comments": "本文提出了一种新颖且实用的方法来应对LLM的分解攻击，该攻击是现有浅层安全对齐技术面临的主要挑战。其创新之处在于引入了外部轻量级序列监视器，并结合精心设计的提示工程，实现了高防御成功率和显著的资源效率提升。构建大型多样化数据集也为后续研究提供了宝贵资源。这项工作对LLM的安全部署具有重要意义，尤其是在需要实时、高效防御的场景。"}}
{"id": "2506.10391", "title": "ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion", "authors": ["Yuanyi Song", "Pumeng Lyu", "Ben Fei", "Fenghua Ling", "Wanli Ouyang", "Lei Bai"], "summary": "Accurate reconstruction of ocean is essential for reflecting global climate\ndynamics and supporting marine meteorological research. Conventional methods\nface challenges due to sparse data, algorithmic complexity, and high\ncomputational costs, while increasing usage of machine learning (ML) method\nremains limited to reconstruction problems at the sea surface and local\nregions, struggling with issues like cloud occlusion. To address these\nlimitations, this paper proposes ReconMOST, a data-driven guided diffusion\nmodel framework for multi-layer sea temperature reconstruction. Specifically,\nwe first pre-train an unconditional diffusion model using a large collection of\nhistorical numerical simulation data, enabling the model to attain physically\nconsistent distribution patterns of ocean temperature fields. During the\ngeneration phase, sparse yet high-accuracy in-situ observational data are\nutilized as guidance points for the reverse diffusion process, generating\naccurate reconstruction results. Importantly, in regions lacking direct\nobservational data, the physically consistent spatial distribution patterns\nlearned during pre-training enable implicitly guided and physically plausible\nreconstructions. Our method extends ML-based SST reconstruction to a global,\nmulti-layer setting, handling over 92.5% missing data while maintaining\nreconstruction accuracy, spatial resolution, and superior generalization\ncapability. We pre-train our model on CMIP6 numerical simulation data and\nconduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The\nresults of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on\nreconstruction, and 0.633 on total, respectively, demonstrating the\neffectiveness and robustness of the proposed framework. Our source code is\navailable at https://github.com/norsheep/ReconMOST.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10391v1", "AI": {"title_translation": "ReconMOST：基于观测引导扩散的多层海温重建", "tldr": "ReconMOST是一种数据驱动的引导扩散模型，用于多层海温重建，解决了传统方法和现有机器学习方法的局限性，实现了高精度、广范围的重建。", "motivation": "准确的海洋重建对于反映全球气候动态和支持海洋气象研究至关重要。传统方法面临数据稀疏、算法复杂和计算成本高的问题。现有的机器学习方法主要局限于海面和局部区域的重建，并存在云遮挡等问题，无法处理多层和全球范围的重建需求。", "method": "本文提出了ReconMOST，一个数据驱动的引导扩散模型框架用于多层海温重建。首先，使用大量历史数值模拟数据预训练一个无条件扩散模型，使其学习到物理一致的海洋温度场分布模式。在生成阶段，利用稀疏但高精度的现场观测数据作为引导点，指导逆向扩散过程，生成准确的重建结果。在缺乏直接观测数据的区域，通过预训练学习到的物理一致空间分布模式实现隐式引导和物理合理的重建。", "result": "ReconMOST将基于机器学习的SST重建扩展到全球、多层设置，在保持重建精度、空间分辨率和卓越泛化能力的同时，处理了超过92.5%的缺失数据。在CMIP6数值模拟数据上预训练模型，并在CMIP6和EN4分析数据上进行了引导重建实验。结果显示，均方误差（MSE）值在引导上达到0.049，在重建上达到0.680，在总计上达到0.633。", "conclusion": "ReconMOST框架有效且鲁棒地解决了多层海温重建中的挑战，通过结合预训练的物理一致性学习和观测数据引导，实现了高精度、大范围的海洋温度场重建。", "translation": "准确的海洋重建对于反映全球气候动态和支持海洋气象研究至关重要。传统方法面临数据稀疏、算法复杂和计算成本高的问题，而机器学习方法的使用日益增多，但仍局限于海面和局部区域的重建问题，并受限于云遮挡等问题。为了解决这些局限性，本文提出了ReconMOST，一个数据驱动的引导扩散模型框架，用于多层海温重建。具体来说，我们首先使用大量历史数值模拟数据预训练一个无条件扩散模型，使模型能够获得物理一致的海洋温度场分布模式。在生成阶段，利用稀疏但高精度的现场观测数据作为引导点，用于逆向扩散过程，生成准确的重建结果。重要的是，在缺乏直接观测数据的区域，通过预训练学习到的物理一致空间分布模式能够实现隐式引导和物理合理的重建。我们的方法将基于机器学习的SST重建扩展到全球、多层设置，在处理超过92.5%缺失数据的同时，保持了重建精度、空间分辨率和卓越的泛化能力。我们在CMIP6数值模拟数据上预训练了模型，并在CMIP6和EN4分析数据上进行了引导重建实验。结果显示，均方误差（MSE）值在引导上达到0.049，在重建上达到0.680，在总计上达到0.633，证明了所提出框架的有效性和鲁棒性。我们的源代码可在https://github.com/norsheep/ReconMOST获取。", "summary": "ReconMOST是一种新型的数据驱动引导扩散模型框架，旨在解决传统方法和现有机器学习模型在多层海温重建中面临的数据稀疏、计算复杂和覆盖范围有限等挑战。该模型通过结合大规模历史数值模拟数据的预训练（学习物理一致性）和稀疏高精度现场观测数据的引导（精细化重建），实现了全球、多层海温的高精度重建，即使在超过92.5%数据缺失的情况下也能保持出色的性能和泛化能力，并已通过CMIP6和EN4数据的实验验证其有效性和鲁棒性。", "keywords": "海温重建, 扩散模型, 机器学习, 数据同化, 气候建模", "comments": "ReconMOST的创新之处在于将扩散模型应用于多层海温重建，并通过结合无条件预训练和观测引导，有效克服了海洋数据稀疏性问题，并确保了重建结果的物理一致性。其能够处理高比例缺失数据并扩展到全球多层设置，使其在海洋科学研究和气候建模中具有重要应用价值。"}}
{"id": "2506.10259", "title": "Meta-learning Representations for Learning from Multiple Annotators", "authors": ["Atsutoshi Kumagai", "Tomoharu Iwata", "Taishi Nishiyama", "Yasutoshi Ida", "Yasuhiro Fujiwara"], "summary": "We propose a meta-learning method for learning from multiple noisy\nannotators. In many applications such as crowdsourcing services, labels for\nsupervised learning are given by multiple annotators. Since the annotators have\ndifferent skills or biases, given labels can be noisy. To learn accurate\nclassifiers, existing methods require many noisy annotated data. However,\nsufficient data might be unavailable in practice. To overcome the lack of data,\nthe proposed method uses labeled data obtained in different but related tasks.\nThe proposed method embeds each example in tasks to a latent space by using a\nneural network and constructs a probabilistic model for learning a\ntask-specific classifier while estimating annotators' abilities on the latent\nspace. This neural network is meta-learned to improve the expected test\nclassification performance when the classifier is adapted to a given small\namount of annotated data. This classifier adaptation is performed by maximizing\nthe posterior probability via the expectation-maximization (EM) algorithm.\nSince each step in the EM algorithm is easily computed as a closed-form and is\ndifferentiable, the proposed method can efficiently backpropagate the loss\nthrough the EM algorithm to meta-learn the neural network. We show the\neffectiveness of our method with real-world datasets with synthetic noise and\nreal-world crowdsourcing datasets.", "comment": "24 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10259v1", "AI": {"title_translation": "元学习表示用于从多位标注者学习", "tldr": "提出一种元学习方法，通过利用相关任务数据和估计标注者能力，在数据量小的情况下从多位嘈杂标注者处有效学习。", "motivation": "现有从多位嘈杂标注者学习的方法需要大量嘈杂标注数据，但在实际应用中数据可能不足。标注者技能和偏见导致标签噪声是普遍问题。", "method": "提出一种元学习方法，利用来自不同但相关任务的标注数据。该方法使用神经网络将每个示例嵌入到潜在空间，并构建一个概率模型，用于学习任务特定分类器，同时估计潜在空间中的标注者能力。神经网络通过期望最大化（EM）算法进行元学习，以优化分类器在少量标注数据上的预期测试分类性能。EM算法的每一步都是闭合形式且可微分，支持高效的损失反向传播。", "result": "该方法在带有合成噪声的真实世界数据集和真实世界众包数据集上显示出有效性。", "conclusion": "提出的元学习方法能够有效解决从多位嘈杂标注者学习时数据不足的问题，并通过结合神经网络的表示学习和概率模型的标注者能力估计，实现了高效的分类器适应和学习。", "translation": "我们提出了一种元学习方法，用于从多个嘈杂标注者那里学习。在许多应用中，例如众包服务，监督学习的标签由多个标注者提供。由于标注者具有不同的技能或偏见，给定的标签可能存在噪声。为了学习准确的分类器，现有方法需要大量的嘈杂标注数据。然而，在实践中可能无法获得足够的数据。为了克服数据不足的问题，所提出的方法利用了在不同但相关任务中获得的标注数据。所提出的方法使用神经网络将任务中的每个示例嵌入到潜在空间中，并构建一个概率模型，用于学习特定任务的分类器，同时估计潜在空间中标注者的能力。该神经网络通过元学习来提高当分类器适应给定少量标注数据时的预期测试分类性能。这种分类器适应通过期望最大化（EM）算法最大化后验概率来执行。由于EM算法中的每一步都可以轻松地以闭合形式计算并且是可微分的，因此所提出的方法可以有效地通过EM算法反向传播损失以元学习神经网络。我们通过带有合成噪声的真实世界数据集和真实世界众包数据集展示了我们方法的有效性。", "summary": "本文提出一种元学习方法，旨在解决从多位嘈杂标注者学习时数据不足的问题。该方法通过利用相关任务的标注数据，将示例嵌入到潜在空间中，并构建概率模型以同时学习任务特定分类器和估计标注者能力。其核心在于通过期望最大化（EM）算法对神经网络进行元学习，以优化分类器在少量新数据上的性能，并实现高效的损失反向传播。实验证明了该方法在真实世界数据集上的有效性。", "keywords": "元学习, 嘈杂标注者, 众包, 概率模型, 期望最大化", "comments": "该论文的创新点在于将元学习应用于从多位嘈杂标注者学习的场景，尤其是在数据稀缺的情况下。通过结合神经网络的表示学习能力和概率模型的标注者能力估计，以及可微分的EM算法，提供了一种高效且有效的数据利用策略。这对于众包等实际应用具有重要意义。"}}
{"id": "2506.10622", "title": "SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis", "authors": ["Sergio Burdisso", "Esaú Villatoro-Tello", "Petr Motlicek"], "summary": "The advancement of conversational AI systems relies on the availability of\nhigh-quality, flexible, and reproducible synthetic dialogues for training,\nevaluation, and benchmarking. SDialog is a modular, extensible Python toolkit\ndesigned to address the challenges of synthetic dialogue generation and\nanalysis. By leveraging instruction-tuned Large Language Models (LLMs), SDialog\nprovides abstractions for personas, orchestration, and scenario management,\nenabling the creation of realistic, diverse, and controllable conversational\ndata for research and development. SDialog supports workflows such as\nmulti-agent simulation and scenario-driven generation, and represents a step\nforward in the standardization of tools and frameworks for synthetic data\ngeneration, a crucial advancement for ensuring reproducibility in today's\nfast-evolving research landscape.", "comment": "https://github.com/idiap/sdialog", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10622v1", "AI": {"title_translation": "SDialog：一个用于合成对话生成与分析的Python工具包", "tldr": "SDialog是一个Python工具包，利用LLMs生成高质量、可控的合成对话数据，以支持会话AI系统的训练、评估和基准测试，并促进数据生成工具的标准化和可复现性。", "motivation": "会话AI系统的发展依赖于高质量、灵活且可复现的合成对话数据，用于训练、评估和基准测试。", "method": "SDialog是一个模块化、可扩展的Python工具包，通过利用指令调优的大型语言模型（LLMs），提供角色、编排和场景管理的抽象，从而生成逼真、多样化和可控的对话数据。它支持多智能体模拟和场景驱动生成等工作流。", "result": "SDialog能够生成逼真、多样化和可控的对话数据，支持多智能体模拟和场景驱动生成等工作流，为会话AI的研究和开发提供高质量的合成数据。", "conclusion": "SDialog代表了合成数据生成工具和框架标准化方面的重要进展，对于确保当今快速发展的研究环境中的可复现性至关重要。", "translation": "对话式AI系统的进步依赖于高质量、灵活且可复现的合成对话数据，用于训练、评估和基准测试。SDialog是一个模块化、可扩展的Python工具包，旨在解决合成对话生成和分析的挑战。通过利用指令调优的大型语言模型（LLMs），SDialog为角色、编排和场景管理提供了抽象，从而能够为研究和开发创建逼真、多样化和可控的对话数据。SDialog支持多智能体模拟和场景驱动生成等工作流，代表了合成数据生成工具和框架标准化方面的重要进展，这是确保当今快速发展的研究环境中可复现性的关键进步。", "summary": "SDialog是一个基于指令调优LLMs的Python工具包，旨在解决合成对话生成与分析的挑战。它通过提供角色、编排和场景管理抽象，生成高质量、多样化且可控的对话数据，支持多智能体模拟等工作流，从而促进会话AI系统的开发，并有助于合成数据生成工具的标准化和研究可复现性。", "keywords": "合成对话, 大型语言模型, Python工具包, 数据生成, 可复现性", "comments": "SDialog的创新之处在于其利用指令调优LLMs生成高质量合成对话数据的能力，并提供了灵活的抽象层（如角色、场景管理）。其重要性在于解决了会话AI系统对可复现、高质量训练和评估数据的需求，并致力于推动合成数据生成工具的标准化，这对于确保AI研究的可信度和可复现性至关重要。"}}
{"id": "2506.10395", "title": "Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation", "authors": ["Zhiyang Xu", "Jiuhai Chen", "Zhaojiang Lin", "Xichen Pan", "Lifu Huang", "Tianyi Zhou", "Madian Khabsa", "Qifan Wang", "Di Jin", "Michihiro Yasunaga", "Lili Yu", "Xi Victoria Lin", "Shaoliang Nie"], "summary": "Recent advances in large language models (LLMs) have enabled multimodal\nfoundation models to tackle both image understanding and generation within a\nunified framework. Despite these gains, unified models often underperform\ncompared to specialized models in either task. A key challenge in developing\nunified models lies in the inherent differences between the visual features\nneeded for image understanding versus generation, as well as the distinct\ntraining processes required for each modality. In this work, we introduce\nPisces, an auto-regressive multimodal foundation model that addresses this\nchallenge through a novel decoupled visual encoding architecture and tailored\ntraining techniques optimized for multimodal generation. Combined with\nmeticulous data curation, pretraining, and finetuning, Pisces achieves\ncompetitive performance in both image understanding and image generation. We\nevaluate Pisces on over 20 public benchmarks for image understanding, where it\ndemonstrates strong performance across a wide range of tasks. Additionally, on\nGenEval, a widely adopted benchmark for image generation, Pisces exhibits\nrobust generative capabilities. Our extensive analysis reveals the synergistic\nrelationship between image understanding and generation, and the benefits of\nusing separate visual encoders, advancing the field of unified multimodal\nmodels.", "comment": "Unified image understanding and generation model", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10395v1", "AI": {"title_translation": "Pisces：一种用于图像理解和生成的自回归基础模型", "tldr": "Pisces是一个自回归多模态基础模型，通过解耦视觉编码和定制训练，在图像理解和生成任务上均实现了有竞争力的性能。", "motivation": "现有统一的多模态模型在图像理解或生成任务上往往不如专用模型，主要挑战在于图像理解和生成所需的视觉特征以及训练过程存在固有差异。", "method": "本文引入了Pisces，一个自回归多模态基础模型，通过新颖的解耦视觉编码架构和为多模态生成优化的定制训练技术来解决挑战。结合细致的数据整理、预训练和微调。", "result": "Pisces在图像理解和图像生成方面均取得了有竞争力的性能。在超过20个图像理解公共基准测试中表现出色，并在GenEval图像生成基准测试中展现出强大的生成能力。", "conclusion": "图像理解和生成之间存在协同关系，并且使用独立的视觉编码器是有益的，这推动了统一多模态模型领域的发展。", "translation": "大型语言模型（LLMs）的最新进展使得多模态基础模型能够在统一框架内处理图像理解和生成。尽管取得了这些进展，但统一模型在任一任务上的性能往往不如专用模型。开发统一模型的关键挑战在于图像理解和生成所需的视觉特征之间固有的差异，以及每种模态所需的独特训练过程。在这项工作中，我们引入了Pisces，一个自回归多模态基础模型，通过新颖的解耦视觉编码架构和为多模态生成优化的定制训练技术来解决这一挑战。结合细致的数据整理、预训练和微调，Pisces在图像理解和图像生成方面均取得了有竞争力的性能。我们在超过20个图像理解公共基准测试中评估了Pisces，它在广泛的任务中表现出强大的性能。此外，在广泛采用的图像生成基准GenEval上，Pisces展现出强大的生成能力。我们广泛的分析揭示了图像理解和生成之间的协同关系，以及使用独立视觉编码器的好处，从而推动了统一多模态模型领域的发展。", "summary": "Pisces是一个自回归多模态基础模型，旨在解决现有统一模型在图像理解和生成任务中性能不佳的问题。它通过引入解耦视觉编码架构和定制训练技术，并结合细致的数据处理，实现了在图像理解和生成任务上的竞争力表现。该模型在多个公共基准测试中验证了其能力，并揭示了图像理解与生成之间的协同作用以及独立视觉编码器的优势。", "keywords": "多模态基础模型, 图像理解, 图像生成, 自回归模型, 解耦视觉编码", "comments": "Pisces的创新之处在于其解耦视觉编码架构和定制训练技术，这有效解决了统一多模态模型在图像理解和生成任务中性能折衷的问题。该研究强调了理解与生成任务协同作用的重要性，并为未来多模态基础模型的设计提供了有价值的见解。"}}
{"id": "2506.10269", "title": "Interior-Point Vanishing Problem in Semidefinite Relaxations for Neural Network Verification", "authors": ["Ryota Ueda", "Takami Sato", "Ken Kobayashi", "Kazuhide Nakata"], "summary": "Semidefinite programming (SDP) relaxation has emerged as a promising approach\nfor neural network verification, offering tighter bounds than other convex\nrelaxation methods for deep neural networks (DNNs) with ReLU activations.\nHowever, we identify a critical limitation in the SDP relaxation when applied\nto deep networks: interior-point vanishing, which leads to the loss of strict\nfeasibility -- a crucial condition for the numerical stability and optimality\nof SDP. Through rigorous theoretical and empirical analysis, we demonstrate\nthat as the depth of DNNs increases, the strict feasibility is likely to be\nlost, creating a fundamental barrier to scaling SDP-based verification. To\naddress the interior-point vanishing, we design and investigate five solutions\nto enhance the feasibility conditions of the verification problem. Our methods\ncan successfully solve 88% of the problems that could not be solved by existing\nmethods, accounting for 41% of the total. Our analysis also reveals that the\nvalid constraints for the lower and upper bounds for each ReLU unit are\ntraditionally inherited from prior work without solid reasons, but are actually\nnot only unbeneficial but also even harmful to the problem's feasibility. This\nwork provides valuable insights into the fundamental challenges of SDP-based\nDNN verification and offers practical solutions to improve its applicability to\ndeeper neural networks, contributing to the development of more reliable and\nsecure systems with DNNs.", "comment": "17 pages, 2 figures. Version revised after ICML 2025 reviews", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10269v1", "AI": {"title_translation": "神经网络验证中半正定松弛的内点消失问题", "tldr": "基于SDP的深度神经网络验证存在“内点消失”问题，导致数值不稳定性和可扩展性受限。本文识别了这一问题，分析了其影响，并提出了五种解决方案，显著提高了问题解决率，同时揭示了传统ReLU约束的负面影响。", "motivation": "半正定规划（SDP）松弛是神经网络验证的一种有前景的方法，但当应用于深度网络时，会遇到“内点消失”的严重限制，导致严格可行性丧失，而这对于SDP的数值稳定性和最优性至关重要。这成为了基于SDP的验证方法扩展性的根本障碍。", "method": "作者设计并研究了五种解决方案来增强验证问题的可行性条件，以解决内点消失问题。此外，他们还对传统的、从先前工作中继承的ReLU单元的下限和上限约束进行了分析，发现这些约束不仅无益反而有害于问题的可行性。", "result": "提出的方法成功解决了现有方法无法解决的88%的问题，占总问题的41%。研究还发现，传统上用于每个ReLU单元的下限和上限约束不仅无益，甚至对问题的可行性有害。", "conclusion": "这项工作为基于SDP的深度神经网络验证中的基本挑战提供了宝贵的见解，并提供了实用的解决方案，以提高其对更深层神经网络的适用性，从而有助于开发更可靠、更安全的DNN系统。", "translation": "半正定规划（SDP）松弛已成为神经网络验证的一种有前途的方法，为带有ReLU激活的深度神经网络（DNN）提供了比其他凸松弛方法更紧密的界限。然而，我们发现SDP松弛在应用于深度网络时存在一个关键限制：内点消失，这导致严格可行性的丧失——这是SDP数值稳定性和最优性的关键条件。通过严格的理论和实证分析，我们证明随着DNN深度的增加，严格可行性很可能丧失，这为基于SDP的验证的扩展设置了根本障碍。为了解决内点消失问题，我们设计并研究了五种解决方案来增强验证问题的可行性条件。我们的方法成功解决了现有方法无法解决的问题中的88%，占总数的41%。我们的分析还揭示，每个ReLU单元的下限和上限的有效约束传统上是从先前的研究中继承而来，没有充分的理由，但实际上不仅无益，甚至对问题的可行性有害。这项工作为基于SDP的DNN验证的基本挑战提供了宝贵的见解，并提供了实用的解决方案来提高其对更深层神经网络的适用性，从而有助于开发更可靠、更安全的DNN系统。", "summary": "本文针对深度神经网络（DNN）验证中半正定规划（SDP）松弛存在的“内点消失”问题进行了研究。该问题导致严格可行性丧失，从而阻碍了基于SDP方法的数值稳定性和可扩展性，尤其是在DNN深度增加时。作者提出了五种解决方案，显著提高了问题解决能力（解决了88%先前无法解决的问题）。此外，他们还发现，传统上继承的ReLU单元约束实际上对可行性有害。这项工作为SDP-based DNN验证提供了关键见解和实用改进，增强了其对更深层网络的适用性，有助于提升DNN系统的可靠性。", "keywords": "半正定规划, 神经网络验证, 内点法, 深度神经网络, ReLU激活", "comments": "本文识别并解决了SDP松弛在深度神经网络验证中一个核心的数值稳定性问题——内点消失，这是一个重要的创新点。通过提出实用解决方案并重新评估传统约束的有效性，该研究不仅提升了SDP方法的适用性和可扩展性，也为未来相关研究提供了宝贵的理论和实践指导，对于开发更可靠的深度学习系统具有重要意义。"}}
{"id": "2506.10627", "title": "NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors", "authors": ["Numaan Naeem", "Sarfraz Ahmad", "Momina Ahsan", "Hasan Iqbal"], "summary": "This paper presents our system for Track 1: Mistake Identification in the BEA\n2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The\ntask involves evaluating whether a tutor's response correctly identifies a\nmistake in a student's mathematical reasoning. We explore four approaches: (1)\nan ensemble of machine learning models over pooled token embeddings from\nmultiple pretrained language models (LMs); (2) a frozen sentence-transformer\nusing [CLS] embeddings with an MLP classifier; (3) a history-aware model with\nmulti-head attention between token-level history and response embeddings; and\n(4) a retrieval-augmented few-shot prompting system with a large language model\n(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,\nconstructs structured prompts, and uses schema-guided output parsing to produce\ninterpretable predictions. It outperforms all baselines, demonstrating the\neffectiveness of combining example-driven prompting with LLM reasoning for\npedagogical feedback assessment. Our code is available at\nhttps://github.com/NaumanNaeem/BEA_2025.", "comment": "6 pages, 2 figures, 1 table", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10627v1", "AI": {"title_translation": "NeuralNexus在BEA 2025共享任务中：用于AI导师错误识别的检索增强提示", "tldr": "本文介绍了NeuralNexus团队在BEA 2025共享任务中用于AI导师错误识别的系统，该系统采用检索增强提示与大型语言模型（LLM）结合，并超越了所有基线。", "motivation": "该研究的动机是参与BEA 2025共享任务中的“错误识别”赛道，评估AI驱动的导师回应是否正确识别了学生数学推理中的错误。", "method": "论文探索了四种方法：1）多个预训练语言模型池化词元嵌入的机器学习模型集成；2）使用[CLS]嵌入和MLP分类器的冻结句子转换器；3）具有词元级历史和响应嵌入之间多头注意力的历史感知模型；4）使用大型语言模型（如GPT 4o）的检索增强少样本提示系统。最终系统通过检索语义相似的示例、构建结构化提示和使用模式引导的输出解析来生成可解释的预测。", "result": "最终系统超越了所有基线。", "conclusion": "将示例驱动提示与大型语言模型推理相结合，对于教学反馈评估是有效的。", "translation": "本文介绍了我们在BEA 2025关于AI驱动导师教学能力评估共享任务中，用于赛道1：错误识别的系统。该任务涉及评估导师的回答是否正确识别了学生数学推理中的错误。我们探索了四种方法：（1）基于多个预训练语言模型（LMs）池化词元嵌入的机器学习模型集成；（2）使用[CLS]嵌入和MLP分类器的冻结句子转换器；（3）具有词元级历史和响应嵌入之间多头注意力的历史感知模型；（4）使用大型语言模型（LLM）即GPT 4o的检索增强少样本提示系统。我们最终的系统检索语义相似的示例，构建结构化提示，并使用模式引导的输出解析来生成可解释的预测。它超越了所有基线，证明了将示例驱动提示与LLM推理相结合进行教学反馈评估的有效性。我们的代码可在https://github.com/NaumanNaeem/BEA_2025获取。", "summary": "本文详细介绍了NeuralNexus团队为BEA 2025共享任务中“错误识别”赛道开发的系统，旨在评估AI导师对学生数学推理错误的识别能力。研究探索了四种方法，其中最成功的是结合GPT-4o的检索增强少样本提示系统。该系统通过检索相似示例、构建结构化提示并解析输出，最终超越了所有基线，证明了将示例驱动提示与大型语言模型推理相结合在教学反馈评估中的有效性。", "keywords": "AI导师, 错误识别, 检索增强提示, LLM, 教学评估", "comments": "该论文的创新之处在于其将检索增强提示与强大的大型语言模型（GPT-4o）应用于教育AI领域中一个具体且具有挑战性的NLP任务。将示例驱动提示与LLM推理相结合的方法，对于教学反馈评估显示出强大的潜力，这对于开发更有效的AI导师至关重要。代码的开源也促进了研究的可复现性和进一步发展。"}}
{"id": "2506.10364", "title": "Can We Infer Confidential Properties of Training Data from LLMs?", "authors": ["Penguin Huang", "Chhavi Yadav", "Ruihan Wu", "Kamalika Chaudhuri"], "summary": "Large language models (LLMs) are increasingly fine-tuned on domain-specific\ndatasets to support applications in fields such as healthcare, finance, and\nlaw. These fine-tuning datasets often have sensitive and confidential\ndataset-level properties -- such as patient demographics or disease prevalence\n-- that are not intended to be revealed. While prior work has studied property\ninference attacks on discriminative models (e.g., image classification models)\nand generative models (e.g., GANs for image data), it remains unclear if such\nattacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark\ntask for evaluating property inference in LLMs under two fine-tuning paradigms:\nquestion-answering and chat-completion. Built on the ChatDoctor dataset, our\nbenchmark includes a range of property types and task configurations. We\nfurther propose two tailored attacks: a prompt-based generation attack and a\nshadow-model attack leveraging word frequency signals. Empirical evaluations\nacross multiple pretrained LLMs show the success of our attacks, revealing a\npreviously unrecognized vulnerability in LLMs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10364v1", "AI": {"title_translation": "我们能否从大型语言模型中推断出训练数据的机密属性？", "tldr": "大型语言模型（LLMs）在敏感数据上进行微调后，容易受到属性推断攻击。本研究引入了一个基准测试和两种定制攻击，成功揭示了LLMs中此前未被识别的漏洞。", "motivation": "大型语言模型（LLMs）越来越多地在特定领域的敏感和机密数据集上进行微调，例如医疗、金融和法律领域。这些数据集通常包含不应被泄露的敏感数据集级别属性（如患者人口统计数据或疾病流行率）。虽然之前的研究已经探讨了判别模型和生成模型上的属性推断攻击，但尚不清楚此类攻击是否适用于LLMs。", "method": "本研究引入了PropInfer，一个用于评估LLMs在两种微调范式（问答和聊天完成）下属性推断的基准任务。该基准构建于ChatDoctor数据集之上，包含多种属性类型和任务配置。此外，研究还提出了两种定制攻击：一种是基于提示的生成攻击，另一种是利用词频信号的影子模型攻击。", "result": "对多个预训练LLMs进行的实证评估表明，所提出的攻击是成功的。", "conclusion": "研究揭示了LLMs中一个此前未被识别的漏洞。", "translation": "大型语言模型（LLMs）越来越多地在特定领域的敏感数据集上进行微调，以支持医疗、金融和法律等领域的应用。这些微调数据集通常具有敏感和机密的数据集级别属性——例如患者人口统计数据或疾病流行率——这些属性不打算被揭示。尽管先前的研究已经探讨了判别模型（例如图像分类模型）和生成模型（例如图像数据的GANs）上的属性推断攻击，但尚不清楚此类攻击是否会转移到LLMs。在这项工作中，我们引入了PropInfer，一个用于评估LLMs在两种微调范式下属性推断的基准任务：问答和聊天完成。我们的基准建立在ChatDoctor数据集之上，包含一系列属性类型和任务配置。我们进一步提出了两种定制攻击：一种是基于提示的生成攻击和一种利用词频信号的影子模型攻击。对多个预训练LLMs进行的实证评估显示了我们攻击的成功，揭示了LLMs中一个此前未被识别的漏洞。", "summary": "本研究引入了PropInfer，一个评估大型语言模型（LLMs）属性推断能力的基准任务，旨在探究LLMs在敏感数据微调后是否存在隐私漏洞。该基准基于ChatDoctor数据集，涵盖问答和聊天完成两种微调范式。研究提出了两种定制攻击方法：基于提示的生成攻击和利用词频的影子模型攻击。实证结果表明这些攻击是成功的，揭示了LLMs中此前未被识别的数据隐私漏洞。", "keywords": "大型语言模型, 属性推断, 数据隐私, 微调, 漏洞", "comments": "这篇论文解决了LLMs在处理敏感数据时一个关键的隐私问题。引入专门的基准测试（PropInfer）和定制的攻击方法是创新性的，它突出显示了一个重要的安全漏洞，并敦促对鲁棒的防御机制进行进一步研究。"}}
{"id": "2506.10425", "title": "It's Not the Target, It's the Background: Rethinking Infrared Small Target Detection via Deep Patch-Free Low-Rank Representations", "authors": ["Guoyi Zhang", "Guangsheng Xu", "Siyang Chen", "Han Wang", "Xiaohu Zhang"], "summary": "Infrared small target detection (IRSTD) remains a long-standing challenge in\ncomplex backgrounds due to low signal-to-clutter ratios (SCR), diverse target\nmorphologies, and the absence of distinctive visual cues. While recent deep\nlearning approaches aim to learn discriminative representations, the intrinsic\nvariability and weak priors of small targets often lead to unstable\nperformance. In this paper, we propose a novel end-to-end IRSTD framework,\ntermed LRRNet, which leverages the low-rank property of infrared image\nbackgrounds. Inspired by the physical compressibility of cluttered scenes, our\napproach adopts a compression--reconstruction--subtraction (CRS) paradigm to\ndirectly model structure-aware low-rank background representations in the image\ndomain, without relying on patch-based processing or explicit matrix\ndecomposition. To the best of our knowledge, this is the first work to directly\nlearn low-rank background structures using deep neural networks in an\nend-to-end manner. Extensive experiments on multiple public datasets\ndemonstrate that LRRNet outperforms 38 state-of-the-art methods in terms of\ndetection accuracy, robustness, and computational efficiency. Remarkably, it\nachieves real-time performance with an average speed of 82.34 FPS. Evaluations\non the challenging NoisySIRST dataset further confirm the model's resilience to\nsensor noise. The source code will be made publicly available upon acceptance.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10425v1", "AI": {"title_translation": "不是目标，而是背景：通过深度无补丁低秩表示重思考红外小目标检测", "tldr": "LRRNet通过直接学习低秩背景表示来改进红外小目标检测，实现了SOTA性能和实时速度。", "motivation": "红外小目标检测（IRSTD）在复杂背景下因低信杂比、目标形态多样性和缺乏独特视觉线索而面临挑战，现有深度学习方法因小目标的内在变异性和弱先验导致性能不稳定。", "method": "本文提出了一种名为LRRNet的端到端红外小目标检测框架。该框架利用红外图像背景的低秩特性，并采用压缩-重建-减法（CRS）范式，直接在图像域中建模结构感知的低秩背景表示，无需基于块的处理或显式矩阵分解。这是首次直接使用深度神经网络以端到端方式学习低秩背景结构。", "result": "LRRNet在多个公共数据集上超越了38种最先进的方法，在检测精度、鲁棒性和计算效率方面表现出色。它实现了实时性能，平均速度达到82.34 FPS。在具有挑战性的NoisySIRST数据集上的评估进一步证实了模型对传感器噪声的弹性。", "conclusion": "LRRNet通过直接学习低秩背景表示，有效解决了红外小目标检测的挑战，并在多个方面取得了最先进的性能，证明了其在实际应用中的潜力。", "translation": "红外小目标检测（IRSTD）在复杂背景下仍然是一个长期存在的挑战，原因在于信杂比（SCR）低、目标形态多样以及缺乏独特的视觉线索。尽管最近的深度学习方法旨在学习判别性表示，但小目标的内在变异性和弱先验往往导致性能不稳定。在本文中，我们提出了一种新颖的端到端IRSTD框架，命名为LRRNet，它利用了红外图像背景的低秩特性。受杂乱场景物理可压缩性的启发，我们的方法采用了一种压缩-重建-减法（CRS）范式，直接在图像域中建模结构感知的低秩背景表示，而无需依赖基于块的处理或显式矩阵分解。据我们所知，这是首次直接使用深度神经网络以端到端方式学习低秩背景结构。在多个公共数据集上进行的大量实验表明，LRRNet在检测精度、鲁棒性和计算效率方面优于38种最先进的方法。值得注意的是，它以82.34 FPS的平均速度实现了实时性能。在具有挑战性的NoisySIRST数据集上的评估进一步证实了模型对传感器噪声的弹性。源代码将在论文接收后公开发布。", "summary": "本文提出了一种名为LRRNet的新型端到端红外小目标检测框架，通过利用红外图像背景的低秩特性。该方法采用压缩-重建-减法（CRS）范式，直接在图像域中学习无补丁的低秩背景表示，从而避免了传统方法的局限性。实验证明，LRRNet在多个公共数据集上显著优于现有SOTA方法，并在准确性、鲁棒性和实时性（82.34 FPS）方面表现出色，特别是在噪声环境下也表现出良好的鲁棒性。", "keywords": "红外小目标检测, 低秩表示, 深度学习, 背景建模, 实时检测", "comments": "这篇论文的创新之处在于它首次提出了一种端到端深度学习方法，通过直接学习图像域中的低秩背景表示来进行红外小目标检测，而不是侧重于目标本身或依赖传统的矩阵分解。这种“无补丁”和“背景优先”的策略有效地解决了小目标特征不明显和现有方法性能不稳定的问题，并通过实时性能展示了其在实际应用中的巨大潜力。"}}
{"id": "2506.10282", "title": "Graph-MLLM: Harnessing Multimodal Large Language Models for Multimodal Graph Learning", "authors": ["Jiajin Liu", "Dongzhe Fan", "Jiacheng Shen", "Chuanhao Ji", "Daochen Zha", "Qiaoyu Tan"], "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in representing and understanding diverse modalities. However,\nthey typically focus on modality alignment in a pairwise manner while\noverlooking structural relationships across data points. Integrating\nmultimodality with structured graph information (i.e., multimodal graphs, MMGs)\nis essential for real-world applications such as social networks, healthcare,\nand recommendation systems. Existing MMG learning methods fall into three\nparadigms based on how they leverage MLLMs: Encoder, Aligner, and Predictor.\nMLLM-as-Encoder focuses on enhancing graph neural networks (GNNs) via\nmultimodal feature fusion; MLLM-as-Aligner aligns multimodal attributes in\nlanguage or hidden space to enable LLM-based graph reasoning; MLLM-as-Predictor\ntreats MLLMs as standalone reasoners with in-context learning or fine-tuning.\nDespite their advances, the MMG field lacks a unified benchmark to fairly\nevaluate across these approaches, making it unclear what progress has been\nmade. To bridge this gap, we present Graph-MLLM, a comprehensive benchmark for\nmultimodal graph learning by systematically evaluating these three paradigms\nacross six datasets with different domains. Through extensive experiments, we\nobserve that jointly considering the visual and textual attributes of the nodes\nbenefits graph learning, even when using pre-trained text-to-image alignment\nmodels (e.g., CLIP) as encoders. We also find that converting visual attributes\ninto textual descriptions further improves performance compared to directly\nusing visual inputs. Moreover, we observe that fine-tuning MLLMs on specific\nMMGs can achieve state-of-the-art results in most scenarios, even without\nexplicit graph structure information. We hope that our open-sourced library\nwill facilitate rapid, equitable evaluation and inspire further innovative\nresearch in this field.", "comment": "16 pages, 4 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10282v1", "AI": {"title_translation": "Graph-MLLM：利用多模态大型语言模型进行多模态图学习", "tldr": "Graph-MLLM是一个多模态图学习的综合基准，系统评估了基于多模态大型语言模型（MLLMs）的三种范式。研究发现，联合考虑节点的多模态属性（特别是将视觉转换为文本描述）以及对MLLMs进行微调，能显著提升图学习性能。", "motivation": "多模态大型语言模型（MLLMs）在表示和理解不同模态方面表现出色，但通常只关注成对的模态对齐，而忽略数据点间的结构关系。将多模态信息与结构化图信息（即多模态图，MMGs）结合对于社交网络、医疗保健和推荐系统等实际应用至关重要。现有MMG学习方法缺乏统一的基准来公平评估，导致进展不明确。", "method": "本文提出了Graph-MLLM，一个用于多模态图学习的综合基准。通过系统评估基于MLLM的三种范式（编码器、对齐器、预测器），并在六个不同领域的数据集上进行广泛实验。", "result": "1. 联合考虑节点的视觉和文本属性有利于图学习，即使使用预训练的文本到图像对齐模型（如CLIP）作为编码器。2. 将视觉属性转换为文本描述比直接使用视觉输入能进一步提高性能。3. 在大多数情况下，对特定MMGs进行MLLMs微调可以达到最先进的结果，即使没有明确的图结构信息。", "conclusion": "Graph-MLLM基准的建立及其开源库将促进多模态图学习领域快速、公平的评估，并激发进一步的创新研究。", "translation": "多模态大型语言模型（MLLMs）在表示和理解不同模态方面展现出卓越的能力。然而，它们通常侧重于成对的模态对齐，而忽略了数据点之间的结构关系。将多模态信息与结构化图信息（即多模态图，MMGs）结合对于社交网络、医疗保健和推荐系统等实际应用至关重要。现有的MMG学习方法根据其利用MLLMs的方式分为三种范式：编码器、对齐器和预测器。MLLM作为编码器侧重于通过多模态特征融合增强图神经网络（GNNs）；MLLM作为对齐器在语言或隐藏空间中对齐多模态属性，以实现基于LLM的图推理；MLLM作为预测器将MLLMs视为具有上下文学习或微调能力的独立推理器。尽管取得了进展，但MMG领域缺乏统一的基准来公平评估这些方法，导致进展不明确。为了弥补这一空白，我们提出了Graph-MLLM，一个用于多模态图学习的综合基准，通过系统评估这三种范式在六个不同领域的数据集上进行。通过大量的实验，我们观察到联合考虑节点的视觉和文本属性有利于图学习，即使使用预训练的文本到图像对齐模型（例如CLIP）作为编码器。我们还发现，与直接使用视觉输入相比，将视觉属性转换为文本描述可以进一步提高性能。此外，我们观察到在特定MMGs上微调MLLMs可以在大多数场景中实现最先进的结果，即使没有明确的图结构信息。我们希望我们的开源库将促进快速、公平的评估，并激发该领域进一步的创新研究。", "summary": "Graph-MLLM引入了一个多模态图学习的综合基准，系统评估了基于MLLMs的三种主要范式（编码器、对齐器、预测器）在六个不同数据集上的表现。研究发现，联合利用节点的视觉和文本属性对图学习有益，尤其当视觉信息转换为文本描述时性能更优。此外，在特定多模态图上对MLLMs进行微调可实现最先进的结果，即使缺乏显式图结构信息。该基准旨在弥补领域内统一评估的空白，并提供开源工具以促进未来研究。", "keywords": "多模态图学习, MLLMs, 基准, 图神经网络, 微调", "comments": "该论文通过提供一个统一的基准，解决了多模态图学习领域的一个关键空白。其发现，特别是关于多模态特征（尤其是文本化的视觉数据）的有效性以及微调MLLMs在处理图数据方面的强大能力，具有重要意义，推动了MLLMs在超越成对模态对齐的应用。开源贡献对于社区的进步也极具价值。"}}
{"id": "2506.10006", "title": "HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction", "authors": ["Jie Qin", "Wei Yang", "Yan Su", "Yiran Zhu", "Weizhen Li", "Yunyue Pan", "Chengchang Pan", "Honggang Qi"], "summary": "Current HER2 assessment models for breast cancer predominantly analyze H&E or\nIHC images in isolation,despite clinical reliance on their synergistic\ninterpretation. However, concurrent acquisition of both modalities is often\nhindered by workflow complexity and cost constraints. We propose an adaptive\nbimodal framework enabling flexible single-/dual-modality HER2 prediction\nthrough three innovations: 1) A dynamic branch selector that activates either\nsingle-modality reconstruction or dual-modality joint inference based on input\ncompleteness; 2) A bidirectional cross-modal GAN performing context-aware\nfeature-space reconstruction of missing modalities; 3) A hybrid training\nprotocol integrating adversarial learning and multi-task optimization. This\narchitecture elevates single-modality H&E prediction accuracy from 71.44% to\n94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%\nreliability with sole IHC inputs. The framework's \"dual-preferred,\nsingle-compatible\" design delivers near-bimodal performance without requiring\nsynchronized acquisition, particularly benefiting resource-limited settings\nthrough IHC infrastructure cost reduction. Experimental validation confirms\n22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with\ncross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251\n(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or\nnative fusion pathways, the system mitigates performance degradation from\nmissing data while preserving computational efficiency (78.55% parameter\nreduction in lightweight variant). This elastic architecture demonstrates\nsignificant potential for democratizing precise HER2 assessment across diverse\nhealthcare settings.", "comment": "7 pages,5 figures,3 tables,submitted to the 33rd ACM International\n  Conference on Multimedia(ACM MM 2025)", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10006v1", "AI": {"title_translation": "动态双向重建实现灵活多模态输入的HER2表达预测", "tldr": "提出了一种通过动态重建缺失模态和自适应推理，实现乳腺癌HER2表达精确预测的框架，即使在单模态输入下也能达到接近双模态的性能。", "motivation": "当前乳腺癌HER2评估模型主要孤立分析H&E或IHC图像，而临床依赖它们的协同解释。然而，同步获取两种模态常受限于工作流程复杂性和成本。本研究旨在解决多模态数据不完整时HER2预测的挑战。", "method": "提出了一个自适应双模态框架，通过以下三项创新实现灵活的单/双模态HER2预测：1）动态分支选择器，根据输入完整性激活单模态重建或双模态联合推理；2）双向跨模态GAN，执行缺失模态的上下文感知特征空间重建；3）混合训练协议，整合对抗性学习和多任务优化。该系统动态地将输入路由到重建增强或原生融合路径。", "result": "将单模态H&E预测精度从71.44%提升至94.25%，双模态精度达到95.09%，单独使用IHC输入时可靠性保持在90.28%。相较于H&E/IHC基线，精度分别提高了22.81%/12.90%，跨模态重建将F1分数提高到0.9609（H&E到IHC）和0.9251（IHC到H&E）。在轻量级变体中实现了78.55%的参数减少，保持了计算效率。", "conclusion": "该框架的“双模态优先，单模态兼容”设计在不要求同步采集的情况下实现了接近双模态的性能，尤其通过降低IHC基础设施成本，有利于资源有限的环境。它减轻了缺失数据导致的性能下降，并展现了在不同医疗环境中普及精确HER2评估的巨大潜力。", "translation": "目前乳腺癌HER2评估模型主要孤立地分析H&E或IHC图像，尽管临床上依赖它们的协同解释。然而，由于工作流程复杂性和成本限制，同时获取两种模态通常受到阻碍。我们提出了一种自适应双模态框架，通过三项创新实现灵活的单/双模态HER2预测：1）一个动态分支选择器，根据输入完整性激活单模态重建或双模态联合推理；2）一个双向跨模态GAN，执行缺失模态的上下文感知特征空间重建；3）一个混合训练协议，整合对抗性学习和多任务优化。该架构将单模态H&E预测精度从71.44%提升至94.25%，同时实现95.09%的双模态精度，并单独使用IHC输入时保持90.28%的可靠性。该框架的“双模态优先，单模态兼容”设计在不要求同步采集的情况下实现了接近双模态的性能，尤其通过降低IHC基础设施成本，有利于资源有限的环境。实验验证证实，与H&E/IHC基线相比，精度分别提高了22.81%/12.90%，跨模态重建将F1分数提高到0.9609（H&E到IHC）和0.9251（IHC到H&E）。通过动态地将输入路由到重建增强或原生融合路径，该系统减轻了缺失数据导致的性能下降，同时保持了计算效率（轻量级变体中参数减少78.55%）。这种弹性架构展示了在不同医疗环境中普及精确HER2评估的巨大潜力。", "summary": "本文提出了一种用于乳腺癌HER2表达预测的自适应双模态框架，旨在解决多模态数据（H&E和IHC图像）不完整的问题。该框架采用动态分支选择器、用于重建缺失模态的双向跨模态GAN以及混合训练协议。实验结果显示，该框架将单模态H&E预测精度显著提升至94.25%，并实现了95.09%的双模态精度。其“双模态优先，单模态兼容”的设计使得在无需同步采集的情况下，也能达到接近双模态的性能，从而使精确的HER2评估更具可及性和效率，尤其适用于资源有限的环境。", "keywords": "HER2预测, 多模态, 图像重建, 深度学习, 乳腺癌", "comments": "该论文的创新之处在于其“双模态优先，单模态兼容”的设计，通过动态重建和自适应推理有效地处理缺失模态。这对于在成本和工作流程限制下难以同时获取所有模态的实际临床环境至关重要。显著的精度提升和计算效率使其成为一个高度实用的解决方案。"}}
{"id": "2506.10641", "title": "Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters", "authors": ["Tatsuya Hiraoka", "Kentaro Inui"], "summary": "Large language models (LLMs) can spell out tokens character by character with\nhigh accuracy, yet they struggle with more complex character-level tasks, such\nas identifying compositional subcomponents within tokens. In this work, we\ninvestigate how LLMs internally represent and utilize character-level\ninformation during the spelling-out process. Our analysis reveals that,\nalthough spelling out is a simple task for humans, it is not handled in a\nstraightforward manner by LLMs. Specifically, we show that the embedding layer\ndoes not fully encode character-level information, particularly beyond the\nfirst character. As a result, LLMs rely on intermediate and higher Transformer\nlayers to reconstruct character-level knowledge, where we observe a distinct\n\"breakthrough\" in their spelling behavior. We validate this mechanism through\nthree complementary analyses: probing classifiers, identification of knowledge\nneurons, and inspection of attention weights.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10641v1", "AI": {"title_translation": "拼写输出并非直截了当：大型语言模型从词元到字符的词元化能力", "tldr": "LLM虽然能准确拼写，但在字符层面处理并不直接，词嵌入层未完全编码字符信息，需中间层和更高层Transformer重构字符知识。", "motivation": "大型语言模型（LLMs）能够高精度地逐字符拼写出词元，但它们在更复杂的字符级任务上（例如识别词元内的组成子成分）却表现挣扎。本研究旨在探究LLMs在拼写输出过程中如何内部表示和利用字符级信息。", "method": "通过三种互补分析方法验证该机制：探测分类器、知识神经元识别和注意力权重检查。", "result": "分析表明，LLM处理拼写输出并非直截了当；词嵌入层未能完全编码字符级信息，特别是首字符之后的信息；LLM依赖中间和更高层的Transformer层来重建字符级知识，在此过程中观察到其拼写行为的明显“突破”。", "conclusion": "LLMs处理字符级信息的方式比预期更复杂，并非直接在嵌入层完成，而是需要通过Transformer的深层结构来逐步重构字符知识。", "translation": "大型语言模型（LLMs）能够高精度地逐字符拼写出词元，但它们在更复杂的字符级任务上（例如识别词元内的组成子成分）却表现挣扎。在这项工作中，我们研究了LLMs在拼写输出过程中如何内部表示和利用字符级信息。我们的分析揭示，尽管拼写输出对人类而言是一项简单的任务，但LLMs并未以直截了当的方式处理它。具体来说，我们展示了词嵌入层并未完全编码字符级信息，尤其是在首字符之后。因此，LLMs依赖于中间和更高层的Transformer层来重建字符级知识，在此过程中我们观察到其拼写行为的明显“突破”。我们通过三种互补分析验证了这一机制：探测分类器、知识神经元识别和注意力权重检查。", "summary": "这项研究探究了大型语言模型（LLMs）如何处理字符级信息，尤其是在拼写输出任务中。研究发现，尽管LLMs能准确拼写，但其词嵌入层并未完全编码字符信息，导致模型需要依靠中间和更高层的Transformer层来重建和利用字符级知识。这一发现表明LLMs内部处理字符的方式比预想的更复杂，并非直截了当。", "keywords": "大型语言模型, 字符级信息, 词元化, Transformer, 词嵌入", "comments": "这项研究揭示了LLMs在看似简单的字符拼写任务中，其内部机制的复杂性。它挑战了词嵌入层会直接编码所有必要信息的直观假设，强调了Transformer深层结构在字符级信息重建中的关键作用。这对于理解LLMs的内部工作原理及其在字符级任务上的局限性具有重要意义。"}}
{"id": "2506.10685", "title": "Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework", "authors": ["Xia Du", "Xiaoyuan Liu", "Jizhe Zhou", "Zheng Lin", "Chi-man Pun", "Zhe Chen", "Wei Ni", "Jun Luo"], "summary": "With the rapid advancements in deep learning, traditional CAPTCHA schemes are\nincreasingly vulnerable to automated attacks powered by deep neural networks\n(DNNs). Existing adversarial attack methods often rely on original image\ncharacteristics, resulting in distortions that hinder human interpretation and\nlimit applicability in scenarios lacking initial input images. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel\nframework generating high-fidelity adversarial examples guided by\nattacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC\nenhances CAPTCHA diversity and supports both targeted and untargeted attacks.\nFor targeted attacks, the EDICT method optimizes dual latent variables in a\ndiffusion model for superior image quality. In untargeted attacks, especially\nfor black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-UAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show BP-UAC\nachieves high attack success rates across diverse systems, generating natural\nCAPTCHAs indistinguishable to humans and DNNs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10685v1", "AI": {"title_translation": "无源对抗性验证码：一种双阶段对抗性验证码框架", "tldr": "提出UAC，一种新的无源对抗性验证码框架，通过LLM生成高质量对抗样本，支持目标和非目标攻击，尤其BP-UAC在黑盒场景下表现出色，能生成人类和DNN都难以区分的自然验证码。", "motivation": "传统验证码方案易受深度学习攻击，现有对抗性攻击方法依赖原始图像特征，导致图像失真且不适用于缺乏初始输入图像的场景。", "method": "提出无源对抗性验证码（UAC）框架，利用大型语言模型（LLM）生成由攻击者指定文本提示引导的高保真对抗样本，增强验证码多样性。针对目标攻击，采用EDICT方法优化扩散模型中的双潜变量以提高图像质量。针对非目标攻击，特别是黑盒场景，引入双路径无源对抗性验证码（BP-UAC），采用多模态梯度和双路径优化进行两步优化策略。", "result": "实验表明，BP-UAC在不同系统上实现了高攻击成功率，并能生成人类和DNN都无法区分的自然验证码。", "conclusion": "UAC框架，特别是BP-UAC，成功解决了传统验证码在深度学习攻击下的脆弱性问题，并克服了现有对抗攻击方法的局限性，能够生成高质量、难以区分的对抗性验证码。", "translation": "随着深度学习的快速发展，传统验证码方案越来越容易受到深度神经网络（DNNs）驱动的自动化攻击。现有的对抗性攻击方法通常依赖于原始图像特征，导致扭曲，阻碍人类解释，并限制了在缺乏初始输入图像的场景中的适用性。为了应对这些挑战，我们提出了无源对抗性验证码（UAC），一种新颖的框架，通过攻击者指定的文本提示生成高保真对抗性样本。UAC利用大型语言模型（LLM），增强了验证码的多样性，并支持目标和非目标攻击。对于目标攻击，EDICT方法优化了扩散模型中的双潜在变量以获得卓越的图像质量。在非目标攻击中，特别是对于黑盒场景，我们引入了双路径无源对抗性验证码（BP-UAC），这是一种采用多模态梯度和双路径优化的两步优化策略，以实现高效的错误分类。实验表明，BP-UAC在不同系统上实现了高攻击成功率，生成了人类和DNN都无法区分的自然验证码。", "summary": "该论文提出了无源对抗性验证码（UAC）框架，旨在应对传统验证码在深度学习攻击下的脆弱性及现有对抗攻击方法的局限性。UAC利用大型语言模型，根据文本提示生成高保真对抗样本，支持目标和非目标攻击。尤其在黑盒场景下，双路径UAC（BP-UAC）通过两步优化策略实现了高效的攻击成功率，并生成了对人类和DNN都难以区分的自然验证码。", "keywords": "对抗性验证码, 深度学习, 无源攻击, 大型语言模型, 黑盒攻击", "comments": "UAC框架通过引入LLM和“无源”概念，解决了现有对抗性攻击方法对原始图像依赖的局限性，极大地提升了对抗样本的生成能力和适用性。BP-UAC在黑盒场景下的高效攻击能力和生成自然验证码的特性，显示了其在增强安全研究和防御策略方面的潜力。"}}
{"id": "2506.10430", "title": "MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment", "authors": ["Shuo wang", "Jihao Zhang"], "summary": "The rapid proliferation of online video content necessitates effective video\nsummarization techniques. Traditional methods, often relying on a single\nmodality (typically visual), struggle to capture the full semantic richness of\nvideos. This paper introduces MF2Summ, a novel video summarization model based\non multimodal content understanding, integrating both visual and auditory\ninformation. MF2Summ employs a five-stage process: feature extraction,\ncross-modal attention interaction, feature fusion, segment prediction, and key\nshot selection. Visual features are extracted using a pre-trained GoogLeNet\nmodel, while auditory features are derived using SoundNet. The core of our\nfusion mechanism involves a cross-modal Transformer and an alignment-guided\nself-attention Transformer, designed to effectively model inter-modal\ndependencies and temporal correspondences. Segment importance, location, and\ncenter-ness are predicted, followed by key shot selection using Non-Maximum\nSuppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm.\nExperimental results on the SumMe and TVSum datasets demonstrate that MF2Summ\nachieves competitive performance, notably improving F1-scores by 1.9\\% and\n0.6\\% respectively over the DSNet model, and performing favorably against other\nstate-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10430v1", "AI": {"title_translation": "MF2Summ：基于时间对齐的多模态融合视频摘要", "tldr": "MF2Summ是一个新的视频摘要模型，它通过融合视觉和听觉信息，并利用跨模态Transformer和对齐引导自注意力Transformer来处理模态间依赖和时间对应，在SumMe和TVSum数据集上表现优于现有方法。", "motivation": "现有的视频摘要方法通常依赖单一模态（主要是视觉），难以捕捉视频的完整语义丰富性。随着在线视频内容的快速增长，需要更有效的视频摘要技术。", "method": "本文提出了MF2Summ，一个基于多模态内容理解的视频摘要模型，整合了视觉和听觉信息。其流程包括五个阶段：特征提取（视觉特征使用GoogLeNet，听觉特征使用SoundNet）、跨模态注意力交互、特征融合（核心是跨模态Transformer和对齐引导自注意力Transformer）、片段预测（预测重要性、位置和中心性）以及关键镜头选择（使用NMS和KTS算法）。", "result": "在SumMe和TVSum数据集上的实验结果表明，MF2Summ取得了有竞争力的性能，F1分数分别比DSNet模型提高了1.9%和0.6%，并且优于其他最先进的方法。", "conclusion": "MF2Summ通过有效融合视觉和听觉信息，并利用时间对齐机制，显著提升了视频摘要的性能，证明了多模态方法在视频内容理解中的优越性。", "translation": "在线视频内容的迅速普及使得有效的视频摘要技术变得必要。传统方法通常依赖单一模态（通常是视觉），难以捕捉视频的完整语义丰富性。本文介绍了MF2Summ，一个基于多模态内容理解的新型视频摘要模型，它整合了视觉和听觉信息。MF2Summ采用五阶段流程：特征提取、跨模态注意力交互、特征融合、片段预测和关键镜头选择。视觉特征使用预训练的GoogLeNet模型提取，而听觉特征则使用SoundNet获取。我们融合机制的核心涉及一个跨模态Transformer和一个对齐引导自注意力Transformer，旨在有效建模模态间依赖和时间对应。随后预测片段的重要性、位置和中心性，然后使用非最大抑制（NMS）和核时间分割（KTS）算法进行关键镜头选择。在SumMe和TVSum数据集上的实验结果表明，MF2Summ取得了有竞争力的性能，F1分数分别比DSNet模型显著提高了1.9%和0.6%，并且优于其他最先进的方法。", "summary": "MF2Summ是一种新型的多模态视频摘要模型，通过结合视觉和听觉信息，解决了传统单模态方法无法捕捉视频完整语义的问题。它采用五阶段处理流程，核心在于利用跨模态Transformer和对齐引导自注意力Transformer进行特征融合和时间对齐。实验结果表明，MF2Summ在多个数据集上均优于现有先进方法，显著提升了视频摘要的F1分数。", "keywords": "视频摘要, 多模态融合, Transformer, 时间对齐, 深度学习", "comments": "MF2Summ的创新点在于其多模态融合策略，特别是引入了跨模态Transformer和对齐引导自注意力Transformer来处理视觉和听觉模态间的复杂依赖和时间对应。这解决了传统方法单一模态的局限性，提升了视频语义理解的全面性。其在F1分数上的提升也验证了该方法的有效性。"}}
{"id": "2506.10313", "title": "Collaborative Min-Max Regret in Grouped Multi-Armed Bandits", "authors": ["Moïse Blanchard", "Vineet Goyal"], "summary": "We study the impact of sharing exploration in multi-armed bandits in a\ngrouped setting where a set of groups have overlapping feasible action sets\n[Baek and Farias '24]. In this grouped bandit setting, groups share reward\nobservations, and the objective is to minimize the collaborative regret,\ndefined as the maximum regret across groups. This naturally captures\napplications in which one aims to balance the exploration burden between groups\nor populations -- it is known that standard algorithms can lead to\nsignificantly imbalanced exploration cost between groups. We address this\nproblem by introducing an algorithm Col-UCB that dynamically coordinates\nexploration across groups. We show that Col-UCB achieves both optimal minimax\nand instance-dependent collaborative regret up to logarithmic factors. These\nbounds are adaptive to the structure of shared action sets between groups,\nproviding insights into when collaboration yields significant benefits over\neach group learning their best action independently.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10313v1", "AI": {"title_translation": "分组多臂老虎机中的协作式最小-最大遗憾", "tldr": "本文研究了分组多臂老虎机中共享探索的影响，目标是最小化协作遗憾（即各组之间的最大遗憾）。作者提出了Col-UCB算法，该算法能够动态协调各组间的探索，并在对数因子内实现了最优的最小-最大和实例相关的协作遗憾。", "motivation": "在分组多臂老虎机设置中，标准算法可能导致各组之间探索成本显著不平衡。本文旨在解决这一问题，以平衡各组或人群之间的探索负担，并最小化协作遗憾。", "method": "本文提出了Col-UCB算法，该算法能够动态协调各组之间的探索。", "result": "Col-UCB算法在对数因子内实现了最优的最小-最大和实例相关的协作遗憾。这些界限适应于各组之间共享行动集的结构，揭示了何时协作比各组独立学习其最佳行动能带来显著优势。", "conclusion": "Col-UCB算法通过动态协调各组间的探索，在分组多臂老虎机中有效解决了探索成本不平衡的问题，并取得了最优的协作遗憾性能，证明了协作在特定结构下能带来显著优势。", "translation": "我们研究了在分组设置中，多臂老虎机中共享探索的影响，其中一组组具有重叠的可行行动集[Baek and Farias '24]。在这种分组老虎机设置中，各组共享奖励观测，目标是最小化协作遗憾，协作遗憾定义为各组之间的最大遗憾。这自然地捕捉了旨在平衡各组或人群之间探索负担的应用——已知标准算法可能导致各组之间探索成本显著不平衡。我们通过引入Col-UCB算法来解决这个问题，该算法动态协调各组之间的探索。我们表明，Col-UCB在对数因子内实现了最优的最小-最大和实例相关的协作遗憾。这些界限适应于各组之间共享行动集的结构，提供了关于何时协作比各组独立学习其最佳行动能带来显著优势的见解。", "summary": "本文研究了在分组多臂老虎机环境中共享探索的影响，其中各组具有重叠的行动集，目标是最小化各组间的最大遗憾（协作遗憾）。为解决标准算法导致的探索成本不平衡问题，作者提出了Col-UCB算法。该算法能动态协调各组间的探索，并实现了最优的最小-最大和实例相关的协作遗憾界限。研究结果还揭示了协作在共享行动集结构下的显著优势。", "keywords": "分组多臂老虎机, 协作遗憾, 探索, Col-UCB, 最小-最大遗憾", "comments": "本文的创新点在于提出了Col-UCB算法来解决分组多臂老虎机中探索负担不平衡的问题，并通过动态协调探索实现了最优的协作遗憾。其重要性在于为平衡多组或多人群的探索成本提供了理论和算法支持，并揭示了协作在特定条件下的显著效益，对于实际应用具有指导意义。"}}
{"id": "2506.10007", "title": "Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space", "authors": ["Kangwei Liu", "Junwu Liu", "Xiaowei Yi", "Jinlin Guo", "Yun Cao"], "summary": "Audio-driven emotional 3D facial animation encounters two significant\nchallenges: (1) reliance on single-modal control signals (videos, text, or\nemotion labels) without leveraging their complementary strengths for\ncomprehensive emotion manipulation, and (2) deterministic regression-based\nmapping that constrains the stochastic nature of emotional expressions and\nnon-verbal behaviors, limiting the expressiveness of synthesized animations. To\naddress these challenges, we present a diffusion-based framework for\ncontrollable expressive 3D facial animation. Our approach introduces two key\ninnovations: (1) a FLAME-centered multimodal emotion binding strategy that\naligns diverse modalities (text, audio, and emotion labels) through contrastive\nlearning, enabling flexible emotion control from multiple signal sources, and\n(2) an attention-based latent diffusion model with content-aware attention and\nemotion-guided layers, which enriches motion diversity while maintaining\ntemporal coherence and natural facial dynamics. Extensive experiments\ndemonstrate that our method outperforms existing approaches across most\nmetrics, achieving a 21.6\\% improvement in emotion similarity while preserving\nphysiologically plausible facial dynamics. Project Page:\nhttps://kangweiiliu.github.io/Control_3D_Animation.", "comment": "Accepted by ICME2025", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10007v1", "AI": {"title_translation": "可控的富有表现力的3D面部动画：通过统一多模态空间中的扩散模型实现", "tldr": "本文提出了一种基于扩散模型的方法，通过统一的多模态情感绑定策略和注意力引导的潜在扩散模型，实现可控且富有表现力的3D面部动画，解决了现有方法在多模态控制和表情多样性方面的局限性。", "motivation": "音频驱动的情感3D面部动画面临两大挑战：1) 依赖单一模态控制信号（视频、文本或情感标签），未能利用其互补优势进行全面的情感操控；2) 确定性基于回归的映射限制了情感表达和非语言行为的随机性，从而限制了合成动画的表现力。", "method": "本文提出了一个基于扩散模型的可控表现力3D面部动画框架。该方法引入了两项关键创新：1) 一种以FLAME为中心的跨模态情感绑定策略，通过对比学习对齐不同模态（文本、音频和情感标签），从而实现从多个信号源的灵活情感控制；2) 一个带有内容感知注意力和情感引导层的注意力基础潜在扩散模型，该模型在保持时间连贯性和自然面部动态的同时，丰富了运动多样性。", "result": "本文方法在大多数指标上优于现有方法，在情感相似性方面提高了21.6%，同时保持了生理上合理的面部动态。", "conclusion": "该方法通过统一的多模态空间和扩散模型，有效地解决了音频驱动3D面部动画在多模态控制和表达多样性方面的挑战，生成了高质量、可控且富有表现力的动画。", "translation": "音频驱动的情感3D面部动画面临两大挑战：(1) 依赖单一模态控制信号（视频、文本或情感标签），未能利用其互补优势进行全面的情感操控；(2) 确定性基于回归的映射限制了情感表达和非语言行为的随机性，从而限制了合成动画的表现力。为了解决这些挑战，我们提出了一种基于扩散模型的可控表现力3D面部动画框架。我们的方法引入了两项关键创新：(1) 一种以FLAME为中心的跨模态情感绑定策略，通过对比学习对齐不同模态（文本、音频和情感标签），从而实现从多个信号源的灵活情感控制；(2) 一个带有内容感知注意力和情感引导层的注意力基础潜在扩散模型，该模型在保持时间连贯性和自然面部动态的同时，丰富了运动多样性。广泛的实验表明，我们的方法在大多数指标上优于现有方法，在情感相似性方面提高了21.6%，同时保持了生理上合理的面部动态。项目页面：https://kangweiiliu.github.io/Control_3D_Animation。", "summary": "本文提出了一种基于扩散模型的可控表现力3D面部动画框架，旨在解决现有音频驱动动画在单一模态控制和确定性映射方面的局限性。该框架引入了两项核心创新：一是通过对比学习在FLAME模型中对齐文本、音频和情感标签等多种模态，实现灵活的多源情感控制；二是一个带有内容感知注意力和情感引导层的潜在扩散模型，以增强运动多样性并保持时间连贯性与自然面部动态。实验结果表明，该方法在情感相似性上显著优于现有方法，同时保持了生理真实性。", "keywords": "3D面部动画, 扩散模型, 多模态, 情感控制, FLAME", "comments": "这篇论文的创新点在于将扩散模型引入3D面部动画生成，并成功地构建了一个统一的多模态空间，通过对比学习实现了多模态情感信号的融合与控制。这解决了传统方法在单一模态控制和表达多样性上的局限性，使得生成的动画更具表现力和可控性。其提出的FLAME中心情感绑定策略和注意力引导的潜在扩散模型是其核心贡献，有望推动可控3D面部动画领域的发展。"}}
{"id": "2506.10687", "title": "Large Language Models for Detection of Life-Threatening Texts", "authors": ["Thanh Thi Nguyen", "Campbell Wilson", "Janis Dalins"], "summary": "Detecting life-threatening language is essential for safeguarding individuals\nin distress, promoting mental health and well-being, and preventing potential\nharm and loss of life. This paper presents an effective approach to identifying\nlife-threatening texts using large language models (LLMs) and compares them\nwith traditional methods such as bag of words, word embedding, topic modeling,\nand Bidirectional Encoder Representations from Transformers. We fine-tune three\nopen-source LLMs including Gemma, Mistral, and Llama-2 using their 7B parameter\nvariants on different datasets, which are constructed with class balance,\nimbalance, and extreme imbalance scenarios. Experimental results demonstrate a\nstrong performance of LLMs against traditional methods. More specifically,\nMistral and Llama-2 models are top performers in both balanced and imbalanced\ndata scenarios while Gemma is slightly behind. We employ the upsampling\ntechnique to deal with the imbalanced data scenarios and demonstrate that while\nthis method benefits traditional approaches, it does not have as much impact on\nLLMs. This study demonstrates a great potential of LLMs for real-world\nlife-threatening language detection problems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10687v1", "AI": {"title_translation": "大型语言模型在识别危及生命文本中的应用", "tldr": "本研究利用大型语言模型（LLMs）有效识别危及生命的文本，并证明其性能优于传统方法。", "motivation": "检测危及生命的语言对于保障处于困境中的个人、促进心理健康和福祉以及预防潜在的伤害和生命损失至关重要。", "method": "本研究使用Gemma、Mistral和Llama-2这三种开源大型语言模型（7B参数变体），在不同类平衡、不平衡和极端不平衡的数据集上进行微调，并与词袋、词嵌入、主题建模和BERT等传统方法进行比较。同时，对不平衡数据场景采用了上采样技术。", "result": "实验结果表明，大型语言模型（LLMs）的表现优于传统方法。具体而言，Mistral和Llama-2模型在平衡和不平衡数据场景中表现最佳，而Gemma略逊一筹。上采样技术对传统方法有益，但对LLMs的影响不大。", "conclusion": "本研究证明了大型语言模型在解决现实世界中危及生命的语言检测问题方面具有巨大潜力。", "translation": "检测危及生命的语言对于保障处于困境中的个人、促进心理健康和福祉以及预防潜在的伤害和生命损失至关重要。本文提出了一种使用大型语言模型（LLMs）识别危及生命文本的有效方法，并将其与词袋、词嵌入、主题建模和BERT等传统方法进行了比较。我们使用Gemma、Mistral和Llama-2这三种开源大型语言模型（7B参数变体）在不同数据集上进行微调，这些数据集构建时考虑了类平衡、不平衡和极端不平衡的情况。实验结果表明，大型语言模型（LLMs）的表现优于传统方法。更具体地说，Mistral和Llama-2模型在平衡和不平衡数据场景中表现最佳，而Gemma略逊一筹。我们采用上采样技术来处理不平衡数据场景，并证明虽然这种方法有利于传统方法，但对LLMs的影响不大。这项研究表明，大型语言模型在解决现实世界中危及生命的语言检测问题方面具有巨大潜力。", "summary": "本研究探讨了使用大型语言模型（LLMs）检测危及生命文本的有效性，并将其性能与传统的自然语言处理方法进行了对比。通过在不同数据平衡条件下对Gemma、Mistral和Llama-2进行微调，结果显示LLMs在性能上显著优于传统方法，特别是Mistral和Llama-2表现突出。研究还发现上采样技术对LLMs性能提升有限，最终证明了LLMs在实际危及生命语言检测中的巨大潜力。", "keywords": "大型语言模型, 危及生命文本检测, 文本分类, 自然语言处理, 心理健康", "comments": "该论文的创新之处在于系统地评估了不同大型语言模型在危及生命文本检测这一关键应用中的表现，并与多种传统方法进行了详细比较。其重要性体现在该技术在心理健康支持和危机干预方面的巨大应用前景。论文也指出上采样对LLMs影响不大，这为LLMs在不平衡数据场景下的应用提供了新的视角。"}}
{"id": "2506.10452", "title": "Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts", "authors": ["Guowei Zhong", "Ruohong Huan", "Mingzhen Wu", "Ronghua Liang", "Peng Chen"], "summary": "Recent advancements in Multimodal Emotion Recognition (MER) face challenges\nin addressing both modality missing and Out-Of-Distribution (OOD) data\nsimultaneously. Existing methods often rely on specific models or introduce\nexcessive parameters, which limits their practicality. To address these issues,\nwe propose a novel robust MER framework, Causal Inference Distiller (CIDer),\nand introduce a new task, Random Modality Feature Missing (RMFM), to generalize\nthe definition of modality missing. CIDer integrates two key components: a\nModel-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal\nInference (MACI) module. MSSD enhances robustness under the RMFM task through a\nweight-sharing self-distillation approach applied across low-level features,\nattention maps, and high-level representations. Additionally, a Word-level\nSelf-aligned Attention Module (WSAM) reduces computational complexity, while a\nMultimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.\nTo tackle OOD challenges, MACI employs a tailored causal graph to mitigate\nlabel and language biases using a Multimodal Causal Module (MCM) and\nfine-grained counterfactual texts. Notably, MACI can independently enhance OOD\ngeneralization with minimal additional parameters. Furthermore, we also\nintroduce the new repartitioned MER OOD datasets. Experimental results\ndemonstrate that CIDer achieves robust performance in both RMFM and OOD\nscenarios, with fewer parameters and faster training compared to\nstate-of-the-art methods. The implementation of this work is publicly\naccessible at https://github.com/gw-zhong/CIDer.", "comment": "Submitted to TAC. The code is available at\n  https://github.com/gw-zhong/CIDer", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10452v1", "AI": {"title_translation": "多模态情感识别在模态缺失和分布偏移下的鲁棒性研究", "tldr": "提出CIDer框架，通过自蒸馏和因果推理解决多模态情感识别中模态缺失和分布偏移问题，表现出更高的鲁棒性、更少参数和更快训练速度。", "motivation": "现有多模态情感识别（MER）方法难以同时处理模态缺失和分布外（OOD）数据，且通常依赖特定模型或引入过多参数，限制了实用性。", "method": "提出Causal Inference Distiller (CIDer) 框架，并引入随机模态特征缺失（RMFM）任务。CIDer包含两个核心组件：1) 模型特定自蒸馏（MSSD）模块，通过权重共享的自蒸馏增强RMFM任务下的鲁棒性，其中包含词级自对齐注意力模块（WSAM）和多模态复合Transformer（MCT）。2) 模型无关因果推理（MACI）模块，利用定制因果图和多模态因果模块（MCM）及细粒度反事实文本来缓解标签和语言偏差，以解决OOD挑战。此外，还引入了新的重新分区MER OOD数据集。", "result": "实验结果表明，CIDer在RMFM和OOD场景下均实现了鲁棒性能，且与现有最先进方法相比，参数更少，训练速度更快。", "conclusion": "CIDer框架有效解决了多模态情感识别中模态缺失和分布偏移的挑战，并在效率和性能上超越现有方法，证明了其在实际应用中的潜力。", "translation": "多模态情感识别（MER）的最新进展在同时解决模态缺失和分布外（OOD）数据方面面临挑战。现有方法通常依赖特定模型或引入过多参数，这限制了它们的实用性。为了解决这些问题，我们提出了一种新颖的鲁棒MER框架——因果推理蒸馏器（Causal Inference Distiller, CIDer），并引入了一项新任务——随机模态特征缺失（Random Modality Feature Missing, RMFM），以泛化模态缺失的定义。CIDer集成了两个关键组件：模型特定自蒸馏（Model-Specific Self-Distillation, MSSD）模块和模型无关因果推理（Model-Agnostic Causal Inference, MACI）模块。MSSD通过应用于低级特征、注意力图和高级表示的权重共享自蒸馏方法，增强了RMFM任务下的鲁棒性。此外，词级自对齐注意力模块（Word-level Self-aligned Attention Module, WSAM）降低了计算复杂性，而多模态复合Transformer（Multimodal Composite Transformer, MCT）促进了高效的多模态融合。为了解决OOD挑战，MACI采用定制的因果图，利用多模态因果模块（Multimodal Causal Module, MCM）和细粒度反事实文本来缓解标签和语言偏差。值得注意的是，MACI可以通过最少的额外参数独立增强OOD泛化能力。此外，我们还引入了新的重新分区MER OOD数据集。实验结果表明，与现有最先进方法相比，CIDer在RMFM和OOD场景中均实现了鲁棒性能，且参数更少，训练速度更快。这项工作的实现已在https://github.com/gw-zhong/CIDer 公开。", "summary": "本文提出一种名为Causal Inference Distiller (CIDer) 的新型鲁棒多模态情感识别（MER）框架，旨在同时解决模态缺失和分布外（OOD）数据问题。CIDer包含模型特定自蒸馏（MSSD）和模型无关因果推理（MACI）两大模块，分别应对随机模态特征缺失（RMFM）任务和OOD挑战。MSSD通过自蒸馏提高鲁棒性并利用WSAM和MCT优化效率；MACI则通过因果图和反事实文本缓解偏差。实验证明，CIDer在鲁棒性、参数量和训练速度方面均优于现有方法。", "keywords": "多模态情感识别, 模态缺失, 分布偏移, 因果推理, 自蒸馏", "comments": "该论文在多模态情感识别领域具有重要创新性，首次尝试同时解决模态缺失和分布偏移两大难题。其提出的CIDer框架，特别是结合自蒸馏和因果推理的思路，为跨模态鲁棒性学习提供了新的范式。引入RMFM任务和重新分区OOD数据集也展现了其对实际应用场景的关注。此外，其在参数效率和训练速度上的提升，进一步验证了该方法的实用价值。"}}
{"id": "2506.10314", "title": "Detecting Sockpuppetry on Wikipedia Using Meta-Learning", "authors": ["Luc Raszewski", "Christine De Kock"], "summary": "Malicious sockpuppet detection on Wikipedia is critical to preserving access\nto reliable information on the internet and preventing the spread of\ndisinformation. Prior machine learning approaches rely on stylistic and\nmeta-data features, but do not prioritise adaptability to author-specific\nbehaviours. As a result, they struggle to effectively model the behaviour of\nspecific sockpuppet-groups, especially when text data is limited. To address\nthis, we propose the application of meta-learning, a machine learning technique\ndesigned to improve performance in data-scarce settings by training models\nacross multiple tasks. Meta-learning optimises a model for rapid adaptation to\nthe writing style of a new sockpuppet-group. Our results show that\nmeta-learning significantly enhances the precision of predictions compared to\npre-trained models, marking an advancement in combating sockpuppetry on open\nediting platforms. We release a new dataset of sockpuppet investigations to\nfoster future research in both sockpuppetry and meta-learning fields.", "comment": "Accepted to ACL 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10314v1", "AI": {"title_translation": "使用元学习检测维基百科上的“傀儡”行为", "tldr": "本文提出使用元学习来检测维基百科上的恶意“傀儡”行为，以解决现有机器学习方法在数据稀缺和特定作者行为适应性方面的不足，结果显示元学习显著提高了预测精度。", "motivation": "维基百科上的恶意“傀儡”检测对于维护可靠信息和防止虚假信息传播至关重要。现有的机器学习方法依赖风格和元数据特征，但缺乏对特定作者行为的适应性，导致在文本数据有限时难以有效建模特定“傀儡”群体的行为。", "method": "本文提出应用元学习技术，通过在多个任务上训练模型来提高数据稀缺环境下的性能。元学习优化模型以快速适应新的“傀儡”群体的写作风格。", "result": "结果显示，与预训练模型相比，元学习显著提高了预测精度。研究团队还发布了一个新的“傀儡”调查数据集，以促进未来在“傀儡”行为和元学习领域的研究。", "conclusion": "元学习显著提升了维基百科上“傀儡”行为检测的精度，是打击开放编辑平台上“傀儡”行为的一项进步。", "translation": "维基百科上的恶意“傀儡”检测对于维护互联网上可靠信息的获取和防止虚假信息传播至关重要。先前的机器学习方法依赖于风格和元数据特征，但并未优先考虑对作者特定行为的适应性。因此，它们难以有效地建模特定“傀儡”群体的行为，尤其是在文本数据有限的情况下。为了解决这个问题，我们提出了元学习的应用，这是一种旨在通过跨多个任务训练模型来改善数据稀缺环境性能的机器学习技术。元学习优化模型，使其能够快速适应新的“傀儡”群体的写作风格。我们的结果表明，与预训练模型相比，元学习显著提高了预测精度，标志着在打击开放编辑平台上的“傀儡”行为方面取得了进展。我们发布了一个新的“傀儡”调查数据集，以促进未来在“傀儡”行为和元学习领域的研究。", "summary": "本研究旨在解决现有机器学习方法在维基百科“傀儡”检测中对特定作者行为适应性差的问题。通过引入元学习技术，该方法能够优化模型以快速适应新的“傀儡”群体的写作风格，尤其在数据稀缺环境下表现优异。实验结果表明，元学习显著提高了预测精度，为打击在线平台上的恶意“傀儡”行为提供了新的有效途径。此外，研究还发布了一个新的数据集以促进相关领域的研究。", "keywords": "元学习, 傀儡检测, 维基百科, 机器学习, 数据稀缺", "comments": "该论文创新性地将元学习应用于维基百科“傀儡”检测，解决了传统机器学习方法在数据稀缺和适应特定行为方面的局限性。其重要性在于提升了在线信息平台的可靠性，对抗虚假信息传播。发布新数据集的举措也对未来的研究具有积极推动作用。"}}
{"id": "2506.10008", "title": "Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics", "authors": ["Yi-Chun Chen"], "summary": "This paper presents a hierarchical knowledge graph framework for the\nstructured understanding of visual narratives, focusing on multimodal media\nsuch as comics. The proposed method decomposes narrative content into multiple\nlevels, from macro-level story arcs to fine-grained event segments. It\nrepresents them through integrated knowledge graphs that capture semantic,\nspatial, and temporal relationships. At the panel level, we construct\nmultimodal graphs that link visual elements such as characters, objects, and\nactions with corresponding textual components, including dialogue and captions.\nThese graphs are integrated across narrative levels to support reasoning over\nstory structure, character continuity, and event progression.\n  We apply our approach to a manually annotated subset of the Manga109 dataset\nand demonstrate its ability to support symbolic reasoning across diverse\nnarrative tasks, including action retrieval, dialogue tracing, character\nappearance mapping, and panel timeline reconstruction. Evaluation results show\nhigh precision and recall across tasks, validating the coherence and\ninterpretability of the framework. This work contributes a scalable foundation\nfor narrative-based content analysis, interactive storytelling, and multimodal\nreasoning in visual media.", "comment": "This paper has been submitted to ACM Multimedia 2025 and is currently\n  under review", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10008v1", "AI": {"title_translation": "结构化图表示用于视觉叙事推理：一个漫画的层次化框架", "tldr": "本文提出了一种用于视觉叙事理解的层次化知识图谱框架，特别关注漫画等多模态媒体，通过构建跨层级集成的知识图谱来支持叙事推理，并在Manga109数据集上验证了其在多种叙事任务中的有效性。", "motivation": "本研究旨在为视觉叙事（特别是漫画等多模态媒体）提供一个结构化的理解框架，以解决现有方法在处理复杂叙事内容时的局限性。", "method": "本文提出了一种层次化知识图谱框架。该方法将叙事内容分解为多层级，从宏观的故事弧到细粒度的事件片段，并通过捕获语义、空间和时间关系的集成知识图谱进行表示。在面板层面，构建多模态图谱，将视觉元素（如角色、物体、动作）与文本组件（如对话、标题）连接起来。这些图谱在叙事层级间进行整合，以支持对故事结构、角色连续性和事件进展的推理。该方法应用于Manga109数据集的手动标注子集。", "result": "将所提出的方法应用于Manga109数据集的一个手动标注子集，并展示了其在支持跨多种叙事任务（包括动作检索、对话追踪、角色出现映射和面板时间线重建）的符号推理能力。评估结果显示，在各项任务中均实现了高精度和高召回率，验证了该框架的连贯性和可解释性。", "conclusion": "这项工作为视觉媒体中的叙事内容分析、交互式故事讲述和多模态推理提供了一个可扩展的基础。", "translation": "本文提出了一种用于结构化理解视觉叙事的层次化知识图谱框架，重点关注漫画等多模态媒体。所提出的方法将叙事内容分解为多个层次，从宏观的故事弧到细粒度的事件片段。它通过捕获语义、空间和时间关系的集成知识图谱来表示这些内容。在面板层面，我们构建多模态图谱，将视觉元素（如角色、物体和动作）与相应的文本组件（包括对话和标题）连接起来。这些图谱在叙事层级间进行整合，以支持对故事结构、角色连续性和事件进展的推理。我们将我们的方法应用于Manga109数据集的一个手动标注子集，并展示了其支持跨各种叙事任务（包括动作检索、对话追踪、角色出现映射和面板时间线重建）的符号推理能力。评估结果显示，在各项任务中均实现了高精度和高召回率，验证了该框架的连贯性和可解释性。这项工作为视觉媒体中的叙事内容分析、交互式故事讲述和多模态推理贡献了一个可扩展的基础。", "summary": "本文提出了一种用于视觉叙事理解的层次化知识图谱框架，特别针对漫画等多模态媒体。该框架将叙事内容分解为多层次，并通过集成的知识图谱表示语义、空间和时间关系。它在面板层面构建多模态图谱，并跨叙事层级整合以支持故事结构和事件进展的推理。在Manga109数据集上的应用表明，该框架在多种叙事任务中实现了高精度和高召回率，为视觉媒体的叙事分析和推理奠定了可扩展的基础。", "keywords": "视觉叙事, 知识图谱, 漫画, 层次化框架, 多模态推理", "comments": "该论文的创新点在于提出了一个层次化的知识图谱框架，能够对视觉叙事进行结构化理解，并有效整合视觉和文本信息。其重要性体现在为漫画等复杂多模态叙事内容的分析、交互式故事讲述和多模态推理提供了强大的基础。该框架的模块化和可扩展性是其主要优势。"}}
{"id": "2506.10715", "title": "Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet", "authors": ["Lorenzo Augello", "John P. McCrae"], "summary": "Open English Wordnet is a key resource published in OntoLex-lemon as part of\nthe linguistic linked open data cloud. There are, however, many links missing\nin the resource, and in this paper, we look at how we can establish hypernymy\nbetween adjectives. We present a theoretical discussion of the hypernymy\nrelation and how it differs for adjectives in contrast to nouns and verbs. We\ndevelop a new resource for adjective hypernymy and fine-tune large language\nmodels to predict adjective hypernymy, showing that the methodology of\nTaxoLLaMa can be adapted to this task.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10715v1", "AI": {"title_translation": "使用语言模型推断形容词上位词以增加开放英语词网的连通性", "tldr": "本论文探讨了如何使用语言模型推断形容词上位词，以增加开放英语词网的连通性，并展示了TaxoLLaMa方法论在此任务上的适用性。", "motivation": "开放英语词网（Open English Wordnet）中存在许多缺失的链接，尤其是在形容词上位词关系方面，这促使研究如何建立形容词之间的上位词关系以增加资源连通性。", "method": "本文首先对上位词关系进行了理论探讨，并阐述了形容词上位词与名词和动词上位词的区别。然后，开发了一个新的形容词上位词资源，并微调大型语言模型来预测形容词上位词，该方法是TaxoLLaMa方法论的改编。", "result": "研究结果表明，TaxoLLaMa方法论可以成功地应用于形容词上位词推断任务。", "conclusion": "本研究的结论是，通过开发新资源并微调大型语言模型，TaxoLLaMa方法论能够有效应用于预测形容词上位词，从而提高开放英语词网的连通性。", "translation": "开放英语词网（Open English Wordnet）是作为语言链接开放数据云的一部分，以OntoLex-lemon格式发布的重要资源。然而，该资源中缺失了许多链接，在本文中，我们研究了如何建立形容词之间的上位词关系。我们对上位词关系及其与名词和动词在形容词中的不同之处进行了理论探讨。我们开发了一个新的形容词上位词资源，并微调大型语言模型来预测形容词上位词，结果表明TaxoLLaMa的方法论可以适应这项任务。", "summary": "本研究旨在解决开放英语词网中形容词上位词链接缺失的问题。论文首先从理论上讨论了形容词上位词关系与名词、动词的区别，随后开发了一个专门的形容词上位词资源。研究通过微调大型语言模型来预测形容词上位词，并成功验证了TaxoLLaMa方法论在此任务上的有效性，从而提升了词网的连通性。", "keywords": "形容词上位词, 语言模型, 开放英语词网, TaxoLLaMa, 语义关系", "comments": "本文的创新之处在于将大型语言模型应用于形容词上位词推断这一特定且复杂的语义关系，并成功地将TaxoLLaMa方法论从其原始应用领域（可能与分类法构建相关）改编到形容词的上位词推断上。这对于完善语言资源、提升语义网络的连通性和精确性具有重要意义。该研究为未来利用LLMs处理更细粒度的词汇语义关系提供了新的思路和实证支持。"}}
{"id": "2506.10315", "title": "PyLO: Towards Accessible Learned Optimizers in PyTorch", "authors": ["Paul Janson", "Benjamin Therien", "Quentin Anthony", "Xiaolong Huang", "Abhinav Moudgil", "Eugene Belilovsky"], "summary": "Learned optimizers have been an active research topic over the past decade,\nwith increasing progress toward practical, general-purpose optimizers that can\nserve as drop-in replacements for widely used methods like Adam. However,\nrecent advances -- such as VeLO, which was meta-trained for 4000 TPU-months --\nremain largely inaccessible to the broader community, in part due to their\nreliance on JAX and the absence of user-friendly packages for applying the\noptimizers after meta-training. To address this gap, we introduce PyLO, a\nPyTorch-based library that brings learned optimizers to the broader machine\nlearning community through familiar, widely adopted workflows. Unlike prior\nwork focused on synthetic or convex tasks, our emphasis is on applying learned\noptimization to real-world large-scale pre-training tasks. Our release includes\na CUDA-accelerated version of the small_fc_lopt learned optimizer architecture\nfrom (Metz et al., 2022a), delivering substantial speedups -- from 39.36 to\n205.59 samples/sec throughput for training ViT B/16 with batch size 32. PyLO\nalso allows us to easily combine learned optimizers with existing optimization\ntools such as learning rate schedules and weight decay. When doing so, we find\nthat learned optimizers can substantially benefit. Our code is available at\nhttps://github.com/Belilovsky-Lab/pylo", "comment": "Accepted at ICML CODEML Workshop 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10315v1", "AI": {"title_translation": "PyLO：在PyTorch中实现可访问的学习型优化器", "tldr": "PyLO是一个PyTorch库，旨在通过提供用户友好的工作流程和CUDA加速，使学习型优化器（如VeLO）更易于访问和应用于大规模真实世界任务，并展示了显著的训练加速。", "motivation": "现有的先进学习型优化器（如VeLO）因依赖JAX且缺乏用户友好的应用包，对更广泛的机器学习社区而言难以访问。", "method": "引入PyLO，一个基于PyTorch的库，通过熟悉的、广泛采用的工作流程将学习型优化器带给更广泛的机器学习社区。PyLO专注于将学习型优化应用于真实世界的大规模预训练任务，并包含了CUDA加速的`small_fc_lopt`架构。它还支持与现有优化工具（如学习率调度和权重衰减）结合使用。", "result": "PyLO提供了显著的速度提升，例如在训练ViT B/16时，吞吐量从39.36样本/秒提高到205.59样本/秒。此外，结合学习型优化器与现有优化工具（如学习率调度和权重衰减）能使其性能显著受益。", "conclusion": "PyLO通过在PyTorch中提供一个用户友好、高效且可扩展的平台，显著提高了学习型优化器的可访问性和实用性，使其能够应用于大规模真实世界的预训练任务，并通过与现有优化工具结合进一步提升性能。", "translation": "在过去十年中，学习型优化器一直是一个活跃的研究课题，在实用、通用优化器方面取得了越来越多的进展，这些优化器可以作为Adam等广泛使用方法的替代品。然而，最近的进展——例如VeLO，它经过4000 TPU月的元训练——在很大程度上仍无法被更广泛的社区所使用，部分原因是它们依赖JAX以及缺乏用于元训练后应用优化器的用户友好型软件包。为了弥补这一差距，我们引入了PyLO，一个基于PyTorch的库，通过熟悉且广泛采用的工作流程将学习型优化器带给更广泛的机器学习社区。与之前专注于合成或凸任务的工作不同，我们的重点是将学习型优化应用于真实世界的大规模预训练任务。我们发布的版本包括(Metz et al., 2022a)中的small_fc_lopt学习型优化器架构的CUDA加速版本，为训练批量大小为32的ViT B/16提供了显著的速度提升——吞吐量从39.36样本/秒提高到205.59样本/秒。PyLO还允许我们轻松地将学习型优化器与现有优化工具（如学习率调度和权重衰减）结合使用。在这样做时，我们发现学习型优化器可以从中受益。我们的代码可在https://github.com/Belilovsky-Lab/pylo获取。", "summary": "PyLO是一个新的PyTorch库，旨在解决先进学习型优化器（如VeLO）因依赖JAX和缺乏用户友好工具而导致的可访问性问题。该库专注于将学习型优化应用于真实世界的大规模预训练任务，并包含一个CUDA加速版本的`small_fc_lopt`优化器，显著提高了训练吞吐量。PyLO还支持与现有优化工具（如学习率调度和权重衰减）的结合，并表明这种结合能带来显著的性能提升，从而使学习型优化器更易于广泛应用。", "keywords": "学习型优化器, PyTorch, 可访问性, 大规模预训练, CUDA加速", "comments": "PyLO的创新之处在于它将先进的学习型优化器引入了更广泛的PyTorch生态系统，解决了现有技术在JAX依赖性和可用性方面的限制。其对真实世界大规模预训练任务的关注以及提供的CUDA加速，对于推动学习型优化器在实际应用中的普及至关重要。此外，与现有优化工具的无缝集成也增加了其实用性和灵活性。"}}
{"id": "2506.10011", "title": "WDMIR: Wavelet-Driven Multimodal Intent Recognition", "authors": ["Weiyin Gong", "Kai Zhang", "Yanghai Zhang", "Qi Liu", "Xinjie Sun", "Junyu Lu", "Linbo Zhu"], "summary": "Multimodal intent recognition (MIR) seeks to accurately interpret user\nintentions by integrating verbal and non-verbal information across video, audio\nand text modalities. While existing approaches prioritize text analysis, they\noften overlook the rich semantic content embedded in non-verbal cues. This\npaper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)\nframework that enhances intent understanding through frequency-domain analysis\nof non-verbal information. To be more specific, we propose: (1) a\nwavelet-driven fusion module that performs synchronized decomposition and\nintegration of video-audio features in the frequency domain, enabling\nfine-grained analysis of temporal dynamics; (2) a cross-modal interaction\nmechanism that facilitates progressive feature enhancement from bimodal to\ntrimodal integration, effectively bridging the semantic gap between verbal and\nnon-verbal information. Extensive experiments on MIntRec demonstrate that our\napproach achieves state-of-the-art performance, surpassing previous methods by\n1.13% on accuracy. Ablation studies further verify that the wavelet-driven\nfusion module significantly improves the extraction of semantic information\nfrom non-verbal sources, with a 0.41% increase in recognition accuracy when\nanalyzing subtle emotional cues.", "comment": "Accepted at IJCAI 2025, 9pages, 6figures", "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10011v1", "AI": {"title_translation": "WDMIR：小波驱动的多模态意图识别", "tldr": "WDMIR是一种新颖的小波驱动多模态意图识别框架，通过在频域分析非语言信息并引入跨模态交互机制，在MIntRec数据集上实现了最先进的性能。", "motivation": "现有的多模态意图识别方法过度侧重文本分析，而忽视了非语言线索中丰富的语义内容，导致对用户意图的理解不够全面和准确。", "method": "本文提出了一种新颖的小波驱动多模态意图识别（WDMIR）框架。具体包括：1) 一个小波驱动融合模块，用于在频域对视频-音频特征进行同步分解和整合，实现时间动态的细粒度分析；2) 一个跨模态交互机制，促进从双模态到三模态的渐进式特征增强，有效弥合语言和非语言信息之间的语义鸿沟。", "result": "在MIntRec数据集上的广泛实验表明，WDMIR方法达到了最先进的性能，准确率比以往方法提高了1.13%。消融研究进一步证实，小波驱动融合模块显著改善了从非语言源中提取语义信息的能力，在分析细微情感线索时识别准确率提高了0.41%。", "conclusion": "该研究成功地开发并验证了WDMIR框架，通过小波驱动的频域分析和跨模态交互机制，有效提升了多模态意图识别的准确性和对非语言信息的利用效率，达到了当前最佳性能。", "translation": "多模态意图识别（MIR）旨在通过整合视频、音频和文本模态的语言和非语言信息来准确解释用户意图。尽管现有方法优先考虑文本分析，但它们常常忽视非语言线索中嵌入的丰富语义内容。本文提出了一种新颖的小波驱动多模态意图识别（WDMIR）框架，通过对非语言信息进行频域分析来增强意图理解。具体来说，我们提出：(1) 一个小波驱动的融合模块，在频域执行视频-音频特征的同步分解和整合，实现时间动态的细粒度分析；(2) 一个跨模态交互机制，促进从双模态到三模态集成的渐进式特征增强，有效弥合语言和非语言信息之间的语义鸿沟。在MIntRec上的广泛实验表明，我们的方法达到了最先进的性能，准确率比以前的方法提高了1.13%。消融研究进一步证实，小波驱动的融合模块显著改善了从非语言源中提取语义信息的能力，在分析细微情感线索时识别准确率提高了0.41%。", "summary": "本文提出了WDMIR（小波驱动多模态意图识别）框架，旨在解决现有MIR方法忽视非语言信息语义内容的问题。WDMIR通过引入小波驱动融合模块，在频域对视频-音频特征进行同步分解和整合，以及通过跨模态交互机制促进特征增强，有效利用非语言信息并弥合语言与非语言信息之间的语义鸿沟。实验证明，WDMIR在MIntRec数据集上实现了最先进的性能，准确率提高了1.13%，尤其在非语言语义信息提取方面表现突出。", "keywords": "多模态意图识别, 小波变换, 频域分析, 非语言信息, 特征融合", "comments": "WDMIR的创新点在于将小波变换引入多模态意图识别，通过频域分析深入挖掘非语言模态（视频、音频）的细粒度信息，这为多模态融合提供了一个新颖且有效视角。其提出的两个核心模块——小波驱动融合模块和跨模态交互机制，有效解决了非语言信息利用不足和模态间语义鸿沟的问题。该方法在准确率上的显著提升，特别是对细微情感线索的识别能力增强，表明其在实际应用中具有重要潜力。"}}
{"id": "2506.10716", "title": "PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models", "authors": ["Ye Yu", "Yaoning Yu", "Haohan Wang"], "summary": "Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve\nstrong performance on mathematical benchmarks using lengthy chain-of-thought\n(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This\ninflates token usage and cost, limiting deployment in latency-sensitive or\nAPI-constrained settings. We introduce PREMISE (PRompt-based Efficient\nMathematical Inference with Strategic Evaluation), a prompt-only framework that\nreduces reasoning overhead without modifying model weights. PREMISE combines\ntrace-level diagnostics with gradient-inspired prompt optimization to minimize\nredundant computation while preserving answer accuracy. The approach jointly\noptimizes brevity and correctness through a multi-objective textual search that\nbalances token length and answer validity. Unlike prior work, PREMISE runs in a\nsingle-pass black-box interface, so it can be applied directly to commercial\nLLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy\n($96\\%\\rightarrow96\\%$ with Claude, $91\\%\\rightarrow92\\%$ with Gemini) while\nreducing reasoning tokens by up to $87.5\\%$ and cutting dollar cost by\n$69$--$82\\%$. These results show that prompt-level optimization is a practical\nand scalable path to efficient LRM inference without compromising reasoning\nquality.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10716v1", "AI": {"title_translation": "PREMISE：大型模型中高效数学推理的可扩展战略提示优化", "tldr": "PREMISE是一个仅基于提示的框架，通过结合诊断和梯度启发式优化，显著减少大型模型在数学推理中的冗余计算和成本，同时保持或提高准确性，解决了冗长CoT推理带来的问题。", "motivation": "大型推理模型（LRMs）在数学基准测试中使用冗长的思维链（CoT）推理，导致不必要的冗余、高昂的token使用和成本，限制了其在延迟敏感或API受限环境中的部署。", "method": "本文提出了PREMISE（基于提示的高效数学推理与战略评估），一个无需修改模型权重，仅通过提示进行优化的框架。PREMISE结合了轨迹级诊断和梯度启发式提示优化，通过多目标文本搜索共同优化简洁性和正确性，以最小化冗余计算同时保持答案准确性。该方法以单次黑盒接口运行，可直接应用于商用LLM。", "result": "在GSM8K、SVAMP和Math500数据集上，PREMISE匹配或超过了基线准确率（Claude从96%到96%，Gemini从91%到92%），同时将推理token减少了高达87.5%，并将成本降低了69%至82%。", "conclusion": "提示级优化是实现高效LMR推理的实用且可扩展的途径，且不损害推理质量。", "translation": "大型推理模型（LRMs），如Claude 3.7 Sonnet和OpenAI o1，通过冗长的思维链（CoT）推理在数学基准测试中取得了强大的性能，但由此产生的轨迹通常不必要地冗长。这会增加token使用和成本，限制了其在延迟敏感或API受限环境中的部署。我们引入了PREMISE（基于提示的高效数学推理与战略评估），一个仅基于提示的框架，无需修改模型权重即可减少推理开销。PREMISE将轨迹级诊断与梯度启发式提示优化相结合，以最小化冗余计算，同时保持答案准确性。该方法通过多目标文本搜索共同优化简洁性和正确性，平衡token长度和答案有效性。与现有工作不同，PREMISE以单次黑盒接口运行，因此可以直接应用于商用LLM。在GSM8K、SVAMP和Math500上，我们匹配或超过了基线准确率（使用Claude时从96%到96%，使用Gemini时从91%到92%），同时将推理token减少了高达87.5%，并将美元成本削减了69%至82%。这些结果表明，提示级优化是实现高效LMR推理的实用且可扩展的途径，且不损害推理质量。", "summary": "本文介绍了PREMISE，一个用于大型模型数学推理的创新性提示优化框架。该框架通过结合轨迹级诊断和梯度启发式优化，显著减少了冗余的思维链推理步骤和token消耗，从而降低了成本，同时保持或提高了推理准确性。PREMISE作为一种黑盒、单次通过的方法，可以直接应用于商用LLM，并在多个数学基准测试中展示了显著的效率提升和成本节约。", "keywords": "提示优化, 数学推理, 大型模型, 效率, 思维链", "comments": "PREMISE的创新之处在于其“仅提示”的方法和“黑盒”兼容性，使其能够直接应用于商业大型语言模型，而无需模型微调或架构修改。通过在不牺牲准确性的前提下大幅削减token使用和成本，该工作为LRM在实际应用中的部署提供了重要的实用性和可扩展性。其多目标优化方法，平衡了简洁性和正确性，是解决当前CoT推理冗余问题的有效策略。"}}
{"id": "2506.10332", "title": "Air in Your Neighborhood: Fine-Grained AQI Forecasting Using Mobile Sensor Data", "authors": ["Aaryam Sharma"], "summary": "Air pollution has become a significant health risk in developing countries.\nWhile governments routinely publish air-quality index (AQI) data to track\npollution, these values fail to capture the local reality, as sensors are often\nvery sparse. In this paper, we address this gap by predicting AQI in 1 km^2\nneighborhoods, using the example of AirDelhi dataset. Using Spatio-temporal\nGNNs we surpass existing works by 71.654 MSE a 79% reduction, even on unseen\ncoordinates. New insights about AQI such as the existence of strong repetitive\nshort-term patterns and changing spatial relations are also discovered. The\ncode is available on GitHub.", "comment": "10 pages, 7 figures. Code available at\n  https://github.com/ASChampOmega/AQI_Forecasting.git", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10332v1", "AI": {"title_translation": "您社区的空气：使用移动传感器数据进行细粒度AQI预测", "tldr": "通过时空GNNs利用移动传感器数据，在1平方公里尺度上对AQI进行精细预测，显著优于现有方法。", "motivation": "政府发布的AQI数据因传感器稀疏而无法反映当地真实情况，空气污染在发展中国家构成重大健康风险。", "method": "使用时空图神经网络（Spatio-temporal GNNs）预测1平方公里社区的AQI，并以AirDelhi数据集为例。", "result": "性能超越现有工作，MSE降低71.654，相对减少79%，即使在未见过的坐标上也能表现良好。发现了AQI的强重复短期模式和变化的 spatial relations 等新见解。", "conclusion": "该方法有效解决了现有AQI数据粒度不足的问题，提供了更准确的本地化预测，并揭示了AQI的新特性。", "translation": "空气污染已成为发展中国家面临的重大健康风险。虽然政府定期发布空气质量指数（AQI）数据来追踪污染情况，但由于传感器通常非常稀疏，这些数值未能捕捉到当地的真实情况。本文旨在解决这一问题，通过以AirDelhi数据集为例，预测1平方公里社区的AQI。我们使用时空图神经网络（Spatio-temporal GNNs）超越了现有工作，将均方误差（MSE）降低了71.654，实现了79%的显著减少，即使在未曾出现的坐标上也能保持良好表现。此外，还发现了关于AQI的新见解，例如存在强烈的重复性短期模式和不断变化的空间关系。代码已在GitHub上提供。", "summary": "本文旨在解决政府发布AQI数据因传感器稀疏而无法反映当地真实情况的问题。研究提出使用时空图神经网络（Spatio-temporal GNNs）结合移动传感器数据，对1平方公里社区的AQI进行精细预测。该方法在AirDelhi数据集上表现出色，将均方误差（MSE）降低了71.654，实现了79%的显著改进，即使在未见过的坐标上也能保持有效。此外，研究还发现了AQI的强重复短期模式和动态空间关系等新见解。", "keywords": "AQI预测, 移动传感器, 时空图神经网络, 空气污染, 细粒度预测", "comments": "本文的创新之处在于利用移动传感器数据和时空图神经网络实现细粒度（1平方公里）的AQI预测，显著提升了预测精度，解决了传统传感器稀疏导致的数据不足问题。其贡献在于提供了更贴近居民日常生活的空气质量信息，对于公共健康和环境治理具有重要意义。"}}
{"id": "2506.10016", "title": "Multimodal Large Language Models: A Survey", "authors": ["Longzhen Han", "Awes Mubarak", "Almas Baimagambetov", "Nikolaos Polatidis", "Thar Baker"], "summary": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text\ngeneration, now spanning diverse output modalities including images, music,\nvideo, human motion, and 3D objects, by integrating language with other sensory\nmodalities under unified architectures. This survey categorises six primary\ngenerative modalities and examines how foundational techniques, namely\nSelf-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement\nLearning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,\nenable cross-modal capabilities. We analyze key models, architectural trends,\nand emergent cross-modal synergies, while highlighting transferable techniques\nand unresolved challenges. Architectural innovations like transformers and\ndiffusion models underpin this convergence, enabling cross-modal transfer and\nmodular specialization. We highlight emerging patterns of synergy, and identify\nopen challenges in evaluation, modularity, and structured reasoning. This\nsurvey offers a unified perspective on MLLM development and identifies critical\npaths toward more general-purpose, adaptive, and interpretable multimodal\nsystems.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.10016v1", "AI": {"title_translation": "多模态大语言模型：一项综述", "tldr": "该综述对多模态大语言模型（MLLMs）进行了全面分析，涵盖了其发展、输出模态、核心技术、架构趋势以及面临的挑战。", "motivation": "多模态大语言模型（MLLMs）已迅速发展，超越了文本生成，通过将语言与其他感知模态整合到统一架构中，现在涵盖了图像、音乐、视频、人体运动和3D对象等多种输出模态。这项综述旨在提供一个统一的视角，并识别通向更通用、适应性更强和可解释的多模态系统的关键路径。", "method": "本综述将六种主要的生成模态进行分类，并考察了基础技术（如自监督学习（SSL）、专家混合（MoE）、基于人类反馈的强化学习（RLHF）和思维链（CoT）提示）如何实现跨模态能力。文章分析了关键模型、架构趋势和新兴的跨模态协同作用，同时强调了可迁移的技术和未解决的挑战。", "result": "综述分析了关键模型、架构趋势和新兴的跨模态协同作用，并强调了可迁移的技术和未解决的挑战。变压器和扩散模型等架构创新支撑了这种融合，实现了跨模态迁移和模块化专业化。文章突出了新兴的协同模式，并指出了评估、模块化和结构化推理方面的开放挑战。", "conclusion": "该综述为多模态大语言模型（MLLM）的发展提供了统一的视角，并确定了通向更通用、适应性更强和可解释的多模态系统的关键路径。", "translation": "多模态大语言模型（MLLMs）已迅速发展，超越了文本生成，通过将语言与其他感知模态整合到统一架构中，现在涵盖了图像、音乐、视频、人体运动和3D对象等多种输出模态。本综述将六种主要的生成模态进行分类，并考察了基础技术，即自监督学习（SSL）、专家混合（MoE）、基于人类反馈的强化学习（RLHF）和思维链（CoT）提示，如何实现跨模态能力。我们分析了关键模型、架构趋势和新兴的跨模态协同作用，同时强调了可迁移的技术和未解决的挑战。变压器和扩散模型等架构创新支撑了这种融合，实现了跨模态迁移和模块化专业化。我们强调了新兴的协同模式，并指出了评估、模块化和结构化推理方面的开放挑战。本综述为多模态大语言模型（MLLM）的发展提供了统一的视角，并确定了通向更通用、适应性更强和可解释的多模态系统的关键路径。", "summary": "本综述全面审视了多模态大语言模型（MLLMs）的快速发展，涵盖了其多样化的输出模态（如图像、音乐、视频、3D对象），以及如何通过统一架构整合语言和其他感知模态。文章分类了六种主要生成模态，并探讨了自监督学习、专家混合、基于人类反馈的强化学习和思维链提示等基础技术如何赋能跨模态能力。此外，综述分析了关键模型、架构趋势和新兴的跨模态协同效应，并指出了可迁移技术及评估、模块化和结构化推理等开放挑战。本研究旨在为MLLM发展提供统一视角，并为构建更通用、适应性强且可解释的多模态系统指明关键路径。", "keywords": "多模态大语言模型, 综述, 跨模态, 基础技术, 开放挑战", "comments": "这篇综述论文的重要性在于它对快速发展的多模态大语言模型领域进行了全面的梳理和总结。它不仅系统地分类了不同的生成模态，还深入探讨了支撑这些模型的核心技术和架构创新。通过识别现有挑战和未来发展路径，该论文为研究人员提供了一个宝贵的参考框架，有助于推动多模态AI系统向更通用、更智能的方向发展。其创新之处在于提供了统一的视角和对跨模态协同作用的深入分析。"}}
{"id": "2506.10341", "title": "Provably Learning from Language Feedback", "authors": ["Wanqiao Xu", "Allen Nie", "Ruijie Zheng", "Aditya Modi", "Adith Swaminathan", "Ching-An Cheng"], "summary": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\n$\\textit{transfer eluder dimension}$ as a complexity measure to characterize\nthe hardness of LLF problems. We show that transfer eluder dimension captures\nthe intuition that information in the feedback changes the learning complexity\nof the LLF problem. We demonstrate cases where learning from rich language\nfeedback can be exponentially faster than learning from reward. We develop a\nno-regret algorithm, called $\\texttt{HELiX}$, that provably solves LLF problems\nthrough sequential interactions, with performance guarantees that scale with\nthe transfer eluder dimension of the problem. Across several empirical domains,\nwe show that $\\texttt{HELiX}$ performs well even when repeatedly prompting LLMs\ndoes not work reliably. Our contributions mark a first step towards designing\nprincipled interactive learning algorithms from generic language feedback.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10341v1", "AI": {"title_translation": "可证明地从语言反馈中学习", "tldr": "本文形式化了从语言反馈中学习（LLF）的问题，引入了“迁移规避维度”作为复杂性度量，并开发了无悔算法HELiX，证明其能有效解决LLF问题，甚至在LLM提示不可靠时也能表现良好。", "motivation": "随着大型语言模型（LLM）智能体的兴起，从观察和语言反馈中进行交互式学习是一个日益受到关注的领域。尽管已经展示了令人印象深刻的经验性演示，但目前仍然缺乏对这些决策问题的原则性框架。", "method": "本文形式化了从语言反馈中学习（LLF）的问题，提出了在潜在奖励下实现学习的充分假设，并引入了“迁移规避维度”作为衡量LLF问题难度的复杂性度量。此外，开发了一种名为$\texttt{HELiX}$的无悔算法，通过序列交互可证明地解决LLF问题，其性能保证与问题的迁移规避维度相关。", "result": "研究表明，迁移规避维度捕获了反馈中的信息如何改变LLF问题学习复杂度的直觉。论文展示了在某些情况下，从丰富的语言反馈中学习可以比从奖励中学习快指数倍。在多个经验领域，$\texttt{HELiX}$算法表现良好，即使在重复提示LLM不可靠时也能有效工作。", "conclusion": "本文的贡献标志着向设计基于通用语言反馈的原则性交互式学习算法迈出了第一步。", "translation": "交互式地从观察和语言反馈中学习是一个日益受到关注的领域，这得益于大型语言模型（LLM）智能体的兴起。尽管已经展示了令人印象深刻的经验性演示，但目前仍然缺乏对这些决策问题的原则性框架。在本文中，我们形式化了从语言反馈中学习（LLF）的问题，提出了在潜在奖励下实现学习的充分假设，并引入了“迁移规避维度”作为衡量LLF问题难度的复杂性度量。我们表明，迁移规避维度捕捉了反馈中的信息如何改变LLF问题学习复杂度的直觉。我们展示了在某些情况下，从丰富的语言反馈中学习可以比从奖励中学习快指数倍。我们开发了一种无悔算法，名为$\texttt{HELiX}$，通过序列交互可证明地解决LLF问题，其性能保证与问题的迁移规避维度相关。在多个经验领域，我们表明$\texttt{HELiX}$即使在重复提示LLM不可靠时也能表现良好。我们的贡献标志着向设计基于通用语言反馈的原则性交互式学习算法迈出了第一步。", "summary": "本文针对大型语言模型驱动的交互式学习中缺乏原则性框架的问题，形式化了从语言反馈中学习（LLF）的问题。研究引入了“迁移规避维度”作为复杂性度量，并证明了其在刻画学习复杂度中的作用，指出丰富的语言反馈可带来指数级加速。论文还提出了一种名为$\texttt{HELiX}$的无悔算法，该算法被证明能够有效解决LLF问题，并在LLM提示不可靠的场景下也展现出优异的经验性能，为设计基于通用语言反馈的原则性交互式学习算法奠定了基础。", "keywords": "语言反馈学习, 迁移规避维度, 无悔算法, 大型语言模型, 交互式学习", "comments": "这篇论文的创新点在于首次对从语言反馈中学习的问题进行了形式化定义，并引入了“迁移规避维度”这一新的复杂性度量来量化学习的难度，这为该领域提供了重要的理论基础。HELiX算法的提出及其在LLM提示不可靠情况下的鲁棒性，也显示了其在实际应用中的潜力。这项工作是构建可证明有效的交互式学习算法的重要一步，对于推动LLM智能体的理论发展和实际应用具有重要意义。"}}
{"id": "2506.10465", "title": "MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models", "authors": ["Yu Huang", "Zelin Peng", "Yichen Zhao", "Piao Yang", "Xiaokang Yang", "Wei Shen"], "summary": "Medical image segmentation is crucial for clinical diagnosis, yet existing\nmodels are limited by their reliance on explicit human instructions and lack\nthe active reasoning capabilities to understand complex clinical questions.\nWhile recent advancements in multimodal large language models (MLLMs) have\nimproved medical question-answering (QA) tasks, most methods struggle to\ngenerate precise segmentation masks, limiting their application in automatic\nmedical diagnosis. In this paper, we introduce medical image reasoning\nsegmentation, a novel task that aims to generate segmentation masks based on\ncomplex and implicit medical instructions. To address this, we propose\nMedSeg-R, an end-to-end framework that leverages the reasoning abilities of\nMLLMs to interpret clinical questions while also capable of producing\ncorresponding precise segmentation masks for medical images. It is built on two\ncore components: 1) a global context understanding module that interprets\nimages and comprehends complex medical instructions to generate multi-modal\nintermediate tokens, and 2) a pixel-level grounding module that decodes these\ntokens to produce precise segmentation masks and textual responses.\nFurthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the\nmedical image reasoning segmentation task. It includes over 10,000 image-mask\npairs and multi-turn conversations, automatically annotated using large\nlanguage models and refined through physician reviews. Experiments show\nMedSeg-R's superior performance across several benchmarks, achieving high\nsegmentation accuracy and enabling interpretable textual analysis of medical\nimages.", "comment": "{\\dag}: Equal contribution", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10465v1", "AI": {"title_translation": "MedSeg-R: 医疗图像中的推理分割与多模态大语言模型", "tldr": "MedSeg-R是一个利用多模态大语言模型进行医疗图像推理分割的端到端框架，能根据复杂指令生成精确分割掩膜并提供文本解释，并引入了新数据集MedSeg-QA。", "motivation": "现有医疗图像分割模型受限于对明确人类指令的依赖，缺乏理解复杂临床问题的推理能力。尽管多模态大语言模型（MLLMs）在医疗问答（QA）任务上有所进展，但多数方法难以生成精确的分割掩膜，这限制了它们在自动医疗诊断中的应用。", "method": "本文引入了医疗图像推理分割这一新任务，旨在根据复杂和隐式医疗指令生成分割掩膜。为此，提出了MedSeg-R，一个端到端框架，利用MLLMs的推理能力解释临床问题，并生成精确的医疗图像分割掩膜。MedSeg-R包含两个核心组件：1) 全局上下文理解模块，用于解释图像和理解复杂医疗指令以生成多模态中间标记；2) 像素级接地模块，用于解码这些标记以生成精确的分割掩膜和文本响应。此外，还引入了MedSeg-QA，一个为该任务量身定制的大规模数据集，包含超过10,000对图像-掩膜对和多轮对话，通过大语言模型自动标注并经医生审阅。", "result": "实验表明MedSeg-R在多个基准测试中表现优异，实现了高分割精度，并能够对医疗图像进行可解释的文本分析。", "conclusion": "MedSeg-R通过结合MLLMs的推理能力和精确的分割生成，成功解决了医疗图像推理分割任务的挑战，并在多个基准测试中展现出卓越性能，为自动医疗诊断提供了新的可能性。", "translation": "医疗图像分割对于临床诊断至关重要，然而现有模型受限于对明确人类指令的依赖，并且缺乏理解复杂临床问题的 সক্রিয়推理能力。尽管多模态大语言模型（MLLMs）的最新进展改善了医疗问答（QA）任务，但大多数方法难以生成精确的分割掩膜，这限制了它们在自动医疗诊断中的应用。在本文中，我们引入了医疗图像推理分割，这是一项旨在根据复杂和隐式医疗指令生成分割掩膜的新任务。为了解决这个问题，我们提出了MedSeg-R，一个端到端框架，它利用MLLMs的推理能力来解释临床问题，同时能够为医疗图像生成相应的精确分割掩膜。它建立在两个核心组件之上：1）一个全局上下文理解模块，用于解释图像并理解复杂的医疗指令以生成多模态中间标记，以及2）一个像素级接地模块，用于解码这些标记以生成精确的分割掩膜和文本响应。此外，我们引入了MedSeg-QA，一个为医疗图像推理分割任务量身定制的大规模数据集。它包括超过10,000对图像-掩膜对和多轮对话，这些数据通过大语言模型自动标注并经过医生审阅。实验表明，MedSeg-R在多个基准测试中表现出色，实现了高分割精度，并能够对医疗图像进行可解释的文本分析。", "summary": "本文提出了一项新任务——医疗图像推理分割，旨在根据复杂医疗指令生成精确的分割掩膜。为解决现有模型缺乏推理能力和分割精度不足的问题，作者开发了MedSeg-R，一个利用多模态大语言模型（MLLMs）进行端到端推理和分割的框架。MedSeg-R包含全局上下文理解和像素级接地两个模块，并引入了大规模数据集MedSeg-QA。实验证明MedSeg-R在分割精度和可解释性方面均表现优异。", "keywords": "医疗图像分割, 推理分割, 多模态大语言模型, MedSeg-R, 医学诊断", "comments": "这项工作通过引入“医疗图像推理分割”这一新任务，并提出MedSeg-R框架，有效地结合了多模态大语言模型的推理能力与医学图像分割的需求。其创新之处在于能够处理复杂和隐式的医疗指令，并生成精确的像素级分割，同时提供可解释的文本分析。此外，构建大规模的MedSeg-QA数据集对于推动该领域的研究具有重要意义。这对于提高自动化医疗诊断的准确性和可信度具有重要潜力。"}}
{"id": "2506.10351", "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation", "authors": ["Yanlong Chen", "Mattia Orlandi", "Pierangelo Maria Rapa", "Simone Benatti", "Luca Benini", "Yawei Li"], "summary": "Physiological signals are often corrupted by motion artifacts, baseline\ndrift, and other low-SNR disturbances, which pose significant challenges for\nanalysis. Additionally, these signals exhibit strong non-stationarity, with\nsharp peaks and abrupt changes that evolve continuously, making them difficult\nto represent using traditional time-domain or filtering methods. To address\nthese issues, a novel wavelet-based approach for physiological signal analysis\nis presented, aiming to capture multi-scale time-frequency features in various\nphysiological signals. Leveraging this technique, two large-scale pretrained\nmodels specific to EMG and ECG are introduced for the first time, achieving\nsuperior performance and setting new baselines in downstream tasks.\nAdditionally, a unified multi-modal framework is constructed by integrating\npretrained EEG model, where each modality is guided through its dedicated\nbranch and fused via learnable weighted fusion. This design effectively\naddresses challenges such as low signal-to-noise ratio, high inter-subject\nvariability, and device mismatch, outperforming existing methods on multi-modal\ntasks. The proposed wavelet-based architecture lays a solid foundation for\nanalysis of diverse physiological signals, while the multi-modal design points\nto next-generation physiological signal processing with potential impact on\nwearable health monitoring, clinical diagnostics, and broader biomedical\napplications.", "comment": "22 pages, 8 figures, 9 tables. Submitted to NeurIPS 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10351v1", "AI": {"title_translation": "PhysioWave：一种用于生理信号表示的多尺度小波变换器", "tldr": "PhysioWave提出了一种新颖的小波变换器，用于捕获生理信号的多尺度时频特征，并引入了针对EMG、ECG和EEG的预训练模型及多模态融合框架，在下游任务和多模态任务中取得了卓越性能。", "motivation": "生理信号常受运动伪影、基线漂移和低信噪比干扰，且具有强非平稳性，传统时域或滤波方法难以有效表示和分析。", "method": "提出了一种新颖的基于小波的方法，旨在捕获各种生理信号的多尺度时频特征。在此基础上，首次引入了两个针对EMG和ECG的大规模预训练模型。此外，通过整合预训练的EEG模型构建了一个统一的多模态框架，其中每个模态通过专用分支引导，并通过可学习的加权融合进行融合。", "result": "针对EMG和ECG的预训练模型在下游任务中取得了卓越性能并设定了新的基线。多模态框架在多模态任务中优于现有方法，有效解决了低信噪比、高受试者间变异性和设备不匹配等挑战。", "conclusion": "所提出的基于小波的架构为分析各种生理信号奠定了坚实基础，而多模态设计则预示着下一代生理信号处理技术，对可穿戴健康监测、临床诊断和更广泛的生物医学应用具有潜在影响。", "translation": "生理信号常常受到运动伪影、基线漂移和其他低信噪比干扰的污染，这给分析带来了重大挑战。此外，这些信号表现出强烈的非平稳性，具有不断演变的尖峰和突然变化，使得它们难以使用传统的时域或滤波方法进行表示。为了解决这些问题，本文提出了一种新颖的基于小波的生理信号分析方法，旨在捕获各种生理信号中的多尺度时频特征。利用这项技术，首次引入了两个针对肌电图（EMG）和心电图（ECG）的大规模预训练模型，在下游任务中取得了卓越性能并设定了新的基线。此外，通过整合预训练的脑电图（EEG）模型，构建了一个统一的多模态框架，其中每个模态通过其专用分支进行引导，并通过可学习的加权融合进行融合。这种设计有效解决了低信噪比、高受试者间变异性和设备不匹配等挑战，在多模态任务中优于现有方法。所提出的基于小波的架构为分析各种生理信号奠定了坚实的基础，而多模态设计则指向下一代生理信号处理，对可穿戴健康监测、临床诊断和更广泛的生物医学应用具有潜在影响。", "summary": "PhysioWave提出了一种创新的多尺度小波变换器，用于有效表示受噪声和非平稳性影响的生理信号。该方法利用小波分析捕获时频特征，并首次引入针对EMG和ECG的大规模预训练模型，在下游任务中表现优异。此外，通过整合EEG模型，构建了一个统一的多模态框架，通过加权融合处理多模态数据，在解决低信噪比和高变异性等挑战方面超越了现有方法。该工作为生理信号分析奠定了基础，并为未来可穿戴健康监测和生物医学应用指明了方向。", "keywords": "生理信号, 小波变换, Transformer, 预训练模型, 多模态融合", "comments": "该论文的创新点在于结合了小波变换和Transformer模型来处理生理信号的非平稳性和噪声问题，并首次提出了针对EMG和ECG的预训练模型。其多模态融合框架也展现了处理复杂生理数据的强大潜力，预示着生理信号处理领域的重要进展，尤其是在可穿戴设备和临床诊断方面具有广阔的应用前景。"}}
{"id": "2506.10766", "title": "One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers", "authors": ["Diana Abagyan", "Alejandro R. Salamanca", "Andres Felipe Cruz-Salinas", "Kris Cao", "Hangyu Lin", "Acyr Locatelli", "Marzieh Fadaee", "Ahmet Üstün", "Sara Hooker"], "summary": "Pretraining massively multilingual Large Language Models (LLMs) for many\nlanguages at once is challenging due to limited model capacity, scarce\nhigh-quality data, and compute constraints. Moreover, the lack of language\ncoverage of the tokenizer makes it harder to address the gap for new languages\npurely at the post-training stage. In this work, we study what relatively cheap\ninterventions early on in training improve \"language plasticity\", or adaptation\ncapabilities of the model post-training to new languages. We focus on tokenizer\ndesign and propose using a universal tokenizer that is trained for more\nlanguages than the primary pretraining languages to enable efficient adaptation\nin expanding language coverage after pretraining. Our systematic experiments\nacross diverse groups of languages and different training strategies show that\na universal tokenizer enables significantly higher language adaptation, with up\nto 20.2% increase in win rates compared to tokenizers specific to pretraining\nlanguages. Furthermore, a universal tokenizer also leads to better plasticity\ntowards languages that are completely unseen in the tokenizer and pretraining,\nby up to 5% win rate gain. We achieve this adaptation to an expanded set of\nlanguages with minimal compromise in performance on the majority of languages\nincluded in pretraining.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10766v1", "AI": {"title_translation": "一个分词器统领一切：通过多语言分词器实现涌现的语言可塑性", "tldr": "研究表明，使用通用分词器预训练多语言大型语言模型能显著提高模型对新语言的适应能力，即便这些语言在预训练或分词器中未曾出现。", "motivation": "预训练大规模多语言LLMs面临模型容量、数据稀缺和计算限制的挑战，且现有分词器语言覆盖不足，难以在后训练阶段有效处理新语言。本研究旨在寻找早期训练中相对廉价的干预措施，以提高模型在后训练阶段对新语言的“语言可塑性”或适应能力。", "method": "专注于分词器设计，提出使用一个为比主要预训练语言更多语言训练的通用分词器，以实现在预训练后扩展语言覆盖时的有效适应。通过对不同语种组和训练策略进行系统实验。", "result": "通用分词器显著提高了语言适应能力，与特定于预训练语言的分词器相比，胜率提升高达20.2%。对于分词器和预训练中完全未见的语言，通用分词器也能带来更好的可塑性，胜率提升高达5%。实现这种适应性扩展的同时，对预训练中包含的大多数语言的性能影响极小。", "conclusion": "使用一个为更多语言训练的通用分词器，可以显著提高多语言LLMs对新语言的适应能力，且对现有语言性能影响甚微。", "translation": "预训练大规模多语言大型语言模型（LLMs）同时处理多种语言面临模型容量有限、高质量数据稀缺以及计算约束的挑战。此外，分词器缺乏语言覆盖使得纯粹在后训练阶段解决新语言的差距变得更加困难。在这项工作中，我们研究了早期训练中相对廉价的干预措施如何提高“语言可塑性”，即模型在后训练阶段对新语言的适应能力。我们专注于分词器设计，并提出使用一个为比主要预训练语言更多语言训练的通用分词器，以实现在预训练后扩展语言覆盖时的有效适应。我们对不同语种组和不同训练策略进行的系统实验表明，通用分词器能够显著提高语言适应能力，与特定于预训练语言的分词器相比，胜率提升高达20.2%。此外，通用分词器还能对分词器和预训练中完全未见的语言带来更好的可塑性，胜率提升高达5%。我们实现了对扩展语言集的这种适应，同时对预训练中包含的大多数语言的性能影响极小。", "summary": "本文研究了通过优化分词器设计来提升大规模多语言LLMs对新语言的适应能力。作者提出并验证了一种通用分词器，该分词器训练的语言范围超出模型主要预训练语言。实验结果表明，这种通用分词器显著提高了模型对已知和完全未见语言的适应性，同时对现有语言的性能影响微乎其微，为解决多语言LLMs的语言覆盖挑战提供了有效途径。", "keywords": "通用分词器, 语言可塑性, 多语言LLMs, 语言适应, 预训练", "comments": "这项工作提出了一个创新的解决方案，通过在早期训练阶段采用通用分词器来解决多语言LLMs在语言覆盖和适应性方面的挑战。其重要性在于，它提供了一种相对廉价且高效的方法，显著提升了模型对新语言的泛化能力，包括那些在预训练中从未出现过的语言，这对于构建更具包容性的全球化AI模型至关重要。"}}
{"id": "2506.10474", "title": "LLMs Are Not Yet Ready for Deepfake Image Detection", "authors": ["Shahroz Tariq", "David Nguyen", "M. A. P. Chamikara", "Tingmin Wu", "Alsharif Abuadbba", "Kristen Moore"], "summary": "The growing sophistication of deepfakes presents substantial challenges to\nthe integrity of media and the preservation of public trust. Concurrently,\nvision-language models (VLMs), large language models enhanced with visual\nreasoning capabilities, have emerged as promising tools across various domains,\nsparking interest in their applicability to deepfake detection. This study\nconducts a structured zero-shot evaluation of four prominent VLMs: ChatGPT,\nClaude, Gemini, and Grok, focusing on three primary deepfake types: faceswap,\nreenactment, and synthetic generation. Leveraging a meticulously assembled\nbenchmark comprising authentic and manipulated images from diverse sources, we\nevaluate each model's classification accuracy and reasoning depth. Our analysis\nindicates that while VLMs can produce coherent explanations and detect\nsurface-level anomalies, they are not yet dependable as standalone detection\nsystems. We highlight critical failure modes, such as an overemphasis on\nstylistic elements and vulnerability to misleading visual patterns like vintage\naesthetics. Nevertheless, VLMs exhibit strengths in interpretability and\ncontextual analysis, suggesting their potential to augment human expertise in\nforensic workflows. These insights imply that although general-purpose models\ncurrently lack the reliability needed for autonomous deepfake detection, they\nhold promise as integral components in hybrid or human-in-the-loop detection\nframeworks.", "comment": "6 pages, 3 figures, and 2 tables. paper is under review", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10474v1", "AI": {"title_translation": "大型语言模型尚未准备好用于深度伪造图像检测", "tldr": "研究发现，ChatGPT、Claude、Gemini和Grok等大型视觉语言模型（VLMs）在零样本评估中，虽然能提供解释并检测表面异常，但作为独立的深度伪造图像检测系统尚不可靠，不过它们在混合或人机协作检测框架中具有增强人类专业知识的潜力。", "motivation": "深度伪造技术的日益复杂对媒体的完整性和公众信任构成重大挑战。与此同时，视觉语言模型（VLMs）作为一种有前景的工具在各个领域崭露头角，引发了人们对其在深度伪造检测中应用潜力的兴趣。", "method": "本研究对四种主要VLM（ChatGPT、Claude、Gemini和Grok）进行了结构化的零样本评估，重点关注三种主要的深度伪造类型：换脸、重演和合成生成。研究利用精心构建的基准数据集（包含来自不同来源的真实和篡改图像），评估了每个模型的分类准确性和推理深度。", "result": "分析表明，虽然VLMs可以产生连贯的解释并检测表面异常，但它们作为独立的检测系统尚不可靠。研究强调了关键的失败模式，例如过度强调风格元素和容易受到误导性视觉模式（如复古美学）的影响。然而，VLMs在可解释性和上下文分析方面表现出优势。", "conclusion": "这些见解表明，尽管通用模型目前缺乏自主深度伪造检测所需的可靠性，但它们有望成为混合或人机协作检测框架中不可或缺的组成部分。", "translation": "深度伪造技术日益复杂，对媒体的完整性和公众信任构成了巨大挑战。与此同时，视觉语言模型（VLMs），即增强了视觉推理能力的大型语言模型，已成为各个领域中极具前景的工具，引发了人们对其在深度伪造检测中应用潜力的兴趣。本研究对四种主要VLM：ChatGPT、Claude、Gemini和Grok进行了结构化的零样本评估，重点关注三种主要的深度伪造类型：换脸、重演和合成生成。我们利用一个精心构建的基准数据集，其中包含来自不同来源的真实和篡改图像，评估了每个模型的分类准确性和推理深度。我们的分析表明，虽然VLMs可以产生连贯的解释并检测表面异常，但它们作为独立的检测系统尚不可靠。我们强调了关键的失败模式，例如过度强调风格元素以及容易受到误导性视觉模式（如复古美学）的影响。然而，VLMs在可解释性和上下文分析方面表现出优势，这表明它们有潜力在法证工作流程中增强人类专业知识。这些见解表明，尽管通用模型目前缺乏自主深度伪造检测所需的可靠性，但它们有望成为混合或人机协作检测框架中不可或缺的组成部分。", "summary": "本研究评估了ChatGPT、Claude、Gemini和Grok四种主流视觉语言模型（VLMs）在零样本深度伪造图像检测方面的能力。结果显示，尽管VLMs能提供解释并识别表面异常，但它们作为独立的检测系统尚不可靠，存在过度关注风格和易受误导性视觉模式影响等缺陷。然而，VLMs在可解释性和上下文分析上的优势，预示着它们在混合或人机协作的深度伪造检测框架中具有辅助人类专家的潜力。", "keywords": "深度伪造检测, 视觉语言模型, 零样本评估, 失败模式, 人机协作", "comments": "这篇论文揭示了当前大型视觉语言模型在深度伪造检测领域的局限性，尤其是在独立检测方面的不可靠性，这对于LMM/VLM的应用研究具有重要指导意义。其创新之处在于，它不仅指出了模型的不足，还提出了在混合或人机协作框架中利用VLM可解释性和上下文分析优势的潜在方向，为未来的研究提供了新的思路。"}}
{"id": "2506.10352", "title": "History-Aware Neural Operator: Robust Data-Driven Constitutive Modeling of Path-Dependent Materials", "authors": ["Binyao Guo", "Zihan Lin", "QiZhi He"], "summary": "This study presents an end-to-end learning framework for data-driven modeling\nof path-dependent inelastic materials using neural operators. The framework is\nbuilt on the premise that irreversible evolution of material responses,\ngoverned by hidden dynamics, can be inferred from observable data.\n  We develop the History-Aware Neural Operator (HANO), an autoregressive model\nthat predicts path-dependent material responses from short segments of recent\nstrain-stress history without relying on hidden state variables, thereby\novercoming self-consistency issues commonly encountered in recurrent neural\nnetwork (RNN)-based models. Built on a Fourier-based neural operator backbone,\nHANO enables discretization-invariant learning. To enhance its ability to\ncapture both global loading patterns and critical local path dependencies, we\nembed a hierarchical self-attention mechanism that facilitates multiscale\nfeature extraction.\n  Beyond ensuring self-consistency, HANO mitigates sensitivity to initial\nhidden states, a commonly overlooked issue that can lead to instability in\nrecurrent models when applied to generalized loading paths. By modeling\nstress-strain evolution as a continuous operator rather than relying on fixed\ninput-output mappings, HANO naturally accommodates varying path discretizations\nand exhibits robust performance under complex conditions, including irregular\nsampling, multi-cycle loading, noisy data, and pre-stressed states. We evaluate\nHANO on two benchmark problems: elastoplasticity with hardening and progressive\nanisotropic damage in brittle solids. Results show that HANO consistently\noutperforms baseline models in predictive accuracy, generalization, and\nrobustness. With its demonstrated capabilities, HANO provides an effective\ndata-driven surrogate for simulating inelastic materials and is well-suited for\nintegration with classical numerical solvers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10352v1", "AI": {"title_translation": "历史感知神经算子：路径依赖材料的鲁棒数据驱动本构建模", "tldr": "HANO是一种新的神经算子模型，通过从短期历史数据预测材料响应，解决了传统RNN在路径依赖材料建模中的自洽性和初始状态敏感性问题，表现出更高的准确性、泛化性和鲁棒性。", "motivation": "传统方法在建模路径依赖的非弹性材料时，存在自洽性问题和对初始隐藏状态的敏感性，导致不稳定性。本研究旨在开发一种鲁棒的数据驱动模型来克服这些挑战。", "method": "提出了历史感知神经算子 (History-Aware Neural Operator, HANO)。HANO是一个自回归模型，从短期应变-应力历史预测路径依赖材料响应，不依赖隐藏状态变量。它基于傅里叶神经算子骨干，实现离散化不变学习，并嵌入了分层自注意力机制以进行多尺度特征提取。", "result": "HANO在弹塑性硬化和脆性固体渐进各向异性损伤两个基准问题上进行了评估，结果显示其在预测精度、泛化能力和鲁棒性方面始终优于基线模型。", "conclusion": "HANO提供了一种有效的数据驱动替代方法，用于模拟非弹性材料，并能很好地与经典数值求解器集成。它通过解决自洽性和初始隐藏状态敏感性问题，提高了对复杂载荷条件下的鲁棒性。", "translation": "本研究提出了一种使用神经算子对路径依赖非弹性材料进行数据驱动建模的端到端学习框架。该框架建立在以下前提之上：材料响应的不可逆演化（由隐藏动力学控制）可以从可观测数据中推断出来。\n我们开发了历史感知神经算子（HANO），这是一种自回归模型，它根据最近应变-应力历史的短片段预测路径依赖材料响应，而不依赖隐藏状态变量，从而克服了循环神经网络（RNN）模型中常见的自洽性问题。HANO基于傅里叶神经算子骨干，实现了离散化不变学习。为了增强其捕获全局加载模式和关键局部路径依赖性的能力，我们嵌入了分层自注意力机制，以促进多尺度特征提取。\n除了确保自洽性，HANO还减轻了对初始隐藏状态的敏感性，这是一个常被忽视的问题，当应用于广义加载路径时，可能导致循环模型的不稳定性。通过将应力-应变演化建模为连续算子而不是依赖固定的输入-输出映射，HANO自然适应不同的路径离散化，并在复杂条件下（包括不规则采样、多周期加载、噪声数据和预应力状态）表现出鲁棒性能。我们在两个基准问题上评估了HANO：硬化弹塑性和脆性固体中的渐进各向异性损伤。结果表明，HANO在预测精度、泛化能力和鲁棒性方面始终优于基线模型。凭借其所展示的能力，HANO为模拟非弹性材料提供了一种有效的数据驱动替代方案，并且非常适合与经典数值求解器集成。", "summary": "本文提出了一种名为历史感知神经算子（HANO）的端到端学习框架，用于数据驱动地建模路径依赖的非弹性材料。HANO作为一种自回归模型，通过利用短期应变-应力历史来预测材料响应，避免了传统RNN模型中常见的自洽性问题和对初始隐藏状态的敏感性。该模型基于傅里叶神经算子并结合了分层自注意力机制，实现了离散化不变学习和多尺度特征提取。实验结果表明，HANO在预测精度、泛化性和鲁棒性方面均优于现有模型，为非弹性材料的模拟提供了一个鲁棒且高效的数据驱动代理。", "keywords": "神经算子, 路径依赖材料, 数据驱动建模, 自回归模型, 非弹性材料", "comments": "HANO的创新之处在于其无需隐藏状态变量的自回归设计，有效解决了RNN在路径依赖材料建模中的核心痛点。傅里叶神经算子骨干和分层自注意力机制的结合，使其在处理复杂加载路径和不同离散化方面表现出卓越的鲁棒性和泛化能力，对于材料科学和工程领域的数值模拟具有重要意义。"}}
{"id": "2506.10021", "title": "From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop", "authors": ["Jordi de la Torre"], "summary": "We propose a novel architecture for integrating large language models (LLMs)\nwith a persistent, interactive Lisp environment. This setup enables LLMs to\ndefine, invoke, and evolve their own tools through programmatic interaction\nwith a live REPL. By embedding Lisp expressions within generation and\nintercepting them via a middleware layer, the system allows for stateful\nexternal memory, reflective programming, and dynamic tool creation. We present\na design framework and architectural principles to guide future implementations\nof interactive AI systems that integrate symbolic programming with neural\nlanguage generation.", "comment": null, "cate": "cs.PL", "url": "http://arxiv.org/abs/2506.10021v1", "AI": {"title_translation": "从工具调用到符号思维：大型语言模型在持久Lisp元编程循环中", "tldr": "该论文提出了一种新颖的架构，将大型语言模型（LLM）与持久的Lisp环境集成，使LLM能够定义、调用和演化自己的工具，从而实现有状态的外部记忆、反射编程和动态工具创建。", "motivation": "该论文旨在提出一种新颖的架构，将大型语言模型（LLM）与持久、交互式的Lisp环境集成，以使LLM能够通过程序化交互来定义、调用和演化自己的工具，从而增强其能力。", "method": "该系统通过将Lisp表达式嵌入到LLM的生成中，并通过中间件层进行拦截，从而实现与实时Lisp REPL的集成。这种方法支持有状态的外部记忆、反射编程和动态工具创建。论文还提出了一个设计框架和架构原则。", "result": "论文提出了一种将LLM与持久Lisp环境集成的新型架构、一个设计框架和架构原则，旨在指导未来集成符号编程与神经网络语言生成的交互式AI系统的实现。", "conclusion": "该论文提供了一个设计框架和架构原则，用于构建未来的交互式AI系统，这些系统能够将符号编程与神经网络语言生成相结合，从而使LLM能够进行更高级、动态的工具使用和符号思维。", "translation": "我们提出了一种新颖的架构，用于将大型语言模型（LLM）与一个持久的、交互式的Lisp环境集成。这种设置使LLM能够通过与实时REPL的程序化交互来定义、调用和演化自己的工具。通过在生成中嵌入Lisp表达式并通过中间件层拦截它们，该系统允许状态外部记忆、反射编程和动态工具创建。我们提出了一个设计框架和架构原则，以指导未来集成符号编程与神经网络语言生成的交互式AI系统的实现。", "summary": "该论文介绍了一种新颖的架构，将大型语言模型（LLM）与持久、交互式的Lisp环境相结合。这种集成使得LLM能够通过与实时REPL的程序化交互来定义、调用和演化自己的工具。通过在LLM生成中嵌入Lisp表达式并由中间件层拦截，该系统实现了有状态的外部记忆、反射编程和动态工具创建。作者提出了一个设计框架和架构原则，以指导未来集成符号编程与神经网络语言生成的交互式AI系统的开发。", "keywords": "大型语言模型, Lisp, 元编程, 工具创建, 符号AI", "comments": "该创新之处在于超越了静态的“工具调用”，转向了一个动态的“元编程循环”，其中LLM可以在实时符号环境中创建和演化自己的工具。这种方法有潜力增强LLM在需要状态性、反射和复杂推理方面的能力，弥合了神经网络AI和符号AI之间的鸿沟。"}}
{"id": "2506.10769", "title": "Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs", "authors": ["Alberto Testoni", "Iacer Calixto"], "summary": "Accurate and well-calibrated uncertainty estimates are essential for\ndeploying large language models (LLMs) in high-stakes domains such as clinical\ndecision support. We present a fine-grained evaluation of uncertainty\nestimation methods for clinical multiple-choice question answering, covering\nten open-source LLMs (general-purpose, biomedical, and reasoning models) across\ntwo datasets, eleven medical specialties, and six question types. We compare\nstandard single-generation and sampling-based methods, and present a case study\nexploring simple, single-pass estimators based on behavioral signals in\nreasoning traces. These lightweight methods approach the performance of\nSemantic Entropy while requiring only one generation. Our results reveal\nsubstantial variation across specialties and question types, underscoring the\nimportance of selecting models based on both the nature of the question and\nmodel-specific strengths.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10769v1", "AI": {"title_translation": "不同的问题，不同的模型：大型语言模型在临床问答中不确定性和校准的细粒度评估", "tldr": "本文对大型语言模型在临床问答中不确定性估计和校准进行了细粒度评估，发现模型选择应基于问题类型和模型优势。", "motivation": "在临床决策支持等高风险领域部署大型语言模型（LLMs）时，准确且校准良好的不确定性估计至关重要。", "method": "评估了十个开源LLM（通用、生物医学和推理模型），涵盖两个数据集、十一个医学专业和六种问题类型。比较了标准单次生成和基于采样的不确定性估计方法，并探讨了基于推理轨迹中行为信号的轻量级单次估计器。", "result": "结果显示不同专业和问题类型之间存在显著差异。轻量级方法在性能上接近语义熵，但只需一次生成。", "conclusion": "选择模型时，应根据问题的性质和模型自身的优势进行选择，因为不确定性估计和校准在不同情境下表现差异大。", "translation": "准确且校准良好的不确定性估计对于在临床决策支持等高风险领域部署大型语言模型（LLMs）至关重要。我们对临床多项选择问答中不确定性估计方法进行了细粒度评估，涵盖了十个开源LLM（通用型、生物医学型和推理型模型），跨越两个数据集、十一个医学专业和六种问题类型。我们比较了标准的单次生成和基于采样的方法，并提出了一个案例研究，探讨了基于推理轨迹中行为信号的简单、单次估计器。这些轻量级方法在性能上接近语义熵（Semantic Entropy），同时只需一次生成。我们的结果揭示了不同专业和问题类型之间存在显著差异，强调了根据问题性质和模型特定优势选择模型的重要性。", "summary": "本文对大型语言模型在临床问答中不确定性估计和校准进行了细致评估。研究比较了十个开源LLM在不同医学专业和问题类型上的表现，并探索了轻量级不确定性估计方法。结果表明，不确定性估计在不同情境下差异显著，因此模型选择应充分考虑问题特性和模型优势。", "keywords": "大型语言模型, 不确定性估计, 临床问答, 模型校准, 细粒度评估", "comments": "这篇论文强调了在临床高风险领域部署LLM时，不确定性估计和校准的重要性。其创新之处在于进行了细粒度的评估，揭示了不同医学专业和问题类型对模型表现的显著影响。提出的轻量级不确定性估计方法在效率和性能之间取得了良好平衡。研究的局限性可能在于其评估范围仅限于多项选择问答，未来可扩展到更复杂的临床任务。"}}
{"id": "2506.10355", "title": "TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree", "authors": ["Yu-Yang Qian", "Yuan-Ze Xu", "Zhen-Yu Zhang", "Peng Zhao", "Zhi-Hua Zhou"], "summary": "Many real-world applications collect data in a streaming environment, where\nlearning tasks are encountered sequentially. This necessitates continual\nlearning (CL) to update models online, enabling adaptation to new tasks while\npreserving past knowledge to prevent catastrophic forgetting. Nowadays, with\nthe flourish of large pre-trained models (LPMs), efficiency has become\nincreasingly critical for CL, due to their substantial computational demands\nand growing parameter sizes. In this paper, we introduce TreeLoRA (K-D Tree of\nLow-Rank Adapters), a novel approach that constructs layer-wise adapters by\nleveraging hierarchical gradient similarity to enable efficient CL,\nparticularly for LPMs. To reduce the computational burden of task similarity\nestimation, we employ bandit techniques to develop an algorithm based on lower\nconfidence bounds to efficiently explore the task structure. Furthermore, we\nuse sparse gradient updates to facilitate parameter optimization, making the\napproach better suited for LPMs. Theoretical analysis is provided to justify\nthe rationale behind our approach, and experiments on both vision transformers\n(ViTs) and large language models (LLMs) demonstrate the effectiveness and\nefficiency of our approach across various domains, including vision and natural\nlanguage processing tasks.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10355v1", "AI": {"title_translation": "TreeLoRA：通过分层LoRA和分层梯度相似性树引导的高效持续学习", "tldr": "TreeLoRA通过分层LoRA和梯度相似性树，为大型预训练模型实现了高效的持续学习，有效解决了灾难性遗忘和计算效率问题。", "motivation": "在流式数据环境中，模型需要在线更新以适应新任务并保留旧知识，防止灾难性遗忘。随着大型预训练模型（LPMs）的普及，其巨大的计算需求和参数规模使得持续学习的效率变得至关重要。", "method": "本文提出了TreeLoRA（低秩适配器K-D树），一种通过利用分层梯度相似性构建层级适配器来实现高效持续学习的方法，特别适用于LPMs。为减少任务相似性估计的计算负担，该方法采用强盗算法（基于下置信界）高效探索任务结构。此外，还使用稀疏梯度更新来优化参数。", "result": "在视觉Transformer（ViTs）和大型语言模型（LLMs）上的实验表明，该方法在视觉和自然语言处理任务等多个领域都具有有效性和高效性。", "conclusion": "TreeLoRA通过其创新的分层适配器构建、高效的任务结构探索和稀疏梯度更新机制，成功地为大型预训练模型实现了高效且有效的持续学习，解决了灾难性遗忘和计算效率挑战。", "translation": "许多现实世界的应用在流式环境中收集数据，学习任务是按顺序遇到的。这需要持续学习（CL）在线更新模型，使其能够适应新任务同时保留过去的知识以防止灾难性遗忘。如今，随着大型预训练模型（LPMs）的蓬勃发展，由于其巨大的计算需求和不断增长的参数规模，效率对于CL变得越来越关键。在本文中，我们引入了TreeLoRA（低秩适配器K-D树），这是一种新颖的方法，它通过利用分层梯度相似性来构建分层适配器，从而实现高效的CL，特别是针对LPMs。为了减少任务相似性估计的计算负担，我们采用强盗技术开发了一种基于下置信界算法来高效探索任务结构。此外，我们使用稀疏梯度更新来促进参数优化，使该方法更适合LPMs。本文提供了理论分析来证明我们方法的基本原理，并且在视觉Transformer（ViTs）和大型语言模型（LLMs）上的实验证明了我们方法在包括视觉和自然语言处理任务在内的各个领域的有效性和效率。", "summary": "本文提出TreeLoRA，一种针对大型预训练模型（LPMs）的高效持续学习方法。该方法通过构建基于分层梯度相似性的层级低秩适配器，有效防止灾难性遗忘。为提高效率，TreeLoRA引入强盗算法进行任务结构探索，并采用稀疏梯度更新。实验证明，该方法在视觉和自然语言处理任务上对ViTs和LLMs均表现出卓越的有效性和高效性。", "keywords": "持续学习, 大型预训练模型, LoRA, 梯度相似性, 效率", "comments": "TreeLoRA的创新之处在于将分层梯度相似性与LoRA适配器结合，并通过强盗算法优化任务相似性估计，显著提升了大型模型持续学习的效率。其理论分析和在不同模态（ViTs和LLMs）上的实验验证，表明了该方法在应对现实世界流式数据挑战方面的强大潜力。"}}
{"id": "2506.10779", "title": "Improving Named Entity Transcription with Contextual LLM-based Revision", "authors": ["Viet Anh Trinh", "Xinlu He", "Jacob Whitehill"], "summary": "With recent advances in modeling and the increasing amount of supervised\ntraining data, automatic speech recognition (ASR) systems have achieved\nremarkable performance on general speech. However, the word error rate (WER) of\nstate-of-the-art ASR remains high for named entities. Since named entities are\noften the most critical keywords, misrecognizing them can affect all downstream\napplications, especially when the ASR system functions as the front end of a\ncomplex system. In this paper, we introduce a large language model (LLM)\nrevision mechanism to revise incorrect named entities in ASR predictions by\nleveraging the LLM's reasoning ability as well as local context (e.g., lecture\nnotes) containing a set of correct named entities. Finally, we introduce the\nNER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses\nfor development and testing. On this dataset, our proposed technique achieves\nup to 30\\% relative WER reduction for named entities.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10779v1", "AI": {"title_translation": "基于上下文LLM修订的命名实体转录改进", "tldr": "本文提出了一种基于LLM的修订机制，利用LLM的推理能力和局部上下文来纠正ASR预测中的命名实体错误，并在新数据集上实现了命名实体词错率的显著降低。", "motivation": "尽管ASR系统在通用语音上表现出色，但命名实体的词错率仍然很高。由于命名实体通常是最关键的关键词，其错误识别会影响所有下游应用，特别是当ASR系统作为复杂系统的前端时。", "method": "引入了一种大语言模型（LLM）修订机制，通过利用LLM的推理能力以及包含正确命名实体集的局部上下文（例如，讲义）来修订ASR预测中不正确的命名实体。此外，还引入了NER-MIT-OpenCourseWare数据集用于开发和测试。", "result": "在NER-MIT-OpenCourseWare数据集上，所提出的技术实现了命名实体高达30%的相对词错率（WER）降低。", "conclusion": "通过结合LLM的推理能力和局部上下文，可以显著提高ASR系统中命名实体的转录准确性。", "translation": "随着建模的最新进展和监督训练数据的增加，自动语音识别（ASR）系统在通用语音上取得了显著的性能。然而，最先进的ASR对命名实体的词错率（WER）仍然很高。由于命名实体通常是最关键的关键词，错误识别它们会影响所有下游应用，特别是当ASR系统作为复杂系统的前端时。在本文中，我们引入了一种大语言模型（LLM）修订机制，通过利用LLM的推理能力以及包含一组正确命名实体的局部上下文（例如，讲义）来修订ASR预测中不正确的命名实体。最后，我们介绍了NER-MIT-OpenCourseWare数据集，其中包含来自麻省理工学院课程的45小时数据，用于开发和测试。在这个数据集上，我们提出的技术实现了命名实体高达30%的相对词错率降低。", "summary": "本文针对ASR系统中命名实体识别率低的问题，提出了一种基于大语言模型（LLM）的修订机制。该机制利用LLM的推理能力和局部上下文（如讲义）中的正确命名实体信息，对ASR预测中的错误命名实体进行修正。研究团队还构建了NER-MIT-OpenCourseWare数据集进行实验。结果显示，该方法在命名实体转录方面实现了高达30%的相对词错率降低，显著提升了ASR对关键信息的识别准确性。", "keywords": "命名实体, LLM, 自动语音识别, 词错率, 上下文", "comments": "本文的创新点在于将LLM的推理能力与局部上下文相结合，用于ASR后处理中的命名实体修正，有效解决了ASR在命名实体识别上的痛点。同时，引入新的NER-MIT-OpenCourseWare数据集也为该领域的研究提供了宝贵的资源。该方法对于需要高精度命名实体识别的下游应用具有重要意义。"}}
{"id": "2506.10489", "title": "Class-Incremental Learning for Honey Botanical Origin Classification with Hyperspectral Images: A Study with Continual Backpropagation", "authors": ["Guyang Zhang", "Waleed Abdulla"], "summary": "Honey is an important commodity in the global market. Honey types of\ndifferent botanical origins provide diversified flavors and health benefits,\nthus having different market values. Developing accurate and effective\nbotanical origin-distinguishing techniques is crucial to protect consumers'\ninterests. However, it is impractical to collect all the varieties of honey\nproducts at once to train a model for botanical origin differentiation.\nTherefore, researchers developed class-incremental learning (CIL) techniques to\naddress this challenge. This study examined and compared multiple CIL\nalgorithms on a real-world honey hyperspectral imaging dataset. A novel\ntechnique is also proposed to improve the performance of class-incremental\nlearning algorithms by combining with a continual backpropagation (CB)\nalgorithm. The CB method addresses the issue of loss-of-plasticity by\nreinitializing a proportion of less-used hidden neurons to inject variability\ninto neural networks. Experiments showed that CB improved the performance of\nmost CIL methods by 1-7\\%.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10489v1", "AI": {"title_translation": "基于高光谱图像的蜂蜜植物来源分类的类增量学习：一项关于持续反向传播的研究", "tldr": "本研究探讨了使用类增量学习（CIL）和持续反向传播（CB）算法，通过高光谱图像对蜂蜜植物来源进行分类。提出的CB方法通过重新初始化部分隐藏神经元来提高CIL性能，实验表明其将大多数CIL方法的性能提高了1-7%。", "motivation": "在全球市场中，蜂蜜因其不同植物来源带来的多样化风味和健康益处而具有不同的市场价值。开发准确有效的植物来源区分技术对于保护消费者利益至关重要。然而，一次性收集所有蜂蜜品种以训练模型进行植物来源区分是不切实际的，因此需要类增量学习（CIL）技术来解决这一挑战。", "method": "本研究在一个真实世界的蜂蜜高光谱成像数据集上，检查并比较了多种类增量学习（CIL）算法。此外，提出了一种新颖的技术，通过与持续反向传播（CB）算法结合来提高类增量学习算法的性能。CB方法通过重新初始化一部分使用较少的隐藏神经元来注入神经网络的可变性，从而解决可塑性丧失的问题。", "result": "实验表明，持续反向传播（CB）方法将大多数类增量学习（CIL）方法的性能提高了1-7%。", "conclusion": "持续反向传播（CB）算法能够有效提高基于高光谱图像的蜂蜜植物来源分类中类增量学习方法的性能。", "translation": "蜂蜜是全球市场上的重要商品。不同植物来源的蜂蜜种类提供多样化的风味和健康益处，因此具有不同的市场价值。开发准确有效的植物来源区分技术对于保护消费者利益至关重要。然而，一次性收集所有蜂蜜产品来训练模型进行植物来源区分是不切实际的。因此，研究人员开发了类增量学习（CIL）技术来解决这一挑战。本研究在一个真实世界的蜂蜜高光谱成像数据集上，检查并比较了多种CIL算法。此外，还提出了一种新颖的技术，通过与持续反向传播（CB）算法结合来提高类增量学习算法的性能。CB方法通过重新初始化一部分使用较少的隐藏神经元来注入神经网络的可变性，从而解决可塑性丧失的问题。实验表明，CB将大多数CIL方法的性能提高了1-7%。", "summary": "本研究旨在解决蜂蜜植物来源分类中难以一次性收集所有数据进行模型训练的问题，引入了类增量学习（CIL）技术。论文在一个真实蜂蜜高光谱图像数据集上评估了多种CIL算法，并提出了一种结合持续反向传播（CB）的新方法。CB通过重新初始化部分隐藏神经元来增强神经网络的可塑性，实验证明其能将大多数CIL方法的性能提高1-7%。", "keywords": "类增量学习, 蜂蜜分类, 高光谱图像, 持续反向传播, 植物来源", "comments": "本研究的创新点在于提出了持续反向传播（CB）算法来改善类增量学习（CIL）中常见的“可塑性丧失”问题，并通过实验证明了其有效性。这对于实际应用中，特别是数据难以一次性获取的场景，具有重要的指导意义和实用价值。"}}
{"id": "2506.10800", "title": "Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints", "authors": ["Wei Sun", "Tingyu Qu", "Mingxiao Li", "Jesse Davis", "Marie-Francine Moens"], "summary": "Efficiently updating multilingual knowledge in large language models (LLMs),\nwhile preserving consistent factual representations across languages, remains a\nlong-standing and unresolved challenge. While deploying separate editing\nsystems for each language might seem viable, this approach incurs substantial\ncosts due to the need to manage multiple models. A more efficient solution\ninvolves integrating knowledge updates across all languages into a unified\nmodel. However, performing sequential edits across languages often leads to\ndestructive parameter interference, significantly degrading multilingual\ngeneralization and the accuracy of injected knowledge. To address this\nchallenge, we propose LangEdit, a novel null-space constrained framework\ndesigned to precisely isolate language-specific knowledge updates. The core\ninnovation of LangEdit lies in its ability to project parameter updates for\neach language onto the orthogonal complement of previous updated subspaces.\nThis approach mathematically guarantees update independence while preserving\nmultilingual generalization capabilities. We conduct a comprehensive evaluation\nacross three model architectures, six languages, and four downstream tasks,\ndemonstrating that LangEdit effectively mitigates parameter interference and\noutperforms existing state-of-the-art editing methods. Our results highlight\nits potential for enabling efficient and accurate multilingual knowledge\nupdates in LLMs. The code is available at\nhttps://github.com/VRCMF/LangEdit.git.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10800v1", "AI": {"title_translation": "通过零空间约束减轻多语言顺序知识编辑中的负面干扰", "tldr": "LangEdit通过零空间约束解决多语言LLM知识编辑中的负面干扰，实现高效、准确的更新并保持多语言泛化能力。", "motivation": "在大型语言模型（LLMs）中高效更新多语言知识，同时保持跨语言的事实表示一致性，是一个长期未解决的挑战。部署独立的编辑系统成本高昂；在统一模型中进行跨语言顺序编辑常导致破坏性参数干扰，显著降低多语言泛化能力和知识注入的准确性。", "method": "提出LangEdit框架，它是一种新颖的零空间约束方法，旨在精确隔离特定语言的知识更新。LangEdit的核心创新在于将每种语言的参数更新投射到先前更新子空间的“正交补空间”上，数学上保证了更新的独立性，同时保留了多语言泛化能力。", "result": "在三种模型架构、六种语言和四项下游任务上进行了全面评估，LangEdit有效减轻了参数干扰，并优于现有最先进的编辑方法。结果表明其在LLMs中实现高效准确的多语言知识更新的潜力。", "conclusion": "LangEdit通过零空间约束有效解决了多语言顺序知识编辑中的负面干扰问题，实现了高效准确的知识更新，同时保持了多语言泛化能力。", "translation": "在大型语言模型（LLMs）中高效更新多语言知识，同时保持跨语言的事实表示一致性，仍然是一个长期未解决的挑战。虽然为每种语言部署单独的编辑系统似乎可行，但这种方法由于需要管理多个模型而导致成本高昂。更有效的解决方案是将所有语言的知识更新集成到一个统一的模型中。然而，在语言之间进行顺序编辑通常会导致破坏性参数干扰，显著降低多语言泛化能力和注入知识的准确性。为了解决这一挑战，我们提出了LangEdit，一个新颖的零空间约束框架，旨在精确隔离特定语言的知识更新。LangEdit的核心创新在于它能够将每种语言的参数更新投射到先前更新子空间的正交补空间上。这种方法在数学上保证了更新的独立性，同时保留了多语言泛化能力。我们在三种模型架构、六种语言和四项下游任务上进行了全面评估，证明LangEdit有效减轻了参数干扰，并优于现有最先进的编辑方法。我们的结果突出了其在LLMs中实现高效准确的多语言知识更新的潜力。代码可在https://github.com/VRCMF/LangEdit.git获取。", "summary": "本论文提出LangEdit，一个基于零空间约束的新型框架，旨在解决大型语言模型中多语言顺序知识编辑导致的负面参数干扰问题。LangEdit通过将特定语言的参数更新投射到先前更新子空间的正交补空间，确保更新独立性并保持多语言泛化能力。实验证明LangEdit有效减轻了干扰，并优于现有SOTA方法，为LLMs的高效多语言知识更新提供了解决方案。", "keywords": "多语言知识编辑, 零空间约束, LLMs, 参数干扰, LangEdit", "comments": "LangEdit的创新点在于引入零空间约束来解决多语言知识编辑中的负面干扰，通过数学保证更新独立性，这对于维护LLMs跨语言知识一致性和泛化能力至关重要。其方法具有理论支撑，并在多方面进行了验证，显示出很高的实用价值和潜力。"}}
{"id": "2506.10503", "title": "Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation", "authors": ["Shuyang Li", "Shuang Wang", "Zhuangzhuang Sun", "Jing Xiao"], "summary": "The Reference Remote Sensing Image Segmentation (RRSIS) task generates\nsegmentation masks for specified objects in images based on textual\ndescriptions, which has attracted widespread attention and research interest.\nCurrent RRSIS methods rely on multi-modal fusion backbones and semantic\nsegmentation heads but face challenges like dense annotation requirements and\ncomplex scene interpretation. To address these issues, we propose a framework\nnamed \\textit{prompt-generated semantic localization guiding Segment Anything\nModel}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse\nlocalization and fine segmentation. In coarse localization stage, a visual\ngrounding network roughly locates the text-described object. In fine\nsegmentation stage, the coordinates from the first stage guide the Segment\nAnything Model (SAM), enhanced by a clustering-based foreground point generator\nand a mask boundary iterative optimization strategy for precise segmentation.\nNotably, the second stage can be train-free, significantly reducing the\nannotation data burden for the RRSIS task. Additionally, decomposing the RRSIS\ntask into two stages allows for focusing on specific region segmentation,\navoiding interference from complex scenes.We further contribute a high-quality,\nmulti-category manually annotated dataset. Experimental validation on two\ndatasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant\nperformance improvements and surpasses existing state-of-the-art models.Our\ncode will be made publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10503v1", "AI": {"title_translation": "语义定位引导的Segment Anything模型用于参考遥感图像分割", "tldr": "PSLG-SAM将参考遥感图像分割任务分解为粗定位和精细分割两阶段，利用SAM进行精确分割，显著减少了标注负担并提升了性能。", "motivation": "当前的参考遥感图像分割（RRSIS）方法依赖多模态融合骨干和语义分割头，但面临密集的标注需求和复杂场景解释的挑战。", "method": "本文提出了PSLG-SAM框架，将RRSIS任务分解为两个阶段：粗定位和精细分割。粗定位阶段使用视觉定位网络粗略定位文本描述的对象。精细分割阶段利用粗定位的坐标引导Segment Anything Model (SAM)，并通过基于聚类的前景点生成器和掩膜边界迭代优化策略进行增强。第二阶段可免训练，显著减少标注数据负担。同时，提出高质量多类别手动标注数据集。", "result": "在RRSIS-D和RRSIS-M两个数据集上的实验验证表明，PSLG-SAM实现了显著的性能提升，并超越了现有的最先进模型。", "conclusion": "本文提出的PSLG-SAM框架通过两阶段分解和引入SAM，有效解决了RRSIS任务中密集标注和复杂场景解释的挑战，显著提升了分割性能。", "translation": "参考遥感图像分割（RRSIS）任务根据文本描述为图像中的指定对象生成分割掩膜，这引起了广泛的关注和研究兴趣。当前的RRSIS方法依赖多模态融合骨干和语义分割头，但面临密集的标注需求和复杂场景解释的挑战。为了解决这些问题，我们提出了一个名为“提示生成语义定位引导的Segment Anything模型”（PSLG-SAM）的框架，它将RRSIS任务分解为两个阶段：粗定位和精细分割。在粗定位阶段，一个视觉定位网络粗略定位文本描述的对象。在精细分割阶段，第一阶段的坐标引导Segment Anything模型（SAM），并通过基于聚类的前景点生成器和掩膜边界迭代优化策略进行增强，以实现精确分割。值得注意的是，第二阶段可以是免训练的，显著减少了RRSIS任务的标注数据负担。此外，将RRSIS任务分解为两个阶段允许专注于特定区域分割，避免复杂场景的干扰。我们还贡献了一个高质量、多类别的手动标注数据集。在两个数据集（RRSIS-D和RRSIS-M）上的实验验证表明，PSLG-SAM实现了显著的性能改进，并超越了现有的最先进模型。我们的代码将公开可用。", "summary": "本文提出了PSLG-SAM框架，用于解决参考遥感图像分割（RRSIS）任务中密集标注和复杂场景解释的挑战。该框架将RRSIS分解为粗定位和精细分割两阶段：首先通过视觉定位网络进行粗定位，然后利用粗定位坐标引导Segment Anything Model (SAM)进行精细分割，并辅以前景点生成和边界优化策略。PSLG-SAM的第二阶段可免训练，显著降低了标注成本。实验证明，PSLG-SAM在RRSIS-D和RRSIS-M数据集上性能优于现有SOTA模型。", "keywords": "遥感图像分割, Segment Anything Model, 语义定位, 两阶段分割, 低标注", "comments": "PSLG-SAM的创新之处在于其两阶段分解策略，特别是将免训练的Segment Anything Model (SAM)引入精细分割阶段，这显著解决了遥感图像分割中长期存在的标注数据密集需求问题。通过聚焦特定区域分割，也有效避免了复杂场景的干扰，提升了分割精度和效率。所贡献的高质量数据集也对该领域的研究具有重要意义。"}}
{"id": "2506.10378", "title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning", "authors": ["Jikai Jin", "Vasilis Syrgkanis", "Sham Kakade", "Hanlin Zhang"], "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10378v1", "AI": {"title_translation": "通过因果表征学习发现语言模型的层次化潜在能力", "tldr": "本文提出了一个因果表征学习框架，用于克服语言模型评估中的挑战，揭示了语言模型能力（通用问题解决、指令遵循、数学推理）的层次化因果结构。", "motivation": "忠实地评估语言模型能力对模型开发至关重要，但严格的因果评估面临复杂混淆效应和高计算成本等方法学挑战。", "method": "提出了一种因果表征学习框架，将观察到的基准性能建模为少数潜在能力因素的线性变换。通过适当控制基础模型作为常见混淆因素，识别这些潜在因素之间的因果关系。", "result": "应用该方法于超过1500个模型和六个基准数据集，识别出一个简洁的三节点线性因果结构，解释了性能差异。该结构揭示了从通用问题解决能力到指令遵循能力再到数学推理能力的明确因果方向。", "conclusion": "研究结果强调了在评估过程中仔细控制基础模型变异的重要性，这对于准确揭示潜在模型能力之间的因果关系至关重要。", "translation": "语言模型能力的忠实评估对于获取可指导模型开发的实用见解至关重要。然而，该领域严格的因果评估面临重大的方法学挑战，包括复杂的混淆效应和与大量再训练相关的过高计算成本。为了解决这些挑战，我们提出了一种因果表征学习框架，其中观察到的基准性能被建模为少数潜在能力因素的线性变换。至关重要的是，在适当控制基础模型作为常见混淆因素后，这些潜在因素被识别为因果相关。将这种方法应用于一个包含来自Open LLM排行榜的六个基准上评估的1500多个模型的综合数据集，我们识别出一个简洁的三节点线性因果结构，该结构可靠地解释了观察到的性能变异。对该因果结构的进一步解释提供了超越简单数值排名的实质性科学见解：具体而言，我们揭示了一个清晰的因果方向，从通用问题解决能力开始，通过指令遵循熟练度发展，最终达到数学推理能力。我们的结果强调了在评估过程中仔细控制基础模型变异的基本作用，这是准确揭示潜在模型能力之间潜在因果关系的关键一步。", "summary": "本文提出了一种新颖的因果表征学习框架，旨在解决语言模型能力评估中存在的混淆和计算成本问题。该框架将模型性能视为少数潜在能力因素的线性组合，并通过控制基础模型来识别这些因素间的因果关系。研究人员将此方法应用于Open LLM排行榜的1500多个模型，成功发现了一个三节点的层次化因果结构，揭示了从通用问题解决到指令遵循再到数学推理的能力演进路径。这强调了在评估时控制基础模型变异的重要性，以准确理解模型能力的内在因果联系。", "keywords": "语言模型, 因果表征学习, 能力评估, 层次结构, 混淆控制", "comments": "这项研究通过引入因果表征学习框架，为语言模型能力的评估提供了一种新颖且严谨的方法，克服了传统评估中混淆变量和计算成本高昂的挑战。其创新之处在于将潜在能力因素建模为因果关联的结构，并成功揭示了语言模型能力之间（如通用问题解决、指令遵循和数学推理）的层次化因果方向，这为深入理解和指导语言模型发展提供了重要的科学见解，超越了简单的性能排名。"}}
{"id": "2506.10822", "title": "ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization", "authors": ["Zhensheng Jin", "Xinze Li", "Yifan Ji", "Chunyi Peng", "Zhenghao Liu", "Qi Shi", "Yukun Yan", "Shuo Wang", "Furong Peng", "Ge Yu"], "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nimproved the reasoning capabilities of Large Language Models (LLMs). However,\nthese methods often suffer from overthinking, leading to unnecessarily lengthy\nor redundant reasoning traces. Existing approaches attempt to mitigate this\nissue through curating multiple reasoning chains for training LLMs, but their\neffectiveness is often constrained by the quality of the generated data and\nprone to overfitting. To address the challenge, we propose Reasoning\nCompression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing\nthe accuracy and length of reasoning trajectory. Specifically, ReCUT employs a\nstepwise exploration mechanism and a long-short switched sampling strategy,\nenabling LLMs to incrementally generate diverse reasoning paths. These paths\nare evaluated and used to construct preference pairs to train two specialized\nmodels (Gemini LLMs)-one optimized for reasoning accuracy, the other for\nshorter reasoning. A final integrated model is obtained by interpolating the\nparameters of these two models. Experimental results across multiple math\nreasoning datasets and backbone models demonstrate that ReCUT significantly\nreduces reasoning lengths by approximately 30-50%, while maintaining or\nimproving reasoning accuracy compared to various baselines. All codes and data\nwill be released via https://github.com/NEUIR/ReCUT.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10822v1", "AI": {"title_translation": "ReCUT：通过逐步试探和偏好优化平衡LLM中的推理长度和准确性", "tldr": "ReCUT通过逐步探索和偏好优化，在保持或提高准确性的同时，显著缩短LLM的推理长度（30-50%），以解决CoT的过度思考问题。", "motivation": "现有的思维链（CoT）提示方法虽然提高了大型语言模型（LLMs）的推理能力，但常导致过度思考，产生不必要的冗长或冗余推理轨迹。现有缓解方法受限于生成数据质量且易过拟合。", "method": "本研究提出了推理压缩通过逐步试探（ReCUT）方法，旨在平衡推理轨迹的准确性和长度。ReCUT采用逐步探索机制和长短切换采样策略，使LLMs能够增量生成多样化的推理路径。这些路径被评估并用于构建偏好对，以训练两个专门模型（Gemini LLMs）——一个优化推理准确性，另一个优化更短推理。最终通过插值这两个模型的参数获得一个集成模型。", "result": "在多个数学推理数据集和骨干模型上的实验结果表明，ReCUT与各种基线相比，显著减少了约30-50%的推理长度，同时保持或提高了推理准确性。", "conclusion": "ReCUT成功地在LLM中平衡了推理长度和准确性，有效解决了CoT过度思考的问题，并提供了一种通过多模型协同和参数插值来优化推理过程的新范式。", "translation": "近期思维链（CoT）提示的进展大幅提升了大型语言模型（LLMs）的推理能力。然而，这些方法常因过度思考而导致不必要的冗长或冗余推理轨迹。现有方法试图通过整理多个推理链来训练LLMs以缓解此问题，但其有效性常受限于生成数据质量且易于过拟合。为解决这一挑战，我们提出了通过逐步试探进行推理压缩（ReCUT），这是一种旨在平衡推理轨迹准确性和长度的新颖方法。具体来说，ReCUT采用逐步探索机制和长短切换采样策略，使LLMs能够逐步生成多样化的推理路径。这些路径被评估并用于构建偏好对，以训练两个专门模型（Gemini LLMs）——一个针对推理准确性进行优化，另一个针对更短推理进行优化。通过对这两个模型的参数进行插值，获得最终的集成模型。在多个数学推理数据集和骨干模型上的实验结果表明，与各种基线相比，ReCUT显著减少了约30-50%的推理长度，同时保持或提高了推理准确性。所有代码和数据将通过https://github.com/NEUIR/ReCUT 发布。", "summary": "ReCUT是一种新颖的方法，旨在解决大型语言模型在思维链推理中过度思考导致推理轨迹过长的问题。它通过逐步探索和长短切换采样生成多样化推理路径，并利用偏好优化训练两个模型（一个注重准确性，一个注重简洁性），最终通过参数插值整合。实验证明，ReCUT在显著缩短推理长度的同时，能保持或提高推理准确性。", "keywords": "LLMs, 思维链, 推理长度, 推理准确性, 偏好优化", "comments": "ReCUT的创新之处在于其双目标优化策略，即同时关注推理的准确性和长度，并通过逐步探索和偏好优化训练专门模型并进行参数插值。这种方法有效缓解了CoT的“过度思考”问题，为LLM在实际应用中提供更高效、更简洁的推理能力，具有重要的实践意义。"}}
{"id": "2506.10505", "title": "J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft", "authors": ["Jin Huang", "Mingqiang Wei", "Zikuan Li", "Hangyu Qu", "Wei Zhao", "Xinyu Bai"], "summary": "Ensuring the safety and extended operational life of fighter aircraft\nnecessitates frequent and exhaustive inspections. While surface defect\ndetection is feasible for human inspectors, manual methods face critical\nlimitations in scalability, efficiency, and consistency due to the vast surface\narea, structural complexity, and operational demands of aircraft maintenance.\nWe propose a smart surface damage detection and localization system for fighter\naircraft, termed J-DDL. J-DDL integrates 2D images and 3D point clouds of the\nentire aircraft surface, captured using a combined system of laser scanners and\ncameras, to achieve precise damage detection and localization. Central to our\nsystem is a novel damage detection network built on the YOLO architecture,\nspecifically optimized for identifying surface defects in 2D aircraft images.\nKey innovations include lightweight Fasternet blocks for efficient feature\nextraction, an optimized neck architecture incorporating Efficient Multiscale\nAttention (EMA) modules for superior feature aggregation, and the introduction\nof a novel loss function, Inner-CIOU, to enhance detection accuracy. After\ndetecting damage in 2D images, the system maps the identified anomalies onto\ncorresponding 3D point clouds, enabling accurate 3D localization of defects\nacross the aircraft surface. Our J-DDL not only streamlines the inspection\nprocess but also ensures more comprehensive and detailed coverage of large and\ncomplex aircraft exteriors. To facilitate further advancements in this domain,\nwe have developed the first publicly available dataset specifically focused on\naircraft damage. Experimental evaluations validate the effectiveness of our\nframework, underscoring its potential to significantly advance automated\naircraft inspection technologies.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10505v1", "AI": {"title_translation": "J-DDL：战斗机表面损伤检测与定位系统", "tldr": "J-DDL是一个针对战斗机的智能表面损伤检测与定位系统，它结合2D图像和3D点云，并采用基于YOLO的优化网络，旨在提高飞机检查的效率和准确性。", "motivation": "确保战斗机的安全和延长其使用寿命需要频繁且彻底的检查。然而，由于飞机表面积巨大、结构复杂以及维护操作需求，人工检测方法在可扩展性、效率和一致性方面存在严重局限性。", "method": "J-DDL系统整合了激光扫描仪和相机捕获的整个飞机表面的2D图像和3D点云，以实现精确的损伤检测和定位。其核心是一个基于YOLO架构的新型损伤检测网络，专门优化用于识别2D飞机图像中的表面缺陷。关键创新包括用于高效特征提取的轻量级Fasternet模块、结合Efficient Multiscale Attention (EMA)模块的优化颈部架构用于卓越的特征聚合，以及引入新型Inner-CIOU损失函数以提高检测精度。在2D图像中检测到损伤后，系统将识别出的异常映射到相应的3D点云上，从而实现飞机表面缺陷的精确3D定位。", "result": "J-DDL不仅简化了检查流程，还确保了对大型复杂飞机外部更全面和详细的覆盖。研究人员开发了第一个公开可用的飞机损伤数据集。实验评估验证了该框架的有效性。", "conclusion": "J-DDL系统显著推动了自动化飞机检测技术的发展，提供了更高效、全面和精确的表面损伤检测与定位方法。", "translation": "确保战斗机的安全和延长其使用寿命需要频繁且彻底的检查。虽然人工检查员可以进行表面缺陷检测，但由于飞机表面积巨大、结构复杂以及维护操作需求，手动方法在可扩展性、效率和一致性方面面临严重限制。我们提出了一种针对战斗机的智能表面损伤检测与定位系统，命名为J-DDL。J-DDL整合了使用激光扫描仪和相机组合系统捕获的整个飞机表面的2D图像和3D点云，以实现精确的损伤检测和定位。我们系统的核心是一个基于YOLO架构的新型损伤检测网络，专门优化用于识别2D飞机图像中的表面缺陷。关键创新包括用于高效特征提取的轻量级Fasternet模块、结合Efficient Multiscale Attention (EMA)模块的优化颈部架构用于卓越的特征聚合，以及引入新型Inner-CIOU损失函数以提高检测精度。在2D图像中检测到损伤后，系统将识别出的异常映射到相应的3D点云上，从而实现飞机表面缺陷在飞机整个表面的精确3D定位。我们的J-DDL不仅简化了检查流程，还确保了对大型复杂飞机外部更全面和详细的覆盖。为了促进该领域的进一步发展，我们开发了第一个专门针对飞机损伤的公开可用数据集。实验评估验证了我们框架的有效性，强调了其在显著推进自动化飞机检测技术方面的潜力。", "summary": "J-DDL是一种为战斗机设计的智能表面损伤检测与定位系统，旨在解决传统人工检测在效率和精度上的局限。该系统结合2D图像和3D点云数据，并采用基于YOLO的创新性深度学习网络，该网络集成了Fasternet模块、EMA模块和Inner-CIOU损失函数以优化检测性能。J-DDL能够实现损伤的精确2D检测和3D定位，从而简化了飞机检查流程并提升了覆盖范围。此外，研究团队还发布了首个公开可用的飞机损伤数据集，并通过实验验证了该框架的有效性，表明其在自动化飞机检测领域具有巨大潜力。", "keywords": "飞机损伤检测, 3D定位, YOLO, 深度学习, 自动化检测", "comments": "该论文提出了一种创新的战斗机表面损伤检测与定位系统J-DDL，其主要创新点在于结合了2D图像和3D点云进行多模态检测，并对基于YOLO的检测网络进行了深度优化，包括引入轻量级Fasternet、EMA模块和Inner-CIOU损失函数，旨在提高检测效率和精度。此外，首次发布公开的飞机损伤数据集对于该领域的研究具有重要推动作用，填补了数据空白。该系统对于提高飞机维护的自动化水平和保障飞行安全具有重要意义。"}}
{"id": "2506.10389", "title": "EQA-RM: A Generative Embodied Reward Model with Test-time Scaling", "authors": ["Yuhang Chen", "Zhen Tan", "Tianlong Chen"], "summary": "Reward Models (RMs), vital for large model alignment, are underexplored for\ncomplex embodied tasks like Embodied Question Answering (EQA) where nuanced\nevaluation of agents' spatial, temporal, and logical understanding is critical\nyet not considered by generic approaches. We introduce EQA-RM, a novel\ngenerative multimodal reward model specifically architected for EQA, trained\nvia our innovative Contrastive Group Relative Policy Optimization (C-GRPO)\nstrategy to learn fine-grained behavioral distinctions. The generative nature\nof EQA-RM provides interpretable, structured reward feedback (beyond simple\nscalars), uniquely enabling test-time scaling to dynamically adjust evaluation\ngranularity, from concise scores to detailed critiques of reasoning and\ngrounding, at inference without retraining. Concurrently, we introduce\nEQARewardBench, a new benchmark built on OpenEQA for standardized EQA reward\nmodel assessment. Demonstrating high sample efficiency, EQA-RM (fine-tuning\nQwen2-VL-2B-Instruct) achieves 61.9\\% accuracy on EQA-RM-Bench with only 700\nsamples, outperforming strong proprietary baselines, including\nGemini-2.5-Flash, GPT-4o, Claude-3.5-Haiku, and open-sourced state-of-the-art\nmodels such as RoVRM and VisualPRM. The code and dataset can be found here\nhttps://github.com/UNITES-Lab/EQA-RM.", "comment": "preprint", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10389v1", "AI": {"title_translation": "EQA-RM：一种具有测试时可伸缩性的生成式具身奖励模型", "tldr": "EQA-RM是一个新的生成式具身奖励模型，专为具身问答设计，通过C-GRPO策略训练，实现了测试时评估粒度调整，并在新基准EQARewardBench上表现优异。", "motivation": "现有的奖励模型在复杂的具身任务（如具身问答EQA）中未被充分探索，无法有效评估代理在空间、时间、逻辑理解方面的细微差别，而通用方法未能考虑到这些关键评估点。", "method": "本文提出了EQA-RM，一个新型生成式多模态奖励模型，专为具身问答（EQA）设计。它通过创新的对比群组相对策略优化（C-GRPO）策略进行训练，以学习细粒度的行为区分。EQA-RM的生成特性提供了可解释的、结构化的奖励反馈，并独有地支持测试时动态调整评估粒度，无需重新训练即可从简洁分数到详细推理和基础批判。此外，本文还引入了EQARewardBench，一个基于OpenEQA构建的新基准，用于标准化EQA奖励模型评估。", "result": "EQA-RM（通过微调Qwen2-VL-2B-Instruct）表现出高样本效率，仅用700个样本就在EQARewardBench上实现了61.9%的准确率，性能优于包括Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku等强大的专有基线模型，以及RoVRM和VisualPRM等开源SOTA模型。", "conclusion": "EQA-RM通过其生成特性和C-GRPO训练策略，有效解决了具身任务中奖励模型评估的挑战，并在具身问答任务上取得了显著的性能提升，为未来具身智能体的评估提供了新的范式和基准。", "translation": "奖励模型（RMs）对于大型模型对齐至关重要，但在复杂的具身任务（如具身问答EQA）中尚未得到充分探索，在这些任务中，对代理的空间、时间、逻辑理解进行细致评估至关重要，但通用方法并未考虑到这一点。我们引入了EQA-RM，这是一种新颖的生成式多模态奖励模型，专门为EQA设计，通过我们创新的对比群组相对策略优化（C-GRPO）策略进行训练，以学习细粒度的行为区分。EQA-RM的生成特性提供了可解释的、结构化的奖励反馈（超越简单的标量），独特地实现了测试时可伸缩性，可以在推理时动态调整评估粒度，从简洁分数到对推理和基础的详细批判，而无需重新训练。同时，我们引入了EQARewardBench，这是一个基于OpenEQA构建的新基准，用于标准化EQA奖励模型评估。EQA-RM（微调Qwen2-VL-2B-Instruct）展示了高样本效率，仅用700个样本就在EQA-RM-Bench上取得了61.9%的准确率，优于包括Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku在内的强大专有基线模型，以及RoVRM和VisualPRM等开源SOTA模型。代码和数据集可在此处找到：https://github.com/UNITES-Lab/EQA-RM。", "summary": "本文提出了EQA-RM，一个专为具身问答（EQA）设计的生成式多模态奖励模型。该模型通过创新的对比群组相对策略优化（C-GRPO）策略训练，能够提供可解释的结构化奖励反馈，并支持测试时动态调整评估粒度。研究还引入了EQARewardBench作为新的标准化评估基准。实验结果表明，EQA-RM在仅使用少量样本的情况下，在EQARewardBench上取得了显著优于现有专有和开源先进模型的性能。", "keywords": "具身问答, 奖励模型, 生成式模型, 测试时可伸缩性, 对比学习", "comments": "EQA-RM的创新之处在于其生成式特性，使得奖励反馈不仅是简单的标量，而且是结构化和可解释的，这对于复杂具身任务的细致评估至关重要。此外，测试时评估粒度的动态调整能力是其独特优势，极大地提高了模型评估的灵活性和效率。引入EQARewardBench也填补了EQA领域标准化奖励模型评估的空白，对社区具有重要贡献。"}}
{"id": "2506.10516", "title": "CogStream: Context-guided Streaming Video Question Answering", "authors": ["Zicheng Zhao", "Kangyu Wang", "Shijie Li", "Rui Qian", "Weiyao Lin", "Huabin Liu"], "summary": "Despite advancements in Video Large Language Models (Vid-LLMs) improving\nmultimodal understanding, challenges persist in streaming video reasoning due\nto its reliance on contextual information. Existing paradigms feed all\navailable historical contextual information into Vid-LLMs, resulting in a\nsignificant computational burden for visual data processing. Furthermore, the\ninclusion of irrelevant context distracts models from key details. This paper\nintroduces a challenging task called Context-guided Streaming Video Reasoning\n(CogStream), which simulates real-world streaming video scenarios, requiring\nmodels to identify the most relevant historical contextual information to\ndeduce answers for questions about the current stream. To support CogStream, we\npresent a densely annotated dataset featuring extensive and hierarchical\nquestion-answer pairs, generated by a semi-automatic pipeline. Additionally, we\npresent CogReasoner as a baseline model. It efficiently tackles this task by\nleveraging visual stream compression and historical dialogue retrieval.\nExtensive experiments prove the effectiveness of this method. Code will be\nreleased soon.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10516v1", "AI": {"title_translation": "CogStream: 上下文引导的流式视频问答", "tldr": "本文提出了一个名为CogStream的新任务，用于解决流式视频推理中上下文信息过载的问题，并引入了一个新数据集和基线模型CogReasoner，该模型通过视觉流压缩和历史对话检索有效处理相关上下文。", "motivation": "尽管视频大型语言模型（Vid-LLMs）在多模态理解方面有所进步，但在流式视频推理中，由于对上下文信息的依赖，依然存在挑战。现有范式将所有可用的历史上下文信息输入Vid-LLMs，导致视觉数据处理的计算负担过重，并且无关上下文会分散模型对关键细节的注意力。", "method": "本文提出了一个名为“上下文引导的流式视频推理”（CogStream）的挑战性任务，模拟真实世界的流式视频场景，要求模型识别最相关的历史上下文信息以推断当前流的问题答案。为支持CogStream，论文提出了一个通过半自动管道生成的大量且分层的问题-答案对的密集标注数据集。此外，还提出了CogReasoner作为基线模型，该模型通过利用视觉流压缩和历史对话检索来高效处理此任务。", "result": "广泛的实验证明了该方法的有效性。", "conclusion": "本文成功定义并解决了流式视频推理中上下文信息过载的挑战，通过引入新的任务、数据集和有效的基线模型CogReasoner，为未来高效的流式视频问答提供了可行的方向。", "translation": "尽管视频大型语言模型（Vid-LLMs）在多模态理解方面有所进步，但在流式视频推理中，由于对上下文信息的依赖，依然存在挑战。现有范式将所有可用的历史上下文信息输入Vid-LLMs，导致视觉数据处理的计算负担过重。此外，包含无关上下文会分散模型对关键细节的注意力。本文引入了一个名为“上下文引导的流式视频推理”（CogStream）的挑战性任务，该任务模拟真实世界的流式视频场景，要求模型识别最相关的历史上下文信息以推断当前流的问题答案。为支持CogStream，我们提出了一个通过半自动管道生成的大量且分层的问题-答案对的密集标注数据集。此外，我们提出了CogReasoner作为基线模型。它通过利用视觉流压缩和历史对话检索来高效处理此任务。广泛的实验证明了该方法的有效性。代码即将发布。", "summary": "本文针对视频大型语言模型（Vid-LLMs）在流式视频推理中面临的上下文信息过载和无关信息干扰问题，提出了一项名为“上下文引导的流式视频推理”（CogStream）的新任务。该任务旨在模拟真实场景，要求模型识别并利用最相关的历史上下文信息进行问答。为支持此任务，研究团队构建了一个包含大量分层问答对的密集标注数据集，并开发了基线模型CogReasoner，其通过视觉流压缩和历史对话检索机制来高效处理上下文。实验结果证明了该方法的有效性。", "keywords": "流式视频问答, 上下文引导, 视频大型语言模型, CogStream, CogReasoner", "comments": "本文创新性地提出了“上下文引导的流式视频推理”这一新任务，旨在解决现有Vid-LLMs在处理流式视频时面临的上下文冗余和信息过载问题。通过引入选择性上下文检索机制，该方法有望显著提升流式视频问答的效率和准确性，具有重要的实际应用价值。数据集的构建和基线模型的提出为该领域的研究奠定了基础。"}}
{"id": "2506.10403", "title": "Time To Impeach LLM-as-a-Judge: Programs are the Future of Evaluation", "authors": ["Tzu-Heng Huang", "Harit Vishwakarma", "Frederic Sala"], "summary": "Large language models (LLMs) are widely used to evaluate the quality of LLM\ngenerations and responses, but this leads to significant challenges: high API\ncosts, uncertain reliability, inflexible pipelines, and inherent biases. To\naddress these, we introduce PAJAMA (Program-As-a-Judge for Automated Model\nAssessment), a new alternative that uses LLMs to synthesize executable judging\nprograms instead of directly scoring responses. These synthesized programs can\nbe stored and run locally, costing orders of magnitude less while providing\ninterpretable, and auditable judging logic that can be easily adapted.\nProgram-based judges mitigate biases, improving judgment consistency by 15.83%\nand reducing biased responses by 23.7% on average compared to a\nQwen2.5-14B-based LLM-as-a-judge. When program judgments are distilled into a\nmodel, PAJAMA outperforms LLM-as-a-judge on the challenging CHAT-HARD subset of\nRewardBench, outperforming metrics by 2.19% on Prometheus and 8.67% on the\nJudgeLM dataset, all at three orders of magnitude lower cost.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10403v1", "AI": {"title_translation": "是时候弹劾LLM-as-a-Judge了：程序是评估的未来", "tldr": "PAJAMA提出使用LLM生成可执行程序进行模型评估，替代直接使用LLM作为评判器，显著降低成本并提高评估一致性和可解释性。", "motivation": "大型语言模型（LLM）作为评估器（LLM-as-a-Judge）存在高API成本、可靠性不确定、管道不灵活和固有偏见等显著挑战。", "method": "引入PAJAMA（Program-As-a-Judge for Automated Model Assessment），该方法使用LLM合成可执行的评判程序，这些程序可以本地存储和运行，提供可解释、可审计的评判逻辑，并且易于调整。", "result": "PAJAMA将评估成本降低了几个数量级；评判一致性平均提高了15.83%；偏见响应平均减少了23.7%。当程序判断结果被提炼到一个模型中时，PAJAMA在RewardBench的CHAT-HARD子集上，在Prometheus上性能指标提高了2.19%，在JudgeLM数据集上提高了8.67%。", "conclusion": "PAJAMA通过程序化评估克服了LLM-as-a-Judge的局限性，实现了更经济、更可靠、更可解释和更高性能的模型评估。", "translation": "大型语言模型（LLM）被广泛用于评估LLM生成内容和响应的质量，但这导致了显著的挑战：高昂的API成本、不确定的可靠性、不灵活的管道以及固有的偏见。为了解决这些问题，我们引入了PAJAMA（Program-As-a-Judge for Automated Model Assessment），这是一种新的替代方案，它使用LLM合成可执行的评判程序，而不是直接对响应进行评分。这些合成的程序可以本地存储和运行，成本降低了几个数量级，同时提供了可解释、可审计的评判逻辑，并且易于调整。基于程序的评判器减轻了偏见，与基于Qwen2.5-14B的LLM-as-a-judge相比，评判一致性平均提高了15.83%，偏见响应减少了23.7%。当程序判断结果被提炼到一个模型中时，PAJAMA在RewardBench具有挑战性的CHAT-HARD子集上优于LLM-as-a-judge，在Prometheus上性能指标提高了2.19%，在JudgeLM数据集上提高了8.67%，所有这些都以低三个数量级的成本实现。", "summary": "本文提出了PAJAMA，一种新颖的模型评估方法，通过让大型语言模型（LLM）生成可执行的评判程序来替代直接使用LLM作为评判器。PAJAMA解决了LLM-as-a-Judge存在的成本高昂、可靠性差、灵活性低和偏见等问题。实验结果表明，PAJAMA显著降低了评估成本，提高了评判一致性并减少了偏见，并在多个评估基准上展现出优越的性能。", "keywords": "LLM评估, 程序生成, PAJAMA, 模型评估, 偏见缓解", "comments": "这篇论文提出了一种创新的LLM评估范式，从“LLM即评判者”转向“LLM生成评判程序”，巧妙地利用了LLM的代码生成能力来克服其作为直接评判者的局限性。其核心创新在于将评估逻辑从黑盒LLM内部转移到可审计、可执行的程序中，这极大地增强了评估过程的透明度、可控性和成本效益。这种方法对于构建更可靠、更公平的LLM评估体系具有重要意义，尤其是在需要大规模、自动化评估的场景下。"}}
{"id": "2506.10848", "title": "Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles", "authors": ["Qingyan Wei", "Yaojie Zhang", "Zhiyuan Liu", "Dongrui Liu", "Linfeng Zhang"], "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.", "comment": "11 pages; 5 figures;", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10848v1", "AI": {"title_translation": "使用SlowFast加速扩散大语言模型：三大黄金原则", "tldr": "本文提出了一种名为SlowFast采样的新型动态采样策略，用于加速扩散大语言模型（dLLMs），通过自适应地在探索性和加速解码阶段之间切换，并遵循三大黄金原则，实现了显著的推理加速。", "motivation": "扩散大语言模型（dLLMs）虽然通过并行生成token降低了推理延迟，但现有采样策略（如基于置信度或半自回归解码）通常存在静态行为，导致效率低下和灵活性受限。", "method": "本文提出了SlowFast采样，一种新颖的动态采样策略，它在探索性和加速解码阶段之间自适应地交替。该方法遵循三大黄金原则：确定性原则、收敛性原则和位置性原则，这些原则指导何时何地可以自信且高效地解码token。此外，该策略与dLLM-Cache集成以减少冗余计算。", "result": "SlowFast采样在LLaDA上实现了高达15.63倍的加速，且准确率下降最小；与缓存结合时，加速比高达34.22倍。该方法在吞吐量上优于强大的自回归基线（如LLaMA3 8B）。", "conclusion": "精心设计的采样策略可以充分发挥扩散大语言模型在快速高质量生成方面的潜力。", "translation": "基于扩散的语言模型（dLLMs）通过实现并行token生成并显著降低推理延迟，已成为传统自回归LLM的一种有前景的替代方案。然而，现有dLLM的采样策略，例如基于置信度或半自回归解码，通常存在静态行为，导致效率低下和灵活性有限。在本文中，我们提出了SlowFast采样，一种新颖的动态采样策略，它自适应地在探索性和加速解码阶段之间交替。我们的方法遵循三大黄金原则：确定性原则、收敛性原则和位置性原则，它们决定了何时何地可以自信且高效地解码token。我们进一步将我们的策略与dLLM-Cache集成，以减少冗余计算。在基准和模型上的大量实验表明，SlowFast采样在LLaDA上实现了高达15.63倍的加速，且准确率下降最小，与缓存结合时高达34.22倍。值得注意的是，我们的方法在吞吐量上优于强大的自回归基线，如LLaMA3 8B，这表明精心设计的采样可以充分发挥dLLM在快速高质量生成方面的潜力。", "summary": "本文提出了一种名为SlowFast采样的新型动态采样策略，旨在加速扩散大语言模型（dLLMs）的推理过程。该策略通过自适应地在探索性与加速解码阶段之间切换，并遵循确定性、收敛性和位置性三大黄金原则，以优化token的解码效率和置信度。此外，它还与dLLM-Cache结合以减少计算冗余。实验结果表明，SlowFast采样在LLaDA上实现了显著的加速（最高15.63倍，结合缓存最高34.22倍），且性能优于现有自回归模型，证实了其在实现快速高质量生成方面的潜力。", "keywords": "扩散大语言模型, 采样策略, SlowFast, 推理加速, 动态解码", "comments": "该论文提出了一种创新的动态采样策略SlowFast，有效解决了扩散大语言模型现有采样策略的效率和灵活性问题。其引入的三大黄金原则为动态解码提供了理论基础，并结合缓存机制进一步提升了性能。实验结果显示出非常显著的加速效果，甚至超越了强大的自回归模型，这对于推动dLLMs的实际应用具有重要意义。"}}
{"id": "2506.10524", "title": "ALBERT: Advanced Localization and Bidirectional Encoder Representations from Transformers for Automotive Damage Evaluation", "authors": ["Teerapong Panboonyuen"], "summary": "This paper introduces ALBERT, an instance segmentation model specifically\ndesigned for comprehensive car damage and part segmentation. Leveraging the\npower of Bidirectional Encoder Representations, ALBERT incorporates advanced\nlocalization mechanisms to accurately identify and differentiate between real\nand fake damages, as well as segment individual car parts. The model is trained\non a large-scale, richly annotated automotive dataset that categorizes damage\ninto 26 types, identifies 7 fake damage variants, and segments 61 distinct car\nparts. Our approach demonstrates strong performance in both segmentation\naccuracy and damage classification, paving the way for intelligent automotive\ninspection and assessment applications.", "comment": "10 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10524v1", "AI": {"title_translation": "ALBERT：用于汽车损伤评估的先进定位和双向编码器表示转换器", "tldr": "ALBERT是一个实例分割模型，专门用于汽车损伤和零件分割，能够准确识别真假损伤并分割汽车零件，并在大规模数据集上表现出色。", "motivation": "为了实现智能汽车检测和评估应用，需要一个能够准确识别和区分汽车损伤（包括真假损伤）以及分割汽车零件的模型。", "method": "本论文介绍了ALBERT，一个利用双向编码器表示的实例分割模型。它融入了先进的定位机制，旨在准确识别和区分真假损伤，并分割单独的汽车零件。该模型在一个大规模、丰富标注的汽车数据集上进行训练，该数据集将损伤分为26种类型，识别7种假损伤变体，并分割61个不同的汽车零件。", "result": "ALBERT在分割准确性和损伤分类方面都表现出强大的性能。", "conclusion": "ALBERT模型为智能汽车检测和评估应用奠定了基础。", "translation": "本文介绍了ALBERT，一个专门为综合性汽车损伤和零件分割设计的实例分割模型。ALBERT利用双向编码器表示的强大功能，结合了先进的定位机制，以准确识别和区分真实和虚假损伤，并分割单个汽车零件。该模型在一个大规模、标注丰富的汽车数据集上进行训练，该数据集将损伤分为26种类型，识别7种虚假损伤变体，并分割61个不同的汽车零件。我们的方法在分割准确性和损伤分类方面均表现出强大的性能，为智能汽车检测和评估应用铺平了道路。", "summary": "ALBERT是一个创新的实例分割模型，专为汽车损伤和零件的综合分割而设计。它利用双向编码器表示和先进的定位机制，能够精确区分真实与虚假损伤，并细致地分割汽车各部件。该模型在一个包含26种损伤类型、7种假损伤变体和61个汽车零件的大规模数据集上进行训练，在分割精度和损伤分类方面均展现出卓越的性能，为智能汽车检测和评估提供了新的解决方案。", "keywords": "实例分割, 汽车损伤评估, 双向编码器表示, 损伤分类, 汽车零件分割", "comments": "ALBERT的创新之处在于其专门针对汽车损伤评估的实例分割能力，特别是能够区分真实与虚假损伤，这在实际应用中具有重要意义。其结合双向编码器表示和先进定位机制的方法，以及在大型、细致标注数据集上的训练，使其在性能上表现出色，为智能汽车检测和评估领域带来了显著进步。"}}
{"id": "2506.10404", "title": "Generative Algorithms for Wildfire Progression Reconstruction from Multi-Modal Satellite Active Fire Measurements and Terrain Height", "authors": ["Bryan Shaddy", "Brianna Binder", "Agnimitra Dasgupta", "Haitong Qin", "James Haley", "Angel Farguell", "Kyle Hilburn", "Derek V. Mallia", "Adam Kochanski", "Jan Mandel", "Assad Oberai"], "summary": "Increasing wildfire occurrence has spurred growing interest in wildfire\nspread prediction. However, even the most complex wildfire models diverge from\nobserved progression during multi-day simulations, motivating need for data\nassimilation. A useful approach to assimilating measurement data into complex\ncoupled atmosphere-wildfire models is to estimate wildfire progression from\nmeasurements and use this progression to develop a matching atmospheric state.\nIn this study, an approach is developed for estimating fire progression from\nVIIRS active fire measurements, GOES-derived ignition times, and terrain height\ndata. A conditional Generative Adversarial Network is trained with simulations\nof historic wildfires from the atmosphere-wildfire model WRF-SFIRE, thus\nallowing incorporation of WRF-SFIRE physics into estimates. Fire progression is\nsuccinctly represented by fire arrival time, and measurements for training are\nobtained by applying an approximate observation operator to WRF-SFIRE\nsolutions, eliminating need for satellite data during training. The model is\ntrained on tuples of fire arrival times, measurements, and terrain, and once\ntrained leverages measurements of real fires and corresponding terrain data to\ngenerate samples of fire arrival times. The approach is validated on five\nPacific US wildfires, with results compared against high-resolution perimeters\nmeasured via aircraft, finding an average Sorensen-Dice coefficient of 0.81.\nThe influence of terrain height on the arrival time inference is also evaluated\nand it is observed that terrain has minimal influence when the inference is\nconditioned on satellite measurements.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10404v1", "AI": {"title_translation": "利用多模态卫星活跃火灾测量和地形高度重建野火进展的生成算法", "tldr": "本研究开发了一种生成算法，利用多模态卫星数据和地形高度重建野火进展，通过条件生成对抗网络（cGAN）训练，并在实际野火中取得了0.81的Dice系数，提高了野火模型的预测能力。", "motivation": "为了解决最复杂的野火模型在多日模拟中与实际观测到的进展存在差异的问题，从而需要进行数据同化。", "method": "本研究开发了一种利用条件生成对抗网络（cGAN）来估计火灾进展的方法。该网络使用VIIRS活跃火灾测量、GOES衍生的点火时间以及地形高度数据作为输入。cGAN通过WRF-SFIRE大气-野火模型对历史野火的模拟进行训练，从而将WRF-SFIRE的物理特性融入估计中。火灾进展由火灾到达时间表示，训练数据通过对WRF-SFIRE解应用近似观测算子获得。模型在火灾到达时间、测量值和地形的元组上进行训练，训练完成后利用真实火灾的测量值和相应的地形数据生成火灾到达时间的样本。", "result": "该方法在五次太平洋美国野火上进行了验证，与通过飞机测量的高分辨率边界进行比较，平均Sorensen-Dice系数达到0.81。研究还评估了地形高度对到达时间推断的影响，发现当推断以卫星测量为条件时，地形影响最小。", "conclusion": "所开发的生成算法能够有效地从多模态卫星数据和地形中重建野火进展，为复杂野火模型的数据同化提供了有价值的工具，并通过0.81的Sorensen-Dice系数证明了其高精度。当使用卫星数据时，地形的影响是最小的。", "translation": "野火发生频率的增加激发了人们对野火蔓延预测的日益增长的兴趣。然而，即使是最复杂的野火模型在多日模拟中也与观测到的进展存在差异，这促使了对数据同化的需求。将测量数据同化到复杂的大气-野火耦合模型中的一个有效方法是根据测量值估计野火进展，并利用这一进展来开发匹配的大气状态。在本研究中，开发了一种从VIIRS活跃火灾测量、GOES衍生的点火时间以及地形高度数据估计火灾进展的方法。一个条件生成对抗网络（cGAN）通过WRF-SFIRE大气-野火模型对历史野火的模拟进行训练，从而允许将WRF-SFIRE的物理特性纳入估计中。火灾进展由火灾到达时间简洁地表示，训练所需的测量值通过对WRF-SFIRE解应用近似观测算子获得，从而消除了训练期间对卫星数据的需求。该模型在火灾到达时间、测量值和地形的元组上进行训练，一旦训练完成，便利用真实火灾的测量值和相应的地形数据生成火灾到达时间的样本。该方法在五次太平洋美国野火上进行了验证，结果与通过飞机测量的高分辨率边界进行了比较，发现平均Sorensen-Dice系数为0.81。地形高度对到达时间推断的影响也进行了评估，并观察到当推断以卫星测量为条件时，地形影响最小。", "summary": "本研究开发了一种利用条件生成对抗网络（cGAN）从多模态卫星活跃火灾测量（VIIRS、GOES）和地形高度数据中重建野火进展（以火灾到达时间表示）的方法。该cGAN利用WRF-SFIRE模型模拟进行训练，从而融入了物理洞察。该方法在五次太平洋美国野火上进行了验证，并取得了0.81的平均Sorensen-Dice系数，证明了其在改善野火蔓延预测数据同化方面的有效性，并指出在以卫星数据为条件时地形影响最小。", "keywords": "野火进展, 生成对抗网络, 卫星测量, 数据同化, 火灾到达时间", "comments": "这篇论文提出了将cGAN应用于解决关键环境问题（野火进展重建）的创新方法。其优势在于整合了多源数据（卫星、地形）并将基于物理的模型（WRF-SFIRE）的洞察融入到数据驱动的生成框架中。0.81的高Sorensen-Dice系数表明其在野火边界重建方面具有良好的准确性，这对于改进野火蔓延预测模型至关重要。关于在卫星数据条件下地形影响最小的发现也提供了一个有趣的实践性见解。"}}
{"id": "2506.10528", "title": "SLICK: Selective Localization and Instance Calibration for Knowledge-Enhanced Car Damage Segmentation in Automotive Insurance", "authors": ["Teerapong Panboonyuen"], "summary": "We present SLICK, a novel framework for precise and robust car damage\nsegmentation that leverages structural priors and domain knowledge to tackle\nreal-world automotive inspection challenges. SLICK introduces five key\ncomponents: (1) Selective Part Segmentation using a high-resolution semantic\nbackbone guided by structural priors to achieve surgical accuracy in segmenting\nvehicle parts even under occlusion, deformation, or paint loss; (2)\nLocalization-Aware Attention blocks that dynamically focus on damaged regions,\nenhancing fine-grained damage detection in cluttered and complex street scenes;\n(3) an Instance-Sensitive Refinement head that leverages panoptic cues and\nshape priors to disentangle overlapping or adjacent parts, enabling precise\nboundary alignment; (4) Cross-Channel Calibration through multi-scale channel\nattention that amplifies subtle damage signals such as scratches and dents\nwhile suppressing noise like reflections and decals; and (5) a Knowledge Fusion\nModule that integrates synthetic crash data, part geometry, and real-world\ninsurance datasets to improve generalization and handle rare cases effectively.\nExperiments on large-scale automotive datasets demonstrate SLICK's superior\nsegmentation performance, robustness, and practical applicability for insurance\nand automotive inspection workflows.", "comment": "10 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10528v1", "AI": {"title_translation": "SLICK：用于汽车保险中知识增强型汽车损坏分割的选择性定位与实例校准", "tldr": "SLICK是一个新颖的框架，通过结构先验和领域知识，实现了精确鲁棒的汽车损坏分割，并在大规模数据集上表现出色，适用于保险和汽车检测。", "motivation": "解决现实世界汽车检测中精确鲁棒的汽车损坏分割挑战，并利用结构先验和领域知识。", "method": "SLICK框架包含五个关键组件：1) 选择性零件分割（使用高分辨率语义骨干和结构先验）；2) 定位感知注意力块（动态聚焦损坏区域）；3) 实例敏感细化头部（利用全景线索和形状先验分离重叠零件）；4) 跨通道校准（通过多尺度通道注意力增强损坏信号并抑制噪声）；5) 知识融合模块（整合合成碰撞数据、零件几何和真实保险数据集）。", "result": "在大规模汽车数据集上的实验表明，SLICK具有卓越的分割性能、鲁棒性和在保险和汽车检测工作流程中的实际适用性。", "conclusion": "SLICK框架通过结合结构先验和领域知识，显著提高了汽车损坏分割的精确度和鲁棒性，并在实际应用中表现出优越性。", "translation": "我们提出了SLICK，一个新颖的框架，用于精确和鲁棒的汽车损坏分割，它利用结构先验和领域知识来解决现实世界的汽车检测挑战。SLICK引入了五个关键组件：(1) 选择性零件分割，使用高分辨率语义骨干，由结构先验引导，即使在遮挡、变形或油漆脱落的情况下也能实现车辆零件分割的手术级精度；(2) 定位感知注意力块，动态聚焦于损坏区域，增强在杂乱和复杂的街道场景中细粒度损坏检测；(3) 实例敏感细化头部，利用全景线索和形状先验来解缠重叠或相邻零件，实现精确的边界对齐；(4) 跨通道校准，通过多尺度通道注意力放大细微的损坏信号，如划痕和凹痕，同时抑制反射和贴花等噪声；以及(5) 知识融合模块，整合合成碰撞数据、零件几何和真实世界保险数据集，以提高泛化能力并有效处理罕见情况。在大型汽车数据集上的实验证明了SLICK卓越的分割性能、鲁棒性以及在保险和汽车检测工作流程中的实际适用性。", "summary": "SLICK是一个创新的汽车损坏分割框架，它通过结合结构先验和领域知识来提高精度和鲁棒性。该框架包含五个核心组件：选择性零件分割、定位感知注意力、实例敏感细化、跨通道校准以及知识融合模块。这些组件协同工作，能够精确识别和分割汽车损坏，即使在复杂场景或存在遮挡的情况下。实验结果验证了SLICK在大规模数据集上的卓越性能和实际应用价值。", "keywords": "汽车损坏分割, 结构先验, 领域知识, 深度学习, 汽车保险", "comments": "SLICK的创新之处在于其多组件集成的方法，特别是结合了结构先验、领域知识和多种注意力机制，以解决汽车损坏分割的复杂挑战。知识融合模块通过整合多源数据，显著增强了模型的泛化能力和对罕见情况的处理能力，这对于实际的汽车保险和检测应用至关重要。"}}
{"id": "2506.10412", "title": "Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series", "authors": ["Ching Chang", "Jeehyun Hwang", "Yidan Shi", "Haixin Wang", "Wen-Chih Peng", "Tien-Fu Chen", "Wei Wang"], "summary": "Time series data in real-world applications such as healthcare, climate\nmodeling, and finance are often irregular, multimodal, and messy, with varying\nsampling rates, asynchronous modalities, and pervasive missingness. However,\nexisting benchmarks typically assume clean, regularly sampled, unimodal data,\ncreating a significant gap between research and real-world deployment. We\nintroduce Time-IMM, a dataset specifically designed to capture cause-driven\nirregularity in multimodal multivariate time series. Time-IMM represents nine\ndistinct types of time series irregularity, categorized into trigger-based,\nconstraint-based, and artifact-based mechanisms. Complementing the dataset, we\nintroduce IMM-TSF, a benchmark library for forecasting on irregular multimodal\ntime series, enabling asynchronous integration and realistic evaluation.\nIMM-TSF includes specialized fusion modules, including a timestamp-to-text\nfusion module and a multimodality fusion module, which support both\nrecency-aware averaging and attention-based integration strategies. Empirical\nresults demonstrate that explicitly modeling multimodality on irregular time\nseries data leads to substantial gains in forecasting performance. Time-IMM and\nIMM-TSF provide a foundation for advancing time series analysis under\nreal-world conditions. The dataset is publicly available at\nhttps://www.kaggle.com/datasets/blacksnail789521/time-imm/data, and the\nbenchmark library can be accessed at\nhttps://anonymous.4open.science/r/IMMTSF_NeurIPS2025.", "comment": "This paper is currently under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10412v1", "AI": {"title_translation": "Time-IMM：一个用于不规则多模态多变量时间序列的数据集和基准", "tldr": "引入了Time-IMM数据集和IMM-TSF基准库，以解决现实世界中不规则多模态多变量时间序列的预测挑战，并证明了多模态建模能显著提升预测性能。", "motivation": "现有基准通常假定数据是干净、规则采样和单模态的，这与医疗、气候建模和金融等实际应用中不规则、多模态、混乱的时间序列数据存在显著差距。本研究旨在弥合这一差距。", "method": "引入了Time-IMM数据集，其设计旨在捕捉多模态多变量时间序列中由原因驱动的不规则性，包含九种类型的不规则性。同时，推出了IMM-TSF基准库，用于不规则多模态时间序列的预测，支持异步集成和实际评估，并包含时间戳到文本融合模块和多模态融合模块，支持近期感知平均和基于注意力的集成策略。", "result": "实证结果表明，在不规则时间序列数据上明确建模多模态可以显著提高预测性能。", "conclusion": "Time-IMM数据集和IMM-TSF基准库为推进现实世界条件下的时间序列分析奠定了基础。", "translation": "在医疗保健、气候建模和金融等实际应用中，时间序列数据通常是不规则、多模态和混乱的，具有不同的采样率、异步模态和普遍存在的缺失值。然而，现有基准通常假定数据是干净、规则采样和单模态的，这在研究和实际部署之间造成了显著差距。我们引入了Time-IMM，一个专门设计用于捕获多模态多变量时间序列中由原因驱动的不规则性的数据集。Time-IMM代表了九种不同类型的时间序列不规则性，分为触发式、约束式和人工制品式机制。作为数据集的补充，我们引入了IMM-TSF，一个用于不规则多模态时间序列预测的基准库，能够实现异步集成和实际评估。IMM-TSF包括专门的融合模块，包括时间戳到文本融合模块和多模态融合模块，它们支持近期感知平均和基于注意力的集成策略。实证结果表明，在不规则时间序列数据上明确建模多模态可以显著提高预测性能。Time-IMM和IMM-TSF为推进现实世界条件下的时间序列分析奠定了基础。该数据集可在https://www.kaggle.com/datasets/blacksnail789521/time-imm/data公开获取，基准库可在https://anonymous.4open.science/r/IMMTSF_NeurIPS2025访问。", "summary": "本研究针对现实世界中时间序列数据不规则、多模态且混乱的挑战，引入了Time-IMM数据集和IMM-TSF基准库。Time-IMM数据集专门设计用于捕获由原因驱动的不规则性，涵盖九种类型。IMM-TSF基准库支持不规则多模态时间序列的预测，并包含多种融合模块。实证结果表明，明确建模多模态能显著提升预测性能。该工作为现实世界条件下的时间序列分析奠定了基础。", "keywords": "时间序列, 不规则, 多模态, 数据集, 基准", "comments": "该论文通过引入Time-IMM数据集和IMM-TSF基准库，填补了现有时间序列研究与现实世界复杂数据之间的空白。其创新之处在于专门设计数据集以捕捉因果驱动的不规则性，并提供支持异步集成和多模态融合的基准库。这对于推动时间序列分析在实际应用中的发展具有重要意义。"}}
{"id": "2506.10877", "title": "Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment", "authors": ["Hongda Sun", "Jiaren Peng", "Wenzhong Yang", "Liang He", "Bo Du", "Rui Yan"], "summary": "Medical dialogue systems (MDS) have emerged as crucial online platforms for\nenabling multi-turn, context-aware conversations with patients. However,\nexisting MDS often struggle to (1) identify relevant medical knowledge and (2)\ngenerate personalized, medically accurate responses. To address these\nchallenges, we propose MedRef, a novel MDS that incorporates knowledge refining\nand dynamic prompt adjustment. First, we employ a knowledge refining mechanism\nto filter out irrelevant medical data, improving predictions of critical\nmedical entities in responses. Additionally, we design a comprehensive prompt\nstructure that incorporates historical details and evident details. To enable\nreal-time adaptability to diverse patient conditions, we implement two key\nmodules, Triplet Filter and Demo Selector, providing appropriate knowledge and\ndemonstrations equipped in the system prompt. Extensive experiments on MedDG\nand KaMed benchmarks show that MedRef outperforms state-of-the-art baselines in\nboth generation quality and medical entity accuracy, underscoring its\neffectiveness and reliability for real-world healthcare applications.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10877v1", "AI": {"title_translation": "通过知识提炼和动态提示调整增强医疗对话生成", "tldr": "MedRef是一个新的医疗对话系统，通过知识提炼和动态提示调整来提高相关医疗知识的识别和个性化、医学准确的响应生成，在MedDG和KaMed基准测试中优于现有SOTA方法。", "motivation": "现有医疗对话系统（MDS）在识别相关医疗知识和生成个性化、医学准确的响应方面存在困难。", "method": "本文提出了MedRef，一个结合了知识提炼和动态提示调整的新型医疗对话系统。它采用知识提炼机制过滤不相关的医疗数据，并设计了一个包含历史和证据细节的全面提示结构。为实现实时适应性，MedRef实现了两个关键模块：Triplet Filter和Demo Selector，用于在系统提示中提供适当的知识和示例。", "result": "在MedDG和KaMed基准测试上的大量实验表明，MedRef在生成质量和医疗实体准确性方面均优于现有最先进的基线方法。", "conclusion": "MedRef通过知识提炼和动态提示调整显著提高了医疗对话生成的质量和准确性，证明了其在现实世界医疗应用中的有效性和可靠性。", "translation": "医疗对话系统（MDS）已成为重要的在线平台，能够与患者进行多轮、上下文感知的对话。然而，现有的MDS常常难以（1）识别相关的医疗知识和（2）生成个性化、医学准确的响应。为了解决这些挑战，我们提出了MedRef，一种结合了知识提炼和动态提示调整的新型MDS。首先，我们采用知识提炼机制来过滤掉不相关的医疗数据，从而提高响应中关键医疗实体的预测。此外，我们设计了一个全面的提示结构，其中包含历史细节和证据细节。为了实现对不同患者情况的实时适应性，我们实施了两个关键模块，即Triplet Filter和Demo Selector，它们在系统提示中提供适当的知识和示例。在MedDG和KaMed基准测试上进行的大量实验表明，MedRef在生成质量和医疗实体准确性方面均优于现有最先进的基线方法，这突显了其在现实世界医疗应用中的有效性和可靠性。", "summary": "本文提出MedRef，一个新型医疗对话系统，旨在解决现有系统在识别相关医疗知识和生成个性化、医学准确响应方面的不足。MedRef通过知识提炼机制过滤不相关医疗数据，并设计动态提示结构以整合历史和证据细节。此外，它引入Triplet Filter和Demo Selector模块以提供实时适应的知识和示例。实验结果表明，MedRef在生成质量和医疗实体准确性方面均优于现有基线，证明了其在医疗应用中的有效性。", "keywords": "医疗对话系统, 知识提炼, 动态提示调整, 生成质量, 医疗实体准确性", "comments": "MedRef的创新之处在于结合了知识提炼和动态提示调整，这有助于解决医疗对话系统在知识准确性和个性化方面的核心挑战。特别是Triplet Filter和Demo Selector模块的设计，提升了系统对复杂医疗场景的实时适应性，使其在实际医疗应用中具有重要价值。"}}
{"id": "2506.10550", "title": "ContextRefine-CLIP for EPIC-KITCHENS-100 Multi-Instance Retrieval Challenge 2025", "authors": ["Jing He", "Yiqing Wang", "Lingling Li", "Kexin Zhang", "Puhua Chen"], "summary": "This report presents ContextRefine-CLIP (CR-CLIP), an efficient model for\nvisual-textual multi-instance retrieval tasks. The approach is based on the\ndual-encoder AVION, on which we introduce a cross-modal attention flow module\nto achieve bidirectional dynamic interaction and refinement between visual and\ntextual features to generate more context-aware joint representations. For\nsoft-label relevance matrices provided in tasks such as EPIC-KITCHENS-100,\nCR-CLIP can work with Symmetric Multi-Similarity Loss to achieve more accurate\nsemantic alignment and optimization using the refined features. Without using\nensemble learning, the CR-CLIP model achieves 66.78mAP and 82.08nDCG on the\nEPIC-KITCHENS-100 public leaderboard, which significantly outperforms the\nbaseline model and fully validates its effectiveness in cross-modal retrieval.\nThe code will be released open-source on\nhttps://github.com/delCayr/ContextRefine-Clip", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10550v1", "AI": {"title_translation": "ContextRefine-CLIP在EPIC-KITCHENS-100多实例检索挑战赛2025中的应用", "tldr": "ContextRefine-CLIP (CR-CLIP) 是一种基于双编码器AVION的视觉-文本多实例检索模型，通过引入跨模态注意力流模块和对称多相似度损失，在EPIC-KITCHENS-100数据集上取得了显著优于基线的性能。", "motivation": "为了解决视觉-文本多实例检索任务，并提高现有模型的效率和准确性，尤其是在处理EPIC-KITCHENS-100等任务中提供的软标签相关性矩阵时。", "method": "该方法基于双编码器AVION，引入了一个跨模态注意力流模块，以实现视觉和文本特征之间的双向动态交互和细化，从而生成更具上下文感知能力的联合表示。对于软标签相关性矩阵，CR-CLIP结合对称多相似度损失（Symmetric Multi-Similarity Loss）进行优化。", "result": "CR-CLIP模型在EPIC-KITCHENS-100公共排行榜上取得了66.78mAP和82.08nDCG的成绩，显著优于基线模型，且未采用集成学习。", "conclusion": "ContextRefine-CLIP (CR-CLIP) 是一种有效且高效的视觉-文本多实例检索模型，其在EPIC-KITCHENS-100数据集上的优异表现充分验证了其在跨模态检索中的有效性。", "translation": "本报告介绍了ContextRefine-CLIP (CR-CLIP)，一种用于视觉-文本多实例检索任务的高效模型。该方法基于双编码器AVION，在此基础上我们引入了一个跨模态注意力流模块，以实现视觉和文本特征之间的双向动态交互和细化，从而生成更具上下文感知能力的联合表示。对于EPIC-KITCHENS-100等任务中提供的软标签相关性矩阵，CR-CLIP可以与对称多相似度损失（Symmetric Multi-Similarity Loss）协同工作，利用细化后的特征实现更准确的语义对齐和优化。在不使用集成学习的情况下，CR-CLIP模型在EPIC-KITCHENS-100公共排行榜上取得了66.78mAP和82.08nDCG的成绩，这显著优于基线模型，并充分验证了其在跨模态检索中的有效性。代码将在https://github.com/delCayr/ContextRefine-Clip开源。", "summary": "本文提出了一种名为ContextRefine-CLIP (CR-CLIP) 的视觉-文本多实例检索模型。该模型在双编码器AVION的基础上，通过引入跨模态注意力流模块，实现了视觉和文本特征的动态交互与细化，从而生成更优的联合表示。结合对称多相似度损失，CR-CLIP能够处理软标签相关性矩阵，并在EPIC-KITCHENS-100数据集上取得了显著超越基线模型的性能，验证了其在跨模态检索任务中的高效性和准确性。", "keywords": "多实例检索, 跨模态注意力, ContextRefine-CLIP, EPIC-KITCHENS-100, 视觉-文本检索", "comments": "CR-CLIP的创新点在于引入了跨模态注意力流模块，实现了视觉与文本特征的双向动态交互与细化，这有助于生成更丰富的上下文感知联合表示。其在EPIC-KITCHENS-100挑战赛中未采用集成学习就取得优异成绩，显示了其单一模型的强大性能和效率。该方法对于提升多实例检索任务的准确性具有重要意义。"}}
{"id": "2506.10419", "title": "Data-Driven Soil Organic Carbon Sampling: Integrating Spectral Clustering with Conditioned Latin Hypercube Optimization", "authors": ["Weiying Zhao", "Aleksei Unagaev", "Natalia Efremova"], "summary": "Soil organic carbon (SOC) monitoring often relies on selecting representative\nfield sampling locations based on environmental covariates. We propose a novel\nhybrid methodology that integrates spectral clustering - an unsupervised\nmachine learning technique with conditioned Latin hypercube sampling (cLHS) to\nenhance the representativeness of SOC sampling. In our approach, spectral\nclustering partitions the study area into $K$ homogeneous zones using\nmultivariate covariate data, and cLHS is then applied within each zone to\nselect sampling locations that collectively capture the full diversity of\nenvironmental conditions. This hybrid spectral-cLHS method ensures that even\nminor but important environmental clusters are sampled, addressing a key\nlimitation of vanilla cLHS which can overlook such areas. We demonstrate on a\nreal SOC mapping dataset that spectral-cLHS provides more uniform coverage of\ncovariate feature space and spatial heterogeneity than standard cLHS. This\nimproved sampling design has the potential to yield more accurate SOC\npredictions by providing better-balanced training data for machine learning\nmodels.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10419v1", "AI": {"title_translation": "数据驱动的土壤有机碳采样：整合谱聚类与条件拉丁超立方优化", "tldr": "一种结合谱聚类和条件拉丁超立方采样的混合方法，可提高土壤有机碳采样的代表性。", "motivation": "土壤有机碳（SOC）监测需要选择有代表性的野外采样点，但传统方法可能忽略重要的环境区域。", "method": "提出了一种名为 spectral-cLHS 的新型混合方法。该方法首先利用谱聚类将研究区域划分为K个同质区域，然后在此基础上在每个区域内应用条件拉丁超立方抽样（cLHS）来选择采样点，以全面捕捉环境条件的多样性。", "result": "在真实的土壤有机碳测绘数据集上，spectral-cLHS 方法比标准 cLHS 方法提供了更均匀的协变量特征空间覆盖和空间异质性。", "conclusion": "这种改进的采样设计通过为机器学习模型提供更平衡的训练数据，有望产生更准确的土壤有机碳预测。", "translation": "土壤有机碳（SOC）监测通常依赖于根据环境协变量选择有代表性的野外采样点。我们提出了一种新颖的混合方法，该方法整合了谱聚类——一种无监督机器学习技术——与条件拉丁超立方抽样（cLHS），以增强SOC采样的代表性。在我们的方法中，谱聚类利用多变量协变量数据将研究区域划分为K个同质区域，然后将cLHS应用于每个区域内，以选择能够共同捕捉环境条件全部多样性的采样点。这种混合的谱聚类-cLHS方法确保即使是次要但重要的环境簇也能被采样，解决了传统cLHS可能忽视这些区域的关键局限性。我们在一个真实的SOC测绘数据集上证明，谱聚类-cLHS比标准cLHS提供了更均匀的协变量特征空间覆盖和空间异质性。这种改进的采样设计有望通过为机器学习模型提供更平衡的训练数据，从而产生更准确的SOC预测。", "summary": "本文提出了一种新颖的混合采样方法 spectral-cLHS，将谱聚类与条件拉丁超立方抽样（cLHS）相结合，旨在提高土壤有机碳（SOC）采样的代表性。该方法首先利用谱聚类划分同质区域，再在区域内应用cLHS，以确保即使是次要但重要的环境簇也能被采样。实验证明，spectral-cLHS 在协变量特征空间覆盖和空间异质性方面优于标准cLHS，有望为机器学习模型提供更平衡的训练数据，从而提高SOC预测的准确性。", "keywords": "土壤有机碳, 谱聚类, 拉丁超立方抽样, 采样设计, 数据驱动", "comments": "该论文的创新之处在于结合了谱聚类和cLHS，有效解决了传统cLHS可能漏采重要但次要环境区域的局限性。这种数据驱动的采样设计对于提高环境监测和预测模型的准确性具有重要意义。"}}
{"id": "2506.10885", "title": "Slimming Down LLMs Without Losing Their Minds", "authors": ["Qingda", "Mai"], "summary": "This paper investigates and validates the impact of fine-tuning on large\nlanguage model performance, focusing on parameter-efficient methods (LoRA and\nQLoRA). We evaluate model capabilities across three key domains: (1)\ncommonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)\nmulti-domain knowledge (MMLU-CS).\n  Our findings demonstrate that: (1) LoRA-based methods effectively improve\ntask-specific performance while maintaining computational efficiency, and (2)\nperformance strongly depends on alignment between fine-tuning dataset and\nbenchmark tasks. The study provides both theoretical insights into\nparameter-efficient mechanisms and practical guidance for developers\nimplementing efficient LLM adaptation with limited resources.", "comment": "10 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10885v1", "AI": {"title_translation": "在不损失性能的情况下精简大型语言模型", "tldr": "该研究调查并验证了LoRA和QLoRA等参数高效微调方法对大型语言模型（LLM）性能的影响，发现这些方法能有效提升特定任务性能并保持计算效率，且性能高度依赖于微调数据集与基准任务的对齐。", "motivation": "该研究旨在调查和验证微调对大型语言模型（LLM）性能的影响，特别是关注参数高效方法（LoRA和QLoRA）。目标是探究如何在不损失性能的情况下精简LLM。", "method": "研究评估了LoRA和QLoRA两种参数高效方法对LLM能力的影响，评估范围涵盖三个关键领域：常识推理（HellaSwag）、数学推理（GSM8K）和多领域知识（MMLU-CS）。", "result": "研究结果表明：1) 基于LoRA的方法能有效提升特定任务性能，同时保持计算效率；2) 性能强烈依赖于微调数据集与基准任务之间的对齐。", "conclusion": "该研究为参数高效机制提供了理论见解，并为在资源有限的情况下实现高效LLM适应的开发人员提供了实践指导。", "translation": "本文调查并验证了微调对大型语言模型性能的影响，重点关注参数高效方法（LoRA和QLoRA）。我们评估了模型在三个关键领域的能力：（1）常识推理（HellaSwag），（2）数学推理（GSM8K），以及（3）多领域知识（MMLU-CS）。我们的研究结果表明：（1）基于LoRA的方法能有效提高特定任务性能，同时保持计算效率；（2）性能强烈依赖于微调数据集与基准任务之间的对齐。本研究为参数高效机制提供了理论见解，并为在资源有限的情况下实现高效LLM适应的开发人员提供了实践指导。", "summary": "本研究深入探讨了LoRA和QLoRA等参数高效微调方法对大型语言模型性能的影响。通过在常识推理、数学推理和多领域知识三个基准数据集上的评估，研究发现LoRA类方法能有效提升LLM在特定任务上的表现，同时保持计算效率。此外，研究强调了微调数据集与基准任务之间对齐程度对模型性能的关键影响，为有限资源下LLM的有效适应提供了理论洞察和实践指导。", "keywords": "LoRA, QLoRA, LLM微调, 参数高效方法, 计算效率", "comments": "这项研究的创新之处在于系统地验证了LoRA和QLoRA在LLM微调中的有效性，尤其强调了其在保持计算效率方面的优势。它不仅提供了理论层面的见解，还为实际开发提供了指导，对于资源受限环境下LLM的部署和优化具有重要意义。研究发现性能高度依赖于数据集与任务的对齐，这一发现对未来微调策略的制定具有指导作用。"}}
{"id": "2506.10887", "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "authors": ["Yixiao Huang", "Hanlin Zhu", "Tianyu Guo", "Jiantao Jiao", "Somayeh Sojoudi", "Michael I. Jordan", "Stuart Russell", "Song Mei"], "summary": "Large language models (LLMs) can acquire new knowledge through fine-tuning,\nbut this process exhibits a puzzling duality: models can generalize remarkably\nfrom new facts, yet are also prone to hallucinating incorrect information.\nHowever, the reasons for this phenomenon remain poorly understood. In this\nwork, we argue that both behaviors stem from a single mechanism known as\nout-of-context reasoning (OCR): the ability to deduce implications by\nassociating concepts, even those without a causal link. Our experiments across\nfive prominent LLMs confirm that OCR indeed drives both generalization and\nhallucination, depending on whether the associated concepts are causally\nrelated. To build a rigorous theoretical understanding of this phenomenon, we\nthen formalize OCR as a synthetic factual recall task. We empirically show that\na one-layer single-head attention-only transformer with factorized output and\nvalue matrices can learn to solve this task, while a model with combined\nweights cannot, highlighting the crucial role of matrix factorization. Our\ntheoretical analysis shows that the OCR capability can be attributed to the\nimplicit bias of gradient descent, which favors solutions that minimize the\nnuclear norm of the combined output-value matrix. This mathematical structure\nexplains why the model learns to associate facts and implications with high\nsample efficiency, regardless of whether the correlation is causal or merely\nspurious. Ultimately, our work provides a theoretical foundation for\nunderstanding the OCR phenomenon, offering a new lens for analyzing and\nmitigating undesirable behaviors from knowledge injection.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10887v1", "AI": {"title_translation": "泛化还是幻觉？理解 Transformer 中的脱离上下文推理", "tldr": "本文认为大型语言模型的泛化和幻觉行为都源于脱离上下文推理（OCR），并从理论和实验上解释了其机制，强调了矩阵分解和梯度下降隐式偏置的关键作用。", "motivation": "大型语言模型（LLMs）在微调后能够获取新知识，但这一过程表现出矛盾的双重性：模型可以从新事实中显著泛化，但也容易产生不正确的信息（幻觉）。这种现象的原因仍知之甚少，因此需要深入理解其背后的机制。", "method": "本文提出脱离上下文推理（OCR）是LLMs泛化和幻觉的共同机制。通过对五种主流LLMs进行实验验证OCR的作用。为了理论化理解，将OCR形式化为合成事实回忆任务，并实验证明一个具有分解输出和值矩阵的单层单头注意力Transformer可以解决该任务，而组合权重的模型则不能。理论分析表明，OCR能力可归因于梯度下降的隐式偏置，该偏置有利于最小化组合输出-值矩阵的核范数的解。", "result": "实验证实OCR确实驱动了泛化和幻觉，这取决于关联概念是否具有因果关系。通过形式化为合成事实回忆任务，发现具有分解输出和值矩阵的Transformer可以学习解决该任务，而具有组合权重的模型则不能，这突出了矩阵分解的关键作用。理论分析揭示了梯度下降的隐式偏置解释了模型为何能高效地关联事实和推论，无论相关性是因果的还是虚假的。", "conclusion": "本研究为理解脱离上下文推理（OCR）现象提供了理论基础，为分析和缓解知识注入中不良行为提供了新的视角。OCR是LLMs泛化和幻觉的统一机制，其背后是矩阵分解和梯度下降隐式偏置的作用。", "translation": "大型语言模型（LLMs）可以通过微调获取新知识，但这个过程表现出令人费解的双重性：模型可以从新事实中显著泛化，但也容易产生不正确的信息（幻觉）。然而，这种现象的原因仍然知之甚少。在这项工作中，我们认为这两种行为都源于一种被称为脱离上下文推理（OCR）的单一机制：即通过关联概念来推断含义的能力，即使这些概念没有因果联系。我们对五种主流LLMs进行的实验证实，OCR确实驱动了泛化和幻觉，这取决于关联概念是否具有因果关系。为了建立对这种现象的严谨理论理解，我们将OCR形式化为合成事实回忆任务。我们经验性地表明，一个具有分解输出和值矩阵的单层单头注意力Transformer可以学习解决这个任务，而具有组合权重的模型则不能，这突出了矩阵分解的关键作用。我们的理论分析表明，OCR能力可以归因于梯度下降的隐式偏置，该偏置有利于最小化组合输出-值矩阵的核范数的解。这种数学结构解释了为什么模型能够以高样本效率学习关联事实和推论，无论这种关联是因果的还是仅仅是虚假的。最终，我们的工作为理解OCR现象提供了理论基础，为分析和缓解知识注入中不良行为提供了新的视角。", "summary": "本文探讨了大型语言模型（LLMs）在微调后同时表现出泛化和幻觉的现象。作者提出，这两种行为都源于一种名为“脱离上下文推理”（OCR）的机制，即模型即使在无因果关系的概念间也能进行关联推断。实验验证了OCR确实驱动了这两种行为，并进一步通过将OCR形式化为合成任务，从理论上揭示了矩阵分解和梯度下降的隐式偏置在其中扮演的关键角色，解释了模型高效学习事实关联的能力，无论其因果性如何。该研究为理解和缓解LLMs知识注入中的不良行为提供了新的理论框架。", "keywords": "脱离上下文推理, 大型语言模型, 泛化, 幻觉, 矩阵分解", "comments": "该论文提出了一个统一的框架来解释LLMs的泛化和幻觉这两种看似矛盾的行为，即脱离上下文推理（OCR），这是一个重要的创新点。通过实验验证和理论分析（特别是矩阵分解和梯度下降隐式偏置的作用），为理解LLMs内部机制提供了深入的洞察。这对于未来设计更可靠、更少幻觉的LLMs具有重要指导意义。"}}
{"id": "2506.10564", "title": "Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics", "authors": ["Imanol Solano", "Julian Fierrez", "Aythami Morales", "Alejandro Peña", "Ruben Tolosana", "Francisco Zamora-Martinez", "Javier San Agustin"], "summary": "Demographic bias in high-performance face recognition (FR) systems often\neludes detection by existing metrics, especially with respect to subtle\ndisparities in the tails of the score distribution. We introduce the\nComprehensive Equity Index (CEI), a novel metric designed to address this\nlimitation. CEI uniquely analyzes genuine and impostor score distributions\nseparately, enabling a configurable focus on tail probabilities while also\nconsidering overall distribution shapes. Our extensive experiments (evaluating\nstate-of-the-art FR systems, intentionally biased models, and diverse datasets)\nconfirm CEI's superior ability to detect nuanced biases where previous methods\nfall short. Furthermore, we present CEI^A, an automated version of the metric\nthat enhances objectivity and simplifies practical application. CEI provides a\nrobust and sensitive tool for operational FR fairness assessment. The proposed\nmethods have been developed particularly for bias evaluation in face biometrics\nbut, in general, they are applicable for comparing statistical distributions in\nany problem where one is interested in analyzing the distribution tails.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10564v1", "AI": {"title_translation": "比较分布尾部平衡：综合公平指数（CEI）及其在操作性人脸生物识别偏见评估中的应用", "tldr": "本文提出了综合公平指数（CEI），一种新的指标，用于检测高性能人脸识别系统中现有的指标难以发现的细微人口偏见，尤其是在分数分布尾部的差异。CEI分析真实和冒充者分数分布，并能够配置对尾部概率的关注，同时考虑整体分布形状。实验证明CEI在检测细微偏见方面优于现有方法，并提出了自动化版本CEI^A。", "motivation": "现有指标在检测高性能人脸识别（FR）系统中的人口偏见方面存在不足，特别是在分数分布尾部的细微差异方面。", "method": "引入了综合公平指数（CEI），这是一种新的指标，旨在解决现有指标在检测分数分布尾部细微差异方面的局限性。CEI独特地单独分析真实和冒充者分数分布，可以配置对尾部概率的关注，同时考虑整体分布形状。此外，提出了CEI^A，一个自动化版本的指标，以增强客观性并简化实际应用。", "result": "广泛的实验（评估了最先进的FR系统、有意偏见的模型和多样化的数据集）证实，CEI在检测现有方法不足的细微偏见方面具有卓越的能力。CEI^A版本增强了客观性并简化了实际应用。", "conclusion": "CEI为操作性人脸识别公平性评估提供了一个鲁棒且敏感的工具。所提出的方法虽然是为人脸生物识别中的偏见评估而开发的，但通常也适用于任何对分析分布尾部感兴趣的问题中的统计分布比较。", "translation": "高性能人脸识别（FR）系统中的人口偏见常常难以被现有指标检测到，尤其是在分数分布尾部的细微差异方面。我们引入了综合公平指数（CEI），这是一种旨在解决这一局限性的新型指标。CEI独特地单独分析真实和冒充者分数分布，从而能够可配置地关注尾部概率，同时还考虑整体分布形状。我们广泛的实验（评估了最先进的FR系统、有意偏见的模型和多样化的数据集）证实，CEI在检测现有方法不足的细微偏见方面具有卓越的能力。此外，我们提出了CEI^A，一个自动化版本的指标，它增强了客观性并简化了实际应用。CEI为操作性人脸识别公平性评估提供了一个鲁棒且敏感的工具。所提出的方法是专门为人脸生物识别中的偏见评估而开发的，但一般来说，它们适用于任何对分析分布尾部感兴趣的问题中的统计分布比较。", "summary": "本文提出了一种新的指标——综合公平指数（CEI），旨在解决高性能人脸识别（FR）系统中现有指标难以检测到的人口偏见问题，特别是分数分布尾部的细微差异。CEI通过单独分析真实和冒充者分数分布，并允许配置对尾部概率的关注，同时考虑整体分布形状。实验结果表明，CEI在检测细微偏见方面优于传统方法。此外，文中还提出了CEI的自动化版本CEI^A，以提高客观性和易用性。该方法为人脸识别的公平性评估提供了一个强大而敏感的工具，并且其适用性可以扩展到其他需要分析分布尾部的数据比较问题。", "keywords": "综合公平指数, 人脸识别, 偏见评估, 分布尾部, 公平性", "comments": "该论文的创新之处在于提出了CEI，一个专门关注分数分布尾部差异的指标，解决了现有指标在检测细微偏见方面的不足。其重要性体现在为人脸识别系统的公平性评估提供了一个更鲁棒和敏感的工具，并且其方法具有通用性，可应用于其他统计分布比较问题。CEI^A的提出也提升了其实用性。"}}
{"id": "2506.10443", "title": "MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices", "authors": ["Zhaode Wang", "Jingbang Yang", "Xinyu Qian", "Shiwen Xing", "Xiaotang Jiang", "Chengfei Lv", "Shengyu Zhang"], "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na variety of tasks. However, their substantial scale leads to significant\ncomputational resource consumption during inference, resulting in high costs.\nConsequently, edge device inference presents a promising solution. The primary\nchallenges of edge inference include memory usage and inference speed. This\npaper introduces MNN-LLM, a framework specifically designed to accelerate the\ndeployment of large language models on mobile devices. MNN-LLM addresses the\nruntime characteristics of LLMs through model quantization and DRAM-Flash\nhybrid storage, effectively reducing memory usage. It rearranges weights and\ninputs based on mobile CPU instruction sets and GPU characteristics while\nemploying strategies such as multicore load balancing, mixed-precision\nfloating-point operations, and geometric computations to enhance performance.\nNotably, MNN-LLM achieves up to a 8.6x speed increase compared to current\nmainstream LLM-specific frameworks.", "comment": "7 pages, 5 figures. Published in the Proceedings of the 6th ACM\n  International Conference on Multimedia in Asia Workshops (MMAsia '24\n  Workshops). The final authenticated version is available at\n  https://dl.acm.org/doi/10.1145/3700410.3702126", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10443v1", "AI": {"title_translation": "MNN-LLM：一种用于在移动设备上快速部署大型语言模型的通用推理引擎", "tldr": "MNN-LLM是一个针对移动设备优化的LLM推理引擎，通过量化、混合存储和性能优化，显著提升了移动端LLM的部署速度和效率。", "motivation": "大型语言模型（LLMs）性能优越，但推理时资源消耗大、成本高。边缘设备推理是解决方案，但面临内存使用和推理速度的挑战。", "method": "本文介绍了MNN-LLM框架，通过模型量化和DRAM-Flash混合存储减少内存使用。它根据移动CPU指令集和GPU特性重新排列权重和输入，并采用多核负载均衡、混合精度浮点运算和几何计算等策略来提高性能。", "result": "MNN-LLM与当前主流的LLM专用框架相比，实现了高达8.6倍的速度提升。", "conclusion": "MNN-LLM成功解决了移动设备上大型语言模型部署的内存和速度挑战，显著提高了推理效率。", "translation": "大型语言模型（LLMs）在各种任务中展现出卓越的性能。然而，其庞大的规模导致推理过程中大量的计算资源消耗，从而产生高昂的成本。因此，边缘设备推理提供了一个有前景的解决方案。边缘推理的主要挑战包括内存使用和推理速度。本文介绍了MNN-LLM，一个专门为加速大型语言模型在移动设备上部署而设计的框架。MNN-LLM通过模型量化和DRAM-Flash混合存储来解决LLMs的运行时特性，有效减少内存使用。它根据移动CPU指令集和GPU特性重新排列权重和输入，同时采用多核负载均衡、混合精度浮点运算和几何计算等策略来提高性能。值得注意的是，与当前主流的LLM专用框架相比，MNN-LLM实现了高达8.6倍的速度提升。", "summary": "本文介绍了MNN-LLM，一个专为在移动设备上快速部署大型语言模型（LLMs）而设计的通用推理引擎。针对LLMs在边缘设备上部署面临的内存和速度挑战，MNN-LLM通过模型量化、DRAM-Flash混合存储有效降低内存消耗。同时，它通过优化权重和输入排列、多核负载均衡、混合精度运算和几何计算等策略，显著提升了推理速度，与现有主流框架相比，速度提升高达8.6倍，为LLM在移动端的广泛应用提供了高效解决方案。", "keywords": "大型语言模型, 移动设备, 推理引擎, 模型量化, 边缘计算", "comments": "该论文的创新点在于为移动设备上的大型语言模型推理提供了一个高效且通用的解决方案。通过结合模型量化、混合存储以及针对移动硬件特性的优化，MNN-LLM显著克服了移动端LLM部署的内存和速度瓶颈，对于推动LLM在边缘设备上的普及和应用具有重要意义。"}}
{"id": "2506.10896", "title": "BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP", "authors": ["Thomas Sounack", "Joshua Davis", "Brigitte Durieux", "Antoine Chaffin", "Tom J. Pollard", "Eric Lehman", "Alistair E. W. Johnson", "Matthew McDermott", "Tristan Naumann", "Charlotta Lindvall"], "summary": "Encoder-based transformer models are central to biomedical and clinical\nNatural Language Processing (NLP), as their bidirectional self-attention makes\nthem well-suited for efficiently extracting structured information from\nunstructured text through discriminative tasks. However, encoders have seen\nslower development compared to decoder models, leading to limited domain\nadaptation in biomedical and clinical settings. We introduce BioClinical\nModernBERT, a domain-adapted encoder that builds on the recent ModernBERT\nrelease, incorporating long-context processing and substantial improvements in\nspeed and performance for biomedical and clinical NLP. BioClinical ModernBERT\nis developed through continued pretraining on the largest biomedical and\nclinical corpus to date, with over 53.5 billion tokens, and addresses a key\nlimitation of prior clinical encoders by leveraging 20 datasets from diverse\ninstitutions, domains, and geographic regions, rather than relying on data from\na single source. It outperforms existing biomedical and clinical encoders on\nfour downstream tasks spanning a broad range of use cases. We release both base\n(150M parameters) and large (396M parameters) versions of BioClinical\nModernBERT, along with training checkpoints to support further research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10896v1", "AI": {"title_translation": "BioClinical ModernBERT：一种用于生物医学和临床自然语言处理的先进长上下文编码器", "tldr": "BioClinical ModernBERT是一种新的领域适应性编码器，通过在大型生物医学和临床语料库上预训练，解决了现有编码器在长上下文处理和领域适应方面的局限性，并在多种下游任务中表现优异。", "motivation": "现有编码器在生物医学和临床自然语言处理（NLP）领域的领域适应性有限，且在长上下文处理方面存在不足，导致其发展慢于解码器模型。", "method": "引入BioClinical ModernBERT，一个基于ModernBERT的领域适应性编码器。它通过在包含超过535亿个token的迄今为止最大的生物医学和临床语料库上进行持续预训练而开发。该模型利用来自不同机构、领域和地理区域的20个数据集进行训练，以解决单一数据源的局限性。同时发布了基础版（1.5亿参数）和大型版（3.96亿参数）。", "result": "BioClinical ModernBERT在四项涵盖广泛用例的下游任务中优于现有的生物医学和临床编码器，并在速度和性能方面有显著提升。", "conclusion": "BioClinical ModernBERT通过大规模预训练和多源数据利用，成功解决了生物医学和临床NLP中现有编码器的局限性，提供了先进的长上下文处理能力和卓越的性能，有望支持进一步的研究。", "translation": "基于编码器的Transformer模型是生物医学和临床自然语言处理（NLP）的核心，因为它们的双向自注意力机制使其非常适合通过判别任务从非结构化文本中高效提取结构化信息。然而，与解码器模型相比，编码器模型的发展速度较慢，导致在生物医学和临床环境中的领域适应性有限。我们引入了BioClinical ModernBERT，这是一种领域适应性编码器，它建立在最近发布的ModernBERT之上，结合了长上下文处理功能，并在生物医学和临床NLP的速度和性能方面进行了实质性改进。BioClinical ModernBERT通过在迄今为止最大的生物医学和临床语料库（超过535亿个token）上进行持续预训练而开发，并通过利用来自不同机构、领域和地理区域的20个数据集，而不是依赖单一来源的数据，解决了现有临床编码器的一个关键局限性。它在涵盖广泛用例的四项下游任务中优于现有的生物医学和临床编码器。我们发布了BioClinical ModernBERT的基础版（1.5亿参数）和大型版（3.96亿参数），以及训练检查点，以支持进一步的研究。", "summary": "BioClinical ModernBERT是一种先进的、领域适应性强的编码器，专为生物医学和临床NLP设计。它基于ModernBERT，通过在迄今为止最大的生物医学和临床语料库（535亿token）上进行持续预训练开发，并利用来自20个不同来源的数据集，解决了现有编码器在长上下文处理和领域适应方面的不足。该模型在速度和性能上均有显著提升，并在四项下游任务中超越了现有模型。研究者发布了其基础版和大型版以支持后续研究。", "keywords": "BioClinical ModernBERT, 生物医学NLP, 临床NLP, 长上下文编码器, 领域适应性", "comments": "这项研究通过引入BioClinical ModernBERT，显著推动了生物医学和临床NLP领域的发展。其创新之处在于结合了长上下文处理能力，并在迄今为止最大的领域特定语料库上进行了大规模预训练，同时通过整合多源数据解决了单一数据源的局限性。这对于从复杂、非结构化的生物医学和临床文本中提取信息至关重要，有望在临床决策支持、药物发现等领域发挥重要作用。其发布的模型和检查点也为社区提供了宝贵的资源。"}}
{"id": "2506.10567", "title": "LRSLAM: Low-rank Representation of Signed Distance Fields in Dense Visual SLAM System", "authors": ["Hongbeen Park", "Minjeong Park", "Giljoo Nam", "Jinkyu Kim"], "summary": "Simultaneous Localization and Mapping (SLAM) has been crucial across various\ndomains, including autonomous driving, mobile robotics, and mixed reality.\nDense visual SLAM, leveraging RGB-D camera systems, offers advantages but faces\nchallenges in achieving real-time performance, robustness, and scalability for\nlarge-scale scenes. Recent approaches utilizing neural implicit scene\nrepresentations show promise but suffer from high computational costs and\nmemory requirements. ESLAM introduced a plane-based tensor decomposition but\nstill struggled with memory growth. Addressing these challenges, we propose a\nmore efficient visual SLAM model, called LRSLAM, utilizing low-rank tensor\ndecomposition methods. Our approach, leveraging the Six-axis and CP\ndecompositions, achieves better convergence rates, memory efficiency, and\nreconstruction/localization quality than existing state-of-the-art approaches.\nEvaluation across diverse indoor RGB-D datasets demonstrates LRSLAM's superior\nperformance in terms of parameter efficiency, processing time, and accuracy,\nretaining reconstruction and localization quality. Our code will be publicly\navailable upon publication.", "comment": "Accepted at ECCV 2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10567v1", "AI": {"title_translation": "LRSLAM：稠密视觉SLAM系统中符号距离场的低秩表示", "tldr": "LRSLAM利用低秩张量分解实现了高效且准确的稠密视觉SLAM。", "motivation": "稠密视觉SLAM在实时性、鲁棒性、大规模场景的可扩展性方面面临挑战。现有的神经隐式场景表示方法计算成本高、内存需求大，即使是基于张量分解的ESLAM也存在内存增长问题。", "method": "提出LRSLAM模型，利用低秩张量分解方法，具体包括六轴（Six-axis）和CP分解，来表示符号距离场。", "result": "LRSLAM比现有最先进的方法实现了更好的收敛速度、内存效率和重建/定位质量。在各种室内RGB-D数据集上的评估表明，LRSLAM在参数效率、处理时间和精度方面表现出卓越的性能，同时保持了重建和定位质量。", "conclusion": "LRSLAM是一种更高效的稠密视觉SLAM模型，通过低秩张量分解克服了现有方法的局限性，在性能和效率上均表现出色。", "translation": "同步定位与建图（SLAM）在自动驾驶、移动机器人和混合现实等各个领域都至关重要。利用RGB-D相机系统的稠密视觉SLAM具有优势，但在实现大规模场景的实时性能、鲁棒性和可扩展性方面面临挑战。最近利用神经隐式场景表示的方法显示出前景，但存在计算成本高和内存需求大的问题。ESLAM引入了基于平面的张量分解，但仍然面临内存增长的困境。为了应对这些挑战，我们提出了一种更高效的视觉SLAM模型，称为LRSLAM，它利用低秩张量分解方法。我们的方法利用六轴和CP分解，比现有最先进的方法实现了更好的收敛速度、内存效率以及重建/定位质量。对各种室内RGB-D数据集的评估表明，LRSLAM在参数效率、处理时间和精度方面表现出卓越的性能，同时保持了重建和定位质量。我们的代码将在发布后公开。", "summary": "本论文提出了LRSLAM，一种利用低秩张量分解（包括六轴和CP分解）的稠密视觉SLAM模型，旨在解决现有方法在实时性、内存效率和大规模场景可扩展性方面的挑战。实验结果表明，LRSLAM在收敛速度、内存效率、重建/定位质量、参数效率、处理时间和精度方面均优于现有先进方法。", "keywords": "稠密视觉SLAM, 低秩张量分解, 符号距离场, SLAM, LRSLAM", "comments": "本文的创新之处在于将低秩张量分解应用于稠密视觉SLAM中的符号距离场表示，有效解决了传统神经隐式表示计算和内存成本高的问题。LRSLAM通过这种方法显著提升了系统的效率和性能，对于推动实时、大规模SLAM系统的发展具有重要意义。"}}
{"id": "2506.10532", "title": "Equivariant Neural Diffusion for Molecule Generation", "authors": ["François Cornet", "Grigory Bartosh", "Mikkel N. Schmidt", "Christian A. Naesseth"], "summary": "We introduce Equivariant Neural Diffusion (END), a novel diffusion model for\nmolecule generation in 3D that is equivariant to Euclidean transformations.\nCompared to current state-of-the-art equivariant diffusion models, the key\ninnovation in END lies in its learnable forward process for enhanced generative\nmodelling. Rather than pre-specified, the forward process is parameterized\nthrough a time- and data-dependent transformation that is equivariant to rigid\ntransformations. Through a series of experiments on standard molecule\ngeneration benchmarks, we demonstrate the competitive performance of END\ncompared to several strong baselines for both unconditional and conditional\ngeneration.", "comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10532v1", "AI": {"title_translation": "等变神经扩散模型用于分子生成", "tldr": "本文提出了一种名为等变神经扩散（END）的新型扩散模型，用于3D分子生成，其关键创新在于可学习的前向过程，并在标准分子生成基准测试中表现出有竞争力的性能。", "motivation": "为了改进分子生成任务中的扩散模型，特别是解决现有等变扩散模型中前向过程预设的局限性，本文旨在通过引入可学习的前向过程来增强生成建模能力。", "method": "本文提出了一种名为等变神经扩散（END）的新型扩散模型，用于3D分子生成。END模型对欧几里得变换具有等变性，其核心创新在于可学习的前向过程，该过程通过时间依赖和数据依赖的变换进行参数化，并且对刚性变换具有等变性，而非预设。", "result": "通过在标准分子生成基准测试上进行一系列实验，END模型在无条件和有条件生成方面都展现出与多个强基线模型相比具有竞争力的性能。", "conclusion": "等变神经扩散（END）模型通过其可学习的前向过程，在3D分子生成任务中取得了与现有最先进模型相当甚至更好的性能，证明了该创新方法的有效性。", "translation": "我们引入了等变神经扩散（END），这是一种用于3D分子生成的新型扩散模型，它对欧几里得变换具有等变性。与当前最先进的等变扩散模型相比，END的关键创新在于其可学习的前向过程，用于增强生成建模。前向过程并非预设，而是通过一个对刚性变换具有等变性的时间依赖和数据依赖的变换进行参数化。通过在标准分子生成基准测试上进行一系列实验，我们证明了END在无条件和有条件生成方面与多个强基线模型相比具有竞争力的性能。", "summary": "本文介绍了一种名为等变神经扩散（END）的新型3D分子生成扩散模型。END模型对欧几里得变换具有等变性，并通过引入可学习的前向过程来提升生成建模能力，该过程通过时间依赖和数据依赖的等变变换进行参数化。实验结果表明，END在标准分子生成基准测试中，无论是无条件还是有条件生成，都展现出与现有强大基线模型相当的竞争力。", "keywords": "等变神经扩散, 分子生成, 扩散模型, 3D生成, 可学习前向过程", "comments": "该论文的创新点在于提出了等变神经扩散（END）模型中可学习的前向过程，这克服了传统扩散模型中前向过程预设的局限性。通过将前向过程参数化为时间依赖和数据依赖的等变变换，END模型增强了其生成建模能力，并在分子生成任务中取得了优异的性能，这对于未来3D分子生成领域的研究具有重要意义。"}}
{"id": "2506.10903", "title": "Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning", "authors": ["Lan Zhang", "Marco Valentino", "Andre Freitas"], "summary": "Autoformalization plays a crucial role in formal mathematical reasoning by\nenabling the automatic translation of natural language statements into formal\nlanguages. While recent advances using large language models (LLMs) have shown\npromising results, methods for automatically evaluating autoformalization\nremain underexplored. As one moves to more complex domains (e.g., advanced\nmathematics), human evaluation requires significant time and domain expertise,\nespecially as the complexity of the underlying statements and background\nknowledge increases. LLM-as-a-judge presents a promising approach for\nautomating such evaluation. However, existing methods typically employ\ncoarse-grained and generic evaluation criteria, which limit their effectiveness\nfor advanced formal mathematical reasoning, where quality hinges on nuanced,\nmulti-granular dimensions. In this work, we take a step toward addressing this\ngap by introducing a systematic, automatic method to evaluate autoformalization\ntasks. The proposed method is based on an epistemically and formally grounded\nensemble (EFG) of LLM judges, defined on criteria encompassing logical\npreservation (LP), mathematical consistency (MC), formal validity (FV), and\nformal quality (FQ), resulting in a transparent assessment that accounts for\ndifferent contributing factors. We validate the proposed framework to serve as\na proxy for autoformalization assessment within the domain of formal\nmathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM\njudges is a suitable emerging proxy for evaluation, more strongly correlating\nwith human assessments than a coarse-grained model, especially when assessing\nformal qualities. These findings suggest that LLM-as-judges, especially when\nguided by a well-defined set of atomic properties, could offer a scalable,\ninterpretable, and reliable support for evaluating formal mathematical\nreasoning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10903v1", "AI": {"title_translation": "超越黄金标准：用于形式数学推理的LLM评委的认知集成", "tldr": "本文提出了一种基于认知和形式化基础的LLM评委集成方法（EFG），用于自动评估自动形式化，结果显示其比粗粒度模型更准确，尤其在评估形式质量方面。", "motivation": "自动形式化在形式数学推理中扮演关键角色，但其自动评估方法仍未被充分探索。人类评估耗时且需要专业领域知识，尤其在复杂数学领域。现有LLM作为评委的方法通常采用粗粒度、通用评估标准，不适用于需要细致、多粒度评估的高级形式数学推理。", "method": "本文引入了一种系统、自动的自动形式化任务评估方法。该方法基于LLM评委的认知和形式化基础集成（EFG），其评估标准涵盖逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），从而实现透明评估。", "result": "实验表明，EFG LLM评委集成是自动形式化评估的合适新兴代理，与人类评估的相关性比粗粒度模型更强，尤其在评估形式质量时。", "conclusion": "研究结果表明，LLM作为评委，特别是当由一组定义明确的原子属性指导时，可以为评估形式数学推理提供可扩展、可解释和可靠的支持。", "translation": "自动形式化通过将自然语言语句自动翻译成形式语言，在形式数学推理中发挥着关键作用。尽管最近使用大型语言模型（LLM）的进展显示出有希望的结果，但自动评估自动形式化的方法仍未得到充分探索。随着领域变得更加复杂（例如，高等数学），人类评估需要大量时间和领域专业知识，特别是当底层语句和背景知识的复杂性增加时。LLM作为评委提出了一种自动化此类评估的有前景的方法。然而，现有方法通常采用粗粒度且通用的评估标准，这限制了它们对于高级形式数学推理的有效性，因为其质量取决于细致、多粒度的维度。在这项工作中，我们通过引入一种系统、自动的方法来评估自动形式化任务，从而弥补这一差距。所提出的方法基于LLM评委的认知和形式化基础集成（EFG），其定义标准涵盖逻辑保持（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），从而产生考虑到不同贡献因素的透明评估。我们验证了所提出的框架，以作为形式数学领域内自动形式化评估的代理。总的来说，我们的实验表明，LLM评委的EFG集成是评估的合适新兴代理，与人类评估的相关性比粗粒度模型更强，尤其在评估形式质量时。这些发现表明，LLM作为评委，特别是当由一组定义明确的原子属性指导时，可以为评估形式数学推理提供可扩展、可解释和可靠的支持。", "summary": "本文提出了一种系统、自动的LLM评委集成方法（EFG），用于评估自动形式化任务。该方法通过结合逻辑保持、数学一致性、形式有效性和形式质量等细致的评估标准，解决了现有LLM作为评委方法在高级形式数学推理中评估能力不足的问题。实验证明，EFG集成模型在评估自动形式化方面与人类评估具有更强的相关性，尤其是在形式质量方面，表明LLM评委在高维评估中具有可扩展、可解释和可靠的潜力。", "keywords": "自动形式化, LLM作为评委, 形式数学推理, 评估, 认知集成", "comments": "这项工作创新性地提出了一个基于认知和形式化基础的LLM评委集成框架（EFG），用于自动形式化评估。它克服了现有LLM评估方法粗粒度的局限性，通过引入多维、细致的评估标准，显著提升了LLM作为评委在复杂数学推理领域中的评估准确性和可靠性。其重要性在于为自动形式化，乃至更广泛的LLM生成内容评估，提供了一个更精确、可解释的评估范式，有望减少对耗时耗力的人工评估的依赖。"}}
{"id": "2506.10568", "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers", "authors": ["Lizhen Wang", "Zhurong Xia", "Tianshu Hu", "Pengrui Wang", "Pengfei Wang", "Zerong Zheng", "Ming Zhou"], "summary": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserves human identities and product-specific details,\nsuch as logos and textures, by injecting paired human-product reference\ninformation and utilizing an additional masked cross-attention mechanism. We\nemploy a 3D body mesh template and product bounding boxes to provide precise\nmotion guidance, enabling intuitive alignment of hand gestures with product\nplacements. Additionally, structured text encoding is used to incorporate\ncategory-level semantics, enhancing 3D consistency during small rotational\nchanges across frames. Trained on a hybrid dataset with extensive data\naugmentation strategies, our approach outperforms state-of-the-art techniques\nin maintaining the identity integrity of both humans and products and\ngenerating realistic demonstration motions. Project page:\nhttps://submit2025-dream.github.io/DreamActor-H1/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10568v1", "AI": {"title_translation": "DreamActor-H1: 通过运动设计的扩散Transformer实现高保真人机产品演示视频生成", "tldr": "DreamActor-H1提出一种基于扩散Transformer的框架，通过精确运动指导和身份保留机制，生成高保真、自然的人机产品演示视频。", "motivation": "在电子商务和数字营销中，生成高保真的人机产品演示视频至关重要，但现有框架在保留人与产品身份或理解人机空间关系方面存在不足，导致不真实和不自然的交互。", "method": "我们提出了一个基于扩散Transformer (DiT) 的框架。通过注入配对的人机产品参考信息和利用额外的掩蔽交叉注意力机制，同时保留人身份和产品特定细节。采用3D身体网格模板和产品边界框提供精确运动指导，实现手势与产品放置的直观对齐。此外，使用结构化文本编码来整合类别级语义，增强帧间小旋转变化时的3D一致性。", "result": "该方法在一个混合数据集上进行训练，并采用了广泛的数据增强策略，在保持人与产品的身份完整性以及生成真实演示动作方面优于现有最先进技术。", "conclusion": "DreamActor-H1有效解决了高保真人机产品演示视频生成中的身份保留和自然交互问题，显著提升了视频的真实性。", "translation": "在电子商务和数字营销中，生成高保真的人机产品演示视频对于有效的产品展示至关重要。然而，大多数现有框架要么未能保留人与产品的身份，要么缺乏对人机空间关系的理解，导致不真实的表现和不自然的交互。为了解决这些挑战，我们提出了一个基于扩散Transformer (DiT) 的框架。我们的方法通过注入配对的人机产品参考信息并利用额外的掩蔽交叉注意力机制，同时保留了人的身份和产品特定细节，例如徽标和纹理。我们采用3D身体网格模板和产品边界框来提供精确的运动指导，从而实现手势与产品放置的直观对齐。此外，结构化文本编码用于整合类别级语义，增强帧间小旋转变化时的3D一致性。我们的方法在一个混合数据集上进行训练，并采用了广泛的数据增强策略，在保持人与产品的身份完整性以及生成真实演示动作方面优于现有最先进技术。项目页面：https://submit2025-dream.github.io/DreamActor-H1/。", "summary": "DreamActor-H1提出了一种基于扩散Transformer的新框架，用于生成高保真的人机产品演示视频。该方法通过引入配对参考信息、掩蔽交叉注意力机制以及利用3D身体网格和产品边界框进行精确运动指导，解决了现有方法在身份保留和自然交互方面的不足。它还结合了结构化文本编码以增强3D一致性。实验表明，该方法在保持人与产品身份完整性及生成真实动作方面优于现有最先进技术。", "keywords": "人机产品演示, 视频生成, 扩散Transformer, 身份保留, 运动指导", "comments": "这篇论文通过引入专门的机制（如配对参考信息、掩蔽交叉注意力、3D运动指导和结构化文本编码）来解决人与产品身份保留以及自然交互的关键挑战，展示了在人机产品演示视频生成领域的显著创新。其基于扩散Transformer的框架在保持高保真度的同时，提升了视频的真实感和实用性，对于电商和数字营销领域具有重要价值。"}}
{"id": "2506.10536", "title": "Data-driven Day Ahead Market Prices Forecasting: A Focus on Short Training Set Windows", "authors": ["Vasilis Michalakopoulos", "Christoforos Menos-Aikateriniadis", "Elissaios Sarmas", "Antonis Zakynthinos", "Pavlos S. Georgilakis", "Dimitris Askounis"], "summary": "This study investigates the performance of machine learning models in\nforecasting electricity Day-Ahead Market (DAM) prices using short historical\ntraining windows, with a focus on detecting seasonal trends and price spikes.\nWe evaluate four models, namely LSTM with Feed Forward Error Correction (FFEC),\nXGBoost, LightGBM, and CatBoost, across three European energy markets (Greece,\nBelgium, Ireland) using feature sets derived from ENTSO-E forecast data.\nTraining window lengths range from 7 to 90 days, allowing assessment of model\nadaptability under constrained data availability. Results indicate that\nLightGBM consistently achieves the highest forecasting accuracy and robustness,\nparticularly with 45 and 60 day training windows, which balance temporal\nrelevance and learning depth. Furthermore, LightGBM demonstrates superior\ndetection of seasonal effects and peak price events compared to LSTM and other\nboosting models. These findings suggest that short-window training approaches,\ncombined with boosting methods, can effectively support DAM forecasting in\nvolatile, data-scarce environments.", "comment": "13 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10536v1", "AI": {"title_translation": "数据驱动的日前市场价格预测：侧重于短期训练集窗口", "tldr": "本研究评估了在短期训练窗口下，机器学习模型在日前电力市场价格预测中的表现，发现LightGBM在检测季节趋势和价格峰值方面表现最佳。", "motivation": "本研究旨在评估机器学习模型在使用短期历史训练数据时，在日前电力市场价格预测中的表现，特别是关注检测季节性趋势和价格峰值的能力。", "method": "研究评估了LSTM with Feed Forward Error Correction (FFEC)、XGBoost、LightGBM和CatBoost四种机器学习模型，在希腊、比利时、爱尔兰三个欧洲能源市场进行测试。使用源自ENTSO-E预测数据的特征集，训练窗口长度从7天到90天不等，以评估模型在数据可用性受限条件下的适应性。", "result": "LightGBM持续实现最高的预测精度和鲁棒性，尤其是在45天和60天的训练窗口下，这平衡了时间相关性和学习深度。此外，LightGBM在检测季节性效应和峰值价格事件方面表现出卓越的性能，优于LSTM和其他提升模型。", "conclusion": "短期窗口训练方法与提升方法相结合，可以有效地支持波动性大、数据稀缺环境下的日前市场价格预测。", "translation": "本研究探讨了机器学习模型在使用短期历史训练窗口预测日前电力市场（DAM）价格方面的表现，重点关注检测季节性趋势和价格峰值。我们评估了四种模型，即带有前馈误差校正（FFEC）的LSTM、XGBoost、LightGBM和CatBoost，并在三个欧洲能源市场（希腊、比利时、爱尔兰）使用源自ENTSO-E预测数据的特征集进行了测试。训练窗口长度从7天到90天不等，以便评估模型在数据可用性受限条件下的适应性。结果表明，LightGBM持续实现最高的预测精度和鲁棒性，尤其是在45天和60天的训练窗口下，这平衡了时间相关性和学习深度。此外，与LSTM和其他提升模型相比，LightGBM在检测季节性效应和峰值价格事件方面表现出卓越的性能。这些发现表明，短期窗口训练方法与提升方法相结合，可以有效地支持波动性大、数据稀缺环境下的DAM预测。", "summary": "本研究评估了在短期训练窗口下，机器学习模型（LSTM with FFEC, XGBoost, LightGBM, CatBoost）在日前电力市场价格预测中的性能，尤其关注季节趋势和价格峰值检测。研究发现，LightGBM在不同欧洲市场和训练窗口长度下表现出最高的预测精度和鲁棒性，尤其是在45-60天窗口，并且在检测季节效应和峰值事件方面优于其他模型。这表明短期训练结合boosting方法能有效应对数据稀缺的日前市场预测。", "keywords": "日前市场价格预测, 机器学习, 短期训练窗口, LightGBM, 电力市场", "comments": "这项研究的创新之处在于其聚焦于“短期训练集窗口”下的日前市场价格预测，这对于数据稀缺或需要快速适应市场变化的场景具有重要意义。LightGBM的优异表现及其对短期窗口的适应性，为实际应用提供了有价值的指导。"}}
{"id": "2506.10910", "title": "Magistral", "authors": ["Mistral-AI", ":", "Abhinav Rastogi", "Albert Q. Jiang", "Andy Lo", "Gabrielle Berrada", "Guillaume Lample", "Jason Rute", "Joep Barmentlo", "Karmesh Yadav", "Kartik Khandelwal", "Khyathi Raghavi Chandu", "Léonard Blier", "Lucile Saulnier", "Matthieu Dinot", "Maxime Darrin", "Neha Gupta", "Roman Soletskyi", "Sagar Vaze", "Teven Le Scao", "Yihan Wang", "Adam Yang", "Alexander H. Liu", "Alexandre Sablayrolles", "Amélie Héliou", "Amélie Martin", "Andy Ehrenberg", "Anmol Agarwal", "Antoine Roux", "Arthur Darcet", "Arthur Mensch", "Baptiste Bout", "Baptiste Rozière", "Baudouin De Monicault", "Chris Bamford", "Christian Wallenwein", "Christophe Renaudin", "Clémence Lanfranchi", "Darius Dabert", "Devon Mizelle", "Diego de las Casas", "Elliot Chane-Sane", "Emilien Fugier", "Emma Bou Hanna", "Gauthier Delerce", "Gauthier Guinet", "Georgii Novikov", "Guillaume Martin", "Himanshu Jaju", "Jan Ludziejewski", "Jean-Hadrien Chabran", "Jean-Malo Delignon", "Joachim Studnia", "Jonas Amar", "Josselin Somerville Roberts", "Julien Denize", "Karan Saxena", "Kush Jain", "Lingxiao Zhao", "Louis Martin", "Luyu Gao", "Lélio Renard Lavaud", "Marie Pellat", "Mathilde Guillaumin", "Mathis Felardos", "Maximilian Augustin", "Mickaël Seznec", "Nikhil Raghuraman", "Olivier Duchenne", "Patricia Wang", "Patrick von Platen", "Patryk Saffer", "Paul Jacob", "Paul Wambergue", "Paula Kurylowicz", "Pavankumar Reddy Muddireddy", "Philomène Chagniot", "Pierre Stock", "Pravesh Agrawal", "Romain Sauvestre", "Rémi Delacourt", "Sanchit Gandhi", "Sandeep Subramanian", "Shashwat Dalal", "Siddharth Gandhi", "Soham Ghosh", "Srijan Mishra", "Sumukh Aithal", "Szymon Antoniak", "Thibault Schueller", "Thibaut Lavril", "Thomas Robert", "Thomas Wang", "Timothée Lacroix", "Valeriia Nemychnikova", "Victor Paltz", "Virgile Richard", "Wen-Ding Li", "William Marshall", "Xuanyu Zhang", "Yunhao Tang"], "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10910v1", "AI": {"title_translation": "Magistral", "tldr": "引入了Magistral，一个基于Mistral的推理模型，通过自研的可扩展强化学习（RL）管道，探索了纯RL训练大型语言模型（LLM）的极限，并展示了其在维持和提升LLM能力方面的有效性。", "motivation": "研究动机是构建一个不依赖现有实现和预训练模型RL痕迹的、自底向上的可扩展强化学习（RL）管道，以探索纯RL训练大型语言模型（LLM）的极限。", "method": "该研究引入了Magistral，Mistral的第一个推理模型，并开发了自有的可扩展强化学习（RL）管道。他们采用自底向上的方法，完全依赖自己的模型和基础设施。研究展示了一个能够探索纯RL训练LLM极限的技术栈，并提出了一种强制模型推理语言的简单方法。", "result": "研究发现，仅通过文本数据进行的强化学习（RL）训练能够保持初始检查点的大部分能力，并且维持或改善了多模态理解、指令遵循和函数调用能力。他们推出了Magistral Medium（基于Mistral Medium 3纯RL训练的推理模型），并开源了Magistral Small（包含Magistral Medium的冷启动数据）。", "conclusion": "该研究得出结论，通过自底向上的方法，仅使用文本数据进行纯强化学习训练，可以有效地维持并提升大型语言模型（LLM）的各项能力，包括多模态理解、指令遵循和函数调用。", "translation": "我们引入了Magistral，这是Mistral的第一个推理模型，也是我们自己的可扩展强化学习（RL）管道。我们没有依赖现有的实现和从先前模型中提取的RL轨迹，而是遵循一种自底向上的方法，完全依赖我们自己的模型和基础设施。值得注意的是，我们展示了一个使我们能够探索纯RL训练大型语言模型（LLM）极限的技术栈，提出了一种强制模型推理语言的简单方法，并表明仅在文本数据上进行RL训练能够保持初始检查点的大部分能力。我们发现，在文本上进行RL训练能够维持或改善多模态理解、指令遵循和函数调用。我们展示了Magistral Medium，它是在Mistral Medium 3的基础上纯粹通过RL训练的推理模型，并且我们开源了Magistral Small（Apache 2.0），其中进一步包含了来自Magistral Medium的冷启动数据。", "summary": "该论文介绍了Magistral，一个基于Mistral的全新推理模型，其核心在于一个自研的可扩展强化学习（RL）管道。研究团队采取自底向上的方法，不依赖现有RL轨迹，而是探索了纯文本数据RL训练大型语言模型（LLM）的潜力。结果表明，这种纯RL训练不仅能保持LLM的初始能力，还能显著提升其在多模态理解、指令遵循和函数调用方面的表现。论文还发布了Magistral Medium和开源的Magistral Small模型。", "keywords": "Magistral, 强化学习, 大型语言模型, 推理, Mistral", "comments": "该论文的创新之处在于其完全自底向上的强化学习（RL）管道，以及对纯RL训练大型语言模型（LLM）极限的探索。它展示了不依赖现有RL痕迹，仅通过文本数据进行RL训练，也能有效提升LLM的推理和多模态能力，这对于未来LLM的训练范式具有重要意义。"}}
{"id": "2506.10573", "title": "Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration", "authors": ["Jun Wang", "Lixing Zhu", "Xiaohan Yu", "Abhir Bhalerao", "Yulan He"], "summary": "Learning medical visual representations from image-report pairs through joint\nlearning has garnered increasing research attention due to its potential to\nalleviate the data scarcity problem in the medical domain. The primary\nchallenges stem from the lengthy reports that feature complex discourse\nrelations and semantic pathologies. Previous works have predominantly focused\non instance-wise or token-wise cross-modal alignment, often neglecting the\nimportance of pathological-level consistency. This paper presents a novel\nframework PLACE that promotes the Pathological-Level Alignment and enriches the\nfine-grained details via Correlation Exploration without additional human\nannotations. Specifically, we propose a novel pathological-level cross-modal\nalignment (PCMA) approach to maximize the consistency of pathology observations\nfrom both images and reports. To facilitate this, a Visual Pathology\nObservation Extractor is introduced to extract visual pathological observation\nrepresentations from localized tokens. The PCMA module operates independently\nof any external disease annotations, enhancing the generalizability and\nrobustness of our methods. Furthermore, we design a proxy task that enforces\nthe model to identify correlations among image patches, thereby enriching the\nfine-grained details crucial for various downstream tasks. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance on multiple downstream tasks, including classification,\nimage-to-text retrieval, semantic segmentation, object detection and report\ngeneration.", "comment": "12 pages, 10 tables and 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10573v1", "AI": {"title_translation": "通过病理级跨模态对齐和关联探索改进医学视觉表示学习", "tldr": "本文提出了一种名为PLACE的新框架，通过病理级跨模态对齐（PCMA）和关联探索来改进医学视觉表示学习，解决了现有方法忽视病理级一致性的问题，并在多项下游任务中实现了最先进的性能。", "motivation": "由于医学领域数据稀缺问题，从图像-报告对中学习医学视觉表示受到了越来越多的关注。主要挑战在于冗长的报告具有复杂的语篇关系和语义病理。以往的工作主要集中在实例级或令牌级跨模态对齐，往往忽略了病理级一致性的重要性。", "method": "本文提出了一种名为PLACE的新颖框架，它通过病理级对齐并利用关联探索丰富细粒度细节，且无需额外的人工标注。具体来说，我们提出了一种新颖的病理级跨模态对齐（PCMA）方法，以最大化图像和报告中病理观察的一致性。为了实现这一点，引入了一个视觉病理观察提取器，从局部令牌中提取视觉病理观察表示。PCMA模块独立于任何外部疾病注释，增强了方法的通用性和鲁棒性。此外，我们设计了一个代理任务，强制模型识别图像块之间的关联，从而丰富对各种下游任务至关重要的细粒度细节。", "result": "实验结果表明，我们提出的框架在包括分类、图像到文本检索、语义分割、目标检测和报告生成在内的多个下游任务中取得了新的最先进性能。", "conclusion": "本文提出的PLACE框架通过病理级跨模态对齐和关联探索，有效解决了医学视觉表示学习中的关键挑战，并在多项下游任务中取得了显著的性能提升，证明了其在医学领域应用中的潜力。", "translation": "从图像-报告对中进行联合学习以获取医学视觉表示，因其在缓解医学领域数据稀缺问题方面的潜力而受到越来越多的研究关注。主要挑战源于报告冗长、语篇关系复杂和语义病理。以往的工作主要集中在实例级或令牌级跨模态对齐，往往忽视了病理级一致性的重要性。本文提出了一种名为PLACE的新颖框架，它通过病理级对齐并利用关联探索丰富细粒度细节，且无需额外的人工标注。具体来说，我们提出了一种新颖的病理级跨模态对齐（PCMA）方法，以最大化图像和报告中病理观察的一致性。为了实现这一点，引入了一个视觉病理观察提取器，从局部令牌中提取视觉病理观察表示。PCMA模块独立于任何外部疾病注释，增强了方法的通用性和鲁棒性。此外，我们设计了一个代理任务，强制模型识别图像块之间的关联，从而丰富对各种下游任务至关重要的细粒度细节。实验结果表明，我们提出的框架在包括分类、图像到文本检索、语义分割、目标检测和报告生成在内的多个下游任务中取得了新的最先进性能。", "summary": "本文提出了一种名为PLACE的创新框架，旨在通过病理级跨模态对齐（PCMA）和关联探索来改进医学视觉表示学习。针对现有方法忽视病理级一致性以及医学报告复杂性带来的挑战，PLACE引入了视觉病理观察提取器和无需外部注释的PCMA模块，以增强图像与报告间病理观察的一致性。同时，通过设计代理任务来探索图像块间的关联，丰富了细粒度特征。实验证明，PLACE在多项下游任务（如分类、检索、分割、检测和报告生成）中均达到了最先进的性能。", "keywords": "医学视觉表示学习, 跨模态对齐, 病理级, 关联探索, 医学图像报告", "comments": "该论文的创新点在于提出了“病理级”的跨模态对齐，这与以往专注于实例级或令牌级的方法不同，更符合医学诊断的实际需求。通过引入不依赖外部注释的PCMA模块和关联探索的代理任务，提高了模型的通用性和对细粒度信息的捕捉能力。其在多项下游任务中取得SOTA表现，表明了该方法在解决医学图像-文本联合学习中数据稀缺和语义复杂性问题方面的巨大潜力。"}}
{"id": "2506.10577", "title": "Graph Neural Networks for Automatic Addition of Optimizing Components in Printed Circuit Board Schematics", "authors": ["Pascal Plettenberg", "André Alcalde", "Bernhard Sick", "Josephine M. Thomas"], "summary": "The design and optimization of Printed Circuit Board (PCB) schematics is\ncrucial for the development of high-quality electronic devices. Thereby, an\nimportant task is to optimize drafts by adding components that improve the\nrobustness and reliability of the circuit, e.g., pull-up resistors or\ndecoupling capacitors. Since there is a shortage of skilled engineers and\nmanual optimizations are very time-consuming, these best practices are often\nneglected. However, this typically leads to higher costs for troubleshooting in\nlater development stages as well as shortened product life cycles, resulting in\nan increased amount of electronic waste that is difficult to recycle. Here, we\npresent an approach for automating the addition of new components into PCB\nschematics by representing them as bipartite graphs and utilizing a node pair\nprediction model based on Graph Neural Networks (GNNs). We apply our approach\nto three highly relevant PCB design optimization tasks and compare the\nperformance of several popular GNN architectures on real-world datasets labeled\nby human experts. We show that GNNs can solve these problems with high accuracy\nand demonstrate that our approach offers the potential to automate PCB design\noptimizations in a time- and cost-efficient manner.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10577v1", "AI": {"title_translation": "图神经网络在印刷电路板原理图中自动添加优化组件的应用", "tldr": "本文提出了一种利用图神经网络自动向PCB原理图添加优化组件的方法，以解决熟练工程师短缺和手动优化耗时的问题，并在实际数据上展示了高精度和省时省成本的潜力。", "motivation": "印刷电路板 (PCB) 原理图的设计和优化对高质量电子设备的开发至关重要。通过添加优化组件（如上拉电阻或去耦电容）来提高电路的鲁棒性和可靠性是一项重要任务。由于熟练工程师短缺和手动优化非常耗时，这些最佳实践常常被忽视，导致后期开发阶段的故障排除成本增加以及产品生命周期缩短，从而增加了难以回收的电子垃圾量。", "method": "将PCB原理图表示为二分图，并利用基于图神经网络 (GNNs) 的节点对预测模型，自动化地向PCB原理图添加新组件。该方法应用于三个高度相关的PCB设计优化任务，并比较了多种流行GNN架构在人类专家标注的真实世界数据集上的性能。", "result": "GNNs 可以高精度地解决这些问题。", "conclusion": "该方法提供了一种时间高效且成本高效的自动化PCB设计优化的潜力。", "translation": "印刷电路板 (PCB) 原理图的设计和优化对于开发高质量电子设备至关重要。其中，一项重要任务是通过添加改善电路鲁棒性和可靠性的组件（例如，上拉电阻或去耦电容器）来优化草图。由于熟练工程师短缺且手动优化非常耗时，这些最佳实践常常被被忽略。然而，这通常会导致后期开发阶段的故障排除成本更高以及产品生命周期缩短，从而增加了难以回收的电子垃圾量。在此，我们提出一种通过将PCB原理图表示为二分图并利用基于图神经网络 (GNNs) 的节点对预测模型，实现自动向PCB原理图添加新组件的方法。我们将我们的方法应用于三个高度相关的PCB设计优化任务，并比较了多种流行GNN架构在人类专家标注的真实世界数据集上的性能。我们展示了GNNs 可以高精度地解决这些问题，并证明我们的方法提供了以省时省成本的方式自动化PCB设计优化的潜力。", "summary": "本文提出了一种基于图神经网络 (GNNs) 的方法，通过将PCB原理图表示为二分图并利用节点对预测模型，实现自动向印刷电路板原理图添加优化组件。该方法旨在解决当前PCB设计中手动优化耗时且易被忽视的问题。研究在三个实际PCB设计优化任务上进行了验证，并与多种GNN架构进行了比较，结果表明GNNs能够高精度地完成这些任务，证明了该方法在自动化PCB设计优化方面的效率和潜力。", "keywords": "图神经网络, PCB设计, 自动优化, 组件添加, 节点对预测", "comments": "这项研究通过引入图神经网络来自动化PCB原理图中的优化组件添加，解决了行业中熟练工程师短缺和手动优化效率低下的痛点。其创新性在于将PCB原理图建模为二分图并应用GNN进行节点对预测，这是一种新颖且有效的自动化设计优化方法。该方法不仅能提高设计效率，降低成本，还能间接减少电子垃圾，具有重要的实际应用价值。"}}
{"id": "2506.10920", "title": "Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization", "authors": ["Or Shafran", "Atticus Geiger", "Mor Geva"], "summary": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10920v1", "AI": {"title_translation": "通过半非负矩阵分解将MLP激活分解为可解释特征", "tldr": "本文提出使用半非负矩阵分解（SNMF）直接分解MLP激活，以识别可解释的特征，该方法在因果评估和人类可解释性方面优于稀疏自编码器（SAEs）。", "motivation": "现有的大语言模型（LLMs）可解释性研究主要集中于识别激活空间中的方向，但常用的稀疏自编码器（SAEs）在因果评估中表现不佳且缺乏内在可解释性，因为它们的学习与模型计算的关联不明确。因此，需要一种新的、无监督的方法来寻找能够捕获可解释特征的方向，并解决SAEs的局限性。", "method": "本文通过使用半非负矩阵分解（SNMF）直接分解多层感知器（MLP）的激活来解决现有方法的局限性。SNMF学习到的特征具有两个关键特性：(a) 它们是共同激活神经元的稀疏线性组合；(b) 它们被映射到其激活输入，从而使其能够直接被解释。", "result": "在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF导出的特征在因果操控方面优于稀疏自编码器（SAEs）和强大的监督基线（均值差异法），同时与人类可解释概念保持一致。进一步分析揭示，特定的神经元组合在语义相关特征中被重复利用，揭示了MLP激活空间中的分层结构。", "conclusion": "这些结果共同表明，SNMF是一种简单而有效的工具，可用于识别可解释特征并剖析大型语言模型中的概念表示。", "translation": "机械可解释性的核心目标是识别大型语言模型（LLMs）中能够因果解释其输出的正确分析单元。早期工作侧重于单个神经元，但神经元通常编码多个概念的证据促使人们转向分析激活空间中的方向。一个关键问题是如何以无监督的方式找到能够捕获可解释特征的方向。当前方法依赖于稀疏自编码器（SAEs）的字典学习，通常在残差流激活上训练，以从头开始学习方向。然而，SAEs在因果评估中常常表现不佳，并且缺乏内在可解释性，因为它们的学习与模型的计算没有明确关联。\n本文通过使用半非负矩阵分解（SNMF）直接分解MLP激活来解决这些局限性，使得学习到的特征(a)是共同激活神经元的稀疏线性组合，并且(b)映射到它们的激活输入，从而使其能够直接被解释。在Llama 3.1、Gemma 2和GPT-2上的实验表明，SNMF导出的特征在因果操控方面优于SAEs和强大的监督基线（均值差异法），同时与人类可解释概念保持一致。进一步分析揭示，特定的神经元组合在语义相关特征中被重复利用，揭示了MLP激活空间中的分层结构。总而言之，这些结果将SNMF定位为一种简单有效的工具，用于识别可解释特征和剖析LLMs中的概念表示。", "summary": "本文提出了一种新颖的方法，即通过半非负矩阵分解（SNMF）直接分解大型语言模型（LLMs）中多层感知器（MLP）的激活，以识别可解释的特征。与现有方法如稀疏自编码器（SAEs）在因果评估和内在可解释性方面的局限性不同，SNMF能够学习到神经元稀疏组合的特征，并将其直接映射到激活输入，从而增强了可解释性。实验结果表明，SNMF在Llama 3.1、Gemma 2和GPT-2上在因果操控表现优于SAEs和监督基线，并且与人类可解释概念对齐，同时揭示了MLP激活空间中的分层结构。这表明SNMF是LLMs中特征识别和概念表示剖析的有效工具。", "keywords": "机械可解释性, 大语言模型, 半非负矩阵分解, MLP激活, 可解释特征", "comments": "本文的创新之处在于直接利用半非负矩阵分解（SNMF）来分解MLP激活，从而实现了比稀疏自编码器（SAEs）更强的因果可解释性和内在可解释性。将学习到的特征直接映射到激活输入，使得特征的解释性更强，这是其重要贡献。此外，发现特定的神经元组合在不同特征间复用，并揭示了MLP激活空间的分层结构，为深入理解LLM内部机制提供了新视角。"}}
{"id": "2506.10934", "title": "Dynamic Epistemic Friction in Dialogue", "authors": ["Timothy Obiso", "Kenneth Lai", "Abhijnan Nath", "Nikhil Krishnaswamy", "James Pustejovsky"], "summary": "Recent developments in aligning Large Language Models (LLMs) with human\npreferences have significantly enhanced their utility in human-AI collaborative\nscenarios. However, such approaches often neglect the critical role of\n\"epistemic friction,\" or the inherent resistance encountered when updating\nbeliefs in response to new, conflicting, or ambiguous information. In this\npaper, we define dynamic epistemic friction as the resistance to epistemic\nintegration, characterized by the misalignment between an agent's current\nbelief state and new propositions supported by external evidence. We position\nthis within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,\n2011), where friction emerges as nontrivial belief-revision during the\ninteraction. We then present analyses from a situated collaborative task that\ndemonstrate how this model of epistemic friction can effectively predict belief\nupdates in dialogues, and we subsequently discuss how the model of belief\nalignment as a measure of epistemic resistance or friction can naturally be\nmade more sophisticated to accommodate the complexities of real-world dialogue\nscenarios.", "comment": "11 pages, 2 figures, 2 tables, CoNLL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10934v1", "AI": {"title_translation": "对话中的动态认知摩擦", "tldr": "大型语言模型（LLMs）在人机协作中取得了进展，但忽视了“认知摩擦”——即更新信念时遇到的内在阻力。本文定义了动态认知摩擦，并将其置于动态认知逻辑框架下，通过一项协作任务的分析，证明该模型能有效预测对话中的信念更新，并讨论了其在现实世界对话中的应用潜力。", "motivation": "现有的将大型语言模型（LLMs）与人类偏好对齐的方法，忽视了“认知摩擦”的关键作用，即在响应新的、冲突的或模糊的信息时更新信念所遇到的内在阻力。", "method": "本文将动态认知摩擦定义为认知整合的阻力，其特征是智能体当前信念状态与外部证据支持的新命题之间的不一致。研究将其置于动态认知逻辑框架内，其中摩擦表现为交互过程中非平凡的信念修正。随后，通过一项情境化协作任务的分析来验证该模型。", "result": "该认知摩擦模型能够有效地预测对话中的信念更新。该信念对齐模型作为认知阻力或摩擦的衡量标准，可以自然地变得更复杂，以适应现实世界对话场景的复杂性。", "conclusion": "认知摩擦模型能够有效预测对话中的信念更新，并且可以进一步完善以适应现实世界对话的复杂性，从而更好地理解和处理人机协作中的信念对齐问题。", "translation": "标题：对话中的动态认知摩擦\n\n摘要：\n近期在将大型语言模型（LLMs）与人类偏好对齐方面的进展显著增强了它们在人机协作场景中的效用。然而，这些方法通常忽视了“认知摩擦”的关键作用，即在响应新的、冲突的或模糊的信息时更新信念所遇到的内在阻力。在本文中，我们将动态认知摩擦定义为认知整合的阻力，其特征是智能体当前信念状态与外部证据支持的新命题之间的不一致。我们将其置于动态认知逻辑（Van Benthem和Pacuit，2011）的框架内，其中摩擦表现为交互过程中非平凡的信念修正。然后，我们从一项情境化协作任务中提出了分析，该分析表明这种认知摩擦模型如何有效地预测对话中的信念更新，随后我们讨论了如何自然地使信念对齐模型作为认知阻力或摩擦的衡量标准变得更加复杂，以适应现实世界对话场景的复杂性。", "summary": "本文探讨了大型语言模型在人机协作中被忽视的“认知摩擦”问题，将其定义为信念更新的阻力。研究将动态认知摩擦置于动态认知逻辑框架下，并在一项协作任务中验证了该模型能有效预测对话中的信念更新。文章指出，该模型可进一步完善以适应复杂现实对话。", "keywords": "认知摩擦, 对话, 大型语言模型, 信念更新, 动态认知逻辑", "comments": "这篇论文通过引入“动态认知摩擦”的概念，为理解LLM在人机协作中信念更新的复杂性提供了新的视角。它将心理学概念与逻辑学框架相结合，展示了理论模型在预测实际对话行为方面的潜力。其创新点在于强调了信念更新过程中的阻力，这对于提升LLM在复杂、不确定环境中的表现至关重要。"}}
{"id": "2506.10575", "title": "Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning", "authors": ["Chun-Mei Feng", "Kai Yu", "Xinxing Xu", "Salman Khan", "Rick Siow Mong Goh", "Wangmeng Zuo", "Yong Liu"], "summary": "Benefited from image-text contrastive learning, pre-trained vision-language\nmodels, e.g., CLIP, allow to direct leverage texts as images (TaI) for\nparameter-efficient fine-tuning (PEFT). While CLIP is capable of making image\nfeatures to be similar to the corresponding text features, the modality gap\nremains a nontrivial issue and limits image recognition performance of TaI.\nUsing multi-label image recognition (MLR) as an example, we present a novel\nmethod, called T2I-PAL to tackle the modality gap issue when using only text\ncaptions for PEFT. The core design of T2I-PAL is to leverage pre-trained\ntext-to-image generation models to generate photo-realistic and diverse images\nfrom text captions, thereby reducing the modality gap. To further enhance MLR,\nT2I-PAL incorporates a class-wise heatmap and learnable prototypes. This\naggregates local similarities, making the representation of local visual\nfeatures more robust and informative for multi-label recognition. For better\nPEFT, we further combine both prompt tuning and adapter learning to enhance\nclassification performance. T2I-PAL offers significant advantages: it\neliminates the need for fully semantically annotated training images, thereby\nreducing the manual annotation workload, and it preserves the intrinsic mode of\nthe CLIP model, allowing for seamless integration with any existing CLIP\nframework. Extensive experiments on multiple benchmarks, including MS-COCO,\nVOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance\nby 3.47% in average above the top-ranked state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10575v1", "AI": {"title_translation": "文本到图像用于多标签图像识别，结合联合提示-适配器学习", "tldr": "本文提出T2I-PAL方法，通过文本到图像生成来弥合CLIP模型中文本作为图像使用的模态鸿沟，并结合提示词与适配器学习，显著提升多标签图像识别性能，同时减少了对大量标注图像的需求。", "motivation": "现有预训练视觉-语言模型（如CLIP）在将文本作为图像（TaI）进行参数高效微调（PEFT）时，存在模态鸿沟问题，限制了图像识别性能，尤其是在多标签图像识别（MLR）中。此外，对大量语义标注训练图像的需求增加了人工标注负担。", "method": "本文提出T2I-PAL方法，其核心设计是利用预训练的文本到图像生成模型从文本描述中生成逼真且多样化的图像，从而缩小模态鸿沟。为进一步增强MLR，T2I-PAL融入了类别热力图和可学习原型，以聚合局部相似性，使局部视觉特征的表示更鲁棒和信息丰富。为了更好的PEFT，该方法进一步结合了提示词微调和适配器学习来提升分类性能。", "result": "在MS-COCO、VOC2007和NUS-WIDE等多个基准测试中，T2I-PAL的识别性能比现有最先进方法平均提升了3.47%。该方法消除了对完全语义标注训练图像的需求，减少了人工标注工作量，并保留了CLIP模型的内在模式，可与现有CLIP框架无缝集成。", "conclusion": "T2I-PAL通过利用文本到图像生成和联合提示-适配器学习，有效解决了使用文本作为图像进行多标签图像识别时的模态鸿沟问题，显著提升了性能，并降低了数据标注成本，展现出强大的实用性和兼容性。", "translation": "受益于图像-文本对比学习，预训练的视觉-语言模型，例如CLIP，允许直接利用文本作为图像（TaI）进行参数高效微调（PEFT）。尽管CLIP能够使图像特征与相应的文本特征相似，但模态鸿沟仍然是一个不容忽视的问题，并限制了TaI的图像识别性能。以多标签图像识别（MLR）为例，我们提出了一种新颖的方法，称为T2I-PAL，用于解决仅使用文本描述进行PEFT时遇到的模态鸿沟问题。T2I-PAL的核心设计是利用预训练的文本到图像生成模型，从文本描述中生成逼真且多样化的图像，从而缩小模态鸿沟。为了进一步增强MLR，T2I-PAL结合了类别热力图和可学习原型。这聚合了局部相似性，使得局部视觉特征的表示对于多标签识别更加鲁棒和信息丰富。为了更好的PEFT，我们进一步结合了提示词微调和适配器学习来增强分类性能。T2I-PAL具有显著优势：它消除了对完全语义标注训练图像的需求，从而减少了人工标注工作量，并且它保留了CLIP模型的内在模式，允许与任何现有CLIP框架无缝集成。在包括MS-COCO、VOC2007和NUS-WIDE在内的多个基准测试中进行的广泛实验表明，我们的T2I-PAL可以将识别性能平均提升3.47%，超过了排名靠前的最先进方法。", "summary": "本文提出T2I-PAL方法，旨在解决基于CLIP的参数高效微调中，利用文本作为图像进行多标签图像识别时存在的模态鸿沟问题。T2I-PAL通过文本到图像生成模型生成图像以弥合鸿沟，并结合类别热力图、可学习原型以及联合提示词与适配器学习来增强识别性能。该方法不仅显著提升了多标签识别的准确性（平均提升3.47%），还减少了对大量语义标注图像的依赖，并保持了与现有CLIP框架的兼容性。", "keywords": "多标签图像识别, 文本到图像生成, 模态鸿沟, 参数高效微调, CLIP", "comments": "T2I-PAL的创新之处在于巧妙地将文本到图像生成技术引入到解决视觉-语言模型中的模态鸿沟问题，特别是在多标签图像识别任务中。通过生成合成图像来弥补真实图像标注的不足，这不仅降低了数据标注成本，也为后续的PEFT提供了更丰富的训练信号。结合提示词和适配器学习，进一步优化了模型性能。该方法对资源受限或数据稀缺的多标签识别场景具有重要意义，其兼容性也使其易于推广。"}}
{"id": "2506.10616", "title": "Non-stationary Online Learning for Curved Losses: Improved Dynamic Regret via Mixability", "authors": ["Yu-Jie Zhang", "Peng Zhao", "Masashi Sugiyama"], "summary": "Non-stationary online learning has drawn much attention in recent years.\nDespite considerable progress, dynamic regret minimization has primarily\nfocused on convex functions, leaving the functions with stronger curvature\n(e.g., squared or logistic loss) underexplored. In this work, we address this\ngap by showing that the regret can be substantially improved by leveraging the\nconcept of mixability, a property that generalizes exp-concavity to effectively\ncapture loss curvature. Let $d$ denote the dimensionality and $P_T$ the path\nlength of comparators that reflects the environmental non-stationarity. We\ndemonstrate that an exponential-weight method with fixed-share updates achieves\nan $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ dynamic regret for mixable losses,\nimproving upon the best-known $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$\nresult (Baby and Wang, 2021) in $d$. More importantly, this improvement arises\nfrom a simple yet powerful analytical framework that exploits the mixability,\nwhich avoids the Karush-Kuhn-Tucker-based analysis required by existing work.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10616v1", "AI": {"title_translation": "弯曲损失函数的非平稳在线学习：通过可混合性改进动态后悔", "tldr": "本文通过引入可混合性概念，改进了非平稳在线学习中弯曲损失函数的动态后悔界限，并提出了一种更简单的分析框架。", "motivation": "动态后悔最小化的研究主要集中在凸函数上，而对具有更强曲率的函数（例如平方损失或逻辑损失）探索不足。", "method": "引入并利用了“可混合性”概念（一种推广了指数凹性的性质，能有效捕捉损失曲率），并采用了一种带有固定共享更新的指数权重方法。", "result": "对于可混合损失函数，该方法实现了 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ 的动态后悔，相较于现有最佳结果 $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$，在维度 $d$ 上有所改进。", "conclusion": "通过利用可混合性，该研究提供了一个简单而强大的分析框架，避免了现有工作所需的基于KKT的分析，从而在处理弯曲损失函数时显著改进了动态后悔。", "translation": "近年来，非平稳在线学习备受关注。尽管取得了相当大的进展，但动态后悔最小化主要集中在凸函数上，对具有更强曲率的函数（例如平方损失或逻辑损失）探索不足。在这项工作中，我们通过展示利用可混合性概念可以显著改善后悔来弥补这一差距，可混合性是一种推广了指数凹性并能有效捕捉损失曲率的性质。令 $d$ 表示维度，$P_T$ 表示反映环境非平稳性的比较器的路径长度。我们证明，一种带有固定共享更新的指数权重方法，对于可混合损失函数，实现了 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$ 的动态后悔，这在 $d$ 上改进了现有最佳结果 $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$（Baby 和 Wang，2021）。更重要的是，这种改进源于一个简单而强大的分析框架，该框架利用了可混合性，避免了现有工作所需的基于Karush-Kuhn-Tucker的分析。", "summary": "本文针对非平稳在线学习中弯曲损失函数（如平方或逻辑损失）的动态后悔最小化问题。通过引入并利用“可混合性”概念，该研究提出了一种带有固定共享更新的指数权重方法，成功地将动态后悔界限从 $\\mathcal{O}(d^{10/3} T^{1/3} P_T^{2/3} \\log T)$ 改进到 $\\mathcal{O}(d T^{1/3} P_T^{2/3} \\log T)$。此项改进不仅获得了更优的维度依赖性，而且得益于一个全新的、无需复杂KKT分析的简洁分析框架。", "keywords": "非平稳在线学习, 动态后悔, 弯曲损失, 可混合性, 指数权重方法", "comments": "该论文的创新点在于引入“可混合性”概念来处理非凸的弯曲损失函数，这是现有研究的空白。其提出的分析框架不仅简化了理论推导，还显著改进了动态后悔界限，特别是对维度 $d$ 的依赖性。这为非平稳在线学习在更广泛的损失函数类型上的应用提供了新的理论基础和方法。"}}
{"id": "2506.10952", "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training", "authors": ["Mozhi Zhang", "Howe Tissue", "Lu Wang", "Xipeng Qiu"], "summary": "We introduce~\\textsc{Domain2Vec}, a novel approach that decomposes any\ndataset into a linear combination of several \\emph{meta-domains}, a new concept\ndesigned to capture the key underlying features of datasets.\n\\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\emph{\\textbf{D}istribution\n\\textbf{A}lignment \\textbf{A}ssumption} (DA$^{2}$), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, \\textsc{Domain2vec} can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\n\\textsc{Domain2Vec} helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\n\\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only\n$51.5\\%$ of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, \\textsc{Domain2Vec} improves\ndownstream performance by an average of $2.83\\%$.", "comment": "Accepted to ICML2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10952v1", "AI": {"title_translation": "Domain2Vec：向量化数据集以无需训练找到最佳数据混合", "tldr": "Domain2Vec通过将数据集分解为元域向量，无需训练即可找到语言模型预训练的最佳数据混合，显著提高效率并降低计算成本。", "motivation": "现有方法在寻找语言模型预训练的最佳数据混合时可能效率不高或计算成本高昂。本文旨在提供一种无需训练即可找到最佳数据混合的方法，以提高效率和可扩展性。", "method": "本文引入了Domain2Vec，这是一种将任何数据集分解为几个“元域”（旨在捕捉数据集关键底层特征的新概念）的线性组合的新方法。Domain2Vec维护一个元域词汇表，并使用分类器将给定数据集分解为对应于该词汇表分布的域向量。这些域向量在“分布对齐假设”（DA²）下，以无需训练的方式识别语言模型（LM）预训练的最佳数据混合。DA²假设当训练集和验证集的数据分布更好地对齐时，可以实现更低的验证损失。此外，Domain2Vec可以无缝集成到以前的工作中，以建模域向量和LM性能之间的关系，从而大大提高效率和可扩展性。", "result": "广泛的实验表明，Domain2Vec有助于找到能够以最小计算开销增强下游任务性能的数据混合。具体来说，Domain2Vec在Pile-CC上实现了相同的验证损失，而所需的计算量仅为在原始The Pile数据集上训练所需计算量的51.5%。在同等计算预算下，Domain2Vec将下游性能平均提高了2.83%。", "conclusion": "Domain2Vec是一种有效且计算效率高的方法，可以在无需训练的情况下找到语言模型预训练的最佳数据混合，从而显著提高下游任务性能并减少计算资源消耗。", "translation": "我们引入了\\textsc{Domain2Vec}，这是一种新颖的方法，它将任何数据集分解为几个\\emph{元域}的线性组合，元域是一个旨在捕获数据集关键底层特征的新概念。\\textsc{Domain2Vec}维护一个元域词汇表，并使用分类器将任何给定数据集分解为一个域向量，该向量对应于此词汇表上的分布。这些域向量能够在\\emph{\\textbf{分}布\\textbf{对}齐\\textbf{假}设}（DA²）下，以无需训练的方式识别语言模型（LM）预训练的最佳数据混合，该假设表明当训练集和验证集的数据分布更好地对齐时，可以实现更低的验证损失。此外，\\textsc{Domain2Vec}可以无缝集成到以前的工作中，以建模域向量和LM性能之间的关系，大大提高了以前方法的效率和可扩展性。广泛的实验表明，\\textsc{Domain2Vec}有助于找到能够以最小计算开销增强下游任务性能的数据混合。具体来说，\\textsc{Domain2Vec}在Pile-CC上实现了相同的验证损失，而所需的计算量仅为在原始The Pile数据集上训练所需计算量的51.5%。在同等计算预算下，\\textsc{Domain2Vec}将下游性能平均提高了2.83%。", "summary": "本文提出了Domain2Vec，一种将数据集向量化的新方法，通过将其分解为“元域”的线性组合来捕捉其核心特征。该方法生成域向量，利用“分布对齐假设”在无需训练的情况下识别语言模型预训练的最佳数据混合。实验证明，Domain2Vec能显著降低计算成本（在Pile-CC上节省近一半计算量）并提高下游任务性能（平均提高2.83%），同时可无缝集成到现有工作中以提升效率和可扩展性。", "keywords": "数据集向量化, 数据混合优化, 语言模型预训练, 元域, 分布对齐假设", "comments": "Domain2Vec的创新之处在于提出了“元域”概念并将数据集向量化，从而实现了无需实际训练即可优化数据混合，这对于大规模语言模型预训练具有重要意义。其主要优势是显著降低了计算成本和时间，这对于资源受限或需要快速迭代的场景非常有价值。它通过“分布对齐假设”连接了数据分布和模型性能，提供了一种新的优化视角。"}}
{"id": "2506.10576", "title": "Harmonizing Geometry and Uncertainty: Diffusion with Hyperspheres", "authors": ["Muskan Dosi", "Chiranjeev Chiranjeev", "Kartik Thakral", "Mayank Vatsa", "Richa Singh"], "summary": "Do contemporary diffusion models preserve the class geometry of\nhyperspherical data? Standard diffusion models rely on isotropic Gaussian noise\nin the forward process, inherently favoring Euclidean spaces. However, many\nreal-world problems involve non-Euclidean distributions, such as hyperspherical\nmanifolds, where class-specific patterns are governed by angular geometry\nwithin hypercones. When modeled in Euclidean space, these angular subtleties\nare lost, leading to suboptimal generative performance. To address this\nlimitation, we introduce HyperSphereDiff to align hyperspherical structures\nwith directional noise, preserving class geometry and effectively capturing\nangular uncertainty. We demonstrate both theoretically and empirically that\nthis approach aligns the generative process with the intrinsic geometry of\nhyperspherical data, resulting in more accurate and geometry-aware generative\nmodels. We evaluate our framework on four object datasets and two face\ndatasets, showing that incorporating angular uncertainty better preserves the\nunderlying hyperspherical manifold. Resources are available at:\n{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/}", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10576v1", "AI": {"title_translation": "协调几何与不确定性：超球面扩散", "tldr": "现有扩散模型在超球面数据上表现不佳，因为它们偏爱欧几里得空间。本文提出HyperSphereDiff，通过引入定向噪声来更好地处理超球面几何和角不确定性，从而提高生成模型的性能。", "motivation": "现有扩散模型依赖各向同性高斯噪声，偏爱欧几里得空间，导致在处理超球面流形等非欧几里得数据时，会丢失类特有的角度几何信息，从而生成性能不佳。", "method": "本文引入HyperSphereDiff模型，通过将超球面结构与定向噪声对齐，从而保留类几何并有效捕获角度不确定性。", "result": "理论和经验表明，HyperSphereDiff方法将生成过程与超球面数据的内在几何对齐，从而产生更准确和几何感知的生成模型。在四个对象数据集和两个人脸数据集上的评估显示，结合角度不确定性更好地保留了底层超球面流形。", "conclusion": "通过将超球面结构与定向噪声对齐，可以有效处理超球面数据，提高扩散模型的生成性能，使其更准确且几何感知。", "translation": "当代扩散模型是否保留了超球面数据的类别几何？标准扩散模型在前向过程中依赖各向同性高斯噪声，本质上偏爱欧几里得空间。然而，许多现实世界问题涉及非欧几里得分布，例如超球面流形，其中类别特定的模式受超圆锥内的角度几何控制。当在欧几里得空间中建模时，这些角度的微妙之处会丢失，导致次优的生成性能。为了解决这一限制，我们引入了 HyperSphereDiff，以将超球面结构与定向噪声对齐，从而保留类别几何并有效捕获角度不确定性。我们从理论和经验上证明，这种方法将生成过程与超球面数据的内在几何对齐，从而产生更准确和几何感知的生成模型。我们在四个对象数据集和两个人脸数据集上评估了我们的框架，结果表明，结合角度不确定性更好地保留了底层超球面流形。资源可在：{https://github.com/IAB-IITJ/Harmonizing-Geometry-and-Uncertainty-Diffusion-with-Hyperspheres/} 获取。", "summary": "本文提出了HyperSphereDiff，一个旨在解决现有扩散模型在处理超球面数据时几何信息丢失问题的模型。通过引入定向噪声并将其与超球面结构对齐，HyperSphereDiff能够保留类别几何并有效捕获角度不确定性。理论和实验结果表明，该方法能生成更准确、几何感知的模型，并在多个数据集上验证了其在保留超球面流形方面的优越性。", "keywords": "扩散模型, 超球面数据, 几何感知, 定向噪声, 不确定性", "comments": "这篇论文的创新点在于认识到标准扩散模型在处理非欧几里得（特别是超球面）数据时的局限性，并提出了一种新颖的方法HyperSphereDiff，通过引入定向噪声来更好地适应数据的内在几何。这对于提高扩散模型在复杂、非欧几里得数据集上的生成性能具有重要意义。"}}
{"id": "2506.10617", "title": "Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code", "authors": ["Reza Karbasi", "Masoud Rahimi", "Abdol-Hossein Vahabie", "Hadi Moradi"], "summary": "This paper addresses the persistent challenge of accurately digitizing\npaper-based electrocardiogram (ECG) recordings, with a particular focus on\nrobustly handling single leads compromised by signal overlaps-a common yet\nunder-addressed issue in existing methodologies. We propose a two-stage\npipeline designed to overcome this limitation. The first stage employs a U-Net\nbased segmentation network, trained on a dataset enriched with overlapping\nsignals and fortified with custom data augmentations, to accurately isolate the\nprimary ECG trace. The subsequent stage converts this refined binary mask into\na time-series signal using established digitization techniques, enhanced by an\nadaptive grid detection module for improved versatility across different ECG\nformats and scales. Our experimental results demonstrate the efficacy of our\napproach. The U-Net architecture achieves an IoU of 0.87 for the fine-grained\nsegmentation task. Crucially, our proposed digitization method yields superior\nperformance compared to a well-established baseline technique across both\nnon-overlapping and challenging overlapping ECG samples. For non-overlapping\nsignals, our method achieved a Mean Squared Error (MSE) of 0.0010 and a Pearson\nCorrelation Coefficient (rho) of 0.9644, compared to 0.0015 and 0.9366,\nrespectively, for the baseline. On samples with signal overlap, our method\nachieved an MSE of 0.0029 and a rho of 0.9641, significantly improving upon the\nbaseline's 0.0178 and 0.8676. This work demonstrates an effective strategy to\nsignificantly enhance digitization accuracy, especially in the presence of\nsignal overlaps, thereby laying a strong foundation for the reliable conversion\nof analog ECG records into analyzable digital data for contemporary research\nand clinical applications. The implementation is publicly available at this\nGitHub repository: https://github.com/masoudrahimi39/ECG-code.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10617v1", "AI": {"title_translation": "基于深度学习的重叠心电图图像数字化与开源Python代码", "tldr": "本文提出了一种基于深度学习的两阶段方法，用于准确数字化纸质心电图记录，特别是解决了信号重叠的问题，并提供了开源代码。", "motivation": "现有心电图数字化方法在处理信号重叠的单导联纸质心电图记录时存在准确性不足的问题，而信号重叠是常见但未被充分解决的挑战。", "method": "提出一个两阶段流程：第一阶段使用基于U-Net的分割网络，在包含重叠信号的增强数据集上训练，以准确分离主心电图轨迹；第二阶段将二进制掩码转换为时间序列信号，通过自适应网格检测模块增强传统数字化技术，以适应不同心电图格式和比例。", "result": "U-Net架构在精细分割任务中实现了0.87的IoU。与基线技术相比，该方法在非重叠信号上MSE为0.0010，Pearson相关系数(rho)为0.9644（基线分别为0.0015和0.9366）；在信号重叠样本上，MSE为0.0029，rho为0.9641（基线分别为0.0178和0.8676），性能显著优越。", "conclusion": "该工作提出了一种有效策略，显著提高了心电图数字化精度，尤其是在存在信号重叠的情况下，为将模拟心电图记录可靠地转换为可分析的数字数据奠定了坚实基础，适用于当代研究和临床应用。", "translation": "本文解决了纸质心电图（ECG）记录准确数字化的持续挑战，特别关注了鲁棒处理受信号重叠影响的单导联——这是现有方法中常见但未被充分解决的问题。我们提出了一个旨在克服这一限制的两阶段流程。第一阶段采用基于U-Net的分割网络，该网络在一个富含重叠信号并经过自定义数据增强的数据集上训练，以准确隔离主要的心电图轨迹。随后的阶段使用既定的数字化技术将这个精炼的二进制掩码转换为时间序列信号，并通过一个自适应网格检测模块进行增强，以提高在不同心电图格式和比例下的通用性。我们的实验结果证明了我们方法的有效性。U-Net架构在精细分割任务中实现了0.87的IoU。至关重要的是，与一种成熟的基线技术相比，我们提出的数字化方法在非重叠和具有挑战性的重叠心电图样本上均表现出卓越的性能。对于非重叠信号，我们的方法实现了0.0010的均方误差（MSE）和0.9644的皮尔逊相关系数（rho），而基线分别为0.0015和0.9366。在信号重叠样本上，我们的方法实现了0.0029的MSE和0.9641的rho，显著优于基线的0.0178和0.8676。这项工作展示了一种有效策略，可显著提高数字化精度，尤其是在存在信号重叠的情况下，从而为将模拟心电图记录可靠地转换为可分析的数字数据，以用于当代研究和临床应用奠定了坚实基础。该实现已在GitHub仓库公开：https://github.com/masoudrahimi39/ECG-code。", "summary": "本论文提出了一种基于深度学习的两阶段方法，用于解决纸质心电图（ECG）记录的数字化挑战，特别是针对信号重叠问题。该方法首先利用U-Net网络分割出ECG轨迹，然后将其转换为时间序列信号，并通过自适应网格检测提高通用性。实验结果表明，与现有基线方法相比，该方法在处理非重叠和重叠ECG信号时均表现出显著优越的性能，有效提高了数字化精度，为ECG数据的可靠数字化提供了解决方案。", "keywords": "心电图数字化, 深度学习, U-Net, 信号重叠, 开源代码", "comments": "该论文的创新点在于提出了一个两阶段的深度学习流程，特别关注并有效解决了心电图数字化中常见的信号重叠问题。U-Net的应用和自适应网格检测模块的结合提升了方法的鲁棒性和通用性。其开源代码的可用性也大大提高了研究的可复现性和实用性，对临床和研究领域将模拟ECG数据转换为数字格式具有重要意义。"}}
{"id": "2506.10582", "title": "Rethinking Random Masking in Self Distillation on ViT", "authors": ["Jihyeon Seong", "Hyunkyung Han"], "summary": "Vision Transformers (ViTs) have demonstrated remarkable performance across a\nwide range of vision tasks. In particular, self-distillation frameworks such as\nDINO have contributed significantly to these advances. Within such frameworks,\nrandom masking is often utilized to improve training efficiency and introduce\nregularization. However, recent studies have raised concerns that\nindiscriminate random masking may inadvertently eliminate critical semantic\ninformation, motivating the development of more informed masking strategies. In\nthis study, we explore the role of random masking in the self-distillation\nsetting, focusing on the DINO framework. Specifically, we apply random masking\nexclusively to the student's global view, while preserving the student's local\nviews and the teacher's global view in their original, unmasked forms. This\ndesign leverages DINO's multi-view augmentation scheme to retain clean\nsupervision while inducing robustness through masked inputs. We evaluate our\napproach using DINO-Tiny on the mini-ImageNet dataset and show that random\nmasking under this asymmetric setup yields more robust and fine-grained\nattention maps, ultimately enhancing downstream performance.", "comment": "4 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10582v1", "AI": {"title_translation": "重新思考ViT自蒸馏中的随机掩码", "tldr": "本研究探讨了在DINO框架下，对学生模型的全局视图应用非对称随机掩码，以提升ViT的自蒸馏性能和鲁棒性。", "motivation": "近期研究表明，在自蒸馏框架中，不加区分的随机掩码可能会无意中消除关键的语义信息，这促使人们开发更明智的掩码策略。", "method": "本研究在DINO框架下探索了随机掩码的作用，具体做法是：仅对学生模型的全局视图应用随机掩码，同时保留学生模型的局部视图和教师模型的全局视图的原始未掩码形式。这种设计利用DINO的多视图增强方案，在通过掩码输入引入鲁棒性的同时，保留了干净的监督信息。", "result": "在mini-ImageNet数据集上使用DINO-Tiny评估了该方法，结果表明，在这种非对称设置下的随机掩码产生了更鲁棒和细粒度的注意力图，最终提升了下游性能。", "conclusion": "在ViT自蒸馏中，采用非对称随机掩码（仅对学生模型的全局视图进行掩码）可以有效提升模型鲁棒性、注意力图质量以及下游任务性能。", "translation": "视觉Transformer (ViT) 在广泛的视觉任务中展现出卓越的性能。特别是，DINO 等自蒸馏框架对这些进步做出了重大贡献。在此类框架中，随机掩码常用于提高训练效率和引入正则化。然而，最近的研究引起了人们的担忧，即不加区分的随机掩码可能会无意中消除关键的语义信息，这促使人们开发更明智的掩码策略。在本研究中，我们探索了随机掩码在自蒸馏设置中的作用，重点关注 DINO 框架。具体来说，我们仅对学生模型的全局视图应用随机掩码，同时保留学生模型的局部视图和教师模型的全局视图的原始未掩码形式。这种设计利用 DINO 的多视图增强方案，在通过掩码输入引入鲁棒性的同时，保留了干净的监督信息。我们使用 DINO-Tiny 在 mini-ImageNet 数据集上评估了我们的方法，结果表明，在这种非对称设置下的随机掩码产生了更鲁棒和细粒度的注意力图，最终提升了下游性能。", "summary": "本研究重新审视了Vision Transformers (ViTs) 自蒸馏框架中随机掩码的应用，特别是针对DINO框架。鉴于现有随机掩码可能破坏语义信息的担忧，作者提出了一种非对称掩码策略：仅对学生模型的全局视图进行随机掩码，而保持学生局部视图和教师全局视图的未掩码状态。该方法利用DINO的多视图增强特性，在保持监督信息纯净的同时，通过掩码输入增强模型鲁棒性。实验结果表明，这种非对称掩码设置能产生更鲁棒、更精细的注意力图，并有效提升下游任务性能。", "keywords": "Vision Transformers, 自蒸馏, 随机掩码, DINO, 注意力图", "comments": "该论文通过提出一种新颖的非对称随机掩码策略，解决了ViT自蒸馏中随机掩码可能损害语义信息的问题。其创新点在于精确控制掩码区域，在引入正则化的同时，最大限度地保留了关键信息，为自监督学习的效率和性能提升提供了有价值的思考。"}}
{"id": "2506.10594", "title": "Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement", "authors": ["Jin Huang", "Honghua Chen", "Mingqiang Wei"], "summary": "The most essential feature of aviation equipment is high quality, including\nhigh performance, high stability and high reliability. In this paper, we\npropose a novel hierarchical error assessment framework for aircraft CAD models\nwithin a manufacturing-and-measurement platform, termed HEA-MM. HEA-MM employs\nstructured light scanners to obtain comprehensive 3D measurements of\nmanufactured workpieces. The measured point cloud is registered with the\nreference CAD model, followed by an error analysis conducted at three\nhierarchical levels: global, part, and feature. At the global level, the error\nanalysis evaluates the overall deviation of the scanned point cloud from the\nreference CAD model. At the part level, error analysis is performed on these\npatches underlying the point clouds. We propose a novel optimization-based\nprimitive refinement method to obtain a set of meaningful patches of point\nclouds. Two basic operations, splitting and merging, are introduced to refine\nthe coarse primitives. At the feature level, error analysis is performed on\ncircular holes, which are commonly found in CAD models. To facilitate it, a\ntwo-stage algorithm is introduced for the detection of circular holes. First,\nedge points are identified using a tensor-voting algorithm. Then, multiple\ncircles are fitted through a hypothesize-and-clusterize framework, ensuring\naccurate detection and analysis of the circular features. Experimental results\non various aircraft CAD models demonstrate the effectiveness of our proposed\nmethod.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10594v1", "AI": {"title_translation": "飞机制造与测量中CAD模型的层次化误差评估", "tldr": "本文提出了一种针对飞机CAD模型的层次化误差评估框架HEA-MM，通过三层分析（全局、零件、特征）来确保航空设备的高质量。", "motivation": "航空设备最本质的特征是高质量，包括高性能、高稳定性和高可靠性。为了确保这一点，需要对飞机CAD模型在制造和测量过程中的误差进行精确评估。", "method": "本文提出了HEA-MM框架，用于飞机CAD模型在制造和测量平台中的误差评估。该框架采用结构光扫描仪获取制造工件的全面3D测量数据。测量点云与参考CAD模型进行配准后，在三个层次进行误差分析：\n1.  **全局层面**：评估扫描点云与参考CAD模型的整体偏差。\n2.  **零件层面**：对点云下的补丁进行误差分析，提出了一种新颖的基于优化的原始细化方法，并通过引入分裂和合并操作来细化粗糙的原始。\n3.  **特征层面**：对CAD模型中常见的圆形孔进行误差分析，为此引入了一种两阶段算法进行圆形孔检测：首先使用张量投票算法识别边缘点，然后通过假设-聚类框架拟合多个圆。", "result": "在各种飞机CAD模型上的实验结果证明了所提出方法的有效性。", "conclusion": "所提出的HEA-MM层次化误差评估框架能够有效评估飞机CAD模型在制造和测量过程中的误差，有助于确保航空设备的高质量。", "translation": "航空设备最本质的特征是高质量，包括高性能、高稳定性和高可靠性。在本文中，我们提出了一种用于飞机CAD模型在制造和测量平台中的新型层次化误差评估框架，称为HEA-MM。HEA-MM采用结构光扫描仪获取制造工件的全面3D测量数据。测量点云与参考CAD模型进行配准，然后进行三个层次的误差分析：全局、零件和特征。在全局层面，误差分析评估扫描点云与参考CAD模型的整体偏差。在零件层面，对点云下的这些补丁进行误差分析。我们提出了一种新颖的基于优化的原始细化方法，以获得一组有意义的点云补丁。引入了分裂和合并两种基本操作来细化粗糙的原始。在特征层面，对CAD模型中常见的圆形孔进行误差分析。为实现这一点，引入了一种两阶段算法用于圆形孔检测。首先，使用张量投票算法识别边缘点。然后，通过假设-聚类框架拟合多个圆，确保圆形特征的准确检测和分析。在各种飞机CAD模型上的实验结果证明了我们所提出方法的有效性。", "summary": "本文提出了一种名为HEA-MM的新型层次化误差评估框架，用于飞机制造和测量中的CAD模型。该框架利用结构光扫描获取3D测量数据，并对点云与CAD模型之间的误差进行全局、零件和特征三个层次的分析。特别地，在零件层面引入了基于优化的原始细化方法，在特征层面开发了两阶段圆形孔检测算法。实验证明了该方法在评估飞机CAD模型误差方面的有效性，有助于保障航空设备的高质量。", "keywords": "层次化误差评估, CAD模型, 飞机制造, 点云分析, 质量控制", "comments": "该论文创新性地提出了一个多层次的误差评估框架，从全局到局部特征（如圆形孔）进行细致分析，这对于确保航空制造精度至关重要。其结合结构光扫描、点云配准、优化细化以及特定特征检测的综合方法，为复杂工业产品的质量控制提供了实用的解决方案。该方法对于提高航空设备的可靠性和性能具有重要意义。"}}
{"id": "2506.10979", "title": "How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?", "authors": ["Sohee Yang", "Sang-Woo Lee", "Nora Kassner", "Daniela Gottesman", "Sebastian Riedel", "Mor Geva"], "summary": "Recent reasoning models show the ability to reflect, backtrack, and\nself-validate their reasoning, which is crucial in spotting mistakes and\narriving at accurate solutions. A natural question that arises is how\neffectively models can perform such self-reevaluation. We tackle this question\nby investigating how well reasoning models identify and recover from four types\nof unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to\nthe question, thoughts misdirecting the question as a slightly different\nquestion, and thoughts that lead to incorrect answers. We show that models are\neffective at identifying most unhelpful thoughts but struggle to recover from\nthe same thoughts when these are injected into their thinking process, causing\nsignificant performance drops. Models tend to naively continue the line of\nreasoning of the injected irrelevant thoughts, which showcases that their\nself-reevaluation abilities are far from a general \"meta-cognitive\" awareness.\nMoreover, we observe non/inverse-scaling trends, where larger models struggle\nmore than smaller ones to recover from short irrelevant thoughts, even when\ninstructed to reevaluate their reasoning. We demonstrate the implications of\nthese findings with a jailbreak experiment using irrelevant thought injection,\nshowing that the smallest models are the least distracted by\nharmful-response-triggering thoughts. Overall, our findings call for\nimprovement in self-reevaluation of reasoning models to develop better\nreasoning and safer systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.10979v1", "AI": {"title_translation": "推理模型在识别和纠正无益思维方面的表现如何？", "tldr": "推理模型能识别无益思维，但难以从中恢复，特别是当这些思维被注入时，且大模型恢复能力可能比小模型差。", "motivation": "探讨推理模型进行自我重新评估的有效性，特别是它们如何识别并从四种无益思维中恢复。", "method": "通过注入四种类型的无益思维（无信息冗余、与问题无关、误导性、导致错误答案）来测试推理模型识别和从中恢复的能力。", "result": "模型能有效识别大多数无益思维，但当这些思维被注入时，模型难以恢复，导致性能显著下降。模型倾向于天真地延续注入的无关思维，表明其自我重新评估能力远未达到元认知水平。观察到非/逆向扩展趋势，即大模型在从短无关思维中恢复时比小模型更困难。越小的模型越不容易被有害响应触发思维分散注意力。", "conclusion": "推理模型需要改进其自我重新评估能力，以开发更好的推理和更安全的系统。", "translation": "最近的推理模型展现了反思、回溯和自我验证其推理的能力，这对于发现错误和得出准确解决方案至关重要。一个自然而然的问题是，模型能多有效地执行这种自我重新评估。我们通过调查推理模型在识别和恢复四种无益思维方面的表现来解决这个问题：无信息冗余思维、与问题无关的思维、将问题误导为略有不同问题的思维，以及导致错误答案的思维。我们发现，模型在识别大多数无益思维方面是有效的，但当这些思维被注入到其思维过程中时，它们难以从中恢复，导致性能显著下降。模型倾向于天真地延续注入的无关思维，这表明它们的自我重新评估能力远未达到一般的“元认知”意识。此外，我们观察到非/逆向扩展趋势，即更大的模型在从短无关思维中恢复时比小模型更困难，即使在被指示重新评估其推理时也是如此。我们通过使用无关思维注入的越狱实验证明了这些发现的含义，表明最小的模型最不容易被触发有害响应的思维分散注意力。总的来说，我们的发现呼吁改进推理模型的自我重新评估能力，以开发更好的推理和更安全的系统。", "summary": "本文探讨了推理模型识别和从无益思维中恢复的能力。研究发现，模型能有效识别无益思维，但当这些思维被注入时，其恢复能力显著下降，导致性能下降。模型倾向于延续注入的无关思维，表明其元认知能力不足。此外，研究发现大模型在恢复方面可能比小模型更差，并通过越狱实验证明了小模型在有害思维注入下表现更佳。研究强调了改进推理模型自我重新评估能力的重要性，以提升推理质量和系统安全性。", "keywords": "推理模型, 自我重新评估, 无益思维, 逆向扩展, 模型安全", "comments": "这项研究揭示了当前推理模型在“元认知”能力上的局限性，特别是在处理内部干扰和错误信息时的恢复能力。逆向扩展趋势的发现（大模型表现更差）尤其重要，它挑战了“越大越好”的普遍假设，并为模型安全性提供了新的视角，即小模型在某些特定对抗性场景下可能更鲁棒。这对于开发更健壮、更安全的AI系统具有重要意义。"}}
{"id": "2506.10601", "title": "Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection", "authors": ["Xinyuan Liu", "Hang Xu", "Yike Ma", "Yucheng Zhang", "Feng Dai"], "summary": "Recent remote sensing tech advancements drive imagery growth, making oriented\nobject detection rapid development, yet hindered by labor-intensive annotation\nfor high-density scenes. Oriented object detection with point supervision\noffers a cost-effective solution for densely packed scenes in remote sensing,\nyet existing methods suffer from inadequate sample assignment and instance\nconfusion due to rigid rule-based designs. To address this, we propose SSP\n(Semantic-decoupled Spatial Partition), a unified framework that synergizes\nrule-driven prior injection and data-driven label purification. Specifically,\nSSP introduces two core innovations: 1) Pixel-level Spatial Partition-based\nSample Assignment, which compactly estimates the upper and lower bounds of\nobject scales and mines high-quality positive samples and hard negative samples\nthrough spatial partitioning of pixel maps. 2) Semantic Spatial Partition-based\nBox Extraction, which derives instances from spatial partitions modulated by\nsemantic maps and reliably converts them into bounding boxes to form\npseudo-labels for supervising the learning of downstream detectors. Experiments\non DOTA-v1.0 and others demonstrate SSP\\' s superiority: it achieves 45.78% mAP\nunder point supervision, outperforming SOTA method PointOBB-v2 by 4.10%.\nFurthermore, when integrated with ORCNN and ReDet architectures, the SSP\nframework achieves mAP values of 47.86% and 48.50%, respectively. The code is\navailable at https://github.com/antxinyuan/ssp.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10601v1", "AI": {"title_translation": "语义解耦空间划分引导的点监督定向目标检测", "tldr": "本文提出了SSP框架，通过像素级空间划分和语义空间划分，解决了点监督定向目标检测中样本分配不足和实例混淆问题，显著提高了检测性能。", "motivation": "遥感图像中高密度场景的定向目标检测面临标注成本高昂的问题。现有点监督方法因僵硬的规则设计导致样本分配不足和实例混淆，无法有效解决密集场景的检测挑战。", "method": "本文提出了SSP（Semantic-decoupled Spatial Partition）框架，一个结合规则驱动先验注入和数据驱动标签净化的统一框架。它包含两个核心创新：1) 像素级空间划分的样本分配：通过对像素图进行空间划分，估计目标尺度的上下限，并挖掘高质量正样本和难负样本。2) 语义空间划分的框提取：根据语义图调制的空间划分提取实例，并可靠地将其转换为边界框，形成伪标签以监督下游检测器的学习。", "result": "在DOTA-v1.0及其他数据集上的实验表明，SSP在点监督下实现了45.78%的mAP，比SOTA方法PointOBB-v2高出4.10%。此外，当与ORCNN和ReDet架构集成时，SSP框架分别实现了47.86%和48.50%的mAP值。", "conclusion": "SSP框架通过其创新的样本分配和框提取机制，有效解决了点监督定向目标检测中的挑战，显著提升了检测性能，证明了其在遥感图像高密度场景应用中的优越性。", "translation": "最近遥感技术的进步推动了图像的增长，使得定向目标检测得到快速发展，但高密度场景的劳动密集型标注阻碍了其发展。点监督的定向目标检测为遥感中密集场景提供了一种经济高效的解决方案，然而现有方法由于僵硬的基于规则的设计，存在样本分配不足和实例混淆的问题。为了解决这个问题，我们提出了SSP（Semantic-decoupled Spatial Partition），一个统一的框架，它协同了规则驱动的先验注入和数据驱动的标签净化。具体来说，SSP引入了两项核心创新：1）像素级空间划分的样本分配，它紧凑地估计目标尺度的上限和下限，并通过像素图的空间划分挖掘高质量的正样本和难负样本。2）语义空间划分的框提取，它从语义图调制的空间划分中导出实例，并可靠地将它们转换为边界框，形成伪标签以监督下游检测器的学习。在DOTA-v1.0及其他数据集上的实验表明，SSP的优越性：在点监督下实现了45.78%的mAP，比SOTA方法PointOBB-v2高出4.10%。此外，当与ORCNN和ReDet架构集成时，SSP框架分别实现了47.86%和48.50%的mAP值。代码可在https://github.com/antxinyuan/ssp获取。", "summary": "本文提出了一种名为SSP（Semantic-decoupled Spatial Partition）的统一框架，用于点监督定向目标检测，旨在解决现有方法在处理高密度遥感场景时样本分配不足和实例混淆的问题。SSP结合了规则驱动的先验注入和数据驱动的标签净化，并引入了像素级空间划分的样本分配和语义空间划分的框提取两项核心创新。实验结果表明，SSP在DOTA-v1.0数据集上取得了显著的性能提升，超越了现有最先进方法，并在集成不同检测器架构时也表现出色。", "keywords": "点监督, 定向目标检测, 空间划分, 遥感图像, 样本分配", "comments": "SSP框架的创新点在于其将规则驱动的先验知识与数据驱动的标签净化相结合，有效解决了点监督定向目标检测中的两大挑战：样本分配和实例混淆。通过像素级和语义级的空间划分，该方法能够更精确地识别和提取目标实例，从而在标注成本高昂的密集遥感场景中提供了高效且高性能的解决方案。其性能提升显著，对实际应用具有重要意义。"}}
{"id": "2506.10630", "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs", "authors": ["Yucong Luo", "Yitong Zhou", "Mingyue Cheng", "Jiahao Wang", "Daoyu Wang", "Tingyue Pan", "Jintao Zhang"], "summary": "To advance time series forecasting (TSF), various methods have been proposed\nto improve prediction accuracy, evolving from statistical techniques to\ndata-driven deep learning architectures. Despite their effectiveness, most\nexisting methods still adhere to a fast thinking paradigm-relying on extracting\nhistorical patterns and mapping them to future values as their core modeling\nphilosophy, lacking an explicit thinking process that incorporates intermediate\ntime series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)\nhave shown remarkable multi-step reasoning capabilities, offering an\nalternative way to overcome these issues. However, prompt engineering alone\npresents several limitations - including high computational cost, privacy\nrisks, and limited capacity for in-depth domain-specific time series reasoning.\nTo address these limitations, a more promising approach is to train LLMs to\ndevelop slow thinking capabilities and acquire strong time series reasoning\nskills. For this purpose, we propose Time-R1, a two-stage reinforcement\nfine-tuning framework designed to enhance multi-step reasoning ability of LLMs\nfor time series forecasting. Specifically, the first stage conducts supervised\nfine-tuning for warmup adaptation, while the second stage employs reinforcement\nlearning to improve the model's generalization ability. Particularly, we design\na fine-grained multi-objective reward specifically for time series forecasting,\nand then introduce GRIP (group-based relative importance for policy\noptimization), which leverages non-uniform sampling to further encourage and\noptimize the model's exploration of effective reasoning paths. Experiments\ndemonstrate that Time-R1 significantly improves forecast performance across\ndiverse datasets.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10630v1", "AI": {"title_translation": "将时间序列预测视为推理：一种基于强化大型语言模型的慢思考方法", "tldr": "本文提出Time-R1，一个两阶段强化微调框架，旨在通过慢思考LLM提升时间序列预测的多步推理能力，显著改善预测性能。", "motivation": "现有时间序列预测方法多采用“快思考”范式，依赖提取历史模式并映射到未来值，缺乏明确的中间时间序列推理过程。新兴的“慢思考”大型语言模型（LLMs）展现出多步推理能力，但单独的提示工程存在计算成本高、隐私风险和领域特定推理能力有限等局限性。为克服这些问题，本文旨在训练LLMs发展慢思考能力并获取强大的时间序列推理技能。", "method": "本文提出了Time-R1，一个两阶段强化微调框架，用于增强LLMs在时间序列预测中的多步推理能力。第一阶段进行监督微调以进行预热适应，第二阶段采用强化学习来提高模型的泛化能力。特别设计了针对时间序列预测的细粒度多目标奖励，并引入了GRIP（group-based relative importance for policy optimization），利用非均匀采样进一步鼓励和优化模型对有效推理路径的探索。", "result": "实验证明，Time-R1显著提高了跨多样数据集的预测性能。", "conclusion": "通过Time-R1框架，成功地将LLMs的慢思考和多步推理能力应用于时间序列预测，并显著提升了预测表现。", "translation": "为了推进时间序列预测（TSF），人们提出了各种方法来提高预测准确性，从统计技术发展到数据驱动的深度学习架构。尽管它们有效，但大多数现有方法仍然遵循一种快速思考范式——依赖于提取历史模式并将其映射到未来值作为其核心建模理念，缺乏结合中间时间序列推理的明确思考过程。与此同时，新兴的慢思考LLMs（例如OpenAI-o1）展现出卓越的多步推理能力，提供了克服这些问题的另一种方法。然而，单独的提示工程存在一些局限性——包括高计算成本、隐私风险以及领域特定时间序列推理能力有限。为了解决这些局限性，一个更有前景的方法是训练LLMs发展慢思考能力并获得强大的时间序列推理技能。为此，我们提出了Time-R1，一个两阶段强化微调框架，旨在增强LLMs的时间序列预测多步推理能力。具体而言，第一阶段进行监督微调以进行预热适应，而第二阶段采用强化学习来提高模型的泛化能力。特别是，我们设计了专门用于时间序列预测的细粒度多目标奖励，然后引入了GRIP（基于群组的策略优化相对重要性），它利用非均匀采样进一步鼓励和优化模型对有效推理路径的探索。实验表明，Time-R1显著提高了跨多样数据集的预测性能。", "summary": "本文提出了一种名为Time-R1的两阶段强化微调框架，旨在将大型语言模型（LLMs）的“慢思考”和多步推理能力引入时间序列预测（TSF）。针对现有TSF方法缺乏明确推理过程以及LLM提示工程的局限性，Time-R1首先通过监督微调进行预热，然后利用强化学习结合细粒度多目标奖励和GRIP（一种基于非均匀采样的策略优化方法）来增强模型泛化能力和有效推理路径的探索。实验证明，Time-R1显著提升了不同数据集上的预测性能。", "keywords": "时间序列预测, 大型语言模型, 强化学习, 慢思考, 多步推理", "comments": "该论文的创新点在于将LLM的“慢思考”范式引入时间序列预测，通过强化学习微调LLM来克服传统方法的局限性。其提出的Time-R1框架，特别是细粒度多目标奖励和GRIP机制，为LLM在特定领域（如时间序列）的深度推理能力培养提供了新的思路。这对于提升时间序列预测的准确性和可解释性具有重要意义，也为LLM的领域适应性研究提供了有价值的贡献。"}}
{"id": "2506.10605", "title": "High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model", "authors": ["Eshan Ramesh", "Nishio Takayuki"], "summary": "We present LatentCSI, a novel method for generating images of the physical\nenvironment from WiFi CSI measurements that leverages a pretrained latent\ndiffusion model (LDM). Unlike prior approaches that rely on complex and\ncomputationally intensive techniques such as GANs, our method employs a\nlightweight neural network to map CSI amplitudes directly into the latent space\nof an LDM. We then apply the LDM's denoising diffusion model to the latent\nrepresentation with text-based guidance before decoding using the LDM's\npretrained decoder to obtain a high-resolution image. This design bypasses the\nchallenges of pixel-space image generation and avoids the explicit image\nencoding stage typically required in conventional image-to-image pipelines,\nenabling efficient and high-quality image synthesis. We validate our approach\non two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi\ndevices and cameras; and a subset of the publicly available MM-Fi dataset. The\nresults demonstrate that LatentCSI outperforms baselines of comparable\ncomplexity trained directly on ground-truth images in both computational\nefficiency and perceptual quality, while additionally providing practical\nadvantages through its unique capacity for text-guided controllability.", "comment": "6 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10605v1", "AI": {"title_translation": "使用预训练潜在扩散模型从WiFi CSI生成高分辨率高效图像", "tldr": "LatentCSI是一种新方法，利用预训练潜在扩散模型从WiFi CSI测量生成高分辨率图像，比现有方法更高效、质量更高，并支持文本引导。", "motivation": "现有的从WiFi CSI生成图像的方法（如GANs）通常复杂且计算密集，面临像素空间图像生成挑战并需要显式图像编码阶段。", "method": "本文提出LatentCSI，该方法利用一个轻量级神经网络将WiFi CSI幅度直接映射到预训练潜在扩散模型（LDM）的潜在空间。随后，在潜在表示上应用LDM的去噪扩散模型，并结合文本引导，最终通过LDM的预训练解码器生成高分辨率图像。这种设计避免了像素空间图像生成和显式图像编码阶段。", "result": "LatentCSI在计算效率和感知质量方面均优于直接在真实图像上训练的、复杂度相当的基线方法，并且提供了独特的文本引导可控性等实际优势。该方法在自收集的宽带CSI数据集和公开的MM-Fi数据集子集上得到了验证。", "conclusion": "LatentCSI是一种有效且高效的从WiFi CSI生成高分辨率图像的方法，它通过利用预训练LDM的潜在空间，克服了传统方法的局限性，并提供了文本引导的额外优势。", "translation": "我们提出LatentCSI，一种利用预训练潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像的新颖方法。与依赖复杂且计算密集型技术（如GANs）的现有方法不同，我们的方法采用一个轻量级神经网络将CSI幅度直接映射到LDM的潜在空间。然后，我们对潜在表示应用LDM的去噪扩散模型，并结合文本引导，最后使用LDM的预训练解码器进行解码，以获得高分辨率图像。这种设计绕过了像素空间图像生成的挑战，并避免了传统图像到图像管道中通常所需的显式图像编码阶段，从而实现了高效、高质量的图像合成。我们在两个数据集上验证了我们的方法：一个我们使用现成的WiFi设备和相机收集的宽带CSI数据集；以及公开可用的MM-Fi数据集的一个子集。结果表明，LatentCSI在计算效率和感知质量方面均优于直接在真实图像上训练的、复杂度相当的基线方法，同时通过其独特的文本引导可控性提供了额外的实际优势。", "summary": "LatentCSI是一种创新方法，利用预训练潜在扩散模型（LDM）从WiFi CSI数据生成高分辨率图像。该方法通过轻量级神经网络将CSI直接映射到LDM的潜在空间，并结合文本引导进行去噪和解码，从而避免了传统方法的复杂性和计算密集性。实验证明，LatentCSI在效率和图像质量上优于现有基线，并增加了文本引导的可控性。", "keywords": "WiFi CSI, 图像生成, 潜在扩散模型, LatentCSI, 文本引导", "comments": "这篇论文通过将WiFi CSI数据与预训练的潜在扩散模型结合，提出了一种新颖且高效的图像生成方法。其创新之处在于绕过了传统的像素空间生成和显式图像编码，直接在潜在空间操作，显著提升了计算效率和图像质量。文本引导能力是其重要的实际优势，为CSI到图像生成提供了新的交互范式。"}}
{"id": "2506.10632", "title": "Hessian Geometry of Latent Space in Generative Models", "authors": ["Alexander Lobashev", "Dmitry Guskov", "Maria Larchenko", "Mikhail Tamm"], "summary": "This paper presents a novel method for analyzing the latent space geometry of\ngenerative models, including statistical physics models and diffusion models,\nby reconstructing the Fisher information metric. The method approximates the\nposterior distribution of latent variables given generated samples and uses\nthis to learn the log-partition function, which defines the Fisher metric for\nexponential families. Theoretical convergence guarantees are provided, and the\nmethod is validated on the Ising and TASEP models, outperforming existing\nbaselines in reconstructing thermodynamic quantities. Applied to diffusion\nmodels, the method reveals a fractal structure of phase transitions in the\nlatent space, characterized by abrupt changes in the Fisher metric. We\ndemonstrate that while geodesic interpolations are approximately linear within\nindividual phases, this linearity breaks down at phase boundaries, where the\ndiffusion model exhibits a divergent Lipschitz constant with respect to the\nlatent space. These findings provide new insights into the complex structure of\ndiffusion model latent spaces and their connection to phenomena like phase\ntransitions. Our source code is available at\nhttps://github.com/alobashev/hessian-geometry-of-diffusion-models.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10632v1", "AI": {"title_translation": "生成模型中潜在空间的Hessian几何", "tldr": "本文提出一种通过重构Fisher信息度量来分析生成模型（包括统计物理模型和扩散模型）潜在空间几何的新方法，揭示了扩散模型潜在空间中的分形相变结构。", "motivation": "分析生成模型（特别是扩散模型）的潜在空间几何，以理解其复杂结构及其与相变等现象的联系。", "method": "通过重构Fisher信息度量来分析生成模型的潜在空间几何。具体方法是近似给定生成样本的潜在变量后验分布，并用此学习定义指数族Fisher度量的对数配分函数。提供了理论收敛保证。", "result": "该方法在Ising和TASEP模型上得到验证，在重构热力学量方面优于现有基线。应用于扩散模型时，揭示了潜在空间中相变的分形结构，其特征是Fisher度量的突然变化。测地线插值在单个相内近似线性，但在相边界处线性度失效，扩散模型在潜在空间中表现出发散的Lipschitz常数。", "conclusion": "这些发现为扩散模型潜在空间的复杂结构及其与相变等现象的联系提供了新见解。", "translation": "本文提出了一种分析生成模型（包括统计物理模型和扩散模型）潜在空间几何的新方法，通过重构Fisher信息度量来实现。该方法近似给定生成样本的潜在变量的后验分布，并利用此来学习定义指数族Fisher度量的对数配分函数。文中提供了理论收敛保证，并在Ising和TASEP模型上验证了该方法，在重构热力学量方面优于现有基线。将该方法应用于扩散模型时，揭示了潜在空间中相变的分形结构，其特征是Fisher度量的突然变化。我们证明了测地线插值在单个相内近似线性，但这种线性在相边界处失效，此时扩散模型相对于潜在空间表现出发散的Lipschitz常数。这些发现为扩散模型潜在空间的复杂结构及其与相变等现象的联系提供了新见解。我们的源代码可在https://github.com/alobashev/hessian-geometry-of-diffusion-models获取。", "summary": "本文提出一种通过重构Fisher信息度量来分析生成模型潜在空间几何的新颖方法。该方法通过近似潜在变量的后验分布并学习对数配分函数来定义Fisher度量。理论上提供了收敛保证，并在Ising和TASEP模型上验证了其优越性。应用于扩散模型时，发现潜在空间存在分形相变结构，并在相边界处表现出非线性和发散的Lipschitz常数。这些发现为理解扩散模型的复杂潜在空间提供了新视角。", "keywords": "潜在空间几何, Fisher信息度量, 生成模型, 扩散模型, 相变", "comments": "这项研究通过引入Fisher信息度量来分析生成模型（特别是扩散模型）的潜在空间几何，提供了一种新颖且理论上严谨的方法。揭示扩散模型潜在空间中分形相变结构和相边界处的非线性行为是重要的发现，有助于深入理解这些模型的内在机制和限制。其创新性在于将统计物理的概念引入到生成模型的分析中。"}}
{"id": "2506.10609", "title": "MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling", "authors": ["Liang Yin", "Xudong Xie", "Zhang Li", "Xiang Bai", "Yuliang Liu"], "summary": "Scene text retrieval has made significant progress with the assistance of\naccurate text localization. However, existing approaches typically require\ncostly bounding box annotations for training. Besides, they mostly adopt a\ncustomized retrieval strategy but struggle to unify various types of queries to\nmeet diverse retrieval needs. To address these issues, we introduce Muti-query\nScene Text retrieval with Attention Recycling (MSTAR), a box-free approach for\nscene text retrieval. It incorporates progressive vision embedding to\ndynamically capture the multi-grained representation of texts and harmonizes\nfree-style text queries with style-aware instructions. Additionally, a\nmulti-instance matching module is integrated to enhance vision-language\nalignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset,\nthe first benchmark designed to evaluate the multi-query scene text retrieval\ncapability of models, comprising four query types and 16k images. Extensive\nexperiments demonstrate the superiority of our method across seven public\ndatasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous\nstate-of-the-art model by 6.4% in MAP on Total-Text while eliminating box\nannotation costs. Moreover, on the MQTR benchmark, MSTAR significantly\noutperforms the previous models by an average of 8.5%. The code and datasets\nare available at https://github.com/yingift/MSTAR.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10609v1", "AI": {"title_translation": "MSTAR：基于注意力循环的无框多查询场景文本检索", "tldr": "MSTAR是一种无框的场景文本检索方法，能够处理多种查询类型，并在多个数据集上表现优异，同时无需边界框标注。", "motivation": "现有场景文本检索方法通常需要昂贵的边界框标注进行训练，并且难以统一各种查询类型以满足多样化的检索需求。", "method": "本文提出了MSTAR（基于注意力循环的多查询场景文本检索），这是一种无框的场景文本检索方法。它通过以下方式实现：1. 结合渐进式视觉嵌入来动态捕获文本的多粒度表示。2. 协调自由风格的文本查询与风格感知指令。3. 集成多实例匹配模块以增强视觉-语言对齐。此外，本文还构建了Multi-Query Text Retrieval (MQTR) 数据集，这是第一个旨在评估模型多查询场景文本检索能力的基准，包含四种查询类型和16k图像。", "result": "MSTAR在七个公共数据集和MQTR数据集上均表现出优越性。在Total-Text数据集上，MSTAR在MAP指标上以6.4%的微弱优势超越了之前的最先进模型，同时消除了边界框标注成本。在MQTR基准测试中，MSTAR平均比现有模型高出8.5%。", "conclusion": "MSTAR通过其无框、多查询处理能力以及优越的性能，显著推进了场景文本检索领域的发展，并解决了传统方法中昂贵标注和查询多样性不足的问题。", "translation": "场景文本检索在准确文本定位的帮助下取得了显著进展。然而，现有方法通常需要昂贵的边界框标注进行训练。此外，它们大多采用定制的检索策略，但难以统一各种查询类型以满足多样化的检索需求。为了解决这些问题，我们引入了MSTAR（基于注意力循环的多查询场景文本检索），这是一种无框的场景文本检索方法。它结合了渐进式视觉嵌入来动态捕获文本的多粒度表示，并协调自由风格的文本查询与风格感知指令。此外，还集成了一个多实例匹配模块以增强视觉-语言对齐。此外，我们构建了Multi-Query Text Retrieval (MQTR) 数据集，这是第一个旨在评估模型多查询场景文本检索能力的基准，包含四种查询类型和16k图像。广泛的实验证明了我们方法在七个公共数据集和MQTR数据集上的优越性。值得注意的是，MSTAR在Total-Text数据集上，在MAP指标上以6.4%的微弱优势超越了之前的最先进模型，同时消除了边界框标注成本。此外，在MQTR基准测试中，MSTAR平均比现有模型高出8.5%。代码和数据集可在https://github.com/yingift/MSTAR 获取。", "summary": "MSTAR是一种创新的无框场景文本检索方法，旨在解决现有方法对边界框标注的依赖和对多样化查询支持不足的问题。它通过渐进式视觉嵌入和多实例匹配来增强文本表示和视觉-语言对齐，并能处理多种查询类型。该研究还引入了MQTR数据集来评估多查询能力。实验结果表明，MSTAR在多个数据集上均表现出色，尤其在无需边界框标注的情况下，性能超越了现有SOTA模型。", "keywords": "场景文本检索, 无框, 多查询, 注意力循环, MQTR数据集", "comments": "该论文的创新点在于提出了无框的场景文本检索方法，显著降低了标注成本。同时，其多查询处理能力提高了模型的通用性。MQTR数据集的构建也为多查询场景文本检索的研究提供了新的基准，具有重要的贡献。"}}
{"id": "2506.10647", "title": "Data Shifts Hurt CoT: A Theoretical Study", "authors": ["Lang Yin", "Debangshu Banerjee", "Gagandeep Singh"], "summary": "Chain of Thought (CoT) has been applied to various large language models\n(LLMs) and proven to be effective in improving the quality of outputs. In\nrecent studies, transformers are proven to have absolute upper bounds in terms\nof expressive power, and consequently, they cannot solve many computationally\ndifficult problems. However, empowered by CoT, transformers are proven to be\nable to solve some difficult problems effectively, such as the $k$-parity\nproblem. Nevertheless, those works rely on two imperative assumptions: (1)\nidentical training and testing distribution, and (2) corruption-free training\ndata with correct reasoning steps. However, in the real world, these\nassumptions do not always hold. Although the risks of data shifts have caught\nattention, our work is the first to rigorously study the exact harm caused by\nsuch shifts to the best of our knowledge. Focusing on the $k$-parity problem,\nin this work we investigate the joint impact of two types of data shifts: the\ndistribution shifts and data poisoning, on the quality of trained models\nobtained by a well-established CoT decomposition. In addition to revealing a\nsurprising phenomenon that CoT leads to worse performance on learning parity\nthan directly generating the prediction, our technical results also give a\nrigorous and comprehensive explanation of the mechanistic reasons of such\nimpact.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10647v1", "AI": {"title_translation": "数据偏移损害思维链：一项理论研究", "tldr": "本研究首次理论性地探讨了数据偏移（包括分布偏移和数据投毒）对思维链（CoT）在大型语言模型中性能的影响，发现CoT在数据偏移下可能导致比直接预测更差的性能。", "motivation": "思维链（CoT）已被证明能有效提升大型语言模型（LLMs）的输出质量，并使其能够解决一些计算上困难的问题，如k-奇偶校验问题。然而，现有研究依赖于两个关键假设：(1) 训练和测试分布相同，以及(2) 训练数据无损坏且推理步骤正确。在现实世界中，这些假设往往不成立，因此有必要严格研究数据偏移对CoT性能造成的损害。", "method": "本研究聚焦于k-奇偶校验问题，通过一个成熟的CoT分解方法，研究了分布偏移和数据投毒这两种数据偏移类型对训练模型质量的联合影响。该研究旨在首次严格量化数据偏移对CoT造成的具体损害。", "result": "研究发现了一个令人惊讶的现象：在数据偏移条件下，思维链（CoT）在学习奇偶校验问题上的表现比直接生成预测更差。技术结果还对这种影响的机制原因给出了严谨而全面的解释。", "conclusion": "该研究首次严格揭示了数据偏移，特别是分布偏移和数据投毒，如何损害思维链（CoT）在大型语言模型中的性能，并指出在某些情况下CoT可能导致负面效果，甚至不如直接预测。", "translation": "思维链（CoT）已被应用于各种大型语言模型（LLMs），并被证明能有效提高输出质量。在最近的研究中，Transformer在表达能力方面被证明具有绝对上限，因此无法解决许多计算上困难的问题。然而，在CoT的加持下，Transformer被证明能够有效解决一些难题，例如k-奇偶校验问题。尽管如此，这些工作依赖于两个必要的假设：(1) 训练和测试分布相同，以及(2) 训练数据无损坏且推理步骤正确。然而，在现实世界中，这些假设并不总是成立。尽管数据偏移的风险已引起关注，但据我们所知，我们的工作是第一个严格研究此类偏移所造成的精确损害。本研究聚焦于k-奇偶校验问题，探讨了两种数据偏移类型：分布偏移和数据投毒，对通过成熟的CoT分解获得的训练模型质量的联合影响。除了揭示CoT在学习奇偶校验问题上的表现比直接生成预测更差这一令人惊讶的现象外，我们的技术结果还对这种影响的机制原因给出了严谨而全面的解释。", "summary": "本研究首次从理论角度深入探讨了数据偏移（包括分布偏移和数据投毒）对大型语言模型中思维链（CoT）性能的影响。以往的CoT研究依赖于理想的无数据偏移假设，但现实情况并非如此。通过聚焦于k-奇偶校验问题，并采用CoT分解方法，研究发现数据偏移不仅损害了CoT的有效性，甚至在某些情况下，CoT在学习奇偶校验问题上的表现比直接预测更差。研究还提供了对这种负面影响的机制性解释。", "keywords": "思维链, 数据偏移, 分布偏移, 数据投毒, k-奇偶校验", "comments": "这项研究具有重要的理论和实践意义。它是首次严格量化数据偏移对CoT性能损害的工作，挑战了人们对CoT在复杂问题上普遍提升性能的认知。特别是“CoT导致比直接预测更差的性能”这一发现，对于理解CoT的局限性及其在非理想数据环境下的行为至关重要。这提示研究者在实际应用CoT时，必须充分考虑数据质量和分布一致性问题。"}}
{"id": "2506.10612", "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling", "authors": ["Suin Lee", "Dae-Shik Kim"], "summary": "We present TexTailor, a novel method for generating consistent object\ntextures from textual descriptions. Existing text-to-texture synthesis\napproaches utilize depth-aware diffusion models to progressively generate\nimages and synthesize textures across predefined multiple viewpoints. However,\nthese approaches lead to a gradual shift in texture properties across\nviewpoints due to (1) insufficient integration of previously synthesized\ntextures at each viewpoint during the diffusion process and (2) the\nautoregressive nature of the texture synthesis process. Moreover, the\npredefined selection of camera positions, which does not account for the\nobject's geometry, limits the effective use of texture information synthesized\nfrom different viewpoints, ultimately degrading overall texture consistency. In\nTexTailor, we address these issues by (1) applying a resampling scheme that\nrepeatedly integrates information from previously synthesized textures within\nthe diffusion process, and (2) fine-tuning a depth-aware diffusion model on\nthese resampled textures. During this process, we observed that using only a\nfew training images restricts the model's original ability to generate\nhigh-fidelity images aligned with the conditioning, and therefore propose an\nperformance preservation loss to mitigate this issue. Additionally, we improve\nthe synthesis of view-consistent textures by adaptively adjusting camera\npositions based on the object's geometry. Experiments on a subset of the\nObjaverse dataset and the ShapeNet car dataset demonstrate that TexTailor\noutperforms state-of-the-art methods in synthesizing view-consistent textures.\nThe source code for TexTailor is available at\nhttps://github.com/Adios42/Textailor", "comment": "Submitted to ICLR 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10612v1", "AI": {"title_translation": "TexTailor：通过有效重采样实现定制化文本对齐纹理", "tldr": "TexTailor是一种新颖的方法，通过有效的重采样和自适应相机位置调整，从文本描述生成一致的物体纹理，解决了现有方法中纹理属性偏移和视图一致性差的问题。", "motivation": "现有的文本到纹理合成方法在使用深度感知扩散模型生成纹理时，存在纹理属性在不同视角间逐渐偏移的问题。这主要是由于在扩散过程中未能充分整合先前合成的纹理，以及纹理合成过程的自回归性质。此外，预定义的相机位置未能考虑物体几何，限制了不同视角纹理信息的有效利用，从而降低了整体纹理一致性。", "method": "TexTailor通过以下方式解决现有问题：1) 应用重采样方案，在扩散过程中重复整合先前合成纹理的信息；2) 在这些重采样纹理上微调深度感知扩散模型。为了缓解训练图像过少导致模型生成高保真图像能力受限的问题，提出了性能保持损失。此外，通过根据物体几何自适应调整相机位置，改进了视图一致性纹理的合成。", "result": "在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视图一致性纹理方面优于现有最先进的方法。", "conclusion": "TexTailor通过有效的重采样和自适应相机位置调整，成功解决了现有文本到纹理合成方法中视图一致性差的问题，能够生成高质量、视图一致的物体纹理。", "translation": "我们提出了TexTailor，一种从文本描述生成一致物体纹理的新颖方法。现有的文本到纹理合成方法利用深度感知扩散模型逐步生成图像并在预定义的多个视点上合成纹理。然而，这些方法导致纹理属性在不同视点之间逐渐偏移，原因在于(1)扩散过程中在每个视点未能充分整合先前合成的纹理，以及(2)纹理合成过程的自回归性质。此外，预设的相机位置未考虑物体的几何形状，限制了从不同视点合成的纹理信息的有效利用，最终降低了整体纹理一致性。在TexTailor中，我们通过(1)应用重采样方案，在扩散过程中重复整合来自先前合成纹理的信息，以及(2)在这些重采样纹理上微调深度感知扩散模型来解决这些问题。在此过程中，我们观察到仅使用少量训练图像会限制模型生成与条件对齐的高保真图像的原始能力，因此我们提出了一种性能保持损失来缓解此问题。此外，我们通过根据物体几何自适应调整相机位置来改进视图一致性纹理的合成。在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视图一致性纹理方面优于现有最先进的方法。TexTailor的源代码可在https://github.com/Adios42/Textailor获取。", "summary": "TexTailor是一种新颖的文本到纹理合成方法，旨在解决现有方法中纹理在不同视角间一致性差的问题。它通过引入重采样方案，在扩散过程中有效整合先前合成的纹理信息，并对深度感知扩散模型进行微调。为应对少量训练数据可能导致的性能下降，该方法提出了性能保持损失。此外，TexTailor还根据物体几何自适应调整相机位置，以进一步提升视图一致性。实验证明，TexTailor在生成视图一致性纹理方面优于现有技术。", "keywords": "文本到纹理，纹理合成，扩散模型，重采样，视图一致性", "comments": "TexTailor的创新之处在于其提出的重采样方案和性能保持损失，有效解决了文本到纹理合成中长期存在的视图一致性问题。通过自适应相机位置调整，该方法还巧妙地利用了物体几何信息，进一步提升了合成质量。这项工作对于虚拟现实、3D内容创作等领域具有重要意义。"}}
{"id": "2506.10680", "title": "Saturation Self-Organizing Map", "authors": ["Igor Urbanik", "Paweł Gajewski"], "summary": "Continual learning poses a fundamental challenge for neural systems, which\noften suffer from catastrophic forgetting when exposed to sequential tasks.\nSelf-Organizing Maps (SOMs), despite their interpretability and efficiency, are\nnot immune to this issue. In this paper, we introduce Saturation\nSelf-Organizing Maps (SatSOM)-an extension of SOMs designed to improve\nknowledge retention in continual learning scenarios. SatSOM incorporates a\nnovel saturation mechanism that gradually reduces the learning rate and\nneighborhood radius of neurons as they accumulate information. This effectively\nfreezes well-trained neurons and redirects learning to underutilized areas of\nthe map.", "comment": "github repository: https://github.com/Radinyn/satsom", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10680v1", "AI": {"title_translation": "饱和自组织映射", "tldr": "提出饱和自组织映射（SatSOM），通过新的饱和机制解决自组织映射（SOM）在持续学习中灾难性遗忘的问题。", "motivation": "神经系统在持续学习中面临灾难性遗忘的挑战，即使是自组织映射（SOMs）也无法幸免。", "method": "SatSOM通过引入一种新颖的饱和机制来改进知识保留，该机制随着神经元积累信息逐渐降低其学习率和邻域半径，从而有效“冻结”训练充分的神经元并将学习重定向到未充分利用的区域。", "result": "Not mentioned in abstract", "conclusion": "SatSOM旨在提高自组织映射在持续学习场景中的知识保留能力。", "translation": "持续学习对神经网络系统提出了根本性挑战，这些系统在接触顺序任务时经常遭受灾难性遗忘。自组织映射（SOMs）尽管具有可解释性和效率，但也不能幸免于此问题。在本文中，我们引入了饱和自组织映射（SatSOM）——SOMs的一种扩展，旨在提高持续学习场景中的知识保留。SatSOM结合了一种新颖的饱和机制，该机制随着神经元积累信息而逐渐降低其学习率和邻域半径。这有效地“冻结”了训练充分的神经元，并将学习重定向到映射中未充分利用的区域。", "summary": "本文提出饱和自组织映射（SatSOM），它是自组织映射（SOM）的扩展，旨在解决持续学习中神经系统常见的灾难性遗忘问题。SatSOM引入了一种独特的饱和机制，该机制能根据神经元积累的信息量，逐步减小其学习率和邻域半径，从而“冻结”已充分训练的神经元，并将学习过程引导至映射中未充分利用的部分，以提高知识保留。", "keywords": "持续学习, 自组织映射, 灾难性遗忘, 饱和机制, 知识保留", "comments": "本文的创新点在于引入了一种新颖的饱和机制，该机制通过动态调整神经元的学习参数来有效应对自组织映射在持续学习中的灾难性遗忘问题。这种方法有望提高SOMs在处理序列数据时的知识保留能力和效率。"}}
{"id": "2506.10633", "title": "Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models", "authors": ["Konstantinos Vilouras", "Ilias Stogiannidis", "Junyu Yan", "Alison Q. O'Neil", "Sotirios A. Tsaftaris"], "summary": "Latent Diffusion Models have shown remarkable results in text-guided image\nsynthesis in recent years. In the domain of natural (RGB) images, recent works\nhave shown that such models can be adapted to various vision-language\ndownstream tasks with little to no supervision involved. On the contrary,\ntext-to-image Latent Diffusion Models remain relatively underexplored in the\nfield of medical imaging, primarily due to limited data availability (e.g., due\nto privacy concerns). In this work, focusing on the chest X-ray modality, we\nfirst demonstrate that a standard text-conditioned Latent Diffusion Model has\nnot learned to align clinically relevant information in free-text radiology\nreports with the corresponding areas of the given scan. Then, to alleviate this\nissue, we propose a fine-tuning framework to improve multi-modal alignment in a\npre-trained model such that it can be efficiently repurposed for downstream\ntasks such as phrase grounding. Our method sets a new state-of-the-art on a\nstandard benchmark dataset (MS-CXR), while also exhibiting robust performance\non out-of-distribution data (VinDr-CXR). Our code will be made publicly\navailable.", "comment": "14 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10633v1", "AI": {"title_translation": "胸部X射线潜在扩散模型中的解剖学弱监督提示调优", "tldr": "本文提出一种解剖学弱监督提示调优框架，以改善预训练潜在扩散模型在胸部X射线图像中多模态对齐问题，并在下游任务如短语接地中取得SOTA表现。", "motivation": "潜在扩散模型在医学影像领域（特别是胸部X射线）中应用不足，且现有模型未能有效对齐放射学报告文本信息与X射线图像中的临床相关区域。", "method": "提出一个微调框架，通过解剖学弱监督提示调优来改善预训练潜在扩散模型的多模态对齐，使其能用于短语接地等下游任务。", "result": "在标准基准数据集MS-CXR上达到了新的SOTA性能，并在分布外数据VinDr-CXR上表现出鲁棒性。", "conclusion": "通过提出的微调框架，可以有效改善潜在扩散模型在胸部X射线图像中的多模态对齐问题，并在相关下游任务中取得优异表现。", "translation": "潜在扩散模型近年来在文本引导图像合成方面表现出色。在自然（RGB）图像领域，最近的工作表明此类模型可以以极少或无需监督的方式适应各种视觉-语言下游任务。相反，文本到图像的潜在扩散模型在医学影像领域仍相对未被充分探索，这主要是由于数据可用性有限（例如，出于隐私考虑）。在这项工作中，我们专注于胸部X射线模态，首先证明了一个标准的文本条件潜在扩散模型尚未学会将自由文本放射学报告中临床相关信息与给定扫描的相应区域对齐。然后，为了缓解这个问题，我们提出了一个微调框架，以改善预训练模型中的多模态对齐，使其可以有效地重新用于短语接地等下游任务。我们的方法在标准基准数据集MS-CXR上创造了新的最先进水平，同时在分布外数据VinDr-CXR上也表现出鲁棒性。我们的代码将公开可用。", "summary": "本文针对胸部X射线领域潜在扩散模型中文本与图像信息未充分对齐的问题，提出了一种解剖学弱监督提示调优的微调框架。该方法旨在改善预训练模型的多模态对齐，从而使其能高效应用于短语接地等下游任务。实验结果表明，该方法在MS-CXR数据集上达到了最先进水平，并在VinDr-CXR数据集上展现出良好的泛化能力。", "keywords": "潜在扩散模型, 胸部X射线, 弱监督学习, 提示调优, 多模态对齐", "comments": "这项工作创新性地将弱监督提示调优应用于医学影像领域的潜在扩散模型，解决了胸部X射线报告与图像对齐的难题。其在有限数据下的高效微调能力以及在分布外数据上的鲁棒性，对于推动医学影像AI发展具有重要意义，尤其是在隐私受限的数据环境下。"}}
{"id": "2506.10703", "title": "Preserving Task-Relevant Information Under Linear Concept Removal", "authors": ["Floris Holstege", "Shauli Ravfogel", "Bram Wouters"], "summary": "Modern neural networks often encode unwanted concepts alongside task-relevant\ninformation, leading to fairness and interpretability concerns. Existing\npost-hoc approaches can remove undesired concepts but often degrade useful\nsignals. We introduce SPLICE-Simultaneous Projection for LInear concept removal\nand Covariance prEservation-which eliminates sensitive concepts from\nrepresentations while exactly preserving their covariance with a target label.\nSPLICE achieves this via an oblique projection that \"splices out\" the unwanted\ndirection yet protects important label correlations. Theoretically, it is the\nunique solution that removes linear concept predictability and maintains target\ncovariance with minimal embedding distortion. Empirically, SPLICE outperforms\nbaselines on benchmarks such as Bias in Bios and Winobias, removing protected\nattributes while minimally damaging main-task information.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10703v1", "AI": {"title_translation": "在线性概念去除下保留任务相关信息", "tldr": "SPLICE是一种新方法，可以在神经网络表示中去除敏感概念，同时精确保留与目标标签的协方差，且对任务信息损害最小。", "motivation": "现代神经网络在编码任务相关信息的同时，经常编码不必要的概念（如敏感属性），导致公平性和可解释性问题。现有事后处理方法可以去除不需要的概念，但通常会损害有用的信号。", "method": "本文引入了SPLICE（Simultaneous Projection for LInear concept removal and Covariance prEservation），通过一种斜投影（oblique projection）来消除表示中的敏感概念，同时精确保留它们与目标标签的协方差。SPLICE通过“剪掉”不需要的方向但保护重要的标签相关性来实现。理论上，它是去除线性概念可预测性并以最小嵌入失真保持目标协方差的唯一解决方案。", "result": "经验上，SPLICE在Bias in Bios和Winobias等基准测试中优于基线方法，成功去除了受保护属性，同时最大限度地减少了对主任务信息的损害。", "conclusion": "SPLICE提供了一种有效且理论上最优的方法，可以在去除神经网络表示中不必要概念的同时，精确保留任务相关信息，从而解决公平性和可解释性问题。", "translation": "标题：在线性概念去除下保留任务相关信息\n摘要：现代神经网络在编码任务相关信息的同时，经常编码不必要的概念，导致公平性和可解释性问题。现有的事后处理方法可以去除不需要的概念，但通常会损害有用的信号。我们引入了SPLICE——同步投影用于线性概念去除和协方差保留——它从表示中消除敏感概念，同时精确保留它们与目标标签的协方差。SPLICE通过一种斜投影实现这一点，该投影“剪掉”了不需要的方向，但保护了重要的标签相关性。理论上，它是去除线性概念可预测性并以最小嵌入失真保持目标协方差的唯一解决方案。经验上，SPLICE在Bias in Bios和Winobias等基准测试中优于基线方法，去除了受保护属性，同时最大限度地减少了对主任务信息的损害。", "summary": "这篇论文介绍了一种名为SPLICE的新方法，旨在解决现代神经网络中存在的偏见和可解释性问题。SPLICE通过一种独特的斜投影技术，能够从神经网络的表示中去除不必要的敏感概念（如性别或种族偏见），同时精确地保留与任务目标相关的有用信息。理论上，SPLICE被证明是实现这一目标的唯一最优解，能够在最小化信息失真的情况下去除概念可预测性并保持目标协方差。实验结果表明，SPLICE在处理偏见数据集时表现优异，有效去除了敏感属性，同时最大限度地保护了核心任务的性能。", "keywords": "概念去除, 协方差保留, 公平性, 神经网络, 偏见消除", "comments": "SPLICE的创新点在于其通过斜投影实现“同时去除和保留”的能力，尤其是在理论上证明其为唯一最优解，这增加了其方法的严谨性。它解决了现有去偏方法在去除偏见时损害有用信号的关键痛点，对提高AI系统的公平性和可解释性具有重要意义。"}}
{"id": "2506.10634", "title": "Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models", "authors": ["Francisco Caetano", "Christiaan Viviers", "Peter H. N. De With", "Fons van der Sommen"], "summary": "Flow Matching has emerged as a powerful framework for learning continuous\ntransformations between distributions, enabling high-fidelity generative\nmodeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new\nformulation that unifies semantic segmentation, classification, and image\ngeneration within a single model. Using a symmetric learning objective,\nSymmFlow models forward and reverse transformations jointly, ensuring\nbi-directional consistency, while preserving sufficient entropy for generative\ndiversity. A new training objective is introduced to explicitly retain semantic\ninformation across flows, featuring efficient sampling while preserving\nsemantic structure, allowing for one-step segmentation and classification\nwithout iterative refinement. Unlike previous approaches that impose strict\none-to-one mapping between masks and images, SymmFlow generalizes to flexible\nconditioning, supporting both pixel-level and image-level class labels.\nExperimental results on various benchmarks demonstrate that SymmFlow achieves\nstate-of-the-art performance on semantic image synthesis, obtaining FID scores\nof 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.\nAdditionally, it delivers competitive results on semantic segmentation and\nshows promising capabilities in classification tasks. The code will be publicly\navailable.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10634v1", "AI": {"title_translation": "对称流匹配：基于分数的生成模型统一图像生成、分割和分类", "tldr": "提出对称流匹配 (SymmFlow) 框架，统一图像生成、语义分割和分类，实现高效采样和SOTA性能。", "motivation": "现有的流匹配框架在生成建模方面表现强大，但需要一个统一的模型来同时处理图像生成、语义分割和分类，并且解决传统方法中掩码与图像之间严格的一一映射问题，提高采样效率并保留语义信息。", "method": "引入对称流匹配（SymmFlow），采用对称学习目标联合建模正向和反向变换，确保双向一致性并保持生成多样性。提出新的训练目标以显式保留流中的语义信息，实现高效采样，并在不进行迭代细化的情况下完成一步分割和分类。支持像素级和图像级类标签的灵活条件设置。", "result": "SymmFlow在语义图像合成上取得了最先进的性能，在CelebAMask-HQ上FID得分11.9，在COCO-Stuff上FID得分7.0，仅需25个推理步骤。在语义分割上表现出有竞争力的结果，并在分类任务中显示出有前景的能力。", "conclusion": "SymmFlow成功地将图像生成、语义分割和分类统一到一个模型中，通过对称学习和新的训练目标实现了高效、高质量的性能，并在多个基准测试中取得了SOTA或有竞争力的结果。", "translation": "流匹配已成为学习分布之间连续变换的强大框架，实现了高保真生成建模。这项工作引入了对称流匹配（SymmFlow），这是一种新的公式，将语义分割、分类和图像生成统一在一个模型中。通过对称学习目标，SymmFlow共同建模正向和反向变换，确保双向一致性，同时保留足够的熵以实现生成多样性。引入了一个新的训练目标，以显式地在流中保留语义信息，其特点是高效采样，同时保留语义结构，允许一步分割和分类而无需迭代细化。与以前在掩码和图像之间施加严格一对一映射的方法不同，SymmFlow推广到灵活的条件设置，支持像素级和图像级类标签。在各种基准测试上的实验结果表明，SymmFlow在语义图像合成方面取得了最先进的性能，在CelebAMask-HQ上以仅25个推理步骤获得了11.9的FID分数，在COCO-Stuff上获得了7.0的FID分数。此外，它在语义分割方面也取得了有竞争力的结果，并在分类任务中显示出有前景的能力。代码将公开发布。", "summary": "本文提出对称流匹配（SymmFlow）框架，通过引入对称学习目标和新的训练目标，成功地将图像生成、语义分割和分类统一到一个单一模型中。SymmFlow能够联合建模正向和反向变换，确保双向一致性，并有效保留语义信息，实现一步式高效采样。实验证明，SymmFlow在语义图像合成方面达到最先进水平，并在分割和分类任务中展现出强大性能。", "keywords": "流匹配, 图像生成, 语义分割, 图像分类, 对称学习", "comments": "SymmFlow的创新之处在于其对称学习目标和统一多任务的能力，特别是能够在一步内完成分割和分类，避免了迭代细化。其对灵活条件设置的支持也扩展了流匹配的应用范围。这标志着生成模型在多任务处理和效率方面迈出了重要一步。"}}
{"id": "2506.10707", "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner", "authors": ["Marco Spinaci", "Marek Polewczyk", "Maximilian Schambach", "Sam Thelin"], "summary": "Tabular in-context learning (ICL) has recently achieved state-of-the-art\n(SOTA) performance on several tabular prediction tasks. Previously restricted\nto classification problems on small tables, recent advances such as TabPFN and\nTabICL have extended its use to larger datasets. While being architecturally\nefficient and well-adapted to tabular data structures, current table-native ICL\narchitectures, being trained exclusively on synthetic data, do not fully\nleverage the rich semantics and world knowledge contained in real-world tabular\ndata. On another end of this spectrum, tabular ICL models based on pretrained\nlarge language models such as TabuLa-8B integrate deep semantic understanding\nand world knowledge but are only able to make use of a small amount of context\ndue to inherent architectural limitations. With the aim to combine the best of\nboth these worlds, we introduce ConTextTab, integrating semantic understanding\nand alignment into a table-native ICL framework. By employing specialized\nembeddings for different data modalities and by training on large-scale\nreal-world tabular data, our model is competitive with SOTA across a broad set\nof benchmarks while setting a new standard on the semantically rich CARTE\nbenchmark.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10707v1", "AI": {"title_translation": "ConTextTab: 一种语义感知的表格上下文学习器", "tldr": "ConTextTab结合了表格原生上下文学习器和语义理解的优势，通过在真实世界数据上训练和使用专门的嵌入，在表格预测任务上实现了SOTA性能，尤其是在语义丰富的基准测试中。", "motivation": "现有的表格上下文学习（ICL）模型存在局限性：表格原生ICL架构（如TabPFN、TabICL）虽然高效且适应表格数据，但由于仅在合成数据上训练，未能充分利用真实世界表格数据中丰富的语义和世界知识；而基于预训练大型语言模型（如TabuLa-8B）的表格ICL模型虽然集成了深度语义理解和世界知识，但受限于架构，只能利用少量上下文。", "method": "本文引入了ConTextTab模型，它将语义理解和对齐整合到表格原生ICL框架中。通过为不同数据模态采用专门的嵌入，并在大规模真实世界表格数据上进行训练。", "result": "ConTextTab模型在广泛的基准测试中与最先进（SOTA）模型具有竞争力，并在语义丰富的CARTE基准测试中树立了新标准。", "conclusion": "ConTextTab成功地将表格原生上下文学习的效率与语义理解能力相结合，克服了现有方法的局限性，并在多个表格预测任务上达到了SOTA性能。", "translation": "表格上下文学习（ICL）最近在多项表格预测任务上取得了最先进（SOTA）的性能。此前仅限于小型表格的分类问题，TabPFN和TabICL等最新进展已将其应用扩展到更大的数据集。尽管在架构上高效且非常适应表格数据结构，但当前的表格原生ICL架构由于仅在合成数据上训练，未能充分利用真实世界表格数据中包含的丰富语义和世界知识。另一方面，基于预训练大型语言模型（如TabuLa-8B）的表格ICL模型集成了深度语义理解和世界知识，但由于固有的架构限制，只能利用少量上下文。为了结合这两者的优点，我们引入了ConTextTab，它将语义理解和对齐整合到表格原生ICL框架中。通过为不同数据模态采用专门的嵌入，并在大规模真实世界表格数据上进行训练，我们的模型在广泛的基准测试中与SOTA模型具有竞争力，同时在语义丰富的CARTE基准测试中树立了新标准。", "summary": "ConTextTab是一种新型的语义感知表格上下文学习器，旨在结合表格原生ICL模型的效率和基于大型语言模型的语义理解能力。它通过为不同数据模态使用专门的嵌入并在大规模真实世界表格数据上进行训练，克服了现有表格ICL模型在语义利用方面的局限性。实验结果表明，ConTextTab在多个基准测试中表现出与SOTA模型相当的竞争力，并在语义丰富的CARTE基准测试中设定了新标准。", "keywords": "表格上下文学习, 语义感知, ConTextTab, 真实世界数据, 表格预测", "comments": "ConTextTab的创新点在于其成功地将表格原生ICL的架构优势与深度语义理解相结合，解决了现有方法在处理真实世界表格数据语义信息时的不足。通过在真实世界数据上训练和使用专门的嵌入，它提升了表格ICL的性能和适用性，特别是在语义丰富的任务上。"}}
{"id": "2506.10639", "title": "GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning", "authors": ["Xiaoyi Bao", "Jindi Lv", "Xiaofeng Wang", "Zheng Zhu", "Xinze Chen", "YuKun Zhou", "Jiancheng Lv", "Xingang Wang", "Guan Huang"], "summary": "Recent progress in diffusion models has greatly enhanced video generation\nquality, yet these models still require fine-tuning to improve specific\ndimensions like instance preservation, motion rationality, composition, and\nphysical plausibility. Existing fine-tuning approaches often rely on human\nannotations and large-scale computational resources, limiting their\npracticality. In this work, we propose GigaVideo-1, an efficient fine-tuning\nframework that advances video generation without additional human supervision.\nRather than injecting large volumes of high-quality data from external sources,\nGigaVideo-1 unlocks the latent potential of pre-trained video diffusion models\nthrough automatic feedback. Specifically, we focus on two key aspects of the\nfine-tuning process: data and optimization. To improve fine-tuning data, we\ndesign a prompt-driven data engine that constructs diverse, weakness-oriented\ntraining samples. On the optimization side, we introduce a reward-guided\ntraining strategy, which adaptively weights samples using feedback from\npre-trained vision-language models with a realism constraint. We evaluate\nGigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17\nevaluation dimensions. Experiments show that GigaVideo-1 consistently improves\nperformance on almost all the dimensions with an average gain of about 4% using\nonly 4 GPU-hours. Requiring no manual annotations and minimal real data,\nGigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and\ndata will be publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10639v1", "AI": {"title_translation": "视频生成新进展：GigaVideo-1 通过自动反馈和 4 GPU 小时微调实现", "tldr": "GigaVideo-1 是一种高效的视频生成微调框架，它通过自动反馈机制（包括提示驱动的数据引擎和奖励引导的训练策略）在 VBench-2.0 上仅用 4 GPU 小时就将性能提升了约 4%，无需人工标注和大量真实数据。", "motivation": "尽管扩散模型显著提升了视频生成质量，但在实例保留、运动合理性、构图和物理合理性等特定维度上仍需微调。现有微调方法依赖人工标注和大量计算资源，限制了它们的实用性。", "method": "提出 GigaVideo-1，一个无需人工监督的高效微调框架。它通过自动反馈机制来释放预训练视频扩散模型的潜力，而非注入外部高质量数据。具体而言，在数据方面，设计了一个提示驱动的数据引擎来构建多样化、面向弱点的训练样本；在优化方面，引入了一种奖励引导的训练策略，利用预训练视觉-语言模型的反馈并结合真实性约束来自适应地加权样本。该方法在 VBench-2.0 基准上，以 Wan2.1 为基线，在 17 个评估维度上进行了评估。", "result": "实验表明，GigaVideo-1 在几乎所有维度上都持续提升了性能，平均增益约 4%，且仅需 4 GPU 小时。它无需手动标注和极少的真实数据。", "conclusion": "GigaVideo-1 展示了在视频生成微调方面的有效性和效率。", "translation": "扩散模型在视频生成质量方面取得了显著进展，但这些模型仍需要微调以改进特定维度，例如实例保留、运动合理性、构图和物理合理性。现有微调方法通常依赖人工标注和大规模计算资源，限制了它们的实用性。在这项工作中，我们提出了 GigaVideo-1，一个高效的微调框架，它在无需额外人工监督的情况下推进了视频生成。GigaVideo-1 没有从外部来源注入大量高质量数据，而是通过自动反馈解锁了预训练视频扩散模型的潜在能力。具体来说，我们关注微调过程的两个关键方面：数据和优化。为了改进微调数据，我们设计了一个提示驱动的数据引擎，用于构建多样化、面向弱点的训练样本。在优化方面，我们引入了一种奖励引导的训练策略，该策略利用预训练视觉-语言模型的反馈并结合真实性约束来自适应地加权样本。我们使用 Wan2.1 作为基线，在 VBench-2.0 基准上对 GigaVideo-1 在 17 个评估维度上进行了评估。实验表明，GigaVideo-1 在几乎所有维度上都持续提升了性能，平均增益约 4%，且仅需 4 GPU 小时。GigaVideo-1 无需人工标注和最少的真实数据，展示了其有效性和效率。代码、模型和数据将公开可用。", "summary": "GigaVideo-1 是一种创新的视频生成微调框架，旨在解决现有方法对人工标注和大量计算资源的依赖。它通过自动反馈机制，包括提示驱动的数据引擎和奖励引导的训练策略，有效提升了预训练视频扩散模型的性能。在 VBench-2.0 基准测试中，GigaVideo-1 仅用 4 GPU 小时就实现了平均 4% 的性能提升，证明了其在无需人工干预和极少真实数据情况下的高效性和有效性。", "keywords": "视频生成, 扩散模型, 微调, 自动反馈, 效率", "comments": "这项工作通过引入自动反馈机制，有效解决了视频生成模型微调中对人工标注和大量计算资源的依赖，具有重要的实用价值。其创新点在于结合了提示驱动的数据生成和奖励引导的优化策略，显著提升了效率和性能。4 GPU 小时即可实现显著提升，显示了该方法的优越性。"}}
{"id": "2506.10751", "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering", "authors": ["Sai Prasanna Teja Reddy Bogireddy", "Abrar Majeedi", "Viswanatha Reddy Gajjala", "Zhuoyan Xu", "Siddhant Rai", "Vaishnav Potlapalli"], "summary": "Automated question answering (QA) over electronic health records (EHRs) can\nbridge critical information gaps for clinicians and patients, yet it demands\nboth precise evidence retrieval and faithful answer generation under limited\nsupervision. In this work, we present Neural, the runner-up in the BioNLP 2025\nArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method\ndecouples the task into (1) sentence-level evidence identification and (2)\nanswer synthesis with explicit citations. For each stage, we automatically\nexplore the prompt space with DSPy's MIPROv2 optimizer, jointly tuning\ninstructions and few-shot demonstrations on the development set. A\nself-consistency voting scheme further improves evidence recall without\nsacrificing precision. On the hidden test set, our method attains an overall\nscore of 51.5, placing second stage while outperforming standard zero-shot and\nfew-shot prompting by over 20 and 10 points, respectively. These results\nindicate that data-driven prompt optimization is a cost-effective alternative\nto model fine-tuning for high-stakes clinical QA, advancing the reliability of\nAI assistants in healthcare.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10751v1", "AI": {"title_translation": "Neural 在 ArchEHR-QA 2025：用于基于证据的临床问答的智能体提示优化", "tldr": "本文介绍了“Neural”，一种在 BioNLP 2025 ArchEHR-QA 共享任务中获得亚军的基于证据的临床问答方法。该方法将任务解耦为证据识别和答案合成，并利用 DSPy 的 MIPROv2 优化器进行智能体提示优化和自洽投票机制，显著优于标准提示方法。", "motivation": "对电子健康记录 (EHR) 进行自动化问答 (QA) 可以弥合临床医生和患者之间的关键信息鸿沟，但这需要在有限监督下进行精确的证据检索和忠实的答案生成。", "method": "该方法将任务解耦为 (1) 句子级证据识别和 (2) 带有明确引用的答案合成。在每个阶段，利用 DSPy 的 MIPROv2 优化器自动探索提示空间，在开发集上联合调整指令和少样本演示。自洽投票机制进一步提高了证据召回率而未牺牲准确性。", "result": "在隐藏测试集上，该方法获得了 51.5 的总分，位居第二，同时分别比标准零样本和少样本提示高出 20 和 10 分。", "conclusion": "数据驱动的提示优化是高风险临床问答中模型微调的一种经济高效的替代方案，有助于提高医疗领域人工智能助手的可靠性。", "translation": "对电子健康记录 (EHR) 进行自动化问答 (QA) 可以弥合临床医生和患者之间的关键信息鸿沟，但这需要在有限监督下进行精确的证据检索和忠实的答案生成。在这项工作中，我们介绍了 Neural，它在 BioNLP 2025 ArchEHR-QA 基于证据的临床问答共享任务中获得亚军。我们提出的方法将任务分解为 (1) 句子级证据识别和 (2) 带有明确引用的答案合成。在每个阶段，我们利用 DSPy 的 MIPROv2 优化器自动探索提示空间，在开发集上联合调整指令和少样本演示。自洽投票机制进一步提高了证据召回率，同时不牺牲准确性。在隐藏测试集上，我们的方法获得了 51.5 的总分，位居第二，同时分别比标准零样本和少样本提示高出 20 和 10 分。这些结果表明，数据驱动的提示优化是高风险临床问答中模型微调的一种经济高效的替代方案，从而提高了医疗领域人工智能助手的可靠性。", "summary": "本文介绍了“Neural”系统，该系统在 BioNLP 2025 ArchEHR-QA 基于证据的临床问答任务中获得亚军。它将任务解耦为证据识别和答案合成，并利用 DSPy 的 MIPROv2 进行智能体提示优化和自洽投票机制。该方法显著优于标准提示技术，表明数据驱动的提示优化是实现可靠临床问答的一种经济有效的方法。", "keywords": "临床问答, 提示优化, 电子健康记录, 基于证据的问答, 智能体AI", "comments": "该论文的创新之处在于利用智能体提示优化（DSPy 的 MIPROv2）作为模型微调的一种经济高效的替代方案，特别适用于高风险的临床问答。任务解耦和自洽投票机制也值得关注。其在竞争激烈的共享任务中取得的优异表现以及提升医疗领域人工智能助手可靠性的潜力，突显了其重要性。"}}
{"id": "2506.10669", "title": "PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis", "authors": ["Marzieh Oghbaie", "Teresa Araújoa", "Hrvoje Bogunović"], "summary": "Background and Objective: Prototype-based methods improve interpretability by\nlearning fine-grained part-prototypes; however, their visualization in the\ninput pixel space is not always consistent with human-understandable\nbiomarkers. In addition, well-known prototype-based approaches typically learn\nextremely granular prototypes that are less interpretable in medical imaging,\nwhere both the presence and extent of biomarkers and lesions are critical.\n  Methods: To address these challenges, we propose PiPViT (Patch-based Visual\nInterpretable Prototypes), an inherently interpretable prototypical model for\nimage recognition. Leveraging a vision transformer (ViT), PiPViT captures\nlong-range dependencies among patches to learn robust, human-interpretable\nprototypes that approximate lesion extent only using image-level labels.\nAdditionally, PiPViT benefits from contrastive learning and multi-resolution\ninput processing, which enables effective localization of biomarkers across\nscales.\n  Results: We evaluated PiPViT on retinal OCT image classification across four\ndatasets, where it achieved competitive quantitative performance compared to\nstate-of-the-art methods while delivering more meaningful explanations.\nMoreover, quantitative evaluation on a hold-out test set confirms that the\nlearned prototypes are semantically and clinically relevant. We believe PiPViT\ncan transparently explain its decisions and assist clinicians in understanding\ndiagnostic outcomes. Github page: https://github.com/marziehoghbaie/PiPViT", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10669v1", "AI": {"title_translation": "PiPViT：基于图像块的视网膜图像分析可视化可解释原型", "tldr": "PiPViT是一种基于ViT的原型模型，通过学习可解释的图像块原型，解决了现有原型方法在医学图像解释性差的问题，在视网膜OCT图像分类上实现竞争性能和有意义的解释。", "motivation": "现有原型方法在输入像素空间的可视化与人类可理解的生物标志物不一致；典型的原型方法学习过于细粒度的原型，在医学图像中对生物标志物和病变的存在及范围解释性较差。", "method": "提出PiPViT（基于图像块的可视化可解释原型），一种固有的可解释原型模型。利用Vision Transformer (ViT) 捕获图像块间的长距离依赖，学习鲁棒的、人类可解释的原型，仅使用图像级标签就能近似病变范围。此外，结合对比学习和多分辨率输入处理，实现跨尺度的生物标志物有效定位。", "result": "在四个视网膜OCT图像分类数据集上，PiPViT实现了与最先进方法相当的定量性能，并提供了更有意义的解释。在独立测试集上的定量评估证实了学习到的原型在语义和临床上是相关的。", "conclusion": "PiPViT能够透明地解释其决策，并协助临床医生理解诊断结果。", "translation": "背景和目标：基于原型的方法通过学习细粒度的部分原型来提高可解释性；然而，它们在输入像素空间中的可视化并不总是与人类可理解的生物标志物一致。此外，众所周知的基于原型的方法通常学习极其细粒度的原型，这在医学成像中解释性较差，而医学成像中生物标志物和病变的存在和范围都至关重要。\n方法：为解决这些挑战，我们提出了PiPViT（基于图像块的可视化可解释原型），一个固有的可解释图像识别原型模型。PiPViT利用视觉Transformer (ViT) 捕获图像块之间的长距离依赖，学习鲁棒的、人类可解释的原型，仅使用图像级标签就能近似病变范围。此外，PiPViT受益于对比学习和多分辨率输入处理，这使得能够有效定位跨尺度的生物标志物。\n结果：我们在四个数据集上对视网膜OCT图像分类中的PiPViT进行了评估，它与最先进的方法相比取得了有竞争力的定量性能，同时提供了更有意义的解释。此外，对独立测试集的定量评估证实了所学原型在语义和临床上是相关的。我们相信PiPViT可以透明地解释其决策，并协助临床医生理解诊断结果。Github页面：https://github.com/marziehoghbaie/PiPViT", "summary": "PiPViT是一个针对视网膜图像分析的创新性可解释原型模型，旨在克服现有原型方法在医学图像解释性方面的局限性。该模型结合了Vision Transformer的长距离依赖捕获能力、对比学习和多分辨率输入处理，通过学习可解释的图像块原型，仅使用图像级标签即可近似病变范围并有效定位生物标志物。实验结果表明，PiPViT在视网膜OCT图像分类任务上表现出与最先进方法相当的性能，并提供了更具临床和语义相关性的解释，有望增强临床医生对诊断结果的理解和信任。", "keywords": "视网膜图像分析, 可解释AI, 原型学习, Vision Transformer, 医学图像", "comments": "PiPViT的创新在于将Vision Transformer引入原型学习，有效地解决了传统原型方法在医学图像中解释性不足的问题，尤其是在捕获病变范围和多尺度生物标志物定位方面的能力。其强调学习人类可解释的、语义和临床相关的原型，对于提升AI在医疗诊断领域的透明度和医生接受度具有重要意义。该方法通过图像级标签学习病变范围的能力也展现了其潜在的应用价值。"}}
{"id": "2506.10772", "title": "Skillful joint probabilistic weather forecasting from marginals", "authors": ["Ferran Alet", "Ilan Price", "Andrew El-Kadi", "Dominic Masters", "Stratis Markou", "Tom R. Andersson", "Jacklynn Stott", "Remi Lam", "Matthew Willson", "Alvaro Sanchez-Gonzalez", "Peter Battaglia"], "summary": "Machine learning (ML)-based weather models have rapidly risen to prominence\ndue to their greater accuracy and speed than traditional forecasts based on\nnumerical weather prediction (NWP), recently outperforming traditional\nensembles in global probabilistic weather forecasting. This paper presents FGN,\na simple, scalable and flexible modeling approach which significantly\noutperforms the current state-of-the-art models. FGN generates ensembles via\nlearned model-perturbations with an ensemble of appropriately constrained\nmodels. It is trained directly to minimize the continuous rank probability\nscore (CRPS) of per-location forecasts. It produces state-of-the-art ensemble\nforecasts as measured by a range of deterministic and probabilistic metrics,\nmakes skillful ensemble tropical cyclone track predictions, and captures joint\nspatial structure despite being trained only on marginals.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10772v1", "AI": {"title_translation": "基于边缘分布的熟练联合概率天气预报", "tldr": "本文提出了一种名为FGN的机器学习模型，用于天气预报，它通过学习模型扰动生成集合预报，并在各种指标上显著优于现有最先进模型，即使只在边缘分布上训练也能捕获联合空间结构。", "motivation": "机器学习（ML）天气模型因其比传统数值天气预报（NWP）更准确和快速而迅速崛起，并已在全球概率天气预报中超越传统集合预报。本文旨在提出一种更优越、可扩展且灵活的模型来进一步提升天气预报性能。", "method": "本文提出了FGN模型，这是一种简单、可扩展且灵活的建模方法。FGN通过学习模型扰动并结合适当约束的模型集合来生成集合预报。它直接训练以最小化每个位置预报的连续秩概率分数（CRPS）。", "result": "FGN模型在各种确定性和概率指标衡量下，能够产生最先进的集合预报。它能做出熟练的集合热带气旋路径预测，并且尽管仅在边缘分布上进行训练，也能捕获联合空间结构。", "conclusion": "FGN模型显著优于当前最先进的天气预报模型，在多个评估指标上表现出色，并能有效捕捉联合空间结构，证明了其在概率天气预报领域的强大能力。", "translation": "机器学习（ML）天气模型因其比传统数值天气预报（NWP）更准确和快速而迅速崛起，最近在全球概率天气预报中超越了传统集合预报。本文提出了FGN，一种简单、可扩展且灵活的建模方法，其性能显著优于当前最先进的模型。FGN通过学习模型扰动和适当约束的模型集合来生成集合预报。它直接训练以最小化每个位置预报的连续秩概率分数（CRPS）。它通过一系列确定性和概率指标衡量，生成了最先进的集合预报，做出了熟练的集合热带气旋路径预测，并且尽管仅在边缘分布上进行训练，也能捕获联合空间结构。", "summary": "本文介绍了一种名为FGN的机器学习天气预报模型，该模型通过学习模型扰动生成集合预报，并直接优化连续秩概率分数（CRPS）。FGN在准确性和速度上均优于传统方法和现有最先进的机器学习模型，在各种确定性和概率指标上均达到领先水平，并能有效捕捉联合空间结构，即使仅基于边缘分布进行训练。", "keywords": "机器学习, 天气预报, 概率预报, 集合预报, FGN", "comments": "FGN的创新之处在于其通过学习模型扰动来生成集合预报，并直接优化CRPS，这使其在性能上显著超越了现有模型。其能够在仅基于边缘分布训练的情况下捕获联合空间结构，这一点尤为重要，因为它简化了训练过程同时保持了对复杂空间关系的理解。该模型的简单性、可扩展性和灵活性也使其具有广泛的应用潜力。"}}
{"id": "2506.10683", "title": "Enhancing Deepfake Detection using SE Block Attention with CNN", "authors": ["Subhram Dasgupta", "Janelle Mason", "Xiaohong Yuan", "Olusola Odeyomi", "Kaushik Roy"], "summary": "In the digital age, Deepfake present a formidable challenge by using advanced\nartificial intelligence to create highly convincing manipulated content,\nundermining information authenticity and security. These sophisticated\nfabrications surpass traditional detection methods in complexity and realism.\nTo address this issue, we aim to harness cutting-edge deep learning\nmethodologies to engineer an innovative deepfake detection model. However, most\nof the models designed for deepfake detection are large, causing heavy storage\nand memory consumption. In this research, we propose a lightweight convolution\nneural network (CNN) with squeeze and excitation block attention (SE) for\nDeepfake detection. The SE block module is designed to perform dynamic\nchannel-wise feature recalibration. The SE block allows the network to\nemphasize informative features and suppress less useful ones, which leads to a\nmore efficient and effective learning module. This module is integrated with a\nsimple sequential model to perform Deepfake detection. The model is smaller in\nsize and it achieves competing accuracy with the existing models for deepfake\ndetection tasks. The model achieved an overall classification accuracy of\n94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse\nFake Face Dataset. Our proposed approach presents a promising avenue for\ncombating the Deepfake challenge with minimal computational resources,\ndeveloping efficient and scalable solutions for digital content verification.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10683v1", "AI": {"title_translation": "使用SE块注意力与CNN增强Deepfake检测", "tldr": "本文提出一种轻量级CNN模型，结合SE块注意力机制，用于Deepfake检测，该模型在保持高准确率的同时显著降低了计算资源消耗。", "motivation": "深度伪造（Deepfake）对信息真实性和安全性构成巨大挑战，传统检测方法不足以应对其复杂性和真实性。现有深度伪造检测模型通常体积庞大，导致存储和内存消耗高。", "method": "提出一种轻量级卷积神经网络（CNN）模型，并集成了Squeeze-and-Excitation（SE）注意力块。SE块旨在执行动态通道特征重新校准，以强调信息丰富的特征并抑制不太有用的特征。该模块与一个简单的顺序模型集成以执行Deepfake检测。", "result": "该模型体积更小，并与现有Deepfake检测模型取得了具有竞争力的准确性。在“Diverse Fake Face Dataset”中的Style GAN数据集上，模型实现了94.14%的整体分类准确率和0.985的AUC-ROC分数。", "conclusion": "所提出的方法为以最小的计算资源对抗Deepfake挑战提供了一条有前途的途径，开发了用于数字内容验证的高效且可扩展的解决方案。", "translation": "在数字时代，Deepfake利用先进的人工智能技术创建高度逼真的篡改内容，对信息真实性和安全性构成了严峻挑战。这些复杂的伪造内容在复杂性和真实性上超越了传统的检测方法。为了解决这个问题，我们旨在利用尖端的深度学习方法来设计一个创新的Deepfake检测模型。然而，大多数用于Deepfake检测的模型都很大，导致存储和内存消耗巨大。在这项研究中，我们提出了一种轻量级卷积神经网络（CNN），结合了Squeeze-and-Excitation（SE）块注意力机制，用于Deepfake检测。SE块模块旨在执行动态通道特征重新校准。SE块允许网络强调信息丰富的特征并抑制不太有用的特征，从而实现更高效和有效的学习模块。该模块与一个简单的顺序模型集成以执行Deepfake检测。该模型体积更小，并且在Deepfake检测任务上与现有模型取得了具有竞争力的准确性。该模型在“Diverse Fake Face Dataset”中的Style GAN数据集上实现了94.14%的整体分类准确率和0.985的AUC-ROC分数。我们提出的方法为以最小的计算资源对抗Deepfake挑战提供了一条有前途的途径，为数字内容验证开发高效且可扩展的解决方案。", "summary": "本文针对Deepfake检测中现有模型体积庞大且资源消耗高的问题，提出了一种轻量级的卷积神经网络（CNN）模型，该模型通过集成Squeeze-and-Excitation（SE）注意力块来动态校准通道特征，从而提高检测效率和效果。实验结果表明，该模型在保持高准确率（94.14%分类准确率，0.985 AUC-ROC）的同时，显著减小了模型体积，为Deepfake检测提供了高效且可扩展的解决方案。", "keywords": "Deepfake检测, CNN, SE块注意力, 轻量级模型, 信息真实性", "comments": "这项研究的创新点在于将SE注意力块集成到轻量级CNN中，有效解决了Deepfake检测模型通常体积庞大、资源消耗高的问题。通过强调重要特征并抑制不重要特征，SE块增强了模型的学习效率和检测性能，同时保持了较低的计算开销，这对于实际部署和资源受限环境下的应用具有重要意义。"}}
{"id": "2506.10775", "title": "Monotone Classification with Relative Approximations", "authors": ["Yufei Tao"], "summary": "In monotone classification, the input is a multi-set $P$ of points in\n$\\mathbb{R}^d$, each associated with a hidden label from $\\{-1, 1\\}$. The goal\nis to identify a monotone function $h$, which acts as a classifier, mapping\nfrom $\\mathbb{R}^d$ to $\\{-1, 1\\}$ with a small {\\em error}, measured as the\nnumber of points $p \\in P$ whose labels differ from the function values $h(p)$.\nThe cost of an algorithm is defined as the number of points having their labels\nrevealed. This article presents the first study on the lowest cost required to\nfind a monotone classifier whose error is at most $(1 + \\epsilon) \\cdot k^*$\nwhere $\\epsilon \\ge 0$ and $k^*$ is the minimum error achieved by an optimal\nmonotone classifier -- in other words, the error is allowed to exceed the\noptimal by at most a relative factor. Nearly matching upper and lower bounds\nare presented for the full range of $\\epsilon$. All previous work on the\nproblem can only achieve an error higher than the optimal by an absolute\nfactor.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10775v1", "AI": {"title_translation": "单调分类的相对近似", "tldr": "本文首次研究了在单调分类中，以最低成本找到一个错误率与最优分类器相比仅有相对因子近似的分类器，并给出了接近匹配的上下界。", "motivation": "之前的研究只能在错误率上实现绝对因子近似，而本文旨在探索实现相对因子近似的最低成本，填补了这一空白。", "method": "文章通过理论分析，为在单调分类中实现相对近似错误率的分类器，提出了接近匹配的成本上下界。", "result": "提出了用于寻找误差最多为最优误差的(1 + ε)倍的单调分类器的成本的近乎匹配的上下界。", "conclusion": "本文首次证明了在单调分类问题中，可以以较低的成本找到一个与最优分类器相比具有相对近似误差的分类器，并提供了理论上的成本界限。", "translation": "在单调分类中，输入是$\\mathbb{R}^d$中的点集$P$，每个点都关联一个隐藏标签，值为$\\{-1, 1\\}$。目标是识别一个单调函数$h$，该函数作为分类器，将$\\mathbb{R}^d$映射到$\\{-1, 1\\}$，且具有较小的“错误”，错误衡量为点集$P$中标签与函数值$h(p)$不同的点数。算法的成本定义为揭示标签的点数。本文首次研究了找到一个单调分类器的最低成本，该分类器的错误率最多为$(1 + \\epsilon) \\cdot k^*$，其中$\\epsilon \\ge 0$，$k^*$是最佳单调分类器实现的最小错误率——换句话说，允许错误率最多比最优值高一个相对因子。本文为$\\epsilon$的整个范围提供了接近匹配的上下界。之前关于该问题的所有工作都只能实现比最优值高一个绝对因子的错误率。", "summary": "本文首次探讨了在单调分类问题中，以最低成本找到一个错误率与最优分类器相比仅存在相对因子近似（即错误率至多为$(1 + \\epsilon) \\cdot k^*$）的分类器。与以往仅能达到绝对因子近似的工作不同，本文为实现相对近似提供了理论基础，并为$\\epsilon$的整个范围提出了近乎匹配的成本上下界。", "keywords": "单调分类, 相对近似, 分类器, 成本, 误差界限", "comments": "这项工作具有重要的理论意义，因为它首次将单调分类的近似误差从绝对因子提升到相对因子，这在实际应用中可能更具鲁棒性。研究成本的上下界为后续算法设计提供了理论指导。"}}
{"id": "2506.10801", "title": "Dense Associative Memory with Epanechnikov Energy", "authors": ["Benjamin Hoover", "Zhaoyang Shi", "Krishnakumar Balasubramanian", "Dmitry Krotov", "Parikshit Ram"], "summary": "We propose a novel energy function for Dense Associative Memory (DenseAM)\nnetworks, the log-sum-ReLU (LSR), inspired by optimal kernel density\nestimation. Unlike the common log-sum-exponential (LSE) function, LSR is based\non the Epanechnikov kernel and enables exact memory retrieval with exponential\ncapacity without requiring exponential separation functions. Moreover, it\nintroduces abundant additional \\emph{emergent} local minima while preserving\nperfect pattern recovery -- a characteristic previously unseen in DenseAM\nliterature. Empirical results show that LSR energy has significantly more local\nminima (memories) that have comparable log-likelihood to LSE-based models.\nAnalysis of LSR's emergent memories on image datasets reveals a degree of\ncreativity and novelty, hinting at this method's potential for both large-scale\nmemory storage and generative tasks.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10801v1", "AI": {"title_translation": "具有Epanechnikov能量的密集关联记忆", "tldr": "本文提出了一种基于Epanechnikov核的新型密集关联记忆能量函数LSR，它实现了指数容量的精确记忆检索，并引入了大量新出现的局部极小值。", "motivation": "受最优核密度估计的启发，本文旨在为密集关联记忆网络提出一种新型能量函数，以解决现有方法（如LSE）的局限性，并提高记忆检索能力和引入新的特性。", "method": "本文提出了一种名为log-sum-ReLU (LSR) 的新型能量函数，该函数基于Epanechnikov核，与常见的log-sum-exponential (LSE) 函数不同。LSR能够实现具有指数容量的精确记忆检索，且无需指数分离函数。它还引入了大量额外的“涌现”局部极小值，同时保持完美的模式恢复。", "result": "经验结果表明，LSR能量具有显著更多的局部极小值（记忆），其对数似然与基于LSE的模型相当。对图像数据集上LSR涌现记忆的分析显示出一定程度的创造性和新颖性。", "conclusion": "LSR能量函数在密集关联记忆网络中表现出卓越的性能，不仅实现了精确的记忆检索和指数容量，还引入了具有创造性和新颖性的涌现记忆，这表明其在大规模记忆存储和生成任务方面的潜力。", "translation": "我们为密集关联记忆（DenseAM）网络提出了一种新型能量函数——对数-和-ReLU（LSR），其灵感来源于最优核密度估计。与常见的对数-和-指数（LSE）函数不同，LSR基于Epanechnikov核，无需指数分离函数即可实现指数容量的精确记忆检索。此外，它在保持完美模式恢复的同时，引入了大量额外的“涌现”局部极小值——这是DenseAM文献中前所未见的特性。经验结果表明，LSR能量具有显著更多的局部极小值（记忆），其对数似然与基于LSE的模型相当。对图像数据集上LSR涌现记忆的分析揭示了其一定程度的创造性和新颖性，暗示了该方法在大规模记忆存储和生成任务方面的潜力。", "summary": "本文提出了一种名为log-sum-ReLU (LSR) 的新型能量函数，用于密集关联记忆网络。该函数基于Epanechnikov核，与传统的log-sum-exponential (LSE) 函数相比，LSR在无需指数分离函数的情况下实现了指数容量的精确记忆检索，并引入了大量以前未见的涌现局部极小值。实验结果表明，LSR具有更多具有可比对数似然的局部极小值，并且其涌现记忆在图像数据集上展现出创造性和新颖性，预示着该方法在记忆存储和生成任务中的潜力。", "keywords": "密集关联记忆, Epanechnikov核, LSR能量, 记忆检索, 局部极小值", "comments": "该论文创新性地将Epanechnikov核引入密集关联记忆网络，提出了LSR能量函数，解决了传统方法在记忆容量和新颖性方面的局限。LSR引入的“涌现”局部极小值是一个重要的发现，不仅增加了记忆数量，还可能为生成任务提供新的视角。这对于大规模记忆存储和探索AI创造力具有重要意义。"}}
{"id": "2506.10689", "title": "Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery", "authors": ["Christopher Gaul", "Eduardo Fidalgo", "Enrique Alegre", "Rocío Alaiz Rodríguez", "Eri Pérez Corral"], "summary": "Accurate automatic screening of minors in unconstrained images demands models\nthat are robust to distribution shift and resilient to the children\nunder-representation in publicly available data. To overcome these issues, we\npropose a multi-task architecture with dedicated under/over-age discrimination\ntasks based on a frozen FaRL vision-language backbone joined with a compact\ntwo-layer MLP that shares features across one age-regression head and four\nbinary under-age heads for age thresholds of 12, 15, 18, and 21 years, focusing\non the legally critical age range. To address the severe class imbalance, we\nintroduce an $\\alpha$-reweighted focal-style loss and age-balanced mini-batch\nsampling, which equalizes twelve age bins during stochastic optimization.\nFurther improvement is achieved with an age gap that removes edge cases from\nthe loss.\n  Moreover, we set a rigorous evaluation by proposing the Overall Under-Age\nBenchmark, with 303k cleaned training images and 110k test images, defining\nboth the \"ASORES-39k\" restricted overall test, which removes the noisiest\ndomains, and the age estimation wild shifts test \"ASWIFT-20k\" of 20k-images,\nstressing extreme pose ($>$45{\\deg}), expression, and low image quality to\nemulate real-world shifts.\n  Trained on the cleaned overall set with resampling and age gap, our multiage\nmodel \"F\" lowers the root-mean-square-error on the ASORES-39k restricted test\nfrom 5.733 (age-only baseline) to 5.656 years and lifts under-18 detection from\nF2 score of 0.801 to 0.857 at 1% false-adult rate. Under the domain shift to\nthe wild data of ASWIFT-20k, the same configuration nearly sustains 0.99 recall\nwhile boosting F2 from 0.742 to 0.833 with respect to the age-only baseline,\ndemonstrating strong generalization under distribution shift. For the under-12\nand under-15 tasks, the respective boosts in F2 are from 0.666 to 0.955 and\nfrom 0.689 to 0.916, respectively.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10689v1", "AI": {"title_translation": "非受限图像中基于多任务和多年龄方法的未成年人检测", "tldr": "该研究提出了一种多任务、多年龄架构，用于在非受限图像中准确筛选未成年人，通过引入新的损失函数和数据采样策略，并在挑战性基准上实现了显著的性能提升。", "motivation": "在非受限图像中准确自动筛选未成年人，需要模型对分布偏移具有鲁棒性，并能应对公开数据中儿童代表性不足的问题。", "method": "提出了一种多任务架构，基于冻结的FaRL视觉-语言骨干网络和紧凑的两层MLP，共享一个年龄回归头和四个二元未成年人检测头（针对12、15、18和21岁阈值）。为解决严重的类别不平衡问题，引入了α-重加权焦点式损失和年龄平衡的小批量采样。此外，通过年龄间隙去除损失中的边缘情况。还提出了“整体未成年人基准”进行严格评估，包括ASORES-39k受限测试和ASWIFT-20k野外偏移测试。", "result": "在ASORES-39k受限测试中，多年龄模型“F”将均方根误差从5.733年（仅年龄基线）降低到5.656年，在1%的成人误报率下，将18岁以下检测的F2分数从0.801提高到0.857。在ASWIFT-20k的域偏移测试中，召回率接近0.99，F2分数从0.742提高到0.833。对于12岁以下和15岁以下任务，F2分数分别从0.666提高到0.955，从0.689提高到0.916。", "conclusion": "该研究提出的多任务、多年龄方法在非受限图像的未成年人检测中表现出强大的泛化能力和显著的性能提升，尤其是在处理分布偏移和数据不平衡方面。", "translation": "在非受限图像中准确自动筛选未成年人，要求模型对分布偏移具有鲁棒性，并能应对公开数据中儿童代表性不足的问题。为了克服这些问题，我们提出了一种多任务架构，该架构具有专门的未成年/超龄判别任务，基于冻结的FaRL视觉-语言骨干网络，并与一个紧凑的两层MLP连接，该MLP在一个年龄回归头和四个二元未成年人检测头（针对12、15、18和21岁的年龄阈值）之间共享特征，重点关注法律关键年龄范围。为了解决严重的类别不平衡问题，我们引入了一种α-重加权焦点式损失和年龄平衡的小批量采样，在随机优化过程中平衡了十二个年龄段。通过年龄间隙进一步提高性能，该间隙从损失中移除了边缘情况。此外，我们通过提出“整体未成年人基准”进行了严格评估，该基准包含30.3万张清理过的训练图像和11万张测试图像，定义了移除最嘈杂域的“ASORES-39k”受限整体测试，以及模拟真实世界偏移的2万张图像的年龄估计野外偏移测试“ASWIFT-20k”，该测试强调极端姿态（>45度）、表情和低图像质量。在经过重采样和年龄间隙处理的清理过的整体数据集上训练后，我们的多年龄模型“F”在ASORES-39k受限测试中将均方根误差从5.733（仅年龄基线）降低到5.656年，并在1%的成人误报率下，将18岁以下检测的F2分数从0.801提高到0.857。在ASWIFT-20k的野外数据域偏移下，相同的配置几乎保持了0.99的召回率，同时相对于仅年龄基线，将F2分数从0.742提高到0.833，这表明在分布偏移下具有强大的泛化能力。对于12岁以下和15岁以下的任务，F2分数分别从0.666提高到0.955，从0.689提高到0.916。", "summary": "该论文提出了一种用于非受限图像中未成年人检测的多任务、多年龄方法。为了解决数据不平衡和分布偏移问题，该方法采用基于FaRL骨干网络的多任务架构，结合年龄回归和多个二元未成年人分类头，并引入了α-重加权焦点式损失和年龄平衡采样策略。研究还构建了新的评估基准。实验结果表明，该方法在年龄估计和未成年人检测方面均显著优于基线模型，并在域偏移下表现出强大的泛化能力。", "keywords": "未成年人检测, 多任务学习, 年龄估计, 分布偏移, 深度学习", "comments": "该论文的创新点在于提出了一个结合多任务学习和多年龄分类的架构，并针对未成年人检测特有的数据不平衡和域偏移问题设计了有效的解决方案（如α-重加权焦点式损失和年龄平衡采样）。其提出的新基准也为未来研究提供了更严格的评估标准，对该领域的实际应用具有重要意义。"}}
{"id": "2506.10805", "title": "Detecting High-Stakes Interactions with Activation Probes", "authors": ["Alex McKenzie", "Urja Pawar", "Phil Blandfort", "William Bankes", "David Krueger", "Ekdeep Singh Lubana", "Dmitrii Krasheninnikov"], "summary": "Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.", "comment": "33 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10805v1", "AI": {"title_translation": "使用激活探针检测高风险交互", "tldr": "本文研究了使用激活探针来检测大型语言模型中的高风险交互。发现这些探针在性能上与大型LLM监测器相当，但计算成本显著降低，并适用于分层监测系统。", "motivation": "为了安全部署大型语言模型（LLMs），监测高风险交互（可能导致重大危害的文本交互）是一个关键但尚未充分探索的目标。", "method": "本文评估了几种在合成数据上训练的探针架构，并测试了它们在多样化、分布外真实世界数据上的泛化能力。", "result": "研究发现，激活探针能够对多样化、分布外真实世界数据进行鲁棒泛化。探针的性能与提示或微调的中型LLM监测器相当，同时计算成本降低了六个数量级。实验还强调了构建资源感知分层监测系统的潜力，其中探针可作为高效的初始过滤器。", "conclusion": "激活探针是检测LLM中高风险交互的一种有效且计算效率高的方法，可作为分层监测系统中的高效初始过滤器。", "translation": "监测是安全部署大型语言模型（LLMs）的一个重要方面。本文研究了激活探针在检测“高风险”交互（即文本表明交互可能导致重大危害）方面的应用，将其视为此类监测的一个关键但尚未充分探索的目标。我们评估了几种在合成数据上训练的探针架构，发现它们对多样化、分布外的真实世界数据表现出鲁棒的泛化能力。探针的性能与通过提示或微调的中型LLM监测器相当，同时计算成本降低了六个数量级。我们的实验还强调了构建资源感知分层监测系统的潜力，其中探针可作为高效的初始过滤器，并标记出需要更昂贵的下游分析的案例。我们发布了新颖的合成数据集和代码库，以鼓励进一步研究。", "summary": "本研究探讨了使用激活探针来监测大型语言模型中的高风险交互，这是LLM安全部署的关键组成部分。通过在合成数据上训练并评估，发现这些探针能有效泛化到真实世界数据，性能与大型LLM监测器相当，但计算效率显著提高六个数量级。研究还提出将探针作为分层监测系统中的高效初始过滤器，并发布了相关数据集和代码库。", "keywords": "激活探针, 高风险交互, LLM监测, 计算效率, 分层监测", "comments": "本文的创新之处在于提出了使用激活探针来高效检测LLM中的高风险交互，并证明了其在计算效率方面的巨大优势（六个数量级）。这对于实际部署LLM的安全性监测具有重要意义，尤其是在资源受限的环境中。其提出的分层监测系统概念也为未来的LLM安全架构提供了新的思路。"}}
{"id": "2506.10710", "title": "Continual Hyperbolic Learning of Instances and Classes", "authors": ["Melika Ayoughi", "Mina Ghadimi Atigh", "Mohammad Mahdi Derakhshani", "Cees G. M. Snoek", "Pascal Mettes", "Paul Groth"], "summary": "Continual learning has traditionally focused on classifying either instances\nor classes, but real-world applications, such as robotics and self-driving\ncars, require models to handle both simultaneously. To mirror real-life\nscenarios, we introduce the task of continual learning of instances and\nclasses, at the same time. This task challenges models to adapt to multiple\nlevels of granularity over time, which requires balancing fine-grained instance\nrecognition with coarse-grained class generalization. In this paper, we\nidentify that classes and instances naturally form a hierarchical structure. To\nmodel these hierarchical relationships, we propose HyperCLIC, a continual\nlearning algorithm that leverages hyperbolic space, which is uniquely suited\nfor hierarchical data due to its ability to represent tree-like structures with\nlow distortion and compact embeddings. Our framework incorporates hyperbolic\nclassification and distillation objectives, enabling the continual embedding of\nhierarchical relations. To evaluate performance across multiple granularities,\nwe introduce continual hierarchical metrics. We validate our approach on\nEgoObjects, the only dataset that captures the complexity of hierarchical\nobject recognition in dynamic real-world environments. Empirical results show\nthat HyperCLIC operates effectively at multiple granularities with improved\nhierarchical generalization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10710v1", "AI": {"title_translation": "实例和类别的持续双曲学习", "tldr": "该论文提出了一种新的持续学习任务，即同时学习实例和类别，并提出了基于双曲空间的算法HyperCLIC，在多粒度上表现出更好的层次泛化能力。", "motivation": "传统的持续学习主要关注实例或类别的分类，但现实世界的应用（如机器人和自动驾驶汽车）要求模型能同时处理两者。这需要模型随时间适应多粒度级别，平衡细粒度实例识别和粗粒度类别泛化。", "method": "论文提出了HyperCLIC，一种利用双曲空间进行持续学习的算法，以建模类和实例之间自然形成的层次结构。该框架结合了双曲分类和蒸馏目标，实现了层次关系的持续嵌入。为了评估多粒度下的性能，引入了持续层次度量。", "result": "在EgoObjects数据集上的实证结果表明，HyperCLIC在多粒度下有效运行，并具有改进的层次泛化能力。", "conclusion": "HyperCLIC通过利用双曲空间处理层次数据，有效地实现了实例和类别的同时持续学习，并展示了改进的层次泛化能力。", "translation": "持续学习传统上专注于对实例或类别进行分类，但现实世界的应用，如机器人和自动驾驶汽车，要求模型能够同时处理两者。为了反映真实场景，我们引入了同时进行实例和类别持续学习的任务。这项任务挑战模型随着时间的推移适应多个粒度级别，这需要平衡细粒度的实例识别与粗粒度的类别泛化。在本文中，我们发现类别和实例自然形成一个层次结构。为了建模这些层次关系，我们提出了HyperCLIC，一种利用双曲空间的持续学习算法，双曲空间因其能够以低失真和紧凑嵌入来表示树状结构而独特地适用于层次数据。我们的框架结合了双曲分类和蒸馏目标，实现了层次关系的持续嵌入。为了评估多粒度下的性能，我们引入了持续层次度量。我们在EgoObjects上验证了我们的方法，这是唯一一个捕捉动态真实世界环境中层次对象识别复杂性的数据集。实证结果表明，HyperCLIC在多粒度下有效运行，并具有改进的层次泛化能力。", "summary": "本文针对持续学习中同时处理实例和类别的空白，提出了一个新的任务，这对现实世界应用至关重要。论文识别出实例和类别自然形成层次结构，并提出了HyperCLIC算法。该算法利用双曲空间建模层次关系，并结合了双曲分类和蒸馏目标。通过引入持续层次度量并在EgoObjects数据集上进行验证，HyperCLIC展示了在多粒度下的有效操作和改进的层次泛化能力。", "keywords": "持续学习, 双曲空间, 实例识别, 类别泛化, 层次学习", "comments": "该论文通过将实例和类别识别相结合，解决了一个新颖且实用的持续学习问题。在这一背景下，利用双曲空间建模层次数据是创新之举。新度量的引入以及在相关数据集上的评估增强了这项工作的价值。"}}
{"id": "2506.10831", "title": "Efficiency Robustness of Dynamic Deep Learning Systems", "authors": ["Ravishka Rathnasuriya", "Tingxi Li", "Zexin Xu", "Zihe Song", "Mirazul Haque", "Simin Chen", "Wei Yang"], "summary": "Deep Learning Systems (DLSs) are increasingly deployed in real-time\napplications, including those in resourceconstrained environments such as\nmobile and IoT devices. To address efficiency challenges, Dynamic Deep Learning\nSystems (DDLSs) adapt inference computation based on input complexity, reducing\noverhead. While this dynamic behavior improves efficiency, such behavior\nintroduces new attack surfaces. In particular, efficiency adversarial attacks\nexploit these dynamic mechanisms to degrade system performance. This paper\nsystematically explores efficiency robustness of DDLSs, presenting the first\ncomprehensive taxonomy of efficiency attacks. We categorize these attacks based\non three dynamic behaviors: (i) attacks on dynamic computations per inference,\n(ii) attacks on dynamic inference iterations, and (iii) attacks on dynamic\noutput production for downstream tasks. Through an in-depth evaluation, we\nanalyze adversarial strategies that target DDLSs efficiency and identify key\nchallenges in securing these systems. In addition, we investigate existing\ndefense mechanisms, demonstrating their limitations against increasingly\npopular efficiency attacks and the necessity for novel mitigation strategies to\nsecure future adaptive DDLSs.", "comment": "Accepted to USENIX Security '25", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10831v1", "AI": {"title_translation": "动态深度学习系统的效率鲁棒性", "tldr": "本文系统性地探讨了动态深度学习系统（DDLSs）的效率鲁棒性，首次提出了效率攻击的全面分类，并分析了现有防御机制的局限性，强调了未来安全自适应DDLSs需要新的缓解策略。", "motivation": "深度学习系统（DLSs）越来越多地部署在实时应用中，包括资源受限的环境。动态深度学习系统（DDLSs）通过根据输入复杂性调整推理计算来提高效率，但这引入了新的攻击面，即效率对抗性攻击，这些攻击利用动态机制来降低系统性能。", "method": "本文系统性地探索了DDLSs的效率鲁棒性，提出了第一个全面的效率攻击分类。作者根据三种动态行为对攻击进行了分类：(i) 对每次推理的动态计算的攻击，(ii) 对动态推理迭代的攻击，以及(iii) 对下游任务的动态输出生成的攻击。通过深入评估，分析了针对DDLSs效率的对抗策略，并识别了保护这些系统的关键挑战。此外，研究了现有的防御机制，展示了它们在对抗日益流行的效率攻击时的局限性。", "result": "通过深入评估，分析了针对DDLSs效率的对抗策略，识别了保护这些系统的关键挑战。研究表明，现有防御机制在对抗日益流行的效率攻击时存在局限性。", "conclusion": "为了确保未来自适应动态深度学习系统的安全，需要新的缓解策略来应对日益流行的效率攻击。", "translation": "深度学习系统（DLSs）越来越多地部署在实时应用中，包括移动和物联网设备等资源受限的环境。为了解决效率挑战，动态深度学习系统（DDLSs）根据输入复杂性调整推理计算，从而降低开销。虽然这种动态行为提高了效率，但它也引入了新的攻击面。特别是，效率对抗性攻击利用这些动态机制来降低系统性能。本文系统性地探讨了DDLSs的效率鲁棒性，首次提出了效率攻击的全面分类。我们将这些攻击根据三种动态行为进行分类：(i) 对每次推理的动态计算的攻击，(ii) 对动态推理迭代的攻击，以及(iii) 对下游任务的动态输出生成的攻击。通过深入评估，我们分析了针对DDLSs效率的对抗策略，并识别了保护这些系统的关键挑战。此外，我们调查了现有的防御机制，展示了它们在对抗日益流行的效率攻击时的局限性，以及为保护未来自适应DDLSs而采取新型缓解策略的必要性。", "summary": "本文系统性地研究了动态深度学习系统（DDLSs）的效率鲁棒性，因为它们在资源受限的实时应用中面临效率对抗性攻击。研究首次提出了效率攻击的全面分类，并根据DDLSs的三种动态行为进行了细致的分类。通过深入评估，作者分析了针对DDLSs效率的对抗策略，识别了安全挑战，并揭示了现有防御机制的局限性，最终强调了开发新型缓解策略以保障未来自适应DDLSs安全的必要性。", "keywords": "动态深度学习系统, 效率鲁棒性, 对抗性攻击, 攻击分类, 系统安全", "comments": "本文创新性地提出了首个针对动态深度学习系统（DDLSs）效率攻击的全面分类，填补了该领域研究的空白。它系统性地分析了DDLSs面临的效率对抗性攻击的威胁，并深入探讨了现有防御机制的不足，这对于保障DDLSs在资源受限环境下的安全部署具有重要意义。该研究不仅揭示了新的攻击面，也为未来设计更鲁棒、更安全的自适应深度学习系统指明了方向。"}}
{"id": "2506.10712", "title": "Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement", "authors": ["Yuqi Shen", "Fengyang Xiao", "Sujie Hu", "Youwei Pang", "Yifan Pu", "Chengyu Fang", "Xiu Li", "Chunming He"], "summary": "Camouflaged Object Detection (COD) presents inherent challenges due to the\nsubtle visual differences between targets and their backgrounds. While existing\nmethods have made notable progress, there remains significant potential for\npost-processing refinement that has yet to be fully explored. To address this\nlimitation, we propose the Uncertainty-Masked Bernoulli Diffusion (UMBD) model,\nthe first generative refinement framework specifically designed for COD. UMBD\nintroduces an uncertainty-guided masking mechanism that selectively applies\nBernoulli diffusion to residual regions with poor segmentation quality,\nenabling targeted refinement while preserving correctly segmented areas. To\nsupport this process, we design the Hybrid Uncertainty Quantification Network\n(HUQNet), which employs a multi-branch architecture and fuses uncertainty from\nmultiple sources to improve estimation accuracy. This enables adaptive guidance\nduring the generative sampling process. The proposed UMBD framework can be\nseamlessly integrated with a wide range of existing Encoder-Decoder-based COD\nmodels, combining their discriminative capabilities with the generative\nadvantages of diffusion-based refinement. Extensive experiments across multiple\nCOD benchmarks demonstrate consistent performance improvements, achieving\naverage gains of 5.5% in MAE and 3.2% in weighted F-measure with only modest\ncomputational overhead. Code will be released.", "comment": "16 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10712v1", "AI": {"title_translation": "用于伪装目标检测细化的不确定性掩码伯努利扩散", "tldr": "提出了一种名为不确定性掩码伯努利扩散（UMBD）的新型生成式细化框架，用于伪装目标检测（COD）的后处理，通过不确定性引导的掩码机制和混合不确定性量化网络（HUQNet）实现了显著的性能提升。", "motivation": "现有伪装目标检测（COD）方法在后处理细化方面仍有巨大潜力尚未充分探索，而伪装目标与背景之间的细微视觉差异带来了固有的挑战。", "method": "提出了不确定性掩码伯努利扩散（UMBD）模型，这是首个专为COD设计的生成式细化框架。UMBD引入了一种不确定性引导的掩码机制，将伯努利扩散选择性地应用于分割质量差的残差区域。为了支持这一过程，设计了混合不确定性量化网络（HUQNet），它采用多分支架构并融合来自多个源的不确定性以提高估计精度，从而在生成采样过程中实现自适应引导。该框架可以无缝集成到现有的基于编码器-解码器的COD模型中。", "result": "在多个COD基准测试中，UMBD框架表现出持续的性能改进，平均在MAE上提升5.5%，在加权F-measure上提升3.2%，且计算开销适中。", "conclusion": "UMBD框架作为首个用于伪装目标检测的生成式细化方法，通过有效利用不确定性引导的扩散过程，显著提升了现有模型的性能，证明了后处理细化在COD领域的巨大潜力。", "translation": "伪装目标检测（COD）由于目标与其背景之间细微的视觉差异而带来了固有的挑战。尽管现有方法取得了显著进展，但在后处理细化方面仍有巨大潜力尚未充分探索。为了解决这一局限性，我们提出了不确定性掩码伯努利扩散（UMBD）模型，这是首个专为COD设计的生成式细化框架。UMBD引入了一种不确定性引导的掩码机制，将伯努利扩散选择性地应用于分割质量差的残差区域，从而实现有针对性的细化，同时保留正确分割的区域。为了支持这一过程，我们设计了混合不确定性量化网络（HUQNet），它采用多分支架构并融合来自多个源的不确定性以提高估计精度。这使得在生成采样过程中能够实现自适应引导。所提出的UMBD框架可以与各种现有的基于编码器-解码器的COD模型无缝集成，将其判别能力与基于扩散的细化的生成优势相结合。在多个COD基准测试中进行的广泛实验表明，性能持续改进，在MAE上平均提升5.5%，在加权F-measure上提升3.2%，且计算开销适中。代码将发布。", "summary": "该论文提出了一种名为不确定性掩码伯努利扩散（UMBD）的新型生成式细化框架，专门用于伪装目标检测（COD）的后处理。UMBD通过引入不确定性引导的掩码机制，选择性地对分割质量差的区域应用伯努利扩散进行精细化。为支持此过程，还设计了混合不确定性量化网络（HUQNet）以提高不确定性估计精度。该框架可与现有COD模型无缝集成，并在多个基准测试中显示出显著的性能提升，例如MAE平均提升5.5%，加权F-measure平均提升3.2%。", "keywords": "伪装目标检测, 扩散模型, 不确定性量化, 后处理细化, 生成模型", "comments": "这项工作具有创新性，因为它首次提出了一个用于伪装目标检测的生成式细化框架，利用不确定性引导的扩散过程。其创新点在于结合了判别性模型与生成式扩散模型的优势，通过精确识别并仅对需要改进的区域进行细化，有效地提升了模型的性能。这种有针对性的细化方法在计算效率和性能提升之间取得了良好的平衡，为COD领域的后处理提供了一个新的范式。"}}
{"id": "2506.10842", "title": "Advanced fraud detection using machine learning models: enhancing financial transaction security", "authors": ["Nudrat Fariha", "Md Nazmuddin Moin Khan", "Md Iqbal Hossain", "Syed Ali Reza", "Joy Chakra Bortty", "Kazi Sharmin Sultana", "Md Shadidur Islam Jawad", "Saniah Safat", "Md Abdul Ahad", "Maksuda Begum"], "summary": "The rise of digital payments has accelerated the need for intelligent and\nscalable systems to detect fraud. This research presents an end-to-end,\nfeature-rich machine learning framework for detecting credit card transaction\nanomalies and fraud using real-world data. The study begins by merging\ntransactional, cardholder, merchant, and merchant category datasets from a\nrelational database to create a unified analytical view. Through the feature\nengineering process, we extract behavioural signals such as average spending,\ndeviation from historical patterns, transaction timing irregularities, and\ncategory frequency metrics. These features are enriched with temporal markers\nsuch as hour, day of week, and weekend indicators to expose all latent patterns\nthat indicate fraudulent behaviours. Exploratory data analysis reveals\ncontextual transaction trends across all the dataset features. Using the\ntransactional data, we train and evaluate a range of unsupervised models:\nIsolation Forest, One Class SVM, and a deep autoencoder trained to reconstruct\nnormal behavior. These models flag the top 1% of reconstruction errors as\noutliers. PCA visualizations illustrate each models ability to separate\nanomalies into a two-dimensional latent space. We further segment the\ntransaction landscape using K-Means clustering and DBSCAN to identify dense\nclusters of normal activity and isolate sparse, suspicious regions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10842v1", "AI": {"title_translation": "使用机器学习模型进行高级欺诈检测：增强金融交易安全性", "tldr": "本研究提出并评估了一个端到端、特征丰富的机器学习框架，用于使用真实世界数据检测信用卡交易异常和欺诈，利用无监督模型（如Isolation Forest、One Class SVM和深度自编码器）以及聚类技术来识别异常行为。", "motivation": "数字支付的兴起加速了对智能且可扩展的欺诈检测系统的需求。", "method": "本研究提出了一个端到端、特征丰富的机器学习框架。首先，将交易、持卡人、商户和商户类别数据集从关系数据库合并，创建统一的分析视图。接着进行特征工程，提取行为信号（如平均支出、偏离历史模式、交易时间不规律和类别频率指标），并用时间标记（如小时、星期几和周末指示器）进行丰富。然后，使用探索性数据分析揭示交易趋势。最后，训练并评估了一系列无监督模型，包括Isolation Forest、One Class SVM和深度自编码器，这些模型将前1%的重建错误标记为异常值。此外，还使用K-Means聚类和DBSCAN来识别正常活动的密集簇并隔离稀疏的可疑区域。", "result": "该框架成功地将前1%的重建错误标记为异常值。PCA可视化展示了每个模型将异常分离到二维潜在空间的能力。K-Means聚类和DBSCAN能够识别正常活动的密集簇并隔离稀疏的可疑区域。", "conclusion": "Not mentioned in abstract", "translation": "数字支付的兴起加速了对智能且可扩展的欺诈检测系统的需求。本研究提出了一个端到端、特征丰富的机器学习框架，用于使用真实世界数据检测信用卡交易异常和欺诈。该研究首先将来自关系数据库的交易、持卡人、商户和商户类别数据集进行合并，以创建一个统一的分析视图。通过特征工程过程，我们提取了行为信号，例如平均支出、偏离历史模式、交易时间不规律和类别频率指标。这些特征通过时间标记（例如小时、星期几和周末指示器）得到丰富，以揭示所有指示欺诈行为的潜在模式。探索性数据分析揭示了所有数据集特征中的上下文交易趋势。使用交易数据，我们训练并评估了一系列无监督模型：Isolation Forest、One Class SVM和一个经过训练以重建正常行为的深度自编码器。这些模型将前1%的重建错误标记为异常值。PCA可视化说明了每个模型将异常分离到二维潜在空间的能力。我们使用K-Means聚类和DBSCAN进一步细分交易场景，以识别正常活动的密集簇并隔离稀疏的可疑区域。", "summary": "本研究旨在应对数字支付增长带来的欺诈检测需求，提出了一个先进的机器学习框架。该框架整合了多源数据，通过精细的特征工程提取行为和时间信号。研究利用Isolation Forest、One Class SVM和深度自编码器等无监督模型识别异常交易，并将前1%的重建错误标记为异常。同时，结合PCA可视化和K-Means、DBSCAN聚类技术，有效区分正常与可疑交易模式，从而增强金融交易的安全性。", "keywords": "欺诈检测, 机器学习, 信用卡欺诈, 无监督学习, 特征工程", "comments": "该论文的创新点在于其端到端的机器学习框架，结合了多源数据整合、丰富的特征工程和多种无监督学习模型（包括深度学习的自编码器）来检测欺诈。其方法全面，从数据预处理到模型评估和可视化，都展现了系统性的方法。论文的重要性在于其为金融交易安全提供了实用的、可扩展的解决方案。不足之处在于摘要中未明确提及模型的性能指标或与现有方法的比较。"}}
{"id": "2506.10871", "title": "Viability of Future Actions: Robust Safety in Reinforcement Learning via Entropy Regularization", "authors": ["Pierre-François Massiani", "Alexander von Rohr", "Lukas Haverbeck", "Sebastian Trimpe"], "summary": "Despite the many recent advances in reinforcement learning (RL), the question\nof learning policies that robustly satisfy state constraints under unknown\ndisturbances remains open. In this paper, we offer a new perspective on\nachieving robust safety by analyzing the interplay between two well-established\ntechniques in model-free RL: entropy regularization, and constraints\npenalization. We reveal empirically that entropy regularization in constrained\nRL inherently biases learning toward maximizing the number of future viable\nactions, thereby promoting constraints satisfaction robust to action noise.\nFurthermore, we show that by relaxing strict safety constraints through\npenalties, the constrained RL problem can be approximated arbitrarily closely\nby an unconstrained one and thus solved using standard model-free RL. This\nreformulation preserves both safety and optimality while empirically improving\nresilience to disturbances. Our results indicate that the connection between\nentropy regularization and robustness is a promising avenue for further\nempirical and theoretical investigation, as it enables robust safety in RL\nthrough simple reward shaping.", "comment": "24 pages, 11 figures, 2 tables. Accepted for publication at ECML-PKDD\n  2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10871v1", "AI": {"title_translation": "动作未来可行性：通过熵正则化在强化学习中实现鲁棒安全", "tldr": "本文通过分析熵正则化和约束惩罚的相互作用，提出了一种在强化学习中实现鲁棒安全的新方法，发现熵正则化能促进未来动作可行性，并通过松弛约束将约束RL问题近似为无约束问题，从而提高对干扰的鲁棒性。", "motivation": "尽管强化学习取得了许多进展，但在未知干扰下学习能够鲁棒满足状态约束的策略问题仍未解决。", "method": "本文通过分析模型无关强化学习中熵正则化和约束惩罚这两种成熟技术的相互作用，提供了一种实现鲁棒安全的新视角。此外，通过惩罚放松严格安全约束，将约束RL问题近似为一个无约束问题，并使用标准模型无关RL方法解决。", "result": "经验上表明，在约束RL中，熵正则化固有地偏向于最大化未来可行动作的数量，从而促进对动作噪声鲁棒的约束满足。通过松弛约束的重新表述，在保持安全性和最优性的同时，经验性地提高了对干扰的弹性。", "conclusion": "熵正则化与鲁棒性之间的联系是一个有前景的研究方向，它通过简单的奖励塑形实现了强化学习中的鲁棒安全。", "translation": "尽管强化学习（RL）最近取得了许多进展，但在未知干扰下学习能够鲁棒满足状态约束的策略问题仍然悬而未决。在本文中，我们通过分析模型无关RL中两种成熟技术——熵正则化和约束惩罚之间的相互作用，提供了一种实现鲁棒安全的新视角。我们通过经验揭示，约束RL中的熵正则化固有地偏向于最大化未来可行动作的数量，从而促进对动作噪声鲁棒的约束满足。此外，我们表明，通过惩罚放松严格安全约束，约束RL问题可以被一个无约束问题任意接近地近似，从而可以使用标准的模型无关RL解决。这种重新表述在保持安全性和最优性的同时，经验性地提高了对干扰的弹性。我们的结果表明，熵正则化与鲁棒性之间的联系是一个有前景的经验和理论研究方向，因为它通过简单的奖励塑形实现了RL中的鲁棒安全。", "summary": "本文提出了一种在强化学习中实现鲁棒安全的新方法，通过深入分析熵正则化和约束惩罚的相互作用。研究发现，熵正则化有助于增加未来可行动作的数量，从而提高对动作噪声的鲁棒性。此外，通过将严格的安全约束转化为惩罚，可以将约束RL问题近似为无约束问题并使用标准方法解决，此方法在保持安全性和最优性的同时，显著提高了对外部干扰的弹性。这表明熵正则化是实现RL鲁棒安全的一个有效途径。", "keywords": "强化学习, 鲁棒安全, 熵正则化, 约束惩罚, 奖励塑形", "comments": "本文的创新之处在于揭示了熵正则化在约束强化学习中促进鲁棒安全的新机制，即通过最大化未来可行动作数量。此外，将约束问题转化为无约束问题的近似方法，为解决带安全约束的强化学习问题提供了实用的途径。这项工作的重要性在于为在复杂、不确定环境下部署强化学习系统提供了更鲁棒的解决方案，并指出了未来研究熵正则化与鲁棒性之间联系的潜力。"}}
{"id": "2506.10730", "title": "IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain", "authors": ["Hong Huang", "Weixiang Sun", "Zhijian Wu", "Jingwen Niu", "Donghuan Lu", "Xian Wu", "Yefeng Zheng"], "summary": "Recent advances in vision-language models, such as CLIP, have significantly\nimproved performance in zero- and few-shot anomaly detection (ZFSAD) tasks.\nHowever, most existing CLIP-based methods assume prior knowledge of categories\nand rely on carefully designed prompts tailored to specific scenarios. While\nthese text prompts capture semantic information in the textual space, they\noften fail to distinguish normal and anomalous instances in the joint embedding\nspace. Moreover, most ZFSAD approaches focus on industrial domains, with\nlimited exploration in medical tasks. To address these limitations, we propose\nIQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query\nembeddings integrating both textual and instance-aware visual information serve\nas more effective indicators of anomalies. Specifically, we introduce\nclass-based and learnable prompting tokens to better adapt CLIP to the medical\nsetting. Furthermore, we design an instance-aware query module that extracts\nregion-level contextual information from both modalities, enabling the\ngeneration of anomaly-sensitive embeddings. Extensive experiments on six\nmedical datasets demonstrate that IQE-CLIP achieves state-of-the-art\nperformance in both zero-shot and few-shot settings. Code and data are\navailable at \\href{https://github.com/hongh0/IQE-CLIP/}{this https URL}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10730v1", "AI": {"title_translation": "IQE-CLIP：用于医学领域零/少样本异常检测的实例感知查询嵌入", "tldr": "IQE-CLIP提出了一种新的框架，通过整合文本和实例感知的视觉信息来有效识别医学领域的零/少样本异常。", "motivation": "现有基于CLIP的方法在零/少样本异常检测（ZFSAD）中存在局限性，例如需要预先的类别知识、依赖精心设计的提示词，且这些提示词无法区分正常和异常实例。此外，大多数ZFSAD方法主要关注工业领域，在医学任务中的探索有限。", "method": "本文提出了IQE-CLIP框架，通过整合文本和实例感知的视觉信息来生成更有效的异常指示器。具体方法包括：引入基于类别的可学习提示词以适应医学环境；设计一个实例感知查询模块，从文本和视觉模态中提取区域级上下文信息，生成对异常敏感的嵌入。", "result": "在六个医学数据集上进行了广泛实验，结果表明IQE-CLIP在零样本和少样本设置下均达到了最先进的性能。", "conclusion": "IQE-CLIP通过结合文本和实例感知的视觉信息，并引入适应医学领域的提示机制和实例感知查询模块，有效解决了医学领域零/少样本异常检测的挑战，并取得了卓越的性能。", "translation": "视觉语言模型（如CLIP）的最新进展显著提高了零/少样本异常检测（ZFSAD）任务的性能。然而，大多数现有的基于CLIP的方法都假设了类别的先验知识，并依赖于针对特定场景精心设计的提示。尽管这些文本提示在文本空间中捕获了语义信息，但它们往往无法在联合嵌入空间中区分正常和异常实例。此外，大多数ZFSAD方法侧重于工业领域，在医学任务中的探索有限。为了解决这些局限性，我们提出了IQE-CLIP，一个用于医学领域ZFSAD的新颖框架。我们表明，结合文本和实例感知视觉信息的查询嵌入可以作为更有效的异常指示器。具体来说，我们引入了基于类别的可学习提示词，以更好地使CLIP适应医学设置。此外，我们设计了一个实例感知查询模块，从两种模态中提取区域级上下文信息，从而能够生成对异常敏感的嵌入。在六个医学数据集上的大量实验表明，IQE-CLIP在零样本和少样本设置下均达到了最先进的性能。代码和数据可在https://github.com/hongh0/IQE-CLIP/获取。", "summary": "本文提出了IQE-CLIP，一个针对医学领域零/少样本异常检测的新框架。该框架旨在克服现有CLIP方法在区分正常与异常实例以及在医学领域应用不足的局限性。IQE-CLIP通过整合文本和实例感知的视觉信息来创建异常敏感的查询嵌入，并引入了适应医学设置的类级别可学习提示词和用于提取区域级上下文信息的实例感知查询模块。实验证明，IQE-CLIP在多个医学数据集上实现了最先进的零样本和少样本异常检测性能。", "keywords": "零/少样本异常检测, 视觉语言模型, CLIP, 医学图像, 实例感知嵌入", "comments": "IQE-CLIP的创新之处在于其将实例感知的视觉信息与文本信息相结合，以提高在医学领域零/少样本异常检测的准确性。其引入的类级别可学习提示词和实例感知查询模块是关键创新点，有效地解决了现有CLIP方法在区分细微异常和适应特定领域方面的不足。这对于医疗诊断自动化具有重要意义。"}}
{"id": "2506.10888", "title": "Lattice Climber Attack: Adversarial attacks for randomized mixtures of classifiers", "authors": ["Lucas Gnecco-Heredia", "Benjamin Negrevergne", "Yann Chevaleyre"], "summary": "Finite mixtures of classifiers (a.k.a. randomized ensembles) have been\nproposed as a way to improve robustness against adversarial attacks. However,\nexisting attacks have been shown to not suit this kind of classifier. In this\npaper, we discuss the problem of attacking a mixture in a principled way and\nintroduce two desirable properties of attacks based on a geometrical analysis\nof the problem (effectiveness and maximality). We then show that existing\nattacks do not meet both of these properties. Finally, we introduce a new\nattack called {\\em lattice climber attack} with theoretical guarantees in the\nbinary linear setting, and demonstrate its performance by conducting\nexperiments on synthetic and real datasets.", "comment": "17 pages including bibliography + 13 pages of supplementary material.\n  Extended version of the article accepted at ECML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10888v1", "AI": {"title_translation": "格点攀爬攻击：针对分类器随机混合的对抗性攻击", "tldr": "本文提出了一种名为“格点攀爬攻击”的新型对抗性攻击，专门针对分类器随机混合（有限混合分类器），并证明其在二元线性设置下的理论保证和在合成及真实数据集上的有效性，解决了现有攻击不适用于此类分类器的问题。", "motivation": "有限混合分类器（即随机集成）已被提出作为提高对抗性攻击鲁棒性的一种方法。然而，现有的攻击已被证明不适用于此类分类器，因此需要一种更原则性的方法来攻击混合分类器。", "method": "本文首先从几何角度分析了攻击混合分类器的问题，并提出了攻击的两个理想特性（有效性和最大性）。接着，指出现有攻击不满足这两个特性。最后，提出了一种新的攻击方法，称为“格点攀爬攻击”，并在二元线性设置下提供了理论保证。", "result": "“格点攀爬攻击”在二元线性设置下具有理论保证，并通过在合成数据集和真实数据集上的实验证明了其性能。", "conclusion": "本文成功引入了一种名为“格点攀爬攻击”的新型对抗性攻击，该攻击针对分类器随机混合，弥补了现有攻击的不足，并得到了理论和实验的支持。", "translation": "有限混合分类器（又称随机集成）已被提出作为提高对抗性攻击鲁棒性的一种方法。然而，现有攻击已被证明不适用于此类分类器。在本文中，我们以原则性的方式讨论了攻击混合分类器的问题，并基于对问题的几何分析引入了攻击的两个理想特性（有效性和最大性）。然后，我们表明现有攻击不满足这两个特性。最后，我们引入了一种新的攻击方法，称为“格点攀爬攻击”，它在二元线性设置下具有理论保证，并通过在合成和真实数据集上进行实验证明了其性能。", "summary": "本文针对有限混合分类器（随机集成）的对抗性攻击问题，指出现有攻击的不足。通过几何分析，提出了攻击应具备的有效性和最大性两个特性，并证明现有攻击不满足。为此，提出了一种名为“格点攀爬攻击”的新型方法，该方法在二元线性设置下具有理论保证，并在合成及真实数据集上验证了其有效性。", "keywords": "对抗性攻击, 分类器混合, 随机集成, 格点攀爬攻击, 鲁棒性", "comments": "本文的创新点在于提出了针对随机混合分类器的新型对抗性攻击——“格点攀爬攻击”，并从几何角度分析了攻击的理想特性，填补了现有攻击在此类分类器上表现不佳的空白。其理论保证和实验验证增加了研究的严谨性。"}}
{"id": "2506.10741", "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework", "authors": ["SiXiang Chen", "Jianyu Lai", "Jialin Gao", "Tian Ye", "Haoyu Chen", "Hengyu Shi", "Shitong Shao", "Yunlong Lin", "Song Fei", "Zhaohu Xing", "Yeying Jin", "Junfeng Luo", "Xiaoming Wei", "Lei Zhu"], "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10741v1", "AI": {"title_translation": "PosterCraft: 在统一框架下重新思考高质量美学海报生成", "tldr": "PosterCraft是一个统一框架，通过多阶段优化生成高质量美学海报，超越现有开源基线。", "motivation": "生成美学海报比简单的设计图像更具挑战性，因为它需要精确的文本渲染、抽象艺术内容的无缝集成、引人注目的布局和整体风格和谐。现有方法存在模块化流程和僵化预定义布局的问题。", "method": "提出PosterCraft，一个统一框架，它摒弃了先前的模块化流程和僵化的预定义布局，允许模型自由探索连贯的、视觉上引人注目的构图。PosterCraft采用精心设计的级联工作流来优化高美学海报的生成：(i) 在新引入的Text-Render-2M数据集上进行大规模文本渲染优化；(ii) 在HQ-Poster100K上进行区域感知监督微调；(iii) 通过n选一偏好优化进行美学文本强化学习；(iv) 联合视觉-语言反馈细化。每个阶段都由一个完全自动化的数据构建管道支持。", "result": "在多项实验中，PosterCraft在渲染精度、布局连贯性和整体视觉吸引力方面显著优于开源基线，接近SOTA商业系统的质量。", "conclusion": "PosterCraft提供了一个有效且统一的框架来生成高质量美学海报，解决了现有方法的局限性，并表现出与商业系统相当的性能。", "translation": "生成美学海报比简单的设计图像更具挑战性：它不仅需要精确的文本渲染，还需要抽象艺术内容的无缝集成、引人注目的布局和整体风格和谐。为了解决这个问题，我们提出了PosterCraft，一个统一的框架，它摒弃了先前的模块化流程和僵化的预定义布局，允许模型自由探索连贯的、视觉上引人注目的构图。PosterCraft采用精心设计的级联工作流来优化高美学海报的生成：(i) 在我们新引入的Text-Render-2M数据集上进行大规模文本渲染优化；(ii) 在HQ-Poster100K上进行区域感知监督微调；(iii) 通过n选一偏好优化进行美学文本强化学习；(iv) 联合视觉-语言反馈细化。每个阶段都由一个完全自动化的数据构建管道支持，该管道根据其特定需求量身定制，无需复杂的架构修改即可实现稳健训练。在多项实验中，PosterCraft在渲染精度、布局连贯性和整体视觉吸引力方面显著优于开源基线，接近SOTA商业系统的质量。我们的代码、模型和数据集可在项目页面找到：https://ephemeral182.github.io/PosterCraft", "summary": "PosterCraft是一个用于生成高质量美学海报的统一框架。它通过放弃传统的模块化流程和固定布局，并采用一个包含文本渲染优化、区域感知微调、强化学习和视觉-语言反馈细化的多阶段级联工作流，解决了美学海报生成中的文本渲染、艺术内容整合和布局和谐等挑战。该框架在多个数据集上进行了训练，并在实验中表现出优于现有开源基线的能力，生成的海报在渲染精度、布局和视觉吸引力方面接近商业系统水平。", "keywords": "美学海报生成, 统一框架, 文本渲染, 布局, 强化学习", "comments": "该论文提出了一种统一的生成美学海报的框架，其创新点在于放弃了传统的模块化和固定布局方法，并引入了多阶段的级联优化流程，特别是引入了新的数据集和强化学习策略。其重要性在于显著提升了海报生成的质量，并提供了一个可与商业系统媲美的开源解决方案。"}}
{"id": "2506.10892", "title": "The Diffusion Duality", "authors": ["Subham Sekhar Sahoo", "Justin Deschenaux", "Aaron Gokaslan", "Guanghan Wang", "Justin Chiu", "Volodymyr Kuleshov"], "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo", "comment": "ICML 2025. We provide the code at: https://github.com/s-sahoo/duo", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10892v1", "AI": {"title_translation": "扩散对偶性", "tldr": "统一状态离散扩散模型通过利用其与高斯扩散的对偶性得到改进。新方法Duo采用课程学习加速训练，并使用离散一致性蒸馏加速采样，在某些情况下性能超越自回归模型并实现少步生成。", "motivation": "统一状态离散扩散模型在快速文本生成方面有潜力，但通常被自回归模型和掩码扩散模型超越。本研究旨在缩小这一性能差距。", "method": "本研究提出了Duo方法，利用统一状态扩散过程从底层高斯扩散中自然涌现的关键见解。该方法将高斯扩散的强大技术应用于改进训练和采样。具体包括：1) 引入由高斯过程指导的课程学习策略；2) 提出了离散一致性蒸馏，将连续设置中的一致性蒸馏适应到离散设置。", "result": "课程学习策略通过减少方差将训练速度提高了一倍。使用课程学习训练的模型在7个基准测试中的3个上，零样本困惑度超过了自回归模型。离散一致性蒸馏将采样速度加快了两个数量级，从而在扩散语言模型中实现了少步生成。", "conclusion": "本论文介绍了Duo方法，通过利用统一状态离散扩散模型与高斯扩散的对偶性，显著改进了模型的训练和采样效率以及性能。", "translation": "统一状态的离散扩散模型因其固有的自我纠正能力，有望实现快速文本生成。然而，它们通常被自回归模型和掩码扩散模型超越。在这项工作中，我们通过利用一个关键见解来缩小这一性能差距：统一状态扩散过程自然地从底层高斯扩散中涌现。我们的方法Duo，将高斯扩散的强大技术应用于改进训练和采样。首先，我们引入了一种由高斯过程指导的课程学习策略，通过减少方差将训练速度提高一倍。使用课程学习训练的模型在7个基准测试中的3个上，零样本困惑度超过了自回归模型。其次，我们提出了离散一致性蒸馏，它将一致性蒸馏从连续设置适应到离散设置。该算法通过将采样速度加快两个数量级，从而在扩散语言模型中实现了少步生成。我们提供了项目页面上的代码和模型检查点：http://s-sahoo.github.io/duo", "summary": "本论文提出了“Duo”方法，旨在缩小统一状态离散扩散模型与自回归和掩码扩散模型之间的性能差距。Duo利用统一状态扩散过程与高斯扩散之间的内在对偶性，并借鉴了高斯扩散的强大技术。它采用了一种高斯引导的课程学习策略，使训练速度加倍，并使模型在某些基准测试中在零样本困惑度上超越自回归模型。此外，Duo还提出了离散一致性蒸馏，将连续一致性蒸馏适应到离散设置，将采样速度加快了两个数量级，从而实现了少步文本生成。", "keywords": "扩散模型, 文本生成, 课程学习, 一致性蒸馏, 离散扩散", "comments": "该研究的创新之处在于利用“扩散对偶性”将高斯扩散的成熟技术应用于改进离散扩散模型。这种方法同时解决了训练效率和推理速度问题，这对于扩散模型在文本生成领域的实际应用至关重要。训练和采样的显著加速，以及与自回归模型相比具有竞争力的性能，突显了这项工作的重要性。"}}
{"id": "2506.10774", "title": "Stroke-based Cyclic Amplifier: Image Super-Resolution at Arbitrary Ultra-Large Scales", "authors": ["Wenhao Guo", "Peng Lu", "Xujun Peng", "Zhaoran Zhao", "Sheng Li"], "summary": "Prior Arbitrary-Scale Image Super-Resolution (ASISR) methods often experience\na significant performance decline when the upsampling factor exceeds the range\ncovered by the training data, introducing substantial blurring. To address this\nissue, we propose a unified model, Stroke-based Cyclic Amplifier (SbCA), for\nultra-large upsampling tasks. The key of SbCA is the stroke vector amplifier,\nwhich decomposes the image into a series of strokes represented as vector\ngraphics for magnification. Then, the detail completion module also restores\nmissing details, ensuring high-fidelity image reconstruction. Our cyclic\nstrategy achieves ultra-large upsampling by iteratively refining details with\nthis unified SbCA model, trained only once for all, while keeping sub-scales\nwithin the training range. Our approach effectively addresses the distribution\ndrift issue and eliminates artifacts, noise and blurring, producing\nhigh-quality, high-resolution super-resolved images. Experimental validations\non both synthetic and real-world datasets demonstrate that our approach\nsignificantly outperforms existing methods in ultra-large upsampling tasks\n(e.g. $\\times100$), delivering visual quality far superior to state-of-the-art\ntechniques.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10774v1", "AI": {"title_translation": "笔画式循环放大器：任意超大尺度图像超分辨率", "tldr": "提出SbCA模型，通过笔画分解和循环策略，解决现有ASISR方法在超大尺度图像超分辨率中性能下降的问题，实现高质量图像重建。", "motivation": "现有的任意尺度图像超分辨率（ASISR）方法在放大因子超出训练数据范围时，性能会显著下降，导致图像模糊。", "method": "提出一个统一模型“笔画式循环放大器（SbCA）”用于超大尺度上采样任务。其核心是“笔画向量放大器”，将图像分解为向量图形表示的笔画进行放大。然后，“细节补全模块”恢复缺失细节。采用“循环策略”通过迭代细化细节来实现超大尺度上采样，该模型只需训练一次，并保持子尺度在训练范围内。", "result": "在合成和真实世界数据集上的实验验证表明，该方法在超大尺度上采样任务（例如×100）中显著优于现有方法，提供远超最新技术的视觉质量，有效解决了分布漂移问题并消除了伪影、噪声和模糊。", "conclusion": "SbCA模型通过其独特的笔画分解和循环策略，成功解决了任意超大尺度图像超分辨率的挑战，显著提升了图像重建质量，超越了现有技术水平。", "translation": "现有任意尺度图像超分辨率（ASISR）方法在放大因子超出训练数据范围时，性能常会显著下降，引入大量模糊。为解决此问题，我们提出了一个统一模型——笔画式循环放大器（Stroke-based Cyclic Amplifier, SbCA），用于超大尺度上采样任务。SbCA的关键在于笔画向量放大器，它将图像分解为一系列以向量图形表示的笔画进行放大。然后，细节补全模块也恢复缺失细节，确保高保真图像重建。我们的循环策略通过使用这个统一的SbCA模型迭代细化细节，实现了超大尺度上采样，该模型只需训练一次即可适用于所有情况，同时保持子尺度在训练范围内。我们的方法有效解决了分布漂移问题，并消除了伪影、噪声和模糊，生成了高质量、高分辨率的超分辨图像。在合成和真实世界数据集上的实验验证表明，我们的方法在超大尺度上采样任务（例如×100）中显著优于现有方法，提供的视觉质量远超最新技术。", "summary": "本文针对现有任意尺度图像超分辨率（ASISR）方法在超大放大倍数下性能下降的问题，提出了一种名为“笔画式循环放大器（SbCA）”的统一模型。SbCA通过将图像分解为向量图形表示的笔画进行放大，并结合细节补全模块和循环策略，实现了对超大尺度图像的有效上采样，同时避免了模糊、伪影和噪声。实验证明，SbCA在超大尺度上采样任务中表现优异，视觉质量显著超越现有SOTA方法。", "keywords": "图像超分辨率, 任意尺度超分辨率, 笔画表示, 循环放大, 向量图形", "comments": "这篇论文通过引入“笔画”这一新颖的中间表示，并结合向量图形的放大特性，为超大尺度图像超分辨率提供了一个创新的解决方案。循环策略的使用也巧妙地避免了模型在训练范围外性能下降的问题。其能够处理超大倍数（如x100）的上采样，显示了该方法的强大潜力和实际应用价值。"}}
{"id": "2506.10911", "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "authors": ["Jari Kolehmainen", "Nikolay Blagoev", "John Donaghy", "Oğuzhan Ersoy", "Christopher Nies"], "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10911v1", "AI": {"title_translation": "NoLoCo：一种大型模型训练的无全归约低通信方法", "tldr": "NoLoCo是一种新的训练大型模型的方法，它通过部分权重平均和Nesterov动量优化器变体实现隐式同步，无需集体通信，显著减少通信开销并加速收敛。", "motivation": "训练大型语言模型在包含数万个加速器的集群上成本高昂且不切实际，因为高带宽互连的扩展很昂贵，限制了可训练模型的规模。现有低通信训练方法仍需对所有模型副本进行参数同步，这在低带宽网络上成本很高。", "method": "本文提出了一种名为NoLoCo的新型优化方法。它在训练期间不显式同步所有模型参数，因此不需要任何集体通信。NoLoCo通过Nesterov动量优化器的一种新颖变体，通过将模型权重与随机选择的另一个权重进行部分平均来隐式同步模型权重。", "result": "NoLoCo在1.25亿到68亿参数的模型上进行了基准测试。与完全分片数据并行训练或广泛使用的低通信训练方法DiLoCo相比，NoLoCo所需的通信开销显著减少。其同步步骤估计比DiLoCo中使用的全归约操作快一个数量级。NoLoCo也没有任何全局阻塞通信，从而减少了加速器空闲时间。与DiLoCo相比，NoLoCo在各种模型大小和加速器数量下，收敛速度最高可提高4%。", "conclusion": "NoLoCo提供了一种有效且高效的低通信大型模型训练方法，通过避免显式集体通信，显著降低了通信开销，并提高了收敛速度，尤其适用于低带宽网络环境。", "translation": "训练大型语言模型通常通过在包含数万个加速器的集群上进行优化来实现，这些集群通过高带宽互连进行通信。扩展这些集群成本高昂且可能不切实际，从而限制了可训练模型的规模。最近的几项研究提出了通信密集度较低的训练方法，避免了对高度连接计算集群的需求。这些最先进的低通信训练方法仍然采用模型参数的同步步骤，当在所有模型副本上执行时，这在低带宽网络上可能会变得代价高昂。\n在这项工作中，我们提出了一种新颖的优化方法NoLoCo，它在训练期间不显式同步所有模型参数，因此不需要任何集体通信。NoLoCo通过Nesterov动量优化器的一种新颖变体，通过将模型权重与随机选择的另一个权重进行部分平均来隐式同步模型权重。我们为我们提出的优化器提供了理论收敛性分析以及语言模型训练的经验结果。\n我们在1.25亿到68亿参数的广泛加速器数量和模型大小范围内对NoLoCo进行了基准测试。我们的方法比完全分片数据并行训练甚至广泛使用的低通信训练方法DiLoCo所需的通信开销显著减少。对于在互联网上训练数百个加速器而言，同步步骤本身估计比DiLoCo中使用的全归约操作快一个数量级。我们也没有任何全局阻塞通信，这减少了加速器空闲时间。与DiLoCo相比，我们还观察到在各种模型大小和加速器数量下，收敛速度最高可提高4%。", "summary": "本文提出了一种名为NoLoCo的新型大型模型训练优化方法，旨在解决现有方法在低带宽网络上通信成本高昂的问题。NoLoCo通过Nesterov动量优化器的变体，利用部分权重平均实现模型权重的隐式同步，从而完全避免了显式的集体通信（如全归约）。理论分析和实证结果表明，NoLoCo显著降低了通信开销，同步速度比DiLoCo快一个数量级，并能实现高达4%的收敛速度提升，同时减少了加速器空闲时间，使其特别适用于大规模、低带宽环境下的模型训练。", "keywords": "低通信训练, 大型模型, 全归约, Nesterov动量, 分布式训练", "comments": "NoLoCo的创新之处在于其通过Nesterov动量优化器变体实现的隐式权重同步机制，完全避免了传统分布式训练中昂贵的全归约操作。这对于在低带宽网络或大规模集群上训练大型模型具有重要意义，因为它显著降低了通信开销并提高了效率。该方法不仅提供了理论收敛性证明，还通过实验验证了其在通信效率和收敛速度上的优势，有望推动大型模型训练的可扩展性和实用性。"}}
{"id": "2506.10778", "title": "SlotPi: Physics-informed Object-centric Reasoning Models", "authors": ["Jian Li", "Wan Han", "Ning Lin", "Yu-Liang Zhan", "Ruizhi Chengze", "Haining Wang", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Hao Sun"], "summary": "Understanding and reasoning about dynamics governed by physical laws through\nvisual observation, akin to human capabilities in the real world, poses\nsignificant challenges. Currently, object-centric dynamic simulation methods,\nwhich emulate human behavior, have achieved notable progress but overlook two\ncritical aspects: 1) the integration of physical knowledge into models. Humans\ngain physical insights by observing the world and apply this knowledge to\naccurately reason about various dynamic scenarios; 2) the validation of model\nadaptability across diverse scenarios. Real-world dynamics, especially those\ninvolving fluids and objects, demand models that not only capture object\ninteractions but also simulate fluid flow characteristics. To address these\ngaps, we introduce SlotPi, a slot-based physics-informed object-centric\nreasoning model. SlotPi integrates a physical module based on Hamiltonian\nprinciples with a spatio-temporal prediction module for dynamic forecasting.\nOur experiments highlight the model's strengths in tasks such as prediction and\nVisual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,\nwe have created a real-world dataset encompassing object interactions, fluid\ndynamics, and fluid-object interactions, on which we validated our model's\ncapabilities. The model's robust performance across all datasets underscores\nits strong adaptability, laying a foundation for developing more advanced world\nmodels.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10778v1", "AI": {"title_translation": "SlotPi：物理信息目标中心推理模型", "tldr": "SlotPi是一个结合了物理知识和目标中心推理的模型，旨在解决现有动态模拟方法中缺乏物理信息和适应性差的问题，并在预测和VQA任务上表现出色。", "motivation": "现有目标中心动态模拟方法在理解和推理物理定律驱动的动力学方面存在两个关键不足：1) 缺乏将物理知识整合到模型中；2) 未能验证模型在多样化场景（特别是涉及流体和物体的场景）中的适应性。", "method": "我们引入了SlotPi，一个基于槽的物理信息目标中心推理模型。SlotPi将一个基于哈密顿原理的物理模块与一个用于动态预测的时空预测模块相结合。", "result": "实验证明SlotPi在基准数据集和流体数据集上的预测和视觉问答（VQA）任务中表现出色。此外，该模型在我们创建的包含物体交互、流体动力学和流体-物体交互的真实世界数据集上验证了其能力，并展示了强大的适应性。", "conclusion": "SlotPi在所有数据集上的稳健性能凸显了其强大的适应性，为开发更先进的世界模型奠定了基础。", "translation": "通过视觉观察理解和推理受物理定律支配的动力学，就像人类在现实世界中的能力一样，带来了巨大的挑战。当前，模仿人类行为的以物体为中心的动态模拟方法已经取得了显著进展，但却忽略了两个关键方面：1）将物理知识整合到模型中。人类通过观察世界获得物理见解，并将这些知识应用于准确推理各种动态场景；2）模型在不同场景下的适应性验证。现实世界的动力学，特别是涉及流体和物体的动力学，需要模型不仅能捕捉物体间的相互作用，还能模拟流体流动特性。为了弥补这些空白，我们引入了SlotPi，一个基于槽的物理信息目标中心推理模型。SlotPi将一个基于哈密顿原理的物理模块与一个用于动态预测的时空预测模块相结合。我们的实验突出了该模型在基准数据集和流体数据集上的预测和视觉问答（VQA）等任务中的优势。此外，我们创建了一个包含物体交互、流体动力学和流体-物体交互的真实世界数据集，并在该数据集上验证了我们模型的能力。该模型在所有数据集上的稳健性能强调了其强大的适应性，为开发更先进的世界模型奠定了基础。", "summary": "本文提出了SlotPi，一个结合了哈密顿原理物理模块和时空预测模块的槽基物理信息目标中心推理模型，旨在解决现有动态模拟方法中缺乏物理知识整合和场景适应性验证的问题。实验表明，SlotPi在预测和VQA任务上，包括流体动态场景，均表现出强大的性能和适应性，为未来世界模型的发展奠定了基础。", "keywords": "物理信息, 目标中心推理, 动力学模拟, 流体动力学, 世界模型", "comments": "SlotPi的创新之处在于其将物理知识（基于哈密顿原理）与目标中心推理相结合，解决了现有模型在物理理解和跨场景适应性方面的不足。其在流体和物体交互等复杂真实世界场景中的验证，显示了其在构建更通用、更鲁棒的世界模型方面的潜力。"}}
{"id": "2506.10914", "title": "Foundation Models for Causal Inference via Prior-Data Fitted Networks", "authors": ["Yuchen Ma", "Dennis Frauen", "Emil Javurek", "Stefan Feuerriegel"], "summary": "Prior-data fitted networks (PFNs) have recently been proposed as a promising\nway to train tabular foundation models. PFNs are transformers that are\npre-trained on synthetic data generated from a prespecified prior distribution\nand that enable Bayesian inference through in-context learning. In this paper,\nwe introduce CausalFM, a comprehensive framework for training PFN-based\nfoundation models in various causal inference settings. First, we formalize the\nconstruction of Bayesian priors for causal inference based on structural causal\nmodels (SCMs) in a principled way and derive necessary criteria for the\nvalidity of such priors. Building on this, we propose a novel family of prior\ndistributions using causality-inspired Bayesian neural networks that enable\nCausalFM to perform Bayesian causal inference in various settings, including\nback-door, front-door, and instrumental variable adjustment. Finally, we\ninstantiate CausalFM and explicitly train a foundation model for estimating\nconditional average treatment effects (CATEs) using back-door adjustment. We\nshow that CausalFM performs competitively for CATE estimation using various\nsynthetic and semi-synthetic benchmarks. In sum, our framework can be used as a\ngeneral recipe to train foundation models for various causal inference\nsettings. In contrast to the current state-of-the-art in causal inference,\nCausalFM offers a novel paradigm with the potential to fundamentally change how\npractitioners perform causal inference in medicine, economics, and other\ndisciplines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10914v1", "AI": {"title_translation": "通过先验数据拟合网络进行因果推断的基础模型", "tldr": "CausalFM是一个基于先验数据拟合网络（PFNs）的框架，通过上下文学习和贝叶斯先验，在各种因果推断设置中训练基础模型，并在条件平均治疗效果（CATEs）估计上表现出色。", "motivation": "先验数据拟合网络（PFNs）被认为是训练表格基础模型的有前景的方法。本文旨在将PFNs应用于各种因果推断设置，以提供一种新的范式，改变实践者进行因果推断的方式，解决现有方法可能存在的局限性。", "method": "本文引入了CausalFM，一个用于训练基于PFN的基础模型的综合框架。首先，它原则性地形式化了基于结构因果模型（SCMs）的因果推断贝叶斯先验的构建，并推导了其有效性标准。在此基础上，提出了一种新的先验分布族，使用受因果关系启发的贝叶斯神经网络，使CausalFM能够在包括后门、前门和工具变量调整在内的多种设置中执行贝叶斯因果推断。最后，实例化并训练了一个使用后门调整估计条件平均治疗效果（CATEs）的基础模型。", "result": "CausalFM在各种合成和半合成基准测试中，在条件平均治疗效果（CATEs）估计方面表现出竞争力。", "conclusion": "CausalFM框架可以作为训练各种因果推断设置的基础模型的通用方法。它提供了一种新颖的范式，有可能从根本上改变实践者在医学、经济学和其他学科中进行因果推断的方式。", "translation": "先验数据拟合网络（PFNs）最近被提出作为训练表格基础模型的一种有前景的方法。PFNs是基于预先指定先验分布生成的合成数据进行预训练的Transformer，并通过上下文学习实现贝叶斯推断。在本文中，我们引入了CausalFM，这是一个用于在各种因果推断设置中训练基于PFN的基础模型的综合框架。首先，我们以一种原则性的方式形式化了基于结构因果模型（SCMs）的因果推断贝叶斯先验的构建，并推导了此类先验有效性的必要标准。在此基础上，我们提出了一种新的先验分布族，该族使用受因果关系启发的贝叶斯神经网络，使CausalFM能够在各种设置中执行贝叶斯因果推断，包括后门、前门和工具变量调整。最后，我们实例化了CausalFM，并明确训练了一个使用后门调整估计条件平均治疗效果（CATEs）的基础模型。我们表明，CausalFM在使用各种合成和半合成基准测试进行CATE估计时表现出竞争力。总而言之，我们的框架可以作为训练各种因果推断设置的基础模型的通用方法。与当前最先进的因果推断方法相比，CausalFM提供了一种新颖的范式，有可能从根本上改变实践者在医学、经济学和其他学科中进行因果推断的方式。", "summary": "本文介绍了CausalFM，一个基于先验数据拟合网络（PFNs）的框架，用于在多种因果推断场景中训练基础模型。CausalFM通过原则性地构建基于结构因果模型（SCMs）的贝叶斯先验，并利用受因果关系启发的贝叶斯神经网络，实现了上下文学习下的贝叶斯因果推断。实验证明，CausalFM在条件平均治疗效果（CATEs）估计上表现出色，为因果推断提供了一种有潜力的新范式。", "keywords": "因果推断, 基础模型, 先验数据拟合网络, 贝叶斯推断, 结构因果模型", "comments": "CausalFM的创新点在于将先验数据拟合网络（PFNs）应用于因果推断领域，并将其与结构因果模型（SCMs）和贝叶斯神经网络相结合，构建了一个全面的基础模型训练框架。这种方法通过上下文学习实现贝叶斯因果推断，为处理不同因果推断设置（如后门、前门、工具变量调整）提供了一种统一且灵活的解决方案。该工作有望简化并改进医学、经济学等领域中因果推断的实践方式，具有重要的应用潜力。"}}
{"id": "2506.10790", "title": "Human-Robot Navigation using Event-based Cameras and Reinforcement Learning", "authors": ["Ignacio Bugueno-Cordova", "Javier Ruiz-del-Solar", "Rodrigo Verschae"], "summary": "This work introduces a robot navigation controller that combines event\ncameras and other sensors with reinforcement learning to enable real-time\nhuman-centered navigation and obstacle avoidance. Unlike conventional\nimage-based controllers, which operate at fixed rates and suffer from motion\nblur and latency, this approach leverages the asynchronous nature of event\ncameras to process visual information over flexible time intervals, enabling\nadaptive inference and control. The framework integrates event-based\nperception, additional range sensing, and policy optimization via Deep\nDeterministic Policy Gradient, with an initial imitation learning phase to\nimprove sample efficiency. Promising results are achieved in simulated\nenvironments, demonstrating robust navigation, pedestrian following, and\nobstacle avoidance. A demo video is available at the project website.", "comment": "https://ibugueno.github.io/hr-navigation-using-event-cameras-and-rl/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10790v1", "AI": {"title_translation": "基于事件相机和强化学习的人机导航", "tldr": "本文提出一种结合事件相机和强化学习的机器人导航控制器，克服了传统视觉方法的局限，实现了实时人机导航和避障。", "motivation": "传统基于图像的机器人导航控制器存在固定帧率、运动模糊和高延迟等问题，难以满足实时人机导航的需求。", "method": "该方法结合事件相机和其他传感器与强化学习，利用事件相机的异步特性，灵活处理视觉信息以实现自适应推理和控制。框架整合了基于事件的感知、额外的距离传感和通过深度确定性策略梯度（DDPG）进行的策略优化，并辅以初始模仿学习阶段以提高样本效率。", "result": "在模拟环境中取得了有前景的结果，成功展示了鲁棒的导航、行人跟随和障碍物避障能力。", "conclusion": "该研究提出的结合事件相机和强化学习的导航方法在模拟环境中表现出色，证明了其在实现实时、以人为中心的机器人导航和避障方面的有效性和潜力。", "translation": "这项工作介绍了一种机器人导航控制器，它将事件相机和其他传感器与强化学习相结合，以实现实时以人为中心的导航和避障。与在固定速率下运行并受运动模糊和延迟影响的传统基于图像的控制器不同，这种方法利用事件相机的异步特性，在灵活的时间间隔内处理视觉信息，从而实现自适应推理和控制。该框架整合了基于事件的感知、额外的距离传感以及通过深度确定性策略梯度进行的策略优化，并辅以初始模仿学习阶段以提高样本效率。在模拟环境中取得了有前景的结果，展示了鲁棒的导航、行人跟随和障碍物避障能力。项目网站上提供了演示视频。", "summary": "本文提出一种基于事件相机和强化学习的机器人导航控制器，旨在实现实时、以人为中心的导航和避障。该方法通过利用事件相机的异步特性，克服了传统图像传感器在运动模糊和延迟方面的限制，能够进行自适应视觉信息处理。框架集成了事件感知、距离传感和深度确定性策略梯度（DDPG）优化，并辅以模仿学习以提高效率。在模拟环境中的实验结果验证了其在鲁棒导航、行人跟随和避障方面的有效性。", "keywords": "人机导航, 事件相机, 强化学习, 避障, DDPG", "comments": "这项工作创新性地将事件相机与强化学习相结合应用于机器人导航，有效解决了传统视觉传感器在快速运动和低延迟场景下的局限，为实时、鲁棒的人机交互导航提供了新的解决方案。其利用事件相机异步特性的方法尤其具有前景，能够更好地适应动态环境。"}}
{"id": "2506.10918", "title": "Sequential-Parallel Duality in Prefix Scannable Models", "authors": ["Morris Yau", "Sharut Gupta", "Valerie Engelmayer", "Kazuki Irie", "Stefanie Jegelka", "Jacob Andreas"], "summary": "Modern neural sequence models are designed to meet the dual mandate of\nparallelizable training and fast sequential inference. Recent developments have\ngiven rise to various models, such as Gated Linear Attention (GLA) and Mamba,\nthat achieve such ``sequential-parallel duality.'' This raises a natural\nquestion: can we characterize the full class of neural sequence models that\nsupport near-constant-time parallel evaluation and linear-time, constant-space\nsequential inference? We begin by describing a broad class of such models --\nstate space models -- as those whose state updates can be computed using the\nclassic parallel prefix scan algorithm with a custom associative aggregation\noperator. We then define a more general class, Prefix-Scannable Models (PSMs),\nby relaxing the state aggregation operator to allow arbitrary (potentially\nnon-associative) functions such as softmax attention. This generalization\nunifies many existing architectures, including element-wise RNNs (e.g., Mamba)\nand linear transformers (e.g., GLA, Mamba2, mLSTM), while also introducing new\nmodels with softmax-like operators that achieve O(1) amortized compute per\ntoken and log(N) memory for sequence length N. We empirically evaluate such\nmodels on illustrative small-scale language modeling and canonical synthetic\ntasks, including state tracking and associative recall. Empirically, we find\nthat PSMs retain the expressivity of transformer-based architectures while\nmatching the inference efficiency of state space models -- in some cases\nexhibiting better length generalization than either.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10918v1", "AI": {"title_translation": "前缀可扫描模型中的序列-并行对偶性", "tldr": "本文定义并研究了一类新的神经网络序列模型——前缀可扫描模型（PSMs），这些模型能够在保持并行训练效率的同时，实现快速的序列推理，并统一了多种现有架构。", "motivation": "现有的神经网络序列模型如Gated Linear Attention (GLA)和Mamba已经实现了并行训练和快速序列推理的“序列-并行对偶性”。本文的动机是尝试刻画出所有支持近乎常数时间并行评估和线性时间、常数空间序列推理的神经网络序列模型的完整类别。", "method": "作者首先将状态空间模型描述为一类可以通过经典并行前缀扫描算法和自定义结合聚合操作来计算状态更新的模型。然后，他们定义了一个更通用的类别——前缀可扫描模型（PSMs），通过放宽状态聚合操作，允许任意（可能非结合的）函数，例如softmax注意力。", "result": "这种泛化统一了许多现有的架构，包括逐元素RNNs（如Mamba）和线性Transformer（如GLA, Mamba2, mLSTM）。同时，它还引入了具有类似softmax操作的新模型，这些模型在每个token上实现了O(1)的摊销计算和序列长度N的log(N)内存。经验评估表明，PSMs保留了基于Transformer架构的表达能力，同时匹配了状态空间模型的推理效率，在某些情况下甚至表现出比两者都更好的长度泛化能力。", "conclusion": "前缀可扫描模型（PSMs）成功地刻画并统一了具有序列-并行对偶性的神经网络序列模型。它们在保持Transformer的表达能力的同时，实现了状态空间模型的推理效率，并在某些情况下展现出更优越的长度泛化能力。", "translation": "现代神经网络序列模型旨在满足并行化训练和快速序列推理的双重需求。最近的发展催生了各种模型，如门控线性注意力（GLA）和Mamba，它们实现了这种“序列-并行对偶性”。这提出了一个自然的问题：我们能否刻画出所有支持近乎常数时间并行评估和线性时间、常数空间序列推理的神经网络序列模型的完整类别？我们首先将一类广泛的模型——状态空间模型——描述为那些其状态更新可以使用经典并行前缀扫描算法和自定义结合聚合操作进行计算的模型。然后，我们通过放宽状态聚合操作，允许任意（可能非结合的）函数，例如softmax注意力，来定义一个更通用的类别——前缀可扫描模型（PSMs）。这种泛化统一了许多现有的架构，包括逐元素RNNs（例如Mamba）和线性Transformer（例如GLA, Mamba2, mLSTM），同时还引入了具有类似softmax操作的新模型，这些模型在每个token上实现了O(1)的摊销计算和序列长度N的log(N)内存。我们对这些模型在说明性的小规模语言建模和规范的合成任务（包括状态跟踪和关联回忆）上进行了经验评估。经验上，我们发现PSMs保留了基于Transformer架构的表达能力，同时匹配了状态空间模型的推理效率——在某些情况下表现出比两者都更好的长度泛化能力。", "summary": "本文引入并详细阐述了前缀可扫描模型（PSMs）这一新的神经网络序列模型类别，旨在解决并行训练与高效序列推理的双重需求。PSMs通过放宽状态聚合操作，将经典的状态空间模型泛化，使其能包含非结合函数，从而统一了包括Mamba和GLA在内的多种现有高效架构。实验结果表明，PSMs在保持Transformer模型表达能力的同时，实现了与状态空间模型相当的推理效率，并在某些场景下展现出更优的长度泛化能力。", "keywords": "序列-并行对偶性, 前缀可扫描模型, 神经网络序列模型, 状态空间模型, Transformer", "comments": "本文创新性地提出了前缀可扫描模型（PSMs）的概念，为理解和设计具有序列-并行对偶性的神经网络序列模型提供了一个统一的理论框架。通过泛化前缀扫描操作，它不仅解释了现有高效模型的工作原理，还为开发新型高效模型指明了方向。其在效率和表达能力上的实证优势，以及在长度泛化上的潜力，都凸显了该研究的重要性。"}}
{"id": "2506.10807", "title": "Prompts to Summaries: Zero-Shot Language-Guided Video Summarization", "authors": ["Mario Barbara", "Alaa Maalouf"], "summary": "The explosive growth of video data intensified the need for flexible\nuser-controllable summarization tools that can operate without domain-specific\ntraining data. Existing methods either rely on datasets, limiting\ngeneralization, or cannot incorporate user intent expressed in natural\nlanguage. We introduce Prompts-to-Summaries: the first zero-shot,\ntext-queryable video summarizer that converts off-the-shelf video-language\nmodels (VidLMs) captions into user-guided skims via large language models\n(LLMs) judging, without the use of training data at all, beating all\nunsupervised and matching supervised methods. Our pipeline (i) segments raw\nvideo footage into coherent scenes, (ii) generates rich scene-level\ndescriptions through a memory-efficient, batch-style VidLM prompting scheme\nthat scales to hours-long videos on a single GPU, (iii) leverages an LLM as a\njudge to assign scene-level importance scores under a carefully crafted prompt,\nand finally, (iv) propagates those scores to short segments level via two new\nmetrics: consistency (temporal coherency) and uniqueness (novelty), yielding\nfine-grained frame importance. On SumMe and TVSum, our data-free approach\nsurpasses all prior data-hungry unsupervised methods. It also performs\ncompetitively on the Query-Focused Video Summarization (QFVS) benchmark,\ndespite using no training data and the competing methods requiring supervised\nframe-level importance. To spur further research, we release VidSum-Reason, a\nnew query-driven dataset featuring long-tailed concepts and multi-step\nreasoning; our framework attains robust F1 scores and serves as the first\nchallenging baseline. Overall, our results demonstrate that pretrained\nmultimodal models, when orchestrated with principled prompting and score\npropagation, already provide a powerful foundation for universal,\ntext-queryable video summarization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10807v1", "AI": {"title_translation": "提示到摘要：零样本语言引导视频摘要", "tldr": "Prompts-to-Summaries是首个零样本、文本可查询的视频摘要器，它利用现成的视频-语言模型和大型语言模型进行判断，无需训练数据，在无监督方法中表现最佳，并与有监督方法持平。", "motivation": "视频数据爆炸式增长，需要灵活、用户可控且无需特定领域训练数据的摘要工具。现有方法依赖数据集或无法整合用户意图。", "method": "本研究提出了Prompts-to-Summaries，一个零样本、文本可查询的视频摘要器。其流程包括：(i) 将原始视频分割成连贯场景；(ii) 通过内存高效的批处理VidLM提示方案生成丰富的场景级描述；(iii) 利用LLM作为判断器，在精心设计的提示下分配场景级重要性分数；(iv) 通过一致性（时间连贯性）和独特性（新颖性）两个新指标将分数传播到短片段级别，从而获得细粒度帧重要性。", "result": "Prompts-to-Summaries在SumMe和TVSum数据集上超越了所有先前依赖数据的无监督方法，并且在Query-Focused Video Summarization (QFVS) 基准测试中表现出竞争力，尽管未使用训练数据而竞争方法需要有监督的帧级重要性。此外，还发布了新的查询驱动数据集VidSum-Reason，其框架在此数据集上取得了稳健的F1分数并作为首个具有挑战性的基线。", "conclusion": "预训练多模态模型，通过精心设计的提示和分数传播，为通用、文本可查询的视频摘要提供了强大的基础。", "translation": "视频数据的爆炸式增长加剧了对灵活、用户可控的摘要工具的需求，这些工具应能在没有特定领域训练数据的情况下运行。现有方法要么依赖数据集，限制了泛化能力，要么无法整合以自然语言表达的用户意图。我们引入了Prompts-to-Summaries：这是第一个零样本、文本可查询的视频摘要器，它将现成的视频-语言模型（VidLMs）字幕通过大型语言模型（LLMs）判断转换为用户引导的摘要，完全无需使用训练数据，其性能超越了所有无监督方法并与有监督方法相匹配。我们的管道 (i) 将原始视频片段分割成连贯的场景，(ii) 通过内存高效的批处理式VidLM提示方案生成丰富的场景级描述，该方案可在单个GPU上处理数小时长的视频，(iii) 利用LLM作为判断器，在精心设计的提示下分配场景级重要性分数，最后，(iv) 通过两个新指标：一致性（时间连贯性）和独特性（新颖性），将这些分数传播到短片段级别，从而产生细粒度的帧重要性。在SumMe和TVSum数据集上，我们这种无需数据的方法超越了所有先前的依赖数据的无监督方法。它在查询焦点视频摘要（QFVS）基准测试中也表现出竞争力，尽管未使用训练数据且竞争方法需要有监督的帧级重要性。为了促进进一步研究，我们发布了VidSum-Reason，这是一个新的查询驱动数据集，具有长尾概念和多步推理；我们的框架取得了稳健的F1分数，并作为第一个具有挑战性的基线。总的来说，我们的结果表明，预训练的多模态模型，当与原则性的提示和分数传播相结合时，已经为通用、文本可查询的视频摘要提供了强大的基础。", "summary": "本论文提出了Prompts-to-Summaries，一个创新的零样本、文本可查询视频摘要框架。该方法无需训练数据，通过将现成的视频-语言模型生成的字幕与大型语言模型的判断相结合，实现用户引导的视频摘要。其核心流程包括视频场景分割、VidLM生成场景描述、LLM进行场景重要性评分，以及通过一致性和独特性指标将分数传播到细粒度片段。实验结果表明，该方法在多个基准测试中超越了无监督方法，并与有监督方法表现相当。此外，本研究还发布了新的查询驱动数据集VidSum-Reason，以推动相关研究。", "keywords": "零样本视频摘要, 语言引导, 视频-语言模型, 大型语言模型, 无监督学习", "comments": "这项工作具有显著的创新性，它首次实现了零样本、文本可查询的视频摘要，完全摆脱了对训练数据的依赖。其核心在于巧妙地结合了现有的视频-语言模型和大型语言模型的能力，通过精心设计的提示和分数传播机制，实现了无需监督数据的卓越性能。这种方法大大提高了视频摘要工具的泛化性和灵活性，对于资源有限或需要快速适应新领域的应用具有重要意义。新数据集的发布也为未来的研究提供了宝贵的资源。"}}
{"id": "2506.10922", "title": "Robustly Improving LLM Fairness in Realistic Settings via Interpretability", "authors": ["Adam Karvonen", "Samuel Marks"], "summary": "Large language models (LLMs) are increasingly deployed in high-stakes hiring\napplications, making decisions that directly impact people's careers and\nlivelihoods. While prior studies suggest simple anti-bias prompts can eliminate\ndemographic biases in controlled evaluations, we find these mitigations fail\nwhen realistic contextual details are introduced. We address these failures\nthrough internal bias mitigation: by identifying and neutralizing sensitive\nattribute directions within model activations, we achieve robust bias reduction\nacross all tested scenarios. Across leading commercial (GPT-4o, Claude 4\nSonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,\nMistral-24B), we find that adding realistic context such as company names,\nculture descriptions from public careers pages, and selective hiring\nconstraints (e.g.,``only accept candidates in the top 10\\%\") induces\nsignificant racial and gender biases (up to 12\\% differences in interview\nrates). When these biases emerge, they consistently favor Black over White\ncandidates and female over male candidates across all tested models and\nscenarios. Moreover, models can infer demographics and become biased from\nsubtle cues like college affiliations, with these biases remaining invisible\neven when inspecting the model's chain-of-thought reasoning. To address these\nlimitations, our internal bias mitigation identifies race and gender-correlated\ndirections and applies affine concept editing at inference time. Despite using\ndirections from a simple synthetic dataset, the intervention generalizes\nrobustly, consistently reducing bias to very low levels (typically under 1\\%,\nalways below 2.5\\%) while largely maintaining model performance. Our findings\nsuggest that practitioners deploying LLMs for hiring should adopt more\nrealistic evaluation methodologies and consider internal mitigation strategies\nfor equitable outcomes.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10922v1", "AI": {"title_translation": "通过可解释性在现实场景中稳健地提升大型语言模型（LLM）的公平性", "tldr": "大型语言模型（LLM）在现实招聘场景中存在偏见，简单的反偏见提示无效，但基于可解释性的内部偏见缓解方法能稳健地减少偏见。", "motivation": "大型语言模型（LLM）越来越多地被部署到高风险的招聘应用中，直接影响人们的职业和生计。然而，先前的简单反偏见提示在现实情境下无法消除人口统计学偏见。", "method": "本文通过内部偏见缓解来解决这些问题：通过识别并中和模型激活中敏感属性的方向，从而在所有测试场景中实现稳健的偏见减少。具体而言，该方法识别与种族和性别相关的方向，并在推理时应用仿射概念编辑，即使使用来自简单合成数据集的方向，干预也具有很强的泛化性。", "result": "研究发现，在领先的商业模型（GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash）和开源模型（Gemma-2 27B, Gemma-3, Mistral-24B）中，添加现实上下文（如公司名称、文化描述、选择性招聘限制）会引起显著的种族和性别偏见（面试率差异高达12%）。当这些偏见出现时，在所有测试模型和场景中，它们始终偏向黑人而非白人候选人，以及女性而非男性候选人。此外，模型可以从大学隶属关系等细微线索推断人口统计学信息并产生偏见，即使检查模型的思维链推理也无法发现这些偏见。本文提出的内部偏见缓解方法能将偏见稳健地降低到非常低的水平（通常低于1%，始终低于2.5%），同时基本保持模型性能。", "conclusion": "研究结果表明，部署LLM进行招聘的从业者应采用更现实的评估方法，并考虑采用内部缓解策略以实现公平结果。", "translation": "大型语言模型（LLM）越来越多地被部署到高风险的招聘应用中，其决策直接影响人们的职业和生计。虽然先前的研究表明简单的反偏见提示可以在受控评估中消除人口统计学偏见，但我们发现当引入现实情境细节时，这些缓解措施会失效。我们通过内部偏见缓解来解决这些失败：通过识别并中和模型激活中敏感属性的方向，我们在所有测试场景中实现了稳健的偏见减少。在领先的商业模型（GPT-4o、Claude 4 Sonnet、Gemini 2.5 Flash）和开源模型（Gemma-2 27B、Gemma-3、Mistral-24B）中，我们发现添加公司名称、来自公共招聘页面的文化描述以及选择性招聘限制（例如“只接受前10%的候选人”）等现实上下文会引发显著的种族和性别偏见（面试率差异高达12%）。当这些偏见出现时，在所有测试模型和场景中，它们始终偏向黑人而非白人候选人，以及女性而非男性候选人。此外，模型可以从大学隶属关系等细微线索推断人口统计学信息并产生偏见，即使检查模型的思维链推理也无法发现这些偏见。为了解决这些限制，我们的内部偏见缓解方法识别与种族和性别相关的方向，并在推理时应用仿射概念编辑。尽管使用来自简单合成数据集的方向，该干预措施仍具有很强的泛化性，能够稳健地将偏见降低到非常低的水平（通常低于1%，始终低于2.5%），同时基本保持模型性能。我们的发现表明，部署LLM进行招聘的从业者应采用更现实的评估方法，并考虑采用内部缓解策略以实现公平结果。", "summary": "该研究探讨了大型语言模型（LLM）在招聘应用中面临的偏见问题。现有简单的反偏见提示在引入现实上下文时会失效，导致显著的种族和性别偏见。为解决此问题，研究提出了一种基于可解释性的内部偏见缓解方法，通过识别并中和模型激活中的敏感属性方向。实验证明，该方法能有效且稳健地将偏见降低到极低水平，同时保持模型性能。研究强调了在LLM招聘应用中采用现实评估方法和内部偏见缓解策略的重要性。", "keywords": "LLM公平性, 偏见缓解, 可解释性, 招聘, 现实场景", "comments": "本文的创新点在于提出了基于可解释性的“内部偏见缓解”方法，通过直接操作模型激活中的敏感属性方向来消除偏见，这比简单的提示工程更为鲁棒。其重要性体现在解决了LLM在现实高风险招聘场景中的公平性问题，为实际部署提供了更可靠的偏见缓解策略。"}}
{"id": "2506.10930", "title": "Developing a High-performance Framework for Speech Emotion Recognition in Naturalistic Conditions Challenge for Emotional Attribute Prediction", "authors": ["Thanathai Lertpetchpun", "Tiantian Feng", "Dani Byrd", "Shrikanth Narayanan"], "summary": "Speech emotion recognition (SER) in naturalistic conditions presents a\nsignificant challenge for the speech processing community. Challenges include\ndisagreement in labeling among annotators and imbalanced data distributions.\nThis paper presents a reproducible framework that achieves superior (top 1)\nperformance in the Emotion Recognition in Naturalistic Conditions Challenge\n(IS25-SER Challenge) - Task 2, evaluated on the MSP-Podcast dataset. Our system\nis designed to tackle the aforementioned challenges through multimodal\nlearning, multi-task learning, and imbalanced data handling. Specifically, our\nbest system is trained by adding text embeddings, predicting gender, and\nincluding ``Other'' (O) and ``No Agreement'' (X) samples in the training set.\nOur system's results secured both first and second places in the IS25-SER\nChallenge, and the top performance was achieved by a simple two-system\nensemble.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10930v1", "AI": {"title_translation": "在自然条件下开发用于语音情感识别的高性能框架：情感属性预测的挑战", "tldr": "该论文提出了一种在自然条件下进行语音情感识别的框架，通过多模态学习、多任务学习和不平衡数据处理，在IS25-SER挑战赛中取得了顶尖性能。", "motivation": "在自然条件下进行语音情感识别（SER）对语音处理领域提出了重大挑战，包括标注者之间的标注分歧和数据分布不平衡问题。", "method": "本论文提出了一个可复现的框架，通过多模态学习、多任务学习和不平衡数据处理来解决上述挑战。具体而言，其最佳系统通过添加文本嵌入、预测性别以及在训练集中包含“其他”（O）和“无共识”（X）样本进行训练。顶尖性能是通过一个简单的双系统集成实现的。", "result": "该系统在IS25-SER挑战赛（任务2）中，在MSP-Podcast数据集上取得了第一和第二名的成绩，其中最佳性能由一个简单的双系统集成实现。", "conclusion": "该论文提出的框架通过有效处理自然语音情感识别中的标注分歧和数据不平衡问题，成功地在挑战赛中取得了卓越的性能。", "translation": "在自然条件下进行语音情感识别（SER）对语音处理领域提出了重大挑战。挑战包括标注者之间的分歧和不平衡的数据分布。本文提出了一个可复现的框架，该框架在自然条件下的情感识别挑战赛（IS25-SER挑战赛）——任务2中取得了卓越（第一名）的性能，并在MSP-Podcast数据集上进行了评估。我们的系统旨在通过多模态学习、多任务学习和不平衡数据处理来解决上述挑战。具体而言，我们最好的系统通过添加文本嵌入、预测性别以及在训练集中包含“其他”（O）和“无共识”（X）样本进行训练。我们的系统结果在IS25-SER挑战赛中获得了第一名和第二名，并且最佳性能是通过一个简单的双系统集成实现的。", "summary": "该论文提出了一个在自然条件下进行语音情感识别（SER）的高性能可复现框架。该框架通过多模态学习、多任务学习和不平衡数据处理来应对标注者分歧和数据不平衡等挑战。其最佳系统通过整合文本嵌入、性别预测以及包含“其他”和“无共识”样本进行训练。该系统在IS25-SER挑战赛中取得了第一和第二名的优异成绩，其中最佳性能由简单的双系统集成实现。", "keywords": "语音情感识别, 自然条件, 多模态学习, 多任务学习, 不平衡数据", "comments": "该论文的创新之处在于其提出的框架有效地解决了自然语音情感识别中存在的标注者分歧和数据不平衡等实际挑战。通过结合多模态和多任务学习，以及在训练中纳入特殊样本（“O”和“X”），显著提升了系统的鲁棒性和性能。在竞争激烈的挑战赛中获得顶尖名次，证明了其方法的有效性和实用性。简单的双系统集成实现最佳性能也体现了方法的效率。"}}
{"id": "2506.10816", "title": "Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders", "authors": ["Hui Yang", "Wei Sun", "Jian Liu", "Jin Zheng", "Jian Xiao", "Ajmal Mian"], "summary": "Hand-object pose estimation from monocular RGB images remains a significant\nchallenge mainly due to the severe occlusions inherent in hand-object\ninteractions. Existing methods do not sufficiently explore global structural\nperception and reasoning, which limits their effectiveness in handling occluded\nhand-object interactions. To address this challenge, we propose an\nocclusion-aware hand-object pose estimation method based on masked\nautoencoders, termed as HOMAE. Specifically, we propose a target-focused\nmasking strategy that imposes structured occlusion on regions of hand-object\ninteraction, encouraging the model to learn context-aware features and reason\nabout the occluded structures. We further integrate multi-scale features\nextracted from the decoder to predict a signed distance field (SDF), capturing\nboth global context and fine-grained geometry. To enhance geometric perception,\nwe combine the implicit SDF with an explicit point cloud derived from the SDF,\nleveraging the complementary strengths of both representations. This fusion\nenables more robust handling of occluded regions by combining the global\ncontext from the SDF with the precise local geometry provided by the point\ncloud. Extensive experiments on challenging DexYCB and HO3Dv2 benchmarks\ndemonstrate that HOMAE achieves state-of-the-art performance in hand-object\npose estimation. We will release our code and model.", "comment": "10 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10816v1", "AI": {"title_translation": "基于掩码自编码器的遮挡感知三维手物姿态估计", "tldr": "HOMAE是一种基于掩码自编码器的遮挡感知三维手物姿态估计算法，通过目标聚焦掩码策略、多尺度SDF预测以及SDF与点云融合来解决手物交互中的严重遮挡问题，并在DexYCB和HO3Dv2基准上达到了最先进的性能。", "motivation": "从单目RGB图像进行手物姿态估计面临严峻挑战，主要原因是手物交互中固有的严重遮挡。现有方法未能充分探索全局结构感知和推理，这限制了它们在处理遮挡手物交互时的有效性。", "method": "本文提出了一种基于掩码自编码器的遮挡感知手物姿态估计算法HOMAE。它采用目标聚焦掩码策略，对关键交互区域施加结构化遮挡，以学习上下文感知特征并推理遮挡结构。该方法进一步整合解码器提取的多尺度特征来预测符号距离场（SDF），以捕获全局上下文和精细几何。为了增强几何感知，它将隐式SDF与从SDF导出的显式点云结合，利用两者的互补优势。", "result": "在具有挑战性的DexYCB和HO3Dv2基准上的广泛实验表明，HOMAE在手物姿态估计方面取得了最先进的性能。", "conclusion": "HOMAE通过其新颖的遮挡感知策略和多模态特征融合，有效解决了手物交互中的遮挡问题，并在3D手物姿态估计领域取得了显著进展。", "translation": "从单目RGB图像进行手物姿态估计仍然是一个重大挑战，这主要由于手物交互中固有的严重遮挡。现有方法未能充分探索全局结构感知和推理，这限制了它们在处理遮挡手物交互时的有效性。为了解决这一挑战，我们提出了一种基于掩码自编码器的遮挡感知手物姿态估计算法，命名为HOMAE。具体来说，我们提出了一种目标聚焦掩码策略，对手物交互区域施加结构化遮挡，鼓励模型学习上下文感知特征并推理遮挡结构。我们进一步整合从解码器提取的多尺度特征来预测符号距离场（SDF），捕获全局上下文和精细几何。为了增强几何感知，我们将隐式SDF与从SDF导出的显式点云结合，利用这两种表示的互补优势。这种融合通过结合SDF的全局上下文和点云提供的精确局部几何，能够更鲁棒地处理遮挡区域。在具有挑战性的DexYCB和HO3Dv2基准上的广泛实验表明，HOMAE在手物姿态估计方面取得了最先进的性能。我们将发布我们的代码和模型。", "summary": "本文提出HOMAE，一种基于掩码自编码器的遮挡感知3D手物姿态估计算法，旨在解决手物交互中严重的遮挡问题。该方法引入了目标聚焦掩码策略，促使模型学习上下文感知特征并推理遮挡结构。通过整合解码器输出的多尺度特征来预测符号距离场（SDF），并将其与显式点云融合，HOMAE有效结合了全局上下文和精细几何信息，从而增强了对遮挡区域的处理能力。实验结果表明，HOMAE在DexYCB和HO3Dv2基准上达到了最先进的性能。", "keywords": "手物姿态估计, 遮挡感知, 掩码自编码器, 符号距离场, 点云", "comments": "该论文通过引入目标聚焦掩码策略和SDF与点云的融合，为解决手物交互中的严重遮挡问题提供了创新思路。其对全局结构感知和精细几何的结合，以及在挑战性基准上的SOTA表现，显示了其在3D手物姿态估计领域的巨大潜力。"}}
{"id": "2506.10943", "title": "Self-Adapting Language Models", "authors": ["Adam Zweiger", "Jyothish Pari", "Han Guo", "Ekin Akyürek", "Yoon Kim", "Pulkit Agrawal"], "summary": "Large language models (LLMs) are powerful but static; they lack mechanisms to\nadapt their weights in response to new tasks, knowledge, or examples. We\nintroduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to\nself-adapt by generating their own finetuning data and update directives. Given\na new input, the model produces a self-edit-a generation that may restructure\nthe information in different ways, specify optimization hyperparameters, or\ninvoke tools for data augmentation and gradient-based updates. Through\nsupervised finetuning (SFT), these self-edits result in persistent weight\nupdates, enabling lasting adaptation. To train the model to produce effective\nself-edits, we use a reinforcement learning loop with the downstream\nperformance of the updated model as the reward signal. Unlike prior approaches\nthat rely on separate adaptation modules or auxiliary networks, SEAL directly\nuses the model's own generation to control its adaptation process. Experiments\non knowledge incorporation and few-shot generalization show that SEAL is a\npromising step toward language models capable of self-directed adaptation. Our\nwebsite and code is available at https://jyopari.github.io/posts/seal.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10943v1", "AI": {"title_translation": "自适应语言模型", "tldr": "大型语言模型（LLMs）功能强大但静态。SEAL框架使LLMs能够通过生成自己的微调数据和更新指令来自适应。它通过强化学习训练模型生成有效的自我编辑，从而实现持久的权重更新。实验表明SEAL在知识整合和少样本泛化方面表现出前景。", "motivation": "大型语言模型（LLMs）功能强大但却是静态的，它们缺乏根据新任务、新知识或新示例来调整其权重的机制。", "method": "SEAL框架通过让LLMs生成自己的微调数据和更新指令来实现自适应。模型会生成“自我编辑”，这些编辑可以重构信息、指定优化超参数或调用工具进行数据增强和基于梯度的更新。通过监督微调（SFT），这些自我编辑导致持久的权重更新。为了训练模型生成有效的自我编辑，该方法使用一个强化学习循环，并将更新模型的下游性能作为奖励信号。与以往依赖独立适应模块或辅助网络的方法不同，SEAL直接使用模型自身的生成来控制其适应过程。", "result": "在知识整合和少样本泛化方面的实验表明，SEAL是迈向能够自主适应的语言模型的有希望的一步。", "conclusion": "SEAL是迈向能够自主适应的语言模型的有希望的一步。", "translation": "大型语言模型（LLMs）功能强大但却是静态的；它们缺乏根据新任务、知识或示例调整其权重的机制。我们引入了自适应LLMs（SEAL），这是一个使LLMs能够通过生成自己的微调数据和更新指令来自适应的框架。给定新的输入，模型会生成一个自我编辑——一种可能以不同方式重构信息、指定优化超参数或调用工具进行数据增强和基于梯度的更新的生成。通过监督微调（SFT），这些自我编辑导致持久的权重更新，从而实现持久适应。为了训练模型生成有效的自我编辑，我们使用一个强化学习循环，并将更新模型的下游性能作为奖励信号。与依赖独立适应模块或辅助网络的现有方法不同，SEAL直接使用模型自身的生成来控制其适应过程。在知识整合和少样本泛化方面的实验表明，SEAL是迈向能够自主适应的语言模型的有希望的一步。我们的网站和代码可在 https://jyopari.github.io/posts/seal 获取。", "summary": "本论文介绍了自适应语言模型（SEAL），这是一个解决大型语言模型静态性质的新颖框架。SEAL使LLMs能够通过生成“自我编辑”来实现自适应，这些编辑包括微调数据和更新指令。这些自我编辑通过监督微调导致持久的权重更新。有效自我编辑的训练是通过一个强化学习循环实现的，该循环使用下游性能作为奖励。一个关键区别在于，SEAL直接利用模型自身的生成进行自适应，这与以往的方法不同。实验证明了SEAL在知识整合和少样本泛化等任务中实现自主适应的潜力。", "keywords": "自适应语言模型, 强化学习, 监督微调, 知识整合, 少样本泛化", "comments": "SEAL提供了一种创新的方法，通过允许LLMs生成自己的适应机制，使其变得动态，这与静态模型或依赖外部模块的模型有显著不同。利用强化学习优化自我编辑以实现持久权重更新尤其值得关注。该框架有潜力增强LLMs在多样化和不断变化的环境中的长期效用和适应性。"}}
{"id": "2506.10584", "title": "Encoding call-by-push-value in the pi-calculus", "authors": ["Benjamin Bennetzen", "Nikolaj Rossander Kristensen", "Peter Buus Steffensen"], "summary": "In this report we define an encoding of Levys call-by-push-value\nlambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both\nsound and complete. We present informal (by-hand) proofs of soundness,\ncompleteness, and all required lemmas. The encoding is specialized to the\ninternal pi-calculus (pi-i-calculus) to circumvent certain challenges\nassociated with using de Bruijn index in a formalization, and it also helps\nwith bisimulation as early-, late- and open-bisimulation coincide in this\nsetting, furthermore bisimulation is a congruence. Additionally, we argue that\nour encoding also satisfies the five criteria for good encodings proposed by\nGorla, as well as show similarities between Milners and our encoding. This\npaper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic\npi-calculus and the local pi-calculus. We begin a formalization of the proof in\nCoq for the soundness and completeness of the encoding in the pi-i-calculus.\nNot all lemmas used in the formalization are themselves formally proven.\nHowever, we argue that the non-proven lemmas are reasonable, as they are proven\nby hand, or amount to Coq formalities that are straightforward given informal\narguments.", "comment": "56 pages", "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.10584v1", "AI": {"title_translation": "在π-演算中编码按值传递调用", "tldr": "本报告定义了Levy的按值传递调用λ-演算（CBPV）在π-演算中的编码，并证明了其健全性和完备性，同时开始了在Coq中的形式化证明。", "motivation": "该研究的动机是定义Levy的按值传递调用λ-演算（CBPV）在π-演算中的编码，并证明该编码的健全性和完备性。", "method": "研究方法包括：定义CBPV在π-演算中的编码；通过非正式（手工）证明来验证编码的健全性、完备性及所有必要引理；将编码专门化到内部π-演算（π-i-演算），以规避de Bruijn索引带来的挑战并简化双模拟；论证编码满足Gorla提出的良好编码的五项标准；展示与Milner编码的相似性；并开始了在Coq中对π-i-演算编码的健全性和完备性证明的形式化工作，尽管并非所有引理都已正式证明。", "result": "该研究定义了一个CBPV在π-演算中的编码，并证明其是健全和完备的。该编码专门用于内部π-演算（π-i-演算），有助于解决形式化中的挑战，并使双模拟特性（早期、晚期和开放双模拟一致，且双模拟是同余）得到改善。此外，该编码满足Gorla提出的良好编码的五项标准，并与Milner的编码有相似之处。", "conclusion": "该论文成功定义了一个健全且完备的按值传递调用λ-演算到π-演算的编码，特别是在内部π-演算中表现良好，并满足了良好编码的标准。尽管初步证明是非正式的，但已开始在Coq中进行形式化验证，这表明了编码的稳健性和研究的深入性。", "translation": "本报告定义了Levy的按值传递调用λ-演算（CBPV）在π-演算中的编码，并证明我们的编码是健全和完备的。我们提供了健全性、完备性以及所有必需引理的非正式（手工）证明。该编码专门针对内部π-演算（π-i-演算），以规避形式化中使用de Bruijn索引相关的某些挑战，并且它也有助于双模拟，因为在这种设置下，早期、晚期和开放双模拟是重合的，此外双模拟是同余的。此外，我们论证了我们的编码也满足Gorla提出的良好编码的五项标准，并展示了Milner编码与我们编码之间的相似性。本文包括CBPV在π-i-演算、异步多态π-演算和局部π-演算中的编码。我们开始在Coq中对π-i-演算中编码的健全性和完备性证明进行形式化。并非所有用于形式化的引理本身都经过了正式证明。然而，我们认为未证明的引理是合理的，因为它们是手工证明的，或者根据非正式论证，它们相当于Coq中直接的形式化。", "summary": "本论文定义了Levy的按值传递调用λ-演算（CBPV）在π-演算中的编码，并主要在内部π-演算（π-i-演算）中对其进行了验证。该编码被证明是健全和完备的，并通过非正式证明进行了支持。它解决了使用de Bruijn索引的挑战，并改进了双模拟的性质。此外，该编码满足Gorla的良好编码标准，并与Milner的编码有相似之处。论文还提及了CBPV在其他π-演算变体中的编码，并正在Coq中进行形式化证明。", "keywords": "CBPV, π-演算, 编码, 健全性, 完备性", "comments": "该论文的创新之处在于提出了CBPV到π-演算的特定编码，并通过引入π-i-演算来解决传统de Bruijn索引和双模拟的挑战。尽管初步证明是非正式的，但其在Coq中启动形式化证明的努力，展示了研究的严谨性。论文的重要性在于为两种重要的计算模型之间提供了坚实的理论桥梁。潜在的局限性在于部分引理的证明仍依赖于手工而非完全形式化，这可能需要后续工作来完全消除人为错误的可能性。"}}
{"id": "2506.10821", "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using", "authors": ["Huaying Yuan", "Zheng Liu", "Junjie Zhou", "Ji-Rong Wen", "Zhicheng Dou"], "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10821v1", "AI": {"title_translation": "VideoDeepResearch：使用智能体工具进行长视频理解", "tldr": "VideoDeepResearch 提出了一种新颖的智能体框架，利用文本推理模型和模块化多模态工具包来解决长视频理解的挑战，并在多个基准测试中显著超越现有 MLLM 基线。", "motivation": "当前的多模态大型语言模型（MLLMs）在长视频理解（LVU）方面面临显著挑战，原因在于任务的固有复杂性和上下文窗口限制。普遍认为解决 LVU 任务需要具有扩展上下文窗口、强大视觉感知能力和熟练领域专业知识的基础 MLLM。", "method": "本文引入了 VideoDeepResearch，一个用于长视频理解的新型智能体框架。该方法仅依赖于一个文本大型推理模型（LRM），并结合了一个模块化的多模态工具包，包括多模态检索器和视觉感知器，这些都已在实践中可用。对于每个 LVU 任务，系统通过推理制定解决问题的策略，同时通过工具使用选择性地访问和利用必要的视频内容。", "result": "VideoDeepResearch 在流行的 LVU 基准测试（包括 MLVU、Video-MME 和 LVBench）上进行了广泛实验。结果表明，VideoDeepResearch 比现有 MLLM 基线取得了显著改进，在 MLVU（测试）、LVBench 和 LongVideoBench 上分别超越了现有最先进水平 9.6%、6.6% 和 3.9%。", "conclusion": "这些发现突出了智能体系统在克服长视频理解问题中关键挑战方面的潜力。", "translation": "长视频理解（LVU）对当前的多模态大型语言模型（MLLM）构成了重大挑战，原因在于任务固有的复杂性和上下文窗口限制。人们普遍认为，解决 LVU 任务需要具备扩展上下文窗口、强大视觉感知能力和熟练领域专业知识的基础 MLLM。在这项工作中，我们通过引入 VideoDeepResearch 挑战了这一普遍看法，这是一个用于长视频理解的新型智能体框架。我们的方法仅依赖于一个文本大型推理模型（LRM），并结合了一个模块化的多模态工具包，包括多模态检索器和视觉感知器，所有这些在实践中都易于获得。对于每个 LVU 任务，系统通过推理制定解决问题的策略，同时通过工具使用选择性地访问和利用必要的视频内容。我们在流行的 LVU 基准测试（包括 MLVU、Video-MME 和 LVBench）上进行了广泛实验。我们的结果表明，VideoDeepResearch 比现有 MLLM 基线取得了显著改进，在 MLVU（测试）、LVBench 和 LongVideoBench 上分别超越了现有最先进水平 9.6%、6.6% 和 3.9%。这些发现突出了智能体系统在克服长视频理解问题中关键挑战方面的潜力。", "summary": "本文提出了 VideoDeepResearch，一个用于长视频理解（LVU）的新型智能体框架，旨在解决现有 MLLM 面临的复杂性和上下文限制问题。该框架利用一个文本大型推理模型（LRM）结合模块化多模态工具包（如检索器和感知器），通过推理制定问题解决策略并按需访问视频内容。实验结果表明，VideoDeepResearch 在 MLVU、LVBench 和 LongVideoBench 等主流 LVU 基准测试上显著优于现有 MLLM 基线，性能提升高达 9.6%。这证明了智能体系统在应对 LVU 挑战方面的巨大潜力。", "keywords": "长视频理解, 智能体系统, 多模态工具, 大推理模型, VideoDeepResearch", "comments": "该论文的创新之处在于挑战了解决长视频理解问题必须依赖于具有超长上下文窗口的强大 MLLM 的普遍观念。它提出了一种更轻量级但高效的智能体框架，通过将文本推理模型与可用的模块化多模态工具结合，实现了对视频内容的按需访问和利用。这种方法为资源受限或需要更高灵活性场景下的长视频理解提供了新的思路，并取得了显著的性能提升，具有重要的实践意义。"}}
{"id": "2506.10946", "title": "GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models", "authors": ["Evelyn Ma", "Duo Zhou", "Peizhi Niu", "Huiting Zhou", "Huan Zhang", "Olgica Milenkovic", "S. Rasoul Etesami"], "summary": "Unlearning in large language models (LLMs) is becoming increasingly important\ndue to regulatory compliance, copyright protection, and privacy concerns.\nHowever, a key challenge in LLM unlearning is unintended forgetting, where the\nremoval of specific data inadvertently impairs the utility of the model and its\nretention of valuable, desired information. While prior work has primarily\nfocused on architectural innovations, the influence of data-level factors on\nunlearning performance remains underexplored. As a result, existing methods\noften suffer from degraded retention when forgetting high-impact data. To\naddress this, we propose GUARD-a novel framework for Guided Unlearning And\nRetention via Data attribution. At its core, GUARD introduces a lightweight\nproxy data attribution metric tailored for LLM unlearning, which quantifies the\n\"alignment\" between the forget and retain sets while remaining computationally\nefficient. Building on this, we design a novel unlearning objective that\nassigns adaptive, nonuniform unlearning weights to samples, inversely\nproportional to their proxy attribution scores. Through such a reallocation of\nunlearning power, GUARD mitigates unintended losses in retention. We provide\nrigorous theoretical guarantees that GUARD significantly enhances retention\nwhile maintaining forgetting metrics comparable to prior methods. Extensive\nexperiments on the TOFU benchmark across multiple LLM architectures demonstrate\nthat GUARD substantially improves utility preservation while ensuring effective\nunlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to\n194.92% in terms of Truth Ratio when forgetting 10% of the training data.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10946v1", "AI": {"title_translation": "GUARD：通过数据归因实现大型语言模型的引导式遗忘与保留", "tldr": "GUARD是一个新颖的框架，通过引入轻量级代理数据归因指标和自适应非均匀遗忘权重，解决了大型语言模型中遗忘过程中意外遗忘的问题，显著提高了模型保留有用信息的能力，同时保持了有效的遗忘。", "motivation": "由于法规遵从、版权保护和隐私问题，大型语言模型（LLMs）中的遗忘变得越来越重要。然而，LLM遗忘中的一个关键挑战是意外遗忘，即移除特定数据会无意中损害模型的效用及其对有价值信息的保留。现有方法在遗忘高影响力数据时常常导致保留性能下降，因为它们主要关注架构创新，而数据层面因素的影响尚未得到充分探索。", "method": "我们提出了GUARD——一个通过数据归因实现引导式遗忘和保留的新颖框架。GUARD引入了一个专为LLM遗忘设计的轻量级代理数据归因指标，该指标量化了遗忘集和保留集之间的“对齐度”，同时保持计算效率。在此基础上，我们设计了一个新颖的遗忘目标，该目标为样本分配自适应的、非均匀的遗忘权重，这些权重与它们的代理归因分数成反比，从而重新分配遗忘能力，减轻保留方面的意外损失。", "result": "GUARD在保持与现有方法相当的遗忘指标的同时，显著增强了保留能力，并提供了严格的理论保证。在TOFU基准上对多种LLM架构进行的广泛实验表明，GUARD在确保有效遗忘的同时，大幅提高了效用保留。值得注意的是，当遗忘10%的训练数据时，GUARD在保留集上的效用损失（以真实比率衡量）减少了高达194.92%。", "conclusion": "GUARD框架通过引入数据归因和自适应遗忘权重，有效解决了大型语言模型遗忘中的意外遗忘问题，显著提升了模型在遗忘高影响力数据时的效用保留和信息保留能力，同时保持了与现有方法相当的遗忘性能。", "translation": "大型语言模型（LLMs）中的遗忘由于法规遵从、版权保护和隐私问题而变得越来越重要。然而，LLM遗忘中的一个关键挑战是意外遗忘，即移除特定数据会无意中损害模型的效用及其对有价值、所需信息的保留。虽然现有工作主要集中在架构创新上，但数据层面因素对遗忘性能的影响仍未得到充分探索。因此，现有方法在遗忘高影响力数据时常常导致保留性能下降。为了解决这个问题，我们提出了GUARD——一个通过数据归因实现引导式遗忘和保留的新颖框架。GUARD的核心是引入一个专为LLM遗忘设计的轻量级代理数据归因指标，该指标量化了遗忘集和保留集之间的“对齐度”，同时保持计算效率。在此基础上，我们设计了一个新颖的遗忘目标，该目标为样本分配自适应的、非均匀的遗忘权重，这些权重与它们的代理归因分数成反比。通过这种遗忘能力的重新分配，GUARD减轻了保留方面的意外损失。我们提供了严格的理论保证，表明GUARD在保持与现有方法相当的遗忘指标的同时，显著增强了保留能力。在TOFU基准上对多种LLM架构进行的广泛实验表明，GUARD在确保有效遗忘的同时，大幅提高了效用保留。值得注意的是，当遗忘10%的训练数据时，GUARD在保留集上的效用损失（以真实比率衡量）减少了高达194.92%。", "summary": "本文提出了GUARD，一个用于大型语言模型（LLMs）的引导式遗忘和保留框架，旨在解决现有遗忘方法中存在的意外遗忘问题。GUARD引入了一个轻量级代理数据归因指标来量化遗忘集和保留集之间的对齐度，并设计了一个新的遗忘目标，根据代理归因分数自适应地分配非均匀遗忘权重。理论分析和实验结果表明，GUARD在保持有效遗忘的同时，显著提高了模型对有用信息的保留和效用保持，尤其是在遗忘高影响力数据时，显著减少了保留集的效用牺牲。", "keywords": "遗忘, 大型语言模型, 数据归因, 保留, 意外遗忘", "comments": "GUARD的创新之处在于其从数据层面而非仅仅架构层面解决了LLM遗忘中的意外遗忘问题。通过引入数据归因度量和自适应权重分配，它提供了一种更精细、更高效的遗忘机制，显著提升了模型在遗忘特定信息后的实用性。其在保留集上效用牺牲减少高达194.92%的结果令人印象深刻，表明了该方法在平衡遗忘和保留方面的强大能力。这对于LLM在实际应用中满足法规要求和保护用户隐私具有重要意义。"}}
{"id": "2506.10840", "title": "Post-Training Quantization for Video Matting", "authors": ["Tianrui Zhu", "Houyuan Chen", "Ruihao Gong", "Michele Magno", "Haotong Qin", "Kai Zhang"], "summary": "Video matting is crucial for applications such as film production and virtual\nreality, yet deploying its computationally intensive models on\nresource-constrained devices presents challenges. Quantization is a key\ntechnique for model compression and acceleration. As an efficient approach,\nPost-Training Quantization (PTQ) is still in its nascent stages for video\nmatting, facing significant hurdles in maintaining accuracy and temporal\ncoherence. To address these challenges, this paper proposes a novel and general\nPTQ framework specifically designed for video matting models, marking, to the\nbest of our knowledge, the first systematic attempt in this domain. Our\ncontributions include: (1) A two-stage PTQ strategy that combines\nblock-reconstruction-based optimization for fast, stable initial quantization\nand local dependency capture, followed by a global calibration of quantization\nparameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine\nCalibration (GAC) method that enables the network to compensate for cumulative\nstatistical distortions arising from factors such as neglected BN layer\neffects, even reducing the error of existing PTQ methods on video matting tasks\nup to 20%. (3) An Optical Flow Assistance (OFA) component that leverages\ntemporal and semantic priors from frames to guide the PTQ process, enhancing\nthe model's ability to distinguish moving foregrounds in complex scenes and\nultimately achieving near full-precision performance even under ultra-low-bit\nquantization. Comprehensive quantitative and visual results show that our\nPTQ4VM achieves the state-of-the-art accuracy performance across different\nbit-widths compared to the existing quantization methods. We highlight that the\n4-bit PTQ4VM even achieves performance close to the full-precision counterpart\nwhile enjoying 8x FLOP savings.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10840v1", "AI": {"title_translation": "视频抠图的训练后量化", "tldr": "本文提出了一个新颖的、通用的视频抠图模型训练后量化（PTQ）框架，通过两阶段策略、统计驱动的全局仿射校准和光流辅助组件，显著提高了量化模型的精度和时间一致性，在超低比特量化下也能达到接近全精度性能。", "motivation": "视频抠图模型计算密集，在资源受限设备上部署到资源受限设备上存在挑战。训练后量化（PTQ）作为一种高效的模型压缩和加速方法，在视频抠图领域仍处于初期，面临维持精度和时间一致性的重大障碍。", "method": "本文提出了一个新颖通用的视频抠图模型训练后量化（PTQ）框架，据称是该领域首次系统性尝试。其贡献包括：1) 两阶段PTQ策略，结合基于块重建的优化和全局量化参数校准。2) 统计驱动的全局仿射校准（GAC）方法，用于补偿累积统计失真。3) 光流辅助（OFA）组件，利用帧的时间和语义先验来指导PTQ过程。", "result": "全面的量化和视觉结果表明，本文提出的PTQ4VM在不同比特宽度下均达到了现有量化方法中的最先进精度性能。特别地，4比特PTQ4VM能够达到接近全精度模型的性能，同时实现8倍的FLOPs节省。", "conclusion": "本文提出的视频抠图训练后量化框架PTQ4VM有效地解决了训练后量化在精度和时间一致性方面的挑战，取得了最先进的结果，并实现了视频抠图模型在资源受限设备上的高效部署。", "translation": "视频抠图对于电影制作和虚拟现实等应用至关重要，但其计算密集型模型在资源受限设备上的部署面临挑战。量化是模型压缩和加速的关键技术。作为一种高效的方法，训练后量化（PTQ）在视频抠图领域仍处于初期阶段，在保持精度和时间一致性方面面临重大障碍。为了解决这些挑战，本文提出了一种新颖通用的PTQ框架，专门为视频抠图模型设计，据我们所知，这是该领域首次系统性尝试。我们的贡献包括：(1) 两阶段PTQ策略，结合基于块重建的优化，用于快速、稳定的初始量化和局部依赖捕获，随后进行量化参数的全局校准以最小化精度损失。(2) 统计驱动的全局仿射校准（GAC）方法，使网络能够补偿由于忽略BN层效应等因素引起的累积统计失真，甚至将现有PTQ方法在视频抠图任务上的误差降低多达20%。(3) 光流辅助（OFA）组件，利用帧的时间和语义先验来指导PTQ过程，增强模型在复杂场景中区分移动前景的能力，最终即使在超低比特量化下也能实现接近全精度的性能。全面的定量和视觉结果表明，与现有量化方法相比，我们的PTQ4VM在不同比特宽度下均达到了最先进的精度性能。我们强调，4比特PTQ4VM甚至实现了接近全精度对应的性能，同时享受8倍的FLOPs节省。", "summary": "本文提出了一种名为PTQ4VM的新型训练后量化（PTQ）框架，专为视频抠图模型设计，旨在解决在资源受限设备上部署计算密集型模型时遇到的精度和时间一致性挑战。该框架包含三项核心贡献：一个两阶段PTQ策略、一种统计驱动的全局仿射校准（GAC）方法以及一个光流辅助（OFA）组件。实验结果表明，PTQ4VM在不同比特宽度下均实现了最先进的精度，尤其在4比特量化下能达到接近全精度模型的性能，并实现了8倍的计算量（FLOPs）节省。", "keywords": "视频抠图, 训练后量化, 模型压缩, 时间一致性, 深度学习", "comments": "该论文首次系统性地提出了针对视频抠图模型的训练后量化框架，其创新点在于结合了多方面的优化策略，特别是引入光流辅助来增强时间一致性，这对于视频任务至关重要。所提出的方法显著提升了量化视频抠图模型的精度和部署效率，对实际应用具有重要意义。"}}
{"id": "2506.10948", "title": "Execution Guided Line-by-Line Code Generation", "authors": ["Boaz Lavon", "Shahar Katz", "Lior Wolf"], "summary": "We present a novel approach to neural code generation that incorporates\nreal-time execution signals into the language model generation process. While\nlarge language models (LLMs) have demonstrated impressive code generation\ncapabilities, they typically do not utilize execution feedback during\ninference, a critical signal that human programmers regularly leverage. Our\nmethod, Execution-Guided Classifier-Free Guidance (EG-CFG), dynamically\nincorporates execution signals as the model generates code, providing\nline-by-line feedback that guides the generation process toward executable\nsolutions. EG-CFG employs a multi-stage process: first, we conduct beam search\nto sample candidate program completions for each line; second, we extract\nexecution signals by executing these candidates against test cases; and\nfinally, we incorporate these signals into the prompt during generation. By\nmaintaining consistent signals across tokens within the same line and\nrefreshing signals at line boundaries, our approach provides coherent guidance\nwhile preserving syntactic structure. Moreover, the method naturally supports\nnative parallelism at the task level in which multiple agents operate in\nparallel, exploring diverse reasoning paths and collectively generating a broad\nset of candidate solutions. Our experiments across diverse coding tasks\ndemonstrate that EG-CFG significantly improves code generation performance\ncompared to standard approaches, achieving state-of-the-art results across\nvarious levels of complexity, from foundational problems to challenging\ncompetitive programming tasks. Our code is available at:\nhttps://github.com/boazlavon/eg_cfg", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10948v1", "AI": {"title_translation": "执行引导的逐行代码生成", "tldr": "本文提出了一种名为EG-CFG的新型神经代码生成方法，该方法在生成过程中融入实时执行信号，显著提高了代码生成性能并达到了最先进水平。", "motivation": "大型语言模型（LLMs）在代码生成方面表现出色，但它们通常在推理过程中不利用执行反馈，而这却是人类程序员经常利用的关键信号。", "method": "本文提出的方法是执行引导的无分类器指导（EG-CFG），它在模型生成代码时动态地整合执行信号，提供逐行反馈以引导生成过程走向可执行的解决方案。EG-CFG采用多阶段过程：首先，进行束搜索以采样每行的候选程序完成；其次，通过针对测试用例执行这些候选来提取执行信号；最后，在生成过程中将这些信号纳入提示中。该方法通过在同一行内的标记之间保持一致的信号并在行边界刷新信号，提供了连贯的指导，同时保留了语法结构。此外，该方法自然支持任务级别的原生并行性，其中多个智能体并行操作，探索多样化的推理路径并共同生成广泛的候选解决方案。", "result": "实验表明，EG-CFG在各种编码任务中显著提高了代码生成性能，与标准方法相比，在从基础问题到具有挑战性的竞争性编程任务等不同复杂性级别上都取得了最先进的结果。", "conclusion": "该方法成功地将实时执行信号整合到代码生成过程中，从而显著提高了性能并取得了最先进的结果。", "translation": "我们提出了一种新颖的神经代码生成方法，该方法将实时执行信号整合到语言模型生成过程中。虽然大型语言模型（LLM）已经展示了令人印象深刻的代码生成能力，但它们在推理过程中通常不利用执行反馈，而这正是人类程序员经常利用的关键信号。我们的方法，执行引导的无分类器指导（EG-CFG），在模型生成代码时动态地整合执行信号，提供逐行反馈，引导生成过程走向可执行的解决方案。EG-CFG采用多阶段过程：首先，我们进行束搜索以采样每行的候选程序完成；其次，我们通过针对测试用例执行这些候选来提取执行信号；最后，我们将这些信号纳入生成过程中的提示中。通过在同一行内的标记之间保持一致的信号并在行边界刷新信号，我们的方法提供了连贯的指导，同时保留了语法结构。此外，该方法自然支持任务级别的原生并行性，其中多个智能体并行操作，探索多样化的推理路径并共同生成广泛的候选解决方案。我们在各种编码任务上的实验表明，与标准方法相比，EG-CFG显著提高了代码生成性能，在从基础问题到具有挑战性的竞争性编程任务等不同复杂性级别上都取得了最先进的结果。我们的代码可在以下地址获取：https://github.com/boazlavon/eg_cfg", "summary": "本文提出了一种名为执行引导的无分类器指导（EG-CFG）的新型神经代码生成方法，该方法通过在代码生成过程中整合实时执行反馈来增强大型语言模型的能力。与传统LLM不同，EG-CFG动态地利用逐行执行候选代码并针对测试用例获取的反馈，以引导生成过程，从而提高生成代码的可执行性和准确性。该方法支持并行处理，并在各种复杂度的编码任务中取得了显著的性能提升和最先进的结果。", "keywords": "代码生成, 执行反馈, 大型语言模型, 无分类器指导, EG-CFG", "comments": "该论文的创新之处在于将实时执行反馈循环地整合到大型语言模型的代码生成过程中，这模仿了人类程序员的调试和迭代开发实践。这代表了代码生成领域的一个重要进步，解决了当前LLM生成代码可能缺乏可执行性的关键限制，使其能够生成更健壮和实际可用的代码。"}}
{"id": "2506.10857", "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos", "authors": ["Jiashuo Yu", "Yue Wu", "Meng Chu", "Zhifei Ren", "Zizheng Huang", "Pei Chu", "Ruijie Zhang", "Yinan He", "Qirui Li", "Songze Li", "Zhenxiang Li", "Zhongying Tu", "Conghui He", "Yu Qiao", "Yali Wang", "Yi Wang", "Limin Wang"], "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.", "comment": "Technical Report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10857v1", "AI": {"title_translation": "VRBench：长叙事视频中的多步推理基准", "tldr": "VRBench是一个用于评估大型模型在长叙事视频中多步推理能力的新基准，解决了现有评估中忽视时间推理和程序有效性的问题。", "motivation": "现有的评估方法在评估大型模型的多步推理能力时，忽视了时间推理和程序有效性。为了解决这些局限性，本文提出了VRBench。", "method": "本文提出了VRBench，第一个专为评估大型模型多步推理能力而设计的长叙事视频基准。它包含1,010个长视频（平均时长1.6小时），以及9,468个人工标注的多步问答对和30,292个带有时间戳的推理步骤。视频通过多阶段过滤过程（包括专家互评）进行整理，以优先考虑情节连贯性。开发了一个人机协作框架，生成连贯的推理链，每个链都需要多个时间接地的步骤，涵盖七种类型。VRBench设计了一个多阶段评估管道，从结果和过程层面评估模型。除了最终结果的MCQ外，还提出了一种进程级LLM引导的评分指标，以多维度全面评估推理链的质量。", "result": "通过对VRBench上12个LLM和16个VLM进行广泛评估，本文进行了彻底的分析，并提供了有价值的见解，推动了多步推理领域的发展。", "conclusion": "VRBench作为首个长叙事视频多步推理基准，通过其全面的数据集和评估框架，有效弥补了现有评估的不足，并为大型模型在复杂推理任务上的发展提供了重要见解。", "translation": "我们提出了VRBench，这是第一个为评估大型模型多步推理能力而精心制作的长叙事视频基准，解决了现有评估中忽视时间推理和程序有效性的局限性。它包括1,010个长视频（平均时长1.6小时），以及9,468个人工标注的多步问答对和30,292个带有时间戳的推理步骤。这些视频通过多阶段过滤过程（包括专家互评）进行整理，以优先考虑情节连贯性。我们开发了一个人机协作框架，生成连贯的推理链，每个链都需要多个时间接地的步骤，涵盖七种类型（例如，事件归因、隐含推理）。VRBench设计了一个多阶段评估管道，用于评估模型的结果和过程级别。除了最终结果的MCQ外，我们提出了一种进程级LLM引导的评分指标，以多维度全面评估推理链的质量。通过对VRBench上12个LLM和16个VLM进行广泛评估，我们进行了彻底的分析，并提供了有价值的见解，推动了多步推理领域的发展。", "summary": "本文介绍了VRBench，一个针对长叙事视频的多步推理基准，旨在解决现有评估在时间推理和程序有效性方面的不足。该基准包含1010个长视频、9468个多步问答对和30292个带时间戳的推理步骤。VRBench利用人机协作框架生成复杂的推理链，并设计了多阶段评估管道，结合LLM引导的评分指标，从结果和过程层面全面评估模型。通过对12个大型语言模型和16个视觉语言模型的广泛评估，VRBench提供了深入的分析和见解，推动了多步推理领域的发展。", "keywords": "多步推理, 长叙事视频, 基准, 大型模型, 视频理解", "comments": "VRBench的创新之处在于它是第一个针对长叙事视频的多步推理基准，特别强调了时间推理和程序有效性，弥补了现有基准的不足。其数据集规模庞大且质量高，通过专家互评确保了情节连贯性。人机协作框架和多阶段评估管道的设计，以及LLM引导的评分指标，都体现了其评估的全面性和深度。这对于推动大型模型在复杂、真实世界视频理解和推理方面的进步具有重要意义。"}}
{"id": "2506.10953", "title": "Build the web for agents, not agents for the web", "authors": ["Xing Han Lù", "Gaurav Kamath", "Marius Mosbach", "Siva Reddy"], "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10953v1", "AI": {"title_translation": "为智能体构建网络，而非为网络构建智能体", "tldr": "当前的网络智能体难以适应为人类设计的网页界面。本文提出了一种范式转变：不应强迫智能体适应人类界面，而应开发专门为智能体优化的“智能体网络界面”（AWI），并提出了六项指导原则，旨在实现更高效、可靠的网络智能体设计。", "motivation": "大型语言模型（LLMs）驱动的网络智能体在自动化网络交互方面前景广阔，但当前方法面临巨大挑战，因为人类设计的界面与LLM能力之间存在根本性不匹配。现有方法难以处理复杂的网络输入，如处理庞大的DOM树、依赖带额外信息的截图或通过API绕过用户界面。", "method": "本文倡导网络智能体研究的范式转变，提出并引入了“智能体网络界面”（Agentic Web Interface, AWI）的概念。AWI是专门为智能体导航网站而设计的界面，其设计遵循六项指导原则，重点强调安全性、效率和标准化，以兼顾所有主要利益相关者的利益。", "result": "Not mentioned in abstract", "conclusion": "本文得出结论，为克服现有网络智能体的根本局限性，应开发一种新的交互范式——智能体网络界面（AWI）。这将通过社区协作，为更高效、可靠和透明的网络智能体设计铺平道路。", "translation": "大型语言模型（LLMs）和多模态对应物最近的进展，激发了人们对开发网络智能体——能够在网络环境中自主导航和完成任务的AI系统——的浓厚兴趣。尽管网络智能体在自动化复杂的网络交互方面前景广阔，但由于人类设计的界面与LLM能力之间存在根本性不匹配，当前的方法面临巨大挑战。当前的方法难以处理网络输入的固有复杂性，无论是处理庞大的DOM树，依赖增强额外信息的截图，还是通过API交互完全绕过用户界面。这份立场文件倡导网络智能体研究的范式转变：与其强迫网络智能体适应为人类设计的界面，我们应该开发一种专门为智能体能力优化的新型交互范式。为此，我们引入了智能体网络界面（AWI）的概念，这是一种专门为智能体导航网站而设计的界面。我们为AWI设计建立了六项指导原则，强调安全性、效率和标准化，以兼顾所有主要利益相关者的利益。这种重新框架旨在克服现有界面的根本局限性，为更高效、可靠和透明的网络智能体设计铺平道路，这将是一个涉及更广泛机器学习社区的协作努力。", "summary": "当前的网络智能体因人类设计的界面与大型语言模型能力不匹配而面临挑战。本立场文件提出一种范式转变：不再让智能体适应人类界面，而是设计专门为智能体优化的“智能体网络界面”（AWI）。论文引入AWI概念，并确立六项强调安全、效率和标准化的指导原则，旨在通过社区协作，实现更高效、可靠、透明的网络智能体设计。", "keywords": "网络智能体, 大型语言模型, 智能体网络界面, 人机交互, 范式转变", "comments": "本文提出了一个关于网络智能体发展的创新性视角，直接解决了当前智能体面临的一个关键瓶颈——即界面不匹配问题。智能体网络界面（AWI）的概念具有创新性，它提出了一种根本性的转变：从让智能体适应人类界面转变为专门为智能体设计界面。其对安全性、效率和标准化的强调，以及对社区协作的呼吁，展现了对未来网络智能体设计实用且负责任的态度。"}}
{"id": "2506.10890", "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation", "authors": ["Zhao Zhang", "Yutao Cheng", "Dexiang Hong", "Maoke Yang", "Gonglei Shi", "Lei Ma", "Hui Zhang", "Jie Shao", "Xinglong Wu"], "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10890v1", "AI": {"title_translation": "CreatiPoster：迈向可编辑和可控的多层图形设计生成", "tldr": "CreatiPoster是一个新颖的AI框架，能从自然语言或资产生成可编辑的多层图形设计，通过独特的协议模型和条件背景模型，在性能上超越现有系统，并支持多种应用，旨在推动AI辅助设计的普及。", "motivation": "当前，创建高质量、可编辑且美观的图形设计既耗时又需要专业技能，尤其是对于初学者而言。现有的AI工具难以准确整合用户资产、保持可编辑性并达到专业视觉效果，而商业系统则依赖于难以复制的庞大模板库。", "method": "本文提出了CreatiPoster框架，该框架能够从自然语言指令或资产生成可编辑的多层设计。它首先使用一个协议模型（RGBA大型多模态模型）生成一个JSON规范，详细描述每个层（文本或资产）的布局、层级、内容和样式，以及一个简洁的背景提示。然后，一个条件背景模型根据渲染的前景层合成连贯的背景。", "result": "CreatiPoster在作者构建的图形设计生成基准测试中，表现优于领先的开源方法和专有商业系统。此外，研究人员还发布了一个包含100,000个无版权多层设计的语料库。CreatiPoster支持画布编辑、文本叠加、响应式调整大小、多语言适应和动画海报等多种应用。", "conclusion": "CreatiPoster通过其创新的框架和卓越的性能，推动了AI辅助图形设计的民主化进程。", "translation": "图形设计在商业和个人环境中都扮演着至关重要的角色，然而，创建高质量、可编辑且美观的图形构图仍然是一项耗时且技能密集型的工作，特别是对于初学者而言。当前的AI工具自动化了部分工作流程，但在准确整合用户提供的资产、保持可编辑性和实现专业视觉吸引力方面仍面临挑战。像Canva Magic Design这样的商业系统依赖于庞大的模板库，这在复制方面是不切实际的。在本文中，我们引入了CreatiPoster，一个可以从可选的自然语言指令或资产生成可编辑、多层构图的框架。一个协议模型，一个RGBA大型多模态模型，首先生成一个JSON规范，详细说明每个层（文本或资产）的精确布局、层级、内容和样式，以及一个简洁的背景提示。然后，一个条件背景模型根据渲染的前景层合成一个连贯的背景。我们构建了一个带有自动化指标的图形设计生成基准，并表明CreatiPoster超越了领先的开源方法和专有商业系统。为了促进进一步的研究，我们发布了一个包含100,000个无版权多层设计的语料库。CreatiPoster支持多种应用，例如画布编辑、文本叠加、响应式调整大小、多语言适应和动画海报，从而推动了AI辅助图形设计的民主化。项目主页：https://github.com/graphic-design-ai/creatiposter", "summary": "CreatiPoster是一个创新的AI框架，旨在解决图形设计中创建可编辑、高质量多层构图的挑战。它通过一个独特的两阶段生成过程实现：首先，一个协议模型（RGBA大型多模态模型）根据自然语言指令或用户资产生成详细的JSON层规范和背景提示；其次，一个条件背景模型合成最终背景。该系统在性能上超越了现有开源和商业方案，并支持多种实际应用，如画布编辑和多语言适应。为促进研究，作者还发布了一个包含10万个多层设计的版权免费语料库，显著推动了AI辅助图形设计的普及。", "keywords": "图形设计, 多层生成, 可编辑, 可控, 人工智能辅助设计", "comments": "CreatiPoster的创新之处在于其独特的分层生成方法，通过JSON规范实现了对多层图形设计的精确控制和高度可编辑性，这显著优于传统依赖于模板或单一图像生成的系统。其将前景层（由协议模型处理）与背景层（由条件背景模型处理）分离的设计，有效解决了用户资产整合和视觉连贯性的难题。此外，发布大规模的版权免费多层设计语料库对社区具有重要意义，将极大地促进该领域的研究和发展。该工作在基准测试上的卓越表现及其在多样化应用场景中的潜力，预示着AI辅助图形设计将迎来更广泛的民主化。"}}
{"id": "2506.10955", "title": "ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems", "authors": ["Aayush Karan", "Kulin Shah", "Sitan Chen"], "summary": "There has been a flurry of activity around using pretrained diffusion models\nas informed data priors for solving inverse problems, and more generally around\nsteering these models using reward models. Training-free methods like diffusion\nposterior sampling (DPS) and its many variants have offered flexible heuristic\nalgorithms for these tasks, but when the reward is not informative enough,\ne.g., in hard inverse problems with low signal-to-noise ratio, these techniques\nveer off the data manifold, failing to produce realistic outputs. In this work,\nwe devise a simple wrapper, ReGuidance, for boosting both the sample realism\nand reward achieved by these methods. Given a candidate solution $\\hat{x}$\nproduced by an algorithm of the user's choice, we propose inverting the\nsolution by running the unconditional probability flow ODE in reverse starting\nfrom $\\hat{x}$, and then using the resulting latent as an initialization for\nDPS. We evaluate our wrapper on hard inverse problems like large box\nin-painting and super-resolution with high upscaling. Whereas state-of-the-art\nbaselines visibly fail, we find that applying our wrapper on top of these\nbaselines significantly boosts sample quality and measurement consistency. We\ncomplement these findings with theory proving that on certain multimodal data\ndistributions, ReGuidance simultaneously boosts the reward and brings the\ncandidate solution closer to the data manifold. To our knowledge, this\nconstitutes the first rigorous algorithmic guarantee for DPS.", "comment": "38 pages, 14 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10955v1", "AI": {"title_translation": "ReGuidance：一种用于提升困难逆问题样本质量的简单扩散包装器", "tldr": "ReGuidance是一个简单的扩散模型包装器，通过反向ODE流将候选解转化为潜在表示作为DPS的初始化，从而显著提升困难逆问题（如大面积修复和高倍超分辨率）的样本质量和测量一致性，并提供了首次针对DPS的严格算法保证。", "motivation": "现有的基于预训练扩散模型解决逆问题的无训练方法（如DPS）在奖励信息不足（例如，低信噪比的困难逆问题）时，会偏离数据流形，无法生成真实输出。", "method": "ReGuidance是一个简单的包装器。给定一个用户选择的算法生成的候选解，该方法通过从该解开始反向运行无条件概率流ODE来反演解，然后将得到的潜在表示作为DPS的初始化。", "result": "在诸如大面积图像修复和高倍超分辨率等困难逆问题上进行评估。当最先进的基线方法明显失败时，ReGuidance在这些基线上应用后显著提升了样本质量和测量一致性。", "conclusion": "ReGuidance在某些多模态数据分布上能同时提升奖励并使候选解更接近数据流形，这构成了DPS的首次严格算法保证。", "translation": "关于使用预训练扩散模型作为解决逆问题（更广泛地说，使用奖励模型引导这些模型）的信息化数据先验，已经开展了大量工作。像扩散后验采样（DPS）及其众多变体等无训练方法为这些任务提供了灵活的启发式算法，但当奖励信息不足时（例如，在低信噪比的困难逆问题中），这些技术会偏离数据流形，无法产生真实的输出。在这项工作中，我们设计了一个简单的包装器ReGuidance，用于提升这些方法的样本真实性和所获得的奖励。给定一个由用户选择的算法生成的候选解$\\\\hat{x}$，我们建议通过从$\\\\hat{x}$开始反向运行无条件概率流ODE来反演该解，然后将得到的潜在表示作为DPS的初始化。我们在诸如大面积图像修复和高倍超分辨率等困难逆问题上评估了我们的包装器。尽管最先进的基线方法明显失败，但我们发现将我们的包装器应用于这些基线之上显著提升了样本质量和测量一致性。我们通过理论补充了这些发现，证明在某些多模态数据分布上，ReGuidance同时提升了奖励并将候选解带到更接近数据流形的位置。据我们所知，这构成了DPS的首次严格算法保证。", "summary": "本文提出了ReGuidance，一个用于提升扩散模型在解决困难逆问题时样本质量的简单包装器。针对现有无训练方法在低信噪比情况下效果不佳的问题，ReGuidance通过反向运行无条件概率流ODE来处理候选解，并将其潜在表示作为扩散后验采样（DPS）的初始化。实验结果表明，ReGuidance在图像修复和超分辨率等任务上显著优于现有基线，能生成更真实、更一致的输出。此外，该工作提供了理论证明，这是首次为DPS提供严格的算法保证，表明ReGuidance能同时提升奖励并使解决方案更接近数据流形。", "keywords": "扩散模型, 逆问题, 样本质量, ReGuidance, 扩散后验采样", "comments": "ReGuidance的创新之处在于其简洁而有效的机制，通过巧妙地利用概率流ODE的反向过程，为DPS提供了更好的初始化，从而解决了现有无训练扩散方法在处理困难逆问题时易偏离数据流形的痛点。该方法不仅在实践中表现出色，还提供了坚实的理论支撑，填补了DPS缺乏严格算法保证的空白，这对于推动扩散模型在逆问题领域的应用具有重要意义。"}}
{"id": "2506.10895", "title": "AIR: Zero-shot Generative Model Adaptation with Iterative Refinement", "authors": ["Guimeng Liu", "Milad Abdollahzadeh", "Ngai-Man Cheung"], "summary": "Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained\ngenerator to a target domain using only text guidance and without any samples\nfrom the target domain. Central to recent ZSGM approaches are directional loss\nwhich use the text guidance in the form of aligning the image offset with text\noffset in the embedding space of a vision-language model like CLIP. This is\nsimilar to the analogical reasoning in NLP where the offset between one pair of\nwords is used to identify a missing element in another pair by aligning the\noffset between these two pairs. However, a major limitation of existing ZSGM\nmethods is that the learning objective assumes the complete alignment between\nimage offset and text offset in the CLIP embedding space, resulting in quality\ndegrade in generated images. Our work makes two main contributions. Inspired by\nthe offset misalignment studies in NLP, as our first contribution, we perform\nan empirical study to analyze the misalignment between text offset and image\noffset in CLIP embedding space for various large publicly available datasets.\nOur important finding is that offset misalignment in CLIP embedding space is\ncorrelated with concept distance, i.e., close concepts have a less offset\nmisalignment. To address the limitations of the current approaches, as our\nsecond contribution, we propose Adaptation with Iterative Refinement (AIR)\nwhich is the first ZSGM approach to focus on improving target domain image\nquality based on our new insight on offset misalignment.Qualitative,\nquantitative, and user study in 26 experiment setups consistently demonstrate\nthe proposed AIR approach achieves SOTA performance. Additional experiments are\nin Supp.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10895v1", "AI": {"title_translation": "AIR: 零样本生成模型自适应与迭代细化", "tldr": "本文提出了AIR，一种新的零样本生成模型自适应（ZSGM）方法，通过迭代细化来解决CLIP嵌入空间中图像和文本偏移的未对齐问题，从而显著提高了生成图像的质量并实现了SOTA性能。", "motivation": "现有的零样本生成模型自适应（ZSGM）方法假设CLIP嵌入空间中图像偏移与文本偏移完全对齐，导致生成图像质量下降。本文旨在解决这一局限性。", "method": "首先，对CLIP嵌入空间中文本偏移和图像偏移的未对齐现象进行了实证研究，发现未对齐程度与概念距离相关。其次，基于这一新发现，提出了“带迭代细化的自适应”（AIR），这是第一个专注于通过迭代细化来提高目标域图像质量的ZSGM方法。", "result": "在26种实验设置下的定性、定量和用户研究一致表明，所提出的AIR方法实现了最先进（SOTA）的性能。", "conclusion": "本文通过识别并解决CLIP嵌入空间中图像和文本偏移的未对齐问题，提出了一种有效的零样本生成模型自适应方法AIR，显著提升了生成图像的质量，并达到了SOTA水平。", "translation": "零样本生成模型自适应（ZSGM）旨在仅使用文本指导，无需目标域的任何样本，将预训练生成器适应到目标域。近期ZSGM方法的核心是方向损失，该损失通过对齐图像偏移与视觉-语言模型（如CLIP）嵌入空间中的文本偏移来利用文本指导。这类似于自然语言处理中的类比推理，其中一对词之间的偏移用于通过对齐这两对之间的偏移来识别另一对中缺失的元素。然而，现有ZSGM方法的一个主要限制是，学习目标假设CLIP嵌入空间中图像偏移与文本偏移之间完全对齐，导致生成图像的质量下降。我们的工作做出了两项主要贡献。受自然语言处理中偏移未对齐研究的启发，作为我们的第一项贡献，我们进行了一项实证研究，分析了CLIP嵌入空间中各种大型公开数据集的文本偏移与图像偏移之间的未对齐。我们重要的发现是，CLIP嵌入空间中的偏移未对齐与概念距离相关，即概念越接近，偏移未对齐程度越小。为了解决当前方法的局限性，作为我们的第二项贡献，我们提出了“带迭代细化的自适应”（AIR），这是第一个专注于基于我们对偏移未对齐的新见解来提高目标域图像质量的ZSGM方法。在26种实验设置下的定性、定量和用户研究一致表明，所提出的AIR方法实现了SOTA性能。更多实验在补充材料中。", "summary": "本文针对零样本生成模型自适应（ZSGM）中现有方法因假设CLIP嵌入空间中图像和文本偏移完全对齐而导致的图像质量下降问题，提出了AIR方法。作者首先通过实证研究揭示了CLIP嵌入空间中偏移未对齐与概念距离相关，即概念越近，未对齐越小。基于此新见解，AIR是首个通过迭代细化来提高目标域图像质量的ZSGM方法。实验结果表明，AIR在多个设置下均取得了最先进的性能。", "keywords": "零样本生成模型自适应, CLIP, 迭代细化, 偏移未对齐, 图像生成", "comments": "该论文的创新点在于首次系统性地研究了CLIP嵌入空间中图像和文本偏移的未对齐问题，并将其与概念距离关联起来，为零样本生成模型自适应提供了新的理论基础。基于这一发现提出的AIR方法，通过迭代细化有效解决了现有方法的局限性，并实现了SOTA性能，这对于生成模型的零样本适应具有重要的实践意义。"}}
{"id": "2506.10959", "title": "Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods", "authors": ["Zhaiming Shen", "Alexander Hsu", "Rongjie Lai", "Wenjing Liao"], "summary": "While in-context learning (ICL) has achieved remarkable success in natural\nlanguage and vision domains, its theoretical understanding--particularly in the\ncontext of structured geometric data--remains unexplored. In this work, we\ninitiate a theoretical study of ICL for regression of H\\\"older functions on\nmanifolds. By establishing a novel connection between the attention mechanism\nand classical kernel methods, we derive generalization error bounds in terms of\nthe prompt length and the number of training tasks. When a sufficient number of\ntraining tasks are observed, transformers give rise to the minimax regression\nrate of H\\\"older functions on manifolds, which scales exponentially with the\nintrinsic dimension of the manifold, rather than the ambient space dimension.\nOur result also characterizes how the generalization error scales with the\nnumber of training tasks, shedding light on the complexity of transformers as\nin-context algorithm learners. Our findings provide foundational insights into\nthe role of geometry in ICL and novels tools to study ICL of nonlinear models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10959v1", "AI": {"title_translation": "理解结构化流形上的上下文学习：将注意力机制与核方法联系起来", "tldr": "本文对结构化几何数据上的上下文学习（ICL）进行了理论研究，通过连接注意力机制和核方法，推导了泛化误差界限，并表明在足够训练任务下，Transformer在流形上的H\"older函数回归中达到了极小极大回归率，且误差与流形的内在维度呈指数关系。", "motivation": "上下文学习（ICL）在自然语言和视觉领域取得了显著成功，但其理论理解，特别是在结构化几何数据背景下，仍未被探索。", "method": "通过建立注意力机制与经典核方法之间的新颖联系，推导了关于提示长度和训练任务数量的泛化误差界限。", "result": "当观察到足够数量的训练任务时，Transformer在流形上的H\"older函数回归中达到了极小极大回归率，该率与流形的内在维度呈指数关系，而非环境空间维度。研究结果还描述了泛化误差如何随训练任务数量的变化而变化。", "conclusion": "本研究为几何在上下文学习中的作用提供了基础性见解，并为研究非线性模型的上下文学习提供了新颖工具。", "translation": "尽管上下文学习（ICL）在自然语言和视觉领域取得了显著成功，但其理论理解——特别是在结构化几何数据背景下——仍未被探索。在这项工作中，我们对流形上H\"older函数的回归ICL进行了理论研究。通过建立注意力机制与经典核方法之间的新颖联系，我们推导了关于提示长度和训练任务数量的泛化误差界限。当观察到足够数量的训练任务时，Transformer在流形上的H\"older函数回归中达到了极小极大回归率，该率与流形的内在维度呈指数关系，而非环境空间维度。我们的结果还描述了泛化误差如何随训练任务数量的变化而变化，这揭示了Transformer作为上下文算法学习器的复杂性。我们的发现为几何在ICL中的作用提供了基础性见解，并为研究非线性模型的ICL提供了新颖工具。", "summary": "本研究首次对结构化流形上H\"older函数回归的上下文学习（ICL）进行了理论探讨。论文通过将注意力机制与经典核方法建立联系，推导了泛化误差界限，并证明在充足训练任务下，Transformer能达到流形上H\"older函数的极小极大回归率，其误差与流形的内在维度呈指数关系。这项工作深入理解了几何在ICL中的作用，并为非线性模型的ICL研究提供了新工具。", "keywords": "上下文学习, 结构化流形, 注意力机制, 核方法, 泛化误差", "comments": "该论文的创新点在于首次将注意力机制与经典核方法联系起来，从而为理解结构化流形上的上下文学习（ICL）提供了坚实的理论基础。它解决了ICL在几何数据上理论理解的空白，并通过量化泛化误差与内在维度和训练任务数量的关系，揭示了Transformer作为算法学习器的复杂性，对于推进ICL的理论发展具有重要意义。"}}
{"id": "2506.10915", "title": "M4V: Multi-Modal Mamba for Text-to-Video Generation", "authors": ["Jiancheng Huang", "Gengwei Zhang", "Zequn Jie", "Siyu Jiao", "Yinlong Qian", "Ling Chen", "Yunchao Wei", "Lin Ma"], "summary": "Text-to-video generation has significantly enriched content creation and\nholds the potential to evolve into powerful world simulators. However, modeling\nthe vast spatiotemporal space remains computationally demanding, particularly\nwhen employing Transformers, which incur quadratic complexity in sequence\nprocessing and thus limit practical applications. Recent advancements in\nlinear-time sequence modeling, particularly the Mamba architecture, offer a\nmore efficient alternative. Nevertheless, its plain design limits its direct\napplicability to multi-modal and spatiotemporal video generation tasks. To\naddress these challenges, we introduce M4V, a Multi-Modal Mamba framework for\ntext-to-video generation. Specifically, we propose a multi-modal diffusion\nMamba (MM-DiM) block that enables seamless integration of multi-modal\ninformation and spatiotemporal modeling through a multi-modal token\nre-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%\ncompared to the attention-based alternative when generating videos at\n768$\\times$1280 resolution. Additionally, to mitigate the visual quality\ndegradation in long-context autoregressive generation processes, we introduce a\nreward learning strategy that further enhances per-frame visual realism.\nExtensive experiments on text-to-video benchmarks demonstrate M4V's ability to\nproduce high-quality videos while significantly lowering computational costs.\nCode and models will be publicly available at\nhttps://huangjch526.github.io/M4V_project.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10915v1", "AI": {"title_translation": "M4V: 用于文本到视频生成的多模态Mamba", "tldr": "M4V是一个用于文本到视频生成的多模态Mamba框架，它通过引入MM-DiM块和奖励学习策略，显著降低了计算成本并提高了视频质量。", "motivation": "文本到视频生成在内容创作和世界模拟方面潜力巨大，但Transformer架构在处理大规模时空数据时计算成本高昂（二次复杂度），限制了实际应用。虽然Mamba架构提供了更高效的线性时间序列建模，但其简单设计不适用于多模态和时空视频生成任务。", "method": "本文提出了M4V，一个用于文本到视频生成的多模态Mamba框架。核心方法包括：1. 提出多模态扩散Mamba (MM-DiM) 块，通过多模态令牌重组设计，实现多模态信息和时空建模的无缝集成。2. 引入奖励学习策略，以减轻长上下文自回归生成过程中视觉质量下降的问题，增强每帧的视觉真实感。", "result": "M4V中的Mamba块在生成768×1280分辨率视频时，与基于注意力的替代方案相比，FLOPs减少了45%。此外，在文本到视频基准测试上，M4V能够生成高质量视频，并显著降低计算成本。", "conclusion": "M4V框架通过创新的多模态Mamba设计和奖励学习策略，有效解决了文本到视频生成中计算效率和视频质量的挑战，实现了高效率和高质量的视频生成。", "translation": "文本到视频生成极大地丰富了内容创作，并有望发展成为强大的世界模拟器。然而，建模广阔的时空空间仍然需要大量的计算，尤其是在使用Transformer时，其在序列处理中会产生二次复杂度，从而限制了实际应用。线性时间序列建模的最新进展，特别是Mamba架构，提供了一种更有效的替代方案。然而，其简单的设计限制了其直接应用于多模态和时空视频生成任务。为了解决这些挑战，我们引入了M4V，一个用于文本到视频生成的多模态Mamba框架。具体来说，我们提出了一个多模态扩散Mamba（MM-DiM）块，通过多模态令牌重组设计，实现了多模态信息和时空建模的无缝集成。因此，M4V中的Mamba块在生成768×1280分辨率视频时，与基于注意力的替代方案相比，FLOPs减少了45%。此外，为了减轻长上下文自回归生成过程中视觉质量下降的问题，我们引入了一种奖励学习策略，进一步增强了每帧的视觉真实感。在文本到视频基准测试上进行的大量实验表明，M4V能够生成高质量视频，同时显著降低计算成本。代码和模型将公开在https://huangjch526.github.io/M4V_project。", "summary": "本文提出了M4V，一个用于文本到视频生成的多模态Mamba框架，旨在解决传统Transformer在处理长时空序列时计算量大的问题。M4V引入了多模态扩散Mamba (MM-DiM) 块，通过独特的令牌重组设计，有效地融合多模态信息并进行时空建模。此外，为了提升长上下文生成中的视觉质量，M4V还采用了奖励学习策略。实验证明，M4V在降低计算成本（FLOPs减少45%）的同时，能生成高质量的视频，展现了Mamba架构在多模态视频生成领域的潜力。", "keywords": "文本到视频生成, Mamba, 多模态, 计算效率, 视频质量", "comments": "这篇论文的创新点在于将Mamba架构引入到多模态文本到视频生成领域，并针对其局限性提出了MM-DiM块和奖励学习策略。通过解决Mamba在多模态和时空建模上的不足，M4V显著提升了计算效率并保证了生成视频的质量，对未来的高效视频生成模型发展具有重要意义。"}}
{"id": "2506.10972", "title": "Farseer: A Refined Scaling Law in Large Language Models", "authors": ["Houyi Li", "Wenzhen Zheng", "Qiufeng Wang", "Zhenyu Ding", "Haoying Wang", "Zili Wang", "Shijie Xuyang", "Ning Ding", "Shuigeng Zhou", "Xiangyu Zhang", "Daxin Jiang"], "summary": "Training Large Language Models (LLMs) is prohibitively expensive, creating a\ncritical scaling gap where insights from small-scale experiments often fail to\ntransfer to resource-intensive production systems, thereby hindering efficient\ninnovation. To bridge this, we introduce Farseer, a novel and refined scaling\nlaw offering enhanced predictive accuracy across scales. By systematically\nconstructing a model loss surface $L(N,D)$, Farseer achieves a significantly\nbetter fit to empirical data than prior laws (e.g., Chinchilla's law). Our\nmethodology yields accurate, robust, and highly generalizable predictions,\ndemonstrating excellent extrapolation capabilities, improving upon Chinchilla's\nlaw by reducing extrapolation error by 433\\%. This allows for the reliable\nevaluation of competing training strategies across all $(N,D)$ settings,\nenabling conclusions from small-scale ablation studies to be confidently\nextrapolated to predict large-scale performance. Furthermore, Farseer provides\nnew insights into optimal compute allocation, better reflecting the nuanced\ndemands of modern LLM training. To validate our approach, we trained an\nextensive suite of approximately 1,000 LLMs across diverse scales and\nconfigurations, consuming roughly 3 million NVIDIA H100 GPU hours. We are\ncomprehensively open-sourcing all models, data, results, and logs at\nhttps://github.com/Farseer-Scaling-Law/Farseer to foster further research.", "comment": "34", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10972v1", "AI": {"title_translation": "Farseer：大型语言模型中一种改进的缩放定律", "tldr": "Farseer提出了一种改进的LLM缩放定律，通过系统构建模型损失曲面，显著提高了预测精度和泛化能力，并减少了外推误差，从而能更可靠地评估训练策略和优化计算分配。", "motivation": "训练大型语言模型（LLMs）成本高昂，导致小规模实验的洞察力难以应用于大规模生产系统，阻碍了创新。Farseer旨在弥补这一关键的缩放差距。", "method": "Farseer通过系统地构建模型损失曲面$L(N,D)$来推导其缩放定律。为了验证该方法，研究人员训练了大约1000个不同规模和配置的LLM，消耗了约300万NVIDIA H100 GPU小时，并开源了所有相关数据。", "result": "Farseer比现有定律（如Chinchilla定律）更符合经验数据，将外推误差减少了433%。它能实现准确、鲁棒且高度泛化的预测，并提供关于最佳计算分配的新见解，更好地反映现代LLM训练的需求。", "conclusion": "Farseer通过提供一种更准确、更可靠的缩放定律，使得小规模实验的结论可以自信地外推到大规模性能预测，从而促进LLM的有效创新和训练优化。", "translation": "训练大型语言模型（LLMs）成本高得令人望而却步，造成了一个关键的缩放差距，即小规模实验的见解往往无法转移到资源密集型生产系统，从而阻碍了高效创新。为了弥合这一差距，我们引入了Farseer，一种新颖且改进的缩放定律，可在不同规模下提供更高的预测精度。通过系统地构建模型损失曲面$L(N,D)$，Farseer比先前的定律（例如Chinchilla定律）能更好地拟合经验数据。我们的方法论产生了准确、鲁棒且高度泛化的预测，展现出卓越的外推能力，将Chinchilla定律的外推误差减少了433%。这使得在所有$(N,D)$设置下都能可靠地评估竞争性训练策略，从而能够自信地将小规模消融研究的结论外推以预测大规模性能。此外，Farseer为最佳计算分配提供了新见解，更好地反映了现代LLM训练的细微需求。为了验证我们的方法，我们训练了大约1000个不同规模和配置的LLM，消耗了大约300万NVIDIA H100 GPU小时。我们正在全面开源所有模型、数据、结果和日志（https://github.com/Farseer-Scaling-Law/Farseer），以促进进一步研究。", "summary": "Farseer提出了一种改进的大型语言模型（LLM）缩放定律，旨在解决小规模实验难以推广到大规模生产的挑战。通过系统构建模型损失曲面$L(N,D)$，Farseer显著提高了预测精度和外推能力，相较于现有定律（如Chinchilla），外推误差降低了433%。这使得研究人员能够更可靠地评估训练策略，并优化计算资源分配，从而促进LLM的有效创新。该研究通过训练大量模型并开源所有数据来验证其方法。", "keywords": "大型语言模型, 缩放定律, 预测精度, 计算分配, Farseer", "comments": "Farseer的创新之处在于提出了一种更精细的缩放定律，通过构建损失曲面显著提升了对LLM性能的预测精度和外推能力。其重要性在于能够降低LLM训练的试错成本，使小规模实验结果更具指导意义，从而加速LLM领域的创新。通过减少433%的外推误差，该研究为LLM的资源分配和训练策略优化提供了强大的工具，并且全面开源数据和模型，有利于后续研究。"}}
{"id": "2506.10941", "title": "VINCIE: Unlocking In-context Image Editing from Video", "authors": ["Leigang Qu", "Feng Cheng", "Ziyan Yang", "Qi Zhao", "Shanchuan Lin", "Yichun Shi", "Yicong Li", "Wenjie Wang", "Tat-Seng Chua", "Lu Jiang"], "summary": "In-context image editing aims to modify images based on a contextual sequence\ncomprising text and previously generated images. Existing methods typically\ndepend on task-specific pipelines and expert models (e.g., segmentation and\ninpainting) to curate training data. In this work, we explore whether an\nin-context image editing model can be learned directly from videos. We\nintroduce a scalable approach to annotate videos as interleaved multimodal\nsequences. To effectively learn from this data, we design a block-causal\ndiffusion transformer trained on three proxy tasks: next-image prediction,\ncurrent segmentation prediction, and next-segmentation prediction.\nAdditionally, we propose a novel multi-turn image editing benchmark to advance\nresearch in this area. Extensive experiments demonstrate that our model\nexhibits strong in-context image editing capabilities and achieves\nstate-of-the-art results on two multi-turn image editing benchmarks. Despite\nbeing trained exclusively on videos, our model also shows promising abilities\nin multi-concept composition, story generation, and chain-of-editing\napplications.", "comment": "Project page: https://vincie2025.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10941v1", "AI": {"title_translation": "VINCIE：从视频中解锁上下文图像编辑", "tldr": "VINCIE提出了一种从视频中直接学习上下文图像编辑模型的方法，通过块因果扩散Transformer在代理任务上训练，并在多轮图像编辑基准上取得了最先进的成果。", "motivation": "现有的上下文图像编辑方法通常依赖于特定任务的流水线和专家模型来策划训练数据，本研究旨在探索是否可以直接从视频中学习上下文图像编辑模型。", "method": "本文提出了一种可扩展的方法来将视频标注为交错的多模态序列。为了有效地从这些数据中学习，设计了一个块因果扩散Transformer，并在三个代理任务上进行训练：下一图像预测、当前分割预测和下一分割预测。此外，还提出了一个新的多轮图像编辑基准。", "result": "模型展现出强大的上下文图像编辑能力，并在两个多轮图像编辑基准上取得了最先进的成果。尽管只在视频上训练，模型还在多概念合成、故事生成和编辑链应用中展现出有前景的能力。", "conclusion": "通过直接从视频中学习，VINCIE模型能够有效地进行上下文图像编辑，并在多个相关任务中展现出强大的泛化能力，为该领域的研究提供了新的方向和基准。", "translation": "上下文图像编辑旨在根据包含文本和先前生成图像的上下文序列来修改图像。现有方法通常依赖于特定任务的流水线和专家模型（例如，分割和图像修复）来策划训练数据。在这项工作中，我们探索是否可以直接从视频中学习上下文图像编辑模型。我们引入了一种可扩展的方法来将视频标注为交错的多模态序列。为了有效地从这些数据中学习，我们设计了一个块因果扩散Transformer，并在三个代理任务上进行训练：下一图像预测、当前分割预测和下一分割预测。此外，我们提出了一个新的多轮图像编辑基准，以推动该领域的研究。大量的实验表明，我们的模型展现出强大的上下文图像编辑能力，并在两个多轮图像编辑基准上取得了最先进的成果。尽管只在视频上训练，我们的模型还在多概念合成、故事生成和编辑链应用中展现出有前景的能力。", "summary": "VINCIE提出了一种新颖的上下文图像编辑方法，通过直接从视频数据中学习，克服了现有方法对特定任务流水线和专家模型的依赖。该方法引入了将视频标注为交错多模态序列的方案，并设计了一个块因果扩散Transformer，在图像和分割预测等代理任务上进行训练。实验证明，VINCIE在多轮图像编辑基准上实现了最先进的性能，并展现了在多概念合成和故事生成等方面的潜力。", "keywords": "上下文图像编辑, 视频学习, 扩散模型, 多模态, 图像生成", "comments": "VINCIE的创新之处在于直接从视频中学习上下文图像编辑，这规避了传统方法对繁琐的专家模型和训练数据策划的依赖，提供了一个更具可扩展性和通用性的解决方案。其在多轮编辑和多模态应用中的表现预示着该领域未来发展的广阔前景。"}}
{"id": "2506.10962", "title": "SpectralAR: Spectral Autoregressive Visual Generation", "authors": ["Yuanhui Huang", "Weiliang Chen", "Wenzhao Zheng", "Yueqi Duan", "Jie Zhou", "Jiwen Lu"], "summary": "Autoregressive visual generation has garnered increasing attention due to its\nscalability and compatibility with other modalities compared with diffusion\nmodels. Most existing methods construct visual sequences as spatial patches for\nautoregressive generation. However, image patches are inherently parallel,\ncontradicting the causal nature of autoregressive modeling. To address this, we\npropose a Spectral AutoRegressive (SpectralAR) visual generation framework,\nwhich realizes causality for visual sequences from the spectral perspective.\nSpecifically, we first transform an image into ordered spectral tokens with\nNested Spectral Tokenization, representing lower to higher frequency\ncomponents. We then perform autoregressive generation in a coarse-to-fine\nmanner with the sequences of spectral tokens. By considering different levels\nof detail in images, our SpectralAR achieves both sequence causality and token\nefficiency without bells and whistles. We conduct extensive experiments on\nImageNet-1K for image reconstruction and autoregressive generation, and\nSpectralAR achieves 3.02 gFID with only 64 tokens and 310M parameters. Project\npage: https://huang-yh.github.io/spectralar/.", "comment": "Project Page: https://huang-yh.github.io/spectralar/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10962v1", "AI": {"title_translation": "SpectralAR：频谱自回归视觉生成", "tldr": "SpectralAR提出了一种新的频谱自回归视觉生成框架，通过将图像转换为有序的频谱token来解决现有自回归模型中空间补丁缺乏因果性的问题，实现了因果性和token效率。", "motivation": "现有的自回归视觉生成方法大多将视觉序列构建为空间补丁，但这与自回归建模的因果性本质相矛盾，因此需要一种能实现视觉序列因果性的新方法。", "method": "提出Spectral AutoRegressive (SpectralAR) 视觉生成框架。首先，通过嵌套频谱分词（Nested Spectral Tokenization）将图像转换为有序的频谱token，这些token代表从低频到高频的分量。然后，利用这些频谱token序列以从粗到细的方式进行自回归生成。", "result": "在ImageNet-1K上进行了图像重建和自回归生成实验。SpectralAR在仅使用64个token和3.1亿参数的情况下，实现了3.02 gFID。", "conclusion": "通过从频谱角度实现视觉序列的因果性，SpectralAR在没有额外复杂性的情况下，同时实现了序列因果性和token效率，并在视觉生成任务上取得了有竞争力的性能。", "translation": "自回归视觉生成因其可扩展性以及与扩散模型相比与其他模态的兼容性而受到越来越多的关注。现有的大多数方法将视觉序列构建为空间补丁以进行自回归生成。然而，图像补丁本质上是并行的，这与自回归建模的因果性质相矛盾。为了解决这个问题，我们提出了一个频谱自回归（SpectralAR）视觉生成框架，该框架从频谱角度实现了视觉序列的因果性。具体来说，我们首先通过嵌套频谱分词将图像转换为有序的频谱token，代表从低频到高频的分量。然后，我们利用频谱token序列以从粗到细的方式进行自回归生成。通过考虑图像中不同级别的细节，我们的SpectralAR在没有额外复杂性的情况下，同时实现了序列因果性和token效率。我们在ImageNet-1K上进行了广泛的图像重建和自回归生成实验，SpectralAR在仅使用64个token和3.1亿参数的情况下，实现了3.02 gFID。项目页面：https://huang-yh.github.io/spectralar/。", "summary": "本文提出了SpectralAR，一个用于视觉生成的频谱自回归框架，旨在解决传统基于空间补丁的自回归模型中因果性不足的问题。SpectralAR通过嵌套频谱分词将图像转换为有序的频谱token，这些token按频率分量排序，并以从粗到细的方式进行自回归生成。该方法有效地实现了序列因果性和token效率，并在ImageNet-1K数据集上展示了优异的性能，以较少的token和参数量获得了竞争力强的gFID分数。", "keywords": "自回归生成, 频谱分词, 视觉生成, 因果建模, SpectralAR", "comments": "该论文的创新点在于将自回归视觉生成从传统的空间域转向频谱域，巧妙地利用了频谱分量的自然顺序性来解决空间补丁并行性与自回归因果性之间的矛盾。这种方法不仅提升了模型的因果建模能力，还带来了token效率的提升，为高效的自回归视觉生成提供了新的视角和解决方案。"}}
{"id": "2506.10982", "title": "Rethinking Losses for Diffusion Bridge Samplers", "authors": ["Sebastian Sanokowski", "Lukas Gruber", "Christoph Bartmann", "Sepp Hochreiter", "Sebastian Lehner"], "summary": "Diffusion bridges are a promising class of deep-learning methods for sampling\nfrom unnormalized distributions. Recent works show that the Log Variance (LV)\nloss consistently outperforms the reverse Kullback-Leibler (rKL) loss when\nusing the reparametrization trick to compute rKL-gradients. While the on-policy\nLV loss yields identical gradients to the rKL loss when combined with the\nlog-derivative trick for diffusion samplers with non-learnable forward\nprocesses, this equivalence does not hold for diffusion bridges or when\ndiffusion coefficients are learned. Based on this insight we argue that for\ndiffusion bridges the LV loss does not represent an optimization objective that\ncan be motivated like the rKL loss via the data processing inequality. Our\nanalysis shows that employing the rKL loss with the log-derivative trick\n(rKL-LD) does not only avoid these conceptual problems but also consistently\noutperforms the LV loss. Experimental results with different types of diffusion\nbridges on challenging benchmarks show that samplers trained with the rKL-LD\nloss achieve better performance. From a practical perspective we find that\nrKL-LD requires significantly less hyperparameter optimization and yields more\nstable training behavior.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.10982v1", "AI": {"title_translation": "重新思考扩散桥采样器的损失函数", "tldr": "本文重新评估了扩散桥采样器的损失函数，发现结合对数导数技巧的rKL损失（rKL-LD）在理论上更合理，并且在实践中一致优于Log Variance (LV) 损失。", "motivation": "现有的研究表明Log Variance (LV) 损失在扩散桥采样器中表现优于逆Kullback-Leibler (rKL) 损失，尤其是在使用重参数化技巧计算rKL梯度时。然而，本文指出，虽然在某些情况下LV损失与rKL损失的梯度等效，但这种等效性不适用于扩散桥或学习扩散系数的情况。基于此，作者认为LV损失对于扩散桥而言缺乏如rKL损失那样通过数据处理不等式所能获得的理论支持，因此需要重新审视并提出更优的损失函数。", "method": "本文通过理论分析，指出Log Variance (LV) 损失在扩散桥或学习扩散系数时与逆Kullback-Leibler (rKL) 损失的梯度不等效，并且LV损失缺乏理论支持。作者提出并分析了结合对数导数技巧的rKL损失（rKL-LD），认为它不仅避免了概念性问题，而且在性能上优于LV损失。通过在不同类型的扩散桥和具有挑战性的基准上进行实验验证，比较了rKL-LD损失和LV损失训练的采样器性能。", "result": "分析表明，使用结合对数导数技巧的rKL损失（rKL-LD）不仅避免了LV损失的概念性问题，而且在实践中一致优于LV损失。实验结果显示，在具有挑战性的基准测试中，使用rKL-LD损失训练的采样器在不同类型的扩散桥上取得了更好的性能。从实际角度来看，rKL-LD需要的超参数优化显著减少，并且训练行为更稳定。", "conclusion": "对于扩散桥采样器，结合对数导数技巧的rKL损失（rKL-LD）在理论上更具合理性，并且在实验中表现出比Log Variance (LV) 损失更优异的性能、更少的超参数优化需求和更稳定的训练行为，因此是更推荐的选择。", "translation": "扩散桥是一类很有前途的深度学习方法，用于从非归一化分布中进行采样。最近的研究表明，当使用重参数化技巧计算rKL梯度时，对数方差（LV）损失始终优于逆Kullback-Leibler（rKL）损失。虽然对于具有不可学习前向过程的扩散采样器，当与对数导数技巧结合使用时，on-policy的LV损失会产生与rKL损失相同的梯度，但这种等效性不适用于扩散桥或学习扩散系数的情况。基于这一见解，我们认为对于扩散桥，LV损失不能代表一个可以通过数据处理不等式像rKL损失那样获得理论支持的优化目标。我们的分析表明，采用对数导数技巧的rKL损失（rKL-LD）不仅避免了这些概念性问题，而且始终优于LV损失。在具有挑战性的基准上使用不同类型扩散桥的实验结果表明，使用rKL-LD损失训练的采样器实现了更好的性能。从实际角度来看，我们发现rKL-LD需要的超参数优化显著减少，并且产生了更稳定的训练行为。", "summary": "本文重新评估了扩散桥采样器的损失函数。尽管Log Variance (LV) 损失在现有研究中表现出色，作者指出其在扩散桥或学习扩散系数时缺乏理论基础。通过深入分析，本文提出并验证了结合对数导数技巧的逆Kullback-Leibler (rKL-LD) 损失。研究表明，rKL-LD不仅解决了LV损失的概念性问题，而且在多种扩散桥和基准测试中表现出更优异的性能，同时需要更少的超参数优化，并提供更稳定的训练过程。", "keywords": "扩散桥, 损失函数, rKL-LD, LV损失, 采样器", "comments": "本文的创新之处在于对扩散桥采样器中损失函数的理论基础进行了深入的重新审视。它挑战了当前流行的Log Variance (LV) 损失的普适性，并从理论和实验两方面论证了结合对数导数技巧的rKL损失（rKL-LD）的优越性。这对于扩散模型，特别是扩散桥领域的研究具有重要意义，因为它为选择更稳健、更具理论支持的损失函数提供了明确的指导，有助于提升采样器的性能和训练稳定性。"}}
{"id": "2506.10963", "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "authors": ["Yuxuan Luo", "Yuhui Yuan", "Junwen Chen", "Haonan Cai", "Ziyi Yue", "Yuwei Yang", "Fatima Zohra Daha", "Ji Li", "Zhouhui Lian"], "summary": "In this paper, we introduce knowledge image generation as a new task,\nalongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation\nBenchmark (MMMG) to probe the reasoning capability of image generation models.\nKnowledge images have been central to human civilization and to the mechanisms\nof human learning--a fact underscored by dual-coding theory and the\npicture-superiority effect. Generating such images is challenging, demanding\nmultimodal reasoning that fuses world knowledge with pixel-level grounding into\nclear explanatory visuals. To enable comprehensive evaluation, MMMG offers\n4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,\n6 educational levels, and diverse knowledge formats such as charts, diagrams,\nand mind maps. To eliminate confounding complexity during evaluation, we adopt\na unified Knowledge Graph (KG) representation. Each KG explicitly delineates a\ntarget image's core entities and their dependencies. We further introduce\nMMMG-Score to evaluate generated knowledge images. This metric combines factual\nfidelity, measured by graph-edit distance between KGs, with visual clarity\nassessment. Comprehensive evaluations of 16 state-of-the-art text-to-image\ngeneration models expose serious reasoning deficits--low entity fidelity, weak\nrelations, and clutter--with GPT-4o achieving an MMMG-Score of only 50.20,\nunderscoring the benchmark's difficulty. To spur further progress, we release\nFLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines\na reasoning LLM with diffusion models and is trained on 16,000 curated\nknowledge image-prompt pairs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10963v1", "AI": {"title_translation": "MMMG：一个用于文本到图像推理的大规模、多学科、多层级生成基准", "tldr": "本文引入了知识图像生成任务和MMMG基准，以评估图像生成模型的推理能力。研究发现现有模型表现不佳，并提出了一个新的评估指标MMMG-Score和基线模型FLUX-Reason。", "motivation": "图像生成模型在生成解释性视觉内容（即知识图像）方面存在推理能力不足的问题。鉴于知识图像对人类文明和学习的重要性，需要一个大规模、多学科的基准来全面评估和促进这一领域的发展。", "method": "1. 引入“知识图像生成”新任务。2. 构建MMMG基准，包含4,456个专家验证的图像-提示对，涵盖10个学科、6个教育级别和多种知识格式。3. 采用统一的知识图谱（KG）表示来明确目标图像的核心实体及其依赖关系。4. 提出MMMG-Score评估指标，结合知识图谱间的图编辑距离衡量事实保真度，并评估视觉清晰度。5. 评估16个最先进的文本到图像生成模型。6. 发布FLUX-Reason作为开放基线，结合推理LLM和扩散模型，并在16,000个策展的知识图像-提示对上进行训练。", "result": "对16个最先进的文本到图像生成模型的全面评估揭示了严重的推理缺陷，包括实体保真度低、关系薄弱和视觉混乱。即使是GPT-4o也仅获得50.20的MMMG-Score，这凸显了该基准的难度。FLUX-Reason基线模型实现了34.45的MMMG-Score。", "conclusion": "现有的文本到图像生成模型在知识图像生成任务中表现出显著的推理能力不足，MMMG基准及其MMMG-Score有效揭示了这些缺陷。该研究为未来改进模型在生成知识图像方面的推理能力提供了挑战和有用的基线。", "translation": "在本文中，我们引入了知识图像生成作为一项新任务，并提出了大规模、多学科、多层级知识图像生成基准（MMMG）来探究图像生成模型的推理能力。知识图像在人类文明和学习机制中一直占据核心地位——双重编码理论和图像优势效应都强调了这一事实。生成此类图像极具挑战性，需要多模态推理，将世界知识与像素级基础融合为清晰的解释性视觉内容。为了实现全面评估，MMMG提供了4,456对经过专家验证的（知识）图像-提示对，涵盖10个学科、6个教育级别以及图表、图示和思维导图等多种知识格式。为了消除评估过程中的混杂复杂性，我们采用了统一的知识图谱（KG）表示。每个知识图谱都明确描绘了目标图像的核心实体及其依赖关系。我们进一步引入了MMMG-Score来评估生成的知识图像。该指标结合了通过知识图谱间的图编辑距离衡量的“事实保真度”和“视觉清晰度评估”。对16个最先进的文本到图像生成模型的全面评估揭示了严重的推理缺陷——实体保真度低、关系薄弱和视觉混乱——其中GPT-4o的MMMG-Score仅为50.20，突显了该基准的难度。为了促进进一步的进展，我们发布了FLUX-Reason（MMMG-Score为34.45），这是一个有效且开放的基线，它将推理LLM与扩散模型相结合，并在16,000对精选知识图像-提示对上进行训练。", "summary": "本文提出了一项新的“知识图像生成”任务和一个名为MMMG的大规模、多学科、多层级基准，旨在评估文本到图像模型的推理能力。MMMG包含4,456个专家验证的图像-提示对，涵盖多领域和教育级别，并采用统一的知识图谱表示。为评估，引入了结合事实保真度和视觉清晰度的MMMG-Score。对16个SOTA模型的评估显示，它们在生成知识图像方面存在严重的推理缺陷（如GPT-4o仅得50.20分）。为促进研究，作者发布了名为FLUX-Reason的开放基线。", "keywords": "知识图像生成, 文本到图像, 推理能力, 基准测试, 知识图谱", "comments": "这篇论文通过引入“知识图像生成”这一新颖且重要的任务，以及一个大规模、多学科的MMMG基准，显著推动了文本到图像生成领域对高级推理能力的评估。其创新点在于将知识图谱引入图像生成评估，并提出了结合事实保真度和视觉清晰度的新颖MMMG-Score，这能更细致地揭示现有模型的推理缺陷。该工作强调了当前最先进模型在理解和可视化复杂知识方面的局限性，并提供了一个有价值的开放基线，有望激发未来在该方向的深入研究。"}}
{"id": "2406.15669", "title": "CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes", "authors": ["Jason Yang", "Ariane Mora", "Shengchao Liu", "Bruce J. Wittmann", "Anima Anandkumar", "Frances H. Arnold", "Yisong Yue"], "summary": "Enzymes are important proteins that catalyze chemical reactions. In recent\nyears, machine learning methods have emerged to predict enzyme function from\nsequence; however, there are no standardized benchmarks to evaluate these\nmethods. We introduce CARE, a benchmark and dataset suite for the\nClassification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1)\nclassification of a protein sequence by its enzyme commission (EC) number and\n(2) retrieval of an EC number given a chemical reaction. For each task, we\ndesign train-test splits to evaluate different kinds of out-of-distribution\ngeneralization that are relevant to real use cases. For the classification\ntask, we provide baselines for state-of-the-art methods. Because the retrieval\ntask has not been previously formalized, we propose a method called Contrastive\nReaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task\nand compare it to the recent method, CLIPZyme. CARE is available at\nhttps://github.com/jsunn-y/CARE/.", "comment": null, "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2406.15669v3", "AI": {"title_translation": "CARE：一个用于酶分类和检索的基准套件", "tldr": "引入了CARE，一个用于评估酶分类和检索机器学习方法的标准化基准数据集。", "motivation": "现有机器学习方法能够从序列预测酶功能，但缺乏标准化基准来评估这些方法。", "method": "引入了CARE基准套件，包含酶分类和酶检索两个任务。为每个任务设计了训练-测试划分以评估分布外泛化能力。为分类任务提供了SOTA方法的基线，并针对首次形式化的检索任务提出了CREEP方法作为基线，并与CLIPZyme进行了比较。", "result": "CARE基准套件已发布，并为酶分类任务提供了SOTA方法的基线。首次形式化了酶检索任务，并提出了CREEP作为其首批基线之一，并与CLIPZyme进行了比较。", "conclusion": "CARE提供了一个急需的标准化基准和数据集套件，用于评估酶分类和检索的机器学习方法，填补了该领域的一个空白。", "translation": "酶是催化化学反应的重要蛋白质。近年来，机器学习方法已出现，能够从序列预测酶功能；然而，目前还没有标准化的基准来评估这些方法。我们引入了CARE，一个用于酶分类和检索（Classification And Retrieval of Enzymes, CARE）的基准和数据集套件。CARE围绕两个任务展开：(1) 通过酶学委员会（EC）编号对蛋白质序列进行分类，以及 (2) 给定化学反应检索EC编号。对于每个任务，我们设计了训练-测试划分，以评估与实际用例相关的不同类型的分布外泛化能力。对于分类任务，我们提供了最先进方法的基线。由于检索任务此前尚未形式化，我们提出了一种名为对比反应-酶预训练（Contrastive Reaction-EnzymE Pretraining, CREEP）的方法，作为该任务的首批基线之一，并将其与近期方法CLIPZyme进行了比较。CARE可在 https://github.com/jsunn-y/CARE/ 获取。", "summary": "本文介绍了CARE，一个用于酶分类和检索的标准化基准和数据集套件，旨在解决当前机器学习方法在酶功能预测评估中缺乏统一标准的问题。CARE包含通过EC编号进行蛋白质序列分类和给定化学反应检索EC编号两个核心任务，并设计了旨在评估分布外泛化能力的训练-测试划分。作者为分类任务提供了现有SOTA方法的基线，并针对首次形式化的检索任务提出了CREEP方法作为初步基线，并与CLIPZyme进行了比较。CARE基准已开源。", "keywords": "酶分类, 酶检索, 基准数据集, 机器学习, 生物信息学", "comments": "CARE通过提供一个标准化基准，解决了酶功能预测领域评估方法缺失的关键问题，具有重要意义。它不仅涵盖了传统的分类任务，还创新性地形式化了酶检索任务，并提出了新的基线方法，推动了该领域的研究进展。"}}
{"id": "2506.10967", "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs", "authors": ["Qizhe Zhang", "Mengzhen Liu", "Lichen Li", "Ming Lu", "Yuan Zhang", "Junwen Pan", "Qi She", "Shanghang Zhang"], "summary": "In multimodal large language models (MLLMs), the length of input visual\ntokens is often significantly greater than that of their textual counterparts,\nleading to a high inference cost. Many works aim to address this issue by\nremoving redundant visual tokens. However, current approaches either rely on\nattention-based pruning, which retains numerous duplicate tokens, or use\nsimilarity-based pruning, overlooking the instruction relevance, consequently\ncausing suboptimal performance. In this paper, we go beyond attention or\nsimilarity by proposing a novel visual token pruning method named CDPruner,\nwhich maximizes the conditional diversity of retained tokens. We first define\nthe conditional similarity between visual tokens conditioned on the\ninstruction, and then reformulate the token pruning problem with determinantal\npoint process (DPP) to maximize the conditional diversity of the selected\nsubset. The proposed CDPruner is training-free and model-agnostic, allowing\neasy application to various MLLMs. Extensive experiments across diverse MLLMs\nshow that CDPruner establishes new state-of-the-art on various vision-language\nbenchmarks. By maximizing conditional diversity through DPP, the selected\nsubset better represents the input images while closely adhering to user\ninstructions, thereby preserving strong performance even with high reduction\nratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency\nby 78\\%, while maintaining 94\\% of the original accuracy. Our code is available\nat https://github.com/Theia-4869/CDPruner.", "comment": "22 pages, 5 figures, code: https://github.com/Theia-4869/CDPruner,\n  project page: https://theia-4869.github.io/CDPruner", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10967v1", "AI": {"title_translation": "超越注意力或相似性：在多模态大语言模型中最大化条件多样性以进行令牌剪枝", "tldr": "提出CDPruner，一种训练无关且模型无关的视觉令牌剪枝方法，通过最大化条件多样性（基于DPP）在MLLMs中实现SOTA性能，显著降低计算成本并保持高准确率。", "motivation": "多模态大语言模型（MLLMs）中视觉令牌输入长度远超文本令牌，导致高推理成本。现有剪枝方法（基于注意力或相似性）存在冗余或忽略指令相关性，导致性能不佳。", "method": "提出CDPruner，通过定义基于指令的视觉令牌条件相似性，并利用行列式点过程（DPP）将令牌剪枝问题重新表述为最大化所选子集的条件多样性。该方法是训练无关和模型无关的。", "result": "CDPruner在各种MLLMs上的广泛实验表明，它在多个视觉-语言基准测试中达到了新的SOTA。通过DPP最大化条件多样性，即使在高削减率下也能保持强大的性能。应用于LLaVA时，FLOPs减少95%，CUDA延迟减少78%，同时保持94%的原始准确率。", "conclusion": "CDPruner通过最大化条件多样性有效解决了MLLMs中的视觉令牌冗余问题，显著降低了推理成本，同时保持了高准确性，并具有广泛适用性。", "translation": "在多模态大语言模型（MLLMs）中，输入视觉令牌的长度通常远大于其文本对应物，导致高昂的推理成本。许多工作旨在通过移除冗余视觉令牌来解决这个问题。然而，现有方法要么依赖于基于注意力的剪枝，保留了大量重复令牌，要么使用基于相似性的剪枝，忽略了指令相关性，从而导致次优性能。在本文中，我们超越了注意力或相似性，提出了一种名为CDPruner的新型视觉令牌剪枝方法，该方法最大化了保留令牌的条件多样性。我们首先定义了以指令为条件的视觉令牌之间的条件相似性，然后利用行列式点过程（DPP）重新表述了令牌剪枝问题，以最大化所选子集的条件多样性。所提出的CDPruner是训练无关且模型无关的，可以轻松应用于各种MLLMs。对各种MLLMs进行的广泛实验表明，CDPruner在各种视觉-语言基准测试上建立了新的最先进水平。通过DPP最大化条件多样性，所选子集能更好地代表输入图像，同时严格遵守用户指令，从而即使在高削减率下也能保持强大的性能。当应用于LLaVA时，CDPruner将FLOPs减少95%，CUDA延迟减少78%，同时保持94%的原始准确率。我们的代码可在https://github.com/Theia-4869/CDPruner获取。", "summary": "本文提出CDPruner，一种新颖的视觉令牌剪枝方法，用于解决多模态大语言模型（MLLMs）中视觉令牌冗余导致的高推理成本问题。该方法通过定义基于指令的条件相似性，并利用行列式点过程（DPP）最大化保留令牌的条件多样性。CDPruner是训练无关且模型无关的，在多个视觉-语言基准测试上实现了SOTA性能，显著降低了计算量和延迟，同时保持了高准确率。", "keywords": "多模态大语言模型, 令牌剪枝, 条件多样性, 行列式点过程, 推理成本", "comments": "这篇论文的创新点在于提出了“条件多样性”的概念，并巧妙地将行列式点过程（DPP）引入到视觉令牌剪枝问题中，以解决现有方法中冗余和指令不相关的问题。其“训练无关”和“模型无关”的特性极大地增强了方法的普适性和实用性，对降低MLLMs的推理成本具有重要意义。"}}
{"id": "2506.10975", "title": "GenWorld: Towards Detecting AI-generated Real-world Simulation Videos", "authors": ["Weiliang Chen", "Wenzhao Zheng", "Yu Zheng", "Lei Chen", "Jie Zhou", "Jiwen Lu", "Yueqi Duan"], "summary": "The flourishing of video generation technologies has endangered the\ncredibility of real-world information and intensified the demand for\nAI-generated video detectors. Despite some progress, the lack of high-quality\nreal-world datasets hinders the development of trustworthy detectors. In this\npaper, we propose GenWorld, a large-scale, high-quality, and real-world\nsimulation dataset for AI-generated video detection. GenWorld features the\nfollowing characteristics: (1) Real-world Simulation: GenWorld focuses on\nvideos that replicate real-world scenarios, which have a significant impact due\nto their realism and potential influence; (2) High Quality: GenWorld employs\nmultiple state-of-the-art video generation models to provide realistic and\nhigh-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes\nvideos generated from diverse generators and various prompt modalities (e.g.,\ntext, image, video), offering the potential to learn more generalizable\nforensic features. We analyze existing methods and find they fail to detect\nhigh-quality videos generated by world models (i.e., Cosmos), revealing\npotential drawbacks of ignoring real-world clues. To address this, we propose a\nsimple yet effective model, SpannDetector, to leverage multi-view consistency\nas a strong criterion for real-world AI-generated video detection. Experiments\nshow that our method achieves superior results, highlighting a promising\ndirection for explainable AI-generated video detection based on physical\nplausibility. We believe that GenWorld will advance the field of AI-generated\nvideo detection. Project Page: https://chen-wl20.github.io/GenWorld", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10975v1", "AI": {"title_translation": "GenWorld：迈向检测AI生成真实世界模拟视频", "tldr": "本文提出了GenWorld，一个用于AI生成视频检测的大规模、高质量、真实世界模拟数据集，并引入了SpannDetector模型，利用多视角一致性有效检测AI生成视频，填补了现有方法在检测高真实度视频方面的空白。", "motivation": "视频生成技术的蓬勃发展威胁了现实世界信息的真实性，加剧了对AI生成视频检测器的需求。然而，高质量真实世界数据集的缺乏阻碍了可靠检测器的开发。", "method": "本文提出了GenWorld数据集，其特点是：1) 真实世界模拟，专注于复制现实场景的视频；2) 高质量，采用多种最先进的视频生成模型来提供逼真和高质量的伪造视频；3) 跨提示多样性，包含来自不同生成器和多种提示模态（如文本、图像、视频）生成的视频。为了解决现有方法在检测世界模型生成的高质量视频方面的不足，本文还提出了一种简单而有效的模型——SpannDetector，利用多视角一致性作为检测真实世界AI生成视频的强判据。", "result": "实验表明，本文提出的方法取得了优越的结果，并为基于物理合理性的可解释AI生成视频检测指明了一个有前景的方向。", "conclusion": "GenWorld数据集将推动AI生成视频检测领域的发展，SpannDetector模型在检测高真实度AI生成视频方面表现出色，强调了基于物理合理性的可解释检测方法的潜力。", "translation": "视频生成技术的蓬勃发展危及了现实世界信息的信誉，并加剧了对AI生成视频检测器的需求。尽管取得了一些进展，但高质量真实世界数据集的缺乏阻碍了可信检测器的发展。在本文中，我们提出了GenWorld，一个用于AI生成视频检测的大规模、高质量、真实世界模拟数据集。GenWorld具有以下特点：(1) 真实世界模拟：GenWorld专注于复制现实世界场景的视频，由于其真实性和潜在影响而具有重要意义；(2) 高质量：GenWorld采用多种最先进的视频生成模型来提供逼真和高质量的伪造视频；(3) 跨提示多样性：GenWorld包含来自不同生成器和各种提示模态（例如文本、图像、视频）生成的视频，提供了学习更通用取证特征的潜力。我们分析了现有方法，发现它们未能检测出世界模型（即Cosmos）生成的高质量视频，揭示了忽略真实世界线索的潜在缺点。为了解决这个问题，我们提出了一种简单而有效的模型SpannDetector，利用多视角一致性作为真实世界AI生成视频检测的强判据。实验表明，我们的方法取得了优越的结果，突出了基于物理合理性的可解释AI生成视频检测的一个有前景的方向。我们相信GenWorld将推动AI生成视频检测领域的发展。项目页面：https://chen-wl20.github.io/GenWorld", "summary": "本文针对AI生成视频检测领域缺乏高质量真实世界数据集的问题，提出了GenWorld数据集，该数据集包含大量模拟真实场景、由多种先进生成模型创建的高质量、多模态提示生成的视频。鉴于现有检测方法无法处理世界模型生成的高质量视频，作者进一步提出了SpannDetector模型，该模型利用多视角一致性作为关键特征进行检测。实验证明，SpannDetector在AI生成视频检测方面表现出色，为基于物理合理性的可解释检测方法开辟了新方向。", "keywords": "AI生成视频检测, GenWorld, SpannDetector, 真实世界模拟, 多视角一致性", "comments": "本文的创新点在于提出了GenWorld数据集，它解决了当前AI生成视频检测领域面临的最大挑战之一，即缺乏高质量、真实世界的训练数据。该数据集的真实世界模拟、高质量和跨提示多样性特点，使其成为推动该领域研究的重要资源。此外，SpannDetector模型通过利用多视角一致性，提供了一种新颖且有效的检测策略，尤其是在处理高真实度视频方面，填补了现有方法的空白。该研究强调了物理合理性在可解释AI生成视频检测中的潜力，具有重要的理论和实践意义。"}}
{"id": "2506.10977", "title": "QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction", "authors": ["Sicheng Zuo", "Wenzhao Zheng", "Xiaoyong Han", "Longchao Yang", "Yong Pan", "Jiwen Lu"], "summary": "3D occupancy prediction is crucial for robust autonomous driving systems as\nit enables comprehensive perception of environmental structures and semantics.\nMost existing methods employ dense voxel-based scene representations, ignoring\nthe sparsity of driving scenes and resulting in inefficiency. Recent works\nexplore object-centric representations based on sparse Gaussians, but their\nellipsoidal shape prior limits the modeling of diverse structures. In\nreal-world driving scenes, objects exhibit rich geometries (e.g., cuboids,\ncylinders, and irregular shapes), necessitating excessive ellipsoidal Gaussians\ndensely packed for accurate modeling, which leads to inefficient\nrepresentations. To address this, we propose to use geometrically expressive\nsuperquadrics as scene primitives, enabling efficient representation of complex\nstructures with fewer primitives through their inherent shape diversity. We\ndevelop a probabilistic superquadric mixture model, which interprets each\nsuperquadric as an occupancy probability distribution with a corresponding\ngeometry prior, and calculates semantics through probabilistic mixture.\nBuilding on this, we present QuadricFormer, a superquadric-based model for\nefficient 3D occupancy prediction, and introduce a pruning-and-splitting module\nto further enhance modeling efficiency by concentrating superquadrics in\noccupied regions. Extensive experiments on the nuScenes dataset demonstrate\nthat QuadricFormer achieves state-of-the-art performance while maintaining\nsuperior efficiency.", "comment": "Project page: https://zuosc19.github.io/QuadricFormer/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10977v1", "AI": {"title_translation": "QuadricFormer：将场景表示为超二次曲面用于三维语义占用预测", "tldr": "QuadricFormer模型创新性地使用几何表达能力强的超二次曲面作为场景基元，高效地进行三维语义占用预测，并在nuScenes数据集上取得了最先进的性能和卓越的效率。", "motivation": "现有的三维占用预测方法，无论是基于密集体素还是稀疏高斯，都存在效率低下或形状表达能力有限的问题，无法高效准确地表示驾驶场景中物体多样化的复杂几何形状。", "method": "本文提出使用几何表达能力强的超二次曲面作为场景基元，以高效表示复杂结构。开发了一个概率超二次曲面混合模型，将每个超二次曲面解释为具有相应几何先验的占用概率分布，并通过概率混合计算语义。在此基础上，提出了QuadricFormer模型，并引入了剪枝-分裂模块以进一步提高建模效率。", "result": "在nuScenes数据集上，QuadricFormer实现了最先进的性能，同时保持了卓越的效率。", "conclusion": "QuadricFormer通过引入超二次曲面作为场景基元，有效解决了三维占用预测中效率与表达能力之间的矛盾，实现了高性能和高效率的统一。", "translation": "三维占用预测对于鲁棒的自动驾驶系统至关重要，因为它能够实现对环境结构和语义的全面感知。大多数现有方法采用基于密集体素的场景表示，忽略了驾驶场景的稀疏性，导致效率低下。最近的工作探索了基于稀疏高斯的对象中心表示，但其椭球形状先验限制了对多样化结构的建模。在真实的驾驶场景中，物体展现出丰富的几何形状（例如，长方体、圆柱体和不规则形状），需要过度密集地填充椭球高斯才能进行准确建模，这导致了低效的表示。为了解决这个问题，我们提出使用几何表达能力强的超二次曲面作为场景基元，通过其固有的形状多样性，用更少的基元高效表示复杂结构。我们开发了一个概率超二次曲面混合模型，该模型将每个超二次曲面解释为具有相应几何先验的占用概率分布，并通过概率混合计算语义。在此基础上，我们提出了QuadricFormer，一个基于超二次曲面的高效三维占用预测模型，并引入了一个剪枝-分裂模块，通过将超二次曲面集中在占用区域，进一步提高建模效率。在nuScenes数据集上的大量实验表明，QuadricFormer在保持卓越效率的同时，实现了最先进的性能。", "summary": "本文针对三维语义占用预测中现有方法效率低下的问题，提出了QuadricFormer模型。该模型创新性地采用几何表达能力更强的超二次曲面作为场景基元，通过构建概率超二次曲面混合模型，并结合剪枝-分裂模块，实现了对复杂场景结构的高效表示和语义预测。实验证明，QuadricFormer在nuScenes数据集上达到了领先的性能和卓越的效率。", "keywords": "三维占用预测, 超二次曲面, 语义感知, 自动驾驶, QuadricFormer", "comments": "这篇论文的创新点在于引入了超二次曲面作为三维场景表示的基本单元，解决了传统体素或高斯表示在效率和多样性方面的局限。超二次曲面能够以更少的基元表示复杂的几何形状，显著提高了三维占用预测的效率，同时保持了高精度。其提出的概率混合模型和剪枝-分裂模块也进一步优化了性能。"}}
{"id": "2506.10978", "title": "Fine-Grained Perturbation Guidance via Attention Head Selection", "authors": ["Donghoon Ahn", "Jiwon Kang", "Sanghyun Lee", "Minjae Kim", "Jaewon Min", "Wooseok Jang", "Saungwu Lee", "Sayak Paul", "Susung Hong", "Seungryong Kim"], "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.", "comment": "Project page: https://cvlab-kaist.github.io/HeadHunter/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10978v1", "AI": {"title_translation": "细粒度扰动引导通过注意力头选择", "tldr": "通过细粒度的注意力头选择，本研究改进了扩散模型的引导和生成质量，实现了对视觉属性的精确控制。", "motivation": "现有扩散模型中的注意力扰动方法缺乏确定扰动应用位置的原则性方法，尤其是在质量相关计算分布于不同层的Diffusion Transformer (DiT) 架构中。", "method": "本文研究了注意力扰动的粒度，从层级到单个注意力头，发现特定注意力头控制着结构、风格和纹理质量等不同的视觉概念。在此基础上，提出了“HeadHunter”框架，用于迭代选择与用户目标对齐的注意力头，实现对生成质量和视觉属性的细粒度控制。此外，引入了SoftPAG，通过将每个选定头的注意力图线性插值到单位矩阵来调节扰动强度和抑制伪影。", "result": "我们的方法不仅缓解了现有层级扰动的过平滑问题，而且通过组合式头部选择实现了对特定视觉风格的定向操作。在现代大型DiT文本到图像模型（包括Stable Diffusion 3和FLUX.1）上进行了验证，在通用质量提升和风格特定引导方面均表现出卓越性能。这项工作首次提供了扩散模型中注意力扰动的头部级分析，揭示了注意力层内的可解释专业化。", "conclusion": "本研究揭示了注意力层内可解释的专业化，并实现了有效扰动策略的实际设计。", "translation": "扩散模型中最近的引导方法通过扰动模型来构建一个隐式弱模型并引导生成远离它，从而引导反向采样。在这些方法中，注意力扰动在分类器无关引导不适用的无条件场景中表现出强大的经验性能。然而，现有的注意力扰动方法缺乏确定扰动应在何处应用的原则性方法，特别是在质量相关计算分布于不同层的Diffusion Transformer (DiT) 架构中。在本文中，我们研究了注意力扰动的粒度，从层级到单个注意力头，发现特定头部控制着结构、风格和纹理质量等不同的视觉概念。基于这一洞察，我们提出了“HeadHunter”，这是一个系统框架，用于迭代选择与用户中心目标对齐的注意力头，从而实现对生成质量和视觉属性的细粒度控制。此外，我们引入了SoftPAG，它将每个选定头的注意力图线性插值到单位矩阵，提供一个连续旋钮来调整扰动强度和抑制伪影。我们的方法不仅缓解了现有层级扰动的过平滑问题，而且通过组合式头部选择实现了对特定视觉风格的定向操作。我们在现代大型基于DiT的文本到图像模型（包括Stable Diffusion 3和FLUX.1）上验证了我们的方法，在通用质量提升和风格特定引导方面均表现出卓越性能。我们的工作首次提供了扩散模型中注意力扰动的头部级分析，揭示了注意力层内的可解释专业化，并实现了有效扰动策略的实际设计。", "summary": "本文针对扩散模型中注意力扰动应用缺乏原则性方法的问题，特别是DiT架构中的挑战，进行了深入研究。作者发现单个注意力头控制着不同的视觉概念，并在此基础上提出了“HeadHunter”框架，以迭代选择注意力头，实现对生成质量和视觉属性的细粒度控制。同时引入了SoftPAG，用于调节扰动强度和抑制伪影。该方法有效缓解了过平滑问题，并能在Stable Diffusion 3和FLUX.1等模型上实现目标风格操作，表现出优异性能。这项工作首次对扩散模型中的注意力扰动进行了头部级分析，揭示了注意力层内的可解释专业化，为设计有效的扰动策略提供了实用指导。", "keywords": "注意力扰动, 扩散模型, 细粒度控制, 注意力头, DiT", "comments": "本论文的创新之处在于将扩散模型中的注意力扰动控制从层级提升到更精细的注意力头级别。通过揭示不同注意力头控制特定视觉概念，并提出“HeadHunter”和“SoftPAG”框架，作者提供了一种更精确、可解释的方式来引导生成过程，有效解决了现有方法的局限性，如过平滑问题。这项工作为理解和控制扩散模型内部机制开辟了新方向，并对高质量图像生成和风格控制具有重要实践意义。"}}
{"id": "2506.10015", "title": "Identifying critical residues of a protein using meaningfully-thresholded Random Geometric Graphs", "authors": ["Chuqiao Zhang", "Sarath Chandra Dantu", "Debarghya Mitra", "Dalia Chakrabarty"], "summary": "Identification of critical residues of a protein is actively pursued, since\nsuch residues are essential for protein function. We present three ways of\nrecognising critical residues of an example protein, the evolution of which is\ntracked via molecular dynamical simulations. Our methods are based on learning\na Random Geometric Graph (RGG) variable, where the state variable of each of\n156 residues, is attached to a node of this graph, with the RGG learnt using\nthe matrix of correlations between state variables of each residue-pair. Given\nthe categorical nature of the state variable, correlation between a residue\npair is computed using Cramer's V. We advance an organic thresholding to learn\nan RGG, and compare results against extant thresholding techniques, when\nparametrising criticality as the nodal degree in the learnt RGG. Secondly, we\ndevelop a criticality measure by ranking the computed differences between the\nposterior probability of the full graph variable defined on all 156 residues,\nand that of the graph with all but one residue omitted. A third parametrisation\nof criticality informs on the dynamical variation of nodal degrees as the\nprotein evolves during the simulation. Finally, we compare results obtained\nwith the three distinct criticality parameters, against\nexperimentally-ascertained critical residues.", "comment": "submitted to Journal of Computational and Graphical Statistics", "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.10015v1", "AI": {"title_translation": "使用有意义阈值的随机几何图识别蛋白质关键残基", "tldr": "该研究提出了三种基于随机几何图（RGG）的方法来识别蛋白质的关键残基，并通过分子动力学模拟跟踪蛋白质演变，最终将结果与实验数据进行比较。", "motivation": "蛋白质关键残基的识别是当前活跃的研究领域，因为这些残基对蛋白质功能至关重要。", "method": "本文提出了三种识别蛋白质关键残基的方法：第一种方法是学习一个随机几何图（RGG），其中每个残基的状态变量作为图的节点，RGG通过残基对之间状态变量的相关性矩阵（使用Cramer's V计算）学习得到，并引入了一种有机阈值化方法来学习RGG，将关键性参数化为学习到的RGG中的节点度，并与现有阈值技术进行比较。第二种方法是通过计算包含所有156个残基的完整图变量的后验概率与省略一个残基的图的后验概率之间的差异来开发关键性度量。第三种关键性参数化方法则反映了蛋白质在模拟过程中演变时节点度的动态变化。", "result": "研究比较了通过三种不同关键性参数获得的结果与实验确定的关键残基。", "conclusion": "文章介绍了三种识别蛋白质关键残基的新方法，并与实验数据进行了比较，但未明确给出哪种方法最优或其识别的准确性结论。", "translation": "蛋白质关键残基的识别是一个活跃的研究领域，因为这些残基对蛋白质功能至关重要。我们提出了三种识别示例蛋白质关键残基的方法，通过分子动力学模拟追踪其演变。我们的方法基于学习一个随机几何图（RGG）变量，其中156个残基中每个残基的状态变量都连接到该图的一个节点上，RGG通过每个残基对之间状态变量的相关性矩阵学习得到。鉴于状态变量的分类性质，残基对之间的相关性使用Cramer's V计算。我们提出了一种有机阈值化方法来学习RGG，并在将关键性参数化为学习到的RGG中的节点度时，将结果与现有阈值技术进行比较。其次，我们通过对所有156个残基上定义的完整图变量的后验概率与除一个残基外所有残基都省略的图的后验概率之间的计算差异进行排序，从而开发了一种关键性度量。第三种关键性参数化方法揭示了蛋白质在模拟过程中演变时节点度的动态变化。最后，我们将通过三种不同关键性参数获得的结果与实验确定的关键残基进行了比较。", "summary": "该研究旨在识别蛋白质的关键残基，提出了三种基于随机几何图（RGG）的新方法。第一种方法通过学习RGG并使用有机阈值化技术，将关键性定义为节点度，并与现有方法比较。第二种方法通过比较完整图与去除单一残基图的后验概率差异来衡量关键性。第三种方法则关注蛋白质演变过程中节点度的动态变化。所有三种方法的结果最终都与实验确定的关键残基进行了比较。", "keywords": "关键残基, 随机几何图, 蛋白质功能, 分子动力学模拟, Cramer's V", "comments": "该论文的创新之处在于提出了三种基于随机几何图的蛋白质关键残基识别方法，并引入了有机阈值化技术。其重要性在于提供了一种新的计算框架来理解蛋白质功能相关的关键位点。"}}
{"id": "2506.10980", "title": "InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model", "authors": ["Junqi You", "Chieh Hubert Lin", "Weijie Lyu", "Zhengbo Zhang", "Ming-Hsuan Yang"], "summary": "Recent advances in 3D scene reconstruction enable real-time viewing in\nvirtual and augmented reality. To support interactive operations for better\nimmersiveness, such as moving or editing objects, 3D scene inpainting methods\nare proposed to repair or complete the altered geometry. However, current\napproaches rely on lengthy and computationally intensive optimization, making\nthem impractical for real-time or online applications. We propose InstaInpaint,\na reference-based feed-forward framework that produces 3D-scene inpainting from\na 2D inpainting proposal within 0.4 seconds. We develop a self-supervised\nmasked-finetuning strategy to enable training of our custom large\nreconstruction model (LRM) on the large-scale dataset. Through extensive\nexperiments, we analyze and identify several key designs that improve\ngeneralization, textural consistency, and geometric correctness. InstaInpaint\nachieves a 1000x speed-up from prior methods while maintaining a\nstate-of-the-art performance across two standard benchmarks. Moreover, we show\nthat InstaInpaint generalizes well to flexible downstream applications such as\nobject insertion and multi-region inpainting. More video results are available\nat our project page: https://dhmbb2.github.io/InstaInpaint_page/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10980v1", "AI": {"title_translation": "InstaInpaint：基于掩码大型重建模型的即时3D场景修复", "tldr": "InstaInpaint是一个快速的3D场景修复框架，它使用参考基的馈送前向方法和自监督掩码微调策略，实现了比现有方法快1000倍的速度，同时保持了最先进的性能。", "motivation": "现有的3D场景修复方法依赖耗时且计算密集型的优化，使其不适用于实时或在线应用，因此需要一种更快的方法。", "method": "提出InstaInpaint，一个基于参考的前馈框架，能在0.4秒内从2D修复建议生成3D场景修复。开发了一种自监督掩码微调策略，用于训练定制的大型重建模型（LRM）。", "result": "InstaInpaint比现有方法快1000倍，并在两个标准基准上保持了最先进的性能。它还能很好地推广到对象插入和多区域修复等下游应用。", "conclusion": "InstaInpaint通过其快速和高性能的3D场景修复能力，解决了现有方法的效率问题，并展示了其在多种应用中的泛化能力。", "translation": "近期3D场景重建的进展使得在虚拟和增强现实中实现实时观看成为可能。为了支持更好的沉浸式交互操作，例如移动或编辑物体，提出了3D场景修复方法来修复或完成改变的几何结构。然而，目前的方法依赖于冗长且计算密集型的优化，这使得它们不适用于实时或在线应用。我们提出了InstaInpaint，一个基于参考的前馈框架，可在0.4秒内从2D修复建议生成3D场景修复。我们开发了一种自监督掩码微调策略，以实现在大规模数据集上训练我们定制的大型重建模型（LRM）。通过广泛的实验，我们分析并确定了几个关键设计，这些设计改善了泛化能力、纹理一致性和几何正确性。InstaInpaint比现有方法提速1000倍，同时在两个标准基准上保持了最先进的性能。此外，我们展示了InstaInpaint能很好地推广到灵活的下游应用，例如物体插入和多区域修复。更多视频结果可在我们的项目页面获取：https://dhmbb2.github.io/InstaInpaint_page/。", "summary": "InstaInpaint是一种新颖的3D场景修复框架，旨在解决现有方法效率低下的问题。它采用基于参考的前馈方法和自监督掩码微调策略来训练一个大型重建模型，从而实现了在0.4秒内完成3D场景修复。该方法比现有技术快1000倍，同时保持了最先进的性能，并展现了在对象插入和多区域修复等应用中的良好泛化能力。", "keywords": "3D场景修复, 实时, 大型重建模型, 自监督, InstaInpaint", "comments": "这篇论文通过提出InstaInpaint，显著提升了3D场景修复的速度，使其能够应用于实时和在线环境，这是一项重要的创新。其采用的自监督掩码微调策略和大型重建模型是其高性能的关键。该方法在速度和性能上的突破，以及良好的泛化能力，使其在VR/AR等领域具有巨大的应用潜力。"}}
{"id": "2506.10981", "title": "SceneCompleter: Dense 3D Scene Completion for Generative Novel View Synthesis", "authors": ["Weiliang Chen", "Jiayi Bi", "Yuanhui Huang", "Wenzhao Zheng", "Yueqi Duan"], "summary": "Generative models have gained significant attention in novel view synthesis\n(NVS) by alleviating the reliance on dense multi-view captures. However,\nexisting methods typically fall into a conventional paradigm, where generative\nmodels first complete missing areas in 2D, followed by 3D recovery techniques\nto reconstruct the scene, which often results in overly smooth surfaces and\ndistorted geometry, as generative models struggle to infer 3D structure solely\nfrom RGB data. In this paper, we propose SceneCompleter, a novel framework that\nachieves 3D-consistent generative novel view synthesis through dense 3D scene\ncompletion. SceneCompleter achieves both visual coherence and 3D-consistent\ngenerative scene completion through two key components: (1) a\ngeometry-appearance dual-stream diffusion model that jointly synthesizes novel\nviews in RGBD space; (2) a scene embedder that encodes a more holistic scene\nunderstanding from the reference image. By effectively fusing structural and\ntextural information, our method demonstrates superior coherence and\nplausibility in generative novel view synthesis across diverse datasets.\nProject Page: https://chen-wl20.github.io/SceneCompleter", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.10981v1", "AI": {"title_translation": "场景补全器：用于生成式新视角合成的密集三维场景补全", "tldr": "SceneCompleter提出一种新的框架，通过在RGBD空间中联合合成新视图，实现3D一致的生成式新视角合成，解决了现有方法中2D补全导致的三维结构失真问题。", "motivation": "现有的生成式新视角合成方法通常先在2D中补全缺失区域，再进行3D恢复，这导致表面过于平滑和几何形状扭曲，因为生成模型难以仅从RGB数据推断3D结构。", "method": "提出SceneCompleter框架，通过密集三维场景补全实现3D一致的生成式新视角合成。它包含两个关键组件：1) 一个几何-外观双流扩散模型，用于在RGBD空间中联合合成新视图；2) 一个场景嵌入器，用于从参考图像编码更全面的场景理解。该方法有效融合了结构和纹理信息。", "result": "该方法在生成式新视角合成中表现出卓越的一致性和合理性，并在不同数据集上得到验证。", "conclusion": "通过在RGBD空间中联合合成并有效融合结构和纹理信息，SceneCompleter能够实现3D一致且视觉连贯的生成式新视角合成，克服了传统2D补全方法的局限性。", "translation": "生成模型在新视角合成（NVS）中获得了显著关注，通过减轻对密集多视角捕获的依赖。然而，现有方法通常陷入传统范式，即生成模型首先在2D中补全缺失区域，然后通过3D恢复技术重建场景，这通常导致表面过于平滑和几何形状扭曲，因为生成模型难以仅从RGB数据推断3D结构。在本文中，我们提出了SceneCompleter，一个通过密集三维场景补全实现3D一致生成式新视角合成的新颖框架。SceneCompleter通过两个关键组件实现了视觉连贯和3D一致的生成式场景补全：(1) 一个几何-外观双流扩散模型，用于在RGBD空间中联合合成新视图；(2) 一个场景嵌入器，用于从参考图像编码更全面的场景理解。通过有效融合结构和纹理信息，我们的方法在多样化数据集上的生成式新视角合成中展示出卓越的一致性和合理性。项目页面：https://chen-wl20.github.io/SceneCompleter", "summary": "SceneCompleter是一个新颖的框架，旨在解决现有生成式新视角合成方法中因2D补全导致的3D结构失真问题。它通过密集三维场景补全实现3D一致的生成式新视角合成，核心在于一个在RGBD空间联合合成视图的几何-外观双流扩散模型和一个增强场景理解的场景嵌入器。该方法有效融合结构和纹理信息，在新视角合成中展现出卓越的一致性和合理性。", "keywords": "新视角合成, 3D场景补全, 生成模型, 扩散模型, 几何一致性", "comments": "这篇论文通过引入RGBD空间中的双流扩散模型和场景嵌入器，创新性地解决了生成式新视角合成中3D几何一致性问题。其方法直接在3D空间进行补全，避免了传统2D补全后3D恢复的弊端，对于提升生成场景的真实感和结构准确性具有重要意义。"}}
{"id": "2506.10031", "title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data", "authors": ["Olga Ovcharenko", "Florian Barkmann", "Philip Toma", "Imant Daunhawer", "Julia Vogt", "Sebastian Schelter", "Valentina Boeva"], "summary": "Self-supervised learning (SSL) has proven to be a powerful approach for\nextracting biologically meaningful representations from single-cell data. To\nadvance our understanding of SSL methods applied to single-cell data, we\npresent scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL\nmethods. Our evaluation spans nine datasets and focuses on three common\ndownstream tasks: batch correction, cell type annotation, and missing modality\nprediction. Furthermore, we systematically assess various data augmentation\nstrategies. Our analysis reveals task-specific trade-offs: the specialized\nsingle-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at\nuni-modal batch correction, while generic SSL methods, such as VICReg and\nSimCLR, demonstrate superior performance in cell typing and multi-modal data\nintegration. Random masking emerges as the most effective augmentation\ntechnique across all tasks, surpassing domain-specific augmentations. Notably,\nour results indicate the need for a specialized single-cell multi-modal data\nintegration framework. scSSL-Bench provides a standardized evaluation platform\nand concrete recommendations for applying SSL to single-cell analysis,\nadvancing the convergence of deep learning and single-cell genomics.", "comment": "Accepted at ICML 2025 (Spotlight)", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10031v1", "AI": {"title_translation": "scSSL-Bench：单细胞数据自监督学习基准测试", "tldr": "scSSL-Bench是一个综合基准，评估了19种自监督学习方法在9个单细胞数据集上的表现，涵盖批次校正、细胞类型注释和缺失模态预测等任务，并评估了数据增强策略，发现不同方法在不同任务上表现各异，且随机掩码是最有效的数据增强技术，强调了对专用单细胞多模态数据集成框架的需求。", "motivation": "为了加深对应用于单细胞数据的自监督学习（SSL）方法的理解。", "method": "提出了scSSL-Bench，一个综合基准，评估了十九种SSL方法。评估涵盖九个数据集，并侧重于三个常见的下游任务：批次校正、细胞类型注释和缺失模态预测。此外，还系统评估了各种数据增强策略。", "result": "分析揭示了任务特异性的权衡：专门的单细胞框架，如scVI、CLAIRE和微调后的scGPT在单模态批次校正方面表现出色；而通用SSL方法，如VICReg和SimCLR，在细胞分型和多模态数据整合方面表现更优。随机掩码成为所有任务中最有效的数据增强技术，超越了领域特定的增强方法。", "conclusion": "结果表明需要一个专门的单细胞多模态数据集成框架。scSSL-Bench提供了一个标准化的评估平台和将SSL应用于单细胞分析的具体建议，推动了深度学习和单细胞基因组学的融合。", "translation": "自监督学习（SSL）已被证明是一种从单细胞数据中提取具有生物学意义的表示的强大方法。为了增进我们对应用于单细胞数据的SSL方法的理解，我们提出了scSSL-Bench，一个综合基准，评估了十九种SSL方法。我们的评估涵盖九个数据集，并侧重于三个常见的下游任务：批次校正、细胞类型注释和缺失模态预测。此外，我们系统地评估了各种数据增强策略。我们的分析揭示了任务特异性的权衡：专门的单细胞框架，如scVI、CLAIRE和微调后的scGPT在单模态批次校正方面表现出色，而通用SSL方法，如VICReg和SimCLR，在细胞分型和多模态数据整合方面表现更优。随机掩码成为所有任务中最有效的数据增强技术，超越了领域特定的增强方法。值得注意的是，我们的结果表明需要一个专门的单细胞多模态数据集成框架。scSSL-Bench提供了一个标准化的评估平台和将SSL应用于单细胞分析的具体建议，推动了深度学习和单细胞基因组学的融合。", "summary": "该论文介绍了scSSL-Bench，一个用于评估单细胞数据自监督学习（SSL）方法的综合基准。该基准测试了19种SSL方法在9个数据集上的表现，涉及批次校正、细胞类型注释和缺失模态预测等下游任务，并系统评估了数据增强策略。研究发现，专业单细胞框架在批次校正上表现突出，而通用SSL方法在细胞分型和多模态整合上更优。随机掩码被证明是最有效的增强技术。论文强调了对专门单细胞多模态数据集成框架的需求，并为SSL在单细胞分析中的应用提供了标准化评估平台和具体建议。", "keywords": "自监督学习, 单细胞数据, 基准测试, 数据增强, 多模态数据整合", "comments": "该论文通过引入一个全面的基准测试平台scSSL-Bench，填补了单细胞数据自监督学习领域的一个重要空白。其创新之处在于系统性地评估了多种SSL方法和数据增强策略，并揭示了不同方法在特定任务上的优势和劣势。这项工作的重要性在于为研究人员提供了应用SSL于单细胞分析的明确指导和标准化评估工具，尤其强调了对未来多模态集成框架的需求，有望加速深度学习与单细胞基因组学领域的融合。"}}
{"id": "2506.10073", "title": "Patient-Specific Deep Reinforcement Learning for Automatic Replanning in Head-and-Neck Cancer Proton Therapy", "authors": ["Malvern Madondo", "Yuan Shao", "Yingzi Liu", "Jun Zhou", "Xiaofeng Yang", "Zhen Tian"], "summary": "Anatomical changes during intensity-modulated proton therapy (IMPT) for\nhead-and-neck cancer (HNC) can shift Bragg peaks, risking tumor underdosing and\norgan-at-risk overdosing. As a result, treatment replanning is often required\nto maintain clinically acceptable treatment quality. However, current manual\nreplanning processes are resource-intensive and time-consuming. We propose a\npatient-specific deep reinforcement learning (DRL) framework for automated IMPT\nreplanning, with a reward-shaping mechanism based on a $150$-point plan quality\nscore addressing competing clinical objectives. We formulate the planning\nprocess as an RL problem where agents learn control policies to adjust\noptimization priorities, maximizing plan quality. Unlike population-based\napproaches, our framework trains personalized agents for each patient using\ntheir planning CT (Computed Tomography) and augmented anatomies simulating\nanatomical changes (tumor progression and regression). This patient-specific\napproach leverages anatomical similarities throughout treatment, enabling\neffective plan adaptation. We implemented two DRL algorithms, Deep Q-Network\nand Proximal Policy Optimization, using dose-volume histograms (DVHs) as state\nrepresentations and a $22$-dimensional action space of priority adjustments.\nEvaluation on five HNC patients using actual replanning CT data showed both DRL\nagents improved initial plan scores from $120.63 \\pm 21.40$ to $139.78 \\pm\n6.84$ (DQN) and $142.74 \\pm 5.16$ (PPO), surpassing manual replans generated by\na human planner ($137.20 \\pm 5.58$). Clinical validation confirms that\nimprovements translate to better tumor coverage and OAR sparing across diverse\nanatomical changes. This work demonstrates DRL's potential in addressing\ngeometric and dosimetric complexities of adaptive proton therapy, offering\nefficient offline adaptation solutions and advancing online adaptive proton\ntherapy.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.10073v1", "AI": {"title_translation": "头颈部癌症质子治疗中用于自动再计划的患者特异性深度强化学习", "tldr": "该研究提出了一种患者特异性深度强化学习框架，用于头颈部癌症质子治疗的自动再计划，以应对解剖学变化并提高治疗质量，结果显示其优于手动再计划。", "motivation": "头颈部癌症的强度调制质子治疗(IMPT)中，解剖学变化可能导致布拉格峰偏移，从而增加肿瘤欠剂量和危及器官过剂量的风险。当前的治疗再计划过程耗费大量资源和时间，因此需要自动化解决方案。", "method": "本研究提出了一种患者特异性深度强化学习(DRL)框架，用于IMPT自动再计划。该框架将计划过程表述为强化学习问题，代理通过调整优化优先级来最大化计划质量，并采用基于150点计划质量分数的奖励塑造机制。与基于人群的方法不同，该框架为每位患者训练个性化代理，利用其规划CT和模拟解剖变化的增强解剖结构。研究实现了Deep Q-Network和Proximal Policy Optimization两种DRL算法，使用剂量体积直方图(DVHs)作为状态表示，并采用22维的优先级调整动作空间。", "result": "在五名头颈部癌症患者的实际再计划CT数据上进行评估，两种DRL代理都将初始计划分数从120.63 ± 21.40提高到DQN的139.78 ± 6.84和PPO的142.74 ± 5.16，均超过了人类计划员生成的手动再计划分数(137.20 ± 5.58)。临床验证证实，这些改进能转化为在不同解剖变化下的更好肿瘤覆盖和OAR保护。", "conclusion": "该工作表明深度强化学习在解决自适应质子治疗的几何和剂量复杂性方面具有潜力，提供了高效的离线自适应解决方案，并推动了在线自适应质子治疗的发展。", "translation": "头颈部癌症 (HNC) 强度调制质子治疗 (IMPT) 期间的解剖学变化可能导致布拉格峰偏移，从而有肿瘤欠剂量和危及器官过剂量的风险。因此，通常需要进行治疗再计划以保持临床可接受的治疗质量。然而，当前的手动再计划过程资源密集且耗时。我们提出了一种患者特异性深度强化学习 (DRL) 框架，用于自动 IMPT 再计划，该框架具有基于 150 点计划质量分数的奖励塑造机制，以解决相互竞争的临床目标。我们将计划过程表述为一个强化学习问题，其中代理学习控制策略以调整优化优先级，从而最大化计划质量。与基于人群的方法不同，我们的框架使用患者的规划 CT（计算机断层扫描）和模拟解剖变化（肿瘤进展和消退）的增强解剖结构为每位患者训练个性化代理。这种患者特异性方法利用了整个治疗过程中的解剖学相似性，从而实现了有效的计划适应。我们使用剂量体积直方图 (DVH) 作为状态表示和 22 维优先级调整动作空间，实现了两种 DRL 算法：深度 Q 网络和近端策略优化。对五名 HNC 患者使用实际再计划 CT 数据进行的评估显示，两种 DRL 代理都将初始计划分数从 120.63 ± 21.40 提高到 139.78 ± 6.84 (DQN) 和 142.74 ± 5.16 (PPO)，超过了人类计划员生成的手动再计划分数 (137.20 ± 5.58)。临床验证证实，这些改进转化为在不同解剖变化下更好的肿瘤覆盖和 OAR 保护。这项工作展示了 DRL 在解决自适应质子治疗的几何和剂量复杂性方面的潜力，提供了高效的离线适应解决方案，并推动了在线自适应质子治疗。", "summary": "本研究提出了一种创新的患者特异性深度强化学习(DRL)框架，旨在自动化头颈部癌症质子治疗中的再计划过程。该框架通过将计划优化建模为强化学习问题，并为每位患者训练个性化代理来应对治疗期间的解剖学变化。实验结果表明，与手动再计划相比，DRL方法显著提高了计划质量分数，并在肿瘤覆盖和危及器官保护方面表现更优，证明了DRL在自适应质子治疗中的巨大潜力。", "keywords": "深度强化学习, 质子治疗, 自动再计划, 头颈部癌症, 患者特异性", "comments": "该论文的创新点在于提出了患者特异性的深度强化学习框架来自动化质子治疗的再计划，这与传统的基于人群的方法不同。通过为每位患者训练个性化代理，并利用奖励塑造机制，有效解决了治疗过程中解剖学变化导致的计划质量下降问题。其重要性在于能够显著提高治疗效率，减少手动再计划所需的时间和资源，并可能改善患者的治疗效果。这代表了自适应质子治疗领域的一个重要进展，为未来的在线自适应治疗奠定了基础。"}}
{"id": "2506.10460", "title": "Equitable Mechanism Design for Facility Location", "authors": ["Toby Walsh"], "summary": "We consider strategy proof mechanisms for facility location which maximize\nequitability between agents. As is common in the literature, we measure\nequitability with the Gini index. We first prove a simple but fundamental\nimpossibility result that no strategy proof mechanism can bound the\napproximation ratio of the optimal Gini index of utilities for one or more\nfacilities. We propose instead computing approximation ratios of the\ncomplemented Gini index of utilities, and consider how well both deterministic\nand randomized mechanisms approximate this. In addition, as Nash welfare is\noften put forwards as an equitable compromise between egalitarian and\nutilitarian outcomes, we consider how well mechanisms approximate the Nash\nwelfare.", "comment": "To appear in Proceedings of IJCAI 2025", "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.10460v1", "AI": {"title_translation": "设施选址的公平机制设计", "tldr": "本文研究了设施选址中最大化代理人之间公平性的策略证明机制。研究发现，没有策略证明机制能够界定效用最优基尼系数的近似比，因此转而探索互补基尼系数和纳什福利的近似性能。", "motivation": "旨在为设施选址问题设计最大化代理人之间公平性的策略证明机制。", "method": "使用基尼系数衡量公平性。证明了无法界定最优基尼系数近似比的不可能性结果。提出计算互补基尼系数的近似比，并考虑确定性和随机机制如何近似互补基尼系数和纳什福利，以及纳什福利的近似。", "result": "证明了对于一个或多个设施，没有策略证明机制可以界定效用最优基尼系数近似比的不可能性结果。提出了计算互补基尼系数近似比的替代方法，并研究了确定性和随机机制对互补基尼系数和纳什福利的近似效果。", "conclusion": "论文证明了在设施选址中，无法通过策略证明机制来界定最优基尼系数的近似比，并提出了转而研究互补基尼系数和纳什福利的近似效果。", "translation": "我们考虑了设施选址中的策略证明机制，旨在最大化代理人之间的公平性。正如文献中常见的那样，我们使用基尼系数来衡量公平性。我们首先证明了一个简单但基础的不可能性结果，即对于一个或多个设施，没有策略证明机制能够界定效用最优基尼系数的近似比。我们转而建议计算效用互补基尼系数的近似比，并考虑确定性和随机机制如何很好地近似这一点。此外，由于纳什福利常被认为是平等主义和功利主义结果之间的一种公平折衷，我们也考虑了机制如何很好地近似纳什福利。", "summary": "本文探讨了设施选址中的策略证明机制，旨在最大化代理人之间的公平性，并以基尼系数衡量。研究发现，不存在能界定最优基尼系数近似比的策略证明机制。为此，作者提出评估互补基尼系数的近似比，并分析确定性与随机机制对其以及纳什福利的近似性能。", "keywords": "设施选址, 机制设计, 公平性, 基尼系数, 纳什福利", "comments": "这项研究揭示了在设施选址中，兼顾策略证明性和最大化公平性（以基尼系数衡量）的内在挑战，即无法界定最优基尼系数的近似比。转而关注互补基尼系数和纳什福利的近似，为该领域提供了一个新的研究视角和衡量标准，具有重要的理论意义。"}}
{"id": "2506.10101", "title": "Fundamental Limits of Learning High-dimensional Simplices in Noisy Regimes", "authors": ["Seyed Amir Hossein Saberi", "Amir Najafi", "Abolfazl Motahari", "Babak H. khalaj"], "summary": "In this paper, we establish sample complexity bounds for learning\nhigh-dimensional simplices in $\\mathbb{R}^K$ from noisy data. Specifically, we\nconsider $n$ i.i.d. samples uniformly drawn from an unknown simplex in\n$\\mathbb{R}^K$, each corrupted by additive Gaussian noise of unknown variance.\nWe prove an algorithm exists that, with high probability, outputs a simplex\nwithin $\\ell_2$ or total variation (TV) distance at most $\\varepsilon$ from the\ntrue simplex, provided $n \\ge (K^2/\\varepsilon^2)\ne^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$, where $\\mathrm{SNR}$ is the signal-to-noise\nratio. Extending our prior work~\\citep{saberi2023sample}, we derive new\ninformation-theoretic lower bounds, showing that simplex estimation within TV\ndistance $\\varepsilon$ requires at least $n \\ge \\Omega(K^3\n\\sigma^2/\\varepsilon^2 + K/\\varepsilon)$ samples, where $\\sigma^2$ denotes the\nnoise variance. In the noiseless scenario, our lower bound $n \\ge\n\\Omega(K/\\varepsilon)$ matches known upper bounds up to constant factors. We\nresolve an open question by demonstrating that when $\\mathrm{SNR} \\ge\n\\Omega(K^{1/2})$, noisy-case complexity aligns with the noiseless case. Our\nanalysis leverages sample compression techniques (Ashtiani et al., 2018) and\nintroduces a novel Fourier-based method for recovering distributions from noisy\nobservations, potentially applicable beyond simplex learning.", "comment": "Extension of our ICML 2023 paper, 44 pages", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10101v1", "AI": {"title_translation": "高维单纯形在噪声状态下学习的根本极限", "tldr": "本文建立了从噪声数据中学习高维单纯形的样本复杂度界限，证明了算法的存在性，并推导了新的信息论下界，解决了关于噪声情况下复杂度与无噪声情况一致性的开放问题。", "motivation": "本文旨在建立从噪声数据中学习高维单纯形的样本复杂度界限，并理解这一学习过程的根本限制，特别是解决一个关于噪声如何影响复杂度与无噪声情况相比的开放问题。", "method": "本文通过证明一个算法的存在性、推导新的信息论下界（扩展了之前的工作）、利用样本压缩技术以及引入一种新颖的基于傅里叶的方法来从噪声观测中恢复分布。", "result": "一个算法存在，在$n \nsupseteq (K^2/\nvarepsilon^2) e^{\nmathcal{O}(K/\\mathrm{SNR}^2)}$的样本条件下，以高概率输出一个与真实单纯形之间$\\ell_2$或TV距离至多为$\\varepsilon$的单纯形。信息论下界表明，在TV距离$\\varepsilon$内进行单纯形估计至少需要$n \nsupseteq \\Omega(K^3 \\sigma^2/\\varepsilon^2 + K/\\varepsilon)$个样本。在无噪声情况下，下界$n \nsupseteq \\Omega(K/\\varepsilon)$与已知上界匹配。解决了开放问题：当$\\mathrm{SNR} \nsupseteq \\Omega(K^{1/2})$时，噪声情况下的复杂度与无噪声情况下的复杂度一致。", "conclusion": "本文建立了在噪声环境中学习高维单纯形的基本样本复杂度限制，提供了一个带有上界的算法，推导了严格的信息论下界，并解决了关于噪声学习复杂度何时与无噪声情况匹配的开放问题。引入的基于傅里叶的方法具有更广泛的应用潜力。", "translation": "在本文中，我们建立了从噪声数据中学习$\\mathbb{R}^K$中高维单纯形的样本复杂度界限。具体来说，我们考虑了从$\\mathbb{R}^K$中未知单纯形中均匀抽取的$n$个独立同分布样本，每个样本都受到未知方差的加性高斯噪声的污染。我们证明存在一个算法，在$n \\ge (K^2/\\varepsilon^2) e^{\\mathcal{O}(K/\\mathrm{SNR}^2)}$的条件下，以高概率输出一个与真实单纯形之间$\\ell_2$或总变差（TV）距离至多为$\\varepsilon$的单纯形，其中$\\mathrm{SNR}$是信噪比。扩展我们之前的工作~\ncitep{saberi2023sample}，我们推导了新的信息论下界，表明在TV距离$\\varepsilon$内进行单纯形估计至少需要$n \\ge \\Omega(K^3 \\sigma^2/\\varepsilon^2 + K/\\varepsilon)$个样本，其中$\\sigma^2$表示噪声方差。在无噪声情况下，我们的下界$n \\ge \\Omega(K/\\varepsilon)$与已知的上界在常数因子内匹配。我们通过证明当$\\mathrm{SNR} \\ge \\Omega(K^{1/2})$时，噪声情况下的复杂度与无噪声情况下的复杂度一致，从而解决了一个开放性问题。我们的分析利用了样本压缩技术（Ashtiani et al., 2018），并引入了一种新颖的基于傅里叶的方法，用于从噪声观测中恢复分布，这可能适用于单纯形学习之外的领域。", "summary": "本文研究了在受高斯噪声污染下学习高维单纯形的基本样本复杂度极限。它为一个恢复单纯形的算法建立了上界，并推导了新的严格信息论下界。一个关键发现是，在足够的信噪比（$\\mathrm{SNR} \\ge \\Omega(K^{1/2})$）下，噪声条件下的学习复杂度与无噪声情况一致，从而解决了一个开放问题。这项工作还引入了一种新颖的基于傅里叶的方法，具有潜在的更广泛应用。", "keywords": "高维单纯形, 样本复杂度, 噪声数据, 信息论, 基于傅里叶的方法", "comments": "本文通过为噪声环境下学习高维单纯形提供上下界，对样本复杂度理解做出了重要贡献。解决关于噪声和无噪声复杂度在特定信噪比条件下对齐的开放问题尤其具有洞察力。引入一种新颖的基于傅里叶的方法来从噪声观测中恢复分布，表明了超越单纯形学习特定问题的潜在重要方法创新。"}}
{"id": "2506.10141", "title": "Diffusion prior as a direct regularization term for FWI", "authors": ["Yuke Xie", "Hervé Chauris", "Nicolas Desassis"], "summary": "Diffusion models have recently shown promise as powerful generative priors\nfor inverse problems. However, conventional applications require solving the\nfull reverse diffusion process and operating on noisy intermediate states,\nwhich poses challenges for physics-constrained computational seismic imaging.\nIn particular, such instability is pronounced in non-linear solvers like those\nused in Full Waveform Inversion (FWI), where wave propagation through noisy\nvelocity fields can lead to numerical artifacts and poor inversion quality. In\nthis work, we propose a simple yet effective framework that directly integrates\na pretrained Denoising Diffusion Probabilistic Model (DDPM) as a score-based\ngenerative diffusion prior into FWI through a score rematching strategy. Unlike\ntraditional diffusion approaches, our method avoids the reverse diffusion\nsampling and needs fewer iterations. We operate the image inversion entirely in\nthe clean image space, eliminating the need to operate through noisy velocity\nmodels. The generative diffusion prior can be introduced as a simple\nregularization term in the standard FWI update rule, requiring minimal\nmodification to existing FWI pipelines. This promotes stable wave propagation\nand can improve convergence behavior and inversion quality. Numerical\nexperiments suggest that the proposed method offers enhanced fidelity and\nrobustness compared to conventional and GAN-based FWI approaches, while\nremaining practical and computationally efficient for seismic imaging and other\ninverse problem tasks.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.10141v1", "AI": {"title_translation": "扩散先验作为FWI的直接正则化项", "tldr": "本文提出一种将预训练的去噪扩散概率模型作为直接正则化项整合到全波形反演（FWI）中的方法，避免了反向扩散采样，提高了反演质量和稳定性。", "motivation": "传统扩散模型在地震成像等物理约束逆问题中应用时，需要解决完整的反向扩散过程并在噪声中间态操作，导致不稳定性和数值伪影，尤其在全波形反演（FWI）等非线性求解器中表现明显，影响反演质量。", "method": "提出一个框架，通过分数重匹配策略，将预训练的去噪扩散概率模型（DDPM）作为基于分数的生成扩散先验直接整合到FWI中。该方法避免了反向扩散采样，在干净图像空间进行反演，并将生成扩散先验作为标准FWI更新规则中的简单正则化项。", "result": "数值实验表明，与传统方法和基于GAN的FWI方法相比，所提方法提供了更高的保真度和鲁棒性，同时在计算上对地震成像和其他逆问题任务仍然实用且高效。", "conclusion": "通过将扩散先验作为直接正则化项整合到FWI中，可以促进稳定的波传播，改善收敛行为和反演质量，为地震成像及其他逆问题提供了有效且高效的解决方案。", "translation": "扩散模型最近在逆问题中显示出作为强大生成先验的潜力。然而，传统应用需要解决完整的反向扩散过程并在噪声中间态操作，这给受物理约束的计算地震成像带来了挑战。特别是，这种不稳定性在全波形反演（FWI）等非线性求解器中表现明显，其中通过噪声速度场的波传播可能导致数值伪影和较差的反演质量。在这项工作中，我们提出了一种简单而有效的框架，通过分数重匹配策略，将预训练的去噪扩散概率模型（DDPM）作为基于分数的生成扩散先验直接整合到FWI中。与传统扩散方法不同，我们的方法避免了反向扩散采样，并且需要更少的迭代次数。我们在干净图像空间中完全进行图像反演，从而无需通过噪声速度模型进行操作。生成扩散先验可以作为标准FWI更新规则中的一个简单正则化项引入，对现有FWI管道的修改最小。这促进了稳定的波传播，并可以改善收敛行为和反演质量。数值实验表明，与传统和基于GAN的FWI方法相比，所提出的方法提供了增强的保真度和鲁棒性，同时对于地震成像和其他逆问题任务仍然实用且计算高效。", "summary": "本文提出一种新颖的框架，将预训练的去噪扩散概率模型（DDPM）作为直接正则化项整合到全波形反演（FWI）中，以解决传统扩散模型在地震成像中因噪声中间态导致的不稳定性问题。该方法通过分数重匹配策略，避免了反向扩散采样，并在干净图像空间进行反演，显著提高了FWI的稳定性和反演质量，且计算效率高。", "keywords": "全波形反演, 扩散模型, 生成先验, 正则化, 地震成像", "comments": "这篇论文的创新点在于将扩散先验直接作为FWI的正则化项，而不是通过复杂的反向扩散过程，极大地简化了扩散模型在物理约束逆问题中的应用。它解决了传统方法在噪声速度场中波传播的不稳定性问题，提高了反演的保真度和鲁棒性，为地震成像等领域提供了高效实用的新思路。"}}
{"id": "2506.10153", "title": "Attention on flow control: transformer-based reinforcement learning for lift regulation in highly disturbed flows", "authors": ["Zhecheng Liu", "Jeff D. Eldredge"], "summary": "A linear flow control strategy designed for weak disturbances may not remain\neffective in sequences of strong disturbances due to nonlinear interactions,\nbut it is sensible to leverage it for developing a better strategy. In the\npresent study, we propose a transformer-based reinforcement learning (RL)\nframework to learn an effective control strategy for regulating aerodynamic\nlift in gust sequences via pitch control. The transformer addresses the\nchallenge of partial observability from limited surface pressure sensors. We\ndemonstrate that the training can be accelerated with two techniques --\npretraining with an expert policy (here, linear control) and task-level\ntransfer learning (here, extending a policy trained on isolated gusts to\nmultiple gusts). We show that the learned strategy outperforms the best\nproportional control, with the performance gap widening as the number of gusts\nincreases. The control strategy learned in an environment with a small number\nof successive gusts is shown to effectively generalize to an environment with\nan arbitrarily long sequence of gusts. We investigate the pivot configuration\nand show that quarter-chord pitching control can achieve superior lift\nregulation with substantially less control effort compared to mid-chord\npitching control. Through a decomposition of the lift, we attribute this\nadvantage to the dominant added-mass contribution accessible via quarter-chord\npitching. The success on multiple configurations shows the generalizability of\nthe proposed transformer-based RL framework, which offers a promising approach\nto solve more computationally demanding flow control problems when combined\nwith the proposed acceleration techniques.", "comment": null, "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.10153v1", "AI": {"title_translation": "流量控制中的注意力机制：基于Transformer的强化学习在强扰动流中升力调节的应用", "tldr": "本文提出了一种基于Transformer的强化学习框架，用于通过俯仰控制在强扰动流中调节气动升力，并通过预训练和迁移学习加速训练，其性能优于传统控制方法并具有良好的泛化能力。", "motivation": "针对弱扰动设计的线性流控制策略在强扰动序列中可能因非线性相互作用而失效。", "method": "提出了一种基于Transformer的强化学习(RL)框架，通过俯仰控制来学习在阵风序列中调节气动升力的有效控制策略。Transformer用于解决有限表面压力传感器带来的部分可观测性挑战。通过专家策略（线性控制）预训练和任务级迁移学习（将针对孤立阵风训练的策略扩展到多阵风）加速训练。研究了枢轴配置，特别是四分之一弦长俯仰控制和中弦长俯仰控制。", "result": "学习到的策略优于最佳比例控制，且随着阵风数量的增加，性能差距增大。在少数连续阵风环境下学习到的控制策略能有效泛化到任意长序列的阵风环境。四分之一弦长俯仰控制与中弦长俯仰控制相比，能以显著更小的控制力实现更好的升力调节，这归因于通过四分之一弦长俯仰可获得的主导附加质量贡献。", "conclusion": "所提出的基于Transformer的RL框架及其加速技术在多种配置下均取得成功，显示出其通用性，为解决计算要求更高的流控制问题提供了一种有前景的方法。", "translation": "针对弱扰动设计的线性流控制策略在强扰动序列中可能因非线性相互作用而失效，但利用其来开发更好的策略是明智的。在本研究中，我们提出了一种基于Transformer的强化学习（RL）框架，用于学习通过俯仰控制在阵风序列中调节气动升力的有效控制策略。Transformer解决了有限表面压力传感器导致的部分可观测性挑战。我们证明了通过两种技术可以加速训练——使用专家策略（此处为线性控制）进行预训练，以及任务级迁移学习（此处为将针对孤立阵风训练的策略扩展到多个阵风）。我们表明，学习到的策略优于最佳比例控制，并且随着阵风数量的增加，性能差距不断扩大。在少量连续阵风环境中学习到的控制策略被证明可以有效泛化到任意长序列的阵风环境。我们研究了枢轴配置，并表明与中弦长俯仰控制相比，四分之一弦长俯仰控制可以以显著更小的控制力实现更优越的升力调节。通过升力分解，我们将这一优势归因于通过四分之一弦长俯仰可获得的主导附加质量贡献。在多种配置上的成功表明了所提出的基于Transformer的RL框架的通用性，结合所提出的加速技术，它为解决计算要求更高的流控制问题提供了一种有前景的方法。", "summary": "本文提出了一种基于Transformer的强化学习（RL）框架，用于在强扰动流中通过俯仰控制调节气动升力。该框架利用Transformer处理有限传感器带来的部分可观测性问题，并通过专家策略预训练和任务级迁移学习来加速训练。研究结果表明，所学策略在多阵风环境下性能显著优于传统比例控制，且能有效泛化到更长的阵风序列。此外，发现四分之一弦长俯仰控制能以更小的控制力实现更好的升力调节，这归因于其对附加质量贡献的有效利用。该框架的通用性及其加速技术为解决复杂的流控制问题提供了新途径。", "keywords": "强化学习, Transformer, 流控制, 升力调节, 俯仰控制", "comments": "本文的创新之处在于将Transformer模型引入强化学习框架来解决流控制中的部分可观测性问题，并结合预训练和迁移学习显著加速了训练过程。其重要性在于提供了一种在复杂非线性流场中实现鲁棒控制的有效方法，特别是在强扰动条件下的升力调节。此外，对不同俯仰配置的深入分析也提供了有价值的工程指导。"}}
{"id": "2506.10558", "title": "StepProof: Step-by-step verification of natural language mathematical proofs", "authors": ["Xiaolin Hu", "Qinghua Zhou", "Bogdan Grechuk", "Ivan Y. Tyukin"], "summary": "Interactive theorem provers (ITPs) are powerful tools for the formal\nverification of mathematical proofs down to the axiom level. However, their\nlack of a natural language interface remains a significant limitation. Recent\nadvancements in large language models (LLMs) have enhanced the understanding of\nnatural language inputs, paving the way for autoformalization - the process of\ntranslating natural language proofs into formal proofs that can be verified.\nDespite these advancements, existing autoformalization approaches are limited\nto verifying complete proofs and lack the capability for finer, sentence-level\nverification. To address this gap, we propose StepProof, a novel\nautoformalization method designed for granular, step-by-step verification.\nStepProof breaks down complete proofs into multiple verifiable subproofs,\nenabling sentence-level verification. Experimental results demonstrate that\nStepProof significantly improves proof success rates and efficiency compared to\ntraditional methods. Additionally, we found that minor manual adjustments to\nthe natural language proofs, tailoring them for step-level verification,\nfurther enhanced StepProof's performance in autoformalization.", "comment": null, "cate": "cs.LO", "url": "http://arxiv.org/abs/2506.10558v1", "AI": {"title_translation": "StepProof：自然语言数学证明的逐步验证", "tldr": "StepProof提出了一种新的自动形式化方法，通过将自然语言数学证明分解为可验证的子证明，实现了细粒度的逐句验证，显著提高了证明成功率和效率。", "motivation": "现有的交互式定理证明器缺乏自然语言接口，而当前的自动形式化方法虽然能将自然语言证明转换为形式化证明，但仅限于验证完整证明，无法进行更细粒度的句子级验证。", "method": "提出了StepProof，一种新颖的自动形式化方法，旨在实现细粒度的逐步验证。StepProof将完整的证明分解为多个可验证的子证明，从而实现句子级别的验证。", "result": "实验结果表明，与传统方法相比，StepProof显著提高了证明成功率和效率。此外，对自然语言证明进行微小的手动调整，使其适应步级验证，进一步提升了StepProof在自动形式化方面的性能。", "conclusion": "StepProof通过引入逐步验证能力，有效解决了现有自动形式化方法在自然语言数学证明细粒度验证方面的局限性，提高了验证的成功率和效率。", "translation": "交互式定理证明器（ITPs）是强大的工具，可以将数学证明形式化验证到公理级别。然而，它们缺乏自然语言接口仍然是一个显著的限制。大型语言模型（LLMs）的最新进展增强了对自然语言输入的理解，为自动形式化铺平了道路——即将自然语言证明转换为可验证的形式化证明的过程。尽管取得了这些进展，但现有的自动形式化方法仅限于验证完整的证明，缺乏更精细的句子级验证能力。为了解决这一空白，我们提出了StepProof，这是一种新颖的自动形式化方法，专为细粒度的逐步验证而设计。StepProof将完整的证明分解为多个可验证的子证明，从而实现句子级验证。实验结果表明，与传统方法相比，StepProof显著提高了证明成功率和效率。此外，我们发现对自然语言证明进行微小的手动调整，使其适应步级验证，进一步提升了StepProof在自动形式化方面的性能。", "summary": "本文提出了StepProof，一种新颖的自动形式化方法，旨在解决现有方法在自然语言数学证明细粒度验证方面的不足。StepProof通过将完整证明分解为多个可验证的子证明，实现了句子级别的逐步验证。实验证明，StepProof显著提高了证明的成功率和效率，并且经过少量手动调整的自然语言证明能进一步优化其自动形式化性能。", "keywords": "StepProof, 自动形式化, 自然语言证明, 逐步验证, 形式化验证", "comments": "StepProof的创新之处在于引入了细粒度的、逐步的验证机制，解决了现有自动形式化方法只能验证完整证明的局限性。这对于提高自然语言数学证明的可靠性和形式化验证的效率具有重要意义。该方法通过分解证明为子证明，为更精确的错误定位和更灵活的交互提供了可能。"}}
{"id": "2506.10168", "title": "Momentum Multi-Marginal Schrödinger Bridge Matching", "authors": ["Panagiotis Theodoropoulos", "Augustinos D. Saravanos", "Evangelos A. Theodorou", "Guan-Horng Liu"], "summary": "Understanding complex systems by inferring trajectories from sparse sample\nsnapshots is a fundamental challenge in a wide range of domains, e.g.,\nsingle-cell biology, meteorology, and economics. Despite advancements in Bridge\nand Flow matching frameworks, current methodologies rely on pairwise\ninterpolation between adjacent snapshots. This hinders their ability to capture\nlong-range temporal dependencies and potentially affects the coherence of the\ninferred trajectories. To address these issues, we introduce \\textbf{Momentum\nMulti-Marginal Schr\\\"odinger Bridge Matching (3MSBM)}, a novel matching\nframework that learns smooth measure-valued splines for stochastic systems that\nsatisfy multiple positional constraints. This is achieved by lifting the\ndynamics to phase space and generalizing stochastic bridges to be conditioned\non several points, forming a multi-marginal conditional stochastic optimal\ncontrol problem. The underlying dynamics are then learned by minimizing a\nvariational objective, having fixed the path induced by the multi-marginal\nconditional bridge. As a matching approach, 3MSBM learns transport maps that\npreserve intermediate marginals throughout training, significantly improving\nconvergence and scalability. Extensive experimentation in a series of\nreal-world applications validates the superior performance of 3MSBM compared to\nexisting methods in capturing complex dynamics with temporal dependencies,\nopening new avenues for training matching frameworks in multi-marginal\nsettings.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10168v1", "AI": {"title_translation": "动量多边际薛定谔桥匹配", "tldr": "该论文引入了3MSBM，这是一个新颖的框架，通过结合动量和多边际条件化，从稀疏数据中推断轨迹，性能优于现有成对方法。", "motivation": "当前从稀疏样本快照推断轨迹的方法依赖于成对插值，这限制了它们捕获长程时间依赖性的能力，并可能影响推断轨迹的连贯性。", "method": "本文提出动量多边际薛定谔桥匹配（3MSBM）框架。它通过将动力学提升到相空间并将随机桥推广到在多个点上进行条件化，来学习满足多个位置约束的随机系统的平滑测度值样条，从而形成一个多边际条件随机最优控制问题。通过最小化变分目标来学习底层动力学，同时固定由多边际条件桥引起的路径。3MSBM作为一种匹配方法，学习在整个训练过程中保留中间边际的传输映射。", "result": "3MSBM显著提高了收敛性和可扩展性。在实际应用中的广泛实验验证了3MSBM在捕获具有时间依赖性的复杂动力学方面优于现有方法的卓越性能。", "conclusion": "3MSBM为在多边际设置中训练匹配框架提供了新途径，克服了成对插值的局限性，并改进了从稀疏数据中推断轨迹的能力。", "translation": "从稀疏样本快照中推断轨迹以理解复杂系统是广泛领域（例如单细胞生物学、气象学和经济学）中的一个基本挑战。尽管桥接和流匹配框架取得了进展，但当前方法依赖于相邻快照之间的成对插值。这阻碍了它们捕获长程时间依赖性的能力，并可能影响推断轨迹的连贯性。为了解决这些问题，我们引入了**动量多边际薛定谔桥匹配 (3MSBM)**，这是一种新颖的匹配框架，用于学习满足多个位置约束的随机系统的平滑测度值样条。这是通过将动力学提升到相空间并推广随机桥以在多个点上进行条件化来实现的，从而形成一个多边际条件随机最优控制问题。然后通过最小化变分目标来学习底层动力学，同时固定由多边际条件桥引起的路径。作为一种匹配方法，3MSBM 学习在整个训练过程中保留中间边际的传输映射，显著提高了收敛性和可扩展性。一系列实际应用中的广泛实验验证了 3MSBM 在捕获具有时间依赖性的复杂动力学方面优于现有方法的卓越性能，为在多边际设置中训练匹配框架开辟了新途径。", "summary": "3MSBM是一种新颖的匹配框架，通过将动力学提升到相空间并利用多边际条件随机最优控制，从稀疏快照中推断复杂系统的轨迹。它克服了现有成对插值方法在捕获长程时间依赖性方面的局限性，显著提高了收敛性和可扩展性，并在实际应用中展现出卓越的性能。", "keywords": "轨迹推断, 薛定谔桥, 多边际, 匹配框架, 随机最优控制", "comments": "该论文通过将轨迹推断从成对插值扩展到多边际设置，引入了显著的进步。通过整合动量并对多个点进行条件化，3MSBM解决了先前方法的关键限制，从而更好地捕获长程时间依赖性。收敛性和可扩展性的改进以及经验证的卓越性能，突显了其创新性和在各种科学领域的潜在影响。"}}
{"id": "2506.10195", "title": "Exploring Topological and Localization Phenomena in SSH Chains under Generalized AAH Modulation: A Computational Approach", "authors": ["Souvik Ghosh", "Sayak Roy"], "summary": "The Su-Schrieffer-Heeger (SSH) model serves as a canonical example of a\none-dimensional topological insulator, yet its behavior under more complex,\nrealistic conditions remains a fertile ground for research. This paper presents\na comprehensive computational investigation into generalized SSH models,\nexploring the interplay between topology, quasi-periodic disorder,\nnon-Hermiticity, and time-dependent driving. Using exact diagonalization and\nspecialized numerical solvers, we map the system's phase space through its\nspectral properties and localization characteristics, quantified by the Inverse\nParticipation Ratio (IPR). We demonstrate that while the standard SSH model\nexhibits topologically protected edge states, these are destroyed by a\nlocalization transition induced by strong Aubry-Andr\\'e-Harper (AAH)\nmodulation. Further, we employ unsupervised machine learning (PCA) to\nautonomously classify the system's phases, revealing that strong localization\ncan obscure underlying topological signatures. Extending the model beyond\nHermiticity, we uncover the non-Hermitian skin effect, a dramatic localization\nof all bulk states at a boundary. Finally, we apply a periodic Floquet drive to\na topologically trivial chain, successfully engineering a Floquet topological\ninsulator characterized by the emergence of anomalous edge states at the\nboundaries of the quasi-energy zone. These findings collectively provide a\nmulti-faceted view of the rich phenomena hosted in generalized 1D topological\nsystems.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.10195v1", "AI": {"title_translation": "在广义AAH调制下SSH链中拓扑和局域化现象的探索：一种计算方法", "tldr": "本文通过计算方法研究了广义SSH模型中拓扑、准周期无序、非厄米性及时间驱动的相互作用，揭示了复杂的拓扑和局域化现象。", "motivation": "Su-Schrieffer-Heeger (SSH) 模型在更复杂、更现实条件下的行为仍是研究热点，本文旨在对广义SSH模型进行全面的计算研究。", "method": "本文使用精确对角化和专门的数值求解器，通过光谱特性和逆参与率（IPR）量化局域化特性来映射系统相空间。此外，采用无监督机器学习（PCA）自主分类系统相。", "result": "研究发现，标准SSH模型的拓扑保护边缘态会被强Aubry-Andr\re-Harper (AAH) 调制引起的局域化转变破坏。强的局域化可能掩盖潜在的拓扑特征。揭示了非厄米趋肤效应，即所有体态在边界处的剧烈局域化。成功地在拓扑平凡链中工程化出Floquet拓扑绝缘体，其特点是准能量区边界出现异常边缘态。", "conclusion": "这些发现共同提供了广义一维拓扑系统中丰富现象的多方面视图。", "translation": "Su-Schrieffer-Heeger (SSH) 模型是​​一维拓扑绝缘体的典型例子，但其在更复杂、更现实条件下的行为仍是研究的沃土。本文对广义SSH模型进行了全面的计算研究，探索了拓扑、准周期无序、非厄米性和时间依赖驱动之间的相互作用。我们使用精确对角化和专门的数值求解器，通过系统的光谱特性和局域化特征（通过逆参与率（IPR）量化）来绘制系统的相空间。我们证明，虽然标准SSH模型表现出拓扑保护的边缘态，但这些态会被强Aubry-Andr\re-Harper (AAH) 调制引起的局域化转变所破坏。此外，我们采用无监督机器学习（PCA）自主分类系统的相，揭示了强局域化可能掩盖潜在的拓扑特征。将模型扩展到非厄米性之外，我们发现了非厄米趋肤效应，即所有体态在边界处的剧烈局域化。最后，我们将周期性Floquet驱动应用于拓扑平凡链，成功地工程化出Floquet拓扑绝缘体，其特点是准能量区边界出现异常边缘态。这些发现共同提供了广义一维拓扑系统中丰富现象的多方面视图。", "summary": "本文对广义Su-Schrieffer-Heeger (SSH) 模型进行了深入的计算研究，探讨了拓扑、准周期无序、非厄米性及时间驱动的复杂相互作用。研究利用精确对角化、数值求解器和无监督机器学习（PCA），分析了系统的光谱和局域化特性。结果表明，强AAH调制可破坏SSH模型的拓扑边缘态，且强局域化会掩盖拓扑特征。此外，研究揭示了非厄米趋肤效应，并成功通过Floquet驱动工程化出具有异常边缘态的Floquet拓扑绝缘体，为理解广义一维拓扑系统提供了多角度的见解。", "keywords": "SSH模型, 拓扑绝缘体, 局域化, 非厄米性, Floquet驱动", "comments": "本文通过结合多种计算方法（精确对角化、数值求解器、PCA），全面分析了广义SSH模型在复杂条件下的行为，特别是引入了非厄米性和时间驱动，并利用机器学习进行相分类，这在拓扑物理研究中具有创新性。研究揭示了拓扑与局域化之间复杂的相互作用，特别是非厄米趋肤效应和Floquet拓扑绝缘体的工程化，为未来对拓扑材料的设计和理解提供了重要的理论基础。"}}
{"id": "2506.10271", "title": "Predicting function of evolutionarily implausible DNA sequences", "authors": ["Shiyu Jiang", "Xuyin Liu", "Zitong Jerry Wang"], "summary": "Genomic language models (gLMs) show potential for generating novel,\nfunctional DNA sequences for synthetic biology, but doing so requires them to\nlearn not just evolutionary plausibility, but also sequence-to-function\nrelationships. We introduce a set of prediction tasks called Nullsettes, which\nassesses a model's ability to predict loss-of-function mutations created by\ntranslocating key control elements in synthetic expression cassettes. Across 12\nstate-of-the-art models, we find that mutation effect prediction performance\nstrongly correlates with the predicted likelihood of the nonmutant.\nFurthermore, the range of likelihood values predictive of strong model\nperformance is highly dependent on sequence length. Our work highlights the\nimportance of considering both sequence likelihood and sequence length when\nusing gLMs for mutation effect prediction.", "comment": "13 pages, 6 figures, accepted to ICML 2025 Generative AI and Biology\n  Workshop", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10271v1", "AI": {"title_translation": "预测进化上不合理的DNA序列功能", "tldr": "本文介绍了Nullsettes任务，评估基因组语言模型预测DNA序列功能丧失突变的能力，发现突变效应预测性能与非突变序列的可能性强相关，且受序列长度影响。", "motivation": "基因组语言模型（gLMs）在合成生物学中生成新型功能性DNA序列方面具有潜力，但它们不仅需要学习进化合理性，还需要学习序列到功能的关系。", "method": "引入了一套名为Nullsettes的预测任务，通过评估模型预测合成表达盒中关键控制元件易位导致的失功能突变的能力来评估模型。在12个最先进的模型上进行了测试。", "result": "突变效应预测性能与非突变序列的预测可能性强烈相关。此外，预测强模型性能的可能性值范围高度依赖于序列长度。", "conclusion": "在使用基因组语言模型进行突变效应预测时，考虑序列可能性和序列长度都非常重要。", "translation": "基因组语言模型（gLMs）在合成生物学中生成新型、功能性DNA序列方面显示出潜力，但这要求它们不仅要学习进化上的合理性，还要学习序列与功能之间的关系。我们引入了一套名为Nullsettes的预测任务，该任务评估模型预测由合成表达盒中关键控制元件易位引起的失功能突变的能力。在12个最先进的模型中，我们发现突变效应预测性能与非突变序列的预测可能性密切相关。此外，预测强模型性能的可能性值范围高度依赖于序列长度。我们的工作强调了在使用基因组语言模型进行突变效应预测时，考虑序列可能性和序列长度的重要性。", "summary": "本文提出Nullsettes预测任务，旨在评估基因组语言模型（gLMs）预测DNA序列功能丧失突变的能力。研究发现，模型的突变效应预测性能与非突变序列的预测可能性呈强相关性，并且这种相关性的可能性值范围受序列长度显著影响。研究强调了在使用gLMs进行突变效应预测时，必须同时考虑序列的可能性和序列长度。", "keywords": "基因组语言模型, DNA序列, 功能预测, 突变效应, Nullsettes", "comments": "这项工作通过引入Nullsettes任务，为评估基因组语言模型在预测DNA序列功能方面的能力提供了一个新颖且重要的基准。它揭示了序列可能性和序列长度对模型性能的关键影响，这对于提高gLMs在合成生物学中的实际应用具有指导意义。"}}
{"id": "2506.10275", "title": "VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for Scalable and Robust Quantum Machine Learning", "authors": ["Jun Qi", "Chao-Han Yang", "Pin-Yu Chen", "Min-Hsiu Hsieh"], "summary": "Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine\nlearning, yet their practical application is hindered by inherent limitations\nsuch as constrained linear expressivity, optimization challenges, and acute\nsensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a\nscalable and robust hybrid quantum-classical architecture designed to overcome\nthese obstacles. By innovatively employing quantum circuits to dynamically\ngenerate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude\nencoding and parameterized quantum operations, VQC-MLPNet substantially expands\nrepresentation capabilities and augments training stability. We provide\nrigorous theoretical guarantees via statistical learning techniques and Neural\nTangent Kernel analysis, explicitly deriving upper bounds on approximation,\nuniform deviation, and optimization errors. These theoretical insights\ndemonstrate exponential improvements in representation capacity relative to\nquantum circuit depth and the number of qubits, providing clear computational\nadvantages over standalone quantum circuits and existing hybrid quantum\narchitectures. Our theoretical claims are empirically corroborated through\nextensive experiments, including classifying semiconductor quantum-dot charge\nstates and predicting genomic transcription factor binding sites, demonstrating\nresilient performance even under realistic IBM quantum noise simulations. This\nresearch establishes a theoretically sound and practically robust framework,\nadvancing the frontiers of quantum-enhanced learning for unconventional\ncomputing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.", "comment": "31 pages, 11 figures, under review", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.10275v1", "AI": {"title_translation": "VQC-MLPNet：一种用于可扩展和鲁棒量子机器学习的非常规混合量子-经典架构", "tldr": "VQC-MLPNet是一种新的混合量子-经典架构，通过量子电路动态生成经典MLP参数，显著解决了变分量子电路的表达能力、优化挑战和噪声敏感性问题，为量子机器学习提供了可扩展和鲁棒的解决方案。", "motivation": "变分量子电路（VQCs）在量子机器学习中面临着受限的线性表达能力、优化挑战以及对量子硬件噪声的严重敏感性等固有局限性，阻碍了其实际应用。", "method": "VQC-MLPNet通过创新性地利用量子电路，通过幅度编码和参数化量子操作，为经典多层感知器（MLPs）动态生成参数。", "result": "理论上，通过统计学习技术和神经正切核分析，推导了近似、均匀偏差和优化误差的上限，证明了表示能力相对于量子电路深度和量子比特数量呈指数级提升，提供了计算优势。经验上，通过分类半导体量子点电荷态和预测基因组转录因子结合位点的广泛实验，证实了其在真实的IBM量子噪声模拟下仍表现出弹性性能。", "conclusion": "本研究建立了一个理论上健全且实践中鲁棒的框架，推动了噪声中等规模量子（NISQ）时代及未来非常规计算范式中量子增强学习的前沿。", "translation": "变分量子电路（VQCs）为量子机器学习提供了一条新颖的途径，但其实际应用受到固有局限性的阻碍，例如受限的线性表达能力、优化挑战以及对量子硬件噪声的严重敏感性。本工作介绍了VQC-MLPNet，一种可扩展且鲁棒的混合量子-经典架构，旨在克服这些障碍。通过创新性地利用量子电路通过幅度编码和参数化量子操作为经典多层感知器（MLPs）动态生成参数，VQC-MLPNet显著扩展了表示能力并增强了训练稳定性。我们通过统计学习技术和神经正切核分析提供了严格的理论保证，明确推导了近似、均匀偏差和优化误差的上限。这些理论见解表明，相对于量子电路深度和量子比特数量，表示能力呈指数级提升，与独立的量子电路和现有混合量子架构相比，提供了明显的计算优势。我们的理论主张通过广泛的实验得到了经验证实，包括对半导体量子点电荷态的分类和对基因组转录因子结合位点的预测，即使在真实的IBM量子噪声模拟下也表现出弹性性能。这项研究建立了一个理论上健全且实践中鲁布的框架，推动了噪声中等规模量子（NISQ）时代及未来非常规计算范式中量子增强学习的前沿。", "summary": "VQC-MLPNet是一种新颖的混合量子-经典架构，旨在解决变分量子电路在量子机器学习中面临的表达能力、优化和噪声敏感性问题。它通过量子电路为经典MLP动态生成参数，显著提升了表示能力和训练稳定性。该架构提供了严格的理论保证，表明其在表示能力上具有指数级优势，并通过实验验证了其在有噪声环境下的鲁棒性能，为量子增强学习提供了一个强大的框架。", "keywords": "混合量子-经典架构, 量子机器学习, 变分量子电路, 神经正切核, 噪声中等规模量子", "comments": "VQC-MLPNet的创新点在于其独特的混合架构，将量子电路用于动态生成经典MLP的参数，而非直接进行计算。这种方法有效解决了现有VQC的局限性，特别是其对噪声的鲁棒性以及表示能力的提升，为NISQ时代的量子机器学习提供了一个有前景的解决方案。其结合理论证明和实证验证，增强了研究的可信度。"}}
{"id": "2506.10293", "title": "Distributionally-Constrained Adversaries in Online Learning", "authors": ["Moïse Blanchard", "Samory Kpotufe"], "summary": "There has been much recent interest in understanding the continuum from\nadversarial to stochastic settings in online learning, with various frameworks\nincluding smoothed settings proposed to bridge this gap. We consider the more\ngeneral and flexible framework of distributionally constrained adversaries in\nwhich instances are drawn from distributions chosen by an adversary within some\nconstrained distribution class [RST11]. Compared to smoothed analysis, we\nconsider general distributional classes which allows for a fine-grained\nunderstanding of learning settings between fully stochastic and fully\nadversarial for which a learner can achieve non-trivial regret. We give a\ncharacterization for which distribution classes are learnable in this context\nagainst both oblivious and adaptive adversaries, providing insights into the\ntypes of interplay between the function class and distributional constraints on\nadversaries that enable learnability. In particular, our results recover and\ngeneralize learnability for known smoothed settings. Further, we show that for\nseveral natural function classes including linear classifiers, learning can be\nachieved without any prior knowledge of the distribution class -- in other\nwords, a learner can simultaneously compete against any constrained adversary\nwithin learnable distribution classes.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10293v1", "AI": {"title_translation": "在线学习中受分布约束的对抗者", "tldr": "本文探讨了在线学习中受分布约束的对抗者，表征了可学习的分布类别，并展示了在某些函数类别中无需先验知识即可学习。", "motivation": "为了理解在线学习中从对抗性设置到随机性设置的连续统一体，本文提出了一个更通用和灵活的受分布约束的对抗者框架，旨在对完全随机和完全对抗性之间的学习设置进行细粒度理解，以实现非平凡的遗憾。", "method": "本文提出了一个受分布约束的对抗者框架，其中实例由对抗者在受约束的分布类别中选择的分布生成。研究通过表征在无感知和自适应对抗者下哪些分布类别是可学习的，来深入了解函数类别和对抗者分布约束之间实现可学习性的相互作用类型。", "result": "本文表征了在无感知和自适应对抗者下可学习的分布类别，揭示了函数类别和分布约束之间实现可学习性的相互作用。研究结果恢复并推广了已知平滑设置的可学习性。此外，对于包括线性分类器在内的几种自然函数类别，学习可以在不具备任何先验分布类别知识的情况下实现，这意味着学习者可以同时与可学习分布类别中的任何受约束对抗者竞争。", "conclusion": "本文成功表征了受分布约束的对抗者下的可学习分布类别，并证明了对于某些函数类别，学习可以在不具备先验分布类别知识的情况下实现，从而弥合了完全随机和完全对抗性在线学习之间的鸿沟。", "translation": "近年来，人们对理解在线学习中从对抗性设置到随机性设置的连续统一体产生了浓厚兴趣，各种框架，包括平滑设置，被提出以弥合这一差距。我们考虑了更通用和灵活的“受分布约束的对抗者”框架，其中实例是从对抗者在某个受约束的分布类别中选择的分布中抽取的 [RST11]。与平滑分析相比，我们考虑了更一般的分布类别，这使得能够对完全随机和完全对抗性之间的学习设置进行细粒度理解，在这种设置下学习者可以实现非平凡的遗憾。我们对在这种情况下对抗无感知和自适应对抗者的可学习分布类别进行了表征，从而深入了解了函数类别和对抗者分布约束之间实现可学习性的相互作用类型。特别是，我们的结果恢复并推广了已知平滑设置的可学习性。此外，我们表明，对于包括线性分类器在内的几种自然函数类别，无需任何先验分布类别知识即可实现学习——换句话说，学习者可以同时与可学习分布类别中的任何受约束对抗者竞争。", "summary": "本文引入了在线学习中受分布约束的对抗者框架，旨在弥合随机设置和完全对抗性设置之间的差距。它表征了对抗无感知和自适应对抗者的可学习分布类别，展示了函数类别和分布约束如何实现可学习性。该工作推广了现有的平滑分析结果，并证明对于几种函数类别，无需先验的特定分布类别知识即可实现学习。", "keywords": "在线学习, 受分布约束对抗者, 可学习性, 平滑分析, 遗憾", "comments": "本文通过提出一个更通用的受分布约束的对抗者框架，超越了平滑分析，对在线学习设置提供了更细致的理解，从而做出了重要贡献。其对可学习分布类别的表征以及在没有先验分布知识的情况下学习的可行性证明是创新的，扩展了在线学习算法在非纯随机也非纯对抗性环境中的适用性。"}}
{"id": "2506.10305", "title": "Self-learning signal classifier for decameter coherent scatter radars", "authors": ["Oleg Berngardt", "Ivan Lavygin"], "summary": "The paper presents a method for automatic constructing a classifier for\nprocessed data obtained by decameter coherent scatter radars. Method is based\nonly on the radar data obtained, the results of automatic modeling of radio\nwave propagation in the ionosphere, and mathematical criteria for estimating\nthe quality of the models. The final classifier is the model trained at data\nobtained by 12 radars of the SuperDARN and SECIRA networks over two years for\neach radar. The number of the model coefficients is 2669. For the\nclassification, the model uses both the calculated parameters of radio wave\npropagation in the model ionosphere and the parameters directly measured by the\nradar. Calibration of radiowave elevation measurements at each radar was made\nusing meteor trail scattered signals. The analysis showed that the optimal\nnumber of classes in the data is 37, of which 25 are frequently observed. The\nanalysis made it possible to choose 14 classes from them, which are confidently\nseparated in other variants of model training. A preliminary interpretation of\n10 of them was carried out. The dynamics of observation of various classes and\ntheir dependence on the geographical latitude of radars at different levels of\nsolar and geomagnetic activity were presented, it was shown that it does not\ncontradict with known physical mechanisms. The analysis showed that the most\nimportant parameters to identify the classes are the shape of the signal\nray-tracing trajectory in its second half, the ray-traced scattering height and\nthe Doppler velocity measured by the radar.", "comment": "30 pages, 10 figures, 4 tables. To be submitted to Advances in Space\n  Research", "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.10305v1", "AI": {"title_translation": "十米波相干散射雷达自学习信号分类器", "tldr": "本文提出了一种自动构建用于十米波相干散射雷达处理数据的自学习信号分类器的方法，该分类器基于雷达数据、电离层无线电波传播模型及质量评估标准进行训练，并识别出关键分类参数和信号类别。", "motivation": "论文旨在为十米波相干散射雷达处理的数据自动构建一个分类器，以实现对雷达信号的自动识别和解释。", "method": "该方法基于雷达获取的数据、电离层中无线电波传播的自动建模结果以及评估模型质量的数学标准。最终分类器是一个在SuperDARN和SECIRA网络中12个雷达两年数据上训练的模型，模型系数为2669个。分类时，模型同时使用了模型电离层中计算的无线电波传播参数和雷达直接测量的参数。每个雷达的无线电波仰角测量通过流星尾迹散射信号进行校准。", "result": "数据中最佳类别数为37个，其中25个是常见类别。从37个类别中选择了14个可以可靠分离的类别，并对其中10个进行了初步解释。展示了不同类别观测的动态及其对雷达地理纬度、太阳和地磁活动水平的依赖性，结果与已知物理机制不矛盾。识别出最重要的分类参数是信号射线追踪轨迹后半部分的形状、射线追踪散射高度和雷达测量的多普勒速度。", "conclusion": "本文成功开发了一个自学习信号分类器，能够自动识别和解释十米波相干散射雷达数据中的多种信号类别，并揭示了关键的分类参数以及这些类别的观测动态与物理机制的一致性。", "translation": "本文提出了一种自动构建用于十米波相干散射雷达处理数据的分类器的方法。该方法仅基于获得的雷达数据、电离层中无线电波传播的自动建模结果以及评估模型质量的数学标准。最终的分类器是在SuperDARN和SECIRA网络中12个雷达两年内获得的数据上训练的模型。模型系数为2669个。为了进行分类，模型同时使用了模型电离层中计算的无线电波传播参数和雷达直接测量的参数。每个雷达的无线电波仰角测量通过流星尾迹散射信号进行校准。分析表明，数据中最佳的类别数为37个，其中25个是经常观测到的。该分析使得从中选择了14个类别，这些类别在模型训练的其他变体中可以自信地分离。对其中10个进行了初步解释。展示了各种类别观测的动态及其对不同太阳和地磁活动水平下雷达地理纬度的依赖性，结果表明这与已知的物理机制不矛盾。分析表明，识别这些类别最重要的参数是信号射线追踪轨迹后半部分的形状、射线追踪散射高度和雷达测量的多普勒速度。", "summary": "本文介绍了一种用于十米波相干散射雷达数据处理的自学习信号分类器。该分类器整合了雷达实测数据、电离层无线电波传播模型及质量评估标准进行训练，并利用SuperDARN和SECIRA网络中12个雷达两年的数据进行模型构建。研究确定了数据中37个最佳类别，并成功分离出14个可信类别，其中10个已进行初步解释。分类器分析了不同类别观测的动态及其与地理纬度、太阳及地磁活动的关联，发现结果与已知物理机制相符。此外，研究还揭示了信号射线追踪轨迹形状、散射高度和多普勒速度是识别类别的关键参数。", "keywords": "自学习分类器, 十米波相干散射雷达, 电离层, 无线电波传播, 信号分类", "comments": "创新性: 提出了一种基于雷达数据、电离层传播模型和质量标准的自学习分类方法，实现了雷达信号的自动分类，这在雷达数据分析中具有重要意义。重要性: 能够自动识别和解释复杂的雷达散射信号，有助于深入理解电离层物理过程，并可能提高雷达数据处理的效率和准确性。局限性: 抽象中未详细说明分类器的具体算法（例如，是神经网络、SVM还是其他），也未提供性能指标（如准确率、召回率等），这使得难以全面评估其分类能力。对10个类别的初步解释也未在摘要中展开。"}}
{"id": "2506.10797", "title": "Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation", "authors": ["Nicholas Summerfield", "Qisheng He", "Alex Kuo", "Ahmed I. Ghanem", "Simeng Zhu", "Chase Ruff", "Joshua Pan", "Anudeep Kumar", "Prashant Nagpal", "Jiwei Zhao", "Ming Dong", "Carri K. Glide-Hurst"], "summary": "Cardiac substructures are essential in thoracic radiation therapy planning to\nminimize risk of radiation-induced heart disease. Deep learning (DL) offers\nefficient methods to reduce contouring burden but lacks generalizability across\ndifferent modalities and overlapping structures. This work introduces and\nvalidates a Modality-AGnostic Image Cascade (MAGIC) for comprehensive and\nmulti-modal cardiac substructure segmentation. MAGIC is implemented through\nreplicated encoding and decoding branches of an nnU-Net-based, U-shaped\nbackbone conserving the function of a single model. Twenty cardiac\nsubstructures (heart, chambers, great vessels (GVs), valves, coronary arteries\n(CAs), and conduction nodes) from simulation CT (Sim-CT), low-field MR-Linac,\nand cardiac CT angiography (CCTA) modalities were manually delineated and used\nto train (n=76), validate (n=15), and test (n=30) MAGIC. Twelve comparison\nmodels (four segmentation subgroups across three modalities) were equivalently\ntrained. All methods were compared for training efficiency and against\nreference contours using the Dice Similarity Coefficient (DSC) and two-tailed\nWilcoxon Signed-Rank test (threshold, p<0.05). Average DSC scores were\n0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. MAGIC\noutperforms the comparison in 57% of cases, with limited statistical\ndifferences. MAGIC offers an effective and accurate segmentation solution that\nis lightweight and capable of segmenting multiple modalities and overlapping\nstructures in a single model. MAGIC further enables clinical implementation by\nsimplifying the computational requirements and offering unparalleled\nflexibility for clinical settings.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.10797v1", "AI": {"title_translation": "模态无关图像级联（MAGIC）用于多模态心脏亚结构分割", "tldr": "本文提出并验证了一种名为MAGIC的模态无关图像级联方法，用于多模态心脏亚结构分割，它在单个模型中实现了高效、准确的分割，并展现出良好的泛化能力和计算效率。", "motivation": "在胸部放射治疗计划中，心脏亚结构对于最小化放射诱导心脏病的风险至关重要。深度学习（DL）提供了高效的方法来减少勾画负担，但其泛化能力在不同模态和重叠结构之间存在不足。", "method": "本研究引入并验证了模态无关图像级联（MAGIC），用于全面和多模态心脏亚结构分割。MAGIC通过nnU-Net基的U形骨干网的复制编码和解码分支实现，保留了单个模型的功能。使用来自模拟CT（Sim-CT）、低场MR-Linac和心脏CT血管造影（CCTA）模态的20个心脏亚结构（包括心脏、心腔、大血管、瓣膜、冠状动脉和传导结）进行手动勾画，并用于训练（n=76）、验证（n=15）和测试（n=30）MAGIC。同时训练了12个比较模型。所有方法通过Dice相似系数（DSC）和双尾Wilcoxon符号秩检验（阈值p<0.05）进行训练效率和与参考轮廓的比较。", "result": "MAGIC在57%的病例中优于比较模型，统计学差异有限。Sim-CT的平均DSC得分为0.75(0.16)，MR-Linac为0.68(0.21)，CCTA为0.80(0.16)。MAGIC提供了一种有效、准确且轻量级的分割解决方案，能够在一个模型中分割多种模态和重叠结构。", "conclusion": "MAGIC提供了一种有效且准确的分割解决方案，该方案轻量级且能够在一个模型中分割多种模态和重叠结构。MAGIC通过简化计算要求并为临床环境提供无与伦比的灵活性，进一步实现了临床实施。", "translation": "心脏亚结构在胸部放射治疗计划中至关重要，以最大限度地降低放射诱导心脏病的风险。深度学习（DL）提供了有效的方法来减少勾画负担，但其在不同模态和重叠结构之间的泛化能力不足。本工作引入并验证了一种模态无关图像级联（MAGIC），用于全面和多模态心脏亚结构分割。MAGIC通过基于nnU-Net的U形骨干网的复制编码和解码分支实现，保留了单个模型的功能。来自模拟CT（Sim-CT）、低场MR-Linac和心脏CT血管造影（CCTA）模态的20个心脏亚结构（心脏、心腔、大血管、瓣膜、冠状动脉和传导结）被手动勾画，并用于训练（n=76）、验证（n=15）和测试（n=30）MAGIC。十二个比较模型（三个模态下的四个分割亚组）被等效训练。所有方法通过训练效率以及与参考轮廓的Dice相似系数（DSC）和双尾Wilcoxon符号秩检验（阈值，p<0.05）进行比较。Sim-CT的平均DSC得分为0.75(0.16)，MR-Linac为0.68(0.21)，CCTA为0.80(0.16)。MAGIC在57%的病例中优于比较模型，统计学差异有限。MAGIC提供了一种有效且准确的分割解决方案，该方案轻量级且能够在一个模型中分割多种模态和重叠结构。MAGIC通过简化计算要求并为临床环境提供无与伦比的灵活性，进一步实现了临床实施。", "summary": "本文提出了一种名为模态无关图像级联（MAGIC）的深度学习方法，用于多模态心脏亚结构分割。该方法基于nnU-Net的U形骨干网，通过复制编码和解码分支实现单个模型的通用性。研究在Sim-CT、MR-Linac和CCTA三种模态的20个心脏亚结构数据集上进行了训练和测试。结果显示，MAGIC在57%的病例中表现优于比较模型，并提供了高效、准确、轻量级且适用于多种模态和重叠结构分割的解决方案，有望简化临床应用。", "keywords": "心脏分割, 多模态, 深度学习, MAGIC, nnU-Net", "comments": "本文提出的MAGIC方法通过其模态无关的特性，解决了深度学习在不同医学图像模态间泛化能力不足的挑战，具有重要的创新性。其在单个模型中处理多模态和重叠结构的能力，显著简化了计算要求，为临床实施提供了更高的灵活性和效率。这对于放射治疗计划中精确的心脏亚结构勾画具有实际应用价值。"}}
{"id": "2506.10433", "title": "Measuring Semantic Information Production in Generative Diffusion Models", "authors": ["Florian Handke", "Félix Koulischer", "Gabriel Raya", "Luca Ambrogioni"], "summary": "It is well known that semantic and structural features of the generated\nimages emerge at different times during the reverse dynamics of diffusion, a\nphenomenon that has been connected to physical phase transitions in magnets and\nother materials. In this paper, we introduce a general information-theoretic\napproach to measure when these class-semantic \"decisions\" are made during the\ngenerative process. By using an online formula for the optimal Bayesian\nclassifier, we estimate the conditional entropy of the class label given the\nnoisy state. We then determine the time intervals corresponding to the highest\ninformation transfer between noisy states and class labels using the time\nderivative of the conditional entropy. We demonstrate our method on\none-dimensional Gaussian mixture models and on DDPM models trained on the\nCIFAR10 dataset. As expected, we find that the semantic information transfer is\nhighest in the intermediate stages of diffusion while vanishing during the\nfinal stages. However, we found sizable differences between the entropy rate\nprofiles of different classes, suggesting that different \"semantic decisions\"\nare located at different intermediate times.", "comment": "4 pages, 3 figures, an appendix with derivations and implementation\n  details, accepted at ICLR DeLTa 2025", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10433v1", "AI": {"title_translation": "生成扩散模型中语义信息产生的测量", "tldr": "本文提出了一种信息论方法来测量生成扩散模型中类语义决策的产生时间，发现语义信息转移在中间阶段最高，但不同类别之间存在差异。", "motivation": "生成扩散模型中生成的图像的语义和结构特征在反向动力学过程中在不同时间出现，这种现象与磁体和其他材料中的物理相变有关。本文旨在引入一种通用的信息论方法来测量这些类语义“决策”在生成过程中何时做出。", "method": "本文引入了一种通用的信息论方法。通过使用最优贝叶斯分类器的在线公式，估计给定噪声状态下类别标签的条件熵。然后，利用条件熵的时间导数来确定噪声状态和类别标签之间信息传输最高的时段。该方法在1D高斯混合模型和在CIFAR10数据集上训练的DDPM模型上进行了演示。", "result": "研究发现，语义信息传输在扩散的中间阶段最高，而在最后阶段消失。然而，不同类别的熵率分布存在显著差异，表明不同的“语义决策”位于不同的中间时间。", "conclusion": "生成扩散模型中的语义信息传输在扩散的中间阶段达到峰值，但具体时间因类别而异，表明不同的语义特征在不同时间得到解析。", "translation": "众所周知，生成图像的语义和结构特征在扩散的反向动力学过程中在不同时间出现，这种现象已与磁体和其他材料中的物理相变相关联。在本文中，我们引入了一种通用的信息论方法来测量这些类语义“决策”在生成过程中何时做出。通过使用最优贝叶斯分类器的在线公式，我们估计了给定噪声状态下类别标签的条件熵。然后，我们使用条件熵的时间导数来确定噪声状态和类别标签之间信息传输最高的时段。我们在1D高斯混合模型和在CIFAR10数据集上训练的DDPM模型上演示了我们的方法。正如预期的那样，我们发现语义信息传输在扩散的中间阶段最高，而在最后阶段消失。然而，我们发现不同类别的熵率分布存在显著差异，这表明不同的“语义决策”位于不同的中间时间。", "summary": "本文提出了一种信息论方法，用于量化生成扩散模型中语义信息出现的时机。通过分析给定噪声状态下类别标签的条件熵，研究表明语义信息传输在扩散的中间阶段最高，并在最终阶段逐渐消失。值得注意的是，这些“语义决策”发生的时间在不同类别之间存在显著差异，这为理解这些模型的内部动态提供了见解。", "keywords": "语义信息, 扩散模型, 信息论, 贝叶斯分类器, 条件熵", "comments": "本文提供了一个新颖的信息论框架，用于理解扩散模型的内部动态，特别是语义信息如何随时间处理。这种方法为先前通过定性观察（特征的出现）的现象提供了定量测量。发现不同类别表现出不同的语义决策时间特别有洞察力，为类别特定的优化或理解模型偏差开辟了途径。"}}
{"id": "2506.10475", "title": "Prediction of steady states in a marine ecosystem model by a machine learning technique", "authors": ["Sarker Miraz Mahfuz", "Thomas Slawig"], "summary": "We used precomputed steady states obtained by a spin-up for a global marine\necosystem model as training data to build a mapping from the small number of\nbiogeochemical model parameters onto the three-dimensional converged steady\nannual cycle. The mapping was performed by a conditional variational\nautoencoder (CVAE) with mass correction. Applied for test data, we show that\nthe prediction obtained by the CVAE already gives a reasonable good\napproximation of the steady states obtained by a regular spin-up. However, the\npredictions do not reach the same level of annual periodicity as those obtained\nin the original spin-up data. Thus, we took the predictions as initial values\nfor a spin-up. We could show that the number of necessary iterations,\ncorresponding to model years, to reach a prescribed stopping criterion in the\nspin-up could be significantly reduced compared to the use of the originally\nuniform, constant initial value. The amount of reduction depends on the applied\nstopping criterion, measuring the periodicity of the solution. The savings in\nneeded iterations and, thus, computing time for the spin-up ranges from 50 to\n95\\%, depending on the stopping criterion for the spin-up. We compared these\nresults with the use of the mean of the training data as an initial value. We\nfound that this also accelerates the spin-up, but only by a much lower factor.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10475v1", "AI": {"title_translation": "海洋生态系统模型稳态的机器学习预测", "tldr": "本文利用条件变分自编码器（CVAE）预测海洋生态系统模型的稳态，并将其作为自旋启动的初始值，成功将模型达到稳态所需的计算时间缩短了50%到95%。", "motivation": "为了显著减少全球海洋生态系统模型达到稳态所需的计算时间，并提高自旋启动过程的效率。", "method": "研究人员使用通过常规自旋启动获得的预计算稳态作为训练数据，利用带有质量校正的条件变分自编码器（CVAE）构建了一个从模型参数到三维稳态年周期的映射。随后，将CVAE的预测结果作为模型自旋启动的初始值，并与使用均匀常数初始值和训练数据均值作为初始值的情况进行了比较。", "result": "CVAE的预测结果能合理近似稳态，但周期性不如原始自旋启动数据。将CVAE预测作为自旋启动的初始值，可显著减少达到稳态所需的迭代次数，从而节省50%至95%的计算时间。与使用训练数据均值作为初始值相比，CVAE方法的加速效果更为显著。", "conclusion": "机器学习技术，特别是条件变分自编码器（CVAE），可以作为海洋生态系统模型自旋启动的有效初始值，大幅度缩短模型达到稳态所需的计算时间，从而提高计算效率。", "translation": "我们使用通过全局海洋生态系统模型自旋启动获得的预计算稳态作为训练数据，以构建从少量生物地球化学模型参数到三维收敛稳态年周期的映射。该映射通过带有质量校正的条件变分自编码器（CVAE）执行。应用于测试数据时，我们表明CVAE获得的预测已经能够合理地近似通过常规自旋启动获得的稳态。然而，这些预测未能达到与原始自旋启动数据相同的年周期性水平。因此，我们将这些预测作为自旋启动的初始值。我们能够证明，与使用原始均匀常数初始值相比，自旋启动中达到预设停止准则所需的迭代次数（对应于模型年数）可以显著减少。减少量取决于所应用的停止准则，该准则衡量解的周期性。自旋启动所需迭代次数和计算时间的节省范围为50%到95%，具体取决于自旋启动的停止准则。我们将这些结果与使用训练数据均值作为初始值的情况进行了比较。我们发现这也加速了自旋启动，但加速因子要低得多。", "summary": "本文提出一种利用条件变分自编码器（CVAE）加速全球海洋生态系统模型自旋启动的方法。研究人员将预计算的稳态作为训练数据，通过CVAE学习从模型参数到稳态年周期的映射。尽管CVAE直接预测的周期性有待提高，但将其预测作为自旋启动的初始值，可显著减少模型达到稳态所需的迭代次数，从而节省50%至95%的计算时间，且优于使用训练数据均值作为初始值的方法。", "keywords": "海洋生态系统模型, 稳态预测, 机器学习, 条件变分自编码器, 自旋启动", "comments": "本文创新性地将机器学习（CVAE）应用于加速复杂的海洋生态系统模型的自旋启动过程，为减少地球系统模型预热阶段的计算成本提供了有效途径。尽管CVAE直接预测的周期性有待提高，但其作为优化初始值的应用效果显著，具有重要的实际意义和潜在的广泛应用前景。"}}
{"id": "2506.10552", "title": "On the role of non-linear latent features in bipartite generative neural networks", "authors": ["Tony Bonnaire", "Giovanni Catania", "Aurélien Decelle", "Beatriz Seoane"], "summary": "We investigate the phase diagram and memory retrieval capabilities of\nbipartite energy-based neural networks, namely Restricted Boltzmann Machines\n(RBMs), as a function of the prior distribution imposed on their hidden units -\nincluding binary, multi-state, and ReLU-like activations. Drawing connections\nto the Hopfield model and employing analytical tools from statistical physics\nof disordered systems, we explore how the architectural choices and activation\nfunctions shape the thermodynamic properties of these models. Our analysis\nreveals that standard RBMs with binary hidden nodes and extensive connectivity\nsuffer from reduced critical capacity, limiting their effectiveness as\nassociative memories. To address this, we examine several modifications, such\nas introducing local biases and adopting richer hidden unit priors. These\nadjustments restore ordered retrieval phases and markedly improve recall\nperformance, even at finite temperatures. Our theoretical findings, supported\nby finite-size Monte Carlo simulations, highlight the importance of hidden unit\ndesign in enhancing the expressive power of RBMs.", "comment": "23 pages, 5 figures", "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.10552v1", "AI": {"title_translation": "关于非线性潜在特征在二分生成神经网络中的作用", "tldr": "本文研究了受限玻尔兹曼机（RBM）的相图和记忆检索能力，发现标准RBM存在容量限制，通过引入局部偏差和更丰富的隐藏单元先验可以显著提高其性能和记忆检索能力。", "motivation": "研究双向能量神经网络（受限玻尔兹曼机，RBM）的相图和记忆检索能力，并探索隐藏单元先验分布（包括二元、多态和ReLU类激活）对其性能的影响，以解决标准RBM作为联想记忆时临界容量降低的问题。", "method": "本文将RBM与Hopfield模型联系起来，并采用无序系统统计物理的分析工具来探索架构选择和激活函数如何影响模型的“热力学”特性。通过理论分析和有限尺寸蒙特卡洛模拟支持了研究结果。", "result": "研究发现，具有二元隐藏节点和广泛连接的标准RBM的临界容量降低，限制了其作为联想记忆的有效性。通过引入局部偏差和采用更丰富的隐藏单元先验，可以恢复有序的检索阶段，显著提高召回性能，即使在有限温度下也是如此。", "conclusion": "隐藏单元的设计对于增强受限玻尔兹曼机（RBM）的表达能力至关重要，通过调整隐藏单元的先验分布和引入局部偏差可以显著提高其记忆检索能力和临界容量。", "translation": "我们研究了二分能量神经网络（即受限玻尔兹曼机，RBM）的相图和记忆检索能力，作为对其隐藏单元施加的先验分布的函数——包括二元、多态和ReLU类激活。通过与霍普菲尔德模型建立联系，并采用无序系统统计物理的分析工具，我们探索了架构选择和激活函数如何塑造这些模型的热力学性质。我们的分析表明，具有二元隐藏节点和广泛连接的标准RBM的临界容量降低，限制了它们作为联想记忆的有效性。为了解决这个问题，我们研究了几种修改，例如引入局部偏差和采用更丰富的隐藏单元先验。这些调整恢复了有序的检索阶段，并显著提高了召回性能，即使在有限温度下也是如此。我们的理论发现得到了有限尺寸蒙特卡洛模拟的支持，突出了隐藏单元设计在增强RBM表达能力方面的重要性。", "summary": "本研究深入探讨了受限玻尔兹曼机（RBM）的相图和记忆检索能力，特别关注隐藏单元的非线性先验分布（如二元、多态和ReLU激活）对其性能的影响。通过统计物理分析和蒙特卡洛模拟，论文揭示了标准RBM在作为联想记忆时存在容量限制。研究表明，通过引入局部偏差和采用更丰富的隐藏单元先验，可以显著提升RBM的记忆检索性能和鲁棒性，强调了隐藏单元设计在增强模型表达能力中的关键作用。", "keywords": "受限玻尔兹曼机, 记忆检索, 隐藏单元, 先验分布, 统计物理", "comments": "本文通过引入统计物理的分析工具，深入探讨了RBM的内在机制及其作为联想记忆的局限性。其创新之处在于提出并验证了通过调整隐藏单元的先验分布和引入局部偏差可以有效提升RBM的性能，为RBM的设计和应用提供了新的思路。该研究对于理解神经网络的记忆机制和提升其学习能力具有重要意义。"}}
{"id": "2506.10572", "title": "Box-Constrained Softmax Function and Its Application for Post-Hoc Calibration", "authors": ["Kyohei Atarashi", "Satoshi Oyama", "Hiromi Arai", "Hisashi Kashima"], "summary": "Controlling the output probabilities of softmax-based models is a common\nproblem in modern machine learning. Although the $\\mathrm{Softmax}$ function\nprovides soft control via its temperature parameter, it lacks the ability to\nenforce hard constraints, such as box constraints, on output probabilities,\nwhich can be critical in certain applications requiring reliable and\ntrustworthy models. In this work, we propose the box-constrained softmax\n($\\mathrm{BCSoftmax}$) function, a novel generalization of the\n$\\mathrm{Softmax}$ function that explicitly enforces lower and upper bounds on\noutput probabilities. While $\\mathrm{BCSoftmax}$ is formulated as the solution\nto a box-constrained optimization problem, we develop an exact and efficient\ncomputation algorithm for $\\mathrm{BCSoftmax}$. As a key application, we\nintroduce two post-hoc calibration methods based on $\\mathrm{BCSoftmax}$. The\nproposed methods mitigate underconfidence and overconfidence in predictive\nmodels by learning the lower and upper bounds of the output probabilities or\nlogits after model training, thereby enhancing reliability in downstream\ndecision-making tasks. We demonstrate the effectiveness of our methods\nexperimentally using the TinyImageNet, CIFAR-100, and 20NewsGroups datasets,\nachieving improvements in calibration metrics.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10572v1", "AI": {"title_translation": "盒约束Softmax函数及其在事后校准中的应用", "tldr": "提出了一种新的盒约束Softmax函数（BCSoftmax），它能对输出概率施加硬性上下限，并通过事后校准方法改善模型置信度，提高可靠性。", "motivation": "现有的Softmax函数缺乏对输出概率施加硬性约束（如盒约束）的能力，这在某些需要可靠和可信模型的应用中至关重要。", "method": "提出了盒约束Softmax（BCSoftmax）函数，它是Softmax函数的一种新颖泛化，明确地对输出概率施加下限和上限。BCSoftmax被公式化为盒约束优化问题的解决方案，并开发了精确高效的计算算法。在此基础上，引入了两种基于BCSoftmax的事后校准方法，通过在模型训练后学习输出概率或logits的上下限来缓解预测模型中的欠置信和过置信问题。", "result": "在TinyImageNet、CIFAR-100和20NewsGroups数据集上通过实验证明了方法的有效性，在校准指标上取得了改进。", "conclusion": "BCSoftmax函数及其事后校准方法能够有效缓解预测模型中的欠置信和过置信问题，从而增强模型在下游决策任务中的可靠性。", "translation": "控制基于Softmax的模型的输出概率是现代机器学习中的一个常见问题。尽管Softmax函数通过其温度参数提供了软控制，但它缺乏对输出概率施加硬性约束（例如盒约束）的能力，这在某些需要可靠和可信模型的应用中至关重要。在这项工作中，我们提出了盒约束Softmax（BCSoftmax）函数，它是Softmax函数的一种新颖泛化，明确地对输出概率施加下限和上限。虽然BCSoftmax被公式化为盒约束优化问题的解决方案，但我们为其开发了一种精确高效的计算算法。作为一个关键应用，我们引入了两种基于BCSoftmax的事后校准方法。所提出的方法通过在模型训练后学习输出概率或logits的下限和上限，从而缓解预测模型中的欠置信和过置信问题，从而增强下游决策任务的可靠性。我们通过使用TinyImageNet、CIFAR-100和20NewsGroups数据集的实验证明了我们方法的有效性，并在校准指标上取得了改进。", "summary": "本文提出了一种名为盒约束Softmax (BCSoftmax) 的新型Softmax函数泛化，旨在解决传统Softmax无法对输出概率施加硬性上下限的问题。BCSoftmax通过解决盒约束优化问题来实现，并提供高效的计算算法。作者进一步基于BCSoftmax开发了两种事后校准方法，通过学习输出概率的边界来缓解模型的欠置信和过置信问题，从而提高模型在下游任务中的可靠性。实验结果表明，该方法在多个数据集上有效提升了校准指标。", "keywords": "盒约束Softmax, 事后校准, 输出概率, 模型可靠性, 置信度控制", "comments": "这项工作创新性地将盒约束引入Softmax函数，解决了传统Softmax缺乏硬性约束能力的问题。其提出的BCSoftmax及其事后校准方法对于提升机器学习模型在关键应用中的可靠性和可信度具有重要意义，特别是在需要精确控制输出概率的场景。方法的有效性和计算效率是其亮点。"}}
{"id": "2506.10660", "title": "Pushing the Limits of Extreme Weather: Constructing Extreme Heatwave Storylines with Differentiable Climate Models", "authors": ["Tim Whittaker", "Alejandro Di Luca"], "summary": "Understanding the plausible upper bounds of extreme weather events is\nessential for risk assessment in a warming climate. Existing methods, based on\nlarge ensembles of physics-based models, are often computationally expensive or\nlack the fidelity needed to simulate rare, high-impact extremes. Here, we\npresent a novel framework that leverages a differentiable hybrid climate model,\nNeuralGCM, to optimize initial conditions and generate physically consistent\nworst-case heatwave trajectories. Applied to the 2021 Pacific Northwest\nheatwave, our method produces temperature anomalies up to 3.7 $^\\circ$C above\nthe most extreme member of a 75-member ensemble. These trajectories feature\nintensified atmospheric blocking and amplified Rossby wave patterns--hallmarks\nof severe heat events. Our results demonstrate that differentiable climate\nmodels can efficiently explore the upper tails of event likelihoods, providing\na powerful new approach for constructing targeted storylines of extreme weather\nunder climate change.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10660v1", "AI": {"title_translation": "突破极端天气的极限：利用可微分气候模型构建极端热浪情景", "tldr": "本研究提出了一种利用可微分混合气候模型（NeuralGCM）构建极端热浪情景的新框架，能够高效探索极端事件的上限，生成比现有集成模型更严重的极端热浪轨迹，为气候变化下的风险评估提供新方法。", "motivation": "在气候变暖的背景下，理解极端天气事件的合理上限对于风险评估至关重要。现有基于大量物理模型的集合方法计算成本高昂，或缺乏模拟罕见、高影响极端事件所需的保真度。", "method": "本研究提出了一种新颖的框架，利用可微分混合气候模型NeuralGCM来优化初始条件，并生成物理上一致的最坏情况热浪轨迹。", "result": "该方法应用于2021年太平洋西北部热浪，产生的温度异常比75个成员集合中最极端的成员高出3.7°C。这些轨迹表现出大气阻塞加剧和罗斯贝波模式增强的特征。", "conclusion": "研究结果表明，可微分气候模型可以有效地探索事件可能性的上限，为构建气候变化下极端天气的特定情景提供了一种强大的新方法。", "translation": "了解极端天气事件的合理上限对于变暖气候下的风险评估至关重要。现有基于大量物理模型的集合方法通常计算成本高昂，或缺乏模拟罕见、高影响极端事件所需的保真度。在此，我们提出了一种新颖的框架，利用可微分混合气候模型NeuralGCM来优化初始条件，并生成物理上一致的最坏情况热浪轨迹。将该方法应用于2021年太平洋西北部热浪，我们的方法产生的温度异常比75个成员集合中最极端的成员高出3.7°C。这些轨迹表现出大气阻塞加剧和罗斯贝波模式增强的特征——这是严重热事件的标志。我们的结果表明，可微分气候模型可以有效地探索事件可能性的上限，为构建气候变化下极端天气的特定情景提供了一种强大的新方法。", "summary": "本论文提出了一种新颖的框架，利用可微分混合气候模型NeuralGCM来构建极端热浪情景，以克服现有基于物理模型的集合方法在计算成本和模拟罕见极端事件保真度方面的局限性。该方法通过优化初始条件生成物理上一致的最坏情况热浪轨迹，并在2021年太平洋西北部热浪中验证，成功生成了比传统集合模型更严重的温度异常。研究表明，可微分气候模型能高效探索极端事件的上限，为气候变化下的风险评估和极端天气情景构建提供了一种新颖且强大的方法。", "keywords": "极端热浪, 可微分气候模型, NeuralGCM, 风险评估, 气候变化", "comments": "这篇论文的创新点在于引入了可微分气候模型（NeuralGCM）来探索极端天气事件的上限，这为传统上计算成本高昂或保真度不足的方法提供了一个高效且强大的替代方案。通过优化初始条件来生成“最坏情况”情景，对于风险评估和气候适应规划具有重要意义。该方法能够生成比现有模型更极端的事件，揭示了潜在的未被充分认识的风险。"}}
{"id": "2506.10664", "title": "Logarithmic Smoothing for Adaptive PAC-Bayesian Off-Policy Learning", "authors": ["Maxime Haddouche", "Otmane Sakhi"], "summary": "Off-policy learning serves as the primary framework for learning optimal\npolicies from logged interactions collected under a static behavior policy. In\nthis work, we investigate the more practical and flexible setting of adaptive\noff-policy learning, where policies are iteratively refined and re-deployed to\ncollect higher-quality data. Building on the success of PAC-Bayesian learning\nwith Logarithmic Smoothing (LS) in static settings, we extend this framework to\nthe adaptive scenario using tools from online PAC-Bayesian theory. Furthermore,\nwe demonstrate that a principled adjustment to the LS estimator naturally\naccommodates multiple rounds of deployment and yields faster convergence rates\nunder mild conditions. Our method matches the performance of leading offline\napproaches in static settings, and significantly outperforms them when\nintermediate policy deployments are allowed. Empirical evaluations across\ndiverse scenarios highlight both the advantages of adaptive data collection and\nthe strength of the PAC-Bayesian formulation.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10664v1", "AI": {"title_translation": "自适应PAC-贝叶斯离策略学习的对数平滑", "tldr": "该研究将PAC-贝叶斯对数平滑（LS）框架扩展到自适应离策略学习场景，通过调整LS估计器，在多轮策略部署下实现了更快的收敛速度和显著的性能提升。", "motivation": "传统的离策略学习主要基于静态行为策略收集的日志交互数据。本研究旨在探索更实际和灵活的自适应离策略学习设置，即策略可以迭代优化并重新部署以收集更高质量的数据，但需要新的方法来处理这种动态性。", "method": "研究将成功的PAC-贝叶斯对数平滑（LS）框架从静态设置扩展到自适应场景，利用了在线PAC-贝叶斯理论的工具。此外，对LS估计器进行了原理性调整，以适应多轮部署。", "result": "在静态设置中，本方法与领先的离线方法表现相当。在允许中间策略部署的情况下，本方法显著优于这些领先的离线方法，并在温和条件下实现了更快的收敛速度。", "conclusion": "自适应数据收集的优势以及PAC-贝叶斯公式的强大之处在离策略学习中得到了体现，通过对数平滑的调整，可以显著提升性能并加速收敛。", "translation": "离策略学习是根据静态行为策略收集的日志交互数据来学习最优策略的主要框架。在这项工作中，我们研究了更实际和灵活的自适应离策略学习设置，其中策略被迭代优化并重新部署以收集更高质量的数据。在静态设置中PAC-贝叶斯学习与对数平滑（LS）的成功基础上，我们利用在线PAC-贝叶斯理论的工具将此框架扩展到自适应场景。此外，我们证明了对LS估计器进行原则性调整可以自然地适应多轮部署，并在温和条件下产生更快的收敛速度。我们的方法在静态设置中与领先的离线方法表现相当，并且在允许中间策略部署时显著优于它们。对各种场景的实证评估突出了自适应数据收集的优势以及PAC-贝叶斯公式的强大之处。", "summary": "本文将PAC-贝叶斯对数平滑（LS）框架扩展到自适应离策略学习，这是一个策略迭代优化和重新部署以收集高质量数据的场景。通过对LS估计器进行原理性调整，该方法能够适应多轮部署，并在温和条件下实现更快的收敛。经验评估表明，该方法在静态设置中与现有最佳离线方法性能相当，而在允许中间策略部署时则显著超越它们，突显了自适应数据收集和PAC-贝叶斯公式的优势。", "keywords": "离策略学习, PAC-贝叶斯, 对数平滑, 自适应学习, 收敛速度", "comments": "本文的创新之处在于将成功的PAC-贝叶斯对数平滑方法从静态离策略学习推广到更具挑战性和实用性的自适应设置。通过引入对LS估计器的调整以适应多轮部署，该研究有效地弥合了理论保证与实际迭代学习之间的差距，并展示了在动态数据收集环境下的显著性能提升，对于强化学习的实际应用具有重要意义。"}}
{"id": "2506.10677", "title": "Practical Improvements of A/B Testing with Off-Policy Estimation", "authors": ["Sakhi Otmane", "Gilotte Alexandre", "Rohde David"], "summary": "We address the problem of A/B testing, a widely used protocol for evaluating\nthe potential improvement achieved by a new decision system compared to a\nbaseline. This protocol segments the population into two subgroups, each\nexposed to a version of the system and estimates the improvement as the\ndifference between the measured effects. In this work, we demonstrate that the\ncommonly used difference-in-means estimator, while unbiased, can be improved.\nWe introduce a family of unbiased off-policy estimators that achieves lower\nvariance than the standard approach. Among this family, we identify the\nestimator with the lowest variance. The resulting estimator is simple, and\noffers substantial variance reduction when the two tested systems exhibit\nsimilarities. Our theoretical analysis and experimental results validate the\neffectiveness and practicality of the proposed method.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10677v1", "AI": {"title_translation": "A/B测试中离线策略估计的实际改进", "tldr": "本文通过引入一组新的无偏离线策略估计器，改进了A/B测试中常用的均值差估计器，从而在保持无偏性的同时显著降低了方差，尤其当测试系统相似时效果更佳。", "motivation": "A/B测试中常用的均值差估计器虽然是无偏的，但其方差可以被改进，以实现更高效的评估。", "method": "引入了一族无偏的离线策略估计器，并从该族中识别出方差最低的估计器。通过理论分析和实验结果验证了所提出方法的有效性和实用性。", "result": "所提出的估计器简单易用，并且当两个被测试系统相似时，能够显著降低方差。理论分析和实验结果均验证了该方法的有效性和实用性。", "conclusion": "本文提出的离线策略估计器为A/B测试提供了实用且有效的改进，通过降低方差提高了评估效率和准确性。", "translation": "我们解决了A/B测试的问题，A/B测试是一种广泛用于评估新决策系统相对于基线系统可能实现的改进的协议。该协议将人群分为两个子组，每个子组都接触一个版本的系统，并通过测量效果之间的差异来估计改进。在这项工作中，我们证明了常用的均值差估计器虽然是无偏的，但可以得到改进。我们引入了一族无偏的离线策略估计器，其方差低于标准方法。在该族中，我们确定了方差最低的估计器。由此产生的估计器简单易行，当两个被测试系统表现出相似性时，可以显著降低方差。我们的理论分析和实验结果验证了所提出方法的有效性和实用性。", "summary": "本文针对A/B测试中评估系统改进的问题，指出常用均值差估计器虽无偏但可改进其方差。作者提出了一族无偏的离线策略估计器，并从中筛选出方差最低的一个。该新型估计器不仅简单，而且在被测系统相似时能显著降低方差。理论分析和实验结果均证实了其有效性和实用性。", "keywords": "A/B测试, 离线策略估计, 方差降低, 估计器, 实验设计", "comments": "本文的创新之处在于提出了一系列用于A/B测试的离线策略估计器，旨在降低标准均值差估计器的方差。这种方差的降低对于提高A/B测试的效率和结果的可靠性至关重要，尤其是在系统间差异细微的情况下。强调“实际改进”和“显著方差降低”突出了其潜在的现实应用价值。"}}
{"id": "2506.10868", "title": "A multi-scale loss formulation for learning a probabilistic model with proper score optimisation", "authors": ["Simon Lang", "Martin Leutbecher", "Pedro Maciel"], "summary": "We assess the impact of a multi-scale loss formulation for training\nprobabilistic machine-learned weather forecasting models. The multi-scale loss\nis tested in AIFS-CRPS, a machine-learned weather forecasting model developed\nat the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS-CRPS is\ntrained by directly optimising the almost fair continuous ranked probability\nscore (afCRPS). The multi-scale loss better constrains small scale variability\nwithout negatively impacting forecast skill. This opens up promising directions\nfor future work in scale-aware model training.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.10868v1", "AI": {"title_translation": "一种用于学习具有适当分数优化的概率模型的多尺度损失公式", "tldr": "本文评估了一种多尺度损失公式对训练概率性机器学习天气预报模型的影响，发现它能在不影响预测技能的情况下更好地约束小尺度变异性。", "motivation": "评估多尺度损失公式对训练概率性机器学习天气预报模型的影响。", "method": "多尺度损失在AIFS-CRPS（一个通过直接优化近公平连续分级概率分数afCRPS训练的机器学习天气预报模型）中进行了测试。", "result": "多尺度损失在不负面影响预测技能的情况下，更好地约束了小尺度变异性。", "conclusion": "该方法为未来尺度感知模型训练开辟了有前景的方向。", "translation": "我们评估了多尺度损失公式在训练概率性机器学习天气预报模型中的影响。该多尺度损失在AIFS-CRPS中进行了测试，AIFS-CRPS是欧洲中期天气预报中心（ECMWF）开发的一种机器学习天气预报模型。AIFS-CRPS通过直接优化近公平连续分级概率分数（afCRPS）进行训练。多尺度损失能更好地约束小尺度变异性，同时不负面影响预测技能。这为未来尺度感知模型训练开辟了有前景的方向。", "summary": "本文评估了一种多尺度损失公式对训练概率性机器学习天气预报模型的影响。该损失函数在ECMWF开发的AIFS-CRPS模型中进行测试，该模型通过优化afCRPS进行训练。研究发现，多尺度损失能有效约束小尺度变异性，且不损害整体预测性能，为未来的尺度感知模型训练提供了新思路。", "keywords": "多尺度损失, 概率模型, 天气预报, CRPS, 尺度感知训练", "comments": "该研究展示了通过引入多尺度损失函数，可以有效提升机器学习天气预报模型在处理小尺度变异性方面的能力，而不会牺牲整体预测准确性。这对于提高天气预报的精细化程度具有重要意义，并为模型训练的优化方向提供了新的视角。"}}
{"id": "2506.10862", "title": "OmniFluids: Unified Physics Pre-trained Modeling of Fluid Dynamics", "authors": ["Rui Zhang", "Qi Meng", "Han Wan", "Yang Liu", "Zhi-Ming Ma", "Hao Sun"], "summary": "High-fidelity and efficient simulation of fluid dynamics drive progress in\nvarious scientific and engineering applications. Traditional computational\nfluid dynamics methods offer strong interpretability and guaranteed\nconvergence, but rely on fine spatial and temporal meshes, incurring\nprohibitive computational costs. Physics-informed neural networks (PINNs) and\nneural operators aim to accelerate PDE solvers using deep learning techniques.\nHowever, PINNs require extensive retraining and careful tuning, and purely\ndata-driven operators demand large labeled datasets. Hybrid physics-aware\nmethods embed numerical discretizations into network architectures or loss\nfunctions, but achieve marginal speed gains and become unstable when balancing\ncoarse priors against high-fidelity measurements. To this end, we introduce\nOmniFluids, a unified physics pre-trained operator learning framework that\nintegrates physics-only pre-training, coarse-grid operator distillation, and\nfew-shot fine-tuning, which enables fast inference and accurate prediction\nunder limited or zero data supervision. For architectural design, the key\ncomponents of OmniFluids include a mixture of operators, a multi-frame decoder,\nand factorized Fourier layers, which enable efficient and scalable modeling of\ndiverse physical tasks while maintaining seamless integration with\nphysics-based supervision. Across a broad range of two- and three-dimensional\nbenchmarks, OmniFluids significantly outperforms state-of-the-art AI-driven\nmethods in flow field reconstruction and turbulence statistics accuracy,\ndelivering 10-100x speedups compared to classical solvers, and accurately\nrecovers unknown physical parameters from sparse, noisy data. This work\nestablishes a new paradigm for efficient and generalizable surrogate modeling\nin complex fluid systems under limited data availability.", "comment": null, "cate": "physics.flu-dyn", "url": "http://arxiv.org/abs/2506.10862v1", "AI": {"title_translation": "OmniFluids：流体动力学统一物理预训练建模", "tldr": "OmniFluids是一个统一的物理预训练算子学习框架，通过整合物理预训练、粗网格蒸馏和少样本微调，在有限或零数据监督下实现流体动力学的高效准确模拟，速度比传统求解器快10-100倍。", "motivation": "传统的计算流体动力学方法计算成本高昂。物理信息神经网络（PINNs）和神经算子需要大量重新训练、仔细调优或大量标记数据。混合物理感知方法速度提升有限且不稳定。", "method": "我们提出了OmniFluids，一个统一的物理预训练算子学习框架，它整合了纯物理预训练、粗网格算子蒸馏和少样本微调。其架构关键组件包括算子混合器、多帧解码器和因子化傅里叶层，旨在高效可扩展地建模各种物理任务并无缝集成物理监督。", "result": "在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计精度方面显著优于最先进的AI驱动方法，比经典求解器提速10-100倍，并能从稀疏、噪声数据中准确恢复未知物理参数。", "conclusion": "这项工作为在有限数据可用性下复杂流体系统中高效、可泛化的代理建模建立了新范式。", "translation": "高保真和高效的流体动力学模拟推动了各种科学和工程应用的进步。传统的计算流体动力学方法具有强大的可解释性和收敛性保证，但依赖于精细的空间和时间网格，导致计算成本过高。物理信息神经网络（PINNs）和神经算子旨在利用深度学习技术加速偏微分方程求解器。然而，PINNs需要大量的重新训练和仔细调优，而纯数据驱动的算子则需要大量的标记数据集。混合物理感知方法将数值离散化嵌入到网络架构或损失函数中，但速度提升有限，并且在平衡粗粒度先验与高保真测量时变得不稳定。为此，我们引入了OmniFluids，一个统一的物理预训练算子学习框架，它整合了纯物理预训练、粗网格算子蒸馏和少样本微调，从而在有限或零数据监督下实现快速推理和准确预测。在架构设计方面，OmniFluids的关键组件包括算子混合器、多帧解码器和因子化傅里叶层，这些组件能够对各种物理任务进行高效和可扩展的建模，同时保持与基于物理的监督无缝集成。在广泛的二维和三维基准测试中，OmniFluids在流场重建和湍流统计精度方面显著优于最先进的AI驱动方法，比经典求解器提速10-100倍，并能从稀疏、噪声数据中准确恢复未知物理参数。这项工作为在有限数据可用性下复杂流体系统中高效、可泛化的代理建模建立了新范式。", "summary": "OmniFluids是一个创新的统一物理预训练算子学习框架，旨在解决传统计算流体动力学方法计算成本高昂以及现有深度学习加速方法（如PINNs）对数据或调优要求过高的问题。该框架通过结合纯物理预训练、粗网格算子蒸馏和少样本微调，实现了在数据有限或缺失情况下的快速准确流体动力学模拟。OmniFluids的关键架构创新包括算子混合器、多帧解码器和因子化傅里叶层。实验证明，OmniFluids在流场重建和湍流统计精度上显著优于现有AI方法，并能实现10-100倍的加速，同时还能从稀疏噪声数据中恢复物理参数，为复杂流体系统的代理建模提供了高效且可泛化的新范式。", "keywords": "流体动力学, 物理预训练, 算子学习, 代理建模, 高效模拟", "comments": "OmniFluids的创新点在于其统一的物理预训练算子学习框架，结合了多种策略（物理预训练、蒸馏、少样本微调），有效解决了传统CFD计算成本高和现有AI方法数据依赖性强的痛点。其在架构上的改进也使其能高效处理多样化的物理任务。这项工作为流体动力学的高效、泛化模拟开辟了新路径，对于科学和工程领域具有重要意义。"}}
{"id": "2506.10872", "title": "The Gittins Index: A Design Principle for Decision-Making Under Uncertainty", "authors": ["Ziv Scully", "Alexander Terenin"], "summary": "The Gittins index is a tool that optimally solves a variety of\ndecision-making problems involving uncertainty, including multi-armed bandit\nproblems, minimizing mean latency in queues, and search problems like the\nPandora's box model. However, despite the above examples and later extensions\nthereof, the space of problems that the Gittins index can solve perfectly\noptimally is limited, and its definition is rather subtle compared to those of\nother multi-armed bandit algorithms. As a result, the Gittins index is often\nregarded as being primarily a concept of theoretical importance, rather than a\npractical tool for solving decision-making problems.\n  The aim of this tutorial is to demonstrate that the Gittins index can be\nfruitfully applied to practical problems. We start by giving an example-driven\nintroduction to the Gittins index, then walk through several examples of\nproblems it solves - some optimally, some suboptimally but still with excellent\nperformance. Two practical highlights in the latter category are applying the\nGittins index to Bayesian optimization, and applying the Gittins index to\nminimizing tail latency in queues.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.10872v1", "AI": {"title_translation": "吉廷斯指数：不确定性下决策的设计原则", "tldr": "本教程旨在通过实例演示吉廷斯指数在解决不确定性下决策问题（包括多臂老虎机和队列优化）中的实际应用，即使在某些情况下是非最优的，也能表现出色。", "motivation": "吉廷斯指数常被视为一个纯理论概念，而非解决实际决策问题的工具。本文旨在证明吉廷斯指数可以有效地应用于实际问题。", "method": "本文通过一个以示例为主导的介绍来阐述吉廷斯指数，并详细讲解了它能解决的几个问题示例，其中一些是最佳解，另一些是次优解但表现优秀。特别提到了将其应用于贝叶斯优化和最小化队列尾部延迟。", "result": "本文展示了吉廷斯指数可以成功应用于实际问题，包括贝叶斯优化和最小化队列尾部延迟，在某些情况下能达到最优性能，在其他情况下虽然是次优但表现出色。", "conclusion": "吉廷斯指数不仅具有理论重要性，也是一个可以有效应用于解决实际决策问题的实用工具。", "translation": "吉廷斯指数是一种能够最优地解决各种涉及不确定性的决策问题的工具，包括多臂老虎机问题、最小化队列平均延迟以及像潘多拉盒子模型这样的搜索问题。然而，尽管有上述示例及其后来的扩展，吉廷斯指数能够完美最优解决的问题空间是有限的，并且与其它多臂老虎机算法的定义相比，其定义相当微妙。因此，吉廷斯指数通常被认为主要是一个具有理论重要性的概念，而非解决决策问题的实用工具。\n本教程旨在证明吉廷斯指数可以有效地应用于实际问题。我们首先通过一个以示例为主导的介绍来阐述吉廷斯指数，然后详细讲解了它能解决的几个问题示例——其中一些是最佳解，另一些是次优解但表现优秀。后一类中的两个实际亮点是将吉廷斯指数应用于贝叶斯优化，以及将其应用于最小化队列尾部延迟。", "summary": "本教程旨在改变吉廷斯指数仅被视为理论概念的看法，通过大量实例展示其在不确定性下决策问题中的实际应用价值。文章首先介绍了吉廷斯指数，然后探讨了它在多臂老虎机、队列优化等问题中的应用，证明其在某些情况下能达到最优解，在其他情况下即使是次优解也能表现出色，尤其强调了其在贝叶斯优化和最小化队列尾部延迟方面的应用。", "keywords": "吉廷斯指数, 不确定性决策, 多臂老虎机, 贝叶斯优化, 队列延迟", "comments": "本文的创新之处在于它试图弥合吉廷斯指数在理论和实践之间的鸿沟。通过提供具体的应用案例，它提升了一个常被视为纯理论工具的实用价值，这对于研究人员和实践者都具有重要意义。它强调了即使是次优解也能带来出色表现，这拓宽了该指数的应用范围。"}}
{"id": "2506.10879", "title": "A Goemans-Williamson type algorithm for identifying subcohorts in clinical trials", "authors": ["Pratik Worah"], "summary": "We design an efficient algorithm that outputs a linear classifier for\nidentifying homogeneous subsets (equivalently subcohorts) from large\ninhomogeneous datasets. Our theoretical contribution is a rounding technique,\nsimilar to that of Goemans and Williamson (1994), that approximates the optimal\nsolution of the underlying optimization problem within a factor of $0.82$. As\nan application, we use our algorithm to design a simple test that can identify\nhomogeneous subcohorts of patients, that are mainly comprised of metastatic\ncases, from the RNA microarray dataset for breast cancer by Curtis et al.\n(2012). Furthermore, we also use the test output by the algorithm to\nsystematically identify subcohorts of patients in which statistically\nsignificant changes in methylation levels of tumor suppressor genes co-occur\nwith statistically significant changes in nuclear receptor expression.\nIdentifying such homogeneous subcohorts of patients can be useful for the\ndiscovery of disease pathways and therapeutics, specific to the subcohort.", "comment": null, "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.10879v1", "AI": {"title_translation": "一种Goemans-Williamson型算法用于临床试验中亚群识别", "tldr": "设计了一种Goemans-Williamson型算法，能有效识别大型非均匀数据集中的同质亚群，并应用于乳腺癌RNA微阵列数据，以发现疾病通路和疗法。", "motivation": "从大型非均匀数据集中识别同质亚群对于发现疾病通路和特定治疗方法至关重要。", "method": "设计了一种高效算法，该算法输出一个线性分类器，用于识别同质子集。其理论贡献是一种类似于Goemans和Williamson（1994）的舍入技术，能将底层优化问题的最优解近似到0.82的因子。", "result": "该算法能够将底层优化问题的最优解近似到0.82的因子。成功地从乳腺癌RNA微阵列数据集中识别出主要由转移性病例组成的同质患者亚群。此外，还系统地识别出甲基化水平和核受体表达发生显著变化的患者亚群。", "conclusion": "识别同质患者亚群对于发现特定于该亚群的疾病通路和治疗方法非常有用。", "translation": "我们设计了一种高效算法，能够从大型非均匀数据集中输出一个线性分类器，用于识别同质子集（等同于亚群）。我们的理论贡献是一种类似于Goemans和Williamson（1994）的舍入技术，能够将底层优化问题的最优解近似到0.82的因子。作为应用，我们使用我们的算法设计了一个简单的测试，可以从Curtis等人（2012）的乳腺癌RNA微阵列数据集中识别出主要由转移性病例组成的同质患者亚群。此外，我们还使用该算法输出的测试系统性地识别了肿瘤抑制基因甲基化水平的统计显著变化与核受体表达的统计显著变化同时发生的患者亚群。识别此类同质患者亚群对于发现特定于该亚群的疾病通路和治疗方法可能很有用。", "summary": "本文设计了一种高效算法，用于从大型非均匀数据集中识别同质亚群，即亚群。其理论核心是一种Goemans-Williamson型的舍入技术，能够以0.82的因子近似优化问题的最优解。作为应用，该算法被用于分析乳腺癌RNA微阵列数据集，成功识别出转移性病例为主的同质患者亚群，并系统地发现甲基化水平和核受体表达同时发生显著变化的亚群。这项工作对于疾病通路和靶向治疗的发现具有重要意义。", "keywords": "Goemans-Williamson算法, 亚群识别, 临床试验, 线性分类器, 优化问题", "comments": "该论文的创新点在于将Goemans-Williamson型的舍入技术应用于临床试验中的亚群识别问题，提供了一种理论上具有近似保证的有效算法。其重要性体现在能够从复杂临床数据中发现有临床意义的同质患者亚群，这对于精准医疗和疾病机制研究具有潜在价值。"}}
{"id": "2506.10899", "title": "Demystifying Spectral Feature Learning for Instrumental Variable Regression", "authors": ["Dimitri Meunier", "Antoine Moulin", "Jakub Wornbard", "Vladimir R. Kostic", "Arthur Gretton"], "summary": "We address the problem of causal effect estimation in the presence of hidden\nconfounders, using nonparametric instrumental variable (IV) regression. A\nleading strategy employs spectral features - that is, learned features spanning\nthe top eigensubspaces of the operator linking treatments to instruments. We\nderive a generalization error bound for a two-stage least squares estimator\nbased on spectral features, and gain insights into the method's performance and\nfailure modes. We show that performance depends on two key factors, leading to\na clear taxonomy of outcomes. In a good scenario, the approach is optimal. This\noccurs with strong spectral alignment, meaning the structural function is\nwell-represented by the top eigenfunctions of the conditional operator, coupled\nwith this operator's slow eigenvalue decay, indicating a strong instrument.\nPerformance degrades in a bad scenario: spectral alignment remains strong, but\nrapid eigenvalue decay (indicating a weaker instrument) demands significantly\nmore samples for effective feature learning. Finally, in the ugly scenario,\nweak spectral alignment causes the method to fail, regardless of the\neigenvalues' characteristics. Our synthetic experiments empirically validate\nthis taxonomy.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10899v1", "AI": {"title_translation": "揭秘工具变量回归中的谱特征学习", "tldr": "本文分析了非参数工具变量回归中谱特征学习的性能，揭示其依赖于谱对齐和特征值衰减，并提出了良好、糟糕和最差三种情况的分类。", "motivation": "为了解决存在隐藏混杂因素的因果效应估计问题，并深入理解非参数工具变量（IV）回归中谱特征学习方法的性能和失效模式。", "method": "推导了基于谱特征的两阶段最小二乘估计器的泛化误差界限，并分析了其性能如何受谱对齐（结构函数与顶部特征函数的匹配程度）和特征值衰减（工具强度）两个关键因素的影响。通过合成实验对提出的分类进行了经验验证。", "result": "性能取决于谱对齐和特征值衰减。在“良好”情况下，当谱对齐强且特征值衰减慢（工具强）时，方法是最佳的。在“糟糕”情况下，谱对齐仍强但特征值快速衰减（工具弱）时，需要更多样本。在“最差”情况下，弱谱对齐导致方法失效，与特征值无关。合成实验验证了这一分类。", "conclusion": "非参数工具变量回归中谱特征学习的性能关键取决于谱对齐和特征值衰减的速度，这决定了工具的强度，从而导致可预测的良好、糟糕和最差的结果。", "translation": "我们解决了在存在隐藏混杂因素的情况下，使用非参数工具变量（IV）回归进行因果效应估计的问题。一种主要策略是采用谱特征——即，学习到的特征，它们跨越连接处理与工具的操作符的顶部特征子空间。我们推导了基于谱特征的两阶段最小二乘估计器的泛化误差界限，并深入了解了该方法的性能和失效模式。我们表明，性能取决于两个关键因素，从而形成清晰的结果分类。在良好情况下，该方法是最佳的。这发生在强谱对齐时，意味着结构函数被条件算子的顶部特征函数很好地表示，并且该算子的特征值衰减缓慢，表明工具变量强。在糟糕情况下，性能下降：谱对齐仍然很强，但特征值快速衰减（表明工具变量较弱）需要显著更多的样本才能有效学习特征。最后，在最差情况下，弱谱对齐导致该方法失败，无论特征值的特性如何。我们的合成实验经验性地验证了这种分类。", "summary": "本文研究了在存在隐藏混杂因素的情况下，非参数工具变量（IV）回归中谱特征学习的因果效应估计问题。论文推导了基于谱特征的两阶段最小二乘估计器的泛化误差界限，并确定了影响其性能的两个关键因素：谱对齐（结构函数与顶部特征函数的匹配程度）和特征值衰减（工具强度）。基于这些因素，论文提出了一种结果分类——良好（强对齐和慢衰减时最佳）、糟糕（强对齐但快速衰减需更多样本）和最差（弱对齐导致失败）——并通过合成实验进行了经验验证。", "keywords": "工具变量回归, 谱特征, 因果推断, 泛化误差界限, 非参数IV", "comments": "本文对非参数工具变量回归中的谱特征学习进行了有价值的理论和实证分析。其主要创新在于推导了泛化误差界限，更重要的是，基于谱对齐和特征值衰减建立了一个清晰的性能结果分类。这揭示了该方法的行为，为实践者提供了关于其何时表现最佳、何时性能下降或失效的关键见解。"}}
{"id": "2506.10908", "title": "Probably Approximately Correct Labels", "authors": ["Emmanuel J. Candès", "Andrew Ilyas", "Tijana Zrnic"], "summary": "Obtaining high-quality labeled datasets is often costly, requiring either\nextensive human annotation or expensive experiments. We propose a method that\nsupplements such \"expert\" labels with AI predictions from pre-trained models to\nconstruct labeled datasets more cost-effectively. Our approach results in\nprobably approximately correct labels: with high probability, the overall\nlabeling error is small. This solution enables rigorous yet efficient dataset\ncuration using modern AI models. We demonstrate the benefits of the methodology\nthrough text annotation with large language models, image labeling with\npre-trained vision models, and protein folding analysis with AlphaFold.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10908v1", "AI": {"title_translation": "大概近似正确标签", "tldr": "本文提出一种结合专家标注和AI预测的方法，以更经济高效地构建高质量标注数据集，确保整体标注错误率低，并适用于文本、图像和蛋白质折叠等领域。", "motivation": "获取高质量的标注数据集通常成本高昂，需要大量人工标注或昂贵的实验。", "method": "本文提出一种方法，通过结合“专家”标签和预训练AI模型的预测来构建标注数据集，从而降低成本。这种方法旨在生成“大概近似正确”的标签，即整体标注错误率在大概率下很小。", "result": "该方法在文本标注（使用大型语言模型）、图像标注（使用预训练视觉模型）和蛋白质折叠分析（使用AlphaFold）中展示了其益处。", "conclusion": "该解决方案能够利用现代AI模型进行严谨而高效的数据集管理。", "translation": "获取高质量的标注数据集通常成本高昂，需要大量人工标注或昂贵的实验。我们提出一种方法，用预训练模型生成的AI预测来补充这些“专家”标签，从而更经济高效地构建标注数据集。我们的方法产生了大概近似正确的标签：以高概率，整体标注错误率很小。该解决方案能够利用现代AI模型进行严谨而高效的数据集管理。我们通过大型语言模型的文本标注、预训练视觉模型的图像标注以及AlphaFold的蛋白质折叠分析，展示了该方法学的益处。", "summary": "本文提出一种创新方法，通过结合少量专家标注与预训练AI模型的预测来经济高效地构建高质量标注数据集。该方法确保了“大概近似正确”的标签，即整体标注错误率在大概率下保持较低水平。研究通过在文本标注、图像标注和蛋白质折叠分析等多个领域进行验证，展示了其有效性，为利用现代AI模型进行高效且严谨的数据集管理提供了新的途径。", "keywords": "数据标注, AI预测, 成本效益, 质量控制, 混合标注", "comments": "本文的创新点在于提出了一种结合人工专家知识和AI模型预测的新颖标注策略，有效解决了高质量数据集获取成本高昂的痛点。其“大概近似正确”的理念为数据标注的效率和准确性之间找到了一个平衡点。该方法的重要性在于它提供了一个普适性框架，可以应用于不同模态的数据标注任务，如文本、图像和生物信息学，极大地扩展了AI在数据准备阶段的应用潜力。"}}
{"id": "2506.10929", "title": "On feature selection in double-imbalanced data settings: a Random Forest approach", "authors": ["Fabio Demaria"], "summary": "Feature selection is a critical step in high-dimensional classification\ntasks, particularly under challenging conditions of double imbalance, namely\nsettings characterized by both class imbalance in the response variable and\ndimensional asymmetry in the data $(n \\gg p)$. In such scenarios, traditional\nfeature selection methods applied to Random Forests (RF) often yield unstable\nor misleading importance rankings. This paper proposes a novel thresholding\nscheme for feature selection based on minimal depth, which exploits the tree\ntopology to assess variable relevance. Extensive experiments on simulated and\nreal-world datasets demonstrate that the proposed approach produces more\nparsimonious and accurate subsets of variables compared to conventional minimal\ndepth-based selection. The method provides a practical and interpretable\nsolution for variable selection in RF under double imbalance conditions.", "comment": "Working paper", "cate": "stat.ME", "url": "http://arxiv.org/abs/2506.10929v1", "AI": {"title_translation": "在双重不平衡数据设置中的特征选择：一种随机森林方法", "tldr": "本文提出了一种新的基于最小深度的随机森林特征选择方法，用于双重不平衡数据，与传统方法相比，该方法能产生更简洁、更准确的变量子集。", "motivation": "在高维分类任务中，特别是在响应变量存在类别不平衡和数据存在维度不对称（$n \\gg p$）的双重不平衡条件下，应用于随机森林（RF）的传统特征选择方法通常会产生不稳定或误导性的重要性排名。", "method": "本文提出了一种基于最小深度的新颖特征选择阈值方案，该方案利用树拓扑来评估变量相关性。", "result": "对模拟和真实世界数据集进行的广泛实验表明，与传统的基于最小深度的选择方法相比，所提出的方法能够生成更简洁、更准确的变量子集。", "conclusion": "该方法为双重不平衡条件下随机森林中的变量选择提供了一种实用且可解释的解决方案。", "translation": "特征选择是高维分类任务中的关键步骤，尤其是在双重不平衡的挑战性条件下，即响应变量存在类别不平衡且数据存在维度不对称（$n \\gg p$）的设置。在这种情况下，应用于随机森林（RF）的传统特征选择方法通常会产生不稳定或误导性的重要性排名。本文提出了一种基于最小深度的新颖特征选择阈值方案，该方案利用树拓扑来评估变量相关性。对模拟和真实世界数据集进行的广泛实验表明，与传统的基于最小深度的选择方法相比，所提出的方法能够生成更简洁、更准确的变量子集。该方法为双重不平衡条件下随机森林中的变量选择提供了一种实用且可解释的解决方案。", "summary": "本文针对双重不平衡数据（类别不平衡和 $n \\gg p$）中随机森林特征选择不稳定的挑战。它引入了一种新颖的基于最小深度的阈值方案，利用树拓扑来评估变量相关性。实验表明，与现有方法相比，这种新方法能够产生更简洁、更准确的特征子集，为这种复杂场景提供了一种实用且可解释的解决方案。", "keywords": "特征选择, 双重不平衡, 随机森林, 最小深度, 变量相关性", "comments": "该论文的创新之处在于针对双重不平衡数据（一个常见且具有挑战性的真实世界场景）对最小深度进行特征选择的适应性。该方法的实用性和可解释性是其显著优势。"}}
{"id": "2506.10944", "title": "Coupled reaction and diffusion governing interface evolution in solid-state batteries", "authors": ["Jingxuan Ding", "Laura Zichi", "Matteo Carli", "Menghang Wang", "Albert Musaelian", "Yu Xie", "Boris Kozinsky"], "summary": "Understanding and controlling the atomistic-level reactions governing the\nformation of the solid-electrolyte interphase (SEI) is crucial for the\nviability of next-generation solid state batteries. However, challenges persist\ndue to difficulties in experimentally characterizing buried interfaces and\nlimits in simulation speed and accuracy. We conduct large-scale explicit\nreactive simulations with quantum accuracy for a symmetric battery cell,\n{\\symcell}, enabled by active learning and deep equivariant neural network\ninteratomic potentials. To automatically characterize the coupled reactions and\ninterdiffusion at the interface, we formulate and use unsupervised\nclassification techniques based on clustering in the space of local atomic\nenvironments. Our analysis reveals the formation of a previously unreported\ncrystalline disordered phase, Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$, in the\nSEI, that evaded previous predictions based purely on thermodynamics,\nunderscoring the importance of explicit modeling of full reaction and transport\nkinetics. Our simulations agree with and explain experimental observations of\nthe SEI formations and elucidate the Li creep mechanisms, critical to dendrite\ninitiation, characterized by significant Li motion along the interface. Our\napproach is to crease a digital twin from first principles, without adjustable\nparameters fitted to experiment. As such, it offers capabilities to gain\ninsights into atomistic dynamics governing complex heterogeneous processes in\nsolid-state synthesis and electrochemistry.", "comment": null, "cate": "cond-mat.mtrl-sci", "url": "http://arxiv.org/abs/2506.10944v1", "AI": {"title_translation": "固态电池中界面演化耦合反应与扩散机制", "tldr": "本研究利用主动学习和深度等变神经网络原子间势，对固态电池中SEI的形成进行了大规模量子精度反应模拟，揭示了一种新的晶态无序相，并阐明了锂蠕变机制。", "motivation": "理解和控制固态电解质界面（SEI）的原子级反应对于下一代固态电池的实用性至关重要。然而，由于实验表征埋藏界面的困难以及模拟速度和精度的限制，挑战依然存在。", "method": "研究通过主动学习和深度等变神经网络原子间势，对对称电池单元进行了大规模、显式的量子精度反应模拟。为了自动表征界面处的耦合反应和相互扩散，采用了基于局部原子环境空间聚类的无监督分类技术。该方法是从第一性原理创建数字孪生，不依赖实验拟合参数。", "result": "分析揭示了SEI中形成了一种以前未报告的晶态无序相Li₂S₀.₇₂P₀.₁₄Cl₀.₁₄，该相规避了之前纯粹基于热力学的预测。模拟结果与SEI形成的实验观察结果一致并对其进行了解释，并阐明了锂蠕变机制，该机制对于枝晶的形成至关重要，其特点是锂沿界面的显著移动。", "conclusion": "本文通过结合先进的模拟技术，成功在原子尺度上揭示了固态电池SEI形成过程中的复杂反应和扩散动力学，包括发现新的相和阐明锂蠕变机制，为深入理解固态合成和电化学中的异质过程提供了关键见解。", "translation": "理解和控制固态电解质界面（SEI）形成的原子级反应对于下一代固态电池的实用性至关重要。然而，由于实验表征埋藏界面的困难以及模拟速度和精度的限制，挑战依然存在。我们通过主动学习和深度等变神经网络原子间势，对对称电池单元Li₃PS₄/Li进行大规模、显式的量子精度反应模拟。为了自动表征界面处的耦合反应和相互扩散，我们制定并使用了基于局部原子环境空间聚类的无监督分类技术。我们的分析揭示了SEI中形成了一种以前未报告的晶态无序相Li₂S₀.₇₂P₀.₁₄Cl₀.₁₄，该相规避了之前纯粹基于热力学的预测，强调了明确建模完整反应和传输动力学的重要性。我们的模拟结果与SEI形成的实验观察结果一致并对其进行了解释，并阐明了锂蠕变机制，这对于枝晶的形成至关重要，其特点是锂沿界面的显著移动。我们的方法是从第一性原理创建数字孪生，没有根据实验调整的参数。因此，它提供了深入了解固态合成和电化学中复杂异质过程的原子动力学的能力。", "summary": "该论文利用主动学习和深度等变神经网络原子间势，对固态电池中的固态电解质界面（SEI）形成过程进行了大规模、量子精度的反应模拟。通过无监督分类技术表征界面反应和扩散，研究发现SEI中存在一种以前未知的晶态无序相Li₂S₀.₇₂P₀.₁₄Cl₀.₁₄，并揭示了锂蠕变机制，这对于枝晶生长至关重要。这项工作通过第一性原理数字孪生方法，为理解固态电池中复杂的异质过程提供了原子层面的深刻见解。", "keywords": "固态电池, 固态电解质界面, 反应扩散, 机器学习势能, 锂枝晶", "comments": "这项工作通过结合先进的机器学习势能模型和大规模模拟，克服了传统方法在模拟复杂固态电池界面反应方面的局限性，特别是在揭示新相形成和锂蠕变机制方面具有创新性。其不依赖实验拟合参数的第一性原理方法，为未来电池材料设计提供了强大的工具。"}}
{"id": "2506.10971", "title": "What Exactly Does Guidance Do in Masked Discrete Diffusion Models", "authors": ["He Ye", "Rojas Kevin", "Tao Molei"], "summary": "We study masked discrete diffusion models with classifier-free guidance\n(CFG). Assuming no score error nor discretization error, we derive an explicit\nsolution to the guided reverse dynamics, so that how guidance influences the\nsampling behavior can be precisely characterized. When the full data\ndistribution is a mixture over classes and the goal is to sample from a\nspecific class, guidance amplifies class-specific regions while suppresses\nregions shared with other classes. This effect depends on the guidance strength\n$w$ and induces distinct covariance structures in the sampled distribution.\nNotably, we observe quantitatively different behaviors in $1$D and $2$D. We\nalso show that for large $w$, the decay rate of the total variation\n($\\mathrm{TV}$) along the reverse dynamics is double-exponential in $w$ for\nboth $1$D and $2$D. These findings highlight the role of guidance, not just in\nshaping the output distribution, but also in controlling the dynamics of the\nsampling trajectory. Our theoretical analysis is supported by experiments that\nillustrate the geometric effects of guidance and its impact on convergence.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.10971v1", "AI": {"title_translation": "引导在掩蔽离散扩散模型中究竟做了什么？", "tldr": "本文研究了带无分类器引导的掩蔽离散扩散模型，推导了引导逆向动力学的显式解，揭示了引导如何影响采样行为，包括放大特定类别区域、抑制共享区域，并指出其对TV衰减率的影响。", "motivation": "研究掩蔽离散扩散模型中无分类器引导（CFG）的作用，以精确表征引导如何影响采样行为。", "method": "通过假设无分数误差和离散化误差，推导出引导逆向动力学的显式解。通过理论分析，并辅以实验来支持研究结果。", "result": "1. 引导能放大类别特定区域并抑制与其他类别共享的区域。2. 这种效应取决于引导强度w，并导致采样分布中不同的协方差结构。3. 在1D和2D中观察到定量上不同的行为。4. 对于大的w，1D和2D中沿逆向动力学的总变差（TV）衰减率是w的双指数。5. 实验支持了引导的几何效应及其对收敛的影响。", "conclusion": "引导不仅塑造了输出分布，还控制了采样轨迹的动态。", "translation": "我们研究了带无分类器引导（CFG）的掩蔽离散扩散模型。假设没有分数误差和离散化误差，我们推导了引导逆向动力学的显式解，从而可以精确地表征引导如何影响采样行为。当完整数据分布是类别混合且目标是从特定类别采样时，引导会放大类别特定区域，同时抑制与其他类别共享的区域。这种效应取决于引导强度w，并在采样分布中诱导不同的协方差结构。值得注意的是，我们观察到1D和2D中定量上不同的行为。我们还表明，对于大的w，1D和2D中沿逆向动力学的总变差（TV）衰减率是w的双指数。这些发现强调了引导的作用，它不仅塑造了输出分布，还控制了采样轨迹的动态。我们的理论分析得到了实验的支持，这些实验阐明了引导的几何效应及其对收敛的影响。", "summary": "本文深入研究了带无分类器引导的掩蔽离散扩散模型，通过推导显式解，精确分析了引导如何影响采样过程。研究发现，引导能放大特定类别区域并抑制共享区域，其效果与引导强度w相关，并影响采样分布的协方差结构。此外，文章还揭示了引导对总变差衰减率的双指数影响。这些发现强调了引导在塑造输出分布和控制采样轨迹中的关键作用，并通过实验进行了验证。", "keywords": "掩蔽离散扩散模型, 无分类器引导, 采样行为, 协方差结构, 总变差", "comments": "该论文通过严谨的理论推导，特别是显式解的得出，对无分类器引导在掩蔽离散扩散模型中的作用提供了精确的量化理解。其创新之处在于揭示了引导如何影响协方差结构以及总变差的衰减率，这对于优化扩散模型的采样效率和生成质量具有重要意义。理论分析与实验验证相结合，增强了研究结果的说服力。"}}
