{"id": "2504.21040", "pdf": "https://arxiv.org/pdf/2504.21040", "abs": "https://arxiv.org/abs/2504.21040", "authors": ["Chenyi Cai", "Kosuke Kuriyama", "Youlong Gu", "Filip Biljecki", "Pieter Herthogs"], "title": "Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels", "categories": ["cs.CV"], "comment": null, "summary": "Urban street environments are vital to supporting human activity in public\nspaces. The emergence of big data, such as street view images (SVIs) combined\nwith multimodal large language models (MLLMs), is transforming how researchers\nand practitioners investigate, measure, and evaluate semantic and visual\nelements of urban environments. Considering the low threshold for creating\nautomated evaluative workflows using MLLMs, it is crucial to explore both the\nrisks and opportunities associated with these probabilistic models. In\nparticular, the extent to which the integration of expert knowledge can\ninfluence the performance of MLLMs in evaluating the quality of urban design\nhas not been fully explored. This study sets out an initial exploration of how\nintegrating more formal and structured representations of expert urban design\nknowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's\ncapability and reliability in evaluating the walkability of built environments\nusing SVIs. We collect walkability metrics from the existing literature and\ncategorize them using relevant ontologies. We then select a subset of these\nmetrics, focusing on the subthemes of pedestrian safety and attractiveness, and\ndevelop prompts for the MLLM accordingly. We analyze the MLLM's ability to\nevaluate SVI walkability subthemes through prompts with varying levels of\nclarity and specificity regarding evaluation criteria. Our experiments\ndemonstrate that MLLMs are capable of providing assessments and interpretations\nbased on general knowledge and can support the automation of multimodal\nimage-text evaluations. However, they generally provide more optimistic scores\nand can make mistakes when interpreting the provided metrics, resulting in\nincorrect evaluations. By integrating expert knowledge, the MLLM's evaluative\nperformance exhibits higher consistency and concentration."}
{"id": "2504.21136", "pdf": "https://arxiv.org/pdf/2504.21136", "abs": "https://arxiv.org/abs/2504.21136", "authors": ["Murali Ramanujam", "Yinwei Dai", "Kyle Jamieson", "Ravi Netravali"], "title": "Legilimens: Performant Video Analytics on the System-on-Chip Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continually retraining models has emerged as a primary technique to enable\nhigh-accuracy video analytics on edge devices. Yet, existing systems employ\nsuch adaptation by relying on the spare compute resources that traditional\n(memory-constrained) edge servers afford. In contrast, mobile edge devices such\nas drones and dashcams offer a fundamentally different resource profile:\nweak(er) compute with abundant unified memory pools. We present Legilimens, a\ncontinuous learning system for the mobile edge's System-on-Chip GPUs. Our\ndriving insight is that visually distinct scenes that require retraining\nexhibit substantial overlap in model embeddings; if captured into a base model\non device memory, specializing to each new scene can become lightweight,\nrequiring very few samples. To practically realize this approach, Legilimens\npresents new, compute-efficient techniques to (1) select high-utility data\nsamples for retraining specialized models, (2) update the base model without\ncomplete retraining, and (3) time-share compute resources between retraining\nand live inference for maximal accuracy. Across diverse workloads, Legilimens\nlowers retraining costs by 2.8-10x compared to existing systems, resulting in\n18-45% higher accuracies."}
{"id": "2504.21154", "pdf": "https://arxiv.org/pdf/2504.21154", "abs": "https://arxiv.org/abs/2504.21154", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel framework for emotion recognition in contemporary\ndance by improving existing Laban Movement Analysis (LMA) feature descriptors\nand introducing robust, novel descriptors that capture both quantitative and\nqualitative aspects of the movement. Our approach extracts expressive\ncharacteristics from 3D keypoints data of professional dancers performing\ncontemporary dance under various emotional states, and trains multiple\nclassifiers, including Random Forests and Support Vector Machines.\nAdditionally, we provide in-depth explanation of features and their impact on\nmodel predictions using explainable machine learning methods. Overall, our\nstudy improves emotion recognition in contemporary dance and offers promising\napplications in performance analysis, dance training, and human--computer\ninteraction, with a highest accuracy of 96.85\\%."}
{"id": "2504.21166", "pdf": "https://arxiv.org/pdf/2504.21166", "abs": "https://arxiv.org/abs/2504.21166", "authors": ["Muhammad Turab", "Philippe Colantoni", "Damien Muselet", "Alain Tremeau"], "title": "Dance Style Recognition Using Laban Movement Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The growing interest in automated movement analysis has presented new\nchallenges in recognition of complex human activities including dance. This\nstudy focuses on dance style recognition using features extracted using Laban\nMovement Analysis. Previous studies for dance style recognition often focus on\ncross-frame movement analysis, which limits the ability to capture temporal\ncontext and dynamic transitions between movements. This gap highlights the need\nfor a method that can add temporal context to LMA features. For this, we\nintroduce a novel pipeline which combines 3D pose estimation, 3D human mesh\nreconstruction, and floor aware body modeling to effectively extract LMA\nfeatures. To address the temporal limitation, we propose a sliding window\napproach that captures movement evolution across time in features. These\nfeatures are then used to train various machine learning methods for\nclassification, and their explainability explainable AI methods to evaluate the\ncontribution of each feature to classification performance. Our proposed method\nachieves a highest classification accuracy of 99.18\\% which shows that the\naddition of temporal context significantly improves dance style recognition\nperformance."}
{"id": "2504.21033", "pdf": "https://arxiv.org/pdf/2504.21033", "abs": "https://arxiv.org/abs/2504.21033", "authors": ["Majid Behravan", "Maryam Haghani", "Denis Gracanin"], "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills."}
{"id": "2504.21012", "pdf": "https://arxiv.org/pdf/2504.21012", "abs": "https://arxiv.org/abs/2504.21012", "authors": ["Makoto Sato"], "title": "Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What underlies intuitive human thinking? One approach to this question is to\ncompare the cognitive dynamics of humans and large language models (LLMs).\nHowever, such a comparison requires a method to quantitatively analyze AI\ncognitive behavior under controlled conditions. While anecdotal observations\nsuggest that certain prompts can dramatically change LLM behavior, these\nobservations have remained largely qualitative. Here, we propose a two-part\nframework to investigate this phenomenon: a Transition-Inducing Prompt (TIP)\nthat triggers a rapid shift in LLM responsiveness, and a Transition Quantifying\nPrompt (TQP) that evaluates this change using a separate LLM. Through\ncontrolled experiments, we examined how LLMs react to prompts embedding two\nsemantically distant concepts (e.g., mathematical aperiodicity and traditional\ncrafts)--either fused together or presented separately--by changing their\nlinguistic quality and affective tone. Whereas humans tend to experience\nheightened engagement when such concepts are meaningfully blended producing a\nnovel concept--a form of conceptual fusion--current LLMs showed no significant\ndifference in responsiveness between semantically fused and non-fused prompts.\nThis suggests that LLMs may not yet replicate the conceptual integration\nprocesses seen in human intuition. Our method enables fine-grained,\nreproducible measurement of cognitive responsiveness, and may help illuminate\nkey differences in how intuition and conceptual leaps emerge in artificial\nversus human minds."}
{"id": "2504.21194", "pdf": "https://arxiv.org/pdf/2504.21194", "abs": "https://arxiv.org/abs/2504.21194", "authors": ["Vedika Srivastava", "Hemant Kumar Singh", "Jaisal Singh"], "title": "Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a novel approach to geolocating images captured from the\nInternational Space Station (ISS) using advanced machine learning algorithms.\nDespite having precise ISS coordinates, the specific Earth locations depicted\nin astronaut-taken photographs often remain unidentified. Our research\naddresses this gap by employing three distinct image processing pipelines: a\nNeural Network based approach, a SIFT based method, and GPT-4 model. Each\npipeline is tailored to process high-resolution ISS imagery, identifying both\nnatural and man-made geographical features. Through extensive evaluation on a\ndiverse dataset of over 140 ISS images, our methods demonstrate significant\npromise in automated geolocation with varied levels of success. The NN approach\nshowed a high success rate in accurately matching geographical features, while\nthe SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided\nenriched geographical descriptions alongside location predictions. This\nresearch contributes to the fields of remote sensing and Earth observation by\nenhancing the accuracy and efficiency of geolocating space-based imagery,\nthereby aiding environmental monitoring and global mapping efforts."}
{"id": "2504.21067", "pdf": "https://arxiv.org/pdf/2504.21067", "abs": "https://arxiv.org/abs/2504.21067", "authors": ["Yuhan Xie", "Yixi Cai", "Yinqiang Zhang", "Lei Yang", "Jia Pan"], "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "This research tackles the challenge of real-time active view selection and\nuncertainty quantification on visual quality for active 3D reconstruction.\nVisual quality is a critical aspect of 3D reconstruction. Recent advancements\nsuch as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have\nnotably enhanced the image rendering quality of reconstruction models.\nNonetheless, the efficient and effective acquisition of input images for\nreconstruction-specifically, the selection of the most informative\nviewpoint-remains an open challenge, which is crucial for active\nreconstruction. Existing studies have primarily focused on evaluating geometric\ncompleteness and exploring unobserved or unknown regions, without direct\nevaluation of the visual uncertainty within the reconstruction model. To\naddress this gap, this paper introduces a probabilistic model that quantifies\nvisual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we\nformulate a criterion, Gaussian Splatting Shannon Mutual Information\n(GauSS-MI), for real-time assessment of visual mutual information from novel\nviewpoints, facilitating the selection of next best view. GauSS-MI is\nimplemented within an active reconstruction system integrated with a view and\nmotion planner. Extensive experiments across various simulated and real-world\nscenes showcase the superior visual quality and reconstruction efficiency\nperformance of the proposed system."}
{"id": "2504.21013", "pdf": "https://arxiv.org/pdf/2504.21013", "abs": "https://arxiv.org/abs/2504.21013", "authors": ["Antoun Yaacoub", "Zainab Assaghir", "Lionel Prevost", "Jérôme Da-Rugna"], "title": "Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented in the 9th Int. Conf. on Computer,\n  Software and Modeling (ICCSM 2025), Roma, Italy, 2025, July 3-5", "summary": "Artificial Intelligence (AI)-generated feedback in educational settings has\ngarnered considerable attention due to its potential to enhance learning\noutcomes. However, a comprehensive understanding of the linguistic\ncharacteristics of AI-generated feedback, including readability, lexical\nrichness, and adaptability across varying challenge levels, remains limited.\nThis study delves into the linguistic and structural attributes of feedback\ngenerated by Google's Gemini 1.5-flash text model for computer science\nmultiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed,\nconsidering three difficulty levels (easy, medium, hard) and three feedback\ntones (supportive, neutral, challenging). Key linguistic metrics, such as\nlength, readability scores (Flesch-Kincaid Grade Level), vocabulary richness,\nand lexical density, were computed and examined. A fine-tuned RoBERTa-based\nmulti-task learning (MTL) model was trained to predict these linguistic\nproperties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and\n0.03 for vocabulary richness. The findings reveal significant interaction\neffects between feedback tone and question difficulty, demonstrating the\ndynamic adaptation of AI-generated feedback within diverse educational\ncontexts. These insights contribute to the development of more personalized and\neffective AI-driven feedback mechanisms, highlighting the potential for\nimproved learning outcomes while underscoring the importance of ethical\nconsiderations in their design and deployment."}
{"id": "2504.21226", "pdf": "https://arxiv.org/pdf/2504.21226", "abs": "https://arxiv.org/abs/2504.21226", "authors": ["Jiaqi Liu", "Ran Tong", "Aowei Shen", "Shuzheng Li", "Changlin Yang", "Lisha Xu"], "title": "MemeBLIP2: A novel lightweight multimodal system to detect harmful memes", "categories": ["cs.CV", "cs.AI"], "comment": "11pages,2 figures, manucripts in preparation", "summary": "Memes often merge visuals with brief text to share humor or opinions, yet\nsome memes contain harmful messages such as hate speech. In this paper, we\nintroduces MemeBLIP2, a light weight multimodal system that detects harmful\nmemes by combining image and text features effectively. We build on previous\nstudies by adding modules that align image and text representations into a\nshared space and fuse them for better classification. Using BLIP-2 as the core\nvision-language model, our system is evaluated on the PrideMM datasets. The\nresults show that MemeBLIP2 can capture subtle cues in both modalities, even in\ncases with ironic or culturally specific content, thereby improving the\ndetection of harmful material."}
{"id": "2504.21216", "pdf": "https://arxiv.org/pdf/2504.21216", "abs": "https://arxiv.org/abs/2504.21216", "authors": ["Minsu Kim", "Eunho Jung", "Yoonsang Lee"], "title": "PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football Player Controller", "categories": ["cs.GR", "cs.RO"], "comment": "Accepted to SIGGRAPH 2025 and ACM TOG", "summary": "We propose PhysicsFC, a method for controlling physically simulated football\nplayer characters to perform a variety of football skills--such as dribbling,\ntrapping, moving, and kicking--based on user input, while seamlessly\ntransitioning between these skills. Our skill-specific policies, which generate\nlatent variables for each football skill, are trained using an existing\nphysics-based motion embedding model that serves as a foundation for\nreproducing football motions. Key features include a tailored reward design for\nthe Dribble policy, a two-phase reward structure combined with projectile\ndynamics-based initialization for the Trap policy, and a Data-Embedded\nGoal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the\ntrained skill policies, the proposed football player finite state machine\n(PhysicsFC FSM) allows users to interactively control the character. To ensure\nsmooth and agile transitions between skill policies, as defined in the FSM, we\nintroduce the Skill Transition-Based Initialization (STI), which is applied\nduring the training of each skill policy. We develop several interactive\nscenarios to showcase PhysicsFC's effectiveness, including competitive trapping\nand dribbling, give-and-go plays, and 11v11 football games, where multiple\nPhysicsFC agents produce natural and controllable physics-based football player\nbehaviors. Quantitative evaluations further validate the performance of\nindividual skill policies and the transitions between them, using the presented\nmetrics and experimental designs."}
{"id": "2504.21016", "pdf": "https://arxiv.org/pdf/2504.21016", "abs": "https://arxiv.org/abs/2504.21016", "authors": ["Ngoc C. Lê", "Hai-Chung Nguyen-Phung", "Thu-Huong Pham Thi", "Hue Vu", "Phuong-Thao Nguyen Thi", "Thu-Thuy Tran", "Hong-Nhung Le Thi", "Thuy-Duong Nguyen-Thi", "Thanh-Huy Nguyen"], "title": "Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. AI4SG-21 The 3rd Workshop on Artificial Intelligence for\n  Social Good at IJCAI 2021", "summary": "The COVID-19 pandemic caused great losses worldwide, efforts are taken place\nto prevent but many countries have failed. In Vietnam, the traceability,\nlocalization, and quarantine of people who contact with patients contribute to\neffective disease prevention. However, this is done by hand, and take a lot of\nwork. In this research, we describe a named-entity recognition (NER) study that\nassists in the prevention of COVID-19 pandemic in Vietnam. We also present our\nmanually annotated COVID-19 dataset with nested named entity recognition task\nfor Vietnamese which be defined new entity types using for our system."}
{"id": "2504.21231", "pdf": "https://arxiv.org/pdf/2504.21231", "abs": "https://arxiv.org/abs/2504.21231", "authors": ["Manikanta Varaganti", "Amulya Vankayalapati", "Nour Awad", "Gregory R. Dion", "Laura J. Brattain"], "title": "T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "submitted to IEEE EMBC 2025", "summary": "Neck ultrasound (US) plays a vital role in airway management by providing\nnon-invasive, real-time imaging that enables rapid and precise interventions.\nDeep learning-based anatomical landmark detection in neck US can further\nfacilitate procedural efficiency. However, class imbalance within datasets,\nwhere key structures like tracheal rings and vocal folds are underrepresented,\npresents significant challenges for object detection models. To address this,\nwe propose T2ID-CAS, a hybrid approach that combines a text-to-image latent\ndiffusion model with class-aware sampling to generate high-quality synthetic\nsamples for underrepresented classes. This approach, rarely explored in the\nultrasound domain, improves the representation of minority classes.\nExperimental results using YOLOv9 for anatomical landmark detection in neck US\ndemonstrated that T2ID-CAS achieved a mean Average Precision of 88.2,\nsignificantly surpassing the baseline of 66. This highlights its potential as a\ncomputationally efficient and scalable solution for mitigating class imbalance\nin AI-assisted ultrasound-guided interventions."}
{"id": "2504.21627", "pdf": "https://arxiv.org/pdf/2504.21627", "abs": "https://arxiv.org/abs/2504.21627", "authors": ["Shin Fujieda", "Chih-Chen Kao", "Takahiro Harada"], "title": "LSNIF: Locally-Subdivided Neural Intersection Function", "categories": ["cs.GR"], "comment": null, "summary": "Neural representations have shown the potential to accelerate ray casting in\na conventional ray-tracing-based rendering pipeline. We introduce a novel\napproach called Locally-Subdivided Neural Intersection Function (LSNIF) that\nreplaces bottom-level BVHs used as traditional geometric representations with a\nneural network. Our method introduces a sparse hash grid encoding scheme\nincorporating geometry voxelization, a scene-agnostic training data collection,\nand a tailored loss function. It enables the network to output not only\nvisibility but also hit-point information and material indices. LSNIF can be\ntrained offline for a single object, allowing us to use LSNIF as a replacement\nfor its corresponding BVH. With these designs, the network can handle hit-point\nqueries from any arbitrary viewpoint, supporting all types of rays in the\nrendering pipeline. We demonstrate that LSNIF can render a variety of scenes,\nincluding real-world scenes designed for other path tracers, while achieving a\nmemory footprint reduction of up to 106.2x compared to a compressed BVH."}
{"id": "2504.21017", "pdf": "https://arxiv.org/pdf/2504.21017", "abs": "https://arxiv.org/abs/2504.21017", "authors": ["Hai-Chung Nguyen-Phung", "Ngoc C. Lê", "Van-Chien Nguyen", "Hang Thi Nguyen", "Thuy Phuong Thi Nguyen"], "title": "ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese", "categories": ["cs.CL", "cs.LG"], "comment": "8 pages. Technical report", "summary": "After two years of appearance, COVID-19 has negatively affected people and\nnormal life around the world. As in May 2022, there are more than 522 million\ncases and six million deaths worldwide (including nearly ten million cases and\nover forty-three thousand deaths in Vietnam). Economy and society are both\nseverely affected. The variant of COVID-19, Omicron, has broken disease\nprevention measures of countries and rapidly increased number of infections.\nResources overloading in treatment and epidemics prevention is happening all\nover the world. It can be seen that, application of artificial intelligence\n(AI) to support people at this time is extremely necessary. There have been\nmany studies applying AI to prevent COVID-19 which are extremely useful, and\nstudies on machine reading comprehension (MRC) are also in it. Realizing that,\nwe created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and\ncan be used to build models and systems, contributing to disease prevention.\nBesides, ViQA-COVID is also the first multi-span extraction MRC dataset for\nVietnamese, we hope that it can contribute to promoting MRC studies in\nVietnamese and multilingual."}
{"id": "2504.21247", "pdf": "https://arxiv.org/pdf/2504.21247", "abs": "https://arxiv.org/abs/2504.21247", "authors": ["Yangyang Qu", "Dazhi Fu", "Jicong Fan"], "title": "Subject Information Extraction for Novelty Detection with Domain Shifts", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised novelty detection (UND), aimed at identifying novel samples, is\nessential in fields like medical diagnosis, cybersecurity, and industrial\nquality control. Most existing UND methods assume that the training data and\ntesting normal data originate from the same domain and only consider the\ndistribution variation between training data and testing data. However, in real\nscenarios, it is common for normal testing and training data to originate from\ndifferent domains, a challenge known as domain shift. The discrepancies between\ntraining and testing data often lead to incorrect classification of normal data\nas novel by existing methods. A typical situation is that testing normal data\nand training data describe the same subject, yet they differ in the background\nconditions. To address this problem, we introduce a novel method that separates\nsubject information from background variation encapsulating the domain\ninformation to enhance detection performance under domain shifts. The proposed\nmethod minimizes the mutual information between the representations of the\nsubject and background while modelling the background variation using a deep\nGaussian mixture model, where the novelty detection is conducted on the subject\nrepresentations solely and hence is not affected by the variation of domains.\nExtensive experiments demonstrate that our model generalizes effectively to\nunseen domains and significantly outperforms baseline methods, especially under\nsubstantial domain shifts between training and testing data."}
{"id": "2504.21018", "pdf": "https://arxiv.org/pdf/2504.21018", "abs": "https://arxiv.org/abs/2504.21018", "authors": ["Enes Özeren", "Yihong Liu", "Hinrich Schütze"], "title": "HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, 15 tables", "summary": "Many pre-trained language models (PLMs) exhibit suboptimal performance on\nmid- and low-resource languages, largely due to limited exposure to these\nlanguages during pre-training. A common strategy to address this is to\nintroduce new tokens specific to the target languages, initialize their\nembeddings, and apply continual pre-training on target-language data. Among\nsuch methods, OFA (Liu et al., 2024a) proposes a similarity-based subword\nembedding initialization heuristic that is both effective and efficient.\nHowever, OFA restricts target-language token embeddings to be convex\ncombinations of a fixed number of source-language embeddings, which may limit\nexpressiveness. To overcome this limitation, we propose HYPEROFA, a\nhypernetwork-based approach for more adaptive token embedding initialization.\nThe hypernetwork is trained to map from an external multilingual word vector\nspace to the PLMs token embedding space using source-language tokens. Once\ntrained, it can generate flexible embeddings for target-language tokens,\nserving as a good starting point for continual pretraining. Experiments\ndemonstrate that HYPEROFA consistently outperforms random initialization\nbaseline and matches or exceeds the performance of OFA in both continual\npre-training convergence and downstream task performance. We make the code\npublicly available."}
{"id": "2504.21248", "pdf": "https://arxiv.org/pdf/2504.21248", "abs": "https://arxiv.org/abs/2504.21248", "authors": ["Ezra Engel", "Lishan Li", "Chris Hudy", "Robert Schleusner"], "title": "Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Facial expression recognition (FER) is a subset of computer vision with\nimportant applications for human-computer-interaction, healthcare, and customer\nservice. FER represents a challenging problem-space because accurate\nclassification requires a model to differentiate between subtle changes in\nfacial features. In this paper, we examine the use of multi-modal transfer\nlearning to improve performance on a challenging video-based FER dataset,\nDynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained\nResNets, OpenPose, and OmniVec networks, we explore the impact of\ncross-temporal, multi-modal features on classification accuracy. Ultimately, we\nfind that these finely-tuned multi-modal feature generators modestly improve\naccuracy of our transformer-based classification model."}
{"id": "2504.21019", "pdf": "https://arxiv.org/pdf/2504.21019", "abs": "https://arxiv.org/abs/2504.21019", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NAACL 2025 main conference", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net."}
{"id": "2504.21263", "pdf": "https://arxiv.org/pdf/2504.21263", "abs": "https://arxiv.org/abs/2504.21263", "authors": ["Jinpeng Wang", "Tianci Luo", "Yaohua Zha", "Yan Feng", "Ruisheng Luo", "Bin Chen", "Tao Dai", "Long Chen", "Yaowei Wang", "Shu-Tao Xia"], "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted by CVPR'25. 10 pages, 5 figures, 6 tables", "summary": "Visual In-Context Learning (VICL) enables adaptively solving vision tasks by\nleveraging pixel demonstrations, mimicking human-like task completion through\nanalogy. Prompt selection is critical in VICL, but current methods assume the\nexistence of a single \"ideal\" prompt in a pool of candidates, which in practice\nmay not hold true. Multiple suitable prompts may exist, but individually they\noften fall short, leading to difficulties in selection and the exclusion of\nuseful context. To address this, we propose a new perspective: prompt\ncondensation. Rather than relying on a single prompt, candidate prompts\ncollaborate to efficiently integrate informative contexts without sacrificing\nresolution. We devise Condenser, a lightweight external plugin that compresses\nrelevant fine-grained context across multiple prompts. Optimized end-to-end\nwith the backbone, Condenser ensures accurate integration of contextual cues.\nExperiments demonstrate Condenser outperforms state-of-the-arts across\nbenchmark tasks, showing superior context compression, scalability with more\nprompts, and enhanced computational efficiency compared to ensemble methods,\npositioning it as a highly competitive solution for VICL. Code is open-sourced\nat https://github.com/gimpong/CVPR25-Condenser."}
{"id": "2504.21020", "pdf": "https://arxiv.org/pdf/2504.21020", "abs": "https://arxiv.org/abs/2504.21020", "authors": ["Jaydip Sen", "Rohit Pandey", "Hetvi Waghela"], "title": "Context-Enhanced Contrastive Search for Improved LLM Text Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This is the pre-review version of our paper, which has been accepted\n  for publication in the IEEE 6th International Conference on Emerging\n  Technologies (INCET). The conference will be organized at Belgaum, India,\n  from May 24 to 26, 2025. This is not the final camera-ready paper, which will\n  be available on IEEE Xplore. The paper is 9 pages long, and it contains 2\n  Figures and 4 Tables", "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\nadvancements in Natural Language Processing (NLP). However, generating\nhigh-quality text that balances coherence, diversity, and relevance remains\nchallenging. Traditional decoding methods, such as bean search and top-k\nsampling, often struggle with either repetitive or incoherent outputs,\nparticularly in tasks that require long-form text generation. To address these\nlimitations, the paper proposes a novel enhancement of the well-known\nContrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with\ncontextual calibration. The proposed scheme introduces several novelties\nincluding dynamic contextual importance weighting, multi-level Contrastive\nSearch, and adaptive temperature control, to optimize the balance between\nfluency, creativity, and precision. The performance of CECS is evaluated using\nseveral standard metrics such as BLEU, ROUGE, and semantic similarity.\nExperimental results demonstrate significant improvements in both coherence and\nrelevance of the generated texts by CECS outperforming the existing Contrastive\nSearch techniques. The proposed algorithm has several potential applications in\nthe real world including legal document drafting, customer service chatbots,\nand content marketing."}
{"id": "2504.21266", "pdf": "https://arxiv.org/pdf/2504.21266", "abs": "https://arxiv.org/abs/2504.21266", "authors": ["Zhifu Zhao", "Hanyang Hua", "Jianan Li", "Shaoxin Wu", "Fu Li", "Yangtao Zhou", "Yang Li"], "title": "CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "In action recognition tasks, feature diversity is essential for enhancing\nmodel generalization and performance. Existing methods typically promote\nfeature diversity by expanding the training data in the sample space, which\noften leads to inefficiencies and semantic inconsistencies. To overcome these\nproblems, we propose a novel Coarse-fine text co-guidance Diffusion model\n(CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in\nthe latent space by leveraging diffusion and multi-granularity textual\nguidance. Specifically, our approach feeds spatio-temporal features extracted\nfrom skeleton sequences into a latent diffusion model to generate diverse\naction representations. Meanwhile, we introduce a coarse-fine text co-guided\nstrategy that leverages textual information from large language models (LLMs)\nto ensure semantic consistency between the generated features and the original\ninputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module\nduring training, incurring no additional inference cost. Extensive experiments\ndemonstrate that CoCoDiff achieves SOTA performance on skeleton-based action\nrecognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and\nKinetics-Skeleton."}
{"id": "2504.21022", "pdf": "https://arxiv.org/pdf/2504.21022", "abs": "https://arxiv.org/abs/2504.21022", "authors": ["Jun Wang", "David Smith Sundarsingh", "Jyotirmoy V. Deshmukh", "Yiannis Kantaros"], "title": "ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Linear Temporal Logic (LTL) has become a prevalent specification language for\nrobotic tasks. To mitigate the significant manual effort and expertise required\nto define LTL-encoded tasks, several methods have been proposed for translating\nNatural Language (NL) instructions into LTL formulas, which, however, lack\ncorrectness guarantees. To address this, we introduce a new NL-to-LTL\ntranslation method, called ConformalNL2LTL, that can achieve user-defined\ntranslation success rates over unseen NL commands. Our method constructs LTL\nformulas iteratively by addressing a sequence of open-vocabulary\nQuestion-Answering (QA) problems with LLMs. To enable uncertainty-aware\ntranslation, we leverage conformal prediction (CP), a distribution-free\nuncertainty quantification tool for black-box models. CP enables our method to\nassess the uncertainty in LLM-generated answers, allowing it to proceed with\ntranslation when sufficiently confident and request help otherwise. We provide\nboth theoretical and empirical results demonstrating that ConformalNL2LTL\nachieves user-specified translation accuracy while minimizing help rates."}
{"id": "2504.21281", "pdf": "https://arxiv.org/pdf/2504.21281", "abs": "https://arxiv.org/abs/2504.21281", "authors": ["Zexin Ji", "Beiji Zou", "Xiaoyan Kui", "Hua Li", "Pierre Vera", "Su Ruan"], "title": "Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal 3D medical image segmentation aims to accurately identify tumor\nregions across different modalities, facing challenges from variations in image\nintensity and tumor morphology. Traditional convolutional neural network\n(CNN)-based methods struggle with capturing global features, while\nTransformers-based methods, despite effectively capturing global context,\nencounter high computational costs in 3D medical image segmentation. The Mamba\nmodel combines linear scalability with long-distance modeling, making it a\npromising approach for visual representation learning. However, Mamba-based 3D\nmulti-modal segmentation still struggles to leverage modality-specific features\nand fuse complementary information effectively. In this paper, we propose a\nMamba based feature extraction and adaptive multilevel feature fusion for 3D\ntumor segmentation using multi-modal medical image. We first develop the\nspecific modality Mamba encoder to efficiently extract long-range relevant\nfeatures that represent anatomical and pathological structures present in each\nmodality. Moreover, we design an bi-level synergistic integration block that\ndynamically merges multi-modal and multi-level complementary features by the\nmodality attention and channel attention learning. Lastly, the decoder combines\ndeep semantic information with fine-grained details to generate the tumor\nsegmentation map. Experimental results on medical image datasets (PET/CT and\nMRI multi-sequence) show that our approach achieve competitive performance\ncompared to the state-of-the-art CNN, Transformer, and Mamba-based approaches."}
{"id": "2504.21023", "pdf": "https://arxiv.org/pdf/2504.21023", "abs": "https://arxiv.org/abs/2504.21023", "authors": ["Sheng Cao", "Mingrui Wu", "Karthik Prasad", "Yuandong Tian", "Zechun Liu"], "title": "Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost", "categories": ["cs.CL", "cs.LG"], "comment": "Published as a conference paper at ICLR 2025", "summary": "The post-training phase of large language models is essential for enhancing\ncapabilities such as instruction-following, reasoning, and alignment with human\npreferences. However, it demands extensive high-quality data and poses risks\nlike overfitting, alongside significant computational costs due to repeated\npost-training and evaluation after each base model update. This paper\nintroduces $Param\\Delta$, a novel method that streamlines post-training by\ntransferring knowledge from an existing post-trained model to a newly updated\nbase model with ZERO additional training. By computing the difference between\npost-trained model weights ($\\Theta_\\text{post}$) and base model weights\n($\\Theta_\\text{base}$), and adding this to the updated base model\n($\\Theta'_\\text{base}$), we define $Param\\Delta$ Model as:\n$\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} +\n\\Theta'_\\text{base}$. This approach surprisingly equips the new base model with\npost-trained capabilities, achieving performance comparable to direct\npost-training. We did analysis on LLama3, Llama3.1, Qwen, and\nDeepSeek-distilled models. Results indicate $Param\\Delta$ Model effectively\nreplicates traditional post-training. For example, the $Param\\Delta$ Model\nobtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains\napproximately 95\\% of Llama3.1-inst model's performance on average.\n$Param\\Delta$ brings a new perspective on how to fully leverage models in the\nopen-weight community, where checkpoints for base and instruct models are\nreadily available and frequently updated, by providing a cost-free framework to\naccelerate the iterative cycle of model development."}
{"id": "2504.21292", "pdf": "https://arxiv.org/pdf/2504.21292", "abs": "https://arxiv.org/abs/2504.21292", "authors": ["ZiYi Dong", "Chengxing Zhou", "Weijian Deng", "Pengxu Wei", "Xiangyang Ji", "Liang Lin"], "title": "Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions", "categories": ["cs.CV"], "comment": null, "summary": "Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT)\narchitectures have revolutionized image generation through transformer-based\nattention mechanisms. The prevailing paradigm has commonly employed\nself-attention with quadratic computational complexity to handle global spatial\nrelationships in complex images, thereby synthesizing high-fidelity images with\ncoherent visual semantics.Contrary to conventional wisdom, our systematic\nlayer-wise analysis reveals an interesting discrepancy: self-attention in\npre-trained diffusion models predominantly exhibits localized attention\npatterns, closely resembling convolutional inductive biases. This suggests that\nglobal interactions in self-attention may be less critical than commonly\nassumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional\nself-attention modules with Pyramid Convolution Blocks\n(\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized\nconvolutional operations while keeping other components frozen,\n\\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based\ncounterparts while reducing computational cost by 6929$\\times$ and surpassing\nLinFusion by 5.42$\\times$ in efficiency--all without compromising generative\nfidelity."}
{"id": "2504.21024", "pdf": "https://arxiv.org/pdf/2504.21024", "abs": "https://arxiv.org/abs/2504.21024", "authors": ["Tianqing Fang", "Hongming Zhang", "Zhisong Zhang", "Kaixin Ma", "Wenhao Yu", "Haitao Mi", "Dong Yu"], "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Agent self-improvement, where the backbone Large Language Model (LLM) of the\nagent are trained on trajectories sampled autonomously based on their own\npolicies, has emerged as a promising approach for enhancing performance. Recent\nadvancements, particularly in web environments, face a critical limitation:\ntheir performance will reach a stagnation point during autonomous learning\ncycles, hindering further improvement. We argue that this stems from limited\nexploration of the web environment and insufficient exploitation of pre-trained\nweb knowledge in LLMs. To improve the performance of self-improvement, we\npropose a novel framework that introduces a co-evolving World Model LLM. This\nworld model predicts the next observation based on the current observation and\naction within the web environment. Leveraging LLMs' pretrained knowledge of\nabundant web content, the World Model serves dual roles: (1) as a virtual web\nserver generating self-instructed training data to continuously refine the\nagent's policy, and (2) as an imagination engine during inference, enabling\nlook-ahead simulation to guide action selection for the agent LLM. Experiments\nin real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a\n10% performance gain over existing self-evolving agents, demonstrating the\nefficacy and generalizability of our approach, without using any distillation\nfrom more powerful close-sourced models. Our work establishes the necessity of\nintegrating world models into autonomous agent frameworks to unlock sustained\nadaptability."}
{"id": "2504.21294", "pdf": "https://arxiv.org/pdf/2504.21294", "abs": "https://arxiv.org/abs/2504.21294", "authors": ["Qianzi Yu", "Yang Cao", "Yu Kang"], "title": "Learning Multi-view Multi-class Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "The latest trend in anomaly detection is to train a unified model instead of\ntraining a separate model for each category. However, existing multi-class\nanomaly detection (MCAD) models perform poorly in multi-view scenarios because\nthey often fail to effectively model the relationships and complementary\ninformation among different views. In this paper, we introduce a Multi-View\nMulti-Class Anomaly Detection model (MVMCAD), which integrates information from\nmultiple views to accurately identify anomalies. Specifically, we propose a\nsemi-frozen encoder, where a pre-encoder prior enhancement mechanism is added\nbefore the frozen encoder, enabling stable cross-view feature modeling and\nefficient adaptation for improved anomaly detection. Furthermore, we propose an\nAnomaly Amplification Module (AAM) that models global token interactions and\nsuppresses normal regions to enhance anomaly signals, leading to improved\ndetection performance in multi-view settings. Finally, we propose a\nCross-Feature Loss that aligns shallow encoder features with deep decoder\nfeatures and vice versa, enhancing the model's sensitivity to anomalies at\ndifferent semantic levels under multi-view scenarios. Extensive experiments on\nthe Real-IAD dataset for multi-view multi-class anomaly detection validate the\neffectiveness of our approach, achieving state-of-the-art performance of\n91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level,\nrespectively."}
{"id": "2504.21025", "pdf": "https://arxiv.org/pdf/2504.21025", "abs": "https://arxiv.org/abs/2504.21025", "authors": ["MD Thamed Bin Zaman Chowdhury", "Moazzem Hossain", "Md. Ridwanul Islam"], "title": "Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh", "categories": ["cs.CL"], "comment": "It has been accepted in IEEE 27th International Conference on\n  Computer and Information Technology (ICCIT). Now, we are waiting for it to\n  get published in IEEE Xplore", "summary": "Road accidents pose significant concerns globally. They lead to large\nfinancial losses, injuries, disabilities, and societal challenges. Accurate and\ntimely accident data is essential for predicting and mitigating these events.\nThis paper presents a novel framework named 'Durghotona GPT' that integrates\nweb scraping and Large Language Models (LLMs) to automate the generation of\ncomprehensive accident datasets from prominent national dailies in Bangladesh.\nThe authors collected accident reports from three major newspapers: Prothom\nAlo, Dhaka Tribune, and The Daily Star. The collected news was then processed\nusing the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework\nefficiently extracts relevant information, categorizes reports, and compiles\ndetailed datasets. Thus, this framework overcomes limitations of manual data\ncollection methods such as delays, errors, and communication gaps. The authors'\nevaluation demonstrates that Llama-3, an open-source model, performs comparably\nto GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it\ncan be considered a cost-effective alternative for similar tasks. The results\nsuggest that the framework developed by the authors can drastically enhance the\nquality and availability of accident data. As a result, it can support critical\napplications in traffic safety analysis, urban planning, and public health. The\nauthors also developed an interface for 'Durghotona GPT' for ease of use as\npart of this paper. Future work will focus on expanding data collection methods\nand refining LLMs to further increase dataset accuracy and applicability."}
{"id": "2504.21302", "pdf": "https://arxiv.org/pdf/2504.21302", "abs": "https://arxiv.org/abs/2504.21302", "authors": ["Zhelun Shen", "Zhuo Li", "Chenming Wu", "Zhibo Rao", "Lina Liu", "Yuchao Dai", "Liangjun Zhang"], "title": "CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 5 figures, accepted for publication in Pattern Recognition", "summary": "Recently, learning-based stereo matching methods have achieved great\nimprovement in public benchmarks, where soft argmin and smooth L1 loss play a\ncore contribution to their success. However, in unsupervised domain adaptation\nscenarios, we observe that these two operations often yield multimodal\ndisparity probability distributions in target domains, resulting in degraded\ngeneralization. In this paper, we propose a novel approach, Constrain\nMulti-modal Distribution (CMD), to address this issue. Specifically, we\nintroduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic\nsoft argmin} to encourage the network to produce predominantly unimodal\ndisparity distributions in the target domain, thereby improving prediction\naccuracy. Experimentally, we apply the proposed method to multiple\nrepresentative stereo-matching networks and conduct domain adaptation from\nsynthetic data to unlabeled real-world scenes. Results consistently demonstrate\nimproved generalization in both top-performing and domain-adaptable\nstereo-matching models. The code for CMD will be available at:\n\\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}."}
{"id": "2504.21026", "pdf": "https://arxiv.org/pdf/2504.21026", "abs": "https://arxiv.org/abs/2504.21026", "authors": ["Manish Pandey", "Nageshwar Prasad Yadav", "Mokshada Adduru", "Sawan Rai"], "title": "Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "With the growing presence of multilingual users on social media, detecting\nabusive language in code-mixed text has become increasingly challenging.\nCode-mixed communication, where users seamlessly switch between English and\ntheir native languages, poses difficulties for traditional abuse detection\nmodels, as offensive content may be context-dependent or obscured by linguistic\nblending. While abusive language detection has been extensively explored for\nhigh-resource languages like English and Hindi, low-resource languages such as\nTelugu and Nepali remain underrepresented, leaving gaps in effective\nmoderation. In this study, we introduce a novel, manually annotated dataset of\n2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized\nas abusive and non-abusive, collected from various social media platforms. The\ndataset undergoes rigorous preprocessing before being evaluated across multiple\nMachine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We\nexperimented with models including Logistic Regression, Random Forest, Support\nVector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing\ntheir performance through hyperparameter tuning, and evaluate it using 10-fold\ncross-validation and statistical significance testing (t-test). Our findings\nprovide key insights into the challenges of detecting abusive language in\ncode-mixed settings and offer a comparative analysis of computational\napproaches. This study contributes to advancing NLP for low-resource languages\nby establishing benchmarks for abusive language detection in Telugu-English and\nNepali-English code-mixed text. The dataset and insights can aid in the\ndevelopment of more robust moderation strategies for multilingual social media\nenvironments."}
{"id": "2504.21307", "pdf": "https://arxiv.org/pdf/2504.21307", "abs": "https://arxiv.org/abs/2504.21307", "authors": ["Siyi Chen", "Yimeng Zhang", "Sijia Liu", "Qing Qu"], "title": "The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable generalization capabilities of diffusion models,\nrecent studies have shown that these models can memorize and generate harmful\ncontent when prompted with specific text instructions. Although fine-tuning\napproaches have been developed to mitigate this issue by unlearning harmful\nconcepts, these methods can be easily circumvented through jailbreaking\nattacks. This indicates that the harmful concept has not been fully erased from\nthe model. However, existing attack methods, while effective, lack\ninterpretability regarding why unlearned models still retain the concept,\nthereby hindering the development of defense strategies. In this work, we\naddress these limitations by proposing an attack method that learns an\northogonal set of interpretable attack token embeddings. The attack token\nembeddings can be decomposed into human-interpretable textual elements,\nrevealing that unlearned models still retain the target concept through\nimplicit textual components. Furthermore, these attack token embeddings are\nrobust and transferable across text prompts, initial noises, and unlearned\nmodels. Finally, leveraging this diverse set of embeddings, we design a defense\nmethod applicable to both our proposed attack and existing attack methods.\nExperimental results demonstrate the effectiveness of both our attack and\ndefense strategies."}
{"id": "2504.21027", "pdf": "https://arxiv.org/pdf/2504.21027", "abs": "https://arxiv.org/abs/2504.21027", "authors": ["Yu Zheng", "Longyi Liu", "Yuming Lin", "Jie Feng", "Guozhen Zhang", "Depeng Jin", "Yong Li"], "title": "UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) holds promise for revolutionizing\nvarious fields traditionally dominated by human expertise. Urban planning, a\nprofessional discipline that fundamentally shapes our daily surroundings, is\none such field heavily relying on multifaceted domain knowledge and experience\nof human experts. The extent to which LLMs can assist human practitioners in\nurban planning remains largely unexplored. In this paper, we introduce a\ncomprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of\nLLMs in urban planning, which encompasses fundamental principles, professional\nknowledge, and management and regulations, aligning closely with the\nqualifications expected of human planners. Through extensive evaluation, we\nreveal a significant imbalance in the acquisition of planning knowledge among\nLLMs, with even the most proficient models falling short of meeting\nprofessional standards. For instance, we observe that 70% of LLMs achieve\nsubpar performance in understanding planning regulations compared to other\naspects. Besides the benchmark, we present the largest-ever supervised\nfine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction\npairs sourced from urban planning exams and textbooks. Our findings demonstrate\nthat fine-tuned models exhibit enhanced performance in memorization tests and\ncomprehension of urban planning knowledge, while there exists significant room\nfor improvement, particularly in tasks requiring domain-specific terminology\nand reasoning. By making our benchmark, dataset, and associated evaluation and\nfine-tuning toolsets publicly available at\nhttps://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the\nintegration of LLMs into practical urban planning, fostering a symbiotic\ncollaboration between human expertise and machine intelligence."}
{"id": "2504.21308", "pdf": "https://arxiv.org/pdf/2504.21308", "abs": "https://arxiv.org/abs/2504.21308", "authors": ["Yunhao Li", "Sijing Wu", "Wei Sun", "Zhichao Zhang", "Yucheng Zhu", "Zicheng Zhang", "Huiyu Duan", "Xiongkuo Min", "Guangtao Zhai"], "title": "AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of text-to-image (T2I) generation approaches has\nattracted extensive interest in evaluating the quality of generated images,\nleading to the development of various quality assessment methods for\ngeneral-purpose T2I outputs. However, existing image quality assessment (IQA)\nmethods are limited to providing global quality scores, failing to deliver\nfine-grained perceptual evaluations for structurally complex subjects like\nhumans, which is a critical challenge considering the frequent anatomical and\ntextural distortions in AI-generated human images (AGHIs). To address this gap,\nwe introduce AGHI-QA, the first large-scale benchmark specifically designed for\nquality assessment of AGHIs. The dataset comprises 4,000 images generated from\n400 carefully crafted text prompts using 10 state of-the-art T2I models. We\nconduct a systematic subjective study to collect multidimensional annotations,\nincluding perceptual quality scores, text-image correspondence scores, visible\nand distorted body part labels. Based on AGHI-QA, we evaluate the strengths and\nweaknesses of current T2I methods in generating human images from multiple\ndimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that\nintegrates the large multimodal model (LMM) with domain-specific human features\nfor precise quality prediction and identification of visible and distorted body\nparts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor\nshowcases state-of-the-art performance, significantly outperforming existing\nIQA methods in multidimensional quality assessment and surpassing leading LMMs\nin detecting structural distortions in AGHIs."}
{"id": "2504.21117", "pdf": "https://arxiv.org/pdf/2504.21117", "abs": "https://arxiv.org/abs/2504.21117", "authors": ["Hanhua Hong", "Chenghao Xiao", "Yang Wang", "Yiqi Liu", "Wenge Rong", "Chenghua Lin"], "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Evaluating natural language generation (NLG) systems is challenging due to\nthe diversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluation offers a scalable alternative\nbut is highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."}
{"id": "2504.21309", "pdf": "https://arxiv.org/pdf/2504.21309", "abs": "https://arxiv.org/abs/2504.21309", "authors": ["Modesto Castrillón-Santana", "Oliverio J Santana", "David Freire-Obregón", "Daniel Hernández-Sosa", "Javier Lorenzo-Navarro"], "title": "An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images", "categories": ["cs.CV", "I.2.10"], "comment": null, "summary": "Facial expression recognition (FER) is a key research area in computer vision\nand human-computer interaction. Despite recent advances in deep learning,\nchallenges persist, especially in generalizing to new scenarios. In fact,\nzero-shot FER significantly reduces the performance of state-of-the-art FER\nmodels. To address this problem, the community has recently started to explore\nthe integration of knowledge from Large Language Models for visual tasks. In\nthis work, we evaluate a broad collection of locally executed Visual Language\nModels (VLMs), avoiding the lack of task-specific knowledge by adopting a\nVisual Question Answering strategy. We compare the proposed pipeline with\nstate-of-the-art FER models, both integrating and excluding VLMs, evaluating\nwell-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show\nexcellent performance for some VLMs in zero-shot FER scenarios, indicating the\nneed for further exploration to improve FER generalization."}
{"id": "2504.21132", "pdf": "https://arxiv.org/pdf/2504.21132", "abs": "https://arxiv.org/abs/2504.21132", "authors": ["Naheed Rayhan", "Md. Ashrafuzzaman"], "title": "LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have demonstrated the\ncapability to generate human like, natural responses across a range of tasks,\nincluding task oriented dialogue and question answering. However, their\napplication in real world, critical scenarios is often hindered by a tendency\nto produce inaccurate information and a limited ability to leverage external\nknowledge sources. This paper introduces the LLM ENHANCER system, designed to\nintegrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to\nenhance data accuracy. The LLMs employed within this system are open source.\nThe data acquisition process for the LLM ENHANCER system operates in parallel,\nutilizing custom agent tools to manage the flow of information. Vector\nembeddings are used to identify the most pertinent information, which is\nsubsequently supplied to the LLM for user interaction. The LLM ENHANCER system\nmitigates hallucinations in chat based LLMs while preserving response\nnaturalness and accuracy."}
{"id": "2504.21325", "pdf": "https://arxiv.org/pdf/2504.21325", "abs": "https://arxiv.org/abs/2504.21325", "authors": ["Abdul Sami", "Avinash Kumar", "Irfanullah Memon", "Youngwon Jo", "Muhammad Rizwan", "Jaeyoung Choi"], "title": "Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, Accepted at ICOIN 2025", "summary": "Automatic font generation (AFG) is the process of creating a new font using\nonly a few examples of the style images. Generating fonts for complex languages\nlike Korean and Chinese, particularly in handwritten styles, presents\nsignificant challenges. Traditional AFGs, like Generative adversarial networks\n(GANs) and Variational Auto-Encoders (VAEs), are usually unstable during\ntraining and often face mode collapse problems. They also struggle to capture\nfine details within font images. To address these problems, we present a\ndiffusion-based AFG method which generates high-quality, diverse Korean font\nimages using only a single reference image, focusing on handwritten and printed\nstyles. Our approach refines noisy images incrementally, ensuring stable\ntraining and visually appealing results. A key innovation is our text encoder,\nwhich processes phonetic representations to generate accurate and contextually\ncorrect characters, even for unseen characters. We used a pre-trained style\nencoder from DG FONT to effectively and accurately encode the style images. To\nfurther enhance the generation quality, we used perceptual loss that guides the\nmodel to focus on the global style of generated images. Experimental results on\nover 2000 Korean characters demonstrate that our model consistently generates\naccurate and detailed font images and outperforms benchmark methods, making it\na reliable tool for generating authentic Korean fonts across different styles."}
{"id": "2504.21165", "pdf": "https://arxiv.org/pdf/2504.21165", "abs": "https://arxiv.org/abs/2504.21165", "authors": ["Mark Huasong Meng", "Ruizhe Wang", "Meng Xu", "Chuan Yan", "Guangdong Bai"], "title": "Detecting Manipulated Contents Using Knowledge-Grounded Inference", "categories": ["cs.CL", "cs.SI"], "comment": "16 pages", "summary": "The detection of manipulated content, a prevalent form of fake news, has been\nwidely studied in recent years. While existing solutions have been proven\neffective in fact-checking and analyzing fake news based on historical events,\nthe reliance on either intrinsic knowledge obtained during training or manually\ncurated context hinders them from tackling zero-day manipulated content, which\ncan only be recognized with real-time contextual information. In this work, we\npropose Manicod, a tool designed for detecting zero-day manipulated content.\nManicod first sources contextual information about the input claim from\nmainstream search engines, and subsequently vectorizes the context for the\nlarge language model (LLM) through retrieval-augmented generation (RAG). The\nLLM-based inference can produce a \"truthful\" or \"manipulated\" decision and\noffer a textual explanation for the decision. To validate the effectiveness of\nManicod, we also propose a dataset comprising 4270 pieces of manipulated fake\nnews derived from 2500 recent real-world news headlines. Manicod achieves an\noverall F1 score of 0.856 on this dataset and outperforms existing methods by\nup to 1.9x in F1 score on their benchmarks on fact-checking and claim\nverification."}
{"id": "2504.21334", "pdf": "https://arxiv.org/pdf/2504.21334", "abs": "https://arxiv.org/abs/2504.21334", "authors": ["Misora Sugiyama", "Hirokatsu Kataoka"], "title": "Simple Visual Artifact Detection in Sora-Generated Videos", "categories": ["cs.CV"], "comment": null, "summary": "The December 2024 release of OpenAI's Sora, a powerful video generation model\ndriven by natural language prompts, highlights a growing convergence between\nlarge language models (LLMs) and video synthesis. As these multimodal systems\nevolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating,\nand interacting with visual content, understanding their limitations and\nensuring their safe deployment becomes essential. This study investigates\nvisual artifacts frequently found and reported in Sora-generated videos, which\ncan compromise quality, mislead viewers, or propagate disinformation. We\npropose a multi-label classification framework targeting four common artifact\nlabel types: label 1: boundary / edge defects, label 2: texture / noise issues,\nlabel 3: movement / joint anomalies, and label 4: object mismatches /\ndisappearances. Using a dataset of 300 manually annotated frames extracted from\n15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50,\nEfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50\nachieved an average multi-label classification accuracy of 94.14%. This work\nsupports the broader development of VidLLMs by contributing to (1) the creation\nof datasets for video quality evaluation, (2) interpretable artifact-based\nanalysis beyond language metrics, and (3) the identification of visual risks\nrelevant to factuality and safety."}
{"id": "2504.21191", "pdf": "https://arxiv.org/pdf/2504.21191", "abs": "https://arxiv.org/abs/2504.21191", "authors": ["Lovedeep Gondara", "Jonathan Simkin", "Graham Sayle", "Shebnum Devji", "Gregory Arbour", "Raymond Ng"], "title": "Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study aims to guide language model selection by investigating: 1) the\nnecessity of finetuning versus zero-shot usage, 2) the benefits of\ndomain-adjacent versus generic pretrained models, 3) the value of further\ndomain-specific pretraining, and 4) the continued relevance of Small Language\nModels (SLMs) compared to Large Language Models (LLMs) for specific tasks.\nUsing electronic pathology reports from the British Columbia Cancer Registry\n(BCCR), three classification scenarios with varying difficulty and data size\nare evaluated. Models include various SLMs and an LLM. SLMs are evaluated both\nzero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning\nsignificantly improved SLM performance across all scenarios compared to their\nzero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was\nconsistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally\nperformed better than the generic SLM after finetuning, especially on harder\ntasks. Further domain-specific pretraining yielded modest gains on easier tasks\nbut significant improvements on the complex, data-scarce task. The results\nhighlight the critical role of finetuning for SLMs in specialized domains,\nenabling them to surpass zero-shot LLM performance on targeted classification\ntasks. Pretraining on domain-adjacent or domain-specific data provides further\nadvantages, particularly for complex problems or limited finetuning data. While\nLLMs offer strong zero-shot capabilities, their performance on these specific\ntasks did not match that of appropriately finetuned SLMs. In the era of LLMs,\nSLMs remain relevant and effective, offering a potentially superior\nperformance-resource trade-off compared to LLMs."}
{"id": "2504.21336", "pdf": "https://arxiv.org/pdf/2504.21336", "abs": "https://arxiv.org/abs/2504.21336", "authors": ["Linshan Wu", "Yuxiang Nie", "Sunan He", "Jiaxin Zhuang", "Hao Chen"], "title": "UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation", "categories": ["cs.CV"], "comment": "The first universal foundation model for grounded biomedical image\n  interpretation", "summary": "Multi-modal interpretation of biomedical images opens up novel opportunities\nin biomedical image analysis. Conventional AI approaches typically rely on\ndisjointed training, i.e., Large Language Models (LLMs) for clinical text\ngeneration and segmentation models for target extraction, which results in\ninflexible real-world deployment and a failure to leverage holistic biomedical\ninformation. To this end, we introduce UniBiomed, the first universal\nfoundation model for grounded biomedical image interpretation. UniBiomed is\nbased on a novel integration of Multi-modal Large Language Model (MLLM) and\nSegment Anything Model (SAM), which effectively unifies the generation of\nclinical texts and the segmentation of corresponding biomedical objects for\ngrounded interpretation. In this way, UniBiomed is capable of tackling a wide\nrange of biomedical tasks across ten diverse biomedical imaging modalities. To\ndevelop UniBiomed, we curate a large-scale dataset comprising over 27 million\ntriplets of images, annotations, and text descriptions across ten imaging\nmodalities. Extensive validation on 84 internal and external datasets\ndemonstrated that UniBiomed achieves state-of-the-art performance in\nsegmentation, disease recognition, region-aware diagnosis, visual question\nanswering, and report generation. Moreover, unlike previous models that rely on\nclinical experts to pre-diagnose images and manually craft precise textual or\nvisual prompts, UniBiomed can provide automated and end-to-end grounded\ninterpretation for biomedical image analysis. This represents a novel paradigm\nshift in clinical workflows, which will significantly improve diagnostic\nefficiency. In summary, UniBiomed represents a novel breakthrough in biomedical\nAI, unlocking powerful grounded interpretation capabilities for more accurate\nand efficient biomedical image analysis."}
{"id": "2504.21202", "pdf": "https://arxiv.org/pdf/2504.21202", "abs": "https://arxiv.org/abs/2504.21202", "authors": ["Ramon Pires", "Roseval Malaquias Junior", "Rodrigo Nogueira"], "title": "Automatic Legal Writing Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent advances in Large Language Models, benchmarks for\nevaluating legal writing remain scarce due to the inherent complexity of\nassessing open-ended responses in this domain. One of the key challenges in\nevaluating language models on domain-specific tasks is finding test datasets\nthat are public, frequently updated, and contain comprehensive evaluation\nguidelines. The Brazilian Bar Examination meets these requirements. We\nintroduce oab-bench, a benchmark comprising 105 questions across seven areas of\nlaw from recent editions of the exam. The benchmark includes comprehensive\nevaluation guidelines and reference materials used by human examiners to ensure\nconsistent grading. We evaluate the performance of four LLMs on oab-bench,\nfinding that Claude-3.5 Sonnet achieves the best results with an average score\nof 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can\nserve as reliable automated judges for evaluating legal writing. Our\nexperiments show that frontier models like OpenAI's o1 achieve a strong\ncorrelation with human scores when evaluating approved exams, suggesting their\npotential as reliable automated evaluators despite the inherently subjective\nnature of legal writing assessment. The source code and the benchmark --\ncontaining questions, evaluation guidelines, model-generated responses, and\ntheir respective automated evaluations -- are publicly available."}
{"id": "2504.21340", "pdf": "https://arxiv.org/pdf/2504.21340", "abs": "https://arxiv.org/abs/2504.21340", "authors": ["Khoa Tuan Nguyen", "Ho-min Park", "Gaeun Oh", "Joris Vankerschaver", "Wesley De Neve"], "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ISBI 2025 \"Challenge 2: Pap Smear Cell Classification\n  Challenge\"", "summary": "We propose a novel approach to cervical cell image classification for\ncervical cancer screening using the EVA-02 transformer model. We developed a\nfour-step pipeline: fine-tuning EVA-02, feature extraction, selecting important\nfeatures through multiple machine learning models, and training a new\nartificial neural network with optional loss weighting for improved\ngeneralization. With this design, our best model achieved an F1-score of\n0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized\nKernel SHAP analysis and identified key features correlating with cell\nmorphology and staining characteristics, providing interpretable insights into\nthe decision-making process of the fine-tuned model. Our code is available at\nhttps://github.com/Khoa-NT/isbi2025_ps3c."}
{"id": "2504.21214", "pdf": "https://arxiv.org/pdf/2504.21214", "abs": "https://arxiv.org/abs/2504.21214", "authors": ["Jinzhao Zhou", "Zehong Cao", "Yiqun Duan", "Connor Barkley", "Daniel Leong", "Xiaowei Jiang", "Quoc-Toan Nguyen", "Ziyi Zhao", "Thomas Do", "Yu-Cheng Chang", "Sheng-Fu Liang", "Chin-teng Lin"], "title": "Pretraining Large Brain Language Model for Active BCI: Silent Speech", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "This paper explores silent speech decoding in active brain-computer interface\n(BCI) systems, which offer more natural and flexible communication than\ntraditional BCI applications. We collected a new silent speech dataset of over\n120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing\n24 commonly used English words for language model pretraining and decoding.\nFollowing the recent success of pretraining large models with self-supervised\nparadigms to enhance EEG classification performance, we propose Large Brain\nLanguage Model (LBLM) pretrained to decode silent speech for active BCI. To\npretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining\nparadigm to learn effective representations from unlabeled EEG data. Unlike\nexisting EEG pretraining methods that mainly follow a masked-reconstruction\nparadigm, our proposed FSTP method employs autoregressive modeling in temporal\nand frequency domains to capture both temporal and spectral dependencies from\nEEG signals. After pretraining, we finetune our LBLM on downstream tasks,\nincluding word-level and semantic-level classification. Extensive experiments\ndemonstrate significant performance gains of the LBLM over fully-supervised and\npretrained baseline models. For instance, in the difficult cross-session\nsetting, our model achieves 47.0\\% accuracy on semantic-level classification\nand 39.6\\% in word-level classification, outperforming baseline methods by\n5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in\nactive BCI systems, offering an innovative solution for EEG language model\npretraining and a new dataset for fundamental research."}
{"id": "2504.21344", "pdf": "https://arxiv.org/pdf/2504.21344", "abs": "https://arxiv.org/abs/2504.21344", "authors": ["Luoting Zhuang", "Seyed Mohammad Hossein Tabatabaei", "Ramin Salehi-Rad", "Linh M. Tran", "Denise R. Aberle", "Ashley E. Prosper", "William Hsu"], "title": "Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection", "categories": ["cs.CV", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Objective: A number of machine learning models have utilized semantic\nfeatures, deep features, or both to assess lung nodule malignancy. However,\ntheir reliance on manual annotation during inference, limited interpretability,\nand sensitivity to imaging variations hinder their application in real-world\nclinical settings. Thus, this research aims to integrate semantic features\nderived from radiologists' assessments of nodules, allowing the model to learn\nclinically relevant, robust, and explainable features for predicting lung\ncancer. Methods: We obtained 938 low-dose CT scans from the National Lung\nScreening Trial with 1,246 nodules and semantic features. The Lung Image\nDatabase Consortium dataset contains 1,018 CT scans, with 2,625 lesions\nannotated for nodule characteristics. Three external datasets were obtained\nfrom UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We\nfinetuned a pretrained Contrastive Language-Image Pretraining model with a\nparameter-efficient fine-tuning approach to align imaging and semantic features\nand predict the one-year lung cancer diagnosis. Results: We evaluated the\nperformance of the one-year diagnosis of lung cancer with AUROC and AUPRC and\ncompared it to three state-of-the-art models. Our model demonstrated an AUROC\nof 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on\nexternal datasets. Using CLIP, we also obtained predictions on semantic\nfeatures, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and\npleural attachment (0.84), that can be used to explain model predictions.\nConclusion: Our approach accurately classifies lung nodules as benign or\nmalignant, providing explainable outputs, aiding clinicians in comprehending\nthe underlying meaning of model predictions. This approach also prevents the\nmodel from learning shortcuts and generalizes across clinical settings."}
{"id": "2504.21233", "pdf": "https://arxiv.org/pdf/2504.21233", "abs": "https://arxiv.org/abs/2504.21233", "authors": ["Haoran Xu", "Baolin Peng", "Hany Awadalla", "Dongdong Chen", "Yen-Chun Chen", "Mei Gao", "Young Jin Kim", "Yunsheng Li", "Liliang Ren", "Yelong Shen", "Shuohang Wang", "Weijian Xu", "Jianfeng Gao", "Weizhu Chen"], "title": "Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities\nin Large Language Models (LLMs) by training them to explicitly generate\nintermediate reasoning steps. While LLMs readily benefit from such techniques,\nimproving reasoning in Small Language Models (SLMs) remains challenging due to\ntheir limited model capacity. Recent work by Deepseek-R1 demonstrates that\ndistillation from LLM-generated synthetic data can substantially improve the\nreasoning ability of SLM. However, the detailed modeling recipe is not\ndisclosed. In this work, we present a systematic training recipe for SLMs that\nconsists of four steps: (1) large-scale mid-training on diverse distilled\nlong-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3)\nRollout DPO leveraging a carefully curated preference dataset, and (4)\nReinforcement Learning (RL) with Verifiable Reward. We apply our method on\nPhi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning\nmodel exceeds, on math reasoning tasks, much larger reasoning models, e.g.,\noutperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and\nDeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate\nthat a carefully designed training recipe, with large-scale high-quality CoT\ndata, is effective to unlock strong reasoning capabilities even in\nresource-constrained small models."}
{"id": "2504.21356", "pdf": "https://arxiv.org/pdf/2504.21356", "abs": "https://arxiv.org/abs/2504.21356", "authors": ["Hong Zhang", "Zhongjie Duan", "Xingjun Wang", "Yingda Chen", "Yuze Zhao", "Yu Zhang"], "title": "Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) aim to integrate multimodal\nunderstanding and generation abilities through a single framework. Despite\ntheir versatility, existing open-source unified models exhibit performance gaps\nagainst domain-specific architectures. To bridge this gap, we present\nNexus-Gen, a unified model that synergizes the language reasoning capabilities\nof LLMs with the image synthesis power of diffusion models. To align the\nembedding space of the LLM and diffusion model, we conduct a dual-phase\nalignment training process. (1) The autoregressive LLM learns to predict image\nembeddings conditioned on multimodal inputs, while (2) the vision decoder is\ntrained to reconstruct high-fidelity images from these embeddings. During\ntraining the LLM, we identified a critical discrepancy between the\nautoregressive paradigm's training and inference phases, where error\naccumulation in continuous embedding space severely degrades generation\nquality. To avoid this issue, we introduce a prefilled autoregression strategy\nthat prefills input sequence with position-embedded special tokens instead of\ncontinuous embeddings. Through dual-phase training, Nexus-Gen has developed the\nintegrated capability to comprehensively address the image understanding,\ngeneration and editing tasks. All models, datasets, and codes are published at\nhttps://github.com/modelscope/Nexus-Gen.git to facilitate further advancements\nacross the field."}
{"id": "2504.21239", "pdf": "https://arxiv.org/pdf/2504.21239", "abs": "https://arxiv.org/abs/2504.21239", "authors": ["Xu Pan", "Ely Hahami", "Zechen Zhang", "Haim Sompolinsky"], "title": "Memorization and Knowledge Injection in Gated LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) currently struggle to sequentially add new\nmemories and integrate new knowledge. These limitations contrast with the human\nability to continuously learn from new experiences and acquire knowledge\nthroughout life. Most existing approaches add memories either through large\ncontext windows or external memory buffers (e.g., Retrieval-Augmented\nGeneration), and studies on knowledge injection rarely test scenarios\nresembling everyday life events. In this work, we introduce a continual\nlearning framework, Memory Embedded in Gated LLMs (MEGa), which injects event\nmemories directly into the weights of LLMs. Each memory is stored in a\ndedicated set of gated low-rank weights. During inference, a gating mechanism\nactivates relevant memory weights by matching query embeddings to stored memory\nembeddings. This enables the model to both recall entire memories and answer\nrelated questions. On two datasets - fictional characters and Wikipedia events\n- MEGa outperforms baseline approaches in mitigating catastrophic forgetting.\nOur model draws inspiration from the complementary memory system of the human\nbrain."}
{"id": "2504.21368", "pdf": "https://arxiv.org/pdf/2504.21368", "abs": "https://arxiv.org/abs/2504.21368", "authors": ["Pramook Khungurn", "Sukit Seripanitkarn", "Phonphrm Thawatdamrongkit", "Supasorn Suwajanakorn"], "title": "Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality", "categories": ["cs.CV", "cs.AI"], "comment": "AI for Content Creation (AI4CC) Workshop at CVPR 2025", "summary": "Diffusion autoencoders (DAEs) are typically formulated as a noise prediction\nmodel and trained with a linear-$\\beta$ noise schedule that spends much of its\nsampling steps at high noise levels. Because high noise levels are associated\nwith recovering large-scale image structures and low noise levels with\nrecovering details, this configuration can result in low-quality and blurry\nimages. However, it should be possible to improve details while spending fewer\nsteps recovering structures because the latent code should already contain\nstructural information. Based on this insight, we propose a new DAE training\nmethod that improves the quality of reconstructed images. We divide training\ninto two phases. In the first phase, the DAE is trained as a vanilla\nautoencoder by always setting the noise level to the highest, forcing the\nencoder and decoder to populate the latent code with structural information. In\nthe second phase, we incorporate a noise schedule that spends more time in the\nlow-noise region, allowing the DAE to learn how to perfect the details. Our\nmethod results in images that have accurate high-level structures and low-level\ndetails while still preserving useful properties of the latent codes."}
{"id": "2504.21252", "pdf": "https://arxiv.org/pdf/2504.21252", "abs": "https://arxiv.org/abs/2504.21252", "authors": ["Xuanzhao Dong", "Wenhui Zhu", "Hao Wang", "Xiwen Chen", "Peijie Qiu", "Rui Yin", "Yi Su", "Yalin Wang"], "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA", "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering (QA) is a reasoning-intensive task that remains\nchallenging for large language models (LLMs) due to hallucinations and outdated\ndomain knowledge. Retrieval-Augmented Generation (RAG) provides a promising\npost-training solution by leveraging external knowledge. However, existing\nmedical RAG systems suffer from two key limitations: (1) a lack of modeling for\nhuman-like reasoning behaviors during information retrieval, and (2) reliance\non suboptimal medical corpora, which often results in the retrieval of\nirrelevant or noisy snippets. To overcome these challenges, we propose\nDiscuss-RAG, a plug-and-play module designed to enhance the medical QA RAG\nsystem through collaborative agent-based reasoning. Our method introduces a\nsummarizer agent that orchestrates a team of medical experts to emulate\nmulti-turn brainstorming, thereby improving the relevance of retrieved content.\nAdditionally, a decision-making agent evaluates the retrieved snippets before\ntheir final integration. Experimental results on four benchmark medical QA\ndatasets show that Discuss-RAG consistently outperforms MedRAG, especially\nsignificantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on\nPubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG."}
{"id": "2504.21385", "pdf": "https://arxiv.org/pdf/2504.21385", "abs": "https://arxiv.org/abs/2504.21385", "authors": ["Shijun Zhou", "Yajing Liu", "Chunhui Hao", "Zhiyuan Liu", "Jiandong Tian"], "title": "IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing", "categories": ["cs.CV"], "comment": null, "summary": "Due to the domain gap between real-world and synthetic hazy images, current\ndata-driven dehazing algorithms trained on synthetic datasets perform well on\nsynthetic data but struggle to generalize to real-world scenarios. To address\nthis challenge, we propose \\textbf{I}mage \\textbf{D}ehazing \\textbf{D}iffusion\n\\textbf{M}odels (IDDM), a novel diffusion process that incorporates the\natmospheric scattering model into noise diffusion. IDDM aims to use the gradual\nhaze formation process to help the denoising Unet robustly learn the\ndistribution of clear images from the conditional input hazy images. We design\na specialized training strategy centered around IDDM. Diffusion models are\nleveraged to bridge the domain gap from synthetic to real-world, while the\natmospheric scattering model provides physical guidance for haze formation.\nDuring the forward process, IDDM simultaneously introduces haze and noise into\nclear images, and then robustly separates them during the sampling process. By\ntraining with physics-guided information, IDDM shows the ability of domain\ngeneralization, and effectively restores the real-world hazy images despite\nbeing trained on synthetic datasets. Extensive experiments demonstrate the\neffectiveness of our method through both quantitative and qualitative\ncomparisons with state-of-the-art approaches."}
{"id": "2504.21299", "pdf": "https://arxiv.org/pdf/2504.21299", "abs": "https://arxiv.org/abs/2504.21299", "authors": ["Zhiting Fan", "Ruizhe Chen", "Zuozhu Liu"], "title": "BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Identifying bias in LLM-generated content is a crucial prerequisite for\nensuring fairness in LLMs. Existing methods, such as fairness classifiers and\nLLM-based judges, face limitations related to difficulties in understanding\nunderlying intentions and the lack of criteria for fairness judgment. In this\npaper, we introduce BiasGuard, a novel bias detection tool that explicitly\nanalyzes inputs and reasons through fairness specifications to provide accurate\njudgments. BiasGuard is implemented through a two-stage approach: the first\nstage initializes the model to explicitly reason based on fairness\nspecifications, while the second stage leverages reinforcement learning to\nenhance its reasoning and judgment capabilities. Our experiments, conducted\nacross five datasets, demonstrate that BiasGuard outperforms existing tools,\nimproving accuracy and reducing over-fairness misjudgments. We also highlight\nthe importance of reasoning-enhanced decision-making and provide evidence for\nthe effectiveness of our two-stage optimization pipeline."}
{"id": "2504.21387", "pdf": "https://arxiv.org/pdf/2504.21387", "abs": "https://arxiv.org/abs/2504.21387", "authors": ["Teodor Boyadzhiev", "Gabriele Lagani", "Luca Ciampi", "Giuseppe Amato", "Krassimira Ivanova"], "title": "Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain", "categories": ["cs.CV"], "comment": "Accepted at the 10th International Euro-Mediterranean Conference\n  (EuroMed 2024)", "summary": "The integration of computer vision and deep learning is an essential part of\ndocumenting and preserving cultural heritage, as well as improving visitor\nexperiences. In recent years, two deep learning paradigms have been established\nin the field of computer vision: convolutional neural networks and transformer\narchitectures. The present study aims to make a comparative analysis of some\nrepresentatives of these two techniques of their ability to transfer knowledge\nfrom generic dataset, such as ImageNet, to cultural heritage specific tasks.\nThe results of testing examples of the architectures VGG, ResNet, DenseNet,\nVisual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is\nthe best in terms of efficiency-computability ratio."}
{"id": "2504.21303", "pdf": "https://arxiv.org/pdf/2504.21303", "abs": "https://arxiv.org/abs/2504.21303", "authors": ["Xiao Xiao", "Yu Su", "Sijing Zhang", "Zhang Chen", "Yadong Chen", "Tian Liu"], "title": "Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit probabilistic output characteristics,\nyet conventional evaluation frameworks rely on deterministic scalar metrics.\nThis study introduces a Bayesian approach for LLM capability assessment that\nintegrates prior knowledge through probabilistic inference, addressing\nlimitations under limited-sample regimes. By treating model capabilities as\nlatent variables and leveraging a curated query set to induce discriminative\nresponses, we formalize model ranking as a Bayesian hypothesis testing problem\nover mutually exclusive capability intervals. Experimental evaluations with\nGPT-series models demonstrate that the proposed method achieves superior\ndiscrimination compared to conventional evaluation methods. Results indicate\nthat even with reduced sample sizes, the approach maintains statistical\nrobustness while providing actionable insights, such as probabilistic\nstatements about a model's likelihood of surpassing specific baselines. This\nwork advances LLM evaluation methodologies by bridging Bayesian inference with\npractical constraints in real-world deployment scenarios."}
{"id": "2504.21403", "pdf": "https://arxiv.org/pdf/2504.21403", "abs": "https://arxiv.org/abs/2504.21403", "authors": ["Yumeng Shi", "Quanyu Long", "Wenya Wang"], "title": "Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Video question answering benefits from the rich information available in\nvideos, enabling a wide range of applications. However, the large volume of\ntokens generated from longer videos presents significant challenges to memory\nefficiency and model performance. To alleviate this issue, existing works\npropose to compress video inputs, but usually overlooking the varying\nimportance of static and dynamic information across different queries, leading\nto inefficient token usage within limited budgets. To tackle this, we propose a\nnovel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust\nstatic and dynamic information needed based on question requirements. Our\nframework first explores different token allocations between static frames,\nwhich preserve spatial details, and dynamic frames, which capture temporal\nchanges. Next, it employs a query-aware attention-based metric to select the\noptimal token combination without model updates. Our proposed framework is\nplug-and-play that can be seamlessly integrated within diverse video-language\nmodels. Extensive experiments show that our method achieves significant\nperformance improvements (up to 5.8%) among various video question answering\nbenchmarks."}
{"id": "2504.21330", "pdf": "https://arxiv.org/pdf/2504.21330", "abs": "https://arxiv.org/abs/2504.21330", "authors": ["Kaixun Yang", "Mladen Raković", "Dragan Gašević", "Guanliang Chen"], "title": "Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES)\ndue to their ability to capture semantic meaning. Traditional fine-tuning\napproaches required technical expertise, limiting accessibility for educators\nwith limited technical backgrounds. However, prompt-based tools like ChatGPT\nhave made AES more accessible, enabling educators to obtain machine-generated\nscores using natural-language prompts (i.e., the prompt-based paradigm).\nDespite advancements, prior studies have shown bias in fine-tuned LLMs,\nparticularly against disadvantaged groups. It remains unclear whether such\nbiases persist or are amplified in the prompt-based paradigm with cutting-edge\ntools. Since such biases are believed to stem from the demographic information\nembedded in pre-trained models (i.e., the ability of LLMs' text embeddings to\npredict demographic attributes), this study explores the relationship between\nthe model's predictive power of students' demographic attributes based on their\nwritten works and its predictive bias in the scoring task in the prompt-based\nparadigm. Using a publicly available dataset of over 25,000 students'\nargumentative essays, we designed prompts to elicit demographic inferences\n(i.e., gender, first-language background) from GPT-4o and assessed fairness in\nautomated scoring. Then we conducted multivariate regression analysis to\nexplore the impact of the model's ability to predict demographics on its\nscoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat\ninfer students' demographics, particularly their first-language backgrounds,\nfrom their essays; (ii) scoring biases are more pronounced when the LLM\ncorrectly predicts students' first-language background than when it does not;\nand (iii) scoring error for non-native English speakers increases when the LLM\ncorrectly identifies them as non-native."}
{"id": "2504.21414", "pdf": "https://arxiv.org/pdf/2504.21414", "abs": "https://arxiv.org/abs/2504.21414", "authors": ["Qi Fan", "Kaiqi Liu", "Nian Liu", "Hisham Cholakkal", "Rao Muhammad Anwer", "Wenbin Li", "Yang Gao"], "title": "Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining", "categories": ["cs.CV"], "comment": null, "summary": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel\nclasses in new domains, which is often challenging due to the diverse\ncharacteristics of target domains and the limited availability of support data.\nMost CD-FSS methods redesign and retrain in-domain FSS models using various\ndomain-generalization techniques, which are effective but costly to train. To\naddress these issues, we propose adapting informative model structures of the\nwell-trained FSS model for target domains by learning domain characteristics\nfrom few-shot labeled support samples during inference, thereby eliminating the\nneed for retraining. Specifically, we first adaptively identify domain-specific\nmodel structures by measuring parameter importance using a novel structure\nFisher score in a data-dependent manner. Then, we progressively train the\nselected informative model structures with hierarchically constructed training\nsamples, progressing from fewer to more support shots. The resulting\nInformative Structure Adaptation (ISA) method effectively addresses domain\nshifts and equips existing well-trained in-domain FSS models with flexible\nadaptation capabilities for new domains, eliminating the need to redesign or\nretrain CD-FSS models on base data. Extensive experiments validate the\neffectiveness of our method, demonstrating superior performance across multiple\nCD-FSS benchmarks."}
{"id": "2504.21372", "pdf": "https://arxiv.org/pdf/2504.21372", "abs": "https://arxiv.org/abs/2504.21372", "authors": ["Máté Gedeon"], "title": "Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speech Event Extraction (SpeechEE) is a challenging task that lies at the\nintersection of Automatic Speech Recognition (ASR) and Natural Language\nProcessing (NLP), requiring the identification of structured event information\nfrom spoken language. In this work, we present a modular, pipeline-based\nSpeechEE framework that integrates high-performance ASR with semantic\nsearch-enhanced prompting of Large Language Models (LLMs). Our system first\nclassifies speech segments likely to contain events using a hybrid filtering\nmechanism including rule-based, BERT-based, and LLM-based models. It then\nemploys few-shot LLM prompting, dynamically enriched via semantic similarity\nretrieval, to identify event triggers and extract corresponding arguments. We\nevaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini)\nhighlighting significant performance gains with o1-mini, which achieves 63.3%\nF1 on trigger classification and 27.8% F1 on argument classification,\noutperforming prior benchmarks. Our results demonstrate that pipeline\napproaches, when empowered by retrieval-augmented LLMs, can rival or exceed\nend-to-end systems while maintaining interpretability and modularity. This work\nprovides practical insights into LLM-driven event extraction and opens pathways\nfor future hybrid models combining textual and acoustic features."}
{"id": "2504.21423", "pdf": "https://arxiv.org/pdf/2504.21423", "abs": "https://arxiv.org/abs/2504.21423", "authors": ["Weicai Yan", "Wang Lin", "Zirun Guo", "Ye Wang", "Fangming Feng", "Xiaoda Yang", "Zehan Wang", "Tao Jin"], "title": "Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision", "categories": ["cs.CV"], "comment": "Accepted at ICLR 2025", "summary": "Prompt learning has demonstrated promising results in fine-tuning pre-trained\nmultimodal models. However, the performance improvement is limited when applied\nto more complex and fine-grained tasks. The reason is that most existing\nmethods directly optimize the parameters involved in the prompt generation\nprocess through loss backpropagation, which constrains the richness and\nspecificity of the prompt representations. In this paper, we propose\nDiffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion\nmodel to generate rich and fine-grained prompt information for complex\ndownstream tasks. Specifically, our approach consists of three stages. In the\nfirst stage, we train a Mask-VAE to compress the masks into latent space. In\nthe second stage, we leverage an improved Diffusion Transformer (DiT) to train\na prompt generator in the latent space, using the masks for supervision. In the\nthird stage, we align the denoising process of the prompt generator with the\npre-trained model in the semantic space, and use the generated prompts to\nfine-tune the model. We conduct experiments on a complex pixel-level downstream\ntask, referring expression comprehension, and compare our method with various\nparameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum\nimprovement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model\nand also outperforms other state-of-the-art methods across multiple metrics.\nThe experimental results validate the effectiveness of our approach and\nhighlight the potential of using generative models for prompt generation. Code\nis available at https://github.com/Kelvin-ywc/diff-prompt."}
{"id": "2504.21421", "pdf": "https://arxiv.org/pdf/2504.21421", "abs": "https://arxiv.org/abs/2504.21421", "authors": ["Linxuan Wang", "Shuiyuan Yu"], "title": "The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors", "categories": ["cs.CL"], "comment": "This paper has been accepted by the 13th International Quantitative\n  Linguistics Conference QUALICO 2025", "summary": "To explore the relationship between dependency distance (DD) and hierarchical\ndistance (HD) in Japanese, we compared the probability distributions of DD and\nHD with and without sentence length fixed, and analyzed the changes in mean\ndependency distance (MDD) and mean hierarchical distance (MHD) as sentence\nlength increases, along with their correlation coefficient based on the\nBalanced Corpus of Contemporary Written Japanese. It was found that the valency\nof the predicates is the underlying factor behind the trade-off relation\nbetween MDD and MHD in Japanese. Native speakers of Japanese regulate the\nlinear complexity and hierarchical complexity through the valency of the\npredicates, and the relative sizes of MDD and MHD depend on whether the\nthreshold of valency has been reached. Apart from the cognitive load, the\nvalency of the predicates also affects the probability distributions of DD and\nHD. The effect of the valency of the predicates on the distribution of HD is\ngreater than on that of DD, which leads to differences in their probability\ndistributions and causes the mean of MDD to be lower than that of MHD."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2504.21463", "pdf": "https://arxiv.org/pdf/2504.21463", "abs": "https://arxiv.org/abs/2504.21463", "authors": ["Haowen Hou", "Zhiyi Huang", "Kaifeng Tan", "Rongchang Lu", "Fei Richard Yu"], "title": "RWKV-X: A Linear Complexity Hybrid Language Model", "categories": ["cs.CL"], "comment": "12 pages", "summary": "In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that\ncombines the efficiency of RWKV for short-range modeling with a sparse\nattention mechanism designed to capture long-range context. Unlike previous\nhybrid approaches that rely on full attention layers and retain quadratic\ncomplexity, RWKV-X achieves linear-time complexity in training and\nconstant-time complexity in inference decoding. We demonstrate that RWKV-X,\nwhen continually pretrained on 64K-token sequences, achieves near-perfect\naccuracy on the 64K passkey retrieval benchmark. It consistently outperforms\nprior RWKV-7 models on long-context benchmarks, while maintaining strong\nperformance on short-context tasks. These results highlight RWKV-X as a\nscalable and efficient backbone for general-purpose language modeling, capable\nof decoding sequences up to 1 million tokens with stable speed and memory\nusage. To facilitate further research and analysis, we have made the\ncheckpoints and the associated code publicly accessible at:\nhttps://github.com/howard-hou/RWKV-X."}
{"id": "2504.21447", "pdf": "https://arxiv.org/pdf/2504.21447", "abs": "https://arxiv.org/abs/2504.21447", "authors": ["Haoran Chen", "Junyan Lin", "Xinhao Chen", "Yue Fan", "Xin Jin", "Hui Su", "Jianfeng Dong", "Jinlan Fu", "Xiaoyu Shen"], "title": "Rethinking Visual Layer Selection in Multimodal LLMs", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 4 figures, submitted to ICCV 2025", "summary": "Multimodal large language models (MLLMs) have achieved impressive performance\nacross a wide range of tasks, typically using CLIP-ViT as their visual encoder\ndue to its strong text-image alignment capabilities. While prior studies\nsuggest that different CLIP-ViT layers capture different types of information,\nwith shallower layers focusing on fine visual details and deeper layers\naligning more closely with textual semantics, most MLLMs still select visual\nfeatures based on empirical heuristics rather than systematic analysis. In this\nwork, we propose a Layer-wise Representation Similarity approach to group\nCLIP-ViT layers with similar behaviors into {shallow, middle, and deep}\ncategories and assess their impact on MLLM performance. Building on this\nfoundation, we revisit the visual layer selection problem in MLLMs at scale,\ntraining LLaVA-style models ranging from 1.4B to 7B parameters. Through\nextensive experiments across 10 datasets and 4 tasks, we find that: (1) deep\nlayers are essential for OCR tasks; (2) shallow and middle layers substantially\noutperform deep layers on reasoning tasks involving counting, positioning, and\nobject localization; (3) a lightweight fusion of features across shallow,\nmiddle, and deep layers consistently outperforms specialized fusion baselines\nand single-layer selections, achieving gains on 9 out of 10 datasets. Our work\noffers the first principled study of visual layer selection in MLLMs, laying\nthe groundwork for deeper investigations into visual representation learning\nfor MLLMs."}
{"id": "2504.21474", "pdf": "https://arxiv.org/pdf/2504.21474", "abs": "https://arxiv.org/abs/2504.21474", "authors": ["Hadi Bayrami Asl Tekanlou", "Jafar Razmara", "Mahsa Sanaei", "Mostafa Rahgouy", "Hamed Babaei Giglou"], "title": "Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 4 figures, accepted to the LLMs4Subjects shared task at\n  SemEval2025", "summary": "This paper presents our system, Homa, for SemEval-2025 Task 5: Subject\nTagging, which focuses on automatically assigning subject labels to technical\nrecords from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage\nOntoAligner, a modular ontology alignment toolkit, to address this task by\nintegrating retrieval-augmented generation (RAG) techniques. Our approach\nformulates the subject tagging problem as an alignment task, where records are\nmatched to GND categories based on semantic similarity. We evaluate\nOntoAligner's adaptability for subject indexing and analyze its effectiveness\nin handling multilingual records. Experimental results demonstrate the\nstrengths and limitations of this method, highlighting the potential of\nalignment techniques for improving subject tagging in digital libraries."}
{"id": "2504.21464", "pdf": "https://arxiv.org/pdf/2504.21464", "abs": "https://arxiv.org/abs/2504.21464", "authors": ["Shamim Rahim Refat", "Ziyan Shirin Raha", "Shuvashis Sarker", "Faika Fairuj Preotee", "MD. Musfikur Rahman", "Tashreef Muhammad", "Mohammad Shafiul Islam"], "title": "VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification", "categories": ["cs.CV"], "comment": "33 pages, 49 figures", "summary": "Diabetic retinopathy is a severe eye condition caused by diabetes where the\nretinal blood vessels get damaged and can lead to vision loss and blindness if\nnot treated. Early and accurate detection is key to intervention and stopping\nthe disease progressing. For addressing this disease properly, this paper\npresents a comprehensive approach for automated diabetic retinopathy detection\nby proposing a new hybrid deep learning model called VR-FuseNet. Diabetic\nretinopathy is a major eye disease and leading cause of blindness especially\namong diabetic patients so accurate and efficient automated detection methods\nare required. To address the limitations of existing methods including dataset\nimbalance, diversity and generalization issues this paper presents a hybrid\ndataset created from five publicly available diabetic retinopathy datasets.\nEssential preprocessing techniques such as SMOTE for class balancing and CLAHE\nfor image enhancement are applied systematically to the dataset to improve the\nrobustness and generalizability of the dataset. The proposed VR-FuseNet model\ncombines the strengths of two state-of-the-art convolutional neural networks,\nVGG19 which captures fine-grained spatial features and ResNet50V2 which is\nknown for its deep hierarchical feature extraction. This fusion improves the\ndiagnostic performance and achieves an accuracy of 91.824%. The model\noutperforms individual architectures on all performance metrics demonstrating\nthe effectiveness of hybrid feature extraction in Diabetic Retinopathy\nclassification tasks. To make the proposed model more clinically useful and\ninterpretable this paper incorporates multiple XAI techniques. These techniques\ngenerate visual explanations that clearly indicate the retinal features\naffecting the model's prediction such as microaneurysms, hemorrhages and\nexudates so that clinicians can interpret and validate."}
{"id": "2504.21475", "pdf": "https://arxiv.org/pdf/2504.21475", "abs": "https://arxiv.org/abs/2504.21475", "authors": ["Serry Sibaee", "Samar Ahmed", "Abdullah Al Harbi", "Omer Nacar", "Adel Ammar", "Yasser Habashi", "Wadii Boulila"], "title": "Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical gap in Arabic natural language processing\nby developing an effective Arabic Reverse Dictionary (RD) system that enables\nusers to find words based on their descriptions or meanings. We present a novel\ntransformer-based approach with a semi-encoder neural network architecture\nfeaturing geometrically decreasing layers that achieves state-of-the-art\nresults for Arabic RD tasks. Our methodology incorporates a comprehensive\ndataset construction process and establishes formal quality standards for\nArabic lexicographic definitions. Experiments with various pre-trained models\ndemonstrate that Arabic-specific models significantly outperform general\nmultilingual embeddings, with ARBERTv2 achieving the best ranking score\n(0.0644). Additionally, we provide a formal abstraction of the reverse\ndictionary task that enhances theoretical understanding and develop a modular,\nextensible Python library (RDTL) with configurable training pipelines. Our\nanalysis of dataset quality reveals important insights for improving Arabic\ndefinition construction, leading to eight specific standards for building\nhigh-quality reverse dictionary resources. This work contributes significantly\nto Arabic computational linguistics and provides valuable tools for language\nlearning, academic writing, and professional communication in Arabic."}
{"id": "2504.21467", "pdf": "https://arxiv.org/pdf/2504.21467", "abs": "https://arxiv.org/abs/2504.21467", "authors": ["Luc Vedrenne", "Sylvain Faisan", "Denis Fortun"], "title": "Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space", "categories": ["cs.CV"], "comment": "14 pages, 19 figures, IEEE Transactions on Image Processing", "summary": "Point cloud rigid registration is a fundamental problem in 3D computer\nvision. In the multiview case, we aim to find a set of 6D poses to align a set\nof objects. Methods based on pairwise registration rely on a subsequent\nsynchronization algorithm, which makes them poorly scalable with the number of\nviews. Generative approaches overcome this limitation, but are based on\nGaussian Mixture Models and use an Expectation-Maximization algorithm. Hence,\nthey are not well suited to handle large transformations. Moreover, most\nexisting methods cannot handle high levels of degradations. In this paper, we\nintroduce POLAR (POint cloud LAtent Registration), a multiview registration\nmethod able to efficiently deal with a large number of views, while being\nrobust to a high level of degradations and large initial angles. To achieve\nthis, we transpose the registration problem into the latent space of a\npretrained autoencoder, design a loss taking degradations into account, and\ndevelop an efficient multistart optimization strategy. Our proposed method\nsignificantly outperforms state-of-the-art approaches on synthetic and real\ndata. POLAR is available at github.com/pypolar/polar or as a standalone package\nwhich can be installed with pip install polaregistration."}
{"id": "2504.21540", "pdf": "https://arxiv.org/pdf/2504.21540", "abs": "https://arxiv.org/abs/2504.21540", "authors": ["Adrian Benton", "Alexander Gutkin", "Christo Kirov", "Brian Roark"], "title": "Improving Informally Romanized Language Identification", "categories": ["cs.CL"], "comment": "16 pages, 14 tables, 4 figures", "summary": "The Latin script is often used to informally write languages with non-Latin\nnative scripts. In many cases (e.g., most languages in India), there is no\nconventional spelling of words in the Latin script, hence there will be high\nspelling variability in written text. Such romanization renders languages that\nare normally easily distinguished based on script highly confusable, such as\nHindi and Urdu. In this work, we increase language identification (LID)\naccuracy for romanized text by improving the methods used to synthesize\ntraining sets. We find that training on synthetic samples which incorporate\nnatural spelling variation yields higher LID system accuracy than including\navailable naturally occurring examples in the training set, or even training\nhigher capacity models. We demonstrate new state-of-the-art LID performance on\nromanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set\n(Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a\npretrained neural model) to 85.4% using a linear classifier trained solely on\nsynthetic data and 88.2% when also training on available harvested text."}
{"id": "2504.21468", "pdf": "https://arxiv.org/pdf/2504.21468", "abs": "https://arxiv.org/abs/2504.21468", "authors": ["Yu Guo", "Guoqing Chen", "Tieyong Zeng", "Qiyu Jin", "Michael Kwok-Po Ng"], "title": "Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion", "categories": ["cs.CV", "65F35, 90C30, 94A08, 68U10"], "comment": null, "summary": "Recovering hidden structures from incomplete or noisy data remains a\npervasive challenge across many fields, particularly where multi-dimensional\ndata representation is essential. Quaternion matrices, with their ability to\nnaturally model multi-dimensional data, offer a promising framework for this\nproblem. This paper introduces the quaternion nuclear norm over the Frobenius\nnorm (QNOF) as a novel nonconvex approximation for the rank of quaternion\nmatrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion\nsingular value decomposition, we prove that solving the QNOF can be simplified\nto solving the singular value $L_1/L_2$ problem. Additionally, we extend the\nQNOF to robust quaternion matrix completion, employing the alternating\ndirection multiplier method to derive solutions that guarantee weak convergence\nunder mild conditions. Extensive numerical experiments validate the proposed\nmodel's superiority, consistently outperforming state-of-the-art quaternion\nmethods."}
{"id": "2504.21547", "pdf": "https://arxiv.org/pdf/2504.21547", "abs": "https://arxiv.org/abs/2504.21547", "authors": ["Aleksei Dorkin", "Kairit Sirts"], "title": "TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval", "categories": ["cs.CL"], "comment": "To appear in the Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025)", "summary": "We present our submission to the Task 5 of SemEval-2025 that aims to aid\nlibrarians in assigning subject tags to the library records by producing a list\nof likely relevant tags for a given document. We frame the task as an\ninformation retrieval problem, where the document content is used to retrieve\nsubject tags from a large subject taxonomy. We leverage two types of encoder\nmodels to build a two-stage information retrieval system -- a bi-encoder for\ncoarse-grained candidate extraction at the first stage, and a cross-encoder for\nfine-grained re-ranking at the second stage. This approach proved effective,\ndemonstrating significant improvements in recall compared to single-stage\nmethods and showing competitive results according to qualitative evaluation."}
{"id": "2504.21472", "pdf": "https://arxiv.org/pdf/2504.21472", "abs": "https://arxiv.org/abs/2504.21472", "authors": ["Jingjing Liu", "Nian Wu", "Xianchao Xiu", "Jianhua Zhang"], "title": "Robust Orthogonal NMF with Label Propagation for Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Non-negative matrix factorization (NMF) is a popular unsupervised learning\napproach widely used in image clustering. However, in real-world clustering\nscenarios, most existing NMF methods are highly sensitive to noise corruption\nand are unable to effectively leverage limited supervised information. To\novercome these drawbacks, we propose a unified non-convex framework with label\npropagation called robust orthogonal nonnegative matrix factorization (RONMF).\nThis method not only considers the graph Laplacian and label propagation as\nregularization terms but also introduces a more effective non-convex structure\nto measure the reconstruction error and imposes orthogonal constraints on the\nbasis matrix to reduce the noise corruption, thereby achieving higher\nrobustness. To solve RONMF, we develop an alternating direction method of\nmultipliers (ADMM)-based optimization algorithm. In particular, all subproblems\nhave closed-form solutions, which ensures its efficiency. Experimental\nevaluations on eight public image datasets demonstrate that the proposed RONMF\noutperforms state-of-the-art NMF methods across various standard metrics and\nshows excellent robustness. The code will be available at\nhttps://github.com/slinda-liu."}
{"id": "2504.21553", "pdf": "https://arxiv.org/pdf/2504.21553", "abs": "https://arxiv.org/abs/2504.21553", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious natural language processing tasks. However, their size presents\nsignificant challenges for deployment and inference. This paper investigates\nthe quantization of LLMs, focusing on the LLaMA architecture and its\nderivatives. We challenge existing assumptions about activation outliers in\nLLMs and propose a novel mixed-precision quantization approach tailored for\nLLaMA-like models. Our method leverages the observation that activation spikes\nin LLaMA architectures are predominantly concentrated in specific projection\nlayers. By applying higher precision (FP16 or FP8) to these layers while\nquantizing the rest of the model to lower bit-widths, we achieve superior\nperformance compared to existing quantization techniques. Experimental results\non LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in\nperplexity and zero-shot accuracy, particularly for 8-bit per-tensor\nquantization. Our approach outperforms general-purpose methods designed to\nhandle outliers across all architecture types, highlighting the benefits of\narchitecture-specific quantization strategies. This research contributes to the\nongoing efforts to make LLMs more efficient and deployable, potentially\nenabling their use in resource-constrained environments. Our findings emphasize\nthe importance of considering model-specific characteristics in developing\neffective quantization pipelines for state-of-the-art language models by\nidentifying and targeting a small number of projections that concentrate\nactivation spikes."}
{"id": "2504.21476", "pdf": "https://arxiv.org/pdf/2504.21476", "abs": "https://arxiv.org/abs/2504.21476", "authors": ["Xinyu Li", "Qi Yao", "Yuanda Wang"], "title": "GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers", "categories": ["cs.CV", "cs.AI"], "comment": "The 34th International Joint Conference on Artificial Intelligence\n  (IJCAI 2025)", "summary": "Garment sewing patterns are fundamental design elements that bridge the gap\nbetween design concepts and practical manufacturing. The generative modeling of\nsewing patterns is crucial for creating diversified garments. However, existing\napproaches are limited either by reliance on a single input modality or by\nsuboptimal generation efficiency. In this work, we present\n\\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing\ncentimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text,\nimage, and incomplete sewing pattern). Our method efficiently encodes 3D sewing\npattern parameters into compact edge token representations, achieving a\nsequence length that is $\\textbf{10}\\times$ shorter than that of the\nautoregressive SewingGPT in DressCode. By employing a diffusion transformer, we\nsimultaneously denoise all edge tokens along the temporal axis, while\nmaintaining a constant number of denoising steps regardless of dataset-specific\nedge and panel statistics. With all combination of designs of our model, the\nsewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared\nto SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well\nas on the largest sewing pattern dataset, namely GarmentCodeData. The project\nwebsite is available at https://shenfu-research.github.io/Garment-Diffusion/."}
{"id": "2504.21589", "pdf": "https://arxiv.org/pdf/2504.21589", "abs": "https://arxiv.org/abs/2504.21589", "authors": ["Lisa Kluge", "Maximilian Kähler"], "title": "DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing", "categories": ["cs.CL", "cs.AI", "cs.DL", "I.2.7"], "comment": "11 pages, 4 figures, submitted to SemEval-2025 workshop Task 5:\n  LLMs4Subjects", "summary": "This paper presents our system developed for the SemEval-2025 Task 5:\nLLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical\nLibrary's Open-Access Catalog. Our system relies on prompting a selection of\nLLMs with varying examples of intellectually annotated records and asking the\nLLMs to similarly suggest keywords for new records. This few-shot prompting\ntechnique is combined with a series of post-processing steps that map the\ngenerated keywords to the target vocabulary, aggregate the resulting subject\nterms to an ensemble vote and, finally, rank them as to their relevance to the\nrecord. Our system is fourth in the quantitative ranking in the all-subjects\ntrack, but achieves the best result in the qualitative ranking conducted by\nsubject indexing experts."}
{"id": "2504.21478", "pdf": "https://arxiv.org/pdf/2504.21478", "abs": "https://arxiv.org/abs/2504.21478", "authors": ["Zherui Zhang", "Changwei Wang", "Rongtao Xu", "Wenhao Xu", "Shibiao Xu", "Yu Zhang", "Li Guo"], "title": "CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from\nthe given pre-trained teacher network to the target student model without\naccess to the real training data. Existing DFKD methods focus primarily on\nimproving image recognition performance on associated datasets, often\nneglecting the crucial aspect of the transferability of learned\nrepresentations. In this paper, we propose Category-Aware Embedding Data-Free\nKnowledge Distillation (CAE-DFKD), which addresses at the embedding level the\nlimitations of previous rely on image-level methods to improve model\ngeneralization but fail when directly applied to DFKD. The superiority and\nflexibility of CAE-DFKD are extensively evaluated, including:\n\\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering\nthe generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance\nwith existing DFKD state-of-the-art methods on image recognition tasks;\n\\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned\nrepresentations demonstrated in downstream tasks."}
{"id": "2504.21604", "pdf": "https://arxiv.org/pdf/2504.21604", "abs": "https://arxiv.org/abs/2504.21604", "authors": ["Bing Wang", "Ximing Li", "Changchun Li", "Bingrui Zhao", "Bo Fu", "Renchu Guan", "Shengsheng Wang"], "title": "Robust Misinformation Detection by Visiting Potential Commonsense Conflict", "categories": ["cs.CL", "cs.CY"], "comment": "11 pages, 2 figures. Accepted by IJCAI 2025. Code:\n  https://github.com/wangbing1416/MD-PCC", "summary": "The development of Internet technology has led to an increased prevalence of\nmisinformation, causing severe negative effects across diverse domains. To\nmitigate this challenge, Misinformation Detection (MD), aiming to detect online\nmisinformation automatically, emerges as a rapidly growing research topic in\nthe community. In this paper, we propose a novel plug-and-play augmentation\nmethod for the MD task, namely Misinformation Detection with Potential\nCommonsense Conflict (MD-PCC). We take inspiration from the prior studies\nindicating that fake articles are more likely to involve commonsense conflict.\nAccordingly, we construct commonsense expressions for articles, serving to\nexpress potential commonsense conflicts inferred by the difference between\nextracted commonsense triplet and golden ones inferred by the well-established\ncommonsense reasoning tool COMET. These expressions are then specified for each\narticle as augmentation. Any specific MD methods can be then trained on those\ncommonsense-augmented articles. Besides, we also collect a novel\ncommonsense-oriented dataset named CoMis, whose all fake articles are caused by\ncommonsense conflict. We integrate MD-PCC with various existing MD backbones\nand compare them across both 4 public benchmark datasets and CoMis. Empirical\nresults demonstrate that MD-PCC can consistently outperform the existing MD\nbaselines."}
{"id": "2504.21487", "pdf": "https://arxiv.org/pdf/2504.21487", "abs": "https://arxiv.org/abs/2504.21487", "authors": ["Hebaixu Wang", "Jing Zhang", "Haonan Guo", "Di Wang", "Jiayi Ma", "Bo Du"], "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in universal image\nrestoration. While existing methods speed up inference by reducing sampling\nsteps, substantial step intervals often introduce cumulative errors. Moreover,\nthey struggle to balance the commonality of degradation representations and\nrestoration quality. To address these challenges, we introduce\n\\textbf{DGSolver}, a diffusion generalist solver with universal posterior\nsampling. We first derive the exact ordinary differential equations for\ngeneralist diffusion models and tailor high-order solvers with a queue-based\naccelerated sampling strategy to improve both accuracy and efficiency. We then\nintegrate universal posterior sampling to better approximate\nmanifold-constrained gradients, yielding a more accurate noise estimation and\ncorrecting errors in inverse inference. Extensive experiments show that\nDGSolver outperforms state-of-the-art methods in restoration accuracy,\nstability, and scalability, both qualitatively and quantitatively. Code and\nmodels will be available at https://github.com/MiliLab/DGSolver."}
{"id": "2504.21605", "pdf": "https://arxiv.org/pdf/2504.21605", "abs": "https://arxiv.org/abs/2504.21605", "authors": ["Jonas Gwozdz", "Andreas Both"], "title": "RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet\nsystematically assessing their reliability with conflicting information remains\ndifficult. We propose an RDF-based framework to assess multilingual LLM\nquality, focusing on knowledge conflicts. Our approach captures model responses\nacross four distinct context conditions (complete, incomplete, conflicting, and\nno-context information) in German and English. This structured representation\nenables the comprehensive analysis of knowledge leakage-where models favor\ntraining data over provided context-error detection, and multilingual\nconsistency. We demonstrate the framework through a fire safety domain\nexperiment, revealing critical patterns in context prioritization and\nlanguage-specific performance, and demonstrating that our vocabulary was\nsufficient to express every assessment facet encountered in the 28-question\nstudy."}
{"id": "2504.21491", "pdf": "https://arxiv.org/pdf/2504.21491", "abs": "https://arxiv.org/abs/2504.21491", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "We propose a result-level category-specific fusion architecture called\nClassWise-CRF. This architecture employs a two-stage process: first, it selects\nexpert networks that perform well in specific categories from a pool of\ncandidate networks using a greedy algorithm; second, it integrates the\nsegmentation predictions of these selected networks by adaptively weighting\ntheir contributions based on their segmentation performance in each category.\nInspired by Conditional Random Field (CRF), the ClassWise-CRF architecture\ntreats the segmentation predictions from multiple networks as confidence vector\nfields. It leverages segmentation metrics (such as Intersection over Union)\nfrom the validation set as priors and employs an exponential weighting strategy\nto fuse the category-specific confidence scores predicted by each network. This\nfusion method dynamically adjusts the weights of each network for different\ncategories, achieving category-specific optimization. Building on this, the\narchitecture further optimizes the fused results using unary and pairwise\npotentials in CRF to ensure spatial consistency and boundary accuracy. To\nvalidate the effectiveness of ClassWise-CRF, we conducted experiments on two\nremote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced\nsemantic segmentation networks. The results show that the ClassWise-CRF\narchitecture significantly improves segmentation performance: on the LoveDA\ndataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on\nthe validation set and by 0.68% on the test set; on the Vaihingen dataset, the\nmIoU improved by 0.87% on the validation set and by 0.91% on the test set.\nThese results fully demonstrate the effectiveness and generality of the\nClassWise-CRF architecture in semantic segmentation of remote sensing images.\nThe full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF."}
{"id": "2504.21625", "pdf": "https://arxiv.org/pdf/2504.21625", "abs": "https://arxiv.org/abs/2504.21625", "authors": ["Jiaming Wang"], "title": "Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability", "categories": ["cs.CL"], "comment": null, "summary": "The ability to follow instructions accurately is fundamental for Large\nLanguage Models (LLMs) to serve as reliable agents in real-world applications.\nWhile existing instruction-following benchmarks are either single-turn or\nintroduce new requirements in each turn without allowing self-correction,\nMeeseeks simulates realistic human-LLM interactions through an iterative\nfeedback process. This design enables models to self-correct based on specific\nrequirement failures, better reflecting real-world user-end usage patterns. The\nbenchmark implements a comprehensive evaluation system with 38 capability tags\norganized across three dimensions: Intent Recognition, Granular Content\nValidation, and Output Structure Validation. Through rigorous evaluation across\nLLMs, Meeseeks provides valuable insights into LLMs' instruction-following\ncapabilities in practical applications."}
{"id": "2504.21495", "pdf": "https://arxiv.org/pdf/2504.21495", "abs": "https://arxiv.org/abs/2504.21495", "authors": ["Junxi Wang", "Jize liu", "Na Zhang", "Yaxiong Wang"], "title": "Consistency-aware Fake Videos Detection on Short Video Platforms", "categories": ["cs.CV", "cs.MM"], "comment": "2025 icic", "summary": "This paper focuses to detect the fake news on the short video platforms.\nWhile significant research efforts have been devoted to this task with notable\nprogress in recent years, current detection accuracy remains suboptimal due to\nthe rapid evolution of content manipulation and generation technologies.\nExisting approaches typically employ a cross-modal fusion strategy that\ndirectly combines raw video data with metadata inputs before applying a\nclassification layer. However, our empirical observations reveal a critical\noversight: manipulated content frequently exhibits inter-modal inconsistencies\nthat could serve as valuable discriminative features, yet remain underutilized\nin contemporary detection frameworks. Motivated by this insight, we propose a\nnovel detection paradigm that explicitly identifies and leverages cross-modal\ncontradictions as discriminative cues. Our approach consists of two core\nmodules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative\nDiagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal\nConsistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used\nto generate pseudo-labels for evaluating cross-modal semantic consistency.\nThen, CMCD extracts [CLS] tokens and computes cosine loss to quantify\ncross-modal inconsistencies. MMCD further integrates multimodal features\nthrough Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF).\nMFF employs a co-attention mechanism to enhance semantic interactions across\ndifferent modalities, while a Transformer is utilized for comprehensive feature\nfusion. Meanwhile, PSF further integrates the fake news probability scores\nobtained in the previous step. Extensive experiments on established benchmarks\n(FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in\nFake videos detection."}
{"id": "2504.21635", "pdf": "https://arxiv.org/pdf/2504.21635", "abs": "https://arxiv.org/abs/2504.21635", "authors": ["Zeina Aldallal", "Sara Chrouf", "Khalil Hennara", "Mohamed Motaism Hamed", "Muhammad Hreden", "Safwan AlModhayan"], "title": "Sadeed: Advancing Arabic Diacritization Through Small Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic text diacritization remains a persistent challenge in natural language\nprocessing due to the language's morphological richness. In this paper, we\nintroduce Sadeed, a novel approach based on a fine-tuned decoder-only language\nmodel adapted from Kuwain 1.5B Hennara et al. [2025], a compact model\noriginally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully\ncurated, high-quality diacritized datasets, constructed through a rigorous\ndata-cleaning and normalization pipeline. Despite utilizing modest\ncomputational resources, Sadeed achieves competitive results compared to\nproprietary large language models and outperforms traditional models trained on\nsimilar domains. Additionally, we highlight key limitations in current\nbenchmarking practices for Arabic diacritization. To address these issues, we\nintroduce SadeedDiac-25, a new benchmark designed to enable fairer and more\ncomprehensive evaluation across diverse text genres and complexity levels.\nTogether, Sadeed and SadeedDiac-25 provide a robust foundation for advancing\nArabic NLP applications, including machine translation, text-to-speech, and\nlanguage learning tools."}
{"id": "2504.21497", "pdf": "https://arxiv.org/pdf/2504.21497", "abs": "https://arxiv.org/abs/2504.21497", "authors": ["Mengting Wei", "Yante Li", "Tuomas Varanka", "Yan Jiang", "Licai Sun", "Guoying Zhao"], "title": "MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a method for video face reenactment that integrates\na 3D face parametric model into a latent diffusion framework, aiming to improve\nshape consistency and motion control in existing video-based face generation\napproaches. Our approach employs the FLAME (Faces Learned with an Articulated\nModel and Expressions) model as the 3D face parametric representation,\nproviding a unified framework for modeling face expressions and head pose. This\nenables precise extraction of detailed face geometry and motion features from\ndriving videos. Specifically, we enhance the latent diffusion model with rich\n3D expression and detailed pose information by incorporating depth maps, normal\nmaps, and rendering maps derived from FLAME sequences. A multi-layer face\nmovements fusion module with integrated self-attention mechanisms is used to\ncombine identity and motion latent features within the spatial domain. By\nutilizing the 3D face parametric model as motion guidance, our method enables\nparametric alignment of face identity between the reference image and the\nmotion captured from the driving video. Experimental results on benchmark\ndatasets show that our method excels at generating high-quality face animations\nwith precise expression and head pose variation modeling. In addition, it\ndemonstrates strong generalization performance on out-of-domain images. Code is\npublicly available at https://github.com/weimengting/MagicPortrait."}
{"id": "2504.21677", "pdf": "https://arxiv.org/pdf/2504.21677", "abs": "https://arxiv.org/abs/2504.21677", "authors": ["Michelle Wastl", "Jannis Vamvas", "Selena Calleri", "Rico Sennrich"], "title": "20min-XD: A Comparable Corpus of Swiss News Articles", "categories": ["cs.CL"], "comment": "10 pages; accepted at SwissText 2025", "summary": "We present 20min-XD (20 Minuten cross-lingual document-level), a\nFrench-German, document-level comparable corpus of news articles, sourced from\nthe Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises\naround 15,000 article pairs spanning 2015 to 2024, automatically aligned based\non semantic similarity. We detail the data collection process and alignment\nmethodology. Furthermore, we provide a qualitative and quantitative analysis of\nthe corpus. The resulting dataset exhibits a broad spectrum of cross-lingual\nsimilarity, ranging from near-translations to loosely related articles, making\nit valuable for various NLP applications and broad linguistically motivated\nstudies. We publicly release the dataset in document- and sentence-aligned\nversions and code for the described experiments."}
{"id": "2504.21544", "pdf": "https://arxiv.org/pdf/2504.21544", "abs": "https://arxiv.org/abs/2504.21544", "authors": ["Uzair Shah", "Marco Agus", "Daniya Boges", "Vanessa Chiappini", "Mahmood Alzubaidi", "Jens Schneider", "Markus Hadwiger", "Pierre J. Magistretti", "Mowafa Househ", "Corrado Calı"], "title": "SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks", "categories": ["cs.CV"], "comment": "Accepted at (CVPRW) 10th IEEE Workshop on Computer Vision for\n  Microscopy Image Analysis (CVMI)", "summary": "We present SAM4EM, a novel approach for 3D segmentation of complex neural\nstructures in electron microscopy (EM) data by leveraging the Segment Anything\nModel (SAM) alongside advanced fine-tuning strategies. Our contributions\ninclude the development of a prompt-free adapter for SAM using two stage mask\ndecoding to automatically generate prompt embeddings, a dual-stage fine-tuning\nmethod based on Low-Rank Adaptation (LoRA) for enhancing segmentation with\nlimited annotated data, and a 3D memory attention mechanism to ensure\nsegmentation consistency across 3D stacks. We further release a unique\nbenchmark dataset for the segmentation of astrocytic processes and synapses. We\nevaluated our method on challenging neuroscience segmentation benchmarks,\nspecifically targeting mitochondria, glia, and synapses, with significant\naccuracy improvements over state-of-the-art (SOTA) methods, including recent\nSAM-based adapters developed for the medical domain and other vision\ntransformer-based approaches. Experimental results indicate that our approach\noutperforms existing solutions in the segmentation of complex processes like\nglia and post-synaptic densities. Our code and models are available at\nhttps://github.com/Uzshah/SAM4EM."}
{"id": "2504.21681", "pdf": "https://arxiv.org/pdf/2504.21681", "abs": "https://arxiv.org/abs/2504.21681", "authors": ["Andrei-Alexandru Manea", "Jindřich Libovický"], "title": "Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders", "categories": ["cs.CL"], "comment": null, "summary": "Most pre-trained Vision-Language (VL) models and training data for the\ndownstream tasks are only available in English. Therefore, multilingual VL\ntasks are solved using cross-lingual transfer: fine-tune a multilingual\npre-trained model or transfer the text encoder using parallel data. We study\nthe alternative approach: transferring an already trained encoder using\nparallel data. We investigate the effect of parallel data: domain and the\nnumber of languages, which were out of focus in previous work. Our results show\nthat even machine-translated task data are the best on average, caption-like\nauthentic parallel data outperformed it in some languages. Further, we show\nthat most languages benefit from multilingual training."}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559", "abs": "https://arxiv.org/abs/2504.21559", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination."}
{"id": "2504.21685", "pdf": "https://arxiv.org/pdf/2504.21685", "abs": "https://arxiv.org/abs/2504.21685", "authors": ["Reem Abdel-Salam", "Mary Adewunmi"], "title": "Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Health Mention Classification (HMC) plays a critical role in leveraging\nsocial media posts for real-time tracking and public health monitoring.\nNevertheless, the process of HMC presents significant challenges due to its\nintricate nature, primarily stemming from the contextual aspects of health\nmentions, such as figurative language and descriptive terminology, rather than\nexplicitly reflecting a personal ailment. To address this problem, we argue\nthat clearer mentions can be achieved through conventional fine-tuning with\nenhanced parameters of biomedical natural language methods (NLP). In this\nstudy, we explore different techniques such as the utilisation of\npart-of-speech (POS) tagger information, improving on PEFT techniques, and\ndifferent combinations thereof. Extensive experiments are conducted on three\nwidely used datasets: RHDM, PHM, and Illness. The results incorporated POS\ntagger information, and leveraging PEFT techniques significantly improves\nperformance in terms of F1-score compared to state-of-the-art methods across\nall three datasets by utilising smaller models and efficient training.\nFurthermore, the findings highlight the effectiveness of incorporating POS\ntagger information and leveraging PEFT techniques for HMC. In conclusion, the\nproposed methodology presents a potentially effective approach to accurately\nclassifying health mentions in social media posts while optimising the model\nsize and training efficiency."}
{"id": "2504.21561", "pdf": "https://arxiv.org/pdf/2504.21561", "abs": "https://arxiv.org/abs/2504.21561", "authors": ["Pengxiang Li", "Zhi Gao", "Bofei Zhang", "Yapeng Mi", "Xiaojian Ma", "Chenrui Shi", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Iterative Trajectory Exploration for Multimodal Agents", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io."}
{"id": "2504.21742", "pdf": "https://arxiv.org/pdf/2504.21742", "abs": "https://arxiv.org/abs/2504.21742", "authors": ["Emelie Hallenberg"], "title": "Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The Greek fictional narratives often termed love novels or romances, ranging\nfrom the first century CE to the middle of the 15th century, have long been\nconsidered as similar in many ways, not least in the use of particular literary\nmotifs. By applying the use of fine-tuned large language models, this study\naims to investigate which motifs exactly that the texts in this corpus have in\ncommon, and in which ways they differ from each other. The results show that\nwhile some motifs persist throughout the corpus, others fluctuate in frequency,\nindicating certain trends or external influences. Conclusively, the method\nproves to adequately extract literary motifs according to a set definition,\nproviding data for both quantitative and qualitative analyses."}
{"id": "2504.21562", "pdf": "https://arxiv.org/pdf/2504.21562", "abs": "https://arxiv.org/abs/2504.21562", "authors": ["Henry John Krumb", "Anirban Mukhopadhyay"], "title": "eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Wireless Capsule Endoscopy is a non-invasive imaging method for the entire\ngastrointestinal tract, and is a pain-free alternative to traditional\nendoscopy. It generates extensive video data that requires significant review\ntime, and localizing the capsule after ingestion is a challenge. Techniques\nlike bleeding detection and depth estimation can help with localization of\npathologies, but deep learning models are typically too large to run directly\non the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and\ndepth estimation are trained on capsule endoscopic images. For monocular depth\nestimation, we distill a large foundation model into the lean NCA architecture,\nby treating the outputs of the foundation model as pseudo ground truth. We then\nport the trained NCA to the ESP32 microcontroller, enabling efficient image\nprocessing on hardware as small as a camera capsule. NCA are more accurate\n(Dice) than other portable segmentation models, while requiring more than 100x\nfewer parameters stored in memory than other small-scale models. The visual\nresults of NCA depth estimation look convincing, and in some cases beat the\nrealism and detail of the pseudo ground truth. Runtime optimizations on the\nESP32-S3 accelerate the average inference speed significantly, by more than\nfactor 3. With several algorithmic adjustments and distillation, it is possible\nto eNCApsulate NCA models into microcontrollers that fit into wireless capsule\nendoscopes. This is the first work that enables reliable bleeding segmentation\nand depth estimation on a miniaturized device, paving the way for precise\ndiagnosis combined with visual odometry as a means of precise localization of\nthe capsule -- on the capsule."}
{"id": "2504.21747", "pdf": "https://arxiv.org/pdf/2504.21747", "abs": "https://arxiv.org/abs/2504.21747", "authors": ["Maxime Bouthors", "Josep Crego", "François Yvon"], "title": "Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data", "categories": ["cs.CL", "I.2.7"], "comment": "13 pages", "summary": "Conventional retrieval-augmented neural machine translation (RANMT) systems\nleverage bilingual corpora, e.g., translation memories (TMs). Yet, in many\nsettings, in-domain monolingual target-side corpora are often available. This\nwork explores ways to take advantage of such resources by retrieving relevant\nsegments directly in the target language, based on a source-side query. For\nthis, we design improved cross-lingual retrieval systems, trained with both\nsentence level and word-level matching objectives. In our experiments with two\nRANMT architectures, we first demonstrate the benefits of such cross-lingual\nobjectives in a controlled setting, obtaining translation performances that\nsurpass standard TM-based models. We then showcase our method on a real-world\nset-up, where the target monolingual resources far exceed the amount of\nparallel data and observe large improvements of our new techniques, which\noutperform both the baseline setting, and general-purpose cross-lingual\nretrievers."}
{"id": "2504.21598", "pdf": "https://arxiv.org/pdf/2504.21598", "abs": "https://arxiv.org/abs/2504.21598", "authors": ["Thomas L. Athey", "Shashata Sawmya", "Nir Shavit"], "title": "Cascade Detector Analysis and Application to Biomedical Microscopy", "categories": ["cs.CV"], "comment": null, "summary": "As both computer vision models and biomedical datasets grow in size, there is\nan increasing need for efficient inference algorithms. We utilize cascade\ndetectors to efficiently identify sparse objects in multiresolution images.\nGiven an object's prevalence and a set of detectors at different resolutions\nwith known accuracies, we derive the accuracy, and expected number of\nclassifier calls by a cascade detector. These results generalize across number\nof dimensions and number of cascade levels. Finally, we compare one- and\ntwo-level detectors in fluorescent cell detection, organelle segmentation, and\ntissue segmentation across various microscopy modalities. We show that the\nmulti-level detector achieves comparable performance in 30-75% less time. Our\nwork is compatible with a variety of computer vision models and data domains."}
{"id": "2504.21773", "pdf": "https://arxiv.org/pdf/2504.21773", "abs": "https://arxiv.org/abs/2504.21773", "authors": ["Junsheng Huang", "Zhitao He", "Sandeep Polisetty", "Qingyun Wang", "May Fung"], "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the widespread application of large language models (LLMs), the issue of\ngenerating non-existing facts, known as hallucination, has garnered increasing\nattention. Previous research in enhancing LLM confidence estimation mainly\nfocuses on the single problem setting. However, LLM awareness of its internal\nparameterized knowledge boundary under the more challenging multi-problem\nsetting, which requires answering multiple problems accurately simultaneously,\nremains underexplored. To bridge this gap, we introduce a novel method,\nMultiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates\nthe learning of answer prediction and confidence estimation during fine-tuning\non instruction data. Extensive experiments demonstrate that our method\noutperforms baselines by up to 25% in average precision."}
{"id": "2504.21614", "pdf": "https://arxiv.org/pdf/2504.21614", "abs": "https://arxiv.org/abs/2504.21614", "authors": ["Daniel Bogdoll", "Rajanikant Patnaik Ananta", "Abeyankar Giridharan", "Isabel Moore", "Gregory Stevens", "Henry X. Liu"], "title": "Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection", "categories": ["cs.CV"], "comment": null, "summary": "With an ever-increasing availability of data, it has become more and more\nchallenging to select and label appropriate samples for the training of machine\nlearning models. It is especially difficult to detect long-tail classes of\ninterest in large amounts of unlabeled data. This holds especially true for\nIntelligent Transportation Systems (ITS), where vehicle fleets and roadside\nperception systems generate an abundance of raw data. While industrial,\nproprietary data engines for such iterative data selection and model training\nprocesses exist, researchers and the open-source community suffer from a lack\nof an openly available system. We present the Mcity Data Engine, which provides\nmodules for the complete data-based development cycle, beginning at the data\nacquisition phase and ending at the model deployment stage. The Mcity Data\nEngine focuses on rare and novel classes through an open-vocabulary data\nselection process. All code is publicly available on GitHub under an MIT\nlicense: https://github.com/mcity/mcity_data_engine"}
{"id": "2504.21776", "pdf": "https://arxiv.org/pdf/2504.21776", "abs": "https://arxiv.org/abs/2504.21776", "authors": ["Xiaoxi Li", "Jiajie Jin", "Guanting Dong", "Hongjin Qian", "Yutao Zhu", "Yongkang Wu", "Ji-Rong Wen", "Zhicheng Dou"], "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate\nimpressive long-horizon reasoning capabilities. However, their reliance on\nstatic internal knowledge limits their performance on complex,\nknowledge-intensive tasks and hinders their ability to produce comprehensive\nresearch reports requiring synthesis of diverse web information. To address\nthis, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs\nto autonomously search the web, navigate web pages, and draft research reports\nduring the reasoning process. WebThinker integrates a \\textbf{Deep Web\nExplorer} module, enabling LRMs to dynamically search, navigate, and extract\ninformation from the web when encountering knowledge gaps. It also employs an\n\\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to\nseamlessly interleave reasoning, information gathering, and report writing in\nreal time. To further enhance research tool utilization, we introduce an\n\\textbf{RL-based training strategy} via iterative online Direct Preference\nOptimization (DPO). Extensive experiments on complex reasoning benchmarks\n(GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive)\ndemonstrate that WebThinker significantly outperforms existing methods and\nstrong proprietary systems. Our approach enhances LRM reliability and\napplicability in complex scenarios, paving the way for more capable and\nversatile deep research systems. The code is available at\nhttps://github.com/RUC-NLPIR/WebThinker."}
{"id": "2504.21646", "pdf": "https://arxiv.org/pdf/2504.21646", "abs": "https://arxiv.org/abs/2504.21646", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection", "categories": ["cs.CV"], "comment": null, "summary": "The success of face recognition (FR) systems has led to serious privacy\nconcerns due to potential unauthorized surveillance and user tracking on social\nnetworks. Existing methods for enhancing privacy fail to generate natural face\nimages that can protect facial privacy. In this paper, we propose\ndiffusion-based adversarial identity manipulation (DiffAIM) to generate natural\nand highly transferable adversarial faces against malicious FR systems. To be\nspecific, we manipulate facial identity within the low-dimensional latent space\nof a diffusion model. This involves iteratively injecting gradient-based\nadversarial identity guidance during the reverse diffusion process,\nprogressively steering the generation toward the desired adversarial faces. The\nguidance is optimized for identity convergence towards a target while promoting\nsemantic divergence from the source, facilitating effective impersonation while\nmaintaining visual naturalness. We further incorporate structure-preserving\nregularization to preserve facial structure consistency during manipulation.\nExtensive experiments on both face verification and identification tasks\ndemonstrate that compared with the state-of-the-art, DiffAIM achieves stronger\nblack-box attack transferability while maintaining superior visual quality. We\nalso demonstrate the effectiveness of the proposed approach for commercial FR\nAPIs, including Face++ and Aliyun."}
{"id": "2504.21800", "pdf": "https://arxiv.org/pdf/2504.21800", "abs": "https://arxiv.org/abs/2504.21800", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "68T50", "I.2.7; H.3.1"], "comment": "11 pages, 5 tables, updated abstract and tables", "summary": "The growing adoption of synthetic data in healthcare is driven by privacy\nconcerns, limited access to real-world data, and the high cost of annotation.\nThis work explores the use of synthetic Prolonged Exposure (PE) therapeutic\nconversations for Post-Traumatic Stress Disorder (PTSD) as a scalable\nalternative for training and evaluating clinical models. We systematically\ncompare real and synthetic dialogues using linguistic, structural, and\nprotocol-specific metrics, including turn-taking patterns and treatment\nfidelity. We also introduce and evaluate PE-specific metrics derived from\nlinguistic analysis and semantic modeling, offering a novel framework for\nassessing clinical fidelity beyond surface fluency. Our findings show that\nalthough synthetic data holds promise for mitigating data scarcity and\nprotecting patient privacy, it can struggle to capture the subtle dynamics of\ntherapeutic interactions. In our dataset, synthetic dialogues match structural\nfeatures of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99),\nhowever, synthetic interactions do not adequately reflect key fidelity markers\n(e.g., distress monitoring). We highlight gaps in existing evaluation\nframeworks and advocate for fidelity-aware metrics that go beyond surface\nfluency to uncover clinically significant failures. Our findings clarify where\nsynthetic data can effectively complement real-world datasets -- and where\ncritical limitations remain."}
{"id": "2504.21650", "pdf": "https://arxiv.org/pdf/2504.21650", "abs": "https://arxiv.org/abs/2504.21650", "authors": ["Haiyang Zhou", "Wangbo Yu", "Jiawen Guan", "Xinhua Cheng", "Yonghong Tian", "Li Yuan"], "title": "HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation", "categories": ["cs.CV"], "comment": "Project homepage: https://zhouhyocean.github.io/holotime/", "summary": "The rapid advancement of diffusion models holds the promise of\nrevolutionizing the application of VR and AR technologies, which typically\nrequire scene-level 4D assets for user experience. Nonetheless, existing\ndiffusion models predominantly concentrate on modeling static 3D scenes or\nobject-level dynamics, constraining their capacity to provide truly immersive\nexperiences. To address this issue, we propose HoloTime, a framework that\nintegrates video diffusion models to generate panoramic videos from a single\nprompt or reference image, along with a 360-degree 4D scene reconstruction\nmethod that seamlessly transforms the generated panoramic video into 4D assets,\nenabling a fully immersive 4D experience for users. Specifically, to tame video\ndiffusion models for generating high-fidelity panoramic videos, we introduce\nthe 360World dataset, the first comprehensive collection of panoramic videos\nsuitable for downstream 4D scene reconstruction tasks. With this curated\ndataset, we propose Panoramic Animator, a two-stage image-to-video diffusion\nmodel that can convert panoramic images into high-quality panoramic videos.\nFollowing this, we present Panoramic Space-Time Reconstruction, which leverages\na space-time depth estimation method to transform the generated panoramic\nvideos into 4D point clouds, enabling the optimization of a holistic 4D\nGaussian Splatting representation to reconstruct spatially and temporally\nconsistent 4D scenes. To validate the efficacy of our method, we conducted a\ncomparative analysis with existing approaches, revealing its superiority in\nboth panoramic video generation and 4D scene reconstruction. This demonstrates\nour method's capability to create more engaging and realistic immersive\nenvironments, thereby enhancing user experiences in VR and AR applications."}
{"id": "2504.21801", "pdf": "https://arxiv.org/pdf/2504.21801", "abs": "https://arxiv.org/abs/2504.21801", "authors": ["Z. Z. Ren", "Zhihong Shao", "Junxiao Song", "Huajian Xin", "Haocheng Wang", "Wanjia Zhao", "Liyue Zhang", "Zhe Fu", "Qihao Zhu", "Dejian Yang", "Z. F. Wu", "Zhibin Gou", "Shirong Ma", "Hongxuan Tang", "Yuxuan Liu", "Wenjun Gao", "Daya Guo", "Chong Ruan"], "title": "DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce DeepSeek-Prover-V2, an open-source large language model designed\nfor formal theorem proving in Lean 4, with initialization data collected\nthrough a recursive theorem proving pipeline powered by DeepSeek-V3. The\ncold-start training procedure begins by prompting DeepSeek-V3 to decompose\ncomplex problems into a series of subgoals. The proofs of resolved subgoals are\nsynthesized into a chain-of-thought process, combined with DeepSeek-V3's\nstep-by-step reasoning, to create an initial cold start for reinforcement\nlearning. This process enables us to integrate both informal and formal\nmathematical reasoning into a unified model. The resulting model,\nDeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural\ntheorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49\nout of 658 problems from PutnamBench. In addition to standard benchmarks, we\nintroduce ProverBench, a collection of 325 formalized problems, to enrich our\nevaluation, including 15 selected problems from the recent AIME competitions\n(years 24-25). Further evaluation on these 15 AIME problems shows that the\nmodel successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of\nthese problems using majority voting, highlighting that the gap between formal\nand informal mathematical reasoning in large language models is substantially\nnarrowing."}
{"id": "2504.21682", "pdf": "https://arxiv.org/pdf/2504.21682", "abs": "https://arxiv.org/abs/2504.21682", "authors": ["Yan Shu", "Weichao Zeng", "Fangmin Zhao", "Zeyu Chen", "Zhenhang Li", "Xiaomeng Yang", "Yu Zhou", "Paolo Rota", "Xiang Bai", "Lianwen Jin", "Xu-Cheng Yin", "Nicu Sebe"], "title": "Visual Text Processing: A Comprehensive Review and Unified Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Visual text is a crucial component in both document and scene images,\nconveying rich semantic information and attracting significant attention in the\ncomputer vision community. Beyond traditional tasks such as text detection and\nrecognition, visual text processing has witnessed rapid advancements driven by\nthe emergence of foundation models, including text image reconstruction and\ntext image manipulation. Despite significant progress, challenges remain due to\nthe unique properties that differentiate text from general objects. Effectively\ncapturing and leveraging these distinct textual characteristics is essential\nfor developing robust visual text processing models. In this survey, we present\na comprehensive, multi-perspective analysis of recent advancements in visual\ntext processing, focusing on two key questions: (1) What textual features are\nmost suitable for different visual text processing tasks? (2) How can these\ndistinctive text features be effectively incorporated into processing\nframeworks? Furthermore, we introduce VTPBench, a new benchmark that\nencompasses a broad range of visual text processing datasets. Leveraging the\nadvanced visual quality assessment capabilities of multimodal large language\nmodels (MLLMs), we propose VTPScore, a novel evaluation metric designed to\nensure fair and reliable evaluation. Our empirical study with more than 20\nspecific models reveals substantial room for improvement in the current\ntechniques. Our aim is to establish this work as a fundamental resource that\nfosters future exploration and innovation in the dynamic field of visual text\nprocessing. The relevant repository is available at\nhttps://github.com/shuyansy/Visual-Text-Processing-survey."}
{"id": "2504.21851", "pdf": "https://arxiv.org/pdf/2504.21851", "abs": "https://arxiv.org/abs/2504.21851", "authors": ["Sichang Tu", "Abigail Powers", "Stephen Doogan", "Jinho D. Choi"], "title": "TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "5 figures, 4 tables", "summary": "Objectives: While Large Language Models (LLMs) have been widely used to\nassist clinicians and support patients, no existing work has explored dialogue\nsystems for standard diagnostic interviews and assessments. This study aims to\nbridge the gap in mental healthcare accessibility by developing an LLM-powered\ndialogue system that replicates clinician behavior. Materials and Methods: We\nintroduce TRUST, a framework of cooperative LLM modules capable of conducting\nformal diagnostic interviews and assessments for Post-Traumatic Stress Disorder\n(PTSD). To guide the generation of appropriate clinical responses, we propose a\nDialogue Acts schema specifically designed for clinical interviews.\nAdditionally, we develop a patient simulation approach based on real-life\ninterview transcripts to replace time-consuming and costly manual testing by\nclinicians. Results: A comprehensive set of evaluation metrics is designed to\nassess the dialogue system from both the agent and patient simulation\nperspectives. Expert evaluations by conversation and clinical specialists show\nthat TRUST performs comparably to real-life clinical interviews. Discussion:\nOur system performs at the level of average clinicians, with room for future\nenhancements in communication styles and response appropriateness. Conclusions:\nOur TRUST framework shows its potential to facilitate mental healthcare\navailability."}
{"id": "2504.21692", "pdf": "https://arxiv.org/pdf/2504.21692", "abs": "https://arxiv.org/abs/2504.21692", "authors": ["Zihan Zhou", "Changrui Dai", "Aibo Song", "Xiaolin Fang"], "title": "Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Successful video analysis relies on accurate recognition of pixels across\nframes, and frame reconstruction methods based on video correspondence learning\nare popular due to their efficiency. Existing frame reconstruction methods,\nwhile efficient, neglect the value of direct involvement of multiple reference\nframes for reconstruction and decision-making aspects, especially in complex\nsituations such as occlusion or fast movement. In this paper, we introduce a\nDynamic Memory Prediction (DMP) framework that innovatively utilizes multiple\nreference frames to concisely and directly enhance frame reconstruction. Its\ncore component is a Reference Frame Memory Engine that dynamically selects\nframes based on object pixel features to improve tracking accuracy. In\naddition, a Bidirectional Target Prediction Network is built to utilize\nmultiple reference frames to improve the robustness of the model. Through\nexperiments, our algorithm outperforms the state-of-the-art self-supervised\ntechniques on two fine-grained video object tracking tasks: object segmentation\nand keypoint tracking."}
{"id": "2504.21015", "pdf": "https://arxiv.org/pdf/2504.21015", "abs": "https://arxiv.org/abs/2504.21015", "authors": ["Aarush Sinha"], "title": "Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Training effective dense retrieval models often relies on hard negative (HN)\nexamples mined from the document corpus via methods like BM25 or cross-encoders\n(CE), processes that can be computationally demanding and require full corpus\naccess. This paper introduces a different approach, an end-to-end pipeline\nwhere a Large Language Model (LLM) first generates a query from a passage, and\nthen generates a hard negative example using \\emph{only} that query text. This\ncorpus-free negative generation contrasts with standard mining techniques. We\nevaluated this \\textsc{LLM Query $\\rightarrow$ LLM HN} approach against\ntraditional \\textsc{LLM Query $\\rightarrow$ BM25 HN} and \\textsc{LLM Query\n$\\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several\nBEIR benchmark datasets. Our results show the proposed all-LLM pipeline\nachieves performance identical to both the BM25 and the computationally\nintensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics.\nThis demonstrates that our corpus-free negative generation method matches the\neffectiveness of complex, corpus-dependent mining techniques, offering a\npotentially simpler and more efficient pathway for training high-performance\nretrievers without sacrificing results. We make the dataset including the\nqueries and the hard-negatives for all three methods publicly available\nhttps://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449."}
{"id": "2504.21699", "pdf": "https://arxiv.org/pdf/2504.21699", "abs": "https://arxiv.org/abs/2504.21699", "authors": ["Abu Mohammed Raisuddin", "Jesper Holmblad", "Hamed Haghighi", "Yuri Poledna", "Maikol Funk Drechsler", "Valentina Donzella", "Eren Erdal Aksoy"], "title": "REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Sensor degradation poses a significant challenge in autonomous driving.\nDuring heavy rainfall, the interference from raindrops can adversely affect the\nquality of LiDAR point clouds, resulting in, for instance, inaccurate point\nmeasurements. This, in turn, can potentially lead to safety concerns if\nautonomous driving systems are not weather-aware, i.e., if they are unable to\ndiscern such changes. In this study, we release a new, large-scale, multi-modal\nemulated rain dataset, REHEARSE-3D, to promote research advancements in 3D\npoint cloud de-raining. Distinct from the most relevant competitors, our\ndataset is unique in several respects. First, it is the largest point-wise\nannotated dataset, and second, it is the only one with high-resolution LiDAR\ndata (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and\nnighttime conditions in a controlled weather environment. Furthermore,\nREHEARSE-3D involves rain-characteristic information, which is of significant\nvalue not only for sensor noise modeling but also for analyzing the impact of\nweather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop\ndetection and removal in fused LiDAR and 4D Radar point clouds. Our\ncomprehensive study further evaluates the performance of various statistical\nand deep-learning models. Upon publication, the dataset and benchmark models\nwill be made publicly available at: https://sporsho.github.io/REHEARSE3D."}
{"id": "2504.21035", "pdf": "https://arxiv.org/pdf/2504.21035", "abs": "https://arxiv.org/abs/2504.21035", "authors": ["Rui Xin", "Niloofar Mireshghallah", "Shuyue Stella Li", "Michael Duan", "Hyunwoo Kim", "Yejin Choi", "Yulia Tsvetkov", "Sewoong Oh", "Pang Wei Koh"], "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "Sanitizing sensitive text data typically involves removing personally\nidentifiable information (PII) or generating synthetic data under the\nassumption that these methods adequately protect privacy; however, their\neffectiveness is often only assessed by measuring the leakage of explicit\nidentifiers but ignoring nuanced textual markers that can lead to\nre-identification. We challenge the above illusion of privacy by proposing a\nnew framework that evaluates re-identification attacks to quantify individual\nprivacy risks upon data release. Our approach shows that seemingly innocuous\nauxiliary information -- such as routine social activities -- can be used to\ninfer sensitive attributes like age or substance use history from sanitized\ndata. For instance, we demonstrate that Azure's commercial PII removal tool\nfails to protect 74\\% of information in the MedQA dataset. Although\ndifferential privacy mitigates these risks to some extent, it significantly\nreduces the utility of the sanitized text for downstream tasks. Our findings\nindicate that current sanitization techniques offer a \\textit{false sense of\nprivacy}, highlighting the need for more robust methods that protect against\nsemantic-level information leakage."}
{"id": "2504.21706", "pdf": "https://arxiv.org/pdf/2504.21706", "abs": "https://arxiv.org/abs/2504.21706", "authors": ["Saber Mehdipour", "Seyed Abolghasem Mirroshandel", "Seyed Amirhossein Tabatabaei"], "title": "Vision Transformers in Precision Agriculture: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting plant diseases is a crucial aspect of modern agriculture - it plays\na key role in maintaining crop health and increasing overall yield. Traditional\napproaches, though still valuable, often rely on manual inspection or\nconventional machine learning techniques, both of which face limitations in\nscalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as\na promising alternative, offering benefits such as improved handling of\nlong-range dependencies and better scalability for visual tasks. This survey\nexplores the application of ViTs in precision agriculture, covering tasks from\nclassification to detection and segmentation. We begin by introducing the\nfoundational architecture of ViTs and discuss their transition from Natural\nLanguage Processing (NLP) to computer vision. The discussion includes the\nconcept of inductive bias in traditional models like Convolutional Neural\nNetworks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive\nreview of recent literature, focusing on key methodologies, datasets, and\nperformance metrics. The survey also includes a comparative analysis of CNNs\nand ViTs, with a look at hybrid models and performance enhancements. Technical\nchallenges - such as data requirements, computational demands, and model\ninterpretability - are addressed alongside potential solutions. Finally, we\noutline potential research directions and technological advancements that could\nfurther support the integration of ViTs in real-world agricultural settings.\nOur goal with this study is to offer practitioners and researchers a deeper\nunderstanding of how ViTs are poised to transform smart and precision\nagriculture."}
{"id": "2504.21051", "pdf": "https://arxiv.org/pdf/2504.21051", "abs": "https://arxiv.org/abs/2504.21051", "authors": ["Jiarui Ye", "Hao Tang"], "title": "Multimodal Large Language Models for Medicine: A Comprehensive Survey", "categories": ["cs.LG", "cs.CL", "cs.MM"], "comment": null, "summary": "MLLMs have recently become a focal point in the field of artificial\nintelligence research. Building on the strong capabilities of LLMs, MLLMs are\nadept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs\nhave gained substantial attention from different domains. Researchers have\nbegun to explore the potential of MLLMs in the medical and healthcare domain.\nIn this paper, we first introduce the background and fundamental concepts\nrelated to LLMs and MLLMs, while emphasizing the working principles of MLLMs.\nSubsequently, we summarize three main directions of application within\nhealthcare: medical reporting, medical diagnosis, and medical treatment. Our\nfindings are based on a comprehensive review of 330 recent papers in this area.\nWe illustrate the remarkable capabilities of MLLMs in these domains by\nproviding specific examples. For data, we present six mainstream modes of data\nalong with their corresponding evaluation benchmarks. At the end of the survey,\nwe discuss the challenges faced by MLLMs in the medical and healthcare domain\nand propose feasible methods to mitigate or overcome these issues."}
{"id": "2504.21718", "pdf": "https://arxiv.org/pdf/2504.21718", "abs": "https://arxiv.org/abs/2504.21718", "authors": ["Shiying Li", "Xingqun Qi", "Bingkun Yang", "Chen Weile", "Zezhao Tian", "Muyi Sun", "Qifeng Liu", "Man Zhang", "Zhenan Sun"], "title": "VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Generating responsive listener head dynamics with nuanced emotions and\nexpressive reactions is crucial for practical dialogue modeling in various\nvirtual avatar animations. Previous studies mainly focus on the direct\nshort-term production of listener behavior. They overlook the fine-grained\ncontrol over motion variations and emotional intensity, especially in\nlong-sequence modeling. Moreover, the lack of long-term and large-scale paired\nspeaker-listener corpora including head dynamics and fine-grained\nmulti-modality annotations (e.g., text-based expression descriptions, emotional\nintensity) also limits the application of dialogue modeling.Therefore, we first\nnewly collect a large-scale multi-turn dataset of 3D dyadic conversation\ncontaining more than 1.4M valid frames for multi-modal responsive interaction,\ndubbed ListenerX. Additionally, we propose VividListener, a novel framework\nenabling fine-grained, expressive and controllable listener dynamics modeling.\nThis framework leverages multi-modal conditions as guiding principles for\nfostering coherent interactions between speakers and listeners.Specifically, we\ndesign the Responsive Interaction Module (RIM) to adaptively represent the\nmulti-modal interactive embeddings. RIM ensures the listener dynamics achieve\nfine-grained semantic coordination with textual descriptions and adjustments,\nwhile preserving expressive reaction with speaker behavior. Meanwhile, we\ndesign the Emotional Intensity Tags (EIT) for emotion intensity editing with\nmulti-modal information integration, applying to both text descriptions and\nlistener motion amplitude.Extensive experiments conducted on our newly\ncollected ListenerX dataset demonstrate that VividListener achieves\nstate-of-the-art performance, realizing expressive and controllable listener\ndynamics."}
{"id": "2504.21318", "pdf": "https://arxiv.org/pdf/2504.21318", "abs": "https://arxiv.org/abs/2504.21318", "authors": ["Marah Abdin", "Sahaj Agarwal", "Ahmed Awadallah", "Vidhisha Balachandran", "Harkirat Behl", "Lingjiao Chen", "Gustavo de Rosa", "Suriya Gunasekar", "Mojan Javaheripi", "Neel Joshi", "Piero Kauffmann", "Yash Lara", "Caio César Teodoro Mendes", "Arindam Mitra", "Besmira Nushi", "Dimitris Papailiopoulos", "Olli Saarikivi", "Shital Shah", "Vaishnavi Shrivastava", "Vibhav Vineet", "Yue Wu", "Safoora Yousefi", "Guoqing Zheng"], "title": "Phi-4-reasoning Technical Report", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that\nachieves strong performance on complex reasoning tasks. Trained via supervised\nfine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected\nfor the right level of complexity and diversity-and reasoning demonstrations\ngenerated using o3-mini, Phi-4-reasoning generates detailed reasoning chains\nthat effectively leverage inference-time compute. We further develop\nPhi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based\nreinforcement learning that offers higher performance by generating longer\nreasoning traces. Across a wide range of reasoning tasks, both models\noutperform significantly larger open-weight models such as\nDeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full\nDeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and\nscientific reasoning, coding, algorithmic problem solving, planning, and\nspatial understanding. Interestingly, we observe a non-trivial transfer of\nimprovements to general-purpose benchmarks as well. In this report, we provide\ninsights into our training data, our training methodologies, and our\nevaluations. We show that the benefit of careful data curation for supervised\nfine-tuning (SFT) extends to reasoning language models, and can be further\namplified by reinforcement learning (RL). Finally, our evaluation points to\nopportunities for improving how we assess the performance and robustness of\nreasoning models."}
{"id": "2504.21749", "pdf": "https://arxiv.org/pdf/2504.21749", "abs": "https://arxiv.org/abs/2504.21749", "authors": ["Leonhard Sommer", "Olaf Dünkel", "Christian Theobalt", "Adam Kortylewski"], "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space", "categories": ["cs.CV"], "comment": null, "summary": "3D morphable models (3DMMs) are a powerful tool to represent the possible\nshapes and appearances of an object category. Given a single test image, 3DMMs\ncan be used to solve various tasks, such as predicting the 3D shape, pose,\nsemantic correspondence, and instance segmentation of an object. Unfortunately,\n3DMMs are only available for very few object categories that are of particular\ninterest, like faces or human bodies, as they require a demanding 3D data\nacquisition and category-specific training process. In contrast, we introduce a\nnew method, Common3D, that learns 3DMMs of common objects in a fully\nself-supervised manner from a collection of object-centric videos. For this\npurpose, our model represents objects as a learned 3D template mesh and a\ndeformation field that is parameterized as an image-conditioned neural network.\nDifferent from prior works, Common3D represents the object appearance with\nneural features instead of RGB colors, which enables the learning of more\ngeneralizable representations through an abstraction from pixel intensities.\nImportantly, we train the appearance features using a contrastive objective by\nexploiting the correspondences defined through the deformable template mesh.\nThis leads to higher quality correspondence features compared to related works\nand a significantly improved model performance at estimating 3D object pose and\nsemantic correspondence. Common3D is the first completely self-supervised\nmethod that can solve various vision tasks in a zero-shot manner."}
{"id": "2504.21400", "pdf": "https://arxiv.org/pdf/2504.21400", "abs": "https://arxiv.org/abs/2504.21400", "authors": ["Sugat Chaturvedi", "Rochana Chaturvedi"], "title": "Who Gets the Callback? Generative AI and Gender Bias", "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "comment": null, "summary": "Generative artificial intelligence (AI), particularly large language models\n(LLMs), is being rapidly deployed in recruitment and for candidate\nshortlisting. We audit several mid-sized open-source LLMs for gender bias using\na dataset of 332,044 real-world online job postings. For each posting, we\nprompt the model to recommend whether an equally qualified male or female\ncandidate should receive an interview callback. We find that most models tend\nto favor men, especially for higher-wage roles. Mapping job descriptions to the\nStandard Occupational Classification system, we find lower callback rates for\nwomen in male-dominated occupations and higher rates in female-associated ones,\nindicating occupational segregation. A comprehensive analysis of linguistic\nfeatures in job ads reveals strong alignment of model recommendations with\ntraditional gender stereotypes. To examine the role of recruiter identity, we\nsteer model behavior by infusing Big Five personality traits and simulating the\nperspectives of historical figures. We find that less agreeable personas reduce\nstereotyping, consistent with an agreeableness bias in LLMs. Our findings\nhighlight how AI-driven hiring may perpetuate biases in the labor market and\nhave implications for fairness and diversity within firms."}
{"id": "2504.21771", "pdf": "https://arxiv.org/pdf/2504.21771", "abs": "https://arxiv.org/abs/2504.21771", "authors": ["Bahram Jafrasteh", "Wei Peng", "Cheng Wan", "Yimin Luo", "Ehsan Adeli", "Qingyu Zhao"], "title": "Anatomical Similarity as a New Metric to Evaluate Brain Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "Generative models enhance neuroimaging through data augmentation, quality\nimprovement, and rare condition studies. Despite advances in realistic\nsynthetic MRIs, evaluations focus on texture and perception, lacking\nsensitivity to crucial anatomical fidelity. This study proposes a new metric,\ncalled WASABI (Wasserstein-Based Anatomical Brain Index), to assess the\nanatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg},\na deep learning-based brain parcellation tool, to derive volumetric measures of\nbrain regions in each MRI and uses the multivariate Wasserstein distance to\ncompare distributions between real and synthetic anatomies. Based on controlled\nexperiments on two real datasets and synthetic MRIs from five generative\nmodels, WASABI demonstrates higher sensitivity in quantifying anatomical\ndiscrepancies compared to traditional image-level metrics, even when synthetic\nimages achieve near-perfect visual quality. Our findings advocate for shifting\nthe evaluation paradigm beyond visual inspection and conventional metrics,\nemphasizing anatomical fidelity as a crucial benchmark for clinically\nmeaningful brain MRI synthesis. Our code is available at\nhttps://github.com/BahramJafrasteh/wasabi-mri."}
{"id": "2504.21435", "pdf": "https://arxiv.org/pdf/2504.21435", "abs": "https://arxiv.org/abs/2504.21435", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "ShaoGuo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "29 pages, 15 figures, CVPR 2025", "summary": "With the rapid development of Multi-modal Large Language Models (MLLMs), an\nincreasing number of benchmarks have been established to evaluate the video\nunderstanding capabilities of these models. However, these benchmarks focus on\n\\textbf{standalone} videos and mainly assess ``visual elements'' like human\nactions and object states. In reality, contemporary videos often encompass\ncomplex and continuous narratives, typically presented as a \\textbf{series}. To\naddress this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting\nof 105 carefully curated narrative-driven series, covering 28 specialized tasks\nthat require deep narrative understanding. Specifically, we first select a\ndiverse set of drama series spanning various genres. Then, we introduce a novel\nlong-span narrative annotation method, combined with a full-information\ntransformation approach to convert manual annotations into diverse task\nformats. To further enhance model capacity for detailed analysis of plot\nstructures and character relationships within series, we propose a novel\nnarrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on\n\\textbf{SeriesBench} indicate that existing MLLMs still face significant\nchallenges in understanding narrative-driven series, while \\textbf{PC-DCoT}\nenables these MLLMs to achieve performance improvements. Overall, our\n\\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of\nadvancing model capabilities to understand narrative-driven series, guiding the\nfuture development of MLLMs. SeriesBench is publicly available at\nhttps://github.com/zackhxn/SeriesBench-CVPR2025."}
{"id": "2504.21789", "pdf": "https://arxiv.org/pdf/2504.21789", "abs": "https://arxiv.org/abs/2504.21789", "authors": ["Alessia Hu", "Regina Beets-Tan", "Lishan Cai", "Eduardo Pooch"], "title": "Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "Paper accepted for publication at 2025 47th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)\n  Copyright 2025 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future media", "summary": "Magnetic Resonance Imaging (MRI) plays an important role in identifying\nclinically significant prostate cancer (csPCa), yet automated methods face\nchallenges such as data imbalance, variable tumor sizes, and a lack of\nannotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which\nincorporates anomaly maps derived from biparametric MRI sequences into a deep\nlearning-based segmentation framework to improve csPCa identification. We\nconduct a comparative analysis of anomaly detection methods and evaluate the\nintegration of anomaly maps into the segmentation pipeline. Anomaly maps,\ngenerated using Fixed-Point GAN reconstruction, highlight deviations from\nnormal prostate tissue, guiding the segmentation model to potential cancerous\nregions. We compare the performance by using the average score, computed as the\nmean of the AUROC and Average Precision (AP). On the external test set, adU-Net\nachieves the best average score of 0.618, outperforming the baseline nnU-Net\nmodel (0.605). The results demonstrate that incorporating anomaly detection\ninto segmentation improves generalization and performance, particularly with\nADC-based anomaly maps, offering a promising direction for automated csPCa\nidentification."}
{"id": "2504.21559", "pdf": "https://arxiv.org/pdf/2504.21559", "abs": "https://arxiv.org/abs/2504.21559", "authors": ["Sangmin Woo", "Kang Zhou", "Yun Zhou", "Shuai Wang", "Sheng Guan", "Haibo Ding", "Lin Lee Cheong"], "title": "Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "NAACL 2025", "summary": "Large Vision Language Models (LVLMs) often suffer from object hallucination,\nwhich undermines their reliability. Surprisingly, we find that simple\nobject-based visual prompting -- overlaying visual cues (e.g., bounding box,\ncircle) on images -- can significantly mitigate such hallucination; however,\ndifferent visual prompts (VPs) vary in effectiveness. To address this, we\npropose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify\noptimal VPs that enhance LVLM responses without needing access to model\ninternals. Our approach employs a pool of candidate VPs and trains a router\nmodel to dynamically select the most effective VP for a given input image. This\nblack-box approach is model-agnostic, making it applicable to both open-source\nand proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR\ndemonstrate that BBVPE effectively reduces object hallucination."}
{"id": "2504.21810", "pdf": "https://arxiv.org/pdf/2504.21810", "abs": "https://arxiv.org/abs/2504.21810", "authors": ["Franko Hrzic", "Mohammadreza Movahhedi", "Ophelie Lavoie-Gagne", "Ata Kiapour"], "title": "A simple and effective approach for body part recognition on CT scans based on projection estimation", "categories": ["cs.CV", "68T01, 65D19", "I.4.0; I.4.10; I.2.1"], "comment": "19 pages, 6 figures", "summary": "It is well known that machine learning models require a high amount of\nannotated data to obtain optimal performance. Labelling Computed Tomography\n(CT) data can be a particularly challenging task due to its volumetric nature\nand often missing and$/$or incomplete associated meta-data. Even inspecting one\nCT scan requires additional computer software, or in the case of programming\nlanguages $-$ additional programming libraries. This study proposes a simple,\nyet effective approach based on 2D X-ray-like estimation of 3D CT scans for\nbody region identification. Although body region is commonly associated with\nthe CT scan, it often describes only the focused major body region neglecting\nother anatomical regions present in the observed CT. In the proposed approach,\nestimated 2D images were utilized to identify 14 distinct body regions,\nproviding valuable information for constructing a high-quality medical dataset.\nTo evaluate the effectiveness of the proposed method, it was compared against\n2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed\nthe others, where it came on top with statistical significance and F1-Score for\nthe best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the\n0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852\n$\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three\ndifferent clinical centers and counted 15,622 CT scans (44,135 labels)."}
{"id": "2504.21578", "pdf": "https://arxiv.org/pdf/2504.21578", "abs": "https://arxiv.org/abs/2504.21578", "authors": ["Kamila Barylska", "Frank Delaplace", "Anna Gogolińska", "Ewa Pańkowska"], "title": "Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks", "categories": ["q-bio.CB", "cs.CL", "03", "F.2; G.0"], "comment": null, "summary": "Diabetes is a civilization chronic disease characterized by a constant\nelevated concentration of glucose in the blood. Many processes are involved in\nthe glucose regulation, and their interactions are very complex. To better\nunderstand those processes we set ourselves a goal to create a Petri net model\nof the glucose regulation in the whole body. So far we have managed to create a\nmodel of glycolysis and synthesis of glucose in the liver, and the general\noverview models of the glucose regulation in a healthy and diabetic person. In\nthis paper we introduce Petri nets models of insulin secretion in beta cell of\nthe pancreas, and glucagon in the pancreas alpha cells. Those two hormones have\nmutually opposite effects: insulin preventing hyperglycemia, and glucagon\npreventing hypoglycemia. Understanding the mechanisms of insulin and glucagon\nsecretion constitutes the basis for understanding diabetes. We also present a\nmodel in which both processes occur together, depending on the blood glucose\nlevel. The dynamics of each model is analysed. Additionally, we transform the\noverall insulin and glucagon secretion system to a Boolean network, following\nstandard transformation rules."}
{"id": "2504.21814", "pdf": "https://arxiv.org/pdf/2504.21814", "abs": "https://arxiv.org/abs/2504.21814", "authors": ["Yixin Gao", "Xiaohan Pan", "Xin Li", "Zhibo Chen"], "title": "Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of AIGC foundation models has revolutionized the\nparadigm of image compression, which paves the way for the abandonment of most\npixel-level transform and coding, compelling us to ask: why compress what you\ncan generate if the AIGC foundation model is powerful enough to faithfully\ngenerate intricate structure and fine-grained details from nothing more than\nsome compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o\nimage generation of OpenAI has achieved impressive cross-modality generation,\nediting, and design capabilities, which motivates us to answer the above\nquestion by exploring its potential in image compression fields. In this work,\nwe investigate two typical compression paradigms: textual coding and multimodal\ncoding (i.e., text + extremely low-resolution image), where all/most\npixel-level information is generated instead of compressing via the advanced\nGPT-4o image generation function. The essential challenge lies in how to\nmaintain semantic and structure consistency during the decoding process. To\novercome this, we propose a structure raster-scan prompt engineering mechanism\nto transform the image into textual space, which is compressed as the condition\nof GPT-4o image generation. Extensive experiments have shown that the\ncombination of our designed structural raster-scan prompts and GPT-4o's image\ngeneration function achieved the impressive performance compared with recent\nmultimodal/generative image compression at ultra-low bitrate, further\nindicating the potential of AIGC generation in image compression fields."}
{"id": "2504.21659", "pdf": "https://arxiv.org/pdf/2504.21659", "abs": "https://arxiv.org/abs/2504.21659", "authors": ["Haotian Luo", "Haiying He", "Yibo Wang", "Jinluan Yang", "Rui Liu", "Naiqiang Tan", "Xiaochun Cao", "Dacheng Tao", "Li Shen"], "title": "AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, long-thought reasoning models achieve strong performance on complex\nreasoning tasks, but often incur substantial inference overhead, making\nefficiency a critical concern. Our empirical analysis reveals that the benefit\nof using Long-CoT varies across problems: while some problems require elaborate\nreasoning, others show no improvement, or even degraded accuracy. This\nmotivates adaptive reasoning strategies that tailor reasoning depth to the\ninput. However, prior work primarily reduces redundancy within long reasoning\npaths, limiting exploration of more efficient strategies beyond the Long-CoT\nparadigm. To address this, we propose a novel two-stage framework for adaptive\nand efficient reasoning. First, we construct a hybrid reasoning model by\nmerging long and short CoT models to enable diverse reasoning styles. Second,\nwe apply bi-level preference training to guide the model to select suitable\nreasoning styles (group-level), and prefer concise and correct reasoning within\neach style group (instance-level). Experiments demonstrate that our method\nsignificantly reduces inference costs compared to other baseline approaches,\nwhile maintaining performance. Notably, on five mathematical datasets, the\naverage length of reasoning is reduced by more than 50%, highlighting the\npotential of adaptive strategies to optimize reasoning efficiency in large\nlanguage models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1"}
{"id": "2504.21831", "pdf": "https://arxiv.org/pdf/2504.21831", "abs": "https://arxiv.org/abs/2504.21831", "authors": ["Anas Anwarul Haq Khan", "Utkarsh Verma", "Prateek Chanda", "Ganesh Ramakrishnan"], "title": "Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce DEEVISum (Distilled Early Exit Vision language model for\nSummarization), a lightweight, efficient, and scalable vision language model\ndesigned for segment wise video summarization. Leveraging multi modal prompts\nthat combine textual and audio derived signals, DEEVISum incorporates Multi\nStage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance\nbetween performance and efficiency. MSKD offers a 1.33% absolute F1 improvement\nover baseline distillation (0.5%), while EE reduces inference time by\napproximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset,\nour best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing\nthe performance of significantly larger models, all while maintaining a lower\ncomputational footprint. We publicly release our code and processed dataset to\nsupport further research."}
{"id": "2504.21716", "pdf": "https://arxiv.org/pdf/2504.21716", "abs": "https://arxiv.org/abs/2504.21716", "authors": ["Marc Glocker", "Peter Hönig", "Matthias Hirschmanner", "Markus Vincze"], "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "We present an embodied robotic system with an LLM-driven agent-orchestration\narchitecture for autonomous household object management. The system integrates\nmemory-augmented task planning, enabling robots to execute high-level user\ncommands while tracking past actions. It employs three specialized agents: a\nrouting agent, a task planning agent, and a knowledge base agent, each powered\nby task-specific LLMs. By leveraging in-context learning, our system avoids the\nneed for explicit model training. RAG enables the system to retrieve context\nfrom past interactions, enhancing long-term object tracking. A combination of\nGrounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating\nsemantic scene understanding for task planning. Evaluation across three\nhousehold scenarios demonstrates high task planning accuracy and an improvement\nin memory recall due to RAG. Specifically, Qwen2.5 yields best performance for\nspecialized agents, while LLaMA3.1 excels in routing tasks. The source code is\navailable at: https://github.com/marc1198/chat-hsr."}
{"id": "2504.21836", "pdf": "https://arxiv.org/pdf/2504.21836", "abs": "https://arxiv.org/abs/2504.21836", "authors": ["Ipek Oztas", "Duygu Ceylan", "Aysegul Dundar"], "title": "3D Stylization via Large Reconstruction Model", "categories": ["cs.CV"], "comment": "Accepted to SIGGRAPH 2025", "summary": "With the growing success of text or image guided 3D generators, users demand\nmore control over the generation process, appearance stylization being one of\nthem. Given a reference image, this requires adapting the appearance of a\ngenerated 3D asset to reflect the visual style of the reference while\nmaintaining visual consistency from multiple viewpoints. To tackle this\nproblem, we draw inspiration from the success of 2D stylization methods that\nleverage the attention mechanisms in large image generation models to capture\nand transfer visual style. In particular, we probe if large reconstruction\nmodels, commonly used in the context of 3D generation, has a similar\ncapability. We discover that the certain attention blocks in these models\ncapture the appearance specific features. By injecting features from a visual\nstyle image to such blocks, we develop a simple yet effective 3D appearance\nstylization method. Our method does not require training or test time\noptimization. Through both quantitative and qualitative evaluations, we\ndemonstrate that our approach achieves superior results in terms of 3D\nappearance stylization, significantly improving efficiency while maintaining\nhigh-quality visual outcomes."}
{"id": "2504.21751", "pdf": "https://arxiv.org/pdf/2504.21751", "abs": "https://arxiv.org/abs/2504.21751", "authors": ["Sizhe Wang", "Zhengren Wang", "Dongsheng Ma", "Yongan Yu", "Rui Ling", "Zhiyu Li", "Feiyu Xiong", "Wentao Zhang"], "title": "CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Real world development demands code that is readable, extensible, and\ntestable by organizing the implementation into modular components and\niteratively reuse pre-implemented code. We term this iterative, multi-turn\nprocess codeflow and introduce CodeFlowBench, the first benchmark designed for\ncomprehensively evaluating LLMs' ability to perform codeflow, namely to\nimplement new functionality by reusing existing functions over multiple turns.\nCodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously\nupdated via an automated pipeline that decomposes each problem into a series of\nfunction-level subproblems based on its dependency tree and each subproblem is\npaired with unit tests. We further propose a novel evaluation framework with\ntasks and metrics tailored to multi-turn code reuse to assess model\nperformance. In experiments across various LLMs under both multi-turn and\nsingle-turn patterns. We observe models' poor performance on CodeFlowBench,\nwith a substantial performance drop in the iterative codeflow scenario. For\ninstance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8%\nin single-turn pattern. Further analysis shows that different models excel at\ndifferent dependency depths, yet all struggle to correctly solve structurally\ncomplex problems, highlighting challenges for current LLMs to serve as code\ngeneration tools when performing codeflow. Overall, CodeFlowBench offers a\ncomprehensive benchmark and new insights into LLM capabilities for multi-turn,\niterative code generation, guiding future advances in code generation tasks."}
{"id": "2504.21846", "pdf": "https://arxiv.org/pdf/2504.21846", "abs": "https://arxiv.org/abs/2504.21846", "authors": ["Hadleigh Schwartz", "Xiaofeng Yan", "Charles J. Carver", "Xia Zhou"], "title": "Active Light Modulation to Counter Manipulation of Speech Visual Content", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "High-profile speech videos are prime targets for falsification, owing to\ntheir accessibility and influence. This work proposes Spotlight, a low-overhead\nand unobtrusive system for protecting live speech videos from visual\nfalsification of speaker identity and lip and facial motion. Unlike predominant\nfalsification detection methods operating in the digital domain, Spotlight\ncreates dynamic physical signatures at the event site and embeds them into all\nvideo recordings via imperceptible modulated light. These physical signatures\nencode semantically-meaningful features unique to the speech event, including\nthe speaker's identity and facial motion, and are cryptographically-secured to\nprevent spoofing. The signatures can be extracted from any video downstream and\nvalidated against the portrayed speech content to check its integrity. Key\nelements of Spotlight include (1) a framework for generating extremely compact\n(i.e., 150-bit), pose-invariant speech video features, based on\nlocality-sensitive hashing; and (2) an optical modulation scheme that embeds\n>200 bps into video while remaining imperceptible both in video and live.\nPrototype experiments on extensive video datasets show Spotlight achieves AUCs\n$\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified\nvideos. Further, Spotlight is highly robust across recording conditions, video\npost-processing techniques, and white-box adversarial attacks on its video\nfeature extraction methodologies."}
{"id": "2504.21798", "pdf": "https://arxiv.org/pdf/2504.21798", "abs": "https://arxiv.org/abs/2504.21798", "authors": ["John Yang", "Kilian Leret", "Carlos E. Jimenez", "Alexander Wettig", "Kabir Khandpur", "Yanzhe Zhang", "Binyuan Hui", "Ofir Press", "Ludwig Schmidt", "Diyi Yang"], "title": "SWE-smith: Scaling Data for Software Engineering Agents", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite recent progress in Language Models (LMs) for software engineering,\ncollecting training data remains a significant pain point. Existing datasets\nare small, with at most 1,000s of training instances from 11 or fewer GitHub\nrepositories. The procedures to curate such datasets are often complex,\nnecessitating hundreds of hours of human labor; companion execution\nenvironments also take up several terabytes of storage, severely limiting their\nscalability and usability. To address this pain point, we introduce SWE-smith,\na novel pipeline for generating software engineering training data at scale.\nGiven any Python codebase, SWE-smith constructs a corresponding execution\nenvironment, then automatically synthesizes 100s to 1,000s of task instances\nthat break existing test(s) in the codebase. Using SWE-smith, we create a\ndataset of 50k instances sourced from 128 GitHub repositories, an order of\nmagnitude larger than all previous works. We train SWE-agent-LM-32B, achieving\n40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art\namong open source models. We open source SWE-smith (collection procedure, task\ninstances, trajectories, models) to lower the barrier of entry for research in\nLM systems for automated software engineering. All assets available at\nhttps://swesmith.com."}
{"id": "2504.21847", "pdf": "https://arxiv.org/pdf/2504.21847", "abs": "https://arxiv.org/abs/2504.21847", "authors": ["Derong Jin", "Ruohan Gao"], "title": "Differentiable Room Acoustic Rendering with Multi-View Vision Priors", "categories": ["cs.CV", "cs.SD"], "comment": "Project Page: https://humathe.github.io/avdar/", "summary": "An immersive acoustic experience enabled by spatial audio is just as crucial\nas the visual aspect in creating realistic virtual environments. However,\nexisting methods for room impulse response estimation rely either on\ndata-demanding learning-based models or computationally expensive physics-based\nmodeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic\nRendering (AV-DAR), a framework that leverages visual cues extracted from\nmulti-view images and acoustic beam tracing for physics-based room acoustic\nrendering. Experiments across six real-world environments from two datasets\ndemonstrate that our multimodal, physics-based approach is efficient,\ninterpretable, and accurate, significantly outperforming a series of prior\nmethods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves\ncomparable performance to models trained on 10 times more data while delivering\nrelative gains ranging from 16.6% to 50.9% when trained at the same scale."}
{"id": "2504.21850", "pdf": "https://arxiv.org/pdf/2504.21850", "abs": "https://arxiv.org/abs/2504.21850", "authors": ["Xindi Wu", "Hee Seung Hwang", "Polina Kirichenko", "Olga Russakovsky"], "title": "COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning", "categories": ["cs.CV"], "comment": "17 pages, 13 figures", "summary": "Multimodal Large Language Models (MLLMs) excel at simple vision-language\ntasks but struggle when faced with complex tasks that require multiple\ncapabilities, such as simultaneously recognizing objects, counting them, and\nunderstanding their spatial relationships. This might be partially the result\nof the fact that Visual Instruction Tuning (VIT), a critical training step for\nMLLMs, has traditionally focused on scaling data volume, but not the\ncompositional complexity of training examples. We propose COMPACT\n(COMPositional Atomic-to-complex visual Capability Tuning), which generates a\ntraining dataset explicitly controlling for the compositional complexity of the\ntraining examples. The data from COMPACT allows MLLMs to train on combinations\nof atomic capabilities to learn complex capabilities more efficiently. Across\nall benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT\nwhile using less than 10% of its data budget, and even outperforms it on\nseveral, especially those involving complex multi-capability tasks. For\nexample, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%\nimprovement on MM-Vet compared to the full-scale VIT on particularly complex\nquestions that require four or more atomic capabilities. COMPACT offers a\nscalable, data-efficient, visual compositional tuning recipe to improve on\ncomplex visual-language tasks."}
{"id": "2504.21853", "pdf": "https://arxiv.org/pdf/2504.21853", "abs": "https://arxiv.org/abs/2504.21853", "authors": ["Jiwen Yu", "Yiran Qin", "Haoxuan Che", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Hao Chen", "Xihui Liu"], "title": "A Survey of Interactive Generative Video", "categories": ["cs.CV"], "comment": null, "summary": "Interactive Generative Video (IGV) has emerged as a crucial technology in\nresponse to the growing demand for high-quality, interactive video content\nacross various domains. In this paper, we define IGV as a technology that\ncombines generative capabilities to produce diverse high-quality video content\nwith interactive features that enable user engagement through control signals\nand responsive feedback. We survey the current landscape of IGV applications,\nfocusing on three major domains: 1) gaming, where IGV enables infinite\nexploration in virtual worlds; 2) embodied AI, where IGV serves as a\nphysics-aware environment synthesizer for training agents in multimodal\ninteraction with dynamically evolving scenes; and 3) autonomous driving, where\nIGV provides closed-loop simulation capabilities for safety-critical testing\nand validation. To guide future development, we propose a comprehensive\nframework that decomposes an ideal IGV system into five essential modules:\nGeneration, Control, Memory, Dynamics, and Intelligence. Furthermore, we\nsystematically analyze the technical challenges and future directions in\nrealizing each component for an ideal IGV system, such as achieving real-time\ngeneration, enabling open-domain control, maintaining long-term coherence,\nsimulating accurate physics, and integrating causal reasoning. We believe that\nthis systematic analysis will facilitate future research and development in the\nfield of IGV, ultimately advancing the technology toward more sophisticated and\npractical applications."}
{"id": "2504.21855", "pdf": "https://arxiv.org/pdf/2504.21855", "abs": "https://arxiv.org/abs/2504.21855", "authors": ["Qihao Liu", "Ju He", "Qihang Yu", "Liang-Chieh Chen", "Alan Yuille"], "title": "ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction", "categories": ["cs.CV"], "comment": "Project Page: https://revision-video.github.io/", "summary": "In recent years, video generation has seen significant advancements. However,\nchallenges still persist in generating complex motions and interactions. To\naddress these challenges, we introduce ReVision, a plug-and-play framework that\nexplicitly integrates parameterized 3D physical knowledge into a pretrained\nconditional video generation model, significantly enhancing its ability to\ngenerate high-quality videos with complex motion and interactions.\nSpecifically, ReVision consists of three stages. First, a video diffusion model\nis used to generate a coarse video. Next, we extract a set of 2D and 3D\nfeatures from the coarse video to construct a 3D object-centric representation,\nwhich is then refined by our proposed parameterized physical prior model to\nproduce an accurate 3D motion sequence. Finally, this refined motion sequence\nis fed back into the same video diffusion model as additional conditioning,\nenabling the generation of motion-consistent videos, even in scenarios\ninvolving complex actions and interactions. We validate the effectiveness of\nour approach on Stable Video Diffusion, where ReVision significantly improves\nmotion fidelity and coherence. Remarkably, with only 1.5B parameters, it even\noutperforms a state-of-the-art video generation model with over 13B parameters\non complex video generation by a substantial margin. Our results suggest that,\nby incorporating 3D physical knowledge, even a relatively small video diffusion\nmodel can generate complex motions and interactions with greater realism and\ncontrollability, offering a promising solution for physically plausible video\ngeneration."}
{"id": "2504.21033", "pdf": "https://arxiv.org/pdf/2504.21033", "abs": "https://arxiv.org/abs/2504.21033", "authors": ["Majid Behravan", "Maryam Haghani", "Denis Gracanin"], "title": "Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional 3D modeling requires technical expertise, specialized software,\nand time-intensive processes, making it inaccessible for many users. Our\nresearch aims to lower these barriers by combining generative AI and augmented\nreality (AR) into a cohesive system that allows users to easily generate,\nmanipulate, and interact with 3D models in real time, directly within AR\nenvironments. Utilizing cutting-edge AI models like Shap-E, we address the\ncomplex challenges of transforming 2D images into 3D representations in AR\nenvironments. Key challenges such as object isolation, handling intricate\nbackgrounds, and achieving seamless user interaction are tackled through\nadvanced object detection methods, such as Mask R-CNN. Evaluation results from\n35 participants reveal an overall System Usability Scale (SUS) score of 69.64,\nwith participants who engaged with AR/VR technologies more frequently rating\nthe system significantly higher, at 80.71. This research is particularly\nrelevant for applications in gaming, education, and AR-based e-commerce,\noffering intuitive, model creation for users without specialized skills."}
{"id": "2504.21067", "pdf": "https://arxiv.org/pdf/2504.21067", "abs": "https://arxiv.org/abs/2504.21067", "authors": ["Yuhan Xie", "Yixi Cai", "Yinqiang Zhang", "Lei Yang", "Jia Pan"], "title": "GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": null, "summary": "This research tackles the challenge of real-time active view selection and\nuncertainty quantification on visual quality for active 3D reconstruction.\nVisual quality is a critical aspect of 3D reconstruction. Recent advancements\nsuch as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have\nnotably enhanced the image rendering quality of reconstruction models.\nNonetheless, the efficient and effective acquisition of input images for\nreconstruction-specifically, the selection of the most informative\nviewpoint-remains an open challenge, which is crucial for active\nreconstruction. Existing studies have primarily focused on evaluating geometric\ncompleteness and exploring unobserved or unknown regions, without direct\nevaluation of the visual uncertainty within the reconstruction model. To\naddress this gap, this paper introduces a probabilistic model that quantifies\nvisual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we\nformulate a criterion, Gaussian Splatting Shannon Mutual Information\n(GauSS-MI), for real-time assessment of visual mutual information from novel\nviewpoints, facilitating the selection of next best view. GauSS-MI is\nimplemented within an active reconstruction system integrated with a view and\nmotion planner. Extensive experiments across various simulated and real-world\nscenes showcase the superior visual quality and reconstruction efficiency\nperformance of the proposed system."}
{"id": "2504.21188", "pdf": "https://arxiv.org/pdf/2504.21188", "abs": "https://arxiv.org/abs/2504.21188", "authors": ["Natnael Alemayehu"], "title": "Light Weight CNN for classification of Brain Tumors from MRI Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages", "summary": "This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis."}
{"id": "2504.21227", "pdf": "https://arxiv.org/pdf/2504.21227", "abs": "https://arxiv.org/abs/2504.21227", "authors": ["Omid Halimi Milani", "Amanda Nikho", "Lauren Mills", "Marouane Tliba", "Ahmet Enis Cetin", "Mohammed H. Elnagar"], "title": "Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 7 figures, accepted at IEEE VLSI Test Symposium (VTS) 2025", "summary": "Deep learning models have great potential in medical imaging, including\northodontics and skeletal maturity assessment. However, applying a model to\ndata different from its training set can lead to unreliable predictions that\nmay impact patient care. To address this, we propose a comprehensive\nverification framework that evaluates model suitability through multiple\ncomplementary strategies. First, we introduce a Gradient Attention Map\n(GAM)-based approach that analyzes attention patterns using Grad-CAM and\ncompares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine\nSimilarity, Pearson Correlation, KL Divergence, and Wasserstein Distance.\nSecond, we extend verification to early convolutional feature maps, capturing\nstructural mis-alignments missed by attention alone. Finally, we incorporate an\nadditional garbage class into the classification model to explicitly reject\nout-of-distribution inputs. Experimental results demonstrate that these\ncombined methods effectively identify unsuitable models and inputs, promoting\nsafer and more reliable deployment of deep learning in medical imaging."}
{"id": "2504.21331", "pdf": "https://arxiv.org/pdf/2504.21331", "abs": "https://arxiv.org/abs/2504.21331", "authors": ["Alfred Yan", "Muhammad Nur Talha Kilic", "Gert Nolze", "Ankit Agrawal", "Alok Choudhary", "Roberto dos Reis", "Vinayak Dravid"], "title": "Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations", "categories": ["cond-mat.mtrl-sci", "cs.CV"], "comment": "33 pages, preliminary version", "summary": "The design of novel materials hinges on the understanding of\nstructure-property relationships. However, our capability to synthesize a large\nnumber of materials has outpaced the ability and speed needed to characterize\nthem. While the overall chemical constituents can be readily known during\nsynthesis, the structural evolution and characterization of newly synthesized\nsamples remains a bottleneck for the ultimate goal of high throughput\nnanomaterials discovery. Thus, scalable methods for crystal symmetry\ndetermination that can analyze a large volume of material samples within a\nshort time-frame are especially needed. Kikuchi diffraction in the SEM is a\npromising technique for this due to its sensitivity to dynamical scattering,\nwhich may provide information beyond just the seven crystal systems and\nfourteen Bravais lattices. After diffraction patterns are collected from\nmaterial samples, deep learning methods may be able to classify the space group\nsymmetries using the patterns as input, which paired with the elemental\ncomposition, would help enable the determination of the crystal structure. To\ninvestigate the feasibility of this solution, neural networks were trained to\npredict the space group type of background corrected EBSD patterns. Our\nnetworks were first trained and tested on an artificial dataset of EBSD\npatterns of 5,148 different cubic phases, created through physics-based\ndynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised\ndeep learning-based domain adaptation method, was utilized to train neural\nnetworks to make predictions for experimental EBSD patterns. We introduce a\nrelabeling scheme, which enables our models to achieve accuracy scores higher\nthan 90% on simulated and experimental data, suggesting that neural networks\nare capable of making predictions of crystal symmetry from an EBSD pattern."}
{"id": "2504.21380", "pdf": "https://arxiv.org/pdf/2504.21380", "abs": "https://arxiv.org/abs/2504.21380", "authors": ["Inês Cardoso Oliveira", "Decebal Constantin Mocanu", "Luis A. Leiva"], "title": "Sparse-to-Sparse Training of Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Diffusion models (DMs) are a powerful type of generative models that have\nachieved state-of-the-art results in various image synthesis tasks and have\nshown potential in other domains, such as natural language processing and\ntemporal data modeling. Despite their stable training dynamics and ability to\nproduce diverse high-quality samples, DMs are notorious for requiring\nsignificant computational resources, both in the training and inference stages.\nPrevious work has focused mostly on increasing the efficiency of model\ninference. This paper introduces, for the first time, the paradigm of\nsparse-to-sparse training to DMs, with the aim of improving both training and\ninference efficiency. We focus on unconditional generation and train sparse DMs\nfrom scratch (Latent Diffusion and ChiroDiff) on six datasets using three\ndifferent methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of\nsparsity in model performance. Our experiments show that sparse DMs are able to\nmatch and often outperform their Dense counterparts, while substantially\nreducing the number of trainable parameters and FLOPs. We also identify safe\nand effective values to perform sparse-to-sparse training of DMs."}
{"id": "2504.21432", "pdf": "https://arxiv.org/pdf/2504.21432", "abs": "https://arxiv.org/abs/2504.21432", "authors": ["Pranav Saxena", "Nishant Raghuvanshi", "Neena Goveas"], "title": "UAV-VLN: End-to-End Vision Language guided Navigation for UAVs", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "A core challenge in AI-guided autonomy is enabling agents to navigate\nrealistically and effectively in previously unseen environments based on\nnatural language commands. We propose UAV-VLN, a novel end-to-end\nVision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs)\nthat seamlessly integrates Large Language Models (LLMs) with visual perception\nto facilitate human-interactive navigation. Our system interprets free-form\nnatural language instructions, grounds them into visual observations, and plans\nfeasible aerial trajectories in diverse environments.\n  UAV-VLN leverages the common-sense reasoning capabilities of LLMs to parse\nhigh-level semantic goals, while a vision model detects and localizes\nsemantically relevant objects in the environment. By fusing these modalities,\nthe UAV can reason about spatial relationships, disambiguate references in\nhuman instructions, and plan context-aware behaviors with minimal task-specific\nsupervision. To ensure robust and interpretable decision-making, the framework\nincludes a cross-modal grounding mechanism that aligns linguistic intent with\nvisual context.\n  We evaluate UAV-VLN across diverse indoor and outdoor navigation scenarios,\ndemonstrating its ability to generalize to novel instructions and environments\nwith minimal task-specific training. Our results show significant improvements\nin instruction-following accuracy and trajectory efficiency, highlighting the\npotential of LLM-driven vision-language interfaces for safe, intuitive, and\ngeneralizable UAV autonomy."}
{"id": "2504.21530", "pdf": "https://arxiv.org/pdf/2504.21530", "abs": "https://arxiv.org/abs/2504.21530", "authors": ["Haifeng Huang", "Xinyi Chen", "Yilun Chen", "Hao Li", "Xiaoshen Han", "Zehan Wang", "Tai Wang", "Jiangmiao Pang", "Zhou Zhao"], "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Recent advancements in robotic manipulation have highlighted the potential of\nintermediate representations for improving policy generalization. In this work,\nwe explore grounding masks as an effective intermediate representation,\nbalancing two key advantages: (1) effective spatial guidance that specifies\ntarget objects and placement areas while also conveying information about\nobject shape and size, and (2) broad generalization potential driven by\nlarge-scale vision-language models pretrained on diverse grounding datasets. We\nintroduce RoboGround, a grounding-aware robotic manipulation system that\nleverages grounding masks as an intermediate representation to guide policy\nnetworks in object manipulation tasks. To further explore and enhance\ngeneralization, we propose an automated pipeline for generating large-scale,\nsimulated data with a diverse set of objects and instructions. Extensive\nexperiments show the value of our dataset and the effectiveness of grounding\nmasks as intermediate guidance, significantly enhancing the generalization\nabilities of robot policies."}
{"id": "2504.21730", "pdf": "https://arxiv.org/pdf/2504.21730", "abs": "https://arxiv.org/abs/2504.21730", "authors": ["Ting Qiao", "Yingjia Wang", "Xing Liu", "Sixing Wu", "Jianbing Li", "Yiming Li"], "title": "Cert-SSB: Toward Certified Sample-Specific Backdoor Defense", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages", "summary": "Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an\nattacker manipulates a small portion of the training data to implant hidden\nbackdoors into the model. The compromised model behaves normally on clean\nsamples but misclassifies backdoored samples into the attacker-specified target\nclass, posing a significant threat to real-world DNN applications. Currently,\nseveral empirical defense methods have been proposed to mitigate backdoor\nattacks, but they are often bypassed by more advanced backdoor techniques. In\ncontrast, certified defenses based on randomized smoothing have shown promise\nby adding random noise to training and testing samples to counteract backdoor\nattacks. In this paper, we reveal that existing randomized smoothing defenses\nimplicitly assume that all samples are equidistant from the decision boundary.\nHowever, it may not hold in practice, leading to suboptimal certification\nperformance. To address this issue, we propose a sample-specific certified\nbackdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic\ngradient ascent to optimize the noise magnitude for each sample, ensuring a\nsample-specific noise level that is then applied to multiple poisoned training\nsets to retrain several smoothed models. After that, Cert-SSB aggregates the\npredictions of multiple smoothed models to generate the final robust\nprediction. In particular, in this case, existing certification methods become\ninapplicable since the optimized noise varies across different samples. To\nconquer this challenge, we introduce a storage-update-based certification\nmethod, which dynamically adjusts each sample's certification region to improve\ncertification performance. We conduct extensive experiments on multiple\nbenchmark datasets, demonstrating the effectiveness of our proposed method. Our\ncode is available at https://github.com/NcepuQiaoTing/Cert-SSB."}
{"id": "2504.21731", "pdf": "https://arxiv.org/pdf/2504.21731", "abs": "https://arxiv.org/abs/2504.21731", "authors": ["Feiyu Lu", "Mengyu Chen", "Hsiang Hsu", "Pranav Deshpande", "Cheng Yao Wang", "Blair MacIntyre"], "title": "Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "In Extended Abstracts of the CHI Conference on Human Factors in\n  Computing Systems (CHI EA '24)", "summary": "Mixed Reality (MR) could assist users' tasks by continuously integrating\nvirtual content with their view of the physical environment. However, where and\nhow to place these content to best support the users has been a challenging\nproblem due to the dynamic nature of MR experiences. In contrast to prior work\nthat investigates optimization-based methods, we are exploring how\nreinforcement learning (RL) could assist with continuous 3D content placement\nthat is aware of users' poses and their surrounding environments. Through an\ninitial exploration and preliminary evaluation, our results demonstrate the\npotential of RL to position content that maximizes the reward for users on the\ngo. We further identify future directions for research that could harness the\npower of RL for personalized and optimized UI and content placement in MR."}
{"id": "2504.21778", "pdf": "https://arxiv.org/pdf/2504.21778", "abs": "https://arxiv.org/abs/2504.21778", "authors": ["Ayman A. Ameen", "Thomas Richter", "André Kaup"], "title": "LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Current learned image compression models typically exhibit high complexity,\nwhich demands significant computational resources. To overcome these\nchallenges, we propose an innovative approach that employs hierarchical feature\nextraction transforms to significantly reduce complexity while preserving bit\nrate reduction efficiency. Our novel architecture achieves this by using fewer\nchannels for high spatial resolution inputs/feature maps. On the other hand,\nfeature maps with a large number of channels have reduced spatial dimensions,\nthereby cutting down on computational load without sacrificing performance.\nThis strategy effectively reduces the forward pass complexity from \\(1256 \\,\n\\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the\nreduced complexity model can open the way for learned image compression models\nto operate efficiently across various devices and pave the way for the\ndevelopment of new architectures in image compression technology."}
