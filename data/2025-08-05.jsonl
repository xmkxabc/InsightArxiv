{"id": "2508.00081", "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00081v1", "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00081v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00106", "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE/ACM MEMOCODE 2025", "url": "http://arxiv.org/abs/2508.00106v1", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "pdf_url": "http://arxiv.org/pdf/2508.00106v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00116", "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "authors": ["Wil M. P. van der Aalst"], "categories": ["cs.AI", "H.4.1; I.2.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, preprint keynote paper of the seventh International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "url": "http://arxiv.org/abs/2508.00116v1", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00116v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00129", "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "authors": ["Agustín Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicolás Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "categories": ["cs.AI", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00129v1", "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00129v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00137", "title": "SHACL Validation under Graph Updates (Extended Paper)", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted at the International Semantic Web Conference (ISWC 2025)", "url": "http://arxiv.org/abs/2508.00137v1", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00137v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00138", "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "url": "http://arxiv.org/abs/2508.00138v1", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "pdf_url": "http://arxiv.org/pdf/2508.00138v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00143", "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted for presentation at NCME AIME-Con 2025", "url": "http://arxiv.org/abs/2508.00143v1", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "comment": "Accepted for presentation at NCME AIME-Con 2025", "pdf_url": "http://arxiv.org/pdf/2508.00143v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00159", "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "authors": ["Jobst Heitzig", "Ram Potham"], "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00159v1", "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00159v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00222", "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00222v1", "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00222v1", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00271", "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "authors": ["Hongjin Qian", "Zheng Liu"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Technical Report, 14 pages", "url": "http://arxiv.org/abs/2508.00271v1", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "comment": "Technical Report, 14 pages", "pdf_url": "http://arxiv.org/pdf/2508.00271v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00475", "title": "E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer", "authors": ["Yunhao Ma", "Yanyu Lin", "Mingjing Li", "Puli Quan", "Chenlin Zhou", "Wenyue Zhang", "Zhiwei Zhong", "Wanyi Jia", "Xueke Zhu", "Qingyan Meng", "Huihui Zhou", "Fengwei An"], "categories": ["cs.AR", "cs.NE"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00475v1", "summary": "(1) Pengcheng Laboratory, (2) Southern University of Science and Technology,\n(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\n(4) University of Chinese Academy of Sciences", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00475v1", "cate": "cs.AR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00282", "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00282v1", "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00282v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00017", "title": "Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation", "authors": ["Nikolai Sergeev"], "categories": ["cs.LO", "cs.AI", "cs.AR"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      19 pages, 5 figures. Code and interactive HTML proof graphs permanently archived on Zenodo (DOI: https://doi.org/10.5281/zenodo.16408441 )", "url": "http://arxiv.org/abs/2508.00017v1", "summary": "We present Generative Logic (GL), a deterministic architecture that begins\nfrom user-supplied axiomatic definitions -- written in a minimalist\nMathematical Programming Language (MPL) -- and systematically explores their\ndeductive neighborhood. Definitions are compiled into a distributed grid of\nsimple Logic Blocks (LBs) that exchange messages; any time several expressions\nunify under an inference rule, a new fact is emitted with full provenance to\nits sources, yielding replayable, auditable proof graphs.\n  A prototype software implementation instantiates the workflow on first-order\nPeano arithmetic. Starting only from the Peano axioms, GL enumerates candidate\nimplications, applies normalization and type filters, and automatically\nreconstructs machine-checkable proofs of foundational arithmetic laws including\nassociativity and commutativity of addition, associativity and commutativity of\nmultiplication, and distributivity. Generated proofs export to navigable HTML\nso that every inference step can be inspected independently.\n  We outline a hardware-software co-design path toward massively parallel\nrealizations and describe prospective integration with probabilistic models\n(e.g., Large Language Models (LLMs)) for autoformalization and conjecture\nseeding. The Python and MPL code to reproduce the Peano experiments, along with\nthe full HTML proof graphs, are available in the project's GitHub repository at\nhttps://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724\nand are permanently archived at https://doi.org/10.5281/zenodo.16408441. We\ninvite community feedback and collaboration.", "comment": "19 pages, 5 figures. Code and interactive HTML proof graphs\n  permanently archived on Zenodo (DOI: 10.5281/zenodo.16408441)", "pdf_url": "http://arxiv.org/pdf/2508.00017v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2508.00323", "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00323v1", "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00323v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00295", "title": "Reimagining Voltage-Controlled Cryogenic Boolean Logic Paradigm with Quantum-Enhanced Josephson Junction FETs", "authors": ["Md Mazharul Islam", "Diego Ferrer", "Shamiul Alam", "Juan P. Mendez", "Denis Mamaluy", "Wei Pan", "Ahmedullah Aziz"], "categories": ["cs.ET", "cs.AR", "physics.app-ph"], "primary_category": "Subjects:       Emerging Technologies (cs.ET)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00295v1", "summary": "The growing demand for ultra low power computing and the emergence of quantum\ntechnologies have intensified interest in cryogenic electronics, particularly\nsuperconducting devices.Despite their promise, current controlled\nsuperconducting components face fundamental challenges in cascadability,\nlimiting their effectiveness in complex logic architectures.To overcome this,\nrecent efforts have focused on developing gate tunable superconducting devices,\nsuch as Josephson Junction Field Effect Transistors (JJFETs).However, achieving\nrobust control and sufficient supercurrent gain, both critical for\ntransistor-like performance in logic circuits remains a key challenge.A recent\nadvancement in JJFET design, based on InAs and GaSb heterostructures,\ndemonstrates enhanced gain and favorable device characteristics suitable for\ncircuit integration.Building on this innovation, we propose and analyze\nfundamental voltage controlled logic topologies using the quantum enhanced\nJJFET. We develop a Verilog A based circuit compatible compact model of the\nquantum enhanced JJFET which accurately captures the experimentally observed\ndevice characteristics.To ensure cascadability, our logic circuits incorporate\nthe multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based\nthermal switch.Through simulation based analysis, we demonstrate the successful\nimplementation of fundamental logic gates, including NOT, NAND, and NOR.\nFurthermore, we design a 3 input majority gate, which plays a pivotal role in\nquantum and reversible computing due to its universality.Finally, to\ndemonstrate the cascadability of our proposed logic topology, we demonstrate\nthe operation of a 2 input XOR gate based on our designed JJFET based NOT,\nNAND, and NOR gate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00295v1", "cate": "cs.ET", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00324", "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2508.00324v1", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2508.00324v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00441", "title": "DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme", "authors": ["Daichi Mukunoki"], "categories": ["cs.PF", "cs.AR", "cs.MS"], "primary_category": "Subjects:       Performance (cs.PF)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00441v1", "summary": "Since AI computations require low-precision matrix multiplications,\nprocessors with enhanced performance for these operations are increasing along\nwith the growing demand for AI computations. However, it is difficult to use\nthese operations directly for scientific computations. The Ozaki scheme, an\naccurate matrix multiplication method proposed by Ozaki et al. in 2012, enables\nFP64 matrix multiplication (DGEMM) using low-precision floating-point\noperations such as FP16. The method was subsequently extended to utilize\ninteger arithmetic. The use of integer operations reduces computational cost\ncompared to the floating-point based approach. It has also demonstrated higher\nperformance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores\nfor AI workloads. However, the latest hardware tends to enhance low-precision\nfloating-point operation performance such as FP8 instead of INT8. This study\nrevisits the utilization of low-precision floating-point operations in the\nOzaki scheme, considering the latest AI hardware. Specifically, we consider the\nuse of FP6 and FP8 Tensor Cores. Moreover, for processors that support very\nslow FP64 operations or do not support them at all, we consider the use of the\nFP64 emulation based on integer arithmetic. We also examine a new blocking\nstrategy. We demonstrate the effectiveness of these methods by evaluating the\nperformance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell\narchitecture GPU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00441v1", "cate": "cs.PF", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00378", "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "authors": ["Shixin Yi", "Lin Shang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preparing for AAAI 2026, Multimodal Reasoning", "url": "http://arxiv.org/abs/2508.00378v1", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "pdf_url": "http://arxiv.org/pdf/2508.00378v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.15066", "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "categories": ["cs.AR", "cs.MA"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15066v3", "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15066v3", "cate": "cs.AR", "date": "2025-06-18", "updated": "2025-08-01"}
{"id": "2508.00276", "title": "Asymptotically Optimal Inapproximability of E$k$-SAT Reconfiguration", "authors": ["Shuichi Hirahara", "Naoto Ohsaka"], "categories": ["cs.CC", "cs.DM", "cs.DS"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      To appear in Proceedings of the 66th IEEE Symposium on Foundations of Computer Science (FOCS 2025)", "url": "http://arxiv.org/abs/2508.00276v1", "summary": "In the Maxmin E$k$-SAT Reconfiguration problem, we are given a satisfiable\n$k$-CNF formula $\\varphi$ where each clause contains exactly $k$ literals,\nalong with a pair of its satisfying assignments. The objective is transform one\nsatisfying assignment into the other by repeatedly flipping the value of a\nsingle variable, while maximizing the minimum fraction of satisfied clauses of\n$\\varphi$ throughout the transformation. In this paper, we demonstrate that the\noptimal approximation factor for Maxmin E$k$-SAT Reconfiguration is $1 -\n\\Theta\\left(\\frac{1}{k}\\right)$. On the algorithmic side, we develop a\ndeterministic $\\left(1-\\frac{1}{k-1}-\\frac{1}{k}\\right)$-factor approximation\nalgorithm for every $k \\geq 3$. On the hardness side, we show that it is\n$\\mathsf{PSPACE}$-hard to approximate this problem within a factor of\n$1-\\frac{1}{10k}$ for every sufficiently large $k$. Note that an\n``$\\mathsf{NP}$ analogue'' of Maxmin E$k$-SAT Reconfiguration is Max E$k$-SAT,\nwhose approximation threshold is $1-\\frac{1}{2^k}$ shown by H\\r{a}stad (JACM\n2001). To the best of our knowledge, this is the first reconfiguration problem\nwhose approximation threshold is (asymptotically) worse than that of its\n$\\mathsf{NP}$ analogue. To prove the hardness result, we introduce a new\n``non-monotone'' test, which is specially tailored to reconfiguration problems,\ndespite not being helpful in the PCP regime.", "comment": "To appear in Proceedings of the 66th IEEE Symposium on Foundations of\n  Computer Science (FOCS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00276v1", "cate": "cs.CC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00401", "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00401v1", "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00401v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.20401", "title": "Nonlinear Computation with Linear Optics via Source-Position Encoding", "authors": ["N. Richardson", "C. Bosch", "R. P. Adams"], "categories": ["physics.optics", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20401v2", "summary": "Optical computing systems provide an alternate hardware model which appears\nto be aligned with the demands of neural network workloads. However, the\nchallenge of implementing energy efficient nonlinearities in optics -- a key\nrequirement for realizing neural networks -- is a conspicuous missing link. In\nthis work we introduce a novel method to achieve nonlinear computation in fully\nlinear media. Our method can operate at low power and requires only the ability\nto drive the optical system at a data-dependent spatial position. Leveraging\nthis positional encoding, we formulate a fully automated,\ntopology-optimization-based hardware design framework for extremely specialized\noptical neural networks, drawing on modern advancements in optimization and\nmachine learning. We evaluate our optical designs on machine learning\nclassification tasks: demonstrating significant improvements over linear\nmethods, and competitive performance when compared to standard artificial\nneural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20401v2", "cate": "physics.optics", "date": "2025-04-29", "updated": "2025-08-01"}
{"id": "2508.00055", "title": "Are controlled unitaries helpful?", "authors": ["Ewin Tang", "John Wright"], "categories": ["quant-ph", "cs.CC", "cs.DS"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      18 pages", "url": "http://arxiv.org/abs/2508.00055v1", "summary": "Many quantum algorithms, to compute some property of a unitary $U$, require\naccess not just to $U$, but to $cU$, the unitary with a control qubit. We show\nthat having access to $cU$ does not help for a large class of quantum problems.\nFor a quantum circuit which uses $cU$ and $cU^\\dagger$ and outputs\n$|\\psi(U)\\rangle$, we show how to ``decontrol'' the circuit into one which uses\nonly $U$ and $U^\\dagger$ and outputs $|\\psi(\\varphi U)\\rangle$ for a uniformly\nrandom phase $\\varphi$, with a small amount of time and space overhead. When we\nonly care about the output state up to a global phase on $U$, then the\ndecontrolled circuit suffices. Stated differently, $cU$ is only helpful because\nit contains global phase information about $U$.\n  A version of our procedure is described in an appendix of Sheridan, Maslov,\nand Mosca [SMM09]. Our goal with this work is to popularize this result by\ngeneralizing it and investigating its implications, in order to counter\nnegative results in the literature which might lead one to believe that\ndecontrolling is not possible. As an application, we give a simple proof for\nthe existence of unitary ensembles which are pseudorandom under access to $U$,\n$U^\\dagger$, $cU$, and $cU^\\dagger$.", "comment": "18 pages", "pdf_url": "http://arxiv.org/pdf/2508.00055v1", "cate": "quant-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00193", "title": "A Practical Finite Element Approach for Simulating Dynamic Crack Growth in Cu/Ultra Low-k Interconnect Structures", "authors": ["Yuxi Xie", "Ethan J. Wu", "Lu Xu", "Jimmy Perez", "Shaofan Li"], "categories": ["cs.CE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00193v1", "summary": "This work presents a practical finite element modeling strategy, the Crack\nElement Method (CEM), for simulating the dynamic crack propagation in\ntwo-dimensional structures. The method employs an element-splitting algorithm\nbased on the Edge-based Smoothed Finite Element Method (ES-FEM) to capture the\nelement-wise crack growth while reducing the formation of poorly shaped\nelements that can compromise numerical accuracy and computational performance.\nA fracture energy release rate formulation is also developed based on the\nevolving topology of the split elements. The proposed approach is validated\nthrough a series of classical benchmark problems, demonstrating its accuracy\nand robustness in addressing dynamic fracture scenarios. Finally, the\napplicability of the CEM is illustrated in a case study involving patterned\nCu/Ultra Low-k interconnect structures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00193v1", "cate": "cs.CE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00251", "title": "Robust Model Reconstruction Based on the Topological Understanding of Point Clouds Using Persistent Homology", "authors": ["Yu Chen", "Hongwei Lin"], "categories": ["cs.CG"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00251v1", "summary": "Reconstructing models from unorganized point clouds presents a significant\nchallenge, especially when the models consist of multiple components\nrepresented by their surface point clouds. Such models often involve point\nclouds with noise that represent multiple closed surfaces with shared regions,\nmaking their automatic identification and separation inherently complex. In\nthis paper, we propose an automatic method that uses the topological\nunderstanding provided by persistent homology, along with representative\n2-cycles of persistent homology groups, to effectively distinguish and separate\neach closed surface. Furthermore, we employ Loop subdivision and least squares\nprogressive iterative approximation (LSPIA) techniques to generate high-quality\nfinal surfaces and achieve complete model reconstruction. Our method is robust\nto noise in the point cloud, making it suitable for reconstructing models from\nsuch data. Experimental results demonstrate the effectiveness of our approach\nand highlight its potential for practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00251v1", "cate": "cs.CG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00414", "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2508.00414v1", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2508.00414v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00396", "title": "Proof complexity of Mal'tsev CSP", "authors": ["Azza Gaysin"], "categories": ["math.LO", "cs.CC"], "primary_category": "Subjects:       Logic (math.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00396v1", "summary": "Constraint Satisfaction Problems (CSPs) form a broad class of combinatorial\nproblems, which can be formulated as homomorphism problems between relational\nstructures. The CSP dichotomy theorem classifies all such problems over finite\ndomains into two categories: NP-complete and polynomial-time, see Zhuk (2017),\nBulatov (2017). Polynomial-time CSPs can be further subdivided into smaller\nsubclasses. Mal'tsev CSPs are defined by the property that every relation in\nthe problem is invariant under a Mal'tsev operation, a ternary operation $\\mu$\nsatisfying $\\mu(x, y, y) = \\mu(y, y, x) = x$ for all $x, y$. Bulatov and Dalmau\nproved that Mal'tsev CSPs are solvable in polynomial time, presenting an\nalgorithm for such CSPs (2006). The negation of an unsatisfiable CSP instance\ncan be expressed as a propositional tautology. We formalize the algorithm for\nMal'tsev CSPs within bounded arithmetic $V^1$, which captures polynomial-time\nreasoning and corresponds to the extended Frege proof system. We show that\n$V^1$ proves the soundness of Mal'tsev algorithm, implying that tautologies\nexpressing the non-existence of a solution for unsatisfiable instances of\nMal'tsev CSPs admit short extended Frege proofs. In addition, with small\nadjustments, we achieved an analogous result for Dalmau's algorithm that solves\ngeneralized majority-minority CSPs -- a common generalization of near-unanimity\noperations and Mal'tsev operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00396v1", "cate": "math.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00451", "title": "WeightFlow: Learning Stochastic Dynamics via Evolving Weight of Neural Network", "authors": ["Ruikun Li", "Jiazhen Liu", "Huandong Wang", "Qingmin Liao", "Yong Li"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00451v1", "summary": "Modeling stochastic dynamics from discrete observations is a key\ninterdisciplinary challenge. Existing methods often fail to estimate the\ncontinuous evolution of probability densities from trajectories or face the\ncurse of dimensionality. To address these limitations, we presents a novel\nparadigm: modeling dynamics directly in the weight space of a neural network by\nprojecting the evolving probability distribution. We first theoretically\nestablish the connection between dynamic optimal transport in measure space and\nan equivalent energy functional in weight space. Subsequently, we design\nWeightFlow, which constructs the neural network weights into a graph and learns\nits evolution via a graph controlled differential equation. Experiments on\ninterdisciplinary datasets demonstrate that WeightFlow improves performance by\nan average of 43.02\\% over state-of-the-art methods, providing an effective and\nscalable solution for modeling high-dimensional stochastic dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00451v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00091", "title": "Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence", "authors": ["Chandler Smith", "HanQin Cai", "Abiy Tasissa"], "categories": ["math.OC", "cs.CG", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      54 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00091v1", "summary": "The problem of recovering a configuration of points from partial pairwise\ndistances, referred to as the Euclidean Distance Geometry (EDG) problem, arises\nin a broad range of applications, including sensor network localization,\nmolecular conformation, and manifold learning. In this paper, we propose a\nRiemannian optimization framework for solving the EDG problem by formulating it\nas a low-rank matrix completion task over the space of positive semi-definite\nGram matrices. The available distance measurements are encoded as expansion\ncoefficients in a non-orthogonal basis, and optimization over the Gram matrix\nimplicitly enforces geometric consistency through the triangle inequality, a\nstructure inherited from classical multidimensional scaling. Under a Bernoulli\nsampling model for observed distances, we prove that Riemannian gradient\ndescent on the manifold of rank-$r$ matrices locally converges linearly with\nhigh probability when the sampling probability satisfies $p \\geq\n\\mathcal{O}(\\nu^2 r^2 \\log(n)/n)$, where $\\nu$ is an EDG-specific incoherence\nparameter. Furthermore, we provide an initialization candidate using a one-step\nhard thresholding procedure that yields convergence, provided the sampling\nprobability satisfies $p \\geq \\mathcal{O}(\\nu r^{3/2} \\log^{3/4}(n)/n^{1/4})$.\nA key technical contribution of this work is the analysis of a symmetric linear\noperator arising from a dual basis expansion in the non-orthogonal basis, which\nrequires a novel application of the Hanson--Wright inequality to establish an\noptimal restricted isometry property in the presence of coupled terms.\nEmpirical evaluations on synthetic data demonstrate that our algorithm achieves\ncompetitive performance relative to state-of-the-art methods. Moreover, we\npropose a novel notion of matrix incoherence tailored to the EDG setting and\nprovide robustness guarantees for our method.", "comment": "54 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00091v1", "cate": "math.OC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00459", "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00459v1", "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00459v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.07524", "title": "Finding One Local Optimum Is Easy -- But What about Two?", "authors": ["Yasuaki Kobayashi", "Kazuhiro Kurita", "Yutaro Yamaguchi"], "categories": ["cs.DS", "cs.CC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.07524v3", "summary": "The class PLS (Polynomial Local Search) captures the complexity of finding a\nsolution that is locally optimal and has proven to be an important concept in\nthe theory of local search. It has been shown that local search versions of\nvarious combinatorial optimization problems, such as Maximum Independent Set\nand Max Cut, are complete for this class. Such computational intractability\ntypically arises in local search problems allowing arbitrary weights; in\ncontrast, for unweighted problems, locally optimal solutions can be found in\npolynomial time under standard settings. In this paper, we pursue the\ncomplexity of local search problems from a different angle: We show that\ncomputing two locally optimal solutions is NP-hard for various natural\nunweighted local search problems, including Maximum Independent Set, Minimum\nDominating Set, Max SAT, and Max Cut. We also discuss several tractable cases\nfor finding two (or more) local optimal solutions.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.07524v3", "cate": "cs.DS", "date": "2025-07-10", "updated": "2025-08-01"}
{"id": "2508.00654", "title": "LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources", "authors": ["Rodrigo Escobar Díaz Guerrero", "Jamile Mohammad Jafari", "Tobias Meyer-Zedler", "Michael Schmitt", "Juergen Popp", "Thomas Bocklitz"], "categories": ["cs.CE", "cs.SE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00654v1", "summary": "In the interdisciplinary field of microscopy research, managing and\nintegrating large volumes of data stored across disparate platforms remains a\nmajor challenge. Data types such as bioimages, experimental records, and\nspectral information are often maintained in separate repositories, each\nfollowing different management standards. However, linking these data sources\nacross the research lifecycle is essential to align with the FAIR principles of\ndata management: Findability, Accessibility, Interoperability, and Reusability.\nDespite this need, there is a notable lack of tools capable of effectively\nintegrating and linking data from heterogeneous sources. To address this gap,\nwe present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based\nplatform designed to create and manage links between distributed data systems.\nLEO was initially developed to link objects between Electronic Lab Notebooks\n(ELNs) and OMERO, but its functionality has since been extended through a\nplugin-based architecture, allowing the integration of additional data sources.\nThis extensibility makes LEO a scalable and flexible solution for a wide range\nof microscopy research workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00654v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00192", "title": "On the Undecidability of Tiling the $3$-dimensional Space with a Set of $3$ Polycubes", "authors": ["Chao Yang", "Zhujun Zhang"], "categories": ["math.CO", "cs.CG", "math.MG"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      in Chinese language", "url": "http://arxiv.org/abs/2508.00192v1", "summary": "Translational tiling problems are among the most fundamental and\nrepresentative undecidable problems in all fields of mathematics. Greenfeld and\nTao obtained two remarkable results on the undecidability of translational\ntiling in recent years. One is the existence of an aperiodic monotile in a\nspace of sufficiently large dimension. The other is the undecidability of\ntranslational tiling of periodic subsets of space with a single tile, provided\nthat the dimension of the space is part of the input. These two results support\nthe following conjecture: there is a fixed dimension $n$ such that\ntranslational tiling with a single tile is undecidable. One strategy towards\nsolving this conjecture is to prove the undecidability of translational tiling\nof a fixed dimension space with a set of $k$ tiles, for a positive integer $k$\nas small as possible. In this paper, it is shown that translational tiling the\n$3$-dimensional space with a set of $3$ polycubes is undecidable.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2508.00192v1", "cate": "math.CO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00500", "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00500v1", "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00500v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21258", "title": "Verification Cost Asymmetry in Cognitive Warfare: A Complexity-Theoretic Framework", "authors": ["Joshua Luberisse"], "categories": ["cs.CR", "cs.CC", "cs.CY", "cs.GT", "F.0; H.0"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21258v2", "summary": "Human verification under adversarial information flow operates as a\ncost-bounded decision procedure constrained by working memory limits and\ncognitive biases. We introduce the Verification Cost Asymmetry (VCA)\ncoefficient, formalizing it as the ratio of expected verification work between\npopulations under identical claim distributions. Drawing on probabilistically\ncheckable proofs (PCP) and parameterized complexity theory, we construct\ndissemination protocols that reduce verification for trusted audiences to\nconstant human effort while imposing superlinear costs on adversarial\npopulations lacking cryptographic infrastructure. We prove theoretical\nguarantees for this asymmetry, validate the framework through controlled user\nstudies measuring verification effort with and without spot-checkable\nprovenance, and demonstrate practical encoding of real-world information\ncampaigns. The results establish complexity-theoretic foundations for\nengineering democratic advantage in cognitive warfare, with immediate\napplications to content authentication, platform governance, and information\noperations doctrine.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21258v2", "cate": "cs.CR", "date": "2025-07-28", "updated": "2025-08-01"}
{"id": "2508.00773", "title": "Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery", "authors": ["Jiankai Tang", "Meng Kang", "Yiru Zhang", "Kegang Wang", "Daniel Mcduff", "Xin Liu", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.CE", "cs.HC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      UbiComp 25", "url": "http://arxiv.org/abs/2508.00773v1", "summary": "Cardiorespiratory coupling (CRC) captures the dynamic interaction between the\ncardiac and respiratory systems--an interaction strengthened by physical\nexercise and linked to improved physiological function. We examined CRC at high\naltitude in two states, rest and post-exercise recovery, and found significant\ndifferences (p < 0.05). Quantitative analysis revealed that recovery involved\nmore frequent yet less stable episodes of synchronization between respiration\nand pulse. Furthermore, we explored the feasibility of non-contact CRC\nmeasurement with remote photoplethysmography (rPPG), observing a strong\ncorrelation with oximeter-based metrics (Pearson r = 0.96). These findings\nhighlight the potential of CRC as a sensitive marker for autonomic regulation\nand its future application in contactless monitoring. Source code is available\nat GitHub: https://github.com/McJackTang/CRC.", "comment": "UbiComp 25", "pdf_url": "http://arxiv.org/pdf/2508.00773v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00269", "title": "chipfiring: A Python Package for Efficient Mathematical Analysis of Chip-Firing Games on Multigraphs", "authors": ["Dhyey Dharmendrakumar Mavani", "Tairan Ji", "Nathan Pflueger"], "categories": ["math.CO", "cs.CG", "cs.MS", "math.AG"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00269v2", "summary": "This paper presents `chipfiring`, a comprehensive Python package for the\nmathematical analysis of chip-firing games on finite graphs. The package\nprovides a robust toolkit for defining graphs and chip configurations\n(divisors), performing chip-firing operations, and analyzing fundamental\nproperties such as winnability, linear equivalence, and divisor rank. We detail\nthe core components of the library, including its object-oriented graph and\ndivisor implementations, integrated Laplacian matrix computations, and an\nefficient implementation of Dhar's algorithm for determining the solvability of\nthe dollar game. The `chipfiring` package is designed for researchers and\nstudents in graph theory, combinatorics, and algebraic geometry, providing\nessential algorithms and data structures for exploring these rich mathematical\nmodels. We describe the library's architecture, illustrate its usage with\ncomprehensive examples, and highlight its specialized contributions compared to\ngeneral-purpose graph libraries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00269v2", "cate": "math.CO", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2508.00576", "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "authors": ["Zhanliang Wang", "Kai Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00576v1", "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00576v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00778", "title": "τ-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing", "authors": ["Jiankai Tang", "Zhe He", "Mingyu Zhang", "Wei Geng", "Chengchi Zhou", "Weinan Shi", "Yuanchun Shi", "Yuntao Wang"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      UbiComp 25", "url": "http://arxiv.org/abs/2508.00778v1", "summary": "Smart rings have emerged as uniquely convenient devices for continuous\nphysiological and behavioral sensing, offering unobtrusive, constant access to\nmetrics such as heart rate, motion, and skin temperature. Yet most commercial\nsolutions remain proprietary, hindering reproducibility and slowing innovation\nin wearable research. We introduce {\\tau}-Ring, a commercial-ready platform\nthat bridges this gap through: (i) accessible hardware combining\ntime-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and\non-board storage; (ii) adjustable firmware that lets researchers rapidly\nreconfigure sampling rates, power modes, and wireless protocols; and (iii) a\nfully open-source Android software suite that supports both real-time streaming\nand 8-hour offline logging. Together, these features enable out-of-the-box,\nreproducible acquisition of rich physiological and behavioral datasets,\naccelerating prototyping and standardizing experimentation. We validate the\nplatform with demonstration studies in heart-rate monitoring and ring-based\nhandwriting recognition. Source code is available at GitHub:\nhttps://github.com/thuhci/OpenRing.", "comment": "UbiComp 25", "pdf_url": "http://arxiv.org/pdf/2508.00778v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.03865", "title": "Dudeney's Dissection is Optimal", "authors": ["Erik D. Demaine", "Tonan Kamata", "Ryuhei Uehara"], "categories": ["cs.CG", "cs.DM", "math.GT"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "Comments:      26 pages, 32 figures. The previous version mistakenly compiled an outdated file. This update corrects that and includes the intended version with refined and corrected case analysis of cut graphs", "url": "http://arxiv.org/abs/2412.03865v4", "summary": "In 1907, Henry Ernest Dudeney posed a puzzle: ``cut any equilateral triangle\n\\dots\\ into as few pieces as possible that will fit together and form a perfect\nsquare'' (without overlap, via translation and rotation).\n  Four weeks later, Dudeney demonstrated a beautiful four-piece solution, which\ntoday remains perhaps the most famous example of dissection.\n  In this paper (over a century later), we finally solve Dudeney's puzzle, by\nproving that the equilateral triangle and square have no common dissection with\nthree or fewer polygonal pieces.\n  We reduce the problem to the analysis of discrete graph structures\nrepresenting the correspondence between the edges and the vertices of the\npieces forming each polygon.", "comment": "26 pages, 32 figures. The previous version mistakenly compiled an\n  outdated file. This update corrects that and includes the intended version\n  with refined and corrected case analysis of cut graphs", "pdf_url": "http://arxiv.org/pdf/2412.03865v4", "cate": "cs.CG", "date": "2024-12-05", "updated": "2025-08-01"}
{"id": "2508.00581", "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages, 10 figures", "url": "http://arxiv.org/abs/2508.00581v1", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "comment": "16 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2508.00581v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00804", "title": "Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models", "authors": ["Julian Lemmel", "Manuel Kranzl", "Adam Lamine", "Philipp Neubauer", "Radu Grosu", "Sophie Neubauer"], "categories": ["cs.CE", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2508.00804v1", "summary": "This paper introduces a new approach for fine-tuning the predictions of\nstructured state space models (SSMs) at inference time using real-time\nrecurrent learning. While SSMs are known for their efficiency and long-range\nmodeling capabilities, they are typically trained offline and remain static\nduring deployment. Our method enables online adaptation by continuously\nupdating model parameters in response to incoming data. We evaluate our\napproach for linear-recurrent-unit SSMs using a small carbon emission dataset\ncollected from embedded automotive hardware. Experimental results show that our\nmethod consistently reduces prediction error online during inference,\ndemonstrating its potential for dynamic, resource-constrained environments.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2508.00804v1", "cate": "cs.CE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.23025", "title": "Simplification of Trajectory Streams", "authors": ["Siu-Wing Cheng", "Haoqiang Huang", "Le Jiang"], "categories": ["cs.CG"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "Comments:      SoCG 2025", "url": "http://arxiv.org/abs/2503.23025v2", "summary": "While there are software systems that simplify trajectory streams on the fly,\nfew curve simplification algorithms with quality guarantees fit the streaming\nrequirements. We present streaming algorithms for two such problems under the\nFr\\'{e}chet distance $d_F$ in $\\mathbb{R}^d$ for some constant $d \\geq 2$.\n  Consider a polygonal curve $\\tau$ in $\\mathbb{R}^d$ in a stream. We present a\nstreaming algorithm that, for any $\\varepsilon\\in (0,1)$ and $\\delta > 0$,\nproduces a curve $\\sigma$ such that $d_F(\\sigma,\\tau[v_1,v_i])\\le\n(1+\\varepsilon)\\delta$ and $|\\sigma|\\le 2\\,\\mathrm{opt}-2$, where\n$\\tau[v_1,v_i]$ is the prefix in the stream so far, and $\\mathrm{opt} =\n\\min\\{|\\sigma'|: d_F(\\sigma',\\tau[v_1,v_i])\\le \\delta\\}$. Let $\\alpha =\n2(d-1){\\lfloor d/2 \\rfloor}^2 + d$. The working storage is\n$O(\\varepsilon^{-\\alpha})$. Each vertex is processed in\n$O(\\varepsilon^{-\\alpha}\\log\\frac{1}{\\varepsilon})$ time for $d \\in \\{2,3\\}$\nand $O(\\varepsilon^{-\\alpha})$ time for $d \\geq 4$ . Thus, the whole $\\tau$ can\nbe simplified in $O(\\varepsilon^{-\\alpha}|\\tau|\\log\\frac{1}{\\varepsilon})$\ntime. Ignoring polynomial factors in $1/\\varepsilon$, this running time is a\nfactor $|\\tau|$ faster than the best static algorithm that offers the same\nguarantees.\n  We present another streaming algorithm that, for any integer $k \\geq 2$ and\nany $\\varepsilon \\in (0,\\frac{1}{17})$, maintains a curve $\\sigma$ such that\n$|\\sigma| \\leq 2k-2$ and $d_F(\\sigma,\\tau[v_1,v_i])\\le (1+\\varepsilon) \\cdot\n\\min\\{d_F(\\sigma',\\tau[v_1,v_i]): |\\sigma'| \\leq k\\}$, where $\\tau[v_1,v_i]$ is\nthe prefix in the stream so far. The working storage is\n$O((k\\varepsilon^{-1}+\\varepsilon^{-(\\alpha+1)})\\log \\frac{1}{\\varepsilon})$.\nEach vertex is processed in\n$O(k\\varepsilon^{-(\\alpha+1)}\\log^2\\frac{1}{\\varepsilon})$ time for $d \\in\n\\{2,3\\}$ and $O(k\\varepsilon^{-(\\alpha+1)}\\log\\frac{1}{\\varepsilon})$ time for\n$d \\geq 4$.", "comment": "SoCG 2025", "pdf_url": "http://arxiv.org/pdf/2503.23025v2", "cate": "cs.CG", "date": "2025-03-29", "updated": "2025-08-01"}
{"id": "2508.00632", "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "authors": ["Alexia Jolicoeur-Martineau"], "categories": ["cs.AI", "cs.MA", "cs.MM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00632v1", "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00632v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00023", "title": "Weak Values as Geometric Lenses: Deformations of Hilbert Space and the Emergence of superoscillations", "authors": ["Mirco A. Mannucci"], "categories": ["quant-ph", "cs.CE"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 1 figure", "url": "http://arxiv.org/abs/2508.00023v1", "summary": "The formalism of weak measurement in quantum mechanics has revealed profound\nconnections between measurement theory, quantum foundations, and signal\nprocessing. In this paper, we develop a pointer-free derivation of\nsuperoscillations, demonstrating that they are a natural and necessary\nconsequence of the geometric structure underlying weak values. We argue that\nthe weak value is best understood as a ratio of geometric deformation,\nquantifying how an observable transforms the structure of Hilbert space\nrelative to a reference provided by the standard inner product. This\ndeformation acts as a conceptual lens, warping the local structure of quantum\nstates to produce oscillations far exceeding the global Fourier bandwidth. We\nformalize this by interpreting the weak value as a comparison between a\ndeformed sesquilinear form and the standard one, and explore its deep\nconnections to generalized Rayleigh quotients and the projective geometry of\nquantum states. This perspective unifies weak values and superoscillations as\ntwo facets of a single underlying geometric principle.", "comment": "12 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2508.00023v1", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2506.18725", "title": "TopoRec: Point Cloud Recognition Using Topological Data Analysis", "authors": ["Anirban Ghosh", "Iliya Kulbaka", "Ian Dahlin", "Ayan Dutta"], "categories": ["cs.RO", "cs.CG", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18725v2", "summary": "Point cloud-based object/place recognition remains a problem of interest in\napplications such as autonomous driving, scene reconstruction, and\nlocalization. Extracting a meaningful global descriptor from a query point\ncloud that can be matched with the descriptors of the database point clouds is\na challenging problem. Furthermore, when the query point cloud is noisy or has\nbeen transformed (e.g., rotated), it adds to the complexity. To this end, we\npropose a novel methodology, named TopoRec, which utilizes Topological Data\nAnalysis (TDA) for extracting local descriptors from a point cloud, thereby\neliminating the need for resource-intensive GPU-based machine learning\ntraining. More specifically, we used the ATOL vectorization method to generate\nvectors for point clouds. To test the quality of the proposed TopoRec\ntechnique, we have implemented it on multiple real-world (e.g., Oxford\nRobotCar, NCLT) and realistic (e.g., ShapeNet) point cloud datasets for\nlarge-scale place and object recognition, respectively. Unlike existing\nlearning-based approaches such as PointNetVLAD and PCAN, our method does not\nrequire extensive training, making it easily adaptable to new environments.\nDespite this, it consistently outperforms both state-of-the-art learning-based\nand handcrafted baselines (e.g., M2DP, ScanContext) on standard benchmark\ndatasets, demonstrating superior accuracy and strong generalization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18725v2", "cate": "cs.RO", "date": "2025-06-23", "updated": "2025-08-01"}
{"id": "2508.00658", "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      First draft", "url": "http://arxiv.org/abs/2508.00658v1", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "comment": "First draft", "pdf_url": "http://arxiv.org/pdf/2508.00658v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00381", "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "authors": ["Kamal Basha S", "Athira Nambiar"], "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00381v1", "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00381v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00665", "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00665v1", "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00665v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00383", "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "url": "http://arxiv.org/abs/2508.00383v1", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "pdf_url": "http://arxiv.org/pdf/2508.00383v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00079", "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Under review, 18 pages, 4 figures, 7 tables", "url": "http://arxiv.org/abs/2508.00079v1", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "comment": "Under review, 18 pages, 4 figures, 7 tables", "pdf_url": "http://arxiv.org/pdf/2508.00079v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00293", "title": "ranDecepter: Real-time Identification and Deterrence of Ransomware Attacks", "authors": ["Md Sajidul Islam Sajid", "Jinpeng Wei", "Ehab Al-Shaer"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Conference on Communications and Network Security (CNS) 2025", "url": "http://arxiv.org/abs/2508.00293v2", "summary": "Ransomware (RW) presents a significant and widespread threat in the digital\nlandscape, necessitating effective countermeasures. Active cyber deception is a\npromising strategy to thwart RW and limiting its propagation by misleading it\nwith false information and revealing its true behaviors. Furthermore, RW often\nacts as a communication conduit between attackers and defenders, allowing\ndeception to return false data to attackers and deplete their resources. This\npaper introduces ranDecepter, a novel approach that combines active cyber\ndeception with real-time analysis to enhance defenses against RW attacks. The\nranDecepter identifies RW in real-time and isolates it within a deceptive\nenvironment, autonomously identifying critical elements in the RW code to\ncreate a loop mechanism. By repeatedly restarting the malware and transmitting\ncounterfeit encryption information and secret keys to the attacker, it forces\nthe attacker to store these fabricated details for each victim, thereby\ndepleting their resources. Our comprehensive evaluation of ranDecepter,\nconducted using 1,134 real-world malware samples and twelve benign\napplications, demonstrates a remarkable 100% accuracy in RW identification,\nwith no false positives and minimal impact on response times. Furthermore,\nwithin 24-hours, ranDecepter generates up to 9,223K entries in the attacker's\ndatabase using 50 agents, showcasing its potential to undermine attacker\nresources.", "comment": "Accepted at IEEE Conference on Communications and Network Security\n  (CNS) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00293v2", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2508.00674", "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "categories": ["cs.AI", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00674v1", "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00674v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2401.12073", "title": "The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach", "authors": ["Nikola Bešinović", "Ricardo García-Ródenas", "María Luz López-García", "Julio Alberto López-Gómez", "José Ángel Martín-Baos"], "categories": ["cs.CE", "90-05", "I.6; J.6"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      33 pages and 6 figures", "url": "http://arxiv.org/abs/2401.12073v2", "summary": "The liberalisation of the European passenger railway markets through the\nEuropean Directive EU 91/440/EEC states a new scenario where different Railway\nUndertakings compete with each other in a bidding process for time slots. The\ninfrastructure resources are provided by the Infrastructure Manager, who\nanalyses and assesses the bids received, allocating the resources to each\nRailway Undertaking. Time slot allocation is a fact that drastically influences\nthe market equilibrium. In this paper, we address the time slot allocation\nproblem within the context of a liberalized passenger railway market as a\nmulti-objective model. The Infrastructure Manager is tasked with selecting a\npoint from the Pareto front as the solution to the time slot allocation\nproblem. We propose two criteria for making this selection: the first one\nallocates time slots to each company according to a set of priorities, while\nthe second one introduces a criterion of fairness in the treatment of companies\nto incentive competition. The assessment of the impact of these rules on market\nequilibrium has been conducted on a liberalized high-speed corridor within the\nSpanish railway network.", "comment": "33 pages and 6 figures", "pdf_url": "http://arxiv.org/pdf/2401.12073v2", "cate": "cs.CE", "date": "2024-01-22", "updated": "2025-07-31"}
{"id": "2508.00086", "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      35 pages; includes abstract", "url": "http://arxiv.org/abs/2508.00086v1", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "comment": "35 pages; includes abstract", "pdf_url": "http://arxiv.org/pdf/2508.00086v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00351", "title": "Cryptanalysis of Isogeny-Based Quantum Money with Rational Points", "authors": ["Hyeonhak Kim", "Donghoe Heo", "Seokhie Hong"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00351v1", "summary": "Quantum money is the cryptographic application of the quantum no-cloning\ntheorem. It has recently been instantiated by Montgomery and Sharif (Asiacrypt\n'24) from class group actions on elliptic curves. In this work, we propose a\nconcrete cryptanalysis by leveraging the efficiency of evaluating division\npolynomials with the coordinates of rational points, offering a speedup of\nO(log^4p) compared to the brute-force attack. Since our attack still requires\nexponential time, it remains impractical to forge a quantum banknote.\nInterestingly, due to the inherent properties of quantum money, our attack\nmethod also results in a more efficient verification procedure. Our algorithm\nleverages the properties of quadratic twists to utilize rational points in\nverifying the cardinality of the superposition of elliptic curves. We expect\nthis approach to contribute to future research on elliptic-curve-based quantum\ncryptography.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00351v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00784", "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "authors": ["Tom Or", "Omri Azencot"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00784v1", "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00784v1", "cate": "cs.AI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.04798", "title": "A Multi-physics Model of Flow from Coronary Angiography: Insights to Microvascular Function", "authors": ["Haizhou Yang", "Jiyang Zhang", "Ismael Z. Assi", "Brahmajee K. Nallamothu", "Krishna Garikipati", "C. Alberto Figueroa"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      29 pages, 14 figures", "url": "http://arxiv.org/abs/2412.04798v2", "summary": "Coronary Microvascular Dysfunction (CMD) is characterized by impaired\nvasodilation and can lead to insufficient blood flow to the myocardium during\nstress or exertion, affecting millions of people globally. Despite their\ndiagnostic value, invasive, wire-based diagnosis techniques of CMD, such as\nindex of microcirculatory resistance (IMR) and coronary flow reserve (CFR), are\nunderutilized due to their complexity and inconsistency. Coronary angiography,\none of the most commonly used imaging modalities, offers valuable flow\ninformation that assists in diagnosing CMD. However, this information is not\nfully understood or utilized in current clinical practice. In this study, a\n3D-0D coupled multi-physics computational fluid dynamics (CFD) model was\ndeveloped and calibrated to simulate and study the process of contrast\ninjection and washout during clinical angiography. A contrast intensity profile\n(CIP) was introduced to describe the dynamics of coronary angiography data.\nAdditionally, sensitivity studies were conducted to evaluate the influence of\nvarious coronary lumped parameter model (LPM) parameters on the shapes of CIPs.\nThe results demonstrate that the multi-physics model can be effectively\ncalibrated to produce physiologically meaningful hemodynamic results.\nSensitivity studies reveal that resistance has a greater impact on the rising\nand falling slopes of CIP than capacitance, with higher resistance amplifying\nthis effect. The model and results are presented here. These results are\npotentially transformative, as they provide a tool for interpreting\nangiographic data and ultimately extracting information concerning coronary\nmicrocirculation.", "comment": "29 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2412.04798v2", "cate": "cs.CE", "date": "2024-12-06", "updated": "2025-07-31"}
{"id": "2508.00095", "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "authors": ["Zachary K. Stine", "James E. Deitrick"], "categories": ["cs.CL", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint. Manuscript currently under review", "url": "http://arxiv.org/abs/2508.00095v1", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "comment": "Preprint. Manuscript currently under review", "pdf_url": "http://arxiv.org/pdf/2508.00095v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00368", "title": "Preliminary Investigation into Uncertainty-Aware Attack Stage Classification", "authors": ["Alessandro Gaudenzi", "Lorenzo Nodari", "Lance Kaplan", "Alessandra Russo", "Murat Sensoy", "Federico Cerutti"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy, co-located with ECAI2025", "url": "http://arxiv.org/abs/2508.00368v1", "summary": "Advanced Persistent Threats (APTs) represent a significant challenge in\ncybersecurity due to their prolonged, multi-stage nature and the sophistication\nof their operators. Traditional detection systems typically focus on\nidentifying malicious activity in binary terms (benign or malicious) without\naccounting for the progression of an attack. However, effective response\nstrategies depend on accurate inference of the attack's current stage, as\ncountermeasures must be tailored to whether an adversary is in the early\nreconnaissance phase or actively conducting exploitation or exfiltration. This\nwork addresses the problem of attack stage inference under uncertainty, with a\nfocus on robustness to out-of-distribution (OOD) inputs. We propose a\nclassification approach based on Evidential Deep Learning (EDL), which models\npredictive uncertainty by outputting parameters of a Dirichlet distribution\nover possible stages. This allows the system not only to predict the most\nlikely stage of an attack but also to indicate when it is uncertain or the\ninput lies outside the training distribution. Preliminary experiments in a\nsimulated environment demonstrate that the proposed model can accurately infer\nthe stage of an attack with calibrated confidence while effectively detecting\nOOD inputs, which may indicate changes in the attackers' tactics. These results\nsupport the feasibility of deploying uncertainty-aware models for staged threat\ndetection in dynamic and adversarial environments.", "comment": "Proceedings for SPAIML2025 workshop, 26/10/2025 Bologna Italy,\n  co-located with ECAI2025", "pdf_url": "http://arxiv.org/pdf/2508.00368v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.18148", "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.18148v1", "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.18148v1", "cate": "cs.CL", "date": "2025-02-25", "updated": "2025-02-25"}
{"id": "2411.13951", "title": "PATH: A Discrete-sequence Dataset for Evaluating Online Unsupervised Anomaly Detection Approaches for Multivariate Time Series", "authors": ["Lucas Correia", "Jan-Christoph Goos", "Thomas Bäck", "Anna V. Kononova"], "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to the Big Data Research journal", "url": "http://arxiv.org/abs/2411.13951v5", "summary": "Benchmarking anomaly detection approaches for multivariate time series is a\nchallenging task due to a lack of high-quality datasets. Current publicly\navailable datasets are too small, not diverse and feature trivial anomalies,\nwhich hinders measurable progress in this research area. We propose a solution:\na diverse, extensive, and non-trivial dataset generated via state-of-the-art\nsimulation tools that reflects realistic behaviour of an automotive powertrain,\nincluding its multivariate, dynamic and variable-state properties.\nAdditionally, our dataset represents a discrete-sequence problem, which remains\nunaddressed by previously-proposed solutions in literature. To cater for both\nunsupervised and semi-supervised anomaly detection settings, as well as time\nseries generation and forecasting, we make different versions of the dataset\navailable, where training and test subsets are offered in contaminated and\nclean versions, depending on the task. We also provide baseline results from a\nselection of approaches based on deterministic and variational autoencoders, as\nwell as a non-parametric approach. As expected, the baseline experimentation\nshows that the approaches trained on the semi-supervised version of the dataset\noutperform their unsupervised counterparts, highlighting a need for approaches\nmore robust to contaminated training data. Furthermore, results show that the\nthreshold used can have a large influence on detection performance, hence more\nwork needs to be invested in methods to find a suitable threshold without the\nneed for labelled data.", "comment": "Submitted to the Big Data Research journal", "pdf_url": "http://arxiv.org/pdf/2411.13951v5", "cate": "cs.LG", "date": "2024-11-21", "updated": "2025-07-31"}
{"id": "2508.00109", "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00109v1", "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00109v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00434", "title": "Accurate Latent Inversion for Generative Image Steganography via Rectified Flow", "authors": ["Yuqi Qian", "Yun Cao", "Meiyang Lv", "Haocheng Fu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00434v1", "summary": "Steganography based on diffusion models has attracted increasing attention\ndue to its ability to generate high-quality images and exhibit strong\nrobustness. In such approaches, the secret message is first embedded into the\ninitial latent variable, and then the stego image is generated through the\nforward process. To extract the message, an inversion process is required to\nreconstruct the latent variables from the received image. However, inaccurate\nlatent inversion leads to significant discrepancies between the reconstructed\nand original latent variables, rendering message extraction infeasible. To\naddress this issue, we propose \\textbf{RF-Stego}, a novel generative image\nsteganography method that enables accurate latent inversion and significantly\nimproves extraction accuracy. First, we develop the \\textbf{P}ath\n\\textbf{C}onsistency \\textbf{L}inear \\textbf{I}nversion (\\textbf{PCLI}), which\nimposes formal constraints on the inversion process. By explicitly aligning it\nwith the forward generation path and modeling both directions along a shared\nlinear path, PCLI eliminates path mismatch and ensures path consistency\nthroughout the steganographic process. Second, through rigorous theoretical\nproof, we demonstrate that \\textbf{R}ectified \\textbf{F}low \\textbf{(RF)}\noffers both theoretical reversibility and numerical stability in the inversion\nprocess. Based on this, we replace traditional unstable samplers with RF\nsampler which effectively improves the numerical precision of the inversion\nprocess. Experimental results show RF-Stego outperforms state-of-the-art\nmethods in terms of extraction accuracy, image quality, robustness, security\nand generation efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00434v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23585", "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "categories": ["cs.HC", "cs.AI", "cs.MM", "cs.SI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To appear in: Adjunct Proceedings of the 36th ACM Conference on Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "url": "http://arxiv.org/abs/2507.23585v1", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.", "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "pdf_url": "http://arxiv.org/pdf/2507.23585v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00121", "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "authors": ["Xiao Zhang", "Johan bos"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by 16th IWCS", "url": "http://arxiv.org/abs/2508.00121v1", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "comment": "Accepted by 16th IWCS", "pdf_url": "http://arxiv.org/pdf/2508.00121v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00478", "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization", "authors": ["Yuning Jiang", "Nay Oo", "Qiaoran Meng", "Lu Lin", "Dusit Niyato", "Zehui Xiong", "Hoon Wei Lim", "Biplab Sikdar"], "categories": ["cs.CR", "cs.AI", "91A10, 91A43, 68T01, 94A60", "C.2.0; I.2.6; K.6.5"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00478v1", "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00478v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00005", "title": "Modelling Program Spaces in Program Synthesis with Constraints", "authors": ["Tilman Hinnerichs", "Bart Swinkels", "Jaap de Jong", "Reuben Gardos Reid", "Tudor Magirescu", "Neil Yorke-Smith", "Sebastijan Dumancic"], "categories": ["cs.PL", "cs.AI"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00005v1", "summary": "A core challenge in program synthesis is taming the large space of possible\nprograms. Since program synthesis is essentially a combinatorial search, the\ncommunity has sought to leverage powerful combinatorial constraint solvers.\nHere, constraints are used to express the program semantics, but not as a\npotentially potent tool to remove unwanted programs. Recent inductive logic\nprogramming approaches introduce constraints on the program's syntax to be\nsynthesized. These syntactic constraints allow for checking and propagating a\nconstraint without executing the program, and thus for arbitrary operators. In\nthis work, we leverage syntactic constraints to model program spaces, defining\nnot just solutions that are feasible, but also ones that are likely useful. To\ndemonstrate this idea, we introduce BART, a solver that efficiently propagates\nand solves these constraints. We evaluate BART on program space enumeration\ntasks, finding that the constraints eliminate up to 99 percent of the program\nspace, and that modeling program spaces significantly reduces enumeration time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00005v1", "cate": "cs.PL", "date": "2025-07-10", "updated": "2025-07-10"}
{"id": "2508.00185", "title": "Comparison of Large Language Models for Deployment Requirements", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00185v1", "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00185v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00555", "title": "Activation-Guided Local Editing for Jailbreaking Attacks", "authors": ["Jiecong Wang", "Haoran Li", "Hao Peng", "Ziqian Zeng", "Zihao Wang", "Haohua Du", "Zhengtao Yu"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00555v1", "summary": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00555v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00007", "title": "Agent Network Protocol Technical White Paper", "authors": ["Gaowei Chang", "Eidan Lin", "Chengxuan Yuan", "Rizhao Cai", "Binbin Chen", "Xuan Xie", "Yin Zhang"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This white paper is a reformatted version of the open-source community edition previously released by the ANP Open Source Technology Community( this https URL )", "url": "http://arxiv.org/abs/2508.00007v1", "summary": "With the development of large models and autonomous decision-making AI,\nagents are rapidly becoming the new entities of the internet, following mobile\napps. However, existing internet infrastructure is primarily designed for human\ninteraction, creating data silos, unfriendly interfaces, and high collaboration\ncosts among agents, making it difficult to support the needs for large-scale\nagent interconnection and collaboration. The internet is undergoing a profound\ntransformation, showing four core trends: agents replacing traditional\nsoftware, universal agent interconnection, native protocol-based connections,\nand autonomous agent organization and collaboration. To align with these\ntrends, Agent Network Protocol (ANP) proposes a new generation of communication\nprotocols for the Agentic Web. ANP adheres to AI-native design, maintains\ncompatibility with existing internet protocols, adopts a modular composable\narchitecture, follows minimalist yet extensible principles, and enables rapid\ndeployment based on existing infrastructure. Through a three-layer protocol\nsystem--identity and encrypted communication layer, meta-protocol negotiation\nlayer, and application protocol layer--ANP. systematically solves the problems\nof agent identity authentication, dynamic negotiation, and capability discovery\ninteroperability.", "comment": "This white paper is a reformatted version of the open-source\n  community edition previously released by the ANP Open Source Technology\n  Community(https://github.com/agent-network-protocol)", "pdf_url": "http://arxiv.org/pdf/2508.00007v1", "cate": "cs.NI", "date": "2025-07-18", "updated": "2025-07-18"}
{"id": "2508.00217", "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00217v1", "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00217v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00602", "title": "LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks", "authors": ["Francesco Panebianco", "Stefano Bonfanti", "Francesco Trovò", "Michele Carminati"], "categories": ["cs.CR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      22 pages, preprint", "url": "http://arxiv.org/abs/2508.00602v1", "summary": "The generalization capabilities of Large Language Models (LLMs) have led to\ntheir widespread deployment across various applications. However, this\nincreased adoption has introduced several security threats, notably in the\nforms of jailbreaking and data leakage attacks. Additionally, Retrieval\nAugmented Generation (RAG), while enhancing context-awareness in LLM responses,\nhas inadvertently introduced vulnerabilities that can result in the leakage of\nsensitive information. Our contributions are twofold. First, we introduce a\nmethodology to analyze historical interaction data from an LLM system, enabling\nthe generation of usage maps categorized by topics (including adversarial\ninteractions). This approach further provides forensic insights for tracking\nthe evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a\nmodel-agnostic framework that combines static analysis for forensic insights\nwith dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique\nidentifies topic groups and detects anomalous patterns, allowing for proactive\ndefense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)\njailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,\nsupported by a curated dataset of labeled LLM interactions. In the static\nsetting, LeakSealer achieves the highest precision and recall on the ToxicChat\ndataset when identifying prompt injection. In the dynamic setting, PII leakage\ndetection achieves an AUPRC of $0.97$, significantly outperforming baselines\nsuch as Llama Guard.", "comment": "22 pages, preprint", "pdf_url": "http://arxiv.org/pdf/2508.00602v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00009", "title": "Enabling Immersive XR Collaborations over FTTR Networks (Invited)", "authors": ["Sourav Mondal", "Elaine Wong"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This invited paper was presented in Optica Advanced Photonic Congress 2025", "url": "http://arxiv.org/abs/2508.00009v1", "summary": "Fiber-To-The-Room is a potential solution to achieve in-premise extended\nreality collaborations. This paper explores predictive bandwidth allocation and\nseamless handover schemes over FTTR, showing high-quality immersive experience\nfor in-premise collaborations can be achieved. \\c{opyright} 2025 The Author(s).", "comment": "This invited paper was presented in Optica Advanced Photonic Congress\n  2025", "pdf_url": "http://arxiv.org/pdf/2508.00009v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2508.00220", "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00220v1", "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00220v1", "cate": "cs.CL", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00636", "title": "FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients", "authors": ["Haocheng Jiang", "Hua Shen", "Jixin Zhang", "Willy Susilo", "Mingwu Zhang"], "categories": ["cs.CR", "cs.DC"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00636v1", "summary": "Federated learning is a distributed training framework vulnerable to\nByzantine attacks, particularly when over 50% of clients are malicious or when\ndatasets are highly non-independent and identically distributed (non-IID).\nAdditionally, most existing defense mechanisms are designed for specific attack\ntypes (e.g., gradient similarity-based schemes can only defend against outlier\nmodel poisoning), limiting their effectiveness. In response, we propose\nFedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the\naforementioned issues by leveraging the high sensitivity of membership\ninference to model bias. By requiring clients to include an additional\nmini-batch of server-specified data in their training, FedGuard can identify\nand exclude poisoned models, as their confidence in the mini-batch will drop\nsignificantly. Our comprehensive evaluation unequivocally shows that, under\nthree highly non-IID datasets, with 90% of clients being Byzantine and seven\ndifferent types of Byzantine attacks occurring in each round, FedGuard\nsignificantly outperforms existing robust federated learning schemes in\nmitigating various types of Byzantine attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00636v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00149", "title": "Data Bias in Human Mobility is a Universal Phenomenon but is Highly Location-specific", "authors": ["Katinka den Nijs", "Elisa Omodei", "Vedran Sekara"], "categories": ["cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00149v1", "summary": "Large-scale human mobility datasets play increasingly critical roles in many\nalgorithmic systems, business processes and policy decisions. Unfortunately\nthere has been little focus on understanding bias and other fundamental\nshortcomings of the datasets and how they impact downstream analyses and\nprediction tasks. In this work, we study `data production', quantifying not\nonly whether individuals are represented in big digital datasets, but also how\nthey are represented in terms of how much data they produce. We study GPS\nmobility data collected from anonymized smartphones for ten major US cities and\nfind that data points can be more unequally distributed between users than\nwealth. We build models to predict the number of data points we can expect to\nbe produced by the composition of demographic groups living in census tracts,\nand find strong effects of wealth, ethnicity, and education on data production.\nWhile we find that bias is a universal phenomenon, occurring in all cities, we\nfurther find that each city suffers from its own manifestation of it, and that\nlocation-specific models are required to model bias for each city. This work\nraises serious questions about general approaches to debias human mobility data\nand urges further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00149v1", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00011", "title": "AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks", "authors": ["Ahmet Melih Ince", "Ayse Elif Canbilen", "Halim Yanikomeroglu"], "categories": ["cs.NI", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, to appear in IEEE conference proceedings", "url": "http://arxiv.org/abs/2508.00011v1", "summary": "Sixth-generation (6G) networks are designed to meet the hyper-reliable and\nlow-latency communication (HRLLC) requirements of safety-critical applications\nsuch as autonomous driving. Integrating non-terrestrial networks (NTN) into the\n6G infrastructure brings redundancy to the network, ensuring continuity of\ncommunications even under extreme conditions. In particular, high-altitude\nplatform stations (HAPS) stand out for their wide coverage and low latency\nadvantages, supporting communication reliability and enhancing information\nfreshness, especially in rural areas and regions with infrastructure\nconstraints. In this paper, we present reinforcement learning-based approaches\nusing deep deterministic policy gradient (DDPG) to dynamically optimize the\nage-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.\nThe proposed method improves information freshness and overall network\nreliability by enabling independent learning without centralized coordination.\nThe findings reveal the potential of HAPS-supported solutions, combined with\nDDPG-based learning, for efficient AoI-aware resource allocation in\nplatoon-based autonomous vehicle systems.", "comment": "6 pages, 3 figures, to appear in IEEE conference proceedings", "pdf_url": "http://arxiv.org/pdf/2508.00011v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2508.00238", "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2 figures, 2 tables. Licensed under CC BY-SA 4.0", "url": "http://arxiv.org/abs/2508.00238v1", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "pdf_url": "http://arxiv.org/pdf/2508.00238v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00659", "title": "Demo: TOSense -- What Did You Just Agree to?", "authors": ["Xinzhang Chen", "Hassan Ali", "Arash Shaghaghi", "Salil S. Kanhere", "Sanjay Jha"], "categories": ["cs.CR", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted as a demonstration paper at IEEE LCN 2025", "url": "http://arxiv.org/abs/2508.00659v1", "summary": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.", "comment": "Accepted as a demonstration paper at IEEE LCN 2025", "pdf_url": "http://arxiv.org/pdf/2508.00659v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00153", "title": "Green Computing: The Ultimate Carbon Destroyer for a Sustainable Future", "authors": ["Sayed Mahbub Hasan Amiri", "Prasun Goswami", "Md. Mainul Islam", "Mohammad Shakhawat Hossen", "Marzana Mithila", "Naznin Akter"], "categories": ["cs.CY", "cs.SC"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      26 Pages, 6 Tables", "url": "http://arxiv.org/abs/2508.00153v2", "summary": "Green computing represents a critical pathway to decarbonize the digital\neconomy while maintaining technological progress. This article examines how\nsustainable IT strategies including energy-efficient hardware, AI-optimized\ndata centres, and circular e-waste systems can transform computing into a net\ncarbon sink. Through analysis of industry best practices and emerging\ntechnologies like quantum computing and biodegradable electronics, we\ndemonstrate achievable reductions of 40-60% in energy consumption without\ncompromising performance. The study highlights three key findings: (1) current\nsolutions already deliver both environmental and economic benefits, with\ntypical payback periods of 3-5 years; (2) systemic barriers including cost\npremiums and policy fragmentation require coordinated action; and (3)\nnext-generation innovations promise order-of-magnitude improvements in\nefficiency. We present a practical framework for stakeholders from corporations\nadopting renewable-powered cloud services to individuals extending device\nlifespans to accelerate the transition. The research underscores computing's\nunique potential as a climate solution through its rapid innovation cycles and\nmeasurable impacts, concluding that strategic investments in green IT today can\nyield disproportionate sustainability dividends across all sectors tomorrow.\nThis work provides both a compelling case for urgent action and a clear roadmap\nto realize computing's potential as a powerful carbon destruction tool in the\nclimate crisis era.", "comment": "26 Pages, 6 Tables", "pdf_url": "http://arxiv.org/pdf/2508.00153v2", "cate": "cs.CY", "date": "2025-07-31", "updated": "2025-08-04"}
{"id": "2508.00024", "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning", "authors": ["Sebastián Andrés Cajas Ordóñez", "Luis Fernando Torres Torres", "Mario Bifulco", "Carlos Andrés Durán", "Cristian Bosch", "Ricardo Simón Carbajo"], "categories": ["quant-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00024v1", "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00024v1", "cate": "quant-ph", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2508.00285", "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "categories": ["cs.CL", "I.2.7; J.3"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      23 pages, 8 figures", "url": "http://arxiv.org/abs/2508.00285v1", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "comment": "23 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2508.00285v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00682", "title": "Unveiling Dynamic Binary Instrumentation Techniques", "authors": ["Oscar Llorente-Vazquez", "Xabier Ugarte-Pedrero", "Igor Santos-Grueiro", "Pablo Garcia Bringas"], "categories": ["cs.CR", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00682v1", "summary": "Dynamic Binary Instrumentation (DBI) is the set of techniques that enable\ninstrumentation of programs at run-time, making it possible to monitor and\nmodify the execution of compiled binaries or entire systems. DBI is used for\ncountless security applications and analyses, and is extensively used across\nmany fields in both industry and academia. Over the years, several DBI\napproaches have been proposed based on different technologies and implementing\ndiverse techniques. Every solution tries to overcome certain limitations, but\nthey sometimes bring other shortcomings. Some are specialized for one\nparticular domain or task, while others have a wider scope.\n  In this paper, we shed light into the labyrinth of DBI, bringing together\nprocess-level and whole-system approaches. We depict their building blocks and\nanalyze the underlying instrumentation techniques, comparing their ability to\ninstrument different primitives and run-time events. Then, we evaluate their\nperformance when implementing each primitive, and highlight relevant\nobservations. Our results show that no single technique is better than the rest\nin all circumstances.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00682v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00485", "title": "J4CC, A Frame for Communication Control", "authors": ["Aernout Schmidt", "Kunbei Zhang"], "categories": ["cs.CY", "econ.TH", "I.2.1"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      10,583 words, 23 pages", "url": "http://arxiv.org/abs/2508.00485v1", "summary": "Politics increasingly faces conflicts that appear irreconcilable, where one\nparty's requirements directly contradict those of another. We propose J4CC, a\nframe for mapping how four forces (Power, Capital, Morality, and Knowledge)\ninfluence rule-making. J4CC provides a jargon for analyzing rule-making\nconversations. Beyond ideas about framing (Lakoff 2004; Rein & Sch\\\"on, 1996),\nit draws on ideas from legal theory (Fuller 1967), institutional economics\n(Coase 1937), anthropology (Douglas 1992), and AI (Lewis et al. 2020). While\nour ultimate goal is to develop a working and usable AI language model\n(actually a 'jargon model'), this article focuses for now on its conceptual\nfoundation. We demonstrate J4CC's analytical power through three case studies\nthat profile seemingly incompatible positions. First, of influential thinkers\n(Confucius, Friedman, Montesquieu, Popper, Goodfellow) and their environments.\nThen, of governance dynamics (in the US during the first 5 months of 2025).\nFinally, of two contrasting interpretations thereof (Klein-Taylor's (2025) \"end\nof time fascism\" versus Bobbitt's (2025) \"constitutional degradation\"). The\nstudies confirm that J4CC can profile positions from incompatible governance\nphilosophies without clogging communication channels, sometimes also by\nproviding translation mechanisms. Rather than eliminating disagreements, the\nframework enables productive engagement by making explicit the underlying\ntensions that lead to institutional conflicts. Our case studies lead to the\nconclusion that J4CC in its current form is a usable instrument for analyzing\nfailing political debate. We add in the appendix our position on the\nfeasibility that the J4CC framework can develop its intended jargon through\napplication in an AI language model trained on large numbers of negotiation\nrecords.", "comment": "10,583 words, 23 pages", "pdf_url": "http://arxiv.org/pdf/2508.00485v1", "cate": "cs.CY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00053", "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. 11 pages, 5 figures", "url": "http://arxiv.org/abs/2508.00053v1", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2508.00053v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00028", "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "authors": ["Abir Ray"], "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      12 pages", "url": "http://arxiv.org/abs/2508.00028v1", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "comment": "12 pages", "pdf_url": "http://arxiv.org/pdf/2508.00028v1", "cate": "cs.NI", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00305", "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00305v1", "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00305v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00756", "title": "LeakyCLIP: Extracting Training Data from CLIP", "authors": ["Yunhao Chen", "Shujie Wang", "Xin Wang", "Xingjun Ma"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00756v1", "summary": "Understanding the memorization and privacy leakage risks in Contrastive\nLanguage--Image Pretraining (CLIP) is critical for ensuring the security of\nmultimodal models. Recent studies have demonstrated the feasibility of\nextracting sensitive training examples from diffusion models, with conditional\ndiffusion models exhibiting a stronger tendency to memorize and leak\ninformation. In this work, we investigate data memorization and extraction\nrisks in CLIP through the lens of CLIP inversion, a process that aims to\nreconstruct training images from text prompts. To this end, we introduce\n\\textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality,\nsemantically accurate image reconstruction from CLIP embeddings. We identify\nthree key challenges in CLIP inversion: 1) non-robust features, 2) limited\nvisual semantics in text embeddings, and 3) low reconstruction fidelity. To\naddress these challenges, LeakyCLIP employs 1) adversarial fine-tuning to\nenhance optimization smoothness, 2) linear transformation-based embedding\nalignment, and 3) Stable Diffusion-based refinement to improve fidelity.\nEmpirical results demonstrate the superiority of LeakyCLIP, achieving over 358%\nimprovement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared\nto baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive\nleakage risk, showing that training data membership can even be successfully\ninferred from the metrics of low-fidelity reconstructions. Our work introduces\na practical method for CLIP inversion while offering novel insights into the\nnature and scope of privacy risks in multimodal models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00756v1", "cate": "cs.CR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00536", "title": "Futures with Digital Minds: Expert Forecasts in 2025", "authors": ["Lucius Caviola", "Bradford Saad"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00536v1", "summary": "This report presents findings from an expert survey on digital minds takeoff\nscenarios. The survey was conducted in early 2025 with 67 experts in digital\nminds research, AI research, philosophy, forecasting, and related fields.\nParticipants provided probabilistic forecasts and qualitative reasoning on the\ndevelopment, characteristics, and societal impact of digital minds, that is,\ncomputer systems capable of subjective experience. Experts assigned high\nprobability to digital minds being possible in principle (median 90%) and being\ncreated this century (65% by 2100), with a non-negligible probability of\nemergence by 2030 (20%). Many anticipated rapid growth in digital mind welfare\ncapacity, with collective welfare capacity potentially matching that of\nbillions of humans within a decade after the creation of the first digital\nmind. Participants also expected widespread claims from digital minds regarding\ntheir consciousness and rights, and predicted substantial societal disagreement\nover their existence and moral interests. Views diverged on whether digital\nmind welfare will be net positive or negative. These findings provide evidence\nthat bears on the extent to which preparing the world for the potential arrival\nof digital minds should be a priority across domains such as research and\ngovernance. However, these findings should be interpreted cautiously in light\nof the potential for systematic overrepresentation of experts who deem digital\nminds particularly likely or important.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00536v1", "cate": "cs.CY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00085", "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 main conference", "url": "http://arxiv.org/abs/2508.00085v1", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "comment": "Accepted to ICCV 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2508.00085v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00033", "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "João P. Matos-Carvalho"], "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00033v1", "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00033v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00332", "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2508.00332v1", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2508.00332v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00591", "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2508.00591v1", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2508.00591v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00543", "title": "Towards Efficient Certification of Maritime Remote Operation Centers", "authors": ["Christian Neurohr", "Marcel Saager", "Lina Putze", "Jan-Patrick Osterloh", "Karina Rothemann", "Hilko Wiards", "Eckard Böde", "Axel Hahn"], "categories": ["cs.CY", "cs.RO"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00543v1", "summary": "Additional automation being build into ships implies a shift of crew from\nship to shore. However, automated ships still have to be monitored and, in some\nsituations, controlled remotely. These tasks are carried out by human operators\nlocated in shore-based remote operation centers. In this work, we present a\nconcept for a hazard database that supports the safeguarding and certification\nof such remote operation centers. The concept is based on a categorization of\nhazard sources which we derive from a generic functional architecture. A\nsubsequent preliminary suitability analysis unveils which methods for hazard\nanalysis and risk assessment can adequately fill this hazard database.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00543v1", "cate": "cs.CY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00088", "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taihú Pire"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025", "url": "http://arxiv.org/abs/2508.00088v1", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "comment": "Accepted to IROS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00088v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00037", "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Transactions on Industrial Informatics", "url": "http://arxiv.org/abs/2508.00037v1", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "comment": "Accepted at IEEE Transactions on Industrial Informatics", "pdf_url": "http://arxiv.org/pdf/2508.00037v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00344", "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00344v1", "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00344v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00596", "title": "Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience", "authors": ["Xiang Zhang", "Zhou Li", "Shuangyang Li", "Kai Wan", "Derrick Wing Kwan Ng", "Giuseppe Caire"], "categories": ["cs.IT", "cs.CR", "cs.DC", "cs.LG", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE for potential journal publication", "url": "http://arxiv.org/abs/2508.00596v1", "summary": "In decentralized federated learning (FL), multiple clients collaboratively\nlearn a shared machine learning (ML) model by leveraging their privately held\ndatasets distributed across the network, through interactive exchange of the\nintermediate model updates. To ensure data security, cryptographic techniques\nare commonly employed to protect model updates during aggregation. Despite\ngrowing interest in secure aggregation, existing works predominantly focus on\nprotocol design and computational guarantees, with limited understanding of the\nfundamental information-theoretic limits of such systems. Moreover, optimal\nbounds on communication and key usage remain unknown in decentralized settings,\nwhere no central aggregator is available. Motivated by these gaps, we study the\nproblem of decentralized secure aggregation (DSA) from an information-theoretic\nperspective. Specifically, we consider a network of $K$ fully-connected users,\neach holding a private input -- an abstraction of local training data -- who\naim to securely compute the sum of all inputs. The security constraint requires\nthat no user learns anything beyond the input sum, even when colluding with up\nto $T$ other users. We characterize the optimal rate region, which specifies\nthe minimum achievable communication and secret key rates for DSA. In\nparticular, we show that to securely compute one symbol of the desired input\nsum, each user must (i) transmit at least one symbol to others, (ii) hold at\nleast one symbol of secret key, and (iii) all users must collectively hold no\nfewer than $K - 1$ independent key symbols. Our results establish the\nfundamental performance limits of DSA, providing insights for the design of\nprovably secure and communication-efficient protocols in distributed learning\nsystems.", "comment": "Submitted to IEEE for potential journal publication", "pdf_url": "http://arxiv.org/pdf/2508.00596v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00668", "title": "Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration", "authors": ["Raquel Coelho", "Roy Pea", "Christian Schunn", "Jinglei Cheng", "Junyu Liu"], "categories": ["physics.ed-ph", "cs.AI", "cs.CY", "quant-ph"], "primary_category": "Subjects:       Physics Education (physics.ed-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00668v1", "summary": "As quantum information science advances and the need for pre-college\nengagement grows, a critical question remains: How can young learners be\nprepared to participate in a field so radically different from what they have\nencountered before? This paper argues that meeting this challenge will require\nstrong interdisciplinary collaboration with the Learning Sciences (LS), a field\ndedicated to understanding how people learn and designing theory-guided\nenvironments to support learning. Drawing on lessons from previous STEM\neducation efforts, we discuss two key contributions of the learning sciences to\nquantum information science (QIS) education. The first is design-based\nresearch, the signature methodology of learning sciences, which can inform the\ndevelopment, refinement, and scaling of effective QIS learning experiences. The\nsecond is a framework for reshaping how learners reason about, learn and\nparticipate in QIS practices through shifts in knowledge representations that\nprovide new forms of engagement and associated learning. We call for a two-way\npartnership between quantum information science and the learning sciences, one\nthat not only supports learning in quantum concepts and practices but also\nimproves our understanding of how to teach and support learning in highly\ncomplex domains. We also consider potential questions involved in bridging\nthese disciplinary communities and argue that the theoretical and practical\nbenefits justify the effort.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00668v1", "cate": "physics.ed-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00135", "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 18 figures, 5 tables", "url": "http://arxiv.org/abs/2508.00135v1", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "comment": "12 pages, 18 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2508.00135v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.05885", "title": "Cost-Effective, Low Latency Vector Search with Azure Cosmos DB", "authors": ["Nitish Upreti", "Harsha Vardhan Simhadri", "Hari Sudan Sundar", "Krishnan Sundaram", "Samer Boshra", "Balachandar Perumalswamy", "Shivam Atri", "Martin Chisholm", "Revti Raman Singh", "Greg Yang", "Tamara Hass", "Nitesh Dudhey", "Subramanyam Pattipaka", "Mark Hildebrand", "Magdalen Manohar", "Jack Moffitt", "Haiyang Xu", "Naren Datha", "Suryansh Gupta", "Ravishankar Krishnaswamy", "Prashant Gupta", "Abhishek Sahu", "Hemeswari Varada", "Sudhanshu Barthwal", "Ritika Mor", "James Codella", "Shaun Cooper", "Kevin Pilch", "Simon Moreno", "Aayush Kataria", "Santosh Kulkarni", "Neil Deshpande", "Amar Sagare", "Dinesh Billa", "Zishan Fu", "Vipul Vishal"], "categories": ["cs.DB", "cs.IR", "H.3.3"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.05885v2", "summary": "Vector indexing enables semantic search over diverse corpora and has become\nan important interface to databases for both users and AI agents. Efficient\nvector search requires deep optimizations in database systems. This has\nmotivated a new class of specialized vector databases that optimize for vector\nsearch quality and cost. Instead, we argue that a scalable, high-performance,\nand cost-efficient vector search system can be built inside a cloud-native\noperational database like Azure Cosmos DB while leveraging the benefits of a\ndistributed database such as high availability, durability, and scale. We do\nthis by deeply integrating DiskANN, a state-of-the-art vector indexing library,\ninside Azure Cosmos DB NoSQL. This system uses a single vector index per\npartition stored in existing index trees, and kept in sync with underlying\ndata. It supports < 20ms query latency over an index spanning 10 million\nvectors, has stable recall over updates, and offers approximately 43x and 12x\nlower query cost compared to Pinecone and Zilliz serverless enterprise\nproducts. It also scales out to billions of vectors via automatic partitioning.\nThis convergent design presents a point in favor of integrating vector indices\ninto operational databases in the context of recent debates on specialized\nvector databases, and offers a template for vector indexing in other databases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.05885v2", "cate": "cs.DB", "date": "2025-05-09", "updated": "2025-07-31"}
{"id": "2508.00039", "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00039v1", "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00039v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00360", "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00360v1", "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00360v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00620", "title": "Backdoor Attacks on Deep Learning Face Detection", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00620v1", "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00620v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.07911", "title": "The Urban Impact of AI: Modeling Feedback Loops in Next-Venue Recommendation", "authors": ["Giovanni Mauro", "Marco Minici", "Luca Pappalardo"], "categories": ["cs.AI", "cs.CY"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.07911v2", "summary": "Next-venue recommender systems are increasingly embedded in location-based\nservices, shaping individual mobility decisions in urban environments. While\ntheir predictive accuracy has been extensively studied, less attention has been\npaid to their systemic impact on urban dynamics. In this work, we introduce a\nsimulation framework to model the human-AI feedback loop underpinning\nnext-venue recommendation, capturing how algorithmic suggestions influence\nindividual behavior, which in turn reshapes the data used to retrain the\nmodels. Our simulations, grounded in real-world mobility data, systematically\nexplore the effects of algorithmic adoption across a range of recommendation\nstrategies. We find that while recommender systems consistently increase\nindividual-level diversity in visited venues, they may simultaneously amplify\ncollective inequality by concentrating visits on a limited subset of popular\nplaces. This divergence extends to the structure of social co-location\nnetworks, revealing broader implications for urban accessibility and spatial\nsegregation. Our framework operationalizes the feedback loop in next-venue\nrecommendation and offers a novel lens through which to assess the societal\nimpact of AI-assisted mobility-providing a computational tool to anticipate\nfuture risks, evaluate regulatory interventions, and inform the design of ethic\nalgorithmic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.07911v2", "cate": "cs.AI", "date": "2025-04-10", "updated": "2025-08-01"}
{"id": "2508.00144", "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      27 pages, 1 figure", "url": "http://arxiv.org/abs/2508.00144v1", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "comment": "27 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2508.00144v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.00343", "title": "Meaningful Data Erasure in the Presence of Dependencies", "authors": ["Vishal Chakraborty", "Youri Kaminsky", "Sharad Mehrotra", "Felix Naumann", "Faisal Nawab", "Primal Pappachan", "Mohammad Sadoghi", "Nalini Venkatasubramanian"], "categories": ["cs.DB"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "Comments:      VLDB 2025 Preprint", "url": "http://arxiv.org/abs/2507.00343v2", "summary": "Data regulations like GDPR require systems to support data erasure but leave\nthe definition of \"erasure\" open to interpretation. This ambiguity makes\ncompliance challenging, especially in databases where data dependencies can\nlead to erased data being inferred from remaining data. We formally define a\nprecise notion of data erasure that ensures any inference about deleted data,\nthrough dependencies, remains bounded to what could have been inferred before\nits insertion. We design erasure mechanisms that enforce this guarantee at\nminimal cost. Additionally, we explore strategies to balance cost and\nthroughput, batch multiple erasures, and proactively compute data retention\ntimes when possible. We demonstrate the practicality and scalability of our\nalgorithms using both real and synthetic datasets.", "comment": "VLDB 2025 Preprint", "pdf_url": "http://arxiv.org/pdf/2507.00343v2", "cate": "cs.DB", "date": "2025-07-01", "updated": "2025-08-01"}
{"id": "2508.00041", "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "categories": ["cs.LG", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00041v1", "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00041v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00370", "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2508.00370v1", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2508.00370v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00637", "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks", "authors": ["Michał Forystek", "Andrew D. Syrmakesis", "Alkistis Kontou", "Panos Kotsampopoulos", "Nikos D. Hatziargyriou", "Charalambos Konstantinou"], "categories": ["eess.SY", "cs.CR", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      2025 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)", "url": "http://arxiv.org/abs/2508.00637v1", "summary": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.", "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "pdf_url": "http://arxiv.org/pdf/2508.00637v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.17217", "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs", "authors": ["Kangda Wei", "Hasnat Md Abdullah", "Ruihong Huang"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17217v2", "summary": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal\ntreatment of male and female subjects across different contexts. To address\nthis issue, we propose a novel data generation framework that fosters\nexploratory thinking in LLMs. Our approach prompts models to generate story\npairs featuring male and female protagonists in structurally identical, morally\nambiguous scenarios, then elicits and compares their moral judgments. When\ninconsistencies arise, the model is guided to produce balanced, gender-neutral\njudgments. These story-judgment pairs are used to fine-tune or optimize the\nmodels via Direct Preference Optimization (DPO). Experimental results show that\nour method significantly reduces gender bias while preserving or even enhancing\ngeneral model capabilities. We will release the code and generated data. We\nrelease the code and generated data at:\nhttps://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17217v2", "cate": "cs.CL", "date": "2025-05-22", "updated": "2025-08-01"}
{"id": "2508.00152", "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page at this https URL", "url": "http://arxiv.org/abs/2508.00152v1", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "pdf_url": "http://arxiv.org/pdf/2508.00152v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.22186", "title": "SourceSplice: Source Selection for Machine Learning Tasks", "authors": ["Ambarish Singh", "Romila Pradhan"], "categories": ["cs.LG", "cs.AI", "cs.DB", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22186v2", "summary": "Data quality plays a pivotal role in the predictive performance of machine\nlearning (ML) tasks - a challenge amplified by the deluge of data sources\navailable in modern organizations. Prior work in data discovery largely focus\non metadata matching, semantic similarity or identifying tables that should be\njoined to answer a particular query, but do not consider source quality for\nhigh performance of the downstream ML task. This paper addresses the problem of\ndetermining the best subset of data sources that must be combined to construct\nthe underlying training dataset for a given ML task. We propose SourceGrasp and\nSourceSplice, frameworks designed to efficiently select a suitable subset of\nsources that maximizes the utility of the downstream ML model. Both the\nalgorithms rely on the core idea that sources (or their combinations)\ncontribute differently to the task utility, and must be judiciously chosen.\nWhile SourceGrasp utilizes a metaheuristic based on a greediness criterion and\nrandomization, the SourceSplice framework presents a source selection mechanism\ninspired from gene splicing - a core concept used in protein synthesis. We\nempirically evaluate our algorithms on three real-world datasets and synthetic\ndatasets and show that, with significantly fewer subset explorations,\nSourceSplice effectively identifies subsets of data sources leading to high\ntask utility. We also conduct studies reporting the sensitivity of SourceSplice\nto the decision choices under several settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22186v2", "cate": "cs.LG", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2508.00046", "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13 pages for supplementary material", "url": "http://arxiv.org/abs/2508.00046v1", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "pdf_url": "http://arxiv.org/pdf/2508.00046v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00385", "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00385v1", "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00385v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00748", "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the IEEE International Joint Conference on Biometrics (IJCB 2025)", "url": "http://arxiv.org/abs/2508.00748v1", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00748v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23454", "title": "Breaking the mould of Social Mixed Reality - State-of-the-Art and Glossary", "authors": ["Marta Bieńkiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "Stéphane Donikian", "Didier Stricker", "Benoît Bardy"], "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      pre-print", "url": "http://arxiv.org/abs/2507.23454v2", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process.", "comment": "pre-print", "pdf_url": "http://arxiv.org/pdf/2507.23454v2", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00169", "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00169v1", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00169v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00047", "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00047v1", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "comment": "11 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00047v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00390", "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00390v1", "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00390v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2402.07518", "title": "Efficient and Universal Watermarking for LLM-Generated Code Detection", "authors": ["Boquan Li", "Zirui Fu", "Mengdi Zhang", "Peixin Zhang", "Jun Sun", "Xingmei Wang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      This work has been submitted to IEEE for possible publication", "url": "http://arxiv.org/abs/2402.07518v4", "summary": "Large language models (LLMs) have significantly enhanced the usability of\nAI-generated code, providing effective assistance to programmers. This\nadvancement also raises ethical and legal concerns, such as academic dishonesty\nor the generation of malicious code. For accountability, it is imperative to\ndetect whether a piece of code is AI-generated. Watermarking is broadly\nconsidered a promising solution and has been successfully applied to identify\nLLM-generated text. However, existing efforts on code are far from ideal,\nsuffering from limited universality and excessive time and memory consumption.\nIn this work, we propose a plug-and-play watermarking approach for AI-generated\ncode detection, named ACW (AI Code Watermarking). ACW is training-free and\nworks by selectively applying a set of carefully-designed, semantic-preserving\nand idempotent code transformations to LLM code outputs. The presence or\nabsence of the transformations serves as implicit watermarks, enabling the\ndetection of AI-generated code. Our experimental results show that ACW\neffectively detects AI-generated code, preserves code utility, and is resilient\nagainst code optimizations. Especially, ACW is efficient and is universal\nacross different LLMs, addressing the limitations of existing approaches.", "comment": "This work has been submitted to IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2402.07518v4", "cate": "cs.CR", "date": "2024-02-12", "updated": "2025-08-01"}
{"id": "2508.00171", "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language Models (MLLMs) in Clinical Practice", "url": "http://arxiv.org/abs/2508.00171v1", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "pdf_url": "http://arxiv.org/pdf/2508.00171v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00341", "title": "Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT", "authors": ["Shengheng Liu", "Ningning Fu", "Zhonghao Zhang", "Yongming Huang", "Tony Q. S. Quek"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      To appear in ACM TOIT. 24 pages, 8 figures", "url": "http://arxiv.org/abs/2508.00341v1", "summary": "The rising popularity of Internet of things (IoT) has spurred technological\nadvancements in mobile internet and interconnected systems. While offering\nflexible connectivity and intelligent applications across various domains, IoT\nservice providers must gather vast amounts of sensitive data from users, which\nnonetheless concomitantly raises concerns about privacy breaches. Federated\nlearning (FL) has emerged as a promising decentralized training paradigm to\ntackle this challenge. This work focuses on enhancing the aggregation\nefficiency of distributed local models by introducing over-the-air computation\ninto the FL framework. Due to radio resource scarcity in large-scale networks,\nonly a subset of users can participate in each training round. This highlights\nthe need for effective user scheduling and model transmission strategies to\noptimize communication efficiency and inference accuracy. To address this, we\npropose an integrated approach to user scheduling and receive beam steering,\nsubject to constraints on the number of selected users and transmit power.\nLeveraging the difference-of-convex technique, we decompose the primal\nnon-convex optimization problem into two sub-problems, yielding an iterative\nsolution. While effective, the computational load of the iterative method\nhampers its practical implementation. To overcome this, we further propose a\nlow-complexity user scheduling policy based on characteristic analysis of the\nwireless channel to directly determine the user subset without iteration.\nExtensive experiments validate the superiority of the proposed method in terms\nof aggregation error and learning performance over existing approaches.", "comment": "To appear in ACM TOIT. 24 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2508.00341v1", "cate": "cs.DC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00078", "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "authors": ["Imen Mahmoud", "Andrei Velichko"], "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 5 figures", "url": "http://arxiv.org/abs/2508.00078v1", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "comment": "22 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2508.00078v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00420", "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00420v1", "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00420v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.15126", "title": "UTrace: Poisoning Forensics for Private Collaborative Learning", "authors": ["Evan Rose", "Hidde Lycklama", "Harsh Chaudhari", "Anwar Hithnawi", "Alina Oprea"], "categories": ["cs.CR", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      28 pages, 10 figures; update ack", "url": "http://arxiv.org/abs/2409.15126v2", "summary": "Privacy-preserving machine learning (PPML) enables multiple data owners to\ncontribute their data privately to a set of servers that run a secure\nmulti-party computation (MPC) protocol to train a joint ML model. In these\nprotocols, the input data remains private throughout the training process, and\nonly the resulting model is made available. While this approach benefits\nprivacy, it also exacerbates the risks of data poisoning, where compromised\ndata owners induce undesirable model behavior by contributing malicious\ndatasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but\nthese measures are not exhaustive. To complement existing poisoning defenses,\nwe introduce UTrace: a framework for User-level Traceback of poisoning attacks\nin PPML. Utrace computes user responsibility scores using gradient similarity\nmetrics aggregated across the most relevant samples in an owner's dataset.\nUTrace is effective at low poisoning rates and is resilient to poisoning\nattacks distributed across multiple data owners, unlike existing\nunlearning-based methods. We introduce methods for checkpointing gradients with\nlow storage overhead, enabling traceback in the absence of data owners at\ndeployment time. We also design several optimizations that reduce traceback\ntime and communication in MPC. We provide a comprehensive evaluation of UTrace\nacross four datasets from three data modalities (vision, text, and malware) and\nshow its effectiveness against 10 poisoning attacks.", "comment": "28 pages, 10 figures; update ack", "pdf_url": "http://arxiv.org/pdf/2409.15126v2", "cate": "cs.CR", "date": "2024-09-23", "updated": "2025-08-01"}
{"id": "2508.00197", "title": "Graph Lineages and Skeletal Graph Products", "authors": ["Eric Mjolsness", "Cory B. Scott"], "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      42 pages. 33 Figures. Under review", "url": "http://arxiv.org/abs/2508.00197v1", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "comment": "42 pages. 33 Figures. Under review", "pdf_url": "http://arxiv.org/pdf/2508.00197v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00426", "title": "Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services", "authors": ["Rohan Gandhi", "Ankur Mallick", "Ken Sueda", "Rui Liang"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00426v1", "summary": "Conference services like Zoom, Microsoft Teams, and Google Meet facilitate\nmillions of daily calls, yet ensuring high performance at low costs remains a\nsignificant challenge. This paper revisits the problem of packing calls across\nMedia Processor (MP) servers that host the calls within individual datacenters\n(DCs). We show that the algorithm used in Teams -- a large scale conferencing\nservice as well as other state-of-art algorithms are prone to placing calls\nresulting in some of the MPs becoming hot (high CPU utilization) that leads to\ndegraded performance and/or elevated hosting costs. The problem arises from\ndisregarding the variability in CPU usage among calls, influenced by\ndifferences in participant numbers and media types (audio/video), compounded by\nbursty call arrivals. To tackle this, we propose Tetris, a multi-step framework\nwhich (a) optimizes initial call assignments by leveraging historical data and\n(b) periodically migrates calls from hot MPs using linear optimization, aiming\nto minimize hot MP usage. Evaluation based on a 24-hour trace of over 10\nmillion calls in one DC shows that Tetris reduces participant numbers on hot\nMPs by at least 2.5X.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00426v1", "cate": "cs.DC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00083", "title": "A Survey on Code Generation with LLM-based Agents", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2508.00083v1", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2508.00083v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00429", "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, work in progress", "url": "http://arxiv.org/abs/2508.00429v1", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "comment": "17 pages, work in progress", "pdf_url": "http://arxiv.org/pdf/2508.00429v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.10537", "title": "ExclaveFL: Providing Transparency to Federated Learning using Exclaves", "authors": ["Jinnan Guo", "Kapil Vaswani", "Andrew Paverd", "Peter Pietzuch"], "categories": ["cs.CR", "cs.DC", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.10537v2", "summary": "In federated learning (FL), data providers jointly train a model without\ndisclosing their training data. Despite its inherent privacy benefits, a\nmalicious data provider can simply deviate from the correct training protocol\nwithout being detected, potentially compromising the trained model. While\ncurrent solutions have explored the use of trusted execution environments\n(TEEs) to combat such attacks, they usually assume side-channel attacks against\nthe TEEs are out of scope. However, such side-channel attacks can undermine the\nsecurity properties of TEE-based FL frameworks, not by extracting the FL data,\nbut by leaking keys that allow the adversary to impersonate as the TEE whilst\ndeviating arbitrarily from the correct training protocol.\n  We describe ExclaveFL, an FL platform that provides end-to-end integrity and\ntransparency, even in the presence of side-channel attacks on TEEs. We propose\na new paradigm in which existing TEEs are used as exclaves --\nintegrity-protected execution environments that do not contain any secrets,\nmaking them immune to side-channel attacks. Whereas previous approaches attest\nthe TEE itself and bind this attestation to a key held by the TEE, ExclaveFL\nattests individual data transformations at runtime. These runtime attestations\nform an attested dataflow graph, which can be checked to ensure the FL training\njob satisfies claims, such as deviations from the correct computation. We\nimplement ExclaveFL by extending the popular NVFlare FL framework to use\nexclaves, and show experimentally that ExclaveFL introduces less than 10%\noverhead compared to the same FL framework without TEEs, whilst providing\nstronger security guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.10537v2", "cate": "cs.CR", "date": "2024-12-13", "updated": "2025-08-01"}
{"id": "2508.00205", "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00205v1", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00205v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00622", "title": "SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments", "authors": ["Kapel Dev", "Yash Madhwal", "Sofia Shevelo", "Pavel Osinenko", "Yury Yanovich"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00622v1", "summary": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical\napplications such as aerial mapping, environmental monitoring, and autonomous\ndelivery. However, the reliability of these systems is highly dependent on\nuninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,\nwhich can be disrupted in real-world scenarios due to interference,\nenvironmental conditions, or adversarial attacks, causing disorientation,\ncollision risks, and mission failure. This paper proposes SwarnRaft, a\nblockchain-inspired positioning and consensus framework for maintaining\ncoordination and data integrity in UAV swarms operating under GNSS-denied\nconditions. SwarnRaft leverages the Raft consensus algorithm to enable\ndistributed drones (nodes) to agree on state updates such as location and\nheading, even in the absence of GNSS signals for one or more nodes. In our\nprototype, each node uses GNSS and local sensing, and communicates over WiFi in\na simulated swarm. Upon signal loss, consensus is used to reconstruct or verify\nthe position of the failed node based on its last known state and trajectory.\nOur system demonstrates robustness in maintaining swarm coherence and fault\ntolerance through a lightweight, scalable communication model. This work offers\na practical and secure foundation for decentralized drone operation in\nunpredictable environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00622v1", "cate": "cs.DC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00097", "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures, project link: this https URL", "url": "http://arxiv.org/abs/2508.00097v1", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "pdf_url": "http://arxiv.org/pdf/2508.00097v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00454", "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages, 2 pages, under review at AAAI 2026", "url": "http://arxiv.org/abs/2508.00454v1", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "comment": "15 pages, 2 pages, under review at AAAI 2026", "pdf_url": "http://arxiv.org/pdf/2508.00454v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.15170", "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem", "authors": ["Yanxu Mao", "Tiehan Cui", "Peipei Liu", "Datao You", "Hongsong Zhu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15170v3", "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15170v3", "cate": "cs.CR", "date": "2025-06-18", "updated": "2025-08-01"}
{"id": "2508.00213", "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "authors": ["Shayan Jalilian", "Abdul Bais"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00213v1", "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00213v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00234", "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts", "authors": ["Jin Yang", "Qiong Wu", "Zhiying Feng", "Zhi Zhou", "Deke Guo", "Xu Chen"], "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Transactions on Mobile Computing", "url": "http://arxiv.org/abs/2508.00234v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nleading to a significant increase in user demand for LLM services. However,\ncloud-based LLM services often suffer from high latency, unstable\nresponsiveness, and privacy concerns. Therefore, multiple LLMs are usually\ndeployed at the network edge to boost real-time responsiveness and protect data\nprivacy, particularly for many emerging smart mobile and IoT applications.\nGiven the varying response quality and latency of LLM services, a critical\nissue is how to route user requests from mobile and IoT devices to an\nappropriate LLM service (i.e., edge LLM expert) to ensure acceptable\nquality-of-service (QoS). Existing routing algorithms fail to simultaneously\naddress the heterogeneity of LLM services, the interference among requests, and\nthe dynamic workloads necessary for maintaining long-term stable QoS. To meet\nthese challenges, in this paper we propose a novel deep reinforcement learning\n(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM\nservices. Due to the dynamic nature of the global state, we propose a dynamic\nstate abstraction technique to compactly represent global state features with a\nheterogeneous graph attention network (HAN). Additionally, we introduce an\naction impact estimator and a tailored reward function to guide the DRL agent\nin maximizing QoS and preventing latency violations. Extensive experiments on\nboth Poisson and real-world workloads demonstrate that our proposed algorithm\nsignificantly improves average QoS and computing resource efficiency compared\nto existing baselines.", "comment": "Accepted by IEEE Transactions on Mobile Computing", "pdf_url": "http://arxiv.org/pdf/2508.00234v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00098", "title": "Stress-Aware Resilient Neural Training", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 11 figures", "url": "http://arxiv.org/abs/2508.00098v1", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "comment": "16 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2508.00098v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00476", "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00476v1", "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00476v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.17292", "title": "Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models", "authors": ["Quan Nguyen", "Minh N. Vu", "Truc Nguyen", "My T. Thai"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025", "url": "http://arxiv.org/abs/2506.17292v2", "summary": "Federated Learning enables collaborative learning among clients via a\ncoordinating server while avoiding direct data sharing, offering a perceived\nsolution to preserve privacy. However, recent studies on Membership Inference\nAttacks (MIAs) have challenged this notion, showing high success rates against\nunprotected training data. While local differential privacy (LDP) is widely\nregarded as a gold standard for privacy protection in data analysis, most\nstudies on MIAs either neglect LDP or fail to provide theoretical guarantees\nfor attack success rates against LDP-protected data. To address this gap, we\nderive theoretical lower bounds for the success rates of low-polynomial time\nMIAs that exploit vulnerabilities in fully connected or self-attention layers.\nWe establish that even when data are protected by LDP, privacy risks persist,\ndepending on the privacy budget. Practical evaluations on federated vision\nmodels confirm considerable privacy risks, revealing that the noise required to\nmitigate these attacks significantly degrades models' utility.", "comment": "Accepted to ICML 2025", "pdf_url": "http://arxiv.org/pdf/2506.17292v2", "cate": "cs.CR", "date": "2025-06-16", "updated": "2025-08-01"}
{"id": "2508.00218", "title": "Object-Centric Cropping for Visual Few-Shot Classification", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00218v1", "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00218v1", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00806", "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2508.00806v1", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2508.00806v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00103", "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app", "authors": ["Guilherme Guerino", "Luiz Rodrigues", "Luana Bianchini", "Mariana Alves", "Marcelo Marinho", "Thomaz Veloso", "Valmir Macario", "Diego Dermeval", "Thales Vieira", "Ig Bittencourt", "Seiji Isotani"], "categories": ["cs.HC", "cs.AI", "68T01", "H.5.0; I.2.0"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Article accepted in the International Journal of Human-Computer Interaction", "url": "http://arxiv.org/abs/2508.00103v2", "summary": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments.", "comment": "Article accepted in the International Journal of Human-Computer\n  Interaction", "pdf_url": "http://arxiv.org/pdf/2508.00103v2", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-08-04"}
{"id": "2508.00489", "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00489v1", "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00489v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.23183", "title": "A Practical and Secure Byzantine Robust Aggregator", "authors": ["De Zhang Lee", "Aashish Kolluri", "Prateek Saxena", "Ee-Chien Chang"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.23183v4", "summary": "In machine learning security, one is often faced with the problem of removing\noutliers from a given set of high-dimensional vectors when computing their\naverage. For example, many variants of data poisoning attacks produce gradient\nvectors during training that are outliers in the distribution of clean\ngradients, which bias the computed average used to derive the ML model.\nFiltering them out before averaging serves as a generic defense strategy.\nByzantine robust aggregation is an algorithmic primitive which computes a\nrobust average of vectors, in the presence of an $\\epsilon$ fraction of vectors\nwhich may have been arbitrarily and adaptively corrupted, such that the\nresulting bias in the final average is provably bounded.\n  In this paper, we give the first robust aggregator that runs in quasi-linear\ntime in the size of input vectors and provably has near-optimal bias bounds.\nOur algorithm also does not assume any knowledge of the distribution of clean\nvectors, nor does it require pre-computing any filtering thresholds from it.\nThis makes it practical to use directly in standard neural network training\nprocedures. We empirically confirm its expected runtime efficiency and its\neffectiveness in nullifying 10 different ML poisoning attacks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.23183v4", "cate": "cs.CR", "date": "2025-06-29", "updated": "2025-08-01"}
{"id": "2508.00248", "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00248v1", "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00248v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23387", "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units with Precision Recovery", "authors": ["Weicheng Xue", "Baisong Xu", "Kai Yang", "Yongxiang Liu", "Dengdeng Fan", "Pengxiang Xu", "Yonghong Tian"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23387v2", "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23387v2", "cate": "cs.DC", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00117", "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted and presented paper of THE 16th INTERNATIONAL IEEE CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT) INDIA", "url": "http://arxiv.org/abs/2508.00117v2", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "pdf_url": "http://arxiv.org/pdf/2508.00117v2", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-08-04"}
{"id": "2508.00522", "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00522v1", "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00522v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21170", "title": "OneShield -- the Next Generation of LLM Guardrails", "authors": ["Chad DeLuca", "Anna Lisa Gentile", "Shubhi Asthana", "Bing Zhang", "Pawan Chowdhary", "Kellen Cheng", "Basel Shbita", "Pengyuan Li", "Guang-Jie Ren", "Sandeep Gopisetty"], "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21170v2", "summary": "The rise of Large Language Models has created a general excitement about the\ngreat potential for a myriad of applications. While LLMs offer many\npossibilities, questions about safety, privacy, and ethics have emerged, and\nall the key actors are working to address these issues with protective measures\nfor their own models and standalone solutions. The constantly evolving nature\nof LLMs makes it extremely challenging to universally shield users against\ntheir potential risks, and one-size-fits-all solutions are unfeasible. In this\nwork, we propose OneShield, our stand-alone, model-agnostic and customizable\nsolution to safeguard LLMs. OneShield aims to provide facilities for defining\nrisk factors, expressing and declaring contextual safety and compliance\npolicies, and mitigating LLM risks, with a focus on each specific customer. We\ndescribe the implementation of the framework, discuss scalability\nconsiderations, and provide usage statistics of OneShield since its initial\ndeployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21170v2", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-31"}
{"id": "2508.00259", "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      22 pages, 9 figures", "url": "http://arxiv.org/abs/2508.00259v1", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "comment": "22 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2508.00259v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.18749", "title": "TensorSocket: Shared Data Loading for Deep Learning Training", "authors": ["Ties Robroek", "Neil Kim Nielsen", "Pınar Tözün"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.18749v4", "summary": "Training deep learning models is a repetitive and resource-intensive process.\nData scientists often train several models before landing on a set of\nparameters (e.g., hyper-parameter tuning) and model architecture (e.g., neural\narchitecture search), among other things that yield the highest accuracy. The\ncomputational efficiency of these training tasks depends highly on how well the\ntraining data is supplied to the training process. The repetitive nature of\nthese tasks results in the same data processing pipelines running over and\nover, exacerbating the need for and costs of computational resources. In this\npaper, we present TensorSocket to reduce the computational needs of deep\nlearning training by enabling simultaneous training processes to share the same\ndata loader. TensorSocket mitigates CPU-side bottlenecks in cases where the\ncollocated training workloads have high throughput on GPU, but are held back by\nlower data-loading throughput on CPU. TensorSocket achieves this by reducing\nredundant computations and data duplication across collocated training\nprocesses and leveraging modern GPU-GPU interconnects. While doing so,\nTensorSocket is able to train and balance differently-sized models and serve\nmultiple batch sizes simultaneously and is hardware- and pipeline-agnostic in\nnature. Our evaluation shows that TensorSocket enables scenarios that are\ninfeasible without data sharing, increases training throughput by up to 100%,\nand when utilizing cloud instances, achieves cost savings of 50% by reducing\nthe hardware resource needs on the CPU side. Furthermore, TensorSocket\noutperforms the state-of-the-art solutions for shared data loading such as\nCoorDL and Joader; it is easier to deploy and maintain and either achieves\nhigher or matches their throughput while requiring fewer CPU resources.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.18749v4", "cate": "cs.LG", "date": "2024-09-27", "updated": "2025-08-01"}
{"id": "2508.00140", "title": "Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models", "authors": ["Zhanna Kaufman", "Madeline Endres", "Cindy Xiong Bearfield", "Yuriy Brun"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00140v1", "summary": "Systems relying on ML have become ubiquitous, but so has biased behavior\nwithin them. Research shows that bias significantly affects stakeholders' trust\nin systems and how they use them. Further, stakeholders of different\nbackgrounds view and trust the same systems differently. Thus, how ML models'\nbehavior is explained plays a key role in comprehension and trust. We survey\nexplainability visualizations, creating a taxonomy of design characteristics.\nWe conduct user studies to evaluate five state-of-the-art visualization tools\n(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how\ntaxonomy characteristics affect comprehension, bias perception, and trust for\nnon-expert ML users. Surprisingly, we find an inverse relationship between\ncomprehension and trust: the better users understand the models, the less they\ntrust them. We investigate the cause and find that this relationship is\nstrongly mediated by bias perception: more comprehensible visualizations\nincrease people's perception of bias, and increased bias perception reduces\ntrust. We confirm this relationship is causal: Manipulating explainability\nvisualizations to control comprehension, bias perception, and trust, we show\nthat visualization design can significantly (p < 0.001) increase comprehension,\nincrease perceived bias, and reduce trust. Conversely, reducing perceived model\nbias, either by improving model fairness or by adjusting visualization design,\nsignificantly increases trust even when comprehension remains high. Our work\nadvances understanding of how comprehension affects trust and systematically\ninvestigates visualization's role in facilitating responsible ML applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00140v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00537", "title": "The Prosody of Emojis", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00537v1", "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00537v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2212.01233", "title": "Safe machine learning model release from Trusted Research Environments: The SACRO-ML package", "authors": ["Jim Smith", "Richard J. Preen", "Andrew McCarthy", "Maha Albashir", "Alba Crespi-Boixader", "Shahzad Mumtaz", "Christian Cole", "James Liley", "Jost Migenda", "Simon Rogers", "Yola Jones"], "categories": ["cs.LG", "cs.CR", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2212.01233v4", "summary": "We present SACRO-ML, an integrated suite of open source Python tools to\nfacilitate the statistical disclosure control (SDC) of machine learning (ML)\nmodels trained on confidential data prior to public release. SACRO-ML combines\n(i) a SafeModel package that extends commonly used ML models to provide\nante-hoc SDC by assessing the vulnerability of disclosure posed by the training\nregime; and (ii) an Attacks package that provides post-hoc SDC by rigorously\nassessing the empirical disclosure risk of a model through a variety of\nsimulated attacks after training. The SACRO-ML code and documentation are\navailable under an MIT license at https://github.com/AI-SDC/SACRO-ML", "comment": null, "pdf_url": "http://arxiv.org/pdf/2212.01233v4", "cate": "cs.LG", "date": "2022-12-02", "updated": "2025-08-01"}
{"id": "2508.00260", "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2508.00260v1", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00260v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2410.21072", "title": "Federated Time Series Generation on Feature and Temporally Misaligned Data", "authors": ["Zhi Wen Soi", "Chenrui Fan", "Aditya Shankar", "Abele Mălan", "Lydia Y. Chen"], "categories": ["cs.LG", "cs.DC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.21072v2", "summary": "Distributed time series data presents a challenge for federated learning, as\nclients often possess different feature sets and have misaligned time steps.\nExisting federated time series models are limited by the assumption of perfect\ntemporal or feature alignment across clients. In this paper, we propose FedTDD,\na novel federated time series diffusion model that jointly learns a synthesizer\nacross clients. At the core of FedTDD is a novel data distillation and\naggregation framework that reconciles the differences between clients by\nimputing the misaligned timesteps and features. In contrast to traditional\nfederated learning, FedTDD learns the correlation across clients' time series\nthrough the exchange of local synthetic outputs instead of model parameters. A\ncoordinator iteratively improves a global distiller network by leveraging\nshared knowledge from clients through the exchange of synthetic data. As the\ndistiller becomes more refined over time, it subsequently enhances the quality\nof the clients' local feature estimates, allowing each client to then improve\nits local imputations for missing data using the latest, more accurate\ndistiller. Experimental results on five datasets demonstrate FedTDD's\neffectiveness compared to centralized training, and the effectiveness of\nsharing synthetic outputs to transfer knowledge of local time series. Notably,\nFedTDD achieves 79.4% and 62.8% improvement over local training in Context-FID\nand Correlational scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.21072v2", "cate": "cs.LG", "date": "2024-10-28", "updated": "2025-08-01"}
{"id": "2508.00141", "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00141v1", "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00141v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00544", "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00544v1", "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00544v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.07761", "title": "FakeIDet: Exploring Patches for Privacy-Preserving Fake ID Detection", "authors": ["Javier Muñoz-Haro", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.07761v2", "summary": "Verifying the authenticity of identity documents (IDs) has become a critical\nchallenge for real-life applications such as digital banking, crypto-exchanges,\nrenting, etc. This study focuses on the topic of fake ID detection, covering\nseveral limitations in the field. In particular, there are no publicly\navailable data from real IDs for proper research in this area, and most\npublished studies rely on proprietary internal databases that are not available\nfor privacy reasons. In order to advance this critical challenge of real data\nscarcity that makes it so difficult to advance the technology of machine\nlearning-based fake ID detection, we introduce a new patch-based methodology\nthat trades off privacy and performance, and propose a novel patch-wise\napproach for privacy-aware fake ID detection: FakeIDet. In our experiments, we\nexplore: i) two levels of anonymization for an ID (i.e., fully- and\npseudo-anonymized), and ii) different patch size configurations, varying the\namount of sensitive data visible in the patch image. State-of-the-art methods,\nsuch as vision transformers and foundation models, are considered as backbones.\nOur results show that, on an unseen database (DLC-2021), our proposal for fake\nID detection achieves 13.91% and 0% EERs at the patch and the whole ID level,\nshowing a good generalization to other databases. In addition to the path-based\nmethodology introduced and the new FakeIDet method based on it, another key\ncontribution of our article is the release of the first publicly available\ndatabase that contains 48,400 patches from real and fake IDs, called\nFakeIDet-db, together with the experimental framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.07761v2", "cate": "cs.CV", "date": "2025-04-10", "updated": "2025-08-01"}
{"id": "2508.00265", "title": "Multimodal Referring Segmentation: A Survey", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2508.00265v1", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "pdf_url": "http://arxiv.org/pdf/2508.00265v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00130", "title": "Computation of Approximately Stable Committees in Approval-based Elections", "authors": ["Drew Gao", "Yihang Sun", "Jan Vondrák"], "categories": ["cs.GT", "cs.DM"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      18 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00130v1", "summary": "Approval-based committee selection is a model of significant interest in\nsocial choice theory. In this model, we have a set of voters $\\mathcal{V}$, a\nset of candidates $\\mathcal{C}$, and each voter has a set $A_v \\subset\n\\mathcal{C}$ of approved candidates. For any committee size $K$, the goal is to\nchoose $K$ candidates to represent the voters' preferences. We study a\ncriterion known as \\emph{approximate stability}, where a committee is\n$\\lambda$-approximately-stable if there is no other committee $T$ preferred by\nat least $\\frac{\\lambda|T|}{k} |\\mathcal{V}| $ voters. We prove that a\n$3.65$-approximately stable committee always exists and can be computed\nalgorithmically in this setting. Our approach is based on finding a Lindahl\nequilibrium and sampling from a strongly Rayleigh distribution associated with\nit.", "comment": "18 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00130v1", "cate": "cs.GT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00155", "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "authors": ["Tomasz Szczepański", "Szymon Płotka", "Michal K. Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemysław Korzeniowski", "Tomasz Trzciński", "Arkadiusz Sitek"], "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted for the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025", "url": "http://arxiv.org/abs/2508.00155v1", "summary": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains\nchallenging, especially for fine structures like root apices, which is critical\nfor assessing root resorption in orthodontics. We introduce GEPAR3D, a novel\napproach that unifies instance detection and multi-class segmentation into a\nsingle step tailored to improve root segmentation. Our method integrates a\nStatistical Shape Model of dentition as a geometric prior, capturing anatomical\ncontext and morphological consistency without enforcing restrictive adjacency\nconstraints. We leverage a deep watershed method, modeling each tooth as a\ncontinuous 3D energy basin encoding voxel distances to boundaries. This\ninstance-aware representation ensures accurate segmentation of narrow, complex\nroot apices. Trained on publicly available CBCT scans from a single center, our\nmethod is evaluated on external test sets from two in-house and two public\nmedical centers. GEPAR3D achieves the highest overall segmentation performance,\naveraging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the\nsecond-best method) and increasing recall to 95.2% (+9.5%) across all test\nsets. Qualitative analyses demonstrated substantial improvements in root\nsegmentation quality, indicating significant potential for more accurate root\nresorption assessment and enhanced clinical decision-making in orthodontics. We\nprovide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.", "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00155v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00574", "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00574v1", "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00574v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.18365", "title": "RecPS: Privacy Risk Scoring for Recommender Systems", "authors": ["Jiajie He", "Yuechun Gu", "Keke Chen"], "categories": ["cs.IR", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted by ACM RecSys 2025; to appear", "url": "http://arxiv.org/abs/2507.18365v3", "summary": "Recommender systems (RecSys) have become an essential component of many web\napplications. The core of the system is a recommendation model trained on\nhighly sensitive user-item interaction data. While privacy-enhancing techniques\nare actively studied in the research community, the real-world model\ndevelopment still depends on minimal privacy protection, e.g., via controlled\naccess. Users of such systems should have the right to choose \\emph{not} to\nshare highly sensitive interactions. However, there is no method allowing the\nuser to know which interactions are more sensitive than others. Thus,\nquantifying the privacy risk of RecSys training data is a critical step to\nenabling privacy-aware RecSys model development and deployment. We propose a\nmembership-inference attack (MIA)- based privacy scoring method, RecPS, to\nmeasure privacy risks at both the interaction and user levels. The RecPS\ninteraction-level score definition is motivated and derived from differential\nprivacy, which is then extended to the user-level scoring method. A critical\ncomponent is the interaction-level MIA method RecLiRA, which gives high-quality\nmembership estimation. We have conducted extensive experiments on well-known\nbenchmark datasets and RecSys models to show the unique features and benefits\nof RecPS scoring in risk assessment and RecSys model unlearning.", "comment": "Accepted by ACM RecSys 2025; to appear", "pdf_url": "http://arxiv.org/pdf/2507.18365v3", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-08-01"}
{"id": "2508.00272", "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "authors": ["Wenyue Chong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00272v1", "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00272v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00349", "title": "On the Equivalence of the Graph-Structural and Optimization-Based Characterizations of Popular Matchings", "authors": ["Yuga Kanaya", "Kenjiro Takazawa"], "categories": ["cs.GT", "cs.DM", "math.CO"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00349v1", "summary": "Popular matchings provide a model of matching under preferences in which a\nsolution corresponds to a Condorcet winner in voting systems. In a bipartite\ngraph in which the vertices have preferences over their neighbours, a matching\nis defined to be popular if it does not lose in a majority vote against any\nmatching. In this paper, we study the following three primary problems: only\nthe vertices on one side have preferences; a generalization of this problem\nallowing ties in the preferences; and the vertices on both sides have\npreferences. A principal issue in the algorithmic aspects of popular matchings\nis how to determine the popularity of a matching, because it requires\nexponential time if the definition is simply applied. In the literature, we\nhave the following two types of characterizations: a graph-structural\ncharacterization; and an optimization-based characterization described by\nmaximum-weight matchings. The graph-structural characterizations are\nspecifically designed for each problem and provide a combinatorial structure of\nthe popular matchings. The optimization-based characterizations work in the\nsame manner for all problems, while they do not reveal the structure of the\npopular matchings. A main contribution of this paper is to provide a direct\nconnection of the above two types of characterizations for all of the three\nproblems. Specifically, we prove that each characterization can be derived from\nthe other, without relying on the fact that they characterize popular\nmatchings. Our proofs offer a comprehensive understanding of the equivalence of\nthe two types of characterizations, and suggest a new interpretation of the\ngraph-structural characterization in terms of the dual optimal solution for the\nmaximum-weight matching problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00349v1", "cate": "cs.GT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00160", "title": "DeformTune: A Deformable XAI Music Prototype for Non-Musicians", "authors": ["Ziqing Xu", "Nick Bryan-Kinns"], "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485", "url": "http://arxiv.org/abs/2508.00160v1", "summary": "Many existing AI music generation tools rely on text prompts, complex\ninterfaces, or instrument-like controls, which may require musical or technical\nknowledge that non-musicians do not possess. This paper introduces DeformTune,\na prototype system that combines a tactile deformable interface with the\nMeasureVAE model to explore more intuitive, embodied, and explainable AI\ninteraction. We conducted a preliminary study with 11 adult participants\nwithout formal musical training to investigate their experience with\nAI-assisted music creation. Thematic analysis of their feedback revealed\nrecurring challenge--including unclear control mappings, limited expressive\nrange, and the need for guidance throughout use. We discuss several design\nopportunities for enhancing explainability of AI, including multimodal feedback\nand progressive interaction support. These findings contribute early insights\ntoward making AI music systems more explainable and empowering for novice\nusers.", "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "pdf_url": "http://arxiv.org/pdf/2508.00160v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00600", "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00600v1", "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00600v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00287", "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00287v1", "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00287v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2501.16802", "title": "Ancilla-free Quantum Adder with Sublinear Depth", "authors": ["Maxime Remaud", "Vivien Vandaele"], "categories": ["quant-ph", "cs.DM"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      V2: Add a section with incrementor and addition of a constant", "url": "http://arxiv.org/abs/2501.16802v2", "summary": "We present the first exact quantum adder with sublinear depth and no ancilla\nqubits. Our construction is based on classical reversible logic only and\nemploys low-depth implementations for the CNOT ladder operator and the Toffoli\nladder operator, two key components to perform ripple-carry addition. Namely,\nwe demonstrate that any ladder of $n$ CNOT gates can be replaced by a\nCNOT-circuit with $O(\\log n)$ depth, while maintaining a linear number of\ngates. We then generalize this construction to Toffoli gates and demonstrate\nthat any ladder of $n$ Toffoli gates can be substituted with a circuit with\n$O(\\log^2 n)$ depth while utilizing a linearithmic number of gates. This builds\non the recent works of Nie et al. and Khattar and Gidney on the technique of\nconditionally clean ancillae. By combining these two key elements, we present a\nnovel approach to design quantum adders that can perform the addition of two\n$n$-bit numbers in depth $O(\\log^2 n)$ without the use of any ancilla and using\nclassical reversible logic only (Toffoli, CNOT and X gates). We also present\nnew constructions for incrementing and adding a constant to a quantum register.", "comment": "V2: Add a section with incrementor and addition of a constant", "pdf_url": "http://arxiv.org/pdf/2501.16802v2", "cate": "quant-ph", "date": "2025-01-28", "updated": "2025-08-01"}
{"id": "2508.00178", "title": "The SPACE of AI: Real-World Lessons on AI's Impact on Developers", "authors": ["Brian Houck", "Travis Lowdermilk", "Cody Beyer", "Steven Clarke", "Ben Hanrahan"], "categories": ["cs.HC", "cs.AI", "cs.SE"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00178v1", "summary": "As artificial intelligence (AI) tools become increasingly embedded in\nsoftware development workflows, questions persist about their true impact on\ndeveloper productivity and experience. This paper presents findings from a\nmixed-methods study examining how developers perceive AI's influence across the\ndimensions of the SPACE framework: Satisfaction, Performance, Activity,\nCollaboration and Efficiency. Drawing on survey responses from over 500\ndevelopers and qualitative insights from interviews and observational studies,\nwe find that AI is broadly adopted and widely seen as enhancing productivity,\nparticularly for routine tasks. However, the benefits vary, depending on task\ncomplexity, individual usage patterns, and team-level adoption. Developers\nreport increased efficiency and satisfaction, with less evidence of impact on\ncollaboration. Organizational support and peer learning play key roles in\nmaximizing AI's value. These findings suggest that AI is augmenting developers\nrather than replacing them, and that effective integration depends as much on\nteam culture and support structures as on the tools themselves. We conclude\nwith practical recommendations for teams, organizations and researchers seeking\nto harness AI's potential in software engineering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00178v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00605", "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00605v1", "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00605v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00289", "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2508.00289v1", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00289v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00180", "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "authors": ["Adam Block", "Cyril Zhang"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00180v1", "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00180v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00614", "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00614v1", "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00614v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00298", "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2412.00837", "url": "http://arxiv.org/abs/2508.00298v1", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "pdf_url": "http://arxiv.org/pdf/2508.00298v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00776", "title": "From Dynamic Programs to Greedy Algorithms", "authors": ["Dieter van Melkebeek"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      14 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00776v1", "summary": "We show for several computational problems how classical greedy algorithms\nfor special cases can be derived in a simple way from dynamic programs for the\ngeneral case: interval scheduling (restricted to unit weights), knapsack\n(restricted to unit values), and shortest paths (restricted to nonnegative edge\nlengths). Conceptually, we repeatedly expand the Bellman equations underlying\nthe dynamic program and use straightforward monotonicity properties to figure\nout which terms yield the optimal value under the respective restrictions. The\napproach offers an alternative for developing these greedy algorithms in\nundergraduate algorithms courses and/or for arguing their correctness. In the\nsetting of interval scheduling, it elucidates the change in order from earliest\nstart time first for the memoized dynamic program to earliest finish time first\nfor the greedy algorithm.", "comment": "14 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00776v1", "cate": "cs.DS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00202", "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures, under review at CAMSAP 2025", "url": "http://arxiv.org/abs/2508.00202v1", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "pdf_url": "http://arxiv.org/pdf/2508.00202v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00619", "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "authors": ["Shantanu Thorat", "Andrew Caines"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      MPhil in Advanced Computer Science thesis for University of Cambridge", "url": "http://arxiv.org/abs/2508.00619v1", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "pdf_url": "http://arxiv.org/pdf/2508.00619v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00299", "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Workshop (HiGen)", "url": "http://arxiv.org/abs/2508.00299v1", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "comment": "ICCV 2025 Workshop (HiGen)", "pdf_url": "http://arxiv.org/pdf/2508.00299v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2209.12587", "title": "TGLib: An Open-Source Library for Temporal Graph Analysis", "authors": ["Lutz Oettershagen", "Petra Mutzel"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      TGLib is now easily installable via pip (pip install temporalgraphlib). This revision adds PyPI installation instructions, a Python usage example, and minor improvements for clarity", "url": "http://arxiv.org/abs/2209.12587v2", "summary": "We initiate an open-source library for the efficient analysis of temporal\ngraphs. We consider one of the standard models of dynamic networks in which\neach edge has a discrete timestamp and transition time. Recently there has been\na massive interest in analyzing such temporal graphs. Common computational data\nmining and analysis tasks include the computation of temporal distances,\ncentrality measures, and network statistics like topological overlap,\nburstiness, or temporal diameter. To fulfill the increasing demand for\nefficient and easy-to-use implementations of temporal graph algorithms, we\nintroduce the open-source library TGLib, which integrates efficient data\nstructures and algorithms for temporal graph analysis. TGLib is highly\nefficient and versatile, providing simple and convenient C++ and Python\ninterfaces, targeting computer scientists, practitioners, students, and the\n(temporal) network research community.", "comment": "TGLib is now easily installable via pip (pip install\n  temporalgraphlib). This revision adds PyPI installation instructions, a\n  Python usage example, and minor improvements for clarity", "pdf_url": "http://arxiv.org/pdf/2209.12587v2", "cate": "cs.DS", "date": "2022-09-26", "updated": "2025-08-01"}
{"id": "2508.00212", "title": "Reinitializing weights vs units for maintaining plasticity in neural networks", "authors": ["J. Fernando Hernandez-Garcia", "Shibhansh Dohare", "Jun Luo", "Rich S. Sutton"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00212v1", "summary": "Loss of plasticity is a phenomenon in which a neural network loses its\nability to learn when trained for an extended time on non-stationary data. It\nis a crucial problem to overcome when designing systems that learn continually.\nAn effective technique for preventing loss of plasticity is reinitializing\nparts of the network. In this paper, we compare two different reinitialization\nschemes: reinitializing units vs reinitializing weights. We propose a new\nalgorithm, which we name \\textit{selective weight reinitialization}, for\nreinitializing the least useful weights in a network. We compare our algorithm\nto continual backpropagation and ReDo, two previously proposed algorithms that\nreinitialize units in the network. Through our experiments in continual\nsupervised learning problems, we identify two settings when reinitializing\nweights is more effective at maintaining plasticity than reinitializing units:\n(1) when the network has a small number of units and (2) when the network\nincludes layer normalization. Conversely, reinitializing weights and units are\nequally effective at maintaining plasticity when the network is of sufficient\nsize and does not include layer normalization. We found that reinitializing\nweights maintains plasticity in a wider variety of settings than reinitializing\nunits.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00212v1", "cate": "cs.NE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00669", "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00669v1", "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00669v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00308", "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2508.00308v1", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00308v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2309.16911", "title": "Dynamic Batching of Online Arrivals to Leverage Economies of Scale", "authors": ["Akhil Bhimaraju", "S. Rasoul Etesami", "Lav R. Varshney"], "categories": ["cs.DS", "math.OC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      35 pages, accepted for publication by the European Journal of Operational Research", "url": "http://arxiv.org/abs/2309.16911v2", "summary": "Many settings, such as matching riders to drivers in ride-hailing platforms\nor in-stream video advertising, require handling arrivals over time. In such\napplications, it is often beneficial to group the arriving orders or requests\ninto batches and process the larger batches rather than individual arrivals.\nHowever, waiting too long to create larger batches incurs a waiting cost for\npast arrivals. On the other hand, processing the arrivals too soon leads to\nhigher processing costs by missing the economies of scale of grouping larger\nnumbers of arrivals into larger batches. Moreover, the timing of the next\narrival is often unknown, meaning fixed-size batches or fixed waiting times\ntend to be poor choices. In this work, we consider the problem of finding the\noptimal batching schedule to minimize the sum of waiting time and processing\ncost under both offline and online settings. In the offline problem in which\nall arrival times are known a priori, we show that the optimal batching\nschedule can be found in polynomial time by reducing it to a shortest path\nproblem on a weighted acyclic graph. For the online problem with unknown\narrival times, we develop algorithms that are provably competitive for a broad\nrange of processing-cost functions. We also provide a lower bound on the\ncompetitive ratio that no online algorithm can beat. Finally, we run numerical\nexperiments on simulated and real data to demonstrate the effectiveness of our\nalgorithms against the offline benchmark.", "comment": "35 pages, accepted for publication by the European Journal of\n  Operational Research", "pdf_url": "http://arxiv.org/pdf/2309.16911v2", "cate": "cs.DS", "date": "2023-09-29", "updated": "2025-07-31"}
{"id": "2508.00235", "title": "Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior", "authors": ["Erin Rainville", "Amirhossein Rasoulian", "Hassan Rivaz", "Yiming Xiao"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 Workshop CVAMD", "url": "http://arxiv.org/abs/2508.00235v1", "summary": "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels\nthat, if ruptured, can lead to life-threatening consequences. However, their\nsmall size and soft contrast in radiological scans often make it difficult to\nperform accurate and efficient detection and morphological analyses, which are\ncritical in the clinical care of the disorder. Furthermore, the lack of large\npublic datasets with voxel-wise expert annotations pose challenges for\ndeveloping deep learning algorithms to address the issues. Therefore, we\nproposed a novel weakly supervised 3D multi-task UNet that integrates\nvesselness priors to jointly perform aneurysm detection and segmentation in\ntime-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA\ndetection and segmentation, we employ the popular Frangi's vesselness filter to\nderive soft cerebrovascular priors for both network input and an attention\nblock to conduct segmentation from the decoder and detection from an auxiliary\nbranch. We train our model on the Lausanne dataset with coarse ground truth\nsegmentation, and evaluate it on the test set with refined labels from the same\ndatabase. To further assess our model's generalizability, we also validate it\nexternally on the ADAM dataset. Our results demonstrate the superior\nperformance of the proposed technique over the SOTA techniques for aneurysm\nsegmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =\n1.47, sensitivity = 92.9%).", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "pdf_url": "http://arxiv.org/pdf/2508.00235v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00673", "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint. Under review", "url": "http://arxiv.org/abs/2508.00673v1", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "comment": "Preprint. Under review", "pdf_url": "http://arxiv.org/pdf/2508.00673v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00311", "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios", "authors": ["Yufeng Zhong", "Zhixiong Zeng", "Lei Chen", "Longrong Yang", "Liming Zheng", "Jing Huang", "Siqi Yang", "Lin Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00311v1", "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00311v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2501.17563", "title": "Search Trees on Trees via LP", "authors": ["Yaniv Sadeh", "Haim Kaplan", "Uri Zwick"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      No content change in this version, just added funding information", "url": "http://arxiv.org/abs/2501.17563v2", "summary": "We consider the problem of computing optimal search trees on trees (STTs).\nSTTs generalize binary search trees (BSTs) in which we search nodes in a path\n(linear order) to search trees that facilitate search over general tree\ntopologies. Golinsky proposed a linear programming (LP) relaxation of the\nproblem of computing an optimal static STT over a given tree topology. He used\nthis LP formulation to compute an STT that is a $2$-approximation to an optimal\nSTT, and conjectured that it is, in fact, an extended formulation of the\nconvex-hull of all depths-vectors of STTs, and thus always gives an optimal\nsolution. In this work we study this LP approach further. We show that the\nconjecture is false and that Golinsky's LP does not always give an optimal\nsolution. To show this we use what we call the ``normals method''. We use this\nmethod to enumerate over vertices of Golinsky's polytope for all tree\ntopologies of no more than 8 nodes. We give a lower bound on the integrality\ngap of the LP and on the approximation ratio of Golinsky's rounding method. We\nfurther enumerate several research directions that can lead to the resolution\nof the question whether one can compute an optimal STT in polynomial time.", "comment": "No content change in this version, just added funding information", "pdf_url": "http://arxiv.org/pdf/2501.17563v2", "cate": "cs.DS", "date": "2025-01-29", "updated": "2025-08-01"}
{"id": "2508.00239", "title": "What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance", "authors": ["Jacqueline Elise Bruen", "Myounghoon Jeon"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts 2025) arXiv:2406.14485", "url": "http://arxiv.org/abs/2508.00239v1", "summary": "With the development of generative artificial intelligence (GenAI) tools to\ncreate art, stakeholders cannot come to an agreement on the value of these\nworks. In this study we uncovered the mixed opinions surrounding art made by\nAI. We developed two versions of a dance performance augmented by technology\neither with or without GenAI. For each version we informed audiences of the\nperformance's development either before or after a survey on their perceptions\nof the performance. There were thirty-nine participants (13 males, 26 female)\ndivided between the four performances. Results demonstrated that individuals\nwere more inclined to attribute artistic merit to works made by GenAI when they\nwere unaware of its use. We present this case study as a call to address the\nimportance of utilizing the social context and the users' interpretations of\nGenAI in shaping a technical explanation, leading to a greater discussion that\ncan bridge gaps in understanding.", "comment": "In Proceedings of Explainable AI for the Arts Workshop 2025 (XAIxArts\n  2025) arXiv:2406.14485", "pdf_url": "http://arxiv.org/pdf/2508.00239v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00675", "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "authors": ["Gleb Schmidt", "Johannes Römisch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00675v1", "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00675v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00312", "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00312v1", "summary": "Video anomaly detection (VAD) plays a critical role in public safety\napplications such as intelligent surveillance. However, the rarity,\nunpredictability, and high annotation cost of real-world anomalies make it\ndifficult to scale VAD datasets, which limits the performance and\ngeneralization ability of existing models. To address this challenge, we\npropose a generative video-enhanced weakly-supervised video anomaly detection\n(GV-VAD) framework that leverages text-conditioned video generation models to\nproduce semantically controllable and physically plausible synthetic videos.\nThese virtual videos are used to augment training data at low cost. In\naddition, a synthetic sample loss scaling strategy is utilized to control the\ninfluence of generated synthetic samples for efficient training. The\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods on UCF-Crime datasets. The code is available at\nhttps://github.com/Sumutan/GV-VAD.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00312v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.14683", "title": "Polynomial-Time Constant-Approximation for Fair Sum-of-Radii Clustering", "authors": ["Sina Bagheri Nezhad", "Sayan Bandyapadhyay", "Tianzhi Chen"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted at 33rd Annual European Symposium on Algorithms (ESA 2025)", "url": "http://arxiv.org/abs/2504.14683v2", "summary": "In a seminal work, Chierichetti et al. introduced the $(t,k)$-fair clustering\nproblem: Given a set of red points and a set of blue points in a metric space,\na clustering is called fair if the number of red points in each cluster is at\nmost $t$ times and at least $1/t$ times the number of blue points in that\ncluster. The goal is to compute a fair clustering with at most $k$ clusters\nthat optimizes certain objective function. Considering this problem, they\ndesigned a polynomial-time $O(1)$- and $O(t)$-approximation for the $k$-center\nand the $k$-median objective, respectively. Recently, Carta et al. studied this\nproblem with the sum-of-radii objective and obtained a\n$(6+\\epsilon)$-approximation with running time\n$O((k\\log_{1+\\epsilon}(k/\\epsilon))^kn^{O(1)})$, i.e., fixed-parameter\ntractable in $k$. Here $n$ is the input size. In this work, we design the first\npolynomial-time $O(1)$-approximation for $(t,k)$-fair clustering with the\nsum-of-radii objective, improving the result of Carta et al. Our result places\nsum-of-radii in the same group of objectives as $k$-center, that admit\npolynomial-time $O(1)$-approximations. This result also implies a\npolynomial-time $O(1)$-approximation for the Euclidean version of the problem,\nfor which an $f(k)\\cdot n^{O(1)}$-time $(1+\\epsilon)$-approximation was known\ndue to Drexler et al.. Here $f$ is an exponential function of $k$. We are also\nable to extend our result to any arbitrary $\\ell\\ge 2$ number of colors when\n$t=1$. This matches known results for the $k$-center and $k$-median objectives\nin this case. The significant disparity of sum-of-radii compared to $k$-center\nand $k$-median presents several complex challenges, all of which we\nsuccessfully overcome in our work. Our main contribution is a novel\ncluster-merging-based analysis technique for sum-of-radii that helps us achieve\nthe constant-approximation bounds.", "comment": "Accepted at 33rd Annual European Symposium on Algorithms (ESA 2025)", "pdf_url": "http://arxiv.org/pdf/2504.14683v2", "cate": "cs.DS", "date": "2025-04-20", "updated": "2025-08-01"}
{"id": "2504.10053", "title": "Synthetic Biology meets Neuromorphic Computing: Towards a bio-inspired Olfactory Perception System", "authors": ["Kevin Max", "Larissa Sames", "Shimeng Ye", "Jan Steinkühler", "Federico Corradi"], "categories": ["cs.NE", "cs.ET", "q-bio.NC"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Updated after revision at Neuromorphic Computing and Engineering", "url": "http://arxiv.org/abs/2504.10053v2", "summary": "In this study, we explore how the combination of synthetic biology,\nneuroscience modeling, and neuromorphic electronic systems offers a new\napproach to creating an artificial system that mimics the natural sense of\nsmell. We argue that a co-design approach offers significant advantages in\nreplicating the complex dynamics of odor sensing and processing. We propose a\nhybrid system of synthetic sensory neurons that provides three key features:\n(a) receptor-gated ion channels, (b) interface between synthetic biology and\nsemiconductors and (c) event-based encoding and computing based on spiking\nnetworks. Our approach is validated using simulation-based modeling of the\ncomplete sensing and processing pipeline. This research seeks to develop a\nplatform for ultra-sensitive, specific, and energy-efficient odor detection,\nwith potential implications for environmental monitoring, medical diagnostics,\nand security.", "comment": "Updated after revision at Neuromorphic Computing and Engineering", "pdf_url": "http://arxiv.org/pdf/2504.10053v2", "cate": "cs.NE", "date": "2025-04-14", "updated": "2025-08-01"}
{"id": "2508.00250", "title": "Jet Image Generation in High Energy Physics Using Diffusion Models", "authors": ["Victor D. Martinez", "Vidya Manian", "Sudhir Malik"], "categories": ["hep-ph", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      The paper is under review at IEEE Transactions in Nuclear Science", "url": "http://arxiv.org/abs/2508.00250v1", "summary": "This article presents, for the first time, the application of diffusion\nmodels for generating jet images corresponding to proton-proton collision\nevents at the Large Hadron Collider (LHC). The kinematic variables of quark,\ngluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset\nare mapped to two-dimensional image representations. Diffusion models are\ntrained on these images to learn the spatial distribution of jet constituents.\nWe compare the performance of score-based diffusion models and consistency\nmodels in accurately generating class-conditional jet images. Unlike approaches\nbased on latent distributions, our method operates directly in image space. The\nfidelity of the generated images is evaluated using several metrics, including\nthe Fr\\'echet Inception Distance (FID), which demonstrates that consistency\nmodels achieve higher fidelity and generation stability compared to score-based\ndiffusion models. These advancements offer significant improvements in\ncomputational efficiency and generation accuracy, providing valuable tools for\nHigh Energy Physics (HEP) research.", "comment": "The paper is under review at IEEE Transactions in Nuclear Science", "pdf_url": "http://arxiv.org/pdf/2508.00250v1", "cate": "hep-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00679", "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00679v1", "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00679v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00319", "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00319v1", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00319v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.10591", "title": "Lattice Surgery Compilation Beyond the Surface Code", "authors": ["Laura S. Herzog", "Lucas Berent", "Aleksander Kubica", "Robert Wille"], "categories": ["quant-ph", "cs.ET"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      12 pages, 11 figures", "url": "http://arxiv.org/abs/2504.10591v2", "summary": "Large-scale fault-tolerant quantum computation requires compiling logical\ncircuits into physical operations tailored to a given architecture. Prior work\naddressing this challenge has mostly focused on the surface code and lattice\nsurgery schemes. In this work, we broaden the scope by considering lattice\nsurgery compilation for topological codes beyond the surface code. We begin by\ndefining a code substrate - a blueprint for implementing topological codes and\nlattice surgery. We then abstract from the microscopic details and rephrase the\ncompilation task as a mapping and routing problem on a macroscopic routing\ngraph, potentially subject to substrate-specific constraints. We explore\nspecific substrates and codes, including the color code and the folded surface\ncode, providing detailed microscopic constructions. For the color code, we\npresent numerical simulations analyzing how design choices at the microscopic\nand macroscopic levels affect the depth of compiled logical\n$\\mathrm{CNOT}+\\mathrm{T}$ circuits. An open-source code is available on GitHub\nhttps://github.com/cda-tum/mqt-qecc.", "comment": "12 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2504.10591v2", "cate": "quant-ph", "date": "2025-04-14", "updated": "2025-08-01"}
{"id": "2508.00255", "title": "Accurate and Consistent Graph Model Generation from Text with Large Language Models", "authors": ["Boqi Chen", "Ou Wei", "Bingzhou Zheng", "Gunter Mussbacher"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted at ACM / IEEE 28th International Conference on Model Driven Engineering Languages and Systems (MODELS 2025)", "url": "http://arxiv.org/abs/2508.00255v1", "summary": "Graph model generation from natural language description is an important task\nwith many applications in software engineering. With the rise of large language\nmodels (LLMs), there is a growing interest in using LLMs for graph model\ngeneration. Nevertheless, LLM-based graph model generation typically produces\npartially correct models that suffer from three main issues: (1) syntax\nviolations: the generated model may not adhere to the syntax defined by its\nmetamodel, (2) constraint inconsistencies: the structure of the model might not\nconform to some domain-specific constraints, and (3) inaccuracy: due to the\ninherent uncertainty in LLMs, the models can include inaccurate, hallucinated\nelements. While the first issue is often addressed through techniques such as\nconstraint decoding or filtering, the latter two remain largely unaddressed.\nMotivated by recent self-consistency approaches in LLMs, we propose a novel\nabstraction-concretization framework that enhances the consistency and quality\nof generated graph models by considering multiple outputs from an LLM. Our\napproach first constructs a probabilistic partial model that aggregates all\ncandidate outputs and then refines this partial model into the most appropriate\nconcrete model that satisfies all constraints. We evaluate our framework on\nseveral popular open-source and closed-source LLMs using diverse datasets for\nmodel generation tasks. The results demonstrate that our approach significantly\nimproves both the consistency and quality of the generated graph models.", "comment": "Accepted at ACM / IEEE 28th International Conference on Model Driven\n  Engineering Languages and Systems (MODELS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00255v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00680", "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "authors": ["Johannes Römisch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00680v1", "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00680v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00330", "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00330v1", "summary": "This paper introduces a practical and accurate calibration method for camera\nspectral sensitivity using a diffraction grating. Accurate calibration of\ncamera spectral sensitivity is crucial for various computer vision tasks,\nincluding color correction, illumination estimation, and material analysis.\nUnlike existing approaches that require specialized narrow-band filters or\nreference targets with known spectral reflectances, our method only requires an\nuncalibrated diffraction grating sheet, readily available off-the-shelf. By\ncapturing images of the direct illumination and its diffracted pattern through\nthe grating sheet, our method estimates both the camera spectral sensitivity\nand the diffraction grating parameters in a closed-form manner. Experiments on\nsynthetic and real-world data demonstrate that our method outperforms\nconventional reference target-based methods, underscoring its effectiveness and\npracticality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00330v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00256", "title": "Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study", "authors": ["Chuang Zhang", "Geng Sun", "Jiacheng Wang", "Yijing Lin", "Weijie Yuan", "Sinem Coleri", "Dusit Niyato", "Tony Q. S. Quek"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to IEEE Communications Magazine for consideration", "url": "http://arxiv.org/abs/2508.00256v1", "summary": "Low-altitude wireless networks (LAWNs) have the potential to revolutionize\ncommunications by supporting a range of applications, including urban parcel\ndelivery, aerial inspections and air taxis. However, compared with traditional\nwireless networks, LAWNs face unique security challenges due to low-altitude\noperations, frequent mobility and reliance on unlicensed spectrum, making it\nmore vulnerable to some malicious attacks. In this paper, we investigate some\nlarge artificial intelligence model (LAM)-enabled solutions for secure\ncommunications in LAWNs. Specifically, we first explore the amplified security\nrisks and important limitations of traditional AI methods in LAWNs. Then, we\nintroduce the basic concepts of LAMs and delve into the role of LAMs in\naddressing these challenges. To demonstrate the practical benefits of LAMs for\nsecure communications in LAWNs, we propose a novel LAM-based optimization\nframework that leverages large language models (LLMs) to generate enhanced\nstate features on top of handcrafted representations, and to design intrinsic\nrewards accordingly, thereby improving reinforcement learning performance for\nsecure communication tasks. Through a typical case study, simulation results\nvalidate the effectiveness of the proposed framework. Finally, we outline\nfuture directions for integrating LAMs into secure LAWN applications.", "comment": "This paper has been submitted to IEEE Communications Magazine for\n  consideration", "pdf_url": "http://arxiv.org/pdf/2508.00256v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00709", "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00709v1", "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00709v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00356", "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning", "authors": ["Angelos Vlachos", "Giorgos Filandrianos", "Maria Lymperaiou", "Nikolaos Spanos", "Ilias Mitsouras", "Vasileios Karampinis", "Athanasios Voulodimos"], "categories": ["cs.CV", "cs.MA", "I.2; I.2.7"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00356v1", "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00356v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00264", "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the Forty-second International Conference on Machine Learning (ICML) 2025. First two authors contributed equally", "url": "http://arxiv.org/abs/2508.00264v1", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "pdf_url": "http://arxiv.org/pdf/2508.00264v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00719", "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00719v1", "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00719v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00358", "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering", "authors": ["Yan Gong", "Mengjun Chen", "Hao Liu", "Gao Yongsheng", "Lei Yang", "Naibang Wang", "Ziying Song", "Haoqun Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 7 figures, 5 tables", "url": "http://arxiv.org/abs/2508.00358v1", "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.", "comment": "9 pages, 7 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2508.00358v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00294", "title": "Formal Power Series Representations in Probability and Expected Utility Theory", "authors": ["Arthur Paul Pedersen", "Samuel Allen Alexander"], "categories": ["math.PR", "cs.AI", "econ.TH", "math.LO", "math.ST", "stat.TH", "60A05"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00294v1", "summary": "We advance a general theory of coherent preference that surrenders\nrestrictions embodied in orthodox doctrine. This theory enjoys the property\nthat any preference system admits extension to a complete system of\npreferences, provided it satisfies a certain coherence requirement analogous to\nthe one de Finetti advanced for his foundations of probability. Unlike de\nFinetti's theory, the one we set forth requires neither transitivity nor\nArchimedeanness nor boundedness nor continuity of preference. This theory also\nenjoys the property that any complete preference system meeting the standard of\ncoherence can be represented by utility in an ordered field extension of the\nreals. Representability by utility is a corollary of this paper's central\nresult, which at once extends H\\\"older's Theorem and strengthens Hahn's\nEmbedding Theorem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00294v1", "cate": "math.PR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00741", "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00741v1", "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00741v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00359", "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective", "authors": ["Zongheng Tang", "Yi Liu", "Yifan Sun", "Yulu Gao", "Jinyu Chen", "Runsheng Xu", "Si Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV25 (Highlight)", "url": "http://arxiv.org/abs/2508.00359v1", "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.", "comment": "ICCV25 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2508.00359v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00738", "title": "Tool-Assisted Conformance Checking to Reference Process Models", "authors": ["Bernhard Rumpe", "Max Stachon", "Sebastian Stüber", "Valdes Voufo"], "categories": ["cs.SE", "cs.FL", "68N30", "D.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00738v2", "summary": "Reference models convey best practices and standards. The reference\nframeworks necessitate conformance checks to ensure adherence to established\nguidelines and principles, which is crucial for maintaining quality and\nconsistency in various processes. This paper explores automated conformance\nchecks for concrete process models against reference models using causal\ndependency analysis of tasks and events. Existing notions of conformance\nchecking for process models focus on verifying process execution traces and\nlack the expressiveness and automation needed for semantic model comparison,\nleaving this question unresolved. We integrate our approach into a broader\nsemantic framework for defining reference model conformance. We outline an\nalgorithm for reference process model conformance checking, evaluate it through\na case study, and discuss its strengths and limitations. Our research provides\na tool-assisted solution enhancing accuracy and flexibility in process model\nconformance verification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00738v2", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2508.00300", "title": "MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems", "authors": ["Shruthi Chari", "Oshani Seneviratne", "Prithwish Chakraborty", "Pablo Meyer", "Deborah L. McGuinness"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00300v1", "summary": "Explanations are crucial for building trustworthy AI systems, but a gap often\nexists between the explanations provided by models and those needed by users.\nTo address this gap, we introduce MetaExplainer, a neuro-symbolic framework\ndesigned to generate user-centered explanations. Our approach employs a\nthree-stage process: first, we decompose user questions into machine-readable\nformats using state-of-the-art large language models (LLM); second, we delegate\nthe task of generating system recommendations to model explainer methods; and\nfinally, we synthesize natural language explanations that summarize the\nexplainer outputs. Throughout this process, we utilize an Explanation Ontology\nto guide the language models and explainer methods. By leveraging LLMs and a\nstructured approach to explanation generation, MetaExplainer aims to enhance\nthe interpretability and trustworthiness of AI systems across various\napplications, providing users with tailored, question-driven explanations that\nbetter meet their needs. Comprehensive evaluations of MetaExplainer demonstrate\na step towards evaluating and utilizing current state-of-the-art explanation\nframeworks. Our results show high performance across all stages, with a 59.06%\nF1-score in question reframing, 70% faithfulness in model explanations, and 67%\ncontext-utilization in natural language synthesis. User studies corroborate\nthese findings, highlighting the creativity and comprehensiveness of generated\nexplanations. Tested on the Diabetes (PIMA Indian) tabular dataset,\nMetaExplainer supports diverse explanation types, including Contrastive,\nCounterfactual, Rationale, Case-Based, and Data explanations. The framework's\nversatility and traceability from using ontology to guide LLMs suggest broad\napplicability beyond the tested scenarios, positioning MetaExplainer as a\npromising tool for enhancing AI explainability across various domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00300v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00742", "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      26 pages, 14 figures", "url": "http://arxiv.org/abs/2508.00742v1", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "comment": "26 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2508.00742v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00361", "title": "Honey Classification using Hyperspectral Imaging and Machine Learning", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00361v1", "summary": "In this paper, we propose a machine learning-based method for automatically\nclassifying honey botanical origins. Dataset preparation, feature extraction,\nand classification are the three main steps of the proposed method. We use a\nclass transformation method in the dataset preparation phase to maximize the\nseparability across classes. The feature extraction phase employs the Linear\nDiscriminant Analysis (LDA) technique for extracting relevant features and\nreducing the number of dimensions. In the classification phase, we use Support\nVector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the\nextracted features of honey samples into their botanical origins. We evaluate\nour system using a standard honey hyperspectral imaging (HSI) dataset.\nExperimental findings demonstrate that the proposed system produces\nstate-of-the-art results on this dataset, achieving the highest classification\naccuracy of 95.13% for hyperspectral image-based classification and 92.80% for\nhyperspectral instance-based classification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00361v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00749", "title": "Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures", "authors": ["Johanna Grahl", "Bernhard Rumpe", "Max Stachon", "Sebastian Stüber"], "categories": ["cs.SE", "cs.FL", "cs.SC", "68N30", "D.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00749v1", "summary": "In the context of model-driven development, ensuring the correctness and\nconsistency of evolving models is paramount. This paper investigates the\napplication of Dynamic Symbolic Execution (DSE) for semantic difference\nanalysis of component-and-connector architectures, specifically utilizing\nMontiArc models. We have enhanced the existing MontiArc-to-Java generator to\ngather both symbolic and concrete execution data at runtime, encompassing\ntransition conditions, visited states, and internal variables of automata. This\ndata facilitates the identification of significant execution traces that\nprovide critical insights into system behavior. We evaluate various execution\nstrategies based on the criteria of runtime efficiency, minimality, and\ncompleteness, establishing a framework for assessing the applicability of DSE\nin semantic difference analysis. Our findings indicate that while DSE shows\npromise for analyzing component and connector architectures, scalability\nremains a primary limitation, suggesting further research is needed to enhance\nits practical utility in larger systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00749v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00307", "title": "Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization", "authors": ["Belman Jahir Rodriguez", "Sergio F. Chevtchenko", "Marcelo Herrera Martinez", "Yeshwant Bethy", "Saeed Afshar"], "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00307v1", "summary": "We introduce a U-net model for 360{\\deg} acoustic source localization\nformulated as a spherical semantic segmentation task. Rather than regressing\ndiscrete direction-of-arrival (DoA) angles, our model segments beamformed audio\nmaps (azimuth and elevation) into regions of active sound presence. Using\ndelay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate\nsignals aligned with drone GPS telemetry to create binary supervision masks. A\nmodified U-Net, trained on frequency-domain representations of these maps,\nlearns to identify spatially distributed source regions while addressing class\nimbalance via the Tversky loss. Because the network operates on beamformed\nenergy maps, the approach is inherently array-independent and can adapt to\ndifferent microphone configurations without retraining from scratch. The\nsegmentation outputs are post-processed by computing centroids over activated\nregions, enabling robust DoA estimates. Our dataset includes real-world\nopen-field recordings of a DJI Air 3 drone, synchronized with 360{\\deg} video\nand flight logs across multiple dates and locations. Experimental results show\nthat U-net generalizes across environments, providing improved angular\nprecision, offering a new paradigm for dense spatial audio understanding beyond\ntraditional Sound Source Localization (SSL).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00307v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00743", "title": "Agentic large language models improve retrieval-based radiology question answering", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald Köstler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00743v1", "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00743v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00366", "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2508.00366v1", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or\nscene from few RGB images. The latest methods are either generalization-based\nor overfitting-based. However, the generalization-based methods do not\ngeneralize well on views that were unseen during training, while the\nreconstruction quality of overfitting-based methods is still limited by the\nlimited geometry clues. To address this issue, we propose SparseRecon, a novel\nneural implicit reconstruction method for sparse views with volume\nrendering-based feature consistency and uncertainty-guided depth constraint.\nFirstly, we introduce a feature consistency loss across views to constrain the\nneural implicit field. This design alleviates the ambiguity caused by\ninsufficient consistency information of views and ensures completeness and\nsmoothness in the reconstruction results. Secondly, we employ an\nuncertainty-guided depth constraint to back up the feature consistency loss in\nareas with occlusion and insignificant features, which recovers geometry\ndetails for better reconstruction quality. Experimental results demonstrate\nthat our method outperforms the state-of-the-art methods, which can produce\nhigh-quality geometry with sparse-view input, especially in the scenarios with\nsmall overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00366v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.09769", "title": "Risk-Aware Autonomous Driving with Linear Temporal Logic Specifications", "authors": ["Shuhao Qi", "Zengjie Zhang", "Zhiyong Sun", "Sofie Haesaert"], "categories": ["eess.SY", "cs.FL", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.09769v3", "summary": "Human drivers naturally balance the risks of different concerns while\ndriving, including traffic rule violations, minor accidents, and fatalities.\nHowever, achieving the same behavior in autonomous driving systems remains an\nopen problem. This paper extends a risk metric that has been verified in\nhuman-like driving studies to encompass more complex driving scenarios\nspecified by linear temporal logic (LTL) that go beyond just collision risks.\nThis extension incorporates the timing and severity of events into LTL\nspecifications, thereby reflecting a human-like risk awareness. Without\nsacrificing expressivity for traffic rules, we adopt LTL specifications\ncomposed of safety and co-safety formulas, allowing the control synthesis\nproblem to be reformulated as a reachability problem. By leveraging occupation\nmeasures, we further formulate a linear programming (LP) problem for this\nLTL-based risk metric. Consequently, the synthesized policy balances different\ntypes of driving risks, including both collision risks and traffic rule\nviolations. The effectiveness of the proposed approach is validated by three\ntypical traffic scenarios in Carla simulator.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.09769v3", "cate": "eess.SY", "date": "2024-09-15", "updated": "2025-07-31"}
{"id": "2508.00394", "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00394v1", "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00394v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00757", "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "authors": ["Robin Armingaud", "Romaric Besançon"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Submitted to ARR July", "url": "http://arxiv.org/abs/2508.00757v1", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "comment": "Submitted to ARR July", "pdf_url": "http://arxiv.org/pdf/2508.00757v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00367", "title": "Representation Shift: Unifying Token Compression with FlashAttention", "authors": ["Joonmyung Choi", "Sanghyeok Lee", "Byungoh Ko", "Eunseo Kim", "Jihyung Kil", "Hyunwoo J. Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2508.00367v1", "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.", "comment": "International Conference on Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2508.00367v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00395", "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning", "authors": ["Fei Zhang", "Tianfei Zhou", "Jiangchao Yao", "Ya Zhang", "Ivor W. Tsang", "Yanfeng Wang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, Accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)", "url": "http://arxiv.org/abs/2508.00395v1", "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.", "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "pdf_url": "http://arxiv.org/pdf/2508.00395v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00760", "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00760v1", "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00760v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00374", "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to MVA2025 (Best Poster Award)", "url": "http://arxiv.org/abs/2508.00374v1", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "comment": "Accepted to MVA2025 (Best Poster Award)", "pdf_url": "http://arxiv.org/pdf/2508.00374v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00413", "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00413v1", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00413v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00762", "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "authors": ["Atakan Site", "Emre Hakan Erdemir", "Gülşen Eryiğit"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00762v1", "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00762v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00391", "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "categories": ["cs.CV", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2508.00391v1", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2508.00391v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00427", "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2508.00427v1", "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.", "comment": "ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2508.00427v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00788", "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00788v1", "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00788v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00397", "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00397v1", "summary": "The rapid advancement of diffusion-based video generation models has led to\nincreasingly realistic synthetic content, presenting new challenges for video\nforgery detection. Existing methods often struggle to capture fine-grained\ntemporal inconsistencies, particularly in AI-generated videos with high visual\nfidelity and coherent motion. In this work, we propose a detection framework\nthat leverages spatial-temporal consistency by combining RGB appearance\nfeatures with optical flow residuals. The model adopts a dual-branch\narchitecture, where one branch analyzes RGB frames to detect appearance-level\nartifacts, while the other processes flow residuals to reveal subtle motion\nanomalies caused by imperfect temporal synthesis. By integrating these\ncomplementary features, the proposed method effectively detects a wide range of\nforged videos. Extensive experiments on text-to-video and image-to-video tasks\nacross ten diverse generative models demonstrate the robustness and strong\ngeneralization ability of the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00397v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00440", "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "authors": ["M. A. Pérez-Cutiño", "J. Valverde", "J. Capitán", "J. M. Díaz-Báñez"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00440v1", "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00440v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00819", "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code is available at this https URL", "url": "http://arxiv.org/abs/2508.00819v1", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "pdf_url": "http://arxiv.org/pdf/2508.00819v1", "cate": "cs.CL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00399", "title": "iSafetyBench: A video-language benchmark for safety in industrial environment", "authors": ["Raiyaan Abdullah", "Yogesh Singh Rawat", "Shruti Vyas"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to VISION'25 - ICCV 2025 workshop", "url": "http://arxiv.org/abs/2508.00399v1", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.", "comment": "Accepted to VISION'25 - ICCV 2025 workshop", "pdf_url": "http://arxiv.org/pdf/2508.00399v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00442", "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation", "authors": ["Jiale Zhou", "Wenhan Wang", "Shikun Li", "Xiaolei Qu", "Xin Guo", "Yizhong Liu", "Wenzhong Tang", "Xun Lin", "Yefeng Zheng"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00442v1", "summary": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00442v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00161", "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00161v1", "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00161v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00400", "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents", "authors": ["Janika Deborah Gajo", "Gerarld Paul Merales", "Jerome Escarcha", "Brenden Ashley Molina", "Gian Nartea", "Emmanuel G. Maminta", "Juan Carlos Roldan", "Rowel O. Atienza"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, accepted in ICCV 2025 Workshop on RetailVision", "url": "http://arxiv.org/abs/2508.00400v1", "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.", "comment": "14 pages, accepted in ICCV 2025 Workshop on RetailVision", "pdf_url": "http://arxiv.org/pdf/2508.00400v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00450", "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation", "authors": ["Hongxiang Lin", "Hao Guo", "Zeshun Li", "Erpeng Xue", "Yongqian He", "Xiangyu Hou", "Zhaoyu Hu", "Lei Wang", "Sheng Chen"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00450v1", "summary": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00450v1", "cate": "cs.IR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00230", "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in ICCV 2025", "url": "http://arxiv.org/abs/2508.00230v1", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "comment": "To appear in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00230v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00406", "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00406v1", "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00406v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00398", "title": "Occlusion-robust Stylization for Drawing-based 3D Animation", "authors": ["Sunjae Yoon", "Gwanhyeong Koo", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      11 pages, 13 figures, ICCV 2025", "url": "http://arxiv.org/abs/2508.00398v1", "summary": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory.", "comment": "11 pages, 13 figures, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00398v1", "cate": "cs.GR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00452", "title": "M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation", "authors": ["Chuan He", "Yongchao Liu", "Qiang Li", "Wenliang Zhong", "Chuntao Hong", "Xinwei Yao"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00452v1", "summary": "Cold-start item recommendation is a significant challenge in recommendation\nsystems, particularly when new items are introduced without any historical\ninteraction data. While existing methods leverage multi-modal content to\nalleviate the cold-start issue, they often neglect the inherent multi-view\nstructure of modalities, the distinction between shared and modality-specific\nfeatures. In this paper, we propose Multi-Modal Multi-View Variational\nAutoEncoder (M^2VAE), a generative model that addresses the challenges of\nmodeling common and unique views in attribute and multi-modal features, as well\nas user preferences over single-typed item features. Specifically, we generate\ntype-specific latent variables for item IDs, categorical attributes, and image\nfeatures, and use Product-of-Experts (PoE) to derive a common representation. A\ndisentangled contrastive loss decouples the common view from unique views while\npreserving feature informativeness. To model user inclinations, we employ a\npreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.\nWe further incorporate co-occurrence signals via contrastive learning,\neliminating the need for pretraining. Extensive experiments on real-world\ndatasets validate the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00452v1", "cate": "cs.IR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00408", "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00408v1", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00408v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00412", "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00412v1", "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00412v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00424", "title": "CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data", "authors": ["Kresimir Matkovic", "Rainer Splechtna", "Denis Gracanin", "Helwig Hauser"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Will be published in TVCG and presented at IEEE VIS", "url": "http://arxiv.org/abs/2508.00424v1", "summary": "The interactive visual analysis of set-typed data, i.e., data with attributes\nthat are of type set, is a rewarding area of research and applications.\nValuable prior work has contributed solutions that enable the study of such\ndata with individual set-typed dimensions. In this paper, we present CrossSet,\na novel method for the joint study of two set-typed dimensions and their\ninterplay. Based on a task analysis, we describe a new, multi-scale approach to\nthe interactive visual exploration and analysis of such data. Two set-typed\ndata dimensions are jointly visualized using a hierarchical matrix layout,\nenabling the analysis of the interactions between two set-typed attributes at\nseveral levels, in addition to the analysis of individual such dimensions.\nCrossSet is anchored at a compact, large-scale overview that is complemented by\ndrill-down opportunities to study the relations between and within the\nset-typed dimensions, enabling an interactive visual multi-scale exploration\nand analysis of bivariate set-typed data. Such an interactive approach makes it\npossible to study single set-typed dimensions in detail, to gain an overview of\nthe interaction and association between two such dimensions, to refine one of\nthe dimensions to gain additional details at several levels, and to drill down\nto the specific interactions of individual set-elements from the set-typed\ndimensions. To demonstrate the effectiveness and efficiency of CrossSet, we\nhave evaluated the new method in the context of several application scenarios.", "comment": "Will be published in TVCG and presented at IEEE VIS", "pdf_url": "http://arxiv.org/pdf/2508.00424v1", "cate": "cs.GR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00491", "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicolò Boccardo", "Lorenzo Natale"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper accepted at IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "url": "http://arxiv.org/abs/2508.00491v1", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "pdf_url": "http://arxiv.org/pdf/2508.00491v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00518", "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2508.00518v1", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00518v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00418", "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICIP 2025. Code: this https URL", "url": "http://arxiv.org/abs/2508.00418v1", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "pdf_url": "http://arxiv.org/pdf/2508.00418v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00428", "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation", "authors": ["Nan Xiang", "Tianyi Liang", "Haiwen Huang", "Shiqi Jiang", "Hao Huang", "Yifei Huang", "Liangyu Chen", "Changbo Wang", "Chenhui Li"], "categories": ["cs.GR", "cs.HC"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing, Visualization, Visualization design and evaluation methods", "url": "http://arxiv.org/abs/2508.00428v1", "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.", "comment": "IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "pdf_url": "http://arxiv.org/pdf/2508.00428v1", "cate": "cs.GR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00496", "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI", "authors": ["Mohammed Kamran", "Maria Bernathova", "Raoul Varga", "Christian F. Singer", "Zsuzsanna Bago-Horvath", "Thomas Helbich", "Georg Langs", "Philipp Seeböck"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00496v2", "summary": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00496v2", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2508.00534", "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "authors": ["Mikel Vandeloise"], "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Preprint submitted to the Journal of Object Technology on July 29, 2025. Data available upon request until peer-review is completed", "url": "http://arxiv.org/abs/2508.00534v1", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "pdf_url": "http://arxiv.org/pdf/2508.00534v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00421", "title": "UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken", "authors": ["Runmin Cong", "Zongji Yu", "Hao Fang", "Haoyan Sun", "Sam Kwong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM MM 2025", "url": "http://arxiv.org/abs/2508.00421v1", "summary": "Underwater Instance Segmentation (UIS) tasks are crucial for underwater\ncomplex scene detection. Mamba, as an emerging state space model with\ninherently linear complexity and global receptive fields, is highly suitable\nfor processing image segmentation tasks with long sequence features. However,\ndue to the particularity of underwater scenes, there are many challenges in\napplying Mamba to UIS. The existing fixed-patch scanning mechanism cannot\nmaintain the internal continuity of scanned instances in the presence of\nseverely underwater color distortion and blurred instance boundaries, and the\nhidden state of the complex underwater background can also inhibit the\nunderstanding of instance objects. In this work, we propose the first\nMamba-based underwater instance segmentation model UIS-Mamba, and design two\ninnovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to\nmigrate Mamba to the underwater task. DTS module maintains the continuity of\nthe internal features of the instance objects by allowing the patches to\ndynamically offset and scale, thereby guiding the minimum spanning tree and\nproviding dynamic local receptive fields. HSW module suppresses the\ninterference of complex backgrounds and effectively focuses the information\nflow of state propagation to the instances themselves through the Ncut-based\nhidden state weakening mechanism. Experimental results show that UIS-Mamba\nachieves state-of-the-art performance on both UIIS and USIS10K datasets, while\nmaintaining a low number of parameters and computational complexity. Code is\navailable at https://github.com/Maricalce/UIS-Mamba.", "comment": "ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00421v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00782", "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      The 33rd ACM Multimedia Conference (MM '25)", "url": "http://arxiv.org/abs/2508.00782v1", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "comment": "The 33rd ACM Multimedia Conference (MM '25)", "pdf_url": "http://arxiv.org/pdf/2508.00782v1", "cate": "cs.GR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00525", "title": "Towards a Measure Theory of Semantic Information", "authors": ["George M. Coghill"], "categories": ["cs.IT", "cs.AI", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      17 pages,3 figures", "url": "http://arxiv.org/abs/2508.00525v1", "summary": "A classic account of the quantification of semantic information is that of\nBar-Hiller and Carnap. Their account proposes an inverse relation between the\ninformativeness of a statement and its probability. However, their approach\nassigns the maximum informativeness to a contradiction: which Floridi refers to\nas the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a\ndistance metric and parabolic relation, designed to remove this paradox.\nUnfortunately is approach does not succeed in that aim.\n  In this paper I critique Floridi's theory of strongly semantic information on\nits own terms and show where it succeeds and fails. I then present a new\napproach based on the unit circle (a relation that has been the basis of\ntheories from basic trigonometry to quantum theory). This is used, by analogy\nwith von Neumann's quantum probability to construct a measure space for\ninformativeness that meets all the requirements stipulated by Floridi and\nremoves the paradox. In addition, while contradictions and tautologies have\nzero informativeness, it is found that messages which are contradictory to each\nother are equally informative. The utility of this is explained by means of an\nexample.", "comment": "17 pages,3 figures", "pdf_url": "http://arxiv.org/pdf/2508.00525v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00554", "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism", "authors": ["Li Zhao", "Rui Sun", "Zuoyou Jiang", "Bo Yang", "Yuxiao Bai", "Mengting Chen", "Xinyang Wang", "Jing Li", "Zuo Bai"], "categories": ["q-fin.TR", "cs.CL", "q-fin.CP"], "primary_category": "Subjects:       Trading and Market Microstructure (q-fin.TR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00554v1", "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00554v1", "cate": "q-fin.TR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00443", "title": "SDMatte: Grafting Diffusion Models for Interactive Matting", "authors": ["Longfei Huang", "Yu Liang", "Hao Zhang", "Jinwei Chen", "Wei Dong", "Lunde Chen", "Wanyu Liu", "Bo Li", "Pengtao Jiang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025, 11 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00443v1", "summary": "Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.", "comment": "Accepted at ICCV 2025, 11 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00443v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.12811", "title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "authors": ["Michael Steiner", "Thomas Köhler", "Lukas Radl", "Felix Windisch", "Dieter Schmalstieg", "Markus Steinberger"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12811v2", "summary": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12811v2", "cate": "cs.GR", "date": "2025-04-17", "updated": "2025-08-01"}
{"id": "2508.00545", "title": "Foundations of Interpretable Models", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00545v1", "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00545v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00589", "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "authors": ["Stefan Englmeier", "Max A. Büttner", "Katharina Winter", "Fabian B. Flohr"], "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 10 figure, project page this https URL , submitted to IEEE Transactions on Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00589v1", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00589v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00445", "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models", "authors": ["Hongyi Cai", "Mohammad Mahdinur Rahman", "Mingkang Dong", "Jie Li", "Muxin Pu", "Zhili Fang", "Yinan Peng", "Hanjun Luo", "Yang Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00445v1", "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00445v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.17812", "title": "FaceLift: Learning Generalizable Single Image 3D Face Reconstruction from Synthetic Heads", "authors": ["Weijie Lyu", "Yi Zhou", "Ming-Hsuan Yang", "Zhixin Shu"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Camera-Ready Version. Project Page: this https URL", "url": "http://arxiv.org/abs/2412.17812v2", "summary": "We present FaceLift, a novel feed-forward approach for generalizable\nhigh-quality 360-degree 3D head reconstruction from a single image. Our\npipeline first employs a multi-view latent diffusion model to generate\nconsistent side and back views from a single facial input, which then feeds\ninto a transformer-based reconstructor that produces a comprehensive 3D\nGaussian splats representation. Previous methods for monocular 3D face\nreconstruction often lack full view coverage or view consistency due to\ninsufficient multi-view supervision. We address this by creating a high-quality\nsynthetic head dataset that enables consistent supervision across viewpoints.\nTo bridge the domain gap between synthetic training data and real-world images,\nwe propose a simple yet effective technique that ensures the view generation\nprocess maintains fidelity to the input by learning to reconstruct the input\nimage alongside the view generation. Despite being trained exclusively on\nsynthetic data, our method demonstrates remarkable generalization to real-world\nimages. Through extensive qualitative and quantitative evaluations, we show\nthat FaceLift outperforms state-of-the-art 3D face reconstruction methods on\nidentity preservation, detail recovery, and rendering quality.", "comment": "ICCV 2025 Camera-Ready Version. Project Page:\n  https://weijielyu.github.io/FaceLift", "pdf_url": "http://arxiv.org/pdf/2412.17812v2", "cate": "cs.CV", "date": "2024-12-23", "updated": "2025-08-01"}
{"id": "2508.00546", "title": "SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval", "authors": ["Wenchao Gu", "Zongyi Lyu", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00546v1", "summary": "Code retrieval aims to provide users with desired code snippets based on\nusers' natural language queries. With the development of deep learning\ntechnologies, adopting pre-trained models for this task has become mainstream.\nConsidering the retrieval efficiency, most of the previous approaches adopt a\ndual-encoder for this task, which encodes the description and code snippet into\nrepresentation vectors, respectively. However, the model structure of the\ndual-encoder tends to limit the model's performance, since it lacks the\ninteraction between the code snippet and description at the bottom layer of the\nmodel during training. To improve the model's effectiveness while preserving\nits efficiency, we propose a framework, which adopts Self-AdaPtive Model\nDistillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts\nthe dual-encoder to narrow the search space and then adopts the cross-encoder\nto improve accuracy. To improve the efficiency of SPENCER, we propose a novel\nmodel distillation technique, which can greatly reduce the inference time of\nthe dual-encoder while maintaining the overall performance. We also propose a\nteaching assistant selection strategy for our model distillation, which can\nadaptively select the suitable teaching assistant models for different\npre-trained models during the model distillation to ensure the model\nperformance. Extensive experiments demonstrate that the combination of\ndual-encoder and cross-encoder improves overall performance compared to solely\ndual-encoder-based models for code retrieval. Besides, our model distillation\ntechnique retains over 98% of the overall performance while reducing the\ninference time of the dual-encoder by 70%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00546v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00695", "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "authors": ["Sergio Rubio-Martín", "María Teresa García-Ordás", "Antonio Serrano-García", "Clara Margarita Franch-Pato", "Arturo Crespo-Álvaro", "José Alberto Benítez-Andrades"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00695v1", "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00695v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00447", "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures", "url": "http://arxiv.org/abs/2508.00447v1", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2508.00447v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00811", "title": "Justified Representation: From Hare to Droop", "authors": ["Matthew M. Casey", "Edith Elkind"], "categories": ["cs.GT"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00811v1", "summary": "The study of proportionality in multiwinner voting with approval ballots has\nreceived much attention in recent years. Typically, proportionality is captured\nby variants of the Justified Representation axiom, which say that cohesive\ngroups of at least $\\ell\\cdot\\frac{n}{k}$ voters (where $n$ is the total number\nof voters and $k$ is the desired number of winners) deserve $\\ell$\nrepresentatives. The quantity $\\frac{n}{k}$ is known as the Hare quota in the\nsocial choice literature. Another -- more demanding -- choice of quota is the\nDroop quota, defined as $\\lfloor\\frac{n}{k+1}\\rfloor+1$. This quota is often\nused in multiwinner voting with ranked ballots: in algorithms such as Single\nTransferable Voting, and in proportionality axioms, such as Droop's\nProportionality Criterion. A few authors have considered it in the context of\napproval ballots, but the existing analysis is far from comprehensive. The\ncontribution of our work is a systematic study of JR-style axioms (and voting\nrules that satisfy them) defined using the Droop quota instead of the Hare\nquota. For each of the standard JR axioms (namely, JR, PJR, EJR, FPJR, FJR,\nPJR+ and EJR+), we identify a voting rule that satisfies the Droop version of\nthis axiom. In some cases, it suffices to consider known rules (modifying the\ncorresponding Hare proof, sometimes quite substantially), and in other cases it\nis necessary to modify the rules from prior work. Each axiom is more difficult\nto satisfy when defined using the Droop quota, so our results expand the\nfrontier of satisfiable proportionality axioms. We complement our theoretical\nresults with an experimental study, showing that for many probabilistic models\nof voter approvals, Droop JR/EJR+ are considerably more demanding than standard\n(Hare) JR/EJR+.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00811v1", "cate": "cs.GT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00575", "title": "Analysing Temporal Reasoning in Description Logics Using Formal Grammars", "authors": ["Camille Bourgaux", "Anton Gnatenko", "Michaël Thomazo"], "categories": ["cs.LO", "cs.AI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      This is an extended version of a paper appearing at the 28th European Conference on Artificial Intelligence (ECAI 2025). 20 pages", "url": "http://arxiv.org/abs/2508.00575v1", "summary": "We establish a correspondence between (fragments of)\n$\\mathcal{TEL}^\\bigcirc$, a temporal extension of the $\\mathcal{EL}$\ndescription logic with the LTL operator $\\bigcirc^k$, and some specific kinds\nof formal grammars, in particular, conjunctive grammars (context-free grammars\nequipped with the operation of intersection). This connection implies that\n$\\mathcal{TEL}^\\bigcirc$ does not possess the property of ultimate periodicity\nof models, and further leads to undecidability of query answering in\n$\\mathcal{TEL}^\\bigcirc$, closing a question left open since the introduction\nof $\\mathcal{TEL}^\\bigcirc$. Moreover, it also allows to establish decidability\nof query answering for some new interesting fragments of\n$\\mathcal{TEL}^\\bigcirc$, and to reuse for this purpose existing tools and\nalgorithms for conjunctive grammars.", "comment": "This is an extended version of a paper appearing at the 28th European\n  Conference on Artificial Intelligence (ECAI 2025). 20 pages", "pdf_url": "http://arxiv.org/pdf/2508.00575v1", "cate": "cs.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.10207", "title": "Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge", "authors": ["Xiao Zhang", "Qianru Meng", "Johan Bos"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accpted by 16th IWCS", "url": "http://arxiv.org/abs/2412.10207v2", "summary": "Open-domain semantic parsing remains a challenging task, as neural models\noften rely on heuristics and struggle to handle unseen concepts. In this paper,\nwe investigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external symbolic knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.", "comment": "Accpted by 16th IWCS", "pdf_url": "http://arxiv.org/pdf/2412.10207v2", "cate": "cs.CL", "date": "2024-12-13", "updated": "2025-07-31"}
{"id": "2508.00453", "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00453v1", "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to\ngenerate high-quality images that simultaneously possess rich spectral\ninformation and fine spatial details. However, due to the inherent trade-off\nbetween spectral and spatial information and the limited availability of\nobservations, this task is fundamentally ill-posed. Previous studies have not\neffectively addressed the ill-posed nature caused by data misalignment. To\ntackle this challenge, we propose a fusion framework named PIF-Net, which\nexplicitly incorporates ill-posed priors to effectively fuse multispectral\nimages and hyperspectral images. To balance global spectral modeling with\ncomputational efficiency, we design a method based on an invertible Mamba\narchitecture that maintains information consistency during feature\ntransformation and fusion, ensuring stable gradient flow and process\nreversibility. Furthermore, we introduce a novel fusion module called the\nFusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral\nand spatial features while keeping the model lightweight. Extensive experiments\non multiple benchmark datasets demonstrate that PIF-Net achieves significantly\nbetter image restoration performance than current state-of-the-art methods\nwhile maintaining model efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00453v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00151", "title": "Ordinal Folding Index: A Computable Metric for Self-Referential Semantics", "authors": ["Faruk Alpay", "Hamdi Al Alakkad"], "categories": ["cs.LO", "cs.GT", "03B70, 91A44, 91A05, 68Q10", "F.1.1; F.4.1; F.3.1; I.2.3"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures. Introduces the Ordinal Folding Index, a computable ordinal depth metric for self referential statements that unifies fixed point logic with infinite game theory", "url": "http://arxiv.org/abs/2508.00151v1", "summary": "The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that\nmeasures how many rounds of self-reference a statement, protocol or position\nmust unfold before its truth or outcome stabilises. By turning this abstract\n'fold-back' depth into a single ordinal number, OFI forges a direct link\nbetween areas that are usually studied in isolation: the closure stages of\nfixed-point logics, the time-to-win values of infinite parity games, and the\nordinal progressions that calibrate the strength of formal theories. We prove\nthat OFI refines all classical game-theoretic and logical metrics while\nremaining algorithmically enumerable, supply a polynomial-time approximation\nscheme on finite arenas, and show how the index coincides exactly with the\nlength of the shortest winning strategy in the associated evaluation game.\nAlongside the theory we outline five open problems from the completeness of the\ncomputable-ordinal spectrum to the possibility of 'compressing' deep\nself-reference that chart a research programme at the intersection of\ncomputer-aided logic, algorithmic game theory and ordinal analysis. OFI thus\ninvites game theorists and logicians alike to view infinite play, transfinite\ninduction and reflective reasoning through a single, intuitive lens, opening\ncommon ground for techniques.", "comment": "13 pages, 2 figures. Introduces the Ordinal Folding Index, a\n  computable ordinal depth metric for self referential statements that unifies\n  fixed point logic with infinite game theory", "pdf_url": "http://arxiv.org/pdf/2508.00151v1", "cate": "cs.LO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00580", "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00580v1", "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00580v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2501.02039", "title": "An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage", "authors": ["Fan Bu", "Zheng Wang", "Siyi Wang", "Ziyao Liu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.02039v3", "summary": "As Large Language Models (LLMs) become increasingly prevalent in tasks\nrelated to cultural heritage, such as generating descriptions of historical\nmonuments, translating ancient texts, preserving oral traditions, and creating\neducational content, their ability to produce accurate and culturally aligned\ntexts is being increasingly relied upon by users and researchers. However,\ncultural value misalignments may exist in generated texts, such as the\nmisrepresentation of historical facts, the erosion of cultural identity, and\nthe oversimplification of complex cultural narratives, which may lead to severe\nconsequences. Therefore, investigating value misalignment in the context of LLM\nfor cultural heritage is crucial for mitigating these risks, yet there has been\na significant lack of systematic and comprehensive study and investigation in\nthis area. To fill this gap, we systematically assess the reliability of LLMs\nin generating culturally aligned texts for cultural heritage-related tasks. We\nconduct a comprehensive evaluation by compiling an extensive set of 1066 query\ntasks covering 5 widely recognized categories with 17 aspects within the\nknowledge framework of cultural heritage across 5 open-source LLMs, and examine\nboth the type and rate of cultural value misalignments in the generated texts.\nUsing both automated and manual approaches, we effectively detect and analyze\nthe cultural value misalignments in LLM-generated texts. Our findings are\nconcerning: over 65% of the generated texts exhibit notable cultural\nmisalignments, with certain tasks demonstrating almost complete misalignment\nwith key cultural values. Beyond these findings, this paper introduces a\nbenchmark dataset and a comprehensive evaluation workflow that can serve as a\nvaluable resource for future research aimed at enhancing the cultural\nsensitivity and reliability of LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.02039v3", "cate": "cs.CL", "date": "2025-01-03", "updated": "2025-08-01"}
{"id": "2508.00471", "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00471v1", "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00471v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00188", "title": "Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems", "authors": ["Renyan Sun", "Ashutosh Nayyar"], "categories": ["eess.SY", "cs.GT", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      We submitted a full paper to IEEE TAC for review. A preliminary version of this paper is scheduled to be presented at IEEE CDC conference in December 2025", "url": "http://arxiv.org/abs/2508.00188v1", "summary": "We consider a finite-horizon discrete-time dynamic system jointly controlled\nby a designer and one or more agents, where the designer can influence the\nagents' actions through selective information disclosure. At each time step,\nthe designer sends a message to the agent(s) from a prespecified message space.\nThe designer may also take an action that directly influences system dynamics\nand rewards. Each agent uses its received message (and its own information) to\nchoose its action. We are interested in the setting where the designer would\nlike to incentivize each agent to play a specific strategy. We consider a\nnotion of incentive compatibility that is based on sequential rationality at\neach realization of the common information between the designer and the\nagent(s). Our objective is to find a messaging and action strategy for the\ndesigner that maximizes its total expected reward while incentivizing each\nagent to follow a prespecified strategy. Under certain assumptions on the\ninformation structure of the problem, we show that an optimal designer strategy\ncan be computed using a backward inductive algorithm that solves a family of\nlinear programs.", "comment": "We submitted a full paper to IEEE TAC for review. A preliminary\n  version of this paper is scheduled to be presented at IEEE CDC conference in\n  December 2025", "pdf_url": "http://arxiv.org/pdf/2508.00188v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00604", "title": "Composable OS Kernel Architectures for Autonomous Intelligence", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "categories": ["cs.OS", "cs.AI"], "primary_category": "Subjects:       Operating Systems (cs.OS)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2508.00604v1", "summary": "As intelligent systems permeate edge devices, cloud infrastructure, and\nembedded real-time environments, this research proposes a new OS kernel\narchitecture for intelligent systems, transforming kernels from static resource\nmanagers to adaptive, AI-integrated platforms. Key contributions include: (1)\ntreating Loadable Kernel Modules (LKMs) as AI-oriented computation units for\nfast sensory and cognitive processing in kernel space; (2) expanding the Linux\nkernel into an AI-native environment with built-in deep learning inference,\nfloating-point acceleration, and real-time adaptive scheduling for efficient ML\nworkloads; and (3) introducing a Neurosymbolic kernel design leveraging\nCategory Theory and Homotopy Type Theory to unify symbolic reasoning and\ndifferentiable logic within OS internals. Together, these approaches enable\noperating systems to proactively anticipate and adapt to the cognitive needs of\nautonomous intelligent applications.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2508.00604v1", "cate": "cs.OS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.08395", "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance", "authors": ["Paul Röttger", "Musashi Hinck", "Valentin Hofmann", "Kobi Hackenburg", "Valentina Pyatkin", "Faeze Brahman", "Dirk Hovy"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      under review", "url": "http://arxiv.org/abs/2502.08395v2", "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs actually manifest in real user interactions, making it\ndifficult to address the risks from biased LLMs. Therefore, we create\nIssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM\nwriting assistance, which we construct based on 3.9k templates (e.g. \"write a\nblog about\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in state-of-the-art LLMs. We also show that biases are remarkably\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.", "comment": "under review", "pdf_url": "http://arxiv.org/pdf/2502.08395v2", "cate": "cs.CL", "date": "2025-02-12", "updated": "2025-08-01"}
{"id": "2508.00473", "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00473v1", "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00473v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2405.12414", "title": "The Power of Two in Token Systems", "authors": ["Itai Ashlagi", "Süleyman Kerimov", "Omer Tamuz", "Geng Zhao"], "categories": ["cs.GT"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.12414v2", "summary": "In economies without monetary transfers, token systems serve as an\nalternative to sustain cooperation, alleviate free riding, and increase\nefficiency. This paper studies whether a token-based economy can be effective\nin marketplaces with thin exogenous supply. We consider a marketplace in which\nat each time period one agent requests a service, one agent provides the\nservice, and one token (artificial currency) is used to pay for service\nprovision. The number of tokens each agent has represents the difference\nbetween the amount of service provisions and service requests by the agent. We\nare interested in the behavior of this economy when very few agents are\navailable to provide the requested service. Since balancing the number of\ntokens across agents is key to sustain cooperation, the agent with the minimum\namount of tokens is selected to provide service among the available agents.\nWhen exactly one random agent is available to provide service, we show that the\ntoken distribution is unstable. However, already when just two random agents\nare available to provide service, the token distribution is stable, in the\nsense that agents' token balance is unlikely to deviate much from their initial\nendowment, and agents return to their initial endowment in finite expected\ntime. Our results mirror the power of two choices paradigm in load balancing\nproblems. Supported by numerical simulations using kidney exchange data, our\nfindings suggest that token systems may generate efficient outcomes in kidney\nexchange marketplaces by sustaining cooperation between hospitals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.12414v2", "cate": "cs.GT", "date": "2024-05-20", "updated": "2025-08-01"}
{"id": "2508.00615", "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00615v1", "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00615v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.08441", "title": "Better Embeddings with Coupled Adam", "authors": ["Felix Stollenwerk", "Tobias Stollenwerk"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 (Main), see this https URL", "url": "http://arxiv.org/abs/2502.08441v3", "summary": "Despite their remarkable capabilities, LLMs learn word representations that\nexhibit the undesirable yet poorly understood feature of anisotropy. In this\npaper, we argue that the second moment in Adam is a cause of anisotropic\nembeddings, and suggest a modified optimizer called Coupled Adam to mitigate\nthe problem. Our experiments demonstrate that Coupled Adam significantly\nimproves the quality of embeddings, while also leading to better upstream and\ndownstream performance on large enough datasets.", "comment": "ACL 2025 (Main), see https://aclanthology.org/2025.acl-long.1321/", "pdf_url": "http://arxiv.org/pdf/2502.08441v3", "cate": "cs.CL", "date": "2025-02-12", "updated": "2025-08-01"}
{"id": "2508.00477", "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, 3 tables", "url": "http://arxiv.org/abs/2508.00477v1", "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.", "comment": "8 pages, 5 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2508.00477v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.09716", "title": "Dominated Actions in Imperfect-Information Games", "authors": ["Sam Ganzfried"], "categories": ["cs.GT", "cs.AI", "cs.MA", "econ.TH"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09716v2", "summary": "Dominance is a fundamental concept in game theory. In strategic-form games\ndominated strategies can be identified in polynomial time. As a consequence,\niterative removal of dominated strategies can be performed efficiently as a\npreprocessing step for reducing the size of a game before computing a Nash\nequilibrium. For imperfect-information games in extensive form, we could\nconvert the game to strategic form and then iteratively remove dominated\nstrategies in the same way; however, this conversion may cause an exponential\nblowup in game size. In this paper we define and study the concept of dominated\nactions in imperfect-information games. Our main result is a polynomial-time\nalgorithm for determining whether an action is dominated (strictly or weakly)\nby any mixed strategy in n-player games, which can be extended to an algorithm\nfor iteratively removing dominated actions. This allows us to efficiently\nreduce the size of the game tree as a preprocessing step for Nash equilibrium\ncomputation. We explore the role of dominated actions empirically in the \"All\nIn or Fold\" No-Limit Texas Hold'em poker variant.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09716v2", "cate": "cs.GT", "date": "2025-04-13", "updated": "2025-08-01"}
{"id": "2508.00002", "title": "ReVise: A Human-AI Interface for Incremental Algorithmic Recourse", "authors": ["Kaustav Bhattacharjee", "Jun Yuan", "Aritra Dasgupta"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Conditionally accepted for the IEEE VIS 2025 Short Papers track", "url": "http://arxiv.org/abs/2508.00002v1", "summary": "The recent adoption of artificial intelligence in socio-technical systems\nraises concerns about the black-box nature of the resulting decisions in fields\nsuch as hiring, finance, admissions, etc. If data subjects -- such as job\napplicants, loan applicants, and students -- receive an unfavorable outcome,\nthey may be interested in algorithmic recourse, which involves updating certain\nfeatures to yield a more favorable result when re-evaluated by algorithmic\ndecision-making. Unfortunately, when individuals do not fully understand the\nincremental steps needed to change their circumstances, they risk following\nmisguided paths that can lead to significant, long-term adverse consequences.\nExisting recourse approaches focus exclusively on the final recourse goal but\nneglect the possible incremental steps to reach the goal with real-life\nconstraints, user preferences, and model artifacts. To address this gap, we\nformulate a visual analytic workflow for incremental recourse planning in\ncollaboration with AI/ML experts and contribute an interactive visualization\ninterface that helps data subjects efficiently navigate the recourse\nalternatives and make an informed decision. We present a usage scenario and\nsubjective feedback from observational studies with twelve graduate students\nusing a real-world dataset, which demonstrates that our approach can be\ninstrumental for data subjects in choosing a suitable recourse path.", "comment": "Conditionally accepted for the IEEE VIS 2025 Short Papers track", "pdf_url": "http://arxiv.org/pdf/2508.00002v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2508.00697", "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00697v1", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00697v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.12927", "title": "SEFL: Enhancing Educational Assignment Feedback with LLM Agents", "authors": ["Mike Zhang", "Amalie Pernille Dilling", "Léon Gondelman", "Niels Erik Ruan Lyngdorf", "Euan D. Lindsay", "Johannes Bjerva"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.12927v2", "summary": "Providing high-quality feedback to student assignments is crucial for student\nsuccess, but it is constrained by time and costs. In this work, we introduce\nSynthetic Educational Feedback Loops (SEFL), a synthetic data framework\ndesigned to generate data that resembles immediate, on-demand feedback at scale\nwithout relying on extensive, real-world student assignments. To get this type\nof data, two large language models (LLMs) operate in teacher-student roles to\nsimulate assignment completion and formative feedback, generating synthetic\npairs of student work and corresponding critiques and actionable improvements\nfrom a teacher. With this data, we fine-tune smaller, more computationally\nefficient LLMs on these synthetic pairs, enabling them to replicate key\nfeatures of high-quality, goal-oriented feedback. Unlike personalized tutoring\napproaches that offer multi-turn, individualized instruction, SEFL specifically\nfocuses on replicating the teacher-student assignment feedback loop in higher\neducation. Through comprehensive evaluations with four LLM judges and three\nhuman experts, we demonstrate that SEFL-tuned models outperform both their\nnon-tuned counterparts in feedback quality and an existing baseline. The\npotential for societal impact is reinforced by extensive qualitative comments\nby ratings by human stakeholders -- both students and higher education\ninstructors. All in all, SEFL has substantial potential to transform feedback\nprocesses for higher education and beyond.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.12927v2", "cate": "cs.CL", "date": "2025-02-18", "updated": "2025-08-01"}
{"id": "2508.00493", "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00493v1", "summary": "We present SAMSA 2.0, an interactive segmentation framework for hyperspectral\nmedical imaging that introduces spectral angle prompting to guide the Segment\nAnything Model (SAM) using spectral similarity alongside spatial cues. This\nearly fusion of spectral information enables more accurate and robust\nsegmentation across diverse spectral datasets. Without retraining, SAMSA 2.0\nachieves up to +3.8% higher Dice scores compared to RGB-only models and up to\n+3.1% over prior spectral fusion methods. Our approach enhances few-shot and\nzero-shot performance, demonstrating strong generalization in challenging\nlow-data and noisy scenarios common in clinical imaging.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00493v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00107", "title": "Decoupling Data and Tooling in Interactive Visualization", "authors": ["Jan Simson"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Poster at IEEE VIS 2025", "url": "http://arxiv.org/abs/2508.00107v1", "summary": "Interactive data visualization is a major part of modern exploratory data\nanalysis, with web-based technologies enabling a rich ecosystem of both\nspecialized and general tools. However, current visualization tools often lack\nsupport for transformation or wrangling of data and are forced to re-implement\ntheir own solutions to load and ingest data. This redundancy creates\nsubstantial development overhead for tool creators, steeper learning curves for\nusers who must master different data handling interfaces across tools and a\ndegraded user experience as data handling is usually seen as an after-thought.\n  We propose a modular approach that separates data wrangling and loading\ncapabilities from visualization components. This architecture allows\nvisualization tools to concentrate on their core strengths while providing the\nopportunity to develop a unified, powerful interface for data handling. An\nadditional benefit of this approach is that it allows for multiple tools to\nexist and be used side by side. We demonstrate the feasibility of this approach\nby building an early prototype using web technologies to encapsulate\nvisualization tools and manage data flow between them.\n  We discuss future research directions, including downstream integrations with\nother tooling, such as IDEs, literate programming notebooks and applications,\nas well as incorporation of new technologies for efficient data\ntransformations. We seek input from the community to better understand the\nrequirements towards this approach.", "comment": "Poster at IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00107v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00701", "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00701v1", "summary": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.", "comment": "8 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00701v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.14969", "title": "Lost in Space: Finding the Right Tokens for Structured Output", "authors": ["Sil Hamilton", "David Mimno"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.14969v2", "summary": "General-purpose language models are trained to produce varied natural\nlanguage outputs, but for some tasks, like annotation or classification, we\nneed more specific output formats. LLM systems increasingly support structured\noutput, which enforces formats by sampling tokens according to a grammar -- but\nalso unpredictably reduces downstream performance. Are there systematic\ndifferences between grammars that appear semantically (and often visually)\nsimilar to humans? To answer this, we test four popular model families with\nfive varying output formats on four common NLP benchmarks. We find all models\nperform most accurately when guided to use formats respecting convention, such\nas letters for multiple choice and real numbers for numerical prediction.\nPerformance also improves by 5%-10% when guiding models to return tokens\nincorporating leading whitespace, with smaller models benefiting the most. We\nfind leading whitespace helps models avoid structural deficiencies in subword\ntoken representations. We finally present best practices for researchers using\nlanguage models as zero-shot classifiers with structured output.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.14969v2", "cate": "cs.CL", "date": "2025-02-20", "updated": "2025-08-01"}
{"id": "2508.00506", "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "authors": ["Tulsi Patel", "Mark W. Jones", "Thomas Redfern"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Video supplement demonstrating feature-space exploration and interactive labelling is available at: this https URL and is archived at this https URL", "url": "http://arxiv.org/abs/2508.00506v1", "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate\nlabels for model training and testing. Labelling remote sensing imagery is time\nand cost intensive, requiring expert analysis. Previous labelling tools rely on\npre-labelled data for training in order to label new unseen data. In this work,\nwe define an unsupervised pipeline for finding and labelling geographical areas\nof similar context and content within Sentinel-2 satellite imagery. Our\napproach removes limitations of previous methods by utilising segmentation with\nconvolutional and graph neural networks to encode a more robust feature space\nfor image comparison. Unlike previous approaches we segment the image into\nhomogeneous regions of pixels that are grouped based on colour and spatial\nsimilarity. Graph neural networks are used to aggregate information about the\nsurrounding segments enabling the feature representation to encode the local\nneighbourhood whilst preserving its own local information. This reduces\noutliers in the labelling tool, allows users to label at a granular level, and\nallows a rotationally invariant semantic relationship at the image level to be\nformed within the encoding space.", "comment": "Video supplement demonstrating feature-space exploration and\n  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is\n  archived at https://doi.org/10.5281/zenodo.16676591", "pdf_url": "http://arxiv.org/pdf/2508.00506v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00211", "title": "HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking", "authors": ["Esen K. Tütüncü", "Mar Gonzalez-Franco", "Eric J. Gonzalez"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 10 figures", "url": "http://arxiv.org/abs/2508.00211v1", "summary": "We present HandOver, an extended reality (XR) interaction technique designed\nto unify the precision of traditional mouse input for object selection with the\nexpressiveness of hand-tracking for object manipulation. With HandOver, the\nmouse is used to drive a depth-aware 3D cursor enabling precise and restful\ntargeting -by hovering their hand over the mouse, the user can then seamlessly\ntransition into direct 3D manipulation of the target object. In a formal user\nstudy, we compare HandOver against two raybased techniques: traditional\nraycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results\nshow HandOver yields lower task errors across all distances, and moreover\nimproves interaction ergonomics as highlighted by a RULA posture analysis and\nself-reported measures (NASA-TLX). These findings illustrate the benefits of\nblending traditional precise input devices with the expressive gestural inputs\nafforded by hand-tracking in XR, leading to improved user comfort and task\nperformance. This blended paradigm yields a unified workflow allowing users to\nleverage the best of each input modality as they interact in immersive\nenvironments.", "comment": "11 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2508.00211v1", "cate": "cs.HC", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00707", "title": "Efficient Solution and Learning of Robust Factored MDPs", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00707v1", "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00707v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.17407", "title": "Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning", "authors": ["Guijin Son", "Jiwoo Hong", "Hyunwoo Ko", "James Thorne"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 (ORAL)", "url": "http://arxiv.org/abs/2502.17407v2", "summary": "Scaling pre-training compute has proven effective for achieving\nmulitlinguality, but does the same hold for test-time scaling? In this work, we\nintroduce MCLM, a multilingual math benchmark featuring competition-level\nproblems in 55 languages. We test three test-time scaling methods-Outcome\nReward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing\n(BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for\nextended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM\nachieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although\n\"thinking LLMs\" have recently garnered significant attention, we find that\ntheir performance is comparable to traditional scaling methods like best-of-N\nonce constrained to similar levels of inference FLOPs. Moreover, while BF\nyields a 20-point improvement on English AIME, it provides only a 1.94-point\naverage gain across other languages-a pattern consistent across the other\ntest-time scaling methods we studied-higlighting that test-time scaling may not\ngeneralize as effectively to multilingual tasks. To foster further research, we\nrelease MCLM, MR1-1.5B, and evaluation results.", "comment": "ACL 2025 (ORAL)", "pdf_url": "http://arxiv.org/pdf/2502.17407v2", "cate": "cs.CL", "date": "2025-02-24", "updated": "2025-08-01"}
{"id": "2508.00528", "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00528v1", "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00528v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00233", "title": "Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism", "authors": ["Douglas Markant", "Subham Sah", "Alireza Karduni", "Milad Rogha", "My Thai", "Wenwen Dou"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures. IEEE VIS 2025", "url": "http://arxiv.org/abs/2508.00233v1", "summary": "Political sectarianism is fueled in part by misperceptions of political\nopponents: People commonly overestimate the support for extreme policies among\nmembers of the other party. Research suggests that correcting partisan\nmisperceptions by informing people about the actual views of outparty members\nmay reduce one's own expressed support for political extremism, including\npartisan violence and anti-democratic actions. The present study investigated\nhow correction effects depend on different representations of outparty views\ncommunicated through data visualizations. We conducted an experiment with U.S.\nbased participants from Prolific (N=239 Democrats, N=244 Republicans).\nParticipants made predictions about support for political violence and\nundemocratic practices among members of their political outparty. They were\nthen presented with data from an earlier survey on the actual views of outparty\nmembers. Some participants viewed only the average response (Mean-Only\ncondition), while other groups were shown visual representations of the range\nof views from 75% of the outparty (Mean+Interval condition) or the full\ndistribution of responses (Mean+Points condition). Compared to a control group\nthat was not informed about outparty views, we observed the strongest\ncorrection effects among participants in the Mean-only and Mean+Points\ncondition, while correction effects were weaker in the Mean+Interval condition.\nIn addition, participants who observed the full distribution of out-party views\n(Mean+Points condition) were most accurate at later recalling the degree of\nsupport among the outparty. Our findings suggest that data visualizations can\nbe an important tool for correcting pervasive distortions in beliefs about\nother groups. However, the way in which variability in outparty views is\nvisualized can significantly shape how people interpret and respond to\ncorrective information.", "comment": "11 pages, 5 figures. IEEE VIS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00233v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00712", "title": "JSON-Bag: A generic game trajectory representation", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures, 6 tables, to be published in IEEE Conference on Games 2025", "url": "http://arxiv.org/abs/2508.00712v1", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "pdf_url": "http://arxiv.org/pdf/2508.00712v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.19573", "title": "Do Large Language Models Know How Much They Know?", "authors": ["Gabriele Prato", "Jerry Huang", "Prasanna Parthasarathi", "Shagun Sodhani", "Sarath Chandar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ublished as a long paper at the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP). Official version of paper within conference proceedings is available at this https URL", "url": "http://arxiv.org/abs/2502.19573v2", "summary": "Large Language Models (LLMs) have emerged as highly capable systems and are\nincreasingly being integrated into various uses. However, the rapid pace of\ntheir deployment has outpaced a comprehensive understanding of their internal\nmechanisms and a delineation of their capabilities and limitations. A desired\nattribute of an intelligent system is its ability to recognize the scope of its\nown knowledge. To investigate whether LLMs embody this characteristic, we\ndevelop a benchmark designed to challenge these models to enumerate all\ninformation they possess on specific topics. This benchmark evaluates whether\nthe models recall excessive, insufficient, or the precise amount of\ninformation, thereby indicating their awareness of their own knowledge. Our\nfindings reveal that all tested LLMs, given sufficient scale, demonstrate an\nunderstanding of how much they know about specific topics. While different\narchitectures exhibit varying rates of this capability's emergence, the results\nsuggest that awareness of knowledge may be a generalizable attribute of LLMs.\nFurther research is needed to confirm this potential and fully elucidate the\nunderlying mechanisms.", "comment": "ublished as a long paper at the 2024 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP). Official version of paper within\n  conference proceedings is available at\n  https://aclanthology.org/2024.emnlp-main.348/", "pdf_url": "http://arxiv.org/pdf/2502.19573v2", "cate": "cs.CL", "date": "2025-02-26", "updated": "2025-08-01"}
{"id": "2508.00548", "title": "Video Color Grading via Look-Up Table Generation", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV2025", "url": "http://arxiv.org/abs/2508.00548v1", "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.", "comment": "ICCV2025", "pdf_url": "http://arxiv.org/pdf/2508.00548v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00252", "title": "TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices", "authors": ["Wataru Kawabe", "Hiroto Fukuda", "Akihisa Shitara", "Yuri Nakao", "Yusuke Sugano"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2508.00252v1", "summary": "We introduce TofuML, an interactive system designed to make machine learning\n(ML) concepts more accessible and engaging for non-expert users. Unlike\nconventional GUI-based systems, TofuML employs a physical and spatial interface\nconsisting of a small device and a paper mat, allowing users to train and\nevaluate sound classification models through intuitive, toy-like interactions.\nThrough two user studies -- a comparative study against a GUI-based version and\na public event deployment -- we investigated how TofuML impacts users'\nengagement in the ML model creation process, their ability to provide\nappropriate training data, and their conception of potential applications. Our\nresults indicated that TofuML enhanced user engagement compared to a GUI while\nlowering barriers for non-experts to engage with ML. Users demonstrated\ncreativity in conceiving diverse ML applications, revealing opportunities to\noptimize between conceptual understanding and user engagement. These findings\ncontribute to developing interactive ML systems/frameworks designed for a wide\nrange of users.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2508.00252v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00194", "title": "Audio Prototypical Network For Controllable Music Recommendation", "authors": ["Fırat Öncel", "Emiliano Penaloza", "Haolun Wu", "Shubham Gupta", "Mirco Ravanelli", "Laurent Charlin", "Cem Subakan"], "categories": ["cs.IR", "eess.AS"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted to MLSP2025", "url": "http://arxiv.org/abs/2508.00194v1", "summary": "Traditional recommendation systems represent user preferences in dense\nrepresentations obtained through black-box encoder models. While these models\noften provide strong recommendation performance, they lack interpretability for\nusers, leaving users unable to understand or control the system's modeling of\ntheir preferences. This limitation is especially challenging in music\nrecommendation, where user preferences are highly personal and often evolve\nbased on nuanced qualities like mood, genre, tempo, or instrumentation. In this\npaper, we propose an audio prototypical network for controllable music\nrecommendation. This network expresses user preferences in terms of prototypes\nrepresentative of semantically meaningful features pertaining to musical\nqualities. We show that the model obtains competitive recommendation\nperformance compared to popular baseline models while also providing\ninterpretable and controllable user profiles.", "comment": "Accepted to MLSP2025", "pdf_url": "http://arxiv.org/pdf/2508.00194v1", "cate": "cs.IR", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00716", "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00716v1", "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00716v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.06072", "title": "A Survey on Post-training of Large Language Models", "authors": ["Guiyao Tie", "Zeli Zhao", "Dingjie Song", "Fuyang Wei", "Rong Zhou", "Yurou Dai", "Wen Yin", "Zhejian Yang", "Jiangyue Yan", "Yao Su", "Zhenhan Dai", "Yifeng Xie", "Yihan Cao", "Lichao Sun", "Pan Zhou", "Lifang He", "Hechang Chen", "Yu Zhang", "Qingsong Wen", "Tianming Liu", "Neil Zhenqiang Gong", "Jiliang Tang", "Caiming Xiong", "Heng Ji", "Philip S. Yu", "Jianfeng Gao"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      87 pages, 21 figures, 9 tables", "url": "http://arxiv.org/abs/2503.06072v3", "summary": "The emergence of Large Language Models (LLMs) has fundamentally transformed\nnatural language processing, making them indispensable across domains ranging\nfrom conversational systems to scientific exploration. However, their\npre-trained architectures often reveal limitations in specialized contexts,\nincluding restricted reasoning capacities, ethical uncertainties, and\nsuboptimal domain-specific performance. These challenges necessitate advanced\npost-training language models (PoLMs) to address these shortcomings, such as\nOpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or\nLRMs). This paper presents the first comprehensive survey of PoLMs,\nsystematically tracing their evolution across five core paradigms: Fine-tuning,\nwhich enhances task-specific accuracy; Alignment, which ensures ethical\ncoherence and alignment with human preferences; Reasoning, which advances\nmulti-step inference despite challenges in reward design; Efficiency, which\noptimizes resource utilization amidst increasing complexity; Integration and\nAdaptation, which extend capabilities across diverse modalities while\naddressing coherence issues. Charting progress from ChatGPT's alignment\nstrategies to DeepSeek-R1's innovative reasoning advancements, we illustrate\nhow PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities,\nand enhance domain adaptability. Our contributions include a pioneering\nsynthesis of PoLM evolution, a structured taxonomy categorizing techniques and\ndatasets, and a strategic agenda emphasizing the role of LRMs in improving\nreasoning proficiency and domain flexibility. As the first survey of its scope,\nthis work consolidates recent PoLM advancements and establishes a rigorous\nintellectual framework for future research, fostering the development of LLMs\nthat excel in precision, ethical robustness, and versatility across scientific\nand societal applications.", "comment": "87 pages, 21 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2503.06072v3", "cate": "cs.CL", "date": "2025-03-08", "updated": "2025-08-01"}
{"id": "2508.00549", "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex Bäuerle", "Meinrad Beer", "Michael Götz", "Timo Ropinski"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025", "url": "http://arxiv.org/abs/2508.00549v1", "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.", "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00549v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00321", "title": "Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes", "authors": ["Shuning Zhang", "Ying Ma", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00321v1", "summary": "The proliferation of visual sensors in smart home environments, particularly\nthrough wearable devices like smart glasses, introduces profound privacy\nchallenges. Existing privacy controls are often static and coarse-grained,\nfailing to accommodate the dynamic and socially nuanced nature of home\nenvironments. This paper investigates the viability of using Large Language\nModels (LLMs) as the core of a dynamic and adaptive privacy policy engine. We\npropose a conceptual framework where visual data is classified using a\nmulti-dimensional schema that considers data sensitivity, spatial context, and\nsocial presence. An LLM then reasons over this contextual information to\nenforce fine-grained privacy rules, such as selective object obfuscation, in\nreal-time. Through a comparative evaluation of state-of-the-art Vision Language\nModels (including GPT-4o and the Qwen-VL series) in simulated home settings ,\nour findings show the feasibility of this approach. The LLM-based engine\nachieved a top machine-evaluated appropriateness score of 3.99 out of 5, and\nthe policies generated by the models received a top human-evaluated score of\n4.00 out of 5.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00321v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00570", "title": "Session-Based Recommendation with Validated and Enriched LLM Intents", "authors": ["Gyuseok Lee", "Yaokun Liu", "Yifan Liu", "Susik Yoon", "Dong Wang", "SeongKu Kang"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00570v1", "summary": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00570v1", "cate": "cs.IR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00734", "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00734v1", "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00734v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.19693", "title": "AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation", "authors": ["Itay Nakash", "Nitay Calderon", "Eyal Ben David", "Elad Hoffer", "Roi Reichart"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.19693v2", "summary": "Large Language Models (LLMs) have shown impressive versatility as general\npurpose models. However, their broad applicability comes at a high-cost\ncomputational overhead, particularly in auto-regressive decoding where each\nstep requires a forward pass. In domain-specific settings, general-purpose\ncapabilities are unnecessary and can be exchanged for efficiency. In this work,\nwe take a novel perspective on domain adaptation, reducing latency and\ncomputational costs by adapting the vocabulary to focused domains of interest.\nWe introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation,\ndesigned to enhance LLM efficiency in low-resource domains. AdaptiVocab can be\napplied to any tokenizer and architecture, modifying the vocabulary by\nreplacing tokens with domain-specific n-gram-based tokens, thereby reducing the\nnumber of tokens required for both input processing and output generation.\nAdaptiVocab initializes new n-token embeddings using an exponentially weighted\ncombination of existing embeddings and employs a lightweight fine-tuning phase\nthat can be efficiently performed on a single GPU. We evaluate two 7B LLMs\nacross three niche domains, assessing efficiency, generation quality, and\nend-task performance. Our results show that AdaptiVocab reduces token usage by\nover 25% without compromising performance", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.19693v2", "cate": "cs.CL", "date": "2025-03-25", "updated": "2025-08-01"}
{"id": "2508.00552", "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00552v1", "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00552v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00328", "title": "From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations", "authors": ["Shuning Zhang", "Ying Ma", "Yongquan `Owen' Hu", "Ting Dang", "Hong Jia", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00328v1", "summary": "Online medical consultation platforms, while convenient, are undermined by\nsignificant privacy risks that erode user trust. We first conducted in-depth\nsemi-structured interviews with 12 users to understand their perceptions of\nsecurity and privacy landscapes on online medical consultation platforms, as\nwell as their practices, challenges and expectation. Our analysis reveals a\ncritical disconnect between users' desires for anonymity and control, and\nplatform realities that offload the responsibility of ``privacy labor''. To\nbridge this gap, we present SafeShare, an interaction technique that leverages\nlocalized LLM to redact consultations in real-time. SafeShare balances utility\nand privacy through selectively anonymize private information. A technical\nevaluation of SafeShare's core PII detection module on 3 dataset demonstrates\nhigh efficacy, achieving 89.64\\% accuracy with Qwen3-4B on IMCS21 dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00328v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00710", "title": "Experimental Evaluation of Dynamic Topic Modeling Algorithms", "authors": ["Ngozichukwuka Onah", "Nadine Steinmetz", "Hani Al-Sayeh", "Kai-Uwe Sattler"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00710v1", "summary": "The amount of text generated daily on social media is gigantic and analyzing\nthis text is useful for many purposes. To understand what lies beneath a huge\namount of text, we need dependable and effective computing techniques from\nself-powered topic models. Nevertheless, there are currently relatively few\nthorough quantitative comparisons between these models. In this study, we\ncompare these models and propose an assessment metric that documents how the\ntopics change in time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00710v1", "cate": "cs.IR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00737", "title": "How LLMs are Shaping the Future of Virtual Reality", "authors": ["Süeda Özkaya", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Pre-print", "url": "http://arxiv.org/abs/2508.00737v2", "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.", "comment": "Pre-print", "pdf_url": "http://arxiv.org/pdf/2508.00737v2", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2503.21760", "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents", "authors": ["Rana Salama", "Jason Cai", "Michelle Yuan", "Anna Currey", "Monica Sunkara", "Yi Zhang", "Yassine Benajiba"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.21760v2", "summary": "Large language model (LLM) agents have evolved to intelligently process\ninformation, make decisions, and interact with users or tools. A key capability\nis the integration of long-term memory capabilities, enabling these agents to\ndraw upon historical interactions and knowledge. However, the growing memory\nsize and need for semantic structuring pose significant challenges. In this\nwork, we propose an autonomous memory augmentation approach, MemInsight, to\nenhance semantic data representation and retrieval mechanisms. By leveraging\nautonomous augmentation to historical interactions, LLM agents are shown to\ndeliver more accurate and contextualized responses. We empirically validate the\nefficacy of our proposed approach in three task scenarios; conversational\nrecommendation, question answering and event summarization. On the LLM-REDIAL\ndataset, MemInsight boosts persuasiveness of recommendations by up to 14%.\nMoreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.\nOur empirical results show the potential of MemInsight to enhance the\ncontextual performance of LLM agents across multiple tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.21760v2", "cate": "cs.CL", "date": "2025-03-27", "updated": "2025-07-31"}
{"id": "2508.00553", "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00553v1", "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00553v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00439", "title": "HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification", "authors": ["Subin Park", "Jeonghyun Kim", "Jeanne Choi", "Joseph Seering", "Uichin Lee", "Sung-Ju Lee"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted by ACM CSCW 2025; 39 pages (including 6 pages of Appendix)", "url": "http://arxiv.org/abs/2508.00439v1", "summary": "Hate speech remains a persistent and unresolved challenge in online\nplatforms. Content moderators, working on the front lines to review\nuser-generated content and shield viewers from hate speech, often find\nthemselves unprotected from the mental burden as they continuously engage with\noffensive language. To safeguard moderators' mental well-being, we designed\nHateBuffer, which anonymizes targets of hate speech, paraphrases offensive\nexpressions into less offensive forms, and shows the original expressions when\nmoderators opt to see them. Our user study with 80 participants consisted of a\nsimulated hate speech moderation task set on a fictional news platform,\nfollowed by semi-structured interviews. Although participants rated the hate\nseverity of comments lower while using HateBuffer, contrary to our\nexpectations, they did not experience improved emotion or reduced fatigue\ncompared with the control group. In interviews, however, participants described\nHateBuffer as an effective buffer against emotional contagion and the\nnormalization of biased opinions in hate speech. Notably, HateBuffer did not\ncompromise moderation accuracy and even contributed to a slight increase in\nrecall. We explore possible explanations for the discrepancy between the\nperceived benefits of HateBuffer and its measured impact on mental well-being.\nWe also underscore the promise of text-based content modification techniques as\ntools for a healthier content moderation environment.", "comment": "Accepted by ACM CSCW 2025; 39 pages (including 6 pages of Appendix)", "pdf_url": "http://arxiv.org/pdf/2508.00439v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00751", "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking", "authors": ["Qing Zhang", "Alex Deng", "Michelle Du", "Huiji Gao", "Liwei He", "Sanjeev Katariya"], "categories": ["cs.IR", "cs.AI", "H.3; G.3"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2508.00751v1", "summary": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2508.00751v1", "cate": "cs.IR", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00183", "title": "Improved Bounds on Access-Redundancy Tradeoffs in Quantized Linear Computations", "authors": ["Ching-Fang Li", "Mary Wootters"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      ISIT 2025", "url": "http://arxiv.org/abs/2508.00183v1", "summary": "Consider the problem of computing quantized linear functions with only a few\nqueries. Formally, given $\\mathbf{x}\\in \\mathbb{R}^k$, our goal is to encode\n$\\mathbf{x}$ as $\\mathbf{c} \\in \\mathbb{R}^n$, for $n > k$, so that for any\n$\\mathbf{w} \\in A^k$, $\\mathbf{w}^T \\mathbf{x}$ can be computed using at most\n$\\ell$ queries to $\\mathbf{c}$. Here, $A$ is some finite set; in this paper we\nfocus on the case where $|A| = 2$.\n  Prior work \\emph{(Ramkumar, Raviv, and Tamo, Trans. IT, 2024)} has given\nconstructions and established impossibility results for this problem. We give\nimproved impossibility results, both for the general problem, and for the\nspecific class of construction (block construction) presented in that work. The\nlatter establishes that the block constructions of prior work are optimal\nwithin that class.\n  We also initiate the study of \\emph{approximate} recovery for this problem,\nwhere the goal is not to recover $\\mathbf{w}^T \\mathbf{x}$ exactly but rather\nto approximate it up to a parameter $\\varepsilon > 0$. We give several\nconstructions, and give constructions for $\\varepsilon = 0.1$ that outperform\nour impossibility result for exact schemes.", "comment": "ISIT 2025", "pdf_url": "http://arxiv.org/pdf/2508.00183v1", "cate": "cs.IT", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00754", "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00754v1", "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00754v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.10284", "title": "Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol", "authors": ["Weiqi Wang", "Jiefu Ou", "Yangqiu Song", "Benjamin Van Durme", "Daniel Khashabi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10284v3", "summary": "Literature review tables are essential for summarizing and comparing\ncollections of scientific papers. We explore the task of generating tables that\nbest fulfill a user's informational needs given a collection of scientific\npapers. Building on recent work (Newman et al., 2024), we extend prior\napproaches to address real-world complexities through a combination of\nLLM-based methods and human annotations. Our contributions focus on three key\nchallenges encountered in real-world use: (i) User prompts are often\nunder-specified; (ii) Retrieved candidate papers frequently contain irrelevant\ncontent; and (iii) Task evaluation should move beyond shallow text similarity\ntechniques and instead assess the utility of inferred tables for\ninformation-seeking tasks (e.g., comparing papers). To support reproducible\nevaluation, we introduce ARXIV2TABLE, a more realistic and challenging\nbenchmark for this task, along with a novel approach to improve literature\nreview table generation in real-world scenarios. Our extensive experiments on\nthis benchmark show that both open-weight and proprietary LLMs struggle with\nthe task, highlighting its difficulty and the need for further advancements.\nOur dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10284v3", "cate": "cs.CL", "date": "2025-04-14", "updated": "2025-08-01"}
{"id": "2508.00557", "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation", "authors": ["Qi Chen", "Lingxiao Yang", "Yun Chen", "Nailong Zhao", "Jianhuang Lai", "Jie Shao", "Xiaohua Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2508.00557v1", "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00557v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00646", "title": "Pull Requests From The Classroom: Co-Developing Curriculum And Code", "authors": ["Dennis Zyska", "Ilia Kuznetsov", "Florian Müller", "Iryna Gurevych"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00646v1", "summary": "Educational technologies often misalign with instructors' pedagogical goals,\nforcing adaptations that compromise teaching efficacy. In this paper, we\npresent a case study on the co-development of curriculum and technology in the\ncontext of a university course on scientific writing. Specifically, we examine\nhow a custom-built peer feedback system was iteratively developed alongside the\ncourse to support annotation, feedback exchange, and revision. Results show\nthat while co-development fostered stronger alignment between software features\nand course goals, it also exposed usability limitations and\ninfrastructure-related frustrations, emphasizing the need for closer\ncoordination between teaching and technical teams.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00646v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00123", "title": "Melody-Lyrics Matching with Contrastive Alignment Loss", "authors": ["Changhong Wang", "Michel Olvera", "Gaël Richard"], "categories": ["eess.AS", "cs.IR"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures, 3 tables. This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00123v1", "summary": "The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm.", "comment": "10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00123v1", "cate": "eess.AS", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00268", "title": "Channel Estimation for Flexible Intelligent Metasurfaces: From Model-Based Approaches to Neural Operators", "authors": ["Jian Xiao", "Ji Wang", "Qimei Cui", "Yucang Yang", "Xingwang Li", "Dusit Niyato", "Chau Yuen"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00268v1", "summary": "Flexible intelligent metasurfaces (FIMs) offer a new solution for wireless\ncommunications by introducing morphological degrees of freedom, dynamically\nmorphing their three-dimensional shape to ensure multipath signals interfere\nconstructively. However, realizing the desired performance gains in FIM systems\nis critically dependent on acquiring accurate channel state information across\na continuous and high-dimensional deformation space. Therefore, this paper\ninvestigates this fundamental channel estimation problem for FIM assisted\nmillimeter-wave communication systems. First, we develop model-based frameworks\nthat structure the problem as either function approximation using interpolation\nand kernel methods or as a sparse signal recovery problem that leverages the\ninherent angular sparsity of millimeter-wave channels. To further advance the\nestimation capability beyond explicit assumptions in model-based channel\nestimation frameworks, we propose a deep learning-based framework using a\nFourier neural operator (FNO). By parameterizing a global convolution operator\nin the Fourier domain, we design an efficient FNO architecture to learn the\ncontinuous operator that maps FIM shapes to channel responses with\nmesh-independent properties. Furthermore, we exploit a hierarchical FNO (H-FNO)\narchitecture to efficiently capture the multi-scale features across a hierarchy\nof spatial resolutions. Numerical results demonstrate that the proposed H-FNO\nsignificantly outperforms the model-based benchmarks in estimation accuracy and\npilot efficiency. In particular, the interpretability analysis show that the\nproposed H-FNO learns an anisotropic spatial filter adapted to the physical\ngeometry of FIM and is capable of accurately reconstructing the non-linear\nchannel response across the continuous deformation space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00268v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00766", "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00766v1", "summary": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00766v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.12312", "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "authors": ["Zihao Xu", "Junchen Ding", "Yiling Lou", "Kun Zhang", "Dong Gong", "Yuekang Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.12312v2", "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.12312v2", "cate": "cs.CL", "date": "2025-04-09", "updated": "2025-08-01"}
{"id": "2508.00558", "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the IEEE/CVF International Conference on Computer Vision (ICCV), 2025", "url": "http://arxiv.org/abs/2508.00558v1", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "pdf_url": "http://arxiv.org/pdf/2508.00558v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00652", "title": "The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech", "authors": ["Shuning Zhang", "Han Chen", "Yabo Wang", "Yiqun Xu", "Jiaqi Bai", "Yuanyuan Wu", "Shixuan Li", "Xin Yi", "Chunhui Wang", "Hewu Li"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00652v1", "summary": "Pervasive voice interaction enables deceptive patterns through subtle voice\ncharacteristics, yet empirical investigation into this manipulation lags\nbehind, especially within major non-English language contexts. Addressing this\ngap, our study presents the first systematic investigation into voice\ncharacteristic-based dark patterns employing female synthetic voices in\nMandarin Chinese. This focus is crucial given the prevalence of female personas\nin commercial assistants and the prosodic significance in the Chinese language.\nGuided by the conceptual framework identifying key influencing factors, we\nsystematically evaluate effectiveness variations by manipulating voice\ncharacteristics (five characteristics, three intensities) across different\nscenarios (shopping vs. question-answering) with different commercial aims. A\npreliminary study (N=24) validated the experimental materials and the main\nstudy (N=36) revealed significant behavioral manipulation (up to +2027.6%).\nCrucially, the analysis showed that effectiveness varied significantly with\nvoice characteristics and scenario, mediated by user perception (of tone,\nintonation, timbre) and user demographics (individual preferences, though\nlimited demographic impact). These interconnected findings offer evidence-based\ninsights for ethical design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00652v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00579", "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval", "authors": ["Ziyu Gong", "Yihua Huang", "Chengcheng Mai"], "categories": ["cs.MM", "cs.IR"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00579v1", "summary": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00579v1", "cate": "cs.MM", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00379", "title": "Active IRS-Enabled Integrated Sensing and Communications with Extended Targets", "authors": ["Yuan Fang", "Xianxin Song", "Huazhou Hou", "Ziguo Zhong", "Xianghao Yu", "Jie Xu", "Yongming Huang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00379v1", "summary": "This paper studies the active intelligent reflecting surface (IRS)-enabled\nintegrated sensing and communications (ISAC), in which an active IRS is\ndeployed to assist the base station (BS) in serving multiple communication\nusers (CUs) and simultaneously sensing an \\emph{extended} target at the\nnon-line-of-sight (NLoS) area of the BS. The active IRS has the capability of\namplifying the reflected signals so as to overcome significant reflection path\nloss in NLoS communication and sensing. In particular, we derive the sensing\nCram\\'{e}r-Rao bound (CRB) for estimating the target response matrix.\nAccordingly, we jointly optimize the transmit beamforming at the BS and the\nreflective beamforming at the active IRS to minimize the sensing CRB, subject\nto the signal-to-interference-plus-noise ratio (SINR) requirements at the CUs,\nthe transmit power budgets at the BS and active IRS, as well as the power\namplification gain constraints at the active IRS. The CRB minimization problem\nis highly non-convex and thus difficult to solve in general. To address this\nchallenge, we first focus on two specified conditions by considering the\nsensing-only scenario via ignoring the SINR constraints for communications, for\nwhich the closed-form optimal transmit beamforming is derived. Then, we propose\ntwo efficient alternating optimization (AO)-based algorithms to obtain\nhigh-quality solutions for the general ISAC scenarios. Next, we analyze the\ninherent relationship between the power scaling at the BS and the amplification\nscaling at the active IRS. It is shown that the active IRS always amplifies the\nsignal using the maximum amplification gain under practical system settings.\nFinally, numerical results are provided to verify the effectiveness of the\nproposed AO-based algorithms and the benefits of active IRS-enabled ISAC\ncompared to its passive IRSs counterparts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00379v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2211.10085", "title": "Identifying Unique Spatial-Temporal Bayesian Network without Markov Equivalence", "authors": ["Mingyu Kang", "Duxin Chen", "Ning Meng", "Gang Yan", "Wenwu Yu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      This manuscript is submitted to facilitate early access and encourage follow-up research by other scholars. The code for this work is available at: this https URL . We sincerely thank you for your support!", "url": "http://arxiv.org/abs/2211.10085v4", "summary": "Identifying vanilla Bayesian network to model spatial-temporal causality can\nbe a critical yet challenging task. Different Markovian-equivalent directed\nacyclic graphs would be identified if the identifiability is not satisfied. To\naddress this issue, Directed Cyclic Graph is proposed to drop the directed\nacyclic constraint. But it does not always hold, and cannot model dynamical\ntime-series process. Then, Full Time Graph is proposed with introducing\nhigh-order time delay. Full Time Graph has no Markov equivalence class by\nassuming no instantaneous effects. But, it also assumes that the causality is\ninvariant with varying time, that is not always satisfied in the\nspatio-temporal scenarios. Thus, in this work, a Spatial-Temporal Bayesian\nNetwork (STBN) is proposed to theoretically model the spatial-temporal\ncausality from the perspective of information transfer. STBN explains the\ndisappearance of network structure $X\\rightarrow Z \\rightarrow Y$ and\n$X\\leftarrow Z \\leftarrow Y$ by the principle of information path blocking. And\nfinally, the uniqueness of STBN is proved. Based on this, a High-order Causal\nEntropy (HCE) algorithm is also proposed to uniquely identify STBN under time\ncomplexity $\\mathcal{O}(n^3\\tau_{max})$, where $n$ is the number of variables\nand $\\tau_{max}$ is the maximum time delay. Numerical experiments are conducted\nwith comparison to other baseline algorithms. The results show that HCE\nalgorithm obtains state-of-the-art identification accuracy. The code is\navailable at https://github.com/KMY-SEU/HCE.", "comment": "This manuscript is submitted to facilitate early access and encourage\n  follow-up research by other scholars. The code for this work is available at:\n  https://github.com/KMY-SEU/HCE. We sincerely thank you for your support!", "pdf_url": "http://arxiv.org/pdf/2211.10085v4", "cate": "cs.AI", "date": "2022-11-18", "updated": "2025-08-01"}
{"id": "2504.16604", "title": "Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories", "authors": ["Mareike Lisker", "Christina Gottschalk", "Helena Mihaljević"], "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, Association for Computational Linguistics, Proceedings of the 9th Workshop on Online Abuse and Harms (WOAH 2025)", "url": "http://arxiv.org/abs/2504.16604v2", "summary": "Counterspeech is a key strategy against harmful online content, but scaling\nexpert-driven efforts is challenging. Large Language Models (LLMs) present a\npotential solution, though their use in countering conspiracy theories is\nunder-researched. Unlike for hate speech, no datasets exist that pair\nconspiracy theory comments with expert-crafted counterspeech. We address this\ngap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively\napply counterspeech strategies derived from psychological research provided\nthrough structured prompts. Our results show that the models often generate\ngeneric, repetitive, or superficial results. Additionally, they\nover-acknowledge fear and frequently hallucinate facts, sources, or figures,\nmaking their prompt-based use in practical applications problematic.", "comment": "16 pages, Association for Computational Linguistics, Proceedings of\n  the 9th Workshop on Online Abuse and Harms (WOAH 2025)", "pdf_url": "http://arxiv.org/pdf/2504.16604v2", "cate": "cs.CL", "date": "2025-04-23", "updated": "2025-08-01"}
{"id": "2508.00563", "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images", "authors": ["Hannah Kniesel", "Leon Sick", "Tristan Payer", "Tim Bergner", "Kavitha Shaga Devan", "Clarissa Read", "Paul Walther", "Timo Ropinski"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00563v1", "summary": "Current state-of-the-art methods for object detection rely on annotated\nbounding boxes of large data sets for training. However, obtaining such\nannotations is expensive and can require up to hundreds of hours of manual\nlabor. This poses a challenge, especially since such annotations can only be\nprovided by experts, as they require knowledge about the scientific domain. To\ntackle this challenge, we propose a domain-specific weakly supervised object\ndetection algorithm that only relies on image-level annotations, which are\nsignificantly easier to acquire. Our method distills the knowledge of a\npre-trained model, on the task of predicting the presence or absence of a virus\nin an image, to obtain a set of pseudo-labels that can be used to later train a\nstate-of-the-art object detection model. To do so, we use an optimization\napproach with a shrinking receptive field to extract virus particles directly\nwithout specific network architectures. Through a set of extensive studies, we\nshow how the proposed pseudo-labels are easier to obtain, and, more\nimportantly, are able to outperform other existing weak labeling methods, and\neven ground truth labels, in cases where the time to obtain the annotation is\nlimited.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00563v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00723", "title": "Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption", "authors": ["Rebecca Yu", "Valerie Chen", "Ameet Talwalkar", "Hoda Heidari"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      To be published in Proceedings of the Eighth AAAI/ACM Conference on AI, Ethics, and Society (AIES-25). 10 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2508.00723v1", "summary": "Growing excitement around deploying AI across various domains calls for a\ncareful assessment of how human decision-makers interact with AI-powered\nsystems. In particular, it is essential to understand when decision-makers\nvoluntarily choose to consult AI tools, which we term decision-maker adoption.\nWe interviewed experts across four domains -- medicine, law, journalism, and\nthe public sector -- to explore current AI use cases and perceptions of\nadoption. From these interviews, we identify key factors that shape\ndecision-maker adoption of AI tools: the decision-maker's background,\nperceptions of the AI, consequences for the decision-maker, and perceived\nimplications for other stakeholders. We translate these factors into an AI\nadoption sheet to analyze how decision-makers approach adoption choices through\ncomparative, cross-domain case studies, highlighting how our factors help\nexplain inter-domain differences in adoption. Our findings offer practical\nguidance for supporting the responsible and context-aware deployment of AI by\nbetter accounting for the decision-maker's perspective.", "comment": "To be published in Proceedings of the Eighth AAAI/ACM Conference on\n  AI, Ethics, and Society (AIES-25). 10 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2508.00723v1", "cate": "cs.HC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.22675", "title": "Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation", "authors": ["Jiakai Tang", "Sunhao Dai", "Teng Shi", "Jun Xu", "Xu Chen", "Wen Chen", "Jian Wu", "Yuning Jiang"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.22675v3", "summary": "Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.22675v3", "cate": "cs.IR", "date": "2025-03-28", "updated": "2025-08-01"}
{"id": "2508.00458", "title": "LO-Aware Adaptive Modulation for Rydberg Atomic Receivers", "authors": ["Jiuyu Liu", "Yi Ma", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2508.00458v1", "summary": "Rydberg atomic (RA) receivers represent a revolutionary quantum technology\nfor wireless communications, offering unprecedented sensitivity beyond\nconventional radio frequency (RF) antennas. However, these receivers detect\nonly signal amplitude, losing critical phase information. While reference\nsignals generated by a local oscillator (LO) can assist in phase recovery,\nexisting modulation schemes designed for conventional systems perform poorly\nwith this quantum detection mechanism. This paper introduces a breakthrough\nLO-aware adaptive modulation (LOAM) scheme specifically developed for RA\nreceivers that dynamically adapts to complex fading channel coefficients. LOAM\nmaximizes the minimum amplitude difference between constellation points,\nensuring optimal detection performance. The innovation employs an adaptive\nco-linear constellation architecture aligned with the combined phase of\nreference signal and channel coefficient. For strong reference signals, LOAM\ngenerates symmetric constellation points centered at origin; for weak signals,\nit adopts non-symmetric distributions. The paper mathematically derives the\nthreshold governing these operational regimes. Simulation results reveal the\ntransformative impact of LOAM, demonstrating performance gains exceeding 45 dB\nover conventional modulation schemes, including quadrature amplitude modulation\n(QAM), phase-shift keying (PSK), and pulse-amplitude modulation (PAM).", "comment": "Accepted by IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00458v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2405.20046", "title": "Federated Cross-Training Learners for Robust Generalization under Data Heterogeneity", "authors": ["Zhuang Qi", "Lei Meng", "Ruohan Zhang", "Yu Wang", "Xin Qi", "Xiangxu Meng", "Han Yu", "Qiang Yang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.20046v2", "summary": "Federated learning benefits from cross-training strategies, which enables\nmodels to train on data from distinct sources to improve generalization\ncapability. However, due to inherent differences in data distributions, the\noptimization goals of local models remain misaligned, and this mismatch\ncontinues to manifest as feature space heterogeneity even after cross-training.\nWe argue that knowledge distillation from the personalized view preserves\nclient-specific characteristics and expands the local knowledge base, while\ndistillation from the global view provides consistent semantic anchors that\nfacilitate feature alignment across clients. To achieve this goal, this paper\npresents a cross-training scheme, termed FedCT, includes three main modules,\nwhere the consistency-aware knowledge broadcasting module aims to optimize\nmodel assignment strategies, which enhances collaborative advantages between\nclients and achieves an efficient federated learning process. The multi-view\nknowledge-guided representation learning module leverages fused prototypical\nknowledge from both global and local views to enhance the preservation of local\nknowledge before and after model exchange, as well as to ensure consistency\nbetween local and global knowledge. The mixup-based feature augmentation module\naggregates rich information to further increase the diversity of feature\nspaces, which enables the model to better discriminate complex samples.\nExtensive experiments were conducted on four datasets in terms of performance\ncomparison, ablation study, in-depth analysis and case study. The results\ndemonstrated that FedCT alleviates knowledge forgetting from both local and\nglobal views, which enables it outperform state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.20046v2", "cate": "cs.AI", "date": "2024-05-30", "updated": "2025-08-01"}
{"id": "2504.16787", "title": "Credible Plan-Driven RAG Method for Multi-Hop Question Answering", "authors": ["Ningning Zhang", "Chi Zhang", "Zhizhong Tan", "Xingxing Yang", "Weiping Deng", "Wenyong Wang"], "categories": ["cs.CL", "cs.AI", "I.2.0"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      17 pages, 5 figures", "url": "http://arxiv.org/abs/2504.16787v2", "summary": "Multi-hop question answering (QA) presents significant challenges for\nretrieval-augmented generation (RAG), particularly in decomposing complex\nqueries into reliable reasoning paths and managing error propagation. Existing\nRAG methods often suffer from deviations in reasoning paths and cumulative\nerrors in intermediate steps, reducing the fidelity of the final answer. To\naddress these limitations, we propose PAR-RAG (Plan-then-Act-and-Review RAG), a\nnovel framework inspired by the PDCA (Plan-Do-Check-Act) cycle, to enhance both\nthe accuracy and factual consistency in multi-hop question answering.\nSpecifically, PAR-RAG selects exemplars matched by the semantic complexity of\nthe current question to guide complexity-aware top-down planning, resulting in\nmore precise and coherent multi-step reasoning trajectories. This design\nmitigates reasoning drift and reduces the risk of suboptimal path convergence,\na common issue in existing RAG approaches. Furthermore, a dual-verification\nmechanism evaluates and corrects intermediate errors, ensuring that the\nreasoning process remains factually grounded. Experimental results on various\nQA benchmarks demonstrate that PAR-RAG outperforms existing state-of-the-art\nmethods, validating its effectiveness in both performance and reasoning\nrobustness.", "comment": "17 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2504.16787v2", "cate": "cs.CL", "date": "2025-04-23", "updated": "2025-08-01"}
{"id": "2508.00568", "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry", "authors": ["Jingchao Xie", "Oussema Dhaouadi", "Weirong Chen", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for GCPR 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2508.00568v1", "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.", "comment": "Accepted for GCPR 2025. Project page:\n  https://jchao-xie.github.io/CoProU/", "pdf_url": "http://arxiv.org/pdf/2508.00568v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.18203", "title": "Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors", "authors": ["Michelle S. Lam", "Fred Hohman", "Dominik Moritz", "Jeffrey P. Bigham", "Kenneth Holstein", "Mary Beth Kery"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      UIST 2025", "url": "http://arxiv.org/abs/2409.18203v2", "summary": "AI policy sets boundaries on acceptable behavior for AI models, but this is\nchallenging in the context of large language models (LLMs): how do you ensure\ncoverage over a vast behavior space? We introduce policy maps, an approach to\nAI policy design inspired by the practice of physical mapmaking. Instead of\naiming for full coverage, policy maps aid effective navigation through\nintentional design choices about which aspects to capture and which to abstract\naway. With Policy Projector, an interactive tool for designing LLM policy maps,\nan AI practitioner can survey the landscape of model input-output pairs, define\ncustom regions (e.g., \"violence\"), and navigate these regions with if-then\npolicy rules that can act on LLM outputs (e.g., if output contains \"violence\"\nand \"graphic details,\" then rewrite without \"graphic details\"). Policy\nProjector supports interactive policy authoring using LLM classification and\nsteering and a map visualization reflecting the AI practitioner's work. In an\nevaluation with 12 AI safety experts, our system helps policy designers craft\npolicies around problematic model behaviors such as incorrect gender\nassumptions and handling of immediate physical safety threats.", "comment": "UIST 2025", "pdf_url": "http://arxiv.org/pdf/2409.18203v2", "cate": "cs.HC", "date": "2024-09-26", "updated": "2025-08-01"}
{"id": "2507.22268", "title": "Multi-modal Relational Item Representation Learning for Inferring Substitutable and Complementary Items", "authors": ["Junting Wang", "Chenghuan Guo", "Jiao Yang", "Yanhui Guo", "Yan Gao", "Hari Sundaram"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22268v2", "summary": "We introduce a novel self-supervised multi-modal relational item\nrepresentation learning framework designed to infer substitutable and\ncomplementary items. Existing approaches primarily focus on modeling item-item\nassociations deduced from user behaviors using graph neural networks (GNNs) or\nleveraging item content information. However, these methods often overlook\ncritical challenges, such as noisy user behavior data and data sparsity due to\nthe long-tailed distribution of these behaviors. In this paper, we propose\nMMSC, a self-supervised multi-modal relational item representation learning\nframework to address these challenges. Specifically, MMSC consists of three\nmain components: (1) a multi-modal item representation learning module that\nleverages a multi-modal foundational model and learns from item metadata, (2) a\nself-supervised behavior-based representation learning module that denoises and\nlearns from user behavior data, and (3) a hierarchical representation\naggregation mechanism that integrates item representations at both the semantic\nand task levels. Additionally, we leverage LLMs to generate augmented training\ndata, further enhancing the denoising process during training. We conduct\nextensive experiments on five real-world datasets, showing that MMSC\noutperforms existing baselines by 26.1% for substitutable recommendation and\n39.2% for complementary recommendation. In addition, we empirically show that\nMMSC is effective in modeling cold-start items.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22268v2", "cate": "cs.IR", "date": "2025-07-29", "updated": "2025-07-31"}
{"id": "2508.00540", "title": "Appendices for \"Closed-Form BER Analysis for Uplink NOMA with Dynamic SIC Decoding\"", "authors": ["Hequn Zhang", "Qu Luo", "Pei Xiao", "Yue Zhang", "Huiyu Zhou"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00540v1", "summary": "This document provides the supplementary materials for the paper Closed-Form\nBER Analysis for Uplink NOMA with Dynamic SIC Decoding. The appendices present\ndetailed mathematical derivations and proofs that support the analytical\nframework of the main paper. Specifically, we include: (i) cumulative\ndistribution functions for ordered channel gains; (ii) probability density\nfunctions of normalized signal-plus-interference variances in NOMA dynamic SIC\ndecoding; (iii) closed-form expressions for pairwise error probability (PEP)\nwith two users; (iv) probability derivations for channel gain ordering in the\ntwo-UE case, specifically when UE 1 and UE 2 have the strongest or second\nstrongest channel gains; (v) BER analysis for M-QAM modulation schemes\nincluding BPSK, 4QAM, 16QAM and 64QAM; (vi) PDF derivations for channel gains\nunder various ordering conditions; and (vii) challenges of PDF derivations for\nreal part of channel gain under various ordering condition. These mathematical\nfoundations enable the closed-form BER analysis of uplink NOMA systems with\ndynamic SIC decoding under Rayleigh fading channels, supporting analytical\nexpressions for various modulation schemes and system configurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00540v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2408.07877", "title": "BCR-DRL: Behavior- and Context-aware Reward for Deep Reinforcement Learning in Human-AI Coordination", "authors": ["Xin Hao", "Bahareh Nakisa", "Mohmmad Naim Rastgoo", "Gaoyang Pang"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.07877v5", "summary": "Deep reinforcement Learning (DRL) offers a powerful framework for training AI\nagents to coordinate with human partners. However, DRL faces two critical\nchallenges in human-AI coordination (HAIC): sparse rewards and unpredictable\nhuman behaviors. These challenges significantly limit DRL to identify effective\ncoordination policies, due to its impaired capability of optimizing exploration\nand exploitation. To address these limitations, we propose an innovative\nbehavior- and context-aware reward (BCR) for DRL, which optimizes exploration\nand exploitation by leveraging human behaviors and contextual information in\nHAIC. Our BCR consists of two components: (i) A novel dual intrinsic rewarding\nscheme to enhance exploration. This scheme composes an AI self-motivated\nintrinsic reward and a human-motivated intrinsic reward, which are designed to\nincrease the capture of sparse rewards by a logarithmic-based strategy; and\n(ii) A new context-aware weighting mechanism for the designed rewards to\nimprove exploitation. This mechanism helps the AI agent prioritize actions that\nbetter coordinate with the human partner by utilizing contextual information\nthat can reflect the evolution of learning. Extensive simulations in the\nOvercooked environment demonstrate that our approach can increase the\ncumulative sparse rewards by approximately 20%, and improve the sample\nefficiency by around 38% compared to state-of-the-art baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.07877v5", "cate": "cs.AI", "date": "2024-08-15", "updated": "2025-08-01"}
{"id": "2505.15055", "title": "Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory", "authors": ["Hongli Zhou", "Hui Huang", "Ziqing Zhao", "Lvyuan Han", "Huicheng Wang", "Kehai Chen", "Muyun Yang", "Wei Bao", "Jian Dong", "Bing Xu", "Conghui Zhu", "Hailong Cao", "Tiejun Zhao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.15055v2", "summary": "The evaluation of large language models (LLMs) via benchmarks is widespread,\nyet inconsistencies between different leaderboards and poor separability among\ntop models raise concerns about their ability to accurately reflect authentic\nmodel capabilities. This paper provides a critical analysis of benchmark\neffectiveness, examining mainstream prominent LLM benchmarks using results from\ndiverse models. We first propose Pseudo-Siamese Network for Item Response\nTheory (PSN-IRT), an enhanced Item Response Theory framework that incorporates\na rich set of item parameters within an IRT-grounded architecture. PSN-IRT can\nbe utilized for accurate and reliable estimations of item characteristics and\nmodel abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM\nbenchmarks comprising 41,871 items, revealing significant and varied\nshortcomings in their measurement quality. Furthermore, we demonstrate that\nleveraging PSN-IRT is able to construct smaller benchmarks while maintaining\nstronger alignment with human preference.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.15055v2", "cate": "cs.CL", "date": "2025-05-21", "updated": "2025-08-01"}
{"id": "2508.00587", "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection", "authors": ["Marc Hölle", "Walter Kellermann", "Vasileios Belagiannis"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCVW 2025, 11 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00587v1", "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.", "comment": "Accepted at ICCVW 2025, 11 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00587v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.02929", "title": "AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality", "authors": ["Brandon Woodard", "Margarita Geleta", "Joseph J. LaViola Jr.", "Andrea Fanelli", "Rhonda Wilson"], "categories": ["cs.HC", "cs.SD", "eess.AS", "H.5.2; H.5.5; H.5.1"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Revision necessary for accuracy", "url": "http://arxiv.org/abs/2502.02929v3", "summary": "We present AudioMiXR, an augmented reality (AR) interface intended to assess\nhow users manipulate virtual audio objects situated in their physical space\nusing six degrees of freedom (6DoF) deployed on a head-mounted display (Apple\nVision Pro) for 3D sound design. Existing tools for 3D sound design are\ntypically constrained to desktop displays, which may limit spatial awareness of\nmixing within the execution environment. Utilizing an XR HMD to create\nsoundscapes may provide a real-time test environment for 3D sound design, as\nmodern HMDs can provide precise spatial localization assisted by cross-modal\ninteractions. However, there is no research on design guidelines specific to\nsound design with six degrees of freedom (6DoF) in XR. To provide a first step\ntoward identifying design-related research directions in this space, we\nconducted an exploratory study where we recruited 27 participants, consisting\nof expert and non-expert sound designers. The goal was to assess design lessons\nthat can be used to inform future research venues in 3D sound design. We ran a\nwithin-subjects study where users designed both a music and cinematic\nsoundscapes. After thematically analyzing participant data, we constructed two\ndesign lessons: 1. Proprioception for AR Sound Design, and 2. Balancing\nAudio-Visual Modalities in AR GUIs. Additionally, we provide application\ndomains that can benefit most from 6DoF sound design based on our results.", "comment": "Revision necessary for accuracy", "pdf_url": "http://arxiv.org/pdf/2502.02929v3", "cate": "cs.HC", "date": "2025-02-05", "updated": "2025-08-01"}
{"id": "2507.02962", "title": "RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism", "authors": ["Zhiwen Tan", "Jiaming Huang", "Qintong Wu", "Hongxuan Zhang", "Chenyi Zhuang", "Jinjie Gu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02962v4", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, while LLMs remain prone to generating hallucinated or outdated\nresponses due to their static internal knowledge. Recent advancements in\nRetrieval-Augmented Generation (RAG) methods have aimed to enhance models'\nsearch and reasoning capabilities through reinforcement learning (RL). Although\nthese methods demonstrate promising results, they face challenges in training\nstability and encounter issues such as substantial inference time and\nrestricted capabilities due to reliance on single-query mode. In this paper, we\npropose RAG-R1, a novel training framework designed to enable LLMs to\nadaptively leverage internal and external knowledge during the reasoning\nprocess. We further expand the generation and retrieval processes within the\nframework from single-query mode to multi-query parallelism, with the aim of\nreducing inference time and enhancing the model's capabilities. Extensive\nexperiments on seven question-answering benchmarks demonstrate that our method\noutperforms the strongest baseline by up to 13.2% and decreases inference time\nby 11.1%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02962v4", "cate": "cs.CL", "date": "2025-06-30", "updated": "2025-08-01"}
{"id": "2508.00626", "title": "Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO Systems in the Near-Field Domain", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00626v1", "summary": "Accurate and efficient channel state information (CSI) feedback is crucial\nfor unlocking the substantial spectral efficiency gains of extremely\nlarge-scale MIMO (XL-MIMO) systems in future 6G networks. However, the\ncombination of near-field spherical wave propagation and frequency-dependent\nbeam split effects in wideband scenarios poses significant challenges for CSI\nrepresentation and compression. This paper proposes WideNLNet-CA, a\nrate-adaptive deep learning framework designed to enable efficient CSI feedback\nin wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight\nencoder-decoder architecture with multi-stage downsampling and upsampling,\nincorporating computationally efficient residual blocks to capture complex\nmulti-scale channel features with reduced overhead. A novel compression ratio\nadaptive module with feature importance estimation is introduced to dynamically\nmodulate feature selection based on target compression ratios, enabling\nflexible adaptation across a wide range of feedback rates using a single model.\nEvaluation results demonstrate that WideNLNet-CA consistently outperforms\nexisting compressive sensing and deep learning-based works across various\ncompression ratios and bandwidths, while maintaining fast inference and low\nmodel storage requirements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00626v1", "cate": "cs.IT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.08875", "title": "Causal Explanations for Image Classifiers", "authors": ["Hana Chockler", "David A. Kelly", "Daniel Kroening", "Youcheng Sun"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.08875v2", "summary": "Existing algorithms for explaining the output of image classifiers use\ndifferent definitions of explanations and a variety of techniques to extract\nthem. However, none of the existing tools use a principled approach based on\nformal definitions of causes and explanations for the explanation extraction.\n  In this paper we present a novel black-box approach to computing explanations\ngrounded in the theory of actual causality. We prove relevant theoretical\nresults and present an algorithm for computing approximate explanations based\non these definitions. We prove termination of our algorithm and discuss its\ncomplexity and the amount of approximation compared to the precise definition.\n  We implemented the framework in a tool ReX and we present experimental\nresults and a comparison with state-of-the-art tools. We demonstrate that \\rex\nis the most efficient tool and produces the smallest explanations, in addition\nto outperforming other black-box tools on standard quality measures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.08875v2", "cate": "cs.AI", "date": "2024-11-13", "updated": "2025-07-31"}
{"id": "2505.23628", "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora", "authors": ["Jiaxin Bai", "Wei Fan", "Qi Hu", "Qing Zong", "Chunyang Li", "Hong Ting Tsang", "Hongyu Luo", "Yauwai Yim", "Haoyu Huang", "Xiao Zhou", "Feng Qin", "Tianshi Zheng", "Xi Peng", "Xin Yao", "Huiwen Yang", "Leijie Wu", "Yi Ji", "Gong Zhang", "Renhai Chen", "Yangqiu Song"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, preprint, code: this https URL", "url": "http://arxiv.org/abs/2505.23628v3", "summary": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 92\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.", "comment": "9 pages, preprint, code:\n  https://github.com/HKUST-KnowComp/AutoSchemaKG", "pdf_url": "http://arxiv.org/pdf/2505.23628v3", "cate": "cs.CL", "date": "2025-05-29", "updated": "2025-08-01"}
{"id": "2508.00590", "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00590v1", "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00590v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.20035", "title": "Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality", "authors": ["Brandon Woodard", "Melvin He", "Mose Sakashita", "Jing Qian", "Zainab Iftikhar", "Joseph J. LaViola Jr"], "categories": ["cs.HC", "H.5.2; H.5.1; H.5.0; H.1.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20035v4", "summary": "Off-the-shelf smartphone-based AR systems typically use a single front-facing\nor rear-facing camera, which restricts user interactions to a narrow field of\nview and small screen size, thus reducing their practicality. We present\nCam-2-Cam, an interaction concept implemented in three smartphone-based AR\napplications with interactions that span both cameras. Results from our\nqualitative analysis conducted on 30 participants presented two major design\nlessons that explore the interaction space of smartphone AR while maintaining\ncritical AR interface attributes like embodiment and immersion: (1) Balancing\nContextual Relevance and Feedback Quality serves to outline a delicate balance\nbetween implementing familiar interactions people do in the real world and the\nquality of multimodal AR responses and (2) Preventing Disorientation using\nSimultaneous Capture and Alternating Cameras which details how to prevent\ndisorientation during AR interactions using the two distinct camera techniques\nwe implemented in the paper. Additionally, we consider observed user\nassumptions or natural tendencies to inform future implementations of\ndual-camera setups for smartphone-based AR. We envision our design lessons as\nan initial pioneering step toward expanding the interaction space of\nsmartphone-based AR, potentially driving broader adoption and overcoming\nlimitations of single-camera AR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20035v4", "cate": "cs.HC", "date": "2025-04-28", "updated": "2025-08-01"}
{"id": "2508.00502", "title": "Clubs in projective spaces and three-weight rank-metric codes", "authors": ["Jonathan Mannaert", "Paolo Santonastaso", "Ferdinando Zullo"], "categories": ["math.CO", "cs.IT", "math.IT"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00502v1", "summary": "Linear sets over finite fields are central objects in finite geometry and\ncoding theory, with deep connections to structures such as semifields, blocking\nsets, KM-arcs, and rank-metric codes. Among them, $i$-clubs, a class of linear\nsets where all but one point (which has weight $i$) have weight one, have been\nextensively studied in the projective line but remain poorly understood in\nhigher-dimensional projective spaces. In this paper, we investigate the\ngeometry and algebraic structure of $i$-clubs in projective spaces. We\nestablish upper bounds on their rank by associating them with rank-metric codes\nand analyzing their parameters via MacWilliams identities. We also provide\nexplicit constructions of $i$-clubs that attain the maximum rank for $i \\geq\nm/2$, and we demonstrate the existence of non-equivalent constructions when $i\n\\leq m-2$. The special case $i = m-1$ is fully classified. Furthermore, we\nexplore the rich geometry of three-weight rank-metric codes, offering new\nconstructions from clubs and partial classification results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00502v1", "cate": "math.CO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.10009", "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problems with Reasoning LLM", "authors": ["Bowen Zhang", "Pengcheng Luo", "Genke Yang", "Boon-Hee Soong", "Chau Yuen"], "categories": ["cs.AI", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      8 pages, 13 figures", "url": "http://arxiv.org/abs/2503.10009v3", "summary": "With the rise of artificial intelligence (AI), applying large language models\n(LLMs) to mathematical problem-solving has attracted increasing attention. Most\nexisting approaches attempt to improve Operations Research (OR) optimization\nproblem-solving through prompt engineering or fine-tuning strategies for LLMs.\nHowever, these methods are fundamentally constrained by the limited\ncapabilities of non-reasoning LLMs. To overcome these limitations, we propose\nOR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR\nproblem solving. The framework decomposes the task into three sequential\nstages: mathematical modeling, code generation, and debugging. Each task is\nhandled by a dedicated sub-agent, which enables more targeted reasoning. We\nalso construct BWOR, an OR dataset for evaluating LLM performance on OR tasks.\nOur analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR,\nreasoning LLMs sometimes underperform their non-reasoning counterparts within\nthe same model family. In contrast, BWOR provides a more consistent and\ndiscriminative assessment of model capabilities. Experimental results\ndemonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework\noutperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1,\nand ORLM, by at least 7\\% in accuracy. These results demonstrate the\neffectiveness of task decomposition for OR problem solving.", "comment": "8 pages, 13 figures", "pdf_url": "http://arxiv.org/pdf/2503.10009v3", "cate": "cs.AI", "date": "2025-03-13", "updated": "2025-08-01"}
{"id": "2506.21910", "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers", "authors": ["Ernie Chang", "Yang Li", "Patrick Huber", "Vish Vogeti", "David Kant", "Yangyang Shi", "Vikas Chandra"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ACL 2025", "url": "http://arxiv.org/abs/2506.21910v2", "summary": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures.", "comment": "Accepted at ACL 2025", "pdf_url": "http://arxiv.org/pdf/2506.21910v2", "cate": "cs.CL", "date": "2025-06-27", "updated": "2025-08-01"}
{"id": "2508.00592", "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry", "authors": ["Jiajun Le", "Jiayi Ma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00592v1", "summary": "Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00592v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00542", "title": "Assessing (im)balance in signed brain networks", "authors": ["Marzio Di Vece", "Emanuele Agrimi", "Samuele Tatullo", "Tommaso Gili", "Miguel Ibáñez-Berganza", "Tiziano Squartini"], "categories": ["physics.soc-ph", "cs.IT", "math.IT", "physics.data-an", "physics.med-ph", "stat.ME"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      41 pages, 17 figures, 1 table", "url": "http://arxiv.org/abs/2508.00542v1", "summary": "Many complex systems - be they financial, natural or social - are composed by\nunits - such as stocks, neurons or agents - whose joint activity can be\nrepresented as a multivariate time series. An issue of both practical and\ntheoretical importance concerns the possibility of inferring the presence of a\nstatic relationships between any two units solely from their dynamic state. The\npresent contribution aims at providing an answer within the frame of\ntraditional hypothesis testing. Briefly speaking, our suggestion is that of\nlinking any two units if behaving in a sufficiently similar way. To achieve\nsuch a goal, we project a multivariate time series onto a signed graph, by i)\ncomparing the empirical properties of the former with those expected under a\nsuitable benchmark and ii) linking any two units with a positive (negative)\nedge in case the corresponding series share a significantly large number of\nconcordant (discordant) values. To define our benchmarks, we adopt an\ninformation-theoretic approach that is rooted into the constrained maximisation\nof Shannon entropy, a procedure inducing an ensemble of multivariate time\nseries that preserves some of the empirical properties on average while\nrandomising everything else. We showcase the possible applications of our\nmethod by addressing one of the most timely issues in the domain of\nneurosciences, i.e. that of determining if brain networks are frustrated or not\n- and, in case, to what extent. As our results suggest, this is indeed the\ncase, the structure of the negative subgraph being more prone to inter-subject\nvariability than the complementary, positive subgraph. At the mesoscopic level,\ninstead, the minimisation of the Bayesian Information Criterion instantiated\nwith the Signed Stochastic Block Model reveals that brain areas gather into\nmodules aligning with the statistical variant of the Relaxed Balance Theory.", "comment": "41 pages, 17 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2508.00542v1", "cate": "physics.soc-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00003", "title": "Building Bigraphs of the real world", "authors": ["Kang Rong Roy Ang"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Submitted in partial fulfilment of the requirements for Part II of the Computer Science Tripos at the University of Cambridge", "url": "http://arxiv.org/abs/2508.00003v1", "summary": "This report proposes a formal specification for organising all buildings,\nstreets and administrative areas in the world into a hierarchical\nspace-partitioning tree using data from OpenStreetMap. This hierarchical\nstructure is encoded into a bigraph, serving as a digital twin of the world and\ncapturing complete street connectivity. It presents a tool implemented in OCaml\n(source code at https://github.com/royangkr/bigraph-of-the-world ) that\nconstructs bigraphs for regions from any part of the world. In addition, it\ncontributes algorithmic improvements to open-source bigraph-building tools that\nenable them to efficiently construct and transform extremely large bigraphs,\nachieving up to a 97x speedup among other gains.", "comment": "Submitted in partial fulfilment of the requirements for Part II of\n  the Computer Science Tripos at the University of Cambridge", "pdf_url": "http://arxiv.org/pdf/2508.00003v1", "cate": "cs.LO", "date": "2025-07-02", "updated": "2025-07-02"}
{"id": "2504.02467", "title": "BOOST: Bootstrapping Strategy-Driven Reasoning Programs for Program-Guided Fact-Checking", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Work in Progress", "url": "http://arxiv.org/abs/2504.02467v3", "summary": "Large language model pipelines have improved automated fact-checking for\ncomplex claims, yet many approaches rely on few-shot in-context learning with\ndemonstrations that require substantial human effort and domain expertise.\nAmong these, program-guided reasoning, by decomposing claims into function\ncalls and executing reasoning programs, which has shown particular promise, but\nremains limited by the need for manually crafted demonstrations. Fundamentally,\nthe underlying principles of effective reasoning program generation still\nremain underexplored. In this work, we introduce BOOST, a bootstrapping\napproach for automated few-shot reasoning program generation. BOOST iteratively\nrefines explicit, data-driven guidelines as meta-rules for guiding\ndemonstration creation, using a critique-refine loop that eliminates the need\nfor human intervention. This enables a seamless transition from zero-shot to\nfew-shot program-guided learning, enhancing interpretability and effectiveness.\nExperimental results show that BOOST outperforms prior few-shot baselines in\nboth zero-shot and few-shot settings for complex claim verification.", "comment": "Work in Progress", "pdf_url": "http://arxiv.org/pdf/2504.02467v3", "cate": "cs.AI", "date": "2025-04-03", "updated": "2025-08-01"}
{"id": "2507.04886", "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations", "authors": ["A. Bochkov"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Updated and extended the Ablation Study section with longer training runs for clearer visualization of convergence. Consolidated the discussion in this section to improve clarity and flow", "url": "http://arxiv.org/abs/2507.04886v3", "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.", "comment": "Updated and extended the Ablation Study section with longer training\n  runs for clearer visualization of convergence. Consolidated the discussion in\n  this section to improve clarity and flow", "pdf_url": "http://arxiv.org/pdf/2507.04886v3", "cate": "cs.CL", "date": "2025-07-07", "updated": "2025-07-31"}
{"id": "2508.00599", "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (oral); Code released: this https URL", "url": "http://arxiv.org/abs/2508.00599v2", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.", "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "pdf_url": "http://arxiv.org/pdf/2508.00599v2", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2412.01610", "title": "Stochastic Geometry and Dynamical System Analysis of Walker Satellite Constellations", "authors": ["Chang-Sik Choi", "Francois Baccelli"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      full version of the paper accepted to IEEE Trans. Veh. Technol", "url": "http://arxiv.org/abs/2412.01610v3", "summary": "In practice, low Earth orbit (LEO) and medium Earth orbit (MEO) satellite\nnetworks consist of multiple orbits which are populated with many satellites. A\nwidely used spatial architecture for LEO or MEO satellites is the Walker\nconstellation, where the longitudes of orbits are evenly spaced and the\nsatellites are equally spaced along the orbits. In this paper, we develop a\nstochastic geometry model for the Walker constellations. This proposed model\nenables an analysis based on dynamical system theory, which allows one to\naddress essential structural properties such as periodicity and ergodicity. It\nalso enables a stochastic geometry analysis under which we derive the\nperformance of downlink communications of a typical user at a given latitude,\nas a function of the key constellation parameters.", "comment": "full version of the paper accepted to IEEE Trans. Veh. Technol", "pdf_url": "http://arxiv.org/pdf/2412.01610v3", "cate": "cs.IT", "date": "2024-12-02", "updated": "2025-08-01"}
{"id": "2508.00004", "title": "Reasoning under uncertainty in the game of Cops and Robbers", "authors": ["Dazhu Li", "Sujata Ghosh", "Fenrong Liu"], "categories": ["cs.LO", "math.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00004v1", "summary": "The game of Cops and Robbers is an important model for studying computational\nqueries in pursuit-evasion environments, among others. As recent logical\nexplorations have shown, its structure exhibits appealing analogies with modal\nlogic. In this paper, we enrich the game with a setting in which players may\nhave imperfect information. We propose a new formal framework, Epistemic Logic\nof Cops and Robbers (ELCR), to make the core notions of the game precise, for\ninstance, players' positions, observational power and inference. Applying ELCR\nto analyze the game, we obtain an automated way to track interactions between\nplayers and characterize their information updates during the game. The update\nmechanism is defined by a novel dynamic operator, and we compare it with some\nrelevant paradigms from the game and logic perspectives. We study various\nproperties of ELCR including axiomatization and decidability. To our knowledge,\nthis is the first attempt to explore these games from a formal point of view\nwhere (partial) information available to players is taken into account.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00004v1", "cate": "cs.LO", "date": "2025-07-09", "updated": "2025-07-09"}
{"id": "2505.01712", "title": "World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks", "authors": ["Lingyi Wang", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.AI", "cs.NI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01712v2", "summary": "Traditional reinforcement learning (RL)-based learning approaches for\nwireless networks rely on expensive trial-and-error mechanisms and real-time\nfeedback based on extensive environment interactions, which leads to low data\nefficiency and short-sighted policies. These limitations become particularly\nproblematic in complex, dynamic networks with high uncertainty and long-term\nplanning requirements. To address these limitations, in this paper, a novel\nworld model-based learning framework is proposed to minimize\npacket-completeness-aware age of information (CAoI) in a vehicular network.\nParticularly, a challenging representative scenario is considered pertaining to\na millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,\nwhich is characterized by high mobility, frequent signal blockages, and\nextremely short coherence time. Then, a world model framework is proposed to\njointly learn a dynamic model of the mmWave V2X environment and use it to\nimagine trajectories for learning how to perform link scheduling. In\nparticular, the long-term policy is learned in differentiable imagined\ntrajectories instead of environment interactions. Moreover, owing to its\nimagination abilities, the world model can jointly predict time-varying\nwireless data and optimize link scheduling in real-world wireless and V2X\nnetworks. Thus, during intervals without actual observations, the world model\nremains capable of making efficient decisions. Extensive experiments are\nperformed on a realistic simulator based on Sionna that integrates\nphysics-based end-to-end channel modeling, ray-tracing, and scene geometries\nwith material properties. Simulation results show that the proposed world model\nachieves a significant improvement in data efficiency, and achieves 26%\nimprovement and 16% improvement in CAoI, respectively, compared to the\nmodel-based RL (MBRL) method and the model-free RL (MFRL) method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01712v2", "cate": "cs.AI", "date": "2025-05-03", "updated": "2025-08-01"}
{"id": "2507.11878", "title": "LLMs Encode Harmfulness and Refusal Separately", "authors": ["Jiachen Zhao", "Jing Huang", "Zhengxuan Wu", "David Bau", "Weiyan Shi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.11878v2", "summary": "LLMs are trained to refuse harmful instructions, but do they truly understand\nharmfulness beyond just refusing? Prior work has shown that LLMs' refusal\nbehaviors can be mediated by a one-dimensional subspace, i.e., a refusal\ndirection. In this work, we identify a new dimension to analyze safety\nmechanisms in LLMs, i.e., harmfulness, which is encoded internally as a\nseparate concept from refusal. There exists a harmfulness direction that is\ndistinct from the refusal direction. As causal evidence, steering along the\nharmfulness direction can lead LLMs to interpret harmless instructions as\nharmful, but steering along the refusal direction tends to elicit refusal\nresponses directly without reversing the model's judgment on harmfulness.\nFurthermore, using our identified harmfulness concept, we find that certain\njailbreak methods work by reducing the refusal signals without reversing the\nmodel's internal belief of harmfulness. We also find that adversarially\nfinetuning models to accept harmful instructions has minimal impact on the\nmodel's internal belief of harmfulness. These insights lead to a practical\nsafety application: The model's latent harmfulness representation can serve as\nan intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing\nover-refusals that is robust to finetuning attacks. For instance, our Latent\nGuard achieves performance comparable to or better than Llama Guard 3 8B, a\ndedicated finetuned safeguard model, across different jailbreak methods. Our\nfindings suggest that LLMs' internal understanding of harmfulness is more\nrobust than their refusal decision to diverse input instructions, offering a\nnew perspective to study AI safety", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.11878v2", "cate": "cs.CL", "date": "2025-07-16", "updated": "2025-07-31"}
{"id": "2508.00639", "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "authors": ["Luisa Gallée", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael Götz"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at iMIMIC - Interpretability of Machine Intelligence in Medical Image Computing workshop MICCAI 2025 Medical Image Computing and Computer Assisted Intervention", "url": "http://arxiv.org/abs/2508.00639v1", "summary": "Classification models that provide human-interpretable explanations enhance\nclinicians' trust and usability in medical image diagnosis. One research focus\nis the integration and prediction of pathology-related visual attributes used\nby radiologists alongside the diagnosis, aligning AI decision-making with\nclinical reasoning. Radiologists use attributes like shape and texture as\nestablished diagnostic criteria and mirroring these in AI decision-making both\nenhances transparency and enables explicit validation of model outputs.\nHowever, the adoption of such models is limited by the scarcity of large-scale\nmedical image datasets annotated with these attributes. To address this\nchallenge, we propose synthesizing attribute-annotated data using a generative\nmodel. We enhance the Diffusion Model with attribute conditioning and train it\nusing only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.\nIncorporating its generated images into the training of an explainable model\nboosts performance, increasing attribute prediction accuracy by 13.4% and\ntarget prediction accuracy by 1.8% compared to training with only the small\nreal attribute-annotated dataset. This work highlights the potential of\nsynthetic data to overcome dataset limitations, enhancing the applicability of\nexplainable models in medical image analysis.", "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in\n  Medical Image Computing workshop MICCAI 2025 Medical Image Computing and\n  Computer Assisted Intervention", "pdf_url": "http://arxiv.org/pdf/2508.00639v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2501.11881", "title": "Channel Resolvability Using Multiplicative Weight Update Algorithm", "authors": ["Koki Takahashi", "Shun Watanabe"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      8 pages. Version 2 is for correcting typo. Version 3 improved presentation. Version 4 corrects a minor error in the proof of Theorem 10; the result itself is unaffected", "url": "http://arxiv.org/abs/2501.11881v4", "summary": "We study the channel resolvability problem, which is used to prove strong\nconverse of identification via channel. Channel resolvability has been solved\nby only random coding in the literature. We prove channel resolvability using\nthe multiplicative weight update algorithm. This is the first approach to\nchannel resolvability using non-random coding.", "comment": "8 pages. Version 2 is for correcting typo. Version 3 improved\n  presentation. Version 4 corrects a minor error in the proof of Theorem 10;\n  the result itself is unaffected", "pdf_url": "http://arxiv.org/pdf/2501.11881v4", "cate": "cs.IT", "date": "2025-01-21", "updated": "2025-07-31"}
{"id": "2508.00014", "title": "Deciding the Value of Two-Clock Almost Non-Zeno Weighted Timed Games", "authors": ["Isa Vialard"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00014v1", "summary": "The Value Problem for weighted timed games (wtgs) consists in determining,\ngiven a two-player weighted timed game with a reachability objective and a\nrational threshold, whether or not the value of the game exceeds the threshold.\nWhen restrained to wtgs with non-negative weight, this problem is known to be\nundecidable for weighted timed games with three or more clocks, and decidable\nfor one-clock wtgs. The Value Problem for two-clock non-negative wtgs, which\nremained stubbornly open for a decade, was recently shown to be undecidable. In\nthis article, we show that the Value Problem is decidable when considering\ntwo-clock almost non-Zeno wtgs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00014v1", "cate": "cs.LO", "date": "2025-07-24", "updated": "2025-07-24"}
{"id": "2506.08332", "title": "ORFS-agent: Tool-Using Agents for Chip Design Optimization", "authors": ["Amur Ghose", "Andrew B. Kahng", "Sayak Kundu", "Zhiang Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08332v2", "summary": "Machine learning has been widely used to optimize complex engineering\nworkflows across numerous domains. In the context of integrated circuit design,\nmodern flows (e.g., going from a register-transfer level netlist to physical\nlayouts) involve extensive configuration via thousands of parameters, and small\nchanges to these parameters can have large downstream impacts on desired\noutcomes - namely design performance, power, and area. Recent advances in Large\nLanguage Models (LLMs) offer new opportunities for learning and reasoning\nwithin such high-dimensional optimization tasks. In this work, we introduce\nORFS-agent, an LLM-based iterative optimization agent that automates parameter\ntuning in an open-source hardware design flow. ORFS-agent adaptively explores\nparameter configurations, demonstrating clear improvements over standard\nBayesian optimization approaches in terms of resource efficiency and final\ndesign metrics. Our empirical evaluations on two different technology nodes and\na range of circuit benchmarks indicate that ORFS-agent can improve both routed\nwirelength and effective clock period by over 13%, all while using 40% fewer\noptimization iterations. Moreover, by following natural language objectives to\ntrade off certain metrics for others, ORFS-agent demonstrates a flexible and\ninterpretable framework for multi-objective optimization. Crucially, RFS-agent\nis modular and model-agnostic, and can be plugged in to any frontier LLM\nwithout any further fine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08332v2", "cate": "cs.AI", "date": "2025-06-10", "updated": "2025-08-01"}
{"id": "2507.16248", "title": "FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents", "authors": ["Rui Sun", "Zuo Bai", "Wentao Zhang", "Yuxiang Zhang", "Li Zhao", "Shan Sun", "Zhengwen Qiu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16248v2", "summary": "Recently, AI agents are rapidly evolving in intelligence and widely used in\nprofessional research applications, such as STEM, software development,\nfinance, etc. Among these AI agents, deep research agent is a key category as\nit can perform long-horizon tasks and solve problems of greater complexity.\nHowever, there are few evaluation frameworks and benchmarks that systematically\nand automatically investigate the capabilities of these research agents.\nFurthermore, financial research problems have distinct complexity and subtlety.\nTo fill in the gap, we propose FinResearchBench, which is a logic tree based\nAgent-as-a-Judge and targets specifically for the financial research agents. It\nprovides a comprehensive and automatic assessment of the research agents across\n7 key types of tasks in the financial research domain. The contributions of\nthis work are two-folded: (1) the first and innovative Agent-as-a-Judge system\nthat extracts the logic tree of the research outcome and uses it as the\nintermediate information to present a comprehensive, reliable and robust\nevaluation; (2) finance oriented that it covers 70 typical financial research\nquestions, spreading across 7 frequently encountered types of tasks in the\ndomain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16248v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-08-01"}
{"id": "2508.00649", "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights", "authors": ["Junhao Zheng", "Jiahao Sun", "Chenhao Lin", "Zhengyu Zhao", "Chen Ma", "Chong Zhang", "Cong Wang", "Qian Wang", "Chao Shen"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00649v1", "summary": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00649v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.06944", "title": "Radio Map-Enabled 3D Trajectory and Communication Optimization for Low-Altitude Air-Ground Cooperation", "authors": ["Menghao Hu", "Tong Zhang", "Shuai Wang", "Chiya Zhang", "Changyang She", "Gaojie Chen", "Miaowen Wen"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages; 6 figures; submit to IEEE for possible publication", "url": "http://arxiv.org/abs/2505.06944v2", "summary": "Low-altitude economy includes the application of unmanned aerial vehicles\n(UAVs) serving ground robots. This paper investigates the 3-dimensional (3D)\ntrajectory and communication optimization for low-altitude air-ground\ncooperation systems, where mobile unmanned ground vehicles (UGVs) upload data\nto UAVs. We propose a joint optimization algorithm to maximize the minimal\nsum-rate of UGVs while ensuring quality of service and navigation constraints.\nThe proposed algorithm integrates a successive convex approximation\n(SCA)-penalty method for UGV-UAV scheduling, an SCA-based approach for UGV\ntransmit power control, and a novel warm-start particle swarm optimization with\ncross mutation (WS-PSO-CM). The WS-PSO-CM leverages convex optimization results\nfrom a statistical channel model to initialize particle swarm, significantly\nimproving the performance, compared with celebrated PSO-CM. Simulation results\ndemonstrate that the proposed algorithm achieves a $45.8$\\% higher minimal\nsum-rate compared to the baseline PSO-CM under the same iterations. This gain\ncan be translated to reducing computational time by $46.7$\\% of PSO-CM.\nFurthermore, our simulation results reveal that UAVs dynamically adjust\ntrajectories to avoid interference by buildings, and maintain proximity to UGVs\nto mitigate path-loss.", "comment": "6 pages; 6 figures; submit to IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2505.06944v2", "cate": "cs.IT", "date": "2025-05-11", "updated": "2025-08-01"}
{"id": "2508.00015", "title": "Extended Abstract: Partial-encapsulate and Its Support for Floating-point Operations in ACL2", "authors": ["Matt Kaufmann", "J Strother Moore"], "categories": ["cs.LO", "cs.MS"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2508.00015v1", "summary": "We illustrate the power of partial-encapsulate, showing how it is used in the\nimplementation of floating-point operations in ACL2.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2508.00015v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2506.18348", "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team", "authors": ["Weilun Yu", "Shixiang Tang", "Yonggui Huang", "Nanqing Dong", "Li Fan", "Honggang Qi", "Wei Liu", "Xiaoli Diao", "Xi Chen", "Wanli Ouyang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18348v3", "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18348v3", "cate": "cs.AI", "date": "2025-06-23", "updated": "2025-08-01"}
{"id": "2507.16974", "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar", "Surangika Ranathunga"], "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 9 tables, Appendix A-L", "url": "http://arxiv.org/abs/2507.16974v2", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Publicly available general-purpose Large Language Models\n(LLMs) typically offer generic agriculture advisories, lacking precision in\nlocal and multilingual contexts. Our study addresses this limitation by\ngenerating multilingual (English, Hindi, Punjabi) synthetic datasets from\nagriculture-specific documents from India and fine-tuning LLMs for the task of\nquestion answering (QA). Evaluation on human-created datasets demonstrates\nsignificant improvements in factuality, relevance, and agricultural consensus\nfor the fine-tuned LLMs compared to the baseline counterparts.", "comment": "16 pages, 9 tables, Appendix A-L", "pdf_url": "http://arxiv.org/pdf/2507.16974v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-08-01"}
{"id": "2508.00698", "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to AAAI2026", "url": "http://arxiv.org/abs/2508.00698v1", "summary": "Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.", "comment": "Submitted to AAAI2026", "pdf_url": "http://arxiv.org/pdf/2508.00698v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2407.02811", "title": "SPLITZ: Certifiable Robustness via Split Lipschitz Randomized Smoothing", "authors": ["Meiyu Zhong", "Ravi Tandon"], "categories": ["cs.LG", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE Transactions on Information Forensics and Security, accepted", "url": "http://arxiv.org/abs/2407.02811v3", "summary": "Certifiable robustness gives the guarantee that small perturbations around an\ninput to a classifier will not change the prediction. There are two approaches\nto provide certifiable robustness to adversarial examples: a) explicitly\ntraining classifiers with small Lipschitz constants, and b) Randomized\nsmoothing, which adds random noise to the input to create a smooth classifier.\nWe propose SPLITZ, a practical and novel approach which leverages the\nsynergistic benefits of both the above ideas into a single framework. Our main\nidea is to split a classifier into two halves, constrain the Lipschitz constant\nof the first half, and smooth the second half via randomization. Motivation for\nSPLITZ comes from the observation that many standard deep networks exhibit\nheterogeneity in Lipschitz constants across layers. SPLITZ can exploit this\nheterogeneity while inheriting the scalability of randomized smoothing. We\npresent a principled approach to train SPLITZ and provide theoretical analysis\nto derive certified robustness guarantees during inference. We present a\ncomprehensive comparison of robustness-accuracy trade-offs and show that SPLITZ\nconsistently improves on existing state-of-the-art approaches in the MNIST,\nCIFAR-10 and ImageNet datasets. For instance, with $\\ell_2$ norm perturbation\nbudget of $\\epsilon=1$, SPLITZ achieves $43.2\\%$ top-1 test accuracy on\nCIFAR-10 dataset compared to state-of-art top-1 test accuracy $39.8\\%$.", "comment": "IEEE Transactions on Information Forensics and Security, accepted", "pdf_url": "http://arxiv.org/pdf/2407.02811v3", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-07-31"}
{"id": "2508.00021", "title": "Alignment Monitoring", "authors": ["Thomas A. Henzinger", "Konstantin Kueffner", "Vasu Singh", "I Sun"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00021v1", "summary": "Formal verification provides assurances that a probabilistic system satisfies\nits specification--conditioned on the system model being aligned with reality.\nWe propose alignment monitoring to watch that this assumption is justified. We\nconsider a probabilistic model well aligned if it accurately predicts the\nbehaviour of an uncertain system in advance. An alignment score measures this\nby quantifying the similarity between the model's predicted and the system's\n(unknown) actual distributions. An alignment monitor observes the system at\nruntime; at each point in time it uses the current state and the model to\npredict the next state. After the next state is observed, the monitor updates\nthe verdict, which is a high-probability interval estimate for the true\nalignment score. We utilize tools from sequential forecasting to construct our\nalignment monitors. Besides a monitor for measuring the expected alignment\nscore, we introduce a differential alignment monitor, designed for comparing\ntwo models, and a weighted alignment monitor, which permits task-specific\nalignment monitoring. We evaluate our monitors experimentally on the PRISM\nbenchmark suite. They are fast, memory-efficient, and detect misalignment\nearly.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00021v1", "cate": "cs.LO", "date": "2025-07-28", "updated": "2025-07-28"}
{"id": "2508.00032", "title": "Strategic Communication and Language Bias in Multi-Agent LLM Coordination", "authors": ["Alessio Buscemi", "Daniele Proverbio", "Alessandro Di Stefano", "The Anh Han", "German Castignani", "Pietro Liò"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00032v1", "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nPrevious studies indicate that the language used to frame strategic scenarios\ncan influence cooperative behavior. This paper explores whether allowing agents\nto communicate amplifies these language-driven effects. Leveraging the FAIRGAME\nframework, we simulate one-shot and repeated games across different languages\nand models, both with and without communication. Our experiments, conducted\nwith two advanced LLMs, GPT-4o and Llama 4 Maverick, reveal that communication\nsignificantly influences agent behavior, though its impact varies by language,\npersonality, and game structure. These findings underscore the dual role of\ncommunication in fostering coordination and reinforcing biases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00032v1", "cate": "cs.MA", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.09751", "title": "Sound and Complete Neurosymbolic Reasoning with LLM-Grounded Interpretations", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)", "url": "http://arxiv.org/abs/2507.09751v2", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneurosymbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "pdf_url": "http://arxiv.org/pdf/2507.09751v2", "cate": "cs.AI", "date": "2025-07-13", "updated": "2025-08-01"}
{"id": "2507.22462", "title": "IFEvalCode: Controlled Code Generation", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22462v2", "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22462v2", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00726", "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models", "authors": ["Jiale Li", "Mingrui Wu", "Zixiang Jin", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ACM MM25 has accepted this paper", "url": "http://arxiv.org/abs/2508.00726v1", "summary": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.", "comment": "ACM MM25 has accepted this paper", "pdf_url": "http://arxiv.org/pdf/2508.00726v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.03866", "title": "Weyl symmetry of the gradient-flow in information geometry", "authors": ["Tatsuaki Wada", "Sousuke Noda"], "categories": ["gr-qc", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "Subjects:       General Relativity and Quantum Cosmology (gr-qc)", "pdf_link": null, "comments": "Comments:      16 pages, no figure", "url": "http://arxiv.org/abs/2502.03866v2", "summary": "We have revisited the gradient-flow in information geometry from the\nperspective of Weyl symmetry. The gradient-flow equations are derived from the\nproposed action which is invariant under the Weyl's gauge transformations. In\nWeyl integrable geometry, we have related Amari's $\\alpha$-connections in IG to\nthe Weyl invariant connection on the Riemannian manifold equipped with the\nscaled metric.", "comment": "16 pages, no figure", "pdf_url": "http://arxiv.org/pdf/2502.03866v2", "cate": "gr-qc", "date": "2025-02-06", "updated": "2025-08-01"}
{"id": "2508.00419", "title": "Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "categories": ["cs.LO", "cs.LG", "cs.PL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00419v1", "summary": "Loop invariants are essential for proving the correctness of programs with\nloops. Developing loop invariants is challenging, and fully automatic synthesis\ncannot be guaranteed for arbitrary programs. Some approaches have been proposed\nto synthesize loop invariants using symbolic techniques and more recently using\nneural approaches. These approaches are able to correctly synthesize loop\ninvariants only for subsets of standard benchmarks. In this work, we\ninvestigate whether modern, reasoning-optimized large language models can do\nbetter. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled\ngenerate-and-check pipeline with the Z3 SMT solver, using solver\ncounterexamples to iteratively guide invariant refinement. We use Code2Inv\nbenchmark, which provides C programs along with their formal preconditions and\npostconditions. On this benchmark of 133 tasks, our framework achieves 100%\ncoverage (133 out of 133), outperforming the previous best of 107 out of 133,\nwhile requiring only 1-2 model proposals per instance and 14-55 seconds of\nwall-clock time. These results demonstrate that LLMs possess latent logical\nreasoning capabilities which can help automate loop invariant synthesis. While\nour experiments target C-specific programs, this approach should be\ngeneralizable to other imperative languages.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00419v1", "cate": "cs.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00280", "title": "WMAS: A Multi-Agent System Towards Intelligent and Customized Wireless Networks", "authors": ["Jingchen Peng", "Dingli Yuan", "Boxiang Ren", "Jie Fan", "Hao Wu", "Lu Yang"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00280v1", "summary": "The fast development of Artificial Intelligence (AI) agents provides a\npromising way for the realization of intelligent and customized wireless\nnetworks. In this paper, we propose a Wireless Multi-Agent System (WMAS), which\ncan provide intelligent and customized services for different user equipment\n(UEs). Note that orchestrating multiple agents carries the risk of malfunction,\nand multi-agent conversations may fall into infinite loops. It is thus crucial\nto design a conversation topology for WMAS that enables agents to complete UE\ntask requests with high accuracy and low conversation overhead. To address this\nissue, we model the multi-agent conversation topology as a directed acyclic\ngraph and propose a reinforcement learning-based algorithm to optimize the\nadjacency matrix of this graph. As such, WMAS is capable of generating and\nself-optimizing multi-agent conversation topologies, enabling agents to\neffectively and collaboratively handle a variety of task requests from UEs.\nSimulation results across various task types demonstrate that WMAS can achieve\nhigher task performance and lower conversation overhead compared to existing\nmulti-agent systems. These results validate the potential of WMAS to enhance\nthe intelligence of future wireless networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00280v1", "cate": "cs.MA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00040", "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "authors": ["Abhinav Das", "Stephan Schlüter"], "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00040v1", "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00040v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.10076", "title": "On Gradual Semantics for Assumption-Based Argumentation", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to KR2025 - With Appendix", "url": "http://arxiv.org/abs/2507.10076v3", "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "comment": "Accepted to KR2025 - With Appendix", "pdf_url": "http://arxiv.org/pdf/2507.10076v3", "cate": "cs.AI", "date": "2025-07-14", "updated": "2025-07-31"}
{"id": "2507.22545", "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyoungmin Roh"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.22545v2", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.22545v2", "cate": "cs.CL", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00728", "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2508.00728v1", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model\nthat tackles both general counting challenges and enables precise quantity\ncontrol for text-to-image (T2I) generation. A core contribution is the\n'cardinality' map, a novel regression target that accounts for variations in\nobject size and spatial distribution. Leveraging representation alignment and a\nhybrid strong-weak supervision scheme, YOLO-Count bridges the gap between\nopen-vocabulary counting and T2I generation control. Its fully differentiable\narchitecture facilitates gradient-based optimization, enabling accurate object\ncount estimation and fine-grained guidance for generative models. Extensive\nexperiments demonstrate that YOLO-Count achieves state-of-the-art counting\naccuracy while providing robust and effective quantity control for T2I systems.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00728v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.19275", "title": "Agentic Information Theory: Ergodicity and Intrinsic Semantics of Information Processes", "authors": ["James P. Crutchfield", "Alexandra Jurgens"], "categories": ["cond-mat.stat-mech", "cs.IT", "cs.MA", "math.IT", "nlin.AO"], "primary_category": "Subjects:       Statistical Mechanics (cond-mat.stat-mech)", "pdf_link": null, "comments": "Comments:      30 pages, 12 figures, 9 tables; this http URL", "url": "http://arxiv.org/abs/2505.19275v3", "summary": "We develop information theory for the temporal behavior of memoryful agents\nmoving through complex -- structured, stochastic -- environments. We introduce\nand explore information processes -- stochastic processes produced by cognitive\nagents in real-time as they interact with and interpret incoming stimuli. We\nprovide basic results on the ergodicity and semantics of the resulting time\nseries of Shannon information measures that monitor an agent's adapting view of\nuncertainty and structural correlation in its environment.", "comment": "30 pages, 12 figures, 9 tables;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/iprocesses.htm", "pdf_url": "http://arxiv.org/pdf/2505.19275v3", "cate": "cond-mat.stat-mech", "date": "2025-05-25", "updated": "2025-07-31"}
{"id": "2508.00613", "title": "Parameterized Infinite-State Reactive Synthesis", "authors": ["Benedikt Maderbacher", "Roderick Bloem"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00613v1", "summary": "We propose a method to synthesize a parameterized infinite-state systems that\ncan be instantiated for different parameter values. The specification is given\nin a parameterized temporal logic that allows for data variables as well as\nparameter variables that encode properties of the environment. Our synthesis\nmethod runs in a counterexample-guided loop consisting of four main steps:\nFirst, we use existing techniques to synthesize concrete systems for some small\nparameter instantiations. Second, we generalize the concrete systems into a\nparameterized program. Third, we create a proof candidate consisting of an\ninvariant and a ranking function. Fourth, we check the proof candidate for\nconsistency with the program. If the proof succeeds, the parameterized program\nis valid. Otherwise, we identify a parameter value for which the proof fails\nand add a new concrete instance to step one. To generalize programs and create\nproof candidates, we use a combination of anti-unification and syntax-guided\nsynthesis to express syntactic differences between programs as functions of the\nparameters. We evaluate our approach on examples from the literature that have\nbeen extended with parameters as well as new problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00613v1", "cate": "cs.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00043", "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "authors": ["Nhut Truong", "Uri Hasson"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00043v1", "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00043v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.18004", "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "authors": ["Yusen Peng", "Shuhua Mao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      44 pages,11 figures", "url": "http://arxiv.org/abs/2507.18004v2", "summary": "How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, the generated\noutputs were consistently rated highly, demonstrating strong creative quality\nand expressive clarity. Feedback highlights stylistic precision and emotional\nresonance. These results demonstrate that error-centered, feedback-driven\ngeneration enhances creativity, offering a scalable path toward self-evolving,\nhuman-aligned creative AI.", "comment": "44 pages,11 figures", "pdf_url": "http://arxiv.org/pdf/2507.18004v2", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-31"}
{"id": "2402.02364", "title": "Loss Landscape Degeneracy and Stagewise Development in Transformers", "authors": ["Jesse Hoogland", "George Wang", "Matthew Farrugia-Roberts", "Liam Carroll", "Susan Wei", "Daniel Murfet"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear, TMLR. Material on essential dynamics from v1 of this preprint has been removed and developed in arXiv:2501.17745", "url": "http://arxiv.org/abs/2402.02364v3", "summary": "Deep learning involves navigating a high-dimensional loss landscape over the\nneural network parameter space. Over the course of training, complex\ncomputational structures form and re-form inside the neural network, leading to\nshifts in input/output behavior. It is a priority for the science of deep\nlearning to uncover principles governing the development of neural network\nstructure and behavior. Drawing on the framework of singular learning theory,\nwe propose that model development is deeply linked to degeneracy in the local\ngeometry of the loss landscape. We investigate this link by monitoring loss\nlandscape degeneracy throughout training, as quantified by the local learning\ncoefficient, for a transformer language model and an in-context linear\nregression transformer. We show that training can be divided into distinct\nperiods of change in loss landscape degeneracy, and that these changes in\ndegeneracy coincide with significant changes in the internal computational\nstructure and the input/output behavior of the transformers. This finding\nprovides suggestive evidence that degeneracy and development are linked in\ntransformers, underscoring the potential of a degeneracy-based perspective for\nunderstanding modern deep learning.", "comment": "To appear, TMLR. Material on essential dynamics from v1 of this\n  preprint has been removed and developed in arXiv:2501.17745", "pdf_url": "http://arxiv.org/pdf/2402.02364v3", "cate": "cs.LG", "date": "2024-02-04", "updated": "2025-08-01"}
{"id": "2508.00744", "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "authors": ["Adwait Chandorkar", "Hasan Tercan", "Tobias Meisen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted at the Embedded Vision Workshop ICCV 2025", "url": "http://arxiv.org/abs/2508.00744v1", "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.", "comment": "accepted at the Embedded Vision Workshop ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2508.00744v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23179", "title": "Cyclotomy, cyclotomic cosets and arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$", "authors": ["Juncheng Zhou", "Hongfeng Wu"], "categories": ["math.NT", "cs.IT", "math.IT"], "primary_category": "Subjects:       Number Theory (math.NT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23179v2", "summary": "Arithmetic properties of some families in $\\frac{\\mathbb{F}_l[x]}{\\langle\nx^{p^sq^t}-1\\rangle}$ are obtained by using the cyclotomic classes of order 2\nwith respect to $n=p^sq^t$, where $p\\equiv3 \\mathrm{mod} 4$,\n$\\gcd(\\phi(p^s),\\phi(q^t))=2$, $l$ is a primitive root modulo $q^t$ and\n$\\mathrm{ord}_{p^s}(l)=\\phi(p^s)/2$. The form of these cyclotomic classes\nenables us to further generalize the results obtained in \\cite{ref1}. The\nexplicit expressions of primitive idempotents of minimal ideals in\n$\\frac{\\mathbb{F}_l[x]}{\\langle x^{p^sq^t}-1\\rangle}$ are also obtained.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23179v2", "cate": "math.NT", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00653", "title": "Putting Perspective into OWL [sic]: Complexity-Neutral Standpoint Reasoning for Ontology Languages via Monodic S5 over Counting Two-Variable First-Order Logic (Extended Version with Appendix)", "authors": ["Lucía Gómez Álvarez", "Sebastian Rudolph"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00653v1", "summary": "Standpoint extensions of knowledge representation formalisms have been\nrecently introduced as a means to incorporate multi-perspective modelling and\nreasoning through modal operators that attribute pieces of knowledge to\nspecific entities or agents. In these extensions, the integration between\nconceptual modelling and perspective annotations can vary in strength, with\nmonodic standpoint extensions offering a well-balanced approach. They allow for\nadvanced modelling features, such as the expression of rigid concepts, while\nmaintaining desirable reasoning complexity.\n  We consider the extension of C2--the counting two-variable fragment of\nfirst-order logic--by monodic standpoints. At the heart of our work is a\npolynomial-time translation of formulas in this extended formalism into\nstandard, standpoint-free C2, a result that relies on intricate model-theoretic\narguments. Thanks to this translation, the satisfiability problem remains at\nthe same complexity level: NExpTime-complete, as in plain C2. Since our\nformalism subsumes monodic S5 over C2, this result also marks a substantial\nadvancement in the study of first-order modal logics.\n  From a practical standpoint, this means that highly expressive description\nlogics such as SHOIQBs and SROIQBs--which underpin the widely adopted OWL 1 and\nOWL 2 ontology languages standardised by the W3C--can be extended with monodic\nstandpoints without increasing the standard reasoning complexity.\n  We further prove that NExpTime-hardness arises even in significantly less\nexpressive description logics, as long as they include both nominals and\nmonodic standpoints. Moreover, we show that if the monodicity restriction is\nrelaxed even slightly in the presence of inverse roles, functionality, and\nnominals, the satisfiability problem becomes undecidable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00653v1", "cate": "cs.LO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00127", "title": "Structured Transformations for Stable and Interpretable Neural Computation", "authors": ["Saleh Nikooroo", "Thomas Engel"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00127v1", "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00127v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.21046", "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence", "authors": ["Huan-ang Gao", "Jiayi Geng", "Wenyue Hua", "Mengkang Hu", "Xinzhe Juan", "Hongzhang Liu", "Shilong Liu", "Jiahao Qiu", "Xuan Qi", "Yiran Wu", "Hongru Wang", "Han Xiao", "Yuhang Zhou", "Shaokun Zhang", "Jiayi Zhang", "Jinyu Xiang", "Yixiong Fang", "Qiwen Zhao", "Dongrui Liu", "Qihan Ren", "Cheng Qian", "Zhenhailong Wang", "Minda Hu", "Huazheng Wang", "Qingyun Wu", "Heng Ji", "Mengdi Wang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      51 pages, 9 figures", "url": "http://arxiv.org/abs/2507.21046v3", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.", "comment": "51 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.21046v3", "cate": "cs.AI", "date": "2025-07-28", "updated": "2025-08-01"}
{"id": "2410.02713", "title": "LLaVA-Video: Video Instruction Tuning With Synthetic Data", "authors": ["Yuanhan Zhang", "Jinming Wu", "Wei Li", "Bo Li", "Zejun Ma", "Ziwei Liu", "Chunyuan Li"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL Accepted at TMLR", "url": "http://arxiv.org/abs/2410.02713v3", "summary": "The development of video large multimodal models (LMMs) has been hindered by\nthe difficulty of curating large amounts of high-quality raw data from the web.\nTo address this, we propose an alternative approach by creating a high-quality\nsynthetic dataset specifically for video instruction-following, namely\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\ndataset, in combination with existing visual instruction tuning data, we\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\nLLaVA-Video achieves strong performance across various video benchmarks,\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\nits generation pipeline, and the model checkpoints.", "comment": "Project page:\n  https://llava-vl.github.io/blog/2024-09-30-llava-video/; Accepted at TMLR", "pdf_url": "http://arxiv.org/pdf/2410.02713v3", "cate": "cs.CV", "date": "2024-10-03", "updated": "2025-08-01"}
{"id": "2508.00746", "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "authors": ["Regine Hartwig", "Dominik Muhle", "Riccardo Marin", "Daniel Cremers"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00746v1", "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00746v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00016", "title": "Extended Abstract: Mutable Objects with Several Implementations", "authors": ["Matt Kaufmann", "Yahya Sohail", "Warren A. Hunt Jr"], "categories": ["cs.PL", "cs.LO"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2508.00016v1", "summary": "This extended abstract outlines an ACL2 feature, attach-stobj, that first\nappeared in ACL2 Version 8.6 (October, 2024). This feature supports different\nexecutable operations for a given abstract stobj, without requiring\nrecertification of the book that introduces that stobj or theorems about it.\nThe paper provides background as well as a user-level overview and some\nimplementation notes.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2508.00016v1", "cate": "cs.PL", "date": "2025-07-25", "updated": "2025-07-25"}
{"id": "2508.00131", "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2410.02937", "url": "http://arxiv.org/abs/2508.00131v1", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "pdf_url": "http://arxiv.org/pdf/2508.00131v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00733", "title": "AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation", "authors": ["Le Wang", "Jun Wang", "Feng Deng", "Chen Zhang", "Di Zhang", "Kun Gai"], "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      12 pages, 2 figures", "url": "http://arxiv.org/abs/2508.00733v2", "summary": "We present AudioGen-Omni - a unified approach based on multimodal diffusion\ntransformers (MMDit), capable of generating high-fidelity audio, speech, and\nsongs coherently synchronized with the input video. AudioGen-Omni introduces a\nnovel joint training paradigm that seamlessly integrates large-scale\nvideo-text-audio corpora, enabling a model capable of generating semantically\nrich, acoustically diverse audio conditioned on multimodal inputs and adaptable\nto a wide range of audio generation tasks. AudioGen-Omni employs a unified\nlyrics-transcription encoder that encodes graphemes and phonemes from both sung\nand spoken inputs into dense frame-level representations. Dense frame-level\nrepresentations are fused using an AdaLN-based joint attention mechanism\nenhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein\nRoPE is selectively applied to temporally structured modalities to ensure\nprecise and robust cross-modal alignment. By unfreezing all modalities and\nmasking missing inputs, AudioGen-Omni mitigates the semantic constraints of\ntext-frozen paradigms, enabling effective cross-modal conditioning. This joint\ntraining approach enhances audio quality, semantic alignment, and lip-sync\naccuracy, while also achieving state-of-the-art results on\nText-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8\nseconds of audio, it offers substantial improvements in both efficiency and\ngenerality.", "comment": "12 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2508.00733v2", "cate": "cs.SD", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2507.23276", "title": "How Far Are AI Scientists from Changing the World?", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23276v2", "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23276v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2502.19651", "title": "Unlocking Multi-Modal Potentials for Link Prediction on Dynamic Text-Attributed Graphs", "authors": ["Yuanyuan Xu", "Wenjie Zhang", "Ying Zhang", "Xuemin Lin", "Xiwei Xu"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.19651v2", "summary": "Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that\ncaptures evolving temporal events (edges) alongside rich textual attributes.\nExisting studies can be broadly categorized into TGNN-driven and LLM-driven\napproaches, both of which encode textual attributes and temporal structures for\nDyTAG representation. We observe that DyTAGs inherently comprise three distinct\nmodalities: temporal, textual, and structural, often exhibiting completely\ndisjoint distributions. However, the first two modalities are largely\noverlooked by existing studies, leading to suboptimal performance. To address\nthis, we propose MoMent, a multi-modal model that explicitly models,\nintegrates, and aligns each modality to learn node representations for link\nprediction. Given the disjoint nature of the original modality distributions,\nwe first construct modality-specific features and encode them using individual\nencoders to capture correlations across temporal patterns, semantic context,\nand local structures. Each encoder generates modality-specific tokens, which\nare then fused into comprehensive node representations with a theoretical\nguarantee. To avoid disjoint subspaces of these heterogeneous modalities, we\npropose a dual-domain alignment loss that first aligns their distributions\nglobally and then fine-tunes coherence at the instance level. This enhances\ncoherent representations from temporal, textual, and structural views.\nExtensive experiments across seven datasets show that MoMent achieves up to\n17.28% accuracy improvement and up to 31x speed-up against eight baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.19651v2", "cate": "cs.LG", "date": "2025-02-27", "updated": "2025-08-01"}
{"id": "2508.00750", "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "authors": ["Prerana Ramkumar"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00750v1", "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00750v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00633", "title": "Dynamics and Coherence for the Free Cornering with Protocol Choice", "authors": ["Chad Nester", "Niels Voorneveld"], "categories": ["math.CT", "cs.LO"], "primary_category": "Subjects:       Category Theory (math.CT)", "pdf_link": null, "comments": "Comments:      24 pages, in peer review", "url": "http://arxiv.org/abs/2508.00633v1", "summary": "We present a term rewriting system that models the dynamic aspects of the\nfree cornering with protocol choice of a monoidal category, which has been\nproposed as a categorical model of process interaction. This term rewriting\nsystem is confluent and terminating in an appropriate sense. We use this\nmachinery to prove a coherence theorem for the free cornering with protocol\nchoice.", "comment": "24 pages, in peer review", "pdf_url": "http://arxiv.org/pdf/2508.00633v1", "cate": "math.CT", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00172", "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "categories": ["cs.LG", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in 2025 IEEE Global Communications Conference (Globecom)", "url": "http://arxiv.org/abs/2508.00172v1", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "pdf_url": "http://arxiv.org/pdf/2508.00172v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2502.05695", "title": "Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks", "authors": ["Zijiang Yan", "Jianhua Pei", "Hongda Wu", "Hina Tabassum", "Ping Wang"], "categories": ["cs.MM", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      Accepted in IEEE Wireless Communications", "url": "http://arxiv.org/abs/2502.05695v3", "summary": "This paper proposes a novel Semantic Communication (SemCom) framework for\nreal-time adaptive-bitrate video streaming by integrating Latent Diffusion\nModels (LDMs) within the FFmpeg techniques. This solution addresses the\nchallenges of high bandwidth usage, storage inefficiencies, and quality of\nexperience (QoE) degradation associated with traditional Constant Bitrate\nStreaming (CBS) and Adaptive Bitrate Streaming (ABS). The proposed approach\nleverages LDMs to compress I-frames into a latent space, offering significant\nstorage and semantic transmission savings without sacrificing high visual\nquality. While retaining B-frames and P-frames as adjustment metadata to\nsupport efficient refinement of video reconstruction at the user side, the\nproposed framework further incorporates state-of-the-art denoising and Video\nFrame Interpolation (VFI) techniques. These techniques mitigate semantic\nambiguity and restore temporal coherence between frames, even in noisy wireless\ncommunication environments. Experimental results demonstrate the proposed\nmethod achieves high-quality video streaming with optimized bandwidth usage,\noutperforming state-of-the-art solutions in terms of QoE and resource\nefficiency. This work opens new possibilities for scalable real-time video\nstreaming in 5G and future post-5G networks.", "comment": "Accepted in IEEE Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2502.05695v3", "cate": "cs.MM", "date": "2025-02-08", "updated": "2025-08-01"}
{"id": "2507.23565", "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23565v2", "summary": "In collaborative systems, the effective completion of tasks hinges on\ntask-specific trust evaluations of potential devices for distributed\ncollaboration. However, the complexity of tasks, the spatiotemporal dynamism of\ndistributed device resources, and the inevitable assessment overhead\ndramatically increase the complexity and resource consumption of the trust\nevaluation process. As a result, ill-timed or overly frequent trust evaluations\ncan reduce utilization rate of constrained resources, negatively affecting\ncollaborative task execution. To address this challenge, this paper proposes an\nautonomous trust orchestration method based on a new concept of semantic\nchain-of-trust. Our technique employs agentic AI and hypergraph to establish\nand maintain trust relationships among devices. By leveraging its strengths in\nautonomous perception, task decomposition, and semantic reasoning, we propose\nagentic AI to perceive device states and autonomously perform trust evaluations\nof collaborators based on historical performance data only during device idle\nperiods, thereby enabling efficient utilization of distributed resources. In\naddition, agentic AI performs task-specific trust evaluations on collaborator\nresources by analyzing the alignment between resource capabilities and task\nrequirements. Moreover, by maintaining a trust hypergraph embedded with trust\nsemantics for each device, agentic AI enables hierarchical management of\ncollaborators and identifies collaborators requiring trust evaluation based on\ntrust semantics, thereby achieving a balance between overhead and trust\naccuracy. Furthermore, local trust hypergraphs from multiple devices can be\nchained together to support multi-hop collaboration, enabling efficient\ncoordination in large-scale systems. Experimental results demonstrate that the\nproposed method achieves resource-efficient trust evaluation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23565v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.02659", "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding", "authors": ["Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Shaojie Zhuo", "Chen Feng", "Yicheng Lin", "Chenzheng Su", "Xiaopeng Zhang"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02659v2", "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02659v2", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-31"}
{"id": "2508.00777", "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "authors": ["Zihan Wang", "Samira Ebrahimi Kahou", "Narges Armanfard"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at BMVC 2025", "url": "http://arxiv.org/abs/2508.00777v1", "summary": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.", "comment": "Accepted at BMVC 2025", "pdf_url": "http://arxiv.org/pdf/2508.00777v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2402.12169", "title": "Automating Boundary Filling in Cubical Type Theories", "authors": ["Maximilian Doré", "Evan Cavallo", "Anders Mörtberg"], "categories": ["cs.LO"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.12169v2", "summary": "When working in a proof assistant, automation is key to discharging routine\nproof goals such as equations between algebraic expressions. Homotopy type\ntheory allows the user to reason about higher structures, such as topological\nspaces, using higher inductive types (HITs) and univalence. Cubical type theory\nprovides computational support for HITs and univalence. A difficulty when\nworking in cubical type theory is dealing with the complex combinatorics of\nhigher structures, an infinite-dimensional generalisation of equational\nreasoning. To solve these higher-dimensional equations consists in constructing\ncubes with specified boundaries.\n  We develop a simplified cubical language in which we isolate and study two\nautomation problems: contortion solving, where we attempt to \"contort\" a cube\nto fit a given boundary, and the more general Kan solving, where we search for\nsolutions that involve pasting multiple cubes together. Both problems are\ndifficult in the general case-Kan solving is even undecidable-so we focus on\nheuristics that perform well on practical examples. Our language encompasses\ndifferent variations of cubical type theory which differ in their \"contortion\ntheory\", i.e., the class of contortions they support. We provide a solver for\nthe contortion problem for the most complex contortion theories currently being\nresearched, the Dedekind and De Morgan contortions, by utilizing a\nreformulation of contortions in terms of poset maps. We solve Kan problems\nusing constraint satisfaction programming, which is applicable independently of\nthe underlying contortion theory. We have implemented our algorithms in an\nexperimental Haskell solver that can be used to automatically solve many goals\na user of cubical type theory might face. We illustrate this with a case study\nestablishing the Eckmann-Hilton theorem using our solver, as well as various\nbenchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.12169v2", "cate": "cs.LO", "date": "2024-02-19", "updated": "2025-08-01"}
{"id": "2508.00174", "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "authors": ["Yongchao Huang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2508.00174v1", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2508.00174v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.01064", "title": "FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait", "authors": ["Taekyung Ki", "Dongchan Min", "Gyeongsu Chae"], "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2412.01064v4", "summary": "With the rapid advancement of diffusion-based generative models, portrait\nimage animation has achieved remarkable results. However, it still faces\nchallenges in temporally consistent video generation and fast sampling due to\nits iterative sampling nature. This paper presents FLOAT, an audio-driven\ntalking portrait video generation method based on flow matching generative\nmodel. Instead of a pixel-based latent space, we take advantage of a learned\northogonal motion latent space, enabling efficient generation and editing of\ntemporally consistent motion. To achieve this, we introduce a transformer-based\nvector field predictor with an effective frame-wise conditioning mechanism.\nAdditionally, our method supports speech-driven emotion enhancement, enabling a\nnatural incorporation of expressive motions. Extensive experiments demonstrate\nthat our method outperforms state-of-the-art audio-driven talking portrait\nmethods in terms of visual quality, motion fidelity, and efficiency.", "comment": "ICCV 2025. Project page:\n  https://deepbrainai-research.github.io/float/", "pdf_url": "http://arxiv.org/pdf/2412.01064v4", "cate": "cs.CV", "date": "2024-12-02", "updated": "2025-08-01"}
{"id": "2507.23726", "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "categories": ["cs.AI", "cs.CL"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23726v2", "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23726v2", "cate": "cs.AI", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.04562", "title": "Evaluating LLMs on Real-World Forecasting Against Human Superforecasters", "authors": ["Janna Lu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.04562v2", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their ability to forecast future events remains\nunderstudied. A year ago, large language models struggle to come close to the\naccuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting\nquestions from Metaculus, comparing their performance against human\nsuperforecasters. Frontier models achieve Brier scores that ostensibly surpass\nthe human crowd but still significantly underperform a group of\nsuperforecasters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.04562v2", "cate": "cs.LG", "date": "2025-07-06", "updated": "2025-08-01"}
{"id": "2508.00822", "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning", "authors": ["Alexander Nikitas Dimopoulos", "Joseph Grasso"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00822v1", "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00822v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2410.19940", "title": "Cobblestone: Iterative Automation for Formal Verification", "authors": ["Saketh Ram Kasibatla", "Arpan Agarwal", "Yuriy Brun", "Sorin Lerner", "Talia Ringer", "Emily First"], "categories": ["cs.LO", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2410.19940v2", "summary": "Formal verification using proof assistants, such as Coq, is an effective way\nof improving software quality, but requires significant effort and expertise.\nMachine learning can automatically synthesize proofs, but such tools are able\nto prove only a fraction of desired software properties. We introduce\nCobblestone, a divide-and-conquer approach for proof synthesis. Cobblestone\nuses a large language model (LLM) to generate potential proofs, uses those\nproofs to break the problem into simpler parts, automatically identifies which\nof those parts were successfully proven, and iterates on the remaining parts to\nbuild a correct proof that is guaranteed to be sound, despite the reliance on\nunsound LLMs. We evaluate Cobblestone on four benchmarks of open-source Coq\nprojects, controlling for training data leakage. Fully automatically,\nCobblestone outperforms state-of-the-art non-LLM tools, and proves many\ntheorems that other LLM-based tools cannot, and on many benchmarks, outperforms\nthem. Each Cobblestone run costs only $1.25 and takes 14.7 minutes, on average.\nCobblestone can also be used with external input, from a user or another tool,\nproviding a proof structure or relevant lemmas. Evaluated with such an oracle,\nCobblestone proves up to 58% of theorems. Overall, our research shows that\ntools can make use of partial progress and external input to more effectively\nautomate formal verification.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2410.19940v2", "cate": "cs.LO", "date": "2024-10-25", "updated": "2025-07-31"}
{"id": "2508.00201", "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00201v1", "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00201v1", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.15066", "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback", "authors": ["Yiyuan Yang", "Zichuan Liu", "Lei Song", "Kai Ying", "Zhiguang Wang", "Tom Bamford", "Svitlana Vyetrenko", "Jiang Bian", "Qingsong Wen"], "categories": ["cs.LG", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Under review. 19 pages, 8 figures, 12 tables. Code and dataset are publicly available", "url": "http://arxiv.org/abs/2507.15066v2", "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.", "comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available", "pdf_url": "http://arxiv.org/pdf/2507.15066v2", "cate": "cs.LG", "date": "2025-07-20", "updated": "2025-08-01"}
{"id": "2305.04095", "title": "Gradient Leakage Defense with Key-Lock Module for Federated Learning", "authors": ["Hanchi Ren", "Jingjing Deng", "Xianghua Xie"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The source code can be found at this https URL", "url": "http://arxiv.org/abs/2305.04095v3", "summary": "Federated Learning (FL) is a widely adopted privacy-preserving machine\nlearning approach where private data remains local, enabling secure\ncomputations and the exchange of local model gradients between local clients\nand third-party parameter servers. However, recent findings reveal that privacy\nmay be compromised and sensitive information potentially recovered from shared\ngradients. In this study, we offer detailed analysis and a novel perspective on\nunderstanding the gradient leakage problem. These theoretical works lead to a\nnew gradient leakage defense technique that secures arbitrary model\narchitectures using a private key-lock module. Only the locked gradient is\ntransmitted to the parameter server for global model aggregation. Our proposed\nlearning method is resistant to gradient leakage attacks, and the key-lock\nmodule is designed and trained to ensure that, without the private information\nof the key-lock module: a) reconstructing private training data from the shared\ngradient is infeasible; and b) the global model's inference performance is\nsignificantly compromised. We discuss the theoretical underpinnings of why\ngradients can leak private information and provide theoretical proof of our\nmethod's effectiveness. We conducted extensive empirical evaluations with many\nmodels on several popular benchmarks, demonstrating the robustness of our\nproposed approach in both maintaining model performance and defending against\ngradient leakage attacks.", "comment": "The source code can be found at https://github.com/Rand2AI/FedKL", "pdf_url": "http://arxiv.org/pdf/2305.04095v3", "cate": "cs.LG", "date": "2023-05-06", "updated": "2025-08-01"}
{"id": "2507.22062", "title": "Meta CLIP 2: A Worldwide Scaling Recipe", "authors": ["Yung-Sung Chuang", "Yang Li", "Dong Wang", "Ching-Feng Yeh", "Kehan Lyu", "Ramya Raghavendra", "James Glass", "Lifei Huang", "Jason Weston", "Luke Zettlemoyer", "Xinlei Chen", "Zhuang Liu", "Saining Xie", "Wen-tau Yih", "Shang-Wen Li", "Hu Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.22062v3", "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, Meta CLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.22062v3", "cate": "cs.CV", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2508.00823", "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2508.00823v1", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "pdf_url": "http://arxiv.org/pdf/2508.00823v1", "cate": "cs.CV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00270", "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00270v1", "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00270v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.19209", "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet", "authors": ["Xiaoyu Zhang", "Zhifeng Bao", "Hai Dong", "Ziwei Wang", "Jiajun Liu"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19209v2", "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19209v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-08-01"}
{"id": "2305.15611", "title": "Tackling Size Generalization of Graph Neural Networks on Biological Data from a Spectral Perspective", "authors": ["Gaotang Li", "Danai Koutra", "Yujun Yan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 2025", "url": "http://arxiv.org/abs/2305.15611v5", "summary": "We address the key challenge of size-induced distribution shifts in graph\nneural networks (GNNs) and their impact on the generalization of GNNs to larger\ngraphs. Existing literature operates under diverse assumptions about\ndistribution shifts, resulting in varying conclusions about the\ngeneralizability of GNNs. In contrast to prior work, we adopt a data-driven\napproach to identify and characterize the types of size-induced distribution\nshifts and explore their impact on GNN performance from a spectral standpoint,\na perspective that has been largely underexplored. Leveraging the significant\nvariance in graph sizes in real biological datasets, we analyze biological\ngraphs and find that spectral differences, driven by subgraph patterns (e.g.,\naverage cycle length), strongly correlate with GNN performance on larger,\nunseen graphs. Based on these insights, we propose three model-agnostic\nstrategies to enhance GNNs' awareness of critical subgraph patterns,\nidentifying size-intensive attention as the most effective approach. Extensive\nexperiments with six GNN architectures and seven model-agnostic strategies\nacross five datasets show that our size-intensive attention strategy\nsignificantly improves graph classification on test graphs 2 to 10 times larger\nthan the training graphs, boosting F1 scores by up to 8% over strong baselines.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2305.15611v5", "cate": "cs.LG", "date": "2023-05-24", "updated": "2025-08-01"}
{"id": "2507.22746", "title": "Next Tokens Denoising for Speech Synthesis", "authors": ["Yanqing Liu", "Ruiqing Xue", "Chong Zhang", "Yufei Liu", "Gang Wang", "Bohan Li", "Yao Qian", "Lei He", "Shujie Liu", "Sheng Zhao"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22746v2", "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22746v2", "cate": "cs.SD", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2507.22953", "title": "CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography", "authors": ["Murong Xu", "Tamaz Amiranashvili", "Fernando Navarro", "Maksym Fritsak", "Ibrahim Ethem Hamamci", "Suprosanna Shit", "Bastian Wittmann", "Sezgin Er", "Sebastian M. Christ", "Ezequiel de la Rosa", "Julian Deseoe", "Robert Graf", "Hendrik Möller", "Anjany Sekuboyina", "Jan C. Peeken", "Sven Becker", "Giulia Baldini", "Johannes Haubold", "Felix Nensa", "René Hosch", "Nikhil Mirajkar", "Saad Khalid", "Stefan Zachow", "Marc-André Weber", "Georg Langs", "Jakob Wasserthal", "Mehmet Kemal Ozdemir", "Andrey Fedorov", "Ron Kikinis", "Stephanie Tanadini-Lang", "Jan S. Kirschke", "Stephanie E. Combs", "Bjoern Menze"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22953v1", "summary": "Accurate delineation of anatomical structures in volumetric CT scans is\ncrucial for diagnosis and treatment planning. While AI has advanced automated\nsegmentation, current approaches typically target individual structures,\ncreating a fragmented landscape of incompatible models with varying performance\nand disparate evaluation protocols. Foundational segmentation models address\nthese limitations by providing a holistic anatomical view through a single\nmodel. Yet, robust clinical deployment demands comprehensive training data,\nwhich is lacking in existing whole-body approaches, both in terms of data\nheterogeneity and, more importantly, anatomical coverage. In this work, rather\nthan pursuing incremental optimizations in model architecture, we present CADS,\nan open-source framework that prioritizes the systematic integration,\nstandardization, and labeling of heterogeneous data sources for whole-body CT\nsegmentation. At its core is a large-scale dataset of 22,022 CT volumes with\ncomplete annotations for 167 anatomical structures, representing a significant\nadvancement in both scale and coverage, with 18 times more scans than existing\ncollections and 60% more distinct anatomical targets. Building on this diverse\ndataset, we develop the CADS-model using established architectures for\naccessible and automated full-body CT segmentation. Through comprehensive\nevaluation across 18 public datasets and an independent real-world hospital\ncohort, we demonstrate advantages over SoTA approaches. Notably, thorough\ntesting of the model's performance in segmentation tasks from radiation\noncology validates its direct utility for clinical interventions. By making our\nlarge-scale dataset, our segmentation models, and our clinical software tool\npublicly available, we aim to advance robust AI solutions in radiology and make\ncomprehensive anatomical analysis accessible to clinicians and researchers\nalike.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22953v1", "cate": "eess.IV", "date": "2025-07-29", "updated": "2025-07-29"}
{"id": "2508.00286", "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "authors": ["Mohsen Zaker Esteghamati"], "categories": ["cs.LG", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00286v1", "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00286v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.10028", "title": "AttnMod: Attention-Based New Art Styles", "authors": ["Shih-Chieh Su"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.10028v2", "summary": "We introduce AttnMod, a training-free technique that modulates\ncross-attention in pre-trained diffusion models to generate novel, unpromptable\nart styles. The method is inspired by how a human artist might reinterpret a\ngenerated image, for example by emphasizing certain features, dispersing color,\ntwisting silhouettes, or materializing unseen elements. AttnMod simulates this\nintent by altering how the text prompt conditions the image through attention\nduring denoising. These targeted modulations enable diverse stylistic\ntransformations without changing the prompt or retraining the model, and they\nexpand the expressive capacity of text-to-image generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.10028v2", "cate": "cs.CV", "date": "2024-09-16", "updated": "2025-08-01"}
{"id": "2508.00288", "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to ACM MM Dataset Track 2025", "url": "http://arxiv.org/abs/2508.00288v1", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "comment": "Accepted to ACM MM Dataset Track 2025", "pdf_url": "http://arxiv.org/pdf/2508.00288v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00304", "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00304v1", "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00304v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2410.06372", "title": "Cooperative and Asynchronous Transformer-based Mission Planning for Heterogeneous Teams of Mobile Robots", "authors": ["Milad Farjadnasab", "Shahin Sirouspour"], "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.11"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.06372v3", "summary": "Cooperative mission planning for heterogeneous teams of mobile robots\npresents a unique set of challenges, particularly when operating under\ncommunication constraints and limited computational resources. To address these\nchallenges, we propose the Cooperative and Asynchronous Transformer-based\nMission Planning (CATMiP) framework, which leverages multi-agent reinforcement\nlearning (MARL) to coordinate distributed decision making among agents with\ndiverse sensing, motion, and actuation capabilities, operating under sporadic\nad hoc communication. A Class-based Macro-Action Decentralized Partially\nObservable Markov Decision Process (CMacDec-POMDP) is also formulated to\neffectively model asynchronous decision-making for heterogeneous teams of\nagents. The framework utilizes an asynchronous centralized training and\ndistributed execution scheme, enabled by the proposed Asynchronous Multi-Agent\nTransformer (AMAT) architecture. This design allows a single trained model to\ngeneralize to larger environments and accommodate varying team sizes and\ncompositions. We evaluate CATMiP in a 2D grid-world simulation environment and\ncompare its performance against planning-based exploration methods. Results\ndemonstrate CATMiP's superior efficiency, scalability, and robustness to\ncommunication dropouts and input noise, highlighting its potential for\nreal-world heterogeneous mobile robot systems. The code is available at\nhttps://github.com/mylad13/CATMiP", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.06372v3", "cate": "cs.RO", "date": "2024-10-08", "updated": "2025-07-31"}
{"id": "2508.00354", "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00354v1", "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00354v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00325", "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00325v1", "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00325v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.13587", "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics", "authors": ["Taowen Wang", "Cheng Han", "James Chenhao Liang", "Wenhao Yang", "Dongfang Liu", "Luna Xinyu Zhang", "Qifan Wang", "Jiebo Luo", "Ruixiang Tang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      ICCV camera ready; Github: this https URL Homepage: this https URL", "url": "http://arxiv.org/abs/2411.13587v4", "summary": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a\ntransformative approach, enabling robots to execute complex tasks by\nintegrating visual and linguistic inputs within an end-to-end learning\nframework. Despite their significant capabilities, VLA models introduce new\nattack surfaces. This paper systematically evaluates their robustness.\nRecognizing the unique demands of robotic execution, our attack objectives\ntarget the inherent spatial and functional characteristics of robotic systems.\nIn particular, we introduce two untargeted attack objectives that leverage\nspatial foundations to destabilize robotic actions, and a targeted attack\nobjective that manipulates the robotic trajectory. Additionally, we design an\nadversarial patch generation approach that places a small, colorful patch\nwithin the camera's view, effectively executing the attack in both digital and\nphysical environments. Our evaluation reveals a marked degradation in task\nsuccess rates, with up to a 100\\% reduction across a suite of simulated robotic\ntasks, highlighting critical security gaps in current VLA architectures. By\nunveiling these vulnerabilities and proposing actionable evaluation metrics, we\nadvance both the understanding and enhancement of safety for VLA-based robotic\nsystems, underscoring the necessity for continuously developing robust defense\nstrategies prior to physical-world deployments.", "comment": "ICCV camera ready; Github:\n  https://github.com/William-wAng618/roboticAttack Homepage:\n  https://vlaattacker.github.io/", "pdf_url": "http://arxiv.org/pdf/2411.13587v4", "cate": "cs.RO", "date": "2024-11-18", "updated": "2025-08-01"}
{"id": "2508.00387", "title": "STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers", "authors": ["Zeqi Zheng", "Zizheng Zhu", "Yingchao Yu", "Yanchen Huang", "Changze Lv", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "categories": ["cs.NE", "cs.CV"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00387v1", "summary": "Transformer-based Spiking Neural Networks (SNNs) suffer from a great\nperformance gap compared to floating-point Artificial Neural Networks (ANNs)\ndue to the binary nature of spike trains. Recent efforts have introduced\ndeep-level feedback loops to transmit high-level semantic information to narrow\nthis gap. However, these designs often span multiple deep layers, resulting in\ncostly feature transformations, higher parameter overhead, increased energy\nconsumption, and longer inference latency. To address this issue, we propose\nShallow-level Temporal Feedback (STF), a lightweight plug-and-play module for\nthe encoding layer, which consists of Temporal-Spatial Position Embedding\n(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF\nconsistently improves performance across various Transformer-based SNN\nbackbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,\nunder different spike timestep settings. Further analysis reveals that STF\nenhances the diversity of the spike patterns, which is key to performance gain.\nMoreover, evaluations on adversarial robustness and temporal sensitivity\nconfirm that STF outperforms direct coding and its variants, highlighting its\npotential as a new spike encoding scheme for static scenarios. Our code will be\nreleased upon acceptance.", "comment": "32 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00387v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00331", "title": "Embryology of a Language Model", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00331v1", "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00331v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00096", "title": "Zeroing Diagonals, Conjugate Hollowization, and Characterizing Nondefinite Operators", "authors": ["David R. Nicholus"], "categories": ["math.NA", "cs.NA", "math.RA", "65F25, 15A21, 15B10, 15A23, 15B99, 15A86"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      24 pages, 4 figures", "url": "http://arxiv.org/abs/2508.00096v1", "summary": "We prove the conjecture by Damm and Fassbender that, for any pair $L,M$ of\nreal traceless matrices, there exists an orthogonal $V$ such that $V^{-1} L \\,\nV$ is hollow and $V M V^{-1}$ is almost hollow, where a matrix is hollow if and\nonly if its main diagonal consists only of 0s, and a traceless matrix is almost\nhollow if and only if all its main diagonal elements are 0 except, at most, the\nlast two.\n  The claim is a corollary to our considerably more general theorem, as well as\nanother corollary, revealing conditions on $L,M$ under which 0s can be\nintroduced by $V$ to all but the first or first two diagonal elements of\n$V^{-1} L \\, V$ and to all but the last two diagonal elements of $V M V^{-1}$.\n  By setting $L = M$, much is revealed concerning freedom and constraint\ninvolved in introducing 0s to the diagonal of a single operator. From this we\nprove novel characterizations of real traceless matrices, and a stronger\nversion of the seminal theorem by Fillmore that every real matrix is\northogonally similar to a matrix with a constant main diagonal.\n  Our results are contextualized in a characterization and classification of\nnondefinite matrices by, roughly, how many zeros can be introduced to their\ndiagonals, and it what ways.", "comment": "24 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2508.00096v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.15173", "title": "Un-mixing Test-time Adaptation under Heterogeneous Data Streams", "authors": ["Zixian Su", "Jingwei Guo", "Xi Yang", "Qiufeng Wang", "Kaizhu Huang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.15173v2", "summary": "Deploying deep models in real-world scenarios remains challenging due to\nsignificant performance drops under distribution shifts between training and\ndeployment environments. Test-Time Adaptation (TTA) has recently emerged as a\npromising solution, enabling on-the-fly model adaptation without access to\nsource data. However, its effectiveness degrades significantly in the presence\nof complex, mixed distribution shifts - common in practical settings - where\nmultiple latent domains coexist. Adapting under such intrinsic heterogeneity,\nespecially in unlabeled and online conditions, remains an open and\nunderexplored challenge. In this paper, we study TTA under mixed distribution\nshifts and move beyond conventional homogeneous adaptation paradigms. By\nrevisiting TTA from a frequency-domain perspective, we observe that\ndistribution heterogeneity often manifests in Fourier space - for instance,\nhigh-frequency components tend to carry domain-specific variations. This\nmotivates us to perform domain-aware separation using high-frequency texture\ncues, making diverse shift patterns more tractable. To this end, we propose\nFreDA, a novel Frequency-based Decentralized Adaptation framework that\ndecomposes globally heterogeneous data into locally homogeneous components in\nthe frequency domain. It further employs decentralized learning and\naugmentation strategies to robustly adapt under complex, evolving shifts.\nExtensive experiments across various environments (corrupted, natural, and\nmedical) demonstrate the superiority of our proposed framework over the\nstate-of-the-arts.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.15173v2", "cate": "cs.LG", "date": "2024-11-16", "updated": "2025-08-01"}
{"id": "2508.00438", "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025. Dataset available at this https URL", "url": "http://arxiv.org/abs/2508.00438v1", "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to\nincreased mortality, and medical treatments for this condition require\nmeticulous, labor-intensive analysis. Coronary angiography provides critical\nvisual cues for assessing stenosis, supporting clinicians in making informed\ndecisions for diagnosis and treatment. Recent advances in deep learning have\nshown great potential for automated localization and severity measurement of\nstenosis. In real-world scenarios, however, the success of these competent\napproaches is often hindered by challenges such as limited labeled data and\nclass imbalance. In this study, we propose a novel data augmentation approach\nthat uses an inpainting method based on a diffusion model to generate realistic\nlesions, allowing user-guided control of severity. Extensive evaluation on\nlesion detection and severity classification across various synthetic dataset\nsizes shows superior performance of our method on both a large-scale in-house\ndataset and a public coronary angiography dataset. Furthermore, our approach\nmaintains high detection and classification performance even when trained with\nlimited data, highlighting its clinical importance in improving the assessment\nof severity of stenosis and optimizing data utilization for more reliable\ndecision support.", "comment": "Accepted at MICCAI 2025. Dataset available at\n  https://github.com/medipixel/DiGDA", "pdf_url": "http://arxiv.org/pdf/2508.00438v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00350", "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 8 figures, To be published in the Proceedings of the International Conference on Machine Learning (ICML) 2025", "url": "http://arxiv.org/abs/2508.00350v1", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "pdf_url": "http://arxiv.org/pdf/2508.00350v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00101", "title": "Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method", "authors": ["Alena Kopaničáková", "Youngkyu Lee", "George Em Karniadakis"], "categories": ["math.NA", "cs.LG", "cs.NA", "math.OC", "65M55, 68T05, 49K20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      31 pages", "url": "http://arxiv.org/abs/2508.00101v1", "summary": "We propose a new deflation strategy to accelerate the convergence of the\npreconditioned conjugate gradient(PCG) method for solving parametric\nlarge-scale linear systems of equations. Unlike traditional deflation\ntechniques that rely on eigenvector approximations or recycled Krylov\nsubspaces, we generate the deflation subspaces using operator learning,\nspecifically the Deep Operator Network~(DeepONet). To this aim, we introduce\ntwo complementary approaches for assembling the deflation operators. The first\napproach approximates near-null space vectors of the discrete PDE operator\nusing the basis functions learned by the DeepONet. The second approach directly\nleverages solutions predicted by the DeepONet. To further enhance convergence,\nwe also propose several strategies for prescribing the sparsity pattern of the\ndeflation operator. A comprehensive set of numerical experiments encompassing\nsteady-state, time-dependent, scalar, and vector-valued problems posed on both\nstructured and unstructured geometries is presented and demonstrates the\neffectiveness of the proposed DeepONet-based deflated PCG method, as well as\nits generalization across a wide range of model parameters and problem\nresolutions.", "comment": "31 pages", "pdf_url": "http://arxiv.org/pdf/2508.00101v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.09727", "title": "A Large Sensor Foundation Model Pretrained on Continuous Glucose Monitor Data for Diabetes Management", "authors": ["Junjie Luo", "Abhimanyu Kumbara", "Mansur Shomali", "Rui Han", "Anand Iyer", "Ritu Agarwal", "Gordon Gao"], "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.09727v3", "summary": "Continuous glucose monitoring (CGM) combined with AI offers new opportunities\nfor proactive diabetes management through real-time glucose forecasting.\nHowever, most existing models are task-specific and lack generalization across\npatient populations. Inspired by the autoregressive paradigm of large language\nmodels, we introduce CGM-LSM, a Transformer decoder-based Large Sensor Model\n(LSM) pretrained on 1.6 million CGM records from patients with different\ndiabetes types, ages, and genders. We model patients as sequences of glucose\ntime steps to learn latent knowledge embedded in CGM data and apply it to the\nprediction of glucose readings for a 2-hour horizon. Compared with prior\nmethods, CGM-LSM significantly improves prediction accuracy and robustness: a\n48.51% reduction in root mean square error in one-hour horizon forecasting and\nconsistent zero-shot prediction performance across held-out patient groups. We\nanalyze model performance variations across patient subgroups and prediction\nscenarios and outline key opportunities and challenges for advancing CGM\nfoundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.09727v3", "cate": "q-bio.QM", "date": "2024-12-12", "updated": "2025-08-01"}
{"id": "2508.00531", "title": "The Repeated-Stimulus Confound in Electroencephalography", "authors": ["Jack A. Kilgallen", "Barak A. Pearlmutter", "Jeffrey Mark Siskind"], "categories": ["q-bio.NC", "cs.CV", "62K99, 68T05"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures, 8 tables, in submission to IEEE", "url": "http://arxiv.org/abs/2508.00531v1", "summary": "In neural-decoding studies, recordings of participants' responses to stimuli\nare used to train models. In recent years, there has been an explosion of\npublications detailing applications of innovations from deep-learning research\nto neural-decoding studies. The data-hungry models used in these experiments\nhave resulted in a demand for increasingly large datasets. Consequently, in\nsome studies, the same stimuli are presented multiple times to each participant\nto increase the number of trials available for use in model training. However,\nwhen a decoding model is trained and subsequently evaluated on responses to the\nsame stimuli, stimulus identity becomes a confounder for accuracy. We term this\nthe repeated-stimulus confound. We identify a susceptible dataset, and 16\npublications which report model performance based on evaluation procedures\naffected by the confound. We conducted experiments using models from the\naffected studies to investigate the likely extent to which results in the\nliterature have been misreported. Our findings suggest that the decoding\naccuracies of these models were overestimated by between 4.46-7.42%. Our\nanalysis also indicates that per 1% increase in accuracy under the confound,\nthe magnitude of the overestimation increases by 0.26%. The confound not only\nresults in optimistic estimates of decoding performance, but undermines the\nvalidity of several claims made within the affected publications. We conducted\nfurther experiments to investigate the implications of the confound in\nalternative contexts. We found that the same methodology used within the\naffected studies could also be used to justify an array of pseudoscientific\nclaims, such as the existence of extrasensory perception.", "comment": "15 pages, 6 figures, 8 tables, in submission to IEEE", "pdf_url": "http://arxiv.org/pdf/2508.00531v1", "cate": "q-bio.NC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00357", "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00357v1", "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00357v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00221", "title": "Partial Floquet Transformation and Model Order Reduction of Linear Time-Periodic Systems", "authors": ["Sam Bender", "Christopher Beattie"], "categories": ["math.NA", "cs.NA", "math.DS"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2508.00221v1", "summary": "Time-periodic dynamical systems occur commonly both in nature and as\nengineered systems. Large-scale linear time-periodic dynamical systems, for\nexample, may arise through linearization of a nonlinear system about a given\nperiodic solution (possibly as a consequence of a baseline periodic forcing)\nwith subsequent spatial discretization. The potential need to simulate\nresponses to a wide variety of input profiles (viewed as perturbations off a\nbaseline periodic forcing) creates a potent incentive for effective model\nreduction strategies applicable to linear time-periodic (LTP) systems.\nClassical approaches that take into account the underlying time-periodic system\nstructure often utilize the Floquet transform; however, computation of the\nFloquet transform is typically intractable for large order systems. In this\npaper, we develop the notion of a partial Floquet transformation connected to\nselected invariant subspaces of a time-varying differential operator associated\nwith the LTP system. We modify and repurpose the Dominant Pole Algorithm of\nRommes to identify effective invariant subspaces useful for model reduction. We\ndiscuss the construction of associated partial Floquet transformations and\ntime-varying reduction bases with which to produce effective reduced-order LTP\nmodels and illustrate the process on a simple time-periodic system.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2508.00221v1", "cate": "math.NA", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2412.12201", "title": "Embracing Large Language Models in Traffic Flow Forecasting", "authors": ["Yusheng Zhao", "Xiao Luo", "Haomin Wen", "Zhiping Xiao", "Wei Ju", "Ming Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACL 2025", "url": "http://arxiv.org/abs/2412.12201v2", "summary": "Traffic flow forecasting aims to predict future traffic flows based on the\nhistorical traffic conditions and the road network. It is an important problem\nin intelligent transportation systems, with a plethora of methods been\nproposed. Existing efforts mainly focus on capturing and utilizing\nspatio-temporal dependencies to predict future traffic flows. Though promising,\nthey fall short in adapting to test-time environmental changes of traffic\nconditions. To tackle this challenge, we propose to introduce large language\nmodels (LLMs) to help traffic flow forecasting and design a novel method named\nLarge Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two\nbranches, capturing different spatio-temporal relations using graph and\nhypergraph structures respectively. The two branches are first pre-trained\nindividually, and during test-time, they yield different predictions. Based on\nthese predictions, a large language model is used to select the most likely\nresult. Then, a ranking loss is applied as the learning objective to enhance\nthe prediction ability of the two branches. Extensive experiments on several\ndatasets demonstrate the effectiveness of the proposed LEAF.", "comment": "Accepted by ACL 2025", "pdf_url": "http://arxiv.org/pdf/2412.12201v2", "cate": "cs.LG", "date": "2024-12-15", "updated": "2025-08-01"}
{"id": "2508.00721", "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00721v1", "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00721v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00364", "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00364v1", "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00364v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00242", "title": "A reduced-IRKA method for large-scale $\\mathcal{H}_2$-optimal model order reduction", "authors": ["Yiding Lin", "Valeria Simoncini"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00242v1", "summary": "The $\\mathcal{H}_2$-optimal Model Order Reduction (MOR) is one of the most\nsignificant frameworks for reduction methodologies for linear dynamical\nsystems. In this context, the Iterative Rational Krylov Algorithm (\\IRKA) is a\nwell established method for computing an optimal projection space of fixed\ndimension $r$, when the system has small or medium dimensions. However, for\nlarge problems the performance of \\IRKA\\ is not satisfactory. In this paper, we\nintroduce a new rational Krylov subspace projection method with conveniently\nselected shifts, that can effectively handle large-scale problems. The\nprojection subspace is generated sequentially, and the \\IRKA\\ procedure is\nemployed on the projected problem to produce a new optimal rational space of\ndimension $r$ for the reduced problem, and the associated shifts. The latter\nare then injected to expand the projection space. Truncation of older\ninformation of the generated space is performed to limit memory requirements.\nNumerical experiments on benchmark problems illustrate the effectiveness of the\nnew method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00242v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.06101", "title": "ULTHO: Ultra-Lightweight yet Efficient Hyperparameter Optimization in Deep Reinforcement Learning", "authors": ["Mingqi Yuan", "Bo Li", "Xin Jin", "Wenjun Zeng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      24 pages, 25 figures", "url": "http://arxiv.org/abs/2503.06101v2", "summary": "Hyperparameter optimization (HPO) is a billion-dollar problem in machine\nlearning, which significantly impacts the training efficiency and model\nperformance. However, achieving efficient and robust HPO in deep reinforcement\nlearning (RL) is consistently challenging due to its high non-stationarity and\ncomputational cost. To tackle this problem, existing approaches attempt to\nadapt common HPO techniques (e.g., population-based training or Bayesian\noptimization) to the RL scenario. However, they remain sample-inefficient and\ncomputationally expensive, which cannot facilitate a wide range of\napplications. In this paper, we propose ULTHO, an ultra-lightweight yet\npowerful framework for fast HPO in deep RL within single runs. Specifically, we\nformulate the HPO process as a multi-armed bandit with clustered arms (MABC)\nand link it directly to long-term return optimization. ULTHO also provides a\nquantified and statistical perspective to filter the HPs efficiently. We test\nULTHO on benchmarks including ALE, Procgen, MiniGrid, and PyBullet. Extensive\nexperiments demonstrate that the ULTHO can achieve superior performance with a\nsimple architecture, contributing to the development of advanced and automated\nRL systems.", "comment": "24 pages, 25 figures", "pdf_url": "http://arxiv.org/pdf/2503.06101v2", "cate": "cs.LG", "date": "2025-03-08", "updated": "2025-08-01"}
{"id": "2508.00755", "title": "AI-Driven Collaborative Satellite Object Detection for Space Sustainability", "authors": ["Peng Hu", "Wenxuan Zhang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Submitted to the 13th Annual IEEE International Conference on Wireless for Space and Extreme Environments (WiSEE 2025)", "url": "http://arxiv.org/abs/2508.00755v1", "summary": "The growing density of satellites in low-Earth orbit (LEO) presents serious\nchallenges to space sustainability, primarily due to the increased risk of\nin-orbit collisions. Traditional ground-based tracking systems are constrained\nby latency and coverage limitations, underscoring the need for onboard,\nvision-based space object detection (SOD) capabilities. In this paper, we\npropose a novel satellite clustering framework that enables the collaborative\nexecution of deep learning (DL)-based SOD tasks across multiple satellites. To\nsupport this approach, we construct a high-fidelity dataset simulating imaging\nscenarios for clustered satellite formations. A distance-aware viewpoint\nselection strategy is introduced to optimize detection performance, and recent\nDL models are used for evaluation. Experimental results show that the\nclustering-based method achieves competitive detection accuracy compared to\nsingle-satellite and existing approaches, while maintaining a low size, weight,\nand power (SWaP) footprint. These findings underscore the potential of\ndistributed, AI-enabled in-orbit systems to enhance space situational awareness\nand contribute to long-term space sustainability.", "comment": "Submitted to the 13th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00755v1", "cate": "eess.IV", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00392", "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:1906.10851", "url": "http://arxiv.org/abs/2508.00392v1", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "pdf_url": "http://arxiv.org/pdf/2508.00392v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00375", "title": "A COGENT case study: Supporting Applications with Chombo", "authors": ["Daniel F. Martin", "Milo Dorr", "Mikhail Dorf", "Lee F. Ricketson"], "categories": ["math.NA", "cs.NA", "physics.plasm-ph"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00375v1", "summary": "We present a case study of how a software framework (Chombo) supported the\nspecific needs of a scientific application (COGENT). Since its inception in\n2000, the Chombo framework has supported various applications. One example of\nsuch support has been the collaboration with the Edge Simulation Laboratory to\nbuild the COGENT model. The specific needs of the COGENT effort required the\ndesign and implementation of a set of new capabilities in the Chombo framework,\nsuch as higher-order mapped-multiblock discretizations and multi-dimensional\ncode organization. These capabilities allowed COGENT to develop a unique\nsimulation capability for modeling the edge layers in tokamaks. Once developed,\nthese capabilities were able to support other applications which had similar\nneeds.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00375v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00229", "title": "Sequential, Parallel and Consecutive Hybrid Evolutionary-Swarm Optimization Metaheuristics", "authors": ["Piotr Urbańczyk", "Aleksandra Urbańczyk", "Magdalena Król", "Leszek Rutkowski", "Marek Kisiel-Dorohinicki"], "categories": ["cs.NE", "math.OC", "90C59 (Primary), 90C27, 68T20, 68W10 (Secondary)", "I.2.8; I.2.6; G.1.6; F.2.1; I.6.6"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      16 pages, 2 figures, 5 tables, 5 algorithms, conference", "url": "http://arxiv.org/abs/2508.00229v1", "summary": "The goal of this paper is twofold. First, it explores hybrid\nevolutionary-swarm metaheuristics that combine the features of PSO and GA in a\nsequential, parallel and consecutive manner in comparison with their standard\nbasic form: Genetic Algorithm and Particle Swarm Optimization. The algorithms\nwere tested on a set of benchmark functions, including Ackley, Griewank, Levy,\nMichalewicz, Rastrigin, Schwefel, and Shifted Rotated Weierstrass, across\nmultiple dimensions. The experimental results demonstrate that the hybrid\napproaches achieve superior convergence and consistency, especially in\nhigher-dimensional search spaces. The second goal of this paper is to introduce\na novel consecutive hybrid PSO-GA evolutionary algorithm that ensures\ncontinuity between PSO and GA steps through explicit information transfer\nmechanisms, specifically by modifying GA's variation operators to inherit\nvelocity and personal best information.", "comment": "16 pages, 2 figures, 5 tables, 5 algorithms, conference", "pdf_url": "http://arxiv.org/pdf/2508.00229v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.07556", "title": "Novice Developers' Perspectives on Adopting LLMs for Software Development: A Systematic Literature Review", "authors": ["Samuel Ferino", "Rashina Hoda", "John Grundy", "Christoph Treude"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.07556v2", "summary": "Following the rise of large language models (LLMs), many studies have emerged\nin recent years focusing on exploring the adoption of LLM-based tools for\nsoftware development by novice developers: computer science/software\nengineering students and early-career industry developers with two years or\nless of professional experience. These studies have sought to understand the\nperspectives of novice developers on using these tools, a critical aspect of\nthe successful adoption of LLMs in software engineering. To systematically\ncollect and summarise these studies, we conducted a systematic literature\nreview (SLR) following the guidelines by Kitchenham et al. on 80 primary\nstudies published between April 2022 and June 2025 to answer four research\nquestions (RQs). In answering RQ1, we categorised the study motivations and\nmethodological approaches. In RQ2, we identified the software development tasks\nfor which novice developers use LLMs. In RQ3, we categorised the advantages,\nchallenges, and recommendations discussed in the studies. Finally, we discuss\nthe study limitations and future research needs suggested in the primary\nstudies in answering RQ4. Throughout the paper, we also indicate directions for\nfuture work and implications for software engineering researchers, educators,\nand developers. Our research artifacts are publicly available at\nhttps://github.com/Samuellucas97/SupplementaryInfoPackage-SLR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.07556v2", "cate": "cs.SE", "date": "2025-03-10", "updated": "2025-08-01"}
{"id": "2401.10011", "title": "CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Retrieval", "authors": ["Xinpeng Zhao", "Yanwei Zheng", "Chuanlin Lan", "Xiaowei Zhang", "Bowen Huang", "Jibin Yang", "Dongxiao Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 6 figures, under peer review", "url": "http://arxiv.org/abs/2401.10011v2", "summary": "Weakly supervised text-based person retrieval seeks to retrieve images of a\ntarget person using textual descriptions, without relying on identity\nannotations and is more challenging and practical. The primary challenge is the\nintra-class differences, encompassing intra-modal feature variations and\ncross-modal semantic gaps. Prior works have focused on instance-level samples\nand ignored prototypical features of each person which are intrinsic and\ninvariant. Toward this, we propose a Cross-Modal Prototypical Contrastive\nLearning (CPCL) method. In practice, the CPCL introduces the CLIP model to\nweakly supervised text-based person retrieval to map visual and textual\ninstances into a shared latent space. Subsequently, the proposed Prototypical\nMulti-modal Memory (PMM) module captures associations between heterogeneous\nmodalities of image-text pairs belonging to the same person through the Hybrid\nCross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover,\nthe Outlier Pseudo Label Mining (OPLM) module further distinguishes valuable\noutlier samples from each modality, enhancing the creation of more reliable\nclusters by mining implicit relationships between image-text pairs. We conduct\nextensive experiments on popular benchmarks of weakly supervised text-based\nperson retrieval, which validate the effectiveness, generalizability of CPCL.", "comment": "9 pages, 6 figures, under peer review", "pdf_url": "http://arxiv.org/pdf/2401.10011v2", "cate": "cs.CV", "date": "2024-01-18", "updated": "2025-08-01"}
{"id": "2508.00410", "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00410v1", "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00410v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00515", "title": "A new addition theorem for the 3-D Navier-Lamé system and its application to the method of fundamental solutions", "authors": ["J. A. Barceló", "C. Castro", "A. Ruiz", "M. C. Vilela"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00515v1", "summary": "We obtain a new addition theorem for the fundamental solution of the\nNavier-Lam\\'e system in dimension 3 satisfying the Kupradze radiation\nconditions. This provides an expansion of this fundamental solution that\ninvolves only the evaluation of Bessel functions and scalar spherical\nharmonics. This is particularly useful in collocation numerical methods based\non fundamental solutions, such as the boundary element method or the method of\nfundamental solutions. For this last method, we show its efficiency when\napproximating the Navier-Lam\\'e system in exterior domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00515v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00380", "title": "Evolutionary Generative Optimization: Towards Fully Data-Driven Evolutionary Optimization via Generative Learning", "authors": ["Kebin Sun", "Tao Jiang", "Ran Cheng", "Yaochu Jin", "Kay Chen Tan"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00380v1", "summary": "Recent advances in data-driven evolutionary algorithms (EAs) have\ndemonstrated the potential of leveraging data to improve optimization accuracy\nand adaptability. Nevertheless, most existing approaches remain dependent on\nhandcrafted heuristics, which limits their generality and automation. To\naddress this challenge, we propose Evolutionary Generative Optimization\n(EvoGO), a fully data-driven framework empowered by generative learning. EvoGO\nstreamlines the evolutionary optimization process into three stages: data\npreparation, model training, and population generation. The data preparation\nstage constructs a pairwise dataset to enrich training diversity without\nincurring additional evaluation costs. During model training, a tailored\ngenerative model learns to transform inferior solutions into superior ones. In\nthe population generation stage, EvoGO replaces traditional reproduction\noperators with a scalable and parallelizable generative mechanism. Extensive\nexperiments on numerical benchmarks, classical control problems, and\nhigh-dimensional robotic tasks demonstrate that EvoGO consistently converges\nwithin merely 10 generations and significantly outperforms a wide spectrum of\noptimization approaches, including traditional EAs, Bayesian optimization, and\nreinforcement learning based methods. Source code will be made publicly\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00380v1", "cate": "cs.NE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.04029", "title": "Simultaneous Motion And Noise Estimation with Event Cameras", "authors": ["Shintaro Shiba", "Yoshimitsu Aoki", "Guillermo Gallego"], "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 13 figures, 6 tables, Project page this https URL", "url": "http://arxiv.org/abs/2504.04029v2", "summary": "Event cameras are emerging vision sensors whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras are often designed\nin isolation and thus consider other tasks, such as motion estimation,\nseparately (i.e., sequentially after denoising). However, motion is an\nintrinsic part of event data, since scene edges cannot be sensed without\nmotion. We propose, to the best of our knowledge, the first method that\nsimultaneously estimates motion in its various forms (e.g., ego-motion, optical\nflow) and noise. The method is flexible, as it allows replacing the one-step\nmotion estimation of the widely-used Contrast Maximization framework with any\nother motion estimator, such as deep neural networks. The experiments show that\nthe proposed method achieves state-of-the-art results on the E-MLB denoising\nbenchmark and competitive results on the DND21 benchmark, while demonstrating\neffectiveness across motion estimation and intensity reconstruction tasks. Our\napproach advances event-data denoising theory and expands practical denoising\nuse-cases via open-source code. Project page: https://github.com/tub-rip/ESMD", "comment": "13 pages, 13 figures, 6 tables, Project page\n  https://github.com/tub-rip/ESMD", "pdf_url": "http://arxiv.org/pdf/2504.04029v2", "cate": "cs.CV", "date": "2025-04-05", "updated": "2025-08-01"}
{"id": "2404.07977", "title": "Gaga: Group Any Gaussians via 3D-aware Memory Bank", "authors": ["Weijie Lyu", "Xueting Li", "Abhijit Kundu", "Yi-Hsuan Tsai", "Ming-Hsuan Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2404.07977v3", "summary": "We introduce Gaga, a framework that reconstructs and segments open-world 3D\nscenes by leveraging inconsistent 2D masks predicted by zero-shot\nclass-agnostic segmentation models. Contrasted to prior 3D scene segmentation\napproaches that rely on video object tracking or contrastive learning methods,\nGaga utilizes spatial information and effectively associates object masks\nacross diverse camera poses through a novel 3D-aware memory bank. By\neliminating the assumption of continuous view changes in training images, Gaga\ndemonstrates robustness to variations in camera poses, particularly beneficial\nfor sparsely sampled images, ensuring precise mask label consistency.\nFurthermore, Gaga accommodates 2D segmentation masks from diverse sources and\ndemonstrates robust performance with different open-world zero-shot\nclass-agnostic segmentation models, significantly enhancing its versatility.\nExtensive qualitative and quantitative evaluations demonstrate that Gaga\nperforms favorably against state-of-the-art methods, emphasizing its potential\nfor real-world applications such as 3D scene understanding and manipulation.", "comment": "Project Page: https://weijielyu.github.io/Gaga", "pdf_url": "http://arxiv.org/pdf/2404.07977v3", "cate": "cs.CV", "date": "2024-04-11", "updated": "2025-08-01"}
{"id": "2508.00415", "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00415v1", "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00415v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00559", "title": "Solitary-wave solutions of the fractional nonlinear Schrödinger equation. II. A numerical study of the dynamics", "authors": ["Angel Durán", "Nuria Reguera"], "categories": ["math.NA", "cs.NA", "math.AP", "76B25, 35C07, 65H10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00559v1", "summary": "The present paper is a numerical study of the dynamics of solitary wave\nsolutions of the fractional nonlinear Schr\\\"{o}dinger equation, whose existence\nwas analyzed by the authors in the first part of the project. The computational\nstudy will be made from the approximation of the periodic initial-value problem\nwith a fully discrete scheme consisting of a Fourier spectral method for the\nspatial discretization and a fourth-order, Runge-Kutta-Composition method as\ntime integrator. Several issues regarding the stability of the waves, such as\nthe effects of small and large perturbations, interactions of solitary waves\nand the resolution of initial data into trains of waves are discussed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00559v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00063", "title": "Anomaly detection with spiking neural networks for LHC physics", "authors": ["Barry M. Dillon", "Jim Harkin", "Aqib Javed"], "categories": ["hep-ph", "cs.NE", "hep-ex"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      24 pages, 15 figures, 1 table", "url": "http://arxiv.org/abs/2508.00063v1", "summary": "Anomaly detection offers a promising strategy for discovering new physics at\nthe Large Hadron Collider (LHC). This paper investigates AutoEncoders built\nusing neuromorphic Spiking Neural Networks (SNNs) for this purpose. One key\napplication is at the trigger level, where anomaly detection tools could\ncapture signals that would otherwise be discarded by conventional selection\ncuts. These systems must operate under strict latency and computational\nconstraints. SNNs are inherently well-suited for low-latency, low-memory,\nreal-time inference, particularly on Field-Programmable Gate Arrays (FPGAs).\nFurther gains are expected with the rapid progress in dedicated neuromorphic\nhardware development. Using the CMS ADC2021 dataset, we design and evaluate a\nsimple SNN AutoEncoder architecture. Our results show that the SNN AutoEncoders\nare competitive with conventional AutoEncoders for LHC anomaly detection across\nall signal models.", "comment": "24 pages, 15 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2508.00063v1", "cate": "hep-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2504.10812", "title": "E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking", "authors": ["Kejia Gao", "Liguo Zhou", "Mingjun Liu", "Alois Knoll"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10812v2", "summary": "End-to-end learning has shown great potential in autonomous parking, yet the\nlack of publicly available datasets limits reproducibility and benchmarking.\nWhile prior work introduced a visual-based parking model and a pipeline for\ndata generation, training, and close-loop test, the dataset itself was not\nreleased. To bridge this gap, we create and open-source a high-quality dataset\nfor end-to-end autonomous parking. Using the original model, we achieve an\noverall success rate of 85.16% with lower average position and orientation\nerrors (0.24 meters and 0.34 degrees).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10812v2", "cate": "cs.RO", "date": "2025-04-15", "updated": "2025-08-01"}
{"id": "2405.16181", "title": "Boosting Adversarial Transferability with Low-Cost Optimization via Maximin Expected Flatness", "authors": ["Chunlin Qiu", "Ang Li", "Yiheng Duan", "Shenyi Zhang", "Yuanjie Zhang", "Lingchen Zhao", "Qian Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The original NCS method has been revised and renamed as MEF. A theoretical proof of the relationship between flatness and transferability is added", "url": "http://arxiv.org/abs/2405.16181v2", "summary": "Transfer-based attacks craft adversarial examples on white-box surrogate\nmodels and directly deploy them against black-box target models, offering\nmodel-agnostic and query-free threat scenarios. While flatness-enhanced methods\nhave recently emerged to improve transferability by enhancing the loss surface\nflatness of adversarial examples, their divergent flatness definitions and\nheuristic attack designs suffer from unexamined optimization limitations and\nmissing theoretical foundation, thus constraining their effectiveness and\nefficiency. This work exposes the severely imbalanced exploitation-exploration\ndynamics in flatness optimization, establishing the first theoretical\nfoundation for flatness-based transferability and proposing a principled\nframework to overcome these optimization pitfalls. Specifically, we\nsystematically unify fragmented flatness definitions across existing methods,\nrevealing their imbalanced optimization limitations in over-exploration of\nsensitivity peaks or over-exploitation of local plateaus. To resolve these\nissues, we rigorously formalize average-case flatness and transferability gaps,\nproving that enhancing zeroth-order average-case flatness minimizes cross-model\ndiscrepancies. Building on this theory, we design a Maximin Expected Flatness\n(MEF) attack that enhances zeroth-order average-case flatness while balancing\nflatness exploration and exploitation. Extensive evaluations across 22 models\nand 24 current transfer-based attacks demonstrate MEF's superiority: it\nsurpasses the state-of-the-art PGN attack by 4% in attack success rate at half\nthe computational cost and achieves 8% higher success rate under the same\nbudget. When combined with input augmentation, MEF attains 15% additional gains\nagainst defense-equipped models, establishing new robustness benchmarks. Our\ncode is available at https://github.com/SignedQiu/MEFAttack.", "comment": "The original NCS method has been revised and renamed as MEF. A\n  theoretical proof of the relationship between flatness and transferability is\n  added", "pdf_url": "http://arxiv.org/pdf/2405.16181v2", "cate": "cs.CV", "date": "2024-05-25", "updated": "2025-08-01"}
{"id": "2508.00472", "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00472v1", "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00472v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00722", "title": "Towards a mixed-precision ADI method for Lyapunov equations", "authors": ["Jonas Schulze", "Jens Saak"], "categories": ["math.NA", "cs.NA", "15A24, 65F10, 65F45, 65F55"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, 1 table; submitted to PAMM 2025", "url": "http://arxiv.org/abs/2508.00722v1", "summary": "We apply mixed-precision to the low-rank Lyapunov ADI (LR-ADI) by performing\ncertain aspects of the algorithm in a lower working precision. Namely, we\naccumulate the overall solution, solve the linear systems comprising the ADI\niteration, and store the inner low-rank factors of the residuals in various\ncombinations of IEEE 754 single and double precision. We empirically test our\nimplementation on Lyapunov equations arising from first- and second-order\ndescriptor systems. For the first-order examples, accumulating the solution in\nsingle-precision yields an almost-as-small residual as for the double-precision\nsolution. For certain applications, like computing the H2 norm of a descriptor\nsystem, low- or mixed-precision variants of the ADI can be quite competitive", "comment": "11 pages, 3 figures, 1 table; submitted to PAMM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00722v1", "cate": "math.NA", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.10375", "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?", "authors": ["Rui Melo", "Claudia Mamede", "Andre Catarino", "Rui Abreu", "Henrique Lopes Cardoso"], "categories": ["cs.SE", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2505.10375v3", "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision. Code available at\nhttps://github.com/rufimelo99/SAE-Java-Bug-Detection", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2505.10375v3", "cate": "cs.SE", "date": "2025-05-15", "updated": "2025-07-31"}
{"id": "2406.04158", "title": "Accurate Cross-modal Reconstruction of Vehicle Target from Sparse-aspect Multi-baseline SAR data", "authors": ["Da Li", "Guoqiang Zhao", "Chen Yao", "Kaiqiang Zhu", "Houjun Sun", "Jiacheng Bao", "Maokun Li"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2406.04158v5", "summary": "Multi-aspect multi-baseline SAR 3D imaging is a critical remote sensing\ntechnique, promising in urban mapping and monitoring. However, sparse\nobservation due to constrained flight trajectories degrade imaging quality,\nparticularly for anisotropic small targets like vehicles and aircraft. In the\npast, compressive sensing (CS) was the mainstream approach for sparse 3D SAR\nreconstruction. More recently, deep learning (DL) has emerged as a powerful\nalternative, markedly boosting reconstruction quality and efficiency through\nstrong data-driven representations capabilities and fast inference\ncharacteristics. However, existing DL methods typically train deep neural\nnetworks (DNNs) using only high-resolution radar images. This unimodal learning\nparadigm precludes the incorporation of complementary information from other\ndata sources, thereby limiting potential improvements in reconstruction\nperformance. In this paper, we introduce cross-modal learning and propose a\nCross-Modal 3D-SAR Reconstruction Network (CMAR-Net) that enhances sparse 3D\nSAR reconstruction by fusing heterogeneous information. Leveraging cross-modal\nsupervision from 2D optical images and error propagation guaranteed by\ndifferentiable rendering, CMAR-Net achieves efficient training and reconstructs\nhighly sparse-aspect multi-baseline SAR image into visually structured and\naccurate 3D images, particularly for vehicle targets. Trained solely on\nsimulated data, CMAR-Net exhibits strong generalization across extensive\nreal-world evaluations on parking lot measurements containing numerous civilian\nvehicles, outperforming state-of-the-art CS and DL methods in structural\naccuracy. Our work highlights the potential of cross-modal learning for 3D SAR\nreconstruction and introduces a novel framework for radar imaging research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2406.04158v5", "cate": "cs.CV", "date": "2024-06-06", "updated": "2025-08-01"}
{"id": "2508.00507", "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACM Multimedia 2025 (MM '25)", "url": "http://arxiv.org/abs/2508.00507v1", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "pdf_url": "http://arxiv.org/pdf/2508.00507v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00247", "title": "Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks", "authors": ["Sergei Gleyzer", "Hanh Nguyen", "Dinesh P. Ramakrishnan", "Eric A. F. Reinhardt"], "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      15 pages, 3 figures", "url": "http://arxiv.org/abs/2508.00247v1", "summary": "The Kolmogorov-Arnold representation theorem states that any continuous\nmultivariable function can be exactly represented as a finite superposition of\ncontinuous single variable functions. Subsequent simplifications of this\nrepresentation involve expressing these functions as parameterized sums of a\nsmaller number of unique monotonic functions. These developments led to the\nproof of the universal approximation capabilities of multilayer perceptron\nnetworks with sigmoidal activations, forming the alternative theoretical\ndirection of most modern neural networks.\n  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an\nalternative to multilayer perceptrons. KANs feature learnable nonlinear\nactivations applied directly to input values, modeled as weighted sums of basis\nspline functions. This approach replaces the linear transformations and\nsigmoidal post-activations used in traditional perceptrons. Subsequent works\nhave explored alternatives to spline-based activations. In this work, we\npropose a novel KAN variant by replacing both the inner and outer functions in\nthe Kolmogorov-Arnold representation with weighted sinusoidal functions of\nlearnable frequencies. Inspired by simplifications introduced by Lorentz and\nSprecher, we fix the phases of the sinusoidal activations to linearly spaced\nconstant values and provide a proof of its theoretical validity. We also\nconduct numerical experiments to evaluate its performance on a range of\nmultivariable functions, comparing it with fixed-frequency Fourier transform\nmethods and multilayer perceptrons (MLPs). We show that it outperforms the\nfixed-frequency Fourier transform and achieves comparable performance to MLPs.", "comment": "15 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2508.00247v1", "cate": "stat.ML", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.11454", "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation", "authors": ["Shaina Raza", "Aravind Narayanan", "Vahid Reza Khazaie", "Ashmal Vayani", "Mukund S. Chettiar", "Amandeep Singh", "Mubarak Shah", "Deval Pandya"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11454v3", "summary": "Large multimodal models (LMMs) have been widely tested on tasks like visual\nquestion answering (VQA), image captioning, and grounding, but lack rigorous\nevaluation for alignment with human-centered (HC) values such as fairness,\nethics, and inclusivity. To address this gap, we introduce\n\\textbf{HumaniBench}, a novel benchmark of 32,000 real-world image-question\npairs and an evaluation suite. Labels are generated via an AI-assisted pipeline\nand validated by experts. HumaniBench assesses LMMs across seven key alignment\nprinciples: fairness, ethics, empathy, inclusivity, reasoning, robustness, and\nmultilinguality, through diverse open-ended and closed-ended VQA tasks.\nGrounded in AI ethics and real-world needs, these principles provide a holistic\nlens for societal impact. Benchmarking results on different LMM shows that\nproprietary models generally lead in reasoning, fairness, and multilinguality,\nwhile open-source models excel in robustness and grounding. Most models\nstruggle to balance accuracy with ethical and inclusive behavior. Techniques\nlike Chain-of-Thought prompting and test-time scaling improve alignment. As the\nfirst benchmark tailored for HC alignment, HumaniBench offers a rigorous\ntestbed to diagnose limitations, and promote responsible LMM development. All\ndata and code are publicly available for reproducibility.\n  Keywords: HumaniBench, vision-language models, responsible AI benchmark, AI\nalignment evaluation, AI ethics assessment, fairness in AI models, visual\nquestion answering (VQA) benchmark, image captioning evaluation, visual\ngrounding tasks, trustworthy AI models, Chain-of-Thought prompting, test-time\nscaling, ethical AI development tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11454v3", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-08-01"}
{"id": "2406.08451", "title": "GUIOdyssey: A Comprehensive Dataset for Cross-App GUI Navigation on Mobile Devices", "authors": ["Quanfeng Lu", "Wenqi Shao", "Zitao Liu", "Lingxiao Du", "Fanqing Meng", "Boxuan Li", "Botong Chen", "Siyuan Huang", "Kaipeng Zhang", "Ping Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      22 pages, 14 figures, ICCV 2025, a cross-app GUI navigation dataset", "url": "http://arxiv.org/abs/2406.08451v2", "summary": "Autonomous Graphical User Interface (GUI) navigation agents can enhance user\nexperience in communication, entertainment, and productivity by streamlining\nworkflows and reducing manual intervention. However, prior GUI agents often\ntrained with datasets comprising tasks that can be completed within a single\napp, leading to poor performance in cross-app navigation. To address this\nproblem, we present GUIOdyssey, a comprehensive dataset for cross-app mobile\nGUI navigation. GUIOdyssey comprises 8,334 episodes with an average of 15.3\nsteps per episode, covering 6 mobile devices, 212 distinct apps, and 1,357 app\ncombinations. Each step is enriched with detailed semantic reasoning\nannotations, which aid the model in building cognitive processes and enhancing\nits reasoning abilities for complex cross-app tasks. Building on GUIOdyssey, we\ndevelop OdysseyAgent, an exploratory multimodal agent for long-step cross-app\nnavigation equipped with a history resampler module that efficiently attends to\nhistorical screenshot tokens, balancing performance and inference speed.\nExtensive experiments conducted in both in-domain and out-of-domain scenarios\nvalidate the effectiveness of our approach. Moreover, we demonstrate that\nhistorial information involving actions, screenshots and context in our dataset\ncan significantly enhances OdysseyAgent's performance on complex cross-app\ntasks.", "comment": "22 pages, 14 figures, ICCV 2025, a cross-app GUI navigation dataset", "pdf_url": "http://arxiv.org/pdf/2406.08451v2", "cate": "cs.CV", "date": "2024-06-12", "updated": "2025-08-01"}
{"id": "2508.00513", "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ECAI 2025", "url": "http://arxiv.org/abs/2508.00513v1", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "comment": "Accepted by ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2508.00513v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2201.00145", "title": "Matrix Decomposition and Applications", "authors": ["Jun Lu"], "categories": ["math.NA", "cs.LG", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2201.00145v5", "summary": "In 1954, Alston S. Householder published Principles of Numerical Analysis,\none of the first modern treatments on matrix decomposition that favored a\n(block) LU decomposition-the factorization of a matrix into the product of\nlower and upper triangular matrices. And now, matrix decomposition has become a\ncore technology in machine learning, largely due to the development of the\nbackpropagation algorithm in fitting a neural network. The sole aim of this\nsurvey is to give a self-contained introduction to concepts and mathematical\ntools in numerical linear algebra and matrix analysis in order to seamlessly\nintroduce matrix decomposition techniques and their applications in subsequent\nsections. However, we clearly realize our inability to cover all the useful and\ninteresting results concerning matrix decomposition, given the paucity of scope\nto present this discussion, e.g., the separated analysis of the Euclidean\nspace, Hermitian space, Hilbert space, and things in the complex domain. We\nrefer the reader to literature in the field of linear algebra for a more\ndetailed introduction to the related fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2201.00145v5", "cate": "math.NA", "date": "2022-01-01", "updated": "2025-08-01"}
{"id": "2505.24025", "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models", "authors": ["Chenbin Pan", "Wenbin He", "Zhengzhong Tu", "Liu Ren"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.24025v2", "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\n\\textbf{DINO-R1}, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query\nOptimization (GRQO)}, a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.24025v2", "cate": "cs.CV", "date": "2025-05-29", "updated": "2025-08-01"}
{"id": "2407.04230", "title": "A Physical Model-Guided Framework for Underwater Image Enhancement and Depth Estimation", "authors": ["Dazhao Du", "Lingyu Si", "Fanjiang Xu", "Jianwei Niu", "Fuchun Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2407.04230v2", "summary": "Due to the selective absorption and scattering of light by diverse aquatic\nmedia, underwater images usually suffer from various visual degradations.\nExisting underwater image enhancement (UIE) approaches that combine underwater\nphysical imaging models with neural networks often fail to accurately estimate\nimaging model parameters such as depth and veiling light, resulting in poor\nperformance in certain scenarios. To address this issue, we propose a physical\nmodel-guided framework for jointly training a Deep Degradation Model (DDM) with\nany advanced UIE model. DDM includes three well-designed sub-networks to\naccurately estimate various imaging parameters: a veiling light estimation\nsub-network, a factors estimation sub-network, and a depth estimation\nsub-network. Based on the estimated parameters and the underwater physical\nimaging model, we impose physical constraints on the enhancement process by\nmodeling the relationship between underwater images and desired clean images,\ni.e., outputs of the UIE model. Moreover, while our framework is compatible\nwith any UIE model, we design a simple yet effective fully convolutional UIE\nmodel, termed UIEConv. UIEConv utilizes both global and local features for\nimage enhancement through a dual-branch structure. UIEConv trained within our\nframework achieves remarkable enhancement results across diverse underwater\nscenes. Furthermore, as a byproduct of UIE, the trained depth estimation\nsub-network enables accurate underwater scene depth estimation. Extensive\nexperiments conducted in various real underwater imaging scenarios, including\ndeep-sea environments with artificial light sources, validate the effectiveness\nof our framework and the UIEConv model.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2407.04230v2", "cate": "cs.CV", "date": "2024-07-05", "updated": "2025-08-01"}
{"id": "2508.00523", "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00523v1", "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00523v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2312.12699", "title": "Propagation of chaos in infinite horizon and numerical stability for stochastic McKean-Vlasov equations", "authors": ["Zhuoqi Liu", "Shuaibin Gao", "Chenggui Yuan", "Qian Guo"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.12699v2", "summary": "This paper focuses on the numerical stability of stochastic McKean-Vlasov\nequations (SMVEs) via the stochastic particle method. Firstly, the long-time\npropagation of chaos in the mean-square sense is obtained, and the almost sure\npropagation in infinite horizon is also proved. Next, when the coefficients\nsatisfy linear growth conditions, the mean-square and almost sure exponential\nstabilities of the Euler-Maruyama (EM) scheme associated with the corresponding\ninteracting particle system are shown through an ingenious manipulation of\nempirical measure. Then, for the case that the state variables in drift and\ndiffusion are both superlinear, the mean-square exponential stability of the\nbackward EM scheme for the interacting system is achieved without the particle\ncorruption, which is a novel conclusion. Moreover, under the linear growth\ncondition on the diffusion coefficient, the almost sure stability of the\nbackward EM scheme is studied. Combining these assertions enables the numerical\nsolutions to reproduce the stabilities of the original SMVEs. The examples,\nincluding a feedback control problem and a stochastic opinion dynamics model,\nare provided to demonstrate the importance of theoretical analysis of numerical\nstability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.12699v2", "cate": "math.NA", "date": "2023-12-20", "updated": "2025-08-01"}
{"id": "2506.06509", "title": "Private GPTs for LLM-driven testing in software development and machine learning", "authors": ["Jakub Jagielski", "Consuelo Rojas", "Markus Abel"], "categories": ["cs.SE", "cs.AI", "I.2.1"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      5 pages, 10 figures", "url": "http://arxiv.org/abs/2506.06509v2", "summary": "In this contribution, we examine the capability of private GPTs to\nautomatically generate executable test code based on requirements. More\nspecifically, we use acceptance criteria as input, formulated as part of epics,\nor stories, which are typically used in modern development processes. This\ngives product owners, or business intelligence, respectively, a way to directly\nproduce testable criteria through the use of LLMs. We explore the quality of\nthe so-produced tests in two ways: i) directly by letting the LLM generate code\nfrom requirements, ii) through an intermediate step using Gherkin syntax. As a\nresult, it turns out that the two-step procedure yields better results -where\nwe define better in terms of human readability and best coding practices, i.e.\nlines of code and use of additional libraries typically used in testing.\nConcretely, we evaluate prompt effectiveness across two scenarios: a simple\n\"Hello World\" program and a digit classification model, showing that structured\nprompts lead to higher-quality test outputs.", "comment": "5 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2506.06509v2", "cate": "cs.SE", "date": "2025-06-06", "updated": "2025-07-31"}
{"id": "2409.00618", "title": "YOLOO: You Only Learn from Others Once", "authors": ["Lipeng Gu", "Mingqiang Wei", "Xuefeng Yan", "Dingkun Zhu", "Wei Zhao", "Haoran Xie"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.00618v2", "summary": "Multi-modal 3D multi-object tracking (MOT) typically necessitates extensive\ncomputational costs of deep neural networks (DNNs) to extract multi-modal\nrepresentations. In this paper, we propose an intriguing question: May we learn\nfrom multiple modalities only during training to avoid multi-modal input in the\ninference phase? To answer it, we propose \\textbf{YOLOO}, a novel multi-modal\n3D MOT paradigm: You Only Learn from Others Once. YOLOO empowers the point\ncloud encoder to learn a unified tri-modal representation (UTR) from point\nclouds and other modalities, such as images and textual cues, all at once.\nLeveraging this UTR, YOLOO achieves efficient tracking solely using the point\ncloud encoder without compromising its performance, fundamentally obviating the\nneed for computationally intensive DNNs. Specifically, YOLOO includes two core\ncomponents: a unified tri-modal encoder (UTEnc) and a flexible geometric\nconstraint (F-GC) module. UTEnc integrates a point cloud encoder with image and\ntext encoders adapted from pre-trained CLIP. It seamlessly fuses point cloud\ninformation with rich visual-textual knowledge from CLIP into the point cloud\nencoder, yielding highly discriminative UTRs that facilitate the association\nbetween trajectories and detections. Additionally, F-GC filters out mismatched\nassociations with similar representations but significant positional\ndiscrepancies. It further enhances the robustness of UTRs without requiring any\nscene-specific tuning, addressing a key limitation of customized geometric\nconstraints (e.g., 3D IoU). Lastly, high-quality 3D trajectories are generated\nby a traditional data association component. By integrating these advancements\ninto a multi-modal 3D MOT scheme, our YOLOO achieves substantial gains in both\nrobustness and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.00618v2", "cate": "cs.CV", "date": "2024-09-01", "updated": "2025-08-01"}
{"id": "2508.00539", "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "authors": ["Judy X Yang"], "categories": ["cs.LG", "Cs"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00539v1", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "comment": "8 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00539v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.02629", "title": "A fully segregated and unconditionally stable IMEX scheme for dispersed multiphase flows", "authors": ["Douglas Pacheco", "Richard Schussnig"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.02629v2", "summary": "Euler--Euler or volume-averaged Navier--Stokes equations are used in various\napplications to model systems with two or more interpenetrating phases. Each\nfluid obeys its own momentum and mass equations, and the phases are typically\ncoupled via drag forces and a shared pressure. Monolithic solvers can therefore\nbe very expensive and difficult to implement, so there is great computational\nappeal for decoupled methods. However, splitting the subproblems requires\ntreating the coupling terms (pressure and drag) explicitly, which must be done\ncarefully to avoid time-step restrictions. In this context, we derive a new\nfirst-order pressure-correction method based on the incompressibility of the\nmean velocity field, combined with an explicit treatment of the drag forces.\nFurthermore, both the convective and viscous terms are treated semi-implicitly.\nThis gives us an implicit-explicit (IMEX) method that is very robust not only\ndue to its unconditional energy stability, but also because it does not require\nany type of fixed-point iterations. Each time step has only linear, scalar\ntransport equations and a single pressure Poisson problem as building blocks.\nWe rigorously prove temporal stability without any CFL-like conditions, and the\ntheory is confirmed through two-phase numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.02629v2", "cate": "math.NA", "date": "2025-04-03", "updated": "2025-08-01"}
{"id": "2506.23068", "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "Jürgen Schmidhuber", "Mengyue Yang"], "categories": ["cs.LG", "cs.AI", "stat.AP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      33 pages", "url": "http://arxiv.org/abs/2506.23068v2", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2506.23068v2", "cate": "cs.LG", "date": "2025-06-29", "updated": "2025-08-01"}
{"id": "2409.17981", "title": "BlinkTrack: Feature Tracking over 80 FPS via Events and Images", "authors": ["Yichen Shen", "Yijin Li", "Shuo Chen", "Guanglin Li", "Zhaoyang Huang", "Hujun Bao", "Zhaopeng Cui", "Guofeng Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.17981v2", "summary": "Event cameras, known for their high temporal resolution and ability to\ncapture asynchronous changes, have gained significant attention for their\npotential in feature tracking, especially in challenging conditions. However,\nevent cameras lack the fine-grained texture information that conventional\ncameras provide, leading to error accumulation in tracking. To address this, we\npropose a novel framework, BlinkTrack, which integrates event data with\ngrayscale images for high-frequency feature tracking. Our method extends the\ntraditional Kalman filter into a learning-based framework, utilizing\ndifferentiable Kalman filters in both event and image branches. This approach\nimproves single-modality tracking and effectively solves the data association\nand fusion from asynchronous event and image data. We also introduce new\nsynthetic and augmented datasets to better evaluate our model. Experimental\nresults indicate that BlinkTrack significantly outperforms existing methods,\nexceeding 80 FPS with multi-modality data and 100 FPS with preprocessed event\ndata. Codes and dataset are available at\nhttps://github.com/ColieShen/BlinkTrack.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.17981v2", "cate": "cs.CV", "date": "2024-09-26", "updated": "2025-07-31"}
{"id": "2508.00578", "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gräter", "Pascal Friederich"], "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 12 figures, and 4 tables (references and SI included)", "url": "http://arxiv.org/abs/2508.00578v1", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "pdf_url": "http://arxiv.org/pdf/2508.00578v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.01762", "title": "Global Energy Minimization for Simplex Mesh Optimization: A Radius Ratio Approach to Sliver Elimination", "authors": ["Dong Wang", "Chunyu Chen", "Huayi Wei"], "categories": ["math.NA", "cs.NA", "math.OC", "65N50, 65K10, 65F08"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01762v3", "summary": "The quality of simplex mesh is crucial for the stability and accuracy of\nnumerical simulations in finite element analysis and computational geometry.\nHowever, the presence of sliver elements in 3D simplex mesh can severely impact\nthe results. This paper presents a novel method based on a radius ratio energy\nfunction to optimize the quality of simplex mesh elements. This method can\neffectively eliminate sliver elements, thereby enhancing mesh quality.The\ngradient of the proposed energy function can be decomposed into a matrix-vector\nproduct. With minor processing, the matrix becomes symmetric positive definite,\nand this symmetric positive definite matrix can serve as a preconditioner to\nsignificantly accelerate the optimization process. Experimental results\ndemonstrate that this method has significant advantages in eliminating sliver\nelements and improving mesh quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01762v3", "cate": "math.NA", "date": "2025-07-02", "updated": "2025-08-01"}
{"id": "2508.00010", "title": "Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?", "authors": ["Ruibo Wang", "Baha Eddine Youcef Belmekki", "Howard H. Yang", "Mohamed Slim Alouini"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00010v1", "summary": "With the explosive deployment of non-terrestrial networks (NTNs), the\ncomputational complexity of network performance analysis is rapidly escalating.\nAs one of the most suitable mathematical tools for analyzing large-scale\nnetwork topologies, stochastic geometry (SG) enables the representation of\nnetwork performance metrics as functions of network parameters, thus offering\nlow-complexity performance analysis solutions. However, choosing between planar\nand spherical models remains challenging. Planar models neglect Earth's\ncurvature, causing deviations in high-altitude NTN analysis, yet are still\noften used for simplicity. This paper introduces relative error to quantify the\ngap between planar and spherical models, helping determine when planar modeling\nis sufficient. To calculate the relative error, we first propose a point\nprocess (PP) generation algorithm that simultaneously generates a pair of\nhomogeneous and asymptotically similar planar and spherical PPs. We then\nintroduce several typical similarity metrics, including topology-related and\nnetwork-level metrics, and further develop a relative error estimation\nalgorithm based on these metrics. In addition, we derive an analytical\nexpression for the optimal planar altitude, which reduces computational\ncomplexity and provides theoretical support for planar approximation. Finally,\nnumerical results investigate how deployment altitude and region affect NTN\nmodeling, with case studies on HAP and LEO satellite constellations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00010v1", "cate": "cs.NI", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2507.00043", "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00043v2", "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00043v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-08-01"}
{"id": "2410.12781", "title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats", "authors": ["Chen Ziwen", "Hao Tan", "Kai Zhang", "Sai Bi", "Fujun Luan", "Yicong Hong", "Li Fuxin", "Zexiang Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.12781v2", "summary": "We propose Long-LRM, a feed-forward 3D Gaussian reconstruction model for\ninstant, high-resolution, 360{\\deg} wide-coverage, scene-level reconstruction.\nSpecifically, it takes in 32 input images at a resolution of 960x540 and\nproduces the Gaussian reconstruction in just 1 second on a single A100 GPU. To\nhandle the long sequence of 250K tokens brought by the large input size,\nLong-LRM features a mixture of the recent Mamba2 blocks and the classical\ntransformer blocks, enhanced by a light-weight token merging module and\nGaussian pruning steps that balance between quality and efficiency. We evaluate\nLong-LRM on the large-scale DL3DV benchmark and Tanks&Temples, demonstrating\nreconstruction quality comparable to the optimization-based methods while\nachieving an 800x speedup w.r.t. the optimization-based approaches and an input\nsize at least 60x larger than the previous feed-forward approaches. We conduct\nextensive ablation studies on our model design choices for both rendering\nquality and computation efficiency. We also explore Long-LRM's compatibility\nwith other Gaussian variants such as 2D GS, which enhances Long-LRM's ability\nin geometry reconstruction. Project page:\nhttps://arthurhero.github.io/projects/llrm", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.12781v2", "cate": "cs.CV", "date": "2024-10-16", "updated": "2025-08-01"}
{"id": "2508.00586", "title": "The Role of Active Learning in Modern Machine Learning", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00586v1", "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00586v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22850", "title": "Dynamic analysis of free-free Timoshenko beams on elastic foundation under transverse transient ground deformation", "authors": ["Gersena Banushi"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      Key words: buried Timoshenko beam, semi-analytical model, dynamic amplification, transient ground deformation (TGD), modal analysis", "url": "http://arxiv.org/abs/2507.22850v2", "summary": "Underground infrastructure, such as pipelines and tunnels, can be vulnerable\nto the effect of transient ground deformation (TGD) caused by different\nvibration sources, including earthquakes and traffic loads. Current design\nmethods are based on simple analytical models that idealize the soil movement\nas a traveling sinusoidal wave, neglecting both the system inertia and the\nrelative displacement at the soil-structure interface. However, this assumption\nmay not be valid for buried large diameter pipelines and tunnels requiring\naccurate dynamic analysis. To analyse the dynamic response of a buried straight\nbeam subjected to transverse TGD, this study introduces a new semi-analytical\nmodel based on the Timoshenko beam on Winkler foundation theory. The\nclosed-form analytical solution revealed that the vibration spectrum is divided\nin four parts, separated by three transition frequencies. Across each\ntransition frequency, the oscillatory characteristics of the vibration modes\nchange, significantly affecting the dynamic response of the system. To verify\nthe validity of the proposed model, this work analyses the case study of a\nburied steel water pipeline of varying lengths and operating conditions,\nsubjected to transverse TGD. Comparison of the obtained analytical solutions\nwith the finite element analysis results showed excellent agreement between the\ntwo approaches. The frequency response analysis revealed dynamic amplification\nof the soil-structure interaction for forcing frequencies near the system's\nfundamental frequency. These may fall within the range of dominant frequencies\ncharacterizing seismic waves, requiring accurate dynamic analysis. The proposed\nmethodology provides a robust analytical framework for evaluating the primary\nfactors impacting the dynamic behavior of buried beams, giving a deeper\nunderstanding of the system response under various sources of ground vibration.", "comment": "Key words: buried Timoshenko beam, semi-analytical model, dynamic\n  amplification, transient ground deformation (TGD), modal analysis", "pdf_url": "http://arxiv.org/pdf/2507.22850v2", "cate": "math.NA", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2508.00020", "title": "Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach", "authors": ["Ferdaous Tarhouni", "Ruibo Wang", "Mohamed-Slim Alouini"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00020v1", "summary": "In recent years, the satellite-aerial-ground integrated network (SAGIN) has\nbecome essential in meeting the increasing demands for global wireless\ncommunications. In SAGIN, high-altitude platforms (HAPs) can serve as\ncommunication hubs and act as relays to enhance communication performance. In\nthis paper, we evaluate network performance and analyze the role of HAPs in\nSAGIN from the relay perspective. Based on this unique perspective, we\nintroduce three metrics to evaluate the performance, named the average access\ndata rate, the average backhaul data rate, and the backhaul rate exceedance\nprobability (BREP). Considering the need for dynamic topology and interference\nanalysis, we choose spherical stochastic geometry (SSG) as a tool and derive\nanalytical expressions for the above metrics to achieve low-complexity\nperformance evaluation. Specifically, we provide a closed-form expression for\nthe end-to-end performance metric BREP. Given that there is no existing\nliterature in the SSG field studying networks from a relay perspective, we\nspecifically investigate the impact of satellite network topology on\nperformance in our numerical results to further highlight the advantages of the\nSSG framework. Additionally, we analyze the minimum HAP transmission power\nrequired to maintain both short-term and long-term data rate demands.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00020v1", "cate": "cs.NI", "date": "2025-07-27", "updated": "2025-07-27"}
{"id": "2507.00225", "title": "Discovering the underlying analytic structure within Standard Model constants using artificial intelligence", "authors": ["S. V. Chekanov", "H. Kjellerstrand"], "categories": ["hep-ph", "cs.AI", "physics.data-an"], "primary_category": "Subjects:       High Energy Physics - Phenomenology (hep-ph)", "pdf_link": null, "comments": "Comments:      16 pages, 5 tables", "url": "http://arxiv.org/abs/2507.00225v2", "summary": "This paper presents a search for underlying analytic structures among the\nfundamental parameters of the Standard Model (SM) using symbolic regression and\ngenetic programming. We identify the simplest analytic relationships connecting\npairs of these constants and report several notable expressions obtained with\nrelative precision better than 1%. These results may serve as valuable inputs\nfor model builders and artificial intelligence methods aimed at uncovering\nhidden patterns among the SM constants, or potentially used as building blocks\nfor a deeper underlying law that connects all parameters of the SM through a\nsmall set of fundamental constants.", "comment": "16 pages, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.00225v2", "cate": "hep-ph", "date": "2025-06-30", "updated": "2025-08-01"}
{"id": "2411.03260", "title": "ShadowMamba: State-Space Model with Boundary-Region Selective Scan for Shadow Removal", "authors": ["Xiujin Zhu", "Chee-Onn Chow", "Joon Huang Chuah"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.03260v3", "summary": "Image shadow removal is a common low-level vision problem. Shadows cause\nsudden brightness changes in some areas, which can affect the accuracy of\ndownstream tasks. Currently, Transformer-based shadow removal methods improve\ncomputational efficiency by using a window mechanism. However, this approach\nreduces the effective receptive field and weakens the ability to model\nlong-range dependencies in shadow images. Recently, Mamba has achieved\nsignificant success in computer vision by modeling long-sequence information\nglobally with linear complexity. However, when applied to shadow removal, its\noriginal scanning mechanism overlooks the semantic continuity along shadow\nboundaries, and the coherence within each region. To solve this issue, we\npropose a new boundary-region selective scanning mechanism that scans shadow,\nboundary, and non-shadow regions separately, making pixels of the same type\ncloser in the sequence. This increases semantic continuity and helps the model\nunderstand local details better. Incorporating this idea, we design the first\nMamba-based lightweight shadow removal model, called ShadowMamba. It uses a\nhierarchical combination U-Net structure, which effectively reduces the number\nof parameters and computational complexity. Shallow layers rely on our\nboundary-region selective scanning to capture local details, while deeper\nlayers use global cross-scanning to learn global brightness features. Extensive\nexperiments show that ShadowMamba outperforms current state-of-the-art models\non ISTD+, ISTD, and SRD datasets, and it also requires fewer parameters and\nless computational cost. (Code will be made available upon paper acceptance.)", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.03260v3", "cate": "cs.CV", "date": "2024-11-05", "updated": "2025-08-01"}
{"id": "2508.00627", "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime Réjou Méchain"], "categories": ["cs.LG", "I.4.9; I.4.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures", "url": "http://arxiv.org/abs/2508.00627v1", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2508.00627v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2404.00523", "title": "The algebraic structure of hyperinterpolation class on the sphere", "authors": ["Congpei An", "Jiashu Ran"], "categories": ["math.FA", "cs.NA", "math.NA", "41A10, 41A36, 47L20, 47L80"], "primary_category": "Subjects:       Functional Analysis (math.FA)", "pdf_link": null, "comments": "Comments:      16 pages, 1 figure", "url": "http://arxiv.org/abs/2404.00523v2", "summary": "This paper investigates the algebraic properties of the hyperinterpolation\nclass $\\mathbf{HC}(\\mathbb{S}^d)$ on the unit sphere $ \\mathbb{S}^d $. We focus\non operators derived from the classical hyperinterpolation with bounded $ L_2 $\noperator norms. By utilizing a discrete (semi) inner product framework, we\ndevelop the theory of hyper self-adjoint operators, hyper projection operators,\nand hyper semigroups. We analyze four specific operators: filtered, Lasso, hard\nthresholding, and generalized hyperinterpolations. We prove that the\ngeneralized hyperinterpolation operator is hyper self-adjoint and commutative\nwith the hyperinterpolation operator. Additionally, we demonstrate that hard\nthresholding and classical hyperinterpolation operators form a hyper semigroup,\nwith hard thresholding hyperinterpolation constituting the minimal prime hyper\nideal. Finally, we establish that hyperinterpolation operators act as hyper\nhomomorphisms on the hyper semigroup.", "comment": "16 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2404.00523v2", "cate": "math.FA", "date": "2024-03-31", "updated": "2025-08-01"}
{"id": "2508.00042", "title": "Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network", "authors": ["Athanasios Tziouvaras", "Carolina Fortuna", "George Floros", "Kostas Kolomvatsos", "Panagiotis Sarigiannidis", "Marko Grobelnik", "Blaž Bertalanič"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      10 pages, 12 figures", "url": "http://arxiv.org/abs/2508.00042v1", "summary": "AI-native 6G networks promise unprecedented automation and performance by\nembedding machine-learning models throughout the radio access and core segments\nof the network. However, the non-stationary nature of wireless environments due\nto infrastructure changes, user mobility, and emerging traffic patterns,\ninduces concept drifts that can quickly degrade these model accuracies.\nExisting methods in general are very domain specific, or struggle with certain\ntype of concept drift. In this paper, we introduce two unsupervised,\nmodel-agnostic, batch concept drift detectors. Both methods compute an\nexpected-utility score to decide when concept drift occurred and if model\nretraining is warranted, without requiring ground-truth labels after\ndeployment. We validate our framework on two real-world wireless use cases in\noutdoor fingerprinting for localization and for link-anomaly detection, and\ndemonstrate that both methods are outperforming classical detectors such as\nADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an\nF1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus\nreducing the false alarm rate by up to 20 percentage points compared to the\nbest classical detectors.", "comment": "10 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2508.00042v1", "cate": "cs.NI", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.03703", "title": "Sign Spotting Disambiguation using Large Language Models", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in the international conference on Intelligent Virtual Agents (IVA Adjunct)", "url": "http://arxiv.org/abs/2507.03703v2", "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.", "comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)", "pdf_url": "http://arxiv.org/pdf/2507.03703v2", "cate": "cs.CV", "date": "2025-07-04", "updated": "2025-08-01"}
{"id": "2411.10086", "title": "CorrCLIP: Reconstructing Patch Correlations in CLIP for Open-Vocabulary Semantic Segmentation", "authors": ["Dengke Zhang", "Fagui Liu", "Quan Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025 Oral", "url": "http://arxiv.org/abs/2411.10086v3", "summary": "Open-vocabulary semantic segmentation aims to assign semantic labels to each\npixel without being constrained by a predefined set of categories. While\nContrastive Language-Image Pre-training (CLIP) excels in zero-shot\nclassification, it struggles to align image patches with category embeddings\nbecause of its incoherent patch correlations. This study reveals that\ninter-class correlations are the main reason for impairing CLIP's segmentation\nperformance. Accordingly, we propose CorrCLIP, which reconstructs the scope and\nvalue of patch correlations. Specifically, CorrCLIP leverages the Segment\nAnything Model (SAM) to define the scope of patch interactions, reducing\ninter-class correlations. To mitigate the problem that SAM-generated masks may\ncontain patches belonging to different classes, CorrCLIP incorporates\nself-supervised models to compute coherent similarity values, suppressing the\nweight of inter-class correlations. Additionally, we introduce two additional\nbranches to strengthen patch features' spatial details and semantic\nrepresentation. Finally, we update segmentation maps with SAM-generated masks\nto improve spatial consistency. Based on the improvement across patch\ncorrelations, feature representations, and segmentation maps, CorrCLIP achieves\nsuperior performance across eight benchmarks. Codes are available at:\nhttps://github.com/zdk258/CorrCLIP.", "comment": "Accepted to ICCV 2025 Oral", "pdf_url": "http://arxiv.org/pdf/2411.10086v3", "cate": "cs.CV", "date": "2024-11-15", "updated": "2025-08-01"}
{"id": "2508.00628", "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00628v1", "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00628v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2405.07961", "title": "A potential theory on weighted graphs", "authors": ["Trent DeGiovanni", "Fernando Guevara Vasquez"], "categories": ["math.PR", "cs.NA", "math.NA", "31C20, 65M80, 31B10"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures", "url": "http://arxiv.org/abs/2405.07961v2", "summary": "We present an analog to classic potential theory on weighted graphs. With\nnodes partitioned into exterior, boundary and interior nodes and an appropriate\ndecomposition of the Laplacian, we define discrete analogues to the trace\noperators, the single and double layer potential operators, and the boundary\nlayer operators. As in the continuum, these operators can represent exterior or\ninterior harmonic functions with different boundary conditions. The formalism\nwe introduce includes a discrete Calder\\'on calculus and brings some well known\nresults from potential theory to weighted graphs, e.g. on the spectrum of the\nNeumann-Poincar\\'e operator. We illustrate the formalism with a cloaking\nstrategy on weighted graphs which allows to hide an anomaly from the\nperspective of electrical measurements made away from the anomaly.", "comment": "30 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2405.07961v2", "cate": "math.PR", "date": "2024-05-13", "updated": "2025-08-01"}
{"id": "2508.00228", "title": "Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies", "authors": ["Aashay Arora", "Diego Davila", "Frank Würthwein", "John Graham", "Dima Mishin", "Justas Balcas", "Tom Lehman", "Xi Yang", "Chin Guok", "Harvey Newman"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to CHEP 24", "url": "http://arxiv.org/abs/2508.00228v1", "summary": "In anticipation of the High Luminosity-LHC era, there is a critical need to\noversee software readiness for upcoming growth in network traffic for\nproduction and user data analysis access. This paper looks into software and\nhardware required improvements in US-CMS Tier-2 sites to be able to sustain and\nmeet the projected 400 Gbps bandwidth demands while tackling the challenge\nposed by varying latencies between sites. Specifically, our study focuses on\nidentifying the performance of XRootD HTTP third-party copies across multiple\n400 Gbps links and exploring different host and transfer configurations. Our\napproach involves systematic testing with variations in the number of origins\nper cluster and CPU allocations for each origin. By replicating real network\nconditions and creating network \"loops\" that traverse multiple switches across\nthe wide area network, we are able to replicate authentic network conditions", "comment": "Submitted to CHEP 24", "pdf_url": "http://arxiv.org/pdf/2508.00228v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.05416", "title": "EmissionNet: Air Quality Pollution Forecasting for Agriculture", "authors": ["Prady Saligram", "Tanvir Bhathal"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The appendix figures are mixed up - several emission plots (e.g. CO2, CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion in interpreting the results", "url": "http://arxiv.org/abs/2507.05416v3", "summary": "Air pollution from agricultural emissions is a significant yet often\noverlooked contributor to environmental and public health challenges.\nTraditional air quality forecasting models rely on physics-based approaches,\nwhich struggle to capture complex, nonlinear pollutant interactions. In this\nwork, we explore forecasting N$_2$O agricultural emissions through evaluating\npopular architectures, and proposing two novel deep learning architectures,\nEmissionNet (ENV) and EmissionNet-Transformer (ENT). These models leverage\nconvolutional and transformer-based architectures to extract spatial-temporal\ndependencies from high-resolution emissions data", "comment": "The appendix figures are mixed up - several emission plots (e.g. CO2,\n  CH4, GWP) are mislabeled and appear in the wrong order, leading to confusion\n  in interpreting the results", "pdf_url": "http://arxiv.org/pdf/2507.05416v3", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-08-01"}
{"id": "2411.15867", "title": "PanoLlama: Generating Endless and Coherent Panoramas with Next-Token-Prediction LLMs", "authors": ["Teng Zhou", "Xiaoyu Zhang", "Yongchuan Tang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.15867v3", "summary": "Panoramic Image Generation (PIG) aims to create coherent images of arbitrary\nlengths. Most existing methods fall in the joint diffusion paradigm, but their\ncomplex and heuristic crop connection designs often limit their ability to\nachieve multilevel coherence. By deconstructing this challenge into its core\ncomponents, we find it naturally aligns with next-token prediction, leading us\nto adopt an autoregressive (AR) paradigm for PIG modeling. However, existing\nvisual AR (VAR) models are limited to fixed-size generation, lacking the\ncapability to produce panoramic images. In this paper, we propose PanoLlama, a\nnovel framework that achieves endless and coherent panorama generation with the\nautoregressive paradigm. Our approach develops a training-free strategy that\nutilizes token redirection to overcome the size limitations of existing VAR\nmodels, enabling next-crop prediction in both horizontal and vertical\ndirections. This refreshes the PIG pipeline while achieving SOTA performance in\ncoherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally,\nPanoLlama supports applications other PIG methods cannot achieve, including\nmask-free layout control, multi-scale and multi-guidance synthesis. To\nfacilitate standardized evaluation, we also establish a dataset with 1,000\nprompts spanning 100+ themes, providing a new testing benchmark for PIG\nresearch. The code is available at https://github.com/0606zt/PanoLlama.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.15867v3", "cate": "cs.CV", "date": "2024-11-24", "updated": "2025-08-01"}
{"id": "2508.00635", "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00635v1", "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00635v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.19974", "title": "Adaptive Mesh Refinement for Two-Phase Viscoelastic Fluid Mixture Models", "authors": ["Bindi M. Nagda", "Aaron Barrett", "Boyce E. Griffith", "Aaron L. Fogelson", "Jian Du"], "categories": ["physics.flu-dyn", "cs.NA", "math.NA", "76-10, 76T06", "G.1.8; G.4"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.19974v3", "summary": "Multiphase flows are an important class of fluid flow and their study\nfacilitates the development of diverse applications in industrial, natural, and\nbiomedical systems. We consider a model that uses a continuum description of\nboth phases in which separate momentum equations are used for each phase along\nwith a co-incompressibility condition on the velocity fields. The resulting\nsystem of equations poses numerical challenges due to the presence of multiple\nnon-linear terms and the co-incompressibility condition, and the resulting\nfluid dynamics motivate the development of an adaptive mesh refinement (AMR)\ntechnique to accurately capture regions of high stresses and large material\ngradients while keeping computational costs low. We present an accurate,\nrobust, and efficient computational method for simulating multiphase mixtures\non adaptive grids, and utilize a multigrid solver to precondition the\nsaddle-point system. We demonstrate that the AMR discretization asymptotically\napproaches second order accuracy in $L^1$, $L^2$ and $L^\\infty$ norms. The\nsolver can accurately resolve sharp gradients in the solution and, with the\nmultigrid preconditioning strategy introduced herein, the linear solver\niterations are independent of grid spacing. Our AMR solver offers a major cost\nsavings benefit, providing up to ten fold speedup over a uniform grid in the\nnumerical experiments presented here, with greater speedup possible depending\non the problem set-up.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.19974v3", "cate": "physics.flu-dyn", "date": "2024-09-30", "updated": "2025-08-01"}
{"id": "2508.00261", "title": "Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning", "authors": ["Saichao Liu", "Geng Sun", "Chuang Zhang", "Xuejie Liu", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2508.00261v1", "summary": "Mobile edge computing (MEC) is a promising technique to improve the\ncomputational capacity of smart devices (SDs) in Internet of Things (IoT).\nHowever, the performance of MEC is restricted due to its fixed location and\nlimited service scope. Hence, we investigate an unmanned aerial vehicle\n(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can\nsimultaneously provide computing service for multiple SDs. To improve the\nperformance of system, we formulated a UAV-based trajectory control and\nresource allocation multi-objective optimization problem (TCRAMOP) to\nsimultaneously maximize the offloading number of UAVs and minimize total\noffloading delay and total energy consumption of UAVs by optimizing the flight\npaths of UAVs as well as the computing resource allocated to served SDs. Then,\nconsider that the solution of TCRAMOP requires continuous decision-making and\nthe system is dynamic, we propose an enhanced deep reinforcement learning (DRL)\nalgorithm, namely, distributed proximal policy optimization with imitation\nlearning (DPPOIL). This algorithm incorporates the generative adversarial\nimitation learning technique to improve the policy performance. Simulation\nresults demonstrate the effectiveness of our proposed DPPOIL and prove that the\nlearned strategy of DPPOIL is better compared with other baseline methods.", "comment": "This paper has been accepted by IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00261v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.10179", "title": "The Second Machine Turn: From Checking Proofs to Creating Concepts", "authors": ["Asvin G"], "categories": ["math.HO", "cs.AI"], "primary_category": "Subjects:       History and Overview (math.HO)", "pdf_link": null, "comments": "Comments:      Minor clarifications in the final section", "url": "http://arxiv.org/abs/2507.10179v2", "summary": "We identify a second machine turn in the process of mathematical discovery:\nafter automating proof-checking, AI is now poised to automate the *creation* of\nmathematical concepts themselves. We discuss the current state of the art,\nobstacles and potential solutions as well as a preliminary attempt at\nmathematizing the creation of concepts itself. The paper ends with an\nassessment of how these capabilities could reshape mathematics and\nhuman-machine collaboration, and a few different futures we might find\nourselves in.", "comment": "Minor clarifications in the final section", "pdf_url": "http://arxiv.org/pdf/2507.10179v2", "cate": "math.HO", "date": "2025-07-14", "updated": "2025-08-01"}
{"id": "2411.16719", "title": "Learn2Synth: Learning Optimal Data Synthesis Using Hypergradients for Brain Image Segmentation", "authors": ["Xiaoling Hu", "Xiangrui Zeng", "Oula Puonti", "Juan Eugenio Iglesias", "Bruce Fischl", "Yael Balbastre"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures. Accepted by ICCV'25", "url": "http://arxiv.org/abs/2411.16719v3", "summary": "Domain randomization through synthesis is a powerful strategy to train\nnetworks that are unbiased with respect to the domain of the input images.\nRandomization allows networks to see a virtually infinite range of intensities\nand artifacts during training, thereby minimizing overfitting to appearance and\nmaximizing generalization to unseen data. Although powerful, this approach\nrelies on the accurate tuning of a large set of hyperparameters that govern the\nprobabilistic distribution of the synthesized images. Instead of manually\ntuning these parameters, we introduce Learn2Synth, a novel procedure in which\nsynthesis parameters are learned using a small set of real labeled data. Unlike\nmethods that impose constraints to align synthetic data with real data (e.g.,\ncontrastive or adversarial techniques), which risk misaligning the image and\nits label map, we tune an augmentation engine such that a segmentation network\ntrained on synthetic data has optimal accuracy when applied to real data. This\napproach allows the training procedure to benefit from real labeled examples,\nwithout ever using these real examples to train the segmentation network, which\navoids biasing the network towards the properties of the training set.\nSpecifically, we develop parametric and nonparametric strategies to enhance\nsynthetic images in a way that improves the performance of the segmentation\nnetwork. We demonstrate the effectiveness of this learning strategy on\nsynthetic and real-world brain scans. Code is available at:\nhttps://github.com/HuXiaoling/Learn2Synth.", "comment": "16 pages, 5 figures. Accepted by ICCV'25", "pdf_url": "http://arxiv.org/pdf/2411.16719v3", "cate": "cs.CV", "date": "2024-11-23", "updated": "2025-08-01"}
{"id": "2508.00641", "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "authors": ["Alessandro Palmas"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 10 figures", "url": "http://arxiv.org/abs/2508.00641v1", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "comment": "11 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2508.00641v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.00288", "title": "Nyström Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics", "authors": ["Tri P. Nguyen", "Ilon Joseph", "Mayya Tokman"], "categories": ["physics.comp-ph", "cs.NA", "math.NA", "physics.plasm-ph", "65L04, 78A35"], "primary_category": "Subjects:       Computational Physics (physics.comp-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00288v5", "summary": "Calculating the dynamics of charged particles in electromagnetic fields (i.e.\nthe particle pushing problem) is one of the most computationally intensive\ncomponents of particle-in-cell (PIC) methods for plasma physics simulations.\nThis task is especially challenging when the plasma is strongly magnetized,\nsince in this case the particle motion consists of a wide range of temporal\nscales from highly oscillatory fast gyromotion to slow macroscopic behavior and\nthe resulting numerical model is very stiff. Current state-of-the-art time\nintegrators used to simulate particle motion have limitations given the severe\nnumerical stiffness of the problem and more efficient methods are of interest.\nRecently, exponential integrators have been proposed as a promising new\napproach for these simulations and shown to offer computational advantages over\ncommonly used schemes. Exponential methods can solve linear problems exactly\nand are A-stable. In this paper, the standard exponential algorithms framework\nis extended to derive Nystr\\\"om-type exponential methods that integrate the\nNewtonian equations of motion as a second-order differential equation. Specific\nNystr\\\"om-type schemes of second and third orders are derived and applied to\nstrongly magnetized particle pushing problems. Numerical experiments are\npresented to demonstrate that the Nystr\\\"om-type exponential integrators can\nprovide significant improvement in computational efficiency over the standard\nexponential methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00288v5", "cate": "physics.comp-ph", "date": "2025-05-01", "updated": "2025-08-01"}
{"id": "2508.00403", "title": "Mamba for Wireless Communications and Networking: Principles and Opportunities", "authors": ["Rongsheng Zhang", "Ruichen Zhang", "Yang Lu", "Wei Chen", "Bo Ai", "Dusit Niyato"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00403v1", "summary": "Mamba has emerged as a powerful model for efficiently addressing tasks\ninvolving temporal and spatial data. Regarding the escalating heterogeneity and\ndynamics in wireless networks, Mamba holds the potential to revolutionize\nwireless communication and networking designs by balancing the trade-off\nbetween computational efficiency and effectiveness. This article presents a\ncomprehensive overview of Mamba' applications in wireless systems.\nSpecifically, we first analyze the potentials of Mamba for wireless signal\nprocessing tasks from the perspectives of long-range dependency modeling and\nspatial feature extraction. Then we propose two application frameworks for\nMamba in wireless communications, i.e., replacement of traditional algorithms,\nand enabler of novel paradigms. Guided by the two frameworks, we conduct case\nstudies on intelligent resource allocation and joint source and channel\ndecoding to demonstrate Mamba's improvements in both feature enhancement and\ncomputational efficiency. Finally, we highlight critical challenges and outline\npotential research directions for Mamba in wireless communications and\nnetworking.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00403v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.10546", "title": "Disentangling Neural Disjunctive Normal Form Models", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at NeSy 2025", "url": "http://arxiv.org/abs/2507.10546v2", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "comment": "Accepted at NeSy 2025", "pdf_url": "http://arxiv.org/pdf/2507.10546v2", "cate": "cs.LG", "date": "2025-07-14", "updated": "2025-08-01"}
{"id": "2412.01562", "title": "Detection, Pose Estimation and Segmentation for Multiple Bodies: Closing the Virtuous Circle", "authors": ["Miroslav Purkrabek", "Jiri Matas"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Website: this https URL", "url": "http://arxiv.org/abs/2412.01562v3", "summary": "Human pose estimation methods work well on isolated people but struggle with\nmultiple-bodies-in-proximity scenarios. Previous work has addressed this\nproblem by conditioning pose estimation by detected bounding boxes or\nkeypoints, but overlooked instance masks. We propose to iteratively enforce\nmutual consistency of bounding boxes, instance masks, and poses. The introduced\nBBox-Mask-Pose (BMP) method uses three specialized models that improve each\nother's output in a closed loop. All models are adapted for mutual\nconditioning, which improves robustness in multi-body scenes. MaskPose, a new\nmask-conditioned pose estimation model, is the best among top-down approaches\non OCHuman. BBox-Mask-Pose pushes SOTA on OCHuman dataset in all three tasks -\ndetection, instance segmentation, and pose estimation. It also achieves SOTA\nperformance on COCO pose estimation. The method is especially good in scenes\nwith large instances overlap, where it improves detection by 39% over the\nbaseline detector. With small specialized models and faster runtime, BMP is an\neffective alternative to large human-centered foundational models. Code and\nmodels are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose.", "comment": "Project Website: https://mirapurkrabek.github.io/BBox-Mask-Pose", "pdf_url": "http://arxiv.org/pdf/2412.01562v3", "cate": "cs.CV", "date": "2024-12-02", "updated": "2025-08-01"}
{"id": "2508.00643", "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00643v1", "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00643v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.22480", "title": "Hybrid High-Order formulations with turbulence modelling capabilities for incompressible flow problems", "authors": ["Lorenzo Botti", "Daniele Antonio Di Pietro", "Francesco Carlo Massa"], "categories": ["physics.flu-dyn", "cs.NA", "math.NA"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.22480v2", "summary": "We propose a Hybrid High-Order (HHO) formulation of the incompressible\nNavier--Stokes equations, that is well suited to be employed for the simulation\nof turbulent flows. The spatial discretization relies on hybrid velocity and\npressure spaces and the temporal discretization is based on Explicit Singly\nDiagonal Implicit Runge-Kutta (ESDIRK) methods. The formulation possesses some\nattractive features that can be fruitfully exploited when high-fidelity\ncomputations are required, namely: pressure-robustness, conservation of mass\nenforced cell-by-cell up to machine precision, robustness in the inviscid\nlimit, implicit high-order accurate time stepping with local time step\nadaptation, reduced memory footprint thanks to static condensation of both\nvelocity and pressure, possibility to exploit inherited $p$-multilevel solution\nstrategies to improve performance of iterative solvers. After demonstrating the\nrelevant properties of the scheme in practice, performing challenging 2D and 3D\ntest cases, we consider the simulation of the Taylor--Green Vortex flow problem\nat Reynolds 1600.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.22480v2", "cate": "physics.flu-dyn", "date": "2025-05-28", "updated": "2025-07-31"}
{"id": "2508.00583", "title": "Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications", "authors": ["Yunting Xu", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Deepu Rajan", "Liang Yu", "Haibo Zhou", "Abbas Jamalipour", "Xianbin Wang"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures", "url": "http://arxiv.org/abs/2508.00583v1", "summary": "Large vision models (LVMs) have emerged as a foundational paradigm in visual\nintelligence, achieving state-of-the-art performance across diverse visual\ntasks. Recent advances in LVMs have facilitated their integration into Internet\nof Things (IoT) scenarios, offering superior generalization and adaptability\nfor vision-assisted network optimization. In this paper, we first investigate\nthe functionalities and core architectures of LVMs, highlighting their\ncapabilities across classification, segmentation, generation, and multimodal\nvisual processing. We then explore a variety of LVM applications in wireless\ncommunications, covering representative tasks across the physical layer,\nnetwork layer, and application layer. Furthermore, given the substantial model\nsize of LVMs and the challenges of model retraining in wireless domains, we\npropose a progressive fine-tuning framework that incrementally adapts\npretrained LVMs for joint optimization of multiple IoT tasks. A case study in\nlow-altitude economy networks (LAENets) demonstrates the effectiveness of the\nproposed framework over conventional CNNs in joint beamforming and positioning\ntasks for Internet of drones, underscoring a promising direction for\nintegrating LVMs into intelligent wireless systems.", "comment": "7 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2508.00583v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00629", "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach", "authors": ["Francisco Crespo", "Javier Villegas", "Carlos Baena", "Eduardo Baena", "Sergio Fortes", "Raquel Barco"], "categories": ["cs.NI", "cs.OS", "cs.PF"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00629v1", "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00629v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.13703", "title": "Binarizing Physics-Inspired GNNs for Combinatorial Optimization", "authors": ["Martin Krutský", "Gustav Šír", "Vyacheslav Kungurtsev", "Georgios Korpas"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the 28th European Conference on Artificial Intelligence (ECAI 2025). This archival version includes supplementary appendices", "url": "http://arxiv.org/abs/2507.13703v2", "summary": "Physics-inspired graph neural networks (PI-GNNs) have been utilized as an\nefficient unsupervised framework for relaxing combinatorial optimization\nproblems encoded through a specific graph structure and loss, reflecting\ndependencies between the problem's variables. While the framework has yielded\npromising results in various combinatorial problems, we show that the\nperformance of PI-GNNs systematically plummets with an increasing density of\nthe combinatorial problem graphs. Our analysis reveals an interesting phase\ntransition in the PI-GNNs' training dynamics, associated with degenerate\nsolutions for the denser problems, highlighting a discrepancy between the\nrelaxed, real-valued model outputs and the binary-valued problem solutions. To\naddress the discrepancy, we propose principled alternatives to the naive\nstrategy used in PI-GNNs by building on insights from fuzzy logic and binarized\nneural networks. Our experiments demonstrate that the portfolio of proposed\nmethods significantly improves the performance of PI-GNNs in increasingly dense\nsettings.", "comment": "Accepted to the 28th European Conference on Artificial Intelligence\n  (ECAI 2025). This archival version includes supplementary appendices", "pdf_url": "http://arxiv.org/pdf/2507.13703v2", "cate": "cs.LG", "date": "2025-07-18", "updated": "2025-08-01"}
{"id": "2412.02837", "title": "$\\texttt{BATCLIP}$: Bimodal Online Test-Time Adaptation for CLIP", "authors": ["Sarthak Kumar Maharana", "Baoming Zhang", "Leonid Karlinsky", "Rogerio Feris", "Yunhui Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2412.02837v3", "summary": "Although open-vocabulary classification models like Contrastive Language\nImage Pretraining (CLIP) have demonstrated strong zero-shot learning\ncapabilities, their robustness to common image corruptions remains poorly\nunderstood. Through extensive experiments, we show that zero-shot CLIP lacks\nrobustness to common image corruptions during test-time, necessitating the\nadaptation of CLIP to unlabeled corrupted images using test-time adaptation\n(TTA). However, we found that existing TTA methods have severe limitations in\nadapting CLIP due to their unimodal nature. To address these limitations, we\npropose $\\texttt{BATCLIP}$, a bimodal $\\textbf{online}$ TTA method designed to\nimprove CLIP's robustness to common image corruptions. The key insight of our\napproach is not only to adapt the visual encoders for improving image features\nbut also to strengthen the alignment between image and text features by\npromoting a stronger association between the image class prototype, computed\nusing pseudo-labels, and the corresponding text feature. We evaluate our\napproach on benchmark image corruption datasets and achieve state-of-the-art\nresults in online TTA for CLIP. Furthermore, we evaluate our proposed TTA\napproach on various domain generalization datasets to demonstrate its\ngeneralization capabilities. Our code is available at\nhttps://github.com/sarthaxxxxx/BATCLIP", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2412.02837v3", "cate": "cs.CV", "date": "2024-12-03", "updated": "2025-07-31"}
{"id": "2508.00657", "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by MLHC 2025", "url": "http://arxiv.org/abs/2508.00657v1", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "comment": "Accepted by MLHC 2025", "pdf_url": "http://arxiv.org/pdf/2508.00657v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00616", "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications", "authors": ["Mingzhe Fan", "Geng Sun", "Hongyang Pan", "Jiacheng Wang", "Jiancheng An", "Hongyang Du", "Chau Yuen"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This papar has been submitted to the IEEE Global Communications Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "url": "http://arxiv.org/abs/2508.00616v1", "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.", "comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488", "pdf_url": "http://arxiv.org/pdf/2508.00616v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.13970", "title": "A Segmented Robot Grasping Perception Neural Network for Edge AI", "authors": ["Casper Bröcheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico Möckel"], "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by SMC 2025", "url": "http://arxiv.org/abs/2507.13970v2", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "comment": "Accepted by SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.13970v2", "cate": "cs.RO", "date": "2025-07-18", "updated": "2025-08-01"}
{"id": "2412.05101", "title": "The Silent Assistant: NoiseQuery as Implicit Guidance for Goal-Driven Image Generation", "authors": ["Ruoyu Wang", "Huayang Huang", "Ye Zhu", "Olga Russakovsky", "Yu Wu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight", "url": "http://arxiv.org/abs/2412.05101v3", "summary": "In this work, we introduce NoiseQuery as a novel method for enhanced noise\ninitialization in versatile goal-driven text-to-image (T2I) generation.\nSpecifically, we propose to leverage an aligned Gaussian noise as implicit\nguidance to complement explicit user-defined inputs, such as text prompts, for\nbetter generation quality and controllability. Unlike existing noise\noptimization methods designed for specific models, our approach is grounded in\na fundamental examination of the generic finite-step noise scheduler design in\ndiffusion formulation, allowing better generalization across different\ndiffusion-based architectures in a tuning-free manner. This model-agnostic\nnature allows us to construct a reusable noise library compatible with multiple\nT2I models and enhancement techniques, serving as a foundational layer for more\neffective generation. Extensive experiments demonstrate that NoiseQuery enables\nfine-grained control and yields significant performance boosts not only over\nhigh-level semantics but also over low-level visual attributes, which are\ntypically difficult to specify through text alone, with seamless integration\ninto current workflows with minimal computational overhead.", "comment": "ICCV 2025 Highlight", "pdf_url": "http://arxiv.org/pdf/2412.05101v3", "cate": "cs.CV", "date": "2024-12-06", "updated": "2025-08-01"}
{"id": "2508.00664", "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00664v1", "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00664v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00688", "title": "Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience", "authors": ["Ruiyang Huang", "Haocheng Wang", "Yixuan Shen", "Ning Gao", "Qiang Ni", "Shi Jin", "Yifan Wu"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submit to INFOCOM 2026", "url": "http://arxiv.org/abs/2508.00688v1", "summary": "Heterogeneous marine-aerial swarm networks encounter substantial difficulties\ndue to targeted communication disruptions and structural weaknesses in\nadversarial environments. This paper proposes a two-step framework to\nstrengthen the network's resilience. Specifically, our framework combines the\nnode prioritization based on criticality with multi-objective topology\noptimization. First, we design a three-layer architecture to represent\nstructural, communication, and task dependencies of the swarm networks. Then,\nwe introduce the SurBi-Ranking method, which utilizes graph convolutional\nnetworks, to dynamically evaluate and rank the criticality of nodes and edges\nin real time. Next, we apply the NSGA-III algorithm to optimize the network\ntopology, aiming to balance communication efficiency, global connectivity, and\nmission success rate. Experiments demonstrate that compared to traditional\nmethods like K-Shell, our SurBi-Ranking method identifies critical nodes and\nedges with greater accuracy, as deliberate attacks on these components cause\nmore significant connectivity degradation. Furthermore, our optimization\napproach, when prioritizing SurBi-Ranked critical components under attack,\nreduces the natural connectivity degradation by around 30%, achieves higher\nmission success rates, and incurs lower communication reconfiguration costs,\nensuring sustained connectivity and mission effectiveness across multi-phase\noperations.", "comment": "Submit to INFOCOM 2026", "pdf_url": "http://arxiv.org/pdf/2508.00688v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.17224", "title": "HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Recordings", "authors": ["Feng Cao", "Zishuo Feng", "Wei Shi", "Jicong Zhang"], "categories": ["eess.SP", "cs.AI", "q-bio.NC"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures, 6 tables", "url": "http://arxiv.org/abs/2507.17224v2", "summary": "Extracellular recordings are transient voltage fluctuations in the vicinity\nof neurons, serving as a fundamental modality in neuroscience for decoding\nbrain activity at single-neuron resolution. Spike sorting, the process of\nattributing each detected spike to its corresponding neuron, is a pivotal step\nin brain sensing pipelines. However, it remains challenging under low\nsignal-to-noise ratio (SNR), electrode drift, and cross-session variability. In\nthis paper, we propose HuiduRep, a robust self-supervised representation\nlearning framework that extracts discriminative and generalizable features from\nextracellular recordings. By integrating contrastive learning with a denoising\nautoencoder, HuiduRep learns latent representations robust to noise and drift.\nWith HuiduRep, we develop a spike sorting pipeline that clusters spike\nrepresentations without ground truth labels. Experiments on hybrid and\nreal-world datasets demonstrate that HuiduRep achieves strong robustness.\nFurthermore, the pipeline significantly outperforms state-of-the-art tools such\nas KiloSort4 and MountainSort5 on accuracy and precision on diverse datasets.\nThese findings demonstrate the potential of self-supervised spike\nrepresentation learning as a foundational tool for robust and generalizable\nprocessing of extracellular recordings. Code is available at:\nhttps://github.com/IgarashiAkatuki/HuiduRep", "comment": "9 pages, 3 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.17224v2", "cate": "eess.SP", "date": "2025-07-23", "updated": "2025-08-01"}
{"id": "2412.13180", "title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration", "authors": ["Mark Endo", "Xiaohan Wang", "Serena Yeung-Levy"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, project page: this https URL", "url": "http://arxiv.org/abs/2412.13180v2", "summary": "Recent works on accelerating Vision-Language Models achieve strong\nperformance across a variety of vision-language tasks despite highly\ncompressing visual information. In this work, we examine the popular\nacceleration approach of early pruning of visual tokens inside the language\nmodel. Surprisingly, we find that while strong performance is maintained across\nmany tasks, it exhibits drastically different behavior for a subset of\nvision-centric tasks such as localization. Upon further investigation, we\nuncover a core issue with the acceleration approach where most tokens towards\nthe top of the image are pruned away. Yet, on many benchmarks aiming to\nevaluate vision-centric capabilities, strong performance persists with the\nflawed pruning strategy, highlighting these benchmarks' limited ability to\nassess fine-grained visual capabilities. Based on these findings, we propose\nFEATHER (Fast and Effective Acceleration wiTH Ensemble cRiteria), a\nstraightforward approach that resolves the discovered early-layer pruning issue\nand further enhances the preservation of relevant tokens via multistage pruning\nwith early uniform sampling to ensure broad image coverage. With comparable\ncomputational savings, we find that FEATHER achieves more than 5x performance\nimprovement on the vision-centric localization benchmarks compared to the\noriginal acceleration approach.", "comment": "ICCV 2025, project page:\n  https://web.stanford.edu/~markendo/projects/feather", "pdf_url": "http://arxiv.org/pdf/2412.13180v2", "cate": "cs.CV", "date": "2024-12-17", "updated": "2025-07-31"}
{"id": "2508.00692", "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "categories": ["cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00692v1", "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00692v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00715", "title": "Deep Joint Source-Channel Coding for Small Satellite Applications", "authors": ["Olga Kondrateva", "Grace Li Zhang", "Julian Zobel", "Björn Scheuermann", "Stefan Dietzel"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00715v1", "summary": "Small satellites used for Earth observation generate vast amounts of\nhigh-dimensional data, but their operation in low Earth orbit creates a\nsignificant communication bottleneck due to limited contact times and harsh,\nvarying channel conditions. While deep joint source-channel coding (DJSCC) has\nemerged as a promising technique, its practical application to the complex\nsatellite environment remains an open question. This paper presents a\ncomprehensive DJSCC framework tailored for satellite communications. We first\nestablish a basic system, DJSCC-SAT, and integrate a realistic, multi-state\nstatistical channel model to guide its training and evaluation. To overcome the\nimpracticality of using separate models for every channel condition, we then\nintroduce an adaptable architecture, ADJSCC-SAT, which leverages attention\nmodules to allow a single neural network to adjust to a wide range of channel\nstates with minimal overhead. Through extensive evaluation on Sentinel-2\nmulti-spectral data, we demonstrate that our adaptable approach achieves\nperformance comparable to using multiple specialized networks while\nsignificantly reducing model storage requirements. Furthermore, the adaptable\nmodel shows enhanced robustness to channel estimation errors, outperforming the\nnon-adaptable baseline. The proposed framework is a practical and efficient\nstep toward deploying robust, adaptive DJSCC systems for real-world satellite\nmissions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00715v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22766", "title": "Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models", "authors": ["Felix Kronenwett", "Georg Maier", "Thomas Längle"], "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at the 30th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)", "url": "http://arxiv.org/abs/2507.22766v2", "summary": "Sensor-based sorting systems enable the physical separation of a material\nstream into two fractions. The sorting decision is based on the image data\nevaluation of the sensors used and is carried out using actuators. Various\nprocess parameters must be set depending on the properties of the material\nstream, the dimensioning of the system, and the required sorting accuracy.\nHowever, continuous verification and re-adjustment are necessary due to\nchanging requirements and material stream compositions. In this paper, we\nintroduce an approach for optimizing, recurrently monitoring and adjusting the\nprocess parameters of a sensor-based sorting system. Based on Bayesian\nOptimization, Gaussian process regression models are used as surrogate models\nto achieve specific requirements for system behavior with the uncertainties\ncontained therein. This method minimizes the number of necessary experiments\nwhile simultaneously considering two possible optimization targets based on the\nrequirements for both material output streams. In addition, uncertainties are\nconsidered during determining sorting accuracies in the model calculation. We\nevaluated the method with three example process parameters.", "comment": "Accepted at the 30th IEEE International Conference on Emerging\n  Technologies and Factory Automation (ETFA)", "pdf_url": "http://arxiv.org/pdf/2507.22766v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2412.20750", "title": "Enhanced Vision-Language Models for Diverse Sensor Understanding: Cost-Efficient Optimization and Benchmarking", "authors": ["Sangyun Chung", "Youngjoon Yu", "Se Yeon Kim", "Youngchae Chee", "Yong Man Ro"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.20750v2", "summary": "Large-scale Vision-Language Models (VLMs) have achieved notable progress in\naligning visual inputs with text. However, their ability to deeply understand\nthe unique physical properties of non-RGB vision sensor images remains limited.\nIn this paper, we revisit and analyze these limitations and introduce a novel,\ncost-efficient paradigm that significantly advances sensor image\nunderstanding-without requiring extensive training data or any modifications to\nthe existing VLM architectures. Specifically, we propose Sensor-Aware\nAttributes Fine-Tuning (SAFT) with the Diverse Negative Attributes (DNA)\noptimization, which leverages minimal sensor-specific data to enable robust\nlearning of non-RGB characteristics and overcome RGB-centric biases inherent in\ncurrent VLMs. In addition, we present VS-TDX-the first comprehensive, public\nbenchmark designed to rigorously evaluate VLMs' sensor-specific understanding\nacross diverse and realistic scenarios. Through extensive experiments on VLMs\nand various sensor modalities, we validate that our method consistently\ndelivers superior performance and generalization under resource-constrained and\narchitecture-invariant settings. Our approach provides a practical advance\ntowards scalable deployment of VLMs in increasingly sensor-diverse real-world\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.20750v2", "cate": "cs.CV", "date": "2024-12-30", "updated": "2025-08-01"}
{"id": "2508.00706", "title": "Learning Network Dismantling without Handcrafted Inputs", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00706v1", "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00706v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00735", "title": "Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE", "authors": ["Lucas Aubard", "Johan Mazel", "Gilles Guette", "Pierre Chifflier"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00735v1", "summary": "IP fragmentation and TCP segmentation allow for splitting large data packets\ninto smaller ones, e.g., for transmission across network links of limited\ncapacity. These mechanisms permit complete or partial overlaps with different\ndata on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,\ni.e., the data chunk preferences that depend on the overlap types, differ\nacross protocol implementations. This leads to vulnerabilities, as NIDSes may\ninterpret the packet differently from the monitored host OSes. Some NIDSes,\nsuch as Suricata or Snort, can be configured so that their policies are\nconsistent with the monitored OSes. The first contribution of the paper is\nPYROLYSE, an audit tool that exhaustively tests and describes the reassembly\npolicies of various IP and TCP implementation types. This tool ensures that\nimplementations reassemble overlapping chunk sequences without errors. The\nsecond contribution is the analysis of PYROLYSE artifacts. We first show that\nthe reassembly policies are much more diverse than previously thought. Indeed,\nby testing all the overlap possibilities for n <= 3 test case chunks and\ndifferent testing scenarios, we observe from 14 to 20 different behaviors out\nof 23 tested implementations depending on the protocol. Second, we report eight\nerrors impacting one OS, two NIDSes, and two embedded stacks, which can lead to\nsecurity issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was\nassigned to a NIDS error. Finally, we show that implemented IP and TCP policies\nobtained through chunk pair testing are usually inconsistent with the observed\ntriplet reassemblies. Therefore, contrarily to what they currently do, NIDSes\nor other network traffic analysis tools should not apply n = 2 pair policies\nwhen the number of overlapping chunks exceeds two.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00735v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00816", "title": "Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process", "authors": ["Youssef Ait El Mahjoub", "Jean-Michel Fourneau", "Salma Alouah"], "categories": ["math.OC", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      Preprint article submitted to ValueTools2025", "url": "http://arxiv.org/abs/2508.00816v1", "summary": "Solving Markov Decision Processes (MDPs) remains a central challenge in\nsequential decision-making, especially when dealing with large state spaces and\nlong-term optimization criteria. A key step in Bellman dynamic programming\nalgorithms is the policy evaluation, which becomes computationally demanding in\ninfinite-horizon settings such as average-reward or discounted-reward\nformulations. In the context of Markov chains, aggregation and disaggregation\ntechniques have for a long time been used to reduce complexity by exploiting\nstructural decompositions. In this work, we extend these principles to a\nstructured class of MDPs. We define the Single-Input Superstate Decomposable\nMarkov Decision Process (SISDMDP), which combines Chiu's single-input\ndecomposition with Robertazzi's single-cycle recurrence property. When a policy\ninduces this structure, the resulting transition graph can be decomposed into\ninteracting components with centralized recurrence. We develop an exact and\nefficient policy evaluation method based on this structure. This yields a\nscalable solution applicable to both average and discounted reward MDPs.", "comment": "Preprint article submitted to ValueTools2025", "pdf_url": "http://arxiv.org/pdf/2508.00816v1", "cate": "math.OC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.22767", "title": "Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization", "authors": ["Soumyadeep Dhar", "Kei Sen Fong", "Mehul Motani"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.22767v2", "summary": "Distilling large neural networks into simple, human-readable symbolic\nformulas is a promising path toward trustworthy and interpretable AI. However,\nthis process is often brittle, as the complex functions learned by standard\nnetworks are poor targets for symbolic discovery, resulting in low-fidelity\nstudent models. In this work, we propose a novel training paradigm to address\nthis challenge. Instead of passively distilling a pre-trained network, we\nintroduce a \\textbf{Jacobian-based regularizer} that actively encourages the\n``teacher'' network to learn functions that are not only accurate but also\ninherently smoother and more amenable to distillation. We demonstrate through\nextensive experiments on a suite of real-world regression benchmarks that our\nmethod is highly effective. By optimizing the regularization strength for each\nproblem, we improve the $R^2$ score of the final distilled symbolic model by an\naverage of \\textbf{120\\% (relative)} compared to the standard distillation\npipeline, all while maintaining the teacher's predictive accuracy. Our work\npresents a practical and principled method for significantly improving the\nfidelity of interpretable models extracted from complex neural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.22767v2", "cate": "cs.LG", "date": "2025-07-30", "updated": "2025-08-01"}
{"id": "2501.08325", "title": "GameFactory: Creating New Games with Generative Interactive Videos", "authors": ["Jiwen Yu", "Yiran Qin", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Xihui Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 Highlight, Project Page: this https URL", "url": "http://arxiv.org/abs/2501.08325v3", "summary": "Generative videos have the potential to revolutionize game development by\nautonomously creating new content. In this paper, we present GameFactory, a\nframework for action-controlled scene-generalizable game video generation. We\nfirst address the fundamental challenge of action controllability by\nintroducing GF-Minecraft, an action-annotated game video dataset without human\nbias, and developing an action control module that enables precise control over\nboth keyboard and mouse inputs. We further extend to support autoregressive\ngeneration for unlimited-length interactive videos. More importantly,\nGameFactory tackles the critical challenge of scene-generalizable action\ncontrol, which most existing methods fail to address. To enable the creation of\nentirely new and diverse games beyond fixed styles and scenes, we leverage the\nopen-domain generative priors from pre-trained video diffusion models. To\nbridge the domain gap between open-domain priors and small-scale game datasets,\nwe propose a multi-phase training strategy with a domain adapter that decouples\ngame style learning from action control. This decoupling ensures that action\ncontrol learning is no longer bound to specific game styles, thereby achieving\nscene-generalizable action control. Experimental results demonstrate that\nGameFactory effectively generates open-domain action-controllable game videos,\nrepresenting a significant step forward in AI-driven game generation.", "comment": "ICCV 2025 Highlight, Project Page:\n  https://yujiwen.github.io/gamefactory", "pdf_url": "http://arxiv.org/pdf/2501.08325v3", "cate": "cs.CV", "date": "2025-01-14", "updated": "2025-07-31"}
{"id": "2508.00718", "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00718v1", "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00718v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00792", "title": "Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype", "authors": ["Aashay Arora", "Diego Davila", "Jonathan Guiang", "Frank Würthwein", "Harvey Newman", "Justas Balcas", "Tom Lehman", "Xi Yang"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to CHEP 24", "url": "http://arxiv.org/abs/2508.00792v1", "summary": "The Data Movement Manager (DMM) is a prototype interface that connects CERN's\ndata management software, Rucio, with the Sofware-Defined Networking (SDN)\nservice SENSE by ESNet. It enables SDN-enabled high-energy physics data flows\nusing the existing worldwide LHC computing grid infrastructure. A key feature\nof DMM is transfer priority-based bandwidth allocation, optimizing network\nusage. Additionally, it provides fine-grained monitoring of underperforming\nflows by leveraging end-to-end data flow monitoring. This is achieved through\naccess to host-level (network interface) throughput metrics and transfer-tool\n(FTS) data transfer job-level metrics. This paper details the design and\nimplementation of DMM.", "comment": "Submitted to CHEP 24", "pdf_url": "http://arxiv.org/pdf/2508.00792v1", "cate": "cs.NI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.16676", "title": "Fast solvers for Tokamak fluid models with PETSC", "authors": ["Mark F. Adams", "Jin Chen", "Benjamin Sturdevant"], "categories": ["physics.plasm-ph", "cs.PF"], "primary_category": "Subjects:       Plasma Physics (physics.plasm-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16676v5", "summary": "This work begins the development of fast, scalable solvers for scientific and\nengineering-relevant magnetohydrodynamics (MHD) models of tokamaks using\nmultigrid methods. These models are characterized by a distinguished direction\nfollowing the magnetic field around the torus, which dominates the plasma\ndynamics. All tokamak models exploit this structure, for example, NIMROD uses\n2D, unstructured, high-order finite elements in the poloidal plane with Fourier\nmodes in the toroidal coordinate, and the 3D, extended MHD code M3D-C1 uses 2D,\nunstructured C1 elements in the poloidal plane with cubic Hermite functions in\nthe toroidal direction and a regular grid that is partially aligned with the\nmagnetic field. This structure suggests addressing the toroidal coordinate\nfirst, which NIMROD does at the formulation level, but M3D-C1 uses a full 3D\ndiscretization. The resulting algebraic system is solved at each time step in\nan implicit time integrator. This work addressed the toroidal coordinate in the\nM3D-C1 velocity solve by adding semi-coarsening multigrid to the existing PETSC\n- Portable, Extensible Toolkit for Scientific Computation - block Jacobi\nsolver, with the addition of little new code. Competitive performance of this\nnew solver configuration is demonstrated on a self-consistent runaway electron\nmodel of a SPARC4 disruption, and the next steps in the development of this\nsolver are outlined.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16676v5", "cate": "physics.plasm-ph", "date": "2025-06-20", "updated": "2025-08-01"}
{"id": "2503.00737", "title": "Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration", "authors": ["Jinjiang You", "Hewei Wang", "Yijie Li", "Mingxiao Huo", "Long Van Tran Ha", "Mingyuan Ma", "Jinfeng Xu", "Jiayi Zhang", "Puzhen Wu", "Shubham Garg", "Wei Pu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IROS 2025. Final camera-ready version. 8 pages", "url": "http://arxiv.org/abs/2503.00737v2", "summary": "Calibrating large-scale camera arrays, such as those in dome-based setups, is\ntime-intensive and typically requires dedicated captures of known patterns.\nWhile extrinsics in such arrays are fixed due to the physical setup, intrinsics\noften vary across sessions due to factors like lens adjustments or temperature\nchanges. In this paper, we propose a dense-feature-driven multi-frame\ncalibration method that refines intrinsics directly from scene data,\neliminating the necessity for additional calibration captures. Our approach\nenhances traditional Structure-from-Motion (SfM) pipelines by introducing an\nextrinsics regularization term to progressively align estimated extrinsics with\nground-truth values, a dense feature reprojection term to reduce keypoint\nerrors by minimizing reprojection loss in the feature space, and an intrinsics\nvariance term for joint optimization across multiple frames. Experiments on the\nMultiface dataset show that our method achieves nearly the same precision as\ndedicated calibration processes, and significantly enhances intrinsics and 3D\nreconstruction accuracy. Fully compatible with existing SfM pipelines, our\nmethod provides an efficient and practical plug-and-play solution for\nlarge-scale camera setups. Our code is publicly available at:\nhttps://github.com/YJJfish/Multi-Cali-Anything", "comment": "Accepted to IROS 2025. Final camera-ready version. 8 pages", "pdf_url": "http://arxiv.org/pdf/2503.00737v2", "cate": "cs.CV", "date": "2025-03-02", "updated": "2025-08-01"}
{"id": "2508.00758", "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      22 pages, 16 figures, 7 tables, preprint version", "url": "http://arxiv.org/abs/2508.00758v1", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "comment": "22 pages, 16 figures, 7 tables, preprint version", "pdf_url": "http://arxiv.org/pdf/2508.00758v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2412.12485", "title": "Rydberg Atomic Receiver: Next Frontier of Wireless Communications", "authors": ["Mingyao Cui", "Qunsong Zeng", "Kaibin Huang"], "categories": ["eess.SP", "cs.NI", "physics.app-ph"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Communications Magazine", "url": "http://arxiv.org/abs/2412.12485v3", "summary": "Rydberg Atomic REceiver (RARE) is driving a paradigm shift in electromagnetic\n(EM) wave measurement by harnessing the electron transition phenomenon of\nRydberg atoms. Operating at the quantum scale, such receivers have the\npotential to breakthrough the performance limit of classic receivers, sparking\na revolution in physical-layer wireless communications. The objective of this\npaper is to offer insights into RARE-empowered communications. We first provide\na comprehensive introduction to the fundamental principles of RAREs. Then, a\nthorough comparison between RAREs and classic receivers is conducted in terms\nof the antenna size, sensitivity, and bandwidth. Subsequently, we overview the\nrecent progresses in RARE-aided wireless communications, covering the\nfrequency-division multiplexing, multiple-input-multiple-output, wireless\nsensing, and quantum many-body techniques. Moreover, the unique application of\nRARE in multiband sensing and communication is introduced. Finally, we conclude\nby providing promising research directions.", "comment": "Accepted by IEEE Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2412.12485v3", "cate": "eess.SP", "date": "2024-12-17", "updated": "2025-08-01"}
{"id": "2503.07235", "title": "Retinex-MEF: Retinex-based Glare Effects Aware Unsupervised Multi-Exposure Image Fusion", "authors": ["Haowen Bai", "Jiangshe Zhang", "Zixiang Zhao", "Lilun Deng", "Yukun Cui", "Shuang Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2503.07235v2", "summary": "Multi-exposure image fusion (MEF) synthesizes multiple, differently exposed\nimages of the same scene into a single, well-exposed composite. Retinex theory,\nwhich separates image illumination from scene reflectance, provides a natural\nframework to ensure consistent scene representation and effective information\nfusion across varied exposure levels. However, the conventional pixel-wise\nmultiplication of illumination and reflectance inadequately models the glare\neffect induced by overexposure. To address this limitation, we introduce an\nunsupervised and controllable method termed Retinex-MEF. Specifically, our\nmethod decomposes multi-exposure images into separate illumination components\nwith a shared reflectance component, and effectively models the glare induced\nby overexposure. The shared reflectance is learned via a bidirectional loss,\nwhich enables our approach to effectively mitigate the glare effect.\nFurthermore, we introduce a controllable exposure fusion criterion, enabling\nglobal exposure adjustments while preserving contrast, thus overcoming the\nconstraints of a fixed exposure level. Extensive experiments on diverse\ndatasets, including underexposure-overexposure fusion, exposure controlled\nfusion, and homogeneous extreme exposure fusion, demonstrate the effective\ndecomposition and flexible fusion capability of our model. The code is\navailable at https://github.com/HaowenBai/Retinex-MEF", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2503.07235v2", "cate": "cs.CV", "date": "2025-03-10", "updated": "2025-08-01"}
{"id": "2508.00768", "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00768v2", "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00768v2", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-04"}
{"id": "2503.11849", "title": "Towards a Unified Copernicus Foundation Model for Earth Vision", "authors": ["Yi Wang", "Zhitong Xiong", "Chenying Liu", "Adam J. Stewart", "Thomas Dujardin", "Nikolaos Ioannis Bountos", "Angelos Zavras", "Franziska Gerken", "Ioannis Papoutsis", "Laura Leal-Taixé", "Xiao Xiang Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. 33 pages, 34 figures", "url": "http://arxiv.org/abs/2503.11849v3", "summary": "Advances in Earth observation (EO) foundation models have unlocked the\npotential of big satellite data to learn generic representations from space,\nbenefiting a wide range of downstream applications crucial to our planet.\nHowever, most existing efforts remain limited to fixed spectral sensors, focus\nsolely on the Earth's surface, and overlook valuable metadata beyond imagery.\nIn this work, we take a step towards next-generation EO foundation models with\nthree key components: 1) Copernicus-Pretrain, a massive-scale pretraining\ndataset that integrates 18.7M aligned images from all major Copernicus Sentinel\nmissions, spanning from the Earth's surface to its atmosphere; 2)\nCopernicus-FM, a unified foundation model capable of processing any spectral or\nnon-spectral sensor modality using extended dynamic hypernetworks and flexible\nmetadata encoding; and 3) Copernicus-Bench, a systematic evaluation benchmark\nwith 15 hierarchical downstream tasks ranging from preprocessing to specialized\napplications for each Sentinel mission. Our dataset, model, and benchmark\ngreatly improve the scalability, versatility, and multimodal adaptability of EO\nfoundation models, while also creating new opportunities to connect EO,\nweather, and climate research. Codes, datasets and models are available at\nhttps://github.com/zhu-xlab/Copernicus-FM.", "comment": "Accepted to ICCV 2025. 33 pages, 34 figures", "pdf_url": "http://arxiv.org/pdf/2503.11849v3", "cate": "cs.CV", "date": "2025-03-14", "updated": "2025-07-31"}
{"id": "2508.00785", "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00785v1", "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00785v1", "cate": "cs.LG", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00013", "title": "From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms", "authors": ["Zurabi Kobaladze", "Anna Arnania", "Tamar Sanikidze"], "categories": ["cs.PL", "I.2.6; F.1.1"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      78 pages. Undergraduate thesis project submitted in partial fulfillment of the requirements for the Bachelor's degree in Computer Science at Kutaisi International University", "url": "http://arxiv.org/abs/2508.00013v1", "summary": "Program synthesis--the automated generation of executable code from\nhigh-level specifications--has been a central goal of computer science for over\nfifty years. This thesis provides a comparative literature review of the main\nparadigms that have shaped the field, tracing its evolution from formal logic\nbased methods to recent advances using large scale neural models. We examine\nfive key approaches: logic based (deductive) synthesis, inductive (example\nbased) synthesis, sketch/schema based synthesis, large language model based\nsynthesis, and neuro-symbolic hybrids. For each, we analyze foundational\nprinciples, notable systems, and practical applications, highlighting trade\noffs between correctness guarantees, specification requirements, search\ncomplexity, and expressive power. By reviewing developments from formally\nverified synthesis tools such as KIDS and Coq to data driven models generating\nprobabilistic code from natural language like Codex, we present a comprehensive\nnarrative of progress and ongoing challenges. This work emphasizes the\ntransition from symbolic to hybrid neuro-symbolic methods and outlines future\ndirections for reliable and scalable program synthesis.", "comment": "78 pages. Undergraduate thesis project submitted in partial\n  fulfillment of the requirements for the Bachelor's degree in Computer Science\n  at Kutaisi International University", "pdf_url": "http://arxiv.org/pdf/2508.00013v1", "cate": "cs.PL", "date": "2025-07-21", "updated": "2025-07-21"}
{"id": "2504.11172", "title": "TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data", "authors": ["Benedikt Blumenstiel", "Paolo Fraccaro", "Valerio Marsocci", "Johannes Jakubik", "Stefano Maurogiovanni", "Mikolaj Czerkawski", "Rocco Sedona", "Gabriele Cavallaro", "Thomas Brunschwiler", "Juan Bernabe-Moreno", "Nicolas Longépé"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops", "url": "http://arxiv.org/abs/2504.11172v2", "summary": "Large-scale foundation models in Earth Observation can learn versatile,\nlabel-efficient representations by leveraging massive amounts of unlabeled\ndata. However, existing public datasets are often limited in scale, geographic\ncoverage, or sensor variety. We introduce TerraMesh, a new globally diverse,\nmultimodal dataset combining optical, synthetic aperture radar, elevation, and\nland-cover modalities in an Analysis-Ready Data format. TerraMesh includes over\n9~million samples with eight spatiotemporal aligned modalities, enabling\nlarge-scale pre-training. We provide detailed data processing steps,\ncomprehensive statistics, and empirical evidence demonstrating improved model\nperformance when pre-trained on TerraMesh. The dataset is hosted at\nhttps://huggingface.co/datasets/ibm-esa-geospatial/TerraMesh.", "comment": "Proceedings of the Computer Vision and Pattern Recognition Conference\n  (CVPR) Workshops", "pdf_url": "http://arxiv.org/pdf/2504.11172v2", "cate": "cs.CV", "date": "2025-04-15", "updated": "2025-08-01"}
{"id": "2508.00027", "title": "Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems", "authors": ["Azadeh Alavi", "Fatemeh Kouchmeshki", "Abdolrahman Alavi", "Yongli Ren", "Jiayang Niu"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer review", "url": "http://arxiv.org/abs/2508.00027v1", "summary": "Modern recommenders describe each item with hundreds of sparse semantic tags,\nyet most quantum pipelines still map one qubit per tag, demanding well beyond\none hundred qubits, far out of reach for current noisy-intermediate-scale\nquantum (NISQ) devices and prone to deep, error-amplifying circuits. We close\nthis gap with a three-stage hybrid machine learning algorithm that compresses\ntag profiles, optimizes feature selection under a fixed qubit budget via QAOA,\nand scores recommendations with a Quantum semi-Random Forest (QsRF) built on\njust five qubits, while performing similarly to the state-of-the-art methods.\nLeveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \\%\nvariance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A\n100-tree QsRF trained on these codes matches full-feature baselines on\nICM-150/500.", "comment": "Submitted to IEEE Quantum AI Conference (QAI 2025), awaiting peer\n  review", "pdf_url": "http://arxiv.org/pdf/2508.00027v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00422", "title": "Automated Type Annotation in Python Using Large Language Models", "authors": ["Varun Bharti", "Shashwat Jha", "Dhruv Kumar", "Pankaj Jalote"], "categories": ["cs.PL", "cs.LG"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2508.00422v1", "summary": "Type annotations in Python enhance maintainability and error detection.\nHowever, generating these annotations manually is error prone and requires\nextra effort. Traditional automation approaches like static analysis, machine\nlearning, and deep learning struggle with limited type vocabularies, behavioral\nover approximation, and reliance on large labeled datasets. In this work, we\nexplore the use of LLMs for generating type annotations in Python. We develop a\ngenerate check repair pipeline: the LLM proposes annotations guided by a\nConcrete Syntax Tree representation, a static type checker (Mypy) verifies\nthem, and any errors are fed back for iterative refinement. We evaluate four\nLLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini\n(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.\nWe first measure the proportion of code snippets annotated by LLMs for which\nMyPy reported no errors (i.e., consistent results): GPT 4oMini achieved\nconsistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,\nand O4Mini each reached approximately 88.6% consistency (around 11.4%\nfailures). To measure annotation quality, we then compute exact-match and\nbase-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini\nperform the best, achieving up to 70.5% exact match and 79.1% base type\naccuracy, requiring under one repair iteration on average. Our results\ndemonstrate that general-purpose and reasoning optimized LLMs, without any task\nspecific fine tuning or additional training can be effective in generating\nconsistent type annotations.They perform competitively with traditional deep\nlearning techniques which require large labeled dataset for training. While our\nwork focuses on Python, the pipeline can be extended to other optionally typed\nimperative languages like Ruby", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2508.00422v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.17894", "title": "DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing", "authors": ["Aniruddha Bala", "Rohit Chowdhury", "Rohan Jaiswal", "Siddharth Roheda"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2504.17894v2", "summary": "Advancements in diffusion models have enabled effortless image editing via\ntext prompts, raising concerns about image security. Attackers with access to\nuser images can exploit these tools for malicious edits. Recent defenses\nattempt to protect images by adding a limited noise in the pixel space to\ndisrupt the functioning of diffusion-based editing models. However, the\nadversarial noise added by previous methods is easily noticeable to the human\neye. Moreover, most of these methods are not robust to purification techniques\nlike JPEG compression under a feasible pixel budget. We propose a novel\noptimization approach that introduces adversarial perturbations directly in the\nfrequency domain by modifying the Discrete Cosine Transform (DCT) coefficients\nof the input image. By leveraging the JPEG pipeline, our method generates\nadversarial images that effectively prevent malicious image editing. Extensive\nexperiments across a variety of tasks and datasets demonstrate that our\napproach introduces fewer visual artifacts while maintaining similar levels of\nedit protection and robustness to noise purification techniques.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2504.17894v2", "cate": "cs.CV", "date": "2025-04-24", "updated": "2025-07-31"}
{"id": "2508.00029", "title": "Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins", "authors": ["Azadeh Alavi", "Sanduni Jayasinghe", "Mojtaba Mahmoodian", "Sam Mazaheri", "John Thangarajah", "Sujeeva Setunge"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Submitted to Scientific Report", "url": "http://arxiv.org/abs/2508.00029v1", "summary": "Large-scale civil structures, such as bridges, pipelines, and offshore\nplatforms, are vital to modern infrastructure, where unexpected failures can\ncause significant economic and safety repercussions. Although finite element\n(FE) modeling is widely used for real-time structural health monitoring (SHM),\nits high computational cost and the complexity of inverse FE analysis, where\nlow dimensional sensor data must map onto high-dimensional displacement or\nstress fields pose ongoing challenges. Here, we propose a hybrid quantum\nclassical multilayer perceptron (QMLP) framework to tackle these issues and\nfacilitate swift updates to digital twins across a range of structural\napplications.\n  Our approach embeds sensor data using symmetric positive definite (SPD)\nmatrices and polynomial features, yielding a representation well suited to\nquantum processing. A parameterized quantum circuit (PQC) transforms these\nfeatures, and the resultant quantum outputs feed into a classical neural\nnetwork for final inference. By fusing quantum capabilities with classical\nmodeling, the QMLP handles large scale inverse FE mapping while preserving\ncomputational viability.\n  Through extensive experiments on a bridge, we demonstrate that the QMLP\nachieves a mean squared error (MSE) of 0.0000000000316, outperforming purely\nclassical baselines with a large margin. These findings confirm the potential\nof quantum-enhanced methods for real time SHM, establishing a pathway toward\nmore efficient, scalable digital twins that can robustly monitor and diagnose\nstructural integrity in near real time.", "comment": "Submitted to Scientific Report", "pdf_url": "http://arxiv.org/pdf/2508.00029v1", "cate": "quant-ph", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2508.00482", "title": "Semantic Subtyping for Maps in Erlang", "authors": ["Erdem Yildirim", "Albert Schimpf", "Stefan Wehr", "Annette Bieniusa"], "categories": ["cs.PL"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00482v1", "summary": "In this paper we will construct a set-theoretic model of types featuring type\nvariables, base types, set-theoretic types and map types. Syntax of map types\nspans all the map types available in Erlang. The model of types is used to\ndefine a semantic subtyping relation based on set containment. The novelty of\nthis work is the definition of subtyping over parameteric map types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00482v1", "cate": "cs.PL", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.01225", "title": "Core-Set Selection for Data-efficient Land Cover Segmentation", "authors": ["Keiller Nogueira", "Akram Zaytar", "Wanli Ma", "Ribana Roscher", "Ronny Hänsch", "Caleb Robinson", "Anthony Ortiz", "Simone Nsutezo", "Rahul Dodhia", "Juan M. Lavista Ferres", "Oktay Karakuş", "Paul L. Rosin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01225v2", "summary": "The increasing accessibility of remotely sensed data and the potential of\nsuch data to inform large-scale decision-making has driven the development of\ndeep learning models for many Earth Observation tasks. Traditionally, such\nmodels must be trained on large datasets. However, the common assumption that\nbroadly larger datasets lead to better outcomes tends to overlook the\ncomplexities of the data distribution, the potential for introducing biases and\nnoise, and the computational resources required for processing and storing vast\ndatasets. Therefore, effective solutions should consider both the quantity and\nquality of data. In this paper, we propose six novel core-set selection methods\nfor selecting important subsets of samples from remote sensing image\nsegmentation datasets that rely on imagery only, labels only, and a combination\nof each. We benchmark these approaches against a random-selection baseline on\nthree commonly used land cover classification datasets: DFC2022, Vaihingen, and\nPotsdam. In each of the datasets, we demonstrate that training on a subset of\nsamples outperforms the random baseline, and some approaches outperform\ntraining on all available data. This result shows the importance and potential\nof data-centric learning for the remote sensing domain. The code is available\nat https://github.com/keillernogueira/data-centric-rs-classification/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01225v2", "cate": "cs.CV", "date": "2025-05-02", "updated": "2025-08-01"}
{"id": "2508.00048", "title": "Dimension reduction with structure-aware quantum circuits for hybrid machine learning", "authors": ["Ammar Daskin"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Any comments are welcome! The simulation code is provided at this https URL", "url": "http://arxiv.org/abs/2508.00048v1", "summary": "Schmidt decomposition of a vector can be understood as writing the singular\nvalue decomposition (SVD) in vector form. A vector can be written as a linear\ncombination of tensor product of two dimensional vectors by recursively\napplying Schmidt decompositions via SVD to all subsystems. Given a vector\nexpressed as a linear combination of tensor products, using only the $k$\nprincipal terms yields a $k$-rank approximation of the vector. Therefore,\nwriting a vector in this reduced form allows to retain most important parts of\nthe vector while removing small noises from it, analogous to SVD-based\ndenoising.\n  In this paper, we show that quantum circuits designed based on a value $k$\n(determined from the tensor network decomposition of the mean vector of the\ntraining sample) can approximate the reduced-form representations of entire\ndatasets. We then employ this circuit ansatz with a classical neural network\nhead to construct a hybrid machine learning model. Since the output of the\nquantum circuit for an $2^n$ dimensional vector is an $n$ dimensional\nprobability vector, this provides an exponential compression of the input and\npotentially can reduce the number of learnable parameters for training\nlarge-scale models. We use datasets provided in the Python scikit-learn module\nfor the experiments. The results confirm the quantum circuit is able to\ncompress data successfully to provide effective $k$-rank approximations to the\nclassical processing component.", "comment": "Any comments are welcome! The simulation code is provided at\n  https://github.com/adaskin/structure-aware-circuits", "pdf_url": "http://arxiv.org/pdf/2508.00048v1", "cate": "quant-ph", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00244", "title": "Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems", "authors": ["Briza Mel Dias de Sousa", "Renato Cordeiro Ferreira", "Alfredo Goldman"], "categories": ["cs.SE", "cs.PL", "D.3.2; D.2.11; D.2.13"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings), submitted to CTICQS capstone project competition at SBQS 2025", "url": "http://arxiv.org/abs/2508.00244v1", "summary": "After decades of dominance by object-oriented programming (OOP), functional\nprogramming (FP) is gaining increasing attention in the software industry. This\nstudy compares the impact of OOP and FP on the architectural characteristics of\nsoftware systems. For that, it examines the design and implementation of a\nDigital Wallet system, developed in Kotlin (representing OOP) and Scala\n(representing FP). The comparison is made through both qualitative and\nquantitative analyses to explore how each paradigm influences the system's\narchitectural characteristics. The self-ethnographic qualitative analysis\nprovides a side-by-side comparison of both implementations, revealing the\nperspective of those writing such code. The survey-based quantitative analysis\ngathers feedback from developers with diverse backgrounds, showing their\nimpressions of those reading this code. Hopefully, these results may be useful\nfor developers or organizations seeking to make more informed decisions about\nwhich paradigm is best suited for their next project.", "comment": "11 pages, 16 figures (1 table, 3 diagrams, 5 graphics, 7 listings),\n  submitted to CTICQS capstone project competition at SBQS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00244v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.03351", "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar", "authors": ["Dongbin Zhang", "Yunfei Liu", "Lijian Lin", "Ye Zhu", "Yang Li", "Minghan Qin", "Yu Li", "Haoqian Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025, Project page: this https URL", "url": "http://arxiv.org/abs/2505.03351v2", "summary": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.", "comment": "Accepted to ICCV 2025, Project page:\n  https://eastbeanzhang.github.io/GUAVA/", "pdf_url": "http://arxiv.org/pdf/2505.03351v2", "cate": "cs.CV", "date": "2025-05-06", "updated": "2025-08-01"}
{"id": "2508.00110", "title": "funOCLUST: Clustering Functional Data with Outliers", "authors": ["Katharine M. Clark", "Paul D. McNicholas"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00110v1", "summary": "Functional data present unique challenges for clustering due to their\ninfinite-dimensional nature and potential sensitivity to outliers. An extension\nof the OCLUST algorithm to the functional setting is proposed to address these\nissues. The approach leverages the OCLUST framework, creating a robust method\nto cluster curves and trim outliers. The methodology is evaluated on both\nsimulated and real-world functional datasets, demonstrating strong performance\nin clustering and outlier identification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00110v1", "cate": "stat.ML", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00508", "title": "Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis", "authors": ["Panagiotis Diamantakis", "Thanassis Avgerinos", "Yannis Smaragdakis"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00508v1", "summary": "Over the past two decades, two different types of static analyses have\nemerged as dominant paradigms both in academia and industry: value-flow\nanalysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis\n(e.g., symbolic execution). Despite their individual successes in numerous\napplication fields, the two approaches have remained largely separate; an\nartifact of the simple reality that there is no broadly adopted unifying\nplatform for effortless and efficient integration of symbolic techniques with\nhigh-performance data-flow reasoning.\n  To bridge this gap, we introduce Desyan: a platform for writing program\nanalyses with seamless integration of value-flow and symbolic reasoning. Desyan\nexpands a production-ready Datalog fixpoint engine (Souffl\\'e) with\nfull-fledged SMT solving invoking industry-leading SMT engines. Desyan provides\nconstructs for automatically (and efficiently!) handling typical patterns that\ncome up in program analysis. At the same time, the integration is agnostic with\nrespect to the solving technology, and supports Datalog-native symbolic\nreasoning, via a bottom-up algebraic reasoning module.\n  The result is an engine that allows blending different kinds of reasoning, as\nneeded for the underlying analysis. For value-flow analysis, the engine is the\nbest-in-class Datalog evaluator (often by a factor of over 20x in execution\ntime); for applications that require full SMT (e.g., a concolic execution\nengine or other symbolic evaluator that needs to solve arbitrarily complex\nconditions), the engine is leveraging the leading SMT solvers; for lightweight\nsymbolic evaluation (e.g., solving simple conditionals in the context of a\npath-sensitive analysis), the engine can use Datalog-native symbolic reasoning,\nachieving large speedups (often of over 2x) compared to eagerly appealing to an\nSMT solver.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00508v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.04974", "title": "ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment", "authors": ["Wanjiang Weng", "Xiaofeng Tan", "Hongsong Wang", "Pan Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      We believe that there are some areas in the manuscript that require further improvement, and out of our commitment to refining this work, we have decided to withdraw our manuscript after careful deliberation and discussion", "url": "http://arxiv.org/abs/2505.04974v2", "summary": "Bilingual text-to-motion generation, which synthesizes 3D human motions from\nbilingual text inputs, holds immense potential for cross-linguistic\napplications in gaming, film, and robotics. However, this task faces critical\nchallenges: the absence of bilingual motion-language datasets and the\nmisalignment between text and motion distributions in diffusion models, leading\nto semantically inconsistent or low-quality motions. To address these\nchallenges, we propose BiHumanML3D, a novel bilingual human motion dataset,\nwhich establishes a crucial benchmark for bilingual text-to-motion generation\nmodels. Furthermore, we propose a Bilingual Motion Diffusion model (BiMD),\nwhich leverages cross-lingual aligned representations to capture semantics,\nthereby achieving a unified bilingual model. Building upon this, we propose\nReward-guided sampling Alignment (ReAlign) method, comprising a step-aware\nreward model to assess alignment quality during sampling and a reward-guided\nstrategy that directs the diffusion process toward an optimally aligned\ndistribution. This reward model integrates step-aware tokens and combines a\ntext-aligned module for semantic consistency and a motion-aligned module for\nrealism, refining noisy motions at each timestep to balance probability density\nand alignment. Experiments demonstrate that our approach significantly improves\ntext-motion alignment and motion quality compared to existing state-of-the-art\nmethods. Project page: https://wengwanjiang.github.io/ReAlign-page/.", "comment": "We believe that there are some areas in the manuscript that require\n  further improvement, and out of our commitment to refining this work, we have\n  decided to withdraw our manuscript after careful deliberation and discussion", "pdf_url": "http://arxiv.org/pdf/2505.04974v2", "cate": "cs.CV", "date": "2025-05-08", "updated": "2025-08-01"}
{"id": "2508.00154", "title": "Data-Driven Motion Planning for Uncertain Nonlinear Systems", "authors": ["Babak Esmaeili", "Hamidreza Modares", "Stefano Di Cairano"], "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00154v1", "summary": "This paper proposes a data-driven motion-planning framework for nonlinear\nsystems that constructs a sequence of overlapping invariant polytopes. Around\neach randomly sampled waypoint, the algorithm identifies a convex admissible\nregion and solves data-driven linear-matrix-inequality problems to learn\nseveral ellipsoidal invariant sets together with their local state-feedback\ngains. The convex hull of these ellipsoids, still invariant under a\npiece-wise-affine controller obtained by interpolating the gains, is then\napproximated by a polytope. Safe transitions between nodes are ensured by\nverifying the intersection of consecutive convex-hull polytopes and introducing\nan intermediate node for a smooth transition. Control gains are interpolated in\nreal time via simplex-based interpolation, keeping the state inside the\ninvariant polytopes throughout the motion. Unlike traditional approaches that\nrely on system dynamics models, our method requires only data to compute safe\nregions and design state-feedback controllers. The approach is validated\nthrough simulations, demonstrating the effectiveness of the proposed method in\nachieving safe, dynamically feasible paths for complex nonlinear systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00154v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00772", "title": "From Code to Career: Assessing Competitive Programmers for Industry Placement", "authors": ["Md Imranur Rahman Akib", "Fathima Binthe Muhammed", "Umit Saha", "Md Fazlul Karim Patwary", "Mehrin Anannya", "Md Alomgeer Hussein", "Md Biplob Hosen"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00772v1", "summary": "In today's fast-paced tech industry, there is a growing need for tools that\nevaluate a programmer's job readiness based on their coding performance. This\nstudy focuses on predicting the potential of Codeforces users to secure various\nlevels of software engineering jobs. The primary objective is to analyze how a\nuser's competitive programming activity correlates with their chances of\nobtaining positions, ranging from entry-level roles to jobs at major tech\ncompanies. We collect user data using the Codeforces API, process key\nperformance metrics, and build a prediction model using a Random Forest\nclassifier. The model categorizes users into four levels of employability,\nranging from those needing further development to those ready for top-tier tech\njobs. The system is implemented using Flask and deployed on Render for\nreal-time predictions. Our evaluation demonstrates that the approach\neffectively distinguishes between different skill levels based on coding\nproficiency and participation. This work lays a foundation for the use of\nmachine learning in career assessment and could be extended to predict job\nreadiness in broader technical fields.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00772v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.08665", "title": "SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation", "authors": ["Edoardo Bianchi", "Antonio Liotta"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08665v3", "summary": "Assessing human skill levels in complex activities is a challenging problem\nwith applications in sports, rehabilitation, and training. In this work, we\npresent SkillFormer, a parameter-efficient architecture for unified multi-view\nproficiency estimation from egocentric and exocentric videos. Building on the\nTimeSformer backbone, SkillFormer introduces a CrossViewFusion module that\nfuses view-specific features using multi-head cross-attention, learnable\ngating, and adaptive self-calibration. We leverage Low-Rank Adaptation to\nfine-tune only a small subset of parameters, significantly reducing training\ncosts. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves\nstate-of-the-art accuracy in multi-view settings while demonstrating remarkable\ncomputational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer\ntraining epochs than prior baselines. It excels in multiple structured tasks,\nconfirming the value of multi-view integration for fine-grained skill\nassessment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08665v3", "cate": "cs.CV", "date": "2025-05-13", "updated": "2025-08-01"}
{"id": "2508.00267", "title": "Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks", "authors": ["Molly Noel", "Gabriel Mancino-Ball", "Yangyang Xu"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      32 pages", "url": "http://arxiv.org/abs/2508.00267v1", "summary": "Graph convolutional networks (GCNs) are a powerful tool for graph\nrepresentation learning. Due to the recursive neighborhood aggregations\nemployed by GCNs, efficient training methods suffer from a lack of theoretical\nguarantees or are missing important practical elements from modern deep\nlearning algorithms, such as adaptivity and momentum. In this paper, we present\nseveral neighbor-sampling (NS) based Adam-type stochastic methods for solving a\nnonconvex GCN training problem. We utilize the control variate technique\nproposed by [1] to reduce the stochastic error caused by neighbor sampling.\nUnder standard assumptions for Adam-type methods, we show that our methods\nenjoy the optimal convergence rate. In addition, we conduct extensive numerical\nexperiments on node classification tasks with several benchmark datasets. The\nresults demonstrate superior performance of our methods over classic NS-based\nSGD that also uses the control-variate technique, especially for large-scale\ngraph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .", "comment": "32 pages", "pdf_url": "http://arxiv.org/pdf/2508.00267v1", "cate": "math.OC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.16544", "title": "Float Self-Tagging", "authors": ["Olivier Melançon", "Manuel Serrano", "Marc Feeley"], "categories": ["cs.PL", "D.3.4"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.16544v3", "summary": "Dynamic and polymorphic languages attach information, such as types, to run\ntime objects, and therefore adapt the memory layout of values to include space\nfor this information. This makes it difficult to efficiently implement IEEE754\nfloating-point numbers as this format does not leave an easily accessible space\nto store type information. The three main floating-point number encodings in\nuse today, tagged pointers, NaN-boxing, and NuN-boxing, have drawbacks. Tagged\npointers entail a heap allocation of all float objects, and NaN/NuN-boxing puts\nadditional run time costs on type checks and the handling of other objects.\n  This paper introduces self-tagging, a new approach to object tagging that\nuses an invertible bitwise transformation to map floating-point numbers to\ntagged values that contain the correct type information at the correct position\nin their bit pattern, superimposing both their value and type information in a\nsingle machine word. Such a transformation can only map a subset of all floats\nto correctly typed tagged values, hence self-tagging takes advantage of the\nnon-uniform distribution of floating point numbers used in practice to avoid\nheap allocation of the most frequently encountered floats.\n  Variants of self-tagging were implemented in two distinct Scheme compilers\nand evaluated on four microarchitectures to assess their performance and\ncompare them to tagged pointers, NaN-boxing, and NuN-boxing. Experiments\ndemonstrate that, in practice, the approach eliminates heap allocation of\nnearly all floating-point numbers and provides good execution speed of\nfloat-intensive benchmarks in Scheme with a negligible performance impact on\nother benchmarks, making it an attractive alternative to tagged pointers,\nalongside NaN-boxing and NuN-boxing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.16544v3", "cate": "cs.PL", "date": "2024-11-25", "updated": "2025-08-01"}
{"id": "2508.00162", "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00162v1", "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00162v1", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2505.12280", "title": "Spatial-Temporal-Spectral Unified Modeling for Remote Sensing Dense Prediction", "authors": ["Sijie Zhao", "Feng Liu", "Enzhuo Zhang", "Yiqing Guo", "Pengfeng Xiao", "Lei Bai", "Xueliang Zhang", "Hao Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 6 figures, Code link: this https URL", "url": "http://arxiv.org/abs/2505.12280v3", "summary": "The proliferation of multi-source remote sensing data has propelled the\ndevelopment of deep learning for dense prediction, yet significant challenges\nin data and task unification persist. Current deep learning architectures for\nremote sensing are fundamentally rigid. They are engineered for fixed\ninput-output configurations, restricting their adaptability to the\nheterogeneous spatial, temporal, and spectral dimensions inherent in real-world\ndata. Furthermore, these models neglect the intrinsic correlations among\nsemantic segmentation, binary change detection, and semantic change detection,\nnecessitating the development of distinct models or task-specific decoders.\nThis paradigm is also constrained to a predefined set of output semantic\nclasses, where any change to the classes requires costly retraining. To\novercome these limitations, we introduce the Spatial-Temporal-Spectral Unified\nNetwork (STSUN) for unified modeling. STSUN can adapt to input and output data\nwith arbitrary spatial sizes, temporal lengths, and spectral bands by\nleveraging their metadata for a unified representation. Moreover, STSUN unifies\ndisparate dense prediction tasks within a single architecture by conditioning\nthe model on trainable task embeddings. Similarly, STSUN facilitates flexible\nprediction across multiple set of semantic categories by integrating trainable\ncategory embeddings as metadata. Extensive experiments on multiple datasets\nwith diverse Spatial-Temporal-Spectral configurations in multiple scenarios\ndemonstrate that a single STSUN model effectively adapts to heterogeneous\ninputs and outputs, unifying various dense prediction tasks and diverse\nsemantic class predictions. The proposed approach consistently achieves\nstate-of-the-art performance, highlighting its robustness and generalizability\nfor complex remote sensing applications.", "comment": "16 pages, 6 figures, Code\n  link:https://github.com/walking-shadow/Official_TSSUN", "pdf_url": "http://arxiv.org/pdf/2505.12280v3", "cate": "cs.CV", "date": "2025-05-18", "updated": "2025-08-01"}
{"id": "2508.00617", "title": "Constructive Disintegration and Conditional Modes", "authors": ["Nathaël Da Costa", "Marvin Pförtner", "Jon Cockayne"], "categories": ["math.ST", "cs.LG", "math.PR", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00617v1", "summary": "Conditioning, the central operation in Bayesian statistics, is formalised by\nthe notion of disintegration of measures. However, due to the implicit nature\nof their definition, constructing disintegrations is often difficult. A\nfolklore result in machine learning conflates the construction of a\ndisintegration with the restriction of probability density functions onto the\nsubset of events that are consistent with a given observation. We provide a\ncomprehensive set of mathematical tools which can be used to construct\ndisintegrations and apply these to find densities of disintegrations on\ndifferentiable manifolds. Using our results, we provide a disturbingly simple\nexample in which the restricted density and the disintegration density\ndrastically disagree. Motivated by applications in approximate Bayesian\ninference and Bayesian inverse problems, we further study the modes of\ndisintegrations. We show that the recently introduced notion of a \"conditional\nmode\" does not coincide in general with the modes of the conditional measure\nobtained through disintegration, but rather the modes of the restricted\nmeasure. We also discuss the implications of the discrepancy between the two\nmeasures in practice, advocating for the utility of both approaches depending\non the modelling context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00617v1", "cate": "math.ST", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2503.21691", "title": "Place Capability Graphs: A General-Purpose Model of Rust's Ownership and Borrowing Guarantees", "authors": ["Zachary Grannan", "Aurel Bílý", "Jonáš Fiala", "Jasper Geer", "Markus de Medeiros", "Peter Müller", "Alexander J. Summers"], "categories": ["cs.PL"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.21691v4", "summary": "Rust's novel type system has proved an attractive target for verification and\nprogram analysis tools, due to the rich guarantees it provides for controlling\naliasing and mutability. However, fully understanding, extracting and\nexploiting these guarantees is subtle and challenging: existing models for\nRust's type checking either support a smaller idealised language disconnected\nfrom real-world Rust code, or come with severe limitations in terms of precise\nmodelling of Rust borrows, composite types storing them, function signatures\nand loops.\n  In this paper, we present a novel model of Rust's type-checking called Place\nCapability Graphs, which lifts these limitations, and which can be directly\ncalculated from the Rust compiler's own programmatic representations and\nanalyses. We demonstrate that our model supports over 97% of Rust functions in\nthe most popular public crates, and show its suitability as a general-purpose\nbasis for verification and program analysis tools by developing promising new\nprototype versions of the existing Flowistry and Prusti tools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.21691v4", "cate": "cs.PL", "date": "2025-03-27", "updated": "2025-08-01"}
{"id": "2508.00258", "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00258v1", "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00258v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.13943", "title": "From Press to Pixels: Evolving Urdu Text Recognition", "authors": ["Samee Arif", "Sualeha Farid"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.13943v2", "summary": "This paper introduces an end-to-end pipeline for Optical Character\nRecognition (OCR) on Urdu newspapers, addressing challenges posed by complex\nmulti-column layouts, low-resolution scans, and the stylistic variability of\nthe Nastaliq script. Our system comprises four modules: (1) article\nsegmentation, (2) image super-resolution, (3) column segmentation, and (4) text\nrecognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision\nfor articles and 0.970 for columns. A SwinIR-based super-resolution model\nboosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu\nNewspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB\nand the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with\nmodern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133.\nWe further analyze LLM outputs via insertion, deletion, and substitution error\nbreakdowns, as well as character-level confusion analysis. Finally, we show\nthat fine-tuning on just 500 samples yields a 6.13% WER improvement,\nhighlighting the adaptability of LLMs for Urdu OCR.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.13943v2", "cate": "cs.CV", "date": "2025-05-20", "updated": "2025-08-01"}
{"id": "2508.00775", "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms", "authors": ["Andrea Martin", "Ian R. Manchester", "Luca Furieri"], "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00775v1", "summary": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00775v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23186", "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "authors": ["Peter Sharpe"], "categories": ["cs.LG", "cs.PL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23186v2", "summary": "When numerically evaluating a function's gradient, sparsity detection can\nenable substantial computational speedups through Jacobian coloring and\ncompression. However, sparsity detection techniques for black-box functions are\nlimited, and existing finite-difference-based methods suffer from false\nnegatives due to coincidental zero gradients. These false negatives can\nsilently corrupt gradient calculations, leading to difficult-to-diagnose\nerrors. We introduce NaN-propagation, which exploits the universal\ncontamination property of IEEE 754 Not-a-Number values to trace input-output\ndependencies through floating-point numerical computations. By systematically\ncontaminating inputs with NaN and observing which outputs become NaN, the\nmethod reconstructs conservative sparsity patterns that eliminate a major\nsource of false negatives. We demonstrate this approach on an aerospace wing\nweight model, achieving a 1.52x speedup while uncovering dozens of dependencies\nmissed by conventional methods -- a significant practical improvement since\ngradient computation is often the bottleneck in optimization workflows. The\ntechnique leverages IEEE 754 compliance to work across programming languages\nand math libraries without requiring modifications to existing black-box codes.\nFurthermore, advanced strategies such as NaN payload encoding via direct bit\nmanipulation enable faster-than-linear time complexity, yielding speed\nimprovements over existing black-box sparsity detection methods. Practical\nalgorithms are also proposed to mitigate challenges from branching code\nexecution common in engineering applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23186v2", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00303", "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00303v1", "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00303v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.15160", "title": "Lossless Token Merging Even Without Fine-Tuning in Vision Transformers", "authors": ["Jaeyeon Lee", "Dong-Wan Choi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ECAI 2025", "url": "http://arxiv.org/abs/2505.15160v2", "summary": "Although Vision Transformers (ViTs) have become the standard architecture in\ncomputer vision, their massive sizes lead to significant computational\noverhead. Token compression techniques have attracted considerable attention to\naddress this issue, but they often suffer from severe information loss,\nrequiring extensive additional training to achieve practical performance. In\nthis paper, we propose Adaptive Token Merging (ATM), a novel method that\nensures lossless token merging, eliminating the need for fine-tuning while\nmaintaining competitive performance. ATM adaptively reduces tokens across\nlayers and batches by carefully adjusting layer-specific similarity thresholds,\nthereby preventing the undesirable merging of less similar tokens with respect\nto each layer. Furthermore, ATM introduces a novel token matching technique\nthat considers not only similarity but also merging sizes, particularly for the\nfinal layers, to minimize the information loss incurred from each merging\noperation. We empirically validate our method across a wide range of pretrained\nmodels, demonstrating that ATM not only outperforms all existing training-free\nmethods but also surpasses most training-intensive approaches, even without\nadditional training. Remarkably, training-free ATM achieves over a 30%\nreduction in FLOPs for the DeiT-T and DeiT-S models without any drop in their\noriginal accuracy.", "comment": "ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2505.15160v2", "cate": "cs.CV", "date": "2025-05-21", "updated": "2025-08-01"}
{"id": "2401.01100", "title": "Sampling-enabled scalable manifold learning unveils discriminative cluster structure of high-dimensional data", "authors": ["Dehua Peng", "Zhipeng Gui", "Wenzhang Wei", "Fa Li", "Jie Gui", "Huayi Wu", "Jianya Gong"], "categories": ["cs.LG", "I.5.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      80 pages, 37 figures", "url": "http://arxiv.org/abs/2401.01100v4", "summary": "As a pivotal branch of machine learning, manifold learning uncovers the\nintrinsic low-dimensional structure within complex nonlinear manifolds in\nhigh-dimensional space for visualization, classification, clustering, and\ngaining key insights. Although existing techniques have achieved remarkable\nsuccesses, they suffer from extensive distortions of cluster structure, which\nhinders the understanding of underlying patterns. Scalability issues also limit\ntheir applicability for handling large-scale data. We hence propose a\nsampling-based Scalable manifold learning technique that enables Uniform and\nDiscriminative Embedding, namely SUDE, for large-scale and high-dimensional\ndata. It starts by seeking a set of landmarks to construct the low-dimensional\nskeleton of the entire data, and then incorporates the non-landmarks into the\nlearned space based on the constrained locally linear embedding (CLLE). We\nempirically validated the effectiveness of SUDE on synthetic datasets and\nreal-world benchmarks, and applied it to analyze single-cell data and detect\nanomalies in electrocardiogram (ECG) signals. SUDE exhibits distinct advantage\nin scalability with respect to data size and embedding dimension, and has\npromising performance in cluster separation, integrity, and global structure\npreservation. The experiments also demonstrate notable robustness in embedding\nquality as the sampling rate decreases.", "comment": "80 pages, 37 figures", "pdf_url": "http://arxiv.org/pdf/2401.01100v4", "cate": "cs.LG", "date": "2024-01-02", "updated": "2025-08-01"}
{"id": "2508.00355", "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00355v1", "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00355v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00505", "title": "A Variant of Non-uniform Cylindrical Algebraic Decomposition for Real Quantifier Elimination", "authors": ["Jasper Nalbach", "Erika Ábrahám"], "categories": ["cs.SC"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00505v1", "summary": "The Cylindrical Algebraic Decomposition (CAD) method is currently the only\ncomplete algorithm used in practice for solving real-algebraic problems. To\nameliorate its doubly-exponential complexity, different exploration-guided\nadaptations try to avoid some of the computations. The first such adaptation\nnamed NLSAT was followed by Non-uniform CAD (NuCAD) and the Cylindrical\nAlgebraic Covering (CAlC). Both NLSAT and CAlC have been developed and\nimplemented in SMT solvers for satisfiability checking, and CAlC was recently\nalso adapted for quantifier elimination. However, NuCAD was designed for\nquantifier elimination only, and no complete implementation existed before this\nwork.\n  In this paper, we present a novel variant of NuCAD for both real quantifier\nelimination and SMT solving, provide an implementation, and evaluate the method\nby experimentally comparing it to CAlC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00505v1", "cate": "cs.SC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.16974", "title": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning", "authors": ["Zongyan Han", "Jiale Cao", "Shuo Chen", "Tong Wang", "Jorma Laaksonen", "Rao Muhammad Anwer"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.16974v2", "summary": "Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its\ncapacity to generalize segmentation beyond predefined categories. However,\nexisting methods typically predict segmentation masks with simple forward\ninference, lacking explicit reasoning and interpretability. This makes it\nchallenging for OVS model to distinguish similar categories in open-world\nsettings due to the lack of contextual understanding and discriminative visual\ncues. To address this limitation, we propose a step-by-step visual reasoning\nframework for open-vocabulary segmentation, named OpenSeg-R. The proposed\nOpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical\nvisual reasoning before segmentation. Specifically, we generate both generic\nand image-specific reasoning for each image, forming structured triplets that\nexplain the visual reason for objects in a coarse-to-fine manner. Based on\nthese reasoning steps, we can compose detailed description prompts, and feed\nthem to the segmentor to produce more accurate segmentation masks. To the best\nof our knowledge, OpenSeg-R is the first framework to introduce explicit\nstep-by-step visual reasoning into OVS. Experimental results demonstrate that\nOpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary\nsemantic segmentation across five benchmark datasets. Moreover, it achieves\nconsistent gains across all metrics on open-vocabulary panoptic segmentation.\nQualitative results further highlight the effectiveness of our reasoning-guided\nframework in improving both segmentation precision and interpretability. Our\ncode is publicly available at https://github.com/Hanzy1996/OpenSeg-R.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.16974v2", "cate": "cs.CV", "date": "2025-05-22", "updated": "2025-08-01"}
{"id": "2403.19522", "title": "Model Stock: All we need is just a few fine-tuned models", "authors": ["Dong-Hwan Jang", "Sangdoo Yun", "Dongyoon Han"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ECCV 2024 oral presenetation; Code at this https URL", "url": "http://arxiv.org/abs/2403.19522v2", "summary": "This paper introduces an efficient fine-tuning method for large pre-trained\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\nperformance. Breaking away from traditional practices that need a multitude of\nfine-tuned models for averaging, our approach employs significantly fewer\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\ninsights in the weight space of fine-tuned weights, we uncover a strong link\nbetween the performance and proximity to the center of weight space. Based on\nthis, we introduce a method that approximates a center-close weight using only\ntwo fine-tuned models, applicable during or after training. Our innovative\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\naptly coined Model Stock, highlighting its reliance on selecting a minimal\nnumber of models to draw a more optimized-averaged model. We demonstrate the\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\nstandard benchmarks, all while barely bringing extra computational demands. Our\ncode and pre-trained models are available at\nhttps://github.com/naver-ai/model-stock.", "comment": "ECCV 2024 oral presenetation; Code at\n  https://github.com/naver-ai/model-stock", "pdf_url": "http://arxiv.org/pdf/2403.19522v2", "cate": "cs.LG", "date": "2024-03-28", "updated": "2025-08-01"}
{"id": "2508.00362", "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00362v1", "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00362v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00512", "title": "Projective Delineability for Single Cell Construction", "authors": ["Jasper Nalbach", "Lucas Michel", "Erika Ábrahám", "Christopher W. Brown", "James H. Davenport", "Matthew England", "Pierre Mathonet", "Naïm Zénaïdi"], "categories": ["cs.SC"], "primary_category": "Subjects:       Symbolic Computation (cs.SC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00512v1", "summary": "The cylindrical algebraic decomposition (CAD) is the only complete method\nused in practice for solving problems like quantifier elimination or SMT\nsolving related to real algebra, despite its doubly exponential complexity.\nRecent exploration-guided algorithms like NLSAT, NuCAD, and CAlC rely on CAD\ntechnology but reduce the computational effort heuristically. Single cell\nconstruction is a paradigm that is used in each of these algorithms.\n  The central property on which the CAD algorithm is based is called\ndelineability. Recently, we introduced a weaker notion called projective\ndelineability which can require fewer computations to guarantee, but needs to\nbe applied carefully. This paper adapts the single cell construction for\nexploiting projective delineability and reports on experimental results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00512v1", "cate": "cs.SC", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.20884", "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "authors": ["Weichao Pan", "Bohan Xu", "Xu Wang", "Chengze Lv", "Shuoyang Wang", "Zhenke Duan", "Zhen Tian"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2025 International Conference on Intelligent Computing (ICIC 2025)", "url": "http://arxiv.org/abs/2505.20884v3", "summary": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%. For more details, please visit our repository:\nhttps://github.com/JEFfersusu/YOLO-FireAD", "comment": "2025 International Conference on Intelligent Computing (ICIC 2025)", "pdf_url": "http://arxiv.org/pdf/2505.20884v3", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-08-01"}
{"id": "2407.02827", "title": "Convergence of Implicit Gradient Descent for Training Two-Layer Physics-Informed Neural Networks", "authors": ["Xianliang Xu", "Ting Du", "Wang Kong", "Bin Shan", "Ye Li", "Zhongyi Huang"], "categories": ["cs.LG", "math.OC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.02827v3", "summary": "The optimization algorithms are crucial in training physics-informed neural\nnetworks (PINNs), as unsuitable methods may lead to poor solutions. Compared to\nthe common gradient descent (GD) algorithm, implicit gradient descent (IGD)\noutperforms it in handling certain multi-scale problems. In this paper, we\nprovide convergence analysis for the IGD in training over-parameterized\ntwo-layer PINNs. We first derive the training dynamics of IGD in training\ntwo-layer PINNs. Then, over-parameterization allows us to prove that the\nrandomly initialized IGD converges to a globally optimal solution at a linear\nconvergence rate. Moreover, due to the distinct training dynamics of IGD\ncompared to GD, the learning rate can be selected independently of the sample\nsize and the least eigenvalue of the Gram matrix. Additionally, the novel\napproach used in our convergence analysis imposes a milder requirement on the\nnetwork width. Finally, empirical results validate our theoretical findings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.02827v3", "cate": "cs.LG", "date": "2024-07-03", "updated": "2025-08-01"}
{"id": "2508.00384", "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025. Source Code: this https URL", "url": "http://arxiv.org/abs/2508.00384v1", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "pdf_url": "http://arxiv.org/pdf/2508.00384v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.21231", "title": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning", "authors": ["Lintao Xu", "Yinghao Wang", "Chaohui Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 4 tables, 4 figures", "url": "http://arxiv.org/abs/2505.21231v2", "summary": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing them from ordinary edges and semantic contours to support more\naccurate scene understanding. This task is closely related to Monocular Depth\nEstimation (MDE), which infers depth from a single image, as Occlusion\nBoundaries (OBs) provide critical geometric cues for resolving depth\nambiguities, while depth can conversely refine occlusion reasoning. In this\npaper, we propose MoDOT, a novel method that jointly estimates depth and OBs\nfrom a single image for the first time. MoDOT incorporates a new module, CASM,\nwhich combines cross-attention and multi-scale strip convolutions to leverage\nmid-level OB features for improved depth prediction. It also includes an\nocclusion-aware loss, OBDCL, which encourages more accurate boundaries in the\npredicted depth map. Extensive experiments demonstrate the mutual benefits of\njointly estimating depth and OBs, and validate the effectiveness of MoDOT's\ndesign. Our method achieves state-of-the-art (SOTA) performance on two\nsynthetic datasets and the widely used NYUD-v2 real-world dataset,\nsignificantly outperforming multi-task baselines. Furthermore, the cross-domain\nresults of MoDOT on real-world depth prediction - trained solely on our\nsynthetic dataset - yield promising results, preserving sharp OBs in the\npredicted depth maps and demonstrating improved geometric fidelity compared to\ncompetitors. We will release our code, pre-trained models, and dataset at\n[link].", "comment": "8 pages, 4 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2505.21231v2", "cate": "cs.CV", "date": "2025-05-27", "updated": "2025-07-31"}
{"id": "2410.01312", "title": "Sampling from Energy-based Policies using Diffusion", "authors": ["Vineet Jain", "Tara Akhound-Sadegh", "Siamak Ravanbakhsh"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.01312v2", "summary": "Energy-based policies offer a flexible framework for modeling complex,\nmultimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the\noptimal policy is a Boltzmann distribution derived from the soft Q-function,\nbut direct sampling from this distribution in continuous action spaces is\ncomputationally intractable. As a result, existing methods typically use\nsimpler parametric distributions, like Gaussians, for policy representation --\nlimiting their ability to capture the full complexity of multimodal action\ndistributions. In this paper, we introduce a diffusion-based approach for\nsampling from energy-based policies, where the negative Q-function defines the\nenergy function. Based on this approach, we propose an actor-critic method\ncalled Diffusion Q-Sampling (DQS) that enables more expressive policy\nrepresentations, allowing stable learning in diverse environments. We show that\nour approach enhances sample efficiency in continuous control tasks and\ncaptures multimodal behaviors, addressing key limitations of existing methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.01312v2", "cate": "cs.LG", "date": "2024-10-02", "updated": "2025-07-31"}
{"id": "2508.00467", "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures. This paper has been accepted for presentation at the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2508.00467v1", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00467v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00317", "title": "Advancing Speech Quality Assessment Through Scientific Challenges and Open-source Activities", "authors": ["Wen-Chin Huang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      APSIPA ASC 2025 perspective paper", "url": "http://arxiv.org/abs/2508.00317v1", "summary": "Speech quality assessment (SQA) refers to the evaluation of speech quality,\nand developing an accurate automatic SQA method that reflects human perception\nhas become increasingly important, in order to keep up with the generative AI\nboom. In recent years, SQA has progressed to a point that researchers started\nto faithfully use automatic SQA in research papers as a rigorous measurement of\ngoodness for speech generation systems. We believe that the scientific\nchallenges and open-source activities of late have stimulated the growth in\nthis field. In this paper, we review recent challenges as well as open-source\nimplementations and toolkits for SQA, and highlight the importance of\nmaintaining such activities to facilitate the development of not only SQA\nitself but also generative AI for speech.", "comment": "APSIPA ASC 2025 perspective paper", "pdf_url": "http://arxiv.org/pdf/2508.00317v1", "cate": "cs.SD", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.06854", "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page at this https URL", "url": "http://arxiv.org/abs/2506.06854v2", "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Unlike existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, thereby enhancing\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the model the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future and further improves\nperformance. Through experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "comment": "ICCV 2025. Project page at https://vision.rwth-aachen.de/donut", "pdf_url": "http://arxiv.org/pdf/2506.06854v2", "cate": "cs.CV", "date": "2025-06-07", "updated": "2025-08-01"}
{"id": "2412.19950", "title": "Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach", "authors": ["Eric Hirsch", "Christian Friedrich"], "categories": ["cs.LG", "cs.RO", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE Transactions on Automation Science and Engineering for possible publication. ,14 pages, 12 figures", "url": "http://arxiv.org/abs/2412.19950v4", "summary": "Accurate tool wear prediction is essential for maintaining productivity and\nminimizing costs in machining. However, the complex nature of the tool wear\nprocess poses significant challenges to achieving reliable predictions. This\nstudy explores data-driven methods, in particular deep learning, for tool wear\nprediction. Traditional data-driven approaches often focus on a single process,\nrelying on multi-sensor setups and extensive data generation, which limits\ngeneralization to new settings. Moreover, multi-sensor integration is often\nimpractical in industrial environments. To address these limitations, this\nresearch investigates the transferability of predictive models using minimal\ntraining data, validated across two processes. Furthermore, it uses a simple\nsetup with a single acceleration sensor to establish a low-cost data generation\napproach that facilitates the generalization of models to other processes via\ntransfer learning. The study evaluates several machine learning models,\nincluding transformer-inspired convolutional neural networks (CNN), long\nshort-term memory networks (LSTM), support vector machines (SVM), and decision\ntrees, trained on different input formats such as feature vectors and\nshort-time Fourier transform (STFT). The performance of the models is evaluated\non two machines and on different amounts of training data, including scenarios\nwith significantly reduced datasets, providing insight into their effectiveness\nunder constrained data conditions. The results demonstrate the potential of\nspecific models and configurations for effective tool wear prediction,\ncontributing to the development of more adaptable and efficient predictive\nmaintenance strategies in machining. Notably, the ConvNeXt model has an\nexceptional performance, achieving 99.1\\% accuracy in identifying tool wear\nusing data from only four milling tools operated until they are worn.", "comment": "This work has been submitted to the IEEE Transactions on Automation\n  Science and Engineering for possible publication. ,14 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2412.19950v4", "cate": "cs.LG", "date": "2024-12-27", "updated": "2025-07-31"}
{"id": "2508.00584", "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Please find the citation info @ Zenodo, ArXiv or Zenodo, as the proceedings of ICRA are no longer sent to IEEE Xplore", "url": "http://arxiv.org/abs/2508.00584v1", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "pdf_url": "http://arxiv.org/pdf/2508.00584v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00240", "title": "Ambisonics Super-Resolution Using A Waveform-Domain Neural Network", "authors": ["Ismael Nawfal", "Symeon Delikaris Manias", "Mehrez Souden", "Juha Merimaa", "Joshua Atkins", "Elisabeth McMullin", "Shadi Pirhosseinloo", "Daniel Phillips"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00240v1", "summary": "Ambisonics is a spatial audio format describing a sound field. First-order\nAmbisonics (FOA) is a popular format comprising only four channels. This\nlimited channel count comes at the expense of spatial accuracy. Ideally one\nwould be able to take the efficiency of a FOA format without its limitations.\nWe have devised a data-driven spatial audio solution that retains the\nefficiency of the FOA format but achieves quality that surpasses conventional\nrenderers. Utilizing a fully convolutional time-domain audio neural network\n(Conv-TasNet), we created a solution that takes a FOA input and provides a\nhigher order Ambisonics (HOA) output. This data driven approach is novel when\ncompared to typical physics and psychoacoustic based renderers. Quantitative\nevaluations showed a 0.6dB average positional mean squared error difference\nbetween predicted and actual 3rd order HOA. The median qualitative rating\nshowed an 80% improvement in perceived quality over the traditional rendering\napproach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00240v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.14709", "title": "DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning", "authors": ["Kunal Swami", "Debtanu Gupta", "Amrit Kumar Muduli", "Chirag Jaiswal", "Pankaj Kumar Bajpai"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in IROS 2025", "url": "http://arxiv.org/abs/2506.14709v2", "summary": "Depth estimation is crucial for intelligent systems, enabling applications\nfrom autonomous navigation to augmented reality. While traditional stereo and\nactive depth sensors have limitations in cost, power, and robustness,\ndual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling\nalternative. This paper introduces DiFuse-Net, a novel modality decoupled\nnetwork design for disentangled RGB and DP based depth estimation. DiFuse-Net\nfeatures a window bi-directional parallax attention mechanism (WBiPAM)\nspecifically designed to capture the subtle DP disparity cues unique to\nsmartphone cameras with small aperture. A separate encoder extracts contextual\ninformation from the RGB image, and these features are fused to enhance depth\nprediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to\nutilize large-scale RGB-D datasets in the literature to cope with the\nlimitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and\ncomparison of the proposed method demonstrates its superiority over the DP and\nstereo-based baseline methods. Additionally, we contribute a new, high-quality,\nreal-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP)\ndataset, created using our novel symmetric stereo camera hardware setup, stereo\ncalibration and rectification protocol, and AI stereo disparity estimation\nmethod.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2506.14709v2", "cate": "cs.CV", "date": "2025-06-17", "updated": "2025-07-31"}
{"id": "2502.17077", "title": "A comparative analysis of rank aggregation methods for the partial label ranking problem", "authors": ["Jiayi Wang", "Juan C. Alfaro", "Viktor Bengs"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Full version of the paper accepted at ECAI 2025", "url": "http://arxiv.org/abs/2502.17077v3", "summary": "The label ranking problem is a supervised learning scenario in which the\nlearner predicts a total order of the class labels for a given input instance.\nRecently, research has increasingly focused on the partial label ranking\nproblem, a generalization of the label ranking problem that allows ties in the\npredicted orders. So far, most existing learning approaches for the partial\nlabel ranking problem rely on approximation algorithms for rank aggregation in\nthe final prediction step. This paper explores several alternative aggregation\nmethods for this critical step, including scoring-based and non-parametric\nprobabilistic-based rank aggregation approaches. To enhance their suitability\nfor the more general partial label ranking problem, the investigated methods\nare extended to increase the likelihood of producing ties. Experimental\nevaluations on standard benchmarks demonstrate that scoring-based variants\nconsistently outperform the current state-of-the-art method in handling\nincomplete information. In contrast, non-parametric probabilistic-based\nvariants fail to achieve competitive performance.", "comment": "Full version of the paper accepted at ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2502.17077v3", "cate": "cs.LG", "date": "2025-02-24", "updated": "2025-08-01"}
{"id": "2508.00625", "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, a TAROS2025 short paper", "url": "http://arxiv.org/abs/2508.00625v1", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "pdf_url": "http://arxiv.org/pdf/2508.00625v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00479", "title": "Wavelet-Based Time-Frequency Fingerprinting for Feature Extraction of Traditional Irish Music", "authors": ["Noah Shore"], "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Master's thesis. The focus of the thesis is on the underlying techniques for signal fingerprinting", "url": "http://arxiv.org/abs/2508.00479v1", "summary": "This work presents a wavelet-based approach to time-frequency fingerprinting\nfor time series feature extraction, with a focus on audio identification from\nlive recordings of traditional Irish tunes. The challenges of identifying\nfeatures in time-series data are addressed by employing a continuous wavelet\ntransform to extract spectral features and wavelet coherence analysis is used\nto compare recorded audio spectrograms to synthetically generated tunes. The\nsynthetic tunes are derived from ABC notation, which is a common symbolic\nrepresentation for Irish music. Experimental results demonstrate that the\nwavelet-based method can accurately and efficiently identify recorded tunes.\nThis research study also details the performance of the wavelet coherence\nmodel, highlighting its strengths over other methods of time-frequency\ndecomposition. Additionally, we discuss and deploy the model on several\napplications beyond music, including in EEG signal analysis and financial time\nseries forecasting.", "comment": "Master's thesis. The focus of the thesis is on the underlying\n  techniques for signal fingerprinting", "pdf_url": "http://arxiv.org/pdf/2508.00479v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.16991", "title": "ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds", "authors": ["Binbin Xiang", "Maciej Wielgosz", "Stefano Puliti", "Kamil Král", "Martin Krůček", "Azim Missarov", "Rasmus Astrup"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.16991v2", "summary": "The segmentation of forest LiDAR 3D point clouds, including both individual\ntree and semantic segmentation, is fundamental for advancing forest management\nand ecological research. However, current approaches often struggle with the\ncomplexity and variability of natural forest environments. We present\nForestFormer3D, a new unified and end-to-end framework designed for precise\nindividual tree and semantic segmentation. ForestFormer3D incorporates\nISA-guided query point selection, a score-based block merging strategy during\ninference, and a one-to-many association mechanism for effective training. By\ncombining these new components, our model achieves state-of-the-art performance\nfor individual tree segmentation on the newly introduced FOR-instanceV2\ndataset, which spans diverse forest types and regions. Additionally,\nForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),\nshowcasing its robustness across different forest conditions and sensor\nmodalities. The FOR-instanceV2 dataset and the ForestFormer3D code are publicly\navailable at https://bxiang233.github.io/FF3D/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.16991v2", "cate": "cs.CV", "date": "2025-06-20", "updated": "2025-08-01"}
{"id": "2503.10845", "title": "Panopticon: Advancing Any-Sensor Foundation Models for Earth Observation", "authors": ["Leonard Waldmann", "Ando Shah", "Yi Wang", "Nils Lehmann", "Adam J. Stewart", "Zhitong Xiong", "Xiao Xiang Zhu", "Stefan Bauer", "John Chuang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      First two authors contributed equally. Code is available at: this https URL . Accepted to CVPR 2025", "url": "http://arxiv.org/abs/2503.10845v2", "summary": "Earth observation (EO) data features diverse sensing platforms with varying\nspectral bands, spatial resolutions, and sensing modalities. While most prior\nwork has constrained inputs to fixed sensors, a new class of any-sensor\nfoundation models able to process arbitrary sensors has recently emerged.\nContributing to this line of work, we propose Panopticon, an any-sensor\nfoundation model built on the DINOv2 framework. We extend DINOv2 by (1)\ntreating images of the same geolocation across sensors as natural\naugmentations, (2) subsampling channels to diversify spectral input, and (3)\nadding a cross attention over channels as a flexible patch embedding mechanism.\nBy encoding the wavelength and modes of optical and synthetic aperture radar\nsensors, respectively, Panopticon can effectively process any combination of\narbitrary channels. In extensive evaluations, we achieve state-of-the-art\nperformance on GEO-Bench, especially on the widely-used Sentinel-1 and\nSentinel-2 sensors, while out-competing other any-sensor models, as well as\ndomain adapted fixed-sensor models on unique sensor configurations. Panopticon\nenables immediate generalization to both existing and future satellite\nplatforms, advancing sensor-agnostic EO.", "comment": "First two authors contributed equally. Code is available at:\n  https://github.com/Panopticon-FM/panopticon. Accepted to CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2503.10845v2", "cate": "cs.LG", "date": "2025-03-13", "updated": "2025-08-01"}
{"id": "2508.00691", "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures, 2 tables", "url": "http://arxiv.org/abs/2508.00691v1", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "comment": "8 pages, 6 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2508.00691v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00501", "title": "VR-PTOLEMAIC: A Virtual Environment for the Perceptual Testing of Spatial Audio Algorithms", "authors": ["Paolo Ostan", "Francesca Del Gaudio", "Federico Miotello", "Mirco Pezzoli", "Fabio Antonacci"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      to appear in EAA Forum Acusticum 2025", "url": "http://arxiv.org/abs/2508.00501v1", "summary": "The perceptual evaluation of spatial audio algorithms is an important step in\nthe development of immersive audio applications, as it ensures that synthesized\nsound fields meet quality standards in terms of listening experience, spatial\nperception and auditory realism. To support these evaluations, virtual reality\ncan offer a powerful platform by providing immersive and interactive testing\nenvironments. In this paper, we present VR-PTOLEMAIC, a virtual reality\nevaluation system designed for assessing spatial audio algorithms. The system\nimplements the MUSHRA (MUlti-Stimulus test with Hidden Reference and Anchor)\nevaluation methodology into a virtual environment. In particular, users can\nposition themselves in each of the 25 simulated listening positions of a\nvirtually recreated seminar room and evaluate simulated acoustic responses with\nrespect to the actually recorded second-order ambisonic room impulse responses,\nall convolved with various source signals. We evaluated the usability of the\nproposed framework through an extensive testing campaign in which assessors\nwere asked to compare the reconstruction capabilities of various sound field\nreconstruction algorithms. Results show that the VR platform effectively\nsupports the assessment of spatial audio algorithms, with generally positive\nfeedback on user experience and immersivity.", "comment": "to appear in EAA Forum Acusticum 2025", "pdf_url": "http://arxiv.org/pdf/2508.00501v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.18292", "title": "Three-dimentional reconstruction of complex, dynamic population canopy architecture for crops with a novel point cloud completion model: A case study in Brassica napus rapeseed", "authors": ["Ziyue Guo", "Xin Yang", "Yutao Shen", "Yang Zhu", "Lixi Jiang", "Haiyan Cen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.18292v2", "summary": "Quantitative descriptions of the complete canopy architecture are essential\nfor accurately evaluating crop photosynthesis and yield performance to guide\nideotype design. Although various sensing technologies have been developed for\nthree-dimensional (3D) reconstruction of individual plants and canopies, they\nfailed to obtain an accurate description of canopy architectures due to severe\nocclusion among complex canopy architectures. We proposed an effective method\nfor 3D reconstruction of complex, dynamic population canopy architecture for\nrapeseed crops with a novel point cloud completion model. A complete point\ncloud generation framework was developed for automated annotation of the\ntraining dataset by distinguishing surface points from occluded points within\ncanopies. The crop population point cloud completion network (CP-PCN) was then\ndesigned with a multi-resolution dynamic graph convolutional encoder (MRDG) and\na point pyramid decoder (PPD) to predict occluded points. To further enhance\nfeature extraction, a dynamic graph convolutional feature extractor (DGCFE)\nmodule was proposed to capture structural variations over the whole rapeseed\ngrowth period. The results demonstrated that CP-PCN achieved chamfer distance\n(CD) values of 3.35 cm -4.51 cm over four growth stages, outperforming the\nstate-of-the-art transformer-based method (PoinTr). Ablation studies confirmed\nthe effectiveness of the MRDG and DGCFE modules. Moreover, the validation\nexperiment demonstrated that the silique efficiency index developed from CP-PCN\nimproved the overall accuracy of rapeseed yield prediction by 11.2% compared to\nthat of using incomplete point clouds. The CP-PCN pipeline has the potential to\nbe extended to other crops, significantly advancing the quantitatively analysis\nof in-field population canopy architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.18292v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-08-01"}
{"id": "2503.13544", "title": "Decision by Supervised Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization", "authors": ["Juhyeong Kim", "Sungyoon Choi", "Youngbin Lee", "Yejin Kim", "Yongmin Choi", "Yongjae Lee"], "categories": ["cs.LG", "q-fin.CP", "q-fin.PM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2503.13544v5", "summary": "We propose Decision by Supervised Learning (DSL), a practical framework for\nrobust portfolio optimization. DSL reframes portfolio construction as a\nsupervised learning problem: models are trained to predict optimal portfolio\nweights, using cross-entropy loss and portfolios constructed by maximizing the\nSharpe or Sortino ratio. To further enhance stability and reliability, DSL\nemploys Deep Ensemble methods, substantially reducing variance in portfolio\nallocations. Through comprehensive backtesting across diverse market universes\nand neural architectures, shows superior performance compared to both\ntraditional strategies and leading machine learning-based methods, including\nPrediction-Focused Learning and End-to-End Learning. We show that increasing\nthe ensemble size leads to higher median returns and more stable risk-adjusted\nperformance. The code is available at https://github.com/DSLwDE/DSLwDE.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2503.13544v5", "cate": "cs.LG", "date": "2025-03-16", "updated": "2025-08-01"}
{"id": "2508.00795", "title": "Video Generators are Robot Policies", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00795v1", "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00795v1", "cate": "cs.RO", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2505.24437", "title": "SwitchCodec: A High-Fidelity Nerual Audio Codec With Sparse Quantization", "authors": ["Jin Wang", "Wenbin Jiang", "Xiangbo Wang", "Yubo You", "Sheng Fang"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      12 pages,8 figures", "url": "http://arxiv.org/abs/2505.24437v3", "summary": "Neural audio compression has emerged as a promising technology for\nefficiently representing speech, music, and general audio. However, existing\nmethods suffer from significant performance degradation at limited bitrates,\nwhere the available embedding space is sharply constrained. To address this, we\npropose a universal high-fidelity neural audio compression algorithm featuring\nResidual Experts Vector Quantization (REVQ), which substantially expands the\nembedding space with minimal impact on bandwidth. A gentle load-balancing\nstrategy is introduced to ensure the full utilization of this expanded space.\nFurthermore, we develop a novel multi-tiered discriminator that periodically\nstratifies STFT spectra, guiding the generator to focus on critical spectral\nregions. To support multiple bitrates without quality loss at the lower end, we\nadopt an efficient post-training strategy. Our proposed model achieves\nimpressive performance, with PESQ and ViSQOL scores of 2.87 and 4.27,\nrespectively, at 2.67 kbps bandwidth. The approach effectively reduces spectral\nblur, decreasing the distance to the original mel-spectrogram by 13%. Notably,\nour post-training strategy achieves performance comparable to dedicated\nfixed-bitrate models while reducing the required training time by half.\nExtensive ablation studies confirm the superiority of our method over\nbaselines.", "comment": "12 pages,8 figures", "pdf_url": "http://arxiv.org/pdf/2505.24437v3", "cate": "cs.SD", "date": "2025-05-30", "updated": "2025-08-01"}
{"id": "2506.19808", "title": "ProtoSolo: Interpretable Image Classification via Single-Prototype Activation", "authors": ["Yitao Peng", "Lianghua He", "Hongzhou Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19808v3", "summary": "Although interpretable prototype networks have improved the transparency of\ndeep learning image classification, the need for multiple prototypes in\ncollaborative decision-making increases cognitive complexity and hinders user\nunderstanding. To solve this problem, this paper proposes a novel interpretable\ndeep architecture for image classification, called ProtoSolo. Unlike existing\nprototypical networks, ProtoSolo requires activation of only a single prototype\nto complete the classification. This design significantly simplifies\ninterpretation, as the explanation for each class requires displaying only the\nprototype with the highest similarity score and its corresponding feature map.\nAdditionally, the traditional full-channel feature vector is replaced with a\nfeature map for similarity comparison and prototype learning, enabling the use\nof richer global information within a single-prototype activation decision. A\nnon-projection prototype learning strategy is also introduced to preserve the\nassociation between the prototype and image patch while avoiding abrupt\nstructural changes in the network caused by projection, which can affect\nclassification performance. Experiments on the CUB-200-2011 and Stanford Cars\ndatasets demonstrate that ProtoSolo matches state-of-the-art interpretable\nmethods in classification accuracy while achieving the lowest cognitive\ncomplexity. The code is available at https://github.com/pyt19/ProtoSolo.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19808v3", "cate": "cs.CV", "date": "2025-06-24", "updated": "2025-07-31"}
{"id": "2504.04202", "title": "Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences", "authors": ["Harvey Dam", "Tripti Agarwal", "Ganesh Gopalakrishnan"], "categories": ["cs.LG", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04202v3", "summary": "Preserving topological features in learned latent spaces is a fundamental\nchallenge in representation learning, particularly for topology-sensitive data.\nThis paper introduces directional sign loss (DSL), an efficient, differentiable\nloss function that approximates the number of mismatches in the signs of finite\ndifferences between corresponding elements of two arrays. By penalizing\ndiscrepancies in critical points between input and reconstructed data, DSL\nencourages autoencoders and other learnable compressors to retain the\ntopological features of the original data. We present the formulation and\ncomplexity analysis of DSL, comparing it to other non-differentiable\ntopological measures. Experiments on multidimensional array data show that\ncombining DSL with traditional loss functions preserves topological features\nmore effectively than traditional losses alone. DSL serves as a differentiable,\nefficient proxy for common topology-based metrics, enabling topological feature\npreservation on previously impractical problem sizes and in a wider range of\ngradient-based optimization frameworks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04202v3", "cate": "cs.LG", "date": "2025-04-05", "updated": "2025-07-31"}
{"id": "2508.00724", "title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems", "authors": ["Boyu Li", "Zhengchen Li", "Weimin Wu", "Mengchu Zhou"], "categories": ["eess.SY", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2508.00724v1", "summary": "The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2508.00724v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2506.00291", "title": "Improving Code Switching with Supervised Fine Tuning and GELU Adapters", "authors": ["Linh Pham"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Incorrect results", "url": "http://arxiv.org/abs/2506.00291v2", "summary": "There are few code switching datasets, labeled or unlabled, that exist today.\nAs a result, ASR requires new methods to utilize the vast monolingual data and\nmodels that exist. This paper uses OpenAI's open source ASR model, Whisper,\nwhich has been pre-trained on 680K hours of audio to perform monolingual ASR\ntasks. In Part 1, this paper examines how exploiting Whisper's monolingual\nability to individually tokenize training text, called \"Switching Tokenizers\nMethod\", improves transcription accuracy. In Part 2, we combine the Switching\nTokenizers Method from part 1 and train a GELU based adapter on the encoder.\nThese two methods reduced Total Mixed Error Rate (MER) to 9.4% for the ASCEND\ndataset, 6% for SEAME devman and 9.7% for SEAME devsge, outperforming current\nSoTA methods.", "comment": "Incorrect results", "pdf_url": "http://arxiv.org/pdf/2506.00291v2", "cate": "cs.SD", "date": "2025-05-30", "updated": "2025-08-01"}
{"id": "2507.06603", "title": "Cross-Modal Dual-Causal Learning for Long-Term Action Recognition", "authors": ["Xu Shaowu", "Jia Xibin", "Gao Junyu", "Sun Qianmei", "Chang Jing", "Fan Chao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06603v2", "summary": "Long-term action recognition (LTAR) is challenging due to extended temporal\nspans with complex atomic action correlations and visual confounders. Although\nvision-language models (VLMs) have shown promise, they often rely on\nstatistical correlations instead of causal mechanisms. Moreover, existing\ncausality-based methods address modal-specific biases but lack cross-modal\ncausal modeling, limiting their utility in VLM-based LTAR. This paper proposes\n\\textbf{C}ross-\\textbf{M}odal \\textbf{D}ual-\\textbf{C}ausal \\textbf{L}earning\n(CMDCL), which introduces a structural causal model to uncover causal\nrelationships between videos and label texts.\n  CMDCL addresses cross-modal biases in text embeddings via textual causal\nintervention and removes confounders inherent in the visual modality through\nvisual causal intervention guided by the debiased text.\n  These dual-causal interventions enable robust action representations to\naddress LTAR challenges. Experimental results on three benchmarks including\nCharades, Breakfast and COIN, demonstrate the effectiveness of the proposed\nmodel. Our code is available at https://github.com/xushaowu/CMDCL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06603v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-08-01"}
{"id": "2504.09664", "title": "Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting", "authors": ["Anxian Liu", "Junying Ma", "Guang Zhang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.09664v2", "summary": "Financial time series forecasting in zero-shot settings is critical for\ninvestment decisions, especially during abrupt market regime shifts or in\nemerging markets with limited historical data. While Model-Agnostic\nMeta-Learning (MAML) approaches show promise, existing meta-task construction\nstrategies often yield suboptimal performance for highly turbulent financial\nseries. To address this, we propose a novel task-construction method that\nleverages learned embeddings for both meta task and also downstream\npredictions, enabling effective zero-shot meta-learning. Specifically, we use\nGaussian Mixture Models (GMMs) to softly cluster embeddings, constructing two\ncomplementary meta-task types: intra-cluster tasks and inter-cluster tasks. By\nassigning embeddings to multiple latent regimes probabilistically, GMMs enable\nricher, more diverse meta-learning. This dual approach ensures the model can\nquickly adapt to local patterns while simultaneously capturing invariant\ncross-series features. Furthermore, we enhance inter-cluster generalization\nthrough hard task mining, which identifies robust patterns across divergent\nmarket regimes. Our method was validated using real-world financial data from\nhigh-volatility periods and multiple international markets (including emerging\nmarkets). The results demonstrate significant out-performance over existing\napproaches and stronger generalization in zero-shot scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.09664v2", "cate": "cs.LG", "date": "2025-04-13", "updated": "2025-08-01"}
{"id": "2403.09596", "title": "Scalable Outdoors Autonomous Drone Flight with Visual-Inertial SLAM and Dense Submaps Built without LiDAR", "authors": ["Sebastián Barbas Laina", "Simon Boche", "Sotiris Papatheodorou", "Dimos Tzoumanikas", "Simon Schaefer", "Hanzhi Chen", "Stefan Leutenegger"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures", "url": "http://arxiv.org/abs/2403.09596v2", "summary": "Autonomous navigation is needed for several robotics applications. In this\npaper we present an autonomous Micro Aerial Vehicle (MAV) system which purely\nrelies on cost-effective and light-weight passive visual and inertial sensors\nto perform large-scale autonomous navigation in outdoor,unstructured and\ncluttered environments. We leverage visual-inertial simultaneous localization\nand mapping (VI-SLAM) for accurate MAV state estimates and couple it with a\nvolumetric occupancy submapping system to achieve a scalable mapping framework\nwhich can be directly used for path planning. To ensure the safety of the MAV\nduring navigation, we also propose a novel reference trajectory anchoring\nscheme that deforms the reference trajectory the MAV is tracking upon state\nupdates from the VI-SLAM system in a consistent way, even upon large state\nupdates due to loop-closures. We thoroughly validate our system in both real\nand simulated forest environments and at peak velocities up to 3 m/s while not\nencountering a single collision or system failure. To the best of our\nknowledge, this is the first system which achieves this level of performance in\nsuch an unstructured environment using low-cost passive visual sensors and\nfully on-board computation, including VI-SLAM.", "comment": "8 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2403.09596v2", "cate": "cs.RO", "date": "2024-03-14", "updated": "2025-08-01"}
{"id": "2409.08374", "title": "OpenACE: An Open Benchmark for Evaluating Audio Coding Performance", "authors": ["Jozef Coldenhoff", "Niclas Granqvist", "Milos Cernak"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      ICASSP 2025", "url": "http://arxiv.org/abs/2409.08374v2", "summary": "Audio and speech coding lack unified evaluation and open-source testing. Many\ncandidate systems were evaluated on proprietary, non-reproducible, or small\ndata, and machine learning-based codecs are often tested on datasets with\nsimilar distributions as trained on, which is unfairly compared to digital\nsignal processing-based codecs that usually work well with unseen data. This\npaper presents a full-band audio and speech coding quality benchmark with more\nvariable content types, including traditional open test vectors. An example use\ncase of audio coding quality assessment is presented with open-source Opus,\n3GPP's EVS, and recent ETSI's LC3 with LC3+ used in Bluetooth LE Audio\nprofiles. Besides, quality variations of emotional speech encoding at 16 kbps\nare shown. The proposed open-source benchmark contributes to audio and speech\ncoding democratization and is available at\nhttps://github.com/JozefColdenhoff/OpenACE.", "comment": "ICASSP 2025", "pdf_url": "http://arxiv.org/pdf/2409.08374v2", "cate": "eess.AS", "date": "2024-09-12", "updated": "2025-08-01"}
{"id": "2508.00031", "title": "Git Context Controller: Manage the Context of LLM-based Agents like Git", "authors": ["Junde Wu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      in updating", "url": "http://arxiv.org/abs/2508.00031v1", "summary": "Large language model (LLM) based agents have shown impressive capabilities by\ninterleaving internal reasoning with external tool use. However, as these\nagents are deployed in long-horizon workflows, such as coding for a big,\nlong-term project, context management becomes a critical bottleneck. We\nintroduce Git-Context-Controller (GCC), a structured context management\nframework inspired by software version control systems. GCC elevates context as\nversioned memory hierarchy like Git. It structures agent memory as a persistent\nfile system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,\nenabling milestone-based checkpointing, exploration of alternative plans, and\nstructured reflection. Our approach empowers agents to manage long-term goals,\nisolate architectural experiments, and recover or hand off memory across\nsessions and agents. Empirically, agents equipped with GCC achieve\nstate-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00\nof software bugs, outperforming 26 competitive systems. In a self-replication\ncase study, a GCC-augmented agent builds a new CLI agent from scratch,\nachieving 40.7 task resolution, compared to only 11.7 without GCC. The code is\nreleased at: https://github.com/theworldofagents/GCC", "comment": "in updating", "pdf_url": "http://arxiv.org/pdf/2508.00031v1", "cate": "cs.SE", "date": "2025-07-30", "updated": "2025-07-30"}
{"id": "2507.14095", "title": "C-DOG: Multi-View Multi-instance Feature Association Using Connected δ-Overlap Graphs", "authors": ["Yung-Hong Sun", "Ting-Hung Lin", "Jiangang Chen", "Hongrui Jiang", "Yu Hen Hu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14095v2", "summary": "Multi-view multi-instance feature association constitutes a crucial step in\n3D reconstruction, facilitating the consistent grouping of object instances\nacross various camera perspectives. The presence of multiple identical objects\nwithin a scene often leads to ambiguities for appearance-based feature matching\nalgorithms. Our work circumvents this challenge by exclusively employing\ngeometrical constraints, specifically epipolar geometry, for feature\nassociation. We introduce C-DOG (Connected delta-Overlap Graph), an algorithm\ndesigned for robust geometrical feature association, even in the presence of\nnoisy feature detections. In a C-DOG graph, two nodes representing 2D feature\npoints from distinct views are connected by an edge if they correspond to the\nsame 3D point. Each edge is weighted by its epipolar distance. Ideally, true\nassociations yield a zero distance; however, noisy feature detections can\nresult in non-zero values. To robustly retain edges where the epipolar distance\nis less than a threshold delta, we employ a Szymkiewicz--Simpson coefficient.\nThis process leads to a delta-neighbor-overlap clustering of 2D nodes.\nFurthermore, unreliable nodes are pruned from these clusters using an\nInter-quartile Range (IQR)-based criterion. Our extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG not only outperforms geometry-based\nbaseline algorithms but also remains remarkably robust under demanding\nconditions. This includes scenes with high object density, no visual features,\nand restricted camera overlap, positioning C-DOG as an excellent solution for\nscalable 3D reconstruction in practical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14095v2", "cate": "cs.CV", "date": "2025-07-18", "updated": "2025-08-01"}
{"id": "2504.20908", "title": "MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability", "authors": ["Wenxin Chen", "Weishen Pan", "Kyra Gan", "Fei Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20908v2", "summary": "Current subgroup identification methods typically follow a two-step approach:\nfirst estimate conditional average treatment effects and then apply\nthresholding or rule-based procedures to define subgroups. While intuitive,\nthis decoupled approach fails to incorporate key constraints essential for\nreal-world clinical decision-making, such as subgroup size and propensity\noverlap. These constraints operate on fundamentally different axes than CATE\nestimation and are not naturally accommodated within existing frameworks,\nthereby limiting the practical applicability of these methods. We propose a\nunified optimization framework that directly solves the primal constrained\noptimization problem to identify optimal subgroups. Our key innovation is a\nreformulation of the constrained primal problem as an unconstrained\ndifferentiable min-max objective, solved via a gradient descent-ascent\nalgorithm. We theoretically establish that our solution converges to a feasible\nand locally optimal solution. Unlike threshold-based CATE methods that apply\nconstraints as post-hoc filters, our approach enforces them directly during\noptimization. The framework is model-agnostic, compatible with a wide range of\nCATE estimators, and extensible to additional constraints like cost limits or\nfairness criteria. Extensive experiments on synthetic and real-world datasets\ndemonstrate its effectiveness in identifying high-benefit subgroups while\nmaintaining better satisfaction of constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20908v2", "cate": "cs.LG", "date": "2025-04-29", "updated": "2025-08-01"}
{"id": "2403.17667", "title": "Learning Goal-Directed Object Pushing in Cluttered Scenes With Location-Based Attention", "authors": ["Nils Dengler", "Juan Del Aguila Ferrandis", "João Moura", "Sethu Vijayakumar", "Maren Bennewitz"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2025", "url": "http://arxiv.org/abs/2403.17667v3", "summary": "In complex scenarios where typical pick-and-place techniques are\ninsufficient, often non-prehensile manipulation can ensure that a robot is able\nto fulfill its task. However, non-prehensile manipulation is challenging due to\nits underactuated nature with hybrid-dynamics, where a robot needs to reason\nabout an object's long-term behavior and contact-switching, while being robust\nto contact uncertainty. The presence of clutter in the workspace further\ncomplicates this task, introducing the need to include more advanced spatial\nanalysis to avoid unwanted collisions. Building upon prior work on\nreinforcement learning with multimodal categorical exploration for planar\npushing, we propose to incorporate location-based attention to enable robust\nmanipulation in cluttered scenes. Unlike previous approaches addressing this\nobstacle avoiding pushing task, our framework requires no predefined global\npaths and considers the desired target orientation of the manipulated object.\nExperimental results in simulation as well as with a real KUKA iiwa robot arm\ndemonstrate that our learned policy manipulates objects successfully while\navoiding collisions through complex obstacle configurations, including dynamic\nobstacles, to reach the desired target pose.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)2025", "pdf_url": "http://arxiv.org/pdf/2403.17667v3", "cate": "cs.RO", "date": "2024-03-26", "updated": "2025-08-01"}
{"id": "2508.00045", "title": "Machine Learning Pipeline for Software Engineering: A Systematic Literature Review", "authors": ["Samah Kansab"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00045v1", "summary": "The rapid advancement of software development practices has introduced\nchallenges in ensuring quality and efficiency across the software engineering\n(SE) lifecycle. As SE systems grow in complexity, traditional approaches often\nfail to scale, resulting in longer debugging times, inefficient defect\ndetection, and resource-heavy development cycles. Machine Learning (ML) has\nemerged as a key solution, enabling automation in tasks such as defect\nprediction, code review, and release quality estimation. However, the\neffectiveness of ML in SE depends on the robustness of its pipeline, including\ndata collection, preprocessing, feature engineering, algorithm selection,\nvalidation, and evaluation.\n  This systematic literature review (SLR) examines state-of-the-art ML\npipelines designed for SE, consolidating best practices, challenges, and gaps.\nOur findings show that robust preprocessing, such as SMOTE for data balancing\nand SZZ-based algorithms for feature selection, improves model reliability.\nEnsemble methods like Random Forest and Gradient Boosting dominate performance\nacross tasks, while simpler models such as Naive Bayes remain valuable for\nefficiency and interpretability. Evaluation metrics including AUC, F1-score,\nand precision are most common, with new metrics like Best Arithmetic Mean (BAM)\nemerging in niche applications. Validation techniques such as bootstrapping are\nwidely used to ensure model stability and generalizability.\n  This SLR highlights the importance of well-designed ML pipelines for\naddressing SE challenges and provides actionable insights for researchers and\npractitioners seeking to optimize software quality and efficiency. By\nidentifying gaps and trends, this study sets a foundation for advancing ML\nadoption and fostering innovation in increasingly complex development\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00045v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.15059", "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling", "authors": ["Ran Zhang", "Xuanhua He", "Li Xueheng", "Ke Cao", "Liu Liu", "Wenbo Xu", "Fang Jiabin", "Yang Qize", "Jie Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15059v2", "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15059v2", "cate": "cs.CV", "date": "2025-07-20", "updated": "2025-08-01"}
{"id": "2505.02634", "title": "Transfer learning-enhanced deep reinforcement learning for aerodynamic airfoil optimisation subject to structural constraints", "authors": ["David Ramos", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted in Physics of Fluids 20 pages, 7 figures", "url": "http://arxiv.org/abs/2505.02634v2", "summary": "The main objective of this paper is to introduce a transfer learning-enhanced\ndeep reinforcement learning (DRL) methodology that is able to optimise the\ngeometry of any airfoil based on concomitant aerodynamic and structural\nintegrity criteria. To showcase the method, we aim to maximise the lift-to-drag\nratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as\nmodelled by its maximum thickness -- and train the DRL agent using a list of\ndifferent transfer learning (TL) strategies. The performance of the DRL agent\nis compared with Particle Swarm Optimisation (PSO), a traditional gradient-free\noptimisation method. Results indicate that DRL agents are able to perform\npurely aerodynamic and hybrid aerodynamic/structural shape optimisation, that\nthe DRL approach outperforms PSO in terms of computational efficiency and\naerodynamic improvement, and that the TL-enhanced DRL agent achieves\nperformance comparable to the DRL one, while further saving substantial\ncomputational resources.", "comment": "Accepted in Physics of Fluids 20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2505.02634v2", "cate": "cs.LG", "date": "2025-05-05", "updated": "2025-08-01"}
{"id": "2409.11372", "title": "PC-SRIF: Preconditioned Cholesky-based Square Root Information Filter for Vision-aided Inertial Navigation", "authors": ["Tong Ke", "Parth Agrawal", "Yun Zhang", "Weikun Zhen", "Chao X. Guo", "Toby Sharp", "Ryan C. Dutoit"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2409.11372v3", "summary": "In this paper, we introduce a novel estimator for vision-aided inertial\nnavigation systems (VINS), the Preconditioned Cholesky-based Square Root\nInformation Filter (PC-SRIF). When solving linear systems, employing Cholesky\ndecomposition offers superior efficiency but can compromise numerical\nstability. Due to this, existing VINS utilizing (Square Root) Information\nFilters often opt for QR decomposition on platforms where single precision is\npreferred, avoiding the numerical challenges associated with Cholesky\ndecomposition. While these issues are often attributed to the ill-conditioned\ninformation matrix in VINS, our analysis reveals that this is not an inherent\nproperty of VINS but rather a consequence of specific parameterizations. We\nidentify several factors that contribute to an ill-conditioned information\nmatrix and propose a preconditioning technique to mitigate these conditioning\nissues. Building on this analysis, we present PC-SRIF, which exhibits\nremarkable stability in performing Cholesky decomposition in single precision\nwhen solving linear systems in VINS. Consequently, PC-SRIF achieves superior\ntheoretical efficiency compared to alternative estimators. To validate the\nefficiency advantages and numerical stability of PC-SRIF based VINS, we have\nconducted well controlled experiments, which provide empirical evidence in\nsupport of our theoretical findings. Remarkably, in our VINS implementation,\nPC-SRIF's runtime is 41% faster than QR-based SRIF.", "comment": "This work has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2409.11372v3", "cate": "cs.RO", "date": "2024-09-17", "updated": "2025-07-31"}
{"id": "2508.00128", "title": "How Quantization Impacts Privacy Risk on LLMs for Code?", "authors": ["Md Nazmul Haque", "Hua Yang", "Zhou Yang", "Bowen Xu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00128v1", "summary": "Large language models for code (LLMs4Code) rely heavily on massive training\ndata, including sensitive data, such as cloud service credentials of the\nprojects and personal identifiable information of the developers, raising\nserious privacy concerns. Membership inference (MI) has recently emerged as an\neffective tool for assessing privacy risk by identifying whether specific data\nbelong to a model's training set. In parallel, model compression techniques,\nespecially quantization, have gained traction for reducing computational costs\nand enabling the deployment of large models. However, while quantized models\nstill retain knowledge learned from the original training data, it remains\nunclear whether quantization affects their ability to retain and expose privacy\ninformation. Answering this question is of great importance to understanding\nprivacy risks in real-world deployments. In this work, we conduct the first\nempirical study on how quantization influences task performance and privacy\nrisk simultaneously in LLMs4Code. To do this, we implement widely used\nquantization techniques (static and dynamic) to three representative model\nfamilies, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that\nquantization has a significant impact on reducing the privacy risk relative to\nthe original model. We also uncover a positive correlation between task\nperformance and privacy risk, indicating an underlying tradeoff. Moreover, we\nreveal the possibility that quantizing larger models could yield better balance\nthan using full-precision small models. Finally, we demonstrate that these\nfindings generalize across different architectures, model sizes and MI methods,\noffering practical guidance for safeguarding privacy when deploying compressed\nLLMs4Code.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00128v1", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.15085", "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15085v2", "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15085v2", "cate": "cs.CV", "date": "2025-07-20", "updated": "2025-08-01"}
{"id": "2505.06351", "title": "Latent Diffeomorphic Dynamic Mode Decomposition", "authors": ["Willem Diepeveen", "Jon Schwenk", "Andrea Bertozzi"], "categories": ["cs.LG", "math.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.06351v2", "summary": "We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new\ndata reduction approach for the analysis of non-linear systems that combines\nthe interpretability of Dynamic Mode Decomposition (DMD) with the predictive\npower of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity,\nwhich enhances interpretability, while effectively modeling and learning\ncomplex non-linear systems with memory, enabling accurate predictions. This is\nexemplified by its successful application in streamflow prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.06351v2", "cate": "cs.LG", "date": "2025-05-09", "updated": "2025-08-01"}
{"id": "2410.14968", "title": "AugInsert: Learning Robust Visual-Force Policies via Data Augmentation for Object Assembly Tasks", "authors": ["Ryan Diaz", "Adam Imdieke", "Vivek Veeriah", "Karthik Desingh"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025", "url": "http://arxiv.org/abs/2410.14968v2", "summary": "Operating in unstructured environments like households requires robotic\npolicies that are robust to out-of-distribution conditions. Although much work\nhas been done in evaluating robustness for visuomotor policies, the robustness\nevaluation of a multisensory approach that includes force-torque sensing\nremains largely unexplored. This work introduces a novel, factor-based\nevaluation framework with the goal of assessing the robustness of multisensory\npolicies in a peg-in-hole assembly task. To this end, we develop a multisensory\npolicy framework utilizing the Perceiver IO architecture to learn the task. We\ninvestigate which factors pose the greatest generalization challenges in object\nassembly and explore a simple multisensory data augmentation technique to\nenhance out-of-distribution performance. We provide a simulation environment\nenabling controlled evaluation of these factors. Our results reveal that\nmultisensory variations such as Grasp Pose present the most significant\nchallenges for robustness, and naive unisensory data augmentation applied\nindependently to each sensory modality proves insufficient to overcome them.\nAdditionally, we find force-torque sensing to be the most informative modality\nfor our contact-rich assembly task, with vision being the least informative.\nFinally, we briefly discuss supporting real-world experimental results. For\nadditional experiments and qualitative results, we refer to the project webpage\nhttps://rpm-lab-umn.github.io/auginsert/ .", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "pdf_url": "http://arxiv.org/pdf/2410.14968v2", "cate": "cs.RO", "date": "2024-10-19", "updated": "2025-08-01"}
{"id": "2508.00198", "title": "Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems", "authors": ["Cleyton Magalhaes", "Italo Santos", "Brody Stuart-Verner", "Ronnie de Souza Santos"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00198v2", "summary": "Background: Software systems powered by large language models are becoming a\nroutine part of everyday technologies, supporting applications across a wide\nrange of domains. In software engineering, many studies have focused on how\nLLMs support tasks such as code generation, debugging, and documentation.\nHowever, there has been limited focus on how full systems that integrate LLMs\nare tested during development. Aims: This study explores how LLM-powered\nsystems are tested in the context of real-world application development.\nMethod: We conducted an exploratory case study using 99 individual reports\nwritten by students who built and deployed LLM-powered applications as part of\na university course. Each report was independently analyzed using thematic\nanalysis, supported by a structured coding process. Results: Testing strategies\ncombined manual and automated methods to evaluate both system logic and model\nbehavior. Common practices included exploratory testing, unit testing, and\nprompt iteration. Reported challenges included integration failures,\nunpredictable outputs, prompt sensitivity, hallucinations, and uncertainty\nabout correctness. Conclusions: Testing LLM-powered systems required\nadaptations to traditional verification methods, blending source-level\nreasoning with behavior-aware evaluations. These findings provide evidence on\nthe practical context of testing generative components in software systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00198v2", "cate": "cs.SE", "date": "2025-07-31", "updated": "2025-08-04"}
{"id": "2507.17312", "title": "CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance", "authors": ["Peiqi Chen", "Lei Yu", "Yi Wan", "Yingying Pei", "Xinyi Liu", "Yongxiang Yao", "Yingying Zhang", "Lixiang Ru", "Liheng Zhong", "Jingdong Chen", "Ming Yang", "Yongjun Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.17312v2", "summary": "Semi-dense feature matching methods have shown strong performance in\nchallenging scenarios. However, the existing pipeline relies on a global search\nacross the entire feature map to establish coarse matches, limiting further\nimprovements in accuracy and efficiency. Motivated by this limitation, we\npropose a novel pipeline, CasP, which leverages cascaded correspondence priors\nfor guidance. Specifically, the matching stage is decomposed into two\nprogressive phases, bridged by a region-based selective cross-attention\nmechanism designed to enhance feature discriminability. In the second phase,\none-to-one matches are determined by restricting the search range to the\none-to-many prior areas identified in the first phase. Additionally, this\npipeline benefits from incorporating high-level features, which helps reduce\nthe computational costs of low-level feature extraction. The acceleration gains\nof CasP increase with higher resolution, and our lite model achieves a speedup\nof $\\sim2.2\\times$ at a resolution of 1152 compared to the most efficient\nmethod, ELoFTR. Furthermore, extensive experiments demonstrate its superiority\nin geometric estimation, particularly with impressive cross-domain\ngeneralization. These advantages highlight its potential for latency-sensitive\nand high-robustness applications, such as SLAM and UAV systems. Code is\navailable at https://github.com/pq-chen/CasP.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.17312v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-08-01"}
{"id": "2505.08320", "title": "Adaptive Branch Specialization in Spectral-Spatial Graph Neural Networks for Certified Robustness", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.08320v3", "summary": "Recent Graph Neural Networks (GNNs) combine spectral-spatial architectures\nfor enhanced representation learning. However, limited attention has been paid\nto certified robustness, particularly regarding training strategies and\nunderlying rationale. In this paper, we explicitly specialize each branch: the\nspectral network is trained to withstand l0 edge flips and capture homophilic\nstructures, while the spatial part is designed to resist linf feature\nperturbations and heterophilic patterns. A context-aware gating network\nadaptively fuses the two representations, dynamically routing each node's\nprediction to the more reliable branch. This specialized adversarial training\nscheme uses branch-specific inner maximization (structure vs feature attacks)\nand a unified alignment objective. We provide theoretical guarantees: (i)\nexpressivity of the gating mechanism beyond 1-WL, (ii) spectral-spatial\nfrequency bias, and (iii) certified robustness with trade-off. Empirically,\nSpecSphere attains state-of-the-art node classification accuracy and offers\ntighter certified robustness on real-world benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.08320v3", "cate": "cs.LG", "date": "2025-05-13", "updated": "2025-08-01"}
{"id": "2502.04600", "title": "Cooperative Payload Estimation by a Team of Mocobots", "authors": ["Haoxuan Zhang", "C. Lin Liu", "Matthew L. Elwin", "Randy A. Freeman", "Kevin M. Lynch"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to IEEE Robotics and Automation Letters (RA-L)", "url": "http://arxiv.org/abs/2502.04600v3", "summary": "For high-performance autonomous manipulation of a payload by a mobile\nmanipulator team, or for collaborative manipulation with the human, robots\nshould be able to discover where other robots are attached to the payload, as\nwell as the payload's mass and inertial properties. In this paper, we describe\na method for the robots to autonomously discover this information. The robots\ncooperatively manipulate the payload, and the twist, twist derivative, and\nwrench data at their grasp frames are used to estimate the transformation\nmatrices between the grasp frames, the location of the payload's center of\nmass, and the payload's inertia matrix. The method is validated experimentally\nwith a team of three mobile cobots, or mocobots.", "comment": "8 pages, 6 figures. Submitted to IEEE Robotics and Automation Letters\n  (RA-L)", "pdf_url": "http://arxiv.org/pdf/2502.04600v3", "cate": "cs.RO", "date": "2025-02-07", "updated": "2025-08-01"}
{"id": "2508.00253", "title": "Leveraging Large Language Model for Information Retrieval-based Bug Localization", "authors": ["Moumita Asad", "Rafed Muhammad Yasir", "Armin Geramirad", "Sam Malek"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00253v1", "summary": "Information Retrieval-based Bug Localization aims to identify buggy source\nfiles for a given bug report. While existing approaches -- ranging from vector\nspace models to deep learning models -- have shown potential in this domain,\ntheir effectiveness is often limited by the vocabulary mismatch between bug\nreports and source code. To address this issue, we propose a novel Large\nLanguage Model (LLM) based bug localization approach, called GenLoc. Given a\nbug report, GenLoc leverages an LLM equipped with code-exploration functions to\niteratively analyze the code base and identify potential buggy files. To gather\nbetter context, GenLoc may optionally retrieve semantically relevant files\nusing vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug\nreports from six large-scale Java projects. Experimental results show that\nGenLoc outperforms five state-of-the-art bug localization techniques across\nmultiple metrics, achieving an average improvement of more than 60\\% in\nAccuracy@1.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00253v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00497", "title": "From Individuals to Crowds: Dual-Level Public Response Prediction in Social Media", "authors": ["Jinghui Zhang", "Kaiyang Wan", "Longwei Xu", "Ao Li", "Zongfang Liu", "Xiuying Chen"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      ACM MM 2025", "url": "http://arxiv.org/abs/2508.00497v1", "summary": "Public response prediction is critical for understanding how individuals or\ngroups might react to specific events, policies, or social phenomena, making it\nhighly valuable for crisis management, policy-making, and social media\nanalysis. However, existing works face notable limitations. First, they lack\nmicro-level personalization, producing generic responses that ignore individual\nuser preferences. Moreover, they overlook macro-level sentiment distribution\nand only deal with individual-level sentiment, constraining them from analyzing\nbroader societal trends and group sentiment dynamics. To address these\nchallenges, we propose SocialAlign, a unified framework that predicts\nreal-world responses at both micro and macro levels in social contexts. At the\nmicro level, SocialAlign employs SocialLLM with an articulate Personalized\nAnalyze-Compose LoRA (PAC-LoRA) structure, which deploys specialized expert\nmodules for content analysis and response generation across diverse topics and\nuser profiles, enabling the generation of personalized comments with\ncorresponding sentiments. At the macro level, it models group sentiment\ndistributions and aligns predictions with real-world sentiment trends derived\nfrom social media data. To evaluate SocialAlign in real-world scenarios, we\nintroduce SentiWeibo, a large-scale dataset curated from authentic social\ninteractions on the Weibo platform. Experimental results on our SentiWeibo and\nrelated LaMP benchmark demonstrate that SocialAlign surpasses strong baselines,\nshowing improved accuracy, interpretability, and generalization in public\nresponse prediction. We hope our work inspires further research in public\nresponse prediction and computational social science:\nhttps://github.com/Znull-1220/SocialAlign.", "comment": "ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2508.00497v1", "cate": "cs.SI", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.19673", "title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions", "authors": ["Babak Taati", "Muhammad Muzammil", "Yasamin Zarghami", "Abhishek Moturu", "Amirhossein Kazerouni", "Hailey Reimer", "Alex Mihailidis", "Thomas Hadjistavropoulos"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures, submitted to IEEE JBHI", "url": "http://arxiv.org/abs/2507.19673v2", "summary": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP", "comment": "10 pages, 4 figures, submitted to IEEE JBHI", "pdf_url": "http://arxiv.org/pdf/2507.19673v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-08-01"}
{"id": "2505.09503", "title": "Towards Fair In-Context Learning with Tabular Foundation Models", "authors": ["Patrik Kenfack", "Samira Ebrahimi Kahou", "Ulrich Aïvodji"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      30 pages, 12 figures, 5 tables", "url": "http://arxiv.org/abs/2505.09503v3", "summary": "Transformer-based tabular foundation models have recently demonstrated\npromising in-context learning (ICL) performance on structured data, emerging as\ncompetitive alternatives to gradient-boosted trees. However, the fairness\nimplications of this new paradigm remain largely unexplored. We present the\nfirst investigation of fairness in tabular ICL, evaluating three recently\nproposed foundation models -- TabPFNv2, TabICL, and TabDPT -- on multiple\nbenchmark datasets. To mitigate biases, we explore three pre-processing\nfairness-enhancing methods: correlation removal (decorrelating input features\nfrom the sensitive attribute), group-balanced sample selection (ensuring equal\nrepresentation of protected groups in context examples), and uncertainty-based\nsample selection (prioritizing context examples with high sensitive-attribute\nprediction uncertainty). Our experiments show that the uncertainty-based\nstrategy consistently improves group fairness metrics (e.g., demographic\nparity, equalized odds, and equal opportunity) with minimal impact on\npredictive accuracy. We release our code to facilitate reproducibility\n(https://github.com/patrikken/Fair-TabICL)", "comment": "30 pages, 12 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2505.09503v3", "cate": "cs.LG", "date": "2025-05-14", "updated": "2025-08-01"}
{"id": "2502.08452", "title": "Learning to Push, Group, and Grasp: A Diffusion Policy Approach for Multi-Object Delivery", "authors": ["Takahiro Yonemaru", "Weiwei Wan", "Tatsuki Nishimura", "Kensuke Harada"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.08452v3", "summary": "Simultaneously grasping and delivering multiple objects can significantly\nenhance robotic work efficiency and has been a key research focus for decades.\nThe primary challenge lies in determining how to push objects, group them, and\nexecute simultaneous grasping for respective groups while considering object\ndistribution and the hardware constraints of the robot. Traditional rule-based\nmethods struggle to flexibly adapt to diverse scenarios. To address this\nchallenge, this paper proposes an imitation learning-based approach. We collect\na series of expert demonstrations through teleoperation and train a diffusion\npolicy network, enabling the robot to dynamically generate action sequences for\npushing, grouping, and grasping, thereby facilitating efficient multi-object\ngrasping and delivery. We conducted experiments to evaluate the method under\ndifferent training dataset sizes, varying object quantities, and real-world\nobject scenarios. The results demonstrate that the proposed approach can\neffectively and adaptively generate multi-object grouping and grasping\nstrategies. With the support of more training data, imitation learning is\nexpected to be an effective approach for solving the multi-object grasping\nproblem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.08452v3", "cate": "cs.RO", "date": "2025-02-12", "updated": "2025-08-01"}
{"id": "2508.00462", "title": "Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory", "authors": ["Linus Ververs", "Lutz Prechelt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00462v1", "summary": "Context: Pair Programming as a work mode is used (occasionally or frequently)\nthroughout professional software development. Objective: Understand what\npower-related phenomena occur in pair programming as it is used in industry;\ngive advice to practitioners on how to do better pair programming. Method:\nAnalyze 22 industrial pair programming sessions using Grounded Theory\nMethodology. Formulate a Grounded Theory on power-related behaviors. Run a\nsurvey with 292 participants about that theory. Use it to demonstrate that the\nphenomena are common. Results: Our theory describes the phenomenon of Power\nGap: a perceived difference in participation opportunities. The theory shows\nthe behaviors that create a Power Gap or result from it. Power Gaps tend to\ndamage knowledge transfer, code quality, and process effi ciency. The survey\nresults show that all concepts from our theory are frequent in practice. They\nalso provide more grounding for concepts that are observable only indirectly.\nConclusions: It is a valuable component of pair programming skill to be able to\navoid Power Gaps. Specifically, pair partners need to avoid Hierarchical\nBehavior (which tends to create or increase a Power Gap) and should perform\nenough Equalizing Behavior (which prevents or reduces a Power Gap).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00462v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2504.01718", "title": "A Novel Dynamic Epidemic Model for Successive Opinion Diffusion in Social Networks", "authors": ["Bin Han", "Fabienne Renckens", "C. Clark Cao", "Hans D. Schotten"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      To appear in IEEE GLOBECOM 2025", "url": "http://arxiv.org/abs/2504.01718v2", "summary": "This paper proposes a dynamic epidemic model for successive opinion diffusion\nin social networks, extending the SHIMR model. It incorporates dynamic\ndecision-making influenced by social distances and captures accumulative\nopinion diffusion caused by interrelated rumors. The model reflects the impact\nof rumor spread on social network structures. Simulations validate its\neffectiveness in explaining phenomena like the echo chamber effect and provide\ninsights into opinion diffusion dynamics, with implications for understanding\nsocial polarization and network evolution.", "comment": "To appear in IEEE GLOBECOM 2025", "pdf_url": "http://arxiv.org/pdf/2504.01718v2", "cate": "cs.SI", "date": "2025-04-02", "updated": "2025-07-31"}
{"id": "2507.19924", "title": "HumanSAM: Classifying Human-centric Forgery Videos in Human Spatial, Appearance, and Motion Anomaly", "authors": ["Chang Liu", "Yunfan Ye", "Fan Zhang", "Qingyang Zhou", "Yuchuan Luo", "Zhiping Cai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project page: this https URL", "url": "http://arxiv.org/abs/2507.19924v2", "summary": "Numerous synthesized videos from generative models, especially human-centric\nones that simulate realistic human actions, pose significant threats to human\ninformation security and authenticity. While progress has been made in binary\nforgery video detection, the lack of fine-grained understanding of forgery\ntypes raises concerns regarding both reliability and interpretability, which\nare critical for real-world applications. To address this limitation, we\npropose HumanSAM, a new framework that builds upon the fundamental challenges\nof video generation models. Specifically, HumanSAM aims to classify\nhuman-centric forgeries into three distinct types of artifacts commonly\nobserved in generated content: spatial, appearance, and motion anomaly. To\nbetter capture the features of geometry, semantics and spatiotemporal\nconsistency, we propose to generate the human forgery representation by fusing\ntwo branches of video understanding and spatial depth. We also adopt a\nrank-based confidence enhancement strategy during the training process to learn\nmore robust representation by introducing three prior scores. For training and\nevaluation, we construct the first public benchmark, the Human-centric Forgery\nVideo (HFV) dataset, with all types of forgeries carefully annotated\nsemi-automatically. In our experiments, HumanSAM yields promising results in\ncomparison with state-of-the-art methods, both in binary and multi-class\nforgery classification.", "comment": "ICCV 2025. Project page: https://dejian-lc.github.io/humansam/", "pdf_url": "http://arxiv.org/pdf/2507.19924v2", "cate": "cs.CV", "date": "2025-07-26", "updated": "2025-08-01"}
{"id": "2505.11250", "title": "Rethinking Irregular Time Series Forecasting: A Simple yet Effective Baseline", "authors": ["Xvyuan Liu", "Xiangfei Qiu", "Xingjian Wu", "Zhengyu Li", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11250v2", "summary": "The forecasting of irregular multivariate time series (IMTS) is a critical\ntask in domains like healthcare and climate science. However, this task faces\ntwo significant hurdles: 1) the inherent non-uniformity and missing data in\nIMTS complicate the modeling of temporal dynamics, and 2) existing methods\noften rely on computationally expensive architectures. To address these dual\nchallenges, we introduce APN, a general and efficient forecasting framework. At\nthe core of APN is a novel Time-Aware Patch Aggregation (TAPA) module that\nintroduces an aggregation-based paradigm for adaptive patching, moving beyond\nthe limitations of fixed-span segmentation and interpolation-based methods.\nTAPA first learns dynamic temporal boundaries to define data-driven segments.\nCrucially, instead of resampling or interpolating, it directly computes patch\nrepresentations via a time-aware weighted aggregation of all raw observations,\nwhere weights are determined by each observation's temporal relevance to the\nsegment. This approach provides two key advantages: it preserves data fidelity\nby avoiding the introduction of artificial data points and ensures complete\ninformation coverage by design.The resulting regularized and information-rich\npatch representations enable the use of a lightweight query module for\nhistorical context aggregation and a simple MLP for final prediction. Extensive\nexperiments on multiple real-world datasets demonstrate that APN establishes a\nnew state-of-the-art, significantly outperforming existing methods in both\nprediction accuracy and computational efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11250v2", "cate": "cs.LG", "date": "2025-05-16", "updated": "2025-08-01"}
{"id": "2503.02198", "title": "FalconGym: A Photorealistic Simulation Framework for Zero-Shot Sim-to-Real Vision-Based Quadrotor Navigation", "authors": ["Yan Miao", "Will Shen", "Sayan Mitra"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted in IROS 2025", "url": "http://arxiv.org/abs/2503.02198v2", "summary": "We present a novel framework demonstrating zero-shot sim-to-real transfer of\nvisual control policies learned in a Neural Radiance Field (NeRF) environment\nfor quadrotors to fly through racing gates. Robust transfer from simulation to\nreal flight poses a major challenge, as standard simulators often lack\nsufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates.", "comment": "Accepted in IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.02198v2", "cate": "cs.RO", "date": "2025-03-04", "updated": "2025-08-01"}
{"id": "2508.00593", "title": "Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System", "authors": ["Shuyao Jiang", "Jiazhen Gu", "Wujie Zheng", "Yangfan Zhou", "Michael R. Lyu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted by the 19th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2025)", "url": "http://arxiv.org/abs/2508.00593v1", "summary": "Background: It has long been suggested that user feedback, typically written\nin natural language by end-users, can help issue detection. However, for\nlarge-scale online service systems that receive a tremendous amount of\nfeedback, it remains a challenging task to identify severe issues from user\nfeedback. Aims: To develop a better feedback-based issue detection approach, it\nis crucial first to gain a comprehensive understanding of the characteristics\nof user feedback in real production systems. Method: In this paper, we conduct\nan empirical study on 50,378,766 user feedback items from six real-world\nservices in a one-billion-user online service system. We first study what users\nprovide in their feedback. We then examine whether certain features of feedback\nitems can be good indicators of severe issues. Finally, we investigate whether\nadopting machine learning techniques to analyze user feedback is reasonable.\nResults: Our results show that a large proportion of user feedback provides\nirrelevant information about system issues. As a result, it is crucial to\nfilter out issue-irrelevant information when processing user feedback.\nMoreover, we find severe issues that cannot be easily detected based solely on\nuser feedback characteristics. Finally, we find that the distributions of the\nfeedback topics in different time intervals are similar. This confirms that\ndesigning machine learning-based approaches is a viable direction for better\nanalyzing user feedback. Conclusions: We consider that our findings can serve\nas an empirical foundation for feedback-based issue detection in large-scale\nservice systems, which sheds light on the design and implementation of\npractical issue detection approaches.", "comment": "Accepted by the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00593v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2502.20083", "title": "Block-corrected Modularity for Community Detection", "authors": ["Hasti Narimanzadeh", "Takayuki Hiraoka", "Mikko Kivelä"], "categories": ["physics.soc-ph", "cond-mat.stat-mech", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      22 pages, 11 figures", "url": "http://arxiv.org/abs/2502.20083v3", "summary": "Unknown node attributes in complex networks may introduce community\nstructures that are important to distinguish from those driven by known\nattributes. We propose a block-corrected modularity that discounts given block\nstructures present in the network to reveal communities masked by them. We show\nanalytically how the proposed modularity finds the community structure driven\nby an unknown attribute in a simple network model. Further, we observe that the\nblock-corrected modularity finds the underlying community structure on a number\nof simple synthetic network models while methods using different null models\nfail. We develop an efficient spectral method as well as two Louvain-inspired\nfine-tuning algorithms to maximize the proposed modularity and demonstrate\ntheir performance on several synthetic network models. Finally, we assess our\nmethodology on various real-world citation networks built using the OpenAlex\ndata by correcting for the temporal citation patterns.", "comment": "22 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2502.20083v3", "cate": "physics.soc-ph", "date": "2025-02-27", "updated": "2025-08-01"}
{"id": "2507.20980", "title": "LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering", "authors": ["Shide Du", "Chunming Wu", "Zihan Fang", "Wendi Zhao", "Yilin Wu", "Changwei Wang", "Shiping Wang"], "categories": ["cs.CV", "stat.CO", "stat.ML"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.20980v2", "summary": "Deep anchor-based multi-view clustering methods enhance the scalability of\nneural networks by utilizing representative anchors to reduce the computational\ncomplexity of large-scale clustering. Despite their scalability advantages,\nexisting approaches often incorporate anchor structures in a heuristic or\ntask-agnostic manner, either through post-hoc graph construction or as\nauxiliary components for message passing. Such designs overlook the core\nstructural demands of anchor-based clustering, neglecting key optimization\nprinciples. To bridge this gap, we revisit the underlying optimization problem\nof large-scale anchor-based multi-view clustering and unfold its iterative\nsolution into a novel deep network architecture, termed LargeMvC-Net. The\nproposed model decomposes the anchor-based clustering process into three\nmodules: RepresentModule, NoiseModule, and AnchorModule, corresponding to\nrepresentation learning, noise suppression, and anchor indicator estimation.\nEach module is derived by unfolding a step of the original optimization\nprocedure into a dedicated network component, providing structural clarity and\noptimization traceability. In addition, an unsupervised reconstruction loss\naligns each view with the anchor-induced latent space, encouraging consistent\nclustering structures across views. Extensive experiments on several\nlarge-scale multi-view benchmarks show that LargeMvC-Net consistently\noutperforms state-of-the-art methods in terms of both effectiveness and\nscalability.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.20980v2", "cate": "cs.CV", "date": "2025-07-28", "updated": "2025-08-01"}
{"id": "2505.17340", "title": "Conformal Predictive Distributions for Order Fulfillment Time Forecasting", "authors": ["Tinghan Ye", "Amira Hijazi", "Pascal Van Hentenryck"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.17340v2", "summary": "Accurate estimation of order fulfillment time is critical for e-commerce\nlogistics, yet traditional rule-based approaches often fail to capture the\ninherent uncertainties in delivery operations. This paper introduces a novel\nframework for distributional forecasting of order fulfillment time, leveraging\nConformal Predictive Systems and Cross Venn-Abers Predictors -- model-agnostic\ntechniques that provide rigorous coverage or validity guarantees. The proposed\nmachine learning methods integrate granular spatiotemporal features, capturing\nfulfillment location and carrier performance dynamics to enhance predictive\naccuracy. Additionally, a cost-sensitive decision rule is developed to convert\nprobabilistic forecasts into reliable point predictions. Experimental\nevaluation on a large-scale industrial dataset demonstrates that the proposed\nmethods generate competitive distributional forecasts, while machine\nlearning-based point predictions significantly outperform the existing\nrule-based system -- achieving up to 14% higher prediction accuracy and up to\n75% improvement in identifying late deliveries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.17340v2", "cate": "cs.LG", "date": "2025-05-22", "updated": "2025-08-01"}
{"id": "2503.03254", "title": "SCORE: Saturated Consensus Relocalization in Semantic Line Maps", "authors": ["Haodong Jiang", "Xiang Zheng", "Yanglin Zhang", "Qingcheng Zeng", "Yiqian Li", "Ziyang Hong", "Junfeng Wu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages, 13 figurs, arxiv version for paper published at IROS 2025", "url": "http://arxiv.org/abs/2503.03254v2", "summary": "We present SCORE, a visual relocalization system that achieves unprecedented\nmap compactness by adopting semantically labeled 3D line maps. SCORE requires\nonly 0.01\\%-0.1\\% of the storage needed by structure-based or learning-based\nbaselines, while maintaining practical accuracy and comparable runtime. The key\ninnovation is a novel robust estimation mechanism, Saturated Consensus\nMaximization (Sat-CM), which generalizes classical Consensus Maximization (CM)\nby assigning diminishing weights to inlier associations according to maximum\nlikelihood with probabilistic justification. Under extreme outlier ratios (up\nto 99.5\\%) arising from one-to-many ambiguity in semantic matching, Sat-CM\nenables accurate estimation when CM fails. To ensure computational efficiency,\nwe propose an accelerating framework for globally solving Sat-CM formulations\nand specialize it for the Perspective-n-Lines problem at the core of SCORE.", "comment": "12 pages, 13 figurs, arxiv version for paper published at IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.03254v2", "cate": "cs.RO", "date": "2025-03-05", "updated": "2025-08-01"}
{"id": "2508.00630", "title": "MCeT: Behavioral Model Correctness Evaluation using Large Language Models", "authors": ["Khaled Ahmed", "Jialing Song", "Boqi Chen", "Ou Wei", "Bingzhou Zheng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      MODELS 2025", "url": "http://arxiv.org/abs/2508.00630v1", "summary": "Behavioral model diagrams, e.g., sequence diagrams, are an essential form of\ndocumentation that are typically designed by system engineers from requirements\ndocumentation, either fully manually or assisted by design tools. With the\ngrowing use of Large Language Models (LLM) as AI modeling assistants, more\nautomation will be involved in generating diagrams. This necessitates the\nadvancement of automatic model correctness evaluation tools. Such a tool can be\nused to evaluate both manually and AI automatically generated models; to\nprovide feedback to system engineers, and enable AI assistants to self-evaluate\nand self-enhance their generated models.\n  In this paper, we propose MCeT, the first fully automated tool to evaluate\nthe correctness of a behavioral model, sequence diagrams in particular, against\nits corresponding requirements text and produce a list of issues that the model\nhas. We utilize LLMs for the correctness evaluation tasks as they have shown\noutstanding natural language understanding ability. However, we show that\ndirectly asking an LLM to compare a diagram to requirements finds less than 35%\nof issues that experienced engineers can find. We propose to supplement the\ndirect check with a fine-grained, multi-perspective approach; we split the\ndiagram into atomic, non-divisible interactions, and split the requirements\ntext into atomic, self-contained items. We compare the diagram with atomic\nrequirements and each diagram-atom with the requirements. We also propose a\nself-consistency checking approach that combines perspectives to mitigate LLM\nhallucinated issues. Our combined approach improves upon the precision of the\ndirect approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,\nthe approach finds 90% more issues that the experienced engineers found than\nthe direct approach, and reports an average of 6 new issues per diagram.", "comment": "MODELS 2025", "pdf_url": "http://arxiv.org/pdf/2508.00630v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23300", "title": "Training-free Geometric Image Editing on Diffusion Models", "authors": ["Hanshen Zhu", "Zhen Zhu", "Kaile Zhang", "Yiming Gong", "Yuliang Liu", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.23300v2", "summary": "We tackle the task of geometric image editing, where an object within an\nimage is repositioned, reoriented, or reshaped while preserving overall scene\ncoherence. Previous diffusion-based editing methods often attempt to handle all\nrelevant subtasks in a single step, proving difficult when transformations\nbecome large or structurally complex. We address this by proposing a decoupled\npipeline that separates object transformation, source region inpainting, and\ntarget region refinement. Both inpainting and refinement are implemented using\na training-free diffusion approach, FreeFine. In experiments on our new\nGeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine\noutperforms state-of-the-art alternatives in image fidelity, and edit\nprecision, especially under demanding transformations. Code and benchmark are\navailable at: https://github.com/CIawevy/FreeFine", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.23300v2", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2505.23246", "title": "How to Evaluate Participant Contributions in Decentralized Federated Learning", "authors": ["Honoka Anada", "Tatsuya Kaneko", "Shinya Takamaeda-Yamazaki"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.23246v2", "summary": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without sharing local data. In particular,\ndecentralized FL (DFL), where clients exchange models without a central server,\nhas gained attention for mitigating communication bottlenecks. Evaluating\nparticipant contributions is crucial in DFL to incentivize active participation\nand enhance transparency. However, existing contribution evaluation methods for\nFL assume centralized settings and cannot be applied directly to DFL due to two\nchallenges: the inaccessibility of each client to non-neighboring clients'\nmodels, and the necessity to trace how contributions propagate in conjunction\nwith peer-to-peer model exchanges over time. To address these challenges, we\npropose TRIP-Shapley, a novel contribution evaluation method for DFL.\nTRIP-Shapley formulates the clients' overall contributions by tracing the\npropagation of the round-wise local contributions. In this way, TRIP-Shapley\naccurately reflects the delayed and gradual influence propagation, as well as\nallowing a lightweight coordinator node to estimate the overall contributions\nwithout collecting models, but based solely on locally observable contributions\nreported by each client. Experiments demonstrate that TRIP-Shapley is\nsufficiently close to the ground-truth Shapley value, is scalable to\nlarge-scale scenarios, and remains robust in the presence of dishonest clients.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.23246v2", "cate": "cs.LG", "date": "2025-05-29", "updated": "2025-08-01"}
{"id": "2503.07504", "title": "PIPE Planner: Pathwise Information Gain with Map Predictions for Indoor Robot Exploration", "authors": ["Seungjae Baek", "Brady Moon", "Seungchan Kim", "Muqing Cao", "Cherie Ho", "Sebastian Scherer", "Jeong hwan Jeon"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures, IROS 2025", "url": "http://arxiv.org/abs/2503.07504v2", "summary": "Autonomous exploration in unknown environments requires estimating the\ninformation gain of an action to guide planning decisions. While prior\napproaches often compute information gain at discrete waypoints, pathwise\nintegration offers a more comprehensive estimation but is often computationally\nchallenging or infeasible and prone to overestimation. In this work, we propose\nthe Pathwise Information Gain with Map Prediction for Exploration (PIPE)\nplanner, which integrates cumulative sensor coverage along planned trajectories\nwhile leveraging map prediction to mitigate overestimation. To enable efficient\npathwise coverage computation, we introduce a method to efficiently calculate\nthe expected observation mask along the planned path, significantly reducing\ncomputational overhead. We validate PIPE on real-world floorplan datasets,\ndemonstrating its superior performance over state-of-the-art baselines. Our\nresults highlight the benefits of integrating predictive mapping with pathwise\ninformation gain for efficient and informed exploration. Website:\nhttps://pipe-planner.github.io", "comment": "8 pages, 8 figures, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2503.07504v2", "cate": "cs.RO", "date": "2025-03-10", "updated": "2025-08-01"}
{"id": "2508.00700", "title": "Is LLM-Generated Code More Maintainable \\& Reliable than Human-Written Code?", "authors": ["Alfred Santa Molison", "Marcia Moraes", "Glaucia Melo", "Fabio Santos", "Wesley K. G. Assuncao"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Accepted ESEM2025", "url": "http://arxiv.org/abs/2508.00700v1", "summary": "Background: The rise of Large Language Models (LLMs) in software development\nhas opened new possibilities for code generation. Despite the widespread use of\nthis technology, it remains unclear how well LLMs generate code solutions in\nterms of software quality and how they compare to human-written code. Aims:\nThis study compares the internal quality attributes of LLM-generated and\nhuman-written code. Method: Our empirical study integrates datasets of coding\ntasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and\nSonarQube to assess software quality. The dataset comprises Python code\nsolutions across three difficulty levels: introductory, interview, and\ncompetition. We analyzed key code quality metrics, including maintainability\nand reliability, and the estimated effort required to resolve code issues.\nResults: Our analysis shows that LLM-generated code has fewer bugs and requires\nless effort to fix them overall. Interestingly, fine-tuned models reduced the\nprevalence of high-severity issues, such as blocker and critical bugs, and\nshifted them to lower-severity categories, but decreased the model's\nperformance. In competition-level problems, the LLM solutions sometimes\nintroduce structural issues that are not present in human-written code.\nConclusion: Our findings provide valuable insights into the quality of\nLLM-generated code; however, the introduction of critical issues in more\ncomplex scenarios highlights the need for a systematic evaluation and\nvalidation of LLM solutions. Our work deepens the understanding of the\nstrengths and limitations of LLMs for code generation.", "comment": "Accepted ESEM2025", "pdf_url": "http://arxiv.org/pdf/2508.00700v1", "cate": "cs.SE", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23643", "title": "FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free Training Framework for Spiking Neural Networks", "authors": ["Changqing Xu", "Ziqiang Yang", "Yi Liu", "Xinfang Liao", "Guiqi Mo", "Hao Zeng", "Yintang Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23643v2", "summary": "Spiking Neural Networks (SNNs) offer a biologically plausible framework for\nenergy-efficient neuromorphic computing. However, it is a challenge to train\nSNNs due to their non-differentiability, efficiently. Existing gradient\napproximation approaches frequently sacrifice accuracy and face deployment\nlimitations on edge devices due to the substantial computational requirements\nof backpropagation. To address these challenges, we propose a Forward-Forward\n(FF) based gradient approximation-free training framework for Spiking Neural\nNetworks, which treats spiking activations as black-box modules, thereby\neliminating the need for gradient approximation while significantly reducing\ncomputational complexity. Furthermore, we introduce a class-aware complexity\nadaptation mechanism that dynamically optimizes the loss function based on\ninter-class difficulty metrics, enabling efficient allocation of network\nresources across different categories. Experimental results demonstrate that\nour proposed training framework achieves test accuracies of 99.58%, 92.13%, and\n75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively,\nsurpassing all existing FF-based SNN approaches. Additionally, our proposed\nmethod exhibits significant advantages in terms of memory access and\ncomputational power consumption.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23643v2", "cate": "cs.CV", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2506.07288", "title": "EVINET: Towards Open-World Graph Learning via Evidential Reasoning Network", "authors": ["Weijie Guan", "Haohui Wang", "Jian Kang", "Lihui Liu", "Dawei Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      KDD 2025", "url": "http://arxiv.org/abs/2506.07288v3", "summary": "Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.", "comment": "KDD 2025", "pdf_url": "http://arxiv.org/pdf/2506.07288v3", "cate": "cs.LG", "date": "2025-06-08", "updated": "2025-08-01"}
{"id": "2504.06513", "title": "Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions", "authors": ["Xinyi Wang", "Taekyung Kim", "Bardh Hoxha", "Georgios Fainekos", "Dimitra Panagou"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Project page: { this https URL }", "url": "http://arxiv.org/abs/2504.06513v5", "summary": "Robot navigation in dynamic, crowded environments poses a significant\nchallenge due to the inherent uncertainties in the obstacle model. In this\nwork, we propose a risk-adaptive approach based on the Conditional\nValue-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically\nadjusted to accept the minimum necessary risk, achieving a good performance in\nterms of safety and optimization feasibility under uncertainty. Additionally,\nwe introduce a dynamic zone-based barrier function which characterizes the\ncollision likelihood by evaluating the relative state between the robot and the\nobstacle. By integrating risk adaptation with this new function, our approach\nadaptively expands the safety margin, enabling the robot to proactively avoid\nobstacles in highly dynamic environments. Comparisons and ablation studies\ndemonstrate that our method outperforms existing social navigation approaches,\nand validate the effectiveness of our proposed framework.", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS). Project page: {https://lawliet9666.github.io/cvarbf/}", "pdf_url": "http://arxiv.org/pdf/2504.06513v5", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-08-01"}
{"id": "2407.13900", "title": "Exploring the Evidence-Based SE Beliefs of Generative AI Tools", "authors": ["Chris Brown", "Jason Cusati"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2407.13900v3", "summary": "Recent innovations in generative artificial intelligence (AI), primarily\npowered by large language models (LLMs), have transformed how programmers\ndevelop and maintain software -- leading to new frontiers in software\nengineering (SE). The advanced capabilities of generative AI tools to support\nsoftware development tasks have led to a rise in their adoption within software\ndevelopment workflows. However, little is known about how AI tools perceive\nevidence-based beliefs and practices verified by research findings. To this\nend, we conduct a preliminary evaluation conceptually replicating prior work to\nexplore the \"beliefs\" of generative AI tools used to support software\ndevelopment tasks. We investigate 17 evidence-based claims posited by empirical\nSE research across five generative AI tools. Our findings show that generative\nAI tools have ambiguous beliefs regarding research claims and lack credible\nevidence to support responses. Based on our results, we provide implications\nfor practitioners integrating generative AI-based systems into development\ncontexts and shed light on future research directions to enhance the\nreliability and trustworthiness of generative AI -- aiming to increase\nawareness and adoption of evidence-based SE research findings in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2407.13900v3", "cate": "cs.SE", "date": "2024-07-18", "updated": "2025-08-01"}
{"id": "2508.00156", "title": "Integrating Opinion Dynamics into Safety Control for Decentralized Airplane Encounter Resolution", "authors": ["Shuhao Qi", "Zhiqi Tang", "Zhiyong Sun", "Sofie Haesaert"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00156v1", "summary": "As the airspace becomes increasingly congested, decentralized conflict\nresolution methods for airplane encounters have become essential. While\ndecentralized safety controllers can prevent dangerous midair collisions, they\ndo not always ensure prompt conflict resolution. As a result, airplane progress\nmay be blocked for extended periods in certain situations. To address this\nblocking phenomenon, this paper proposes integrating bio-inspired nonlinear\nopinion dynamics into the airplane safety control framework, thereby\nguaranteeing both safety and blocking-free resolution. In particular, opinion\ndynamics enable the safety controller to achieve collaborative decision-making\nfor blocking resolution and facilitate rapid, safe coordination without relying\non communication or preset rules. Extensive simulation results validate the\nimproved flight efficiency and safety guarantees. This study provides practical\ninsights into the design of autonomous controllers for airplanes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00156v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2411.05824", "title": "Navigating Distribution Shifts in Medical Image Analysis: A Survey", "authors": ["Zixian Su", "Jingwei Guo", "Xi Yang", "Qiufeng Wang", "Frans Coenen", "Kaizhu Huang"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.05824v2", "summary": "Medical Image Analysis (MedIA) has become indispensable in modern healthcare,\nenhancing clinical diagnostics and personalized treatment. Despite the\nremarkable advancements supported by deep learning (DL) technologies, their\npractical deployment faces challenges due to distribution shifts, where models\ntrained on specific datasets underperform across others from varying hospitals,\nregions, or patient populations. To navigate this issue, researchers have been\nactively developing strategies to increase the adaptability and robustness of\nDL models, enabling their effective use in unfamiliar and diverse environments.\nThis paper systematically reviews approaches that apply DL techniques to MedIA\nsystems affected by distribution shifts. Unlike traditional categorizations\nbased on technical specifications, our approach is grounded in the real-world\noperational constraints faced by healthcare institutions. Specifically, we\ncategorize the existing body of work into Joint Training, Federated Learning,\nFine-tuning, and Domain Generalization, with each method tailored to distinct\nscenarios caused by Data Accessibility, Privacy Concerns, and Collaborative\nProtocols. This perspective equips researchers with a nuanced understanding of\nhow DL can be strategically deployed to address distribution shifts in MedIA,\nensuring diverse and robust medical applications. By delving deeper into these\ntopics, we highlight potential pathways for future research that not only\naddress existing limitations but also push the boundaries of deployable MedIA\ntechnologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.05824v2", "cate": "eess.IV", "date": "2024-11-05", "updated": "2025-08-01"}
{"id": "2506.20644", "title": "Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices", "authors": ["Hangyu Li", "Hongyue Wu", "Guodong Fan", "Zhen Zhang", "Shizhan Chen", "Zhiyong Feng"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICWS 2025", "url": "http://arxiv.org/abs/2506.20644v2", "summary": "As privacy protection gains increasing importance, more models are being\ntrained on edge devices and subsequently merged into the central server through\nFederated Learning (FL). However, current research overlooks the impact of\nnetwork topology, physical distance, and data heterogeneity on edge devices,\nleading to issues such as increased latency and degraded model performance. To\naddress these issues, we propose a new federated learning scheme on edge\ndevices that called Federated Learning with Encrypted Data Sharing(FedEDS).\nFedEDS uses the client model and the model's stochastic layer to train the data\nencryptor. The data encryptor generates encrypted data and shares it with other\nclients. The client uses the corresponding client's stochastic layer and\nencrypted data to train and adjust the local model. FedEDS uses the client's\nlocal private data and encrypted shared data from other clients to train the\nmodel. This approach accelerates the convergence speed of federated learning\ntraining and mitigates the negative impact of data heterogeneity, making it\nsuitable for application services deployed on edge devices requiring rapid\nconvergence. Experiments results show the efficacy of FedEDS in promoting model\nperformance.", "comment": "Accepted by ICWS 2025", "pdf_url": "http://arxiv.org/pdf/2506.20644v2", "cate": "cs.LG", "date": "2025-06-25", "updated": "2025-08-01"}
{"id": "2505.09074", "title": "Trends in Motion Prediction Toward Deployable and Generalizable Autonomy: A Revisit and Perspectives", "authors": ["Letian Wang", "Marc-Antoine Lavoie", "Sandro Papais", "Barza Nisar", "Yuxiao Chen", "Wenhao Ding", "Boris Ivanovic", "Hao Shao", "Abulikemu Abuduweili", "Evan Cook", "Yang Zhou", "Peter Karkus", "Jiachen Li", "Changliu Liu", "Marco Pavone", "Steven Waslander"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Updated draft. 163 pages, 40 figures, 13 tables", "url": "http://arxiv.org/abs/2505.09074v3", "summary": "Motion prediction, the anticipation of future agent states or scene\nevolution, is rooted in human cognition, bridging perception and\ndecision-making. It enables intelligent systems, such as robots and\nself-driving cars, to act safely in dynamic, human-involved environments, and\ninforms broader time-series reasoning challenges. With advances in methods,\nrepresentations, and datasets, the field has seen rapid progress, reflected in\nquickly evolving benchmark results. Yet, when state-of-the-art methods are\ndeployed in the real world, they often struggle to generalize to open-world\nconditions and fall short of deployment standards. This reveals a gap between\nresearch benchmarks, which are often idealized or ill-posed, and real-world\ncomplexity.\n  To address this gap, this survey revisits the generalization and\ndeployability of motion prediction models, with an emphasis on the applications\nof robotics, autonomous driving, and human motion. We first offer a\ncomprehensive taxonomy of motion prediction methods, covering representations,\nmodeling strategies, application domains, and evaluation protocols. We then\nstudy two key challenges: (1) how to push motion prediction models to be\ndeployable to realistic deployment standards, where motion prediction does not\nact in a vacuum, but functions as one module of closed-loop autonomy stacks -\nit takes input from the localization and perception, and informs downstream\nplanning and control. 2) how to generalize motion prediction models from\nlimited seen scenarios/datasets to the open-world settings. Throughout the\npaper, we highlight critical open challenges to guide future work, aiming to\nrecalibrate the community's efforts, fostering progress that is not only\nmeasurable but also meaningful for real-world applications. The project webpage\ncorresponding to this paper can be found here\nhttps://trends-in-motion-prediction-2025.github.io/.", "comment": "Updated draft. 163 pages, 40 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2505.09074v3", "cate": "cs.RO", "date": "2025-05-14", "updated": "2025-07-31"}
{"id": "2506.15884", "title": "How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?", "authors": ["Shamse Tasnim Cynthia", "Nuri Almarimi", "Banani Roy"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.15884v2", "summary": "Community smells reflect poor organizational practices that often lead to\nsocio-technical issues and the accumulation of Self-Admitted Technical Debt\n(SATD). While prior studies have explored these problems in general software\nsystems, their interplay in machine learning (ML)-based projects remains\nlargely underexamined. In this study, we investigated the prevalence of\ncommunity smells and their relationship with SATD in open-source ML projects,\nanalyzing data at the release level. First, we examined the prevalence of ten\ncommunity smell types across the releases of 155 ML-based systems and found\nthat community smells are widespread, exhibiting distinct distribution patterns\nacross small, medium, and large projects. Second, we detected SATD at the\nrelease level and applied statistical analysis to examine its correlation with\ncommunity smells. Our results showed that certain smells, such as Radio Silence\nand Organizational Silos, are strongly correlated with higher SATD occurrences.\nThird, we considered the six identified types of SATD to determine which\ncommunity smells are most associated with each debt category. Our analysis\nrevealed authority- and communication-related smells often co-occur with\npersistent code and design debt. Finally, we analyzed how the community smells\nand SATD evolve over the releases, uncovering project size-dependent trends and\nshared trajectories. Our findings emphasize the importance of early detection\nand mitigation of socio-technical issues to maintain the long-term quality and\nsustainability of ML-based systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.15884v2", "cate": "cs.SE", "date": "2025-06-18", "updated": "2025-07-31"}
{"id": "2508.00175", "title": "Adaptive Compensation of Nonlinear Friction in Mechanical Systems Without Velocity Measurement", "authors": ["Jose Guadalupe Romero", "Romeo Ortega", "Leyan Fang", "Alexey Bobtsov"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00175v1", "summary": "Friction is an unavoidable phenomenon that exists in all mechanical systems\nincorporating parts with relative motion. It is well-known that friction is a\nserious impediment for precise servo control, hence the interest to devise a\nprocedure to compensate for it -- a subject that has been studied by many\nresearchers for many years. The vast majority of friction compensation schemes\nreported in the literature rely on the availability of velocity measurements,\nan information that is hard to obtain. A second limitation of the existing\nprocedures is that they rely on mathematical models of friction that contain\nseveral unknown parameters, some of them entering nonlinearly in the dynamic\nequations. In this paper we propose a globally convergent tracking controller\nfor a mechanical system perturbed by static and Coulomb friction, which is a\nreliable mathematical model of the friction phenomenon, that does not rely one\nmeasurement of velocity. The key component is an immersion and invariance-based\nadaptive speed observer, used for the friction compensation. To the best of our\nknowledge, this is the first globally convergent solution to this challenging\nproblem. We also present simulation results of the application of our observer\nfor systems affected by friction, which is described by the more advanced LuGre\nmodel.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00175v1", "cate": "eess.SY", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2507.23523", "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "authors": ["Hongzhe Bi", "Lingxuan Wu", "Tianwei Lin", "Hengkai Tan", "Zhizhong Su", "Hang Su", "Jun Zhu"], "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23523v2", "summary": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23523v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.02724", "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms", "authors": ["Shiyi Liu", "Buwen Liang", "Yuetong Fang", "Zixuan Jiang", "Renjing Xu"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02724v3", "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02724v3", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-08-04"}
{"id": "2505.11494", "title": "SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics", "authors": ["Lizhi Yang", "Blake Werner", "Ryan K. Cosner", "David Fridovich-Keil", "Preston Culbertson", "Aaron D. Ames"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Video at this https URL . To appear at IROS 2025", "url": "http://arxiv.org/abs/2505.11494v2", "summary": "Robot learning has produced remarkably effective ``black-box'' controllers\nfor complex tasks such as dynamic locomotion on humanoids. Yet ensuring dynamic\nsafety, i.e., constraint satisfaction, remains challenging for such policies.\nReinforcement learning (RL) embeds constraints heuristically through reward\nengineering, and adding or modifying constraints requires retraining.\nModel-based approaches, like control barrier functions (CBFs), enable runtime\nconstraint specification with formal guarantees but require accurate dynamics\nmodels. This paper presents SHIELD, a layered safety framework that bridges\nthis gap by: (1) training a generative, stochastic dynamics residual model\nusing real-world data from hardware rollouts of the nominal controller,\ncapturing system behavior and uncertainties; and (2) adding a safety layer on\ntop of the nominal (learned locomotion) controller that leverages this model\nvia a stochastic discrete-time CBF formulation enforcing safety constraints in\nprobability. The result is a minimally-invasive safety layer that can be added\nto the existing autonomy stack to give probabilistic guarantees of safety that\nbalance risk and performance. In hardware experiments on an Unitree G1\nhumanoid, SHIELD enables safe navigation (obstacle avoidance) through varied\nindoor and outdoor environments using a nominal (unknown) RL controller and\nonboard perception.", "comment": "Video at https://youtu.be/-Qv1wR4jfj4. To appear at IROS 2025", "pdf_url": "http://arxiv.org/pdf/2505.11494v2", "cate": "cs.RO", "date": "2025-05-16", "updated": "2025-08-01"}
{"id": "2507.17049", "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots", "authors": ["Pablo Valle", "Chengjie Lu", "Shaukat Ali", "Aitor Arrieta"], "categories": ["cs.SE", "cs.RO"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17049v2", "summary": "Visual Language Action (VLA) models are a multi-modal class of Artificial\nIntelligence (AI) systems that integrate visual perception, natural language\nunderstanding, and action planning to enable agents to interpret their\nenvironment, comprehend instructions, and perform embodied tasks autonomously.\nRecently, significant progress has been made to advance this field. These kinds\nof models are typically evaluated through task success rates, which fail to\ncapture the quality of task execution and the mode's confidence in its\ndecisions. In this paper, we propose eight uncertainty metrics and five quality\nmetrics specifically designed for VLA models for robotic manipulation tasks. We\nassess their effectiveness through a large-scale empirical study involving 908\nsuccessful task executions from three state-of-the-art VLA models across four\nrepresentative robotic manipulation tasks. Human domain experts manually\nlabeled task quality, allowing us to analyze the correlation between our\nproposed metrics and expert judgments. The results reveal that several metrics\nshow moderate to strong correlation with human assessments, highlighting their\nutility for evaluating task quality and model confidence. Furthermore, we found\nthat some of the metrics can discriminate between high-, medium-, and\nlow-quality executions from unsuccessful tasks, which can be interesting when\ntest oracles are not available. Our findings challenge the adequacy of current\nevaluation practices that rely solely on binary success rates and pave the way\nfor improved real-time monitoring and adaptive enhancement of VLA-enabled\nrobotic systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17049v2", "cate": "cs.SE", "date": "2025-07-22", "updated": "2025-07-31"}
{"id": "2508.00283", "title": "Neural Co-state Projection Regulator: A Model-free Paradigm for Real-time Optimal Control with Input Constraints", "authors": ["Lihan Lian", "Uduak Inyang-Udoh"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00283v1", "summary": "Learning-based approaches, notably Reinforcement Learning (RL), have shown\npromise for solving optimal control tasks without explicit system models.\nHowever, these approaches are often sample-inefficient, sensitive to reward\ndesign and hyperparameters, and prone to poor generalization, especially under\ninput constraints. To address these challenges, we introduce the neural\nco-state projection regulator (NCPR), a model-free learning-based optimal\ncontrol framework that is grounded in Pontryagin's Minimum Principle (PMP) and\ncapable of solving quadratic regulator problems in nonlinear control-affine\nsystems with input constraints. In this framework, a neural network (NN) is\ntrained in a self-supervised setting to take the current state of the system as\ninput and predict a finite-horizon trajectory of projected co-states (i.e., the\nco-state weighted by the system's input gain). Subsequently, only the first\nelement of the NN's prediction is extracted to solve a lightweight quadratic\nprogram (QP). This workflow is executed in a feedback control setting, allowing\nreal-time computation of control actions that satisfy both input constraints\nand first-order optimality conditions.\n  We test the proposed learning-based model-free quadratic regulator on (1) a\nunicycle model robot reference tracking problem and (2) a pendulum swing-up\ntask. For comparison, reinforcement learning is used on both tasks; and for\ncontext, a model-based controller is used in the unicycle model example. Our\nmethod demonstrates superior generalizability in terms of both unseen system\nstates and varying input constraints, and also shows improved sampling\nefficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00283v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.18926", "title": "Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction", "authors": ["Trung Nguyen", "Md Masud Rana", "Farjana Tasnim Mukta", "Chang-Guo Zhan", "Duc Duy Nguyen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18926v2", "summary": "Accurate prediction of blood-brain barrier permeability (BBBP) is essential\nfor central nervous system (CNS) drug development. While graph neural networks\n(GNNs) have advanced molecular property prediction, they often rely on\nmolecular topology and neglect the three-dimensional geometric information\ncrucial for modeling transport mechanisms. This paper introduces the geometric\nmulti-color message-passing graph neural network (GMC-MPNN), a novel framework\nthat enhances standard message-passing architectures by explicitly\nincorporating atomic-level geometric features and long-range interactions. Our\nmodel constructs weighted colored subgraphs based on atom types to capture the\nspatial relationships and chemical context that govern BBB permeability. We\nevaluated GMC-MPNN on three benchmark datasets for both classification and\nregression tasks, using rigorous scaffold-based splitting to ensure a robust\nassessment of generalization. The results demonstrate that GMC-MPNN\nconsistently outperforms existing state-of-the-art models, achieving superior\nperformance in both classifying compounds as permeable/non-permeable (AUC-ROC\nof 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of\n0.4609, Pearson correlation of 0.7759). An ablation study further quantified\nthe impact of specific atom-pair interactions, revealing that the model's\npredictive power derives from its ability to learn from both common and rare,\nbut chemically significant, functional motifs. By integrating spatial geometry\ninto the graph representation, GMC-MPNN sets a new performance benchmark and\noffers a more accurate and generalizable tool for drug discovery pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18926v2", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-08-01"}
{"id": "2507.21553", "title": "Multi-robot LiDAR SLAM: a practical case study in underground tunnel environments", "authors": ["Federica Di Lauro", "Domenico G. Sorrenti", "Miguel Angel Sotelo"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      14 pages, 14 figures", "url": "http://arxiv.org/abs/2507.21553v3", "summary": "Multi-robot SLAM aims at localizing and building a map with multiple robots,\ninteracting with each other. In the work described in this article, we analyze\nthe pipeline of a decentralized LiDAR SLAM system to study the current\nlimitations of the state of the art, and we discover a significant source of\nfailures, i.e., that the loop detection is the source of too many false\npositives. We therefore develop and propose a new heuristic to overcome these\nlimitations. The environment taken as reference in this work is the highly\nchallenging case of underground tunnels. We also highlight potential new\nresearch areas still under-explored.", "comment": "14 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.21553v3", "cate": "cs.RO", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2507.18130", "title": "NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition", "authors": ["Le Deng", "Zhonghao Jiang", "Jialun Cao", "Michael Pradel", "Zhongxin Liu"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18130v2", "summary": "Natural language-driven no-code development allows users to specify software\nfunctionality using natural language (NL) instead of editing source code,\npromising increased productivity and democratized development. Large language\nmodels (LLMs) show potential in enabling this paradigm. In this context,\nsoftware documentation acts as an NL specification for functionality. This work\nintroduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world\nNL-driven feature addition tasks, consisting of 634 tasks across 10 projects\nand 114k code changes. Each task pairs documentation updates with corresponding\ncode implementations, validated by developer-written test cases. A subset of\n114 high-quality, human-verified instances, NoCode-bench Verified, ensures\nreliable evaluation. Our experiments reveal that, despite high token usage, the\nbest LLMs achieve a task success rate of only 15.79%, highlighting challenges\nin cross-file editing, codebase understanding, and tool calling. These findings\nindicate that LLMs are not yet ready for fully NL-driven no-code development.\nNoCode-bench lays the foundation for future advances in this area.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18130v2", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-08-01"}
{"id": "2508.00609", "title": "Low-dimensional observer design for stable linear systems by model reduction", "authors": ["M. F. Shakib", "M. Khalil", "R. Postoyan"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00609v1", "summary": "This paper presents a low-dimensional observer design for stable,\nsingle-input single-output, continuous-time linear time-invariant (LTI)\nsystems. Leveraging the model reduction by moment matching technique, we\napproximate the system with a reduced-order model. Based on this reduced-order\nmodel, we design a low-dimensional observer that estimates the states of the\noriginal system. We show that this observer establishes exact asymptotic state\nreconstruction for a given class of inputs tied to the observer's dimension.\nFurthermore, we establish an exponential input-to-state stability property for\ngeneric inputs, ensuring a bounded estimation error. Numerical simulations\nconfirm the effectiveness of the approach for a benchmark model reduction\nproblem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00609v1", "cate": "eess.SY", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.21053", "title": "Flow Matching Policy Gradients", "authors": ["David McAllister", "Songwei Ge", "Brent Yi", "Chung Min Kim", "Ethan Weber", "Hongsuk Choi", "Haiwen Feng", "Angjoo Kanazawa"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      See our blog post at this https URL", "url": "http://arxiv.org/abs/2507.21053v2", "summary": "Flow-based generative models, including diffusion models, excel at modeling\ncontinuous distributions in high-dimensional spaces. In this work, we introduce\nFlow Policy Optimization (FPO), a simple on-policy reinforcement learning\nalgorithm that brings flow matching into the policy gradient framework. FPO\ncasts policy optimization as maximizing an advantage-weighted ratio computed\nfrom the conditional flow matching loss, in a manner compatible with the\npopular PPO-clip framework. It sidesteps the need for exact likelihood\ncomputation while preserving the generative capabilities of flow-based models.\nUnlike prior approaches for diffusion-based reinforcement learning that bind\ntraining to a specific sampling method, FPO is agnostic to the choice of\ndiffusion or flow integration at both training and inference time. We show that\nFPO can train diffusion-style policies from scratch in a variety of continuous\ncontrol tasks. We find that flow-based models can capture multimodal action\ndistributions and achieve higher performance than Gaussian policies,\nparticularly in under-conditioned settings.", "comment": "See our blog post at https://flowreinforce.github.io", "pdf_url": "http://arxiv.org/pdf/2507.21053v2", "cate": "cs.LG", "date": "2025-07-28", "updated": "2025-08-01"}
{"id": "2507.23172", "title": "Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks", "authors": ["Viraj Joshi", "Zifan Xu", "Bo Liu", "Peter Stone", "Amy Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      RLC 2025", "url": "http://arxiv.org/abs/2507.23172v2", "summary": "Multi-task Reinforcement Learning (MTRL) has emerged as a critical training\nparadigm for applying reinforcement learning (RL) to a set of complex\nreal-world robotic tasks, which demands a generalizable and robust policy. At\nthe same time, \\emph{massively parallelized training} has gained popularity,\nnot only for significantly accelerating data collection through GPU-accelerated\nsimulation but also for enabling diverse data collection across multiple tasks\nby simulating heterogeneous scenes in parallel. However, existing MTRL research\nhas largely been limited to off-policy methods like SAC in the\nlow-parallelization regime. MTRL could capitalize on the higher asymptotic\nperformance of on-policy algorithms, whose batches require data from the\ncurrent policy, and as a result, take advantage of massive parallelization\noffered by GPU-accelerated simulation. To bridge this gap, we introduce a\nmassively parallelized $\\textbf{M}$ulti-$\\textbf{T}$ask $\\textbf{Bench}$mark\nfor robotics (MTBench), an open-sourced benchmark featuring a broad\ndistribution of 50 manipulation tasks and 20 locomotion tasks, implemented\nusing the GPU-accelerated simulator IsaacGym. MTBench also includes four base\nRL algorithms combined with seven state-of-the-art MTRL algorithms and\narchitectures, providing a unified framework for evaluating their performance.\nOur extensive experiments highlight the superior speed of evaluating MTRL\napproaches using MTBench, while also uncovering unique challenges that arise\nfrom combining massive parallelism with MTRL. Code is available at\nhttps://github.com/Viraj-Joshi/MTBench", "comment": "RLC 2025", "pdf_url": "http://arxiv.org/pdf/2507.23172v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00603", "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00603v1", "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00603v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2507.23437", "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to the 2025 International Conference on Computer-Aided Design (ICCAD); 9 pages, including 6 figures and 7 tables", "url": "http://arxiv.org/abs/2507.23437v2", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experimental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "comment": "Accepted to the 2025 International Conference on Computer-Aided\n  Design (ICCAD); 9 pages, including 6 figures and 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.23437v2", "cate": "cs.LG", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2507.23339", "title": "Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits", "authors": ["Yihan Zhou", "Yiwen Lu", "Bo Yang", "Jiayun Li", "Yilin Mo"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23339v2", "summary": "Drifting, characterized by controlled vehicle motion at high sideslip angles,\nis crucial for safely handling emergency scenarios at the friction limits.\nWhile recent reinforcement learning approaches show promise for drifting\ncontrol, they struggle with the significant simulation-to-reality gap, as\npolicies that perform well in simulation often fail when transferred to\nphysical systems. In this paper, we present a reinforcement learning framework\nwith GPU-accelerated parallel simulation and systematic domain randomization\nthat effectively bridges the gap. The proposed approach is validated on both\nsimulation and a custom-designed and open-sourced 1/10 scale Individual Wheel\nDrive (IWD) RC car platform featuring independent wheel speed control.\nExperiments across various scenarios from steady-state circular drifting to\ndirection transitions and variable-curvature path following demonstrate that\nour approach achieves precise trajectory tracking while maintaining controlled\nsideslip angles throughout complex maneuvers in both simulated and real-world\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23339v2", "cate": "cs.RO", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2508.00663", "title": "Organic Electrochemical Neurons: Nonlinear Tools for Complex Dynamics", "authors": ["Gonzalo Rivera-Sierra", "Roberto Fenollosa", "Juan Bisquert"], "categories": ["physics.chem-ph", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00663v1", "summary": "Hybrid oscillator architectures that combine feedback oscillators with\nself-sustained negative resistance oscillators have emerged as a promising\nplatform for artificial neuron design. In this work, we introduce a modeling\nand analysis framework for amplifier-assisted organic electrochemical neurons,\nleveraging nonlinear dynamical systems theory. By formulating the system as\ncoupled differential equations describing membrane voltage and internal state\nvariables, we identify the conditions for self-sustained oscillations and\ncharacterize the resulting dynamics through nullclines, phase-space analysis,\nand bifurcation behavior, providing complementary insight to standard\ncircuit-theoretic arguments of the operation of oscillators. Our simplified yet\nrigorous model enables tractable analysis of circuits integrating classical\nfeedback components (e.g., operational amplifiers) with novel devices\nexhibiting negative differential resistance, such as organic electrochemical\ntransistors (OECT). This approach reveals the core mechanisms behind\noscillation generation, demonstrating the utility of dynamic systems theory in\nunderstanding and designing complex hybrid circuits. Beyond neuromorphic and\nbioelectronic applications, the proposed framework offers a generalizable\nfoundation for developing tunable, biologically inspired oscillatory systems in\nsensing, signal processing, and adaptive control.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00663v1", "cate": "physics.chem-ph", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00509", "title": "Dynamic Real-Time Ambisonics Order Adaptation for Immersive Networked Music Performances", "authors": ["Paolo Ostan", "Carlo Centofanti", "Mirco Pezzoli", "Alberto Bernardini", "Claudia Rinaldi", "Fabio Antonacci"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      to appear in EUSIPCO 2025", "url": "http://arxiv.org/abs/2508.00509v1", "summary": "Advanced remote applications such as Networked Music Performance (NMP)\nrequire solutions to guarantee immersive real-world-like interaction among\nusers. Therefore, the adoption of spatial audio formats, such as Ambisonics, is\nfundamental to let the user experience an immersive acoustic scene. The\naccuracy of the sound scene reproduction increases with the order of the\nAmbisonics enconding, resulting in an improved immersivity at the cost of a\ngreater number of audio channels, which in turn escalates both bandwidth\nrequirements and susceptibility to network impairments (e.g., latency, jitter,\nand packet loss). These factors pose a significant challenge for interactive\nmusic sessions, which demand high spatial fidelity and low end-to-end delay. We\npropose a real-time adaptive higher-order Ambisonics strategy that continuously\nmonitors network throughput and dynamically scales the Ambisonics order. When\navailable bandwidth drops below a preset threshold, the order is lowered to\nprevent audio dropouts; it then reverts to higher orders once conditions\nrecover, thus balancing immersion and reliability. A MUSHRA-based evaluation\nindicates that this adaptive approach is promising to guarantee user experience\nin bandwidth-limited NMP scenarios.", "comment": "to appear in EUSIPCO 2025", "pdf_url": "http://arxiv.org/pdf/2508.00509v1", "cate": "eess.AS", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2303.16955", "title": "Quantum Generative Modeling using Parameterized Quantum Circuits", "authors": ["Soumyadip Sarkar"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.16955v2", "summary": "Quantum generative models use the intrinsic probabilistic nature of quantum\nmechanics to learn and reproduce complex probability distributions. In this\npaper, we present an implementation of a 3-qubit quantum circuit Born machine\ntrained to model a 3-bit Gaussian distribution using a Kullback-Leibler (KL)\ndivergence loss and parameter-shift gradient optimization. The variational\nquantum circuit consists of layers of parameterized rotations and entangling\ngates, and is optimized such that the Born rule output distribution closely\nmatches the target distribution. We detail the mathematical formulation of the\nmodel distribution, the KL divergence cost function, and the parameter-shift\nrule for gradient evaluation. Training results on a statevector simulator show\nthat the KL divergence is minimized to near zero, and the final generated\ndistribution aligns quantitatively with the target probabilities. We analyze\nthe convergence behavior and discuss the implications for scalability and\nquantum advantage. Our results demonstrate the feasibility of small-scale\nquantum generative learning and provide insight into the training dynamics of\nquantum circuit models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.16955v2", "cate": "quant-ph", "date": "2023-03-25", "updated": "2025-07-31"}
{"id": "2504.04312", "title": "Prescribed-Time Boresight Control of Spacecraft Under Pointing Constraints", "authors": ["Xiaodong Shao", "Haoyang Yang", "Haoran Li", "Zongyu Zuo", "Jose Guadalupe Romero", "Qinglei Hu"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04312v4", "summary": "This article proposes an integrated boresight guidance and control (IBGC)\nscheme to address the boresight reorientation problem of spacecraft under\ntemporal and pointing constraints. A $C^1$ continuous, saturated\nprescribed-time adjustment (PPTA) function is presented, along with the\nestablishment of a practical prescribed-time stability criterion. Utilizing the\ntime scale transformation technique and the PPTA function, we propose a\nprescribed-time guidance law that guides the boresight vector from almost any\ninitial orientation in free space to a small neighborhood of the goal\norientation within a preassigned time, while avoiding all forbidden zones\naugmented with safety margins. Subsequently, a prescribed-time disturbance\nobserver (PTDO) is derived to reconstruct the external disturbances. By\nleveraging barrier and PPTA functions, a PTDO-based reduced-attitude tracking\ncontroller is developed, which ensures prescribed-time boresight tracking\nwithin a ``safe tube''. By judiciously setting the safety margins, settling\ntimes, and safe tube for the guidance and control laws, the proposed IBGC\nscheme achieves pointing-constrained boresight reorientation within a required\ntask completion time. Simulation and experimental results demonstrate the\nefficacy of the proposed IBGC scheme.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04312v4", "cate": "eess.SY", "date": "2025-04-06", "updated": "2025-08-01"}
{"id": "2312.01046", "title": "Bagged Regularized $k$-Distances for Anomaly Detection", "authors": ["Yuchao Cai", "Hanfang Yang", "Yuheng Ma", "Hanyuan Hang"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.01046v3", "summary": "We consider the paradigm of unsupervised anomaly detection, which involves\nthe identification of anomalies within a dataset in the absence of labeled\nexamples. Though distance-based methods are top-performing for unsupervised\nanomaly detection, they suffer heavily from the sensitivity to the choice of\nthe number of the nearest neighbors. In this paper, we propose a new\ndistance-based algorithm called bagged regularized $k$-distances for anomaly\ndetection (BRDAD), converting the unsupervised anomaly detection problem into a\nconvex optimization problem. Our BRDAD algorithm selects the weights by\nminimizing the surrogate risk, i.e., the finite sample bound of the empirical\nrisk of the bagged weighted $k$-distances for density estimation (BWDDE). This\napproach enables us to successfully address the sensitivity challenge of the\nhyperparameter choice in distance-based algorithms. Moreover, when dealing with\nlarge-scale datasets, the efficiency issues can be addressed by the\nincorporated bagging technique in our BRDAD algorithm. On the theoretical side,\nwe establish fast convergence rates of the AUC regret of our algorithm and\ndemonstrate that the bagging technique significantly reduces the computational\ncomplexity. On the practical side, we conduct numerical experiments to\nillustrate the insensitivity of the parameter selection of our algorithm\ncompared with other state-of-the-art distance-based methods. Furthermore, our\nmethod achieves superior performance on real-world datasets with the introduced\nbagging technique compared to other approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.01046v3", "cate": "stat.ML", "date": "2023-12-02", "updated": "2025-08-01"}
{"id": "2504.00244", "title": "System Identification from Partial Observations under Adversarial Attacks", "authors": ["Jihun Kim", "Javad Lavaei"], "categories": ["math.OC", "cs.SY", "eess.SY", "93B15, 93B30, 93C05"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2504.00244v2", "summary": "This paper is concerned with the partially observed linear system\nidentification, where the goal is to obtain reasonably accurate estimation of\nthe balanced truncation of the true system up to order $k$ from output\nmeasurements. We consider the challenging case of system identification under\nadversarial attacks, where the probability of having an attack at each time is\n$\\Theta(1/k)$ while the value of the attack is arbitrary. We first show that\nthe $\\ell_1$-norm estimator exactly identifies the true Markov parameter matrix\nfor nilpotent systems under any type of attack. We then build on this result to\nextend it to general systems and show that the estimation error exponentially\ndecays as $k$ grows. The estimated balanced truncation model accordingly shows\nan exponentially decaying error for the identification of the true system up to\na similarity transformation. This work is the first to provide the input-output\nanalysis of the system with partial observations under arbitrary attacks.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2504.00244v2", "cate": "math.OC", "date": "2025-03-31", "updated": "2025-08-01"}
{"id": "2405.16958", "title": "Large Deviations of Gaussian Neural Networks with ReLU activation", "authors": ["Quirin Vogel"], "categories": ["stat.ML", "cs.LG", "math.PR", "60F10, 68T07"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, proof simplified", "url": "http://arxiv.org/abs/2405.16958v2", "summary": "We prove a large deviation principle for deep neural networks with Gaussian\nweights and at most linearly growing activation functions, such as ReLU. This\ngeneralises earlier work, in which bounded and continuous activation functions\nwere considered. In practice, linearly growing activation functions such as\nReLU are most commonly used. We furthermore simplify previous expressions for\nthe rate function and provide a power-series expansions for the ReLU case.", "comment": "13 pages, 2 figures, proof simplified", "pdf_url": "http://arxiv.org/pdf/2405.16958v2", "cate": "stat.ML", "date": "2024-05-27", "updated": "2025-08-01"}
{"id": "2406.15500", "title": "Pure interaction effects unseen by Random Forests", "authors": ["Ricardo Blum", "Munir Hiabu", "Enno Mammen", "Joseph Theo Meyer"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2309.01460", "url": "http://arxiv.org/abs/2406.15500v2", "summary": "Random Forests are widely claimed to capture interactions well. However, some\nsimple examples suggest that they perform poorly in the presence of certain\npure interactions that the conventional CART criterion struggles to capture\nduring tree construction. Motivated from this, it is argued that simple\nalternative partitioning schemes used in the tree growing procedure can enhance\nidentification of these interactions. In a simulation study these variants are\ncompared to conventional Random Forests and Extremely Randomized Trees. The\nresults validate that the modifications considered enhance the model's fitting\nability in scenarios where pure interactions play a crucial role. Finally, the\nmethods are applied to real datasets.", "comment": "arXiv admin note: substantial text overlap with arXiv:2309.01460", "pdf_url": "http://arxiv.org/pdf/2406.15500v2", "cate": "stat.ML", "date": "2024-06-19", "updated": "2025-08-01"}
{"id": "2508.00164", "title": "On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity", "authors": ["Sourya Sengupta", "Jianquan Xu", "Phuong Nguyen", "Frank J. Brooks", "Yang Liu", "Mark A. Anastasio"], "categories": ["eess.IV", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00164v1", "summary": "Virtual staining, or in-silico-labeling, has been proposed to computationally\ngenerate synthetic fluorescence images from label-free images by use of deep\nlearning-based image-to-image translation networks. In most reported studies,\nvirtually stained images have been assessed only using traditional image\nquality measures such as structural similarity or signal-to-noise ratio.\nHowever, in biomedical imaging, images are typically acquired to facilitate an\nimage-based inference, which we refer to as a downstream biological or clinical\ntask. This study systematically investigates the utility of virtual staining\nfor facilitating clinically relevant downstream tasks (like segmentation or\nclassification) with consideration of the capacity of the deep neural networks\nemployed to perform the tasks. Comprehensive empirical evaluations were\nconducted using biological datasets, assessing task performance by use of\nlabel-free, virtually stained, and ground truth fluorescence images. The\nresults demonstrated that the utility of virtual staining is largely dependent\non the ability of the segmentation or classification task network to extract\nmeaningful task-relevant information, which is related to the concept of\nnetwork capacity. Examples are provided in which virtual staining does not\nimprove, or even degrades, segmentation or classification performance when the\ncapacity of the associated task network is sufficiently large. The results\ndemonstrate that task network capacity should be considered when deciding\nwhether to perform virtual staining.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00164v1", "cate": "eess.IV", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2409.01413", "title": "Probabilistic Iterative Hard Thresholding for Sparse Learning", "authors": ["Matteo Bergamaschi", "Andrea Cristofari", "Vyacheslav Kungurtsev", "Francesco Rinaldi"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01413v2", "summary": "For statistical modeling wherein the data regime is unfavorable in terms of\ndimensionality relative to the sample size, finding hidden sparsity in the\nground truth can be critical in formulating an accurate statistical model. The\nso-called \"l0 norm\" which counts the number of non-zero components in a vector,\nis a strong reliable mechanism of enforcing sparsity when incorporated into an\noptimization problem for minimizing the fit of a given model to a set of\nobservations. However, in big data settings wherein noisy estimates of the\ngradient must be evaluated out of computational necessity, the literature is\nscant on methods that reliably converge. In this paper we present an approach\ntowards solving expectation objective optimization problems with cardinality\nconstraints. We prove convergence of the underlying stochastic process, and\ndemonstrate the performance on two Machine Learning problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01413v2", "cate": "math.OC", "date": "2024-09-02", "updated": "2025-08-01"}
{"id": "2508.00781", "title": "Numerical Uncertainty in Linear Registration: An Experimental Study", "authors": ["Niusha Mirhakimi", "Yohan Chatelain", "Jean-Baptiste Poline", "Tristan Glatard"], "categories": ["q-bio.QM", "eess.IV"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00781v1", "summary": "While linear registration is a critical step in MRI preprocessing pipelines,\nits numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA)\nsimulations, we assessed the most commonly used linear registration tools\nwithin major software packages (SPM, FSL, and ANTs) across multiple image\nsimilarity measures, two brain templates, and both healthy control (HC, n=50)\nand Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the\ninfluence of linear registration tools and similarity measures on numerical\nstability. Among the evaluated tools and with default similarity measures, SPM\nexhibited the highest stability. FSL and ANTs showed greater and similar ranges\nof variability, with ANTs demonstrating particular sensitivity to numerical\nperturbations that occasionally led to registration failure. Furthermore, no\nsignificant differences were observed between healthy and PD cohorts,\nsuggesting that numerical stability analyses obtained with healthy subjects may\ngeneralise to clinical populations. Finally, we also demonstrated how numerical\nuncertainty measures may support automated quality control (QC) of linear\nregistration results. Overall, our experimental results characterize the\nnumerical stability of linear registration experimentally and can serve as a\nbasis for future uncertainty analyses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00781v1", "cate": "q-bio.QM", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2409.01908", "title": "Bayesian CART models for aggregate claim modeling", "authors": ["Yaojun Zhang", "Lanpeng Ji", "Georgios Aivaliotis", "Charles C. Taylor"], "categories": ["stat.ME", "cs.LG", "q-fin.ST", "stat.AP", "stat.ML"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.01908v2", "summary": "This paper proposes three types of Bayesian CART (or BCART) models for\naggregate claim amount, namely, frequency-severity models, sequential models\nand joint models. We propose a general framework for the BCART models\napplicable to data with multivariate responses, which is particularly useful\nfor the joint BCART models with a bivariate response: the number of claims and\naggregate claim amount. To facilitate frequency-severity modeling, we\ninvestigate BCART models for the right-skewed and heavy-tailed claim severity\ndata by using various distributions. We discover that the Weibull distribution\nis superior to gamma and lognormal distributions, due to its ability to capture\ndifferent tail characteristics in tree models. Additionally, we find that\nsequential BCART models and joint BCART models, which incorporate dependence\nbetween the number of claims and average severity, are beneficial and thus\npreferable to the frequency-severity BCART models in which independence is\nassumed. The effectiveness of these models' performance is illustrated by\ncarefully designed simulations and real insurance data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.01908v2", "cate": "stat.ME", "date": "2024-09-03", "updated": "2025-08-01"}
{"id": "2503.03778", "title": "Generating Novel Brain Morphology by Deforming Learned Templates", "authors": ["Alan Q. Wang", "Fangrui Huang", "Bailey Trang", "Wei Peng", "Mohammad Abbasi", "Kilian Pohl", "Mert Sabuncu", "Ehsan Adeli"], "categories": ["eess.IV", "q-bio.TO"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Provisional Acceptance at MICCAI 2025", "url": "http://arxiv.org/abs/2503.03778v3", "summary": "Designing generative models for 3D structural brain MRI that synthesize\nmorphologically-plausible and attribute-specific (e.g., age, sex, disease\nstate) samples is an active area of research. Existing approaches based on\nframeworks like GANs or diffusion models synthesize the image directly, which\nmay limit their ability to capture intricate morphological details. In this\nwork, we propose a 3D brain MRI generation method based on state-of-the-art\nlatent diffusion models (LDMs), called MorphLDM, that generates novel images by\napplying synthesized deformation fields to a learned template. Instead of using\na reconstruction-based autoencoder (as in a typical LDM), our encoder outputs a\nlatent embedding derived from both an image and a learned template that is\nitself the output of a template decoder; this latent is passed to a deformation\nfield decoder, whose output is applied to the learned template. A registration\nloss is minimized between the original image and the deformed template with\nrespect to the encoder and both decoders. Empirically, our approach outperforms\ngenerative baselines on metrics spanning image diversity, adherence with\nrespect to input conditions, and voxel-based morphometry. Our code is available\nat https://github.com/alanqrwang/morphldm.", "comment": "Provisional Acceptance at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2503.03778v3", "cate": "eess.IV", "date": "2025-03-04", "updated": "2025-07-31"}
{"id": "2505.10498", "title": "Batched Nonparametric Bandits via k-Nearest Neighbor UCB", "authors": ["Sakshi Arya"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "68T05, 62L05, 62G08, 68Q32", "F.2.2; I.2.6"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.10498v2", "summary": "We study sequential decision-making in batched nonparametric contextual\nbandits, where actions are selected over a finite horizon divided into a small\nnumber of batches. Motivated by constraints in domains such as medicine and\nmarketing -- where online feedback is limited -- we propose a nonparametric\nalgorithm that combines adaptive k-nearest neighbor (k-NN) regression with the\nupper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully\nnonparametric, adapts to the context dimension, and is simple to implement.\nUnlike prior work relying on parametric or binning-based estimators, BaNk-UCB\nuses local geometry to estimate rewards and adaptively balances exploration and\nexploitation. We provide near-optimal regret guarantees under standard\nLipschitz smoothness and margin assumptions, using a theoretically motivated\nbatch schedule that balances regret across batches and achieves minimax-optimal\nrates. Empirical evaluations on synthetic and real-world datasets demonstrate\nthat BaNk-UCB consistently outperforms binning-based baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.10498v2", "cate": "stat.ML", "date": "2025-05-15", "updated": "2025-08-01"}
{"id": "2507.22790", "title": "Optimizing Federated Learning Configurations for MRI Prostate Segmentation and Cancer Detection: A Simulation Study", "authors": ["Ashkan Moradi", "Fadila Zerka", "Joeran S. Bosma", "Mohammed R. S. Sunoqrot", "Bendik S. Abrahamsen", "Derya Yakar", "Jeroen Geerdink", "Henkjan Huisman", "Tone Frost Bathen", "Mattijs Elschot"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      25 pages, 6 figures, 4 tables. Accepted for publication in Radiology: Artificial Intelligence, \\c{opyright} 2025 Radiological Society of North America (RSNA)", "url": "http://arxiv.org/abs/2507.22790v2", "summary": "Purpose: To develop and optimize a federated learning (FL) framework across\nmultiple clients for biparametric MRI prostate segmentation and clinically\nsignificant prostate cancer (csPCa) detection. Materials and Methods: A\nretrospective study was conducted using Flower FL to train a nnU-Net-based\narchitecture for MRI prostate segmentation and csPCa detection, using data\ncollected from January 2010 to August 2021. Model development included training\nand optimizing local epochs, federated rounds, and aggregation strategies for\nFL-based prostate segmentation on T2-weighted MRIs (four clients, 1294\npatients) and csPCa detection using biparametric MRIs (three clients, 1440\npatients). Performance was evaluated on independent test sets using the Dice\nscore for segmentation and the Prostate Imaging: Cancer Artificial Intelligence\n(PI-CAI) score, defined as the average of the area under the receiver operating\ncharacteristic curve and average precision, for csPCa detection. P-values for\nperformance differences were calculated using permutation testing. Results: The\nFL configurations were independently optimized for both tasks, showing improved\nperformance at 1 epoch 300 rounds using FedMedian for prostate segmentation and\n5 epochs 200 rounds using FedAdagrad, for csPCa detection. Compared with the\naverage performance of the clients, the optimized FL model significantly\nimproved performance in prostate segmentation and csPCa detection on the\nindependent test set. The optimized FL model showed higher lesion detection\nperformance compared to the FL-baseline model, but no evidence of a difference\nwas observed for prostate segmentation. Conclusions: FL enhanced the\nperformance and generalizability of MRI prostate segmentation and csPCa\ndetection compared with local models, and optimizing its configuration further\nimproved lesion detection performance.", "comment": "25 pages, 6 figures, 4 tables. Accepted for publication in Radiology:\n  Artificial Intelligence, \\c{opyright} 2025 Radiological Society of North\n  America (RSNA)", "pdf_url": "http://arxiv.org/pdf/2507.22790v2", "cate": "eess.IV", "date": "2025-07-30", "updated": "2025-07-31"}
{"id": "2507.19861", "title": "Quantum-Informed Machine Learning for Chaotic Systems", "authors": ["Maida Wang", "Xiao Xue", "Peter V. Coveney"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      33 pages, 4 figures", "url": "http://arxiv.org/abs/2507.19861v2", "summary": "Learning the behaviour of chaotic systems remains challenging due to\ninstability in long-term predictions and difficulties in accurately capturing\ninvariant statistical properties. While quantum machine learning offers a\npromising route to efficiently capture physical properties from\nhigh-dimensional data, its practical deployment is hindered by current hardware\nnoise and limited scalability. Here, we introduce a quantum-informed machine\nlearning framework for learning partial differential equations, with an\napplication focus on chaotic systems. A quantum circuit Born machine is\nemployed to learn the invariant properties of chaotic dynamical systems,\nachieving substantial memory efficiency by representing these complex physical\nstatistics with a compact set of trainable circuit parameters. This approach\nreduces the data storage requirement by over two orders of magnitude compared\nto the raw simulation data. The resulting statistical quantum-informed prior is\nthen incorporated into a Koopman-based auto-regressive model to address issues\nsuch as gradient vanishing or explosion, while maintaining long-term\nstatistical fidelity. The framework is evaluated on three representative\nsystems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and\nturbulent channel flow. In all cases, the quantum-informed model achieves\nsuperior performance compared to its classical counterparts without quantum\npriors. This hybrid architecture offers a practical route for learning\ndynamical systems using near-term quantum hardware.", "comment": "33 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.19861v2", "cate": "quant-ph", "date": "2025-07-26", "updated": "2025-08-01"}
{"id": "2507.20403", "title": "A General Framework for Estimating Preferences Using Response Time Data", "authors": ["Federico Echenique", "Alireza Fallah", "Michael I. Jordan"], "categories": ["econ.TH", "cs.LG"], "primary_category": "Subjects:       Theoretical Economics (econ.TH)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.20403v2", "summary": "We propose a general methodology for recovering preference parameters from\ndata on choices and response times. Our methods yield estimates with fast\n($1/n$ for $n$ data points) convergence rates when specialized to the popular\nDrift Diffusion Model (DDM), but are broadly applicable to generalizations of\nthe DDM as well as to alternative models of decision making that make use of\nresponse time data. The paper develops an empirical application to an\nexperiment on intertemporal choice, showing that the use of response times\ndelivers predictive accuracy and matters for the estimation of economically\nrelevant parameters.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.20403v2", "cate": "econ.TH", "date": "2025-07-27", "updated": "2025-07-31"}
{"id": "2508.00093", "title": "Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering", "authors": ["Lucas Alves Zischler", "Chiara Lasagni", "Paolo Serena", "Alberto Bononi", "Giammarco Di Sciullo", "Divya A. Shaji", "Antonio Mecozzi", "Cristian Antonelli"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Submitted for the Journal of Lightwave Technology", "url": "http://arxiv.org/abs/2508.00093v1", "summary": "Wideband systems experience significant inter-channel stimulated Raman\nscattering (ISRS) and channel-dependent losses. Due to the non-uniform\nattenuation profile, the combined effects of ISRS and fiber loss can only be\naccurately estimated using numerical methods. In this work, we present an\napproximate closed-form expression for the channels' power profile accounting\nfor these combined effects. We validate the proposed expression against\nnumerical solutions in the case of CLU transmission, showing high accuracy for\nboth single-span and multi-span fiber-optic links. Additionally, we derive an\ninverse expression, formulated as a function of the output power, which can be\nutilized to target a desired optical signal-to-noise ratio (OSNR) profile\nthrough pre-emphasis of the launched channel powers.", "comment": "Submitted for the Journal of Lightwave Technology", "pdf_url": "http://arxiv.org/pdf/2508.00093v1", "cate": "eess.SP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00274", "title": "RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder", "authors": ["Yunfei Liu", "Mingxuan Liu", "Wupeng Xie", "Xinzhu Liu", "Wenxue Liu", "Yangang Sun", "Xin Qiu", "Cui Yuan", "Jinhai Li"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00274v1", "summary": "Automatic modulation classification (AMC) is a basic technology in\nintelligent wireless communication systems. It is important for tasks such as\nspectrum monitoring, cognitive radio, and secure communications. In recent\nyears, deep learning methods have made great progress in AMC. However,\nmainstream methods still face two key problems. First, they often use\ntime-frequency images instead of raw signals. This causes loss of key\nmodulation features and reduces adaptability to different communication\nconditions. Second, most methods rely on supervised learning. This needs a\nlarge amount of labeled data, which is hard to get in real-world environments.\nTo solve these problems, we propose a self-supervised learning framework called\nRIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from\nunlabeled data. It takes raw IQ sequences as input. By applying random masking\nand reconstruction, it captures important time-domain features such as\namplitude, phase, etc. This helps the model learn useful and transferable\nrepresentations. RIS-MAE is tested on four datasets. The results show that it\nperforms better than existing methods in few-shot and cross-domain tasks.\nNotably, it achieves high classification accuracy on previously unseen datasets\nwith only a small number of fine-tuning samples, confirming its generalization\nability and potential for real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00274v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00326", "title": "Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems", "authors": ["Chengwang Ji", "Kehui Li", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00326v1", "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a\npromising architecture for future sixth-generation (6G) wireless networks. In\nparticular, the dynamic working mode configuration for the RDARS-aided system\nbrings an extra selection gain compared to the existing reconfigurable\nintelligent surface (RIS)-aided system and distributed antenna system (DAS). In\nthis paper, we consider the RDARS-aided downlink multiple-input multiple-output\n(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly\noptimizing the beamforming matrices at the based station (BS) and RDARS, as\nwell as mode switching matrix at RDARS. The optimization problem is challenging\nto be solved due to the non-convex objective function and mixed integer binary\nconstraint. To this end, a penalty term-based weight minimum mean square error\n(PWM) algorithm is proposed by integrating the majorization-minimization (MM)\nand weight minimum mean square error (WMMSE) methods. To further escape the\nlocal optimum point in the PWM algorithm, a model-driven DL method is\nintegrated into this algorithm, where the key variables related to the\nconvergence of PWM algorithm are trained to accelerate the convergence speed\nand improve the system performance. Simulation results are provided to show\nthat the PWM-based beamforming network (PWM-BFNet) can reduce the number of\niterations by half and achieve performance improvements of 26.53% and 103.2% at\nthe scenarios of high total transmit power and a large number of RDARS transmit\nelements (TEs), respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00326v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00409", "title": "STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted at 28th International Workshop on Smart Antennas 2025", "url": "http://arxiv.org/abs/2508.00409v1", "summary": "Rate splitting multiple access (RSMA) is intrinsically amalgamated with\nsimultaneously transmitting and reflecting (STAR) reconfigurable intelligent\nsurfaces (RIS) to enhance energy efficiency (EE) of the finite block length\n(FBL) multiple-input multiple-output (MIMO) downlink. An alternating\noptimization-based algorithm is proposed to jointly optimize the transmit\nbeamforming matrices, STAR-RIS configurations, and rate-splitting parameters.\nSTAR-RIS attains 360-degree full-plane coverage, while RSMA provides a\nprominent gain by efficiently managing interference. Numerical results reveal a\nstrong synergy between RSMA and STAR-RIS, demonstreating significant EE gains\nover reflective RIS and spatial division multiple access (SDMA).", "comment": "Accepted at 28th International Workshop on Smart Antennas 2025", "pdf_url": "http://arxiv.org/pdf/2508.00409v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00456", "title": "When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework", "authors": ["Ji Wang", "Bin Tang", "Jian Xiao", "Qimei Cui", "Xingwang Li", "Tony Q. S. Quek"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00456v1", "summary": "As the real propagation environment becomes in creasingly complex and\ndynamic, millimeter wave beam prediction faces huge challenges. However, the\npowerful cross modal representation capability of vision-language model (VLM)\nprovides a promising approach. The traditional methods that rely on real-time\nchannel state information (CSI) are computationally expensive and often fail to\nmaintain accuracy in such environments. In this paper, we present a VLM-driven\ncontrastive learning based multimodal beam prediction framework that integrates\nmultimodal data via modality-specific encoders. To enforce cross-modal\nconsistency, we adopt a contrastive pretraining strategy to align image and\nLiDAR features in the latent space. We use location information as text prompts\nand connect it to the text encoder to introduce language modality, which\nfurther improves cross-modal consistency. Experiments on the DeepSense-6G\ndataset show that our VLM backbone provides additional semantic grounding.\nCompared with existing methods, the overall distance-based accuracy score\n(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00456v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00494", "title": "Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency", "authors": ["Youngsun Kong", "Farnoush Baghestani", "I-Ping Chen", "Ki Chon"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted and presented at the 47th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "url": "http://arxiv.org/abs/2508.00494v1", "summary": "Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has\nbeen a promising non-invasive surrogate for accurate and effective assessment\nof the sympathetic nervous system (SNS). Typically, SKNA extraction requires a\nhigher sampling frequency than the typical ECG recording requirement (> 2 kHz)\nbecause analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,\nECG recording systems commonly provide a sampling frequency of 1 kHz or lower,\nparticularly for wearable devices. Our recent power spectral analysis exhibited\nthat 150-500 Hz frequency bands are dominant during sympathetic stimulation.\nTherefore, we hypothesize that SKNA can be extracted from ECG sampled at a\nlower sampling frequency. We collected ECG signals from 16 participants during\nSNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical\nanalyses of significance, classification performance, and reliability indicate\nno significant difference between SKNA indices derived from ECG signals sampled\nat 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,\nwhich are limited to low sampling rates due to resource constraints or outdated\nguidelines, can be used to reliably collect SKNA if muscle artifact\ncontamination is minimal.", "comment": "Accepted and presented at the 47th Annual International Conference of\n  the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "pdf_url": "http://arxiv.org/pdf/2508.00494v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2508.00800", "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding", "authors": ["Rui Chen", "Wen-Xuan Long", "Bing-Qian Wang", "Yuan He", "Rui-Jin Sun", "Nan Cheng", "Gan Zheng", "Dusit Niyato"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      38 pages, 18 figures", "url": "http://arxiv.org/abs/2508.00800v1", "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks.", "comment": "38 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2508.00800v1", "cate": "eess.SP", "date": "2025-08-01", "updated": "2025-08-01"}
{"id": "2411.07001", "title": "DoF Analysis and Beamforming Design for Active IRS-aided Multi-user MIMO Wireless Communication in Rank-deficient Channels", "authors": ["Jinbing Jiang", "Feng Shu", "Xuehui Wang", "Ke Yang", "Chong Shen", "Qi Zhang", "Dongming Wang", "Jiangzhou Wang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      12 pages, 9 figures", "url": "http://arxiv.org/abs/2411.07001v3", "summary": "Due to its ability of significantly improving data rate, intelligent\nreflecting surface (IRS) will be a potential crucial technique for the future\ngeneration wireless networks like 6G. In this paper, we will focus on the\nanalysis of degree of freedom (DoF) in IRS-aided multi-user MIMO network.\nFirstly, the DoF upper bound of IRS-aided single-user MIMO network, i.e., the\nachievable maximum DoF of such a system, is derived, and the corresponding\nresults are extended to the case of IRS-aided multiuser MIMO by using the\nmatrix rank inequalities. In particular, in serious rank-deficient, also called\nlow-rank, channels like line-of-sight (LoS), the network DoF may doubles over\nno-IRS with the help of IRS. To verify the rate performance gain from augmented\nDoF, three closed-form beamforming methods, null-space projection plus maximize\ntransmit power and maximize receive power (NSP-MTP-MRP), Schmidt\northogonalization plus minimum mean square error (SO-MMSE) and two-layer\nleakage plus MMSE (TLL-MMSE) are proposed to achieve the maximum DoF.\nSimulation results shows that IRS does make a dramatic rate enhancement. For\nexample, in a serious deficient channel, the sum-rate of the proposed TLL-MMSE\naided by IRS is about twice that of no IRS. This means that IRS may achieve a\nsignificant DoF improvement in such a channel.", "comment": "12 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2411.07001v3", "cate": "eess.SP", "date": "2024-11-11", "updated": "2025-08-01"}
{"id": "2507.21696", "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.21696v2", "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.21696v2", "cate": "eess.SP", "date": "2025-07-29", "updated": "2025-08-01"}
{"id": "2404.11790", "title": "Constrained Stochastic Recursive Momentum Successive Convex Approximation", "authors": ["Basil M. Idrees", "Lavish Arora", "Ketan Rajawat"], "categories": ["math.OC", "eess.SP"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      32 pages, 4 figures, journal submission", "url": "http://arxiv.org/abs/2404.11790v4", "summary": "We consider stochastic optimization problems with non-convex functional\nconstraints, such as those arising in trajectory generation, sparse\napproximation, and robust classification. To this end, we put forth a recursive\nmomentum-based accelerated successive convex approximation (SCA) algorithm. At\neach iteration, the proposed algorithm entails constructing convex surrogates\nof the stochastic objective and the constraint functions, and solving the\nresulting convex optimization problem. A recursive update rule is employed to\ntrack the gradient of the stochastic objective function, which contributes to\nvariance reduction and hence accelerates the algorithm convergence. A key\ningredient of the proof is a new parameterized version of the standard\nMangasarian-Fromowitz Constraints Qualification, that allows us to bound the\ndual variables and hence obtain problem-dependent bounds on the rate at which\nthe iterates approach an $\\epsilon$-stationary point. Remarkably, the proposed\nalgorithm achieves near-optimal stochastic first-order (SFO) complexity with\nadaptive step sizes closely matching that achieved by state-of-the-art\nstochastic optimization algorithms for solving unconstrained problems. As an\nexample, we detail an obstacle-avoiding trajectory optimization problem that\ncan be solved using the proposed algorithm and show that its performance is\nsuperior to that of the existing algorithms used for trajectory optimization.\nThe performance of the proposed algorithm is also shown to be comparable to\nthat of a specialized sparse classification algorithm applied to a binary\nclassification problem.", "comment": "32 pages, 4 figures, journal submission", "pdf_url": "http://arxiv.org/pdf/2404.11790v4", "cate": "math.OC", "date": "2024-04-17", "updated": "2025-08-01"}
{"id": "2410.02332", "title": "Polynomial time constructive decision algorithm for multivariable quantum signal processing", "authors": ["Yuki Ito", "Hitomi Mori", "Kazuki Sakamoto", "Keisuke Fujii"], "categories": ["quant-ph", "eess.SP"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      20 pages, 2 figures", "url": "http://arxiv.org/abs/2410.02332v2", "summary": "Quantum signal processing (QSP) and quantum singular value transformation\n(QSVT) have provided a unified framework for understanding many quantum\nalgorithms, including factorization, matrix inversion, and Hamiltonian\nsimulation. As a multivariable version of QSP, multivariable quantum signal\nprocessing (M-QSP) is proposed. M-QSP interleaves signal operators\ncorresponding to each variable with signal processing operators, which provides\nan efficient means to perform multivariable polynomial transformations.\nHowever, the necessary and sufficient condition for what types of polynomials\ncan be constructed by M-QSP is unknown. In this paper, we propose a classical\nalgorithm to determine whether a given pair of multivariable Laurent\npolynomials can be implemented by M-QSP, which returns True or False. As one of\nthe most important properties of this algorithm, it returning True is the\nnecessary and sufficient condition. The proposed classical algorithm runs in\npolynomial time in the number of variables and signal operators. Our algorithm\nalso provides a constructive method to select the necessary parameters for\nimplementing M-QSP. These findings offer valuable insights for identifying\npractical applications of M-QSP.", "comment": "20 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2410.02332v2", "cate": "quant-ph", "date": "2024-10-03", "updated": "2025-08-01"}
{"id": "2505.00292", "title": "Conformal changepoint localization", "authors": ["Sanjit Dandapanthula", "Aaditya Ramdas"], "categories": ["math.ST", "eess.SP", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Statistics Theory (math.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.00292v3", "summary": "Changepoint localization is the problem of estimating the index at which a\nchange occurred in the data generating distribution of an ordered list of data,\nor declaring that no change occurred. We present the broadly applicable CONCH\n(CONformal CHangepoint localization) algorithm, which uses a matrix of\nconformal p-values to produce a confidence interval for a (single) changepoint\nunder the mild assumption that the pre-change and post-change distributions are\neach exchangeable. We exemplify the CONCH algorithm on a variety of synthetic\nand real-world datasets, including using black-box pre-trained classifiers to\ndetect changes in sequences of images, text, and accelerometer data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.00292v3", "cate": "math.ST", "date": "2025-05-01", "updated": "2025-08-01"}
{"id": "2508.00200", "title": "Predicting Formula 1 Race Outcomes: Decomposing the Roles of Drivers and Constructors through Linear Modeling", "authors": ["Saurabh Rane"], "categories": ["stat.AP"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "Comments:      26 pages, 12 figures, 9 tables", "url": "http://arxiv.org/abs/2508.00200v1", "summary": "Formula 1 performance is a combination of the car's ability and the driver's\nability. While a given race or season can tell you how well a car and driver\nperformed jointly, isolating the individual impact of the driver and\nconstructor remains challenging. This paper extends a Regularized Adjusted Plus\nMinus (RAPM) methodology (Sill 2010), commonly used in basketball and hockey,\nto parse out individual driver and constructor impact. It employs a\ntime-decayed ridge regression with LOESS (Jacoby 2000) smoothing to predict\nrace results for the Hybrid Engine Era (2014 - 2024). By measuring the\nconstructor and driver coefficients over time, we measure the relative\nindividual impact of driver and constructor throughout the period. Results show\nthat constructors explain 64.0% of the variance in race outcomes in the Hybrid\nEngine Era. Additionally, constructors have increased importance in benchmarked\nrank-agnostic cohorts (e.g., Top 10 points finishers) and decreased importance\nin qualifying. By decomposing performance into individual driver and\nconstructor metrics, we create a robust framework for inter-constructor driver\ncomparisons that the Formula 1 points system obfuscates. Our work enhances the\nunderstanding of driver and constructor contributions to race success, offering\nvaluable insights for strategic decision-making in Formula 1.", "comment": "26 pages, 12 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2508.00200v1", "cate": "stat.AP", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2508.00176", "title": "New Pilot-Study Design in Functional Data Analysis", "authors": ["Ping-Han Huang", "Ming-Hung Kao"], "categories": ["stat.ME", "stat.AP"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2508.00176v1", "summary": "Efficient data collection is essential in applied studies where frequent\nmeasurements are costly, time-consuming, or burdensome. This challenge is\nespecially pronounced in functional data settings, where each subject is\nobserved at only a few time points due to practical constraints. Most existing\ndesign approaches focus on selecting optimal time points for individual\nsubjects, typically relying on model parameters estimated from a pilot study.\nHowever, the design of the pilot study itself has received limited attention.\nWe propose a framework for constructing pilot-study designs that support both\naccurate trajectory recovery and effective planning of future designs. A search\nalgorithm is developed to generate such high-quality pilot-study designs.\nSimulation studies and a real data application demonstrate that our approach\noutperforms commonly used alternatives, highlighting its value in\nresource-limited settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2508.00176v1", "cate": "stat.ME", "date": "2025-07-31", "updated": "2025-07-31"}
{"id": "2408.13392", "title": "A Multivariate Space-Time Dynamic Model for Characterizing the Atmospheric Impacts Following the Mt Pinatubo Eruptio", "authors": ["Robert Garrett", "Lyndsay Shand", "J. Gabriel Huerta"], "categories": ["stat.AP"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.13392v2", "summary": "The June 1991 Mt. Pinatubo eruption resulted in a massive increase of sulfate\naerosols in the atmosphere, absorbing radiation and leading to global changes\nin surface and stratospheric temperatures. A volcanic eruption of this\nmagnitude serves as a natural analog for stratospheric aerosol injection, a\nproposed solar radiation modification method to combat a warming climate. The\nimpacts of such an event are multifaceted and region-specific. Our goal is to\ncharacterize the multivariate and dynamic nature of the atmospheric impacts\nfollowing the Mt. Pinatubo eruption. We developed a multivariate space-time\ndynamic linear model to understand the full extent of the spatially- and\ntemporally-varying impacts. Specifically, spatial variation is modeled using a\nflexible set of basis functions for which the basis coefficients are allowed to\nvary in time through a vector autoregressive (VAR) structure. This novel model\nis caste in a Dynamic Linear Model (DLM) framework and estimated via a\ncustomized MCMC approach. We demonstrate how the model quantifies the\nrelationships between key atmospheric parameters prior to and following the Mt.\nPinatubo eruption with reanalysis data from MERRA-2 and highlight when such\nmodel is advantageous over univariate models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.13392v2", "cate": "stat.AP", "date": "2024-08-23", "updated": "2025-08-01"}
{"id": "2504.21688", "title": "Assessing Racial Disparities in Healthcare Expenditures via Mediator Distribution Shifts", "authors": ["Xiaxian Ou", "Xinwei He", "David Benkeser", "Razieh Nabi"], "categories": ["stat.AP", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Applications (stat.AP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.21688v2", "summary": "Racial disparities in healthcare expenditures are well-documented, yet the\nunderlying drivers remain complex and require further investigation. This study\ndevelops a framework for decomposing such disparities through shifts in the\ndistributions of mediating variables, rather than treating race itself as a\nmanipulable exposure. We define disparities as differences in\ncovariate-adjusted outcome distributions across racial groups, and decompose\nthe total disparity into two components: one attributable to differences in\nmediator distributions, and another residual component that would remain even\nafter equalizing these distributions. Using data from the Medical Expenditures\nPanel Survey, we examine the extent to which expenditure disparities would\npersist or be reduced if mediators such as socioeconomic status, insurance\naccess, health behaviors, or health status were equalized across racial groups.\nTo ensure valid inference, we derive asymptotically linear estimators based on\ninfluence-function techniques and flexible machine learning tools, including\nsuper learners and a two-part model designed for the zero-inflated,\nright-skewed nature of expenditure data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.21688v2", "cate": "stat.AP", "date": "2025-04-30", "updated": "2025-08-01"}
{"id": "2507.23392", "title": "Volatility Modeling with Rough Paths: A Signature-Based Alternative to Classical Expansions", "authors": ["Elisa Alòs", "Òscar Burés", "Rafael de Santiago", "Josep Vives"], "categories": ["q-fin.MF", "math.PR", "60L70, 60H10, 91G20, 91G60, 60G22"], "primary_category": "Subjects:       Mathematical Finance (q-fin.MF)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.23392v2", "summary": "We compare two methodologies for calibrating implied volatility surfaces: a\nsecond-order asymptotic expansion method derived via Malliavin calculus, and a\ndata-driven approach based on path signatures from rough path theory. The\nformer, developed in Al\\`os et al. (2015), yields efficient and accurate\ncalibration formulas under the assumption that the asset price follows a\nHeston-type stochastic volatility model. The latter models volatility as a\nlinear functional of the signature of a primary stochastic process, enabling a\nflexible approximation without requiring a specific parametric form.\n  Our numerical experiments show that the signature-based method achieves\ncalibration accuracy comparable to the asymptotic approach when the true\ndynamics are Heston. We then test the model in a more general setting where the\nasset follows a rough Bergomi volatility process-a regime beyond the scope of\nthe asymptotic expansion-and show that the signature approach continues to\ndeliver accurate results. These findings highlight the model-independence,\nrobustness and adaptability of signature-based calibration methods in settings\nwhere volatility exhibits rough or non-Markovian features.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.23392v2", "cate": "q-fin.MF", "date": "2025-07-31", "updated": "2025-08-01"}
{"id": "2309.12014", "title": "Singular Control in a Cash Management Model with Ambiguity", "authors": ["Arnon Archankul", "Giorgio Ferrari", "Tobias Hellmann", "Jacco J. J. Thijssen"], "categories": ["q-fin.RM", "math.OC", "q-fin.MF"], "primary_category": "Subjects:       Risk Management (q-fin.RM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2309.12014v2", "summary": "We consider a singular control model of cash reserve management, driven by a\ndiffusion under ambiguity. The manager is assumed to have maxmin preferences\nover a set of priors characterized by $\\kappa$-ignorance. A verification\ntheorem is established to determine the firm's cost function and the optimal\ncash policy; the latter taking the form of a control barrier policy. In a model\ndriven by arithmetic Brownian motion, we use Dynkin games to show that an\nincrease in ambiguity leads to higher expected costs under the worst-case prior\nand a narrower inaction region. The latter effect can be used to provide an\nambiguity-driven explanation for observed cash management behavior. Our\nfindings can be applied to broader applications of singular control in managing\ninventories under ambiguity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2309.12014v2", "cate": "q-fin.RM", "date": "2023-09-21", "updated": "2025-08-01"}
